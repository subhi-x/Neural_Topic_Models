RESEARCH Open AccessA novel method to identify pre-microRNAin various species knowledge base onvarious speciesTianyi Zhao1, Ningyi Zhang1, Ying Zhang2, Jun Ren3, Peigang Xu1, Zhiyan Liu1, Liang Cheng4* and Yang Hu3*From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: More than 1/3 of human genes are regulated by microRNAs. The identification of microRNA (miRNA)is the precondition of discovering the regulatory mechanism of miRNA and developing the cure for geneticdiseases. The traditional identification method is biological experiment, but it has the defects of long period,high cost, and missing the miRNAs that but also many other algorithms only exist in a specific period or lowexpression level. Therefore, to overcome these defects, machine learning method is applied to identify miRNAs.Results: In this study, for identifying real and pseudo miRNAs and classifying different species, we extracted98 dimensional features based on the primary and secondary structure, then we proposed the BP-Adaboostmethod to figure out the overfitting phenomenon of BP neural network by constructing multiple BP neuralnetwork classifiers and distributed weights to these classifiers. The novel method we proposed, from the 4evaluation terms, have achieved greatly improvement on the effect of identifying true pre-RNA compared toother methods. And from the respect of identifying species of pre-RNA, the novel method achieved moreaccuracy than other algorithms.Conclusions: The BP-Adaboost method has achieved more than 98% accuracy in identifying real and pseudomiRNAs. It is much higher than not only BP but also many other algorithms. In the second experiment, restricted bythe data, the algorithm could not get high accuracy in identifying 7 species, but also better than other algorithms.Keywords: Pre-miRNA identification, BP neural network, AdaboostBackgroundMicroRNAs(miRNAs) are a class of small single-strandand non-coding RNA molecules of approximately 22 nu-cleotides in length, miRNAs play important roles in manybiological process including affecting stability, metabolism,signal translation, disease development and translation ofmRNAs [1]. Meanwhile, miRNAs are also very importantin the treatment of diseases, such as: cancer [2], Xchromosomal defects [3], DiGeorge disease [4], etc. As thedevelopment of science and technology, people pay moreand more attention to miRNA research, amount of novelmiRNAs are discovered, the number and functional fea-tures are far beyond our imagination [5, 6]. The mainchallenge of studying miRNAs is how to find miRNAs andthe action sites, at present the main methods for identi-fying miRNAs are cDNA clone and sequencing andcomputational prediction, the expression of cDNA se-quencing method is low and costs amount of time andfunding [712]. Therefore, computational method aremore prevalent, several algorithms have been proposedto detect pre-miRNAs, the main challenge is to dis-criminate the real pre-miRNAs from the pseudo onesand identify novel miRNAs.* Correspondence: liangcheng@hrbmu.edu.cn; huyang@hit.edu.cn4College of Bioinformatics Science and Technology, Harbin MedicalUniversity, Harbin 150001, China3School of Life Science and Technology, Harbin Institute of Technology,Harbin 150001, Peoples Republic of ChinaFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30DOI 10.1186/s13326-017-0143-zRecently, for miRNAs identification, machine learningtechniques have been widely used. Sequence compositionand structural conformation features are applied to trainthe learning system, then the classifiers employ multiplefeatures to obtain the final prediction. Xue et al. discov-ered the significant difference of local contiguous sub-sequence between real and pseudo miRNAs. Therefore,they applied the three-character group local structure-sequence features to describe the samples, and based onSVM they proposed the triplet-SVM to identify novelmiRNAs and miRNAs from specific species [13]. Zhao etal. employed parallel triplet local structure-sequence fea-ture, however they chose the first nucleotide of the con-tiguous triplet group as the local structure-sequencefeature, and add two MFE related features and twonucleotide pairing features, then apply the SVM classifierPMirP [14]. Jiang et al. added MFE and P-value as the fea-tures based on the feature set in [13], and proposed theclassifier Mipred based on RF method [15], Limin Jiang. et[16] applied BP neural network to identify real and pseudopre-miRNAs, and proved the superiority of BP neural net-work by comparing with triplet-SVM?RF methods.Neural network [17] and other classifiers of data driv-ing tend to occur overfitting phenomenon. BP neuralnetwork is a widely used classification algorithm, it hasstrong self-learning ability and is particularly suitable forsolving internal mechanism problems. However, thealgorithm tends to be overfitting and the output is un-stable. The boosting algorithm [18] integrates multipleweak classifiers to obtain a strong classifier and avoidoverfitting phenomenon. Freund [19] promoted theboosting algorithm to Adaboost(adaptive boosting) sothat the new algorithm can be more suitable for practicalapplications.Therefore, in this study, we proposed BP-Adaboostalgorithm to establish multiple BP neural network classi-fiers and distribute the weights of classifiers throughAdaboost framework. Eventually a strong classifier withhigh accuracy is obtained.MethodsFeature extractionN-gram frequencyIn the recent years, for pre-miRNA identifying, studies haveshown that the local primary sequence is crucial to the pre-miRNA sequence [20]. Therefore, the n-gram frequency isthe most commonly used feature in the primary sequencefeature selecting [21, 22]. However, there is still no exactcriteria for choosing the value of n. Thus, n is often chosenby comparing the effect of n-gram frequency with differentn-values. In this study, we chose n as 3. Thus, for a cer-tain sequence, there are 64(43) combinations in atriple-nucleotide group, then we computed the frequencyoccurrence of these 64 combinations in the sequence.Energy characteristics featuresSome studies showed that the minimum free energy(MFE) indicates the stability of a secondary structure. Thereal pre-miRNA sequences have a lower minimum freeenergy than that of the randomly generated pre-miRNAsequences. Therefore, the minimum free energy of a pre-miRNA sequence is also considered as a feature in distin-guishing the pre-miRNA sequences. RNAfold is used tocompute the MFE value of a secondary structure.Structural-diversity based featuresThe base-pair of nucleotide in the sequences is also a re-markable characteristic in distinguishing real and pseudopre-miRNAs. The traditional nucleotide pairing are A-Upairing and C-G pairing, but in pre-miRNA sequencesthere are also other forms of nucleotide pairing, such asthe G-U pairing. Therefore, in this study, the G-Upairing is also included as one of the features.Triple structure sequenceTo highly specify the primary sequence features, the sec-ondary structure is also a significant feature. The softwareRNAfold is employed to calculate the potential secondarystructure. In the predicted secondary structure, there aretwo states for each nucleotide of the sequence, matchedor non-matched, indicated by brackets ( or ), and dots ..In this study, the two brackets are not distinguished,which means every ) is replaced by (. For any three nu-cleotides groups, there are 8 (2×3) possible characterscombinations, including (((, ((., (.(, (.., .((, .(., ..( and .Considering the first nucleotide of the three charactersgroup, then there are 32 (4 × 8) different combinations,which are denoted as A(((, U(.., etc. For a given sequence,the 32 dimensional feature vector is sufficient informationfor miRNA identification. Then the calculated 32D featureis employed to train the classifier.Using the feature extraction methods which wementioned above, we can extract 98D features from anypre-miRNA sequence in total.First we integrate the obtained pre-sequence into pri-mary sequence and secondary structure sequence. Forthe primary sequence, we choose the n-gram parame-ter(n = 3) and extracted 64 dimensional feature. Inaddition to calculating the potential structure, the soft-ware RNAfold is applied to predict the second structuresequence. In this structure, we extracted 32 dimensionalfeatures according to the triple structure sequence. Wealso extract the energy characterization MFE as a featureof pre-miRNA sequence. The possible nucleotide pairingG-U is included as the last feature. Therefore, altogetherwe extracted 98 features.The Flow chart of Feature extraction is illustratedin Fig. 1.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 54 of 79Methods and frameworkBP-AdaboostDue to BP neural network tends to be caught inoverfitting phenomenon and unstable output, in thisstudy, we proposed a new method BP-Adaboostbased on BP neural network. We employed BPneural network as a weak classifier to establish mul-tiple classification model by training repeatedly. Fi-nally, a strong classifier is obtained after adjustingthe weights through Adaboost.The framework is shown in Fig. 2.First, we establish N BP network classifiers by theextracted features and their corresponding labels. Whiletraining and establishing classifiers, each classifier willget a corresponding weight. In the end, we obtained astrong classifier by combining these N weight-distributedclassifiers.The construction of BP neural network classifierTo accomplish the construction of classifier, we alsoneed to set the various parameters besides the ob-tained features and the corresponding labels.First, we need to choose the number of nodes in thehidden layer, since there is no specific criterion atpresent, we choose the number through the empiricalformula as followed.M ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiN þ Lp þ a ð1ÞThe number of nodes(M) equals to a constant(a ?[1, 10]) plus the square root of the number of fea-ture dimensions(N) plus the output(L). In this study,N = 98, L = 1. From the formula above, in thisstudy, we choose M = 12.After setting the number of nodes (12) and hiddenlayers(3) of the BP neural network, the structure ofBP network is 9812-1. Then set the paramaters(Epochs, The learning rate, Error bounds) and func-tions(Performance function, Transfer function ofhidden layer nodes, Transfer function of outputnodes,The training function) of BP network. Theparameters and functions of BP network are listedin Table 1.Method processFor a given set of multiple classification training dataT = {(x1, y1), ? , (xN, yN)}, the input data xN ? X Rn,with an arbitrary integer label yN. First, initialize theweight distribution, the initial weight of each sampleis 1/N. Then train the sample to get the first classi-fier, and reduce the weight of the correct classifica-tion samples while raising the weight of improperclassification samples. By the statistics of the weightsFig. 1 Flow chart of Feature extractionFig. 2 Frame of BP-AdaboostThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 55 of 79of improper classification samples, the weights of cor-responding classifiers are obtained. Repeat the processabove, we can get multiple classifiers and the corre-sponding weights, then the ultimate strong classifierare obtained. The method process is as followed,ResultsData descriptionThe dataset of pre-miRNAs was downloaded from http://bioinf.sce.carleton.ca/SMIRP [23]. There are 7 speciessamples in the data set and each species has both a posi-tive sample set and a negative sample set.These 7 species are Anolis carolinensis?Arabidopsislyrata?Arabidopsis thaliana?Drosophila melanogas-ter?Drosophila pseudoobscura?Epstein barrvirus?Xeno-pus tropicalis. As each species has a negative sample set,altogether we obtain 8 classes of pre-miRNAs, one of themis pseudo pre-miRNAs. The total number of the gene se-quences of the whole data set is 12,846, among them 9264sequences are pseudo pre-miRNAs, and the rest 3582 ofthem are true pre-miRNAs.In this article, we distinguish the real pre-miRNAs fromthe pseudo ones before classifying these 7 species. V-foldcross-validation with moderate computational complexityis widely used for model selection. Usually, a value of Vbetween 5 and 10 is selected based on experience. In thisstudy, V = 10. First, the 12,846 sequences are randomlydivided into 10 groups, and choose 9 of them to be train-ing samples. The last one is tested as the testing set for atotal of 10 training times. The final statistical results areaveraged.Evaluation criteriaThe four kinds of prediction results are true positive(TP), false positive (FP), true negative (TN), and falsenegative (FN). Many evaluation indicators can be usedfor the classification results. First, the accuracy rate(ACC) is the proportion of the correct classification.Precision and recall are common used evaluation criteriain pattern recognition, precision represents the propor-tion of true positive samples of the classified positivesamples, and recall represents the proportion of cor-rectly classified positive samples of the whole positivesamples, specificity represents the proportion of the cor-rectly classified negative samples of the whole negativesamples, the computational formula is as follows,ACC ¼ TP þ TNTP þ FP þ TN þ FN ð2Þprecision ¼ TPTP þ FP ð3Þrecall ¼ TPTP þ FN ð4Þspecificity ¼ TNTN þ FP ð5ÞThe authentic classification of pre-miRNAsIn this study, the label of pseudo pre-miRNAs is 0,and the label of real pre-miRNAs is 1(for allspecies).Figure 3 shows the curves of the four error metrics for10 experiments, blue dot-solid line is the ACC curve,purple dotted line is precision curve, red solid line isTable 1 Parameters and functions of BP neural networkSetting items The value setEpochs 50The learning rate 0.1Performance function MSEError bounds 0.01Transfer function of hidden layer nodes TansigTransfer function of output nodes PurelinThe training function TrainlmThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 56 of 79recall curve, and black dash-dotted line is specificitycurve. It is observed that the fluctuation of the precisioncurve is relatively large, and the rest three curves aremore stable.The error statistics of the average results of 10 experi-ments are shown in the Table 2. The table shows thatBP-Adaboost algorithm is superior to other 4 algorithmsin these 4 accuracy assessment, and Naïve Bayes is theworst. The accuracy of BP-Adaboost algorithm reaches98.22%, and this represents the superiority and effective-ness of this novel method we proposed in distinguishingreal and pseudo mi-RNAs. The table also shows the ac-curacy of BP neural network is only second to BP-Adaboost algorithm, and thats the reason why wechoose the BP neural network combined with Adaboostalgorithm. Due to the randomness of BP network, byweighting multiple classifiers to obtain the final resultseffectively improves the classification accuracy andstability.Species classification of pre-miRNAsThe Fig. 4 shows the classification results from thestatistic of each classified real pre-miRNA sequence.There are 7 species in the graph, red bar showsthe true number of the species, blue bar shows thecorrectly classified number. From the graph we cantell the accuracy of the classification of some speciesare not ideal, the reason is the number of pseudopre-miRNA sequences is large. In the cross valid-ation, real pre-miRNA sequences of the samples arenot enough, so the number of real pre-miRNA se-quences is smaller after the classification. Therefore,the samples of some kind of species are more likelynot enough.The classification accuracy of each species is shown asTable 3.It can be seen from the table that the accuracy of BP-Adaboost algorithm is superior to other algorithms, al-though the accuracy of the rest 4 algorithms is higher insome specific species, the accuracy of the algorithm inthis study is the highest in total.DiscussionsIn this paper, we proposed a new method to identify thepre-miRNAs. We use the Adabost algorithm to generateten BP classifiers to finish the identification. It can pro-vide a new thinking of solving the problem of miRNAsidentification. We use several original algorithms tocompare with our method, and found that our methodcan achieve better performance than them. Although themethod can fully play the generalization of BP, and makethe whole method hardly over-fit, it still has the prob-lems such as: its performance is not ideal in solvingmulti-classification problem with imbalanced samples,its training time is longer than normal BP algorithm. Inthe respect of experiment, the method should be testedin many other data sets to verify the effectiveness ofBP-Ababoost.Fig. 3 Results of 4 error standards of ten experimentsTable 2 Comparison of the BP-Adaboost with alternativemodelsAlgorithm ACC Precision Recall SpecificityBP-Adaboost 0.9822 0.9576 0.9797 0.9830BP 0.9541 0.9429 0.9736 0.9800Random Forest 0.9336 0.9270 0.9744 0.9772Naïve Bayes 0.7026 0.4831 0.9721 0.5987SVM 0.8811 1 0.5729 1The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 57 of 79ConclusionsThe identification of miRNAs is significant for human tostudy its function and understand its network regulationmechanism, discovering more novel miRNAs can alsopromote the prediction of miRNA target genes and thedevelopment of new drugs. In this study, we proposed amethod combined BP neural network with Adaboost al-gorithm, it can effectively overcome the defects of un-stable output and overfitting phenomenon, our methodobtained a strong classifier by integrating multiple weekclassifiers (BP neural network classifiers) and distribut-ing the weights to them. The data set of traditional clas-sification of real and pseudo pre-miRNA sequencescombined the real and pseudo sequences of one speciestogether, in this study, we combined the real and pseudosequences of 7 different species together, which in-creased the diversity and difficulty of the classification.In the end, we obtained a high accuracy identificationresult of real and pseudo pre-miRNAs. Beyond that, inthis study we also classified 7 different species from pre-miRNAs which is the part that few people are payingattention to. Due to that, the sample data is not enough,though the accuracy of our classifier is higher than othermethods, but the overall classification result is still needto be proved. However, the method we proposed is stillable to provide guidance for the miRNA identification.AbbreviationsACC: Accuracy rate; BP-Adaboost: Back Propagation neural network fusedwith Adaboost algorithm; FN: False Negative; FP: False Positive; miRNA: microRNA;MSE: Mean Square Error; RF: Random Forest algorithm; SVM: Support VectorMachine algorithm; TN: True Negative; TP: True PositiveAcknowledgmentsYang Hu, Zhiyan Liu and Liang Cheng are the corresponding author. TianyiZhao, Ningyi Zhang and Ying Zhang are the co-first author.FundingThis work was supported by the National Natural Science Foundation ofChina (No: 61,571,152 and 61,502,125, $2000), the National High-techR&D Program of China (863 Program, $2500) [Nos: 2014AA021505,2015AA020101, 2015AA020108], the National Science and TechnologyMajor Project [Nos: 2013ZX03005012 and 2016YFC1202302, $1000],Heilongjiang Postdoctoral Fund (Grant No. LBH-Z15179, $800), and ChinaPostdoctoral Science Foundation (Grant No. 2016 M590291, $500).Availability of data and materialsThe dataset of pre-miRNAs was downloaded from http://bioinf.sce.carleton.ca/SMIRPAbout this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsTZ and NZ implemented the first version of the BP-Adaboost. JR, PX, ZL, LCupdated the algorithm. YZ and YH wrote the manuscript. All authors readand approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Fig. 4 Results of 7 species classify by BP-AdaboostTable 3 Accuracys comparison of the BP-Adaboost withalternative models in 7 speciesSpecies BP-Adaboost BP RF Naïve Bayes SVMAnolis carolinensis 0.66 0.10 0.78 0.69 0.14Arabidopsis lyrata 0.39 0.25 0.53 0.21 0Arabidopsis thaliana 0.45 0.23 0.67 0.54 0Drosophila melanogaster 0.61 0.20 0.51 0.75 0.21Drosophila pseudoobscura 0.79 0.35 0.31 0.41 0.14Epstein barrvirus 0.42 0.26 0.24 0.06 0.10Xenopus tropicalis 0.68 0.43 0.45 0.07 0Total 0.57 0.29 0.51 0.30 0.22The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 58 of 79Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims in publishedmaps and institutional affiliations.Author details1Department of Computer Science and Technology, Harbin Institute ofTechnology, Harbin 150001, Peoples Republic of China. 2Department ofPharmacy, Heilongjiang Province Land Reclamation Headquarters GeneralHospital, Harbin 150088, China. 3School of Life Science and Technology,Harbin Institute of Technology, Harbin 150001, Peoples Republic of China.4College of Bioinformatics Science and Technology, Harbin MedicalUniversity, Harbin 150001, China.Published: 20 September 2017RESEARCH Open AccessDisease Compass a navigation system fordisease knowledge based on ontology andlinked data techniquesKouji Kozaki1* , Yuki Yamagata2, Riichiro Mizoguchi3, Takeshi Imai4 and Kazuhiko Ohe4AbstractBackground: Medical ontologies are expected to contribute to the effective use of medical information resourcesthat store considerable amount of data. In this study, we focused on disease ontology because the complicatedmechanisms of diseases are related to concepts across various medical domains. The authors developed a RiverFlow Model (RFM) of diseases, which captures diseases as the causal chains of abnormal states. It represents causesof diseases, disease progression, and downstream consequences of diseases, which is compliant with the intuitionof medical experts. In this paper, we discuss a fact repository for causal chains of disease based on the diseaseontology. It could be a valuable knowledge base for advanced medical information systems.Methods: We developed the fact repository for causal chains of diseases based on our disease ontology andabnormality ontology. This section summarizes these two ontologies. It is developed as linked data so thatinformation scientists can access it using SPARQL queries through an Resource Description Framework (RDF) modelfor causal chain of diseases.Results: We designed the RDF model as an implementation of the RFM for the fact repository based on theontological definitions of the RFM. 1554 diseases and 7080 abnormal states in six major clinical areas, which areextracted from the disease ontology, are published as linked data (RDF) with SPARQL endpoint (accessible API).Furthermore, the authors developed Disease Compass, a navigation system for disease knowledge. Disease Compasscan browse the causal chains of a disease and obtain related information, including abnormal states, through twoweb services that provide general information from linked data, such as DBpedia, and 3D anatomical images.Conclusions: Disease Compass can provide a complete picture of disease-associated processes in such a way thatfits with a clinicians understanding of diseases. Therefore, it supports user exploration of disease knowledge withaccess to pertinent information from a variety of sources.Keywords: Disease ontology, Definition of diseases, River flow model of disease, Linked data, Navigation systemBackgroundRecently, medical information resources that store consid-erable amount of data have become available. Semantictechnologies are expected to contribute to the effectiveuse of such information resources, and medical ontologiessuch as the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT, http://www.nlm.nih.gov/research/umls/Snomed/snomed_main.html main.html)and the Ontology of General Medical Sciences (OGMS)[1] have been developed to realize sophisticated medicalinformation systems. Although medical ontologies consistof various domains, such as diseases, anatomies, drugs,and clinical information, disease is a particularly import-ant concept because diseases have complicated mecha-nisms that are deeply related to concepts across manymedical domains. Therefore, in this study, we have fo-cused on a disease ontology.Although disease ontologies such as the Human DiseaseOntology (DOID) [2] and the Infectious Disease Ontology(IDO) [3] exist, they primarily focus on the ontological* Correspondence: kozaki@ei.sanken.osaka-u.ac.jp1The Institute of Scientific and Industrial Research, Osaka University, 8-1Mihogaoka, Ibaraki, Osaka 567-0047, JapanFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 DOI 10.1186/s13326-017-0132-2definition of a disease with related properties, i.e., staticaspects of diseases are the main concern. While theOGMS provides an ontological representation model ofdisease disposition, it does not capture a complete pictureof disease-associated processes.In contrast, we proposed a definition of a disease thatcaptures the causal chain of the abnormal states in acomputational model known as the River Flow Model(RFM) of diseases [4, 5]. Our disease ontology consistsof rich information about these causal chains, whichprovides domain-specific knowledge about diseases andanswers questions such as What disorder/abnormalstate causes a disease? or How might the disease ad-vance, and what symptoms may appear? Consequently,we believe that the ontology could be a valuable know-ledge base for advanced medical information systems.In this paper, we discuss a fact repository for causalchains of disease based on our disease ontology. It is de-veloped as linked data so that information scientistscan access it using friendly SPARQL queries throughan RDF model for causal chain of diseases. We de-signed the RDF model as an implementation of theRFM for the fact repository while ontological defini-tions of the RFM are discussed in our previous work[4, 5]. It provides known knowledge about mechanismof disease to support education for novice clinicians,differential diagnosis, decision making for medicaltreatment and so on.In this paper, we also describe Disease Compass, anavigation system for disease knowledge based on theRDF model of our disease ontology. The system has twospecial features. First, users can browse disease know-ledge according to the causal chains of diseases definedin the disease ontology. Second, users can obtain relatedinformation about the selected disease from linked datasources. Thus, Disease Compass helps users make senseof disease knowledge from various relevant sources.The remainder of this paper is organized as follows.The methods used to develop our disease ontologyand navigation system are introduced in Methods sec-tion. In Results section, we describe the disease ontol-ogy in detail and discuss how it can be published aslinked data and Disease Compass. In Discussion sec-tion, we discuss our contributions from the perspec-tive of the ontological definition of disease andmedical information systems. Conclusions and sugges-tions for future work are presented in Conclusionssection.This paper is an extended version of the conferencepaper presented in ICBO2015. It is mainly added the fol-lowing 3 topics; 1) details of integration of biomedicalabnormal states discussed in Integration of biomedicalabnormal states section, 2) details of development ofDisease Compass, especially about its navigationfunction for general causal chains, and 3) Some concreteexamples of how the system is used with discussions.MethodsWe developed the fact repository for causal chains ofdiseases based on our disease ontology [4, 5] and abnor-mality ontology [6, 7]. This section summarizes thesetwo ontologies.Definition of a DiseaseBasic definitionBased on the RFM, we define a disease as follows [4].Definition 1A disease is a dependent continuant constituted of oneor more causal chains of clinical disorders (abnormalstates) that appear in a human body and is initiated byat least one disorder.In this definition, by clinical disorders (abnormalstates), we mean states in a human body which consistsof causal chains of a disease. They are associated withdysfunctional or otherwise pathological functioning inorganisms while they may not be abnormal in somecases. They are defined in our Abnormality Ontology [7]as mentioned in Abnormality Ontology section.Then, what is a causal chain of disorders? Although itlooks like a process, it is a dependent continuant. It ispossible to compare a causal chain of disorders to awaterfall, river flow, or a forest fire. Here, we show howa disease is a dependent continuant rather than aprocess. The following is an informal account of ourview. This topic is extensively discussed with ontologicaldefinitions of related concepts in the literature [5].A causal chain is composed of one or more pairs ofentities, such as a causal event and an effect event, inwhich the latter has been caused by the former. In thecase of multiple-pair chains, the effect becomes anothercause that causes another effect. What makes clinicalcausal chains special is that causal entities are usuallystill active when the effect entity has been caused.Therefore, the two entities overlap in temporal space. Inthe case of continuant entities, by overlap we meanthat the intervals of active states of neighboring continu-ants overlap, i.e., the causal continuant maintains itsstate when the effect state has been caused.Let us examine how well a flowing river matches acausal chain of a disease. The river itself enacts branch-ing, changing shape, extending, diminishing, etc. A rivercould be created when a lake overflows, e.g., after aheavy rainstorm. Initially, the flow is minimal and,potentially, temporary. Here, overflow from a lake wouldcorrespond to an etiological disorder in a clinical causalchain. If the initial flow increases, the water extends inlength and is recognized as a river. After emerging as aKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 2 of 18river (as a disease), it extends further to another lake orto the sea. While extending, it branches (branching canbe the appearance of another disorder). Eventually, theriver may dry up due to climate change (cure). Thus, thelife of a river corresponds well to the life of a disease.Note that a river is defined as an enactor of those pro-cesses, and Definition 1 suggests that a disease is definedas an enactor of its manifestation process. Thus, in con-cordance with OGMS, both a river and a disease arecontinuants; however, a river is an independent continuantand a disease (causal chain) is a dependent continuant thatdepends on a bearer, i.e., an organism.This informal observation is supported by ontologicalaccounts of processes and objects [8]. Although we omitdetails because of space limitations, we present the ana-logy to support the definition of a disease as a dependententity of a new type that differs from both a dispositionand a process.GranularityWe do not specify any particular granularity of disorderand causal chains because we believe granularity shouldbe determined flexibly according to the necessity of de-scription of each disease. That is, we define diseasesbased on the most agreeable medical knowledge at thistime because the current medical knowledge changesas time goes. However, with regard to the originalcause, we should trace the causal chain back to thecell level rather than to the genome level. When wedefine diseases generally, granularity is not an issue;however, it matters when we define a particular dis-ease in the ontology.In addition, we do not impose specific time resolutionon the causal processes, so that, if necessary, we can in-clude rapid processes, such as fractures. After receiving astrong external pressure, a bone undergoes a very quickdestruction process resulting in fracture. The causalprocess can be captured by much finer time resolutionthan those involved in ordinary pathological processescaptured at the clinical level. Fractures can be handled bythe disease model discussed in the next section.Related workHere we compare the definitions of diseases in theOGMS and RFM.(1)Dispositions are introduced in the course of diseasedevelopment in the human body. A disposition is apotentiality. In the current OGMS, realization of thispotentiality takes the form of chains of physical/physiological changes. Thus, disease and diseasecourse are distinguished, that is, the former isdependent continuant and the latter a process. Webelieve this use of disease is counterintuitive toclinicians; thus, we propose a disease definition thatallows the disease to be understood as a causal chainof abnormal states.(2)Consider how a particular disease is identified. Forexample, when explaining diabetes, OGMS refers to anelevated level of glucose in the blood. However, itprovides an insufficient account of why the explanationof diabetes must include elevated level of glucose.What role does this elevated level play in diabetes?Why must elevated level of glucose in the blood beincluded for diabetes but nothing else? It must besomething specific to the disease of interest, i.e., eachrealization of the disease must involve an entity of thissort. For OGMS, emphasis is placed on the dispositionand the disorder (a certain disordered body part) inwhich this disposition inheres. We believe that thereference to elevated glucose level suggests a need foran additional entity, which is included in our diseasemodel. Thus, we introduce the notion of core causalchain, which roughly corresponds to so-called mainpathological/etiological condition(s).We know that defining such an entity type, i.e., causalchains, discussed in (2) is difficult because such causalchains are not always definite for each disease becausethey vary from one patient to another. Hence, OGMS useof disposition is a mere potentiality. In the case of latentdiabetes, for example, there is no elevated level of glucosein the blood, although there is a disposition thereto. Ac-cordingly, for latent diabetes, we follow OGMS in recog-nizing the need for something other than just elevatedlevel of glucose in the blood. However, we think thatsomething more is requiredsomething that is essentialfor each particular disease. In the case of diabetes, thiswould be the deficiency of functioning of insulin, becausethis must have occurred for all patients who suffer fromdiabetes. To address this issue, we draw on OGMS notionof homeostasis and introduce the term disturbance ofhomeostasis to explain what we consider as the essentialcore of each disease. Disturbance of homeostasis can becaused through the concretization of a disposition, or itcan be caused by some outside trigger, e.g., an injury.We agree with OGMS in that a disease is a dependentcontinuant, and its definition is expected to address the fol-lowing conditions: (1) the existence of its pre-clinical mani-festation, (2) the fact that it can cause another disease, and(3) variation in the disease course from patient to patient[1]. We have attempted to find a disease definition that sat-isfies these conditions using an RFM [4].Abnormality OntologyThree-Layer Ontological Model of Abnormal StatesHere we discuss the abnormal states used in our diseaseontology to define diseases. The reliability and utility ofKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 3 of 18the disease definitions are considerably dependent onthe quality of the abnormal states. To develop abnormalstates consistently, we have developed an abnormalityontology [6, 7] with a three-layer structure: Level 1: Generic abnormal statesLevel 1 defines very fundamental (or generic)concepts, which do not depend on any structuralentity, i.e., object independent states. They arecommonly found in several objects, and can beusable in several domains besides medicine, such asmachinery, materials, and aviation. Level 2: Object-dependent abnormal statesLevel 2 has been developed by identifying the targetobject and specializing generic abnormal states atLevel 1 with consistency. The top level concepts atLevel 2 are dependent on generic structures, such aswall-type structure, tubular structure, andbursiform structure, which are common and areused in several domains. Level 3: Specific context-dependent abnormal statesLevel 3 consists of context-dependent abnormalstates, which refer to the Level 2 abnormal states todefine them, and are specialized into specificdisease-dependent ones.Level 1 defines very fundamental and generic con-cepts, e.g., small in area, hypofunction, etc., whichare commonly used in clinical medicine and other do-mains. Therefore, they do not have a target entity whichhas the abnormal state. Level 2 concepts are dependenton objects. In the lower level of the tree, concepts aredesigned to represent abnormalities at specific humanorgan/tissue/cell levels. For example, by specifying smallin area at Level 1, tube narrowing, where the cross-sectional area of a tubular structure has become nar-rowed, is defined at Level 2. This is further specified in thedefinitions vascular stenosis (blood vessel-dependent),arterial stenosis, coronary artery stenosis (coronaryartery-dependent), and so on. Level 3 concepts are cap-tured as specific disease-dependent (context-dependent)abnormal states. For example, coronary artery stenosisat Level 2 is defined as a constituent of ischemic heartdisease at Level 3. In the proposed ontological ap-proach, common concepts can be kept distinct fromspecific concepts and can be defined appropriately ac-cording to their context.Representation of Abnormal StateIn medicine, abnormal states are interpreted from the di-verse perspectives of specialists, such as clinicians, pathol-ogists, biologists, and geneticists, and correspondingly avariety of representations of abnormal states are used.Therefore, we have classified abnormal states into threecategories: a property (e.g., hypertension), a qualitativerepresentation (e.g., blood pressure is high), and a quanti-tative representation (e.g., blood pressure 180 mm Hg).Their interdependence is formulated in a Property-Attribute interoperable representation framework forabnormal states we proposed in previous work [6, 7].We capture all abnormal states as properties repre-sented by a tuple: <Property (P), Property Value (Vp)>,e.g., <stenosis, true>. Apparently, any state requires tem-poral specification as well as its bearer. For simplicity,these temporal indexes are omitted. However, the beareris specified to represent the fact that it is in a state, as dis-cussed below. We specify the property by decomposing itinto a tuple: <Attribute (A), Attribute Value (V)>. TheAttribute Value can be either a Qualitative Value (Vql) ora Quantitative Value (Vqt). For example, arterial stenosisis decomposed into < cross-sectional area (A), small(Vql) > as a qualitative representation, or < cross-sectionalarea (A), 5 mm2 (Vqt) > as a quantitative representation.Then, we introduce Object to identify the targetobject, and we represent an abnormal state as a triple:<Object (O), Attribute (A), Value (V)>. This is the basicform of abnormalities in our representation model. Inaddition, we introduce Sub-Object (SO) as an ad-vanced representation of what will be focused on. Forexample, in the case of hyperglycemia, since the glu-cose concentration (A) means the ratio of the focusedobject (SO) relative to the whole mixture (O), the repre-sentation of hyperglycemia is a quadruple, <blood (O),glucose (SO), concentration (A), high (V)>. In an ad-vanced representation, colonic polyposis is describedas < colon (O), polyp (SO), number (A), many (V) > .Our model can deal with both clinical test data andabnormal states in disease definitions. Clinical test datacan be represented in the form <Object (O), Attribute(A), Quantitative Value (Vqt) > (OAVqt), which can beconverted into a property representation form <Object(O), Property (A), Property Value (Vp) > (OPVp) via aqualitative representation form. For example, in terms ofthe state of hypertension, our model ensures interoper-ability among the forms < blood (O), pressure (A),180 mmHg (Vqt)>, <blood pressure, high>, and < hyper-tension, true>. Therefore, our model realizes interoper-ability between test data and abnormal states in thedefinition of diseases. The ontological foundation for theconcepts discussed thus far is given by an upper ontol-ogy, i.e., YAMATO [9].Ontology EditingWe used the Hozo (http://www.hozo.jp) [10] ontologyediting tool. Hozo is based on an ontological theory ofrole [11] and has a sophisticated graphical user interface.Although Hozo uses a proprietary ontology format basedon XML, it can export ontologies in Web OntologyKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 4 of 18Language (OWL) [12]. An API for use the Hozo formatis also available at the website of Hozo. We also devel-oped a graphical tool that allows clinicians to edit a dis-ease definition intuitively without prior knowledge ofontology construction. The tool can export disease defi-nitions in the Hozo ontology format.System Development based on Linked Data TechnologyThere are several approaches for system developmentbased on ontologies. A typical approach is to use APIsfor ontology processing. Because our disease ontology isconstructed using Hozo, we can develop application sys-tems using APIs for Hozo ontologies. We can also usethe OWL API because Hozo has an OWL export func-tion. However, linked data technology is particularly effi-cient for developing applications across multiple datasetson the web. Therefore, we adopted an alternative ap-proach to publish the disease ontology as linked data sothat it can be used to develop an application system easily.At the same time, the schema of RFM is published athttp://rfm.hozo.jp/ the Hozo format and the OWL format.If the users are familiar with OWL, they can use the OWLversion in spite of our disease ontology is currently pub-lished only as Linked Data through its SPARQL endpoint(see Disease ontology as linked data section).ResultsComputational Model of DiseasesCore causal chain of a diseaseBased on the disease ontology based on RFM, we build acomputational model of diseases to make it easier todefine particular diseases. In the following, we dividediseases into (Type 1) those whose etiological andpathological processes are well understood and (Type 2)other diseases.Type 1 diseases are identified by their inherent etio-logical/pathological process(es). Type 2 diseases includeso-called syndromes and are typically represented interms of criteria for diagnosis. We deal with Type 1 dis-eases first. Note that every Type 1 disease should have aclue to identify the disease. In other words, we shouldbe able to find something similar to the so-called mainpathological/etiological condition(s) that theoreticallycharacterize(s) the disease. As stated above, this is whatOGMS should include. We know that Type 2 diseasesnecessarily employ criteria for diagnosis to identify thembecause of a lack of knowledge about their etiological/pathological processes. However, this does not meanType 2 disease is excluded from our disease model asdiscussed below, which we share with OGMS.We also need a formulation to organize diseases in anis-a hierarchy in a disease model. According to our def-inition, a disease can be represented as a directed graphconsisting of disorders as nodes and causal links. An is-arelation between diseases using an inclusion relationshipbetween causal chains can be described as follows.Definition 2: Is-a relation between diseasesDisease A is a super class of disease B if all causal chainsat the class level of disease A are included in those ofdisease B. The inclusion of nodes (disorders) is deter-mined by considering an is-a relation between thenodes, as well as the sameness of nodes.Definition 3: Core causal chain of a diseaseThe causal chain of a disease included in the chains ofall its subclass diseases is called the core causal chain.Definition 3 helps us capture the necessary and suffi-cient conditions of a particular disease systematically,which roughly corresponds to the so-called main patho-logical/etiological conditions. Figure 1 shows one of themain types of diabetes constituted by correspondingtypes of causal chains. The most generic type in this ex-ample is (non-latent) diabetes, which is constituted bythe following chain:deficiency of insulin? elevated level of glucose in theblood.The next lower subclasses include Type-I diabetes,which is constituted by:destruction of pancreatic beta cells? lack of insulin I inthe blood? deficiency of insulin? elevated level ofglucose in the blood,and steroid diabetes, which is constituted by:long-term steroid treatment?? deficiency ofinsulin? elevated level of glucose in the blood.If a doctor wanted a hierarchy that represents diabetes-caused blindness, it would be:deficiency of insulin? elevated level of glucose in theblood?? loss of sight.Although we explain the disease model using Type 1diseases, the model is also applicable to Type 2 diseasesbecause of the flexibility of granularity and degree ofbeing well-understood. These two kinds of flexibilitycan be exploited according to each disease. In the caseof Type 2 diseases, we could employ an unknowncausal node linking to just a few of the symptoms thatare typically observed in the syndrome under consider-ation. Note that this model can capture a seemingly iso-lated symptom by combining it with an unknown causeKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 5 of 18to form a causal network. It also captures diseases withmultiple causal chains.In addition, the proposed model can distinguish, for ex-ample, between diabetes with blindness and diabetes-driven blindness by specifying the core causal chain offocus. In summary, the disease model yielded by the pro-posed definition of disease (Definition 1, Disease ontologyas linked data section) covers a wide range of diseases. Infact, we have constructed models of 6051 diseases from 12different divisions in our ontology, which shows the ex-pressive power of the proposed disease model.Types of causal chains in disease definitionsIn theory, when we define a disease, we can considerthree types of causal chains that appear in the definitionof disease.General Causal Chains are all possible causal chainsof (abnormal) states in a human body. They can bereferred to by any disease definition.The Core Causal Chain is a causal chain that appearsin all patients that have the disease.Derived Causal Chains are causal chains that are ob-tained by tracing general disease chains upstream ordownstream from the core causal chain. Upstream chainsimply possible causes of the disease, whereas downstreamchains imply possible symptoms in a patient sufferingfrom the disease.The core causal chain represents a stable definition ofthe disease. That is, it defines only such causal chainsthat appear in all patients. On the other hand, the gen-eral causal chains and derived causal chains are not partof the definition but possible causal chains which mightnot appear in some patients. That is, the general causalchains and derived causal chains represents possibilitieshow causal chains could be extended.Figure 1 shows how the main types of diabetes arecomposed of corresponding types of causal chains. Thefigure shows that diabetes subtypes are defined byextending the diseases core causal chain according to itsderived causal chains (upstream or downstream).Note that it is obviously difficult to define all generalcausal chains in advance, because it is impossible toknow all possible states in the human body and theircausal relationships. To overcome this difficulty, we de-fine general causal chains by generalizing the core/de-rived causal chains of every disease defined by cliniciansusing a bottom-up approach. We asked clinicians todefine only core causal chains and typical derived causalchains of each disease according to their existing know-ledge and information that can be found in textbooks.General causal chains are then defined by generalizingthese definitions.The scope of the model for diseaseIn this section, we discuss the scope of the proposedmodel of diseases. This paper focuses on how to capturediseases and the implementation of diseases as causalchains based on the RFM. As mentioned in Definition ofa Disease section, we assume that ontological definitionsof causal chains, diseases, and abnormalities (disorders)etc. are out of the scope of this paper since they are dis-cussed in our previous papers [4, 5].At first, we have to distinguish the following two prob-lems when we represent a disease.1) How we can represent causal chains of diseases as acomputational model2) How far the current medical knowledge reveals thecausal mechanism of diseases?Fig. 1 Types of diabetes composed of causal chainsKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 6 of 18This paper focuses on the former and the latter prob-lem is out of scope. That is, we discuss how the modelcapture the current medical knowledge about diseasesbased on the RFM.We here classify difficulties for defining diseases ascausal chains into the following two types;1) Problems of grain size to describe causal chains.2) How far we can follow causes of diseases.For the former, we use the Abnormality Ontology weproposed in our previous work [7]. It supports multilevels representations of abnormal states according totheir grain size.For the latter, we carefully investigated how we shouldrepresent causal chains of diseases through discussionswith medical experts including clinicians. As the result,we introduced a flexible representation model which wecan describe causal chains according to a range of givenknowledge even if some causes of the disease are un-known. When causes of an abnormal state is unknown,we represent it unknown node in the causal chain. Itmeans that the model can represent causal chainsaccording to how far we can follow causes of diseases.On the other hand, when we know that an abnormalstate has several causes, we can also represent it usingmultiple chains. Please note that multiple chains can berepresented using AND/OR graphs which is a simpleand basic knowledge representation. Although represen-tations of AND/OR in OWL are somewhat complicated,this is why we publish causal chains of diseases as notOWL but simple RDF graph as discussed in Diseaseontology as linked data section.Implementing the Disease OntologyWe developed the disease ontology using Hozo.Although Hozo is based on an ontological theory ofroles and has its own ontology representation model, weshow an OWL representation of the ontology to aidunderstandability. Note that we use a simplified OWLrepresentation of the disease ontology to provide anoverview; however, it does not support the full semanticsof Hozo. The detailed semantics of Hozo are discussedin the literature [12].Figure 2 shows an OWL representation of angina pec-toris, whose causal chain is shown in Fig. 3. Abnormalstates that appear in the disease are listed using theowl:Restriction properties on the hasCoreState/hasDerives-State properties. The former represents abnormal states inits core causal chain, and the latter represents those in itsderived causal chain as defined by a clinician. The causalrelationships among them are represented by hasCause/hasResult properties. If the probability of the causal rela-tionship is high, hasProbableCause/hasProbableResultproperties are used instead. However, how this probabilityis determined is beyond the scope of this paper. Causalchains (states and causal relationships among them) incore causal chains are necessary (Definition 3); therefore,the owl:someValuesFrom properties are used. On the otherhand, because causal chains in derived causal chains arepossible, owl:allValuesFrom properties are used to repre-sent the possible causes/results. If there are more thantwo possible causes/results, owl:unionOf is used to listthem. The definitions of diseases refer to the definitions ofabnormal states, which represent the possible causesand results, as shown in Fig. 4. General disease chainsare represented as an aggregation of the definitions ofabnormal states.Our disease ontology has been developed in collabor-ation with clinicians from 13 fields, such as cardiology,Fig. 2 Class definition of angina pectoris in OWLFig. 3 Causal chain of angina pectorisKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 7 of 18neurosurgery, and allergology. As of May 11, 2013, it con-tained approximately 6302 disease concepts and 21,669disorder (abnormal state) concepts with the causal rela-tionships that exist among them. We keep to revise themmainly focusing on major diseases.Disease ontology as linked dataBasic policy to publish the disease ontologies as linked dataThe standard format for linked data is RDF; thus, onemight consider it easy to publish ontologies in RDFformats using OWL or RDF(S) as linked data. However,ontology languages, such as OWL, are primarily designedfor class descriptions, and there is an assumption that thelanguage will be used for reasoning based on logic. Incontrast, for linked data, finding and tracing connectionsbetween instances is the primary task. Therefore, for con-venience or efficiency, OWL and RDF(S) are not alwaysappropriate for linked data because of their complicatedgraph structures.For example, when we obtain a general disease chain,which is probably caused by myocardial_ischemia, wemust repeat SPARQL queries to obtain RDF graphs,which include blank nodes, such as those shown in Fig. 5.Furthermore, when we obtain the definitions of adisease, we must perform more complicated queries toobtain graphs that correspond to OWL descriptions,such as those shown in Fig. 3, with restrictions inheritedfrom the diseases super classes. These queries and graphpatterns are intuitively very different from the diseasechains we want to produce. It is due to restrictions fromthe super classes.This is problematic, especially when the conceptualstructures of the ontology are intended for use as aknowledge base with rich semantics. Consequently, wehave designed an RDF data model in order to publishour disease ontology as linked data [13].RDF model for causal chains of diseasesOnce the disease ontology was constructed, informationabout the causal chains of diseases was extracted andconverted into RDF format as linked data. We call thisdataset Disease Chain-LD, and it consists of diseases,abnormal states, and the relationships among them. Ab-normal states are represented by instances of the Abnor-mal_State type, and the causal relationships betweenthem are represented by hasCause and hasResult, whichare inverse properties. The abnormal states connected bythese properties are a possible cause/result; therefore, gen-eral disease chains can be obtained by collecting all abnor-mal states according to these connections.Diseases are represented by instances of Disease type.Abnormal states that constitute a core causal chain anda derived causal chain of a disease are represented byhasCoreState and hasDerivedState properties, respect-ively. Is-a (sub-class-of ) relationships between diseasesand abnormal states are represented by subDiseaseOf/subStateOf properties rather than rdfs:subClassOf be-cause the diseases and abnormal states are representedas RDF resources, whereas rdfs:subClassOf is a propertybetween rdfs:Classes.Figure 6 shows an example of an RDF representation ofdiseases. It represents disease A and its sub-disease diseaseB, whose causal chains are shown in Fig. 7. Note that thecausal chains consist of abnormal states and the causalrelationships among them. Therefore, when we obtain adiseases core causal chain or derived causal chain, wemust obtain both the abnormal states connected to thedisease by hasCoreState/hasDerivedState properties andthe causal relationships among them. Although causal re-lationships are described without determining whetherthey are included in the causal chains of certain diseases,we can identify the difference of abnormal states whichdiseases include by assessing whether the abnormal statesat both ends of hasCause/hasResult properties are con-nected to the same disease by hasCoreState/hasDerived-State properties. Furthermore, for a disease that has asuper disease, such as disease B in Fig. 7, in addition toobtaining the causal chain directly connected with theFig. 4 Class definition of myocardial ischemia in OWLFig. 5 RDF graph of a general disease chain in OWLKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 8 of 18disease, we must also obtain the causal chains of the superdisease, and these chains must be aggregated.We have published the disease ontology as linked databased on our RDF model. It includes the definitions of1.554 diseases and 7080 abnormal states in six major clin-ical areas, which were extracted from the disease ontologyon June 24, 2014. At this point, the dataset contains61,473 triples. Although the disease ontology includes thedefinitions of diseases in 13 clinical areas, we have pub-lished only those parts that were well reviewed by clini-cians. A SPARQL endpoint to access the disease ontologyis published at http://lodc.med-ontology.jp/. The userscan find concrete examples of causal chains of diseases inRDF through this endpoint using SPARQL queries as dis-cussed in the next section. Furthermore, we also provideuser friendly navigation system for causal chains of diseaseas discussed in Development of Disease Compass section.Example QueriesThe processing is not complicated; it requires only sim-ple procedural reasoning. We can obtain the causalchains that define a disease through several SPARQLqueries to the dataset [13].Figure 8 shows example queries to obtain an abnor-mal state in the dataset. Because all abnormal statesare defined as individual resources of the Abnormal_Statetype, we can obtain them using the query shown in (a1).When we want to obtain the causes/result of a se-lected abnormal state, we can follow the hasCause/hasResult properties. For example, (a2) is a query toobtain all causes of the selected abnormal state. Fur-thermore, we can obtain a general disease chain thatincludes the abnormal state using the query shown in(a3). This query means to follow all of the hasCause/hasResult properties recursively from the selectedabnormal state.On the other hand, Fig. 9 shows example queries toobtain the definitions of diseases. We can obtain alldiseases in the dataset using query (d1), which issimilar to query (a1). When we want to obtain allsuper diseases (super class) of a selected disease, wecan use the query show in (d2). To obtain the corecausal chains or derived causal chains of a selecteddisease, we can use query (d3) or (d4), respectively.By combining queries (d2), (d3), and (d4), we can ob-tain all causal chains that appear in the definitions ofthe disease using the query shown in (d5). Further-more, when we want to obtain a list of causal rela-tionships that appear in the causal chain of thedefinition of the diseases rather than a list of abnor-mal states, we can use the query shown in (d6). Thisquery finds all properties among all abnormal statesthat appear in the definition of the selected disease.We believe that all of the above queries are easy tounderstand and intuitive for many people. This is asignificant advantage of our RDF model compared toour original OWL disease ontology.Fig. 7 Causal chains of diseases shown in Fig. 2Fig. 8 Example queries. Here, dont: represents a prefix of the DiseaseChain-LD and < abn_id > and < dis_id > represent the id of a selectedabnormal state and disease, respectivelyFig. 6 RDF representation of diseaseKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 9 of 18Integration of biomedical abnormal statesMapping to Other Resources As illustrated in the pre-vious section, our ontology provides three levels ofabnormal states, from generic to disease-specific.Level 1 in our ontology defines generic concepts thatcorrespond to the PATO concepts [14], and those con-cepts can be mapped to related PATO concepts (Fig. 10).The lower Level 2 concepts are human anatomicalstructure-dependent abnormal states that correspond toHuman Phenotype Ontology (HPO) concepts [15]. Bycreating links between Level 2 concepts and HPO con-cepts, it becomes possible to navigate from the HPOconcepts to the upper generic PATO concepts. Level 3provides disease-specific abnormal states, such as myo-cardial ischemia in ischemic heart disease and chestpain in angina pectoris. In the revised version 11 of theInternational Classification of Diseases (ICD), diseasescontain causal properties information [16]; therefore,we plan to map our Level 3 concepts to the correspond-ing ICD concepts. Level 3 abnormal states are describedin the causal chains of diseases [4]. By mapping our dis-ease concepts of disease ontology to the ICD, ICD userscan understand the causal relationships of the abnormalstates in diseases. Our ontology also allows users to navi-gate related concepts in other resources, such as HPOand PATO.Mapping our ontology to other resources in order tointegrate various data related to abnormalities will alsoprovide benefits to the users of other resources. First,one can find concepts from generic to specialized termseasily by referring to the single is-a tree in our abnor-mality ontology. For example, although HPO does notconsider consistent is-a relationships in terms of sten-osis, by referring to arterial stenosis at Level 2 in ourontology through mapping, HPO users can obtain the fol-lowing is-a relationships: arterial stenosis is-a vascularstenosis is-a narrowing tube is-a small in area.1 Sincesmall area is linked to a PATO concept, via our ontol-ogy, users might find orthologous concepts of other spe-cies. In particular, human phenotypes can be linked to thephenotypes of model organisms, e.g., mouse and rat, if theset composed of Attribute (A) and Value (V) are identicaland the Object (O) has structural similarity. PATO2YA-MATO attempts to integrate phenotype descriptions res-iding in differently structured comparison contexts [17].By applying PATO2YAMATO, mapping concepts acrossspecies and integrating knowledge from various speciesmay be possible.We also plan to link the components of the <Object(O), Attribute (A), Value (V) > representation of abnormalstates to other resources. For example, we suppose thatObject (O) can be linked to concepts in the FoundationalModel of Anatomy (FMA) ontology [18]. By mappingmedical terminologies, such as SNOMED-CT[19] andMeSH terms [20], it will be possible to retrieve biomedicalarticles related to abnormal states or diseases. It will bealso useful for the users of these terms to understand howtheir research subjects are involved in various abnormalstates in the human body relative to diseases.Trial integration In order to assess the feasibility ofthe above approach, we conducted a trial integration ofsome examples taken from the three-level ontology ofabnormal states, the disease ontology, and some typicalexternal resources, in which we used 386 abnormalstates consisting of 279 abnormal states that are superclasses of 107 states that are the bottom level classesappearing in 12 typical diseases from three medical spe-cializations, i.e., cardiovascular medicine, neurology, andgastroenterology. Mapping an abnormal state defined inthe abnormal state ontology to related concepts in exter-nal resources was performed manually after detectingcandidates using a perfect string match algorithm.Fig. 9 Example queries to obtain definitions of diseases. In thisfigure dont: represents a prefix of the Disease Chain LOD, and <dis_id > represents the id of a selected diseaseKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 10 of 18Table 1 shows the mapping results for PATO, HPO,MeSH, and SNOMED-CT for each level of abnormalstates. As can be seen in the table, 52 abnormal states inPATO are mapped to those among 134 states at Level 1of the abnormal ontology, none in HPO, two in MeSH,and none in SNOMED-CT. No abnormal state is foundamong the 107 states at Level 3 in the four externalsources because they are disease-specific abnormalstates. It is interesting to find that abnormal states at Level2 have more corresponding states in external resourcesthan those at other levels. This is because those at Level 1are too abstract and those at Level 3 are too specific. Someexamples of mapping results are shown in Table 2.Another interesting finding is that our abnormalontology can fill the conceptual gap between abstractPATO concepts and organ-specific HPO concepts. Forexample, mitral valve insufficiency in HPO, which meansan imperfect state of the closure function of the mitralvalve, corresponds to mitral incompetence at Level 2 ofour abnormal state ontology. On the other hand, PATOsinsufficient corresponds to dysfunction at Level 1 of ourabnormal state ontology. Then, these two are connectedvia valve incompetence in our ontology. In addition, thefact that valve incompetence subsumes tricuspid incom-petence demonstrates that the concepts at Level 2 of ourabnormal state ontology can help find hidden subsump-tion relations between concepts in PATO and HPO.Abnormality Ontology as Linked DataWhile causal chains of abnormal states are published aslinked data based on the RDF model for the causalchains of diseases, we export the is-a hierarchy of abnor-mal states in the abnormality ontology in OWL formatusing the OWL export function in Hozo [12] and dir-ectly publish it as linked data because it does not haveparticularly complicated conceptual structures.Development of Disease CompassDisease CompassTo exploit the value of a disease ontology as a know-ledge source for advanced medical information systems,it is important that the users can navigate the ontologyeasily and intuitively according to their interests. Med-ical experts may not find it easy to use SPARQL queriesto obtain information about disease chains. Therefore,we have developed Disease Compass as a navigationsystem to explore the disease ontology. We designedthe system so that users without experience withFig. 10 Integration of abnormal states in biomedicineTable 1 Mapping between abnormal state ontology andexternal resources. Some examples of mapping results areshown in Table 2Concepts Our Ontology PATO HPO MeSH SNOMEDCTLevel 1 134 52 0 2 0Level 2 145 2 27 28 17Level 3 107 0 0 0 0Total 386 54 27 30 17Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 11 of 18ontologies or linked data can easily explore diseaseknowledge and related information.It is available at http://lodc.med-ontology.jp/ (demomovies are also available as attached files).System architectureFigure 11 shows the Disease Compass system architec-ture. The system obtains disease knowledge from Dis-ease Chain-LD, which is converted from the diseaseontology. It also has mapping information with otherLinked Open Data (LOD) and web services, and it canobtain related information through these mappings. Al-though the system currently has mappings to onlyDBpedia and BodyPart3D, it will be possible to extendmappings to other LOD sources using existing ap-proaches [21] to generate such linkages.Technically, the system uses two methods to accessthese mapped datasets, i.e., SPARQL queries for linkeddata and an API for web services. If related resources(ontologies and other datasets) are published as LOD,the system can be extended easily to link such relatedinformation using SPARQL, which is the major benefitof using linked data techniques. In addition, manylinked datasets include links to other data. For example,DBpedia includes links to major medical codes, such asICD10 and MeSH; thus, the system can follow theselinks through mappings between Disease Chain-LD andDBpedia.Disease Compass is a web service that is supported onPCs, tablets, and smartphones. It is implemented usingVirtuoso as its RDF database and HTML 5 forvisualization of disease chains and other information. Allmodules of the system provide APIs for other web ser-vices. This allows other web services to use all the func-tions of Disease Compass so that their modules willwork with other related services.Table 2 Some examples of mapping results between abnormal state ontology and external resources. Blank cells mean thatabnormal states defined in our ontology are not existent in other resourcesConcepts Our Ontology PATO HPO MeSH SNOMEDCTLevel 1 structural abnormalityLevel 1 material degeneration PATO:0002037degenerationLevel 1 hardening PATO:0000386hardLevel 1 size abnormalityLevel 1 large in size PATO:0000586increased sizeLevel 1 hyperfunction PATO:0001625increasedfunctionalityLevel 1 dysfunctionLevel 1 movement abnormality D009069MovementDisordersLevel 2 narrowed cross-sectional area of tubeLevel 2 hardening of wallLevel 2 cellular tissue necrosis PATO:0000647necroticD009336NecrosisLevel 2 conoronary artery stenosis HP:0005145Coronary arterystenosisD023921Coronary Stenosis233970002Coronary arterystenosisLevel 2 arterial occlusion 2929001Occlusion of arteryLevel 2 coronary artery occlusion D054059Coronary Occlusion63739005Coronary occlusionLevel 2 chest pain HP:0100749Chest painD002637Chest pain29857009Chest painLevel 3 coronary artert stenosis inarteriosclerosisLevel 3 esophagel stenosis in esophagitisKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 12 of 18User interface for navigationDisease Compass supports the following types of naviga-tion of the disease ontology.1. Navigation for definition of disease based on theRFM of a disease: navigating definitions of diseasesbased on causal chains with links to other systems/datasets2. Navigation for general causal chains in a humanbody: browsing possible causal chains of abnormalstates (disorder) in the human body3. Navigation for definition of abnormal state(clinical disorder): browsing the is-a hierarchy ofabnormal state (clinical disorder) ontology withmappings to other resourcesThe above are related, i.e., the user can freely accessother systems.Navigation for definition of diseaseFigure 12 shows the Disease Compass user interface fornavigating the definition of a disease. Users select a dis-ease according to the is-a hierarchy of diseases, or theysearch a disease chain by disease name or the abnormalstate included in the disease. The system visualizes thedisease chains of the selected diseases in a user-friendlyrepresentation in the center of the window. The systemalso obtains and displays information related to theselected disease and abnormal state from the linked webservices, such as general information from linked data(DBpedia) and 3D images of anatomies.DBpedia is a linked open dataset extracted from Wiki-pedia [22]. We use DBpedia English (http://dbpedia.org)and Japanese (http://ja.dbpedia.org). DBpedia providesgeneral information about diseases; however, medicalexperts have not approved this content. Nevertheless, wesuggest that the content is sufficiently valuable to pro-vide an overview of various diseases. In addition,DBpedia also provides links to major medical termin-ology and codes, such as ICD10 and MeSH, whichallows users to gather specialized information about agiven disease. This technology, with which related infor-mation from other web resources (e.g., ontologies, medicalcodes, and datasets) can be obtained through mappings, iseasy to apply to other linked data. We plan to extend thetarget linked data in the near future.A web service named BodyPart3D/Anatomography [23]is employed to generate 3D images of anatomies. The tar-get area of the image is decided by Disease Compass,which combines all targets of abnormal states appearingin the definition (causal chains) of the selected diseasechain. Subsequently, the system highlights the part of the3D image that is the target of the selected abnormal statein the disease chain.The functionality of Disease Compass is enabled becauseof the successful combination of our disease ontology andother web resources based on linked data technologies. Asa result, Disease Compass allows users to explore diseaseknowledge and related information through various webresources.An additional movie file shows a demonstration of navi-gation for definition of disease [see Additional file 1].Navigation for general causal chainsWhen viewing the definition of a disease, the user selectsa target abnormal state and can use the click menu totrace the causes and/or effects that form the selectedabnormal state. Then, a view for navigating generalcausal chains is shown. In this view, users can browsethe possible causal chains of abnormal states (disorders)in the human body through different diseases.Figure 13 shows an example of navigation for generalcausal chains whose starting point is heart failure. Thered node represents the starting point, purple nodes areeffects, and green nodes are causes. By right clicking,diseases that include the selected abnormal state areFig. 11 Disease Compass system architectureKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 13 of 18listed, and the system shows the definition of theselected disease when its name is clicked in the list. Notethat some abnormal states shown in the view appear indifferent diseases when they are linked to the samechain. In particular, it is important that the user can ob-tain both the derived causal chains defined by the clin-ician directly and the causal chains derived by tracingthe general causal chains through all clinical areas.An additional movie file shows a demonstration of navi-gation for general causal chains [see Additional file 2].Navigation for definition of abnormal state (clinical disorder)As discussed in Development of Disease Compass sec-tion, we investigated the differences in the hierarchicalstructure of biomedical resources and conducted a trialintegration of our abnormality ontology and relatedresources, such as PATO, HPO, and MeSH, based onontological theory [9]. As a result, we developed a proto-type of the abnormality ontology as linked data with abrowsing system. By mapping information from otherresources, users can access disease knowledge throughour abnormality ontology and through other openresources (Fig. 14).For example, ID of PATO shown in the definition oflow force/decrease in force is used to jump into theOntobee browser, which allows users to see various re-lated information defined by PATO. Similarly, clicking aMeSH ID shown in the definition pane leads users toFig. 12 Disease Compass user interfaceFig. 13 View for navigating general causal chainsKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 14 of 18the corresponding terms defined in PubMed. Forexample, myocardial ischemia in the ontology of abnor-mal states is mapped to the corresponding MeSH term,i.e., myocardial ischemia (MeSH ID: D17202), fromwhich users can retrieve all relevant information anno-tated by myocardial ischemia in NCBI.DiscussionThe primary feature of the RFM of disease is a diseasemodel based on the causal chains of clinical disorders. Itis an appealing alternative representation of existing dis-ease ontology, such as the DOID and the OGMS. Themodel captures the possibilities of clinical disorders(abnormal states) as causal chains and represents dis-eases by overlapping them. It can intuitively representthe causes of diseases, disease progression, and thedownstream consequences of diseases to medical ex-perts. Through these representations, the RFM of a dis-ease can provide a broad picture of disease-associatedprocesses in a way that fits well with the clinical under-standings of diseases.Publishing the disease ontology based on the RFMas linked data allows users to access rich knowledge/information in the disease ontology through a stand-ard API. Furthermore, Disease Compass provides awell-organized graphical navigation function for thedisease ontology as linked data with related web re-sources by mapping information.In fact, Disease Compass, which allows users to navi-gate disease definitions with the help of abnormal states,enables users to learn whether an abnormal state is adisease cause or effect by identifying its position in thecausal chain of the disease. For example, in the case ofhypertension, users can easily find diseases that arecaused by hypertension, including hypertension disease incardiovascular medicine, and those that cause hyperten-sion as a symptom. For example, the user may find thatchronic kidney disease, in which hypertension appears inthe upper stream of its causal chain, causes variousinflammation, whereas, for Liddle Syndrome, hyperten-sion appears in the lower stream of its causal chain as aresult of hyperactivity of the epithelial Na channel ofamiloride-sensitive. Thus, users can learn that an abnor-mal state can be a cause or an effect (symptom) of dis-eases thanks to the causal chain model of diseases inDisease Compass.Disease Compass also allows users to compare multiplediseases to find unexpected commonalities. For example,in the case of ischemia, for myocardial infarction in car-diovascular medicine and ischemic cerebrovascular diseasein neurology, although the locations where the corre-sponding abnormal states occur differ, both causal chainsshare a similar path up to ischemia (Fig. 15). In fact, bothcausal chains have structural disorders, such as stenosisand occlusion, or ischemia is caused by a spasm viadecreased blood flow, and eventually necrosis occurs ineither case. After necrosis occurs, succeeding symptomsare quite different according to myocardial necrosis or ne-crosis of brain cells, as shown in Fig. 15 (a)), in whichsymptoms (e.g., ventricular wall motion abnormalities)occur in the cardiovascular system, and in Fig. 15 (b), inwhich different symptoms (e.g., paralysis) are caused inlocations governed by the nervous system, thereby reflect-ing the differences between respective organs.Disease Compass helps users uncover hidden relationsbetween different diseases across divisions (Fig. 16). ItFig. 14 Browsing system for the abnormality ontology as linked dataKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 15 of 18can be done through general causal chains which includecommon abnormal states between different diseasesacross medical divisions. For example, while heart fail-ure is a typical disease in cardiovascular internal medi-cine, Disease Compass can show that it can be caused byautoimmunity in systemic scleroderma in allergology andrheumatology. Another disease that causes heart failurein different medical fields is renal arteriovenous fistula(aneurysmal type) whose abnormal state, i.e., renalarteriovenous shunt, can also cause heart failure, whichwould be informative for novices because not all text-books mention this rare fact.In summary, the benefits of abnormal states organized interms of the subsumption relation between states are signifi-cant. This helps users fill conceptual gaps between conceptsin external resources and reveal hidden commonality be-tween diseases in different medical fields. This can be real-ized because all diseases are described in terms of the causalchains of abnormal states and are organized in an ontology.ConclusionsThis paper has discussed a navigation system for diseaseknowledge based on a disease ontology and linked datatechnologies. Our ontology defines diseases based on thecausal chains of abnormal states (disorders), and abrowsing system allows users to explore the definitionsof diseases with related information obtained fromlinked data. We believe that this system will allow usersto gain a broader understanding of diseases according totheir interests and intentions.(a)(b)Fig. 15 Interaction between abnormal ontology and Disease CompassKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 16 of 18The following list shows the summary of our contributionto clinicians and information scientists;1. For clinicians They can access the basic information of diseaseswith causal chains through navigation using theDisease Compass. That is, they can know possiblecauses and/or effects of the disease of interest. They can find (retrieve) diseases according tocauses or effects. They can access related resources throughmapping information.2. For information scientists 6302 diseases and 21,699 abnormal states aredefined by clinicians as a fact repository causalchains of diseases based on the RFM. It shows thatthe RFM was applicable to a variety of diseases. 1554 diseases and 7080 abnormal states in sixmajor clinical areas, which are extracted from theabove RFM-diseases, are published as linked data(RDF) with SPARQL endpoint (accessible API).We call the linked data Disease Chain LD. Infor-mation scientists can access it using friendlySPARQL queries through a RDF model we de-signed for causal chain of diseases. A navigation system for disease knowledge usingthe Disease Chain LD, called Disease Compass, isdeveloped. It provides navigating functions forcausal chain of diseases and related informationthrough links to other resources with GUI. Itshows how the Disease Chain LD can be used fordeveloping information systems.The system was evaluated informally by medical ex-perts in several meetings and workshops, and positivecomments were received. A full-scale user evaluation isto be conducted in future.Future work will also include extending the relatedresources using linked data and development of morepractical applications using the Disease Chain-LD.The system is also subject to continuous improve-ment, including bug fixes and development of newfunctions. There remain a few topics on diseases toexplore. One is a notion of an imbalance model thatmodels pre-clinical manifestation based on the dis-turbance of homeostasis and roughly corresponds toOGMS disposition [1]. Another topic is identitytracking of a disease to capture its progression [24].We must consider these significant topics and theircomputational models.The latest version of Disease Compass is available athttp://lodc.med-ontology.jp/.Endnotes1Though the type 1 and type 2 diseases are not med-ical terms, we use these terms only in this paper to men-tion two kinds of diseases.2We defined small in area asa property that an area is smaller than a given threshold.On the other hand, narrowing tube is defined that across section of a tube is smaller than a given threshold.In the same ways, definitions of arterial/vascular sten-osis are the cross section of an artery/blood vessel issmaller than a given threshold. Considering these defi-nitions, we consider that arterial stenosis is-a vascularstenosis is-a narrowing tube is-a small in area.Additional filesAdditional file 1: A demonstration of navigation for definition ofdisease using Disease Compass. (MP4 28362 kb)Additional file 2: A demonstration of navigation for general causalchains of disease using Disease Compass. (MP4 45031 kb)Fig. 16 Causal relationships of heart failureKozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 17 of 18AbbreviationsDOID: Human Disease Ontology; IDO: Infectious Disease Ontology;LOD: Linked Open Data; OGMS: Ontology of General Medical Sciences;OWL: Web Ontology Language; RDF: Resource Description Framework;RFM: River Flow Model; SNOMED-CT: Systematized Nomenclature ofMedicine-Clinical TermsAcknowledgementsThis paper is an extended version of the paper presented at the 5thInternational Conference on Biomedical Ontology (ICBO2015), July 2730,2015, Lisbon, Portugal.The authors are deeply grateful to Drs. Natsuko Ohtomo, Aki Hayashi,Takayoshi Matsumura, Ryota Sakurai, Satomi Terada, Kayo Waki, and others atthe University of Tokyo Hospital for describing the disease ontology andproviding us with their broad clinical knowledge. We would also like tothank other team members, Drs. Yoshimasa Kawazoe, Masayuki Kajino, andEmiko Shinohara, from the University of Tokyo for their useful discussionsrelated to biomedicine.The authors would like to thank Enago (www.enago.jp) for the Englishlanguage review.FundingThis research is supported in part by the Japan Society for the Promotion ofScience (JSPS) through its FIRST Program and the Ministry of Health, Labourand Welfare, Japan.Availability of data and materialsDeveloped ontologies and software are available at http://lodc.med-ontology.jp/Authors contributionsKK designed and implemented the Disease Compass. YY developed ontologyof abnormal states. RM defined most of the theory. TI and KO have led ourmedical ontology project and developed disease ontology. All authors readand approved the final manuscript.Authors informationKouji Kozaki, Associate professor at The Institute of Scientific and IndustrialResearch, Osaka University.Yuki Yamagata, Researcher at National Institute of Biomedical Innovation.Riichiro Mizoguchi, Research Professor at Research Center for Service ScienceSchool of Knowledge Science, Japan Advanced Institute of Science andTechnology.Takeshi Imai, Assistant Professor at Department of Medical Informatics,Graduate School of Medicine, The University of TokyoKazuhiko Ohe, Professor at Department of Medical Informatics, GraduateSchool of Medicine, The University of Tokyo.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1The Institute of Scientific and Industrial Research, Osaka University, 8-1Mihogaoka, Ibaraki, Osaka 567-0047, Japan. 2National Institute of BiomedicalInnovation, Health and Nutrition, 7-6-8, Saito-Asagi, Ibaraki, Osaka 567-0085,Japan. 3Research Center for Service Science, Japan Advanced Institute ofScience and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan.4Graduate School of Medicine, The University of Tokyo, 7-3-1, Hongo,Bunkyo-ku, Tokyo 113-0033, Japan.Received: 15 February 2016 Accepted: 6 June 2017Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 DOI 10.1186/s13326-017-0118-0RESEARCH Open AccessBioFed: federated query processing overlife sciences linked open dataAli Hasnain1*, Qaiser Mehmood1, Syeda Sana e Zainab1, Muhammad Saleem2, Claude Warren Jr3,Durre Zehra1, Stefan Decker1 and Dietrich Rebholz-Schuhmann1AbstractBackground: Biomedical data, e.g. from knowledge bases and ontologies, is increasingly made available followingopen linked data principles, at best as RDF triple data. This is a necessary step towards unified access to biological datasets, but this still requires solutions to query multiple endpoints for their heterogeneous data to eventually retrieve allthe meaningful information. Suggested solutions are based on query federation approaches, which require thesubmission of SPARQL queries to endpoints. Due to the size and complexity of available data, these solutions have tobe optimised for efficient retrieval times and for users in life sciences research. Last but not least, over time, thereliability of data resources in terms of access and quality have to be monitored. Our solution (BioFed) federates dataover 130 SPARQL endpoints in life sciences and tailors query submission according to the provenance information.BioFed has been evaluated against the state of the art solution FedX and forms an important benchmark for the lifescience domain.Methods: The efficient cataloguing approach of the federated query processing system BioFed, the triple patternwise source selection and the semantic source normalisation forms the core to our solution. It gathers and integratesdata from newly identified public endpoints for federated access. Basic provenance information is linked to theretrieved data. Last but not least, BioFed makes use of the latest SPARQL standard (i.e., 1.1) to leverage the full benefitsfor query federation. The evaluation is based on 10 simple and 10 complex queries, which address data in 10 majorand very popular data sources (e.g., Dugbank, Sider).Results: BioFed is a solution for a single-point-of-access for a large number of SPARQL endpoints providing lifescience data. It facilitates efficient query generation for data access and provides basic provenance information incombination with the retrieved data. BioFed fully supports SPARQL 1.1 and gives access to the endpoints availabilitybased on the EndpointData graph. Our evaluation of BioFed against FedX is based on 20 heterogeneous federatedSPARQL queries and shows competitive execution performance in comparison to FedX, which can be attributed tothe provision of provenance information for the source selection.Conclusion: Developing and testing federated query engines for life sciences data is still a challenging task.According to our findings, it is advantageous to optimise the source selection. The cataloguing of SPARQL endpoints,including type and property indexing, leads to efficient querying of data resources over the Web of Data. This couldeven be further improved through the use of ontologies, e.g., for abstract normalisation of query terms.Keywords: Life sciences dataset, Linked open data, SPARQL query federation*Correspondence: ali.hasnain@insight-centre.org1Insight Centre for Data Analytics, National University of Ireland (NUIG),Galway, IrelandFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 2 of 19BackgroundThe Web provides access to large-scale sets of interlinkeddata from heterogeneous scientific domains, and  in par-ticular for the life science researchers  develops into asource of reference data from scientific experiments [1].The comprehensive set of Linked Open Data (LOD)1 cov-ers over 60 billion triples provided by more than 1000different data sets. The small but important portion of theLinked Open Data cloud is composed of the Life ScienceLinked Open Data (LS-LOD), which results to 8% (83 datasets) of the overall LOD cloud2. The life science data con-tributes significantly to the ongoing research in semanticWeb technologies, since the life science research commu-nity gathers and exposes their expertise in form of highquality ontologies, which support innovative retrievalmethods across distributed SPARQL endpoint engines aspresented in this publication.Significant contributions in terms of data integrationand data provision have been made available from theBio2RDF project3, the Linked Life Data initiative4, theNeurocommons group5, through the Healthcare and LifeSciences knowledge base6 (HCLS Kb), from the LinkedCancer GenomeAtlas (Linked TCGA) [2, 3], and theW3CHCLSIG Linking Open Drug Data (LODD) initiative7.The outcomes from these initiatives develop themselvesinto reference data sources that feed the existing life sci-ence expertise back into the ongoing large-scale research,for example into high-throughput gene sequencingresearch with a need to access the full body of biomedicaldata [4]. As a natural consequence, a single point of accessand reference to the life sciences (LS) data is an impor-tant step forward in using the data and  eventually in mastering the data deluge.It has already been an important step forward tointegrate and RDF-ize the existing biological knowledgesources to make the data available according to semanticWeb and open data principles, but this is not sufficientfor efficient data access. Further improvements have todeal with the access to the existing SPARQL endpoints,access to the meta-data of the data repositories, balancingaccess overheads against query efficiency and ultimately,the efficient use of all technological advancements alto-gether [5]. The resulting solution should cope with thesize and the complexity of the data, and should still pro-vide full access to the data in a way that a researchercan formulate and explore complex queries in refer-ence to the full amount of integrated data without largeprocessing overheads, i.e. the the heterogeneity of thedata should not impair the identification of meaning-ful results [68]. In particular, the exploitation of refer-ence meta-data information and the use of state of theart federation technologies form an important step forthe evaluation of such an engine in a real-life use casescenario.The integration of heterogeneous data sources takesplace in research teams that make the result available as aSPARQL endpoint, leading to the challenge of combiningdisparate SPARQL endpoints with the help of federationengines, which a priori rely on the federation of queriesbeing delivered across the distributed resources [9]. Thelatest SPARQL standard, i.e. SPARQL 1.1, is a key techno-logical advancement to assemble federated queries (withthe help of the SERVICE query option), and is sup-ported by SWobjects8, Apache Jena9 and dotNetRDF10.This resulted into the development of different systems[1015] capable of executing queries in a federated envi-ronment and claiming that this approach is sufficientlygeneric for processing federated queries over any otherdata set. However, specific draw-backs have to be consid-ered that will be addressed in the presented solution: First, the federation of queries does not enforce thatthe queries deliver the expected results, i.e. access tometa-data information from the SPARQL endpointsshould improve the outcomes. Second, preparing meaningful and productiveSPARQL queries remains to form a skillful task andprofits from domain expertise (e.g., from the domainontologies) as well as the meta-data information fromthe data sources. Last, once the meta-data information has been usedto define the query (to be federated acrossendpoints), optimisations solutions should apply toenable efficient, i.e. speedy, response times.BioFed is a federated query engine that makes useof state of the art semantic Web technologies to querylarge and heterogeneous data sets in the life sciencesdomain: the federation covers 130 public SPARQL end-points optimised for LS-LOD. It offers a single-point-of-access for distributed LS data enabling scientists toaccess the data from reliable sources without extensiveexpertise in SPARQL query formulation (for SPARQL1.1, online user interface with drop down menus). Itsautonomous resource discovery approach identifies rel-evant triple patterns, matches types according to theirlabels as a basic semantic normalisation approach, andoptimises the retrieval based on source selection strate-gies for efficient response times. New public endpoints areadded through a cataloguing mechanism based on sourceselection [16]. The provided provenance information cov-ers the sources queried, the number of triples returned andthe retrieval time.The remaining part of this paper is organised as fol-lows: we present related work in Section Related work.Then we present the methodologies covering the imple-mentation details including discovery, source selectionand query re-writing (Section Methods). BioFed salientfeatures are presented in Section BioFed salient features.Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 3 of 19The results and the evaluation against the query engineFedX is given in Section Results and discussion. SectionConclusions covers the conclusion, discussion andfuture work.Related workAdvances in federated query processingmethods form thekey achievement for federated query engines that auto-matically access data from multiple endpoints. Each ofthe suggested solutions follows slightly different principlesand even goals, and realises different trade-offs betweenspeed, completeness, and flexibility requirements, whichare partially imposed by the status of technologicaladvancements at that time the data sources ready for use.Umbrich et al. [17, 18] proposed  in a straight forwardway  a Qtree-based index structure that summarises thecontent of data sources for query execution over the Webof Data. The index gives access to the data in the SPARQLendpoint, but comes with significant draw-backs such asa lack of access to relational information, e.g., from themeta-data of the SPARQL endpoint, the overheads in pre-processing the existing data, and the consequence of out-of-date indexes and index rebuilding needs.In terms of advanced index assisted approaches, theSHARE project registry [19] stores the index informa-tion as OWL class definitions and instances of themyGrid ontology. Similarly, OpenLifeData [20], indexedBio2RDF using its semantically rich entity-relationshipsand exposed it as SADI services after registering in theSHARE registry. SHARE project stores the set of distinctpredicates for all the endpoints. The source selection isperformed by matching the predicate of the triple pat-tern against the set of predicates of all indexed endpoints.All the endpoints which contain matching predicates areselected as relevant sources for that triple pattern.Kaoudi et al. [21] propose a federated query techniqueon top of distributed hash tables (DHT), which is a simi-lar approach to the indexing techniques used by Umbrichet al. The DHT-based optimiser makes use of three greedyoptimisation algorithms for best plan selection. Overall,the authors achieve good query execution times, but sufferfrom the same disadvantages as the previous solution.Avalanche [22] gathers endpoint data sets statistics andbandwidth availability on-the-fly before the query feder-ation, which increases the overall query execution time.Vandervalk et al. [23], presented two approaches for queryoptimisation in a distributed environment, requiring basicstatistics regarding RDF predicates to query the remoteSPARQL endpoints. For one approach a static queryplan is computed in advance of query execution, usinggraph algorithms for finding minimum spanning trees.Whereas, in the second approach, the planning and exe-cution of the query are evaluated to follow an independentquery plan.Quilitz and Leser [24] have developed DARQ for thefederation of queries across SPARQL endpoints. It opti-mises the selection of relevant data sources on the bases ofdata descriptions, e.g., usage of predicates in the endpoint,and statistical information, to optimise the routing ofqueries to associated endpoints. This approach is straightforward, but could exploit better the distribution of triplesgiven from a specific data source.Langegger et al. in [25] describe a similar solution usinga mediator approach, which continuously monitors theSPARQL endpoints for any changes in the data sets andupdates the service descriptions automatically. They solvethe problem of out-of-date descriptions, but unfortunatelythe authors have introduced the restriction that all sub-jects of triple statements must be variables for the boundpredicate requirement of DARQ.Schwarte et al. [11] have build FedX, which is a queryfederation engine for the Web of Data and which doesnot require an index for accessing the distributed data.FedXmakes use of SPARQL ASK queries to enquire aboutthe content and to determine the endpoints with relevantinformation. This approach provides sufficiently fast dataretrieval as compared to other prior art techniques [26],however, it under-exploits data provide from the endpointup front to optimise the query generation.Saleem et al. [13] presented DAW, a duplicate-awarefederated query approach over the Web of Data. Itmakes use of the min-wise independent permutations[27] and compact data summaries to extend existingSPARQL query federation engines in order to achievethe same query recall values while querying fewerSAPRQL endpoints, which is a very specific optimi-sation solution for source selection. HiBISCuS [14] isan efficient hypergraph based source selection approachfor SPARQL query federation over multiple SPARQLendpoints.SPLENDID [26] exploits Vocabulary of InterlinkedDatasets (VoID) descriptions that are provided from theSPARQL endpoints, and makes use of SPARQL ASKqueries to determine relevant sources for the queryingof specific triple patterns. This leads to the result thatSPLENDID is able to federate more expressive queries incomparison to the previous solutions, but has not beentested on the very specific case of distributed SPARQLendpoints for the life sciences with their high complexityof data.Other optimisation techniques have also beenattempted. Li and Heflin [28] have built a tree structurethat supports federated query processing over heteroge-neous sources and uses a reasoner to answer queries overthe selected sources and their corresponding ontologies.This approach offers new ways to use class specificationsfor complex querying, but has not been tested againstchallenging life science use cases either.Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 4 of 19ELITE [29] is an entailment-based federated queryprocessing engine. It makes use of the ontology-baseddata access, R-tree based indexing, query rewriting, andDL-Lite formalism to retrieve more complete resultswhich other systems may miss due to no reasoning overgiven query.Ludwig and Tran [30] propose a mixed query enginethat assumes to encounter incomplete knowledge aboutthe sources to select and discover new sources duringrun time, which would not scale sufficiently in the caseof complex data and larger numbers of SPARQL end-points. Acosta et al. [31] present ANAPSID, an adaptivequery engine that adapts query execution schedulersto SPARQL endpoints data availability and run-timeconditions, which would not scale to the life sciencedomain either.In BioFed we exploit the potential of VoID descriptors the state of the art approach for describing any dataset inorder to catalogue the classes and properties from remoteSPARQL endpoints. This cataloguing mechanism facil-itates query federation mechanism to access data frommultiple heterogeneous biological datasources and offersthe opportunity to support the user of the retrieval enginewith efficient query formulation tools: the queries arebuild on the basis of existing data and then distributedto the relevant endpoints through the source selectionapproach. For this, BioFed adopts a hybrid source selec-tion approach [1], i.e., we make use of both index andSPARQL ask queries.Moreover BioFed covers the full range of publicSPARQL endpoints in health care and life sciencesdomain, including Bio2RDF, which is a significant scope interms of number of endpoints and complexity of data, andwill remain to form a significant challenge for the seman-tic data integration of the near future. BioFed provides asingle point of access for LS data with other importantinformation e.g., provenance due to which some queriesmay take longer when compared to the other tools likeFedX, whereas provenance is the key for the life sciencesdomain targeted by BioFed. One smaller-scale alternativeapproach is Topfed [3] which is a TCGA tailored federatedquery engine.Furthermore, the information in the captured cataloguedoesnt rely on semantically rich entity-relationships,which would require complete knowledge of the definedschema, which  in return  is difficult to access for mostof the used resources. Our focus is to cover a wide range oflarge-scale SPARQL endpoints and to catalogue sufficientinformation to achieve efficient querying of the federatedresources.It is worth noticing that the current interface providedby BioFed supports designing a basic set of SPARQLqueries using a set of Query Elements (Qe) [16, 32, 33].Different concepts and properties from endpoints actsas Qe in order to formulate SPARQL queries. Advancedand state of the Art query builders e.g., KnowledgeEx-plorer [34] and SPARQL Assist [35] make use of theoriginal ontologies/vocabularies and provide an auto-complete mechanism to write a SPARQL query, but webelieve BioFed interface is a step towards building a basicSPARQL query that queries over multiple LS SPARQLendpoints as BioFed offers the set of concepts and proper-ties in a particular context that can easily be selected fromdrop-down menu in order to formulate SPARQL query.MethodsGeneral architectureThe general architecture of BioFed is given in Fig. 1. Givena SPARQL query, the first step is to parse the query andget the individual triple patterns (Step 1). The next step isthe triple-pattern-wise source selection (TPWSS).Definition 1 (Total Triple Pattern-wise SourcesSelected) Let Q = {t1, . . . , tm} be a SPARQL query contain-ing triple patterns t1, . . . , tm,R = {Rt1 , . . . ,Rtm} be the cor-responding relevance set containing relevant data sourcessets Rt1 , . . . ,Rtm for triple patterns t1, . . . , tm, respectively.We define TTPWSS = ?Rti?R? |Rti | be the total triplepattern-wise sources selected for query Q, i.e., the sumof the magnitudes of relevant data sources sets over allindividual triple patterns Q.The TPWSS identify relevant (also called capable)sources against individual triple patterns of the query(Step 2B). BioFed performs this step by using the dis-covery approach presented in Hasnain et al. [16]. Thisdiscovery enumerates the known endpoints and relateseach endpoint with one ormore graphs andmaps the localvocabulary to the vocabulary of the graph (Step2A). Step3 is to convert the given SPARQL 1.0 query into corre-sponding SPARQL 1.1 query. This step is known as QueryRe-writing and further explained below. BioFed makesuse of the TPWSS information and the SPARQL SER-VICE clause to rewrite the required SPARQL 1.1 query.The resulting SPARQL 1.1 query is executed on top ofthe Apache Jena query engine and the results are returnedback (Step 4). In the following, we will explain each ofthese steps in detail.BioFed is designed as a Domain Specific Query Engine(DSQE) that transforms expressions between existingvocabularies, i.e. for the vocabularies used by SPARQLendpoints, and combines those expressions into a sin-gle federated query using SPARQL Service calls. Itdoes occur, that a single node is translated into multi-ple nodes (e.g., drug may become both a molecule and asmallMolecule) leading to multiple triples being createdas a cross product from all possible node translations.The resulting statement is executed and then returns theHasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 5 of 19Fig. 1 BioFed architecture. ARDI comes from previous work by Hasnain et al. [4, 16]results to the user. The algebra rewriter examines eachsegment of the BGP triples and attempts to expand theterms based on the vocabulary mapping into terms of theendpoint graphs and stores the result for each.Autonomous Resource Discovery and Indexing (ARDI)The ARDI comprises a catalogue of LS-LOD and a set offunctions to perform standard queries against it [4, 16].The methodology for developing the ARDI consistsof two stages namely catalogue generation and linkgeneration. The methodology for catalogue generationrelies on retrieving all types (distinct concepts) fromeach SPARQL endpoint and all associated propertieswith corresponding instances. URI patterns and exam-ple resources were also collected during this process.Data was retrieved from more than 130 public SPARQLendpoints11 and organised in an RDF document - theLS-LOD catalogue. The list of SPARQL endpoints wascaptured from publicly available Bio2RDF data sets and bysearching for data sets in CKAN12 tagged life science orhealthcare.The methodology for link generation is presented in[16] where naïve, named entity and domain matchingapproaches for weaving the types together was dis-cussed. We later extended our linking mechanism toincorporate Regex Matching [16].BioFed utilises the ARDI to perform TPWSS. The ARDIis also used to determine the URLs of the SPARQL end-points, and provides the base data for the endpoint selec-tion logic. The ARDI examines each node in each triple inthe BGP. For each triple it determines if there is a matchin the ARDI or if the triple should be left unmatched (e.g.,an RDF:type predicate). Each unmatched node is passedthrough unchanged.The RDF in Listing 1 is an illustrative example ofa portion of the catalogue generated for the KEGGHasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 6 of 19SPARQL endpoint13. VoID is used for describing thedata set and for linking it with the catalogue entries:the void#Dataset being described in this catalogueentry is KEGG SPARQL endpoint. In cases whereSPARQL endpoints were available through mirrors (e.g.,most Bio2RDF endpoints are available through Car-leton Mirror URLs) or mentioned using alternative URLs(e.g., http://kegg.bio2rdf.org/sparql), theseSantana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 DOI 10.1186/s13326-017-0127-zRESEARCH Open AccessOntological interpretation of biomedicaldatabase contentFilipe Santana da Silva1,2 , Ludger Jansen3 , Fred Freitas1 and Stefan Schulz4*AbstractBackground: Biological databases store data about laboratory experiments, together with semantic annotations, inorder to support data aggregation and retrieval. The exact meaning of such annotations in the context of a databaserecord is often ambiguous. We address this problem by grounding implicit and explicit database content in aformal-ontological framework.Methods: By using a typical extract from the databases UniProt and Ensembl, annotated with content from GO, PR,ChEBI and NCBI Taxonomy, we created four ontological models (in OWL), which generate explicit, distinctinterpretations under the BioTopLite2 (BTL2) upper-level ontology. The first three models interpret database entries asindividuals (IND), defined classes (SUBC), and classes with dispositions (DISP), respectively; the fourth model (HYBR) is acombination of SUBC and DISP. For the evaluation of these four models, we consider (i) database content retrieval,using ontologies as query vocabulary; (ii) information completeness; and, (iii) DL complexity and decidability. Themodels were tested under these criteria against four competency questions (CQs).Results: IND does not raise any ontological claim, besides asserting the existence of sample individuals and relationsamong them. Modelling patterns have to be created for each type of annotation referent. SUBC is interpretedregarding maximally fine-grained defined subclasses under the classes referred to by the data. DISP attempts toextract truly ontological statements from the database records, claiming the existence of dispositions. HYBR is a hybridof SUBC and DISP and is more parsimonious regarding expressiveness and query answering complexity. For each ofthe four models, the four CQs were submitted as DL queries. This shows the ability to retrieve individuals with IND,and classes in SUBC and HYBR. DISP does not retrieve anything because the axioms with disposition are embedded inGeneral Class Inclusion (GCI) statements.Conclusion: Ambiguity of biological database content is addressed by a method that identifies implicit knowledgebehind semantic annotations in biological databases and grounds it in an expressive upper-level ontology. The resultis a seamless representation of database structure, content and annotations as OWL models.Keywords: Ontology, Interpretation, Biological database, OWL, Data semanticsBackgroundBiological databases store data about summarized resultsfrom laboratory experiments. Apart from numeric andunstructured text entries, they usually include seman-tic annotations, characterized by identifiers from domainontologies, to enhance database entries with standardisedmeaning. For instance, database records from the Uni-fied Protein Resource (UniProt) [1] are annotated with*Correspondence: stefan.schulz@medunigraz.at4Institute for Medical Informatics, Statistics and Documentation, MedicalUniversity of Graz, Auenbruggerplatz 2/V, 8036 Graz, AustriaFull list of author information is available at the end of the articleterms taken from the Protein Ontology (PRO) [2] andthe Gene Ontology (GO) [3]. It is mainly via their use asannotation vocabularies that bio-ontologies have becomeimportant resources for the management of biomedicalresearch data.As much as these domain ontologies, in isolation, obeyformal principles and good practice guidelines [4, 5],as little the meaning of the annotations themselves hasbeen formalized so far. The exact interpretation of whatit means when, e.g., in a UniProt record the proteinPRO:Methionine synthase is linked to the biological pro-cess GO:Methylation, is left to the user, mainly due to© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 2 of 14limited representation of UniProt Core [6]. UniProt Coreincludes the description on database fields related to eachother, but without formalization and links to GO (forexample). This can constitute a source of misunderstand-ing and hamper correct data interpretation, leading todoubtful or wrong conclusions.Although the meaning of semantic annotations indatabase records may seem trivial for domain experts,human interpretation of large numbers of records istedious and time-consuming. Laukens and colleagues [7],among others, have highlighted the difficulty of interpret-ing database content in the context of proteomics. Thereason for this is that there is still a divide between biolog-ical databases and the semantic technologies developedfor biomedical ontologies. Scattered data need to be inte-grated into a coherent picture, which is complicated byambiguity and lack of interoperability.On the one hand, there are rich and well-curateddatabases with highly structured tabular content but lim-ited ontological explicitness. Like most content of tabulardata structures, these databases require implicit back-ground assumptions for their correct interpretation.Imagine, for example, a database table with three fieldsProtein, Organism and Phenotype, filled with the symbolsProt1, Org1, and Phen1. Such a table is open to multi-ple interpretations, among which only one is the intendedone, viz. that organisms of the type Org1 in which proteinProt1 is dysfunctional are at risk to develop the patholog-ical phenotype Phen1. This interpretation is not formallydescribed anywhere, because it is assumed that databasecurators and users would not succumb to erroneous inter-pretations, such as that all proteins of Prot1 are includedin at least one organism of type Org1, or that organ-isms of type Org1 have as part at least one protein of thetype Prot1 and exhibit specifically at least a Phen1. There-fore, a formal description would be fundamental for thecorrect interpretation of the database content in othercontexts.On the other hand, there is an increasing number ofbiomedical ontologies in which logic-based axioms pro-vide precise descriptions, which indeed enable formalreasoning. Such axioms are expressed in Description Log-ics (DL) [8] using the Web Ontology Language OWL2[9]. DL queries can be answered based on satisfiabilitytesting and class subsumption. For instance, such queriesenable to retrieve Parkinsons disease in a query whensearching for diseases that affect the extra-pyramidalsystem, if Parkinsons disease has been formally char-acterised as a disorder located in the basal ganglia ofthe brain, and the latter as part of the extra-pyramidalsystem.This division between database content and structureon the one hand (with its implicit meaning) and ontol-ogy content on the other hand (with its explicit meaning)is, currently, an obstacle towards querying both together.Given this picture, several questions arise:i. How can the implicit knowledge about entities andrelationships described in the structure of abiological database be represented?ii. How can the content of databases be interpreted, i.e.,which domain entities are represented by the dataelements and their connections?iii. Are structure and content of biological databases ofontological nature?iv. If this is the case, how can they be translated intoaxioms or assertions in a commonly used ontologylanguage, and which representational patterns mightbe considered?v. Once database structure and content are expressedby formal-ontological means, how can existingbio-ontologies be plugged into this structure?vi. Given a seamless integration among thesecomponents, are there benefits for content retrieval,regarding correctness, completeness, anduser-friendliness?vi. Is such a system capable to accommodate largeamounts of data in biological databases, alsoconsidering the size of a domain ontology?Addressing questions i-iv, we hypothesise that there arefeasible ways to express implicit and explicit databasecontent by formal-ontological means and combine thiscontent with pre-existing domain ontologies.Regarding question v, previous work has shown howcontent of tables in scientific publications can be inter-preted on formal grounds [10]. Question vi has beenaddressed in [11], which introduced the reasoning capa-bilities of querying highly axiomatised bio-ontologies.Question vii needs to be addressed after answering ques-tions i-iv, but is beyond the scope of the present paper.We will demonstrate how entities referenced by a typ-ical extract from a biomedical database can be inter-preted under several ontological viewpoints, viz. regard-ing the introduction of individuals (IND), the additionof new axioms to existing classes (DISP) and the intro-duction of additional defined classes (SUBC and HYBR).The resulting OWL models are, then, tested under threeaspects:i. Database content retrieval: classes or individuals areretrieved by means of DL queries;ii. Information completeness: is the interpretationgenerated able to answer user queries?iii. DL complexity and decidability: in order to solve DLqueries, there should be theoretical guarantees thatthe machine performs under a reasonable cost andfinite time (complexity) and always finishes its task(decidability).Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 3 of 14MethodsThis section describes the ontology engineering princi-ples we subscribed to, as well as the data we gathered toexemplify our approach.Engineering principlesFirstly, we believe that ontology structure and contentshould be driven by the underlying reality, rather thanby specific application needs. We subscribe to the prin-ciples of the OBO Foundry [4], and emphasise the useof a principled upper-level ontology, here BioTopLite2(BTL2) [12], which offers a set of high-level classes,together with constraining axioms, using a small numberof core relations. Classes like Organism, Mono molecularentity, and Body part facilitate the alignment with otherontologies like GO, PRO, SNOMEDCT and ChEBI. BTL2can also be aligned with most of BFO [13] and the OBORelation Ontology [14]. BTL2 regards all instances of itsclasses as implicitly time-indexed, thus solving the ambi-guity problem of using binary relations for the caseswhere BFO2 [13] requires ternary ones, which are notexpressible in OWL [15].The fundamental role of Description Logics (DLs) [8]is justified by the widespread use of the Web OntologyLanguage OWL2 [9], supported by popular editors andclassifiers [16]. We use OWL-DL, which corresponds tothe language specification SROIQ [17], and which com-bines expressiveness with complete and finite reasoningpower. OWL2 supports classes, binary relations (objectproperties), and individuals, together with related axiomsand assertions, for which we will use the OWL2 Manch-ester Syntax [18]. Important for DL is the distinctionbetween ABox and TBox. The TBox contains termi-nological" class-level axioms, i.e. the ontological contentproper, whereas the ABox contains contingent asser-tions" about individuals.DispositionsReal world entities are often described in terms of dis-positions, i.e., tendencies of something to act in a certainmanner under given circumstances resulting from natu-ral constitution, nature, quality, or orderly arrangement.Saying that all animals are organisms is a universal state-ment; stating that all humans are able to develop diabetesmellitus type 2 is a dispositional statement. Several works[12, 1921] have suggested to include dispositions inbiomedical ontologies; e.g., the disposition to pump bloodis present in all healthy organs of the type Heart.Large parts of biomedical database content seem to bedispositional: In biochemistry, a statement that a proteinA participates in a process B does probably not meanthat all instances of A constantly participate in a pro-cess of type B, but rather that all instances of A have thedisposition to participate in such a process. Biomedicalobservations yield statistical results, which indicate thatparticipants of an experiment are ascribed to certain capa-bilities (e.g. to participate in B under certain experimentalconditions) [19, 22].Information content entitiesFinally, database content as such needs ontologicalscrutiny, as highlighted in [7]. Database content is onto-logically best characterised as information content. Thisrequires a strict distinction between (i) the databasecontent proper and (ii) the entities in the world ref-erenced by the former. As well as the data in clinicaldocuments, biomedical database content is connected bya specific relation (often named represents, isAbout,or denotes) with biomedical entities. Such informationcontent entities do not necessarily denote particulars (i.e.,instances) in the domain described. A myocardial infarc-tion record entry about a patient recently admitted to theemergency room may have the attribute probable, evenif the patient does (in fact) not have any heart problem.Similarly, a database entry on, e.g., the relation betweenprotein Pk and phenotype Ti in an organism Om may beaffected by experimentation, reporting, or curation errors.Running exampleFor the analysis reported in this paper, we selected atypical biological database example (cf. Table 1), gener-ated by joining data from UniProt [1] and Ensembl [23]by standard database querying (Additional file 1). Thiswas performed in order to retrieve all related recordsto the metabolism of homocysteine and other sulphu-rated amino acids, like methionine and cysteine (see [24]for more information regarding homocysteine metabolicpathway).From UniProt (release 2015_01), we retrieved 21,868records, and (exactly) 1000 from Ensembl (release 78). Allsample data were retrieved on January 22nd, 2015. Datafrom the NCBI Taxonomy (2015AA) were incorporatedat the end of the retrieval process, adding the taxonomyidentifiers of the organisms from which data are recordedin UniProt and Ensembl.Using the ontology editor Protégé v.5, supported by theDL classifier HermiT [16] v.1.8.3, we created four OWL2models, each of which followed a different strategy. Theywere created according to the data organisation presentedin Table 1, based on a sample record (Table 2). Termsfor individuals were created according to the same orga-nization, but identified by a bold lower-case letter and arandom number, like p1001 or m2001 as terms for anindividual protein and molecule (respectively).The four OWLmodels uniformly represent all informa-tion entities (database content) as individuals. The modelsdiffer, however, in the way how referents of this informa-tion are interpreted, viz. (i) as individuals (Additional file 2),Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 4 of 14Table 1 Typical data record from the joined databases Uniprot and Ensembl. The abstraction introduces the symbols of the exampleontologiesField Source Content AbstractionProtein (PR) UniProt Cystathionine gamma-lyase Prot1Organism (NCBI Taxonomy) NCBI Taxonomy via UniProt Rattus norvegicus (Rat) Org1Processes (not distinguishingbetween Biological process andMolecular function in GeneOntology (GO))GO via UniProt hydrogen sulfide biosyntheticprocess; negative regulation ofapoptotic signaling pathway;positive regulation of I-kappaBkinase/NF-kappaB signaling;protein homotetramerization;protein sulfhydrationBProc1, BProc2, . . . , BProckCell components (GO_cc) GO via UniProt cytoplasm; nucleus;extracellular vesicular exosome;CComp1, CComp2, . . . , CCompxSmall molecules (ChEBI) ChEBI Homocysteine Mol1,Mol2, . . . ,MolyPhenotypes Ensembl Amino acid metabolism errors;cataract; Gamma-cystathionasedeficiencyPhen1, Phen2, . . . , Phenz(ii) as fully defined subclasses (Additional files 3 and 4) (iii)as disposition (Additional file 5) classes.In the following, names of individuals are picked outin bold face with lower case initials, in contrast to classnames in italics with leading upper case character. Sym-bols that include white spaces are enclosed in singlequotes, e.g., has part.In order to test the fitness of these models, four com-petency questions (CQs) were formulated in natural lan-guage and then reformulated as DL queries (cf. Table 3)in order to emulate typical query operations over ontolo-gies and databases, performed by biomedical researchers.Q1 aims at retrieving biological processes in which cer-tain proteins participate; Q2 retrieves the cellular com-ponent(s) a given organism includes, together with theproteins found in them. Q3 retrieves proteins recordedas participant of biological processes in a given organism.Finally, Q4 retrieves organisms able to exhibit a specificphenotype.ResultsTable 1 represents the typical structure of the data ana-lyzed in this work. It is categorized and organized by thefollowing structure: one protein term (e.g., CBS) ; one taxon term (e.g., Rattus norvegicus);Table 2 Schematic view over UniProt, NCBI Taxonomy andEnsembl dataProtein Organism Bio Process Cell component Molecule PhenotypeProt1 Org1 BProc1 ; CComp1 ; Mol1 ; Phen1 ;Bproc2 ; CComp2 ; Mol2 ; Phen2 ;Bproc3 CComp3 Mol3 Phen3 one to many terms for GO biological processes or GOmolecular function (e.g., Blood vessel remodelling ); one to many terms for GO cellular components (e.g.,Cytoplasm); zero to many terms for phenotypes (e.g., Endocrinepancreas increased size ); one to many terms for small molecules (e.g.,Homocysteine)This structure was imported from UniProt andexpanded with mappings to Ensembl via identifiers. Fol-lowing [25], we treat terms from GO Molecular functionas referring to processes. This is supported by the factthat the latter ones are named activities in GO; andheuristically, by the fact that in experiments molecularfunctions are always discovered through their realizations,i.e., through the observation of processes or their results.Table 3 Queries translated into DL queriesQ1  Which biological processes have proteins of the kind Prot1as participant?Biological process and (has participant some Prot1)Q2  In which cellular locations is Prot1 active in organisms of thetype Org1?Cellular component and (is included in some Org1) and(includes some Proti)Q3  Which proteins are involved in processes of the type BProc1 inorganisms of the type Org1?Protein and (is participant in some BProc1) and (is included insome Org1)Q4  Which organisms are able to exhibit a specific phenotype Phen1?Organism and (is bearer of some (Disposition and (has realizationonly Phen1)))Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 5 of 14Even if all terms from the database are understood, thereare still numerous open questions regarding the precisemeaning of such a database record. We fill this gap byeliciting the necessary implicit knowledge from a domainexpert familiar with the process of database population,performing an in-depth ontological analysis in the lineof Gangemi et al. [26]. This analysis begins with the for-mal categorization of relations and basic classes, undera suitable upper-level ontology. This was done by manu-ally aligning the top-level classes of the domain ontolo-gies GO, ChEBI and PR under the top-level ontologyBTL2 [12].Once the entities are categorised, the following ques-tions need to be answered: How are the structural elements of a database (i.e.tables, fields) related to each other? Whichknowledge is missing that is required for correctlyunderstanding these relations? Which expressiveness is required to axiomatise thecontent in a logic-based language in an appropriateway to represent all implicit and explicit content? Which additional entities need to be included intothe ontology (e.g., Dysfunctionality and Dispositionin the above example)? Which compromises and simplifications may beneeded? Which propositions are categorical, whichones are dispositional? [19] Do we have to includeABox entities (individuals)?When it comes to an ontology-based representationof database content (as exemplified in Table 1), we facethree interpretation challenges: (i) the data points and col-umn headers, (ii) the relation between the data pointsand the column headers, and (iii) the relations among thecolumns.Task (i) is facilitated by the fact that many of the contentterms are already represented in biomedical ontologieslike GO. Besides, the natural language terms used as fieldlabels can easily be aligned to content from other ontolo-gies. In our case, most field labels could be aligned withBTL2.Task (ii) will normally be accounted for by the subclassor instantiation relation: the content terms denote classesor instances of the class denoted by the field label. E.g.,Cystathionine gamma-lyase subClassOf Protein, Rattusnorvegicus subclassOf Organism, etc.Task (iii) requires reference to the implicit knowl-edge a scientist is likely to have. For example, aUniProt record that points to Methylation, Bos tau-rus and Methionine synthase expresses that in a givenexperiment with cattle tissue an instance of Methioninesynthase was observed to participate in a methylationprocess.In the following, we investigate four differentapproaches for representing the meaning of the contentand structure of biological databases:1. Representation as sample individuals (IND);2. Representation as defined maximally fine-grainedsubclasses, seeing as referents of the informationentities in the database (SUBC);3. Representation with dispositional properties (DISP);4. Hybrid representation with subclasses anddispositions (HYBR).Our sample ontologies include one Protein class (Prot1),one Organism class (Org1), and three subclasses of eachof Cell Component (CComp1,...,3), Biological Process(BProc1,...,3), Small Molecule (Mol1,...,3), and Phenotype(Phen1,...,3), respectively (Table 2).Representation as individuals (IND)The first representation is motivated by the fact thata database entry is about a concrete experiment, inwhich individual entities in space and time are described,e.g., a piece of biological material, a certain amount ofmolecules, the phenotype of an individual rat, etc. Thisview is agnostic with respect to whether the observedphenomena are manifestations of natural laws or not.In this perspective, our sample data report that individ-ual protein molecules p1001, p1002, . . . of the type Prot1exist in some particular cell components cc1001, cc2001,. . . of the types CComp1,...,n of some organisms o1001,o1002, . . . of the type Org1. Biomolecular process individ-uals bp1001, bp2001, . . . that are members of the classesBProc1,...,m include moleculesm1001,m2001, . . . of the typeMol1,...,k (specific to Org1). Finally, the dysfunctions of theproteins p1001, p1002, . . . cause the organisms o1001, o1002,. . . to display one ormore phenotypes ph1001, ph2001, . . . ofthe type Phen1,...,n (Table 2).We are aware that only collections of molecules (andnever single molecules) and activities thereof are observed[22]. However, assuming that the observation of thebehaviour of collective individuals allows us to deducewhat happens at the level of individuals (as done whendescribing chemical reactions or biochemical pathwayswith symbols denoting single molecular entities), we herepopulate the ABox with single, non-collective, sampleentities and the relations among them. Index numbers arealigned arbitrarily.In the following we describe our interpretationapproach. For instance, individual protein molecules inindividual organisms are active in processes, e.g., withincell components, like:p1001 is included in cc1001cc1001 is included in o1001Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 6 of 14We also introduce instances for protein molecules thatparticipate in process instances within an organism:p1004 is participant in bp1001p1004 is included in o1004Proteinmolecules participate, within a particular organ-ism, in process instances (e.g., bp1001) that synthesisespecific molecules (e.g.,m1001):p1010 is participant in bp1001bp1001 has participant m1001p1010 is included in o1010Whenever the database fields for processes, molecules,or cell components have more than one entry, thedatabase, unfortunately, leaves open which processesinvolve which molecules and where they are located.Ideally, this information might be retrieved from othersources. Otherwise, a relation between an individualprocesses and molecules participating in them can beexpressed by referring to an appropriate process individ-ual bp1001 and an appropriate individual moleculem1001.An analogous strategy is possible to express the participa-tion of cell components in processes.bp1001 includes m1001There are organisms with specific phenotypes, in whichthere is a protein of a certain type, which is howeverdysfunctional. Dysfunctionalities can be represented asqualities, here also expressed as the individual d1001.p1013 is included in o1013o1013 includes ph1001p1013 is bearer of d001For these data to be interpreted in a DL context, ABoxentities (in this scenario) are to be understood as arbi-trary individuals that participate in a specific experiment.For the sake of simplicity, for each assertion that can bederived from the database, new terms for individuals arecreated.Another simplifying assumption of this approach is thatall database terms are non-empty, i.e., they actually refer tosome existing entity. Each information-content individualin the database needs to represent an existing individualinvolved in the experiment. This is, of course, problem-atic if the data is wrong due to curation errors, or if thebiological processes recorded did not really happen.Representation as multiple subclasses (SUBC)The second approach interprets database terms as refer-ring to maximally fine-grained defined classes. The nam-ing of these new subclasses follows strict naming criteriaas exemplified below. This is important for extracting theoriginal class names from the subclass names, becauseonly the former ones are interesting for querying. Forinstance, the database represents a protein class Prot1that is connected with an organism class Org1 and a bio-process class BProc1. Accordingly, we create the classesProt1_in_Org1_in_BProc1, Org1_with_Prot1_and_BProc1,and BProc1_in_Org1_with_Prot1 with appropriate full def-initions (Fig. 1).We leave open whether these defined classes are empty.In a way, defined classes are nothing more than logicalartefacts. For this reason, the creation of such definedOWL classes has a modest ontological engagement. Nev-ertheless, these defined classes can serve as the referentsof the data instances [27].In order to fully incorporate the idea that databaseentries are individuals that refer to classes by means ofannotations, we create the following description logicformula for each database entity:databaseEntryx type represents only(DefinedClass1 or DefinedClass2 or . . . or DefinedClassN )Bearing this representation in mind, querying can belimited to the expression in parentheses, which bringstwo advantages, viz. that neither individuals and norvalue restrictions would impact the performance of thereasoner.In the following, the modelling patterns are given forproteins, organisms, small molecules, biological processesand phenotypes. Here, the index variable i denotes arecord, in which field (e.g., for protein) is filled exactlyFig. 1 Example of subclass creation and relations enabled to be usedin class definitionsSantana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 7 of 14once; hence the notation Proti1 . Accordingly, the notationfor organisms is Orgi1 , because there is exactly one organ-ism type referred to by a record. The other fields maybe multiply filled; therefore the notation is, e.g., BProc1 ,BProc2 , . . . , BProcm .Proteins: We introduce classes for dysfunctional pro-teins as well as for organism-specific proteins and theircombination:Proti1_Dysf equivalentTo Proti1 andis bearer of some DysfunctionalProti1_in_Orgi1 equivalentTo Proti1 andis part of some Orgi1Proti1_Dysf_in_Orgi1 equivalentToProti1_Dysf and Proti1_in_Orgi1Specifically, subclasses are created to represent the pos-sible links among classes denoted by annotations within arecord. For instance, the subclass Proti1_in_Orgi1 is gener-ated to express that we deal with a protein of an organismof a certain type Orgi1 . In addition, subclasses are intro-duced for phenotypes, processes, cell components andmolecules:Proti1_Dysf _in_Orgi1_with_Phen1,...,o equivalentToProti1_Dysf _in_Orgi1 andis part of some (Orgi1 and(includes some Phen1,...,o))Proti1_in_Orgi1_in_BProci,...,m equivalentToProti1_in_ Orgi1 and is participant in some BProc1,...,mProti1_in_Orgi1_in_CCompi1,...,n equivalentToProti1_ in_Orgi1 and is included in some CComp1,...,nProti1_in_Orgi1_with_Mol1,...,k equivalentToProti1_in_Orgi1 and (is participant in some(Process and(has participant someMol1,...,k )))Organisms: Classes are introduced for organisms withproteins in general, and for organisms with organism-specific proteins in particular. The latter ones are alsospecialized by phenotypes, processes and molecules:Orgi1_with_Proti1 equivalentTo Orgi1 andhas part some Proti1Orgi1_with_Proti1_Dysf equivalentTo Orgi1 and(has part some Proti1_Dysf )Orgi1_with_Phen1,...,o_and_Proti1_Dysf equivalentToOrgi1_with_Proti1_Dysf and includes some Phen1,...,oOrgi1_with_Proti1_and_BProc1,...,m equivalentToOrgi1 and (has part some (Proti1 and(is participant in some BProc1,...,m)))Orgi1_with_Proti1_and_Mol1,...,kequivalentTo Orgi1 and(has part some Proti1 ) and(is participant in some (Process and(has participant someMol1,...,k)))Small molecules: We introduce classes for smallmolecules contained in organisms, and further specifythese classes by stating the type of the proteins with whichthese small molecules interact, i.e., with which they arerelated by participating in the same biological processes.Mol1,...,k_in_Orgi1 equivalentToMol1,...,k andis part of some Orgi1Mol1,...,k_in_Orgi1_with_Proti1equivalentToMol1,...,k_in_Orgi1 and(is participant in some (Process and(has participant some Proti1 )))Processes: Subclasses are introduced for the partici-pating proteins which are included in a certain type oforganism.BProc1,...,m_in_Orgi1_with_Proti1equivalentTo BProc1,...,m and(has participant some Proti1 ) and(is included in some Orgi1 )Phenotypes: Subclasses are introduced for associateddysfunctional proteins and their respective organisms.Phen1,...,o_in_Orgi1_with_Proti1_Dysf ?equivalentTo Phen1,...,o and(is included in some Orgi1_with_Proti1_Dysf )The querying strategy for this representation modelis to check whether specific subclasses are retrieved ornot. For instance, if we want to retrieve processes withProti1_in_Orgi1 , the corresponding DL query isProcess and (has participant some Proti1 ) and(is included in some Orgi1 )The automated reasoner delivers a list with the corre-sponding defined subclasses, such as:BProc1_in_Orgi1_with_Proti1 ,BProc2_in_Orgi1_with_Proti1 orBProc3_in_Orgi1_with_Proti1 .A disadvantage of the SUBC interpretation is that itrequires the introduction of classes that are not to befound in the ontologies used for annotation (such as GOor PRO) and that these classes are retrieved by the abovequery. For querying purposes, their superclasses must beidentified, viz. BProc1, BProc2, and BProc3. This requiressome post-processing of the results as explained below.Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 8 of 14Thus, subclasses for all types of entities referred to ina database are created, which is on the one hand highlyprolific, because every possible association of entries intable fields must be combined into a new defined class.On the other hand, the expressiveness power of theDL dialect needed is reduced to the EL++ [28], cor-responding to OWL2-EL, which is known for its goodscalability [28].Representation with dispositions (DISP)In the representational patterns IND and SUBC, databaseentries were seen as observations about individuals, eitherrepresented as existing ABox entities or as specific,potentially empty, subclasses. Whereas INDmakes strongexistential claims, stating that the content of a field isinterpreted as representing an actually existing biologicalindividual, the ontological engagement of SUBC is moremodest, as it allows empty classes (although non-denotingdatabase entries are rather the exception than the norm).Both IND and SUBC avoid to claim any universal state-ment of the form For all A there is some B for any classA referred to by database.In contrast, the DISP pattern goes a step further, assum-ing that the database content has been created to giveinsights into scientific regularities in the sense that allmembers of a class have a disposition to behave in a certainway, thus exhibiting a law of nature.To ascribe a disposition for a certain process P to anobject m does not imply that m actually and at all timesparticipates in an instance of P. It implies only that thephysical structure of m allows m to participate in pro-cesses of the type P. The proposed modelling pattern inDL is the following [29]:Object1 and Object2 and . . . and Objectn subclassOfis bearer of some (Disposition and(has realization only Process1))whereObject1 refers to a class; andObject2 toObjectn referto other classes, or to statements of the type ClassA andrelation some ClassB.The bearers of dispositions are independent continu-ants [19, 20]. Thus, possible bearers of dispositions, inour case organisms, proteins, small molecules and cellcomponents.For organisms and proteins, we create a series of gen-eral class inclusions (GCIs) in OWL, with the class ofinterest (e.g. Proti1 ) intersected with the constraining con-ditions at the left hand side (e.g. is part of some Orgi1 ).Dispositions are, then, ascribed to organism-specific pro-teins within certain cellular components. We introducedispositions to perform biological processes that have cer-tain kinds of molecules as output. Here is the generalpattern.Proti1 and is part of some Orgi1 subClassOfis bearer of some (Disposition andhas realization only BProc1,...,m) andis bearer of some (Disposition andhas realization only (Process andhas participant someMol1,...,k))In this and the next formula, the restrictionis included in some(CComp1 or CComp2 or . . . or CCompx)could be added. However, this restriction is rather weakdue to the disjunction, which may leave room for severalclasses to be added.As a rule, dispositions have realisation conditions. Therealisation of the disposition of a protein to participatein a given biological process depends, among others, onthe chemical environment within the organism and thecell component. Such dispositions are introduced for allproteins of the type Proti1 , under the condition that theyare included in Orgi1 as well as in one or more cellularcomponents (CComp1,...,n). These dispositions are definedin terms of the process types BProc1,...,m processes, or interms of unspecified processes in which one ormore smallmolecules (Mol1,...,k ) participate.Our interpretation of the example is that the abilityto exhibit a certain pathological phenotype is attributedto organisms in virtue of having a dysfunctional protein.Again, the table does not tell us which kind of dysfunc-tion affects which kind of process that results in whichphenotype:Orgi1 and (includes some (Proti1 and(is bearer of some Dysfunctional))) subClassOfis bearer of some (Disposition and(has realization only Phen1,...,o))Formally, we could characterize a class of smallmolecules as bearing dispositions in the following way:Mol1 orMol2 or . . . orMolksubclassOf is bearer of some (Disposition and(has realization only (Process and(has participant some Proti1 ) and(is included in some Orgi1 ) and(is included in some(CComp1 or CComp2 or . . . or CCompn)))))As we said, dispositions could theoretically also beascribed to cell components, as these are also independentcontinuants. However, according to the shared back-ground assumptions of biologists, cellular components arenot participants but only the locations of the biomolecularprocesses under scrutiny. That an entity bears a disposi-tion of being the arena in which a process might take placeSantana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 9 of 14would require the extension of either the notion of dispo-sition or the notion or participation. Therefore, we refrainfrom ascribing dispositions to cell components.The use of general class inclusions (GCIs), i.e. the useof complex class expressions on the left hand side of theaxiom, is a straightforward application of the above pat-tern. However, this strategy does not support retrievalpurposes, as DL queries only retrieve simple names ofclasses or individuals, but not complex expressions.Hybrid class-level representation (HYBR)To avoid complex class expressions on the left hand sideof GCIs, a feasible approach that supports DL querieson dispositions would require equivalence axioms as thefollowing:Orgi1_with_Proti1_Dysf equivalentTo Orgi1 and(has part some (Proti1 and(is bearer of some Dysfunctional)))Here,Dysfunctional is a class that qualifies a given Proti1as being causally related to a pathological phenotype.The class Orgi1_with_Proti1_Dysf can then be used onthe left hand side of an axiom that states the disposi-tions of organisms of the type Orgi1 under the conditionof having dysfunctional proteins of the type Proti1 . Thiscorresponds to the modelling pattern SUBC.In our example, this means that the SUBC modelrequires n defined classes for organisms of the type Orgi1that have dysfunctional proteins of the type Proti1 andwhich include a phenotype Phen1,...,o, whereas the DISPapproach requires one axiom with organisms of the typeOrgi1 that have dysfunctional proteins of the type Proti1 at the left hand side, with expressions on Phen1,...,o at theright hand side:Orgi1_with_Proti1_Dysf subClassOfis bearer of some (Disposition and(has realization only Phen1,...,o))This leads to a hybrid approach in which subclass def-initions are still needed. The hybrid representation maybe preferred as being more parsimonious, which howeverhas to be traded off against the increase in DL expressive-ness, viz. from OWL-EL to OWL-DL, at least when DISP(like proposed for SUBC) avoiding generation of a hugenumber of very specific subclasses, as in SUBC.Evaluating representation scenariosWe created four DL queries (Q1Q4) (cf. Table 3) toevaluate (i) database content retrieval, using ontologies asquery vocabulary; (ii) information completeness; and (iii)DL complexity and decidability. Q1 aims at retrieving bio-logical processes in which certain proteins participate; Q2aims at retrieving the cellular component(s) a given organ-ism includes, together with the proteins found in them.Q3aims at retrieving proteins recorded as participant of bio-logical processes in a given organism. Finally, Q4 aims atretrieving organisms able to exhibit a specific phenotype.Queries on SUBC or HYBR models require further pro-cessing, because they retrieve the subclasses introducedin the models, e.g., Phen1,...,k_in_Orgi1_with Proti1_Dysf,whereas the user is only interested in retrieving the classesused in the annotation, such as Phen1,...,k in our case.This is easily achieved by extracting the originalclass names from the constructed names of eachretrieved class; e.g., Phen1,...,k is extracted fromPhen1,...,k_in_Orgi1_with Proti1_Dysf .Results from Q1Q4 are displayed in Table 4. Apartfrom the OWL profiles required, the result shows howindividuals can be retrieved with IND, and classes in two-step queries for SUBC and HYBR. DISP does not retrieveanything due to the use of GCIs without class definitions.As expected, SUBC generates more classes and axiomsthan DISP and HYBR. In IND, there are more axiomsthan in SUBC, DISP and HYBR due to the large amountof relationships created among the individuals while anOWL model following the IND strategy may not includeany class definitions. IND and SUBC were not able toretrieve Q4, which includes a disposition axiom and canbe answered only by HYBR.In the context of an integrative framework, combiningontologised databases and bio-ontologies, interestingvariations of these competency questions can be imag-ined. These variations can exploit the axiomatic contentof the linked ontologies, such as subclass axioms or rolerestrictions. Expressed in DL queries, these variationswould require none or minor syntactic variations:Table 4 Query results together with characteristics of the four ontology implementations (without importing BTL2)Model Q1 Q2 Q3 Q4 Classes Axioms Individuals OWL profileIND bp1001, cc1001, p1004  24 207 51 OWL-DLbp2001, cc2001,bp3001 cc3001SUBC BProc1 CComp1 Proti1  68 149 0 OWL-ELDISP     29 70 0 OWL-DLHYBR BProc1 CComp1 Proti1 Orgi1 48 129 0 OWL-DLSantana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 10 of 14 In Q1, a query could target a number of biologicalprocesses by a common ancestor process, or a phaseof a certain process provided by GO; In Q2 and Q3, the organism could be substituted by abiological taxon or other groupings of organisms,such as provided by the NCBI taxonomy orSNOMED CT (organism branch); In Q1 and Q3, processes could be clustered byquerying for metabolite characteristics. This can be(for instance) provided by GO extensions, like theGO  ChEBI linkage. In Q4, phenotypes could be queried through howthey are characterised, for instance by certain bodylocations. This can be achieved such as provided bySNOMED CT body structure and disorder.Users should choose an interpretation approach thataccounts for their respective requirements and fits tothe computational resources available. With IND, thewhole semantic expressivity belongs to the ontology theindividuals are imported into; there is no guarantee thatthis ontology is expressive enough to support reasoningand querying, whereas the patterns provided by SUBCand HYBR come with axioms that fulfil this task.Our results indicated that DISP and HYBR promisebetter results when reasoning over biomedical databases.However, limitations may arise for these approaches dueto the nontrivial use of dispositions and scalability prob-lems, because the reasoning complexity increases withhigher expressivity. In these respects, SUBC might be themost parsimonious solution, as it may be less problem-atic for scaling when applying reasoning and performingqueries, with the expense of simulating relations to avoidthe complexity that comes with the use of dispositions.DiscussionRecently, ontology-aided interpretation of databases hasemerged as a research topic in the biomedical domain,e.g., for disambiguating the sense of free-text keywordsin query generation to access data repositories [30], or asa means to interpret proteomics data [31]. As biomedi-cal observation databases, (e.g.) for proteomics, are stillinterpreted manually [7], led to the suggestion of annota-tion tools that support data interpretation. In these works,authors suggest a deeper use of ontologies to supportinterpretation, which is something that goes beyond ofwhat is currently performed with functional annotations.Aiming to attain this purpose, we have proposed fourrepresentation strategies: IND, SUBC, DISP and HYBR.Interpreting data as individuals (IND)The representation pattern IND is completely based onsingle individuals (ABox entities), present in the underly-ing experimental assays the results of which are referred toby the database content. This approach, similarly to ontol-ogy population [32], refrains from raising any ontologicalclaim apart from asserting the existence of individualsand relations among them. The ABox entities can then beretrieved by DL queries, but the performance problemsof large ABoxes with expressive TBoxes are known [47]and may therefore hamper the theoretical issue of scal-ability. In addition, the assertion of existence is an esti-mation, because data may exhibit errors, especially whennot manually curated and, e.g., extracted from literatureabstracts by natural language processing.IND andOntology-based Data AccessPreviously, OWL models have been created in whichOWL axioms and assertions were automatically gener-ated from database schemes [33]. These models, how-ever, represent (first of all) data (information entities)and not the reality denoted by the data. Our approach,in contrast, aims at representing the latter, e.g, to whichclasses the information entities denotes and further rela-tions among them. In addition, relations extracted fromdatabases are semantically idiosyncratic and shallow, e.g.,neglecting the complexity of the underlying reality, ofwhich a database schema represents nothing more than acustomized view.For instance, database integration following theOntology-Based Data Access [34] (OBDA) approachrelies on a limited set of ontological relations that areprovided by ontologies. In OBDA, integration relies onconnecting information present in databases with ontolo-gies, without discussing which interpretation of the datais more appropriate, i.e., whether the data refer to indi-viduals, classes, or classes of disposition bearers (neitherof which is expressed in the database nor defined in theontology). In practice, OBDA enables the user to retrieveindividuals from a database virtually, e.g., by means ofan ontology used as query vocabulary and an engine toconvert queries in SPARQL [35] to its respective SQLequivalent, or retrieve RDF triples such as in Bio2RDF[36] or the UniProt SPARQL Endpoint [37]. Such inter-pretation issues may be not so relevant for daily databaseusage, e.g., accessing or retrieving queries; but for biolog-ical databases, which include data from real experiments,raising them is quite relevant.Approaches that rely on SPARQL queries, like OBDA,do not go further into how data are to be interpreted,which is crucial for the biomedical domain. E.g., queriescreated in SPARQL and ontologies formalized in OWLemploy different semantics, e.g., of which the latterenables more complex reasoning tasks (e.g.,classificationand consistency checking) than the former. Reasoningis crucial for validating content interpreted according tothe semantics provided by ontologies, which frequentlyemploy OWL.Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 11 of 14Opposed to the stance that ontology artefacts should,first, represent purpose-oriented data structures, wheredifferent use cases might require different, partly incom-patible design decisions [38], we reinforce the interop-erability aspect of ontologies, which we consider to berepresentational artefacts whose representational unitsare intended to designate classes or types in reality andto relate them to each other [39], which also requiresagreement on a set of high-level categories and relations.Databases and temporal contextsCeusters and Smith [40] describe an approach called Ref-erent Tracking, which is mainly devoted to the identifica-tion of individuals from Electronic Health Records (EHR).Referent tracking is based on the generation of triples inorder to record how individuals are related to each otherwithin a specific context. This approach is similar to ourIND strategy, but equally affected by the problems of non-referring representational units [41], e.g., in case of falsediagnoses or abandoned care plans.The domain upper-level ontology BTL2 had been cre-ated with the purpose of enforcing temporal contextsfor continuant individuals [15]. Whereas in EHRs timeindexing is necessary to represent patients histories, thebiological annotation case described in this paper refrainsfrom temporal indexing, which may become relevantwhen further describing the annotation process itself,where temporal changes occur as data is automaticallyannotated and later reviewed by human curators.Interpreting data as subclasses (SUBC)The inability to represent non-denoting database informa-tion was addressed by the SUBCmodelling patterns whichcreated a defined subclass for each putative referent. Ourapproach for this modelling is agnostic to whether suchclasses are instantiated or empty, as their only rationale isto act as referents of information entities in the database.Therefore, this representation can (in a way) be consideredontologically neutral in the sense that we only describepotentially instantiated classes without being committedto the actual existence of any instances. Instead, theOWL model for SUBC exemplify a way to represent dis-course, regardless of whether meaningful or nonsensical.However, we have shown that an OWL-EL extract rep-resented with SUBC successfully retrieves the desireddatabase content.On many occasions, researchers already use ontologyterms in biological databases to express relations amongclasses, such as that in certain types of organisms, cer-tain biological processes are performed by or with the aidof certain proteins. In such cases, the SUBC modelling ismore natural and will reflect the observed reality.However, one has to deal with a problem that so oftenappears in the area of knowledge representation, knownas the frame problem. When one ascribes a certain logi-cal property to a class, it means that all members shouldpossess it. But in biology, there are always exceptionsand variations that arguably falsify universal statementsabout classes. This all-or-nothing stance can be seenas a drawback of the SUBC approach, which has beenextensively discussed. The usefulness of a SUBC approachhas been proven in practice in the realms of knowledgerepresentation applications; nevertheless, proposals toaccommodate exceptions [42], modal [43], and even prob-abilistic, fuzzy solutions [44] have appeared both in KRand DL [45, 46].Interpreting data with dispositions (DISP) and the hybridrepresentation (HYBR)The DISP and HYBR representation strategies, attemptsto extract ontological statements in a stricter sense,i.e. accounts of scientific laws expressed by universallyquantified statements about all members of a class. Thisis possible by introducing dispositions, e.g., by statingthat all organisms with a certain dysfunctional protein arepredisposed to develop certain pathological phenotypesunder certain conditions only.The DISP approach may be considered ontologicallyproblematic, as it is quite promiscuous in ascribing dispo-sitions on class level. What is observed in an experiment isthe outcome of a particular process (which might be a col-lective process). From the observation of the outcome, itis inferred that particular process happened, which givessupport to the assumption that the participating partic-ulars have had the disposition to participate in such aprocess.The problem lies in the extrapolation from the obser-vation of a single case to all members of a certain class such inductive inferences are notoriously difficult. Theymay be quite safe when describing the behaviour of smallmolecules: knowing that one particular molecule has acertain disposition, we can quite safely assume that othermolecules of the same kind share this disposition, as wecan think of no intrinsic property that could make a dif-ference here. However, on the biological level, systems aremuch more complex. If a gene defect in a certain individ-ual organism increases the risk for, e.g., diabetes mellitus,it does not exclude the possibility that in other organismswith the same gene defect there is no such risk. We would,that is, not be justified to ascribe an increased diabetesrisk to the latter population (though we were justified toascribe them a certain tendency to do so [19]).There is no principled contradiction between SUBC andDISP. The fact that the class inclusion axioms proposedin DISP to introduce conditions are not suitable for DLquerying, approximates the second and the third mod-elling approach in the sense that the latter also benefitsfrom fully defined subclasses. Therefore, the combinationSantana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 12 of 14of these two modelling styles (HYBR) proved to yield thebest retrieval results with all four competency questions.General remarksIn this sense, the need for analysing and formalisingthe reality behind the database schemes was confirmedby our effort when creating and querying ontologicallyfounded interpretation models. Current use of biologicaldatabases might indeed demonstrate that a flat tabularstructure with the fields Protein, Organism, Process,Cellular component, Molecule and Phenotypemight workfor most standard queries. Its ontological interpretationunder a common upper-level representation aiming ata formal description of the domain itself and not justof a specific view thereof, creates added value for morecomplex queries that require semantic and not only syn-tactic integration of biomedical ontology resources.Entries from biomedical databases derive mostly fromharvesting scientific literature or, otherwise, from theresults of experiments. The veracity of these reportscan be roughly assumed, but any precise representationshould take into account that experimental, measurement,reporting, and curation errors might occur, so that a cer-tain number of entries in biological databases may be falseor even contradictory. This requires accounting for theunderlying domain knowledge that does not surface inthe database schema. Examples for these missing linksare, in our examples, that the phenotypes listed in thedatabase record are at least partly conditioned by proteindysfunctions.We do not claim that our interpretation approach is theonly possible one, or that it is exhaustive. In any case, itmight be incomplete and should therefore require refine-ment and extension by domain experts. For example, aphenotype might not only be the result of the dysfunc-tion of a protein, but may also be caused by the completeabsence of this protein in an organism.The real world applicability of the proposed approacheshas to be assessed with large datasets in the light ofcomputational constraints.ConclusionInterpretations of biological database content tend tobe ambiguous. Accordingly, we formulated the followingquestions:i. How can the implicit knowledge about entities andrelationships described in the structure of abiological database be represented?ii. How can the content of databases be interpreted, i.e.,which domain entities are represented by the dataelements and their connections?iii. Are structure and content of biological databases ofontological nature?iv. If this is the case, how can they be translated intoaxioms or assertions in a commonly used ontologylanguage, and which representational patterns mightbe considered?Answering (i), we presented a method that formalises theimplicit knowledge behind the schemas of databases likeUniProt and Ensembl. In order to account for (ii), wegrounded all classes in an expressive upper-level ontology.The result is (iii) a seamless representation of databasestructure, content and annotations as (iv) an OWLmodel.Four different ontological interpretations of databasecontent were developed and compared. The first andthe second strategy represent data individuals denot-ing either individual processes and their participants(IND), or defined classes of such entities, using maximallyexpressive OWL class terms (SUBC), respectively. Thethird strategy (DISP) makes stronger claims by universallyascribing dispositions to some of the continuant classesinvolved. The fourth strategy (HYBR) combines elementsfrom SUBC and DISP.The usefulness of the representations was assessed by aseries of competency questions formalised as DL queries,for which the hybrid representation of database referentsas subclasses together with dispositions (HYBR) yieldedthe most convincing result when considering expressiv-ity and reasoning. However, the SUBC may be well suitedfor automating interpretation, as its expressiveness scalesbetter for reasoning tasks over a large amount of data.Adding dispositional properties may constitute a usefuladd-on, although it is epistemically problematic to auto-mate the ascription of dispositions to classes based oncursory evidence on sample individuals gathered in labexperiments.Additional filesAdditional file 1: Sample data with records retrieved from UniProt andEnsembl. (XLSX 15.4 kb)Additional file 2: IND representation example. (OWL 37.3 kb)Additional file 3: SUBC representation example. (OWL 69.2 kb)Additional file 4: HYBR representation example. (OWL 31.6 kb)Additional file 5: DISP representation example. (OWL 14.7 kb)AcknowledgementsThis work was funded by Conselho Nacional de Aperfeiçoamento de Pessoal deNível Superior (CAPES) 3914/2014-03 and Conselho Nacional deDesenvolvimento Científico e Tecnológico (CNPq) 140698/2012-4.Authors contributionsAll authors contributed equally to the manuscript. FSS wrote the document,reviewed and managed comments from other authors. LJ has written andcontributed to the ontological basics of the manuscript, as well as reviewedand commented on content and organization. FF and SS reviewed andsupervised the thesis from which the whole material of this paper is based in.All authors read and approved the final manuscript.Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 13 of 14Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Centro de Informática, Universidade Federal de Pernambuco, Av. JornalistaAnibal Fernandes, 50.740-560, Recife, Brazil. 2Núcleo de Telessaúde,Universidade Federal de Pernambuco, Av. Prof. Moraes Rego, 50670-420,Recife, Brazil. 3Institut für Philosophie, Universität Rostock, D-18051, Rostock,Germany. 4Institute for Medical Informatics, Statistics and Documentation,Medical University of Graz, Auenbruggerplatz 2/V, 8036 Graz, Austria.Received: 1 November 2016 Accepted: 7 April 2017Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 DOI 10.1186/s13326-017-0125-1RESEARCH Open AccessLearning from biomedical linked data tosuggest valid pharmacogenesKevin Dalleau1, Yassine Marzougui1,2, Sébastien Da Silva1, Patrice Ringot1, Ndeye Coumba Ndiaye3and Adrien Coulet1*AbstractBackground: A standard task in pharmacogenomics research is identifying genes that may be involved in drugresponse variability, i.e., pharmacogenes. Because genomic experiments tended to generate many false positives,computational approaches based on the use of background knowledge have been proposed. Until now, onlymolecular networks or the biomedical literature were used, whereas many other resources are available.Method: We propose here to consume a diverse and larger set of resources using linked data related either to genes,drugs or diseases. One of the advantages of linked data is that they are built on a standard framework that facilitatesthe joint use of various sources, and thus facilitates considering features of various origins. We propose a selection andlinkage of data sources relevant to pharmacogenomics, including for example DisGeNET and Clinvar. We use machinelearning to identify and prioritize pharmacogenes that are the most probably valid, considering the selected linkeddata. This identification relies on the classification of genedrug pairs as either pharmacogenomically associated ornot and was experimented with two machine learning methods random forest and graph kernel, which results arecompared in this article.Results: We assembled a set of linked data relative to pharmacogenomics, of 2,610,793 triples, coming from sixdistinct resources. Learning from these data, random forest enables identifying valid pharmacogenes with aF-measure of 0.73, on a 10 folds cross-validation, whereas graph kernel achieves a F-measure of 0.81. A list of topcandidates proposed by both approaches is provided and their obtention is discussed.Keywords: Linked data, Pharmacogenomics, Data mining, Knowledge discovery from databases, Machine learning,Valid pharmacogenesBackgroundPharmacogenomics (PGx) studies how individual genevariations cause variability in drug responses [1]. Wellestablished knowledge in PGx constitutes a basis forimplementing personalized medicine, i.e., a medicine tai-lored to each patient by considering in particular her/hisgenomic context. The state of the art of this domainlies both in the biomedical literature and in specializeddatabases [2, 3], but a large part of it is controversial,and not yet applicable to medicine. Indeed, this resultsfrom studies difficult to reproduce and that do not fulfillstatistical validation standards for two main reasons: the*Correspondence: adrien.coulet@loria.frEqual contributors1LORIA (CNRS, Inria Nancy-Grand Est, University of Lorraine), CampusScientifique, Nancy, FranceFull list of author information is available at the end of the articlesmall size of populations involved in studies because ofthe rarity of gene variants studied and the potential coac-tion of several variants [4, 5]. It is consequently of interestto the PGx community to explore any source of evidencethat may contribute to confirming or moderating PGxstate of the art. So far, existing works used either molec-ular network databases or the biomedical literature (seeDiscovery of pharmacogenes subsection). We proposein this work to explore how other resources, and partic-ularly Linked Open Data (LOD) may be useful in thisdomain.Linked open dataLOD are constituting a large and growing collection ofdatasets that present the main advantages of being rep-resented in a standard format (based on both RDF andURIs) and partially connected to each other and to domain© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 2 of 12knowledge represented within semantic web ontologies[6]. For these reasons, LOD offer novel opportunitiesfor the development of successful data integration andknowledge discovery campaign, as required for the dis-covery of novel pharmacogenes. LOD are part of a com-munity effort to build a semantic web, where web anddata resources can be interpreted both by humans andmachines. The recent availability of LOD is particularlybeneficial to the life sciences, where relevant data arespread over various data sources with no agreement ona unique representation of biological entities [7]. Conse-quently, data integration is an initial challenge one facesif one wants to mine life science data considering sev-eral data sources. Various initiatives such as Bio2RDF, theEBI platform, PDBj and Linked Open Drug Data (LODD)aim at pushing life sciences data into the LOD cloud withthe idea of facilitating their integration [811]. It resultsfrom these initiatives a large collection of life-science data,unequally connected but in a standard format and avail-able for mining. Despite good will and emerging standardpractices for publishing data as LOD, several drawbacksmake their use still challenging [12, 13]. Among exist-ing difficulties we can cite the limited amount of linksbetween datasets and the limits of implementations offederated queries.Pharmacogenomics data and linked dataPharmGKB is a comprehensive database about PGx thatincludes manually annotated genedrug relationships [3].Recently, annotations of PharmGKB have been completedwith a level of evidence going from 1 to 4, distinguish-ing well validated genedrug relationships (level= 1 ? 2)from insufficiently validated ones (34), thus point-ing at knowledge in need for additional investigations[14]. PharmGKB does not provide its data in RDF, butparts of PharmGKB have been transformed and pub-lished in RDF by contributors of the Bio2RDF project,thus enabling SPARQL queries [15]. Clinical annotationsof PharmGKB are however not freely available. Theirusage is granted through a license agreement, prevent-ing the data from being redistributed, thus publishedas Linked Open Data. Many other databases providesdata that are indirectly relevant to PGx. For instances,DrugBank [16] provides drugtarget relationships; Clin-Var [17] provides gene variantphenotype relationships;SIDER [18, 19] and Medi-Span provides drugphenotyperelationships such as drug adverse events or indications[20]. Medi-Span is a proprietary database of WoltersKluwer Health (Indianapolis, IN) aiming at providingdrug clinical data to clinicians. DGIdb (The Drug GeneInteraction database) is another interesting initiative thatintegrates quasi-exhaustively data about genedrug rela-tionships, considering 15 distinct sources [21]. DisGenetis a data integration initiative that focuses on genediseaserelationships and provides data in RDF, including parts ofClinVar and OMIM [22].Data integration effort clearly oriented to PGx applica-tions are less common, particularly if considering seman-tic web approaches [23]. Hoehndorf et al. integrated andmade available a set of PGx related data that includesPharmGKB, DrugBank and CTD (the Comparative Tox-icogenomics Database), using semantic web technologies[24]. They used the integrated dataset to identify pathwaysthat may be perturbed in PGx. In this effort of publish-ing PGx data, Coulet et al. extracted about 40,000 PGxrelationships from the biomedical literature and publishedthem in the form of RDF statements [25].Mining linked dataSuggesting valid pharmacogenes in this work is seen asproposing novel genedrug relationships from an RDFgraph, which in turn can be described as a link predictionproblem. Many works have focused on the link predictionproblem, studying various approaches such as machinelearning [26, 27], graph mining [2830], identity resolu-tion [31, 32] and data visualisation [33]. Some of thesemethods obtain good results, but all are dependent fromthe input graphs (its quality, topology, etc.) and are hardto reuse for new applications. Recently, de Vries and deRooij proposed a complete framework for applying GraphKernel (GK) in an adaptive manner to RDF graphs [34].GK are machine learning methods that have the ability todeal directly with graph data, particularly by computingkernel functions that evaluate similarity between graphsor pieces of graphs [35]. The framework of de Vries andde Rooij is implemented in an open source library namedMustard [36]. It enables classifying RDF instances consid-ering their neighborhood in the graph. This neighborhoodis encoded within features such as labels of edges or graphsubstructures such as walks (i.e., linear paths) or sub-graphs. In the work we present here, we reused Mustardand fitted its capability of instance classification to thecase of link prediction.In relation with PGx research, Percha et al. mined theset of RDF statements extracted from text by Coulet et al.with a Random Forest (RF) algorithm and successfullypredicted drugdrug interactions [37]. With the aim ofpredicting pharmacogenes, we experimented as Perchaet al. with the RF algorithm in the preliminary stage ofthis work [38]. First results we obtained with RF are hereupdated and compared with GK approaches.Discovery of pharmacogenesHansen et al. proposed a method based on a logistic clas-sifier to generate candidate pharmacogenes, using datafrom PharmGKB, DrugBank, and proteinprotein inter-actions from InWeb [39]. An issue with this approachis that PharmGKB and DrugBank are manually curatedDalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 3 of 12from the literature and are consequently expensive tomaintain and update. Garten et al. answered this issueby proposing an automatic method that consider directly(and only) the literature [40]. They improved the resultsobtained by Hansen et al. by considering genedrug pairsco-occurring in sentences of the PGx literature. Recently,Funk et al. proposed also to use the biomedical literature,plus GO annotations, to identify pharmacogenes [41].They obtain a high F-measure and AUC-ROC (0.86 and0.86), but proposed a coarse-grained classification that isonly binary (pharmacogene or not), avoiding any rankingof the candidates.Semantic web technologies have also been experi-mented for PGx knowledge discovery. Dumontier andVillanueva-Rosales proposed a knowledge representationof the domain and benefit from reasoning mechanismsto answer sophisticated queries related to depressiondrugs [42]. Coulet et al. used patient data to instantiate adescription logics knowledge base, then extracted associ-ation rules from it to identify gene variantdrug responseassociations [43]. More generally, advantages that seman-tic web technologies may offer to PGx and personalizedmedicine are listed in [23].We present here a method that consists in mining a setof diverse linked data sources to help validating uncer-tain genedrug relationships. This method can be dividedin three steps: first, selecting and connecting relevantPGx linked data; second, formatting linked data to trainand compare two machine learning algorithms (RF andGK); third, classify and rank candidate pharmacogeneswith these two approaches. The paper is organized asfollow: next section presents our methods for preparing,then learning from the linked data; next, Results Sectionpresents the evaluation and the use of the two machinelearning approaches we considered and brings elementsof interpretation; the two last sections discuss our resultsand conclude on this work.MethodsData preparationData selection Initial step is to select a set of data thatinclude relevant data about PGx genedrug relationships.Figure 1 gives a general overview of the type of data weconsider for this study: three types of entities, gene, pheno-type and drug; and relationships between them, i.e., genephenotype, phenotypedrug and genedrug relationships.Fig. 1 Overview of the type of entities and relationships considered and their origin. Entities are of three distinct types: Gene, Phenotype and Drug.GenePhenotype relationships are coming from ClinVar and DisGeNET, PhenotypeDrug relationships from SIDER and Medi-Span, GeneDrugrelationships from DrugBank. In addition, we included gene and drug entities from PharmGKB to enable building the training and test sets.Equivalence mappings are defined between entities of the same type but of different origin. In addition to entityentity relationships, we considersome attributes that are specific to entities, such as the ATC class of drug that is a drug attribute. Naming of different parts of the data (e.g., GP links,gene attributes) is used later in the step of formatting of the linked data. The detailed schema of the data is provided Fig. 2Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 4 of 12We selected data sources manually but oriented our selec-tion to sources providing typed relationships and limitedourselves to two sources per relationship. As a result,we selected ClinVar and DisGeNET for genephenotype;SIDER andMedi-Span for phenotypedrug; DrugBank forgenedrug relationships. PharmGKB completes the set ofdata sources to enable building the training and test sets(see Training and test sets subsection).Data RDFization The second step is about turningselected data in a standardized RDF graph, available athttps://pgxlod.loria.fr. We benefit from the fact that Dis-GeNET [44], SIDER [45] and DrugBank [46] are alreadyavailable online in the form of LOD and reused them.DisGeNET includes data from ClinVar, but because itincludes only a part of it, we made our own RDFversion of ClinVar following guidelines and scripts ofthe Bio2RDF project. We completed the Bio2RDF ver-sion of PharmGKB locally with genedrug relationshipsmanually annotated by PharmGKB but not openly dis-tributed [15]. Similarly, we transformed drug indicationsand side-effects from Medi-Span in the form of RDFtriples and loaded them into our SPARQL server. Forthe management of RDF data, we rely on Blazegraph, agraph database system that provides support for RDF andSPARQL. Medi-Span data, as PharmGKB clinical anno-tations are protected by a license agreement and can notbe redistributed. This explains why we are providing acontrolled access to our set of PGx linked data. We pro-pose to open this dataset, on demand, with licensees.Figure 2 presents the detailed schema (i.e., type of enti-ties and relationships) of the linked data we selected andconsider for mining. Figure 3 presents an example ofdata from the PGx linked data, instantiating the schemapresented Fig. 2. The SPARQL query returning data pre-sented in Fig. 3 is provided in Additional file 1. OtherSPARQL queries, such as the one provided in Additionalfile 2, may be built by considering the partial data schemapresented Fig. 2.Mapping definition To define mappings, we first reliedon standard identifiers such as NCBI Gene ID found inDisGeNET and ClinVar URIs and UMLS CUI found inDisGeNET, ClinVar, SIDER and Medi-Span. We definedregular expressions over URIs to isolate identifiers andwhen twomatch, we define amapping. Figure 3 shows twoFig. 2 Schema of the pharmacogenomic linked data selected for this study. Entities are related to either Genes, Phenotypes (or Diseases) or Drugs.We artificially enriched the data with an additional type of entity: genedrug pairs. These entities link exactly one gene and one drug and are thenodes of the graph we classify either as associated or not associated from a PGx point of view, to valid candidate pharmacogenes. For mappingPersoneni et al. Journal of Biomedical Semantics  (2017) 8:29 DOI 10.1186/s13326-017-0137-xRESEARCH Open AccessDiscovering associations betweenadverse drug events using pattern structuresand ontologiesGabin Personeni1* , Emmanuel Bresso1, Marie-Dominique Devignes1, Michel Dumontier2,3,Malika Smaïl-Tabbone1 and Adrien Coulet1,3AbstractBackground: Patient data, such as electronic health records or adverse event reporting systems, constitute anessential resource for studying Adverse Drug Events (ADEs). We explore an original approach to identify frequentlyassociated ADEs in subgroups of patients.Results: Because ADEs have complex manifestations, we use formal concept analysis and its pattern structures, amathematical framework that allows generalization using domain knowledge formalized in medical ontologies.Results obtained with three different settings and two different datasets show that this approach is flexible and allowsextraction of association rules at various levels of generalization.Conclusions: The chosen approach permits an expressive representation of a patient ADEs. Extracted associationrules point to distinct ADEs that occur in a same group of patients, and could serve as a basis for a recommandationsystem. The proposed representation is flexible and can be extended to make use of additional ontologies and variouspatient records.Keywords: Adverse drug event, Association rules, Ontologies, Patient data, Pattern structures, PharmacovigilanceBackgroundAdverse Drug Events (ADEs) occur unevenly in differ-ent groups of patients. Their causes are multiple: genetic,metabolic, interactions with other substances, etc. Patientdata, in the form of either Electronic Health Records(EHRs) or adverse effects reports have been successfullyused to detect ADEs [1, 2]. We hypothesize that miningEHRs may reveal that subgroups of patients sensitive tosome drugs are also sensitive to others. In such a case, sev-eral ADEs, each caused by different drugs, could be foundto occur frequently in a subgroup of patients. While thisis known to be true in certain classes of drugs, we furtherhypothesize that such associations can be found acrossdifferent classes. We propose a method to identify thesefrequently associated ADEs in patients subgroups.*Correspondence: gabin.personeni@loria.fr1LORIA (CNRS, Inria NGE, Université de Lorraine), Campus Scientifique, F-54506Vanduvre-lès-Nancy, FranceFull list of author information is available at the end of the articleThe main issue to reach this goal is that ADE manifes-tations are complex and that they are reported in variablemanners. Indeed, ADEs are not limited to the simple caseof one drug causing one phenotype but may be an asso-ciation between several drugs and several phenotypes.Furthermore, these drugs and phenotypes can be reportedusing different vocabularies and with varying levels ofdetail. For instance, two clinicians may report the sameADE caused by warfarin, an anticoagulant drug, either aswarfarin toxicity or with amore precise description suchas ulcer bleeding caused by warfarin. As such, biomed-ical ontologies provide helpful resources to consider thesemantic relationships between ADEs.In [3], Roitmann et al. proposed a vector represen-tation of patient ADE profiles: a patient is representedby a feature vector in which each feature is one phe-notype experienced by the patient. All phenotypes arehere considered as independent features. This representa-tion is used with clustering algorithms to group patients© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 2 of 13into clusters in which prevalent drugs and phenotypescan be identified. This work could be expanded by con-sidering biomedical ontologies coupled with a semanticsimilarity measure such as the one described in Devigneset al. [4], to cluster together patients taking distinct butsimilar drugs and expressing distinct but similar pheno-types. However, a limitation of a vector representationis that it aggregates all ADEs of a patient in a singleobject. In this paper, we propose a representation of theADEs of a patient that preserves the distinctness of theseevents.In [5], Winnenburg et al. extracted drug-phenotypepairs from the litterature to explore the relationshipsbetween drugs, drug classes and their adverse reactions.Adverse event signals are computed both at the drug anddrug class levels. This work illustrates that some drugclasses can be associated with a given adverse effect, andfurther investigates the association at the individual druglevel. In cases where the association with the adverseeffect is present for every drug in the class, it demonstratesthe existence of a class effect. Otherwise, the associationis present for only some drugs of the class, and cannot beintrinsically attributed to the class itself. This result showsthat it is possible to consider ADEs either at the invidid-ual drug level or at the drug class level. The approachwe propose in this paper addresses this possibility, bothat the level of ADE representation and inside the datamining approach itself, which allows generalization withbiomedical ontologies. In addition, we are also capableof detecting ADE associations involving different classesof drugs.For this purpose, we use an extension of Formal ConceptAnalysis (FCA) [6] called pattern structures [7] in combi-nation with ontologies to enable semantic comparison ofADEs. FCA has been successfully used for signal detec-tion in pharmacovigilance: in [8, 9], FCA is used to detectsignals in a dataset of ADEs described with several drugscausing a phenotype. In this case, FCA permits to minefor associations between a set of drugs and a phenotype.In this article, pattern structures allow us to extend thedescriptions of ADEs with biomedical ontologies, and tomine higher-order associations, i.e., associations betweenADEs.We experimented with two types of datasets. A firstdataset was extracted from EHRs of patients diag-nosed with Systemic Lupus Erythematosus (SLE), asevere autoimmune disease. Such patients frequentlyexperience ADEs as they often take multiple anddiverse drugs indicated for SLE or derived patholo-gies [10]. Our second dataset was extracted fromthe U.S. Food & Drug Administration Adverse EventReporting System (FAERS). This dataset was linkedto biomedical ontologies thanks to a novel resource,AEOLUS [11].MethodsADE definitionAn ADE is a complex event in that it may often involveseveral drugs, and manifest through several phenotypes.An ADE can then be characterized by a set of drugs,and a set of phenotypes. To facilitate comparison betweenADEs, we consider sets of active ingredients of drugs,rather than sets of commercial drug names. In the rest ofthis article, we use the term drug to denote an activeingredient. In this study, we represent an ADE as a pair(Di,Pi), where Di is a set of drugs, and Pi is a set of phe-notypes. Table 1 presents examples of ADEs that could beextracted from the EHRs, and will serve here as a run-ning example. Table 2 provides the origin and label of eachontology class code used in this article.SLE EHR dataset from STRIDEOur first dataset is a set of 6869 anonymized EHRs ofpatients diagnosed with SLE, extracted from STRIDE, theEHR data warehouse of Stanford Hospital and Clinics [12]between 2008 and 2014. It documents about 451,000 hos-pital visits with their relative dates, diagnoses encodedas ICD-9-CM phenotype codes (International Classifica-tion of Diseases, Ninth Revision, Clinical Modification)and drug prescriptions as a list of their ingredients, repre-sented by RxNorm identifiers.We first establish a list of ADE candidates for eachpatient EHR. From each two consecutive visits in the EHR,we extract the set of drugs Di prescribed during the firstvisit and the diagnoses Pi reported during the second. Theinterval between the two consecutive visits must be lessthan 14 days, as it is reasonable to think that a side effectshould be observed in such a time period after prescrip-tion. Moreover, Table 3 shows that increasing this intervaldoes not significantly increase the number of patients inour dataset. An ADE candidate Ci is thus a pair of setsCi = (Di,Pi). We retain in Pi only phenotypes reportedas a side effect for at least one drug of Di in the SIDER4.1 database of drug indications and side effects [13]. Weremove candidates where Pi is empty. Furthermore, weremove an ADE candidate (D1,P1) if there exists for thesame patient another ADE candidate (D2,P2) such thatTable 1 Example of a dataset containing 3 patients with 2 ADEseach, in lexicographic orderPatient ADEsP1 ({acetaminophen},{ICD 599.9}) ({prednisone},{ICD 599.8})P2 ({prednisone},{ICD 599.8}) ({prednisone},{ICD 719.4})P3 ({acetaminophen},{ICD 719.4}) ({acetaminophen, prednisone},{ICD 599.9})Class labels: ICD 599.8 is other specified disorders of the urethra and urinary tract,ICD 599.9 is unspecified disorders of the urethra and urinary tract, ICD 719.4 ispain in jointPersoneni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 3 of 13Table 2 This table provides the origin and label of each ontologyclass code used in this articleOntology code Ontology LabelA02B ATC Drugs for peptic ulcer andgastro-oesophageal reflux diseaseA02BC ATC Proton pump inhibitorsA04A ATC Antiemetics and antinauseantsA06A ATC Drugs for constipationA07A ATC Intestinal antiinfectivesB01A ATC Antithrombotic agentsB03X ATC Other antianemic preparationsB05X ATC I.V. solution additivesC01BB03 ATC TocainideC03C ATC High-ceiling diureticsC05B ATC Antivaricose therapyC07A ATC Beta blocking agentsC08D ATC Selective calcium channel blockers withdirect cardiac effectsC08DB ATC Benzothiazepine derivativesC09A ATC Ace inhibitors, plainC10A ATC Lipid modifying agents, plainG04BE ATC Drugs used in erectile dysfunctionG04BE04 ATC YohimbinH02A ATC Corticosteroids for systemic use, plainH02AA03 ATC DesoxycortoneH02AB ATC GlucocorticoidsH02AB07 ATC PrednisoneN02A ATC OpioidsN02B ATC Other analgesics and antipyreticsN02BE01 ATC Paracetamol / AcetaminophenN05B ATC AnxiolyticsN05C ATC Hypnotics and sedativesN06BC ATC Xanthine derivativesN06BC01 ATC CaffeineR05D ATC Cough suppressants, excl. combinationswith expectorantsR06A ATC Antihistamines for systemic useR06AA ATC aminoalkyl ethersR06AA09 ATC DoxylamineS01A ATC AntiinfectivesS01AX ATC Other antiinfectives in ATC280-289 ICD-9-CM Diseases of the blood and blood-formingorgans280 ICD-9-CM Iron deficiency anemias285.9 ICD-9-CM Anemia, unspecified287.5 ICD-9-CM Thrombocytopenia, unspecified427.31 ICD-9-CM Atrial fibrillation428 ICD-9-CM Heart failureTable 2 This table provides the origin and label of each ontologyclass code used in this article (Continued)428.0 ICD-9-CM Congestive heart failure, unspecified428.9 ICD-9-CM Heart failure, unspecified580-629 ICD-9-CM Diseases of the genitourinary system580 ICD-9-CM Acute glomerulonephritis586 ICD-9-CM Renal failure, unspecified599.8 ICD-9-CM Other specified disorders of urethra andurinary tract599.9 ICD-9-CM Unspecified disorder of urethra and urinary tract710-739 ICD-9-CM Diseases of the musculoskletal system andconnective tissue710 ICD-9-CM Diffuse diseases of connective tissue719.4 ICD-9-CM Pain in jointThe ontologies used in this article are described in the Medical Ontologies sectionon page 4D1 ? D2: indeed, reiterated prescriptions of drugs mayindicate that they are safe for this patient.In such cases, where several ADEs have comparable setsof drugs, we only retain the ADEwith themaximal set, i.e.,themost specialized set of drugs. Indeed, as we aim to findassociations between different ADEs, we thus avoid con-sidering multiple times such similar sets of drugs. Finally,we keep only patients having experienced at least twoADEs, as our goal is to mine frequently associated ADEs.After filtering, we obtain a total of 3286 ADEs for 548patients presenting at least two ADEs.FAERS datasetFAERS publishes a database gathering ADEs reported bypatients, healthcare professional and drug manufacturersin the United States. It is used for postmarketing phar-macovigilance by the U.S. Food & Drug Administration,data mining of signals in pharmacovigilance [2] or ofadverse drug-drug interactions [14]. A recently-publishedresource, AEOLUS [11] maps FAERS drugs and pheno-types representations to RxNorm and SNOMED CT (Sys-tematized Nomenclature of Medicine  Clinical Terms)respectively. We used this tool to rebuild a database ofFAERS reports, linked to RxNorm and SNOMED CT,from the fourth quarter of 2012 to the second quarter of2016 included.Table 3 Number of patients with at least 2 selected ADEs andnumber of ADEs for these patients, for different maximuminter-visit interval in daysInterval (days) 1 2 6 10 14 18 22 26 30|Patients| 434 461 498 526 548 555 558 564 576|ADEs| 2396 2587 2902 3110 3286 3388 3454 3501 3621Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 4 of 13Each FAERS report lists a set of prescribed drugs Diand the a of experienced phenotypes Pi. Thus, we can for-malize each report as a pair of sets (Di,Pi). These reportsare grouped in cases, enabling us to identify additionalreports that follow up an initial ADE. We selected, inthe FAERS database, cases with multiple reported ADEs,excluding ADEs where the set of drugs is included inanother ADE of the same case. With these constraints, weextract 570 cases with two or more distinct ADEs, for atotal of 1148 ADEs.Medical ontologiesWe use three medical ontologies, considering only theirclass hierarchy, to enable semantic comparisons of drugsand phenotypes when comparing ADEs: ICD-9-CM describes classes of phenotypes, as it isused in STRIDE to describe diagnoses; SNOMED CT is an ontology of medical terms, whichwe use to describe the phenotypes of FAERS, usingthe mappings provided by AEOLUS; The Anatomical Therapeutic Chemical ClassificationSystem (ATC) describes classes of drugs. In thiswork, we used only the three most specific levels ofATC: pharmacological subgroups, chemicalsubgroups and chemical substances.Association rule miningAssocation rule mining [15] is a method for discoveringfrequently associated items in a dataset. Association rulemining is performed on a set of transactions, representedas sets of items. Association Rules (ARs) are composedof two sets of items L and R, and are noted L ? R.Such a rule is interpreted as when L occurs in a tran-scation, R also occurs. Note that ARs do not express anycausal or temporal relationship between L and R. ARsare qualified by several metrics, including confidence andsupport. The confidence of a rule is the proportion oftransactions containing L that also contains R. The sup-port of a rule is the number of transactions containingboth L and R. For instance, if a rule A,B ? C has aconfidence of 0.75 and a support of 5, then, C occurs in34 of the transactions where A and B occur, and A,B,Coccur together in 5 transactions. Note that the supportmay also be represented relatively to the total number oftransactions in the dataset, e.g., 5500 for a dataset of 500transactions.Several algorithms for association rule mining, such asApriori, have been proposed, based on frequent itemsets[16]. Such frequent itemsets can be identified using anitemset lattice [17]. FCA offers facilities for building lat-tices, identifying frequent itemsets and association rulemining [18]. In the following section, we present FCA andits extension pattern structures, as a method to mine ARs.Formal concept analysis and pattern structuresFormal Concept Analysis (FCA) [6] is a mathematicalframework for data analysis and knowledge discovery. InFCA a dataset may be represented as a concept lattice,i.e., a hierarchical structure in which a concept representsa set of objects sharing a set of properties. In classicalFCA, a dataset is composed of a set of objects, where eachobject is described by a set of binary attributes. Accord-ingly, FCA permits describing patients with the ADEsthey experienced represented as binary attributes, as illus-trated in Table 4. The AR ADE1 ? ADE3 that can beextracted from this dataset has a support of 2 and a con-fidence of 23 . This AR expresses that two thirds of thepatients that experienced ADE1 also experienced ADE3,and that the rule was verified by 2 patients (P1 and P3)in the dataset. However, FCA does not take into accountthe similarity between attributes. For instance, both ADE3and ADE4 could be caused by the same drugs, while pre-senting slightly different phenotypes. In such a case, wemay want to extract a rule expressing that patients whoexperienced ADE1 also experienced an ADE similar toADE3 or ADE4.Accordingly, approaches extracting ARs from sets ofbinary attributes are limited as the similarity of attributesis not considered. This is the case of algorithms such asApriori, or classical FCA approaches.We propose to intro-duce a more detailed representation of patients ADEs,along with a fine-grained similarity operator.Pattern structures generalize FCA in order to work witha set of objects with descriptions not only binary but ofany nature such as sets, graphs, intervals [7, 19]. Par-ticularly, pattern structures have been used to leveragebiomedical knowledge contained in ontology-annotateddata [20].A pattern structure is a triple (G, (D,), ?), where: G is a set of objects, in our case, a set of patients, D is a set of descriptions, in our case, representationsof a patients ADEs, ? is a function that maps objects to their descriptions.  is a meet operator such that for two descriptions Xand Y in D, X  Y is the similarity of X and Y : X  Yis a description of what is common betweendescriptions X and Y . It defines a partial order ? onelements of D. X ? Y denotes that Y is a morespecific description than X, and is by definitionTable 4 Example of a binary table to be used for extraction ofassociations between ADEs using Formal Concept Analysis (FCA)Patient ADE1 ADE2 ADE3 ADE4P1 × ×P2 × ×P3 × × ×Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 5 of 13equivalent to X  Y = X. Generalization on objectdescriptions is performed through the use of themeet operator. In the following section, we definethree distinct meet operators (1, 2, 3) that enableconsidering similarities between ADE descriptions atdifferent levels of granularity. This section alsoillustrates the application of pattern structures.In pattern structures, the derivation operator . definesa Galois connection between sets of objects and descrip-tions, as follows:A = g?A?(g) for a set of objects Ad = {g ? G | d ? ?(g)} for a description dIntuitively, A is the most precise description for the setof objects A, and d is the set of objects described by adescription more specific than d. A pattern concept is apair (A, d) with A = d and d = A. Pattern structuresenable building a lattice of pattern concepts, which allowassociating a set of patients with a shared description oftheir ADEs, based on their similarity.In our study, G is the set of patients that are relatedthrough ? to the description of their ADEs in D. Wehave designed different experiments using pattern struc-tures, each providing their own definition of the triple(G, (D,), ?).Experimental designIn this section, we describe three experiments to extractARs betweenADEs. Each one defines a different represen-tation of patient ADEs and a different setting of patternstructures, making increasing use of ontologies.Experiment 1: Pattern structurewithout semantic comparisonTable 4 presents a naive representation of patient ADEs.However, we want a representation that takes into accountsimilarity between ADEs, instead of considering ADEsas independent attributes. Accordingly, we propose inthis first experiment a representation that groups ADEswith high level phenotypes and we define an operator tocompare their sets of drugs.We define here the pattern structure (G, (D1,1), ?1):objects are patients, and a patient description of D1 isa vector of sub-descriptions, with first-level ICD-9-CMclasses as dimensions. Each sub-description is a set ofdrug prescriptions, i.e., a set of sets of drugs. For instance,considering only the two ICD-9-CM classes of Table 5:?1,ICD 580-629(P1) = {{prednisone}, {acetaminophen}}?1,ICD 710-739(P1) = ?Here, ADEs are decomposed w.r.t. their phenotypes.Sub-descriptions are associated to a first-level ICD-9-CMTable 5 Example of representation of patient ADEs for(G, (D1,1), ?1), with two first-level ICD-9-CM classes: diseases ofthe genitourinary system (580-629), and of the musculoskeletalsystem and connective tissue (710-739)Patient ICD 580-629 (genitourinary system) ICD 710-739(musculoskeletal system)P1 {{prednisone}, {acetaminophen}} ?P2 {{prednisone}} {{prednisone}}P3 {{prednisone, acetaminophen}} {{acetaminophen}}class to represent ADEs: the patient presents a phe-notype of that class after taking a prescription in thatsub-description. In the example presented in Table 5, thepatient P1 experienced an ADE with a phenotype fromthe ICD-9-CM class 580-629 twice: once after prescrip-tion of prednisone, and another time after prescription ofacetaminophen.We define a sub-description as a set of prescriptions,where none of the prescriptions are comparable to eachother by the partial order ?. We then define the meetoperator 1, such that, for every pair of descriptions(X,Y ) ofD1:X 1 Y = max(?, {x ? y | (x, y) ? X × Y})where max(?i, S) is the unique subset of maximal ele-ments of a set S given any partial order ?i. Formally,max(?i, S) = {s | x.(s ?i x)}. In the present case,it retains only the most specific set of drugs prescribedin the description. For instance, given four drugs d1through d4:{{d1, d2, d3}} 1 {{d1, d2}, {d2, d4}}= max (?, {{d1, d2, d3} ? {d1, d2}, {d1, d2, d3} ? {d2, d4}})= max (?, {{d1, d2}, {d2}})= {{d1, d2}}We only retain {d1, d2} since {d2} ? {d1, d2} and {d1, d2}is the only ?-maximal element. Indeed, the semantic of{d2}  a prescription that contains the drug d2  is moregeneral than the semantic of {d1, d2}  a prescription thatcontains both the drugs d1 and d2.Given that each patient has a description for eachfirst-level ICD-9-CM class, the meet operator definedfor a sub-description can be applied to a vector of sub-descriptions:?1(P1) 1 ?1(P2) = ??1,1(P1), . . . , ?1,n(P1)?1??1,1(P2), . . . , ?1,n(P2)?= ??1,1(P1) 1 ?1,1(P2), . . . ,?1,n(P1) 1 ?1,n(P2)?Figure 1 shows the semi-lattice associated with this pat-tern structure and the data in Table 5. Nevertheless, thisPersoneni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 6 of 13Fig. 1 Semi-lattice representation of the data in Table 5 using thepattern structure (G, (D1,1), ?1), where arrows denote the partialorder ?1example shows that in the absence of semantics betweendescriptions, generalization rapidly produces empty setsdevoid of information.Experiment 2: Extending the pattern structure with a drugontologyUsing a drug ontology permits to find associationsbetween ADEs related to classes of drugs rather thanindividual drugs. Thus, we extend the pattern structuredescribed previously to take into account a drug ontol-ogy: ATC. Each drug is replaced with its ATC class(es), asshown in Table 6. We notice that the fact that one drugcan be associated with several ATC classes is handled byour method as sets of drugs become represented as sets ofATC classes.We define this second pattern structure (G, (D2,2), ?2)where descriptions of D2 are sets of prescriptions withdrugs represented as their ATC classes. In order to com-pare sets of classes from an ontology O, we define anintermediate meet operator O , for x and y any two setsof classes ofO:x O y = max(, {LCA (cx, cy) | (cx, cy) ? x × y})where LCA(cx, cy) is the least common ancestor of cx andcy in O, and  is the ordering defined by the class hierar-chy ofO. For any set of classes S, max(, S) is the subset ofmost specific ontology classes of S (they have no descen-dant in S). Thus, x O y is the subset of most specificancestors of classes in x and y. From O we define thepartial order ?O , which compares two sets of ontologyclasses, x and y, such that x ?O y ? x O y = x andx ?O y denotes that y is a more specific set of ontologyTable 6 Example of representation of patient ADEs for(G, (D2,2), ?2)Patient ICD 580-629 (genitourinary system) ICD 710-739(musculoskeletal system)P1 {{H02AB07},{N02BE01}} ?P2 {{H02AB07}} {{H02AB07}}P3 {{H02AB07, N02BE01}} {{N02BE01}}P4 {{H02AA03}} ?Class labels: H02AA03 is desoxycortone, H02AB07 is prednisone, N02BE01 isacetaminophenclasses than x. We then define the meet operator 2 suchthat for every pair of descriptions (X,Y ) ofD2:X 2 Y = max(?O ,{x O y | (x, y) ? X × Y})This pattern structure allows generalization of ADEsinvolving different drugs that share a pharmacologicalsubgroup. For instance:?(P1) 2 ?(P4) = ?{{H02AB07}, {N02BE01}} ,??2?{{H02AA03}},??= ?max(?O , {{H02AB07} O {H02AA03},{N02BE01} O {H02AA03}}),??= ?max(?O , {{H02A}, {}}),??= ?{{H02A}},??Here, we use O to compare sets of drugs. Com-parison of {H02AA03} (desoxycortone) and {H02AB07}(prednisone) yields their common ancestor in theontology: {H02A} (corticosteroids for systemic use,plain). We observe that {N02BE01} (acetaminophen) and{H02AA03} (desoxycortone) only have the root  of theontology in common, thus {N02BE01} O {H02AA03} ={}. The max function excludes it from the final result,as it is redundant with {H02A}, since {} ?O {H02A}.The vector ?{{H02A}},?? represents the closest general-ization of the descriptions of patients P1 and P4, and canbe read as: drugs of the class H02A (corticosteroids forsystemic use, plain) are associated with a phenotype in theICD-9-CM class diseases of the genitourinary system (580-629), and no drugs are associated to the ICD-9-CM classdiseases of musculoskeletal system and connective tissue(710-739).Experiment 3: Extending the pattern structure with a drugand a phenotype ontologyWe define a third pattern structure that permits the use ofboth ATC and a phenotype ontology for better specializa-tion of phenotypes compared to the previous experiment.As this experimental design can be applied to both theEHR and FAERS datasets, we design a pattern structurethat can operate with any drug and phenotype ontologies.We apply it to our EHR dataset with ATC and ICD-9-CM,and to the FAERS dataset with ATC and SNOMED CT.To avoid over-generalization, we excluded the twomost-general levels of ICD-9-CM and the three most-generallevels of SNOMED CT. Table 7 illustrates the data repre-sentation used with this pattern structure, using ATC andICD-9-CM. Here, ADEs are represented as vectors ?Di,Pi?with two dimensions: the set of drugs Di associated withthe set of phenotypes Pi. A patient description is then a setof such vectors.Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 7 of 13Table 7 Example of representation of patient ADEs for(G, (D3,3), ?3)Patient DescriptionP1 {?{H02AB07},{ICD 599.8}?, ?{N02BE01},{ICD 599.9}?}P2 {?{H02AB07},{ICD 599.9}?, ?{H02AB07},{ICD 719.4}?}P3 {?{H02AB07,N02BE01},{ICD 599.9}?, ?{N02BE01},{ICD 719.4}?}Class labels: H02AA03 is desoxycortone, H02AB07 is prednisone, N02BE01 isacetaminophen, ICD 599.8 is other specified disorders of the urethra and urinarytract, ICD 599.9 is unspecified disorders of the urethra and urinary tract, ICD 719.4is pain in jointWe define the pattern structure (G, (D3,3), ?3), wheredescriptions of D3 are sets of ADEs. We first definean intermediate meet operator ADE on our ADEsrepresentations:vx ADE vy = ?Dx,Px? ADE ?Dy,Py?=??????Dx O Dy,Px O Py? if both dimensions containat least one non-root class??,?? otherwise.The operator ADE applies the ontology meet opera-tor O on both dimensions of the vector representing theADE, using either ATC or ICD-9-CM as the ontology O.Both dimensions of the resulting vector needs to containnon-root ontology classes for it to constitute a represen-tation of an ADE. If it is not the case, we set it to ??,?? toignore it in further generalizations.We define the meet operator 3 such that for every pairof descriptions (X,Y ) ofD3:X 3 Y = max(?ADE ,{vx ADE vy |(vx, vy) ? X × Y})Compared to 2, 3 introduces a supplementary levelof computation with ADE , which generalizes ADEs andapplies O to an additional ontology: ICD-9-CM.Extraction and evaluation of associations rulesThe pattern structures described previously can be usedto build concept lattices, where each concept associates aset of patients with the similarity of their ADEs descrip-tions. Such a concept lattice allows for identifying fre-quent ADEs descriptions, which can be used for extract-ing Association Rules (ARs). An AR is identified betweentwo related concepts in the lattice, with descriptions ?(l)and ?(r) such that ?(l) < ?(r). Thus, such an AR com-prises a left-hand side L = ?(l) and a right-hand sideR = ?(r) ? ?(l), where ? denotes set difference. Such arule is noted L ? R.This process can be expected to generate a large amountof rules, among which ARs serving our goal of detect-ing associations between ADEs must be identified. Wetherefore filter ARs according to the following conditions: The right-hand side R of the AR contains at least oneADE, noted as (DR,PR) for which there is no ADE(DL,PL) in the left-hand side L such that either DRand DL are ?O comparable, or PR and PL are ?Ocomparable. This condition ensures that theright-hand side of the rule introduces new drugs andphenotypes unrelated to those of the left-hand side,i.e., the association between the ADEs of both sides isnot trivial. As patients in the EHR dataset are treated forSystemic Lupus Erythematosus (SLE), rules must notinclude related phenotypes (ICD-9-Cm class 710 anddescendants).ARs extracted from the SLE patients EHR datasetwere evaluated by computing their support in the entireSTRIDE EHR dataset. Selected ARs with the largest sup-port were transformed into SQL queries, in order toretrieve matching patients from the STRIDE database.Statistical analysis of the extracted ADE associationsFigures 2 and 3 show an overview of ATC drug classesassociated by the ARs extracted in the third EHR experi-ment. We isolated every pair of ATC classes associated byARs, i.e., one ATC class or one of its subclass is presentin the left-hand side of the AR, and one is present inits right-hand side. Figure 2 shows the frequency of suchassociations and Fig. 3 shows, for the significant ones,the difference to the frequency obtained if the associationwould be random. For each pair (l, r) of ATC classes, wesearch for the set of rules of the form L ? R, such thatl or one of its subclasses appears in L and r or one of itssubclasses appears in R and compute their combined sup-port. The combined support of a set of rules is the numberof patients described by at least one of these rules. Thecombined support of all rules having class l in L or classr in R is also calculated and indicated at the beginning ofeach row for l classes and at the top of each column for rclasses. Cells of the Fig. 2 indicate, for each (l, r), the ratiobetween (i) the combined support of ARs where l appearsin L and r appears in R and (ii) the combined support ofARs where l appears in L. This ratio denotes how oftenthe extracted rules associate an ADE where a drug froml with an ADE where drug from r is involved. Note thatthe total of all ratios is greater than 1 for each row as onerule can associate more than two ATC classes, and onepatient can verify more than one rule. Fig. 3 shows sig-nificant (p < 0.001, Z-test) deviations from the expectedvalues of these ratios. For each ATC class appearing inright-hand sides of ARs, the expected ratio was computedas the combined support of rules where that class appearsin the right-hand side divided by the combined supportof all rules. A Z-test was used to assess significance atp < 0.001 of such deviations.Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 8 of 13Fig. 2 Heatmap of the distribution of drug classes associations found in Experiment 3 within the EHR population. On the left, ATC classes appearingin the left-hand side of Association Rules (ARs) and the combined support of the corresponding rules. At the top, ATC classes appearing in theright-hand side of ARs and the combined support of the corresponding rules. Values in cells denote the ratio between (i) the combined support ofARs where the left ATC class appears in the left-hand side and the top ATC class appears in right-hand side; and (ii) the combined support of ARswhere the left ATC class appears in the left-hand side. For instance, the combined support of rules where Beta-Blocking Agents (C07A) appears inthe left-hand side is 39, and the combined support of the subset of these rules where High-Ceiling Diuretics (C03C) appears in the right-hand side is72% (0.72) of 39ResultsWe present in this section the results of the experimentsdescribed previously. As the first two experiments makeuse of the tree structure of ICD-9-CM to simplify therepresentation of ADEs (as specified in Methods, FAERSphenotypes are mapped onto SNOMED CT rather thanICD-9-CM), they were applied only to the EHR dataset.The third experimental design offers a generalization ofthe approach to any drug and phenotype ontologies, andwas applied to both the EHR and FAERS datasets.We thuspresent the results of four experiments: three experimentson our EHR dataset using all three experimental designs,and a fourth one on the FAERS dataset using the thirdexperimental design.Overview of resultsThe four experiments result in four concept lattices, fromwhich we extract Association Rules (ARs) of the formL ? R. Empirically, we only retain ARs with a support ofat least 5, and a confidence of at least 0.75. Table 8 presentssome statistics about this process in our four experiments.We observe that the third experiment generates amuch larger concept lattice from the EHR dataset thanfrom the FAERS dataset, despite their similar numberof patients. Nevertheless, we obtain after filtering onlytwice as many rules from the EHR dataset in comparisonwith the FAERS dataset. Moreover, rules extracted fromFAERS have generally larger support values. These resultscan be explained by the differences between the twodatasets: the EHR dataset is built from ADEs extractedfrom EHRs of patients diagnosed with SLE, while theFAERS dataset gathers ADEs reported from the generalpopulation. Futhermore, the higher number of ADEs perpatient in the EHR dataset tends to increase similaritiesbetween patients, thus increasing the number of gener-ated concepts.Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 9 of 13Fig. 3 Statistical significance of the distribution of extracted ADE associations in Experiment 3 within the patient population. The ratio in each cell ofFig. 2 was compared to its expected value assuming a proportional distribution of ATC classes in the right-hand side. Empty cells indicate that thedifference between the observed and expected ratios is not significant (p > 0.001, Z-test). Other cells show the difference between the observedand expected ratios, and this difference is significant (p < 0.001, Z-test). p-values where computed using a standard normal table, assuming normaldistributions centered on expected ratiosFigures 2 and 3 show an overview of ATC drug classespresent in ADEs associated by the ARs extracted in thethird EHR experiment. Figure 2 shows the frequencyof such associations and Fig. 3 shows, for the signifi-cant ones, the difference to the frequency obtained if theassociation would be random. Figure 3 highlights a fewpositive deviations from the expected association ratios.For instance, we find that ADEs involving Beta-BlockingAgents (C07A) are associated strongly with ADEs involv-ing High-Ceiling Diuretics (C03C). Both classes of drugsare involved in antihypertensive therapy, either separatelyor in combination. Thus, it is likely that a certain num-ber of patients are prescribed with these two classes ofdrugs. Our results suggest that among these patients,some could experience distinct ADEs involving each class.We also observe that ADEs involving AntithromboticTable 8 Statistics about the processes of lattice building and Association Rule (AR) extraction, implemented in JavaExperiment 1 (EHR) 2 (EHR) 3 (EHR) 3 (FAERS)Number of patients 548 548 548 570Number of ADEs 3286 3286 3286 1148Lattice size (number of concepts) 1.9 million 2.3 million 2.5 million 22,700ARs extracted 5 million 7 million 9 million 18,500ARs retained after filtering 772 1907 913 493ARs with a support of at least 8 8 50 15 151Maximum support 9 10 10 27Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 10 of 13Agents (B01A) are significantly associated with otherADEs involving the same class of drugs. Thus, it appearsthat the proposed approach reveals significant associ-ations of ADEs involving either the same or differentclasses of drugs.Examples of extracted association rulesTable 9 presents examples of ADE associations obtainedfor the three experiments performed on EHRs. In fact,nearly the same rule is found here with varying general-ization levels across the three experiments. Note that forreadability and comparison purpose, all ARs are expressedin the third experiment formalism. In this example, weobserve that the AR from experiment 2 is more generalthan the AR from experiment 1 (R06A is a super-class ofdoxylamine in ATC). In the third experiment, more spe-cialized phenotypes are obtained (for instance ICD 586 isa sub-class of ICD 580-629). For each experiment, ADEscan involve a combination of two or more drugs or drugclasses. ARs may also associate a pair of ADEs on the left-hand side with a single ADE on the right-hand side as inour thrid experiment.The complete set of filtered rules for each experimentis available online at https://github.com/g-a-perso/ADE-associations/.An overview of the 11 ARs extracted from the thirdexperiment on EHR with support greater than or equal to8 is presented in Table 10. For instance, we produce thefollowing AR, with support 10 and confidence 0.77:{?{Benzothiazepine derivatives} , {Congestive heart failure}?}? {?{Drugs for peptic ulcer and GORD} , {Atrial fibrillation}?}This rule expresses that 1013 of patients who presentcongestive heart failure (ICD 428.0) after prescription ofbenzothiazepine derivatives (C08DB), also present atrialfibrillation (ICD 427.31) after prescription of a drug forpeptic ulcer and gastro-esophageal reflux disease (A02B).This rule holds for 10 patients.Support of EHR rules in STRIDEOur EHR dataset is only a small part of the total STRIDEdata warehouse that contains about 2 million EHRs. Wetherefore evaluated the support of the 11 ARs listed inTable 10 in the whole STRIDE data warehouse. EachAR was transformed into an SQL query to retrieve thepatients verifying the rule. Table 10 reports the support inthe dataset of SLE-diagnosed patients as S1 and the sup-port in the entire STRIDE database as S2. In all cases thesupport raises from S1 to S2 and the increase ratio variesfrom 2 to 36. This illustrates that the ARs extracted fromthe SLE EHRs can be relevant to patients outside of theinitial dataset.DiscussionADE extractionWe observed a large quantitative difference between theresults of our experiments on EHRs and on FAERS. This isexplained by the different nature of the two datasets: whilethe FAERS dataset gathers self-reported ADEs, we builtthe EHR dataset from ADEs we extracted. As the extrac-tion of ADEs from EHR is not the core of this work, weused a simple method that we do not evaluate here.This method has inherent limitations. Particularly, thereis uncertainty as whether the extracted events are actu-ally caused by the concerned drugs. We acknowledge thatour method for ADE detection is not as robust as dispro-portionality score algorithms [21]. In particular, we couldconsider confounding factors such as age, sex, comor-bidities or concomitant drugs. Nevertheless, we filteredextracted ADEs using SIDER in order to retain only phe-notypes that are known as side effects of the drugs listedin that ADE.Another limitation is that we are considering only drugingredients, whereas one ingredient may be prescribed invarious forms (for instance, eye drops or tablets). Not con-sidering the form of the drug may result in imprecise ADEdefinitions, as one phenotype may be caused by only someforms of the ingredient. Using the unambiguous encod-ing of prescriptions of the STRIDE EHR dataset wouldaddress this limitation, but was not available in this study.Table 9 Example of one extracted rule with varying generalization levels across the three experiments on EHRsExperiment Rule Support1 (EHR) {?{yohimbine, doxylamine, vancomycin, caffeine}, {ICD 580-629}?} ?{?{doxylamine, tocainide}, {ICD280-289}?}52 (EHR) {?{G04BE, N06BC}, {ICD 580-629}?} ?{?{R06A}, {ICD 280-289}?} 93 (EHR) {?{G04BE, N06BC}, {ICD 586}?, ?{A02B, N06BC}, {ICD 586}?} ?{?{R06AA}, {ICD 285.9}?} 5Class labels: A02B is drugs for peptic ulcer and gastro-oesophagal disease, G04BE is drugs used in erectile dysfunction, N06BC is xanthine derivatives, R06A isantihistamines for systemic use, R06AA is aminoalkyl ethers ICD 280-289 is diseases of the blood and blood-forming organs, ICD 285.9 is anemia, unspecified, ICD580-629 is diseases of the genitourinary system, ICD 586 is renal failure, unspecified. Here, yohimbine belongs to the class G04BE (drugs used in erectile dysfunction),caffeine belongs to the classe N06BC (xanthine derivatives) and doxylamine belongs to the class R06AA (aminoalkyl ethers)Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 11 of 13Table 10 A selection of 11 Association Rules based on theirsupport in the SLE EHRs datasetRule S1 S2{?{Anilides}, {Thrombocytopenia, unsp.}?, 9 326?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}? {?{Opioids}, {Anemia, unsp.}?}{?{Serotonin (5HT3) antagonists}, {Thrombocytopenia,unsp.}?,8 256?{Anilides}, {Thrombocytopenia, unsp.}?,?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}? {?{Opioids}, {Anemia, unsp.}?}{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 9 176?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}? {?{Opioids}, {Anemia, unsp.}?,?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?}{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 8 157?{Anilides}, {Thrombocytopenia, unsp.}?,?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}? {?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?,?{Opioids}, {Anemia, unsp.}?}{?{Benzothiazepine derivatives}, {Congestive heart failure,unsp.}?}10 129? {?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?}{?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?, 8 66?{ACE inhibitors, plain}, {Atrial fibrillation}?,?{Anilides}, {Atrial fibrillation}?}? {?{Serotonin (5HT3) antagonists}, {Heart failure}?,?{Drugs for peptic ulcer and GORD}, {Congestive heartfailure, unsp.}?}{?{Serotonin (5HT3) antagonists}, {Atrial fibrillation}?, 8 64?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?,?{ACE inhibitors, plain}, {Atrial fibrillation}?}? {?{Electrolyte solutions}, {Congestive heart failure,unsp.}?,?{Osmotically acting laxatives}, {Heart failure}?}{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 10 49?{Anilides}, {Thrombocytopenia, unsp.}?,?{Glucocorticoids}, {Thrombocytopenia, unsp.}?}? {?{Opioids}, {Anemia, unsp.}?,?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?}{?{Proton pump inhibitors}, {Congestive heart failure,unsp.}?,9 37?{Antithrombotic agents, Anilides, Opium alkaloids andderivatives}, {Heart failure}?,?{Anilides}, {Congestive heart failure, unsp.}?, ?{Anxiolytics},{Heart failure}?,?{Electrolyte solutions}, {Congestive heart failure, unsp.}?}Table 10 A selection of 11 Association Rules based on theirsupport in the SLE EHRs dataset (Continued)? {?{Opioids}, {Anemia, unsp.}?}{?{Sulfonamides, plain}, {Congestive heart failure, unsp.}?, 8 33?{Antithrombotic agents, Anilides, Opium alkaloids andderivatives},{Heart failure}?,?{Proton pump inhibitors}, {Congestive heart failure,unsp.}?,?{Anxiolytics}, {Heart failure}?,?{Anilides}, {Congestive heart failure, unsp.}?,?{Electrolyte solutions}, {Congestive heart failure, unsp.}?,?{Sulfonamides, plain, R05D}, {Heart failure}?}? {?{Opioids}, {Anemia, unsp.}?}{?{Anilides, Opium alkaloids and derivatives, Proton pumpinhibitors}, {Heart failure}?,8 31?{Anilides, Proton pump inhibitors}, {Congestive heart fail-ure, unsp.}?,?{Antithrombotic agents, Anilides, Opium alkaloids andderivatives},{Heart failure}?,?{Anxiolytics}, {Congestive heart failure, unsp.}?,?{Electrolyte solutions}, {Congestive heart failure, unsp.}?}? {?{Opioids}, {Anemia, unsp.}?}S1 denotes the support in the dataset used to extract the AR, and S2 denotes itssupport in the entire STRIDE datasetFor these reasons, ADEs extracted from EHRs likelypresent a relatively high rate of false positives. This is alsoreflected in the size of the concept lattice we generatedfrom that dataset, as noise increase the number of possiblegeneralizations (see Table 8).ADE representationWhile pattern structures permit detailed descriptions ofADEs, the algorithmic complexity of comparing thosedescriptions and building the concept lattice needs to beconsidered. In particular, the size of the concept latticethat needs to be generated proves to be a limiting factorto scale the approach on larger datasets. We observed thatthe size of the lattice increases as we use more detaileddescriptions of ADEs.One apparent limitation of this work is the absence oftemporal relationships between ADEs. We voluntarily didnot consider that aspect because the order of occurrenceof ADEs can vary between patients. However, in cases ofinterest, this order can be checked in patient EHRs as pat-tern structure concepts retain patient identifiers as wellas their description. Preliminary investigation for a givensubset of patient EHRs reveals that the ADEs of the left-hand side of an AR can occur either before or after theADEs of the right-hand side of the rule.Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 12 of 13In our experiments on EHRs, we only considered sideeffect phenotypes occuring in a timeframe of 14 daysafter a prescription, whereas an ADE may manifest muchlater after the initial prescription. Thus, we only extractedassociations between rather short-term ADEs. The repre-sentation of ADEs used in the different experiments couldbe expanded with data about the actual delay betweenthe prescription and the observed phenotypes. This wouldallow for mining associations in a dataset of both short-term and long-term ADEs, while retaining the abilityto discriminate between these different manifestations.In particular, this could permit extracting associationsbetween short-term and long-term ADEs, where short-term toxicity to a given drug could be used as a predictorof the long-term toxicity of another.Associations between ADEsWe use association rule mining to extract associationsbetween frequently co-occuring ADEs. A limitation ofthat approach is that we cannot infer any causal rela-tionship between these ADEs. However, it appears moremeaningful to investigate potential common causes ofADEs associated through an AR, rather than to search adirect causal relationship between involved ADEs. Besidesconcerns on the quality of the association itself, this lim-its its interpretation and exploitation: without a properexplanation of the relationship of the two ADEs, the rulescannot be used to guide drug prescription. They can how-ever raise vigilance towards the possible occurence of anadditionall ADE.A large amount of ARs can be extracted from ourconcept lattices. We automatically filtered a subset ofthese ARs by excluding rules that do not fit the scopeof the study. While the approach we proposed is flex-ible, it is difficult to compare ARs extracted from verydifferent datasets and expressed with different ontolo-gies. Therefore, we tested selected rules obtained fromour SLE-oriented EHR dataset on the whole STRIDEdatabase. The results of these tests indicate that rulesextracted from a subset of EHRs (here patients diag-nosed with SLE) can apply to a more general set ofpatients (Table 10). Indeed, SLE patients are suscepti-ble to multiple occurrences of ADEs caused by a widerange of drugs. EHRs of such patients, used in conjunc-tion with biomedical ontologies may then be used toidentify frequently associated ADEs. We now need toprioritize these ARs with respect to their importance interms of cost and risk of the phenotypes present in theirright-hand side.ConclusionsWe explore in this paper an approach based on patternstructures to mine EHRs and adverse event reporting sys-tems for commonly associated ADEs. Pattern structurespermit to work with an expressive representation of ADEs,which takes into account the multiplicity of drugs andphenotypes that can be involved in a single event. Patternstructures also allow to enhance this representation withdiverse biomedical ontologies, enabling semantic compar-ison of ADEs. To our knowledge, this is the first approachable to consider such detailed representations to mineassociations between frequently associated ADEs. Theproposed approach is also flexible and can be applied tovarious EHRs and adverse event reporting systems, alongwith any linked biomedical ontology. We demonstratedthe genericity of the approach on two different datasets,each of them linked to two of three distinct biomedicalontologies.The kind of extracted ARs presented in this articlecould serve as a basis for a recommandation system.For instance, such a system could recommand vigilancetowards the possible occurence of an ADE based on theADE history of the patient. Drugs involved in ARs of inter-est could be investigated, in light of the current knowledgeof their mechanisms, to look for possible common causesbetween associated ADEs. Our chosen representation forADEs could be further extended to include additionalproperties of drugs and phenotypes, such as drugs targetsannotated with Gene Ontology classes. This could permitto search for association rules taking into account the drugmechanisms.AbbreviationsADE: Adverse drug events; AR: Association rule; ATC: Anatomical therapeuticchemical classification system; EHR: Electronic health record; FAERS: Food &Drug Administration adverse event reporting system; FCA: Formal conceptanalysis; ICD-9-CM: International classification of diseases, ninth revision,clinical modification; SLE: Systemic lupus erythematosus; SNOMED CT:Systematized nomenclature of medicineclinical termsAcknowledgementsWe acknowledge the participants of the Bio-Ontologies SIG conference fortheir constructive feedback on the preliminary results of this work.FundingThis project is supported by the PractiKPharma project, grantANR-15-CE23-0028, funded by the French National Research Agency and bySnowflake an Inria associate team, and the France-Stanford Center forInterdisciplinary Studies.Availability of data andmaterialsThe complete set of filtered association rules for each experiment is availableonline at https://github.com/g-a-perso/ADE-associations/.Authors contributionsGP, MDD, MS and AC designed the experiments and wrote the manuscript. GPdeveloped necessary code and ran the experiments. EB prepared the FAERSdataset for mining and advised on technical aspects of the work. MD providedaccess to the STRIDE database and advised in the manipulation of this data.MDD and MD advised about biomedical use cases and interpreted the results.All authors read and approved the final manuscript.Ethics approval and consent to participateThe work was done with IRB approval (#24883) at Stanford University.Consent for publicationNot applicablePersoneni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 13 of 13Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1LORIA (CNRS, Inria NGE, Université de Lorraine), Campus Scientifique, F-54506Vanduvre-lès-Nancy, France. 2Institute of Data Science, MaastrichtUniversity, 6200 MD Maastricht, Netherlands. 3Stanford Center for BiomedicalInformatics Research, Stanford, USA.Received: 3 November 2016 Accepted: 1 August 2017SOFTWARE Open AccessLiterature evidence in open targets - atarget validation platform?enay Kafkas1,2*, Ian Dunham1,2 and Johanna McEntyre1,2AbstractBackground: We present the Europe PMC literature component of Open Targets - a target validation platform thatintegrates various evidence to aid drug target identification and validation. The component identifies target-diseaseassociations in documents and ranks the documents based on their confidence from the Europe PMC literaturedatabase, by using rules utilising expert-provided heuristic information. The confidence score of a given documentrepresents how valuable the document is in the scope of target validation for a given target-disease association bytaking into account the credibility of the association based on the properties of the text. The component serves theplatform regularly with the up-to-date data since December, 2015.Results: Currently, there are a total number of 1168365 distinct target-disease associations text mined from>26 million PubMed abstracts and >1.2 million Open Access full text articles. Our comparative analyses on thecurrent available evidence data in the platform revealed that 850179 of these associations are exclusivelyidentified by literature mining.Conclusions: This component helps the platforms users by providing the most relevant literature hits for agiven target and disease. The text mining evidence along with the other types of evidence can be exploredvisually through https://www.targetvalidation.org and all the evidence data is available for download in jsonformat from https://www.targetvalidation.org/downloads/data.Keywords: Target validation, Text mining, Target-disease associations, Document ranking, Information retrievalBackgroundUnderstanding the underlying mechanisms of diseases iscrucial in translational research. Discovering the associationbetween drug target and disease has become a main focusfor scientists since it is key for developing new drugs or re-purposing them. Scientists gather various evidence repre-senting different aspects of target-disease associations suchas gene expression changes and the role of genetic varia-tions to increase understanding. Such evidence can bestored in structured databases and requires integration toobtain complete and comprehensive knowledge in targetvalidation studies.Motivated by this, the Target Validation Platform(https://targetvalidation.org) [1] integrates different evi-dence from various resources with the aim of assistingscientists to identify and prioritise drug targets (proteinsand their genes) associated with diseases and phenotypes.The evidence includes common disease genetic evidencebased on GWAS study results from GWAS Catalog [2],rare Mendelian disease evidence based on ClinVar [3]clinical variant information from EVA and text minedtarget-disease associations from the Europe PMC(https://europepmc.org/) literature database [4] (see Table 3for a complete list of evidence types).Europe PMC contains over 33 million records and ex-pands at a rate of over a million articles per yearonearticle every two minutes as scientists publish their find-ings continuously. Text mining target-disease associa-tions is crucial for an integrated platform like the TargetValidation Platform, since it provides a high volume ofcomplementary and up-to-date data to the other type ofevidences, otherwise the knowledge would stay hiddenin millions of documents.In this study, we present the Europe PMC Open Tar-gets literature component that identifies target-diseaseassociations in documents and ranks the documents* Correspondence: kafkas@ebi.ac.uk1European Molecular Biology Laboratory (EMBL-EBI), European BioinformaticsInstitute, Wellcome Genome Campus, Hinxton CB10 1SD, UK2Open Targets, Wellcome Genome Campus, Hinxton CB10 1SD, UK© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Kafkas et al. Journal of Biomedical Semantics  (2017) 8:20 DOI 10.1186/s13326-017-0131-3according to their confidence based on rules utilisingexpert-provided heuristic information. Our main aim isto provide a scalable, robust and continuous text-miningservice to the community for a real-world and veryimportant applicationtarget validation. Many of theprevious studies focused on extracting gene-diseaseassociation from the literature [57]. However, only afew of them specifically focused on developing methodsfor integrated resources; DisGeNET [8] and DISEASES [9]for example cover various types of evidence for targetvalidation. These two systems provide confidence scoresfor target-disease associations extracted from Medlineabstracts for a given disease or target and dont providevery regular updates to the data. In DisGeNET, the tar-get-disease text mining method is based on a machinelearning approach while in DISEASES, target-diseaseassociations are extracted based on scoring their co-occurrences according to their confidence. In compari-son to DisGeNET and DISEASES, our system operateson full text articles in addition to abstracts, and ranksdocuments according to the confidence for a given tar-get-disease association rather than ranking the associa-tions extracted from the whole set of Medline abstracts.More specifically, we calculate a document confidencescore for each given (article, target, disease) triple whichrepresents how valuable the document is in the scope oftarget validation for the given target-disease association(see "Document scoring" section). However, the confi-dence score of a given target-disease association is handledat the platform level and calculated based on all theevidence data in the platform by using a harmonic sumapproach (see [1] for the details). This confidence scoreat the association level represents the overall credibilityof the evidence for a given target-disease association.Our approach to target-disease extraction differs fromthese systems, and probably many other traditionaltext-mining studies, in that we rely on heuristic infor-mation from experts/users for developing the system.The platform was first launched in December, 2015 andis publicly available at https://targetvalidation.org. Sincethen, our system has served the platform regularly(monthly) with up-to-date data.ImplementationResources usedThe literature source that we used in the study is theEurope PMC database. Europe PMC is one of the largestbiomedical literature databases in the world which pro-vides public access to >30.4 million abstracts and >3.3million full text articles from PubMed and PubMedCentral. In our analyses, we used the latest version ofthe Open Access full text articles (http://europepmc.org/ftp/archive/v.2016.06/) (~1.2 Million), and all of thePubMed abstracts (~26 Million) from the database.Two comprehensive resources, UniProt and the Ex-perimental Factor Ontology (EFO) are used to identifytarget and disease names in text, respectively. These tworesources are chosen as the reference resources by OpenTargets. The data providers of the platform are asked toground their target and disease entities in to these refer-ence resources so as to integrate the evidence in theplatform. Therefore, two dictionaries are generated andrefined from the human part of the SwissProt Database(the annotated part of UniProt, Release 2015_10) (http://www.uniprot.org/) and disease and phenotype parts ofEFO (http://www.ebi.ac.uk/efo/) (Release 2.74) beforeapplying text mining. In the refining process, we filteredout the terms that would introduce potentially very highnumbers of false positives. These are the terms havingcharacter length < 3 (e.g. A is a gene name) and termsthat are ambiguous with common English words (e.g.Large is a protein name as well). In addition, we gener-ated term variations by replacing the widely used Greekletters in gene/disease names with their symbols (e.g. re-placing alpha with ?). The final target and disease dic-tionaries consisted of a total of 104,434 and 29,846terms respectively. These dictionaries are available fromftp://ftp.ebi.ac.uk/pub/databases/pmc/otar/.Target and disease name annotationWe used the Europe PMC text-mining pipeline, which isbased on Whatizit [10], to annotate target and diseasenames in text with the two dictionaries described above.Although we reduce a very high level of ambiguity byapplying the dictionary refinement process before textmining the documents, some target and disease nameabbreviations could still be ambiguous with some othernames. For example, ALS which is an abbreviation usedfor Amyotrophic Lateral Sclerosis, is ambiguous withAdvanced Life Support in some articles (e.g. seePMID:26811420). Therefore, we implemented and useda disease and target name abbreviation filter for screen-ing out the potential false positive abbreviations intro-duced during the annotation process. Our tool differsfrom the available abbreviation finders, such as [11]since it behaves rather as a filter specifically for potentialfalse positive target and disease name abbreviations an-notated based on our dictionaries.The abbreviation filter operates based on several rulesusing heuristic information. Regular expressions are usedfor identifying the text sequences in the form of X..Y. Z. (XYZ). The text in parentheses (i.e. (XYZ)) isidentified as a gene/disease name abbreviation candidateif it is in the uppercase form, has length <6 (the lengthwas decided by manually analysing a random subset ofthe Uniprot and EFO dictionaries) and annotated bythe system either as a disease or a gene name, whereas,the text located immediately before the parentheses isKafkas et al. Journal of Biomedical Semantics  (2017) 8:20 Page 2 of 9identified as the potential long form. For example, inthe following sentence from the article havingPMID:26811420; The guidelines form the basis for alllevels of resuscitation training, now from first aid toadvanced life support (ALS), the italicised textmatches with our pattern defined above. ALS wouldbe the abbreviation candidate and advanced life sup-port would be the potential long form. Documentsmatching the pattern above are analysed manually byan expert to come up with heuristics that we can applyin filtering the ambiguous abbreviation. Abbreviationcandidates satisfying one of the following rules arekept as true target/disease abbreviations, otherwise,they are filtered out:For disease name abbreviation candidates: If any of the EFO long forms of the abbreviationcandidate exists in the document If the long form extracted from the text containsany of the keywords (disease, disorder, syndrome,defect, etc.) that can be used to describe a diseaseFor gene or protein name abbreviation candidates: If (XYZ) appears more than 3 times in thedocument body (this rule applies to OA full textdocuments only) If the long form matches any of the terms fromSwissProt or Enzymes (http://enzyme.expasy.org/) If the long form ends with (-ase/-ases) OR it containsany of the keywords (factor, receptor, gene, proteinetc.) that can be used to describe a target name If at least 3 sentences for full text and at least 2sentences for abstracts contain one of the keywords:mutation, SNP, variation, gene, inhibit, variation,variant, polymorphism, mutant, isoform, protein,enzyme, activate, antibody, transcription, tumorsuppressor, express, overexpress, regulator, receptor,oncogene along with the protein name abbreviationcandidate and a disease name.Target-disease association identificationOur association extraction method is based on identifi-cation of target-disease co-occurrences at the sentencelevel and applying several filtering rules to reduce noisepossibly introduced by the high sensitivity, low specifi-city co-occurrence method. Our filtering rules utiliseheuristic information from a careful manual analysis ofthe text data to filter out potential false positive associ-ations. More specifically, the manual analyses are con-ducted iteratively by analysing a randomly selected setof results and identifying the reasons behind the falsepositives in the results so that we could formulate themas filtering rules to tune our system.The system applies the following filtering rules:1. Filter out all type of articles except Researcharticles (e.g. Reviews, Case Reports).2. Filter out target-disease associations appearing in theMazo et al. Journal of Biomedical Semantics  (2017) 8:47 DOI 10.1186/s13326-017-0158-5RESEARCH Open AccessA histological ontology of the humancardiovascular systemClaudia Mazo1* , Liliana Salazar2, Oscar Corcho3, Maria Trujillo1 and Enrique Alegre4AbstractBackground: In this paper, we describe a histological ontology of the human cardiovascular system developed incollaboration among histology experts and computer scientists.Results: The histological ontology is developed following an existing methodology using Conceptual Models (CMs)and validated using OOPS!, expert evaluation with CMs, and how accurately the ontology can answer theCompetency Questions (CQ). It is publicly available at http://bioportal.bioontology.org/ontologies/HO and https://w3id.org/def/System.Conclusions: The histological ontology is developed to support complex tasks, such as supporting teachingactivities, medical practices, and bio-medical research or having natural language interactions.Keywords: Ontology, Human histology, Fundamental tissues, Organs, Cardiovascular systemBackgroundMorphological sciences experts knowledge is an impor-tant source in histology studies and practices for humanstudies at cellular, tissue, organ and system levels. Asmanyother domains, histology domain also suffers from prob-lems like vocabulary heterogeneity, the use of ambiguouslanguage, semantic differences and subjectivity that mayaffect research, analysis and information retrieval pro-cesses. Different terms are used to designate the sameconcept or structure or the same term is used withdifferent meanings, in different texts.Two main challenges are identified in the histologydomain [1]: (i) communicate specifically, clearly and pre-cisely histology concepts and (ii) represent or modelknowledge from histology data sources in order to inter-act and process it automatically. These challenges requirea profound analysis of the structure and the conceptsof histological terminologies. This analysis can be doneby constructing histological domain ontologies. The useof ontologies for representing knowledge is common inmedical applications, such as anatomy, and histology*Correspondence: claudia.mazo@correounivalle.edu.coEqual contributors1Computer and Systems Engineering School, Universidad del Valle, Cali,ColombiaFull list of author information is available at the end of the articleamong others. The union between ontologies and medi-cal information is considered as a necessary alternative tosolve main problems regarding those sources of informa-tion [24].The term ontology has many definitions depending onthe author and the way an ontology is built and used bycomputer systems. One of the most widespread definitionof ontology is: Ontology is an explicit and formal spec-ification of a shared conceptualisation [5]. Ontologiescreate models to formalise knowledge in the same waythat it is used. From a histology perspective, an ontologywould consist of concepts defined by histological knowl-edge. Additionally, relations, attributes, rules and axiomsenrich and contribute to expand the vocabulary used toformalise knowledge. On the other hand, a taxonomyis a set of definitions that are organised by a hierarchythat starts at the most general description and gets morerefined and specific terms as the hierarchy goes down.Many ontologies and taxonomies are available in elec-tronic form with Open Source licenses. Ones of thebest known medical taxonomies are: GALEN [6] (basicclinical concepts  fracture, bone, and so on  con-trolling combinations of related concepts  bone frac-tures  and complex concepts  clavicle fracture),UMLS (Unified Medical Language System) [7], MeSH(Medical Subject Heading) [8], Kingsbury Center for© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 2 of 15Cancer Care Glossary [9], MedicineNet Medical Dictio-nary [10], Multilingual Glossary of Technical, and Pop-ular Medical Terms in nine European Languages [11],ICD (International Classification of Diseases) [12] amongothers [13]. Some ontologies are used in web retrievalsystems [14], identification of relations between dis-eases [15], and diagnosis [16], among others [13]. Someontologies are used in web retrieval systems [14], iden-tification of relations between diseases [15], and diagno-sis [16], among others. Uberon ontology is an anatomyontology, which is a common standard used by thebiomedical research community [17]. However, noneof these ontologies covers histological knowledge ofthe human cardiovascular system without pathologiesin the same kind of guidance and organisation to ourresearch.In this paper, we describe our work to build a histolog-ical ontology of the human cardiovascular system. Thiswork is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Generic1 license. Weselected the cardiovascular system because it is one ofthe most committed to the development of diseases asso-ciated with modern life. To the best of our knowledge,and after a careful search in the most relevant reposito-ries, there is no a histological ontology in the literature,thus we consider this one to be a relevant contributionto the research community in the histology domain. Weleft the histological ontology publicly available at http://bioportal.bioontology.org/ontologies/HO, the documen-tation at https://w3id.org/def/System and the OWL filesat https://github.com/claxima/HistologicalOntology.The rest of the paper is structured as follows: themethodology to build the histological ontology is pre-sented in Methods section; the evaluation and theresults are presented and discussed in Results section; inDiscussion section we analyse the obtained results; andsome conclusions are presented in Conclusions section.MethodsThe NeOn methodology is one of the most used method-ologies for ontology engineering [18]. This methodologydoes not prescribe a rigid ontology development work-flow, but instead it suggests nine scenarios for developingontologies. The methodology covers commonly occur-ring situations which mostly focus on reusing, merging,restructuring and re-engineering ontological resources.Taking into account that we will create a histologicalontology without reusing ontological resources, accord-ing to our analysis of the State-of-the-Art, we decidedto use the methodology proposed in [19]. This method-ology consists of the following steps: (i) identification ofpurpose, scope, CQs and scenarios, (ii) identification ofthose ontologies we could reuse, (iii) domain analysis andknowledge acquisition, (iv) iterative building of informalontology models, (v) formalisation and (vi) evaluation.Wemodify minimally this methodology in steps i, iii and vi,Fig. 1 presents the resulting steps. Firstly, wemerge step (i)and (iii) which will be our first step called capturing expertand histological knowledge. Secondly, we use three eval-uation criteria  detecting pitfalls, expert evaluation andanswering CQs  while [19] uses two evaluation criteria CMs and the Protégé axiom language plug-in providedby Protégé.Capturing expert and histological knowledgeIn this step, the aim is domain knowledge extraction usinga set of knowledge capture activities  meetings, dis-cussions, histology classes, among others. We planned aseries of activities with the experts through which thefoundations of our ontology were built: purpose, scope,CQs and scenarios. We hosted a series of meetings withthe group of histology experts conformed by members ofthe research group Teblami2, from the Universidad delValle3, in which the domain experts discussed the termi-nology and the structure used to describe the processes toFig. 1Methodology to develop ontologiesMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 3 of 15analyse a histological sample. The experts team was com-prised by histology professors, with more than five yearsof teaching and research experience, and biomedical grad-uate students, with mayor on histology, all of them formedthe research histology area.The questions to answer, at this stage, were the follow-ing: (i) what is the ontology going to be used for?, (ii) whatdo we want the ontology to be aware of?, (iii) what is thescope of the knowledge that we want to have in the ontol-ogy?, and (iv) how is the ontology going to be used?, theanswers are provided in the following subsections.Purpose, Scope and ScenariosCommonly, ontology development is not the final goal ofthe process. Instead, an ontology becomes an artefact tobe used by other systems. Under this perspective, the pur-pose is defined by the main reasons that can lead to creat-ing an ontology [20]. Our Ontology was constructed for:(i) sharing a common understanding of histology knowl-edge between people and machines in processes suchas automatic recognition and identification of cells, tis-sues and organs; (ii) allowing reuse of domain knowledge;(iii) allowing change specifications of histology knowl-edge, if changes occur in it. Therefore, our main targetcommunity are both, medical professors and biomedicalresearchers. In addition, explicit specifications of histol-ogy knowledge are useful for users who should learn themeaning of histological terms, to specialised users thatwant to develop a semantic visual information retrievalsystem, or to other users that want to label automati-cally histological images or teach to students histologicalsstructures and relations.This work is focused on the human cardiovascular sys-tem, which is one of the most committed to the devel-opment of diseases associated with modern life. Threescenarios are described to illustrate and motivate the usefor this histological ontology. These scenarios are laterused to develop a set of CQs and to indicate how theontology would be used in these cases.Professor: a histology expert works as a professor in auniversity teaching histology of the cardiovascular sys-tem. The expert teaches different group levels, coveringhistology of cells, tissues, organs and systems. The pro-fessor should cover each topic considering components,relations and organisations. Additionally, she/he may alsopromote self-learning to on-campus students and facili-tate on-line learning to external or remote students.Biomedical research: a researcher is interested in work-ing with a big data set of histological images, whichare not labelled. The researcher has to label each his-tological image with cells, tissue and organs using acontrolled vocabulary, in short time, reducing subjectiv-ity and increasing precision. Additionally, the researchershould search and recover images according to presentstructures to develop different steps in her or hisresearch.Medical: a histology expert works in a hospital analysingsamples in the cardiovascular system context. Whenreceiving a sample, the histologist analyses, labels andvalidates different characteristics of the sample.Having defined the purpose, scope, and scenarios ofthe ontology, we discussed the CQs with our histologyexperts. These CQs were used at a later stage in order toevaluate the resulting ontology.Competency questions (CQs)CQs are the kind of questions for which we wantthe ontology to be able to provide support for repre-sentation or reasoning processes. Additionally, thosequestions are essential for evaluating ontologies [21].Experts should express the CQs in natural languagewithout any constraint. Based on the above scenarios,we have identified four categories of CQs: classifica-tions, properties, constraints and inferences. Examplesof those CQs are presented in Table 1, and https://github.com/claxima/HistologicalOntology/blob/master/CompetencyQuestions.pdf contains the completedocument.Table 1 Examples of CQsClassificationWhat are the organs of the cardiovascular system?What is the composition of the myocardium?What are the muscular arteries?PropertiesWhat are the tunics in veins?Which is the constitution of a media tunic?What are the structures present in the large veins?ConstraintsA simple epithelial tissue cannot be stratifiedA capillary is only composed of endotheliumAn organ can have three tunics maximumInferencesIf a set of cells is close to a light region, then the tissue is probably anepithelial tissueIf an organ has a thin media tunic as well as a thick adventitia tunic anda wide light region, it is probably a veinIf an organ has a thick media tunic and a small light region, it is probablyan arteryMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 4 of 15Classes and propertiesIn this step, we illustrate the construction of our ontol-ogy and explain its primitive classes and properties. Thecore classes of our histological ontology are: cells, tis-sues, organs and systems. These are the main structuresto represent. Some examples of relevant properties are:layers, cell morphology, ducts, specialisation, mechanismof secretion, nature of secretion, valves and nodes. Someexamples of object properties of histology ontology areincluded in Table 2.A modular implementation taking into account tissues,organs and systems was used in our ontology to facilitateintegration and/or reuse of histological data.Two tasks were developed in this stage: (i) build the glos-sary of terms with their definitions and synonyms, and(ii) build the taxonomy of concepts. Figure 2 shows thecomplete glossary of terms obtained for the human car-diovascular system. Figures 3, 4, 5, 6, 7, 8 and 9 showthe CMs which represent the taxonomies for cells, tissuesand organs; these taxonomies are divided to show in moredetail the different components and relations.Identifying reusable ontologiesOntology research and analysis were carried out to assesswhether there were elements that could be reused in ourproposal [22]. For that, we took into account the histo-logical and the anatomical perspectives. BioPortal [23]contains some histological terms. However, this thesaurushas different kind of guidance to our research due tothe fact that its organisation does not contain a specificorder and some terms are randomly located, for this rea-son it cannot be reused. BioPortal [24] contains conceptssimilar to those required in our ontology such as tissuesand cells. Nevertheless, this is a human histopathologi-cal ontology which contain abnormal cell types which canoccur in either disease states or disease models, then thisontology cannot be used in our research. Additionally,this ontology does not contain the organs of the cardio-vascular system nor the classification of tissues since itis focused on retinal, mammary, urethral, among others.Finally, some terms can be referenced as individual con-cepts. BioPortal [23] and [24] have similar terms to thoserequired in our research, for instance terms related to theepithelial tissue. Nevertheless, these concepts are linkedTable 2 Object properties in histology ontologyProperty Domain class Range class Inverse propertyisOrganOf Organ System hasOrganisTypeOf TypeOrgan Organ hasTypeisCellOf Cell Tissue hasCellisMorphologyOf Cell morphology Epihelial tissue hasMorphologyhasNumberLayer Epithellial tissue Number layer isNumberLayerOfby a different route, tissues blood vessels. These ontolo-gies contain many concepts but the hierarchical relationsamong them are not detailed in depth. Under this con-dition, if the hierarchy is represented as a tree, some ofits branches are left inconclusive. This case is seen, forinstance, for muscle tissue. Concepts are linked in one-way allowing to connect from a large to a small structurebut not reverse. Due to the way the concepts are organ-ised, the methods to search for a concept may not appearlogical nor intuitive. Hence, the user may need specialisedknowledge or spend more time and effort (e.g. exhaus-tive search) in finding possible routes for these terms.BioPortal [25] contains the cardiovascular system and itsorgans. It is a complete ontology and close to what issought in our research. However, some terms are not inthis ontology such as the type of epithelium, connectiveand muscle tissues, which has another classification cutaneous, corneal and lymphatic. Moreover, it is a fairlycomplete cardiovascular system and organs ontology. Ithas large shortcomings regarding the fundamental tissues epithelial tissue and muscle tissue can be referencedas individual terms. Uberon, the Uber-anatomy ontol-ogy, [17] is an anatomy ontology representing a varietyof entities classified according to traditional anatomicalcriteria such as structure, function and developmental lin-eage. Uberon ontology takes into account Cardiovascularsystem. However, Uberon represents anatomical struc-tures grouped in high-level categories and it is organisedaccording to traditional anatomical classification criteria,being different to our histological classification criteria.BioPortal [26] is a mouse ontology with an adult grossanatomy focus, for this reason it does not contain micro-scopic terms such as cells, fibres, and tissue with histo-logical information. However, this ontology contains somesimilar organ and system terms which can be referencedas individual concepts in our ontology.Finally, we did not find in the State-of-the-Art an ontol-ogy of histology neither a similar organisation of hierar-chies of histology terms that we may be able to reuse.We followed a top-down approach [27] where histologyexperts work together to identify requirements and createthe CMs. Finally, we did not reuse any available ontol-ogy, nevertheless, there is an open door to include termswhich are related to existing ontologies by linking usingrdfs:sameAs and rdfs:seeAlso.Iterative building of informal ontology modelsWe use CMs in each step of our methodology. CMs aregraphs comprised of nodes connected by arcs represent-ing concepts and relations between them [28] (see Fig. 10).CMs are useful to share and capture knowledge, to facil-itate communication with experts as well as to formaliseuse cases, and for evaluation purposes. Figure 10 illus-trates the classification of the muscular tissue, in twoMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 5 of 15Fig. 2 Glossary of human cardiovascular systemways: (i) muscular tissue is classified into smooth andstriated, (ii) striated muscular tissue is classified intoskeletal and cardiac.Histology and expert knowledge are representedusing instances and relations with as much detail aspossible in CMs. Concept-predicate structures areeasily identified with this knowledge modelling. Sub-jects are entities that perform or receive an action,whereas the predicate is everything that may besaid about a subject. The subjects, predicates andobjects are extracted from histological knowledgemanually.Fig. 3 Taxonomy of main cells observed in a sample of the circulatory systemMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 6 of 15Fig. 4 Taxonomy of the fundamental tissues. The epithelial tissue is not completely displayed here to improve visualisationClasses and subclasses were identified using the CMsrepresentation; for example, epithelial tissue is_a funda-mental tissue and simple flat epithelium is_an epithelialtissue. Similarly, attributes were obtained. For instance,has_attribute or is_attribute_of. An iterative process wascarried out to represent histological and expert knowl-edge by providing a full narration of the instances,specific properties, and relations. Experts did a valida-tion process after obtaining our representation of theknowledge.Fig. 5 Taxonomy of the epithelial tissueMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 7 of 15Fig. 6 Taxonomy of histological classification of the circulatory systemFormalisationInformal models obtained, in the last step, with CMsare converted into formal models which are computa-tionally valid, using Web Ontology Language Overview(OWL) [29]. Formal languages enable the encodingof knowledge and often include reasoning rules. Ourhistological ontology is expressed in OWL and imple-mented using Protégé [30].The transformation from CMs models into an OWLmodel requires an interdisciplinary work. DomainFig. 7 Taxonomy of histological classification of layers: a layers of the heart. b layers of blood vesselsMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 8 of 15Fig. 8 Taxonomy of classification of anatomical regions present in the heartexperts develop part of the ontology by modelling theirknowledge, with the assistance of knowledge engineers.Experts defined classes, properties and relations, withas much detail as possible, to obtain a consistent OWLmodel. Interdisciplinary work has advantages andchallenges. One of the most important advantages isthe possibility of covering topics in more depth, consid-ering that there are many and varied perspectives forexploring a topic, to develop important discoveries. Chal-lenges include arranging time for meetings, developinga common language and a knowledge baseline, dealingproactively with expectations and misunderstandings,focusing on a CM, and providing timely feedback.ResultsIn this section we present the results obtained usinga three-fold approach to validate our ontology beforeputting it into use. First of all, we detected someof the most common pitfalls using OOPS!. Secondly,we performed expert evaluation using conceptual mod-els. Thirdly, we evaluated how accurately the ontologyanswered our CQs.Detecting PitfallsWe used OOPS! [31], a web tool for detecting the mostcommon pitfalls in ontologies.OOPS! detects warnings incases such as: reasoning problems, naming conventions,unconnected elements, modelling as well as reasoningproblems and many others described in the catalogue.This evaluation enables to improve the maintainability,the accessibility and the clarity of the ontology.After executing OOPS! with the histological ontology,we obtained a summary of the pitfalls encountered as pre-senting in Figs. 11 and 12. Figures show two pitfalls beingFig. 9 Taxonomy of classification of anatomical sectors present in the heartMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 9 of 15Fig. 10 CM representationdetected as well as one suggestion and one warning ineach case.Expert evaluationWe use CMs for evaluating the ontology taking intoaccount that CMs represent the conceptual scaffold of theknowledge we are representing. Although several criteriaare used to validate ontologies, we are interested in theformal correctness of the ontology, as described in [32]:(i) completeness based on covering all terms related to thecardiovascular system, (ii) duplication errors to eliminateontology elements which are redundant, (iii) disjunctionerrors to define a class as a conjunction of distinct classes,and (iv) consistency and coherence based on checking ifthe current definitions have been accurately represented syntactically and semantically.Abacha and Zweigenbaum [33] propose a validationof medical ontologies through simple questions withonly two possible answers (Yes/No) and a textual feed-back. This method makes the evaluation easier formedical experts and they can interpret feedback eas-ier. We used this method through the construction ofa survey. The elaboration of this survey was addressedwith four basic objectives: (i) identify elements thatneed to be validated, (ii) organise the elements to bevalidated, (iii) identify the characteristics to be val-idated in these elements, and (iv)interpret the feed-back and make the necessary updates. We have madethe complete survey publicly available at the follow-ing URL http://survey-megaspace.rhcloud.com/survey/index.php/656146?lang=es. The second step consists inproviding the survey to our group of experts. The thirdstep consists in interpreting experts feedback to vali-date or modify the ontology. We applied two differentsurveys. The first survey was applied in order to doan initial evaluation on the first version of our ontol-ogy, which was enhanced following the expert recom-mendations. This survey was taken by 20 students inthe third year of Medicine and Surgery at Universi-dad del Valle. The second survey was taken by 51experts from Latin America with different specialties(See Fig. 13), from which 32 have over 10 years ofexperience. Additionally, the action fields are 22 pro-fessor, 1 researcher and 28 both. The results of thesurveys are summarised in Figs. 14, 15 and 16. Takinginto account our criteria to evaluate, the experts eval-uation tackles issues concerning concepts and logicalrelations.Fig. 11 Evaluation results for tissuesMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 10 of 15Fig. 12 Evaluation results for organs and systemWhere possible, the first version of the ontology wasenhanced by following the students recommendations.However, one of the drawbacks of the first survey was thelack of experience of the participants. For this reason, theiranswers were previously revalidated by an expert in orderto take them into account.Each evaluated criterion increased, when it is com-pared to the first survey, by (i) completeness 35, 196%,(ii) duplication and disjunction 17, 156%, (iii) consistencyand coherence 20, 000%. The results confirm that thenew version had improved regarding the first oneusing the experts suggestions. Additionally, our ontol-ogy was designed in a modular way that enables aneasy integration or reuse. In this way, the integrationof other systems, such as the digestive and the respira-tory, can be done without modifying the cardiovascularsystem.Answering CQsWe evaluate the capability of the ontology to answerthe CQs, using SPARQL [34]. SPARQL was used torepresent the CQs to retrieve data from the ontologyaccording to the query. SPARQL queries were created toverify if the ontology gives a correct answer for each CQ,https://github.com/claxima/HistologicalOntology/blob/master/SPARQL_Queries.pdf contains the complete doc-ument. CQ, SPARQL query and a figure with the resultobtained are presented in the following examples:Fig. 13 a Experts by country of the second survey. b Experts by specialty of the second survey. Quantity represents the number of expertsMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 11 of 15a bFig. 14 Completeness: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage of experts perquestion and Question represents the associated number to a questionCQ-0: What are the fundamental tissues? Figure 17shows the obtained results.PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX owl: <http://www.w3.org/2002/07/owl#>PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX tissue: <https://w3id.org/def/Tissue#>SELECT ?s ?nameWHERE { ?s rdfs:subClassOf tissue:Tejido;rdfs:label ?name .}CQ-1: What are the types of connective proper tissue?Figure 18 shows the obtained results.PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX owl: <http://www.w3.org/2002/07/owl#>PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX tissue: <https://w3id.org/def/Tissue#>SELECT ?s ?nameWHERE { ?s rdfs:subClassOf tissue:TejidoConectivoAdultoPropiamenteDicho;rdfs:label ?name}a bFig. 15 Duplication and disjunction: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage ofexperts per question and Question represents the associated number to a questionMazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 12 of 15a bFig. 16 Consistency and coherence: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage ofexperts per question and Question represents the associated number to a questionCQ-2: What are the layers present in the heart?Figure 19 shows the obtained results.PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX owl: <http://www.w3.org/2002/07/owl#>PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX organ: <https://w3id.org/def/Organ#>SELECT ?s ?nameWHERE { ?s rdfs:subClassOforgan:TunicaCoraz\{o}n;rdfs:label ?name .}CQ-3: Which are the elastic arteries? Figure 20 shows theobtained results.PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX owl: <http://www.w3.org/2002/07/owl#>PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX organ: <https://w3id.org/def/Organ#>SELECT ?s ?nameWHERE { ?s rdfs:subClassOf organ:OrganoArteriaElastica;rdfs:label ?name}DiscussionA three-fold approach to validate the histological ontologywas used  detecting pitfalls using OOPS!, expert eval-uation using CMs, and how accurately the ontology cananswer the Competency Questions (CQ).Regarding the detecting pitfalls, the results suggestthat the domain and range axioms are equal for twoobject properties and a warning refers to the conven-tion used. However, those are not pitfalls in our case anddo not affect the correctness of our ontology. It doesFig. 17 Obtained results for CQ-0Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 13 of 15Fig. 18 Obtained results for CQ-1not represent a problem, since it is about appearance orstyle of the ontology and does not compromise the properontology functioning.The results shown that the experts agreed with thefollowing aspects of our ontology: completeness, dupli-cation and disjunction, and consistency. Completenesswas tackled by the first question in each CM; somerelevant concepts were added to the ontology afterthe first evaluation. Duplication and disjunction wereevaluated based on the second question at each CMand we have also ensured that there were neitherduplication nor conflict in the concepts. Consistencyand coherence were covered in the third question ateach CM.The obtained results in the experts survey were crucialfor us due to the feedback provided based on the largeexperience in histology. This means that the feedback wasvaluable for our research and the fact that we obtainedpositive results makes it possible to put the ontologyinto use.The criteria for an ontology evaluation (consistency,completeness, conciseness, expandability and sensitive-ness) are used to addresses the possible types of errorsmade and the future use. Exist reliable indications ofthe quality of terms and definitions in ontologies andtaxonomies [31]. However, the results obtained can-not be compared to other approaches in the state-of-the-art because these other works addressed differentdisciplines. Additionally, a key factor in the ontologyevaluation is to evaluate and compare the ideas within thearea [32].ConclusionsIn this paper, we presented a histological ontology ofthe human cardiovascular system. The ontology enablesto represent histological knowledge with the purposeof processing, inferring and obtaining new, and morecomplete, knowledge. The histological ontology wasbuilt from histological analysis perspective, potentiat-ing its use in teaching, medical practices and biomed-ical research. We believe that our ontology meets thecurrent need for teaching and learning the conceptsof the cardiovascular system, using tissues withoutpathologies.In the future, we will extend the ontology to othersystems using the same methodology adopted for thisontology. Extending the ontology is possible taking intoaccount that the ontology was implemented in a mod-ular way  tissues, organs and systems. Moreover, wewill use the ontology in four specific applications: (i)SOFTWARE Open AccessPIBAS FedSPARQL: a web-based platformfor integration and exploration ofbioinformatics datasetsMarija Djokic-Petrovic1,2* , Vladimir Cvjetkovic2, Jeremy Yang3,4, Marko Zivanovic5 and David J. Wild3AbstractBackground: There are a huge variety of data sources relevant to chemical, biological and pharmacologicalresearch, but these data sources are highly siloed and cannot be queried together in a straightforward way.Semantic technologies offer the ability to create links and mappings across datasets and manage them as asingle, linked network so that searching can be carried out across datasets, independently of the source. Wehave developed an application called PIBAS FedSPARQL that uses semantic technologies to allow researchersto carry out such searching across a vast array of data sources.Results: PIBAS FedSPARQL is a web-based query builder and result set visualizer of bioinformatics data. As anadvanced feature, our system can detect similar data items identified by different Uniform Resource Identifiers(URIs), using a text-mining algorithm based on the processing of named entities to be used in Vector Space Modeland Cosine Similarity Measures. According to our knowledge, PIBAS FedSPARQL was unique among the systemsthat we found in that it allows detecting of similar data items. As a query builder, our system allows researchersto intuitively construct and run Federated SPARQL queries across multiple data sources, including global initiatives,such as Bio2RDF, Chem2Bio2RDF, EMBL-EBI, and one local initiative called CPCTAS, as well as additional user-specifieddata source. From the input topic, subtopic, template and keyword, a corresponding initial Federated SPARQL queryis created and executed. Based on the data obtained, end users have the ability to choose the most appropriate datasources in their area of interest and exploit their Resource Description Framework (RDF) structure, which allows usersto select certain properties of data to enhance query results.Conclusions: The developed system is flexible and allows intuitive creation and execution of queries for an extensiverange of bioinformatics topics. Also, the novel similar data items detection algorithm can be particularly useful forsuggesting new data sources and cost optimization for new experiments. PIBAS FedSPARQL can be expanded withnew topics, subtopics and templates on demand, rendering information retrieval more robust.Keywords: Federated SPARQL query, Bioinformatics, Data integration, Ontologies, Data mining and information retrievalBackgroundMotivationNowadays, large amounts of bioinformatics data are pub-licly available to researchers of the life science community.These data and associated annotations are accessiblethrough heterogeneous databases hosted as part of manyindependent and highly specialized resources and repre-sented in different formats, conventions, vocabularies andontologies. Still, modern research in bioinformatics greatlydepends on the availability and efficient use of these data.Scientific research often requires access to various datapoints across scattered and highly distributed sources.This makes finding relevant data for scientific researchprojects a difficult and laborious task. With the rapidaccumulation of bioinformatics data, this issue has onlybecome more important and challenging.* Correspondence: m.djokic@kg.ac.rs1Virtual World Services GmbH, Asperner Heldenplatz 6, 1220 Wien, Austria2Department of Mathematics and Informatics, Faculty of Science, Universityof Kragujevac, Radoja Domanovica 12, Kragujevac 34000, SerbiaFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 DOI 10.1186/s13326-017-0151-zThe lack of integrated solutions that would contrib-ute to better results and discovering of new knowledgeis a primary issue in the bioinformatics community [1].Hence, the bioinformatics community has increasinglytaken to employing Semantic Web technologies forbetter and easier data integration. The benefits of thisapproach include aggregation of heterogeneous datausing explicit semantics, and simplified annotation andexpression of rich and well-defined models for dataaggregation and searching [2]. Therefore, the grandvision and practical technologies of the Semantic Weboffer a possibility of solving longstanding problems ofdata integration in bioinformatics [3].Motivated and influenced by the ongoing needs ofsupporting the research activities of the PIBAS (CPCTAS-LCMB) Research Center (RC) [4], the authors have suc-cessfully employed Semantic Web technologies, enablingintegration of external and internal bioinformatics data-sets. RC is a laboratory for testing bioactive substanceswhich are candidates for use in pharmaceutical therapeu-tics. Work at RC includes monitoring of in vitro effects ofactive substances in cell lines of different origin (primarilycancer cell lines) and primary cells isolated from othertypes of tissue. Experiments carried out in RC includemeasuring the effectiveness of a substance in inhibiting aspecific biological function (IC50) in human cancer celllines and quantifying the mechanisms of apoptosis, migra-tion and angiogenesis. The experimental data obtained atRC are varied and complex and represent intertwined re-lationships among various terms and concepts used at RC.This complex data structure is represented as an ontology[5]. The ontology simplifies the search for experimentaldata and comprises a formal, rigorous representation ofthe conceptual model of the domain.The main subjects that RC staff are interested in areinformation about targets, bioassays and cell lines usedin earlier experiments. In addition to the PIBAS ontol-ogy [5], which provides internal support to RC staff,supplementary information can be extracted from glo-bal initiatives such as Bio2RDF [6], Chem2Bio2RDF [7]and the EMBL-EBI platform [8]. For example, informationabout targets can be found in ChEMBL [9], BindingDB[10] and Drugbank [11] datasets, form the EMBL-EBI,Chem2Bio2RDF and Bio2RDF initiatives, respectively. Thenecessary information for bioassays can be found inChEMBL and Pubchem [12] datasets form the EMBL-EBIand Chem2Bio2RDF initiatives, respectively. Informationabout cell lines can be found in ChEMBL and ChemBank[13] datasets from the EMBL-EBI and Chem2Bio2RDFinitiatives, respectively. Another search requirement is in-vestigation of actual research results in publications. Forexample, information about publications can be found viaPubMed [14], from the Bio2RDF initiative, as well as inthe local Reference ontology [15] developed for internaluse at RC. In previous work [16], the authors focused onintegration of these initiatives. Based on manually entereddata, such as InChi, InChiKey, SMILES or molecular for-mula, the system offers templates and generates staticFederated SPARQL queries [17] for retrieval of relevantinformation. This system has been very helpful in discov-ering new knowledge, but in the light of ever-increasingvolume of experimental data, the needs of RC mandatedthe development of a new system. One of the main re-quirements in this regard was the inclusion of relevantand new datasets in predefined queries to make it possibleto find complementary information about data items (tar-gets, bioassays and cell lines). An additional requirementwas the capability to detect similar data items to increasethe performance of experiments and lower processingcosts. This is one of the major challenges in the bioinfor-matics community, as the data items are represented bydistinct URIs at different endpoints [18], which necessi-tated a serious effort to discover and compare their com-mon properties.In order to meet the above-mentioned requirements ofRC, the authors developed PIBAS FedSPARQL,1 a plat-form based on Semantic Web technologies that allowsend users to easily provide input data and run predefinedFederated SPARQL queries across multiple data sourcesand detect similar data items, among data obtained from aquery. For the process of detecting similar data items, theauthors developed a text-mining algorithm based on theprocessing of object values (strings) of the named entitiesto be used in Vector Space Model (VSM) [19] and CosineSimilarity Measures (CSM) [20]. Also, one of the featuresof PIBAS FedSPARQL is the capability of filtering resultsobtained by a query. Filtering is based on a projection ofRDF data sources included in the query. Searching andsorting of results is also offered. Users can add additionaldata source if they are interested in querying endpointthat is not contained in the predefined query. The systemcan also be extended with new topics, subtopics and tem-plates on demand.FeaturesAdhering to the philosophy of Arsic et al. [16], the au-thors implemented the following SPARQL features: Federation: Federated SPARQL queries over remoteendpoints, gather novel and complementary dataabout targets, bioassays and cell lines in real time.This eliminates constant update monitoring. Scalability: Data integration with user-specified datasources is possible. Furthermore, end users have theability to choose the most appropriate data sourcesin their area of interest and exploit their RDFstructure. This allows them to select certainproperties of data sources to improve query results.Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 2 of 20 Advancement: Detecting similar data items using amethod based on text-mining. This feature is helpfulfor optimizing the costs of new experiments. Availability: Locally used RC data are now publicand available to the entire bioinformaticscommunity.The rest of the paper is organized as follows: The nextsubsection represents a survey on related works. In theImplementation section, we present the architecture ofPIBAS FedSPARQL. In the Methods section, we describeall features of PIBAS FedSPARQL and highlight our al-gorithm for similar data items detection, explaining it indetail and presenting a use case. In the Results sectionwe present the results obtained through an evaluation.In the Conclusions and future work section, apart frompresenting the final remarks, we also outline a possibleapproach for future work. The section Appendices con-tains various definitions used in our study.Related workIn modern biology and chemistry, exploiting the diversekinds of available data about a topic of interest is challen-ging, as data are spread over many sources. Bioinformaticsdatasets are highly distributed and heterogeneous, and thisheterogeneity exists at many levels including data formats,conventions and meaning. Due to these factors, trad-itional approaches for data searching often deliver un-satisfactory results. The need for an integrated solutionhas led many organizations to use the Semantic Web,because of its wide range of possibilities. The SemanticWeb is recognized as a common framework that allowsdata to be used and shared across applications anddatabase boundaries [21].Initiatives such as Bio2RDF [6] and LODD [22] addressthe problem of connecting biological and drug data.Bio2RDF has transfigured and interrelated many biologicaldatabases, offering a platform for constructing queriesacross these data sources. The LODD initiative integratesvarious sources of drug data, motivated by domain-awarescientific questions. Chem2Bio2RDF [7] aggregates datafrom various data sources that are contained in Bio2RDFand LODD. It covers around 25 distinct datasets with con-nected compounds, drugs, pathways, side effects, genes,diseases and PubMed documents. Chem2Bio2RDF alsoincludes a tool to facilitate queries and a set of compre-hensive functions to address specific research requests.EMBL-EBI [8] contains a wide range of freely accessiblemolecular data sources, such as UniProt [23], ChEMBLand Reactome [24]. Open PHACTS [25] is a unique initia-tive developed as a shared platform for integration andknowledge discovery. It constitutes an approach based onthe Semantic Web to address bottlenecks in drug discov-ery. The project mainly focuses on distinct informationsources, lack of standards and information overcharge asmajor issues. Its goals are establishing open standards andcreating infrastructure for research cooperation. Projectssuch as LinkHub [26], SWIT [27] and BioGateway [28]also offer their solutions for the integration of bioinfor-matics data.All the solutions mentioned above have many datasetsin common and together they combine vast amounts ofbioinformatics data. Besides profound background know-ledge about the underlying data sources, users also needto have solid command of the SPARQL query language tosuccessfully access the data. SPARQL is an RDF query lan-guage used to retrieve and control data stored in RDFgraphs [29]. SPARQL also allows executing queries thatare distributed over multiple endpoints, so-called Feder-ated SPARQL queries [30]. Generally, SPARQL has acomplex syntax that is difficult to work with for inexperi-enced users and, consequently, querying data is a problemfor many researchers. Therefore, a number of existing ap-plications strive to provide a user-friendly interface forbrowsing bioinformatics data or to allow users to performFederated SPARQL queries. Several of these solutions aredescribed below.SPARQLGraph [31] is a web-based platform for the vis-ual creation and execution of biological SPARQL queries.The graphical query builder allows end users to create andshare query graphs in a simple way. Several template quer-ies are provided, offering a great starting point for buildingnew graphs and assisting researchers in finding answers tobiological questions. In the SPARQLGraph the datasetsare integrated in the interface internally and no otherdatasets are supported. In PIBAS FedSPARQL some data-sets are integrated and end users can also add an outsidedataset if they want to query endpoints that are not in thelist of integrated datasets. Both interfaces provide templatequeries in multiple datasets and enable end users tochoose from these datasets to facilitate direct querying.QueryMed [18] allows queries relevant to a wide rangeof biomedical topics. It runs federated queries acrossmultiple SPARQL endpoints. QueryMed is designed tobe accessible to users who are not familiar with theunderlying ontologies or the SPARQL query language.The system allows users to select the data sources theywish to use. Users can also add additional data sources.After retrieval of the initial result set, query results canbe filtered to improve their relevance. As an advancedsearch feature, the system also allows users to exploitthe underlying structure of the RDF data to improvequery results. This solution is the most similar to ourapproach, but the main difference lies in the fact thatPIBAS FedSPARQL offers a feature for finding similardata items in the retrieved result set.Twinkle [32] provides a stand-alone graphical userinterface to load and edit SPARQL queries. In this case,Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 3 of 20users are expected to know what is already available atthe SPARQL endpoints and to write the queries that canbe used to directly query remote SPARQL endpoints.This approach is the opposite of ours: initial PIBAS Fed-SPARQL queries are predefined, while conversance ofSPARQL is necessary for adding new datasets. AlthoughTwinkle was mostly designed as a general purposesystem, it only supports a small number of specificSPARQL endpoints, while PIBAS FedSPARQL allowsusers to add any new SPARQL endpoint.GoWeb [33] was created for answering queries on bio-medical data. It lets users run old-style keyword-basedweb searches with ontology search features. After a key-word search, documents can be filtered based on thebiomedical annotations they contain. Nevertheless, inGoWeb the exact queried sources are not transparentand cannot be selected or customized by end users as inPIBAS FedSPARQL.The SMART [34] query tool is a web-based applicationthat allows biology researchers to run SPARQL queriesover multiple data sources. Their queries are constructedusing a description logic written in the Manchester OWLsyntax [35]. In contrast, PIBAS FedSPARQL allows endusers to intuitively run predefined queries by selectingtopics, subtopics, templates and entering keywords with-out requiring background knowledge about the SPARQLsyntax.BioQueries [36] lets users to design and share SPARQLqueries that can simplify and reduce many common andfrequent bioinformatics data retrieval tasks. The BioQu-eries interface provides context-specific anchoring forqueries via the use of placeholders. Queries are repre-sented as a sentence with one or more gaps where a usercan enter context-specific information. In the PIBAS Fed-SPARQL system, Federated SPARQL queries are displayedas a corresponding virtual sentence based on the items se-lected and keyword entered.FedX [37] runs queries over either Sesame repositor-ies2 or SPARQL endpoints. During the initial phase, itloads the list of data sources without its statisticalinformation. The source selection is done by sendingSPARQL ASK queries. The size of intermediate result isminimized by a rule-based join optimizer according to acost estimation. By contrast, PIBAS FedSPARQL pre-serves intermediate results because it is very importantfor RC staff to gain all relevant data.To overcome the problem of querying multiple datasources, which can vary in their RDF representations, pro-ficiency in SPARQL is essential, but usually not sufficient,for successful information retrieval from such datasources. Identifying relevant data sources and discoveringtheir capabilities and the type of data they contain is aprocess known as source discovery [38] and a necessarypre-step for determining whether a particular data sourcematches researchers demands. There are often many al-ternative ways of carrying out source discovery [38], all ofvarying efficiency, and SPARQL experts have to choosefrom these options. Our approach for solving thesechallenges is based on close co-operation with RC experts.In order to fulfill the requirements of RC, we carried out asource discovery process and arrived at Bio2RDF, Chem2-Bio2RDF and EMBL-EBI as viable data sources (initia-tives). Then, a series of small SPARQL queries werecreated from pattern queries that were partly handpickedfrom initiative examples and handcrafted. Furthermore,we interoperated between data sources, tracking and link-ing related instances, which we received as results fromexecuting the series of the SPARQL queries. Assessing theresults, we picked up suitable handcrafted pattern queriesand created the final SPARQL queries for each require-ment. Thus, PIBAS FedSPARQL federates data by execut-ing already predefined Federated SPARQL queries andthis is different from a federated query engine BioFed [39]that is able to federate more than 130 public SPARQLendpoints. In BioFed queries are built based on existingdata and then distributed to the relevant endpointsthrough a source selection approach.Although integrated approaches in the bioinformaticsdomain are available, there are still a number of challengesthat must be addressed in order to make such resourcesaccessible to researchers. Data warehousing within bio-informatics information infrastructures in order to enablesemantic interoperability between its various stakeholders,is one of the main challenges [40]. A simple form of a datawarehouse that is focused on a single subject is called adata mart [41]. Depending on the requirements andcomplexity of the system, there are several types of imple-mentation of data warehousing. For example, OpenPHACTS [25] uses a bottom-up approach, where the datamarts are created first and then combined into a single,all-encompassing data warehouse. Generally, in datamanagement, semantic warehousing is a methodology ofdigitalizing text data using similar functions as datawarehousing such as ETL (extract, transform, load) [40].In PIBAS FedSPARQL authors do not use semanticwarehousing, although the VSM approach employed canbe seen as a data mart solution in the sense that extractedsemantic information (text) is transformed and preparedfor usage in CSM.One of the most intriguing problems in the bioinformat-ics community is finding similar data items across thesame or different initiatives [18]. PIBAS FedSPARQLoffers a flexible and interesting way to overcome thischallenge using a method based on text-mining. We applyVSM on terms, which are actually words or phrases frombiological or chemical areas, and then compare the vec-tors using CSM. This algorithm is described in detail inthe Methods section.Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 4 of 20The study of semantic similarity between words haslong been an integral part of information retrieval,natural language processing and the Semantic Web.Semantic similarity between entities changes over timeand across domains. The rest of this paragraph outlinessome traditional approaches to identifying semanticsimilarity. Given a taxonomy of concepts, a straightfor-ward method to calculate similarity between two words(concepts) is to find the length of the shortest pathconnecting the two words in the taxonomy [42]. If aword is polysemous, then multiple paths might existbetween the two words. In such cases, only the shortestpath between any two senses of the words is consideredfor calculating similarity. A problem that is frequentlyacknowledged in relation to this approach is that it re-lies on the notion that all links in the taxonomy repre-sent a uniform distance. Resnik [43] proposed asimilarity measure using information content. This ap-proach defines the similarity between two concepts C1and C2 in the taxonomy as the maximum of the infor-mation content of all concepts C that subsume both C1and C2. The similarity between two words, then, isdefined as the maximum of the similarity between anyconcepts that the words belong to. Resnik used Word-Net [44] as taxonomy and calculated information con-tent using the Brown corpus [45]. Matsuo et al. [46]used a similar approach to measure the similarity be-tween words and apply their method in a graph-basedword-clustering algorithm.Semantic similarity measures have been used in manySemantic Web applications. Ehrig et al. [47] describes aframework that aims at comparing concepts across ontol-ogies, and not ontologies themselves. This is similar toour solution, where we only compare object values(concepts). David et al. [48] present a number of measuresfor ontology matching and state that simple measures likeCosine Similarity on a term-frequency vector give accur-ate results. This is also the measure method we use in oursystem.In our previous work, we demonstrated the power ofontology-based information system [5]. A new ontologywas developed for RC that contains encoded knowledgeabout local experimental structure and an ontologicaldatabase was created that contains data from individualexperiments. Additionally, to make it possible to findrelevant information essential for the further perform-ance of local experiments, a local approach for runningstatic Federated SPARQL queries over CPCTAS [5],Bio2RDF, Chem2Bio2RDF and EMBL-EBI was created[16]. Currently, RC wanted to expand the search anddiscover complementary data by adding new datasetand finding similar data items to potentially narrowdown the choice of materials and methods for futureexperiments. In this paper, the PIBAS FedSPARQL sys-tem is described, which implements these ontological,database and strategic approaches.ImplementationArchitecture overviewThe PIBAS FedSPARQL architecture is shown in Fig. 1.The main components are user interface and query engine.The user interface enables users to construct simple andFig. 1 PIBAS FedSPARQL architecture overview. The architecture consists of two main layers: query engine and user interface. The user interfaceenables users to construct simple and advanced queries and view the results of their execution. The query engine preforms a series ofdemanding processes that needs to be done before queries can be executed. The main query engine component, Data Source Manager, scansthe local DataSources ontology, reads the users input and passes the information through the Query preparation component to the SPARQLquery runner component, where the queries are executed. The Dataset projection component plays a role in the Dynamic query filter feature,allowing users to easily discover the structure of underlying datasets included in Federated SPARQL queries. The Detecting Similar Data Itemscomponent identifies similar data items from results retrieved after running predefined queries or queries extended with new datasetsDjokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 5 of 20advanced queries and view the results of their execu-tion, while the query engine executes queries across re-mote SPARQL endpoints. PIBAS FedSPARQL wasimplemented in PHP and Python. The JQuery library3was used to develop an interactive and user-friendlyinterface, while sparqllib4 was used to run FederatedSPARQL queries. The list of available datasets used forcreating predefined Federated SPARQL queries isplaced in the local DataSources ontology [49] devel-oped using Protégé 4.0.2 [50].User interfaceThe user query interface was implemented in HTML,JQuery and JavaScript. Its core components are:Initial query interface Users can choose from prede-fined topics, subtopics and templates. The selection ofsubtopics is limited by of the topic selected. This alsoapplies to the relation between topics and templates. Allrelations reflect the needs of the researchers at RC. EveryFig. 2 Representations of basic relations in the DataSources ontology in the Protégé editor a) Topic Biology b) Subtopic BiologyTarget c)Template Found targets for the drug and some of its properties d) PIBAS/CPCTAS dataset instance. This figure shows screenshots of thelocal ontology DataSources in the Protégé ontology editor [50]. The ontology contains information about initiatives and datasets includedin predefined Federated SPARQL queries. Each dataset in the ontology is represented as an instance of a certain class. The object propertyconectedWith connects dataset instances with template instances. Every Subtopic class instance is connected with a Template class instancethrough the object property hasTemplate. Every Topic class instance is connected with a Subtopic class instance through the objectproperty hasSubTopicDjokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 6 of 20template is based on a form of an underlying predefinedFederated SPARQL query.Predefined query extension This component allowsend users to add new datasets to the predefined Feder-ated SPARQL queries.Dynamic query filter This component allows end usersto select the desired datasets, load the properties avail-able for these datasets and dynamically expand Feder-ated SPARQL queries with selected properties.Result presentation This component allows end usersto view the results of predefined queries in table form.One column shows retrieved results as URI or string,while another column displays data source and initiativename. End users can also apply a dynamic query filter toview the results organized by source. In both cases, thecolumns can be sorted and searched based on enteredtext.Query enginePIBAS FedSPARQL runs Federated SPARQL queries onour local JOSEKI endpoint.5 Before the queries can beexecuted, a series of demanding processes need to beperformed. These tasks are carried out by the followingcomponents:Data source ontology This component implies theDataSources ontology that contains the patterns of pre-defined queries for all templates as well as informationabout datasets that are initially included in queries.Data source manager This component scans the datasource ontology and uses the corresponding datasets in-formation to fulfill the user requirements. The datasource manager also keeps track of predefined datasetsand the datasets included in extended queries.Table 1 Representation of current (sub)topics and templates inthe DataSources ontologyTopic Subtopic Template/Template label* KeywordBiology Targets Find targets for the drug/1 InChiKeyChemogenomic Assays  Find assays for the drug/2 SMILECell lines  Find cell lines for the drug/3 InChiKeyResearch Papers Find papers with a title for thekeyword/4No restrictionaTemplate labels are used in Table 2 and Table 6Fig. 3 Predefined query of Template2 for its pre-selected datasets. This figure shows the predefined Federated SPARQL query of the templateFind targets for the drug. This query covers the PIBAS/CPTAS, Drugbank/Bio2RDF, ChEMBL/EMBL-EBI and BindingDB/Chem2Bio2RDF datasets.All predefined Federated SPARQL queries in the local DataSources ontology contain %s characters which represent objects values that will bereplaced with the keyword entered by the user. The last %s character will be replaced with a particular pattern query if a new dataset is addedusing the Add new dataset featureDjokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 7 of 20Dataset projection This component returns propertiesfor every dataset included in Federated SPARQL queries.End users can choose from a number of properties basedon their description.Query preparation This component is in charge oftranslating and preparing the requirements of end usersinto valid Federated SPARQL queries. Requirements in-clude selecting options from the initial query interface,adding new endpoints to predefined queries and dynamicquery filtering.SPARQL query runner This component executes Fed-erated SPARQL queries.Detecting similar data items This component detectssimilar data items (URIs) from results retrieved afterrunning predefined queries or queries extended withnew datasets. Similar data items are shown on a newweb page.Table 2 List of RDF datasets integrated in PIBAS FedSPARQLPIBAS FedSPARQLData source Triples TemplatelabelReference or dataset linkCPCTASPIBAS dataset 437 1; 2; 3 [5]Referencedataset42.089 4 [15]EMBL-EBIChEMBL 425.304.329 1; 2; 3 https://www.ebi.ac.uk/chembl/Chem2Bio2RDFBindingDB 20.484 1 https://www.bindingdb.org/bind/index.jspPubchem 78.000.000 2 https://www.ncbi.nlm.nih.gov/pcassayBio2RDFDrugbank 3.672.531 1 http://www.drugbank.ca/PubMed 5.005.343.905 4 http://www.ncbi.nlm.nih.gov/pubmed/Fig. 4 Running of predefined query in PIBAS FedSPARQL a) Initial user interface b) Results after running predefined query. The initial userinterface allows users to create queries in a very simple way by selecting a (sub)topic, template and entering a keyword. By clicking on the Runquery button, the predefined Federated SPARQL query is executed and users receive results in the form of a table. The first column shows theretrieved results as URI or string. The second column displays the data source and initiative name. The icon in the top-right corner of the tableshows statistical information about the retrieved data, including data source name, initiative name and the number of obtained data items perdata sourceDjokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 8 of 20MethodsRunning of predefined queriesInformation about initiatives and datasets included inpredefined queries is placed in the local ontology Data-Sources. Each dataset is represented as an instance,while each template is connected to a dataset instanceusing the object property connectedWith. With respectto their purpose, the same dataset can be associatedwith a variety of templates. Every template belongs to acorresponding subtopic. Each subtopic has its owntopic. For example, the topic Biology has the subtopicBiologyTarget while it is connected to Template2 (seeFig. 2). Template2 is created based on the followingpreselected datasets: PIBAS/CPCTAS, BindingDB/Chem2Bio2RDF, Drugbank/Bio2RDF and ChEMBL/EMBL-EBI.Currently, the DataSources ontology contains topicsthat are created in accordance with the requirementsof RC experts. Topics are divided into three areas:Biology, Chemogenomic and Research. All (sub)topicsand templates are changeable and can easily be modi-fied or added to. Templates can be modified in variousways. For example, the template Find targets for thedrug, which requires the InChiKey value, can be trans-formed into a template that requires another value,such as SMILES. This change necessitates a manualmodification in the predefined query. Templates canbe expanded with one or more new datasets. Similarly,datasets can also be excluded from templates. A repre-sentation of all topics and their relations in PIBASFedSPARQL is shown in Table 1.The property hasInitialQuery of each template repre-sents a predefined Federated SPARQL query that runsacross preselected datasets. Pattern queries for everydataset are collected from initiative examples and partsof them are handcrafted. Figure 3 shows the predefinedquery of Template2. All %s characters that representobjects in the predefined query will be replaced with thekeyword entered by the end user, while the aftermostcharacter is reserved for an additional dataset.At the moment, PIBAS FedSPARQL uses datasets(Table 2) from the EMBL-EBI, Bio2RDF and Chem2-Bio2RDF platforms. These are, used to establish thepredefined Federated SPARQL queries. CPCTAS, as unionof the PIBAS and Reference dataset, covers all the men-tioned topics currently used for templates. Seeking tomeet the needs of RC staff and highlight the importanceof small laboratories, we have related the PIBAS datasetwith templates from the Biology and Chemogenomictopics. The Reference dataset, as collection of ontologies,RESEARCH Open AccessRevealing protein functions based onrelationships of interacting proteins andGO termsZhixia Teng1,2*, Maozu Guo2*, Xiaoyan Liu2, Zhen Tian2 and Kai Che2From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: In recent years, numerous computational methods predicted protein function based on the protein-proteininteraction (PPI) network. These methods supposed that two proteins share the same function if they interact with eachother. However, it is reported by recent studies that the functions of two interacting proteins may be just related. It willmislead the prediction of protein function. Therefore, there is a need for investigating the functional relationship betweeninteracting proteins.Results: In this paper, the functional relationship between interacting proteins is studied and a novel method, called asGoDIN, is advanced to annotate functions of interacting proteins in Gene Ontology (GO) context. It is assumed that thefunctional difference between interacting proteins can be expressed by semantic difference between GO term and itsrelatives. Thus, the method uses GO term and its relatives to annotate the interacting proteins separately according totheir functional roles in the PPI network. The method is validated by a series of experiments and compared with theconcerned method. The experimental results confirm the assumption and suggest that GoDIN is effective onpredicting functions of protein.Conclusions: This study demonstrates that: (1) interacting proteins are not equal in the PPI network, and their functionmay be same or similar, or just related; (2) functional difference between interacting proteins can be measured by theirdegrees in the PPI network; (3) functional relationship between interacting proteins can be expressed by relationshipbetween GO term and its relatives.Keywords: Protein function, Interacting protein, Gene ontology, Directed networkBackgroundCharacterizing protein functions is critical to understand-ing biological pathway, investigating disease and develop-ing drugs [1, 2]. To elucidate protein functions, numerousresearch efforts have been made based on techniques ran-ging from sequence homology detection to text mining ofscientific literature. However, only some of proteins areannotated with functional information for well-studiedmodel organisms so far. The situations would be evenworse for the other organisms.Recently, biological network provides chance of study-ing gene and its products (e.g protein, microRNA) atsystem level [3, 4]. It is widely recognized that a proteinperforms functions according to its partners in protein-protein interaction (PPI) network. This recognition hasmotivated the development of numerous network-basedmethods for predicting protein function. These methodsare proposed on the principle of guilt-by-association(GBA), that is, the closer the two proteins are in the net-work the more similar are their functions [5]. Thesenetwork-based methods can be roughly grouped intotwo major classes: direct annotation methods [611]* Correspondence: tengzhixia@nefu.edu.cn; maozuguo@hit.edu.cn1Department of Information Management and Information System,Northeast Forestry University, Harbin 150040, China2Department of Computer Science and Engineering, Harbin Institute ofTechnology, Harbin 150001, China© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27DOI 10.1186/s13326-017-0139-8and model-assisted methods [1215]. The comprehen-sive reviews of these methods can be found in [5, 16].The direct annotation methods suppose that the inter-acting proteins share the same function and inferredprotein functions by means of propagating the knownfunctional annotations of its neighbors along the net-work edges. The model-assisted methods assume thatproteins in the same group perform the same function.They firstly identify functional groups of proteins, andthen annotate each group with the known functional an-notations of the groups members. In recent years, chi etal. [17] proposed a method named CIA, which iterativelyupdated annotations of a protein according to functionalsimilarity between the protein and its partners. Wang[18] put forward a method named FCML to predict pro-tein function by multi-label learning. The FCML tookfunctional association between Gene Ontology (GO)terms [19] under consideration when it worked. Almostall of these methods predicted protein functions usingPPI network and GO terms. In these methods, predict-ing protein function is to associate term with protein ac-cording to functional semantic information of the term.The result of predicting is named as annotation of pro-teins and an annotation is represented by a term. Thesemethods have promoted the development of the proteinfunctional predicting. However, most of them ignoredsome crucial information which affect the quality ofprediction:(1)The PPI network is usually supposed as non-directional. In fact, it is commonplace in the PPInetwork that regulation relationship, upstream-downstream relations between interacting proteinswhen they are involved in signal transduction, tran-scriptional regulation, cell cycle or metabolism [20].Moreover, it is reported by recent studies [2123]that GBA is the exception rather than the rule inthe PPI network and protein functions are deter-mined by specific and critical interactions. Hencethe relationship between interacting proteins mayaffect their functions and should be considered inthe process of predicting protein functions.(2)In GO context, a series of standard terms aredefined to describe characteristics of gene products(i.e. protein), and the terms are arranged as directedacyclic graph (DAG) hierarchy according tofunctional associations of them. Therefore, thefunctional information is not only expressed bysemantics of terms but also contained in thehierarchy. Thus, the predictions of protein functionsmay be misled if the functional associations of termsare ignored. In fact, the information underlying inGO hierarchy are crucial for functional predicting ofproteins.In this paper, we mainly study two problems: (1) howto measure the functional difference between interactingproteins; (2) how to demonstrate functional differencebetween the interacting proteins in GO context. Tosolve above problems, we advance a novel method topredict protein functions by diffusing GO terms in thedirected PPI network (GoDIN). Firstly, the relationshipbetween interacting proteins is generalized as functionalproactive-reactive. It is assumed that the proactiveprotein performs fewer and more specific functions thanthe reactive protein. And then a directed PPI network isgenerated according to the functional proactive-reactiverelationships of interacting proteins. Secondly, acoefficient variation is defined to measure functionaldifference between interacting proteins. Finally, func-tional associations of GO terms are taken into consid-eration in the process of annotating interactingproteins. By a proposed iterative algorithm, GO termsare allocated to describe protein functions in the PPInetwork under the control of coefficient variations.The method will be illustrated in the followingsection.MethodsFunctional relationship between interacting proteinsAs reported, many proteins play functional roles that aredifferent from their neighbors in the PPI network. Forexample, a protein annotated with terms: RNA trans-port, RNA binding may involve in translation mech-anism and bind with diverse functional unrelatedproteins [23]. For instance, the function of proteinswhich help others fold correctly may be unrelated tothat of their partners. These proteins are more likely tobe hubs than others in the PPI network. The hubs oftenhave many partners and may involve in several differentbiological activities. In general, a protein is multi-functional if it takes part in many different biological ac-tivities. As reported [22], the more multi-functionality ofa protein is, the less specific is its function. Besides, Gil-lis et al. also found that the multi-functionality of a pro-tein is highly correlated with its degree in the PPInetwork. Specifically, a protein with high degree mayperform general function so that they could collaboratewith other proteins in diverse biological activities. It canbe considered that the low degree proteins are proactiveand the high degree proteins are reactive in biologicalactivities. Thus, the relationship between interactingproteins can be generalized as functional proactive-reactive according to their degrees in PPI network.Let the PPI network be generalized as a digraph, inwhich a node presents a protein and an arch linkstwo interacting proteins, oriented from the low degreeone to the high degree one. Note that, two interactingproteins with equal degree are linked with aThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 10 of 79bidirectional arch. Accordingly, as displayed in Fig. 1,a novel directed PPI network is generated from theoriginal undirected PPI network.As discussed above, the functional specificity maydescent on the direction from proactive protein to re-active protein. Thus, the descent direction of func-tional specificity between two interacting proteins isdefined as (1). In the formula, O(vi, vj) represents thedescent direction of functional specificity between thetwo interacting proteins vi and vj; d(.) denotes the de-gree of a protein in the PPI network. The formulameans that: vi plays more specific functions than vj ifO(vi, vj) =1; vi play general functions than vj if O(vi,vj) = ?1; vi and vj are equal in the network and theyshare the same function if O(vi, vj) =0.O vi; ; vj  ¼1; d við Þ < d vj ;0; d við Þ ¼ d vj ;?1; d við Þ > d vj :8><>:ð1ÞMeasuring functional difference between interactingproteinsHere the functional difference between two interact-ing proteins is measured. It is considered that a pro-tein perform specific functions if the protein isinvolved in few activities, vice versa. In the PPI net-work, the number of connections of a protein can re-flect the number of activities the protein involves in.Thus, the functional specificity of a protein can bemeasured by degree of the protein in the PPI net-work. For two interacting proteins, their functionaldifference may be determined by the specificity differ-ence of the functions which are performed by theirinteraction. Accordingly, a coefficient variation is de-fined to measure the functional difference betweentwo interacting proteins. The functional coefficientvariation between two interacting proteins vi and vj ismarked as CV(vi, vj) and can be measured by (2).CV vi; ; vj  ¼ 1d við Þ?1d vj  ð2ÞAnnotate the interacting proteins with GO terms basedon their functional differenceIn traditional methods, the known GO terms of aprotein were directly associated with interacting partnersof the protein. These methods ignored the functionaldifference between the interacting proteins. In fact, thefunctions of interacting proteins may be same or similar,or related but different. Therefore, the relatives ofknown terms of a protein are selected to annotate inter-acting partners of the protein in our method.To select relatives, it is supposed that an ideal termcan annotate functions of neighbors exactly. Semanticvalue of the ideal term can be estimated based on thoseof the known terms of interacting proteins and func-tional coefficient variation and descent direction of func-tional specificity between them. In our method, semanticvalue of a term gm is marked as S(gm) and computed by(3). In the formula, dep(gm) is the depth of gm, anddesc(gm) is number of descendants of gm, and Gtotal isthe total number of terms in GO hierarchy. Equation (3)is proved to be effective on calculating semantic valuesof terms in [24]. The semantic value of a term is big ifthe term has few descendants or lies at deep level in GOhierarchy. The bigger the semantic value of the term is,the more specific is the function described by the term.S gm  ¼ dep gm : 1?log desc gm þ 1 log Gtotalð Þ ð3ÞFor interacting proteins vi and vj, gm is a known termof the protein vi; an ideal term gm* can be inferred fromgm to annotate protein vj; and semantic value of gm* ,Fig. 1 A simplified example of generating directed PPI networkThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 11 of 79S(gm* ) can be computed by (4). Equation (4) is applicablefor propagating terms between interacting proteins nomatter which one of them is annotated.S gm  ¼ 1þ CV vi; ; vj  ?O vi;;vjð Þ:S gm  ð4ÞBased on semantic value of the ideal term, one ormore relatives of the known term are selected to anno-tate protein vj by (5). In the formula, R(gm) representsthe set of relatives of gm and gmr is a relative of gm. Todo this, the relatives of the known term, which are themost similar with the ideal term in term of the semanticvalue, are selected to describe functions of the protein vj.In practice, the selected relatives may be grandparents,parents, siblings, children or grandchildren of the knownterms.argmingrm?R gmð ÞS grm ?S gm   ð5ÞGenerally speaking, this process provides three kindsof predictions: (1) some ancestors of the known terms ofthe proactive protein may be appropriate to describe thereactive protein; (2) some descendants of the knownterms of the reactive protein can annotate the proactiveprotein; (3) terms of two interacting proteins can beshared directly by them if the proteins are equal in thePPI network.Diffusing functional information in the PPI networkTo mine functional information as much as possible, aniterative algorithm is designed to diffuse GO terms inthe whole PPI network. As described in Fig. 2, the algo-rithm includes four steps as following.Step 1: Select seed proteins from annotated proteins ofwhich proactive partners have not been annotated yet;Step 2: Select relatives of known terms of seed pro-teins to describe functions of their interacting partnersaccording to formulas (4) and (5);Step 3: Update terms of seed proteins based on theirannotated reactive partners according to formulas (4)and (5);Step 4: Remove seed proteins from the annotated pro-teins; the edges related to the seed proteins cannot me-diate diffusing between interacting proteins; and go tostep 1 until all proteins in the PPI network are annotatedor there does not exist annotated partners for remainedunannotated proteins.Time complexity analysisGiven a PPI network including n proteins, the time com-plexity of determining functional relationship betweenproteins is O(n2). Similarly, the time complexity of meas-uring functional difference between proteins is O(n2) too.If the proteins is at most annotated by p GO terms, andthe maximum degree of the proteins is k, the time com-plexity of diffusing functional information between twoproteins is O(p × k). Accordingly, diffusing functional in-formation in the whole PPI network is O(m × p × k) ifthere are m proteins are annotated in the PPI network.Based on these analysis, the time complexity of theGoDIN should be O(n2) + O(n2) + O(m × p × k). Becausethe maximal value of m is n and the maximal value of k isn-1, the time complexity of the GoDIN is about O(n2).A simple example of GoDINTo make our method clearly, a simple example is illus-trated in Fig. 3. In the Background of the Fig. 3, someGO terms are organized as DAG, in which terms arelinked with arches oriented from child to parent. Aswell, semantic values of the terms are all listed in theBackground. Initially, protein M is annotated with termg7, L is annotated with term g4, and the other proteinsin the subnetwork are unannotated. The functionalproactive-reactive relationship between interacting pro-teins has been marked by an arch oriented from the pro-active protein to the reactive protein. To revealfunctions of the other proteins, annotations of M and Lare diffused between interacting proteins iteratively. InFig. 2 Algorithm for diffusing GO terms in the whole PPI networkThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 12 of 79our example, the diffusing process is finished throughsix iterations. In each round, some functional inferencesare made and key information of inferences is displayedin a table. The key information include the descent dir-ection of functional specificity between two interactingproteins (O(vi, vj)), functional coefficient variant betweenseed protein and its neighbor (CV(vi, vj)), known term(gm) and its semantic value (S(gm)), semantic value ofideal term (S(gm* )) and selected relative of the knownterm (gr*).In the first iteration, M is regarded as a seed proteinand its neighbors include E, L and J. According to theformula (1) and (2), O(M, E) is 1 and CV(M, E) is 1/6.The known GO term of M is g7 and the semantic valueof g7, S(g7) is 0.35. By replacing parameters in the for-mula (4) with these data, S(g7* ) is estimated as 0.408. Ac-cording to the formula (5), g8 is appropriate to annotateprotein E. Similarly, the annotations of L and J arepredicted by the same means. Note that, because L hasbeen annotated before diffusion, Ls term g4 should alsobe diffused to the seed proteins M. According to TruePath Rule (TPR), g4 also annotates M if g4 is an ancestorof g7. Thus, the annotations of M cannot be changed byGO term g4. In addition, the protein M cannot be se-lected as a seed protein again and arches M ? J,M ? E, M?L cannot be used to diffuse GO termsagain.In the second iteration, J, E and L are candidates forseed proteins. Because the protein L has a proactive an-notated partner E, L cannot be taken as a seed protein.Therefore, J and E are selected as seed proteins. Accord-ing to the formula (1), O(J, A) is 0, which means thatthe protein J and A share the same function. Thus, pro-tein A can be annotated with term g8, which is also canbe inferred though the formula (4) and (5). Differentfrom L, A has not been annotated at all before diffusion,Fig. 3 A simple example of how to diffusing GO terms though directed PPI networkThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 13 of 79so it does not need to infer annotations of J from thoseof A. As for the seed protein E and its partner L, O(A,L) is ?1 and CV(A, L) = 1/6 in term of the formula (1)and (2). Based on these parameters and S(g8),S(g8* ) = (1 + 1/6)-1 × 0.4 = 0.343. Therefore, term g7 isselected to annotate L in term of the formula (5). Afterthat, protein J and E cannot be regarded as seed proteinsand arches J?A and E ? L cannot be used in the otheriterations.The processes of the 3rd, 4th, 5th iterations are similarto the previous iterations. Due to Space Limitations, thedetails of these iterations are not described here. In the6th iteration, it can be found that all proteins in the sub-network have been annotated already and no arch whichcan mediate diffusing between interacting proteins re-mains. Thus, the iteration is terminated and the diffus-ing of GO terms though the subnetwork is finished. Theresult of inferences are collected and listed in the table.Experiments and discussionsExperimental datasetsThree high reliable PPI networks of saccharoinyces cere-visiae (Krogan, DIP, BioGRID) are used to study the per-formance of the proposed method GoDIN. Krogan [25]consists of interactions with probabilities above 0.273.The latest version of DIP was downloaded from databaseof interacting protein (http://dip.doe-mbi.ucla.edu/dip/)[26] on July 7, 2013. BioGRID consists of the physical in-teractions of saccharoinyces cerevisiae and it was down-loaded from biological general repository for interactiondatasets (http://thebiogrid.org/download.php) [27] onMarch 10, 2015. At the same time, the functional anno-tations of proteins of saccharoinyces cerevisiae weredownload from GO website and the annotations withevidence code IEA (Inferred from Electronic Annota-tion), NR (Not Recorded), ND (No biological Dataavailable), or IC (Inferred by Curator) were excluded.The basic information of the three PPI networks arelisted in Table 1. In the table, #PPI is the number of in-teractions in the network; #Proteins is the number ofproteins in the network; #Annotated proteins is thenumber of the proteins with GO annotations; MF, BPand CC represent the annotation aspects: molecularfunction, biological process and cellular componentrespectively.Performance measuresThree widely-used measures: precision (P), recall (R)and f-measure (F) are employed to measure performanceof GoDIN and other related methods. The measures areconsistent with the famous Critical Assessment of Func-tional Annotations (CAFA) experiments [28]. P is theaverage precision of predictions about proteins on whichat least one prediction was made. R is average recall ofpredictions on all target proteins. F is a harmonic meanbetween P and R, which gives an intuitive number forcomparisons of the concerned methods. Supposed that xrepresents a target protein and K (x) is a set of knownterms of x, P can be calculated as Eq. (6). In Eq. (6), P(x)is the set of predictive annotations; S is the target pro-tein set for testing; m is the number of proteins which atleast have one predictive term. Similarly, R and F can becomputed by Eq. (7) and Eq. (8) respectively.P ¼ 1mXx?SK xð Þ?P xð Þj jP xð Þj j ð6ÞR ¼ 1Sj jXx?SK xð Þ?P xð Þj jK xð Þj j ð7ÞF ¼ 2P:RP þ R ð8ÞFunctional relationship between interacting proteinsTo study functional relationships of interacting proteins,the interactions of Krogan, DIP and BioGRID are ana-lyzed thoroughly. Firstly, annotations of proteins in thenetworks are processed and the terms with evidencecode IPI (Inferred from Physical Interaction), IGI (In-ferred from Genetic Interaction) are excluded to avoidcircular judgement. Secondly, the interactions are com-posed of two annotated proteins are selected for analysis.Finally, the selected interactions are grouped into: (1)the same annotation group, (2) the similar annotationgroup and (3) the related annotation group. The sameannotation group consists of interactions which arecomposed of proteins with the same term. The similarannotation group consists of interactions which arecomposed of proteins with different terms of the samesub-ontology. Usually, the terms of the same sub-ontology are similar. The related annotation group con-sists of interactions which are composed of proteins onlywith terms of different sub-ontologies. The results ofanalysis are displayed in Table 2. In the table, #PPIt isthe number of interactions in the network; #PPI is thenumber of interactions in the group; Pct(%) presents thepercentage of interactions in the group.From Table 2, it can be seen that nearly 60% of inter-actions in the three networks belong to the first group;about 40% of interactions belong to the second group;Table 1 Basic information of the three PPI networksNetwork #PPI #Proteins #Annotated proteinsMF BP CCKrogan 7123 2708 2109 2424 2570DIP 22,613 5097 3415 3941 4207BioGRID 59,748 5640 4106 4754 5100The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 14 of 79only less than 1% of interactions belong to the thirdgroup. As far as we know, none of methods relying onPPI network could annotate the interacting protein cor-rectly in the third group. The traditional methods sup-posed that the interacting proteins share the same term.Thus, about 40% of functional predictions may not becorrect. Meanwhile, the results suggest that the majorityof interacting proteins share the same or similar terms,which is consistent with basic assumptions of GoDIN.Functional difference between interacting proteinsTo investigate the influence of the degree on functionaldifference between proteins, the annotations and degreesof interacting proteins are analyzed. The analysis is per-formed on Krogan, DIP, BioGRID and the interactions ofthe networks are grouped into: the same annotation groupand the similar annotation group. The former consists ofinteractions which are composed of proteins with thesame term, and the latter includes interactions of whichthe proteins are annotated by similar terms. The resultsderived from the same annotation groups and the similarannotation groups are illustrated in Table 3 and Table 4separately. In the tables, #PPI is the number of interac-tions in the group; #SameDeg is the number of interac-tions in which the interacting proteins with the samedegree in the group; #DiffDeg is the number of interac-tions in which the interacting proteins with different de-grees in the group; Pct(%) presents the percentage ofinteractions in the group. The results suggest that the ma-jority of the interacting proteins have different degrees inthe three networks. Some of the interacting proteins withdifferent degrees are annotated by similar terms while theothers share the same term.To explain this phenomenon, coefficient variation isused to measure functional difference between the inter-acting proteins. The coefficient variations of proteinswith different degrees in the same annotation group arecompared with those in the similar group. As shown inFig. 3, the box-whisker plots are used to display the dis-tributions of coefficient variations of different groups. Inthe figure, the distributions of the coefficient variationsin the same annotation groups are represented bydashed boxes and lines. Meanwhile, the distributions ofcoefficient variations in the similar annotation groupsare represented by solid boxes and lines. As known, thebottom and top of the boxes are always the first andthird quartiles of coefficient variations, and the bands in-side the boxes are the second quartiles (the median) ofcoefficient variations, and the hollow spots inside theboxes are the averages of coefficient variations. For clear,the same annotation groups of the three networks:Krogan, DIP and BioGRID are marked as SameKrogan,SameDIP, SameBIO respectively. Accordingly, the similarannotation groups of those networks are signed as Simi-larKrogan, SimilarDIP and SimilarBIO.As displayed in Fig. 4, coefficient variations in the twodifferent groups of the same network show obvious dif-ferent distributions. According to the median, average,the first and third quartiles, the coefficient variations inthe same annotation groups are higher than those in thesimilar annotation groups. This may suggest that thefunctional differences of interacting proteins in the sameannotation groups are smaller than those in the similarannotation groups. Sometimes, although the degrees ofproteins are different, the functional coefficient variationbetween the proteins is tiny small. Therefore, the func-tional difference between the proteins may be negligibleand they share the same term. This just explains whysome of the interacting proteins with different degreesshare the same term. According to the results and ana-lysis, the coefficient variation defined by GoDIN is ef-fective to measure the functional difference betweeninteracting proteins. The functional coefficient variationbetween interacting proteins can be considered as apositive clue to predict protein function.Table 2 Functional relationship of interacting proteinsNetwork #PPIt Same annotation Similar annotation #Related annotation#PPI Pct(%) #PPI Pct(%) #PPI Pct(%)Krogan 6931 4114 59.36 2794 40.31 23 0.33DIP 20,050 9816 48.96 10,182 50.78 52 0.26BioGRID 58,765 30,154 51.31 28,464 48.44 147 0.25Table 3 Relationship between function and degree ofinteracting proteins in the same annotation groupNetwork #PPI #SameDeg Pct(%) #DiffDeg Pct(%)Krogan 4114 243 5.9 3871 94.1DIP 9816 531 5.4 9285 94.6BioGRID 30,154 406 1.3 29,748 98.7Table 4 Relationship between function and degree ofinteracting proteins in the similar annotation groupNetwork #PPI #SameDeg Pct(%) #DiffDeg Pct(%)Krogan 2794 91 3.26 2703 96.74DIP 10,182 159 1.56 10,023 98.44BioGRID 28,464 188 0.6 28,276 99.4The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 15 of 79Comparison with the related methodsTo test the performances of GoDIN, we take FunFlow[7], CIA [17], FCML [18] as comparing methods. Thesecomparing methods have been discussed in the intro-duction. They are three typical methods of predictingprotein function based on PPI network and GO contextrespectively. The comparisons are performed on Krogan,DIP and BioGRID from three annotation aspects: mo-lecular function (MF), biological process (BP) and cellu-lar component (CC) respectively. Figures 4, 5 and 6show the precision, recall and F-measure of thesemethods on different networks and annotation aspects.As shown in Fig. 5, the precision of GoDIN is compar-able to the best methods: CIA and FCML on Krogan.Meanwhile, GoDIN shows better precision than the othermethods on DIP and BioGRID. FunFlow performs betterthan the others on DIP but it shows the lower precisionthan other methods on Krogan and BioGRID. In GoDIN,the functional differences of interacting proteins are con-sidered and the differences of terms are used to demon-strate the functional differences during predicting proteinfunction. This is why GoDIN shows better performancesthan the others in term of the precision. The functionalrelationships of terms are also considered thoroughly inCIA and FCML, but they pay no attention to the func-tional differences of interacting proteins. FunFlow ignoresthe functional relationships of terms in the process of pre-dicting protein function so that it performs not as well asthe others.In addition, it is also found that all of the methods showrelatively low accuracy. This may be due to two issues: (1)the large number of GO terms; (2) the dependency of GOterms. The influence of the above issues will be more ob-vious while the proteins are annotated by more terms.This would be a place to start the future study.As displayed in Fig. 6, FunFlow shows the best recall onalmost all of the networks while GoDIN performs betteron most of the networks and annotation aspects thanFCML and CIA. The performances of CIA are not betterthan those of FCML. This may be attributed to globalcharacteristics and local characteristics of PPI network.Specifically, CIA only takes local characteristics of PPI net-work into consideration in predicting protein functionswhile the other methods consider both global and localcharacteristics of PPI network. This may be the reasonswhy the recall of CIA is lower than those of the othermethods. Besides, some proteins in the datasets are anno-tated by shallow terms, and the misjudgments on theseFig. 4 Comparison of coefficient variants based on differentannotation groupsFig. 5 Comparison of precision of the related methodsFig. 6 Comparison of recall of the related methodsFig. 7 Comparison of F-measure of the related methodsThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 16 of 79proteins have obvious negative impact on the recall. Thiswould be a place to start our future study.As shown in Fig. 7, GoDIN performs almost as well asthe best method FCML on Krogan while shows the best F-measure on DIP and BioGRID. FunFlow performs not bet-ter than the others on all of the networks. Overall, GoDINshows better performances than the three methods interms of metrics: precision, recall and F-measures whenthey are applied to predict protein functions.ConclusionsPredicting protein function based on PPI network is ahotspot of biological research in recent years. In thispaper, the functional relationship between interactingproteins is studied and a novel method of protein func-tion prediction is proposed based on the relationship.To validate the effectiveness of the method, a series ofanalysis and experiments are performed on the threehigh reliable networks from the different annotation as-pects. The results suggest that: (1) interacting proteinsare not equal in the PPI network, and their functionmay be same or similar, or just related; (2) functionaldifference between interacting proteins can be measuredby their degrees in the PPI network; (3) functional rela-tionship between interacting proteins can be expressedby semantic relationship between GO term and its rela-tives; (4) compared with the other concerned methods,GoDIN has high precision and f-measure and it is effect-ive on predicting protein function.AcknowledgementsWe would like to thank the editors and the anonymous reviewers for theircomments that led to significant improvements in our manuscript.FundingPublication of this article was funded by Fundamental Research Funds forthe Central Universities (DB13AB02, DL13AB02) and Natural ScienceFoundation of China (61,671,189, 61,271,346, 61,571,163, 61,532,014 and91,335,112).Availability of data and materialsNot applicable.About this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1,2017: Selected articles from the BiologicalOntologies and Knowledge bases workshop. The full contents of thesupplement are available online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsConceived and designed the approach: ZXT, MZG. Implemented theapproach and performed the experiments: ZXT,ZT, KC. Analyzed the results:ZXT, MZG, XYL, ZT. Contributed to the writing of the manuscript: ZXT, MZG,XYL. All the authors have approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Published: 20 September 2017Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 DOI 10.1186/s13326-016-0111-zRESEARCH Open AccessConsolidating drug data on a global scaleusing Linked DataMilos Jovanovik* and Dimitar TrajanovAbstractBackground: Drug product data is available on the Web in a distributed fashion. The reasons lie within theregulatory domains, which exist on a national level. As a consequence, the drug data available on the Web areindependently curated by national institutions from each country, leaving the data in varying languages, with avarying structure, granularity level and format, on different locations on the Web. Therefore, one of the mainchallenges in the realm of drug data is the consolidation and integration of large amounts of heterogeneous data intoa comprehensive dataspace, for the purpose of developing data-driven applications. In recent years, the adoption ofthe Linked Data principles has enabled data publishers to provide structured data on the Web and contextuallyinterlink them with other public datasets, effectively de-siloing them. Defining methodological guidelines andspecialized tools for generating Linked Data in the drug domain, applicable on a global scale, is a crucial step toachieving the necessary levels of data consolidation and alignment needed for the development of a global datasetof drug product data. This dataset would then enable a myriad of new usage scenarios, which can, for instance,provide insight into the global availability of different drug categories in different parts of the world.Results: We developed a methodology and a set of tools which support the process of generating Linked Data in thedrug domain. Using them, we generated the LinkedDrugs dataset by seamlessly transforming, consolidating andpublishing high-quality, 5-star Linked Drug Data from twenty-three countries, containing over 248,000 drug products,over 99,000,000 RDF triples and over 278,000 links to generic drugs from the LOD Cloud. Using the linked nature ofthe dataset, we demonstrate its ability to support advanced usage scenarios in the drug domain.Conclusions: The process of generating the LinkedDrugs dataset demonstrates the applicability of themethodological guidelines and the supporting tools in transforming drug product data from various, independentand distributed sources, into a comprehensive Linked Drug Data dataset. The presented user-centric and analyticalusage scenarios over the dataset show the advantages of having a de-siloed, consolidated and comprehensivedataspace of drug data available via the existing infrastructure of the Web.Keywords: Methodology, Drugs, Drug products, Healthcare, Linked data, Open data, Data consolidation, ToolsBackgroundAccessing comprehensive drug and healthcare data onthe Web can be a challenging task. The main reasonlies in the fact that the data is available in different for-mats, at distributed Web locations. Additionally, mostof the drug and healthcare datasets are published forspecific purposes only, so consequentially, they contain*Correspondence: milos.jovanovik@finki.ukim.mkFaculty of Computer Science and Engineering, Ss. Cyril and MethodiusUniversity in Skopje, Rugjer Boshkovikj 16, P.O. Box 393, 1000 Skopje,Macedonialimited data. For instance, national drug repositories con-tain information about drug products which are approvedand sold in the country [17]; websites aimed for thegeneral public [813] contain descriptive drug use infor-mation, such as target, dosage, packaging and warnings;websites aimed for professionals [1416] contain morespecific drug data, such as active ingredients, chemicalformulas, drug-drug interactions, food-drug interactions,toxicity, etc. Websites such as DrugBank [14, 17] con-tain more comprehensive drug data. However, the drugentries in their dataset are generic drugs, i.e. active ingre-dients of drugs, and not actual drug products which can© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 2 of 24be bought by patients. Various mobile and web appli-cations [1822] contain information about drug prod-ucts, but the data they use is country-specific and isnot available to the interested parties in an open for-mat. Global drug product repositories exist as well [2325], but they either do not provide means of getting thedata in an open format, or are locked behind a pay-wall.Additionally, some of them provide data for drugs reg-istered in countries from specific world regions only, orsolely provide the name of the drug product in a specificcountry.The Linked Data approachAccording to [2629], the emergence of the Linked Dataprinciples has introduced new ways to integrate and con-solidate data from various and distributed sources. TheLinked Data principles provide means and standards forrepresenting, storing and retrieving data over the exist-ing infrastructure of theWeb. This enables publishing andcontextual linking of data on the Web and with it, cre-ating a Web of data, as opposed to the current Web ofdocuments.With this, the Web becomes a distributed network forstandards-based data access, usable by software agentsand machines. The interlinked nature of the distributeddatasets provides new use-cases for the end-users, whichare generally unavailable over isolated datasets. TheLinked Data approach solves the issue of having datasilos in traditional relational database systems, silos whichcannot link to other databases without a specific codewritten for the task, or specific mapping created in a datawarehousing solution.Additionally, the schema-flexibility of RDF providesmeans of independent data management and evolution,which is very well suited for the nature of theWeb, but alsofor non-Web use in environments where data de-siloingand consolidation is needed [30, 31].So far, as a result of the adoption of Linked Data prin-ciples by data publishers, the Linked Open Data (LOD)Cloud [32] has been populated with 1104 interlinkeddatasets [33]. These datasets, published on the Web fol-lowing the Linked Data principles, belong to 8 differentdomains: government, publications, life sciences, user-generated content, cross-domain, media, geographic andsocial web.MotivationDrug and healthcare data is already available on the Web,both as Linked Data in the LOD Cloud and as regulardata on websites and in mobile applications, intended forhuman consumption. However, the drug data availablecurrently as Linked Data is comprised of drug entitieswhich are generic drugs, i.e. active ingredients of drugs,not actual drug products registered in a specific country,under a specific name, with a specific dosage form,strength and price, for which an end-user might be inter-ested. Such end-users are the patients, the pharmacists,the doctors, but also medical institutions, pharmaceuti-cal companies, etc, which need access to drug productsfrom specific countries for a multitude of user-centric andanalytical scenarios [34]: accessing general informationabout drugs which can be bought in a country, accessinginformation about the availability of different drugs anddrug categories in different countries, accessing pricinginformation, etc.The national drug data are generally available on theWeb, but on regular webpages and not as Linked Data.In order to transform the data from the national drugregistries into high-quality, 5-star Linked Data [35], wepropose a set of methodological guidelines which aim toassist drug data publishers and other interested partiesinto generating a global Linked Drug Data dataset, con-sisting of official data about drugs registered for use indifferent countries. One such global Linked Drug Datasetwould be accessible by using W3C standards via the exist-ing Web infrastructure and would enable a myriad of newdrug-analysis scenarios, including the above-mentionedones. It would enable further data exploitations via devel-opment of innovative applications and services which usethe data gathered from national drug registries of differentcountries in the world.Such methodological guidelines should include stepsaimed towards modeling and aligning the data, trans-forming the data into 5-star Linked Data, publishing thecreated datasets on the Web and defining use-cases ordeveloping applications on top of the dataset. They shouldbe aimed towards assisting drug data owners and pub-lishers, such as the national governing bodies, medicalinstitutions, pharmaceutical companies, pharmacies, etc.,in publishing their data in the same aligned, Linked Dataformat. Their data, once transformed into Linked DrugData and interlinked with the drug data already pub-lished using the same methodology, could be used fordata-based analytics (by the medical institutions, pharma-ceutical companies and governing bodies) or for reachingpotential customers (by pharmacies).In this paper, we propose a set of such methodolog-ical guidelines for consolidating drug data on a globalscale, using Linked Data. In order to support our method-ological guidelines, we have developed a set of open andpublicly available tools, which can be reused in the pro-cess of applying the steps from themethodology over drugdata from any country.Using the proposed methodological guidelines andtools, we generated the LinkedDrugs dataset which con-sists of drug product data from twenty-three countries,and therefore validated the methodology. For this pur-pose we developed an automated system which gathersJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 3 of 24drug data from the official national drug registries oftwenty-three different countries, executes data cleaning,aligns and transforms the data into 5-star Linked Data andpublishes them on the Web in a common, aligned andconsolidated Linked Data dataset. We then demonstrate aset of user-centric and analytical usage scenarios over thegenerated dataset, which are otherwise unavailable or verytime-consuming in a scenario where a user works with thedata available on the Web in HTML webpages.Related workLinked Data projects in the healthcare domainNumerous projects and efforts have already worked ontransforming drug and healthcare data into Linked Data.According to [33], there are currently 83 life sciencedatasets in the LODCloud. These datasets contain health-care data from various subdomains, such as drugs, dis-eases, genes, interactions, clinical trials, enzymes, etc. Themost notable are the Linking Open Drug Data (LODD)project, Bio2RDF and the Semantic Web Health Care andLife Sciences Interest Group at W3C.The LODD project [36] focuses on interlinking dataabout drugs already existing on the Web, as described in[37, 38]. The data ranges from impact of the drugs ongene expression to results of clinical trials. The aim ofthe project is to enable answering of interesting scientificand business questions by interlinking previously sepa-rated data about drugs and healthcare. As part of theirwork, they have collected datasets with over 8,000,000RDF triples, interlinked with more than 370,000 RDFlinks. However, it seems that the project has not beenupdated for some time now, and Bio2RDF has taken overthe hosting and continual publication of the datasets.Bio2RDF [39] is an open-source project which cre-ates RDF datasets from various life science resourcesand databases, and interconnects them following theLinked Data principles into one comprehensive network[4042]. The latest release of Bio2RDF contains around11 billion triples which are part of 35 datasets [43].These datasets hold various healthcare data: clinical trials(ClinicalTrials), drugs (DrugBank, LinkedSPL, NDC), dis-eases (Orphanet), bioactive compounds (ChEMBL), genes(GenAge, GenDR, GOA, HGNC, HomoloGene, MGD,NCBI Gene, OMIM, PharmGKB, SGD, WormBase), pro-teins (InterPro, iProClass, iRefIndex), gene-protein inter-actions (CTD), biomedical ontologies (BioPortal), sideeffects (SIDER), terminology (Resource Registry, MeSH,NCBI taxonomy), mathematical models of biological pro-cesses (BioModels), publications (PubMed), etc.One of the datasets, which is a part of the LODDcloud and Bio2RDF, is the DrugBank dataset, described in[17]. It provides RDF data about drugs, such as chemical,pharmacological and pharmaceutical information, takenfrom an the existing DrugBank database [14, 17] of druginformation. The DrugBank RDF dataset [44] containsover 766,000 RDF triples for 4,770 drugs. These drugs aregeneric drugs, i.e. active ingredients in drug products.The World Wide Web Consortium (W3C) has estab-lished the SemanticWeb for Health Care and Life SciencesInterest Group (HCLS IG) [45] to help organizations fromthe health domain in their adoption of the Semantic Webtechnologies. It is comprised of experts from around 30W3C member organizations: research centers, universi-ties, companies, health institutions, etc. Its mission isto develop and support the use of the technologies ofthe Semantic Web in the fields of healthcare, life sci-ences, clinical research and translational medicine [46]. Itis comprised of various subgroups, which are focused onmaking the biomedical data available in RDF, developingand maintaining biomedical ontologies, etc.In recent years, our research team gained experience inthe drug and healthcare domain by applying the LinkedData principles and the Semantic Web technologies in theseveral different scenarios.We have transformed and pub-lished the drug product data from the Health InsuranceFund of Macedonia as 5-star Linked Data, by connectingit to the LODD and LOD Cloud datasets via the Drug-Bank dataset [47]. We have since extended this datasetwith 5-star Linked Data about the Macedonian medicalinstitutions and drug availability lists from pharmacies[48]. We have also used Linked Data for an analysis ofthe connections between drugs and their interactions withfood, and recipes from different national cuisines, result-ing in findings that uncovered the ingredients and cuisinesmost responsible for negative food-drug interactions indifferent parts of the world [49]. These projects helped usgain insight in the domain and identify the challenges ofapplying Linked Data in the domain.Linked DatamethodologiesThere are a few methodologies defined in the Linked Datadomain, which deal with the process of generating, pub-lishing and using Linked Data. They are mainly focusedon government data, but some are domain independent.The W3C Government Linked Data Working Group hascreated official guidelines for publishing and accessingopen (government) data using the Linked Data principles[50], and with it they suggest three existing methodologieswhich can be used with the Linked (Government) Datalifecycle. These three methodologies are: (a) the method-ology of Hyland et al., (b) the methodology of Hausenblaset al. and (c) the methodology of Villazón-Terrazas et al.Hyland et al. [51] define a methodology for Linked Gov-ernment Data, which consists of six steps. Their method-ology is based on the specifications and best practices bythe W3C, and consists of the following steps: (1) Identify,(2) Model, (3) Name, (4) Describe, (5) Convert, (6) Pub-lish, and (7) Maintain. The methodology contains mostJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 4 of 24steps which are part of the generally accepted Linked Datalifecycle, but is missing guidelines on how to use the gen-erated Linked Data. The authors believe that the usage ofthe generated dataset should be left to the users and otherinterested parties, and according to them, is not a task forthe Linked Data publisher.Hausenblas et al. [52] state that the existing data man-agement approaches - which assume control over the data,the schema and the data generation - cannot be used in theenvironment of the Web, due to its open and decentral-ized nature. Their methodology consist of the followingsteps: (1) Data awareness, (2) Modeling, (3) Publishing, (4)Discovery, (5) Integration, and (6) Use-cases. It also cov-ers most of the general Linked Data lifecycle steps, butdoes not provide detailed guidelines for the process ofpublishing the generated Linked Data dataset on theWeb.Similarly to Hyland et al., based on their experiencein linked government data production, Villazón-Terrazaset al. [53] define a set of methodological guidelines forgenerating, publishing and exploiting Linked Govern-ment Data. Their lifecycle consists of the following steps:(1) Specify, (2) Model, (3) Generate, (4) Publish, and (5)Exploit. This is the only existing methodology in theLinked Data domain which covers all of the lifecycle steps,but unfortunately is focused on government data.In addition to these three methodologies selected bythe W3C Government Linked Data Working Group, theLOD2 Project has developed an updated Linked Datalifecycle for extracting, creating, enriching, linking andmaintaining Linked Data [54]. The Linked Data lifecyclesupported by the LOD2 integrated environment consistsof (1) Extraction, (2) Storage, (3) Authoring, (4) Interlink-ing, (5) Classification, (6) Quality, (7) Evolution/Repair,and (8) Search/Browsing/Exploration. Even though this isthe only methodology which provides software tools forthe denoted steps, and the number of steps here is largerthan in the other methodologies, it still misses some keyelements of the Linked Data lifecycle, such as the datamodeling, the definition of the URI format for the enti-ties and the ways of publishing the generated dataset. Theprovided tools are also general, and cannot be appliedin a specific domain without further work and domainknowledge.ResultsWe developed a methodology and a set of supportingtools for the drug domain, which allowed us to streamlinethe incremental process of generating high-quality LinkedData of drug products from twenty-three countries. Thisnewly created dataset of drug product data, the Linked-Drugs dataset [55], currently consists of over 248,000drug products, interlinked with over 91,000,000 relationsdenoting similarity between them, and with over 278,000links to their corresponding active ingredients availableas Linked Data in the LOD Cloud. The LinkedDrugsdataset enables a novel way of using drug product data, byunlocking new user-centric and analytical usage scenar-ios, previously unavailable over isolated and siloed drugdata repositories. These scenarios utilize the consolidatedand aligned nature of the dataset and its contextual linksto entities from the LOD Cloud. This is achieved by auto-matically generating the additional relations in our datasetwhich link the drugs between themselves, and link thedrugs with drug entities and active ingredients publishedas part of other LOD Cloud datasets. Using W3C stan-dards over the existing infrastructure of the Web, we arethen able to retrive data from these distributed datasets,and present them to the end-users in a comprehensivemanner.Here, we will provide a brief overview of the developedmethodological guidelines and their tools aimed at assist-ing the data publishers during the specific steps in themethodology, while a more in-depth description is pro-vided in the Methods section. We will then present theLinkedDrugs dataset, along with the process of develop-ing the automated system which constructs the dataset.Then, we will present an overview of newly enabledusage scenarios over the consolidated drug productdata.Themethodological guidelinesOur methodological guidelines for consolidating drugdata using the Linked Data approach improve upon theexisting Linked Data methodologies and contain steps,activities and tools which are specific to the drug datadomain. We used our experience in the domain of LinkedHealthcare Data to develop guidelines which aim to guidedata publishers through the process of generating highquality, 5-star Linked Data in order to interlink, align andconsolidate drug data from different national drug reg-istries or other sources of drug data. The alignment andrelationship between the existing methodologies and ourguidelines is outlined in Table 5.Our methodology consists of five steps (Fig. 1):(1) Domain and Data Knowledge, (2) Data Modeling andAlignment, (3) Transformation into 5-star Linked Data,(4) Publishing the Linked Data Dataset on the Web, and(5) Use-cases, Applications and Services. These steps havebeen developed with reuse as a primary goal; therefore,their main focus is the encouragement of data publishersin the drug domain to develop, modify and use reusablecomponents during the steps of the methodology. Thismakes the Linked Drug Data lifecycle modular, i.e. con-structed of loosely-coupled components which can bereused in the domain. Here, by loosely-coupled we meancomponents which can be used separately when neces-sary, but which also form a seamless workflow for gener-ating a high-quality, 5-star Linked Drug Data dataset. TheJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 5 of 24Fig. 1 The methodology for consolidating drug data using the Linked Data approachreuse of such components reduces development time andincreases productivity [56, 57].Methodology supporting toolsAs part of the methodological guidelines, we developeda set of tools as reusable components which simplify theexecution of the specific steps of the methodology. Theirintent is to support the Linked Drug Data generation pro-cess for both people from the drug domain which do nothave deeper knowledge of Linked Data, and Linked Datapublishers which do not have deeper knowledge of thedrug domain. The set consists of (a) the RDF schema,(b) the CSV template, (c) the OpenRefine transforma-tion script, (d) the SPARQL-based tool for extendingand interlinking the dataset and (e) the web-based toolfor automated transformation, interlinking and publish-ing of the generated Linked Drug Data dataset. Thesereusable components are open and publicly available onGitHub [58].The RDF schema is a common data schema for allnational drug data repositories, used for modeling of drugproducts on a global scale (Fig. 2). Its goal is to providealignment of drug data from different sources, with dif-ferent format and different levels of data granularity, inorder to enable simpler data exploitation. It is comprisedof classes and properties from the Schema.org vocabulary[59], which is a novel approach in the drug data domain.The CSV template represents the necessary formal tem-plate for the data which is being prepared for transfor-mation, i.e. the data which will be annotated with ourRDF schema. It contains 39 columns which representthe different data fields needed from the source data forthe transformation process. They inlude the URI of thedrug, its brand name, generic name(s), manufacturer, ATCcode(s), active ingredient(s), strength, cost, license infor-mation, etc. They are modelled to fit the RDF schema,which encompasses all data necessary for high-qualitymodeling of the domain.The OpenRefine transformation script is a reusabletool which helps automate the transformation process,while ensuring compliance of the generated data withthe defined RDF schema and therefore provides aligned,high-quality 5-star Linked Data for the drug domain. Itsintent is to lower the bounds of transforming data intoRDF and Linked Data for data publishers which are notdeeply involved and experienced in the Semantic Weband Linked Data practices. Through several sub tasks,this reusable script for the OpenRefine-based suite oftools (e.g. LODRefine, BatchRefine), interlinks the drugproducts between themselves based on the therapeutic,pharmacological and chemical properties of the drugs,links the drug products to generic drugs, i.e. active ingre-dients from the LOD Cloud datasets and transforms thedata into RDF format.The SPARQL-based tool for extending and interlink-ing the dataset is comprised of two reusable SPARQLqueries. The first query extends the dataset by assigningATC codes to all drugs from the dataset which miss thisinformation. It does so by using the interlinked nature ofthe dataset, i.e. the links the drug products have to genericdrugs and active ingredients from the LOD Cloud. Thesecond query detects all pairs of drugs from the datasetwhich have the same ATC code, and then interlinks themwith properties from the RDF schema which denote thetherapeutic, pharmacological and chemical similarity ofthe drugs.The web-based tool for automated transformation,interlinking and publishing of the generated Linked DrugData dataset is intended for data publishers which gen-erate Linked Drug Data with the previous tools. Thedata publishers can upload the generated Linked Datadataset(s) on the LinkedDrugs project website [60], andafter a human-based quality assessment, the dataset willbe automatically published. For this we use a publiclyavailable Virtuoso instance [61], from which the newdataset is available on the Web as Linked Data, via itsSPARQL endpoint [62].LinkedDrugs: global linked drug products datasetWe applied the outlined steps of the methodology andthe set of tools as part of an automated system fortransforming and generating 5-star Linked Drug Datafrom twenty-three countries: Austria, Azerbaijan, CostaRica, Cyprus, Egypt, Estonia, Ireland, Macedonia, Malta,Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 6 of 24Fig. 2 The RDF vocabulary designed for the drug data domain, comprised of Schema.org classes and properties. For dataset interoperability, it alsouses the classes from the ATC Classification Ontology and properties from the DrugBank Ontology and RDFSNetherlands, New Zealand, Nigeria, Norway, Romania,Russia, Serbia, Slovakia, Slovenia, South African Republic,Spain, Uganda, Ukraine and USA. The countries werechosen to represent the global diversity and to show thata holistic solution for generating Linked Drug Data on aglobal scale is possible. Currently, the generated Linked-Drugs dataset contains over 99,000,000 RDF triples, whichrepresent data for over 248,000 drug products fromthe denoted countries. The dataset also contains over91,000,000 schema:relatedDrug relations betweenthe drugs, and over 278,000 rdfs:seeAlso relations togeneric drugs from DrugBank and DBpedia.This automated system and its workflow represent aconcrete example of applying the methodological guide-lines and supporting tools presented in this paper, andthus serve as their validation scenario.Generating the LinkedDrugs datasetThe national drug registries of many countries aroundthe world are available online. We analyzed the nationaldrug registry websites of 31 countries in order to define acommon set of properties, i.e. a schema skeleton, for thetarget dataset. These steps of domain analysis and RDFschema definition correspond to the activities denoted inStep I and Step II of the methodological guidelines, whichare already done and we directly use them for our specificapplication.In order to design, test and validate our automated sys-tem for gathering 2-star drug data from the national drugregistries and generating 5-star Linked Data from the drugdomain on a global scale, we selected a subset of twenty-three countries. We aimed for a diverse subset, which willencompass different global regions.The drug registries of these countries are availableonline. Their websites are listed in the project page onGitHub [58]. The drug data from most of the nationalregistries is available in a structured format in HTMLpages, intended for human consumption via searchingand browsing on the website itself. The data from asmaller group of countries is available via structured filesin Microsoft Excel or PDF formats, available for directdownload.The automated system gathers the data, performs datacleaning, aligns the data with the predefined schemaJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 7 of 24skeleton, uses the transformation script and the SPARQLquery to transform the data to RDF, extend it with missingATC codes and add links to drugs from the same datasetand drugs from DrugBank and DBpedia, thus turningthe dataset into a Linked Data dataset. The workflow ofthese actions is depicted in Fig. 3. These steps representthe activities defined in Step III of the methodologicalguidelines.Data gathering and staging. In order to create a sus-tainable system for Linked Drug Data, we had to design away to collect the necessary data from the national drugregistries, on a scheduled basis. Therefore, we developeda set of specialized web crawler applications which crawlthe designated drug registry websites, collect the neces-sary data, clean it and store it in a predefined CSV format(Fig. 3). We need to use a set of such crawlers as the targetwebsites differ in structure and available data. The out-put CSV files from the crawlers use the predefined CSVstructure described above.Like most of the data available on the Web, the drugdata available on the national drug registry websites isnot evenly structured, nor completely clean. We thereforeneeded to extend our crawlers with functionalities whichperform data cleaning tasks and work to detect data fromall variations of the source webpages.As per the suggestions in Step III of the guidelines,we used their existing webpage URLs of the drugs astheir unique URIs, e.g. https://lekovi.zdravstvo.gov.mk/drugsregister/detailview/53457. As many of the drug enti-ties contain information about more than one genericname, manufacturer, active ingredient and strength, thecrawlers are tasked with splitting them into the corre-sponding columns in the CSV files. Additionally, infor-mation such about the cost of the drug and its strengthare split into separate fields, to match the CSV template.The crawlers also take care about the specific formatsneeded for some of the columns, such as the dates, thecountry codes, the currency codes, and the prescriptionstatus.The drug data from the several of the countries werean exception, as they are available for download asMicrosoft Excel or PDF files. For these datasets, thecrawlers have to restructure the columns from the sourcedata and generate a CSV file with the correct columnnames according to the CSV template. For these drugs,we generate custom URIs as identifiers, which have theformat http://linkeddata.finki.ukim.mk/lod/data/loddw/drugs/countryCode#drugID. Here, drugID is an ID gen-erated by the crawler, countryCode is a three-lettercountry code (according to [63]) of the country where thedrug was registered and the other parts of the URI identifythe project and the datatype on our Linked Data website:/lod/data/loddw is the project and /drugs is thecategorization of the data.The result of this stage in the workflow, in our case withthe twenty-three selected countries, is a set of twenty-three separate CSV files which follow the CSV template.The only difference is that some of the CSV files can haveno values in some columns, due to the data being unavail-able online. When we get all twenty-three CSV files, thefirst part of the workflow is done andwe can continue withthe second part.Transformation to Linked Data. The CSV files canbe combined into one CSV file, or remain separate. Theonly difference will be in the performance of the nextstep which can be done as a single longer process, oras twenty-three separate and shorter processes. To keepthe processing time per transformation shorter, we usetwenty-three separate CSV files, each representing thedrug dataset of a different country.We send the twenty-three CSV files to a BatchRe-fine instance [64], which represents a wrapper over anFig. 3Workflow: Transforming 2-star data from different national drug data registries to 5-star Linked Drug DataJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 8 of 24OpenRefine instance with the RDF extension, that canbe used as a REST-based service. The HTTP POST callsto the BatchRefine REST-based service are done with thereusable BASH script developed as part of the methodol-ogy supporting tools and contain (a) the CSV file whichneeds to be transformed and (b) the OpenRefine trans-formation script defined as a supporting tool for theguidelines. The result of the call is a transformed RDF out-put, which contains part of the generated Linked DrugData dataset.The output of our BatchRefine transformations aretwenty-three RDF files in Turtle format. These RDF filesare a Linked Data dataset: they contain 5-star data aboutthe drugs from the twenty-three countries, along withlinks to generic drugs from the LOD Cloud - more specif-ically, links to generic drugs from both DBpedia andDrugBank. As we will see later in the text, we can use theselinks to fetch data about the drugs from our dataset whichwe dont have on our end and which do not exist on thesource national drug registry websites, but can be foundin other datasets on the Web and can potentially prove tobe of interest for the end-users.After the transformation with BatchRefine is done, weload the RDF files into a Virtuoso instance [61] using aBASH script. All RDF files are loaded into the same RDFgraph. The latest run of the workflow (Fig. 3) resulted in248,746 distinct drugs in this step, represented with a totalof 7,245,294 RDF triples and with 278,542 outgoing linksto drugs from the LOD Cloud.After the RDF data has been stored into an RDF graphin Virtuoso, we use the SPARQL queries for extend-ing the dataset with missing ATC codes and interlink-ing related drugs. We execute the SPARQL queries overour dataset stored in Virtuoso, using a BASH script.In the latest run of the workflow (Fig. 3), 38,758 newATC codes were added for drug products which didnot have an ATC code in the source registry. Then,91,782,500 schema:relatedDrug triples were addedbetween similar drug products, i.e. 45,891,250 pairs ofdrugs from our dataset were identified to have the samefunction, but exist under different brand names, are fromdifferent countries, or are produced by different manufac-turers, or maybe have a different packaging size, strength,cost, etc. As we will see further in the text, we can utilizethese interlinkings for providing the users with alternativedrugs they may aquire for treating their condition, eitherin the same of in a different country.The workflow shown in Fig. 3 is activated on a sched-uled period, currently set at one month. In order to handlethe data changes during updates, we backup the RDFgraph holding our dataset and replace it with the newlycreated RDF graph. With this we employ versioning andmaintain the default graph identifier to always denote thelatest version of the LinkedDrugs dataset.According to the recommendations in Step IV ofour guidelines, the dataset needs to be published onthe Web, where it will be publicly available. There-fore, we published our LinkedDrugs dataset accord-ing to the best practices for publishing Linked Data[50], via a permanent, dereferenceable URI, which sup-ports HTTP content negotiation: http://linkeddata.finki.ukim.mk/lod/data/drugs# [65]. The dataset is hostedat a live Virtuoso instance [61], in a named RDFgraph <http://linkeddata.finki.ukim.mk/lod/data/drugs#>which holds the latest version of the dataset, publiclyavailable via a SPARQL endpoint [62] which serves asa REST-based service. Additionally, data dumps of thedataset are available on Datahub [55].Usage scenariosWith the automated system and its workflow we startwith twenty-three different, distributed and browsabledatasets, available on the Web and intended for humanconsumption via HTMLwebpages, and using themethod-ological guidelines and tools we manage to create a con-solidated dataset of interlinked and schema-aligned drugproducts from different countries, additionally linked togeneric drugs and active ingredients from the LODCloud.In order to demonstrate the advantages of having the drugdata in a 5-star data quality format, and therefore imple-ment the recommendations from Step V of the method-ological guidelines, we will show a few use-case scenariosvia SPARQL queries. The most basic scenario would be toselect all data about a single drug of interest, which is verysimple and straight-forward, and therefore omitted here.Since the Virtuoso SPARQL endpoint at [62] can be usedas a REST-based service, these SPARQL queries could beused from any type of application to always access andexploit the latest data available.Interlinked drug data. The interlinking cre-ated between the drugs from the dataset, with theschema:relatedDrug triples, can be utilized in ause-case which allows the end-users to discover drugsfrom the same country or another country which havethe same therapeutic, pharmacological and chemicalproperties as the drug of his/her interest. This is a usefulfeature when the drug of interest is not available or whenthe user is traveling abroad. Getting information aboutdrugs with the same properties and their respective pricescan be useful for determining the drug from a specificcategory which is affordable in the specific case. This canbe used by pharmacists, doctors and even patients forgathering information and determining the appropriatetreatment.An example SPARQL query which can be used to iden-tify the drugs from all countries which have the sametherapeutic, pharmacological and chemical properties asthe drug of interest, is shown below (Query 1).Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 9 of 24Query 11 prefix schema: <http://schema.org/>2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>34 SELECT ?drug ?name ?gtin ?strengthValue ?strengthUnit ?costPerUnit5 ?costCurrency ?manufacturerName ?prescriptionStatus ?country6 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>7 WHERE {8 <http://www.legemiddelverket.no//Legemiddelsoek/Sider/Legemiddelvisning.9 aspx?pakningId=c5a02a30-d471-4ba0-8197-38955f384dd8>10 schema:relatedDrug ?drug .11 OPTIONAL { ?drug schema:name ?name }12 OPTIONAL { ?drug schema:gtin13 ?gtin }13 OPTIONAL { ?drug schema:prescriptionStatus ?prescriptionStatus }14 OPTIONAL { ?drug schema:addressCountry ?country }15 OPTIONAL {16 ?drug schema:cost ?costEntity .17 ?costEntity schema:costPerUnit ?costPerUnit ;18 schema:costCurrency ?costCurrency .19 }20 OPTIONAL {21 ?drug schema:availableStrength ?strengthEntity .22 ?strengthEntity schema:strengthValue ?strengthValue ;23 schema:strengthUnit ?strengthUnit .24 }25 OPTIONAL {26 ?drug schema:manufacturer ?manufacturerEntity .27 ?manufacturerEntity schema:legalName ?manufacturerName .28 }29 }30 ORDER BY ?nameIn the query, only a handful of data of interest of therelated drugs are selected, but depending on the specificuse-case, they can be expanded. Query 1 can also be mod-ified to include the specific drug of interest, by modifyingthe drug URI in line 8. In our example in Query 1, weuse the Norwegan drug Airomir as an example, and getresults for over 300 distinct and related drugs from manydifferent countries in the dataset. The query and its fullresults can be viewed online on Seminant [66], at http://seminant.com/queries/5803e77573656d19eb6c5d00. Par-tial results are shown in Table 1.Linked LOD drug data. The main advantage of hav-ing links between data from different and distributeddatasets is the ability to query them from a sin-gle point, over the existing infrastructure of the Web,using W3C standards such as HTTP, SPARQL andRDF. As we have rdfs:seeAlso links from drugsin our dataset to corresponding generic drugs fromthe DrugBank and DBpedia datasets, we can utilizethem to get additional information about the drugsfrom our dataset whenever we are browsing them.Such additional information will come from the Drug-Bank and DBpedia datasets, and can include additionaldrug description, the interactions the drug has withother drugs or with certain foods, the drug mech-anism of action, the drug pharmacology, absorption,biotransformation and toxicity, the list of alternativebrand names and a list of webpages for the drug on otherlocations on the Web. This data is not available on theoriginal national drug registry websites, which are thesource for our dataset; it is data retrieved directly fromthe distributed DrugBank and DBpedia dataset, usingSPARQL federation [67].An example of a federated SPARQL query which selectsinformation about a drug of interest from the DrugBankand DBpedia datasets is shown below (Query 2).Table 1 Partial results from Query 1Drug product Manufacturer CountryActivent Sr Medical Union Pharmaceuticals -EgyptEGAerolin 100mcg/doseInhalerEGAeroline 400 Inhaler EGAerotropa Pharco B International-egpyt EGAgolin Agog Pharma Ltd UGAiromir iNova Pharmaceuticals(New Zealand)NZAiromir Autohaler Teva Sweden AB NOAiromir Autohaler iNova Pharmaceuticals(New Zealand)NZAiromir Autohaler 100microgramosTeva Pharma S.L.U. ESJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 10 of 24Query 21 prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>2 prefix schema: <http://schema.org/>3 prefix drugbank: <http://wifo5-04.informatik.uni-mannheim.de/drugbank/4 resource/drugbank/>5 prefix dbo: <http://dbpedia.org/ontology/>6 prefix dbp: <http://dbpedia.org/property/>78 SELECT ?loddrug ?genericName9 (group_concat(distinct ?brandName; separator = ", ") AS ?brandNames)10 ?comment ?description ?biotransformation ?affectedOrganism ?absorption11 ?chemicalFormula ?toxicity12 (group_concat(distinct ?foodInteraction; separator = " ") AS ?foodInteractions)13 (group_concat(distinct concat(?interactingDrugLabel, : , ?interactionStatus);14 separator = ". ") AS ?drugInteractions)15 (group_concat(distinct ?url; separator = ", ") AS ?urls)16 WHERE {17 GRAPH <http://linkeddata.finki.ukim.mk/lod/data/drugs#> {18 <http://www.aemps.gob.es/cima/especialidad.do?metodo=19 verPresentaciones&codigo=79539>20 rdfs:seeAlso ?loddrug .21 }22 SERVICE <http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql> {23 OPTIONAL { ?loddrug drugbank:description ?desc . }24 OPTIONAL { ?loddrug drugbank:genericName ?gname . }25 OPTIONAL { ?loddrug drugbank:brandName ?bname . }26 OPTIONAL { ?loddrug drugbank:biotransformation ?biotransformation . }27 OPTIONAL { ?loddrug drugbank:affectedOrganism ?affectedOrganism . }28 OPTIONAL { ?loddrug drugbank:absorption ?absorption . }29 OPTIONAL { ?loddrug drugbank:chemicalFormula ?chemicalFormula . }30 OPTIONAL { ?loddrug drugbank:foodInteraction ?foodInteraction . }31 OPTIONAL { ?loddrug foaf:page ?page . }32 OPTIONAL { ?loddrug drugbank:toxicity ?toxicity . }33 OPTIONAL {34 ?drugInteractionEntity drugbank:interactionDrug1 ?loddrug ;35 drugbank:interactionDrug2 ?interactingDrug ;36 drugbank:text ?interactionStatus .37 ?interactingDrug rdfs:label ?interactingDrugLabel .38 }39 }40 SERVICE <http://dbpedia.org/sparql> {41 OPTIONAL {42 ?loddrug dbo:abstract ?abstract .43 FILTER (langMatches(lang(?abstract), "en"))44 }45 OPTIONAL {46 ?loddrug rdfs:label ?label .47 FILTER (langMatches(lang(?label), "en"))48 }49 OPTIONAL {50 ?loddrug rdfs:comment ?comment .51 FILTER (langMatches(lang(?comment), "en"))52 }53 OPTIONAL { ?loddrug dbp:tradename ?tradename . }54 OPTIONAL { ?loddrug dbo:wikiPageExternalLink ?externalLink . }55 }56 BIND(IF(bound(?abstract), ?abstract, ?desc) as ?description)57 BIND(IF(bound(?bname), ?bname, ?tradename) as ?brandName)58 BIND(IF(bound(?gname), ?gname, ?label) as ?genericName)59 BIND(IF(bound(?page), ?page, ?externalLink) as ?url)60 }The query selects some very important data about thedrug of interest and its active ingredient from Drug-Bank and DBpedia. The most significant are the chemical,biological and pharmacological properties of the drug,along with its interactions with food and with other drugs.This data is not always available on the national drug dataregistries, but is of high importance for the end-users,especially the pharmacists and doctors who may requireJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 11 of 24Table 2 Partial results from Query 2Description (from DBpedia)Duloxetine (Cymbalta, and generics) is a serotonin-norepinephrine reuptake inhibitor (SNRI) created by Eli Lilly. It is mostly prescribed for major depres-sive disorder, generalized anxiety disorder, fibromyalgia and neuropathic pain. Duloxetine failed to receive US approval for stress urinary incontinenceamid concerns over liver toxicity and suicidal events; however, it was approved for this indication in the UK, where it is recommended as an add-onmedication in stress urinary incontinence instead of surgery.Food InteractionsFood does not affect maximum levels reached, but delays it (from 6 to 10 hours) and total product exposure appears to be reduced by only 10 percent.People taking this product who drink large amounts of alcohol are exposed to a higher risk of liver toxicity. Take without regard to meals.Drug InteractionsAmitriptyline: Possible increase in the levels of this agent when used with duloxetine.Ciprofloxacin: Ciprofloxacin increases the effect/toxicity of duloxetine.Desipramine: Possible increase in the levels of this agent when used with duloxetine.Flecainide: Possible increase in the levels of this agent when used with duloxetine.Fluvoxamine: Fluvoxamine increases the effect and toxicity of duloxetine.Imipramine: Possible increase in the levels of this agent when used with duloxetine.Isocarboxazid: Possible severe adverse reaction with this combination.Nortriptyline: Possible increase in the levels of this agent when used with duloxetine.Phenelzine: Possible severe adverse reaction with this combination.Propafenone: Possible increase in the levels of this agent when used with duloxetine.Rasagiline: Possible severe adverse reaction with this combination.Thioridazine: Increased risk of cardiotoxicity and arrhythmias.Tranylcypromine: Possible severe adverse reaction with this combinationthem when determining treatment for acute conditionsof a patient who is already on a treatment of a chronicmedical condition.An example run of Query 2, for the drug prod-uct Duloxetina from Spain, results in details forthe generic drug Duloxetine from both Drug-Bank and DBpedia: http://seminant.com/queries/5803e9b973656d19eba65e00. Among other details, it alsoshows the 3 specific food - drug interactions the drugis involved in, along with the 13 drug - drug interac-tions it has. Partial results from the query are shown inTable 2.Analytics. Aside from the use-case scenarios for end-users, our LinkedDrugs dataset can be used for analyticalqueries as well. These analytical queries allow interestedparties to gain insight into the drug markets of differentcountries, allowing them to analyze the available consol-idated data using a single entry point for querying andusing a single query language. The analytics could be built-in in specific analytic applications, or can be executed withseparate and standalone SPARQL queries.To get a better understanding of the analytical possibil-ities over consolidated drug data from multiple countries,we will look at an example query which identifies the mostcommon drug categories per country. This would allowthe user, e.g. pharmaceutical company, to gain a betterknowledge on the national drug markets and make aninformed decision about placing their drug in the countryof interest. A general SPARQL query for this analyticalscenario is given below (Query 3):Query 31 prefix schema: <http://schema.org/>2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>34 SELECT count (distinct ?drug) as ?count ?atc ?country5 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>6 WHERE {7 ?drug a schema:Drug ;8 schema:addressCountry ?country ;9 drugbank:atcCode ?atcCode .10 FILTER (strlen(?atcCode) > 3)11 BIND(SUBSTR(xsd:string(?atcCode), 1, 3) AS ?atc)12 }13 GROUP BY ?country ?atc14 ORDER BY DESC (?count)Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 12 of 24Table 3 Partial results from Query 3Drugs ATC Prefix Country5362 C09 RO2152 C09 ES1536 J01 RU1488 C09 NL1270 N05 US976 J01 ZA758 C09 IE709 C09 SK707 N02 NZA sample run of Query 3 shows that Romania, Spain,Netherlands, Ireland and Slovakia have most drugs (5,362,2,152, 1,488, 758 and 709, respectively) in the categoryof agents acting on the renin-angiotensin system (ATCC09), Russia and South African Republic have most drugs(1,536 and 976, respectively) in the category of antibac-terials for systemic use (ATC J01), while USA has 1,270drugs in the psycholeptics category (ATC N05). Thesepartial results are shown in Table 3. The full results fromthe query are available at http://seminant.com/queries/5803ebc473656d19ebac5e00.Another analytical scenario would be to assess the aver-age drug price per drug category, per country. It could beused by medical authorities in a country to determine thecost situation per category in other countries and use theinformation for local regulations. It could also be used by apharmaceutical company to determine the price range fora new drug, before it goes to market. An example SPARQLquery which can be used for such an analysis is givenbelow (Query 4):A sample run of Query 4 identifies that the ATC drugcategories with the highest average price in Norway aredrugs for disorders of the musculo-skeletal system (ATCM09), respiratory system products (ATC R07) and ali-mentary tract and metabolism products (ATC A16). InMacedonia they are alimentary tract and metabolismproducts (ATC A16), pituitary and hypothalamic hor-mones and analogues (ATC H01) and antihemorrhagics(ATC B02), while in Australia and Slovenia they are respi-ratory system products (ATC R07) and in South AfricanRepublic they are immunosuppressants (ATC L04), hema-tological agents (ATC B06) and alimentary tract andmetabolism products (ATC A16). These partial results areshown in Table 4, while the full results which include othercountries as well, are available at http://seminant.com/queries/5803ed6e73656d19eb537e00.In cases when an inter-country comparison of the pric-ing is necessary, an application could use a currencyconverter to transform the values to the same currency ofchoice, and make the comparison.DiscussionThe generated LinkedDrugs dataset aims to be the firstcomprehensive, consolidated and aligned dataset of drugproduct data on a global scale. In its first version, thedataset consisted of only seven countries; currently, thatnumber has grown to twenty-three. As we have themethodology and the tools in place, the addition of newcountries is more or less straightforward. Even thoughLinked Data datasets with drug data already exist in theLOD Cloud, they consist solely of generic drugs, i.e. activeingredients. Contrary to this, our LinkedDrugs datasetconsists of drug products which are registered in a specificQuery 41 prefix schema: <http://schema.org/>2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>34 SELECT (?totalCost / ?drugCount as ?averageCost) ?costCurrency ?atc5 ?drugCount ?country6 WHERE {7 SELECT count (distinct ?drug) as ?drugCount8 sum (xsd:float(?cost)) as ?totalCost9 ?costCurrency ?atc ?country10 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>11 WHERE {12 ?drug a schema:Drug ;13 schema:addressCountry ?country ;14 drugbank:atcCode ?atcCode ;15 schema:cost ?costEntity .16 ?costEntity schema:costPerUnit ?costPerUnit ;17 schema:costCurrency ?costCurrency .18 FILTER (strlen(?atcCode) > 3)19 BIND(SUBSTR(xsd:string(?atcCode), 1, 3) AS ?atc)20 FILTER (?costPerUnit != "0"^^xsd:double)21 BIND(REPLACE(?costPerUnit, ",", ".") AS ?cost)22 }23 }24 GROUP BY ?country ?atc ?costCurrency25 ORDER BY DESC (?averageCost)Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 13 of 24Table 4 Partial results from Query 4Avg. price Currency ATC Prefix Country93480.60 NOK M09 NO47221.40 NOK R07 NO39557.30 NOK A16 NO32021.40 MKD A16 MK28837.20 MKD H01 MK27478.20 MKD B02 MK22500.00 AUD R07 AU17822.00 SVN R07 SI13360.10 EUR V10 CY10679.50 ZAR LO4 ZA10127.10 ZAR B06 ZA9880.81 ZAR A16 ZAcountry, have a specific name, dosage form, amount,strength, barcode, manufacturer, license, price, ATC code,etc. Since these drug products from our dataset are linkedto the existing generic drugs and active ingredients fromthe LOD Cloud, the novel usage scenarios greatly exceedwhat is currently possible with Linked Drug Data onthe Web.The developed methodological guidelines and support-ing tools intend to encourage and lower the boundariesfor data publishers from the domain to contribute tothe LinkedDrugs dataset, further extending its potentialand value. Since the adoption of the developed tools canpresent a potential hurdle, we believe that the transfor-mation process presented in this paper can serve as anadditional guide.Our methodological guidelines extend the existingLinked Data methodologies, briefly presented in thesection Background . Here, we will make an explicitcomparison of the similarities and differences in theapproaches among the existing methodologies, and ourfindings and proposal to group them into five gen-eral steps. These five general steps have allowed us todefine our own methodological guidelines for the drugdata domain, by defining tasks and tools specific for thedomain and adding them to the corresponding generalsteps. These specific tasks and tools have been the resultof our research in the domain, as well as our previous workwith generating, publishing and using healthcare LinkedData datasets.The existing Linked Data methodologies have a vary-ing number of steps, but still generally cover the sameactivities. The main difference in the methodologies is thegrouping of actions within different steps and on differentlevels of granularity. Apart from some explicit differences,which we will further examine, they cover the palette ofactions involved in the process of generating and publish-ing a Linked Data dataset, and thus can be grouped intofive general steps.From Table 5, we can see that all existing LinkedData methodologies define the knowledge of the data, itsdomain and the existing datasets from the LOD Cloud asthe first step(s). The actions they cover in the steps relateto these tasks, and while somemethodologies focus on thedata domain, others recommend knowledge of existingLinked Data datasets, as well.The second step refers to the tasks of modeling the data,locating and selecting the appropriate ontology or vocab-ulary, extending existing ontologies and vocabularies toTable 5 Aligning our methodological guidelines with existing Linked Data methodologies. Additionally, our approach focuses onlifecycle reuse by step modularityOur Methodology Hyland et al. Hausenblas et al. Villazón-Terrazas et al. LOD2I. Domain and Data Knowledge 1. Identify 1. Data Awareness 1. Specify 1. Extraction2. StorageII. Data Modeling and Alignment 2. Model 2. Modeling 2. Model3. NameIII. Transformation into 5-star Linked Data 4. Describe 3. Publishing 3. Generate 3. Authoring5. Convert 4. Discovery 4. Interlinking5. Integration 5. Classification6. Quality7. Evolution/RepairIV. Publishing the dataset on the Web 6. Publish 4. PublishV. Use-cases, Apps and Services 6. Use cases 5. Exploit 8. Search/Browsing/ExplorationJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 14 of 24match the original data schema, creating a custom ontol-ogy or vocabulary from scratch and mapping it to existingontologies, and defining the URI naming scheme for theontology classes and properties. The methodologies ofHyland et al. [51], Hausenblas et al. [52] and Villazón-Terrazas et al. [53] explicitly define these tasks, whereasthe LOD2 methodology does not define them. For thisstep, we defined an RDF schema which can be reusedwhen working with drug data from national drug reg-istries or other sources. The provided CSV templateallows the data publisher to align the data in the cor-rect format in a CSV file, and prepare it for the nextstep.The third step is focused on transforming the sourcedata into RDF, creating links and 5-star Linked Datadatasets, and creatingmetadata descriptions of the datasetand its links to other datasets. All four methodologiesdefine these tasks, with the LOD2 methodology being themost specific one. The LOD2 methodology, being the lat-est one, understandably contains activities which includeclassification, quality control, data evolution and version-ing. The use of the VoID vocabulary is explicitly statedin this phase in the methodologies of Hyland et al. andHausenblas et al. The methodology of Villazón-Terrazaset al. defines this task in its next step, 4. Publish, butas it contains other tasks which better fit in our nextstep, we left it out of this one. Here, we developed anOpenRefine transformation script which can be used withany source drug data formatted with the CSV templatefrom the previous step, in order to get high quality, 5-starLinked Drug Data. For the purpose of generating addi-tional links between similar drugs in the dataset itself, wedeveloped a SPARQL-based tool which can be used overany Linked Drug Dataset generated with the OpenRefinetransformation script.The fourth step defines the tasks for publishing thedataset on the Web. The exact actions which should beundertaken are defined in the methodologies of Hylandet al. and Villazón-Terrazas et al. The other two method-ologies do not define such tasks and steps. In order tosimplify this step for data publishers, we provide a web-based tool for automated publishing of the generatedLinked Drug Data dataset. The tool also interlinks the newdataset with the consolidated LinkedDrugs dataset, whichalready contains drug product data from twenty-threenational drug repositories.The fifth step consists of tasks for defining use-casescenarios or development of specific applications and ser-vices which take advantage of the newly created LinkedData dataset and its links to other datasets. Themethodol-ogy of Hyland et al. does not specify such an activity, argu-ing that the data publisher does not have to think aboutpotential use and reuse of the data while working on gen-erating Linked Data from the source dataset. According tothem, the data publisher should just make sure he/she cor-rectly transforms, describes and publishes the data, and letthe users and the interested community know about thedataset. However, Hausenblas et al., Villazón-Terrazas etal. and the LOD2 Project team explicitly state that the laststep of the Linked Data generation process should consistof defining use-cases and/or developing applications andservices.The tools we developed as support for the steps ofthe methodology aim to help data publishers from thedomain. They can aid both data publishers from the drugdomain which do not necessarily have prolific experiencewith Linked Data, as well as Linked Data publishers whichare not very familiar with the domain of drugs and health-care. These open-source and reusable tools are supposedto lower the bounds for interested parties to get involvedin the domain and include their datasets in the globalLODD and LOD Cloud.ConclusionsThe amounts of data available on the Web representa goldmine for data-driven applications and services[68]. Unfortunately, the data is available in different for-mats and is distributed over various locations. This isa huge obstacle which blocks progress in data retrievalin many domains. The healthcare domain is no differ-ent: the national drug authorities publish their data onthe Web in different formats and granularity levels, andthere is no comprehensive method for retrieving andusing them.The Linked Data concept provides new ways of pub-lishing and connecting data from different distributedsources, which allows data consolidation. It also pro-vides a new spectrum of use-case scenarios which canbe useful for generating new business value for busi-nesses and independent developers, by allowing themto develop innovative applications and services in thedomain. The opportunities which lie within the creationof new use-case scenarios from Open Data and LinkedData are a field whose potential is becoming increasinglyrecognized [69].Motivated by this, we propose methodological guide-lines for using Linked Data principles to consolidatedrug data, and provide a set of tools which intend toaid the data publishers in the steps of the methodol-ogy. Our methodological guidelines extend the existingLinked Data methodologies, both general and aimed forgovernment data. We combine the steps from the exist-ing methodologies into five steps which are necessary fordeveloping a sustainable Linked Data dataset. Each ofthese steps is extended to include specific tasks, actionsand tools which are important for the drug data domain.The aim of our methodological guidelines is to enable thecreation of Linked Drug Data datasets which are very wellJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 15 of 24aligned between themselves and which can then easily beconsolidated into a comprehensive system or dataspace.We implemented our guidelines and tools as part ofan automated system for transformation and publish-ing of the 2-star and 3-star data from twenty-threenational drug registries into consolidated and aligned5-star Linked Drug Data. The system provides reg-ular updates which make sure the created Linked-Drugs dataset always contains new data. In order tocreate the dataset, we use the defined common RDFschema, the CSV template, the OpenRefine transforma-tion script, as well as the SPARQL queries for extendingthe dataset and interlinking the drugs from the dataset.With this, we ensure data alignment with other LODand LODD datasets. Currently, our generated Linked-Drugs dataset contains over 99,000,000 RDF triples, whichrepresent data for over 248,000 drug products fromtwenty-three countries. The dataset also contains over91,000,000 schema:relatedDrug relations betweenthe drugs, and over 278,000 rdfs:seeAlso relations togeneric drugs and active ingredients from DrugBank andDBpedia.We further present new usage scenarios enabled byLinked Data, aimed both for end-users and analytical pur-poses, which utilize the consolidated and aligned natureof the dataset and its contextual links to entities from theLOD Cloud. We achieve this by automatically generat-ing the additional relations in our dataset which link thedrugs between themselves, and link the drugs with drugentities published as part of other LOD Cloud datasets.Using W3C standards and the Semantic Web technolo-gies, we are then able to retrive data from these distributeddatasets, and present them to the end-users in a compre-hensive manner.As future work, we plan to extend the number ofnational drug registries from the current twenty-three.The design of the supporting system requires that we onlymodify the data gathering and staging part of the work-flow, while the transformation and publishing processremains the same. We also intend to extend the reconcil-iation tasks in OpenRefine, by employing reconciliationfor the company names. This would provide additionallinks in the dataset between the drugs and the manu-facturers, represented by company entities on the LODCloud. Given that the names of the companies and theirdetails are sometimes on local languages, depending onthe drug registry, we would also add a translation ser-vice in the workflow. We also plan to add an automatedVoID metadata generation task for the dataset, whichis a recommendation from Step III. Regarding applica-tions built on top of the datasets, we have already starteddeveloping user-focused and analytical applications whichprovide the end-users with detailed information about adrug product and its interaction with other drugs andfoods, but also provide insight into the drug product avail-ability on a global scale. All of these additional featureswould further add to the benefit of having consolidatedand aligned drug data from various countries in one place,accessible via the existing infrastructure of the Web andvia existing W3C standards.MethodsThemethodological guidelinesBased on our experience with applying the Linked Dataprinciples in the healthcare and drug domain and onthe existing Linked Data methodologies, we developed aset of methodological guidelines for consolidating drugdata using the Linked Data approach. These guidelinesimprove upon the existing Linked Data methodologiesand contain steps, activities and tools which are specificto the drug data domain. Their purpose is to guide datapublishers through the process of generating high qual-ity, 5-star Linked Data in order to interlink, align andconsolidate drug data from different national drug reg-istries or other sources of drug data. The alignment andrelationship between the existing methodologies and ourguidelines is outlined in Table 5.Along with the guidelines, we have developed a setof tools which simplify the execution of the specificsteps of the methodology. Their intention is to supportthe Linked Drug Data generation process for both peo-ple from the drug domain which do not have deeperknowledge of Linked Data, and Linked Data publish-ers which do not have deeper knowledge of the drugdomain. These tools are open and publicly available onGitHub [58].Our methodological guidelines consist of the followingsteps (Fig. 1):I. Domain and Data KnowledgeII. Data Modeling and AlignmentIII. Transformation into 5-star Linked DataIV. Publishing the Linked Data dataset on the WebV. Use-cases, Applications and ServicesThese steps have been developed with reuse as a pri-mary goal; therefore, their main focus is the encourage-ment of data publishers in the drug domain to develop,modify and use reusable components during the stepsof the methodology. This makes the Linked Drug Datalifecycle modular, i.e. constructed of loosely-coupledcomponents which can be reused in the domain. Theseloosely-coupled components can be used separately whennecessary, but also form a seamless workflow for generat-ing a high-quality, 5-star Linked Drug Data dataset. Thereuse of such components, like in other software devel-opment cases, reduces development time and increasesproductivity [56, 57].Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 16 of 24Step I: Domain and data knowledgeThe first step corresponds to the first steps from the exist-ing methodologies: it is important for the data publisherto know the domain and the data in it very well. Thisunderstanding of the data source schema and semanticsis crucial for the following steps which will involve datamodeling, schema alignment and data transformation.In the drug data domain, if this is the first time the datapublisher comes across Linked Data, our advice is to firstget familiar with the 5-star data system fromTim Berners-Lee [35], the four principles of Linked Data [70], and theLOD Cloud [32]. After that, a study of the LODD Project[36], the Bio2RDF Project [43] and the DrugBank LinkedData dataset [44] is recommended. This will help the datapublisher to get a better insight into the Linked Drug Dataand Linked Healthcare Data domains, the types of datawhich exist in them, their schema, their similarities anddifferences and their existing and potential links. It willalso help him/her determine the ontologies and vocabu-laries already used in the domain, which can be importantfor the next step.In a general case, when working with any other domain,it is important for the data publisher to get familiar withthe domain in question and with the meaning of thedataset selected for transformation. For this, a consultwith a domain expert is usually necessary and there-fore advised. Another approach is to explore the existingLinked Data datasets which are similar to or from thesame domain as the one of interest. For this, the Datahubportal [71] and the LOD Cloud cache instance [72] couldbe used.Step II: Datamodeling and alignmentIn the next step, the data publisher should focus on datamodeling and alignment with other existing or futuredatasets. The data publisher has to choose the correctschema for the dataset, in order to annotate it correctly,i.e. use the data fields which are necessary for the finaluse-cases, annotate the fields unambiguously and with thecorrect semantics and make the correct schema choiceswhich will allow seamless alignment with other datasets.Additionally, the data publisher has to define the URInaming scheme for the data entities, and optionally for theontology or vocabulary classes and properties.Data schema. In the drug data domain, studying thedatasets from the LODD and Bio2RDF projects can helpget an insight into the ontologies and vocabularies usedin the domain. Some of the ontologies and vocabular-ies which a data publisher needs to have in mind are:Schema.org [59], DBpedia Ontology [73], UMBEL [74],DICOM [75], the DrugBank Ontology used for the data at[44], as well as other biomedical ontologies [76].In order to support the data publishers from thedrug domain in this step, we designed a reusable RDFschema for the data, shown in Fig. 2. The schema canbe used by data publishers working with drug datafrom national drug registries or other sources. Theschema is comprised of classes and properties fromthe Schema.org vocabulary [59]: the schema:Drugclass, along with a large set of properties whichinstances of the class can have, such as name,code, activeIngredient, nonProprietaryName,availableStrength, cost, manufacturer,relatedDrug, description, url, etc. Addition-ally, in order to align the drug data with generic drugsfrom DrugBank, we use the properties brandName,genericName, atcCode and dosageForm from theDrugBank Ontology. In order to annotate the links whichthe drug product entities will have to generic drug enti-ties from the LOD Cloud dataset, the rdfs:seeAlsorelation is used.In general, when working with data from anotherdomain, the data schema is defined with the choice ofvocabularies or ontologies to be used. The principles ofontology engineering and usage have been developed forthis purpose exactly: to maximise the chances of reuse,and therefore allow better alignment between datasets[77]. This means the agent should always try to reuse anexisting vocabulary or ontology, giving advantage to thosewhich are most widely used. A few tools for ontology andvocabulary discovery exist, and the data publisher shoulduse them in this stage. The two most notable are LinkedOpen Vocabularies (LOV) [78] and DERI Vocabularies[79], which also provide usage statistics which can be usedto assess the impact of a given vocabulary or ontology ina specific domain. Our choice of the Schema.org vocabu-lary follows the reusability paradigm: it is the most widelyand generally used vocabulary across the Web.However, datasets tend to have specific fields, which arenot covered by existing ontologies. In this cases, the exist-ing ontology or vocabulary should be extended, or a newone should be defined. However, each time a new ontol-ogy is developed, it is important to define the mappingsbetween the new classes and properties and the classesand properties from other ontologies, in order to enableontology matching and RDF-based reasoning, for schemaalignment. In order to avoid defining specific new prop-erties, we reused some properties from the Schema.orgvocabulary which are currently not explicitly intended foruse with schema:Drug entities. An example of suchproperties is the schema:addressCountry propertywhich should be used for an address, but we use it inour schema to denote the country in which the drug isregistered.Another important approach in this step is the use ofupper-level ontologies and vocabularies; they can pro-vide a schema for many different and specific domains,due to their generality. Having two or more datasetsJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 17 of 24annotated with the same upper-level ontology or vocab-ulary allows interlinking and inference between them,i.e. it improves the alignment which is crucial for dataconsolidation.Data alignment. For the data alignment task, the datapublisher needs to make sure that the generated datasetwill be well aligned with existing and future datasets fromthe same domain. In order to aid the data publishers withthis, as well as help them in preparing the drug data forthe transformation step, we developed a CSV templateintended for the drug domain [58]. This CSV template canbe used with the drug data and is comprised of the fieldsnecessary for applying the RDF schema from Fig. 2.The data publisher interested in publishing Linked DrugData should use this CSV template for the data, followingthe specifics defined for each of the fields. These specificsassure that the data will be of high quality and completelyaligned with other drug data generated using the samemethodological guidelines.URI formats. From the URI naming scheme perspec-tive, in the domain of drug data it is important to deter-mine the types of entities which exist in the dataset. Thiswill help in defining the entity URIs for the Linked Datadataset. According to the Linked Data principles, eachentity in the dataset - along with the classes and proper-ties in the ontology - needs to have a unique indentifier inthe form of an HTTP URI. In order to provide better per-formance when using the dataset in the future, our experi-ence suggests using separate URL paths for different entitytypes, e.g. http://example.com/drug/, http://example.com/interaction/, http://example.com/disease/, etc. An additional recommendation isto use slash-based URIs, instead of hash-based ones. Thismay result in using an additional HTTP request by themachine accessing the URI, but it provides better perfor-mance when accessing large datasets [80].However, to simplify this step for the drug data publish-ers, we advise the use of the existing webpage URLs ofthe drugs from the national registry websites, which arealready unique. According to the Linked Data principles,the entity URI should denote a Web location where theend-users and agents can get more information about theentity, so our approach satisfies the condition.Step III: Transformation into 5-star Linked DataDuring the third step, the source dataset should be trans-formed into a 5-star Linked Data dataset. The process oftransformation can be executed in many different ways,and with various software tools, e.g. OpenRefine [81],LODRefine [82], D2R Server [83], Virtuoso [84], SilkFramework [85], etc.To help the data publishers from the drug domain andto automate this step, we developed a reusable OpenRe-fine transformation script [58]. This transformation scriptis specifically designed for the drug data domain, and theRDF schema and CSV template from Step II. It contains aset of actions which generate RDF from the inputed CSVfile which contains drug data. In the process, it also locatesassociated generic drugs from the DrugBank and DBpe-dia datasets for each drug product in source dataset, andextends the generated RDF with links between the drugsfrom the dataset and the corresponding drugs from theLOD Cloud.The transformation script can be reused with anyOpen-Refine instance which has the RDF extension. It can beapplied on any drug data dataset formatted with the CSVtemplate from Step II. As a result, it will generate a LinkedDrug Data dataset annotated with the RDF schema fromStep II (Fig. 2).The RDF schema from Step II defines relations betweenthe drug products from the dataset as well. These relationsare denoted with the schema:relatedDrug relation(Fig. 2). In order to provide means for generating RDFtriples which interconnect the drugs from the dataset, wedeveloped a SPARQL query [58] which can be executedover the dataset generated with the OpenRefine trans-formation script. The SPARQL query detects all pairs ofdrugs from the dataset which have the same ATC code- and therefore have the same therapeutic, pharmaco-logical and chemical properties - and creates two triplesconnecting the first drug to the second one with theschema:relatedDrug property, and vice-versa.In a general case, in order to make the correct choicesabout the tools to be used for the transformation pro-cess, it is important to distinguish the characteristics ofthe dataset first. The nature of the dataset will determineif (a) the transformation is a one-time task, a task whichwill have to be executed on a given time interval (e.g. oncea month), or a continually running task; (b) old versionsof the transformed dataset are necessary for versioningand as backup, if during future transformations only thechanges in the data are needed for transformation, i.e.delta updates are performed, or if older data are no longernecessary for the particular use-case; (c) manual or auto-mated data cleansing is needed before the first transfor-mation and/or subsequent transformations; (d) the sourcedataset is always available at the same location and isaccessible via the same interfaces. These specifics of thedataset in question can then help the data publisher deter-mine if the transformation task can be fully or partiallyautomated, and identify the parts of the transformationworkflow which require human attention and input.Adding metadata about the newly created Linked Datadataset is significant from the data reuse perspective -using vocabularies such as VoID [86] help ubiquitouslydetermine the characteristics of the dataset and the linksthe dataset has to other Linked Data datasets, throughsoftware agents. VoID metadata contains informationJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 18 of 24about the name, description and category of the dataset,versioning information and update frequency, contactinformation, the license under which the dataset is madeavailable, the links to the SPARQL endpoints and URIlookup endpoints, used vocabularies and their propertiesand classes. It also explicitly defines the links between thedataset and other Linked Data sets, defined in the datasetitself. The use of the VoID vocabulary is explicitly stated inthe corresponding steps in the methodologies of Hylandet al., Hausenblas et al. and Villazón-Terrazas et al.In the domain of drug data providing new and updateddata is very important - old data has no importanceto the end-user, except for analytics. This means thatthe data publisher should anticipate the change rate ofthe source dataset and correctly design the workflow ofrefreshing data from the source dataset to the Linked Datadataset. This would translate to creating a sustainabil-ity plan which will transform new data and add it to theLinkedData dataset, remove old data and provide version-ing. Depending on the size of the source dataset, the datapublisher can choose to re-transform the source dataseton each update, or to provide means for performing deltaupdates. Providing versioning is also important, as newtransformations can sometimes result in errors, renderingthe dataset unusable.Step IV: Publishing the Linked Data dataset on theWebIn the forth step, the generated 5-star Linked Data dataset,along with its VoID metadata, should be published on theWeb. This should be done following theW3C recommen-dations for publishing Linked Data on theWeb [50], whichsuggest enabling direct URI resolution, providing a REST-ful API, providing a SPARQL endpoint, and/or providingthe dataset as a file for download.There is a large palette of tools and software platformswhich allow seamless Linked Data publishing. Amongthem are D2R Server [83] and Virtuoso [84], which allowLinked Data publishing of datasets which are originallyin an RDF file (Turtle, N3, RDF/XML, JSON-LD, etc.), aCSV file, or in a relational database. These platforms thenallow access to the Linked Data dataset via HTML pages,via RDF file downloads and via a SPARQL endpoint whichcan be used as a RESTful API as well.When creating a Linked Drug Data dataset, werecommend adding and interlinking it with the globalLinkedDrugs dataset which will consist of all suchdatasets generated by different parties, using theseguidelines. To enable this, we have developed a web-based tool for uploading the generated datasets [60],which after a human-based quality assessment triggersan automated process for interlinking the new datasetwith the existing LinkedDrugs datasets and publishingit according to the Linked Data principles and bestpractices.We also recommend publishing the dataset atDatahub.io [71] under the healthcare and drugscategories, as well as adding the #linkeddrugs tag.Additionally, we advise joining the LOD Cloud [87]. Boththese actions will enable higher visibility of the dataset.Another important part of this step is the announce-ment of the newly created Linked Data dataset to the pub-lic. For this, information about the dataset along with itsVoID metadata should be published on popular data por-tals, such as Datahub.io [71]. This announcement shouldalso be done via existing communication channels of thedata publisher and his/her organization. In order to facil-itate further use and reuse of the dataset, it is importantto provide a form or a contact email address for inter-ested parties to be able to report data or access issues, andprovide feedback. On the organization side, it is impor-tant that these reports and requests are attended to ina timely fashion; otherwise the usability of the dataset issignificantly lowered.Step V: Use-cases, applications and servicesThe last step refers to defining use-case scenarios and/ordeveloping specific applications and services which willuse the data from the newly created Linked Data dataset,to showcase the (re)usability of the dataset and its links toother Linked Data datasets. This will present the poten-tial of the contextually linked datasets to future interestedparties.When creating a Linked Drug Data dataset, poten-tial use-case scenarios, applications and services shouldinclude contextually linked data from the LODD datasetsand the Bio2RDF datasets. The LODD datasets, especiallythe DrugBank linked dataset, contain data about genericdrugs, i.e. active ingredients, along with their pharma-ceutical and pharmacological properties, targets, brandnames, food interactions, drug interactions, etc. Since thenational drug data registries contain information aboutdrug products, one-to-one mappings between the enti-ties from such datasets and the DrugBank and DBpediadatasets are not possible. Instead, using our RDF schemafrom Step II and the OpenRefine transformation scriptfrom Step III, each entity from the dataset will be linked toone or more generic drugs/active ingredients from Drug-Bank and DBpedia, based on its ATC code [88]. Thisway, the drugs from the dataset get a contextual link tothe generic drugs, and from there, to all of its proper-ties and characteristics. Additionally, the existing linksfrom the DrugBank and DBpedia generic drugs to otherhealthcare datasets can be further exploited, as they alsorepresent contextual links. Such links currently point toLinkedCT and Bio2RDF. To demonstrate the usability ofthe generated Linked Drug Data datasets, we provideexample use-cases on the project website [60] and onGitHub [58].Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 19 of 24In a general case, the use-cases can be text-basedscenarios, specific SPARQL queries, or prototype appli-cations, describing the ways in which the data fromthe new dataset can be browsed, retrieved and used.Here, a specific focus should be given on how thelinks to other Linked Data datasets can be exploitedto reach other data, not present in the original datasource, to extend its context. With this, the data pub-lisher will show to interested parties that the originaldataset has more value when combined with datasetsfrom the same or similar context, instead of being usedin an isolated scenario. Besides such use-case, the sameeffects of the Linked Data dataset can be showcasedwith the development of applications and services. Theybring more visibility to the general (re)usability of theLinked Data dataset, but generally require more time andeffort.The created use-cases, applications and/or services,should be shared and announced to the public, along withthe dataset itself and its VoID metadata. The use of thesame channels from the previous step is advised.Methodology supporting toolsAs part of the methodological guidelines, with the intentto provide help to the data publishers working in thedrug data domain, we designed and developed a setof tools. These tools consist of the RDF schema, theCSV template, the OpenRefine transformation script, theSPARQL-based tool for interlinking related drugs and theweb-based tool for automated transformation, interlink-ing and publishing of the generated Linked Drug Datadataset.RDF schemaIn order to model the domain of drug products on a globalscale, we needed to create one common schema for allnational drug data repositories, and then use it for anno-tating the drug data. With it, the goal was to providealignment of drug data from different sources, with differ-ent format and different levels of data granularity, in orderto enable simpler data exploitation.First, we analyzed the national drug data repositories of31 countries1 and the analysis helped us define a com-mon set of properties which exist and which we wantto use in our dataset. This set consisted of 24 proper-ties, including the brand name of the drug, the genericname, the ATC code, the EAN code (barcode), the listof active ingredients, the drug strength, dosage form,cost, manufacturer, the country it was registered in, thedetails about its license, etc. Not all national drug datarepositories provide all of the data and properties weselected for our schema, but we did not want to decideagainst using those properties - they are useful whereavailable.Following the best practices in ontology and vocabularyuse [77], we started by considering reuse of classes andproperties from existing vocabularies. We used the com-mon set of properties we defined in the previous step, andfound that the Schema.org vocabulary [59] was fully appli-cable for our set. The Schema.org vocabulary, as part of itsHealth and Lifesciences Extension [89], contains a defini-tion of the class schema:Drug and contains a large set ofproperties applicable to it [90]. As we can see on Fig. 2, theRDF schema uses the DrugBank ontology and the RDFSontology, as well, for interoperability purposes.Schema.org is a joint initiative of Google, Bing, Yahooand Yandex, as a common vocabulary intended for struc-tured markup on web pages [9193]. It is used by thesesearch engines to introduce rich snippets about people,events, products, movies, restaurants, books, tv shows,etc. It is also used in Googles Knowledge Graph, in emailsconfirming reservations and receipts (from restaurants,hotels, airlines, etc.) both from Gmail and MicrosoftsCortana, it is used for rich pins on Pinterest, as well asfromApples Siri [94]. Its use on theWeb has been increas-ing in the past few years, more rapidly than the more rig-orous general-purpose vocabularies and ontologies beforeit [95]. Its success is mainly attributed to its simplicity:it uses a generally flat hierarchy of classes, so that theboundaries of implementation from data publishers andwebmasters is kept low.The growing use of the Schema.org vocabulary, as wellas its domain generality, has put the vocabulary in aposition in which it is being used for aligning existingontologies and datasets. This is happening in the health-care domain, as well [96]. With the release of Schema.orgversion 3.0 [97], the medical and healthcare related terms[98] have been moved to the Health and LivesciencesExtension [89], to enable and ensure future collabora-tive development of the terms by the Healthcare SchemaVocabulary community group at W3C [99, 100]. Thisplan for a long-term support by the community from thedomain instills sufficient certainty for us to choose theSchema.org vocabulary, instead of the domain specificontologies [76], to provide a common schema for drugproducts on a global scale.In order to provide some alignment between the gen-erated datasets and the LODD and DrugBank datasets,we use several properties from the DrugBank ontologyto describe the drug products. More specifically, we usedrugbank:brandName, drugbank:genericName,drugbank:atcCode and drugbank:dosageFormas additional properties for the same values denotedby schema:name, schema:nonProprietaryName,schema:code and schema:dosageForm, respec-tively. We do this for simplifying the SPARQL federatedqueries when working with data from our LinkedDrugsdataset and the DrugBank dataset. Additionally, each drugJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 20 of 24product from the LinkedDrugs dataset is an instance of aspecific class from the ATC Classification Ontology [101],in order to classify the drug according to the ATC classi-fication system, based on its ATC code(s). We also choserdfs:seeAlso as it is the most widely used relation forinterlinking similar entities in the LOD Cloud [33].Just as any other RDF schema, vocabulary and ontol-ogy, the RDF schema selected for our Linked Drug Datadatasets can be evolve in time; it can be extended andmodified in the future by us or third-parties, as the field ofdrug data evolves.CSV templateIn order to enable data publishers to annotate their drugdata with the RDF schema from Fig. 2, we need a for-mal template for the data which is being prepared fortransformation, and a formal transformation process. Forthe former, we define a CSV template, available publiclyand as open-source [58]. The CSV template contains 39columns which represent the different data fields neededfrom the source data for the transformation process. Theyinlude the URI of the drug, its brand name, genericname(s), manufacturer(s), ATC code(s), active ingredi-ent(s), strength, cost, etc. They are modelled to fit withthe RDF schema, which encompasses all data necessaryfor high-quality modeling of the domain.The data type of the different columns is usually a sim-ple text value, except where we note otherwise. Someimportant notes regarding the field data types include:the strength value is divided into an integer-value col-umn denoting the strength, while the unit is part of atext-value column denoting the strenght unit; similarly,the cost of the drug is separated into a float value anda currency value; the several date columns need to beformatted as YYYY-MM-DD; the prescription statusshould be enumerated as either OTC or Prescrip-tionOnly; the currency code needs to comply with theISO standard for denoting currencies [102]; the coun-try where the drug is registered in needs to be denotedusing a country code accoding to an ISO standard [63]; ifthere are multiple generic names, manufacturers or activeingredients, they should be denoted one-per-column inthe available genericNameN, manufacturerN andactiveIngredientN columns, respectively, etc. Thedetails about the other column data types are available onthe project website [58].The CSV template uses a vertical line character (|)as a delimiter, since the regular CSV separators suchas a comma (,) and a semicolon (;) are very oftenpresent in the cell values when working with drug data,and can be misinterpreted. It is important to note thatthe order of the columns in the CSV template is notrelevant, if used with our OpenRefine transformationscript.As with the RDF schema, the CSV template is open andpublicly available, and therefore can be extended or modi-fied in the future by both us and third-parties, as the drugdata field evolves and more Linked Drug Data dataset arebeing created.OpenRefine transformation scriptStep III of the methodology contains the task of trans-forming the source data into the RDF schema selectedin Step II. Since we defined an RDF schema which canbe applied in the drug data domain for drug productswhich are registered in different countries, we also providea tool which can help automate the transformation pro-cess, while ensuring compliance of the generated data withthe defined RDF schema and therefore providing aligned,high-quality 5-star Linked Data for the drug domain. Theintent of this tool is to lower the bounds of transformingdata into RDF and Linked Data for data publishers whichare not deeply involved and experienced in the SemanticWeb and Linked Data practices.We provide this Linked Data generation tool in the formof an OpenRefine transformation script. OpenRefine [81]is an open-source software for working with structureddata, usually CSV, TSV, XML, etc. It provides users withfunctionalities for working with large datasets: the userscan record their action over a small set of example rows,and then apply them over the entire source data. Here, theactions can include data transformations, merging, datacleaning tasks, manipulation of the columns, etc. It alsohas an RDF extension which allows reconciliation of cellvalues against RDF data from SPARQL endpoints. Thisallows linking of cell values with entities from a SPARQLendpoint, for unambiguous identification of entities. Italso allows mapping of the source data into RDF, by defin-ing an RDF skeleton. The output of this action is anRDF file generated from the source data, according to thedefinitions in the RDF skeleton.The ability of OpenRefine to save the user actions andthen export them in JSON format, allows reuse of certainsets of actions for different datasets. This gives us the abil-ity to define the data transformation which can be reusedover different source drug datasets, which have the samecolumns. As we have a CSV template, we can use this aspart as our set of tools. The defined list of data transforma-tion actions we created is what we have as our OpenRefinetransformation script [58].Our OpenRefine transformation script is designed fordata complying with the CSV template, and its output isa Linked Drug Data dataset which uses our RDF schema.The transformation script contains three actions:A. reconcile the columns genericName1,genericName2, ..., genericName5 againstDBpedia,Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 21 of 24B. reconcile the column atcCode against theDrugBank dataset, andC. create an RDF schema skeletonAction A. uses the RDF extension feature of OpenRe-fine which uses the cell value from a selected columnto find potential entities from a given SPARQL endpointwhich can be matched to the entity denoted by the row.In our case, we use the five genericName columns -which hold the generic name of the active ingredient ofthe drug entity - and we try to match each of them to adbo:Drug entity from the DBpedia SPARQL endpoint,using its rdfs:label value. If the reconciliation servicefinds a matching candidate entity, we use it in step C. tocreate an RDF triple which links the drug entity from ourCSV dataset with the matched candidates from DBpedia,via an rdfs:seeAlso relation, for instance:Example 1@prefix dbp: <http://dbpedia.org/resource/>@prefix mkd: <https://lekovi.zdravstvo.gov.mk/drugsregister/detailview/>mkd:55446 rdfs:seeAlso dbp:Clopidogrel .Action B. does a similar reconciliation, but on theatcCode column from the CSV dataset and against theDrugBank endpoint. It tries to find matches betweenthe value of the atcCode column on our side andthe drugbank:atcCode value of drugbank:drugsinstances from the endpoint. Unlike the situation in A.,here we can have more than one matching candidatefrom DrugBank. The reason is that there can be multipledrugbank:drugs instances which have the same ATCcode, i.e. share the same therapeutic, pharmacological andchemical properties. Similar as in A., we use all matchingcandidates from the reconciliation in step C. to create RDFtriples which link the drug entity from our CSV dataset tothe matched drug entities from DrugBank, such as:Example 2@prefix mkd: <https://lekovi.zdravstvo.gov.mk/drugsregister/detailview/>@prefix dbd: <http://wifo5-04...uni-mannheim.de/drugbank/resource/drugs/>mkd:841690570 rdfs:seeAlso dbd:DB00201 ;rdfs:seeAlso dbd:DB00316 .Action C. creates the RDF schema skeleton, which con-tains the rules for mapping the consolidated CSV file intoRDF. In the RDF schema skeleton (Fig. 2), we define map-pings between the CSV columns and certain RDF triplepatterns. Some of the mappings are straight-forward, suchas the mappings of the brand name, the generic name,the dosage form, the country, the url, the description, etc.For them, we define the URI of the drug as a subject,we denote a specific property for the triple, and then wedefine the value of the column as a literal or an objectof the triple. For instance, the brand name of a drug ismapped into RDF triples with the following format:Mapping Example 1<drug-URI> schema:name <value-of-brandName-column> ;drugbank:brandName <value-of-brandName-column> .However, other mappings are more complex. Mappingsof values such as the ATC code, the cost, the strength,the manufacturer, the license details, etc., need new enti-ties to be created, entities of different types. For instance,in order to add the information about the ATC code tothe drug entity, we need to create a new blank node oftype schema:MedicalCode, which has two additionaltriples: one with the schema:codeValue property andone with the schema:codingSystem property. ThisATC code mapping can be represented with:Mapping Example 2<drug-URI> schema:code <blank-node-ID> .<blank-node-ID> rdf:type schema:MedicalCode;schema:codeValue <value-of-atcCode-column> ;schema:codingSystem ATC.The license mappings were the most complex, whichwe can see from Fig. 2. Aside from using OpenRefinesuser interface for defining the RDF skeleton, we usedits GREL language for mapping the reconciliation resultsfrom actions A. and B. into rdfs:seeAlso triples.As a result of the transformation script, a Linked Datadataset with links to the LOD Cloud is created. Similarlyas the other tools, the transformation script is availableas an open-source JSON file, which can be extendedand modified in the future. As a support for it, wealso developed a BASH script which sends the CSV filewith drug data, formatted according to our CSV tem-plate, along with the OpenRefine transformation scriptto a running BatchRefine service [58]. The result fromthis call is the RDF output representing the transformeddataset.SPARQL-based tool for extending and interlinking the datasetOnce the drug dataset is transformed into a Linked Datadataset with the other tools, an additional step is requiredin Step III to create the internal links between drugswhich share the same functionality, i.e. share the sameJovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 22 of 24therapeutic, pharmacological and chemical properties, inorder to create a better basis for use-cases.We need to cre-ate links between drugs from the dataset which have thesame function, i.e. are aimed to treat the same condition.To create these links, we use drugs ATC codes. Accord-ing to theWorld Health Organization coding scheme [88],if two drugs have the same ATC code, they share thesame function. For this purpose, we define a reusableSPARQL query [58] which detects all pairs of drugs fromthe dataset which have the same ATC code, and usingthe schema:relatedDrug property creates a pair oftriples for them, for instance:Example 3@prefix rus: <http://www.vidal.ru/drugs/>@prefix mkd: <https://lekovi.zdravstvo.gov.mk/drugsregister/detailview/>rus:trombopol__22439 schema:relatedDrugmkd:51201 .mkd:51201 schema:relatedDrugrus:trombopol__22439 .These two triples create a two-way link between thedrugs in the dataset, denoting their functional similarity.The SPARQL query results with storing the newly createdRDF triples in the same RDF graph where the dataset isalready stored. These interlinkings can be utilizes for pro-viding the users with alternative drugs they may requirefor treating their condition, either in the same of in adifferent country.Since not all source registries contain the ATC codeinformation, and in order to increase the number of inter-linked drug products from the dataset and support betterdata analytics, we define an additional reusable SPARQLquery [58] which assignes ATC codes to all drugs fromthe dataset which miss this information. The SPARQLquery detects drug products without an ATC code, findsthe generic drug from DBpedia which the drug is linkedto with the rdfs:seeAlso relation, gets the ATC codeof the DBpedia generic drug and assigns it to the drugproduct in question. Since the SPARQL query for inter-linking drugs from the dataset depends on the ATC code,this SPARQL query for extending the dataset with missingATC code values should be executed first.Both SPARQL queries are parametrized and should beedited before execution. They can be executed over theLinked Data storage used for storing the Linked Datadataset generated with the other tools.Web-based tool for automated transformation, interlinkingand publishingThe generated Linked Drug Data dataset needs to be pub-lished on theWeb according to the Linked Data principlesand best practices, as advised in Step IV. In order to aid thedata publishers, this step can be automatically executedby using a web-based tool we provide. The data publish-ers can upload the generated Linked Data dataset(s) onthe LinkedDrugs project website [60], and after a human-based quality assessment, the dataset will be automaticallypublished. For this we use a publicly available Virtuosoinstance [61], from which the new dataset is available onthe Web as Linked Data, via its SPARQL endpoint [62].The RDF graph identifier is returned to the data publisherafter the successful upload process.Besides publishing finished Linked Drug Data datasets,the web-based tool and its automated process can alsoexecute the previous steps of the methodology for thedata publisher: (a) they can generate an interlinkedLinked Data dataset from an input CSV file, and (b)they can interlink drugs with schema:relatedDrugrelations from an input RDF file. For the former, theuploaded CSV file needs to be generated following ourCSV template, and based on it, the predefined RDFschema and the OpenRefine transformation script, ourweb-based tool and its server-side process will gener-ate the Linked Data dataset. Using the SPARQL-basedtool from above, it will then generate links betweenthe drugs from the dataset, based on their ATC codes.For the latter, the web-based tool directly creates theschema:relatedDrug relations between similar drugsfrom the uploaded Linked Drug Data dataset in RDF.With this, we provide the convenience to move mostof the data processing from the methodological guide-lines away from the data publishers, and simplify theirworkflow.When a data publisher uses our web-based tool at[60] to publish a Linked Drug Data dataset, our sys-tem also adds it to the global Linked Drug Data dataset- the LinkedDrugs dataset - by storing it in anotherRDF graph and generating schema:relatedDrugtriples for linking the drugs from the new dataset withthe drugs from the existing datasets in LinkedDrugs,and vice-versa. The LinkedDrugs dataset then containsdata for drug products provided by different publish-ers, including our team, and is available via a perma-nent, dereferenceable URI, which supports HTTP contentnegotiation [65].Endnote1Austria, Azerbaijan, Belgium, Canada, Costa Rica,Croatia, Cyprus, Czech Republic, Egypt, Estonia, EUsEuropean Medicines Agency, France, Hungary, Ireland,Italy, Macedonia, Malta, Netherlands, New Zealand,Nigeria, Norway, Romania, Russia, Serbia, Slovakia,Slovenia, South African Republic, Spain, Uganda, Ukraineand USA.Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 23 of 24AcknowledgementsThe authors would like to thank Simona Micevska, Angjela Davitkova, DamjanGjurovski and Marjan Georgiev, who helped with the extensive data gatheringprocess. The work in this paper was partially financed by the Faculty ofComputer Science and Engineering, Ss. Cyril and Methodius University inSkopje, as part of the SemBigData: Using Semantic Web Technologies toConnect and Explore Big Data research project.Authors contributionsMJ and DT designed the research. MJ developed the methodologicalguidelines and the supporting tools. MJ developed the automated dataconsolidation system and the LinkedDrugs dataset. MJ and DT analyzed thedata and designed the usage scenarios. MJ and DT contributed to themanuscript. Both authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 28 April 2016 Accepted: 17 December 2016RESEARCH Open AccessOntology-based specification, identificationand analysis of perioperative risksAlexandr Uciteli1*, Juliane Neumann2*, Kais Tahar1, Kutaiba Saleh3*, Stephan Stucke4, Sebastian Faulbrück-Röhr4,André Kaeding4, Martin Specht3, Tobias Schmidt3, Thomas Neumuth2, Andreas Besting5, Dominik Stegemann5,Frank Portheine5 and Heinrich Herre1*AbstractBackground: Medical personnel in hospitals often works under great physical and mental strain. In medical decision-making, errors can never be completely ruled out. Several studies have shown that between 50 and 60% of adverseevents could have been avoided through better organization, more attention or more effective security procedures.Critical situations especially arise during interdisciplinary collaboration and the use of complex medical technology, forexample during surgical interventions and in perioperative settings (the period of time before, during and after surgicalintervention).Methods: In this paper, we present an ontology and an ontology-based software system, which can identify risks acrossmedical processes and supports the avoidance of errors in particular in the perioperative setting. We developed a practicabledefinition of the risk notion, which is easily understandable by the medical staff and is usable for the software tools. Based onthis definition, we developed a Risk Identification Ontology (RIO) and used it for the specification and the identification ofperioperative risks.Results: An agent system was developed, which gathers risk-relevant data during the whole perioperative treatment processfrom various sources and provides it for risk identification and analysis in a centralized fashion. The results of such an analysisare provided to the medical personnel in form of context-sensitive hints and alerts. For the identification of the ontologicallyspecified risks, we developed an ontology-based software module, called Ontology-based Risk Detector (OntoRiDe).Conclusions: About 20 risks relating to cochlear implantation (CI) have already been implemented. Comprehensive testinghas indicated the correctness of the data acquisition, risk identification and analysis components, as well as the web-basedvisualization of results.Keywords: Perioperative risks, Ontology, Risk definition, Risk specification, Risk identification, Risk analysis, Agent systemBackgroundPatient safety is a quality objective and an importantfactor of the quality of treatment in hospitals in general[1]. Prevention of medical errors and risks is a significantmethod to improve patient safety. Medical personneloften work under great physical and mental strain. Inmedical decision-making, errors can never be completelyruled out [2]. In 2000, the report To Err is Human [3]was published by the Institute of Medicine of the USNational Academy of Sciences (IOM). This attractedgreat international attention and moved the topics ofmedical risks, errors and patient safety into the focus ofthe scientific interest. The IOM concluded in the reportthat from 2.9 to 3.7% of all patients admitted to hospitalsin the USA sustain an adverse event. In 70% of thesecases, the patient retains no or only minor damage, 7%lead to permanent damage and 14% cause the patientsdeath. The study also showed that between 50 and 60%of these adverse events could have been avoided throughbetter organization, more attention or more effectivesecurity procedures. Analyses show that the number of* Correspondence: auciteli@imise.uni-leipzig.de;juliane.neumann@medizin.uni-leipzig.de; kutaiba.saleh@med.uni-jena.de;heinrich.herre@imise.uni-leipzig.de1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),University of Leipzig, Leipzig, Germany2Innovation Center Computer Assisted Surgery (ICCAS), University of Leipzig,Leipzig, Germany3Jena University Hospital, Jena, GermanyFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 DOI 10.1186/s13326-017-0147-8medical errors in Germany is also not negligible. Accord-ing to a report by the Robert Koch Institute [4], the inci-dence of suspected medical errors is approximately 40,000cases across the country per year. Hence, the estimatederror recognition rate of 30% corresponds to the rate ofapproximately 12,000 recognized medical errors per year.Since the publication of To Err Is Human, riskmanagement and patient safety has consistently remaineda topic of interest for scientific studies as well as for sug-gestions of goals for improvements [5]. Critical situationsarise especially during interdisciplinary collaboration andthe use of complex medical technology, for exampleduring surgical interventions and in perioperative settings.Especially the oversight of medically relevant treatmentdata or an incomplete medical history may lead to incor-rect treatment [6].We present an ontology and a conception for anontology-based software tool, which can identify andanalyze risks across medical processes. Furthermore, thetool supports the avoidance of errors in the perioperativesetting. The results of the risk analysis are conveyed tomedical personnel in form of context sensitive hints andalerts. The software architecture is designed to respondnot only to risks within a single treatment step, but toalso consider the patients entire stay in the hospital. Fora practical implementation in the clinical environment,the cochlear implantation (CI) was selected as a surgicaluse case at Jena University Hospital. For this purpose,medical and technical treatment risks were analyzed andmedical guidelines and standards were taken into ac-count. In addition, data and information sources weredefined based on an anonymized CI patient record.Further sources of critical events were collected byundertaking of qualitative interviews with technical,nursing and medical personnel participating in a CItreatment process. On this basis, risk situations weredefined and integrated into ontological models. This workis a part of the OntoMedRisk project [7] funded by theGerman Federal Ministry of Education and Research.MethodsIntroduction in General Formal Ontology (GFO)The development of the intended ontologies and of theneeded ontological analyses are carried out within thetop-level ontology GFO [8, 9]. In GFO, the entities ofthe world are classified into categories and individuals.Categories can be instantiated, but individuals are notinstantiable. GFO allows for categories of higher order,i.e. there are categories whose instances are themselvescategories, for example the category species. Spatio-temporal individuals are classified along two axes, thefirst one explicates the individuals relation to time andspace, and the second one describes the individualsdegree of existential independence.Spatio-temporal individuals are classified into continu-ants, presentials and processes. Continuants persistthrough time and have a lifetime. A particular kind of con-tinuant corresponds to ordinary objects such as cars, balls,trees, etc. These are called material objects: they carry aunity, consist of matter and occupy space. The lifetime ofa continuant is presented by a time interval of non-zeroduration; such time intervals are called chronoids in GFO[10]. Continuants are individuals, which may change, forexample, an individual cat C crossing the street. Then, atevery point in time t of crossing, C exhibits a snapshotC(t). These snapshots differ in their properties. Further,the cat C may lose parts while crossing, though, remainingthe same entity. The entities C(t) are individuals of theirown, called presentials; they are wholly present at aparticular point in time, being a time boundary. If thecontinuant is a material object M, the presentials exhibitedby M at point in time t, denoted by M(t), are called mater-ial structures. Presentials cannot change, because anychange needs an extended time interval or two coincidingtime boundaries.Processes are temporally extended entities that happenin time, for example a run; they can never be whollypresent at a point in time. Processes have temporalparts, being themselves processes. If a process P istemporally restricted to a point in time then it yields apresential M, which is called a process boundary of P[10]. Hence, presentials have two different origins, theymay be snapshots of continuants or parts of processboundaries [9]. There is a duality between processes andpresentials, the latter are wholly present at a point intime, whereas this is never true for processes. The corre-sponding classes/sets of individuals, denoted by thepredicates Cont(x), Pres(x), and Proc(x), are assumed tobe pair-wise disjoint. Processes are the most basic kindof entity, because they form a ground for presentials andcontinuants, and determine the coherence of spatiotem-poral reality. A boundary of a process P is defined by therestriction of this process to a point in time of itstemporal extension. We postulate that any presential is apart of some process boundary.The integration between material objects and pro-cesses is proposed in the integration law in GFO, whichstates that for every material object M, being a continu-ant, there is a process Proc(M), the boundaries of whichcoincide with the presentials exhibited by M. There areseveral basic relations which canonically connectprocesses, presentials, and continuants [8, 9].Spatio-temporal individuals, according to the secondaxis, are classified with respect to their complexity andtheir degree of existential independency. Attributivesdepend on bearers, which can be continuants, presen-tials, and processes. Situations are parts of reality, whichcan be comprehended as a coherent whole [11]. MaterialUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 2 of 14situations are composed of material objects, which areconnected by relators, and relators are instances of rela-tions. Situoids are processes, which satisfy principles ofcoherence, comprehensibility, and continuity. A surgicalintervention is an example of a process or a situoid. Asnapshot of this situoid at a certain point in time is asurgical presentic situation, which has a spatial locationand includes various entities such that a coherent wholeis established.There is a variety of types of attributives, among them,qualities, roles, functions, dispositions, and structuralfeatures. Properties are categories, the instances of whichare attributives. According to the different types ofattributives (relational roles, qualities, structural features,individual functions, dispositions, factual, etc.) we distin-guish quality properties and role properties, and the roleproperties are classified into relational role properties(abr. relational properties) as well as social role proper-ties (social properties).Ontological definition of the risk notionThe solution of all philosophical problems related to thenotion of risk is out of the scope of this paper. Instead,we focus on a practicable definition of the risk notion,which can be easily understood by medical staff and isusable for the software tools. Our definition of the risknotion has been developed in close cooperation with do-main experts (medical staff ). Based on this definition, itshould be possible for the medical staff to specify therelevant risk types, and for the software to identify andto analyze the risk in a particular treatment situation.There are various definitions of the notion of risk.One of the most known/popular definitions is presentedin [12]. The authors divide the notion of risk into threecomponents, which are associated to the followingquestions:1. What can happen, i.e., what can go wrong?(scenario)2. How likely is it that that will happen? (probability ofthe scenario)3. If it does happen, what are the consequences?(consequence of the scenario)A risk, then, is a triple which consists of a scenario,the probability of that scenario, and consequence of thatscenario.Furthermore, there are several standards investigatingthe notion of risk. The ISO/IEC 27005:2008 [13] definesthe notion of risk (information security risk) as poten-tial that a given treat will exploit vulnerabilities of anasset or group of assets and thereby cause harm to theorganization; the OHSAS 18001:2007 [14] - as a com-bination of the likelihood of an occurrence of ahazardous event or exposure(s) and the severity of injuryor ill health that can be caused by the event or expo-sure(s); and the ISO 31000 (Risk management) [15] - asan effect of uncertainty on objectives.In [16] the authors analyze 11 common definitions ofrisk and characterize them based on three categories: (a)risk as a concept based on events, consequences and un-certainties; (b) risk as a modeled, quantitative concept(reflecting the aleatory uncertainties); and (c) subjectiverisk descriptions. Most definitions belong to category (a),the rest can be interpreted both in the sense of (b) or (c).The common ground of most risk definitions is thatall of them consider a risk as involving a possibility forthe occurrence of a particular event or situation. Most ofthese definitions consider such events as adverse ones.The ontological analysis of risk is carried out withinthe framework of GFO and takes into account the avail-able definitions of risk. The analysis is built upon theontology of situations and situation types, which partlyuses ideas presented in [11, 17]. Adverse situations aresituations that contain adverse events. In this paper weuse the notion of adverse event/situation not only in thesense of Any untoward occurrence that may presentduring treatment with a pharmaceutical product butwhich does not necessarily have a causal relation to thetreatment [18], but we also include events/situationsthat are not related to medical interventions.The notion of a possible situation is established withinthe framework of a particular actualist representationism,which postulates that possible situations are abstract en-tities, the existence of which is consistent with the currentlyavailable knowledge about the actual world. This view ispartly influenced by [1921] and is subsequently explicatedfor material situations. Material situations are composed ofmaterial facts, which are constituted by material objects andconnecting relators. An example of a material fact is aspatio-temporal entity that is denoted by the expressionJohns drinking a beer. Associated to this fact we may con-struct the relational proposition John is drinking a beer.There is a difference between a fact and the correspondingproposition. A proposition is an abstract entity, which canbe satisfied by facts (which are parts of reality). Arbitrary ab-stract situations are sets of relational propositions, whichare not necessarily abstracted from real, i.e. actual situations.An abstract situation S is realized by an actual situation S? ifany relational proposition in S is satisfied in the situation S?.An abstract situation S, related to a domain D, is said to bepossible if it is consistent with the currently available know-ledge about D, the domain experts agreed on. Hence, a pos-sible situation has the potential to be realized by an actualsituation. A (spatiotemporal) situation S is said to be a risksituation if it satisfies certain conditions, which imply thatfor one of its possible succeeding situations S? any of itsrealizing situations is an adverse situation.Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 3 of 14We hold that a risk exists in a situation, that it de-pends on it, and, hence, that it can be considered as asituations property. We distinguish between single (insense of gfo:Property [8]) and composite properties, thelatter being composed of single ones and which can bedisassembled by the relation gfo:has_part.Definition 1. A composite property CP is a propertythat has as parts several single properties SP1, ..., SPn.Definition 2. A risk for an adverse situation of typeAST is a composite property CP such that everysituation S possessing the property CP has a possiblesucceeding situation of type AST, which can be realizedwith a certain probability.Definition 3. A risk is a composite property CP forwhich there exists an adverse situation AST such that CPis a risk for the adverse situation AST (as defined by 2).Definition 4. A risk situation is a situation having atleast one risk (Fig. 1). In this paper, we consider risk sit-uations as situations with a risk recognized as relevantby the medical community and non-risk situations assituations with no risk recognized as relevant by themedical community.Example 1. The risk of a bacterial infection duringcochlear implantation in infants depends on variousparameters, such as the infants age, the correspondingbone thickness of the skull and the inner ear structure.If the child is younger than 5 months, the bone thick-ness mostly remains below 2 mm. Thus, the risk ofpenetrating the skull and injuring the dura mater duringsurgery increases so that the risk of the bacterial duramater infection (meningitis) increases as well. Theground-truth probability for the adverse event of duramater infection during CI is about 59% [22]. For men-ingitis prevention, the patient has to be vaccinatedagainst pneumococcus, meningococcus and haemophilusinfluenzae type b several weeks before the surgery (indi-cation phase). In addition, an antibiotic preventionshould be performed right before the surgery. Accordingto our definition an increased risk for acquiring menin-gitis can be represented as a composite property,consisting of three single properties, namely, the youngage (< 5 month), the absence of a meningitis vaccination,as well as the absence of an antibiotic prevention. Thisexample is used in this paper for further explanations.ResultsRisk Identification Ontology (RIO)We developed a Risk Identification Ontology (RIO,Fig. 2), which is built upon the ontological model of thenotion of risk. This ontology is used for the specificationand the identification of perioperative risks. The ontol-ogy RIO is founded in the GFO. As the starting point weconsider the treatment process, which may consist ofvarious treatment phases (gfo:has_part). The completetreatment as well as the phases are complex processes(gfo:Situoid). The treatment has a particular temporalextension, called the treatment time (gfo:Chronoid).According to GFO processes are projected (gfo:project-s_to) onto their time intervals. For every point in time(gfo:Time_boundary) of the treatment exists (gfo:exist-s_at) exactly one treatment situation (gfo:Situation). Apoint in time of the treatment is according to GFO aboundary of the treatment time (gfo:boundary_of),whereas the corresponding treatment situation is aboundary of the treatment itself.For each treatment phase, particular points in time ofrisk detection (PTRD) can be defined. The treatment sit-uations, existing at these points in time, are analyzedwith respect to the existence of risks. Such situations arecalled potential risk situations (PRS), because they doFig. 1 Definition of the risk notion (the white arrows represent the is-a relation)Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 4 of 14not necessarily contain risks. Situations and in particulartreatment situations possess various properties (gfo:-Property). These properties may belong to the situation,but also to the participants, as, for example physicians(doctors), medical instruments, and, most important, tothe patients. We consider these properties also asproperties of the current treatment situation (gfo:ha-s_property). Properties of the potential risk situationsthat are relevant for the estimation of the risk are calledKPIs (Key Performance Indicators) in this paper. Ac-cording to Definitions 14 a particular combination of asubset of the KPIs of a PRS (for example, age ofpatient = 3 months, menginitis vaccination = false) is arisk if the PRS may lead to an adverse situation at a laterpoint in time (rio:succeeding_situation).A PRS may contain various risks, and risks of the sametype (the instances of the same risk class) may occur indistinct PRS and may lead (rio:risk_for_adverse_situa-tion) to distinct adverse situations (the instances of thesame adverse situation class). Each KPI is associatedwith potential risk situations, whereas the risk situationsadditionally possess the composite risk properties.Furthermore, the risks can be related to those treatmentphases for which they are relevant (rio:risk_in_phase). Arisk is relevant in a particular phase, if all required KPIvalues for the risk assessment need to be recorded (e.g.according to external or internal hospital guidelines) andneed to be available in this phase in a respectivedatabase to prevent the risk from being realized in anadverse situation. Adverse situations may exhibit variousdegrees of severity and risks may possess various prob-abilities for the occurrence of adverse situations.With help of the RIO the risks in a current potentialrisk situation are identified by the software componentFig. 2 Risk Identification Ontology (RIO)Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 5 of 14OntoRiDe, and, hence the situation can be classifiedeither as a risk or as a non-risk situation.Risk specificationPerioperative risk assessmentFor the development of a perioperative risk identificationontology the recognition and assessment of potentialmedical, technical, organizational, and human riskfactors are an essential prerequisite. Therefore, an exten-sive risk assessment was performed for an otorhinolar-yngological use case. The insertion of cochlear implants(CI) was chosen in order to demonstrate the featuresand benefits of the ontology-based risk identificationsystem. The perioperative medical and technical riskfactors, procedure related complications and their com-plication rates as well as prevention strategies wereextracted from peer-reviewed publications and evidence-based best-practice guidelines of the German Society ofOto-Rhino-Laryngology, Head and Neck Surgery [23]. Inaddition, entries of the Critical Incident Reporting System(CIRS) of the University Hospital Jena (Germany) and anexample of an anonymized patient record were analyzedfor organization and human-related risk assessment. Thederived risk characteristics, potential following adverse sit-uations and their causes were used to describe relevantperioperative and cross-process risks factors.Perioperative process modelingThe information of risk factors and of potentially adverseevents has to be provided to the responsible medicalpersonnel at the right time by offering appropriatecontext-sensitive hints and alerts. Therefore, the medicaland organizational processes have to be taken into ac-count. The general perioperative workflow of the CI treat-ment was modeled and visualized in a process diagram, asevent-driven process chain (EPC). In the following, bothgeneralized and use-case specific treatment phases weredefined in the formal process model. The generalizedtreatment phases are depicted in Fig. 3. Besides the CItreatment process, the defined phases are suitable forrepresenting various elective surgeries and interventions.The treatment process was modeled by representingthe sequence of clinical activities, treatment decisions,parallel processes and possible events, the involved per-sons as well as resources, like data and documents, med-ical devices, or IT systems. In addition, the identifiedrisk factors, complications, and prevention activitieswere integrated in the process model.By mapping the identified risk factors to the dedicatedactivities and treatment phases, the process model wasthen used subsequently for further risk assessment andperioperative risk modeling. This enabled over 120 po-tential perioperative risks to be identified and alsomapped to their related process step in the processmodel.Perioperative risks modelingIn the next step the identified potential risk factors, ad-verse situations, and critical incidents, which are relatedto cochlear implantation interventions, were examinedin an extensive risk analysis. Thereof, a risk classificationfor formal risk specification was derived. The identifiedrisk factors were subsequently classified into differentcategories of medical, organizational, technical, orhuman-related risks. Thus, the treatment phases werecategorized into risk detection phases, in which thecorresponding risk is relevant and could potentially leadto an adverse situation. Additionally, there is a categoryfor cross-process risks, which could lead anytime to anadverse situation, e.g. the risk of dizziness and falls orthe high bleeding risk during surgery due to anticoagu-lant medication.For each treatment phase, different KPIs were defined,which allow the identification of specific perioperativerisks. The KPIs are linked with operators and a certaindata range to a conditional expression of a possible riskfactor (e.g., c1: Age_in_months IN [0, 5), c4: Vaccina-tion_status == no, Fig. 4, Example 1). The KPI datatype values could be for instance a Boolean value, text,date, or number. A combination of these conditional ex-pressions is formalized as a risk specification rule. If therisk specification rule becomes true, due to the values oftheir conditions and KPIs, there is a high occurrenceprobability of adverse situations, which have to be alsospecified for each risk. In addition, for each adverse situ-ation an occurrence probability and a severity were de-fined (the severity is defined on a separate spreadsheet).In the risk specification, the KPIs were described alongwith their possible acquisition sources. Therefore, the riskspecification defines both the required measurementphases and the measurement sources, like patient-relatedFig. 3 Treatment phasesUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 6 of 14data and sensor data, e.g. data from the digital patient rec-ord, the hospital information system, checklists, or situa-tions in actual process execution. In Fig. 4 a riskspecification based on Example 1 is presented.The tool RIOGen, developed within the project, gener-ates ontological entities from the risk specification andinserts them into RIO. For every risk condition, forexample, a subclass of the corresponding KPI is inserted.Here the class names are automatically generatedaccording to certain rules. For every condition class ananonymous equivalent class is created as property re-striction, based on the property has_data_value (Fig. 5).Then, for each risk a subclass of rio:Risk is created. Thename for the subclass is defined in the risk specification(e.g., Risk Name: Infection_Risk_001, Fig. 4). For the risksubclass, an equivalent anonymous class is also definedwhich is based on the has_part property and on the cor-responding condition classes; this anonymous class rep-resents the risk specification rule (Fig. 6). Furthermore,the treatment phases are created and connected withthose KPIs and risks which are relevant for them. Fi-nally, we define the connections between risks and thoseadverse situations, which possibly evolve from them, asannotations (incl. probability and severity, Fig. 7). Wespecified the probability as annotation (as_probability) ofthe annotation relating to the adverse situation(risk_for_adverse_situation).Ontology-based Risk Detector (OntoRiDe)We developed an ontology-based software module,called Ontology-based Risk Detector (OntoRiDe), whichallows the identification of the ontologically specifiedrisks. This tool receives the KPIs of the current potentialrisk situation as an input parameter, and carries out therisk specification rule, which is contained in the ontol-ogy; then it classifies the current situation as a risk ornon-risk situation and returns the results. If the currentKPIs satisfy one of the rules (i.e., at least one risk is rec-ognized) then the considered situation is a risk situation,otherwise it is a non-risk situation.Further information, which the tool returns to theuser, includes the description of the existing risks, thetreatment phases, in which the risks are relevant, butalso the adverse situations, which may evolve from them(with the probability of occurrence and degree of sever-ity). The most important functionality is the possibilityto recognize the risks, but, furthermore, to determineand provide for every recognized risk all combinationsof current KPIs that are responsible for every recognizedrisk. Using this information the user is able to eliminateall of the riskscauses.In the following, we briefly sketch the functionalities ofthe OntoRiDe. For every risk class the corresponding riskspecification rule, which is specified as an anonymousequivalent class (Fig. 6), is interpreted and transformedFig. 4 Risk specificationFig. 5 Risk conditionsUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 7 of 14into a disjunctive normal form (by stepwise execution ofthe de Morgan rules and of the law of distributivity). Anyof the conjunctions presents a possible explanation for therisk (e.g., c1 AND c4 AND c6 and c3 AND c5 ANDc6, Fig. 4). Then, the single conditions (Fig. 5) arechecked, i.e., it is determined whether the current KPIvalue is included in the specified value range. If all condi-tions of the conjunction are satisfied, then the correspond-ing KPIs and further information are provided for the useras explanation.We did not use a standard DL reasoner. Instead, weimplemented suitable functions in OntoRiDe, which arerelevant for the specific risk identification problem.Firstly, we want to apply rules, which cannot be easilyinterpreted by standard reasoners, especially rules whichcontain mathematical expressions or predefined con-stants. Such special types of rules are implemented bythe OntoRiDe. Secondly, standard reasoners carry outvarious tasks, such as checking the consistency, classifi-cation, and realization. However, most of these standardtasks are not relevant for the identification of risks. Thisleads to a reduced efficiency of the overall system, if astandard reasoner is utilized for the interpretation of riskspecification rules. Finally, OntoRiDe must provide theuser with all possible explanations about the existence ofa risk in the current situation in an understandable way.The problem of detection and exploration of all possibleexplanations or justifications of an entailment is a well--known task, for the solution of which there exists severalmethods and tools [2426]. Furthermore, there are vari-ous investigations about the cognitive complexity andthe understanding of the considered justifications [27,28]. In this context a justification of an entailment isunderstood to be the minimal set of axioms sufficient toproduce an entailment [24]. In [27, 28] the understand-ability of justifications and the corresponding readingstrategies of OWL users are analyzed. The details ofseveral user studies show that ontology developers findcertain justifications very difficult to understand and towork with. We developed a very simple form of explan-ation, which is understandable for the medicalpersonnel. The OntoRiDe translates the risk specifica-tion rules into a disjunctive normal form and checks allconditions of the respective conjunctions. By thisprocedure all KPI combinations, verified by the rule astrue, and the corresponding conditions (value ranges),can be provided for the user in form of understandableexplanations (e.g., age < 5 month and vaccination = noand antibiotic prevention = false).In this way, we identify all and only relevant risks inthe current situation, as well as provide all possible ex-planations for them, so that all requirements have beenfulfilled. Although the OntoRiDe is not a reasoner, it issound and complete with regard to our problem.Fig. 6 Risk specification ruleFig. 7 Annotations of risk and adverse situationUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 8 of 14Agent systemOntoRiDe is embedded into an agent system, which isdeveloped within the project OntoMedRisk. The purposeof this system is to conveniently access data, that is dis-tributed across various data sources within a hospital ina unified manner. In this way, the agent system deriveselementary information for identifying risk situations.The data has to be collected by the agent system and isdetermined by a set of KPIs. They represent risk-relevant parameters, which have to be monitored by theagent system throughout the entire perioperative treat-ment process. The collected KPI-related data is providedfor the risk identification and analysis in a centralizedfashion. The results of those analyses are then forwardedto the medical staff as context-sensitive hints and alerts.The goal of OntoMedRisk is to reduce the risks of ad-verse situations and complications through early and ad-equate interventions.The functional architecture of the agent system isshown in Fig. 8. The agent system is integrated into thehospital information system from which it collectspatient and risk related data. Besides the data and agentrelated components, the agent system also includes thefunctional components OntoRiDe and OntoRA (Ontology-based Risk Analysis). The software-based agent system hasbeen implemented using the Java Agent DevelopmentFramework (JADE) [29]. JADE embodies a framework, aplatform and the middleware for a FIPA-standardized(Foundation for Intelligent Physical Agents, [30]) develop-ment of multiagent systems. The main functions of aJADE-based agent system can be categorized into supplyingagent behavior and agent communication. The agentscommunicate in an asynchronous, message-based fashion,using the Agent Communication Language (ACL) [30].The internal data storage (FHIRbase) of the agent system isbased upon the HL7-FHIR specification [31]. Therefore,Fig. 8 Architecture of the agent systemUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 9 of 14the data within the agent system is represented as FHIR re-sources. The agent system models, for example, the infor-mation received from OntoRiDe as FHIR RiskAssessmentResource and saves it in the FHIRbase for further analysis.We have been able to map all relevant risk information toFHIR. The input KPIs have been saved, for example, as Ris-kAssessment.basis (indicates the source data considered aspart of the assessment (FamilyHistory, Observations, Proce-dures, Conditions, etc.)), the possible adverse situations as RiskAssessment.prediction.outcome (one of the potentialoutcomes for the patient (e.g. remission, death, a particularcondition)), the probability of an adverse situation  as Ris-kAssessment.prediction.probability (how likely is the out-come), and the explanations for a detected risk  asRiskAssessment.prediction.rationale (additional informationexplaining the basis for the prediction) [31].The continuous patient-specific risk monitoring relatesto the treatment phases of the perioperative treatmentprocess. Based on the supplied phase information,OntoRiDe provides a phase-specific KPI set to the AgentController. Using this information, the Agent Controllergenerates patient-specific Data Retrieval Agents, whichmanage the KPI sets and periodically send requests tothe Data Access Agents. Those agents are specificallytailored for each data source in order to fetch data cor-rectly. The collected KPI data is sent back to therequesting Data Retrieval Agents and stored in theFHIRbase. Based on a trigger, the Risk CommunicationAgent fetches the patient-specific KPI data from thisdatabase and sends it to OntoRiDe for risk identificationpurposes. The risk reports resulting from this identifica-tion process are then forwarded to OntoRA for furtherprocessing. The purpose of OntoRA is to analyze theidentified risk situations and to provide the results in aweb interface, which can be accessed by medical staffwithin the hospital information system.Therefore, OntoRA implements a responsive, web-baseduser interface hosted on the Apache Tomcat platform [32],which allows the development of a platform-independentsolution, lowering costs and increasing flexibility.The server-sided component of the application consistsof two parts, a backend for the web content and a web ser-vice to which the agent system can send data. The webservice stores the received data in a MongoDB database[33] hosted within the hospital information system. If aclient requests data, the backend takes care of this requestby fetching the data from the database and sending it tothe client. The client-side uses a responsive approach,which allows the usage of web interfaces on multiple de-vices, such as desktop PCs, tablets, and phones. Toachieve this, a combination of HTML5 [34], JQuery [35]and Bootstrap 3 [36] is used. The user interface consists oftwo web pages, a patient overview and a page containing apatients risks, which are displayed in the users webbrowser. The user can select the patient of interest, whoserisks are to be displayed. In this view, the risks are orderedby the severity of each risk-event combination. Afterselecting a risk tile, detailed information like the risk de-scription or risk parameters are displayed (Fig. 9).The agent system is currently deployed at the JenaUniversity Hospital. Referring to Fig. 8, the hospitalinformation system in which the agent system is inte-grated into is displayed in Fig. 10. The agent system hasto collect data from various data sources within thesame subnet (1) and from a FHIR server, which holdspatient-related data (2). Because of several linked sub-nets, the agent system also has to request KPI data froma communication server (3) in order to access data fromremote data sources in different subnets.Related workSeveral approaches towards the formal representation ofrisks and adverse events through ontologies are de-scribed in the literature. We analyzed these existing on-tologies for their potential to detect perioperative risksin hospitals, but we concluded that none of these ontol-ogies and tools could be applied to our project.Bouamrane et al. [3739] report on the developmentof an ontology-based system to support clinical decisionmaking. The support is provided in a two-step process.First, the developed system calculates risk scores usingnumerical formulas. In this step, the system does notuse the developed ontology but computes numericvalues using an open-source Java-based rule engine(JBoss Rules). After calculating the relevant risk scores,the DL reasoner (Pellet) classifies the patient into severalpredefined categories for risks, recommended tests andprecaution protocols, using the OWL-DL representationof the patient medical history profile and the decisionsupport ontology. The decision support ontology is di-vided into three domains: a risk assessment ontology, arecommended test ontology and a precaution protocolontology. The aim of the risk assessment ontology is todetect potential risks of intra-operative and post-operative complications in a given formal representationof a patient medical profile.Similar to the system of Bouamrane, our approach alsoprovides two components of decision support namelyOntoRiDe and OntoRA (Fig. 8). They can perform simi-lar tasks as those of Bouamranes system. In addition,OntoRiDe will also use the self-developed RIO for riskidentification similarly to the usage of the risk assessmentontology. However, there are also important differencesbetween the two ontologies and systems. The risk assess-ment ontology focuses only on the patients risk related tointra-operative and post-operative complications such ascardio-vascular and respiratory risks, whereas RIO coversvarious risk types such as special and general treatmentUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 10 of 14risks, technical risks, organizational risks etc. Thesecond significant difference is that our approach in-tegrates the treatment process, its steps, and situa-tions in the risk conceptualization. In this way, it ispossible to analyze and identify cross process risks orrisk situations so that errors, especially in the peri-operative field, could be avoided.In [40] Third et al. describe a model for representingscientific knowledge of risk factors in medicine. Thismodel enables the clinical experts to encode the riskassociations between biological, demographic, lifestyleand environmental elements and clinical outcomes inaccordance with evidence from the clinical literature.The major advantage of our approach in comparisonFig. 10 Integration of the agent system into the hospital information system of the Jena University HospitalFig. 9 Visualization of risk information in the web interface of OntoRAUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 11 of 14with the model developed by Third is the formal repre-sentation of cross process risks that can lead to potentialadverse situations during different treatment phases. An-other added value of our approach is that it can alsocover risks related to human and environmental factorssuch as technical or organizational risks. These types ofrisks are not considered in Thirds model.In [41] an ontology of the Open Process Task Model(OPT-Model) is presented. This ontology is primaryintended as a generic knowledge base, which implementsthe various influences of processes and their relations inmedical environments, for a prospective risk analysis.The advantage of RIO over the OPT-model-ontology isthat it provides an accurate risk analysis. By using RIO,OntoRiDe is able to perform risk classifications accord-ing to the risk occurrence time. This process allows usto identify the point in time and treatment phase onwhich a risk arise. Another further benefit of RIO is theimplicitly embedded risk specification, which meets thespirit of evidence-based medicine. This implicit domainknowledge is encoded in OWL rules and can be inferredautomatically using ontological reasoning to assesscurrent perioperative risk situations.In [42] the authors report a clinical decision supportsystem (CDSS) for undergoing surgery based on domainontology and rules reasoning in the setting of hospital-ized diabetic patients. Similar to our approach this sys-tem uses logical rules to complement the domainknowledge with implicitly embedded risk specificationand clinical domain knowledge. The important upside ofour approach is that it does not make restrictions basedon certain diseases such as diabetes mellitus, whereasCDSS focuses only on glycemic management of diabeticpatients undergoing surgery.The Ontology of Adverse Events (OAE) [43] and theOntology of Vaccine Adverse Events (OVAE) [44](Marcos, Zhao, and He 2013), which was developedbased on OAE, describe data relating to adverse events.The OAE was designed to standardize and integrate datarelating to adverse events that occur after medical inter-vention. The OVAE is used for representing and analyz-ing adverse events associated with US-licensed humanvaccines. In OAE the notion adverse event is defined asa pathological bodily process that occurs after a medicalintervention (e.g., following a vaccination), while a risk isrepresented by a factor associated with the occurrenceof an adverse event. The work presented here focuses in-stead on the risk situations and proposes a genericmodel for the risk specification in the perioperative area.Thus, we do not restrict ourselves to risks that are caus-ally and exclusively related to medical interventions.Contrary to OAE, our approach also considers other risktypes such as technical and organizational risks. More-over, we use the term adverse situation in order toavoid excluding situations that are not related to medicalinterventions.We also analyzed several conversion tools such asExcel2OWL, Mapping Master and Populus [4547] fortheir potential to build an expressive formal ontologyfrom our risk specification spreadsheet, but we con-cluded that none of these tools could be applied to ourproject. In fact, our Excel spreadsheet contains domainspecific logical rules (see Figs. 4 and 6) that are not cov-ered in these software solutions. We therefore decidedto develop RIOGen, a Java tool that enables us toautomatically generate RIO entities from the risk specifi-cation template.DiscussionWe elaborated an ontological foundation of the notionof risk, upon which we developed a Risk IdentificationOntology (RIO). With help of RIO perioperative riskscan be specified, whereas OntoRiDe can be used to iden-tify risks in a given treatment situation. This allows therecognition of risk situations and supports the avoidanceof possible adverse effects or consequences. Further-more, we implemented an agent system to realize theontology-based approach. This agent system gathersduring the whole perioperative treatment process risk-relevant data from various sources and provides it forthe risk identification respectively the risk analysis in acentralized fashion. The results of those analyses aretransmitted to the medical personnel in form of contextsensitive hints and alerts.None of the presented approaches (s. Related work)can answer competency questions such as Which treat-ment situation could be a potential risk situation?,Which properties or KPIs are responsible for an actualrisk situation? and Which risk situation belongs towhich treatment phase?. The aim of RIO and OntoRiDeis to solve this issue.Our approach has the following limitations: 1. Onlyknown und specified risks can be identified by thesystem; 2. All required data (KPIs) must be available inthe respective source systems in electronic form. There-fore, the system can only react to known and correctlyspecified risks to which the required data was electronic-ally recorded.Future workFurther development of the agent system will encompassthe implementation of interfaces for different 3rd partydata sources in collaboration with their original vendors.To facilitate the expansion of the agent system, a devel-oper package for Data Access Agents will be released,providing interfaces for integrating additional datasources in conformance to the given specifications.Furthermore, it is intended to expand and to optimizeUciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 12 of 14the application of the agent system to cater foradditional use cases and to better support mobile devicesin order to provide real-time feedback and improveusability. Finally, future work could include a machine-learning approach, where the agent system recognizesadverse events by itself and derives risks, which subse-quently will be monitored to prevent the repeatedoccurrence of these adverse events.The presented Risk Identification Ontology could beused for the ontology-based analysis of clinical studiesfor different medical applications and use cases. Futurework will include further analysis and clinical evaluationstudies.Our present work raises the question of what are theformal, ontological connections between a risk, itsadverse situation and its probability. This question willalso be examined and discussed in the future.ConclusionWe developed the Risk Identification Ontology and anontology-based agent system, which can identify andanalyze risks across medical processes and supports theavoidance of errors in the perioperative setting. About20 risks relating to cochlear implantations have alreadybeen implemented. Comprehensive testing has shownthat a stable and platform-independent deployment ofall components on different virtual machines wassuccessful. Further testing using the FHIR server as asource for KPI data has illustrated the correctness of thedata collection, risk identification and risk analysiscomponents, as well as the web-based visual representa-tion of results. The test system contains a web-basedform for entering the test data sets, which are thenstored on the FHIR server. The domain experts (medicalstaff ) have tested the functionality and usability of thesystem based on practice-relevant test data. Accordingto the interviews with domain experts, the systemcurrently meets all specified requirements.AbbreviationsACL: Agent Communication Language; CDSS: Clinical decision support system;CI: Cochlear implantation; CIRS: Critical Incident Reporting System; EPC: Event-driven process chain; FHIR: Fast Healthcare Interoperability Resources;FIPA: Foundation for Intelligent Physical Agents; GFO: General Formal Ontology;IOM: Institute of Medicine of the US National Academy of Sciences; JADE: JavaAgent Development Framework; KPI: Key Performance Indicator; OAE: Ontologyof Adverse Events; OntoRA: Ontology-based Risk Analysis; OntoRiDe: Ontology-based Risk Detector; OPT-Model: Open Process Task Model; OVAE: Ontology ofVaccine Adverse Events; PRS: Potential risk situation; PTRD: Point in time of riskdetection; RIO: Risk Identification OntologyAcknowledgementsAn earlier version of the paper has been presented at ODLS 2016(Ontologies and Data in Life Sciences) in Halle (Saale).This work is a part of the OntoMedRisk project funded by the GermanFederal Ministry of Education and Research (reference number 01IS14022).We acknowledge support from the German Research Foundation (DFG) andLeipzig University within the program of Open Access Publishing.FundingThis work has been funded by the German Federal Ministry of Education andResearch (BMBF) in the KMU-innovativ funding program under referencenumber 01IS14022 as part of the OntoMedRisk project [7]. The aim of thefunding initiative is the strengthening of innovation capacity of small andmedium sized enterprises in Germany. Nevertheless, the funding bodyplayed no role in the design of the study and collection, analysis, andinterpretation of data and in writing the manuscript.Availability of data and materialsThe datasets generated and/or analyzed during the current study are notpublicly available due to them containing information that could compromiseresearch participants consent and contrast with the projects funding objectivewith the aim of strengthening the innovation capacity of small and mediumsized enterprises in Germany, but are available from the corresponding authoron reasonable request.Authors contributionsAU designed and implemented RIO, RIOGen and OntoRiDe, made substantialcontributions to conception of the risk notion and developed the methodologyfor risk specification. KT focused on the analysis and discussion of recent relatedworks and contributed to the development of RIOGen and OntoRiDe. HH wasresponsible for project management, conception of the risk notion andsemantic foundation of RIO using GFO. JN and TN performed risk assessmentfor the otorhinolaryngological use case, interpreted the clinical data regardingthe occurance of adverse situations and described the perioperative andcross-process risks factors. MS provided expertise for selection of the use case,medical/technical supervision, and usability assessment. TS has contributed byinferring/defining the correct key performance indicators and risks in theotorhinolaryngological medical domain. KS has carried out the definition ofassociated risks in a clinical environment, the configuration of the FHIR serveras well as the agent-based system and the mapping of FHIR resources at JenaUniversity Hospital. AK specified the contents and required functionalities of theagent system. SFR designed key-functionalities of the agent system and specifiedrelated dependencies. SS designed the architecture, specified the functionalities interms of the implementation and implemented the agent system. FP wasresponsible for project management and conception of OntoRA. AB developedthe communication between OntoRiDe and OntoRA and the data storage levelof the application. DS developed the user interface and the communicationbetween server and client. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),University of Leipzig, Leipzig, Germany. 2Innovation Center ComputerAssisted Surgery (ICCAS), University of Leipzig, Leipzig, Germany. 3JenaUniversity Hospital, Jena, Germany. 4GMC Systems mbH, Ilmenau, Germany.5SurgiTAIX AG, Herzogenrath, Germany.Received: 8 February 2017 Accepted: 30 August 2017The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34DOI 10.1186/s13326-017-0144-yRESEARCH Open AccessInvestigations on factors influencingHPO-based semantic similarity calculationJiajie Peng, Qianqian Li and Xuequn Shang*From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China.16 December 2016AbstractBackground: Although disease diagnosis has greatly benefited from next generation sequencing technologies, it isstill difficult to make the right diagnosis purely based on sequencing technologies for many diseases with complexphenotypes and high genetic heterogeneity. Recently, calculating Human Phenotype Ontology (HPO)-basedphenotype semantic similarity has contributed a lot for completing disease diagnosis. However, factors which affectthe accuracy of HPO-based semantic similarity have not been evaluated systematically.Results: In this study, we proposed a new framework called HPOFactor to evaluate these factors. Our model includesfour components: (1) the size of annotation set, (2) the evidence code of annotations, (3) the quality of annotationsand (4) the coverage of annotations respectively.Conclusions: HPOFactor analyzes the four factors systematically based on two kinds of experiments: causative geneprediction and disease prediction. Furthermore, semantic similarity measurement could be designed based on thecharacteristic of these factors.Keywords: Biological ontology, Semantic similarity, Human phenotype ontologyIntroductionIn the last few years, disease diagnosis has greatly ben-efited from the rapid development of next generationsequencing (NGS) technologies [13]. However, it isdifficult to make the right diagnosis purely based onsequencing technologies for many diseases with complexphenotypes and high genetic heterogeneity. Because thegenetic variants always relate to the complex clinical phe-notypic characteristics. This kind of relation is difficult tounderstand [46].Recently, tools to measure phenotypic characteristicshave received increasing attention. Patient phenotypes aredefined as the entire physical, biochemical and physiolog-ical makeup of a patient which determined by both genet-ically and environmentally [7]. Phenotype data can helppeople to understand the relation between the genetic*Correspondence: shang@nupu.edu.cnNorthwestern Polytechnical University, 127 West Youyi Road, 710072 Xian,Chinavariances and biological process activities. Advanced phe-notype data analysis have played an important role inexplaining gene function and understanding biologicalmechanism in biomedical research [811]. One of thekey steps in phenotype data analysis is to precisely mea-sure the similarity between phenotypes, and combine thisknowledge with the disease diagnosis process to improvedisease diagnosis efficiency. Therefore, a formal and con-trolled vocabulary is required to unify the representationof phenotypes and phenotype attributes.It has been proved in many applications that ontologyis effective to represent biomedical information as termsand their directed relationships with a directed acyclicgraph (DAG) [1218]. In order to meet the demand, anontology called Human Phenotype Ontology (HPO) wasconstructed to describe the abnormal human phenotypesencountered in human Mendelian disease by Robinsonet al. in 2008 [7]. Currently, HPO has been widely usedto provide the unified and structured vocabulary to rep-resent the phenotypic features encountered in human© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 62 of 79diseases [19]. HPO is always combined with next gen-eration sequencing data analysis to support the humandisease diagnosis [20, 21].In order to improve diagnostic efficiency, many compu-tational methods have been proposed to measure the phe-notypic similarity between patient and historical diseasedata (or genes) [22, 23]. Among these computational mea-surements, calculating HPO-based phenotype semanticsimilarity has played an important role in completingdisease diagnosis process.Recently, several measurements have been developedto compute HPO-based phenotype semantic similarity[2325]. Although ontology-based semantic similaritymeasurement has been extensively studied in the last tenyears [2633], it is still a difficult task to measure thephenotype similarity based on HPO structure and anno-tations. The reason is that many factors could affect theaccuracy of HPO-based phenotype semantic similarity,such as the number of annotations per gene/disease, theevidence code of annotations, the coverage of annotationsand the quality of annotations [22].To figure out how different factors affect the perfor-mance of ontology-based semantic similarity measure-ment, some methods have been proposed to evaluatedifferent involved factors. To test whether different edi-tions of Gene Ontology (GO) would result in differentsemantic similarities, Gillis et al. proposed an evaluationframework based on protein interaction networks [34].The result shows that 3 and 20% of genes are not semanti-cally similar to themselves between monthly GO editionsand between biennially GO editions. The semantic sim-ilarities are only stable over short-period GO editions.Skunca et al. proposed a novel method to systematicallyevaluate the quality of the computationally inferred GOannotations [35]. The reliability of electronic GO anno-tations is defined as the proportion of electronic annota-tions confirmed by the experimental annotations in thefuture release of GO. The coverage is defined as the pro-portion of experimental annotations predicted by the elec-tronic annotations in an older release of GO. The resultshows that the electronic GO annotations have high qual-ity, which could lead to accurate semantic similarity. Bothof the aforementioned methods are based on the histori-cal versions of ontology. These methods cannot be used toevaluate the factors that affect the performance of HPO-based semantic similarity measurement, since the histor-ical versions of HPO are not available currently (personalcommunication with the founder of HPO). Furthermore,other factors may also affect the accuracy of HPO-basedsemantic similarity. First, HPO contains large numbers ofannotations with different evidence code indicating thedifferent levels of evidences supporting the annotation.Second, HPO is a growing data source. The coverage andquality of annotations may vary with the updating of HPOdata source. Third, the number of HPO terms annotatingdifferent diseases/genes may be different. These factorsare all related to the HPO-based semantic similarity calcu-lation. It is difficult to evaluate each factor individually. Itis challenging and demanding to test whether these factorswould affect the accuracy of HPO-based semantic simi-larity. The evaluation of different factors may guide thedesign of HPO-based semantic similarity measurementand support the human disease diagnosis. However, to thebest of our knowledge, no method has been proposed toevaluate the factors that affect the accuracy of HPO-basedsemantic similarity.In this article, we proposed a new framework namedHPOFactor to evaluate the effect of four factors involvedin the HPO-based semantic similarity calculation sep-arately. The contribution of our present study are asfollows. To the best of our knowledge, HPOFactor is the firstframework that is specially designed for evaluatingthe factors involved in HPO-based semanticsimilarity calculation; We develop a method to generate different versionsof the HPO annotations with different coverage andquality levels; We test the minimal size of annotation set that doesnot affect the accuracy of HPO-based semanticsimilarity.MethodsWe proposed HPOFactor, a new framework to evalu-ate the factors that affect the performance of phenotypesemantic similarity measurement based on human phe-notype ontology (HPO). The proposed framework hasfour parts. First, it tested whether changing the sizeof phenotype annotations would affect the performanceof phenotype semantic similarity measurement. Second,it tested whether using annotations with different evi-dence codes would affect the performance of phenotypesemantic similarity measurement. Third, it tested whetherchanging the annotation coverage would affect the per-formance of phenotype semantic similarity measurementby randomly deleting the HPO annotations. Last, it testedwhether varying the quality of HPO annotations wouldaffect the performance of phenotype semantic similaritymeasurement by randomly swapping the existing annota-tions of different HPO terms. The diagram of the wholeframework is shown in Fig. 1.Calculating HPO-based semantic similarityHPO provides a structured and controlled vocabulary todescribe the human phenotypes and the genes/diseasesassociated with the phenotypes [7]. Using the unifieddescription from HPO, the semantic similarity betweenThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 63 of 79Fig. 1 -The workflow of HPOFactorgene and patient or between disease and patient can becalculated. Based on the HPO-based semantic similar-ity, we can predict whether a patient associates with agene or has certain disease. For example, we can rankthe candidate genes based on its similarity with thepatient to predict the patient-associated genes. The phe-notypes of a patient can be observed in clinical treat-ment and the gene/disease phenotype set can be obtainedfrom database like HPO. Since the phenotype sets ofpatient, gene and disease are all able to be unified byHPO terms, calculating the similarity between patient andgene/disease is equal to calculating the similarity betweentwo sets of HPO terms.Let P1 and P2 be two phenotype term sets correspond-ing to a patient and a disease (or gene) respectively. P1represents a set of phenotype terms of a patient observedin clinical treatment. P2 represents a set of phenotypeterms of a disease (or gene) obtained from HPO database.Adopting the approach in [22], the semantic similar-ity between a patient and a gene (or disease) can becalculated by aggregating the pair-wise phenotype simi-larity between terms across P1 and P2. Given two phe-notype sets, their HPO-based similarity is calculated asfollows.sim (P1,P2) =12 × simset (P1 ? P2) +12× simset (P2 ? P1)(1)where simset(P1 ? P2) represents the similarity fromP1 to P2. For each phenotype p1 in P1, we calculate thesimilarity between p1 and each phenotype in P2. Thenthe highest similarity score is selected as the similar-ity between p1 and phenotype set P2. The average ofall similarities between each phenotype in P1 and P2 isdefined as the similarity from P1 to P2. Mathematically,simset(P1 ? P2) is defined as follows.simset(P1 ? P2)=avg?? ?p1?P1maxp2?P2simterm(p1, p2)??(2)where simterm(p1, p2) represents semantic similaritybetween two phenotypes p1 and p2. It is noted that thesimilarity from phenotype set P1 to P2 is different fromthe similarity from phenotype set P2 to P1. Therefore,Eq. 1 averages the two dissymmetric similarities as thesimilarity between two phenotype sets.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 64 of 79To calculate simterm(p1, p2), let S(p1, p2) be the set of allcommon ancestors of p1 and p2. pmin is the term that hasthe minimal annotations in S(p1, p2). Given two pheno-types p1 and p2, their similarity simterm(p1, p2) is definedas follows.simterm(p1, p2) = ? log NpminN (3)where Npmin is the number of annotations of pmin (includ-ing annotations of itself and its descendants) and N is thetotal number of annotations involved in HPO.Based on this semantic similarity measurement, we willevaluate the factors that affect HPO-based semantic simi-larity in the following subsections.Test the effect of the size of annotation setIn the process of calculating semantic similarity measure-ment introduced in last subsection, one of the key factorsis the size of annotation set of compared genes or diseases.The size is usually large in the HPO branches for thosewell studied ones. Therefore, the size of annotation set isnot a stable factor in the semantic similarity calculation.In this subsection, we proposed a method to test whetherthe size of annotation set would affect the precision ofsemantic similarity.Given a set of query patients Q, each element q in Qhas an annotation set obtained from clinical treatmentsaved as Pq. Given a set of genes/diseases H involved inHPO database, each element h in H has an annotation setobtained fromHPO database saved as Ph. We changed thesize threshold of annotations s and calculate the seman-tic similarity at different thresholds. Given the threshold s,the detail of the method is described as follows. For eachelement h in H, we randomly selected s phenotypes fromPh, saved as Psh. This step is represented mathematically inEq. 4.Psh = RandomSelection(Ph, s) (4)For each query patient q inQ, we calculate the similaritybetween Pq and Psh for each h in H using Eq. 1. Then, weranked all elements in H based on the similarities with Pqsaved as Horder (see Eq. 5).Horder = Rank(H , {y|y = sim(Pq,Psh), h ? H})(5)At last, we can test whether the known patient associ-ated element (gene or disease) has a high rank in Horder .The higher the rank is, the better the performance of thesemantic similarity measurement is.Test the effect of annotationswith different evidence codesIn HPO, the annotations are supported by different evi-dences. When the HPO project was initialed, most anno-tations were extracted from the OMIM database [36]by parsing the clinical features. These annotations arelabeled by the evidence code IEA representing inferredfrom electronic annotation. There are also other evi-dences, such as PCS representing inferred from publicclinical study and biomedical literature, ICE representinginferred from individual clinical experience, ITM rep-resenting inferred by text-mining technique and TASrepresenting inferred from traceable author statement.In this subsection, we test whether using different anno-tations with different evidence codes would affect theprecision of HPO-based semantic similarity. First, theannotations in HPO are grouped based on the evidencecodes. Given the annotation setA,Ae represents the anno-tation set with evidence e. For each evidence code e, weonly use annotations contained in Ae to calculate thesemantic similarity between phenotypes. Given a set ofgenes/diseases H, the annotation set of each element h inH is obtained from Ae, saved as Phe. Similar with the pro-cess described in last subsection, we rank all elements inH based on the similarities with the phenotypes of querypatient.Horder = Rank(H , {y|y = sim(Pq,Phe), h ? H})(6)Finally, we could see which evidence code can lead the bestperformance.Test the effect of annotation qualityTo determine whether annotation quality was one ofthe factors that control the performance of HPO-basedsemantic similarity, we re-ran semantic similarity mea-surement by varying the quality of HPO annotation. Tothis end, we varied the HPO annotation quality by ran-domly swapping the phenotype-annotation associationsin HPO. For example, assume that d1 ? p1 and d2 ?p2 are two disease-phenotype pairs randomly selectedfrom HPO. After the swapping process, we get two newpairs d1 ? p2 and d2 ? p1 to replace the originaltwo pairs. Given the original HPO annotation set A, wecan generate a low quality set Au by randomly swappingthe phenotype-annotation associations. To make surethe quality be decreased, the new generated phenotype-annotation associations should not be contained in theset of original HPO phenotype-annotation associations.u represents different quality levels, such as swapping20% phenotype-annotation associations, 40% phenotype-annotation associations.Au has the same size with A butdifferent quality level. For each low quality level u, we usethe low quality annotation setAu to calculate the semanticsimilarity between phenotypes. The annotation set of eachelement h in H is got from Au, saved as Phu. Comparisonof the performance of semantic similarity using annota-tion sets with different quality level could test whetherthe annotation quality was a key factor of the HPO-basedsemantic similarity measurement.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 65 of 79Test the effect of annotation coverageCurrently, HPO is not complete. Much unknown knowl-edge and knowledge in the literature are not includedin the HPO database. Therefore, it is critical to testwhether annotation coverage was a key factor for HPO-based semantic similarity measurement. To this end, werandomly delete the annotations from annotation set Ato generate a low coverage annotation set Ac. c repre-sents different coverage levels, such as randomly delet-ing 20% of the annotations in A, deleting 40% of theannotations in A. For each coverage level c, we usethe low coverage annotation set Ac to calculate thesemantic similarity between phenotypes. Given a set ofgenes/diseases H, the annotation set of each element hin H is obtained from Ac, saved as Phc. By comparingthe results on the annotation sets with different cover-age levels, we can test whether the annotation cover-age is a key factor for HPO-based semantic similaritycalculation.ResultsData preparationThe Human Phenotype Ontology (HPO) data used inour experiment was downloaded from the HPO officialwebsite (http://human-phenotype-ontology.github.io/)on April 1st, 2016. It includes 459,452 gene annota-tions and 78,313 disease annotations. HPOFactor wasimplemented with Python language.We used the curated clinical phenotype features in[22] to generate simulated patients for experiments. Theassociated phenotypes, disease causative genes and pene-trance of each phenotype of the diseases are available inthe dataset. For each disease, we simulated 100 patients.The simulation process is described as follows. To con-sider the gender-specificity of phenotypes, we first sim-ulated the gender of each patient. A random number fgwas generated. Then, the patients gender is assigned asfollows:{fg > 0.5 ,malefg ? 0.5 ,famale (7)Second, given a phenotype p of a patient, a ran-dom number rp was generated. Let fp be the pene-trance of this phenotype associated with the assigneddisease. If rp < fp, the phenotype p was assignedto the patient. It is noted that each simulated patientmust have at least one phenotype. Finally, 3300 patientswas generated. For each patient, we know its diseasecausative gene and associated disease. Therefore, weadopted the evaluation criterion from [22] to test whetherthe causative gene or associated disease of a patientcan be identified based on the HPO-based semanticsimilarity.Evaluation for the size of annotation setIn this experiment, we compared the results of using dif-ferent sizes of annotation set to identify the disease associ-ated with the patient. The size threshold s used in Eq. 4 isfrom 1 to 10. The result shows that the patient associateddiseases have low ranks when the number of annotationsis small, indicating low performance (see Fig. 2). Partic-ularly, when s = 1 and s = 2, the ranks of most truepatient associated diseases are lower than the 450. Figure 2shows that the performance improved with the increaseof the size of annotation set. Noted that the performancebecome stable when s > 5.We also compared the results of using different sizesof annotation set to identify the causative gene. Thegene annotations in HPO are richer than the diseaseannotations (see the Data preparation subsection). Tosee the global distribution, we set the gene set thresh-old s as {1, 5, 10, . . . , 45, 50}. Similar with the result ofidentifying disease, the causative genes have low rankswhen the number of annotations is small (see Fig. 3).When s = 1, the ranks of most causative genes arelower than 500. It is shown that the performance ofHPO-based semantic similarity improved steadily withthe increase of the number of annotations. The perfor-mance keeps stable when the size of annotations is largerthan 25.The result shows an important guidance for the HPO-based semantic similarity calculation that the result maybe more reliable when the number of annotations is largeenough.Evaluation for the annotations with different evidencecodesIn this part, we test whether using annotations with dif-ferent evidence codes would affect result of identifyingthe disease associated with the patient. We do not testthe performance for causative gene identification sincethe gene annotations in HPO do not have evidence codescurrently. We only compare three evidence codes: IEA,TAS and PCS, since other evidence codes do not haveenough number of annotations. To avoid the bias result-ing from the lack of annotation, we did the experimenton the size of annotations sets which are larger than 5.We choose the size threshold since the experiment inlast subsection shows that the performance become stablewhen s > 5.Figure 4 shows that using annotations with PCS evi-dence code performs better than using the annotationswith IEA and TAS evidence code. Specifically, whenthe ranking threshold is 5, the ratio of patients forPCS is 0.993, which is higher than IEA and TAS (thenumber is 0.901 and 0.580 respectively). The ratio ofpatients for PCS is 0.997, when the ranking thresholdis 10. In comparison, the ratios of patients satisfyingThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 66 of 79Fig. 2 -The rank of disease by changing the size of phenotype annotation set. The x-axis is the number of HPO annotations. The y-axis is the rank ofdisease associated with the query patientFig. 3 -The rank of causative gene by changing the size of phenotype annotation set. The x-axis is the number of HPO annotations. The y-axis is therank of causative gene of the query patientThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 67 of 79Fig. 4 -The rank of disease by the phenotype with different evidence code. The x-axis is the ranking threshold for the disease. The y-axis is the ratioof patients satisfying the ranking thresholdthe threshold are 0.906 and 0.609 for IEA and TASrespectively.Evaluation for the annotation qualityTo test the effect of annotation quality to the perfor-mance of HPO-based semantic similarity, we comparedthe results of using annotation sets with different quali-ties to identify the patient associated diseases (Fig. 5(a)) orcausative genes (Fig. 5(b)). Overall, the result shows thatthe performance goes downwith the decrease of the anno-tation quality in both experiments. It is shows that theperformance decreases significantly when more than 40%annotations become noise.In the associated disease identification experiment,when the ranking threshold is 10, the ratio of patients sat-isfying the threshold is 0.967 for original annotation set.In comparison, the ratios of patients satisfying the thresh-old are 0.933, 0.862, 0.637 and 0.198 for annotation setswith 20%, 40%, 60% and 80% noise respectively. Further-more, the statistical test shows that the result for originalannotation set is significantly different with 40%, 60% and80% set (Tukey test, p-value < 0.05).Fig. 5 -The rank of disease (a) and causative gene (b) by varying the quality of phenotype annotations. The x-axis is the ranking threshold for thedisease/causative gene. The y-axis is the ratio of patients satisfying the ranking thresholdThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 68 of 79Fig. 6 -The rank of disease (a) and causative gene (b) by changing the coverage of phenotype annotations. The x-axis is the ranking threshold forthe disease/causative gene. The y-axis is the ratio of patients satisfying the ranking thresholdIn the causative gene identification experiment, whenthe ranking threshold is 10, the ratio of patients satisfy-ing the threshold is 0.951 for original annotation set. Incomparison, the ratios of patients satisfying the thresholdare 0.909, 0.863, 0.496 and 0.005 for annotation sets with20%, 40%, 60% and 80% noise respectively. Furthermore,the statistical test shows that the result for original anno-tation set is significantly different with 40%, 60% and 80%set (Tukey test, p-value < 0.05).Evaluation for the annotation coverageTo test the effect of annotation coverage to the perfor-mance of HPO-based semantic similarity, we randomlydelete the annotations and use annotation sets with differ-ent coverage levels to identify the associated disease andcausative genes. The result shows that the performance ofHPO-based semantic similarity decreased with the reduc-tion of the annotations (Fig. 6(a) and (b)). However, therewas no significant difference when the deleted annota-tions are less than 60% (Tukey test, p-value > 0.05). Itindicates that HPO-based semantic similarity is more sen-sitive to the quality of annotations than the coverage ofannotations.DiscussionIn this article, we proposed a novel framework calledHPOFactor to evaluate the factors that may affect theaccuracy of HPO-based semantic similarity. HPOFactorevaluates four factors involved in the HPO-based seman-tic similarity: size of annotation set, evidence code ofannotations, quality of annotations and coverage of anno-tations. Particularly, we found the performance of HPO-based semantic similarity decreased steadily with thereduction of coverage and quality of annotations. Therewas no significant difference among different coveragelevels (p-value > 0.05), but there was significant differenceamong different quality levels (p-value < 0.05), indicatingthat quality is more important than coverage. This isimportant because not all human diseases and genes areannotated in current HPO, but existing annotations inHPO have high quality.ConclusionRecently, the rapid development of next generationsequencing techniques have significantly accelerated dis-ease diagnosis. However, it remains challenging to makethe right diagnosis formany diseases with complex pheno-types and high genetic heterogeneity. Hence, HPO-basedphenotype similarity become an important part of com-pleting disease diagnosis.The evaluation result can make the HPO-based seman-tic similarity better used in phenotype-based causativegene prediction and disease prediction. In the future, wewill evaluate the combination effects of different factorson HPO-based semantic similarity. Furthermore, we willdesign semantic similarity measurement based on thecharacteristic of these factors.FundingThis project has been funded by the National Natural Science Foundation ofChina (Grant No. 61332014, 61272121); the Start Up Funding of theNorthwestern Polytechnical University (Grant No. G2016KY0301); theFundamental Research Funds for the Central Universities (Grant No.3102016QD003). The publication costs for this article were funded byNorthwestern Polytechnical University.Availability of data andmaterialsThe datasets during and/or analysed during the current study available fromthe corresponding author on reasonable request.About this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Ethics approval and consent to participateNot applicable.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 69 of 79Authors contributionsJP and XS conceived the project; JP and QL designed the algorithm andexperiments; JP and QL wrote this manuscript. All authors read and approvedthe final manuscript.Consent for publicationNot applicableCompeting interestsThe authors declare that there are no competing interests.Published: 20 September 2017RESEARCH Open AccessDrug target ontology to classify andintegrate drug discovery dataYu Lin1, Saurabh Mehta1,3, Hande Küçük-McGinty1,2, John Paul Turner5, Dusica Vidovic1,5, Michele Forlin1,5,Amar Koleti1, Dac-Trung Nguyen7, Lars Juhl Jensen6, Rajarshi Guha7, Stephen L. Mathias4, Oleg Ursu4,Vasileios Stathias5, Jianbin Duan1,2, Nooshin Nabizadeh1, Caty Chung1, Christopher Mader1, Ubbo Visser2,Jeremy J. Yang4, Cristian G. Bologa4, Tudor I. Oprea4* and Stephan C. Schürer1,5*AbstractBackground: One of the most successful approaches to develop new small molecule therapeutics has been tostart from a validated druggable protein target. However, only a small subset of potentially druggable targets hasattracted significant research and development resources. The Illuminating the Druggable Genome (IDG) projectdevelops resources to catalyze the development of likely targetable, yet currently understudied prospective drugtargets. A central component of the IDG program is a comprehensive knowledge resource of the druggablegenome.Results: As part of that effort, we have developed a framework to integrate, navigate, and analyze drug discoverydata based on formalized and standardized classifications and annotations of druggable protein targets, the DrugTarget Ontology (DTO). DTO was constructed by extensive curation and consolidation of various resources. DTOclassifies the four major drug target protein families, GPCRs, kinases, ion channels and nuclear receptors, based onphylogenecity, function, target development level, disease association, tissue expression, chemical ligand and substratecharacteristics, and target-family specific characteristics. The formal ontology was built using a new software tool toauto-generate most axioms from a database while supporting manual knowledge acquisition. A modular, hierarchicalimplementation facilitate ontology development and maintenance and makes use of various external ontologies, thusintegrating the DTO into the ecosystem of biomedical ontologies. As a formal OWL-DL ontology, DTO contains assertedand inferred axioms. Modeling data from the Library of Integrated Network-based Cellular Signatures (LINCS) programillustrates the potential of DTO for contextual data integration and nuanced definition of important drug targetcharacteristics. DTO has been implemented in the IDG user interface Portal, Pharos and the TIN-X explorer of proteintarget disease relationships.Conclusions: DTO was built based on the need for a formal semantic model for druggable targets including variousrelated information such as protein, gene, protein domain, protein structure, binding site, small molecule drug,mechanism of action, protein tissue localization, disease association, and many other types of information. DTOwill further facilitate the otherwise challenging integration and formal linking to biological assays, phenotypes,disease models, drug poly-pharmacology, binding kinetics and many other processes, functions and qualitiesthat are at the core of drug discovery. The first version of DTO is publically available via the website http://drugtargetontology.org/, Github (http://github.com/DrugTargetOntology/DTO), and the NCBO Bioportal(http://bioportal.bioontology.org/ontologies/DTO). The long-term goal of DTO is to provide such anintegrative framework and to populate the ontology with this information as a community resource.* Correspondence: toprea@salud.unm.edu; sschurer@miami.eduEqual contributors4Department of Internal Medicine, Translational Informatics Division,University of New Mexico School of Medicine, Albuquerque, NM, USA1Center for Computational Science, University of Miami, Coral Gables, FL, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Lin et al. Journal of Biomedical Semantics  (2017) 8:50 DOI 10.1186/s13326-017-0161-xBackgroundThe development and approval of novel small moleculetherapeutics (drugs) is highly complex and exceedinglyresource intensive, being estimated at over one billiondollars for a new FDA approved drug. The primary reasonfor attrition in clinical trials is the lack of efficacy, whichhas been associated with poor or biased target selection [1].Although the drug target mechanism of action is not re-quired for FDA approval, a target-based mechanistic under-standing of diseases and drug action is highly desirable anda preferred approach of drug development in the pharma-ceutical industry. Following the advent of the HumanGenome, several research groups in academia as well asindustry have focused on the druggable genome i.e. thesubsets of genes in the human genome that express pro-teins that have the ability to bind drug-like small molecules[2]. The researchers have estimated the number of drug-gable targets ranging from few hundreds to several thou-sands [3]. Furthermore, it has been suggested by severalanalyses that only a small fraction of likely relevant drug-gable targets are extensively studied, leaving a potentiallyhuge treasure trove of promising, yet understudied (dark)drug targets to be explored by pharmaceutical companiesand academic drug discovery researchers. Not only is thereambiguity about the number of the druggable targets, butthere is also a need of systematic characterization and an-notation of the druggable genome. A few research groupshave made efforts to address these issues and have indeeddeveloped several useful resources, e.g. IUPHAR/BPS Guideto PHARMACOLOGY (GtoPdb/IUPHAR) [4], PANTHER[5], Therapeutic Target Database (TTD) [6], Potential DrugTarget Database (PDTD) [7], covering important aspects ofthe drug targets. However, to the best of our knowledge, apublically available structured knowledge resource of drugtarget classifications and relevant annotations for the mostimportant protein families, one that facilitates querying, dataintegration, re-use, and analysis does not currently exist.Content in the above-mentioned databases is scattered andin some cases inconsistent and duplicated, complicating dataintegration and analysis.The Illuminating the Druggable Genome (IDG) project(http://targetcentral.ws/) has the goal to identify andprioritize new prospective drug targets among likely tar-getable, yet currently poorly or not at all annotated pro-teins; and by doing so to catalyze the development ofnovel drugs with new mechanisms of action. Data com-piled and analyzed by the IDG Knowledge ManagementCenter (IDG-KMC) shows that the globally marketeddrugs stem from only 3% of the human proteome. Theseresults also suggest that the substantial knowledge deficitfor understudied drug targets may be due to an unevendistribution of information and resources [8].In the context of the IDG program we have been devel-oping the Drug Target Ontology (DTO). Formal ontologieshave been quite useful to facilitate harmonization, integra-tion, and analysis of diverse data in the biomedical andother domains. DTO integrates and harmonizes knowledgeof the most important druggable protein families: kinases,GPCRs, ion channels and nuclear hormone receptors.DTO content was curated from several resources and theliterature, and includes detailed hierarchical classificationsof proteins and genes, tissue localization, disease associ-ation, drug target development level, protein domain infor-mation, ligands, substrates, and other types of relevantinformation. DTO content sources were chosen by domainexperts based on relevance, coverage and completeness ofthe information available through them. Most resourcesMork et al. Journal of Biomedical Semantics  (2017) 8:8 DOI 10.1186/s13326-017-0113-5RESEARCH Open Access12 years on  Is the NLMmedical textindexer still useful and relevant?James Mork* , Alan Aronson and Dina Demner-FushmanAbstractBackground: Facing a growing workload and dwindling resources, the US National Library of Medicine (NLM)created the Indexing Initiative project in 1996. This cross-library teams mission is to explore indexing methodologiesfor ensuring quality and currency of NLM document collections. The NLM Medical Text Indexer (MTI) is the mainproduct of this project and has been providing automated indexing recommendations since 2002. After all of thistime, the questions arise whether MTI is still useful and relevant.Methods: To answer the question about MTI usefulness, we track a wide variety of statistics related to howfrequently MEDLINE indexers refer to MTI recommendations, how well MTI performs against human indexing, andhow often MTI is used. To answer the question of MTI relevancy compared to other available tools, we haveparticipated in the 2013 and 2014 BioASQ Challenges. The BioASQ Challenges have provided us with an unbiasedcomparison between the MTI system and other systems performing the same task.Results: Indexers have continually increased their use of MTI recommendations over the years from 15.75% of thearticles they index in 2002 to 62.44% in 2014 showing that the indexers find MTI to be increasingly useful. The MTIperformance statistics show significant improvement in Precision (+0.2992) and F1 (+0.1997) with modest gains inRecall (+0.0454) over the years. MTI consistency is comparable to the available indexer consistency studies. MTIperformed well in both of the BioASQ Challenges ranking within the top tier teams.Conclusions: Based on our findings, yes, MTI is still relevant and useful, and needs to be improved and expanded.The BioASQ Challenge results have shown that we need to incorporate more machine learning into MTI while stillretaining the indexing rules that have earned MTI the indexers trust over the years. We also need to expand MTIthrough the use of full text, when and where it is available, to provide coverage of indexing terms that are typicallyonly found in the full text. The role of MTI at NLM is also expanding into new areas, further reinforcing the idea thatMTI is increasingly useful and relevant.Keywords: Indexing methods, Text categorization, MeSH, MEDLINE, Machine learning, BioASQBackgroundFor more than 150 years, the US National Library ofMedicine (NLM) has provided access to the biomedicalliterature through the analytical efforts of human index-ers. Since 1966, access has been provided in the form ofelectronically searchable document surrogates consistingof bibliographic citations, descriptors assigned by index-ers from the Medical Subject Headings (MeSH®) [1] con-trolled vocabulary and, since 1975, author abstracts formany citations.*Correspondence: jmork@mail.nlm.nih.govUS National Library of Medicine, 8600 Rockville Pike, Bethesda, USAThe MEDLINE®/PubMed® database (MEDLINE) con-tains over 23 million citations. It currently grows at therate of about 760,000 citations per year and covers over5600 international biomedical journals in 36 languages.Human indexing consists of reviewing the full text ofeach article, rather than just the abstract or summary,and assigning Descriptors from theMeSH vocabulary thatrepresent the central concepts as well as every other topicthat is discussed to a significant extent.MeSH vocabularyIn the 2015 MeSH vocabulary, there are 27,455 Descrip-tors, which are often referred to as MeSH Headings (e.g.,© The Author(s). 2017 COPYRIGHT NOTICE. The article is a work of the United States Government; Title 17 U.S.C 105 provides thatcopyright protection is not available for any work of the United States government in the United States. Additionally, this is anopen access article distributed under the terms of the Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0), which permits worldwide unrestricted use, distribution, and reproduction in anymedium for any lawful purpose.Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 2 of 10Lung). The scope of main heading descriptors may berefined further by selections from a collection of 83 top-ical MeSH Subheadings which are also known as Qual-ifiers (e.g., Lung/abnormalities means that the article isabout the abnormalities associated with the Lung morethan the Lung itself ). In addition the vocabulary con-tains 225,067 Supplementary Concept Records (formerlycalled Supplementary Chemicals) consisting of chemicals,drugs, proteins, and diseases. Each Supplementary Con-cept Record is linked to one or more MeSH Heading viatheir Heading Mapped to entries (e.g., Achondroplasticdwarfism is linked to MeSH Main Heading Achondropla-sia). MeSH Check Tags are a special type of MeSH Head-ing that are required to be included for each article andcover species, sex, human age groups, and pregnancy (e.g.,Male) [2].Impact of MEDLINE indexingSince 1990, there has been a steady and sizeable increasein the number of articles indexed for MEDLINE, becauseof both an increase in the number of in-scope articles injournals that are already being indexed and, to a lesserextent an increase in the number of indexed journals.NLM expects to index over one million articles annuallywithin a few years.MEDLINE Indexing has been used by librarians andresearchers from its inception in 1879 by John ShawBillings [3] and is currently used by an even larger com-munity through PubMed [4]. PubMed uses the MED-LINE Indexing as part of their Automatic Term Mappingquery expansion [5] and through their result filteringwhich depends on MEDLINE Indexing for determiningspecies, sex, and ages [6]. Other recent examples of spe-cific uses of MEDLINE Indexing include the results ofTREC Genomics track (2003  2007) [7] and TREC Clin-ical Decision Support track (2014 - ongoing) [8] whichshow that the judicial use of manual MEDLINE indexingin faceted retrieval or for query expansion leads to at leastmoderate, and in some cases to significant improvementsin Mean Average Precision (MAP). For example, fusion ofan implementation of Okapi BM25 ranking function withBoolean searches for gene names in MeSH fields resultedin 71.5% improvement in MAP over the Okapi rankingfunction alone and placed third in the 2003 Genomicstrack evaluation [9].To cope with the workload growth that outpaces thegrowth of resources, NLM started the Indexing Initiativeproject in 1996. This cross-library team is tasked withexploring and implementing indexing methodologies toensure that MEDLINE and other NLM document col-lections maintain their quality and currency and therebycontribute to NLMs mission of maintaining quality accessto the biomedical literature.NLMmedical text indexerThe NLM Medical Text Indexer (MTI) is the main prod-uct of the Indexing Initiative and has been providingindexing recommendations based on the MeSH vocabu-lary since 2002. In 2011, NLM expanded MTIs role bydesignating a select set of journals where MTI performsparticularly well as MTI first-line (MTIFL) journals. Theinitial list of 14 MTIFL journals has grown to include 230journals in 2014. In 2014, MeSH on Demand [10] wasdeveloped in collaboration with the NLM MeSH Sectionproviding a simplified user interface toMTI. In its first fullmonth of operation, the interface provided MeSH-basedkey terms for 140,940 English text documents submittedto it. MTI was also used on a regular basis between 2002and 2012 to provide fully-automated keyword indexing forNLMs Gateway [11] meeting abstract collection, whichwas not manually indexed.MTI produces semi-automated indexing recommenda-tions based on the MeSH controlled vocabulary and is indaily use to assist Indexers, Catalogers, and NLMs His-tory of Medicine Division (HMD) in their subject analysisefforts. Although mainly used in indexing efforts for pro-cessing MEDLINE citations [12] consisting of identifier,title, and abstract, MTI is also capable of processing arbi-trary text, which is the primary mode of text processedby the new MeSH on Demand interface. MTI providesan ordered list of MeSH Main Headings, Subheadings(MEDLINE processing only), and Check Tags as a finalresult.The NLM Medical Text Indexer (MTI) [13] combinesand ranks terms suggested by three modules depictedin Fig. 1. Figure 1 also shows the logic flow as text isprocessed through the various components of the MTIsystem. Each of the major MTI components is very brieflydescribed below.MetaMap indexing [14]A method that applies a ranking function to UMLSMetathesaurus concepts [15] identified by MetaMap [16].The Restrict to MeSH [17] mapping algorithm whichfinds the closest matching MeSH Heading(s) to a UMLSMetathesaurus concept is used by MTI to map the UMLSMetathesaurus concepts identified by MetaMap Indexingto the required MeSH Descriptors.PubMed related citations [18]The related citations of a document are those documentsin theMEDLINE/PubMed database that are themost sim-ilar to it. MTI simply requests a list of PubMed UniqueIdentifiers (PMID) for these related citations that havebeen indexed and then extracts the MeSH Descriptorsfrom each of the citations.Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 3 of 10Fig. 1MTI processing flow diagramMachine learning [1921]Twelve of the 40 MeSH Terms listed in Table 1 that MTIconsiders Check Tags (Adolescent; Adult; Aged; Aged, 80and over; Child, Preschool; Female; Humans; Infant; Male;Middle Aged; Swine; and Young Adult) are reliably (correct80.62% of the time) identified using a machine learningalgorithm that is trained on citations in the MEDLINEdatabase that were indexed in the last three years. Thesetwelve terms used for Machine Learning are highlightedin bold text in Table 1.Once MTI has the set of ranked lists of MeSH MainHeadings produced by the methods described so far, thevarious lists must be clustered into a single ranked list ofrecommendations through our Clustering and RankingModule [22]. Once all of the recommendations are rankedand selected, MTI has a post processing feature that val-idates all of the recommendations and adds or removesselect terms based on the targeted end-user. Full end-to-end processing of MEDLINE citations takes approxi-mately 30 - 45 seconds depending on citation length andcomplexity.In addition to MEDLINE processing, current uses ofMTI where the filtering and results are specifically tunedincludeMTI First Line (MTIFL) andMeSHonDemand.The human curation of MTIFL results is called MTIFLCompletion. MTIFL Completion starts with MTIFL pro-viding the initial indexing for a citation and then a humanindexer completes the indexing process by adding anymissed terms and removing any incorrect terms providedby MTIFL. The MTIFL Completion citation then goesthrough the normal manual review process. MeSH onDemand [10] is a new use of MTI added in 2014 in collab-oration with the NLM MeSH Section. MeSH on Demandis a very simplified interface to the MTI system. TheMeSH on Demand interface allows users to provide anytext (e.g., MEDLINE citation or free text) as input andprovides a list of relevant MeSH Descriptors and MeSHSupplementary Concepts that summarizes the input textand a list of the top ten citations related to the text inPubMed as a result. These results are very heavily filteredin favour of terms with high confidence. Although thesenew uses of MTI are qualitative indicators of its potentialMork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 4 of 10Table 1 MeSH terms MTI considers check tagAdolescent History, 18th CenturyAdult History, 19th CenturyAged History, 20th CenturyAged, 80 and over History, 21st CenturyAnimals History, AncientBees History, MedievalCats HorsesCattle HumansCercopithecus aethiops InfantChick Embryo Infant, NewbornChild MaleChild, Preschool MiceCricetinae Middle AgedDogs PregnancyFemale RabbitsGuinea Pigs RatsHistory of Medicine SheepHistory, 15th Century SwineHistory, 16th Century United StatesHistory, 17th Century Young AdultAll bolded check tags represent machine learning suggested check tagsusefulness, the goal of this work is to quantitatively esti-mate the MTI use and evaluate the quality of its servicescompared to other available tools. This paper presentsour internal log-based evaluation of MTI as well as theresults of evaluating MTI in the BioASQ Challenges. EachBioASQ Challenge is a series of challenges on biomedicalsemantic indexing and question answering with the aim ofadvancing the state of the art accessibility for researchersand clinicians to biomedical text [23].MethodsTo answer the questions of whether or notMTI is still use-ful and relevant, we have used two different approachesevaluating MTI from both an internal and an externalviewpoint. We track a large number of statistical mark-ers for MTI on a monthly basis including how everysingle MeSH Heading is performing, how MTI performsfor each journal, how each of the three input meth-ods (MetaMap Indexing, PubMed Related Citations, andMachine Learning) performs individually and in com-binations with the two other methods, how often MTIrecommendations are referred to by the indexers, and howmuchMTI is used other than for providing NLM Indexingrecommendations.We used the Hooper Measure of Indexing Consis-tency [24] shown in Fig. 2, to calculate the consistencypercentages for MTI, MTIFL, and previously publishedindexer consistency studies by Lancaster [25], Leonard[26], Marcetich and Schuyler [27], and Funk and Reid [28].For the purpose of computing the consistency percentagesforMTI andMTIFL, |N| is the human indexer and |M|is either MTI or MTIFL.We used the descriptions for the various study cate-gories found in the Funk and Reid [28] paper to correlatethe appropriate MTI andMTIFL results to the proper his-torical study categories. We have also used these descrip-tions to identify equivalent categories from some of theother historical studies to fill in the results. For exam-ple: The definition of the Descriptors (DESC) categoryfrom Funk and Reid is equivalent to the Checktags andMain Headings Only category used in the Lancaster andLeonard studies.We do not track how well MTI and MTIFL performwhen identifying the Central-concept main headings, sowe were not able include that metric in our evaluation.For an external evaluation, MTI participated in theLarge-scale online biomedical semantic indexing task ofthe 2013 and 2014 BioASQ Challenges [23]. This task isdesigned to parallel the human indexing currently beingdone at NLM. During each of the BioASQ Challenges,MTI was impartially and rigorously compared to systemsdeveloped by a world-wide community of researchers andindustrial teams all performing the same task. We do notconsider evaluation of MTI using manual indexing biasedbecause we exclude citations that rely on MTI First Lineindexing (MTIFL) from the evaluation and for the cita-tions included in the evaluation MTI recommendationsare used at the indexers discretion. BioASQ provided uswith solid data on how MTI performance compares toFig. 2 Hoopers measure of indexing consistencyMork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 5 of 10other state of the art systems and contributes an outsideperspective on MTI. The BioASQ Challenges consisted ofthree batches of six weekly sets of data to be processedfor a total of 18 sets each year. Each data set was pro-cessed by the various systems and the results returnedto the BioASQ organizers within a 24 h period to makesure none of the citations would have been indexed yetby an indexer which may have biased the results. MTIFLand later default MTI were used as baselines through-out the BioASQ Challenges. A winner was picked foreach of the three batches based on the best performingsingle run of the six possible runs for each batch. So,each BioASQ Challenge had three identified winning sys-tems, one for each of the three batches. Participants werenot required to participate in all of the runs during theBioASQ Challenge.ResultsIs the NLMmedical text indexer used?The contract indexers are paid by the article indexed; ifthey did not feel MTI was useful, they would simply stopreferring to the recommendations made by MTI. A recentquote from one of the indexers nicely illustrates the use-fulness of MTI: . . . from our perspective, its not so muchthat MTI is STILL useful to the task of indexing, its thatit is increasingly very useful to the task of indexing . . . therehas been a real shift in perspective on MTI. Indexers usedto view it as not helpful . . . now (most) view it as extremelyhelpful and overall very accurate. Figures 3 and 4 illus-trate how daily requests of MTI by the indexers havecontinually increased from 15.75% of indexing production(299.78 average daily requests) in 2002 to 62.44% of index-ing production (2997.40 average daily requests) in 2014,an almost 10-fold increase. This continued and steadilyincreasing use of MTI by the indexers indicates that theystill consider MTI to be useful for their task of indexing.Another measure of whether or not MTI is useful andrelevant is monitoring its use outside of the NLM index-ing purposes. Table 2 details the number of MTI requestsfor 2012, 2013, and 2014 excluding any of our usage. Wecapture the total number of items: either free text orMEDLINE citations that were processed by MTI; numberof MeSH on Demand requests (only available for 2014),and the number of different domains that the web requestscome from. These numbers include web requests throughour Interactive MTI web page, Batch MTI web page, WebAPI interface, and the new MeSH on Demand interface.These numbers do not include the daily MTI and MTIFLprocessing of MEDLINE citations, our BioASQ process-ing, or the testing that is done for the NLM indexingefforts.A number of outside researchers, authors, and institu-tions around the world use MTI and MeSH on Demandfor various reasons. We do not track who is using oursystems or what they are processing, so the only way weknow what people are doing with our tools is by inter-acting with them when there are questions or they needassistance. We know from these interactions that peopleare using MTI, MTIFL, and MeSH on Demand to identifyMeSH keywords for biomedical related course materials,MeSH keywords for their research papers, and to helpsummarize text they are working with.Is the NLMmedical text indexer relevant?We only started tracking MTI performance statistics in2007. In 2007, MTI statistics showed Precision of 0.3019,Recall of 0.5163, and F1 of 0.3810. In 2014, the MTIstatistics show significant improvement in Precision andF1 with modest gains in Recall reflecting our focus onimproving MTI Precision over the years: Precision of0.6003 (+0.2992), Recall of 0.5617 (+0.0454), and F1 of0.5807 (+0.1997). Figure 5 illustrates the performancechanges of MTI between 2007 and 2014 using Precision,Recall, and F1 measures. Figure 5 also shows MTIFL F1results between 2011 and 2014. It is clear from Fig. 5 thatjournals added to the MTIFL program are some of the topperformers with the F1 score (0.7018) dramatically higherthan the overall MTI performance (0.5807).MTI Referenced as a % of Indexing ProductionFig. 3 Percentage of indexing production referenced via MTIMork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 6 of 10Average Indexer Daily Use of MTIFig. 4 Average daily usage of MTI by indexersThe MTI statistics for 2014 also show that MTIs con-sistency with the human indexers is comparable to theavailable indexer consistency studies. Table 3 details howboth MTI and MTIFL compare with the previously pub-lished indexer consistency studies. Table 3 includes infor-mation on when each study was performed, how manyarticles were involved in the study, and where availablewhat percentage of consistency was observed using theHooper Measure of Indexing Consistency [24]. Each ofthe included study categories is described below using theFunk and Reid [28] descriptions as a basis and updat-ing the details to conform to todays MeSH and Indexingpractices: Checktags (CT): Checktags are a special type ofMeSH term required to be included for each articleand cover species, sex, human age groups, historicalperiods, pregnancy, and various types of researchsupport (e.g., Male). Geographics (GEO): These are MeSH terms fromthe Z (Geographicals) MeSH Tree (e.g., Paris, IndianOcean). Descriptors (DESC): All MeSH terms includingGeographicals and Checktags. These were calledChecktags & Main Headings Only in the Lancaster[25] and Leonard [26] studies. Main headings (MH):MeSH terms which are notGeographicals or Checktags (e.g., Lung). All main headings (no Checktags):MeSH termsincluding Geographicals, excluding Checktags.Table 2 MTI web usage statistics 2012  20142012 2013 2014MTI Requests 44,970 42,919 87,549# Items processed 3,148,431 7,963,477 11,294,998MeSH on demand requests   225,750# Different domains 118 124 147The MTI and MTIFL sets in Table 3 include results forall of the citations completed between November 2013and November 2014 (one standard indexing year). TheMTIFL set of 27,068 documents is included in the MTIsuperset of 673,125 documents.We also have anecdotal evidence from the NLM Index-ing staff stating their feeling is that new indexers are com-ing up to speed and being more productive faster due inpart to MTIs recommendations. The MTI recommenda-tions help new indexers who are not yet as familiar withthe entire set of 27,000+ terms in theMeSH Vocabulary asmore experienced indexers by providing suggestions theymay not be aware of and helping them to limit the scopeof terms they might be looking to use. We also have moreexperienced indexers who rarely, if ever, use MTI recom-mendations because they are able to index faster withoutreferring to the recommendations.External evaluationMTI was used as the baseline system in the 2013 and 2014BioASQ Challenges. MTI performed well in both chal-lenges ranking within the top tier teams. Tables 4 and 5highlight the results of the 2013 and 2014 BioASQ Chal-lenges respectively. The statistics shown in Tables 4 and5 are unofficial results based on snapshots taken of theBioASQ Results web page on the given dates identifiedfor each table. Tables 4 and 5 both contain the results forthe winning team, MTIFL, and MTI. We have includedthe number of articles completed during each batch, theSystem Name as provided by the competitors, Precision,Recall, and F1 measure for each of the winning systemsand the results for both MTI and MTIFL. Please note thatthe default MTI results were not included as a baselineuntil the third batch of the 2013 BioASQ Challenge - upto that point we only provided baseline results based onMTIFL filtering.In each of the BioASQ Challenges, MTI and MTIFLwere very competitive with the winning systems. In 2013,the largest difference in F1 between the winning systemMork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 7 of 10Fig. 5MTI and MTIFL performance 2007  2014and MTI/MTIFL was 0.0256 (0.5793  0.5537 in batch 2).In 2014, the difference in F1 between the winning systemand MTI/MTIFL was a little wider at 0.0453 (0.6317 0.5864 in batch 3).DiscussionThe five-fold increase in MTI use by NLM Indexers andthe MTI Web Usage statistics detailed in Table 2 pro-vide an indication of how relevant MTI is by showingan increasingly high demand for MTI recommendations.The important thing to note here is that the requestsfor MTI processing come from researchers, authors, andinstitutions around the world. For 2014, the data show asignificant increase in the number of requests for MTIrecommendations and a wider audience of users acrossmore domains. In 2014, we also added a new access pointto MTI with the MeSH on Demand interface which isalready showing high use. These usage statistics show asustained and increasing demand for MTI which is a verystrong indication that MTI is still relevant.The MTIFL consistency results in Table 3 (described inthe Results section) echo the performance gains we seein Fig. 5 when compared to MTI and reflect the fact thatonly journals where MTI performs very well are addedto the MTIFL program. The MTIFL consistency resultscome close to the Funk and Reid [28] consistency resultsand the differences may simply reflect the large disparityin the number of articles involved (760 vs 27,068).MTI and MTIFL performance in the BioASQ Chal-lenges and the fact that both were designated as baselinesfor the Challenges show that MTI is still relevant.The benefits of having a challenge like BioASQ pushingsystems to improve is evident by how much improvementin performance the winning system, MTI, and MTIFLshow over the first BioASQ Challenge. The highest F1measure for a winning system in 2013 was 0.5816 while in2014 it was increased to 0.6317 (+0.0501) [23]. MTI andMTIFL did not show improvement in F1, but, did haveimprovements in Precision from a high of 0.6127 in 2013to a high of 0.6400 (+0.0273) in 2014 reflecting our pushto focus on improving Precision over Recall the last fewyears in both MTI and MTIFL.The benefits of participating in the 2013 and 2014BioASQ Challenges for MTI were two-fold:1. MTI was rigorously and without bias compared tosystems developed by a world-wide community ofresearchers and industrial teams all performing thesame task.2. The challenges provided a forum for the freeexchange of methods and ideas allowing the MTIteam to incorporate the best practices explored byTable 3 Inter-indexer consistency statistics - past and present studiesMarcetich & SchuylerLancaster Leonard Manual Computer Funk & Reid MTI MTIFLYear of study 1968 1975 1981 1981 1983 2014 2014Number of articles 16 100 50 50 760 673,125 27,068Checktags (CT)     74.70% 62.01% 70.91%Geographics (GEOG)     56.60% 41.52% 57.24%Descriptors (DESC) 46.10% 48.20%   55.40% 40.85% 53.97%Main headings (MH)     48.20% 35.17% 48.89%All main headings (no Checktags)   39% 43%  35.29% 49.12%Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 8 of 10Table 4 2013 BioASQ results as of October 21, 2013 for winningsystem and MTI/MTIFLBatch # Articles System name Precision Recall F11 10,681 System3 0.5602 0.5735 0.5668MTIFL 0.5940 0.5196 0.55432 11,808 System1 0.5921 0.5670 0.5793MTIFL 0.6127 0.5050 0.55373 9828 MTI 0.5610 0.6193 0.5887MTIFL 0.6027 0.5653 0.5834System1 0.5873 0.5760 0.5816the participating teams. Incorporating some of theseapproaches into the MTI workflow in 20132014improved the Precision of MTI indexing suggestionsby 4.44% (Recall was improved by 0.08% and F1 by2.23%) [29, 30].Participating in the BioASQ Challenges also providedus with a renewed interest in machine learning. The 2013winning system developed by Tsoumakas, et al. [31] was apurely machine learning system. In the past, we ran sev-eral experiments [1921] to see if machine learning mightbe able to assist MTI and found it to be successful fora handful of MeSH Terms. During our experiments, weran into problems with unbalanced training sets due tothe infrequency of most of the MeSH Terms where wehave a very small set of positive examples in compari-son to the set of negative examples. In the end, only theresults for some of the most frequently used MeSH Termswere viable enough to incorporate into MTI. In the firstBioASQ Challenge, we learned that Tsoumakas et al. wereable to successfully overcome this problem and performedslightly better than MTI in most of the weekly sets asshown in Table 4 (described in the Results section).Another interesting topic from the BioASQ Chal-lenges that we had not pursued before with MTI butTable 5 2014 BioASQ results as of August 5, 2014 for winningsystem and MTI/MTIFLBatch # Articles System name Precision Recall F11 17,061 Asclepius 0.5958 0.5923 0.5941MTI 0.5908 0.5614 0.5757MTIFL 0.6284 0.5199 0.56902 17,073 Antinomyra SYS1 0.6189 0.5863 0.6022MTI 0.6012 0.5621 0.5810MTIFL 0.6176 0.5367 0.57433 18,256 Antinomyra SYS1 0.6527 0.6120 0.6317MTI 0.6099 0.5646 0.5864MTIFL 0.6400 0.5257 0.5773which proved beneficial in the BioASQ Challenges was alearning-to-rank method used by Mao and Lu [32, 33].Our analysis of the MTI recommendations not providedto the indexers shows that MTI incorrectly assigns lowscores and removes many of the actual indexing termsused by the human indexers. The learning-to-rank algo-rithms seem to identify these abandoned and ignoredterms allowing the system to move them up higher in theranked list. In fact Mao and Lu used the MTI results asone of their features in their approach.The winning system in the second and third batchesof the 2014 BioASQ Challenge (Antinomyra) was devel-oped by Liu et al. [34], their system combines the supportvector machines explored by Tsoumakas et al. [31] andthe learning-to-rank approach by Mao and Lu [32, 33]into a system that outperformed either approach indi-vidually as shown in Table 5 (described in the Resultssection).Competing in the BioASQ Challenges also provided theimpetus for us to explore why MTI was missing some ofthe terms that the human indexers use. The main rea-son we found for missing the most frequently occurringMeSH Terms (Check Tags) was that the necessary infor-mation was contained in the full text available to indexers,but not in the Title or Abstract that MTI was using tocompute its recommendations. This specific informationtends to be found in the Methods section of the fulltext where the authors describe how their experimentswere structured. Usually this is where we see informa-tion on the type of experiment subjects (Animal, Humans,or both), sex of the subjects (Male or Female), age ofthe subjects (Infant, Newborn; Infant; Child, Preschool;Child; Adolescent; Young Adult; Adult; Middle Aged; Aged;and Aged, 80 and over), and if an Animal study, whatkind of animals (Mice, Rats, Hamsters, etc.). A simpleexample of this can be seen in Fig. 6 where we have high-lighted the descriptions of the experiment subjects in theTitle, Abstract, and Full Text. For PMID 24000132, Fig. 6illustrates how the author provided only a very generaldescription of rats for the experiment subjects in theTitle and Abstract and nothing about what sex the ratswere, or what specific type of rats they were. The full texton the other hand includes very specific information in theMethods section of the paper letting us know the sub-jects were Male Sprague-Daley rats in the experiment.This information from the full text is critical to MTIbecause recommending just Ratswould only provide one-third of the correct answer. The human indexer would useMale, Rats, and Rats, Sprague-Dawley.Future workWe are currently looking at several ways to incorporatemachine learning and learning-to-rank either intoMTI, oras a starting point for a next generation MTI.Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 9 of 10Fig. 6 Title and abstract versus full text example (PMID: 24000132)One very promising approach we are investigating isto use Wilbur and Kims Stochastic Gradient Descentapproach [35] as a starting point for a next generationMTIand then add in lookup lists, machine learning, indexingrules, and filtering from the existingMTI system. The pre-liminary indications are encouraging showing that the twosystems are in fact complementary.Mao and Lu [32, 33] are also seeing very good resultswith their learning-to-rank algorithm which uses MTI asone of the features. We are currently working with themto see if MTI can use their ranking results to try to salvagesome of the abandoned MTI recommendations.We intend to start working with full text (e.g., fromPubMed Central) to see if we can improve MTI perfor-mance with a focused look at the full text. Only 10% ofthe articles MTI processes have XML full text in PubMedCentral, but it would provide us with data to explorefull text.MTI is also being considered to possibly expand its roleby assisting with mapping OLDMEDLINE [36] terms tothe latest version of the MeSH Vocabulary for citationsoriginally printed in hardcopy indexes published priorto 1966, and the possibility of providing keywords forcitations that normally would not be humanly indexedto provide additional access points that would assist inretrieval.ConclusionAfter twelve years and two BioASQ Challenges it was aperfect time to look around and perform a reality checkto determine if MTI was indeed still useful and rele-vant. In this paper we have presented several qualitativeand quantitative reasons why we think that MTI is infact still useful and relevant. The statistics on how muchMTI is used by the indexers and by people outside of theUS National Library of Medicine show that MTI usagecontinues to grow. The unbiased external review of MTIby the BioASQ Challenges where MTI provided two ofthe baseline systems showed us that MTI is still one ofthe benchmarks for biomedical semantic indexing; but italso proved that we have room for improvement, and evenprovided possible research avenues to make some of thoseimprovements to MTI. For the first time, the BioASQChallenges also provided us with a third-party mechanismto compare MTI against other world-class systems in anunbiased and principled manner.AbbreviationsCT: MeSH check tag; MeSH: Medical Subject Headings; MH: MeSH mainheadings, also described as descriptors; HMD: US National Library of MedicineHistory of Medicine Division; MTI: NLM medical text indexer; MTIFL: NLMMedical text indexer first line; NLM: US National Library of Medicine; PMID:PubMed unique identifier; SH: MeSH subheadings, also described as qualifiersAcknowledgementsThis work was supported by the Intramural Research Program of the NationalInstitutes of Health and the National Library of Medicine. We would like tothank our colleagues François Lang and Willie Rogers for providing direct andindirect support of MTI. We would also like to extend special acknowledgmentto Hua Florence Chang who was the original creator of MTI. Florencesforesight has provided us with a robust and tunable program. We would alsolike to take this opportunity to thank George Paliouras and the entire BioASQTeam for organizing the BioASQ Challenges and providing the opportunity toevaluate MTI. Finally, we would like to thank the NLM indexers and Indexingstaff for their continued support and collaboration they have provided overthe last twelve years teaching the MTI team how they index and ensuring thatMTI succeeds.Authors contributionsAll authors contributed to the design and implementation of the variousexperiments included in this paper. All authors have participated in writingand editing the manuscript. All authors discussed, read and approved themanuscript. JGM is the lead developer for the MTI system.Competing interestsThe authors all declare that they have no competing interests.Received: 29 June 2016 Accepted: 11 January 2017Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 10 of 10RESEARCH Open AccessSNPPhenA: a corpus for extracting rankedassociations of single-nucleotidepolymorphisms and phenotypes fromliteratureBehrouz Bokharaeian1*, Alberto Diaz1, Nasrin Taghizadeh2, Hamidreza Chitsaz3 and Ramyar Chavoshinejad4AbstractBackground: Single Nucleotide Polymorphisms (SNPs) are among the most important types of genetic variationsinfluencing common diseases and phenotypes. Recently, some corpora and methods have been developed withthe purpose of extracting mutations and diseases from texts. However, there is no available corpus, for extractingassociations from texts, that is annotated with linguistic-based negation, modality markers, neutral candidates, andconfidence level of associations.Method: In this research, different steps were presented so as to produce the SNPPhenA corpus. They includeautomatic Named Entity Recognition (NER) followed by the manual annotation of SNP and phenotype names,annotation of the SNP-phenotype associations and their level of confidence, as well as modality markers. Moreover,the produced corpus was annotated with negation scopes and cues as well as neutral candidates that play crucialrole as far as negation and the modality phenomenon in relation to extraction tasks.Result: The agreement between annotators was measured by Cohens Kappa coefficient where the resulting scoresindicated the reliability of the corpus. The Kappa score was 0.79 for annotating the associations and 0.80 for theconfidence degree of associations. Further presented were the basic statistics of the annotated features of thecorpus in addition to the results of our first experiments related to the extraction of ranked SNP-Phenotype associations.The prepared guideline documents render the corpus more convenient and facile to use. The corpus, guidelines andinter-annotator agreement analysis are available on the website of the corpus: http://nil.fdi.ucm.es/?q=node/639.Conclusion: Specifying the confidence degree of SNP-phenotype associations from articles helps identify the strength ofassociations that could in turn assist genomics scientists in determining phenotypic plasticity and the importance ofenvironmental factors. What is more, our first experiments with the corpus show that linguistic-based confidencealongside other non-linguistic features can be utilized in order to estimate the strength of the observed SNP-phenotypeassociations. Trial Registration: Not ApplicableKeywords: SNP, Phenotype, Relation extraction, Negation, Modality, Degree of confidence* Correspondence: behrou.bo@usm.es1Facultad informatica, Complutense University of Madrid, Calle Profesor JoséGarcía Santesmases, 9, 28040 Madrid, SpainFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 DOI 10.1186/s13326-017-0116-2BackgroundBackgroundAn SNP is a single base mutation occurring at the DNAlevel. Variations in DNA sequences can affect how humansdevelop diseases and respond to pathogens, chemicals,drugs, and other agents [1]. There exist an approximate tento thirty million SNPs in humans [2]. As a result of the in-creasing number of related articles, the use of automatic as-sociation extraction in determining the associations ofmutations (e.g. SNPs) and their consequences is increasingin biological systems and genotype-phenotype studies.In genetic epidemiology, GWA study refers to theprocess of examining several common genetic variantsin different people so as to discover a possible correlationbetween a variant and a phenotype trait. A phenotype isan organisms recognizable characteristics or traits such asits development, biochemical or physiological properties,behavior, and the concomitant products of that behavior[3]. The large amount of data generated from these studies[4] necessitates the need to develop an automatic ap-proach in order to facilitate the study of the extracted as-sociations. Recently, a few corpora and methods havebeen developed with the aim of extracting mutation anddisease associations from texts such as [5] and [6]. Thereis, on the other hand, no available corpus for extractingthe association of SNP-phenotypes from texts annotatedwith negation, modality, and the confidence degree ofsuch associations. The need for different levels of annota-tion for biomedical associations has been considered incertain biomedical resources such as PharmGKB [7]. Itcollects information about the impact of human geneticvariations in drug responses that have been annotatedwith four levels of evidence.In this paper, we described and discussed the processof constructing ranked SNP-phenotype association corpus(SNPPhenA), inter-annotator agreement analyses and theresults of some utilized baseline methods during an initialexperiment. In most cases, implementing a biomedicaltext-mining system is a difficult task as the basic scientificcommunication components  i.e. journals and data-bases  are designed to be read by humans, not ma-chines or computers. In order to address this problem,xml was selected as the main format for the produced cor-pus. Furthermore, biomedical Natural Language Process-ing (BioNLP) systems (e.g. relation extraction) have beenmostly applied to abstracts as, though concise, they aremore readily available. Also, abstracts are deemed as goodtargets for information extraction (IE) because they are asuccinct and summarized version of an article [8], hencethe selection of abstracts in the present research.MotivationSeveral named entities have been investigated during thebiomedical relation extraction task, few of which aresuitable candidates for annotating with confidence de-grees, which is the major aim of the research whenidentifying the strength (severity) of associations or in-teractions. The reason for this is that there are no ad-equate biomedical agreements. For instance, Drug-drugInteractions (DDI) or Protein-protein Interactions (PPI)are two biomedical relations discussed by a myriad ofresearchers. However, it is difficult even for a humanexpert to reliably classify the strength or severity ofDDIs or PPIs according to confidence level, a problemexisting due to the variation in the types of related ex-periments and the paucity associated with the methodsof quantifying and estimating the significance of boththe research method and the association. Most GWAstudies that report SNP-phenotype associations aregenerally based on case-control researches [9] initiallytested for statistically significant differences betweenthe proportion of exposed subjects among cases andcontrols. Accordingly, to gauge the research significanceof the result, researchers are encouraged to, more oftenthan not, report a level of evidence by considering p-values and study size.Both preparing a reliable corpus annotated with confi-dence level in associations and developing an automatedtool for this purpose are evidently more difficult for ahost of other biomedical named entities that may requiredifferent models of study [7]. For instance, comparingand finding an acceptable agreement of confidence levelfor an association reported in a case-control experimentbeside to a case study reported association would bemore difficult and challenging. In addition, it is difficultto identify the strength and severity of associations (orinteractions) in a sentence explaining a biochemicalmechanism occurring in many corpora such as DDI andProtein-related associations because every chemical reac-tion may precipitate different sequences within the body.Consequently, insofar as NLP, ranked SNP-phenotypeassociation extraction based on confidence level is con-sidered to be a more feasible task in comparison withmany other biomedical association extraction tasks.Additionally, it is worth mentioning that specifying neu-tral candidates and the effects of negation annotated inthe corpus is influenced by measured confidence level ofassociation between two entities, elaborated in the follow-ing sections. This shows how crucial it is to have reliableannotations for confidence level in associations as well asan automated method for identifying them.Yet another objective of the present was to identifythe association of such phenotypes as quantitative traitsinstead of diseases with SNPs, variously studied by re-searchers. Such extension is significant because manyphenotypes can be detected during the sub-clinical phaseof a disease history, hence determining their associationwith an SNP entails a more early diagnosis and treatmentBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 2 of 13of the disease. Certain phenotypes, it should be noted, areimportant risk factors for the disease.Related tasks and phenomenaOne of the linguistic-based phenomena discussed in thispaper is negation. According to linguistics [10], neg-ation refers to a morphosyntactic operation wherein alexical item or construction is denied or whose meaningbecomes inverted by another lexical item. Likewise, thelexical item representing the negation is referred to asthe negator. Commonly used in clinical and biomedicaltext documents, negation is a significant cause of lowprecision in automated information retrieval systems. Inthe prepared corpus, the marked sentences were anno-tated with negation scopes and cues. A sample of a ne-gated sentence can be found in Fig. 1, wherein the SNPand phenotypes are written in bold font.The other linguistically-driven phenomenon employedhere is linguistic modality. Generally, modal expressionsare words that state modality which is the expression ofthe subjective attitudes and opinions of the presenterabout a possible fact or to control a probable actionincluding intentions, possibility, probability, necessity,obligation [11]. In this research, linguistic-based modalsand speculation analyses were made use of in order todetermine the confidence level of the SNP-phenotypeassociation candidates in the corpus. The linguistic-basedconfidence level of an extracted biomedical associationcan provide an estimate for the reliability of the obtainedassociation and the strength of the biomedical association.Figure 2 demonstrates the sample of a sentence in thecorpus with three modality markers. The modality ana-lysis of a sentence and the linguistic-based confidencelevel of associations can be utilized in addition to othernon-linguistic features so as to obtain more accurateannotations.Named Entity Recognition (NER) is the first step to-wards extracting associations and relations as well asmaking related corpora within biomedical texts [12]. Itis crucial to notice that the characteristics of NER in thebiomedical domain are different from those in the news-wire domain [13]. Identifying mutations in texts is amongthe most difficult NER tasks in BioNLP, investigated in amyriad of studies such as [1416]. EMU is another muta-tion tagger effective in reducing the annotation time ofarticles candidate for mutation related associations [17]. Itshould be noted that implementing a state-of-the-artautomated SNP and phenotype NER is not the objectiveof this research. Rather, it is the first step toward produ-cing an association extraction corpus, where, the productof the automated algorithm is subsequently checkedmanually.The rest of the paper is organized as follows: The nextsection reviews some of the related works; section threepresents the methodology of the paper; section four isdedicated to the evaluation and results; and the last sec-tion concludes the paper.Related worksIn this section, we are going to introduce some of therelevant works about preparing the datasets used forextracting mutation related entities including disease aswell as different methods of annotating negation andlevels of confidence in the biomedical domain.Mutation association extraction methods and corporaBesides classical relation extraction tasks in the BioNLPdomain such as protein-protein and gen-disease, certainnovel methods and corpora have been developed withthe aim of extracting mutation/polymorphism and dis-ease associations, among which, mention can be madeof BRONCO [18] and Variome [19]. BRONCO containsmore than four hundred variants and their associationswith genes, diseases, drugs and cell lines in the contextof cancer, all extracted from 108 full-text articles. Var-iome covers 12 types of relations annotated in 10 full-text articles. While BRONCO includes more documents,both corpora annotate several types of relations, suchas mutation-disease association, as binary relations ona full-text level. On the other hand, the advantages ofabstract-level relation extraction over full-text werementioned in the introduction section. Therefore, theprepared corpus in this research was provided on anabstract level.PKDE4J [5] and Dimex [6] are two methods forextracting mutation and disease association, the latterbeing a rule-based unsupervised mutation-disease asso-ciation extraction working on the abstract level. ThePKDE4J, however, is a supervised method that employsa rich set of rules to detect the used features. Bothmethods work on usual binary relations that determinewhether or not there exist an association; neither methodconsiders the degree of certainty or confidence [20].Fig. 1 A sample sentence in the corpus within a negation cue and scopeBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 3 of 13developed another related miner system that gathersheterogeneous data from a variety of literature sourcesin order to draw new inferences as to the target pro-tein families. Likewise, Ravikumar and his colleagues[21] developed an automated extraction tool in order toobtain protein-specific residue associations from theliterature. Another similar automated approach wasproposed by [22], which extracts impacts and relatedinformation from literature. In another recent study,Klein et al. proposed the principal infrastructure for thebenchmarking of mutation text mining systems [23].The corpus prepared in this research was annotatedwith negation cues and scopes, modality markers, andneutral association candidates. Such linguistic featureswere conducive to the extraction of more accurate infor-mation about the extracted SNP-phenotype associations.Fig. 2 A sample of a sentence with three modality markersFig. 3 Different steps for producing the SNPPhenA corpusBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 4 of 13Annotating the modality and degree of confidenceAs mentioned earlier, modality indicates the degree towhich a certain observation is possible, probable, likely,certain, permitted, or prohibited. A host of studies havebeen conducted for the identification of modality andspeculation in NLP; very few, however, have beenemployed for the classification of modality language inbioscience texts.Although several studies such as [24] have been con-ducted within the linguistics community as to hedgingin scientific texts, in neither is there direct relevance tothe task of classifying from an NLP and machine learn-ing perspective.Light and his colleagues conducted one of the very fewdirect studies [25], where the speculation identificationis introduced using examples from the biomedical domain.They address the question of whether there is sufficientagreement among researches as to what constitutes aspeculative assertion that renders the task viable from acomputational perspective. Despite the fact that Light at-tempts to separate the two sides of speculation (strongand weak), he fails to glean sufficient evidence for such areliable distinction. They conclude that having a reliabledistinction between speculative and non-speculative sen-tences is feasible, and reliable automated methods mightalso be developed.It is noteworthy that in addition to the preponderanceof biomedical relation extraction annotations that merelyinclude usual binary association information, there existcertain others containing extra-linguistic information in-cluding POS, negation, and speculations information. Asan example, the Genia corpus [26], along with biologicalevents, contains annotations for three levels of uncer-tainty. Nonetheless, to the best of our knowledge, all ofthe mutation related corpora have only been annotatedwith binary associations. In the current study, the corpuswas enriched through adding more linguistic informationsuch as the linguistic based confidence level of associations,modality markers, and neutral association candidates.Negation annotationIn general, two negation detection methods have beendeveloped to annotate the employed corpora: A linguistic-based approach and an event-oriented approach. Amongother negation annotated corpora, one may refer to thetwo most well-known: the linguistically-focused, scope-based BioScope [27] and the event-oriented Genia [26]. InBioScope, scopes recognize the position of the key negatedevent within the sentence, with each argument of the keyevents coming under the scope, as well. Genia, on thecontrary, independently deals with modality within theevents. In a Genia event, biological concepts (relations andevents) are annotated for negation, yet no linguistic cuesare annotated. In fact, the objective of the BioScope cor-pus is to approach this language phenomenon in a general,task-independent, and linguistically-oriented manner. Itcan further automatically recognize negation scopes andcues in sentences.Fig. 4 A sample of SNP and phenotype named entity recognition in the corpusTable 1 Some of the most occurred phenotypes in the corpusPhenotype/phenotypic trait Num. of abstractshealth risk 40smoking 33Obesity 25metabolic syndrome 16hypertension 10insulin sensitivity 9hypertriglyceridemia 7glucose metabolism 6impaired glucose tolerance 5longevity 4body mass intake 4cognitive performance 4skin pigmentation 3AIDS 3Table 2 Eight of most occurred SNPs in the SNPPhenA corpusand number of contained abstractsSNP Number of abstractsrs12255372 78rs429358 55rs7412 46rs4680 38rs1051730 25rs662799 20rs1799971 18rs1800629 14Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 5 of 13NegDDI-DrugBank is another corpus that was anno-tated by the authors of the previous work with scopes ofnegation and negation cues [28]. The automatic extrac-tion of Drug-Drug interactions from the text is held tobe highly significant, as two corpus versions (in 2011and 2013) were prepared in this regard. Concerning thehigh rate of negated sentences in the DDI corpus, acomplete set of sentences within DDI 2011 (with a totalof 5806 sentences and 579 files) was automatically anno-tated with negation scopes and cues. The results were,then, manually checked by three experts to address pos-sible mistakes within the course of the automated process[29]. Adding a new XML negation-tag containing negationcues and negation scopes, the NegDDI-DrugBank corpuswas established.Corpus constructionIn this section, the steps followed in the construction ofthe SNPPhenA corpus are explained. The entire processconsists of three major steps of collecting documents,automatically and manually recognizing the SNP andphenotypes, and annotating the associations and the re-lated information (Fig. 3). The last step entails annotat-ing the association candidates, the confidence level ofassociations, the modality markers and the negationscopes and cues of the sentences.In order to have consistent annotations, all annotatorswere given the same instruction which includes a pellu-cid definition of the entities and their relationships, rulesand conventions of annotating the confidence level ofassociations and complete examples for each type oftags. The annotation guideline also contains rules fortackling linguistic phenomena such as negation cues andmodality markers. Moreover, this document presentsdifferent types questions raised and retorted by the an-notators during the annotation process. The annotationguideline can be found on the website of the corpus.In the end, 360 XML files were generated comprisedof the abstract texts, SNPs, Phenotypes, and the SNP-phenotype associations in the selected sentences. ThePhenotypes, SNP names and the association candidateswere annotated as xml element tags for each nominatedsentence in the abstract. Next, the annotations and thefinal product were manually checked. The producedSNPPhenA corpus is available for public use 1. So as tobetter fathom and employ the corpus, brat stand-offannotation format of the files is also available at thewebsite of the corpus. The next subsection is dedicatedto the abstracts collection process 2.Abstract retrievalInformation provided by the http://www.gopubme-d.org/ search engine was used to collect genome-wideassociation abstracts. GoPubMed is a webserver allowingusers to explore PubMed search results with GeneOntology [30]. Twenty popular SNPs were used as queryterms enumerated popular by http://www.snpedia.com/website; the extracted list of abstracts was shortened viaselecting those comprised of popular disease names. Thelist was finally truncated again through choosing thosethat have candidate sentences consisting of both types ofentities. We collected a total of 360 abstracts (including2625 sentences) with at least one candidate sentencewith an SNP and a phenotype name. There were 483 keysentences containing at least one SNP and one pheno-type name that were annotated with the xml elementFig. 5 A sample of two annotated associations between two SNPs and a phenotype in the SNPPhenA corpusFig. 6 Samples of positive association candidate between highlighted two SNPs and a phenotypeBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 6 of 13SENTENCE. The total number of SNP names anno-tated in the SNPPhenA corpus was 875. It is worthmentioning the SNPPhenA is a sentence-level corpusand sentences merely including SNP or Phenotype werenot annotated.The next step was to perform an automatic NamedEntity Recognition, followed by a manual checking ofsentences with candidate relations for SNPs and pheno-type names, as explained in the section below.Named entity recognition (NER)An essential part of biomedical NLP is to detect biomed-ical named entities [31]. During the construction process,two Named Entity Recognitions were done on SNPs andPhenotypes. These two tasks are minutely explained in thetwo following subsections. A sample of implementedNERs is shown in Fig. 4.Phenotype NERA phenotype is the appearance of an organism in termsof its morphology, development, physiology, behaviorand its concomitant products [3]. Although there are da-tabases containing disease names and popular phenotypenames, no compendious database of phenotypes is yetavailable.In this regard, a dictionary-based NER task was imple-mented by combing two more complete and pertinentdatabases. The prepared dictionary includes a list fromthe Comparative Toxicogenomics Database (CTD) fordisease names [32]. Also included is the phenotypeontology prepared in the blast project [33]. The collectedlist of phenotypes includes 65,530 phenotype names alongwith more than twelve thousand disease names and theirsynonyms.The phenotype names were initially recognized auto-matically by the prepared dataset. Manual checks weresubsequently made by two experts in order to identifymissed or inexact phenotypes.A short list of the most frequent phenotypes is shownin Table 1 where the top two phenotypes in the corpusare health risk and smoking.SNP NERThe inconsistent description of biological data elementsrenders the relation extraction tasks challenging. Namesassociated with polymorphism are particularly problematicbecause historical or common names are, more often thannot, employed instead of standard nomenclature [34],specifically in candidate gene association studies. Whatis more, it is hard to find the links between historicalor common SNP names and refSNP [35]. To addressthis issue, we implemented a database containing bothrefSNP(rs) and historical names, matched with theircorresponding rsID numbers, while utilizing the VariantName Mapper(VNM) tool [36]. The VNM tool consists ofhistorical names matched with their corresponding rsIDnumbers extracted from multiple open-access databases,including SNP500Cancer [37], SNPedia [38], pharmGKB[39]. The database was utilized for extracting the differentSNP names.Similar to the phenotype NER process, SNP name an-notations were initially checked manually by two biologyexperts and verified by a third professional annotator. Ashort list of the most frequent SNPs is shown in Table 2.Annotating the candidate SNP-phenotype associationsThis section deals with the process of annotating theassociated candidates which includes the annotation ofthe SNP-phenotype associations, the confidence level ofassociated candidates, modality markers, and negationscopes and cues in the negated sentences.Annotating the SNP-phenotype associationsFollowing the collection of abstracts and the determin-ation of the SNP and phenotype candidate names, theassociations between SNP and phenotype were manuallyannotated by three gurus in genetics (Fig. 5). The SNP-Fig. 7 Samples of negative association candidate between highlighted six SNPs and a phenotypeFig. 8 A sample of neutral association candidate with used highlighted entitiesBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 7 of 13phenotype candidates were classified into three cat-egories of positive, negative and neutral. The positiveSNP-phenotype relation candidates are those withclearly indicated associations (Fig. 6). In contrast, negativeSNP-phenotype relation candidates are those in which alack of association is evident (Fig.7). In addition to the typ-ical classes of relationships, a neutral class is defined forthose that fall between the two other classes, where thepresence or absence of association is not remarked in thesentence (see Fig. 8).As Fig. 8 shows, the presence or absence of associationis neither mentioned between rs4689 and anorexianervosa, nor can it be identified with a high level ofconfidence, hence, the association between the SNP andthe phenotype was annotated as neutral.In more precise terms, an SNP-Phenotype associationcandidate is identified as neutral if:(i) The absence or presence of association betweenSNP-phenotype cannot be specified from the sentence(or container clause) with a confidence level of morethan zero.(ii) The status of presence or lack of association be-tween the SNP and the phenotype does not change frompositive to negative or vice versa if the sentence (or con-tainer clause) is negated and SNP and phenotype namesare located in the scope of the negation.(iii) The confidence level of association between SNPand the phenotype does not change if a modal marker isutilized in the sentence and both entities are located inthe scope of modality.The association in Fig. 9, for instance, is neutral andthe used negation cue (no) does not change the statusof the association between the SNP and the phenotypes.It is worth mentioning that in most relation extractioncorpora, neutral candidates were considered to be partof the negative (non-positive) class. Considering them asa separate class of associations allows researchers to con-duct different types of experiments. More details as to therole of neutral candidates in biomedical relation extractiontasks can be found in the authors other study [40].Similar to the previous steps, the manual checking wasinitially performed by two experts, and in order to sortout the issue of contradictory confidence levels, the ver-dict of a third expert annotator was taken into account.Annotating the level of confidence of the SNP-PhenotypeassociationsIn spite of the fact that genetic components have the in-structions for the growth and development of each individ-ual, a persons phenotype is influenced by environmentalfactors during embryonic development and throughout life.Environmental factors can stem from a variety of influencessuch as diet, climate, illness and level of stress. For instance,the capability to taste food is a phenotype estimated, by sci-entists, to be 85% influenced by genetic inheritance [41].Nevertheless, environmental factors such as dry mouth orrecently eaten food could affect such ability.Phenotypic plasticity is the ability of a genotype togenerate more than one phenotype due to various envi-ronments [42]. The plasticity is considered to be high ifenvironmental factors have a strong influence. Con-versely, if the phenotypic plasticity is low, the genotypecan be made use of so as to reliably predict the pheno-type. The degree of influence environmental factors haveon a persons ultimate phenotype is, not infrequently, amatter of heated scientific debate.Fig. 9 A sample of neutral association candidate with a negation cueFig. 10 A sample of a strong association that has been mentioned to have a strong degree of confidenceBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 8 of 13Differing phenotypic plasticities alongside possibleunknown genetic components are the two reasons whyGWA study uses confidence level in order to describethe strength of association. The linguistic-based confi-dence level of the reported association ultimately yieldsinformative data leading to the determination of pheno-typic plasticity.However, there is no available data source or automatedmethod for extracting confidence level from the obtainedresults. This is when the presence of such a tool and datasource is critical and conducive to reviewing literatures.For this purpose, the confidence levels of positive asso-ciation candidates in the corpus were annotated by aguru in human genetics. Based on the strength of thelinguistic correlation between each individual phenotypeand the relevant SNP mentioned in the abstract, theconfidence level of associations was categorized into weak,moderate, and strong. Moreover, when the association isneutral (ASSOCIATION= neutral), the degree of confi-dence is set to zero. The confidence levels were assortedconsidering modality, adverbs and the reported statisticalresults (p-value). Detailed information about the annota-tion guidelines can be seen in the guidelines document,available on the website of the corpus. The process, all thesame, is demonstrated here via some samples.The sentence shown in Fig. 10, for example, is consid-ered to have a high confidence level as it indicates founda significant genotype effect.The sample mentioned in Fig. 11, on the other hand,is annotated as having a weak confidence level becauseof the might be clause. However, there exist certaincases that fall under both two categories such as thesample below (see Fig. 12), annotated as moderate.The annotation of confidence level was carried out bytwo biology experts both of whom had the same opinionregarding 86% of the association candidates in the wholecorpus. In order to sort out the issue of contradictoryconfidence levels (14%), the opinion of a third guru an-notator was considered.Linguistic based negation detection and modalitymarkersIdentifying negative statements is essential in order toobtain accurate information from the text data. The sen-tence in Fig. 13 demonstrates the importance of consider-ing negation where there is no association between APOE(rs429358) and bvFTD; however, if the negation hadbeen neglected, an incorrect association might have beenidentified.A rule-based system, proposed by [43], was initiallyutilized in order to annotate the negation scopes andcues. During the process, a set of negation cues such asnot, lack, were detected making use of Bioscopesguidelines. Negation cues indicate that a negation existsin a sentence. Considering the syntactic context, thescopes of negation and negation cues were subsequentlydetermined, a task already done in a previous work by theauthors [28] annotating the DrugDDI 2011 corpus. Inorder to preclude any possible mistakes, manual checkswere made by an expert following the automated process.In addition to the negation cue and scopes, modalitymarkers were annotated during the annotating process.The employed modality markers obtained from the listwere already provided in [44], which is an extension ofthe list provided by [45] for the biomedical domain. Theprocess includes an automated annotation, followed byan expert performing the manual check. The five morefrequent annotated modal markers in the corpus are:suggest, more, strong, observe, and show.Evaluation and resultsIn this section, inter-annotator agreement analyses andthe calculated scores are initially presented; then someof the basic statistics of the produced corpus will beFig. 11 A sample of a weak association that has been mentioned to have a weak degree of confidenceFig. 12 A sample of moderate association that has been mentioned to have a moderate degree of confidenceBokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 9 of 13demonstrated; and finally, the results obtained from ourfirst experiment using the corpus are presented.Inter-annotator agreementIn order to evaluate the quality of the corpus and the re-liability of the annotations, the inter-annotator agree-ment score was measured for the task of classifyingcandidate sentences into positive, negative and neutralclasses, and also for the task of determining the confi-dence level of the association. As was mentioned before,two annotators had independently tagged the corpus. Inthe case of disagreement between two tags, a third anno-tator was asked to decide on the correct one. For thetask of classifying candidate sentences, inter-annotatoragreement was 91%, which means that in 91% of cases,the two annotators agreed. Additionally, we computedCohens Kappa coefficient [46] for the two annotators;this coefficient takes into account the degree of agree-ment that could be expected to occur by chance and iscomputed as follows:? ¼ po?pe1?peWhere Po is the relative observed agreement amongannotators, and pe is the hypothetical probability ofchance agreement. The Kappa value was 0.79 for thetwo annotators. In general, ? = 1 indicates a completeagreement. Furthermore, ? < 0 shows that there is noagreement between annotators other than what wouldbe expected by chance (as given by pe).As far as the task of annotating the confidence level ofthe association with four categories (zero, weak, medium,strong), annotators agreed in 87% of the occasions; yet theKappa value was 0.80 which is satisfactory.Characteristics of the SNPPhenA corpusThis section provides detailed statistics as to the linguis-tic and nonlinguistic properties of the corpus. The basicproperties of the corpus are presented in Table 3 whichincludes the statistics of the produced corpus in terms oftest and training parts. As the table shows, the candi-dates with a positive association comprise the largestcategory while the negatively associated candidates con-stitute the smallest category.Table 4 provides the detailed analyses concerning thedifferent types of SNP-phenotype association candidates.Additionally, as mentioned earlier, the key negatedsentences in the corpus were annotated with scopes ofnegation and negation cues. As Table 4 shows, 16.8% ofthe sentences have at least one negation cue. Furtheranalysis shows that not and no with respective occur-rences of 35 and 38 were the most frequent negationcues. According to the conducted analyses, each sen-tence in the corpus had an average of 76.9 tokens, 1.7SNPs, and 1.2 phenotypes.As illustrated in Table 3, 76.3% of the samples are distin-guished (i.e. they are positive and negative association can-didates). It can, therefore, be concluded that the annotatedsentences were mostly expressed as a direct mechanism orassociation between one or more SNPs and a phenotype.Additionally, as Table 4 shows, 63.8% of the candidatesentences have at least one clause connector, while36.2% do not have one. The result of statistical analysison the clause connectors further indicates that 9.7%(=87/895) of instances had concessive clauses.Fig. 13 A sample of a negated sentence with negation cue and scopeTable 3 Basic statistics of the SNPPhenA corpus in terms of testand train partsItem Train Test TotalFiles 270 90 360Sentences 1940 685 2625Key sentences 362 121 483SNP 691 244 935Phenotypes 496 158 654SNP-Phenotype association candidates 935 365 1300Neutral candidates 142 166 308Negative candidates 91 29 120Positive candidates 702 170 872Table 4 Statistics of different types of SNP-phenotype associationcandidates in the SNPPhenA corpusItem Number Percentage (%)Total SNP-phenotype association candidates 1300 100Candidate with at least one negation cue 218 16.8Candidates with only one negation cue 188 14.5Candidates with clause connectors 823 63.8Candidates without clause connector 470 36.2Weak degree of confidence candidates 515 39.6Moderate degree of confidence candidates 124 9.5Strong degree of confidence positivecandidates233 17.9Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 10 of 13ExperimentThe results of our first experiments with the corpus arepresented in this subsection. Although several mutation-related association extraction methods have recentlybeen developed, automatically measuring the confidencelevel in an association is a novel task. Consequently, ourfirst experiments were evaluated via certain baseline ker-nel methods for the two subtasks.In order to categorize the associations, we employedthe two kernel methods that have been expansively madeuse of in the relation extraction task; the local contextkernel [47] and sub-tree kernel [48]. Additionally, thebinary Bag of Word (BOW) method was carried out onthe corpus so as to predict the degree of confidence forthe associations. In all the experiments, the training partof the prepared corpus was used for training the classifierand the test part was employed for testing the system(Tables 5, 6 and 7).Table 5 shows the performance of the two utilizedbaseline methods, applied to all three types of candi-dates. The reported f-score was measured for the detec-tion of positive SNP-phenotype association candidates.Table 6 further indicates the performance of the baselinemethods were only applied to the positive and negativeassociation candidates.The results of the confidence level prediction of associa-tions are presented in Table 7 where the best f-measure isrelated to the candidate expressions of associations with aweak confidence level, while the worst result is obtainedfor the moderate confidence level.The lower performance of identifying the confidencelevel of association in comparison with the associationextraction method demonstrates that the simple featuresused in the binary BOW may not have enough informationto surmount the task and more linguistic features are re-quired. Moreover, the difficulty of the task might be precipi-tated by the fact that during the annotation process, theannotators employed the mentioned p-value number as acomplementary factor for identifying the confidence cat-egory, which was the case with 20% of the candidate sen-tences. It can, accordingly, be concluded that accuratelyidentifying ranked association from biomedical articlesrequires more linguistic features including dependencyparsing, lemmatizing and features related to identifyingthe significance degree of the biomedical statistical tests.A simple version of the baseline method can be foundonline 3. It is indispensible to mention that the onlinesystem may have a worse performance in comparisonwith the reported results in this section due to the ab-sence of manual checking during the NER task as wellas the omission of the negation detection step.All the kernel method experiments were carried outby a support vector machine with SMO [49] implemen-tation. Weka API [50] was used as the implementationplatform.Conclusion and future workIn this research, a SNPPhenA corpus was developed inorder to extract the ranked associations of SNPs andphenotypes from GWA studies. The process entailedcollecting relevant abstracts, Named Entity Recognition,and annotating the associations, negation, modalitymarkers, and the confidence level of the associations.As opposed to the previous biomedical relation extrac-tion corpora containing true and false types of relations, theannotated associations in the corpus were divided intothree classes: positive, negative and neutral candidates. Theneutral candidates were those SNP-phenotype candidatesthat showed no clear evidence as to the presence or lack ofassociation between the SNPs and phenotypes. Identifyingneutral candidates is critical for the negation process as thestatus of such candidates and their corresponding level ofconfidence do not change when they are located in thescope of negation terms; the status of distinguished associ-ation candidates, on the other hand, change in such cases.Similarly, the confidence level, certainty or uncertainty of aneutral candidate, does not change if it is located in thescope of a speculation or modality term. Hence, determin-ing the effect of negation as well as modality terms requiresthe identification of neutral candidates.Table 5 Comparative f-score results for the test SNPPhenA partfor two kernel methods with all types of candidates (positive,negative and neutral class)Method LCK Subtree kernelF1 71.3% 57.7%Recall 68.7% 51.8%Precision 69.2% 50.3%Table 6 Obtained comparative results for the test SNPPhenAcorpus for the two investigated kernel methods with non-neutralcandidates (positive and negative class)Method LCK Subtree kernelF1 63.4% 45.7%Recall 59.8% 41.3%Precision 56.6% 40.1%Table 7 Obtained results for the calculating confident intervalof the positive association of the test part of the SNPPhenAcorpus by bag of words methodParameter Weak degreeof confidenceModerate degreeof confidenceStrong degreeof confidenceF1 69.5% 32.6% 35.3%Recall 66.4% 30.5% 34.2%Precision 65.3% 31.6% 32.2%Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 11 of 13Not to be forgotten is the fact that the SNPPhenA cor-pus must be considered as an initial step in extractinggraded associations from literature, which could resultin the idea of a fuzzy relation extraction task that can beemployed so as to construct better biomedical ontologies.Furthermore, it is important for future researches toemploy more linguistic-based and non-linguistic-basedfactors that could be utilized to determine the confi-dence of the reported associations. Credibility of thegenotyping techniques (such as MLPA or RFLP) and thevalidity of the research through graph-based networkanalyses can be employed in the process of identifyingthe overall confidence level of the reported associations.Endnotes1https://figshare.com/s/b18f7ff4ed8812e265e82https://figshare.com/s/f19191317056d6835b383http://snpphenotypeext-nilg.rhcloud.com/Additional fileAdditional file 1: Abstract files of SNPPhenA corpus. (ZIP 651 kb)AcknowledgementThe authors acknowledge Dr. Mariana Neves (Hasso-Plattner-Institute,University of Potsdam, Germany) for her very helpful comments and foradvice regarding the usage of brat and pubannotaion tools.The authors also acknowledge Dr. MT Pilehvar (Cambridge University, UK) forher useful comments and suggestions for organizing the paper.FundingNot applicable.Availability of data and materialsThe prepared corpus (SNPPhenA) is available at this address: XML format:https://figshare.com/s/b18f7ff4ed8812e265e8: BRAT format: https://figshare.com/s/f19191317056d6835b38: Simple online version of theassociation extractor is available here: (http://snpphenotypeext-nilg.rhcloud.com/): Web site of the corpus: http://nil.fdi.ucm.es/?q=node/639:Annotation guideline: http://nil.fdi.ucm.es/sites/default/files/guidline.pdf: DTD:http://nil.fdi.ucm.es/sites/default/files/SNPPhenA_DTD.zip: Kappa calculation:https://figshare.com/s/f1fe27ca17022fd4a698: Document Text Files (Additionalfile 1): https://figshare.com/s/47886f335fb0beaf3099Authors contributionThe constructing the corpus was managed by BB, preparing the files of thecorpus as well as carrying out the baseline methods was performed by thefirst author. Moreover the annotating the negation scope and cues wasperformed by BB. The basic structure of the paper and some details of theexperiments and presenting the results were performed by AD. All authorsread and approved the final manuscript. NT (University of Tehran, Tehran,Iran) developed a program to optimize the corpus and helped in writingthe guideline document. HC (Colorado state university, Colorado, US)helped in preparing the inter-annotator measurement and also preparingand coordinating the two annotator. He also helped in structure of thepaper and figures. RC (External Collaborator, Royan Institute for ReproductiveBiomedicine, Tehran, Iran) helped in annotation of the corpus as well as givesome guidance in biological aspects of the study.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Author details1Facultad informatica, Complutense University of Madrid, Calle Profesor JoséGarcía Santesmases, 9, 28040 Madrid, Spain. 2School of Electrical andComputer Engineering, College of Engineering, University of Tehran, Tehran,Iran. 3Department of Computer Science, Colorado State University, FortCollins, CO 80523, USA. 4External Collaborator, Reproductive BiomedicineResearch Center, Royan Institute for Reproductive Biomedicine, Tehran, Iran.Received: 5 July 2016 Accepted: 13 January 2017Park et al. Journal of Biomedical Semantics  (2017) 8:25 DOI 10.1186/s13326-017-0134-0RESEARCH Open AccessTowards a more molecular taxonomy ofdiseaseJisoo Park1* , Benjamin J. Hescott1 and Donna K. Slonim1,2AbstractBackground: Disease taxonomies have been designed for many applications, but they tend not to fully incorporatethe growing amount of molecular-level knowledge of disease processes, inhibiting research efforts. Understandingthe degree to which we can infer disease relationships from molecular data alone may yield insights into how toultimately construct more modern taxonomies that integrate both physiological and molecular information.Results: We introduce a new technique we call Parent Promotion to infer hierarchical relationships between diseaseterms using disease-gene data. We compare this technique with both an established ontology inference method(CliXO) and a minimum weight spanning tree approach. Because there is no gold standard molecular diseasetaxonomy available, we compare our inferred hierarchies to both the Medical Subject Headings (MeSH) category Cforest of diseases and to subnetworks of the Disease Ontology (DO). This comparison provides insights about theinference algorithms, choices of evaluation metrics, and the existing molecular content of various subnetworks ofMeSH and the DO. Our results suggest that the Parent Promotion method performs well in most cases. Performanceacross MeSH trees is also correlated between inference methods. Specifically, inferred relationships are moreconsistent with those in smaller MeSH disease trees than larger ones, but there are some notable exceptions that maycorrelate with higher molecular content in MeSH.Conclusions: Our experiments provide insights about learning relationships between diseases from disease genesalone. Future work should explore the prospect of disease term discovery from molecular data and how best tointegrate molecular data with anatomical and clinical knowledge. This study nonetheless suggests that disease geneinformation has the potential to form an important part of the foundation for future representations of the diseaselandscape.Keywords: Disease Ontology inference, Disease tree inference, Pairwise disease similarity, Disease gene association,Medical Subject Headings tree, Disease Ontology, Hierarchical clustering, Parent PromotionBackgroundThe recent growth in availability of genomic and clini-cal data allows for the discovery of new molecular-levelmechanistic models of disease. However, existing dis-ease taxonomies and ontologies are often focused oneither physiological characterizations of disease, some-times using decades-old criteria, or on the organizationaland billing needs of hospitals. Automatically inferringcommonmolecular links between related diseases is mademore difficult by the limited molecular representation*Correspondence: jisoo.park@tufts.edu1Department of Computer Science, Tufts University, 161 College Avenue,Medford, MA 02155, USAFull list of author information is available at the end of the articlein current taxonomies [1], leading some researchers tomanually group related disorders for individual projects(for example, PheWAS analysis [2] or network-based dis-ease gene prioritization [3]). Yet such manual efforts limitconsistency and reproducibility. To further advance suchresearch and biomedical knowledge in the genomic era,a recent National Academy of Sciences working grouphas called for the development of new disease taxonomiesbetter suited to incorporate molecular information [4].A truly modern taxonomy would presumably combineclinical, physiological, and molecular data. The questionwe address here is the degree to which we can infer ameaningful disease taxonomy simply using disease geneinformation. In this, we were inspired by efforts by TreyIdekers group to infer a version of the Gene Ontology© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 2 of 11using pairwise similarity scores between genes [5, 6].Their CliXO algorithm, for example, sorts gene pairs bya pairwise similarity score and incrementally uses thesescores to group together cliques of similar genes. Theresulting ontology forms a Directed Acyclic Graph (DAG)of sets of genes. As in that work, here we are not argu-ing that we should ultimately construct a disease hierarchyautomatically in this way. However, learning how we candiscover the relationships in existing disease taxonomiesfrom disease gene data is a first step towards developingnew hierarchies of disease that integrate the clinical infor-mation used in todays taxonomies with genomic data.Such integrated taxonomies are needed to better supportresearch in molecular medicine [7].To infer a disease taxonomy, we would like to sim-ply cluster diseases hierarchically based on associatedgenes from a large gene-disease database. However, ifthe items we are clustering are diseases, the internalnodes of any hierarchical clustering method will corre-spond to unnamed sets of diseases. While some of thesemay be informative, identifying them is a challenge. Wetherefore introduce here an algorithm called Parent Pro-motion, based on hierarchical clustering, that addressesthis problem.We acknowledge that we are deliberately blurring thedistinction here between an ontology of disease [8] anda disease taxonomy [9]. In this manuscript, we focus onlearning a hierarchical characterization of disease usingexisting disease terminology, yet incorporating molecu-lar relationships. Such a description may be able to betteridentify novel relationships between disorders that do notappear clinically similar but that arise from similar under-lying genotypes. Yet we are not expecting here to compre-hensively infer disease relationships as in most ontologies,in part because the current project ignores the clinical andanatomical characteristics built into many existing tax-onomies. Accordingly, we frequently use the term diseasehierarchy to encompass our inferred hierarchies as wellas those to which we compare.One important question is how to evaluate our inferredhierarchies of disease when there is no existing gold stan-dard. However, there are a handful of existing taxonomiesand disease ontologies that are somewhat suitable formolecular analyses and comparisons [4]. Medical SubjectHeadings (MeSH) is a hierarchical structure of controlledbiological vocabularies used to index articles in MED-LINE [10].MeSH includesmanymedical concepts beyonddiseases, but here we refer to MeSH category C, a com-prehensive set of 26 trees that represent relationshipsbetween diseases. SNOMED-CT provides an organizedterminology for clinical terms [11]; this is one of the mostdetailed terminologies available, but there are restrictionson its distribution. The Unified Medical Language Sys-tem (UMLS) metathesaurus includes disease terms frommultiple taxonomies; while it is not intended to be anontology, its semantic network can identify some relation-ships between terms [12]. The Disease Ontology (DO)also integrates the knowledge and relationships from sev-eral taxonomies, including MeSH, SNOMED-CT, andICD [13].Initially, because of the high coverage and availability ofMeSH and its simple structure, we chose to compare ourinferred hierarchies to the MeSH forest of disease terms.Although it is not necessarily a gold standard for the prob-lem we are trying to solve, we can use such a comparisonto identify the strengths and limitations of different infer-ence methods. In addition, identifying individual MeSHdisease trees that are more consistent with the hierar-chies inferred from disease-gene data helps in assessingthe molecular content of existing domains in MeSH. Wehave also extended our assessments by comparison to theDisease Ontology, which is a more complex process forreasons detailed below.Even after fixing a reference hierarchy for comparison,the question of how to assess correctness remains. Manyof the standard network and graph comparison metricsare inappropriate for our problem. One that does makesense is a strict variant of Edge Correctness [14] thatasks how many parent-child relationships we get right.We therefore use Edge Correctness as one measure ofaccuracy.One limitation of Edge Correctness, however, is that thedistances between pairs of terms are not uniform [15].That is, two diseases that are separated by more thanone taxonomic link may be more closely related to eachother than two other diseases in a direct parent-childrelationship. We therefore also introduce the notion ofAncestor Correctness, a feature-based similaritymeasure-ment [16] that assesses our ability to properly identifyancestry without concern about distances.Finally, neither Edge Correctness nor Ancestor Correct-ness penalizes an algorithm for false positives (inferrededges not in the reference hierarchy). This is fine forinference methods like Parent Promotion that build trees,which all have the same number of edges for a fixed setof disease nodes, but not for comparison to ontology-learning approaches that can add arbitrary numbers ofedges. Accordingly, we also compute a variation of hier-archical precision and recall [17], analagous to AncestorCorrectness, that accounts for both false positives andfalse negatives.MethodsReference taxonomiesTo quantify performance of various disease hierarchyinference methods, we compare our inferred taxonomiesto the 2016 Medical Subject Headings (MeSH) disease trees[10] and the Disease Ontology (DO) [18], downloadedPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 3 of 11on August 5, 2016. From both datasets, we excludediseases for which we cannot find any associated genes,because our methods would then have no way to learnabout how they relate to other diseases. However, exclud-ing diseases can disconnect our reference hierarchies. Toreconnect them, we therefore add edges from a deletednodes parents to all of its closest descendants that do haveassociated genes.We note that the MeSH trees allow repeated diseasenames, resulting in multiple nodes with the same namein different parts of the tree. We treat these terms asif they were the same node, effectively matching againstthe corresponding DAG. However, given that the originalstructure is a tree, most of these DAGs end up being fairlytree-like.Because the Disease Ontology is substantially largerthan any of the individual MeSH trees, we extractedsmaller DAGs from the full DO to facilitate algo-rithm comparison. To find these smaller DAGs, wesearched through the DO starting at the most generalterm. A term became a root of a DO subnetwork ifits name approximately corresponded to the name ofthe root of one of the 26 MeSH trees and if it hadat least 100 DO terms as descendants. This approachidentified four new DAGs that can be described ascovering mostly Cardiovasular Disease, Gastrointesti-nal Disease, Musculoskeletal Disease, and NervousSystem Disease.Table 1 reports the sizes and topology of these four sub-networks of the DO. All are fairly tree-like; only smallnumbers of nodes have more than one parent, and thetotal number of edges is not that much larger than thenumber of nodes. We note that it is not necessarily thecase that all disease nodes in the DAG labeled Muscu-loskeletal Disease, for example, actually correspond tomusculoskeletal disorders, because the Disease Ontologyand MeSH are organized according to different princi-ples. We therefore acknowledge that each subnetworkof the DO may contain terms that map to several dif-ferent MeSH disease trees. Nonetheless, we use theselabels as shorthand ways to refer to the chosen DOsubnetworks.Withheld MeSH subtrees for method developmentWe selected four small subtrees from MeSH that we usedfor refining our computational methods. These are theMeSH subtrees rooted at the terms Infant PrematureDiseases, Dementia, Respiration Disorders, and EyeDiseases, giving us a range of subtrees of different sizesand complexity (Table 2). Note that the MeSH tree rootedat Eye Diseases includes 149 disease terms and 178edges, indicating that several terms appear multiple times,althoughwe allow a node with a given name to appear onlyonce in each inferred hierarchy.Although we show the performance of the inferencemethods on these subtrees separately in Additional file 1,we did not think it fair to include them in our over-all MeSH results because we used them to tune ourmethods. Accordingly, we removed the subtrees rootedat these nodes from the relevant disease trees in MeSHbefore evaluating the different methods performance.Only one whole disease tree, C11 (Eye Diseases), wasremoved, because the entire C11 tree was used formethoddevelopment.There are two other MeSH disease trees that were alsoremoved before evaluation: C21, Diseases of Environ-mental Origin, which included only 3 diseases with asso-ciated genes, and C22, Animal Diseases, which containedno diseases with associated genes. We therefore reportaveraged MeSH results over the remaining 23 MeSH dis-ease categories.Disease genesWe use disease genes to calculate pairwise similarityof diseases. For our comparison to MeSH, we gathereddisease-gene associations from the Online MendelianInheritance in Man (OMIM) database [19] and the Geno-pedia compendium in the HuGE database of HumanGenetic Epidemiology [20], both downloaded on February3rd, 2016. OMIM contains human genes, phenotypes(typically specific diseases), and information about rela-tionships between them. In particular, OMIM phenotypesinclude Mendelian disorders, whose associated genes areeither known or not yet known, as well as mutations thatincrease susceptibility to infection, cancer, or drugs [21].Table 1 Subnetworks of the Disease OntologyRoot disease #Diseases (nodes) #Edges #Nodes with 1 parent #Nodes with 2 parents #Nodes with 3 parentsDisease 2,039 2,095 1,982 55 1Cardiovascular disease 141 141 139 1 0Gastrointestinal disease 115 118 110 4 0Musculoskeletal disease 133 135 129 3 0Nervous System disease 308 324 291 15 1The entire Disease Ontology (root = Disease) and four subnetworks of various sizes extracted from it. The original DO and its subnetworks are tree-like: 1) the numbers ofedges are close to n ? 1, where n is the number of nodes and 2) only a small fraction of nodes have 2 or more parentsPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 4 of 11Table 2 Four MeSH subtrees of various sizes used for methoddevelopmentRoot disease #Diseases (nodes) #EdgesInfant, Premature, Diseases 6 5Dementia 13 12Respiration disorders 23 22Eye diseases 149 178Genopedia includes links to articles on epidemiologicalstudies that identify gene-disease interactions. The major-ity of these are discovered through association stud-ies; linkage mapping and animal studies are specificallyexcluded [20]. We combined disease-gene associationsfrom the two databases as in our previous work [1],using the MEDIC merged disease vocabulary (down-loaded from the Comparative Toxicogenomics Database[22] on February 3rd, 2016). This combined data set con-tains 2755 diseases and 12,873 genes.To infer hierarchies based on DO terms with thisdisease-gene data, however, required converting theMeSH disease terms to DO terms. The DO obo file pro-vides synonym information for this conversion. However,because not every MeSH term has a DO equivalent, norvice-versa, the mapped disease gene data set included1790 DO terms with 12,230 associated genes. The DiseaseOntology actually includes 6932 disease nodes, so theresulting DAG of diseases with associated genes waslargely disconnected.For the DO analysis, we therefore augmented the dis-ease gene data with disease-gene associations from theDISEASES database [23] (downloaded on August 5th,2016) which directly uses DO terms. We used the filteredversion of the DISEASES database which provides non-redundant disease-gene association pairs, and selectedonly associations derived from experiments or databasecuration (knowledge), which we expect to be of rela-tively high confidence. The DISEASES data included 772disease terms and 13,059 genes. When combined withthe mapped data from the MeSH comparison, the totalyielded 2039 DO terms with 16,404 associated genes, pro-ducing a sufficiently connected ontology for our purposes.Although this number of disease genes seems high, notethat our genes are really referring to entities with distinctHGNC official gene symbols, as reported in the NCBIGene database and associated with some disease term inthe databases described. Some HGNC symbols refer todistinct subunits of genes, while a few (under 3.5%) referto non-coding sequences that have either been shown toplay a regulatory role in disease, or that are locations ofSNPs linked to disease in GWAS studies. At most 250such non-coding entities are implicated in more than onedisease and might therefore potentially play a role in ouranalyses.Measuring pairwise similarityFor our inference algorithms we needmethods tomeasuresimilarities both between pairs of diseases and betweenpairs of genes. To calculate pairwise similarity betweendiseases A and B, disease_sim(A,B), let GA be the set ofassociated genes for diseaseA andGB the set of associatedgenes for disease B. We then use the Jaccard Index [24] torepresent the similarity between the disease gene sets asfollows:disease_sim(A,B) = Jaccard(GA,GB) = |GA ? GB||GA ? GB|To calculate pairwise similarity between genes g1 andg2, gene_sim(g1, g2), we do the opposite, as we are inter-ested in measuring the similarity of diseases with respectto their associated genes:gene_sim(g1, g2) = Jaccard(Dg1 ,Dg2) =|Dg1 ? Dg2 ||Dg1 ? Dg2 |whereDg1 is the set of diseases associated with gene g1 andDg2 is the set of diseases associated with gene g2.Note that no information about the relationshipsbetween diseases other than this measure of overlappingdisease genes is incorporated into this similarity matrix orused by our inference algorithms.Inference strategiesClique Extracted Ontology (CliXO)To use CliXO to generate disease ontologies, we begin bycreating a matrix containing the Jaccard similarity scorebetween genes as defined above. CliXO uses this similar-ity matrix as input. It also relies on two parameters: ?,which represents the amount of noise allowed in formingcliques, and ? , which represents missing data. The algo-rithm is demonstrated to be relatively robust to variationin ? , so we set ? = 0.5 as done by the CliXO team [5].Variation in ? has higher impact on the results, so tuningit to the data set is suggested. We chose ? = 0.05 becauseit produced reasonable-sized output graphs in our initialexperiments on the four MeSH subtrees in Table 2.Initially, CliXO returns a DAG whose internal nodescorrespond to sets of genes, not to specific disease termsin the reference ontology. We then used the ontologyalignment technique of [6] to align the resulting ontol-ogy to the MeSH reference or to the Disease Ontology,in order to identify disease terms in the output DAG.Accordingly, some of the disease terms may not be repre-sented in the CliXO output, because they fail to map toany node. (Fig. 1 demonstrates the topological differencefor a small example; note that the CliXO output on theright maps only 5 of the 6 disease nodes.)Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 5 of 11a) b)Fig. 1 Topological difference between MeSH and the corresponding inferred ontology using CliXO. a A MeSH subtree containing prematuritycomplications. b Corresponding Disease Ontology inferred using CliXO and ontology alignment. Drawn in Cytoscape v. 3.3.0 [30]Parent PromotionWe introduce a new technique we call Parent Promotionthat focuses on similarities in disease genes. The idea isto group diseases by their similarity scores and use hierar-chical clustering to form subgroups. Parent-child relationsare then created from these subgroups by counting cita-tion frequency in PubMed.Specifically, we transform the pairwise similarity scoreinto a distance by subtracting it from 1. We then performcomplete-linkage hierarchical clustering on the diseaseterms using the hclust function in R with these dis-tances. Internal nodes in this dendrogram correspond tosets of diseases. To convert the resulting dendrogram toa hierarchy with a single disease at each node, we iden-tify the number of disease-related articles in PubMedfor each disease in a cluster using the NCBIs E-utilities(http://www.ncbi.nlm.nih.gov/books/NBK25501/).Working up from the bottom of the dendrogram, thedisease term with the most citations is promoted tobecome the parent, with all other diseases in the clus-ter left as its children. Once defined as a child, a diseasedoes not have another chance to be promoted. That is,we only consider the most recently promoted disease andits siblings in a cluster when deciding the next parent.Figure 2 shows an example of how the dendrogram guidesthe Parent Promotion process.Notice that the inferred tree created by the Parent Pro-motion technique always has the same number of diseases(nodes) as the reference. However, the number of edgesmay differ from that of the reference, which may be eitherimplicitly or explicitly a DAG. In either case, Parent Pro-motion may therefore produce a result with fewer edges.Minimumweight spanning treeWe also compared our new Parent Promotion methodto the standard technique of finding a Minimum WeightSpanning Tree (MWST) [25] over the complete networkof disease terms, with pairwise similarity scores betweendiseases as edge weights. The idea behind this is that arepresentation of the relationships between diseases thatconnects all the disease terms by their highest diseasegene similarity represents a minimum-length descriptionof the data that seems likely to capture real disease rela-tionships. The MWST is unrooted, so we choose thedisease with themost related PubMED articles as the root.Evalution metricsComparing the inference methods remains challengingdue to the topological differences of the output. In par-ticular, both Parent Promotion and MWST produce treeswhose n nodes are exactly those of the reference hierar-chy. In contrast, the DAG output by the CliXO methoda) b) c)Fig. 2 How the Parent Promotion method transforms a dendrogram created by hierarchical clustering. a Dendrogram for diseases of infants bornpreterm. Hierarchical clustering builds a tree whose internal nodes are hard to interpret. b Parent Promotion finds the most general disease termfrom each cluster and promotes it as an internal node. An internal node becomes the parent of all other nodes in the same cluster. Disease term 3has the most citations and keeps being selected for promotion until it becomes the root. Disease term 6 has more citations than 5 and is promotedas the parent of 5. However, it later becomes a child of 3 because it has fewer citations than 3. c Final tree built by Parent PromotionPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 6 of 11may be much larger (as in Fig. 1). We use multiple meth-ods to quantify and compare performance despite thesedifferences.Edge Correctness (EC)Inspired by the notion of Edge Correctness (EC) usedin network alignment [14] we measure the number ofedges that are identical to those in the reference hierar-chy. Unlike in the network alignment problem, which usesEdge Correctness as a proxy for node correctness, for thisproblem we know the node correctness and wish to mea-sure correctly inferred edges. We count edges as correctlymatched if and only if the parent child relations (boththe edges and the directions of the edges) are preserved.To create an overall score we calculate the percentage ofedges in the reference that also appear in the inferredontology.Ancestor Correctness (AC)While Edge Correctness (EC) can measure how well twonetworks are aligned, it may not be the best methodfor evaluating disease taxonomies. In particular, dis-eases separated by multiple taxonomic links may stillbe closely related to each other, so EC can underesti-mate performance by ignoring the ancestor-descendantrelationship. EC also rewards successfully matched edgeswith no penalty for incorrect ones. This propertymay favor CliXO, which tends to produce DAGs withmany edges.To address the first shortcoming, we introduce thenotion of Ancestor Correctness (AC). For a disease x,let xref be a node representing x in the reference ontol-ogy and xinf be a node representing x in our inferredhierarchy. Also let A(x) be the set of all ancestorsof x in the appropriate hierarchy. Then for a specificdisease xinf in the inferred taxonomy we can mea-sure how well it matches the reference by calculatingAncestorJaccard = Jaccard(A(xref ),A(xinf )). We can thenapply AncestorJaccard globally by averaging across all dis-eases in the inferred network. We report this average asour AC score for the inferred network. Note that we onlyconsider diseases existing in both hierarchies. However,we exclude diseases that are roots in both because they donot have any ancestors.Ancestor Precision and Recall (AP and AR)Ancestor Correctness (AC) provides a good estimate oftopological similarity in terms of the number of preservedancestors of mapped nodes. However, it still does notpenalize false positives.To address this problem, we adapt the HierarchicalPrecision (HP) and Hierarchical Recall (HR) measure-ments from Verspoor et al. [17]. These measurementscompare the sets of all ancestors of a disease in theinferred hierarchy to the ancestors of the same term in thereference. Informally, HP is the fraction of xs ancestorsin the inferred hierarchy that are correct, while HR is thefraction of true ancestors of x that are also predicted by aninference method to be ancestors of x.More specifically, for a disease x, let xref be the node inthe reference and xinf be the node in the inferred ontology.Then our HP and HR are calculated as follows:HP(xref , xinf ) =|A(xref ) ? A(xinf )||A(xinf )| (1)HR(xref , xinf ) =|A(xref ) ? A(xinf )||A(xref )| (2)We also calculate an F score using HP and HR as:F(x) = 2 × HP(x) × HR(x)HP(x) + HR(x) (3)Finally, we define Ancestor Precision (AP) and Ances-tor Recall (AR) to be the average of HP and HR across alldiseases in our reference hierarchy.ResultsComparison to MeSHWe ran all three algorithms on the disease gene dataand disease terms from each of the 23 MeSH trees.Table 3 reports the averaged performance across all 23trees for each method and the different evaluation crite-ria. Across this data set, we see that Parent Promotionon average outperforms CliXO and MWST for almostall evaluation measures. The only exception is AncestorRecall, for which MWST slightly edges out Parent Pro-motion. Detailed performance on eachMeSH disease treeis shown in Additional file 1; in most cases the meth-ods relative performance is similar to that in Table 3.The detailed table also shows that, for each evaluationcriterion, performance of the different methods is highlycorrelated across the 23 disease trees, suggesting thatsome trees are more consistent with the disease gene datathan others.Comparison to the Disease OntologyWe first attempted to reconstruct all of the Disease Ontol-ogy reflected in our disease-gene data set (2095 edgesconnecting 2039 DO terms). However, we could not com-pare the performance of all three inference methods onthis full data set because running CliXO, which has atits core the computationally hard problem of findingcliques, was infeasible on a data set this large and complex.Nonetheless, we found that Parent Promotion consistentlyoutperformed MWST on this large data set. Specifically,Parent Promotion had an EC of 0.07 compared toMWSTsEC of 0.05, an AC of 0.23 compared to MWSTs AC of0.04, and an F score of 0.40 compared to MWSTs 0.08.We used the subnetworks of DO listed in Table 1 tocompare all three methods. Table 4 shows the results ofPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 7 of 11Table 3 Average performance of inference methods across the MeSH treesMethod EC (± stdev) AC (± stdev) AP (± stdev) AR (± stdev) F (± stdev)Parent Promotion 0.13 (± 0.06) 0.30 (± 0.10) 0.46 (± 0.16) 0.47 (± 0.14) 0.47 (± 0.15)CliXO 0.12 (± 0.10) 0.22 (± 0.12) 0.30 (± 0.14) 0.38 (± 0.17) 0.33 (± 0.15)MWST 0.07 (± 0.04) 0.11 (± 0.07) 0.13 (± 0.08) 0.48 (± 0.18) 0.21 (± 0.11)Average Edge Correctness (EC), Ancestor Correctness (AC), Ancestor Precision (AP), Ancestor Recall (AR) and F-score across the different trees in the MeSH forest. Standarddeviation is shown in parentheses. Best performance across different inference techniques is highlighted in italicall three methods on these subnetworks of DO. We againsee that in most cases Parent Promotion outperformsCliXO and MWST for each evaluation measure, with theexception of Musculosketal Disease, where CliXO out-performs Parent Promotion and MWST. Again, MWSToften has good Ancestor Recall despite unimpressive per-formance on most other metrics.Figure 3 shows an example of one of the larger con-nected components inferred by Parent Promotion usingthe DO data. All edges in the figure occur in both theDisease Ontology and the inferred tree. Although theinferred tree is relatively flat, the figure demonstrates thatinference method is capturing some logical relationshipsbetween diseases.Data sources and quantity matterWe investigated the influence of the type and amountof data using Parent Promotion on the MeSH diseasetrees. First, we tried using data from just OMIM or justGenopedia. OMIM has a higher percentage of monogenicdiseases identified using classical methods such as posi-tional cloning, while Genopedia has a higher percentageof GWAS data. On the other hand, OMIM includes muchless data, containing just 2434 genes linked to 1173 dis-orders, whereas Genopedia contains 12,527 genes impli-cated in 2499 disorders. Therefore, it is not surprisingthat performance on the Genopedia data exceeds thaton the OMIM data, nearly across the board. The excep-tion, interestingly, is C16, Congenital, Hereditary, andNeonatal Diseases and Abnormalities, where the OMIM-only version outperforms Genopedia-only by the AC, AP,and F measures. This seems likely to be because thisMeSH tree includes many hereditary disorders whosegenes are particularly likely to be included in OMIM.Detailed results for this comparison appear in Additionalfile 2. (EC is omitted because it is uninformative for manyof the smaller data sets.)In most cases, furthermore, the combination of thetwo data sources is better than either alone. There area few cases where performance declines slightly withboth compared to just Genopedia, but in those cases theOMIM data actually adds just a handful of genes thatarent already in the Genopedia data, and the changesin performance are small, consistent with small randomperturbations.To further explore the hypothesis that more data pro-duces better results, we also ran an experiment where werandomly removed 25% or 50% of the disease-gene asso-ciations from each MeSH tree, and again tried to infertrees via Parent Promotion. On average, performanceon all measures improved with more data, although theeffects on most individual trees were modest (results arein Additional file 3).DiscussionOverall, these experiments have provided some impor-tant insights into what can and cannot be learned aboutdisease relationships from disease genes alone.Table 4 Evaluation results for four DO subnetworksEdge Correctness Ancestor Correctness F-score (Ancestor precision, ancestor recall)Parent Parent ParentRoot disease Promotion CliXO MWST Promotion CliXO MWST Promotion CliXO MWSTCardiovascular disease 0.06 0.09 0.07 0.32 0.18 0.11 0.50 0.27 0.21(0.57, 0.44) (0.24, 0.30) (0.13, 0.48)Gastrointestinal disease 0.17 0.13 0.03 0.37 0.26 0.14 0.55 0.39 0.26(0.56, 0.53) (0.36, 0.42) (0.18, 0.48)Musculoskeletal disease 0.16 0.08 0.10 0.15 0.26 0.09 0.26 0.41 0.17(0.44, 0.18) (0.42, 0.40) (0.16, 0.19)Nervous System disease 0.13 0.07 0.09 0.29 0.17 0.10 0.46 0.30 0.19(0.70, 0.34) (0.26, 0.34) (0.13, 0.34)Average Edge Correctness (EC), Ancestor Correctness (AC), Ancestor Precision (AP), Ancestor Recall (AR) and F-score across four DO subnetworks. Standard deviation is shownin parentheses. Best performance across different inference techniques is highlighted as italicPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 8 of 11Fig. 3 Parent Promotion tree using DO data. Subtree of the disease tree built by Parent Promotion on DO musculoskeletal system disease data thatis an exact match to nodes and edges in the DOThe correlations observed across the MeSH trees sug-gest that disease relationships in some MeSH categoriesare easier to learn than others. Correctness appears tobe higher for smaller trees, perhaps simply because thereare fewer possibilities. However, there are some largedisease subtrees with higher AC and EC scores, espe-cially Endocrine System Diseases (C19), Nutritional andMetabolic Diseases (C18), and Respiratory Tract Dis-eases (C08).It is possible that the MeSH hierarchy in these areas isbetter defined by molecular data, or that there are sim-ply more disease genes known in these areas than in someothers. One observation is that these categories includeseveral well-studied complex diseases with high publichealth impact. For example, C19 includes diabetes andovarian and pancreatic cancer; C18 also includes diabetes,plus obesity and related conditions; and C08 featuresasthma, COPD, and several types of lung cancer. Whichexact properties of a set of diseases contribute most to thesuccess of inference algorithms is an important questionfor future work.On the Musculoskeletal Disease DO subnetwork,CliXO outperforms Parent Promotion by several criteria.Parent Promotion struggles with this region of the Dis-ease Ontology, in part because the term MusculosketalDisease has fewer PubMed citations than the less gen-eral term Bone Disease. The latter is therefore promotedincorrectly to become the root, while the former remainslow in the inferred tree.We also notice that despite its relatively poor per-formance overall, MWST seems to have good AncestorRecall in many cases, sometimes even beating other meth-ods. This may be because MWST tends to infer tall, thintrees rather than short and broad ones. Figure 4 illustratesthis tendency. A node has more ancestors in tall, thin treesthan in broad trees, and as a result, is more likely to shareancestors with the reference.By attempting to infer relationships for each MeSH dis-ease category separately, or within specific subnetworksof the Disease Ontology, most of the work described herehas only a limited ability to detect novel molecular con-nections across diseases currently thought to be unrelated.However, we can begin to address the question of whethersuch discovery is possible with these methods by lookingat the performance of Parent Promotion on data from thefull Disease Ontology, and by examining inferred edgesconnecting pairs of disease terms that are not directlyconnected in the DO.We found 1900 such pairs. Most of these make unsur-prising connections. For example, progressive muscularatrophy was, in our inferred hierarchy, directly con-nected to spinal muscular atrophy because they share 34genes (all of those associated with the first disease term).Other pairs may span different medical domains and tis-sues yet have well-known commonalities that are alreadydescribed in existing hierarchies (e.g. rheumatoid arthri-tis and type I diabetes mellitus, both of which are listed asautoimmune disorders in MeSH).However, there are other inferred edges whose rela-tionships are plausible but not currently characterized.For example, liver cirrhosis and pre-eclampsia share anedge in our inferred hierarchy because they have largeand highly overlapping sets of associated genes. Thesedisorders initially appear to affect very different anotom-ical systems and processes; both the Disease OntologyandMeSH categorize pre-eclampsia under cardiovasculardisease/hypertension (MeSH also lists it as a pregnancycomplication), while cirrhosis is represented primarilyas a liver disease in both hierarchies. Yet there is evi-dence that cirrhosis elevates the risk of pre-eclampsiaduring pregnancy [26]. There are also specific cases(e.g. HELLP syndrome, characterized by hemolysis, ele-vated liver enzymes, and low platelet count) that linkliver dysfunction with increased pre-eclampsia risk [27].As another example, fatty liver disease is also surpris-ingly linked to pterygium or surfers eye, character-ized by fleshy growths of the eye that are linked tosunlight exposure. Molecular markers associated withPark et al. Journal of Biomedical Semantics  (2017) 8:25 Page 9 of 11a)b)c)Fig. 4 A MeSH tree rooted at Respiration Disorder and corresponding inferred disease trees. a The MeSH tree containing Respiration Disorderand its descendants. b The disease tree inferred by Parent Promotion on data from the tree in a). c The disease tree inferred by MWST from the samedata. MWST builds a taller and slimmer tree. As a result, most diseases have more ancestors in c) than in a) or b). This leads MWST to have goodperformance with respect to Ancestor Recall (AR)pterygium appear to be associated with cell migrationor involved with epithelial-to-mesenchymal transition(EMT) [28], a class of genes also thought to play arole in how the liver responds to injury such as thatcaused by fatty liver disease [29]. Future work explor-ing the implication of such potential connections may bewarranted.ConclusionsWe have demonstrated that it is possible to recover muchof the structure of both MeSH disease trees and theDO from molecular data alone. However, this work is apreliminary analysis, and there is much more to learn.Although our aim in this project has been only toinfer gene-based relationships between disease terms inexisting taxonomic systems, one ultimate goal for a 21st-century disease taxonomy is the inference of new diseaseterms based on molecular information [4, 7]. Classifica-tion of cancer or autism subtypes based on underlyinggenetic contributions, for example, might be possible insuch a system.The examples in the previous section of discoveringlinks across apparently disparate disease types raise thepossibility that novel connections in the inferred hierar-chies for the full Disease Ontology data may correspondto novel disease subtypes with commonmolecular causes.Thus the discovery of new disease terms could arisefrom future work based on such analyses. Of the meth-ods described here, CliXO is the only one that mightdirectly address this problem, by inferring internal nodescorresponding to sets of genes and then by finding newmethods to map these gene sets into plausible diseaseclasses. Further exploration of its abilities to do so, orextension of clustering-based methods analogous to Par-ent Promotion to incorporate comparable possibilities, iswarranted.Taxonomy inference using data from diseases acrossorgan systems and tissues, such as that in the full Dis-ease Ontology data set, may also lead to improvedcategorization of disease processes. Subgraphs of theinferred hierarchies may represent disease groups spe-cific to certain anatomical systems, and investigation ofdisease genes associated with such a subgraph mightprovide some insights into anatomical expression and rel-evance of disease genes. However, to identify inferredsubgraphs representing specific anatomical systems wewould need a comprehensive mapping between DO termsand these systems. The development of such a mappingand further interpretation of the substructure in suchbroad inferred hierarchies remains an interesting openquestion.Future work may also include exploring the incorpora-tion of tissue specific gene expression to integrate relevanttissues and organs with the molecular level data, andto look more broadly at ways to combine clinical andmolecular data. We also have not yet fully explored therange of relevant tree- and DAG-inference methods fromthe machine-learning community. However, the currentresults leave us optimistic that by including molecularinformation, it will be possible to construct integrated dis-ease taxonomies that better support medical research inthe genomic era.Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 10 of 11Additional filesAdditional file 1: Performance of three disease hierarchy inferencealgorithms (Parent Promotion, CliXO, MWST): Edge Correctness, AncestorCorrectness, Ancestor Precision/Recall and F-score for 23 MeSH trees.(PDF 78 kb)Additional file 2: Performance of Parent Promotion using disease-geneassociation information in OMIM, Genopedia and combination of two:Ancestor Correctness, Ancestor Precision/Recall and F-score for 23 MeSHtrees. (PDF 61 kb)Additional file 3: Change in performance of Parent Promotiondepending on the size of disease-gene association information: EdgeCorrectness, Ancestor Correctness, Ancestor Precision/Recall and F-scorefor 23 MeSH trees. (PDF 53 kb)AbbreviationsAC: Ancestor Correctness; AP: Ancestor precision; AR: Ancestor recall; CliXO:Clique Extracted Ontology; DAG: Directed acyclic graph; DO: DiseaseOntology; EC: Edge Correctness; HP: Hierarchical precision; HR: Hierarchicalrecall; HuGE database: Human genome epidemiology database; ICD:International classification of diseases; MeSH: Medical subject heading; MWST:Minimum weight spanning tree; NCBI: National Center for BiotechnologyInformation; OMIM: Online Mendelian inheritance in man; PheWAS: Phenomewide association studies; SNOMED CT: Systematized nomenclature ofmedicine, clinical terms; UMLS: Unified medical language systemAcknowledgementsWe thank Karin Verspoor for suggesting hierarchical precision and recall,Stephan Schürer for recommending the DISEASES database, Trey Ideker andMichael Kramer for providing the CliXO code and thoughtful advice, InbarFried for comments about measurement for network alignment, andmembers of the Tufts Bioinformatics and Computational Biology group forhelpful feedback and comments.FundingResearch reported in this publication was supported by NIH awardR01HD076140. The content is solely the responsibility of the authors and doesnot necessarily represent the official views of the National Institutes of Health.Availability of data andmaterialsNot applicable.Authors contributionsDKS, BJH, and JP conceived and designed the overall research strategy,analyzed the experimental results and wrote the manuscript. JP implementedthe Parent Promotion method and ran the experiments. All authors read andapproved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationAll authors read and approved the final version of the manuscript.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Computer Science, Tufts University, 161 College Avenue,Medford, MA 02155, USA. 2Department of Integrative Physiology andPathobiology, Tufts University School of Medicine, 145 Harrison Avenue,Boston, MA 02111, USA.Received: 8 November 2016 Accepted: 17 July 2017Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 DOI 10.1186/s13326-017-0119-zRESEARCH Open AccessEvaluating the effect of annotation sizeon measures of semantic similarityMaxat Kulmanov1,2 and Robert Hoehndorf1,2*AbstractBackground: Ontologies are widely used as metadata in biological and biomedical datasets. Measures of semanticsimilarity utilize ontologies to determine how similar two entities annotated with classes from ontologies are, andsemantic similarity is increasingly applied in applications ranging from diagnosis of disease to investigation in genenetworks and functions of gene products.Results: Here, we analyze a large number of semantic similarity measures and the sensitivity of similarity values to thenumber of annotations of entities, difference in annotation size and to the depth or specificity of annotation classes.We find that most similarity measures are sensitive to the number of annotations of entities, difference in annotationsize as well as to the depth of annotation classes; well-studied and richly annotated entities will usually show highersimilarity than entities with only few annotations even in the absence of any biological relation.Conclusions: Our findings may have significant impact on the interpretation of results that rely on measures ofsemantic similarity, and we demonstrate how the sensitivity to annotation size can lead to a bias when using semanticsimilarity to predict protein-protein interactions.Keywords: Semantic similarity, Ontology, Gene ontologyBackgroundSemantic similaritymeasures are widely used for datamin-ing in biology and biomedicine to compare entities orgroups of entities in ontologies [1, 2], and a large numberof similarity measures has been developed [3]. The sim-ilarity measures are based on information contained inontologies combined with statistical properties of a cor-pus that is analyzed [1]. There are a variety of uses forsemantic similarity measures in bioinformatics, includ-ing classification of chemicals [4], identifying interactingproteins [5], finding candidate genes for a disease [6], ordiagnosing patients [7].With the increasing use of semantic similarity measuresin biology, and the large number of measures that havebeen developed, it is important to identify a method toselect an adequate similarity measure for a particular pur-pose. In the past, several studies have been performed*Correspondence: robert.hoehndorf@kaust.edu.sa1Computational Bioscience Research Center, King Abdullah University ofScience and Technology, 23955-6900, Thuwal, Saudi Arabia2Computer, Electrical and Mathematical Sciences and Engineering Division,King Abdullah University of Science and Technology, 23955-6900, Thuwal,Saudi Arabiathat evaluate semantic similarity measures with respect totheir performance on a particular task such as predictingprotein-protein interactions through measures of func-tion similarity [810]. While such studies can provideinsights into the performance of semantic similarity mea-sures for particular use cases, they do not serve to identifythe general properties of a similarity measure, and thedataset to be analyzed, based on which the suitability ofa semantic similarity measure can be determined. Specif-ically, when using semantic measures, it is often usefulto know how the annotation size of an entity affectsthe resulting similarity, in particular when the corpus towhich the similarity measure is applied has a high vari-ance in the number of annotations. For example, somesemantic similarity measures may always result in highersimilarity values when the entities that are compared havemore annotations and may therefore be more suitableto compare entities with the same number of annota-tions. Furthermore, the difference in annotation size canhave a significant effect on the similarity measure sothat comparing entities with the same number of anno-tations may always lead to higher (or lower) similarity© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 2 of 10values than comparing entities with a different number inannotations.Here, we investigate features of a corpus such as thenumber of annotations to an entity and the variance (ordifference) in annotation size on the similarity measuresusing a large number of similarity measures implementedin the Semantic Measures Library (SML) [11]. We findthat different semantic similarity measures respond differ-ently to annotation size, leading to higher or lower seman-tic similarity values with increasing number of annota-tions. Furthermore, the difference in the number of anno-tations affects the similarity values as well. Our resultshave an impact on the interpretation of studies that usesemantic similarity measures, and we demonstrate thatsome biological results may be biased due to the choiceof the similarity measure. In particular, we show that theapplication of semantic similarity measures for predictingprotein-protein interactions can result in a bias, similarlyto other guilt-by-association approaches [12], in whichthe sensitivity of the similarity measure to the annotationsize confirms a bias present in protein-protein interactionnetworks so that well-connected and well-annotated pro-teins have, on average, a higher similarity by chance thanproteins that are less well studied.MethodsGeneration of test dataWe perform all our experiments using the Gene Ontology(GO) [13], downloaded on 22 December 2015 from http://geneontology.org/page/download-ontology and HumanPhenotype Ontology (HPO) [14], download on 1 April2016 from http://human-phenotype-ontology.github.io/downloads.html in OBO Flatfile Format. The version ofGO we use consists of 44,048 classes (of which 1941 areobsolete) and HPO consists of 11,785 classes (of which112 are obsolete). We run our experiments on several dif-ferent sets of entities annotated with different numberof GO or HPO classes and one set of entities anno-tated with GO classes from specific depth of the graphstructure. The first set contains 5500 entities and we ran-domly annotated 100 entities each with 1, 2, . . . , 54, 55 GOclasses. We generate our second set of entities annotatedwith HPO classes in the same fashion. The third set is aset of manually curated gene annotations from the yeastgenome database file (gene_associations.sgd.gz) down-loaded on 26 March 2016 from http://www.yeastgenome.org/download-data/curation. The dataset consists of 6108genes with annotations sizes varying from 1 to 55, andeach group of the same size contains a different number ofgene products. We ignore annotations with GO evidencecode ND (No Data). The fourth set contains 1700 entitieswhich is composed of 17 groups. Each group have 100 ran-domly annotated entities with GO classes from the samedepth of the ontology graph structure.Computing semantic similarityAfter the random annotations were assigned to the enti-ties, we computed the semantic similarity between eachpair of entities using a large set of semantic similaritymeasures. We include both groupwise measures and pair-wise measures with different strategies of combining them[1]. Groupwise similarity measures determine similaritydirectly for two sets of classes. On the other hand, indirectsimilarity measures first compute the pairwise similari-ties for all pairs of nodes and then apply a strategy forcomputing the overall similarity. Strategies for the latterinclude computing the mean of all pairwise similarities,computing the Best Match Average, and others [1].Furthermore, most semantic similarity measures relyon assigning a weight to each class in the ontology thatmeasures the specificity of that class. We performed ourexperiments using an intrinsic information content mea-sure (i.e., a measure that relies only on the structureof the ontology, not on the distribution of annotations)introduced by [15].The semantic similarity measures we evaluated includethe complete set of measures available in the SemanticMeasures Library (SML) [11], and the full set of measurescan be found at http://www.semantic-measures-library.org. The SML reduces an ontology to a graph structurein which nodes represent classes and edges in the graphrepresent axioms that hold between these classes [16, 17].The similarity measures are then defined either betweennodes of this graph or between subgraphs.The raw data and evaluation results for all similaritymeasures are available as Additional file 1: Table S1. Thesource code for all experiments is available on GitHub athttps://github.com/bio-ontology-research-group/pgsim.Measuring correlationIn order to measure the sensitivity of the similarity mea-sures to the number of annotations we calculated Spear-man and Pearson correlation coefficients between set ofannotations sizes and the set of average similarity of onesize group to all the others. In other words, we first com-puted the average similarities for each entity in a groupwith fixed annotation size and computed the average sim-ilarity to all entities in our corpus. For calculating thecorrelation coefficients we used SciPy library [18].Protein-protein interactionsWe evaluate our results using protein-protein interac-tion data from BioGRID [19] for yeast, downloaded on26 March 2016 from http://downloads.yeastgenome.org/curation/literature/interaction_data.tab. The file contains340,350 interactions for 9868 unique genes. We filteredthese interactions using the set of 6108 genes from theyeast genome database and our final interaction datasetincludes 224,997 interactions with 5804 unique genes.Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 3 of 10Then we compute similarities between each pair of genesusing simGIC measure [1] and Resniks similarity mea-sure [20] combined with Average and Best Match Average(BMA) strategies and generate similarity matrices. Addi-tionally, we create a dataset with random GO annotationsfor the same number of genes, and the same number ofannotations for each gene. We also generate the similaritymatrices for this set using the same similarity measures.To evaluate our results, we use the similarity values asa prediction score, and compute the receiver operatingcharacteristic (ROC) curves (i.e., a plot of true positiverate as function of false positive rate) [21] for each similar-ity measure by treating pairs of genes that have a knownPPI as positive and all other pairs of proteins as negatives.In order to determine if our results are valid forprotein-protein interaction data from other organisms,we perform a similar evaluation with mouse and humaninteractions. We downloaded manually curated genefunction annotations from http://www.geneontology.org/gene-associations/ for mouse (gene_associations.mgi.gz)and human (gene_associations.goa_human.gz) on 12November 2016. The mouse annotations contain 19,256genes with annotations size varying from 1 to 252 andhuman annotations contain 19,256 genes with annota-tions size varying from 1 to 213. We generate randomannotations with the same annotations sizes for bothdatasets and compute similarity values using Resnikssimilarity measure combined with BMA strategy. Forpredicting protein-protein interactions we use BioGRIDinteractions downloaded on 16 November 2016 fromhttps://thebiogrid.org/download.php. There are 38,513gene interactions for mouse and 329,833 interactions forhuman.Gene-Disease associationsTo evaluate our results with differnt ontologies, we aimto predict genedisease associations using phenotypicsimilarity between genes and diseases. We use mousephenotype annotations and mouse genedisease associ-ations downloaded from http://www.informatics.jax.org/downloads/reports/index.html (MGI_PhenoGenoMP.rptandMGI_Geno_Disease.rpt). The dataset contains 18,378genes annotated with Mammalian Phenotype Ontology(MPO) [22] classes with size varying from 1 to 1671, and1424 of genes have 1770 associations with 1302Mendeliandiseases. We downloaded Mendelian disease phenotypeannotations from http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/lastStableBuild/ and generatedrandom annotations with the same sizes for both gene anddisease annotation datasets. We computed similarity ofeach gene to each disease by computing the Resniks simi-larity measure combined with BMA strategy between setsof MPO terms and HPO terms based on PhenomeNETOntology [6]. Using this similarity value as a predictionscore we computed ROC curves for real and randomannotations.Results and discussionOur aim is to test threemain hypothesis. First, we evaluatewhether the annotation size has an effect on similar-ity measures, and quantify that effect using measures ofcorrelation and statistics. We further evaluate whetherannotation size has an effect on the variance of similarityvalues. Second, we evaluate whether the difference in thenumber of annotations between the entities that are com-pared has an effect on the similarity measure, and quan-tify the effects through measures of correlation. Third,we evaluate whether the depth of the annotation classeshas an effect on similarity measures. Finally, we classifysemantic similarity measures in different categories basedon how they behave with respect to annotation size, differ-ences in annotation size and depth of annotation classes,using the correlation coefficients between similarity value.To measure the effects of annotation size, we fix thenumber of annotations of entities in our test corpus, andcompare those with a certain number of annotations to allother entities. As we have generated 100 entities for eachof the 55 annotation sizes in our corpus, we obtain a distri-bution of 550,000 (100 × 5500) similarity values for eachannotation size. In the resulting distribution of similarityvalues, we compute average (arithmetic mean) similarityand variance. To determine if, and how much, the sim-ilarity values increase with annotation size, we computeSpearman and Pearson correlation coefficients for eachsimilarity measure. The results for a selected set of simi-larity measures are shown in Table 1, and for Resniks sim-ilarity measure [20] (with the Best Match Average strategyfor combining pairwise measures) and the simGIC mea-sure [1] in Fig 1.We find that, in general and across almostall similarity measures, similarity values increase with thenumber of annotations associated with an entity. The vari-ance in the average similarities, however, either increasesor decreases with the annotation size, depending on thesimilarity measure.To determine whether the results we obtain also hold fora real biological dataset, we further evaluated the semanticsimilarity between yeast proteins using a set of selectedsemantic similarity measures. We find that the results inour test corpus are also valid for the semantic similarlyof yeast proteins. Figure 1 shows the average similarity ofyeast proteins as a function of the annotation size for twosemantic similarity measures.For example, the protein YGR237C has only a singleannotation, and the average similarly, using the simGICmeasure, is 0.035 across the set of all yeast proteins. Onthe other hand, protein CDC28, a more richly annotatedprotein with 55 annotations, has as average similarly 0.142(more than 4-fold increase). These results suggest thatKulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 4 of 10Table 1 Spearman and Pearson correlation coefficients between similarity value and absolute annotation size as well as betweenvariance in similarity value and annotation sizeSimilarity measure Spearman PearsonYeast Synthetic GO Synthetic HPO Yeast Synthetic GO Synthetic GOAverage Variance Average Variance Average Variance Average Variance Average Variance Average VarianceGIC (GraphInformationContent)0.929780 0.251586 0.970924 0.773449 0.953247 0.980159 0.861348 0.117734 0.831167 0.744321 0.802873 0.958817NTO (NormalizedTerm Overlap)0.178345 0.860012 0.248990 0.976335 0.123304 0.988240 0.014072 0.682683 0.009088 0.574883 0.158914 0.593458UI (UnionIntersection)0.892631 0.298097 0.879582 0.934921 0.729942 0.995599 0.788675 0.030649 0.777515 0.914405 0.736711 0.935415BMA withJiang, Conrath19970.960133 0.892027 0.998773 0.993506 0.999351 0.996609 0.892576 0.812184 0.895020 0.629497 0.907974 0.692269BMA withLin 19980.980519 0.800362 0.998918 0.994733 0.999134 0.998052 0.925181 0.772250 0.896497 0.638574 0.917599 0.677309BMA withResnik 19950.980519 0.717457 0.998773 0.994228 0.998918 0.998124 0.939044 0.703981 0.895107 0.642652 0.917738 0.675426BMA withSchlicker 20060.980519 0.800362 0.998918 0.994733 0.999134 0.998052 0.925181 0.772250 0.896497 0.638574 0.917599 0.677309some entities have, on average and while comparing simi-larity to exactly the same set of entities, higher similarity,proportional to the number of annotations they have.As our second experiment, we evaluate whether thedifference in annotation size has an effect on the similar-ity measure. We follow the same strategy as in our firstexperiment: we have used the same datasets but measuredthe average similarities as function of absolute differenceof compared entities. For the annotation sizes from 1 to55 we get 55 groups of similarities with annotation sizedifference from 0 to 54, and for each group we com-puted average similarity and variance in similarity values.Furthermore, we computed Pearsson and Spearman cor-relation coefficients between annotation size differenceand average similarities to determine the sensitivity of thesimilarity to annotation size difference. Figure 1 shows ourresults using synthetic data as well as functional anno-tations of yeast proteins for Resniks similarity measure(using the Best Match Average strategy) and the simGICmeasure, and Table 2 summarizes the results. Full resultsare available as supplementary material. We find that formost measures, average similarity decreases as the dif-ference in annotation size increases, while the variancein similarity values behaves differently depending on thesimilarity measure.In our third experiment, we evaluate whether the depthof the annotation classes has an effect on the similaritymeasure. We use our fourth dataset which we randomlygenerated based on the depth of classes in the GO. Themaximum depth in GO is 17, and we generate 17 groupsof random annotations. We then compute the averagesimilarity of the synthetic entities within one group toall the other groups, and report Pearsson and Spearmancorrelation coefficients between annotation class depthand average similarities to determine the sensitivity ofthe similarity to annotation class depth. Figure 1 showsour results using synthetic data as well as functionalannotations of yeast proteins for Resniks similarity mea-sure (using the Best Match Average strategy) and thesimGIC measure, and Table 2 summarizes the results. Wefind that for most measures, average similarity increaseswith the depth of the annotations, i.e., the more spe-cific a class is the higher the average similarity to otherclasses.A classification of similarity measuresOur finding allows us to broadly group semantic similar-ity measures into groups depending on their sensitivity toannotation size and difference in annotation size. We dis-tinguish positive correlation (Pearsson correlation > 0.5),no correlation (Pearsson correlation between ?0.5 and0.5), and negative correlation (Pearsson correlation< 0.5),and classify the semantic similarity measures based onwhether they are correlated with annotation size, dif-ference in annotation size, and depth. Additional file 1:Table S1 provides a comprehensive summary of ourresults.By far the largest group of similarity measures has apositive correlation between annotation size and simi-larity value, and a negative correlation between varianceand annotation size. Popular similarity measures such asResniks measure [20] with the Best Match Average com-bination strategy, and the simGIC similarity measure [23],fall in this group. A second group of similarity measuresKulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 5 of 10Fig. 1 The distribution of similarity values as a function of the annotation size (top), annotation size difference (middle) and annotation class depth(bottom) for Resniks measure (using the Best Match Average strategy) and the simGIC measurehas no, or only small, correlation between annotation sizeand similarity values, and might therefore be better suitedto compare entities with a large variance in annotationsizes. The Normalized TermOverlap (NTO)measure [24]falls into this group. Finally, a third group results in lowersimilarity values with increasing annotation size.Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 6 of 10Table 2 Spearman and Pearson correlation coefficients between similarity value and difference in annotation size as well as betweenvariance in similarity value and difference in annotation sizeSimilarity measure Spearman PearsonYeast Synthetic GO Synthetic HPO Yeast Synthetic GO Synthetic GOAverage Variance Average Variance Average Variance Average Variance Average Variance Average VarianceGIC (GraphInformationContent)0.895310 0.931818 0.999928 0.999784 0.999784 0.997835 0.875583 0.503795 0.964250 0.484246 0.963553 0.496135NTO (NormalizedTerm Overlap)0.901443 0.233045 0.999784 0.961833 0.999784 0.959524 0.882986 0.192168 0.990210 0.848649 0.993038 0.849263UI (UnionIntersection)0.909524 0.924459 1.000000 0.658658 1.000000 0.518687 0.906605 0.596963 0.963476 0.547645 0.963569 0.508495BMA with Jiang,Conrath 19970.283838 0.925830 0.902597 0.521861 0.891486 0.770130 0.074788 0.850654 0.834208 0.495874 0.848264 0.735985BMA withLin 19980.462843 0.674892 0.901587 0.552237 0.891126 0.731530 0.303157 0.707318 0.836486 0.517670 0.852998 0.693744BMA withResnik 19950.578211 0.579149 0.901587 0.537807 0.891126 0.699856 0.442458 0.487544 0.835991 0.507179 0.854007 0.670199BMA withSchlicker 20060.462843 0.674892 0.901587 0.552237 0.891126 0.731530 0.303157 0.707318 -0.836486 0.517670 0.852998 0.693744Impact on data analysisIn order to test our results on an established biologicaluse case involving computation of semantic similarity, weconducted an experiment by predicting protein-proteininteractions using the similarity measures. Prediction ofprotein-protein interactions is often used to evaluateand test semantic similarity measures [810], but simi-lar methods and underlying hypotheses are also used forcandidate gene prioritization [25] in guilt-by-associationapproaches [12].We use our manually curated set of yeast gene anno-tations and then generated random GO annotations foreach protein in this set while maintaining the annotationsize fixed. Specifically, to generate a completely randomannotation dataset, we replace each GO annotation ofeach protein in our yeast dataset by a random GO class.Thereby, the number of annotations for each proteinremains constant, while the content of the annotation isreplaced by a random GO class. We then compute pair-wise semantic similarity between the proteins, once usingthe real annotations and additionally using the randomlygenerated annotations, and we use the resulting rankingas prediction of a protein-protein interaction. Using realprotein-protein interactions from the BioGRID database[19], we compute the true positive rate and false positiverate of the predictions for each rank and plot the receiveroperating characteristic (ROC) curves for both cases. TheROC curves are shown in Fig. 2 for simGIC and Resniksimilarity measure. For example, for predicting PPIs usingResniks similarity measure and the BMA strategy, thearea under the ROC curve (ROC AUC) using real biolog-ical annotations is 0.69, while the ROC AUC for randomannotations is 0.65. Despite the complete randomizationof the annotations, ROC AUC is significantly (p ? 10?6,one-sided Wilcoxon signed rank test) better than ran-dom. We repeat this experiment with human and mousePPIs and Resniks similarity measure (Fig. 3, and find thatin each case, random annotations provide a predictivesignal. For mouse PPIs, ROC AUC with random annota-tions is 0.63 while real GO annotations result in a ROCAUC of 0.74, and for human PPIs, ROC AUC with ran-dom annotations is 0.54 and 0.58 with real annotations.In both cases, the ROC curves are significantly betterthan random (p ? 10?6, one-sided Wilcoxon signed ranktest).We further test if this phenomenon also holds for otherapplications of semantic similarity, in particular diseasegene prioritization through phenotype similarity. For thispurpose, we use the PhenomeNET systems [6, 26] andcompare the semantic similarity associated with loss offunction mouse models and human disease phenotypes.Using real annotations, ROC AUC is 0.90, while the ROCAUC for random phenotype annotations is 0.73 (Fig. 4),demonstrating that the phenomenon also holds for otheruse cases besides predicting PPIs.The good performance in predicting PPIs in the absenceof biological information is rather surprising. We hypoth-esized that well-studied proteins generally have moreknown functions and more known interactions, and alsothat genes involved in several diseases have more phe-notype annotations. The Pearson correlation coefficientbetween the number of interactions and the number offunctions in our yeast dataset is 0.34, in the human dataset0.23, and 0.36 in the mouse PPI dataset. Similarly, in ourKulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 7 of 10Fig. 2 ROC Curves for protein-protein interaction prediction using random annotations and interaction data from BioGRID for yeastdataset of genedisease associations, there is a correla-tion between the number of phenotype annotations andthe number of genedisease associations (0.42 Pearsoncorrelation coefficient). While the correlations are rela-tively small, there is nevertheless a bias that is confirmedby selecting a similarity measure that follows the sameKulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 8 of 10Fig. 3 ROC Curves for protein-protein interaction prediction using random annotations and interaction data from BioGRID for mouse and humanFig. 4 ROC Curves for gene-disease association prediction using PhenomeNet Ontology with mouse phenotype from MGI and OMIM diseasephenotype annotations compared with random annotationsKulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 9 of 10bias. We tested whether the same phenomenon occurswith another similarity measure that is not sensitive tothe annotation size or difference in annotation size. UsingResniks measure with the Average strategy for combin-ing the similarity values, we obtain a ROC AUC of 0.52when predicting yeast PPIs. Although this ROC AUC isstill significantly better than random (p ? 10?6, one-sidedWilcoxon signed rank test), the effect is much lowercompared to other measures.In the context of gene networks, prior research hasshown that the amount of functional annotation and net-work connectivity may result in biased results for certaintypes of analyses, leading the authors to conclude thatthe guilt by association principle holds only in excep-tional cases [12]. Our analysis suggests that similar biasesmay be introduced in applications of semantic similaritymeasures such that heavily annotated entities will have,on average and without the presence of any biologicalrelation between entities, a higher similarity to other enti-ties than entities with only few annotations. A similarbut inverse effect exists for differences in annotation size.Consequently, comparing entities with many annotations(e.g., well-studied gene products or diseases) to entitieswith few annotations (e.g., novel or not well-studied geneproducts) will result, on average, in the lowest similar-ity values, while comparing well-studied entities to otherwell-studied entities (both with high annotation size andno or only small differences in annotation size) will resultin higher average similarity for most similarity measureseven in the absence of any biological relation.ConclusionsWe find that the annotation size of entities clearly playsa role when comparing entities through measures ofsemantic similarity, and additionally that the difference inannotation size also plays a role. This has an impact onthe interpretation of semantic similarity values in severalapplications that use semantic similarity as a proxy forbiological similarity, and the applications include priori-tizing candidate genes [6], validating text mining results[27], or identifying interacting proteins [10]. Similarly toa previous study on protein-protein interaction networks[12], we demonstrate that the sensitivity of similarity mea-sures to annotation size can lead to a bias when predict-ing protein-protein interactions. These results should betaken into account when interpreting semantic similarityvalues.In the future, methods need to be identified to correctfor the effects of annotation size and difference in annota-tion size. Adding richer axioms to ontologies or employingsimilarity measures that can utilize axioms such as dis-jointness between classes [28] does not on its own sufficeto remove the bias we identify, mainly because the rela-tion between annotated entities (genes or gene products)and the classes in the ontologies does not consider dis-jointness axioms. It is very common for a gene product tobe annotated to two disjoint GO classes, because one geneproduct may be involved in multiple functions (such asvocalization behavior and transcription factor activity)since gene products are not instances of GO classes butrather are related by a has function relation (or similar) tosome instance of the GO class. A possible approach couldbe to rely on the exact distribution of similarity values forindividual entities [29] and use a statistical tests to deter-mine the significance of an observed similarity value. Analternative strategy could rely on expected similarity val-ues based on the distribution of annotations in the corpusand the structure of the ontology and adjusting similar-ity values accordingly so that only increase over expectedsimilarity values are taken into consideration.Additional fileAdditional file 1: Supplementary Table. (PDF 17 kb)AbbreviationsAUC: Area under curve; BMA: Best match average; GO: Gene ontology; HPO:Human phenotype ontology; NTO: Normalized term overlap; PPI:Protein-protein interaction; ROC: Receiver operating characteristic; SML:Semantic measures libraryFundingThis research was supported by funding from the King Abdullah University ofScience and Technology.Availability of data andmaterialsAll source code developed for this study is available from https://github.com/bio-ontology-research-group/pgsim.Authors contributionsRH conveived of the study, MK performed the experiments and evaluation, allauthors interpreted the results and wrote the manuscript. Both authors haveread and approved the final version of the manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Received: 15 October 2016 Accepted: 1 February 2017RESEARCH Open AccessBuilding a semantic web-based metadatarepository for facilitating detailed clinicalmodeling in cancer genome studiesDeepak K. Sharma1, Harold R. Solbrig1, Cui Tao2, Chunhua Weng3, Christopher G. Chute4 and Guoqian Jiang1*AbstractBackground: Detailed Clinical Models (DCMs) have been regarded as the basis for retaining computable meaningwhen data are exchanged between heterogeneous computer systems. To better support clinical cancer datacapturing and reporting, there is an emerging need to develop informatics solutions for standards-based clinicalmodels in cancer study domains. The objective of the study is to develop and evaluate a cancer genome studymetadata management system that serves as a key infrastructure in supporting clinical information modeling incancer genome study domains.Methods: We leveraged a Semantic Web-based metadata repository enhanced with both ISO11179 metadatastandard and Clinical Information Modeling Initiative (CIMI) Reference Model. We used the common data elements(CDEs) defined in The Cancer Genome Atlas (TCGA) data dictionary, and extracted the metadata of the CDEs usingthe NCI Cancer Data Standards Repository (caDSR) CDE dataset rendered in the Resource Description Framework(RDF). The ITEM/ITEM_GROUP pattern defined in the latest CIMI Reference Model is used to represent reusablemodel elements (mini-Archetypes).Results: We produced a metadata repository with 38 clinical cancer genome study domains, comprising a richcollection of mini-Archetype pattern instances. We performed a case study of the domain clinical pharmaceuticalin the TCGA data dictionary and demonstrated enriched data elements in the metadata repository are very usefulin support of building detailed clinical models.Conclusion: Our informatics approach leveraging Semantic Web technologies provides an effective way to build aCIMI-compliant metadata repository that would facilitate the detailed clinical modeling to support use casesbeyond TCGA in clinical cancer study domains.Keywords: Detailed Clinical Models (DCMs), Clinical Information Modeling Initiative (CIMI), Common Data Elements(CDEs), The Cancer Genome Atlas (TCGA), Cancer Studies, Semantic Web TechnologiesBackgroundDetailed Clinical Models (DCMs) have been regarded asthe basis for retaining computable meaning when data areexchanged between heterogeneous computer systems [1].Several independent clinical information modeling initia-tives have emerged, including Health Level 7 (HL7)Detailed Clinical Models (DCM) [2], ISO/CEN EN13606/Open-EHR Archetype [3], Intermountain HealthcareClinical Element Models (CEMs) [4], and the ClinicalInformation Model in the Netherlands [5]. The collectiveclinical information modeling community has recently ini-tiated an international collaboration effort known as theClinical Information Modeling Initiative (CIMI) [6]. Theprimary goal of CIMI is to provide a shared repository ofdetailed clinical information models based on commonformalism.While the primary focus of these modeling efforts hasbeen on interoperability between electronic health rec-ord (EHR) systems, there are also emerging interests inthe use of detailed clinical models in the context of* Correspondence: jiang.guoqian@mayo.edu1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,Rochester, MN 55905, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 DOI 10.1186/s13326-017-0130-4clinical research and broad secondary use of EHR data. Atypical use case is the Office of the National Coordinator(ONC) Strategic Health IT Advanced Research ProjectsArea 4 (SHARPn) [7, 8], in which the IntermountainHealthcare CEMs have been adopted for normalizing pa-tient data for the purpose of secondary use. In the contextof clinical research, for example, Clinical Data InterchangeStandards Consortium (CDISC) intends to buildreusable domain-specific templates under its SHAREproject [9, 10].To better support clinical cancer data capturing andreporting, there is an emerging need to develop inform-atics solutions for standards-based clinical models inclinical cancer study domains. For example, NationalCancer Institute (NCI) has implemented the CancerData Standards Repository (caDSR) [11], together with acontrolled terminology service (known as EnterpriseVocabulary Services  EVS), as the infrastructure to sup-port a variety of use cases from different clinical cancerstudy domains. NCI caDSR has adopted the ISO 11179metadata standard that specifies a standard data struc-ture for a common data element (CDE) [12, 13].The use case in this study is based on The CancerGenome Atlas (TCGA) Biospecimen Core Resource(BCR) data dictionary [14]. The data dictionary is usedto create clinical data collection forms for different clin-ical cancer genome study domains. TCGA clinical datainclude vital status at time of report, disease-specificdiagnostic information, initial treatment regiments andparticipant follow-up information [15]. The data dic-tionary groups a preferred set of CDEs per TCGAcancer study domain and renders them as an XMLSchema document. All clinical data collected are vali-dated against these schemas, which provides a layer ofstandards-based data quality control. All the CDEs arerecorded in the NCI caDSR repository, the implementa-tion of which is based on the ISO 11179 standard. Weenvision that cataloging a preferred set of CDEs foreach clinical cancer study domain is analogous to iden-tifying or creating preferred Detailed Clinical Modelsfor a given domain.The objective of the study is to develop and evaluate acancer genome study metadata management system thatserves as a key infrastructure in supporting clinicalinformation modeling in cancer genome study domains.We leveraged a Semantic Web-based metadata reposi-tory enhanced with both the ISO11179 metadatastandard and the Clinical Information Modeling Initia-tive (CIMI) Reference Model (RM). We used theCIMI-compliant archetype patterns to represent pre-ferred set of CDEs used in the TCGA data dictionaryand identified additional data elements from caDSRfor a given domain. And then we loaded a RDF-metadata repository with data elements based on thesearchetype patterns. We hypothesize that clinical infor-mation modeling tools can leverage such metadata reposi-tory to reuse data elements already widely adopted byclinical genomic research studies (e.g., TCGA studies).MethodsMaterialsISO 11179 and its OWL representationsISO 11179 is an international standard known as theISO/IEC 11179 Metadata Registry (MDR) standard [12].It consists of six parts. Part 3 of the standard uses ameta-model to describe the information modeling of ametadata registry, which provides a mechanism for un-derstanding the precise structure and components ofdomain-specific models.Figure 1 shows a diagram illustrating the high-leveldata description meta-model in the ISO 11179 specifica-tion. The Data Element is one of the foundational con-cepts in the specification. ISO 11179 also specifies therelationships and interfaces between data elements,value sets (i.e., enumerated value domains) and standardterminologies.Several Semantic Web-based representations of theISO 11179 Part 3 meta-model have been created for pro-jects including the XMDR project [16], Semantic MDRin a European SALUS project [17] and CDISC2RDF inFDA PhUSE Semantic Technology project [18]. In thepresent study, we utilize a meta-model schema in OWL/RDF developed in the CDISC2RDF project, which is asubset of ISO 11179 Part 3 meta-model.Reference model in UMLThe CIMI Reference Model (RM) is an informationmodel from which CIMIs clinical models (i.e., arche-types) are derived [6]. The CIMI DCMs are expressed asformal constraints on the underlying RM. The CIMIReference Model is represented in the Unified ModelingLanguage (UML). The September 5, 2014 version of theCIMI Reference Model (v2.0.1) had four packages: 1)CIMI Core Model; 2) Data Value Types; 3) PrimitiveTypes and 4) Party. While the core CIMI ReferenceModel Classes are defined in the CIMI Core Modelpackage, the Party package defines the generic conceptsof PARTY, ROLE and related details for describing po-tential demographic attributes. Both of these packagesutilize the types declared in the Data Value Types andPrimitive Types packages.Figure 2 shows partial view of UML Class diagram ofthe CIMI Core Model. The classes ITEM, ITEM_GROUP,and ELEMENT form very generic pattern (referred asITEM/ITEM_GROUP Pattern here onwards) that can beused recursively to represent almost any clinical informa-tion. The ITEM_GROUP class represents the groupingvariant of ITEM as an ordered list whereas the ELEMENTSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 2 of 11class represents a leaf ITEM which carries no further re-cursion. Figure 3 shows Archetype Definition Language(ADL) [19] definition of a Body Temperature archetype,which illustrates how ITEM_GROUP and ELEMENT canbe combined when representing a clinical concept.The caDSR CDE datasetNCI caDSR is part of the NCI Cancer Common Onto-logical Representation Environment (caCORE) infra-structure and uses caCORE resources to support datastandardization in cancer clinical research studies [11].The system includes an administrator web interface foroverall system and CDE management activities. Inte-grated with caCORE Enterprise Vocabulary Services(EVS), the CDE Curation Tool aids developers in con-sumption of NCI controlled vocabulary and standardterminologies for naming and defining CDEs.NCI caDSR provides the ability to download CDEsin either Excel or XML format [20], which we used todownload an XML image of all non-retired productionCDEs (i.e., CDEs with Workflow status NOT = RE-TIRED) as of August 7, 2014. Figure 4 shows an XMLrendering of the CDE Pharmacologic Substance BeginOccurrence Month Number from the NCI caDSR.The TCGA data dictionaryThe Cancer Genome Atlas (TCGA), a joint venture sup-ported by the NCI and the National Human GenomeFig. 1 High-level data description meta-model in ISO 11179 specificationFig. 2 CIMI Core Model in UML DiagramSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 3 of 11Fig. 3 The definition section of an archetype for a CIMI Body temperature concept. The definition is rendered in archetype definitionlanguage (ADL)Fig. 4 The CDE Pharmacologic Substance Begin Occurrence Month Number in XML recorded in the NCI caDSRSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 4 of 11Research Institute (NHGRI), is a comprehensive and co-ordinated effort to accelerate the understanding of themolecular basis of cancer through the application ofgenome analysis technologies, including large-scale gen-ome sequencing. Being a component of TCGA ResearchNetwork, the Biospecimen Core Resource (BCR) servesas the centralized tissue processing and clinical data col-lection center. A BCR data dictionary has been producedusing the standard CDEs from NCI caDSR. The CDEs inthe data dictionary are publicly available in the XML for-mat. In this project, we will download a snapshot of thedata dictionary from the TCGA website [14]. Figure 5shows a TCGA data dictionary variable Month Of DrugTherapy Start is annotated with the CDE Pharmaco-logic Substance Begin Occurrence Month Number fromthe NCI caDSR.MethodsFigure 6 shows the system architecture of our proposedapproach. The system comprises four layers: a RDFtransformation layer; a RDF store-based persistencelayer; a semantic services layer and an authoring applica-tion layer. This paper focuses on transformation layerand persistence layer.RDF transformation of caDSR and TCGA datasetsThe XML2RDF tool, developed by the Redefer project[21], was used to transform the XML-based TCGA datadictionary and the XML-based caDSR production CDEsinto a corresponding RDF representation. We loaded theresulting RDF datasets into a 4store instance, an open-source RDF triple-store and exposed them via aSPARQL endpoint, allowing us to use the SPARQLquery language to preform semantic queries across thedatasets.OWL-based schema for CIMI Reference Model and ISO11179We used the latest version of CIMI Reference Model(v2.0.1) in the XML Metadata Interchange (XMI) for-mat. We then converted the CIMI Reference Modelfrom XMI to RDF format using the Redefer XML2RDFtransformation services [21]. We then defined theSPARQL queries to retrieve the UML based elements ofthe CIMI Reference Model such as classes, attributesand associations. We created a JAVA program that pro-duces an OWL rendering of the CIMI Reference Modelusing the UML2OWL mappings specified by the ObjectManagement Group (OMG) Ontology Definition meta-model (ODM) standard [22]. We finally harmonized andFig. 5 A TCGA data dictionary variable Month Of Drug Therapy Start annotated with the CDE Pharmacologic Substance Begin OccurrenceMonth Number that is originally recorded in the NCI caDSRSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 5 of 11created an OWL-based schema for CIMI ReferenceModel and ISO11179.Defining and populating reusable archetype patternsWe defined reusable archetype patterns that capturethe clinical cancer domains defined in the TCGAdata dictionary, their associated CDEs and the meta-data structures (Object Class, Property, ValueDomain, etc.) recorded in the caDSR data repository.We then defined a collection of SPARQL queries toretrieve the metadata elements from both the TCGAdata dictionary and the caDSR CDE dataset. Figure 7shows a SPARQL query example that retrieves allCDEs of the domain clinical pharmaceuticaldefined in the TCGA data dictionary and their meta-data recorded in caDSR CDE dataset. We also devel-oped a JAVA program that populates all reusablearchetype patterns in TCGA clinical cancer domainsinto the instance data using the OWL-based schemathat we created.Fig. 6 System architecture of our proposed approachFig. 7 A SPARQL query example that retrieves all CDEs of the domain clinical pharmaceuticalSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 6 of 11Evaluation of clinical utilityWe performed a case study for the domain ClinicalPharmaceutical to demonstrate clinical utility of ourapproach. Specifically, we demonstrated how manyproperties and enumerated value domains are enrichedfor the domain through the ISO 11179-based data ele-ments recorded in the NCI caDSR. We then evaluateclinical utility of the enriched data elements using aMedication template defined in CDISC Clinical DataAcquisition Standards Harmonization (CDASH) stand-ard [23]. We created the alignment between the CDISCMedication template and the CDEs retrieved from thedomain Clinical Pharmaceutical and the alignment con-sensus was achieved through a series of discussionsamong the project team members.ResultsIn total, the TCGA data dictionary contains 38 clinicalcancer domains and 775 CDEs, which covers 21 cancertypes. Table 1 shows a list of examples showing the clin-ical cancer domains and the number of CDEs in eachdomain.We created an OWL rendering of CIMI ReferenceModel and harmonized it with the ISO 11179 metadatamodel schema, in which all classes defined in the CIMIReference Model are asserted as the subclasses of anISO 11179 class mms:AdministeredItem. Figure 8 showsa screenshot of Protégé 4 environment illustrating theclass hierarchy of OWL-based schema for harmonizedCIMI Reference Model with ISO 11179 model.We populated reusable archetype patterns against theOWL-based schema and produced a metadata reposi-tory based in RDF format. The repository covers all 38clinical cancer study domains, comprising 316 distinctobject classes, 4719 distinct properties, 1015 non-enumerated value domains and 1795 enumerated valuedomains (i.e., value sets).Table 2 shows two pattern examples extracted fromthe TCGA domain clinical pharmaceutical. Pattern 1captures a number of CDEs asserted in the TCGA datadictionary; Pattern 2 captures equivalent metadatastructures (Object Class, Property, Value Domain, etc.)recorded in the caDSR data repository. The 7 CDEscaptured in Pattern 1 have their Object Class in com-mon that is Pharmacologic Substance. The Pharma-cologic Substance is linked with three Propertyinstances: Begin Occurrence, End Occurrence andContinue Occurrence. The properties are associatedwith 4 Value Domains: Event Year Number, EventMonth Number, Event Day Number, and Yes NoCharacter Indicator.Evaluation resultsAs a case study, we looked into the domain ClinicalPharmaceutical that contains 18 CDEs. We retrieved theobject classes recorded in caDSR and identified 11 dis-tinct object classes. And then, we retrieved globally inthe caSDR CDE datasets for all properties and valuedomains associated with the 11 object classes. Figure 9shows a bar graph illustrating the enrichment for the do-main Clinical Pharmaceutical by data element, property,value domain and enumerated value domain. The graphindicated that the domain is greatly enriched with prop-erties and value domains associated with those 11 objectclasses, which forms a pool of data elements that couldbe used to build detailed clinical models in this domain.To evaluate clinical utility of our approach, we alignedthe data elements between CDASH Medication andTCGA Clinical Pharmaceutical. Table 3 shows the align-ment results. Out of 20 CDASH data elements with theirdata collection questions, 9 of them aligned with theCDEs asserted in the TCGA data dictionary whereas 10of them aligned with those enriched data elements iden-tified from our system. This shows that the addition ofthe enriched data elements can not only guide us toevaluate a data dictionary by identifying the gaps, butalso provide a pool of data elements to choose from tohelp build clinical models. We believe that the resultsdemonstrated that enriched data elements are useful inbuilding a clinical model for the use cases beyond ori-ginal TCGA data dictionary.DiscussionIn this study, we first transformed the TCGA data diction-ary and the caDSR CDE dataset from their XML formatto the RDF-based representations. This transformationmakes it easier to query caDSR metadata elements thatcorrespond to the CDEs defined in the TCGA data dic-tionary. The TCGA data dictionary terminology bindingsTable 1 A list of examples showing TCGA clinical cancer studydomainsClinical CancerDomainsNumberof CDEsNotesclinical shared 98clinical laml 49 Acute Myeloid Leukemia [LAML]clinical cesc 47 Cervical squamous cell carcinoma andendocervical adenocarcinoma [CESC]clinical lgg 33 Brain Lower Grade Glioma [LGG]clinical lihc 31 Liver hepatocellular carcinoma [LIHC]clinical prad 25 Prostate adenocarcinoma [PRAD]clinical paad 23 Pancreatic adenocarcinoma [PAAD]clinical thca 20 Thyroid carcinoma [THCA]clinical shared stage 19clinical pharmaceutical 18Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 7 of 11enable exploration of additional metadata associatedwith CDEs that would otherwise be challenging toassociate programmatically. These newly discoveredelements help get better insight about the gaps in theirproper and efficient usage in the models that datadictionaries intend to represent. Second, the CIMIReference Model offers a simple recursive pattern(with its ITEM, ITEM_GROUP and ELEMENT clas-ses) to represent CDEs in each TCGA cancer genomestudy sub-domain, as instances. The CIMI ReferenceModel is transformed from its UML format to a corre-sponding OWL representation and harmonized it witha subset of ISO 11179 metadata model. As indicatedabove, the transformation of the TCGA data diction-ary, caDSR CDEs, CIMI Reference Model, ISO 11179into RDF normalizes their representation and makes iteasier to query the content using a standard SPARQLend-point. Finally, we performed a case study in thedomain of Clinical Pharmaceutical and demonstratedthe clinical utility of our proposed approach. We con-sider that this approach is novel as to our best know-ledge this is the first attempt trying to reuse the CDEsrecorded in the caDSR for supporting creating clinicalinformation models based on the CIMI ReferenceModel.The metadata repository system proposed in this studyhas the following three major implications. The first im-plication is that the system would enable producing aFig. 8 A screenshot of Protégé 4 environment showing an OWL-based schema. The schema is for a CIMI Reference Model harmonized with ISO11179 modelTable 2 Two pattern examples extracted from the TCGAdomain clinical pharmaceuticalPattern 1 Pattern 2clinical pharmaceutical [ITEM_GROUP] clinical pharmaceutical[ITEM_GROUP]Pharmacologic Substance[ITEM_GROUP]Year Of Drug Therapy Start [ELEMENT] Begin Occurrence[ITEM_GROUP]Month Of Drug Therapy Start [ELEMENT] Event Year Number[ELEMENT]Day Of Drug Therapy Start [ELEMENT] Event Month Number[ELEMENT]Event Day Number[ELEMENT]Year Of Drug Therapy End [ELEMENT] End Occurrence[ITEM_GROUP]Month Of Drug Therapy End [ELEMENT] Event Year Number[ELEMENT]Day Of Drug Therapy End [ELEMENT] Event Month Number[ELEMENT]Event Day Number[ELEMENT]Therapy Ongoing [ELEMENT] Continue Occurrence[ITEM_GROUP]Yes No Character Indicator[ELEMENT]Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 8 of 11Fig. 9 A bar graph showing the enrichment for the domain Clinical Pharmaceutical. The enrichment by data element, property, value domainand enumerated value domain is illustratedTable 3 Alignment results of data elements between CDASH Medication and TCGA Clinical PharmaceuticalQuestion Text Prompt Data ElementNameTCGA CDEs or Enriched Data ElementsWere any medications taken? Any meds AdministeredWhat is the medication/treatment identifier? CM number Identifier; Unique IdentifierWhat was the term for the medication/therapy taken? Medication or Therapy Drug NameDid the subject take < specific medication/treatment > ? <specific medication/treatment>Cytokine Administered; PlaceboBevacizumab Administered; HER2/neu AdministeredWhat were the active ingredients? Active Ingredients PubChem Compound IdentifierFor what indication was the medication/therapy taken? Indication IndicationWhat was the ID for the adverse events(s) for which themedication was taken?AE ID Toxicity Description; Toxicity GradeWhat was the ID of the medical history condition(s)for which the medication was taken?MH IDWhat was the individual dose of the medical/therapy? Dose Prescribed DoseWhat was the total daily dose of the medication therapy? Total Daily Dose Cumulative Agent Total DoseWhat was the unit of the medical/therapy? Dose Unit Total Dose Units;Prescribed Dose UnitsWhat was the dose form of the medication/therapy? Dose Form Pharmaceutical Dosage Form CodeWhat was the frequency of the medication/therapy? Frequency Number CyclesWhat was the route of administration of themedication/therapy?Route Route Of AdministrationWhat was the start date of the medication/therapy? Start Date Year Of Drug Therapy Start;Month Of Drug Therapy Start;Day Of Drug Therapy StartWhat was the start time of the medication/therapy? Start Time Agent Administered Begin TimeWas the medication/therapy taken prior to the study? Taken Prior to Study? Prior Therapy Treatment RegimenWhat was the end date of the medication/therapy? End Date Year Of Drug Therapy End; Month Of Drug Therapy End;Day Of Drug Therapy EndWhat was the end time of the medication/therapy? End Time Agent Administered End TimeIs the medication/therapy still ongoing? Ongoing Therapy OngoingBold italic font indicates an enriched data elementSharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 9 of 11profile of CIMI-compliant detailed clinical models forTCGA clinical cancer study domains by leveraging thebest practice of detailed clinical modeling in CIMI com-munity. Pattern 1 as shown in Table 2 is designed tocapture a preferred set of CDEs and metadata for eachdomain asserted in the TCGA data dictionary. The se-mantics captured in Pattern 1 should be equivalent tothose asserted in the TCGA XML Schemas. In otherwords, Pattern 1 serves as the CIMI-compliant represen-tation of a preferred set of CDEs in a TCGA cancerstudy domain.The second implication is that we gained new insightson how the ISO 11179 standard could interact with theCIMI Reference Model for supporting detailed clinicalmodeling. The added value would ultimately be the abil-ity to represent ISO 11179 based constructs as con-straints on CIMI Reference Model. Pattern 2 is designedto capture equivalent metadata structures (Object Class,Property, Value Domain, etc.) of a CDE informed by ISO11179. As shown in Table 2, Pattern 2 is represented ina post-coordination manner following certain rules. Theapproach used in Pattern 2 is similar to the dissectionapproach that is a common practice used in the termin-ology space for development of reusable terminologies.The dissection approach was originally used by theGALEN project [24]. In fact, the components in themetadata structure are usually annotated with conceptcodes from a standard terminology. In NCI caDSR, NCIThesaurus has been largely used for the annotation pur-pose. Taking a look at Pattern 2 as shown in Table 2,Pharmacologic Substance, an object class, has NCItcode C1909 annotated; Begin Occurrence, a property,has NCI codes C25431:C25275 annotated. In addition,the post-coordination-based approach enabled us to glo-bally retrieve all properties associated with a particularobject class. For example, there are globally 40 proper-ties associated with the object class PharmacologicSubstance in NCI caDSR, resulting in additional 37more properties and 5 more associated value domains.Figure 9 also shows such enrichment for the domainClinical Pharmaceutical. We believe that our approachwould produce a rich collection of archetype patternsand constraints (e.g., datatypes, value sets, terminologybindings, etc.) that could be used to facilitate detailedclinical modeling in clinical cancer study domain for usecases beyond TCGA.The third implication is that we demonstrated thevalue of using Semantic Web technologies and tools inbuilding such metadata repository. First, we created anOWL rendering of CIMI Reference Model. Thisallowed us to seamlessly integrate the CIMI ReferenceModel with an existing OWL-based ISO 11179 model.We envision that CIMI Reference Model and ISO11179 are two complementary standards that couldgreatly enhance the detailed clinical modeling and itsmetadata management. Second, we used XML2RDFTransformation technology to transform the XML-based TCGA data dictionary and the XML-basedcaDSR CDE dataset into a RDF-based format. Thisallows us to use standard SPARQL query language todefine queries to retrieve metadata of a CDE acrossdatasets while this enables a high-throughput approachfor globally searching metadata of nearly 50,000 CDEsrecorded in the NCI caDSR. Third, we populated re-usable archetype patterns against the OWL-basedschema using a RDF-based representation. This willallow us to leverage the built-in OWL DL reasoningcapability and the RDF validation tools such as ShapeExpressions [25] to check the consistency and dataquality of CIMI-compliant detailed clinical models.ConclusionIn summary, we developed a use case-driven approachthat enables a Semantic Web-based metadata repositoryin support of authoring detailed clinical models in clin-ical cancer study domains. Future work will include 1)developing Semantic Web-based RESTful services forthe archetype patterns recorded in the metadata reposi-tory; 2) building quality assurance mechanism forCIMI-compliant detailed clinical models leveragingOWL DL reasoning and RDF validation tools; 3) creat-ing tools for authoring detailed clinical models usingthe metadata repository as the backend; 4) developingtools that enable the transformation of detailed clinicalmodels between RDF/OWL-based format and ADL-based format.AbbreviationsADL: Archetype definition language; BCR: Biospecimen core resource;caDSR: Cancer data standards repository; CDASH: Clinical data acquisitionstandards harmonization; CDISC: Clinical data interchange standardsconsortium; CEMS: Clinical element models; CIMI: Clinical informationmodeling initiative; CDEs: Common data elements; DCMs: Detailed clinicalmodels; EHR: Electronic health record; EVS: Enterprise vocabulary services;MDR: Metadata registry; NCI: National Cancer Institute; NHGRI: NationalHuman Genome Research Institute; ODM: Ontology definition meta-model;OMG: Object management group; ONC: Office of the National Coordinator;RDF: Resource description framework; RM: Reference model; SHARPn: Strategichealth it advanced research projects area 4; TCGA: The cancer genome atlas;UML: Unified modeling languageAcknowledgmentsThe authors would like to thank Julie Evans and Dr. Rebecca Kush fromCDISC, for their kindly support and input.FundingThe study is supported in part by a NCI U01 Project  caCDE-QA (U01CA180940). The funding body did not participate in the design of the studyand collection, analysis, and interpretation of data and in writing the manuscript.Availability of data and materialsAll schemas and datasets produced in this study can be accessible publiclyat: https://github.com/gqjiang/cimi2rdf.Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 10 of 11Authors contributionsConceived and designed the study: GJ, HRS. Developed the system: DS, HRS,GJ. Designed and conducted the system evaluation: DS, HRS, GJ, CT, CW.Wrote the paper: GJ, DS, HRS. Reviewed and edited the paper: CT, CW, CGC.All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,Rochester, MN 55905, USA. 2University of Texas Health Science Center atHouston, Houston, TX, USA. 3Columbia University, New York, NY, USA. 4JohnsHopkins University, Baltimore, MD, USA.Received: 16 May 2016 Accepted: 30 May 2017RESEARCH Open AccessThe bacterial interlocked process ONtology(BiPON): a systemic multi-scale unifiedrepresentation of biological processes inprokaryotesVincent J. Henry 1,2, Anne Goelzer2* , Arnaud Ferré1, Stephan Fischer2, Marc Dinh2, Valentin Loux2,Christine Froidevaux1 and Vincent Fromion2AbstractBackground: High-throughput technologies produce huge amounts of heterogeneous biological data at all cellularlevels. Structuring these data together with biological knowledge is a critical issue in biology and requiresintegrative tools and methods such as bio-ontologies to extract and share valuable information. In parallel, thedevelopment of recent whole-cell models using a systemic cell description opened alternatives for data integration.Integrating a systemic cell description within a bio-ontology would help to progress in whole-cell data integrationand modeling synergistically.Results: We present BiPON, an ontology integrating a multi-scale systemic representation of bacterial cellularprocesses. BiPON consists in of two sub-ontologies, bioBiPON and modelBiPON. bioBiPON organizes the systemicdescription of biological information while modelBiPON describes the mathematical models (including parameters)associated with biological processes. bioBiPON and modelBiPON are related using bridge rules on classes duringautomatic reasoning. Biological processes are thus automatically related to mathematical models. 37% of BiPONclasses stem from different well-established bio-ontologies, while the others have been manually defined andcurated. Currently, BiPON integrates the main processes involved in bacterial gene expression processes.Conclusions: BiPON is a proof of concept of the way to combine formally systems biology and bio-ontology. Theknowledge formalization is highly flexible and generic. Most of the known cellular processes, new participants ornew mathematical models could be inserted in BiPON. Altogether, BiPON opens up promising perspectives forknowledge integration and sharing and can be used by biologists, systems and computational biologists, and theemerging community of whole-cell modeling.Keywords: Systems biology, Multi-scale systemic description, Prokaryotic biological processes, Mathematicalmodels, Biological ontology* Correspondence: Anne.Goelzer@inra.frEqual contributors2INRA, UR1404, MaIAGE, Université Paris-Saclay, Jouy-en-Josas, FranceFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Henry et al. Journal of Biomedical Semantics  (2017) 8:53 DOI 10.1186/s13326-017-0165-6BackgroundSystems biology emerged as a promising framework tointegrate the whole-cell for different model-organisms[13]. However, current cell representations usually referto specific model organisms, which limits in practice thetransfer of whole-cell models to non-model organisms.In contrast, bio-ontologies are a suitable framework forsystematically describing biological objects and thus fa-cilitating knowledge transfer among organisms [4, 5]. Inthis paper, we address the following question: how tocombine systems biology and bio-ontology?Systems biology has its roots in engineering science andconceptualizes the cell as a system composed of interactingsub-systems [1, 611]. In this context, cellular processes aretypically described as biological subsystems whose inputs(e.g. metabolites, proteins, or sequences, etc.) are convertedinto outputs by dedicated molecular machines. The mo-lecular machines are usually composed of proteins, con-sume energy and chemical building blocks, and display acharacteristic of operation. This operation can be static ordynamic, deterministic or/and stochastic and is generallydescribed by a formal mathematical model having inputs,outputs and model parameters. For example, a mathemat-ical model can be a nonlinear function or a set of ordinarydifferential equations. The systemic representation of cellsis an efficient framework to interrelate all cellular entities(metabolites, proteins, cellular processes, sequences, etc.),together with their physical or biochemical properties (e.g.kinetic parameters, etc.) [1, 2]. System biologists thus neednow an adequate format of systemic description of thewhole cell to transfer and share their models. Existing stan-dardized formats for file exchange are adequate to exchangemathematical models for specific cell processes [12, 13],but remain limited to describe a whole-cell model, i.e. a sys-temic multi-scale representation of interacting complexsubsystems.Bio-ontologies have been developed to formalize andintegrate different pieces of biological knowledge [4].The well-established Gene Ontology (GO) integrates themolecular functions of gene products (GO-MF) with cel-lular components (GO-CC) and biological processes(GO-BP) [14]. The combined sub-ontologies are com-monly used to annotate and characterize gene products[5, 15], but there are also other useful bio-ontologies.The Ontology of Microbial Phenotypes links the pheno-types of bacteria to cellular processes [16]. The Ontologyof Genes and Genomes provides a list of genes from dif-ferent organisms including prokaryotes [17], while the Se-quence Ontology (SO) provides a detailed description ofpolymers and polymer sequence patterns [18]. At anotherlevel, the Pathway Ontology (PW) provides a classificationof metabolic, signaling and altered eukaryotic pathways[19]. Independently, ChEBI (Chemical Entities of Bio-logical Interest) acts as a reference for the classification ofgeneral chemicals according to their chemical structuresand modifications [20]. The Systems Biology Ontology(SBO) provides a controlled vocabulary for kinetic param-eters and mathematical models of biological processes[21]. Taken together, the existing bio-ontologies cover theconcepts necessary to the systemic representation of cells,i.e., biological processes, molecules and mathematicalmodels of biological processes. However, the systemic rep-resentation of the whole cell cannot be handled withoutthe addition of further logical relations between existingontologies.In this paper, we demonstrates that a systemic multi-scale representation of biological processes, the typicalperspective of systems biology, can be formally describedas an ontology, and how this ontology can be built basedon existing sparse bio-ontologies. As a proof of concept,we developed the Bacterial interlocked Process ONtol-ogy (BiPON) and showed that a) heterogeneous bio-logical processes can be described with the systemicrepresentation and b) be linked automatically to math-ematical models, and that c) information about theseprocesses can be enriched by automatic reasoning. As ause case, we focus on bacterial gene expression pro-cesses, which are well established and representative ofknown biological processes. They cover, among manyother things, combination of polymers, sequence pat-terns, single molecules or complexes within biologicalprocesses, as well as cyclic or branched-point processes.We demonstrated on the use case how a systemic repre-sentation of living cells can be formally described and in-tegrated into an ontological model, and what benefitsensue from automatic reasoning on this ontology.MethodsDescription of biological processes, corpus building andentity taggingIn the absence of an exhaustive controlled vocabulary insystems biology, we use hereafter the notion of a bio-logical process, which comprises the notions of (a) bio-logical reaction and biochemical reaction as in KEGG(Kyoto Encyclopedia of Genes and Genomes [22]) Reac-tions database, (b) biological phenomenon, biologicalpathway and biochemical pathway as in PW or KEGGPathway database, and finally (c) biological process asin GO-BP. Moreover, we use the notion of a chemicalentity to denote any type of biological compound, in-cluding metabolites, proteins, protein complexes, poly-mers, to cite a few.To develop a dedicated systemic representation foreach biological process involved in the bacterial gene ex-pression, we applied the standard state-of-art approachof system engineering. The approach involves two maintasks. (A) We first gathered up-to-date available bio-logical information about the biological process. (B) WeHenry et al. Journal of Biomedical Semantics  (2017) 8:53 Page 2 of 16then converted the biological information into a sys-temic representation using boxes, arrows, inputs andoutputs, and a mathematical model. We describe andapply below the approach (A) and (B) on a specific ex-ample (the formation of the 30S initiation complex) forillustrative purposes. Note that the approach is genericand can be applied on any biological process.(A)We collected up-to-date knowledge about the bio-logical processes from scientific literature (books,peer-reviewed original articles, and reviews; see Add-RESEARCH Open AccessConstructing an integrated gene similaritynetwork for the identification of diseasegenesZhen Tian1, Maozu Guo1*, Chunyu Wang1, LinLin Xing1, Lei Wang2 and Yin Zhang2From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: Discovering novel genes that are involved human diseases is a challenging task in biomedical research.In recent years, several computational approaches have been proposed to prioritize candidate disease genes. Most ofthese methods are mainly based on protein-protein interaction (PPI) networks. However, since these PPI networkscontain false positives and only cover less half of known human genes, their reliability and coverage are very low.Therefore, it is highly necessary to fuse multiple genomic data to construct a credible gene similarity network andthen infer disease genes on the whole genomic scale.Results: We proposed a novel method, named RWRB, to infer causal genes of interested diseases. First, we constructfive individual gene (protein) similarity networks based on multiple genomic data of human genes. Then, an integratedgene similarity network (IGSN) is reconstructed based on similarity network fusion (SNF) method. Finally, we employeethe random walk with restart algorithm on the phenotype-gene bilayer network, which combines phenotype similaritynetwork, IGSN as well as phenotype-gene association network, to prioritize candidate disease genes. We investigate theeffectiveness of RWRB through leave-one-out cross-validation methods in inferring phenotype-gene relationships. Resultsshow that RWRB is more accurate than state-of-the-art methods on most evaluation metrics. Further analysis shows thatthe success of RWRB is benefited from IGSN which has a wider coverage and higher reliability comparing with currentPPI networks. Moreover, we conduct a comprehensive case study for Alzheimers disease and predict some noveldisease genes that supported by literature.Conclusions: RWRB is an effective and reliable algorithm in prioritizing candidate disease genes on the genomicscale. Software and supplementary information are available at http://nclab.hit.edu.cn/~tianzhen/RWRB/.Keywords: Gene Ontology, Gene similarity networks, Similarity network fusion, Disease gene identificationBackgroundPrioritization of candidate disease genes is a fundamentalchallenge in human health with applications to understanddisease mechanisms, diagnosis and therapy [15]. Manyhuman diseases are complex and polygenic, involvinglinking genomic variation to clinical phenotype. Traditionallinkage analyses and association study have conducted sus-ceptible genomic interval in the chromosomes [68].However, since the susceptible locus may contain severalhundreds of genes, computational approaches are widelyaccepted to further infer causal genes that are associatedwith interested diseases [911].Given a disease and its disease genes, the target ofprioritization is usually to measure the similarity bet-ween candidate genes and the disease genes [1, 12, 13].It is generally believed that it is the abnormal expressionof disease genes that lead to the diseases happen. Thedisease genes are also called causal genes or diseaserelated genes for the diseases sometimes. Many methodswhich take the guilt by association principle have been* Correspondence: maozuguo@hit.edu.cn1School of Computer Science and Engineering, Harbin Institute ofTechnology, Harbin 150001, Peoples Republic of ChinaFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32DOI 10.1186/s13326-017-0141-1proposed to prioritize candidate genes based on a com-prehensive range of biological information [10, 1421].They are devoted to fully characterize genes (or corre-sponding gene products), to measure the similaritybetween known disease genes and candidate genes moreprecisely and reliably. These methods are usually calledfeature-based methods [22]. The metric of similarity isgenerally based on sequence-based features of genes[2325], functional annotation of genes [13, 26, 27] andprotein-protein interaction data [28, 29]. The ultimategoal is to discriminate disease genes and non-diseasegenes based on certain characteristics of genes [30, 31].More recently, many methods [3238] make use ofphenotype similarity between diseases to prioritize candi-date disease genes [39, 40]. This is because phenotypicsimilarity of diseases can help increase the total number ofknown disease genes for less studied disease phenotypes[41]. The underlying assumption for these methods is thatsimilar phenotypes are caused by functionally relatedgenes [12, 42]. These methods are usually called similarity-based methods [22]. Lage [2] built a Bayesian model basedon PPI network and phenotype similarity network, andthen prioritized the candidate genes with the help of can-didate protein complex. Kohler [32] first grouped diseasesinto families and then employed a random walk fromknown disease genes in its family to prioritize candidategenes. Later, Wu [33] put forward a regression model,named CIPHER, to exploit phenotype-gene associations.More recently, Li [35] first constructed a heterogeneousnetwork by making the best use of the phenotype simila-rity network and gene network as well as the phenotype-gene relationship information. Then they employed therandom walk model, called RWRH, to infer disease genes.Most methods for prioritizing candidate disease genesabove mainly rely on PPI networks. However, currentPPI networks mainly have two shortcomings. One is thatthe coverage of the available PPI networks is typicallylow [29, 43, 44]. Since the curated physical interactionsare generally preferred, they often lead to insufficientcoverage in human genome [45]. This may result in aserious problem that some known disease genes cannotbe mapped into the PPI networks. To address this issue,several researchers [6, 4648] have attempted to con-struct gene semantic similarity network. For instance, Li[6] employ a random walk with restart algorithm on themultigraphs, which merges various genomic networks toenlarge the range of candidate genes and increase thenoise tolerance of networks. However, these differentgenomic networks do not integrate indeed. The weightsassigned to different networks are also difficult toconfirm.The other is the low reliability of PPI networks [49].Since a single data source is prone of bias and incom-pleteness, integration of various genomic data sources ishighly demanded for the study of disease gene prio-ritization [6, 10, 50, 51]. Although multiple data sourcesare available, most methods only access one or two of thesedatabases, which all have their limitations. Chen [52]proposed a method, called BRIDGE, which utilize a mul-tiple regression model with lasso penalty to prioritize thecandidate genes by integrating disease phenotype similarity.Zhang [53] adopted a Bayesian regression approach tointegrate multiple PPI networks. The approach takes thestrength of association between a query disease and acandidate gene as a score to prioritize candidate genes.However, to the best of our knowledge, constructing andintegrating multiple gene similarity networks for prioriti-zing disease genes has not been investigated well. As aresult, there is still a need for the improvement in thesedisease gene prioritization methods.Motivated by the observations above, we proposed therandom walk with restart on phenotype-gene bilayernetwork (RWRB) algorithm to prioritize candidate genesof diseases. We firstly construct five individual genesimilarity networks based on genomic data of genes.Then we obtain an integrated gene similarity network(IGSN) via the similarity network fusion (SNF)method. After that, combining the phenotype similaritynetwork, phenotype-gene association network andIGSN, a phenotype-gene bilayer network is constructed.In the end, we employ the RWRB algorithm on thephenotype-gene bilayer network and prioritize candidatedisease genes on the whole genomic scale. On thebenchmark datasets, RWRB performs better than otherleading approaches. The framework of our proposedmethod is shown in Fig. 1. It is noteworthy that, to takeadvantage of more abundant genome data related togenes, we treat sequence and domain similarity betweenproteins as the similarity between their correspondingprotein-coding genes. Therefore, the similarity betweengenes or proteins is collectively called gene similarity tosimplify in this article.MethodsDatasetsPhenotype similarity networkIn OMIM database, a phenotype is defined as a MIMrecord. The similarity between phenotypes has beencalculated by text mining of MIM records [54]. Wedownloaded the phenotype similarity network [39],which contains pairwise similarity scores for 5080phenotypes, covering the majority of recorded humanphenotypes in this database.Phenotype-gene association networkThe phenotype-gene relationship data is downloaded fromthe OMIM database (http://omim.org/). After filter outphenotypes which do not belong to the phenotypeThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 28 of 79similarity network above and have no known disease genes,we collect 2133 phenotypes and 1893 disease genes involv-ing 2386 phenotype-gene associations totally.Gene dataGene Ontology (GO) and Gene Ontology Annotation(GOA) data of human is download from the GO website(http://geneontology.org, dated November 2, 2015). Thenumbers of annotated genes in cellular component(CC), molecular function (MF) and biological process(BP) ontologies are 16,938, 18,225, and 17,072, respec-tively. Here, we consider all types of annotations whichcontains Inferred from Electronic Annotations. Aminoacid sequences of proteins are obtained from the UniProtdatabase [55]. The number of protein sequence in humandatabase is 18,830. Domains of proteins are downloadedfrom PFAM database (http://www.sanger.ac.uk/Software/Pfam) [56]. Here, we only collected Pfam-A, a collectionof manually curated and functionally assigned domains,instead of Pfam-B, which is computationally derivedcollection of domains, to ensure accuracy in measuringthe similarity between proteins. The number of humanproteins annotated by Pfam-A is 18,523 involving 5333kinds of domains in this database.Construction of gene similarity networks based on genomicdata of genesConstructing gene functional similarity networks based ongene ontologyGO is a standardized and controlled vocabulary to de-scribe genes and gene product attributes. It comprisesthree orthogonal ontologies: CC, MF and BP, respec-tively. In our research, CC, MF and BP ontology has3817, 9943 and 27,864 terms, respectively.Functional similarity between genes can be inferredfrom the semantic relationships of their GO terms[51, 57]. In this work, the functional similaritybetween two genes is measured by Wang method[58] taking BMA strategy because of its an outstand-ing performance. For the sake of three ontologies areindependent, the functional similarity between genescan be measured from three different ontologies.Therefore, we obtain network on CC, MF and BPontology, respectively.Fig. 1 The flow chart of the proposed methodThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 29 of 79Constructing protein similarity network based on proteinsequenceWe used bitscores calculated by the Basic Local AlignmentSearch Tool (BLAST) to create our sequence homologydataset. First of all, we performed an all-versus-all compari-son between proteins with an expectation-value thresholdof 10?6. Then, the similarity between proteins was norma-lized according to their corresponding bitscores ofproteins. Then, applying this operation to all protein pairs,we got the similarity network of protein sequences.Constructing protein similarity network based onprotein domainsWe calculated the Jaccard scores [59] between proteindomain set as domain similarity of proteins. The Jaccardscore between proteins p1 and p2 is defined as Dp1?Dp2=Dp1?Dp2 , which is the ratio of the number of commondomains between p1 and p2 over the total number ofdomains in p1 and p2. Dp denotes the domain set ofproteinp. There are totally 18,526 proteins involving5333 kinds of domain used in our analysis. Applying thisoperation to all protein pairs, thus we constructed adomain similarity network.The overlap among the five aspects of annotationinformation about genes (proteins) above is unexpec-tedly large, as shown in Fig. 2. Numbers in the figuredenote the number of genes that annotated by thecorresponding information in each part, where CC,MF and BP denote corresponding annotations of genes.Seq and Domain denote amino acid sequences anddomain of proteins.Integrating gene similarity networks based on SNFmethodWe have constructed five gene similarity networks basedon BP, CC, MF, sequence and domain information ofgenes. In this subsection, we will employ SNF method[60] to integrate these five networks.Suppose W(m) (here m = 1,2,3,4,5) denotes one of theadjacent matrices of gene similarity networks, we useFig. 2 A brief statistic about the number of genes (proteins) annotated by the corresponding informationThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 30 of 79Eq. (1) to compute the normalized weighted matrix ofW(m), which can be defined as:P mð Þij ¼W mð Þij2Pk?iW mð Þikif j?i12if j ¼ i8>>><>>>:ð1ÞThe normalization used here is free of the scale ofself-similarity in the diagonal entries. It can avoid nu-merical instabilities and ?jP(i, j) = 1 still holds.At the same time, we define the local kernel matrixS mð Þi;j , which is calculated by Eq. (2)SðmÞij ¼W ðmÞijPk?ViðmÞWðmÞikif j?ViðmÞ;0 otherwise8><>: ð2Þwhere V mð Þi denotes a set which contains K nearest neigh-bors of gene i in the matrix W(m). Since local similarities(high values) are more reliable than remote ones, we filterout the low similarity neighbors and set these similaritiesto zero. The K most similar genes for each gene in thenetworks are preserved. The local neighborhoods arefurther exploited to measure the local affinities amonggenes [61]. Therefore, S(m) keeps the local structure ofW(m).In summary, P(m) carries the full information aboutthe similarity of each gene to all others, whereas S(m)only encodes the similarity to the K most similar genes.Here, P(m) and S(m) are called status matrices and kernelmatrix [60], respectively.To fuse the similarity networks, SNF takes the inter-active process of the following update equation:P mð Þtþ1 ¼ S mð Þ 1M?1Xn?mP nð Þt ! S mð Þ Tð3Þwhere m is the index of corresponding adjacent matricesof similarity networks, and t is the iteration number. Itshould be noted that we perform normalization on P mð Þtþ1as in Eq. (1) after each iteration. Another way to think ofthe updating rule (3) isPðmÞtþ1ði; jÞ ¼Xh?ViðmÞXl?VjðmÞSðmÞi;h 1M?1Xn?mPtðnÞ !h;l SðmÞj;lð4ÞBecause the similarity information is only propagatedthrough the common neighborhood between genes, SNFis robust to noise existing in genome data. Besides, iftwo genes gi and gj have common neighbors in all ofsimilarity matrices, it should be well believed that theyhave the high similarity. Whats more, SNF benefits thefact that even if gi and gj are not very similar in one datatype, their similarity can be measured in another datatype and this similarity information can be propagatedthrough the fusion process [60, 62]. The illustrativeexample for fusing two networks based on SNF isshown in Fig. 3.Fig. 3 Illustrative example of SNF steps. (a) Gene-gene similarity matrices based on CC and MF ontology, respectively. (b) Gene functional similaritynetworks. Genes are represented by nodes and pairwise similarities between genes are represented by edges. (c) Network fusion by SNF updatesiteratively, making them more similar with each step. (d) The iterative network fusion results in convergence to the final integrated network. Edge colorindicates which data type has contributed to the given similarityThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 31 of 79Finally, after t steps of iteration, these five matriceswill converge to a single integrated matrix, which can becomputed as:P ¼ 1MXMm¼1PtðmÞ ð5ÞWe obtain the primary integrated gene similar networkin this step.Clustering coefficient-based threshold selectionThe five gene similarity networks are fused as a primaryintegrated gene similar network, whose nodes representthe genes and edges represent the similarity betweengenes. However, there is still a serious problem needing tobe addressed that how similar between two genes can beconnected in the network. Because most molecularnetworks follow a power law or lognormal distribution[12], we should set an appropriate threshold to ensure thatthe primary integrated gene similarity network meets thisdemand. The similarity between genes which is greaterthan the proper threshold will be connected by edges.Otherwise, the similarity will be set to zero [46]. In thisresearch, we adopt the clustering-coefficient-basedthreshold selection method to select a proper thresholdfor the primary integrated gene similarity network.The clustering coefficient of a gene i in the network isdefined as:Ci ¼ 2Eiki ki?1ð Þ ð6Þwhere Ei represents the number of edges between the ki(>1) first neighbors of gene i. The clustering coefficientof a network is defined as the average clusteringcoefficient of its all nodes.C ¼ 1KXki>1Ci ð7Þwhere K denotes the total number of nodes in thenetwork.The threshold selection for a network can be regardedas a process, where edges are removed from the initiallycomplete graph by gradually increasing the similaritythreshold between genes. For each threshold r, we canconstruct a network by the means of filtering out thesimilarity lower than the threshold r. It is generallybelieve that the clustering coefficient of molecular net-works, denoted byC(r), should be significantly higherthan the that of the corresponding random network,which is denoted by C0(r).Therefore, we formulate a discrete optimization prob-lem, in which the cutoff threshold should meet thedemandC ¼ minjrj : C rj ?C0 rj > C rjþ1 ?C0 rjþ1  ð8Þover a set of thresholds 0 = r0 < r1 < ? < rj ? 1 < rJ = 1. InEq. (8), rj + 1 = rj + 0.001; C(r)and C0(r) denote theclustering coefficients of the gene similarity network andthe corresponding random network at the threshold r,respectively. The aim of this procedure is to find the firstlocal maximum, which means the first stop of monoton-ically increasing of C(rj) ?C0(rj).On the other hand, the clustering coefficient of acorresponding random network is determined byC0 ¼k2?k 2k3Nð9Þwhere N is the total number of nodes in a network,k ¼ 1=NPNi¼1ki k2 ¼ 1=NPNi¼1k2i .Finally, after threshold selection for the primaryintegrated gene similarity network, the IGSN that weneed is constructed. It is represented as G(V, E, t), whereV = { g1, g2, ? , gN} denotes the genes involving in IGSN,and E = {eij = ? gi, gj ? |sim(gi, gj) > t} represents the edgesbetween genes with values greater than threshold t.Construction of the phenotype-gene bilayer networkWe have got three networks, which are phenotypesimilarity network, IGSN and phenotype-gene associationnetwork respectively. In this subsection, we make use ofthe three networks above to construct a phenotype-genebilayer network. The construction process of phenotype-gene bilayer network is illustrated in Fig. 4.Suppose AP(m ×m), BGP(m × n) and WG(n × n) areadjacency matrices for phenotype similarity network,phenotype-gene association network and IGSN respect-ively, where m and n represent the number of pheno-types and genes in their respective networks. Theadjacency matrix of the phenotype-gene bilayer networkis denoted asA ¼ AP BGPBPG WG 	(10)where BGP is the transpose of BPG.Prioritizing candidate disease genes based on RWRBThe RWRB is a ranking algorithm, which simulates arandom walker moving from the seed nodes to theirimmediate neighbors randomly and staying at thecurrent node(s) based on the probability transitionmatrix [32, 63]. As for a random walk on the bilayernetwork, we first construct the transition matrix Mbased on matrix A, which is defined asThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 32 of 79M ¼ ?MP 1??ð ÞMGP1??ð ÞMPG ?MG 	ð11Þwhere MP, MGP and MG are the row-normalizing matri-ces of AP, BPG and WG respectively; ? controls thejumping probability between two similarity networks,which are phenotype similarity network and IGSN. Thenthe initial vector P(0) (at t = 0) can be defined asfollows:P 0ð Þ ¼ 1??ð Þu 0ð Þ?v 0ð Þ 	ð12Þwhere u(0) and v(0) denote the initial probability vectorfor phenotype similarity network and IGSN. The param-eter ? ? (0, 1) is used to weight the importance of pheno-type similarity network and IGSN. The effect of theparameters ? and ? on RWRB will be shown in the resultsection. P(t) represents a vector in which the i-th elementholds the probability of finding the random walker onnode i at step t.Based on the vector P(0), P(t) and the transitionmatrix M, the probability vector at step t + 1 can begiven byP t þ 1ð Þ ¼ 1??ð ÞMTP tð Þ þ rP 0ð Þ ð13Þwhere ? ? (0, 1) indicates the restart probability. At eachstep, the random walker has a probability ? to return theseed nodes.After some steps, the walking process is converged ifthe change between P(t) and P(t + 1) is lower than 10?6.The steady probability P(?) is represented as P ?ð Þ¼ 1??ð Þu ?ð Þ?v ?ð Þ 	. As a result, genes which belong to thecontrol set are ranked according to their probabilityscores in P(?). Gene which has the maximum in P(?)among all the control gene set is considered as the mostprobable gene that associates the phenotype.Evaluation metrics of prediction performancePhenotypes in OMIM database mainly have three types[33, 35]: susceptible chromosomal locus and severalrelated disease genes are known; susceptible locus isknown, but no related genes are known; locus and re-lated causal genes are unknown, but the phenotype isknown. Therefore, we use three leave-one-out cross-validation experiments, i.e. linkage interval, genome-widescan and ab initio, which are detailedly introduced andused in [35, 43], to validate our method.Firstly, as for some phenotypes that susceptiblechromosomal locus and several related disease genes areknown, we take the cross validation against a linkageinterval experiment [43]. In each round of validation,one phenotype-gene link is removed. We define the geneassociates with the removed link as the held out gene.The phenotype and the rest disease genes related to thisFig. 4 The construction process of phenotype-gene bilayer networkThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 33 of 79phenotype are used as the seed nodes. At the same time,we define the control gene set that consists of the heldout disease gene and its 99 nearest genes according tothe NCBI refGene location. The performance of RWRBis investigated by the capability to recover the held outdisease gene from the control gene set. We call this aslinkage interval experiment.Secondly, since there are some phenotypes thathave no susceptible chromosomal locus but havealready experimental validated disease genes, we takethe validation against genes in the genome-wide scale.In this experiment, we also remove a phenotype-generelationship and use the rest disease gene associatedwith this phenotype as the seed nodes. Different tolinkage interval experiment, the control gene setconsists all the genes in the genome-wide scale exceptthe held out disease gene. The performance of RWRBis investigated by the rank of held out gene in thecontrol gene set. We call this as the genome-widescan experiment.Thirdly, as for some phenotypes without any knowndisease genes and susceptible chromosomal locus, weidentify disease genes for these kinds of phenotypes fromthe whole-genome scale. In this experiment, we first re-move all the associations between this phenotype and itsdisease genes, then run the RWRB algorithm whichtreats this phenotype as seed node. In this situation, thecontrol gene set is defined as all the genes that in thewhole networks. Similar to genome-wide scan experi-ment, the performance of RWRB is investigated by therank of held out gene in the control gene set. We callthis as ab initio experiment. The detail explanations forthe three approaches have been described by Li [35] andJiang [37].At the same time, we also define three metrics to in-vestigate the performance of RWRB. First is number ofsuccessful predictions (NSP). For each experimentabove, in each round of validation, if the held out diseasegene is ranked as top 1 among the control gene set, weconsider it a successful prediction. Further, for a set ofvalidation runs in each experiment, we sum up thenumber of successful predictions and treat it as ametric that represents effectiveness of algorithms.Second is the mean rank ratio (MRR), which is definedas the average rank ratios of all held genes in controlgene sets in all validation runs. Third is the receiver op-eration characteristic (ROC) curve. We plots the sensi-tivity versus 1-specificity which subject to the thresholdseparating the prediction classes [10]. Sensitivity refersto the percentage of disease genes that are ranked abovea particular threshold, while specificity refers to thefraction of control genes rank below the threshold. Wevary the threshold from 0.0 to 1.0 with the scale 0.01,and draw the ROC curve. It is well accepted that smallerMRR and larger AUC and NSP values indicate betterperformance for a prioritization method [43].ResultsFirst of all, we will investigate the performance of RWRBon three kinds of experiments. Then, we assess the effectof parameters in RWRB algorithm. After that, theproposed algorithm is compared with two similarity-based methods, which are CIPHER [33] and RWRH [35]and two feature-based methods which are PUDI [15]and PriDiGe [14]. Finally, we predict novel causal genesfor Alzheimers disease and other common diseasesbased on RWRB algorithm.The performance of RWRBIn this subsection, we will investigate the performance ofRWRB on the three experiments using the three metrics.The detail results are shown in Table 1. The ROC curveson linkage interval and genome-wide scan experimentsare shown in Fig. 5.As is shown in Table 1, the results of RWRB on NSP,MRR and AUC metrics for linkage interval experimentis 1384, 18.28, 0.8505, respectively. Then we further in-vestigate the performance of RWRB on genome-widescan experiment and obtain a NSP of 311, a MRR of22.17 and an AUC of 0.8417. In the end, we perform thecross-validation approach against ab initio experiment.The results on NSP, MRR and AUC are 223, 29.64 and0.8144, respectively.As is known to us, a random guess will yield a MRR of50%, and an AUC of 50%, suggesting that the effective-ness of RWRB in uncovering disease gene. Meanwhile,the results also show the reliability of IGSN.Then we further analyze the detail distribution ofdisease genes ranked in the control gene set for linkageinterval and genome-wide scan experiment. The resultsare presented in Fig. 6. As for linkage interval experi-ment, we find that there are 1554 disease genes rankingin top 10, where 1384 disease genes are rank one. 230disease genes are ranked between 11 and 20, and 157disease genes are ranked between 30 and 50.As for genome-wide scan experiment, there are 498disease genes ranking between 1 and 10. The number ofdisease genes between 11 and 50 is 422. As we can seefrom the results, most of held out genes can be rank intop 100. The results on three experiments demonstrateTable 1 The results of RWRB on the three experimentsExperiment NSP MRR AUCLinkage interval 1384 18.28 0.8505Genome-wide scan 311 22.17 0.8417ab initio 223 29.64 0.8144The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 34 of 79that RWRB has a high accuracy in inferring diseasegenes on the genomic scale.Effect of parameters on RWRBThere are totally three parameters in RWRB, which are? ? and ?. The parameter ? denotes the restart probabil-ity in Eq. (1). It has been well accepted that the param-eter ? has a slight effect on the results and here we fix itat 0.7 [35]. Next, we will investigate the influence ofparameter ? and ? for RWRB on the NSP metric.The parameter ? represents the jumping probabilitybetween phenotype similarity network and IGSN.According to [35], larger ? will introduce more mutualinformation between phenotype similarity network andIGSN. To investigate the effect of this parameter on theperformance of RWRB, we tested our algorithm ondifferent values of ? ranging from 0.1 to 0.9 with an in-crement of 0.1.Results are shown in Table 2. The performance is im-proved with the increase from 0.1 to 0.6 on the whole.However, the performance is slightly decreased from 0.6to 0.9. As for the linkage interval experiment, RWRBgets the best performance at ? = 0.6, while RWRB getsthe largest NSP at ? = 0.7 on the genome-wide scanexperiment. The best results for ab initio experiment is225 when ? = 0.6. Therefore, we suggest that the best ?Fig. 5 ROC curves of RWRB on linkage interval and genome-wide scan experimentsFig. 6 The distribution of disease genes ranked in top 100The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 35 of 79value is 0.6 or 0.7 for RWRB on the experiments above.Results demonstrate that the RWRB algorithm success-fully makes the best use of the relationships betweenphenotype similarity network and IGSN.As is known to us, ? controls the impact of seed phe-notypes and seed genes in the initial vector. To validatethe effect of parameter ? on RWRB, we tested ouralgorithm on different values of ? ranging from 0.1 to0.9 with the scale 0.1. We run RWRB on linkage intervaland genome-wide scan, ab initio experiments, and evalu-ate its performance on the NSP metric. As is shown inTable 3, the performance is improved with the increasefrom 0.1 to 0.6 on both experiments. However, the per-formance is slightly decreased from 0.6 to 0.9. As a re-sult, the algorithm performs best when ? at 0.6. Thissuggests that IGSN is more importance than phenotypesimilarity network for RWRB.Comparison with similarity-based methodsWe compare RWRB with similarity-based methodswhich are RWRH [35] and CIPHER [33], respectively.The author [33] defines two topological distance on thebasis of two different neighborhood systems: shortestpath (SP) and direct neighbor (DN). Therefore, two ver-sions of CIPHER are represented as CIPHER-SP andCIPHER-DN, respectively. The results of each methodon NSP metric are presented in Table 4.Because the number of phenotype-gene associations inRWRB, CIPHER and RWRH models are different, wecompute the successful prediction percentages for eachmethod, which is defined as the ratio between NSP andthe total phenotype-gene associations in their corre-sponding datasets. The experimental results are listedin Table 5.As for the linkage interval experiment, RWRB gets 1384successful predictions, while RWRH, CIPHER-SP andCIPHER-DN obtain 814, 709, 765 successful predictions,respectively. The percentage of successful prediction forRWRB is 0.58 which is the highest in all three methods.As for the genome-wide scan experiment, the controlgene set is defined as the whole genes in IGSN. RWRBget 311 successful predictions, while RWRH, CIPHER-SP and CIPHER-DN obtain 245, 153, 165 suc cessfulpredictions, respectively. Then number of successful pre-dictions of RWRB is largest in the three methods. How-ever, the percentage of successful predictions for RWRBis 0.13 which is lower than that of RWRH (0.17).On the ab initio experiment, there are 223 successfulpredictions by RWRB, while RWRH, CIPHER-SP andCIPHER-DN successfully predicted 201,140 and 157cases, respectively. However, the percentage of successfulpredictions of RWRH is the highest in the threemethods which is 0.14, whereas the other three methodsare almost neck and neck.Comparison with feature-based methodsAt the same time, we compare RWRB with two featurebased methods which are PUDI [15] and PriDiGe [14].Here we only compare the precision (p), recall (r) and F-measure (F) of these three methods, since they are fromdifferent type of methods.The metrics about precision, recall and F-measure forPUDI and ProDiGe have been introduced by Yang [15].Here, we will also use these metrics to evaluate the per-formance of RWRB on linkage interval experiment. Inthe experiment, we take the leave-one-out cross-validationmethod. For the precision of RWRB, we define it asTable 2 Performances of RWRB at different values of ? onNSP metric? Linkage interval Genome-wide scan ab initio0.1 1306 295 1650.2 1320 299 1690.3 1337 304 1810.4 1349 309 2090.5 1384 311 2230.6 1393 317 2250.7 1386 319 2110.8 1361 308 2060.9 1357 304 174To validate the effect of parameter ? on RWRB at different values, we fix ? at0.5. Best results are in boldTable 3 Performances of RWRB at different values of ? on NSPmetric? Linkage interval Genome-wide scan ab initio0.1 1286 299 1750.2 1310 307 1870.3 1344 310 1930.4 1368 310 2060.5 1384 311 2230.6 1392 319 2270.7 1391 317 2170.8 1378 315 2030.9 1354 306 172To we validate the effect of parameter ? at different values, we fix ? at 0.5.Best results are in boldTable 4 The performance of each method on the NSP metricAlgorithms Linkage interval Genome-wide scan ab initioRWRH 814 245 201CIPHER-SP 709 153 140CIPHER-DN 765 165 157RWRB 1384 311 223Note: Best results are in boldThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 36 of 79the ratio between NSP and the number of all validationruns. For the recall of RWRB, we define it as the ratio be-tween the number of the held out genes whose rank pro-portions are higher than 0.5 and the number of all theheld out genes. The F-measure is the harmonic mean ofprecision and recall, which is defined as F = 2?p?r/(p + r).The results for PUDI, ProDiGe and RWRB are shownin Table 6. From the results, we can find that RWRBachieves 82.3% recall and ranks first in the threemethods. Method PUDI wins 72.3% precision which is13.7 and 0.9 better than RWRB and ProDiGe method,respectively. At the same time, method PUDI achieves76.5% F-measure which is 2.0% and 10.2% better thanRWRB and ProDiGe method, respectively. In this groupexperiment, method PUDI performs best and RWRBranks second overall.Prioritizing Alzheimers disease and other commondisease genes by RWRB: A case studyIn this subsection, we will use RWRB to predict novelcausal genes of interested diseases. To validate theeffectiveness of our method, we will check whether ourpredicted disease genes have been already found toassociate with the diseases in literature. Here, we select16 multifactorial diseases which are used in [37] and listthe top 10 candidate genes for each disease. The resultsare shown in Table 7. Here, we only select Alzheimersdisease (AD) as the case study to verify the performanceof RWRB.AD is a progressive disease that usually starts slowlyand gets worse over time. In general, it causes 60% to70% of cases of dementia. The cause of AD has not beencompletely understood so far. The primary task is todiscover the disease genes to understand the nosogenesisof genetic disease. There are many phenotypes for AD.Here we select 104,300 as target phenotype to prioritizedisease gene. The corresponding susceptible region forMIM:104,300 is 6p22.As is shown in Table 7, the first prediction of RWRBfor MIM:104,300 is NOS2, which plays an importantrole in neuroinflammation by generating nitric oxide(NO), a critical signaling and redox factor in the brain[64]. Further, the levels of NO fall in the brain to athreshold may promote A? mediated damage. Thepredicted gene NOS2 has a large impact on AD. Thesecond prediction gene for MIM:104,300 is NOS1. Inthe brain and peripheral nervous system, nitric oxidedisplays many properties for a neurotransmitter. The au-thor [65] suggests that short alleles of the NOS1 exon1fVNTR interacting with the epsilon 4 allele tend tomarkedly increase the AD risk [65]. The fourth predictedgene for AD is APBB1. A trinucleotide deletion of theAPBB1 gene was a factor protecting against late-onsetAD. Cousin [66] reported the results of a case/controlstudy and confirmed this relationship. The eighth predic-tion is gene PGBD1. It locates at 6p22 which is the sus-pectable region of MIM:104,300. Whats more, it currentlyshows significant association in AlzGene according toGenome-wide association study. Its gene product is spe-cifically expressed in the brain and has been identified asthe key factors of AD. The results above show that thecombination of the similarity network integration and theidentification algorithm can successfully predict candidategenes for interested disease.Conclusions and discussionIn this paper, we propose a novel method, named RWRB,to infer causal genes of interested diseases. We firstly con-struct five gene similarity networks based on five differenttypes of genome data. Then we employ SNF method to in-tegrate these gene similarity networks and get IGSN. Afterthat, we perform RWRB to prioritize disease genes. RWRBis compared with the state-of-the-art models and achievesa better performance on most evaluation metrics. Next,we will discuss the highlights of this article.The advantages of IGSNThe main object of our research is to overcome two draw-backs of current PPI networks, i.e., their low reliability andcoverage. As a result, we construct the IGSN in this re-search. Firstly, since IGSN is fused based on the five gene(protein) similarity networks, its reliability should be higherthan existing that of PPI networks. The prioritization of dis-ease genes can be benefited from IGSN. Secondly, IGSNcan significantly improve the coverage of human genescomparing current PPI networks. It covers 19,065 genes,which is twice the number of genes in HPRD network.Therefore, the number of phenotype-gene associations inRWRB algorithm is 2386, which is almost twice that inRWRH and CIPHER methods whose number is 1444. As aTable 5 The successful prediction percentages for each methodAlgorithms Linkage interval Genome-wide scan ab initioRWRH 0.56 0.17 0.14CIPHER-SP 0.49 0.11 0.10CIPHER-DN 0.52 0.12 0.11RWRB 0.58 0.13 0.09Note: Best results are in boldTable 6 Overall comparison among different methodsMethods Precision Recall F-measurePUDI 72.3 81.0 76.5ProDiGe 72.4 75.9 74.1RWRB 58.6 82.3 69.4Note: Best results are in boldThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 37 of 79result, the proposed method can make the best use ofphenotype-gene associations in OMIM database. Thirdly,since IGSN is a single network which integrates multiplegene similarity networks, there is no need for it to assignweight values to different subnetworks.Threshold selection for IGSNThe threshold selection is very important to the qualityof IGSN. This is because the threshold affects reliabilityof IGSN, and may further determine the performance ofRWRB. As shown in Fig. 7, the first stop ofTable 7 Top-10 predicted causal genes of 16 multifactorial diseasesPhenotype name Phenotype ID Top ten predictions for each phenotype by RWRBAlzheimers disease 104,300 NOS2 NOS1 APBB3 APBB1 EPX LPO APLP1 PGBD1 POR MTRRBreast cancer 114,480 RB1 PTEN AR TP63 TP73 SDHD BUB1B GNAS PHB2 TSC1Colon cancer 114,500 RB1 PTEN SDHD BRCA1 MLH1 MSH2 BRCA2 CREBBP TP63 TP73Diabetes mellitus 125,853 INSR APOA5 VDR HMGA2 SLC2A2 LPL GHR INS USF1 LMNAGastric cancer 137,215 IL36A IL36G IL1A IL1F10 IL37 IL36B IL36RN APC IL18 MSH2Atrial fibrillation 147,050 WAS SELL PAFAH2 SELE TIMD4 HAVCR2 IL13 IKBKG TNFRSF13B ICOSProstate cancer 176,807 HIP1R BRCA1 TP53 STK11 FGFR3 ZFHX4 SDHD RNASEL PRODH MSH2Schizophrenia 181,500 SYN3 SYN1 MAPT DDO PRNP CHI3L2 APOL3 CHIA CHIT1 APOL1Leukemia 190,685 FLNA FGFR2 RET GLI3 NF1 COL1A1 COL2A1 EVC TBX1 FLNBLung cancer 211,980 TP53 CDKN2A RB1 SDHD NRAS CYP2D6 BRCA1 CYLD DICER1 PTENZellweger 214,100 FGFR2 FLNA COL2A1 MECP2 FGFR3 FLNB TP63 GLI3 GJA1 COL11A1Leukemia 253,310 SMN1 GBA LMNA VAPB ATP7A ALS2 COL6A2 BSCL2 DCTN1 COL2A1Asthma 600,807 IL2RG SCGB1D2 SCGB1D4 SCGB1D1 PAFAH2 SBDS WAS IGHM HPS1 ALOXE3Leukemia 601,626 BCR PDGFRB PRF1 KMT2A BRCA2 MPL MLLT1 MCL1 MLLT6 RPS14Obesity 601,665 FFAR4 GNAS SLC6A14 ASIP ENPP3 SDC1 ENPP2 SDC2 SDC4 MLNTuberculosis 607,948 CD2AP C5 SCNN1B CFTR TICAM2 FAM218A TLR1 TLR4 TLR6 SOCS2Note: Predicted disease genes which are supported by literature are in bold for Alzheimers diseaseFig. 7 Cluster coefficient under each threshold for primary integrated gene similarity network. Black arrow points to the first peaks of the curveand rectangular boxes show the corresponding threshold value. Red curve represents the cluster coefficient of the primary integrated gene similaritynetwork (CCP), and green curve denotes the cluster coefficient of the corresponding random network (CCR) at different thresholds. Blue curve depictsthe difference of cluster coefficient (DCC) between the two networks above. In this experiment, we select the best threshold at r = 0.005, andconstruct IGSN based under this thresholdThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 38 of 79monotonically increasing of DCC (See legend of Fig. 7)occurs at r = 0.005, which indicates that this threshold isthe most appropriate value to construct IGSN. Underthis threshold, the IGSN has 19,065 genes in ourexperiment.We further investigate the degree distributions ofIGSN under the selected threshold. Many previousstudies [67] have found that distribution of node con-nectivity of molecular networks follows a power law.However, some other research [68] argued that there aresome distributions, such as the lognormal distribution,which can also depict the degree distribution better thanpower law. In this research, we employ two models,which are Gaussian distribution and Lognormal distribu-tion, to investigate the distributions of IGSN. In order toincrease contrast, we import two other leading PPInetworks, which are BioGRID and HPRD networks.The fitting performance on the distributions for eachnetwork is represented by R-squares (R2). R2 provides ameasure of how well the data fits a certain model. As isshown in Fig. 8, we find that the degree of IGSN fits thelognormal distribution best, while BioGRID and HPRDprefer to fit the power law distribution. As is shown inFig. 8 (c) and (d), the R2 results of IGSN for Gaussianand Lognormal distribution are 0.87 and 0.94, respect-ively. The R2 results of BioGRID and HPRD for fittingPower law are 0.91 and 0.92, respectively, which areshown in Fig. 8 (a) and (b). The degree distribution re-sult shows that IGSN has the characteristics of molecu-lar networks, rather than those of random networks.Therefore, IGSN is a meaningful biological network.In the future, our research should further be improvedfrom the following aspects. First, other genomic data ofgenes needs to be integrated. Although we haveFig. 8 The graphic view of degree distribution fitting results for BioGRID (a), HPRD (b) and IGSN (c, d). According to their performance on R2, theresults for IGSN fitting the Gaussian and Lognormal distribution are 0.87 and 0.92, shown with (c) and (d) respectively, while the results forBioGRID (c) and HPRD (d) are 0.91 and 0.92 respectivelyThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 39 of 79measured the similarity between genes based on fivetypes of genome data, other information of genes isneeded to be integrated to the similarity networks.Second, how to fuse the different similarity networksproperly is important to the ultimate integrated network.Many previous studies have attempted to integratedifferent semantic similarity network and gene expressionnetworks. However, some methods only assign equalweighted to these networks and simply add them together,while some others apply these networks separately. TheSNF method used in this article may overcome the draw-backs above. However, the identification of the integratednetwork is not a trivial assessment because there is no dir-ect way to ascertain its rationality and correctness. In ourresearch, we resort to degree distribution of integrated net-work and find it fit the lognormal distribution best. Thisonly shows the rationality from one property of the inte-grated network. Therefore, we need to study more fusedmethods of network further and make the integrated net-work be in line with the characteristics of biologicalnetworks.FundingM. Guo is supported by National Natural Science Foundation of China (61,271,346,61,571,163, and 61,532,014) and the National Key Research and Development PlanTask of China (Grant No. 2016YFC0901902). C. Wang is supported by NaturalScience Foundation of China (61402132), and X. Liu is supported by NaturalScience Foundation of China (91,335,112, 61,671,189). Publication costs for thisarticle was funded by National Natural Science Foundation of China (61571163).Availability of data and materialsThe dataset(s) supporting the conclusions of this article were downloadedfrom the relevant public databases.About this supplementThis article has been published as part of Journal of Biomedical Semantics Volume 8Supplement 1, 2017: Selected articles from the Biological Ontologies and Knowledgebases workshop. The full contents of the supplement are available online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsZT proposed the idea, implemented the experiments and drafted the manuscript.MG initiated the idea, conceived the whole process and finalized the paper. CW,LX, LW and YZ helped with data analysis and revised the manuscript. All authorshave read and approved the final manuscript.Ethics approval and consent to participateThe human GO annotations are publicly available to all researchers and are free ofacademic usage fees. There are no ethics issues. No human participants orindividual clinical data are involved with this study.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims in publishedmaps and institutional affiliations.Author details1School of Computer Science and Engineering, Harbin Institute ofTechnology, Harbin 150001, Peoples Republic of China. 2Institute of HealthService and Medical Information Academy of Military Medical SciencesBeijing, Beijing 100850, China.Published: 20 September 2017Deléger et al. Journal of Biomedical Semantics  (2017) 8:37 DOI 10.1186/s13326-017-0135-zRESEARCH Open AccessDesign of an extensive informationrepresentation scheme for clinical narrativesLouise Deléger1,2, Leonardo Campillos2, Anne-Laure Ligozat2,3 and Aurélie Névéol2*AbstractBackground: Knowledge representation frameworks are essential to the understanding of complex biomedicalprocesses, and to the analysis of biomedical texts that describe them. Combined with natural language processing(NLP), they have the potential to contribute to retrospective studies by unlocking important phenotyping informationcontained in the narrative content of electronic health records (EHRs). This work aims to develop an extensiveinformation representation scheme for clinical information contained in EHR narratives, and to support secondary useof EHR narrative data to answer clinical questions.Methods: We review recent work that proposed information representation schemes and applied them to theanalysis of clinical narratives. We then propose a unifying scheme that supports the extraction of information toaddress a large variety of clinical questions.Results: We devised a new information representation scheme for clinical narratives that comprises 13 entities, 11attributes and 37 relations. The associated annotation guidelines can be used to consistently apply the scheme toclinical narratives and are https://cabernet.limsi.fr/annotation_guide_for_the_merlot_french_clinical_corpus-Sept2016.pdf.Conclusion: The information scheme includes many elements of the major schemes described in the clinical naturallanguage processing literature, as well as a uniquely detailed set of relations.Keywords: Knowledge representation, Clinical natural language processingIntroductionThe progressive adoption of electronic health records(EHRs) is paving the way towards making available largeamounts of data for research. Raw EHR data may be trans-formed into clinically relevant information and then beused in traditional or translational research [1]. Naturallanguage processing is essential to phenotyping EHR databecause of the amount of clinical information buried inthe narrative content.The path towards EHRs is nonetheless not free of chal-lenges [1]. One of the hurdles is the coexistence of severalinformation models for representing clinical informationavailable in EHRs. The Clinical Document Architecture(CDA) in the Health Level 7 (HL7) framework coexists*Correspondence: neveol@limsi.fr2LIMSI, CNRS, Université Paris - Saclay, Rue John von Neumann, 91405 Orsay,FranceFull list of author information is available at the end of the articlewith the Clinical Element Model (CEM) [2] and otherstandards such as the openEHR [3] and the ISO 13606 [4].Developing equivalent clinical models is a key elementto achieve the semantic interoperability of EHR systems[5]. The Clinical Information Modeling Initiative (CIMI)[6] and the SemanticHealthNet (SHN) [7] initiative areinternational efforts towards this goal. It can be arguedthat informationmodels rely on terminologies that specifythe concepts used in the model [8]; for instance, medi-cations in RxNorm10 or clinical terms in SNOMED CT.However, information and terminology models tend to bedesigned by different groups with dissimilar data struc-tures. Some researchers have indeed attempted to validatethe use of terminologies in EHR-based standards (e.g.SNOMED CT in the HL7 Clinical Document Architec-ture) [9].In this paper we will focus on a text-based represen-tation of information that is text-anchored (i.e. mentionsof clinical entities) or that may be derived from text data© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Deléger et al. Journal of Biomedical Semantics  (2017) 8:37 Page 2 of 18(e.g. relations between entities identified in clinical texts).Addressing the unification of information models is outof our scope here. Our goal is to put forth a represen-tation scheme that will support secondary use of EHRdata for conducting a large variety of retrospective studies.More specifically, we aim to support information extrac-tion from clinical narratives in order to answer clinicalquestions such as: What is the prevalence of incidentalfindings in patients with suspected thromboembolic dis-ease?, What is the contribution of CT venography in thediagnosis of thromboembolic disease? or What are thetypes and grades of toxicities experienced by colon cancerpatients receiving FOLFOX therapy?.Simple and fast low-level annotations have alreadyyielded good results in mining large datasets, as exem-plified in LePendu et al.s study on myocardial infarctionadverse drug effects in rheumatoid arthritis [10]. Anotherstudy showed the benefit of exploiting medical concepts,modality and relations between concepts extracted fromclinical narratives for accurate patient phenotyping [11].Furthermore, recent research has shown that informationextraction from unstructured clinical narratives is essen-tial to many clinical applications, including secondary useof EHRs for clinical trial eligibility [12].Overall, the information representation landscapebroadly includes two types of representations. First,ontologies or encyclopedic representations that are verydetailed and removed from any direct application, withthe goal of providing a formal representation of domainor subdomain knowledge. Second, a number of text-basedrepresentations of information that are very-well suited toan application they were designed for. Our need is for arepresentation scheme with a broad scope that remainsclose to applications grounded in clinical text. The goal isto identify a representation that may connect easily withmajor knowledge sources used in clinical Natural Lan-guage Processing, while covering many aspects of clinicalknowledge covered in EHR narratives. We conducted areview of annotation projects and associated annotationschemes for clinical narratives. We found that while allexisting schemes had merit, no single scheme covered allthe aspects of knowledge representation that we sought, inparticular with respect to fine-grained relations betweenclinical concepts. We then designed a new informationrepresentation scheme that related to existing schemesand attempted to integrate best representation practices.This article describes a new information representationscheme devised from on-going analysis of clinical narra-tives. This scheme has been applied to annotate a largecorpus of French clinical reports described in [13], but isintended to be generally applicable to clinical narratives inseveral languages and medical specialties. The contribu-tion of this paper is two-fold: first, we present an extensivereview of annotation projects and associated annotationschemes for clinical narratives. Second, we provide mate-rial for the annotation of clinical narratives, including anew annotation scheme, companion annotation guide-lines, and insight on how to devise an annotation method-ology for a new project.BackgroundRepresentation of information in clinical text corporaEthical issues need to be considered before carrying outresearch on clinical narratives. Privacy issues require sup-plementary measures to de-identify patient data beforereleasing the corpus for research. De-identification is usu-ally performed by removing or replacing Personal HealthIdentifiers with surrogates [14]. This is one of the reasonswhy clinical corpora are less available than corpora in thebiological domain [15, 16].Improvements in clinical information processing havebeen reported by adopting adequate annotation frame-works [11, 15, 1719]. These have been developed intwo levels of representation. A low-level annotation isconcerned with linguistically and clinically grounded rep-resentations to use within a document. This level is con-cerned with defining (in annotation guidelines) mentionsof clinical and linguistic interest, and then marking theseinstances in clinical text. Most annotation efforts in thebiomedical NLP community have followed this trend,especially within the organisation of research challenges.The second level of representation is a high-levelannotation that prioritizes formally integrating all theannotated linguistic and clinical data. That is, this levelprioritizes processing the annotated information for rea-soning over the whole EHR in a computationally action-able way. Within the context of the Strategic HealthIT Advanced Research Project (SHARPn), [20] and [21]developed a higher-level formal (OWL) clinical EHR rep-resentation (implemented in cTAKES [22]). This repre-sentation is based on the low-level annotation frameworkexplained in [23]. The SHARPn normalized data has beenthus converted automatically to the Resource Descrip-tion Framework (RDF) format by using the CEM-OWLspecification. The Biological Expression Language (BEL)[24] seems to be a mix between the low and high-level ofannotation for life science text (vs. clinical).Our work has carried out a low-level annotation, but ourscheme can likely be compatible with a high-level repre-sentation in the long-run. In the following section, we willreview other low-level annotation frameworks of clinicalcorpora.Related workIn this section, we focus on well-documented frameworksissued from medium-scale projects, or schemes that havebeen widely used in shared tasks or challenges. Additionalexamples of annotation efforts of clinical data for specificDeléger et al. Journal of Biomedical Semantics  (2017) 8:37 Page 3 of 18applications or experiments, where the representationscheme or annotation work is not the main focus, arereported in [2536] (inter alia). These will not be reviewedin detail herein, as we chose to provide an in-depth analy-sis of efforts providing rich annotation guidelines that werelied on to build our own scheme.We refer the readers toa recent review of the litterature in clinical NLP for a morecomplete overview of the field [37].We review the annotation schemes outlined in Table 1,in chronological order. Note that we classified the anno-tations in the Informatics for Integrating Biology and theBedside (i2b2) challenges as entities or attributes, in orderto make clearer the comparison between schemes. How-Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 DOI 10.1186/s13326-017-0129-xRESEARCH Open AccessData- and expert-driven rule inductionand filtering framework for functionalinterpretation and description of gene setsAleksandra Gruca* and Marek SikoraAbstractBackground: High-throughput methods in molecular biology provided researchers with abundance ofexperimental data that need to be interpreted in order to understand the experimental results. Manual methods offunctional gene/protein group interpretation are expensive and time-consuming; therefore, there is a need todevelop new efficient data mining methods and bioinformatics tools that could support the expert in the process offunctional analysis of experimental results.Results: In this study, we propose a comprehensive framework for the induction of logical rules in the form ofcombinations of Gene Ontology (GO) terms for functional interpretation of gene sets. Within the framework, wepresent four approaches: the fully automated method of rule induction without filtering, rule induction method withfiltering, expert-driven rule filtering method based on additive utility functions, and expert-driven rule inductionmethod based on the so-called seed or expert terms  the GO terms of special interest which should be included intothe description. These GO terms usually describe some processes or pathways of particular interest, which are relatedto the experiment that is being performed. During the rule induction and filtering processes such seed terms are usedas a base on which the description is build.Conclusion: We compare the descriptions obtained with different algorithms of rule induction and filtering andshow that a filtering step is required to reduce the number of rules in the output set so that they could be analyzed bya human expert. However, filtering may remove information from the output rule set which is potentially interestingfor the expert. Therefore, in the study, we present two methods that involve interaction with the expert during theprocess of rule induction. Both of them are able to reduce the number of rules, but only in the case of the methodbased on seed terms, each of the created rule includes expert terms in combination with the other terms. Furtheranalysis of such combinations may provide new knowledge about biological processes and their combination withother pathways related to genes described by the rules. A suite of Matlab scripts that provide the functionality of acomprehensive framework for the rule induction and filtering presented in this study is available free of charge at:http://rulego.polsl.pl/framework.Keywords: Functional description, Gene Ontology, Logical rules, Expert-driven rule inductionBackgroundIntrodutionOver 20 years ago, high-throughput technologies for theanalysis of genomic data opened a new era in molecu-lar biology and genetics. Since the beginning of the so-called genomic era, advanced tools and techniques such*Correspondence: aleksandra.gruca@polsl.plInstitute of Informatics, Silesian University of Technology, Akademicka 16,44-100 Gliwice, Polandas DNA microarrays [1] and next-generation sequencing(NGS) [2] systems allow for studying genomes, analyz-ing cellular processes and interactions, which is the firststep of research leading to diagnosis of diseases andinvention of new drug, and treatment discovery [35].However, to be effective, todays genomic technologiesrequire not only reagents and sophisticated laboratoryinstruments but also application of new software, algo-rithms, and knowledge discovery techniques in order© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 Page 2 of 14to process and analyze huge amount of experimentaldata [68].Many of the experiments using genomic technologiesare focused on searching of co-regulated genes that playan important role in some biological processes partic-ularly interesting from the experimental point of view.Typically, genes that work coordinately as genemodules orgene networks are seen as groups characterized by similarexpression levels and can be found by applying clusteringmethods to the expression data [913]. However, the func-tional analysis and interpretation of gene clusters obtainedin such a way are difficult and time-consuming, especiallyif each gene composing the group is manually analyzed byan expert in the field, based on his or her experience andliterature searches.To help the expert during such analysis, a lot of toolshave been invented and successfully applied during lastyears. One of the most frequently used tools is the GeneOntology (GO) database, which is a collaborative effort toaddress the need for consistent descriptions of gene prod-ucts across databases [14]. The information in the GOdatabase is divided into three separate structures in theform of directed acyclic graphs (DAGs): Biological Process(BP), Molecular Function (MF) and Cellular Component(CC). Each node of the graph has a label t called theGene Ontology term and has a unique seven-digit num-ber, name, short description, and defined relationship toone or more terms in the same domain.The information included in the GO database is pro-vided on different levels of specificity: the terms foundcloser to the root of the graph (higher in the hierarchy) aregeneral descriptions, and as the graph is traversed downto its leaves, the terms become more and more specific.The important part of GO database are annotations thatassociate gene products with particular terms in GeneOntology graph. Each gene product can be annotated tozero or more terms of any ontology on any level of theGO graph. Annotations are independent of each other, butshould be made on the most detailed level in the ontologyas annotating to a particular term implies annotation to allits parent terms up to the root.In this paper, we describe a comprehensive frameworkfor functional description of gene sets based on the so-called logical rules that are combinations of GO terms.The presented approach involves (i) method of rule induc-tion which takes into account the structure of GeneOntology database, (ii) method of rule interestingnessassessment based on various subjective and objective cri-teria, and (iii) the method of rule filtering that allowsremoving the rules that are uninteresting from the expertpoint of view from the output rule set. Finally, (iv) wepresent a new, semi-interactive method of rule inductionwhich allows the expert to influence the process of rulegeneration by providing a set of so-called seed or expertterms, that is the GO terms of special interest, whichshould be included into the description. These GO termsusually describe some processes of particular interest, fre-quently related to the experiment that is being performed.During the rule induction and filtering process suchseed terms are used as a base on which the descriptionis built.Using Gene Ontology database for functional analysisThe first approach to the automated functional interpre-tation was the so-called single-term analysis in which,based on the results of the statistical test, a list ofover-represented GO terms describing gene groups wasobtained. A number of tools were created based on theidea of single-term analysis, which is still the most com-mon approach used for functional interpretation of genesets [15].Another approach to the methods of automated func-tional interpretation was the introduction of moreadvanced tools such as RuleGO [16] or GeneCodis[17] that search for the so-called logical rules thatinclude combinations of GO terms. The rationalestanding behind such approach is that the combina-tions of GO terms are more specific and thereforecan show significance, whereas single terms do notshow statistically significant enrichment or depletion. Ifwe analyze GO terms separately, some of them maybe too general to be included in the list of stati-cally significant terms; however, their combination withother terms may present some novel and interestinginformation.In our previous research [18], we showed that the num-ber of possible statistically significant combinations ofco-existing pathways is huge and that a filtering step isrequired in order to reduce the number of possible results.However, frequently, an expert who designs an experi-mentmight be interested in some specific process or eventrelated to the research. For example, in cancer researchsearching for a gene signature, which could be potentiallyuseful for diagnosis or could suggest novel drug targets,one may look for genes involved in particular biologicalprocess or network related to transformation of normalcells into cancer cells. Therefore, there is a concern thatautomated filtering methods could remove some rulesthat consists of GO terms potentially interesting to theexpert. To address this issue, we propose a new method-ology of rule induction and filtering which allows forincluding the expert domain knowledge into rule genera-tion and filtering process. The new approach is based onthe RuleGO algorithm, and it allows the expert to influ-ence the process of rule generation by defining the GOterms of special interests, which are then included intothe rules and preserved in the output rule set after thefiltering step.Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 Page 3 of 14Related workSo far, to find co-appearance of Gene Ontology terms,association rule induction algorithms were applied.Caramona-Saez et al. [19] proposed a method that com-bines expression data and biological information. Later,in another study, Caramona-Saez et al. [20] introducedthe Genecodis web-based tool for integrated analysis ofannotations from different sources. The method uses theApriori algorithm [21] to discover sets of annotations thatfrequently cooccur in the analyzed group of genes. Asimilar tool that allows finding combinations of anno-tations from many different sources such as functionalcategories, gene regulation, sequence properties, evolu-tion, and conservation was presented by Hackenberg etal. [22]. Also, Gruca [23] applied FP-growth algorithm tofind combinations of GO terms for functional descriptionof genes.Research on the induction of rules that combine geneexpression data and biological information was also per-formed [2426]. For example, in Lopez et al. [25], genegroups described by similar values of the so-called struc-tural features (e.g., gene length, the number of nucleotidesin the coding sequence, gene G+C content) with the cor-responding GO terms are also joined by means of associa-tion rules. Hvidsten et al. [27] proposed conditional rulesof the form "IF conjunction of conditions describing timeseries of gene expression profile THENGO term". In a ruleconclusion, a set of Gene Ontology terms describing thegroup were included.Rule induction techniques mentioned earlier have twobasic drawbacks that can make obtained rules difficult oreven impossible to interpret. First, known rule inductionmethods do not consider the fact that hierarchy of GOterms could result in replacing a conjunction of attributeswith one, more specific GO term at the lowest level in theGO graph hierarchy. Second, all the methods mentionedearlier lead to generate a huge number of rules withoutproviding more advanced (apart from a p-value and a rulecoverage) methods of rule interestingness evaluation andrule filtering.In a previous study [18], we proposed the rule inductionalgorithm which takes into account the structure of theGene Ontology graph and the method of selection of themost important GO terms. The selection method is basedon the Rough Set Theory [28] and the asymmetrical indis-cernibility relation. However, the number of induced ruleswas still too large. Therefore, another method for rule fil-tering based on subjective rule attractiveness measure wasproposed in Gruca and Sikora [29].The problem of finding the minimal subset of the setof rules, which has lower complexity and simultaneouslymaximizes the value of the specified criterion (e.g., over-all classification accuracy) is NP-complete and computa-tionally expensive. For descriptive purpose or when theclassification ability is not the most important feature,the rule elimination procedures (rule filtering) are basedon the minimum interestingness requirements (typicallysome well-known rule interestingness measures are cho-sen) [30, 31]. Some papers also refer to multicriteria ruleevaluation, and in such a case, machine learning [32] andmulticriteria decision-making [33] methods are applied.These methods can be called supervised because they useinformation obtained from an expert. For example, Lenca[33] apply the PROMETHEE method [34] to select inter-estingness measure which is able to order a rule set in amanner most similar to the order provided by an expert.In biological or medical applications, it is very impor-tant to determine the rules containing information thatis interesting for a user. However, automatic selection ofelementary conditions included in the rule premises isthe main principle of rule induction algorithms, and rulesinduced in this way may not always include knowledgethat is interesting and useful to the user.To date, few studies have described how to designthe induction algorithm in such a way that it takes intoAmith and Tao Journal of Biomedical Semantics  (2017) 8:17 DOI 10.1186/s13326-017-0124-2RESEARCH Open AccessModulated evaluation metrics fordrug-based ontologiesMuhammad Amith and Cui Tao*AbstractBackground: Research for ontology evaluation is scarce. If biomedical ontological datasets and knowledgebases areto be widely used, there needs to be quality control and evaluation for the content and structure of the ontology. Thispaper introduces how to effectively utilize a semiotic-inspired approach to ontology evaluation, specifically towardsdrug-related ontologies hosted on the National Center for Biomedical Ontology BioPortal.Results: Using the semiotic-based evaluation framework for drug-based ontologies, we adjusted the quality metricsbased on the semiotic features of drug ontologies. Then, we compared the quality scores before and after tailoring.The scores revealed a more precise measurement and a closer distribution compared to the before-tailoring.Conclusion: The results of this study reveal that a tailored semiotic evaluation produced a more meaningful andaccurate assessment of drug-based ontologies, lending to the possible usefulness of semiotics in ontology evaluation.Keywords: Ontology, Ontology evaluation, Quality assessment, Drug ontologies, Semiotics, Metrics, KnowledgebasesBackgroundGiven a scenario where a researcher is to choose twodistinctly independent ontologies that cover a specificdomain, how would the researcher know which is suit-able between the two? Or given another scenario where aknowledge engineer is developing an ontological knowl-edgebase, how would she evaluate the quality of theontology and know what to measure? This paper aimsto provide a direction in the area of ontology evaluationusing a system shaped by the theory of semiotics  thestudy of meaning for signs and symbols, specifically forbiomedical ontologies.Biomedical ontologies have influencedmedical researchwith the impact and efforts of the Gene Ontology [1],UMLS [2], SNOMED [3], etc. It is assumed that ontolog-ical knowledgebases for biomedicine will grow to covermany other sub-domains. Already, an NIH-funded ini-tiative, the National Center for Biomedical Ontologies(NCBO), exist to provide tools and hosting supportfor ontologies, and an active community of biomedi-cal researchers formed the Open Biomedical Ontologies*Correspondence: cui.tao@uth.tmc.eduSchool of Biomedical Informatics, University of Texas Health Science Center,Fannin Street, Houston, Texas, USA(OBO) Foundry [4] for rigorous standards for biomedicalontologies.Semiotics is formally defined as the the study of signsand symbols and how they are used [5]. Abstractly, anontology, with its terms and labels, can be a symbolic rep-resentation or signifier of a domain space that describe aphysical manifestation of the real world. However, framingthe ontology domain in semiotics is inherently common.While touching upon the three branches of semiotics,Sowa made a philosophical-oriented explanation of howthe study of signs relate to 1) the syntax of an ontology(syntactic), 2) the meaning and logic derived from the syn-tax (semantics ), and 3) the users or agents that interpretor utilize the signs (pragmatics) [6]. Approaching ontol-ogy evaluation from the semiotic frame is a natural choiceto assess the overall craftsmanship of the ontology.Our research questions in this study focus on 1) whethera semiotic-based approach for ontology evaluation canprovide meaningful assessments for biomedical ontolo-gies, and 2) whether this approach can be tailored forspecific types of ontologies to providemore accurate qual-ity assessments. The use-case focus will be drug-relatedontologies hosted on the National Center for BiomedicalOntology BioPortal.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 2 of 8National Center of Biomedical OntologiesThe National Center for Biomedical Ontologies (NCBO)is a NIH-funded program to provide support tools, anda repository to store a wide range of ontologies fromthe biomedical field. Based on a random survey sam-ple of selected ontologies conducted from August 2015(n = 200), the authors of this paper searched for pub-lished studies that coincided with the development andthe release of the ontology. The outcome of this brief sur-vey revealed that most of the ontologies from this sampledid not have any documented evidence of any evaluation(n1 = 183). A relatively small number had some evidenceof any evaluation (n2 = 17). We can surmise that there is aneed for evaluation, and that many biomedical ontologieslack any formal evaluation.Also from our review, we noted that if there wasany documented evidence of evaluations, the evaluationfocused on a specific type of assessment. Some reportstatistical-related information denoting the number ofontological elements (classes, properties, etc.) or struc-tural elements (depth, breadth, etc.). Others reportedquery-based or competency questions-driven approachesto evaluate the degree to which the ontology fulfills a use-case. A few utilized subject matter experts to review thegeneral content, and a few measured some specific appli-cation tasks. Broadly, ontology evaluation appears to bediversified and focused.Semiotic Framework for Ontology EvaluationWhile there are no agreed standard for ontology eval-uation, researchers have proposed various evaluationapproaches, such as, metric-based evaluation [7, 8], cov-erage of domain [9, 10], use-case and requirement assess-ment [11], and comparison with other ontologies sharingthe same domain [7, 12]. In this study, we applied ametrics-based method that is rooted in semiotic theory,and also tailored this method to compare with ontologiesin a similar domain.A semiotic framework approach for ontology evalua-tion [13] was proposed by Burton-Jones, et al, nearly adecade ago when DAML-based ontologies were in exis-tence. Reorganizing the intrinsic and extrinsic views ofontologies, it aims to be a holistic, domain-independent,and customizable approach to evaluate a wide range ofontologies by framing it in semiotic theory. Scores aredenoted by the pillars of semiotics  pragmatic, syntac-tic, and semantic. An additional score, social, denotesan ontologys ranking with other ontologies in a com-munity. We intend to apply this metric suite for thisstudy. To derive some of those scores, external software,like a triple store or WordNet-based APIs, are required.Detailed discussion of the scoring metric is providedhere at [13], but we will summarize the aspects of themetric in the following sub-sections. The Eq. (1) belowdescribe the overall quality evaluation score based on thefour scores.Q = wq1 ? S + wq2 ? E + wq3 ? P + wq4 ? O (1)The scores range from 0 to 1, where 1 is the highest and0 is the lowest. Each of them weighted equally, yet thereare mechanisms to tailor the weights to provide moreinfluence of a certain aspect or diminish its influence. Forexample, if one were to measure the quality of an ontol-ogy that serves as a hierarchal terminology of terms, thenit would make sense to decrease the weight of the syn-tactic score since it may under-utilize ontology features.(2-5) describe the underlying derivatives of the individualscores and their sub-scores.SyntacticEncoded ontologies enable machines to process and inter-pret the knowledge embedded in the knowledgebase. Thesyntactic score (2) describes the encoded readability of theontology. Lawfulness (SL) and richness (SR), sub-scores ofthe syntactic score, represent conformity of the syntax,and the utilization of the ontology syntactic features. SL iscalculated by the number of axiom-level violations basedon the OWL 2 standards over the total number of axioms.The figures can be obtained using the OWL API. SR isbased on the number of ontological features utilized overthe total number of ontological features.S = ws1 ? SL + ws2 ? SR (2)SemanticTerms or labels are one of the fundamental buildingblocks of ontological knowledgebases. The semantic score(3) rates the terms understandability from 3 sub-scores.Interpretability (EI) rates the ontologys terms from cal-culating the percentage of terms with at least one wordsense. Consistency (EC) denotes the percentage of termsthat are uniform among the ontology or lack of duplicateterms (number of duplicates over total number of terms),and clarity (EA) reveals how each term in the ontology areambiguous based on the average number of word sensesfor each term (the average word sense per term over thenumber of terms).E = we1 ? EI + we2 ? EC + we3 ? EA (3)PragmaticPragmatic score (4) is composed of three sub-scores,which includes comprehensiveness (PO), accuracy (PU),and relevancy (PR). Comprehensiveness scores an ontol-ogys domain coverage based on the percentage num-ber of instances, classes, and properties of the ontologyto a group of ontologies. Accuracy and relevancy areunique. The former requires domain experts to reviewand assess the veracity of facts evoked from the ontology percentage of truthful statements. Relevancy varies andAmith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 3 of 8depends on possible use-case of the ontology. For exam-ple, if evaluators are concerned about the ontologys abilityto preform semantic-based searches, then a percentageof how successful queries is recorded as the relevancyscore. (4) represents the composition of the pragmaticscore.P = wp1 ? PO + wp2 ? PU + wp3 ? PR (4)SocialWhile not particularly related to semiotics, the socialscore (5) is an assessment of the ontologys standingin comparison with other ontologies. The authority (OT)sub-score is based on the percentage number of links thatthe ontology extends with other ontologies and the history(OH) sub-score is the percentage based on the number oftimes the ontology was accessed.O = wo1 ? OT + wo2 ? OH (5)In the following sections, we will describe the method-ology for utilization of the metric suite, and briefly discussdrug-based ontological datasets. Afterward, the paper willdiscuss the results and impact of our results for drug-based ontologies.MethodsWe experimented with a set of biomedical ontologiesfrom NCBO Bioportal that have the most visits (based onSeptember 2015 data), according to the NCBO website. Atotal of 66 ontologies were sampled, but 2 were removeddue to issues with the serialization of the files. With the 64we calculated an aggregation of the scores and producedthe basic statistics (mean,median, etc.) from them. Table 1shows the results of this effort.Table 1 NCBO sample aggregate scoresQuality Mean Std. Deviation Min MaxSyntactic .64 .14 .18 .85Lawfulness .92 .16 .27 1Richness .36 .18 .07 .69Semantic .88 .15 .09 .99Interpretability .88 .14 .01 1Consistency .84 .40 -.17 1Clarity .96 .13 .14 1Pragmatic .02 .07 0 .52Comprehensiveness .02 .07 0 .52Social .02 .02 0 .13History .02 .02 0 .13Overall Score .39 .05 .21 .48We also gathered a set of drug-related ontologies (SeeDrug Ontologies) and preformed the same aggregationscoring (Table 2). In addition, we also examined each ofthe scores to understand the quality of each drug ontol-ogy and the whole set in general. Finally, we tailored themetrics rooted on strengths and weakness of the drugontologies, and compared the non-tailored and tailoredaggregation.Drug OntologiesWe reviewed the list of available biomedical ontologiesthat were drug-related for selection in our study. The listbelow are the drug ontologies used: RxNORM [14] VANDF (Veterans Health Administration NationalDrug File) [15] DRON (Drug Ontology) [16] DINTO (Drug-Drug Interaction Ontology) [17] DIKB (Drug Interaction Knowledgebase) [18] VO (Vaccine Ontology) [19] PVOnto (Pharmacovigilance Ontology) [20]The National Drug Data File, the National Drug File Reference Terminology, and Master Drug Data Base Clin-ical Drugs were not included in our experiment dueunavailability of a downloadable file for testing.The study utilized the latest version of OWL-APIv4.2.3 [21], MIT JWI v2.4 (for word senses) [22], apache-commons-lang v3.4 [23], and minimal-json v0.9.4 [24] todevelop Java software code to calculate the scores. Foreach of the downloaded ontologies, we collected scoresfrom the software and recorded the values. Scores thatrelied on total times accessed and the number of classes,instances, and properties were collected from NCBOsRESTful API.Table 2 Drug ontology scores (Equal Weighted)Quality Mean Std. Deviation Min MaxSyntactic .67 .11 .56 .85Lawfulness .97 .04 .91 1Richness .36 .19 .15 .69Semantic .83 .09 .69 .99Interpretability .80 .31 .1 1Consistency .73 .25 .37 1Clarity 1 .01 .98 1Pragmatic .14 .26 5.98E-04 .52Comprehensiveness .14 .26 5.98E-04 .52Social .14 .36 0 .01History .14 .36 0 .01Overall Score .45 .10 .31 .59Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 4 of 8ResultsThe results are detailed in the subsequents sub-sections.Certain scores were neglected due to lack of resourcesto calculate them (authority, relevancy, and accuracy).Equal weighted (EW) evaluation scoring was used (6).Pragmatic score was simply the comprehensiveness due tolack of resources to calculate accuracy and relevancy, andthe social score was only the history score for the samereasons described.QEW =(0.25 ? S)+(0.25?P)+(0.25?E)+(0.25 ? O) (6)NCBO Bioportal Score (Sample size = 64)Table 1 depicts the values resulting from the arithmeticmean of the evaluation scores for the top 64 viewedontologies from September 2015. Themean for the overallquality score for the sample amounted to 0.39 (? = 0.05).To calculate the comprehensiveness score which requiredknowing the number of classes, instances, and properties,we tallied a total of 1,277,993, and a total accessed (for thehistory score) at 152,424 based on the entire set, throughSeptember 2015.Semantic quality, from the sample set appeared to bestrongest with 0.88, and the weakest aspect appearedto be social and pragmatic quality. At a more granu-lar level, clarity which measured ambiguity of terms andlabels revealed a score of 0.96. Lawfulnesswhichmeasuredadherence to ontology standards was also high at 0.92.Drug Ontology ScoringEqual weighted scoresTable 2 provides data from equal weighted evaluationscoring for the set of drug ontologies we assessed. 0.45(? = 0.10) is the average mean for the 7 drug ontologies.The total number of classes, instances, and propertiesused to derive the comprehensiveness score was 169,862,and the total number of times the ontology was accessedwas 351,616. This was used to formulate the history score(social).From the results and similar to the previous sample set,semantic quality was the prominent with 0.83 (0.88 forNCBO). For the sub-scores, clarity and lawfulness bothexhibited high ratings, 1 and 0.97 respectively.Drug ontology-influencedmodulated scoresFrom the scores generated earlier, we devised a methodto customize the metrics to accommodate the set of drugontologies by modifying the weights. The semantic, prag-matic, syntactic, and social were 0.83, 0.14, 0.67, and 0.14.The values were converted proportionally to give weightsfor semantic, pragmatic, syntactic, and social (0.46, 0.08,0.38, and 0.08). With the new values, we replaced theweights to attain (7), and recalculated our data. Table 3shows the results from the modulated scoring with eachdrug ontology with the unmodified scores,Qmod andQEWTable 3 Examination of the weighted scoresQEW Qmod Diff S+E P+ORxNORM 0.64 0.69 0.05 0.70 0.11DIKB 0.44 0.75 0.31 0.88 0.00DINTO 0.41 0.69 0.28 0.81 0.01PVOnto 0.38 0.66 0.28 0.76 0.00VANDF 0.35 0.57 0.22 0.67 0.02VO 0.37 0.63 0.26 0.74 0.00DRON 0.53 0.64 0.11 0.70 0.35?(? ) 0.45 (0.10) 0.66 (0.05) 0.21 0.75 0.07respectively. These values were the overall final scores forQmod and QEW .Qmod=(0.38?S)+(0.08?P)+(0.46?E)+(0.08?O) (7)From Table 3, RxNORM under the equal weighted eval-uation metric amounted to 0.64 (6) and the modulatedscore of 0.69 (7). Similar increases as a result of the modu-lated scoring produced the same result for the other drugontologies. The means of the overall scores were 0.45 and0.66 (before and after, respectively).DiscussionIn this section, the paper will discuss how the equalweighted drug ontologies compared to the sample set ofNCBO ontologies (also equal weighted). The purpose isto assess how an ontology or a group of specific type ofontologies align with the quality of biomedical ontologies.Also, this section will compare the equal weighted scor-ing of drug ontologies and the modulated scoring of drugontologies. This will assess whether the modulated met-rics represented the drug ontologies better than the equalweighted version. Lastly, the paper will further examineeach individual scores of each drug ontology.Comparative results with NCBO sample dataWhen calculating the comprehensiveness and historyscore, we utilized the total number of ontological elementsand total times accessed relative to the set they belongto. Therefore, we will neglected comparison betweenpragmatic and social and focused on the other scoresbetween the NCBO sample and the drug ontology scores,both of which were equal weighted. Without the afore-mentioned scores, the overall average mean of the finalquality score were both 0.38, keeping the weights at0.25 for syntactic and semantic. Closer inspection of thevalues between the two tables (Tables 1 and 2) revealsome close alignment with the greater body with NCBOontologies from the sample. Syntactic and its related sub-scores resemble the same values, however, the semanticAmith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 5 of 8quality scoresmight have some deviation. The consistencysub-score, which scores an ontologys term uniformity(minimal duplication of terms and labels), appear to bedistinguishable with NCBO sample aggregate (0.73 to0.84). This could possible reveal that some drug ontolo-gies may have some duplicated labels, and may haveto resolve those duplication if the ontology is to bedeemed consistent in its domain space within the semi-otic framework. Since we are utilizing a sample set fromNCBO, any conclusion drawn should be cautiously con-sidered. Nonetheless, one way of evaluating on ontology,particularly one that is under-development is to com-pare the scores with the greater body of biomedicalontologies.Comparative results with modulated drug ontology scoresWe compared the overall quality scores (6) and the anal-ogous modulated overall quality score (7) for each ofthe drug ontologies (Table 3). With the equal weightedapproach, RxNORM and DRON produced higher qual-ity scores (0.64 and 0.53). Examining their respec-tive scores, specifically looking at S (syntactic) and E(semantic) together (S+E), we noted that both RxNORMand DRON were below average compared to otherdrug ontologies (Table 3). However, looking at just P(pragmatic) and O (social) together (P + O), RxNORMand DRON score above average, while the rest of thedrug ontologies rates below average. So the relatively highoverall score of RxNORM and DRON was mainly dueto their advantage of being accessed more and beingmore comprehensive than the other drug ontologies,which alluded to some unfairness in the equal weightedmetrics.Focusing the attention on the modulated weightedscores for the drug ontologies, DIKB ended being the bet-ter quality drug ontology over RxNORM with an overallscore of 0.75 than RxNORMs 0.69. DINTO also yieldeda score of 0.69. All of the drug ontologies exhibited anincrease (? = 0.21, ? = 0.1), but RxNORM and DRONproduced the smallest gains (0.05 and 0.11). Because themodulated scoring increased the weights for syntactic andsemantic, where the quality scores of DIKB, DINTO, andPVOnto exhibited relatively high values, DIKB, DINTO,and PVOnto reported the largest gains. Also with thelessen weights for pragmatic and social, RxNORM andDRON did not have the high quality score that it hadpreviously.The average for the entire drug ontology for the equalweighted metrics was 0.45 (? = 0.10) and for modu-lated weighted was 0.66 (? = 0.05). Figure 1 shows asimple histogram of both the equal weighted and mod-ulated weighted overall score. In general, the modulatedmetric that we formulated, what could be, a more faith-ful and authentic scoring for drug ontologies. The impactFig. 1 Density plot of overall quality scoresof this specific effort could provide direction for knowl-edge engineers to utilize the semiotic framework to tailorit for specific groups of ontologies. Also, it could be a starttowards a standard metric for any new drug ontologiesunder-development or introduced.Individual drug ontology scoresFor each of the drug ontologies, Table 4 provides anexamination of individual scores and sub-scores. The fol-lowing subsections will discuss some observations of thesevalues.Syntactic levelDIKB, DINTO, and DRON exhibited strong seman-tic quality (S) as evident by the high scores. Lookingat both DIKB and DINTOs richness (SR) and syntac-tic (SL) sub-scores both rated very high, revealing lowontological violations and utilized more ontological fea-tures. DRONs richness score was below the average,yet the average was particularly high. The strength ofDRON was due to the utilization of many ontologi-cal features. Both RxNORM and VANDF rated belowaverage for syntactic quality, and both had the lowestrichness and syntactic, indicating relatively lower thanaverage use of ontological features and more standardsviolations.Because of the very high syntactic (SL) score, there wasa high standard of adherence to syntactical aspect withdrug ontologies. Richness (SR) varied among them as thescores were differed greatly where half preformed betterthan average. Observationally, the drug ontologies thatAmith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 6 of 8Table 4 Individual drug ontology quality scoresSL SR S EI EC EA E PO P OH O QEW QmodRxNORM 0.91 0.21 0.56 0.97 0.54 1.00 0.83 0.22 0.22 0.96 0.96 0.64 0.69DIKB 1.00 0.67 0.84 0.96 0.87 0.98 0.93 0.00 0.00 0.00 0.00 0.44 0.75DINTO 1.00 0.49 0.75 0.80 0.88 1.00 0.88 0.03 0.03 0.00 0.00 0.41 0.69PVOnto 1.00 0.15 0.58 0.93 0.96 0.99 0.95 0.00 0.00 0.00 0.00 0.38 0.66VANDF 0.91 0.21 0.56 0.96 0.37 1.00 0.77 0.05 0.05 0.03 0.03 0.35 0.57VO 1.00 0.38 0.69 0.89 0.51 1.00 0.79 0.00 0.00 0.00 0.00 0.37 0.63DRON 0.96 0.44 0.70 0.10 0.98 1.00 0.69 0.71 0.71 0.01 0.01 0.53 0.64Mean 0.97 0.36 0.67 0.80 0.73 1.00 0.83 0.14 0.14 0.14 0.14 0.45 0.66Median 1.00 0.38 0.69 0.93 0.87 1.00 0.83 0.03 0.03 0.00 0.00 0.41 0.66St Dev 0.04 0.19 0.11 0.31 0.25 0.01 0.09 0.26 0.26 0.36 0.36 0.10 0.05Min 0.91 0.15 0.56 0.10 0.37 0.98 0.69 0.00 0.00 0.00 0.00 0.35 0.57Max 1.00 0.67 0.84 0.97 0.98 1.00 0.95 0.71 0.71 0.96 0.96 0.64 0.75exhibited stronger syntactic richness tend to have highersemantic (S) score.Semantic levelExamining the semantic quality, DIKB, DINTO, andPVOnto displayed the highest scores. All three denotebetter than average sub-scores for interpretability (EI),consistency (EC), and clarity (EA)  ontological termsexpressiveness, uniqueness, and ambiguity. DINTOassessed less ambiguity, DIKBs unique trait appear tobe interpretability, and PVOnto strong point was theconsistent usage of terms and labels. VANDF rated lowerthan average and lowest of the group for semantic quality.This was due to consistency being drastically lower, eventhough it exhibited expressive terms and less ambiguityof the terms.Overall, clarity is exemplary among the drug ontologies,indicating less ambiguity among the terms, however theyvary with consistency and interpretability. Drug ontologiescould benefit from better selection of terms and findingterms with better expressiveness (terms with at least oneword sense).Pragmatic levelNoted earlier, pragmatic (P) score was limited by the useof comprehensiveness (PO) sub-score. To reiterate, com-prehensiveness was determined by the number classes,instances, and properties over the total of those elementsin a set. Both DRON and RxNORM exhibited higher thanthe median score for (P). DRON had substantially promi-nent pragmatic score with 0.71 (? = 0.14, ? = 0.26).Scores that denoted 0.00 had values very low to display totwo significant digits. Prolific drug ontologies tended tobe large in size and scope.Social levelSimilar to pragmatic (P), the social (O) score was deter-mined by one sub-score  history (OH). Social mea-sures the ranking of the ontology among the community.RxNORM indicated a very prominent score of 0.96 (? =0.14, ? = 0.36). With a median among them being 0,most of the drug ontologies compared to RxNORM didnot have same level access or popularity. It is difficult todetermine ways to improve history (number of times ofaccessed) of ontologies that are not as prolific. However,if community ranking of an ontology is important to aresearcher or developer, this score would be an interestingfactor to consider in any decision making for biomedicalontology selection or usage.Limitations and Future DirectionThis study utilized the Burton-Jones, et al. semiotic eval-uation metric suite to assess NCBO ontologies, and drug-related ontologies. Despite our efforts in revealing newfindings about drug ontologies and establishing a methodto tailor evaluation for a set of ontologies, some of whatwas presented had some limitations.One of them is the sample set of NCBO ontologies. Inthe future, we would ideally like to have a larger body ofontologies from NCBO to generate a more representa-tive score for comparative purposes with other ontologiesor a group of ontologies, as we have shown in this study.With a larger set, it is also possible to look at other factorsthat can be considered for evaluation, like breadth, num-ber of children nodes, etc. Also, a few of the scores wecould not produce values due to lack of time and humanresources to preform reviews for scores like accuracy orrelevancy. However, the benefit of the semiotic frameworkAmith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 7 of 8Fig. 2 Ontokeeper screenshotfor ontology evaluation is the openness to customize themetric to suit certain situations, like the lack of subjectmatter experts.Initially, we investigated the option for an automatedapproach to determine appropriate weights for the ontolo-gies. However, we deduced that tailoring the weights issubjective, and that an automated approach would likelyprovide weights independently of a priori knowledge. Yetone possibility that was considered, and perhaps a futurepossibility, was investigating the use of genetic program-ming algorithms [25] to approximate weights for the drugontologies, and then apply k-fold validation to establish ifthe suggested weights are useful. Supervised learning orother related approaches are potential options.SEMS (Semiotic Evaluation Metric Suite) aka OntokeeperAnother direction we are engaged is to develop a front-end tool for users to evaluate ontologies very quickly, andalso to have some suggested ideas for users to improvethe ontology based on the scores [26]. The prototype web-based tool was called SEMS (Semiotic Evaluation MetricSuite), now called Ontokeeper, which supports most ofthe automated score generation, and will facilitate the col-lection of feedback from subject matter experts to assistin the calculation of the accuracy score. Figure 2 shows asample screenshot of the updated version of Ontokeeper.ConclusionUsing a semiotic framework for ontology evaluation,this paper demonstrated a tailored metric that closelyapproximated the quality of a set of NCBO drug ontolo-gies. The scores and sub-scores from examination indi-cated that NCBO drug ontologies could improve withgreater use of syntactic ontological features, better selec-tion of terms and terms with expressive quality, and per-haps improve consistency among the terms and labels.Through the use of a multidimensional metric-basedapproach, our efforts may be one of several promisingdirections for biomedical ontology evaluation that needsfurther investigation.AcknowledgementsResearch was partially supported by the National Library Of Medicine of theNational Institutes of Health under Award Number R01LM011829 and theCancer Prevention Research Institute of Texas (CPRIT) Training Grant#RP160015.Authors contributionsMA developed the draft and produced the data. TC revised the draft andreviewed the results. Both authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 22 November 2016 Accepted: 17 March 2017Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 DOI 10.1186/s13326-017-0136-ySOFTWARE Open AccessRDFIO: extending Semantic MediaWiki forinteroperable biomedical data managementSamuel Lampa1* , Egon Willighagen2, Pekka Kohonen3,4, Ali King5, Denny Vrandec?ic´6,Roland Grafström3,4 and Ola Spjuth1AbstractBackground: Biological sciences are characterised not only by an increasing amount but also the extremecomplexity of its data. This stresses the need for efficient ways of integrating these data in a coherent description ofbiological systems. In many cases, biological data needs organization before integration. This is not seldom acollaborative effort, and it is thus important that tools for data integration support a collaborative way of working. Wikisystems with support for structured semantic data authoring, such as Semantic MediaWiki, provide a powerful solutionfor collaborative editing of data combined with machine-readability, so that data can be handled in an automatedfashion in any downstream analyses. Semantic MediaWiki lacks a built-in data import function though, which hindersefficient round-tripping of data between interoperable Semantic Web formats such as RDF and the internal wiki format.Results: To solve this deficiency, the RDFIO suite of tools is presented, which supports importing of RDF data intoSemantic MediaWiki, with metadata needed to export it again in the same RDF format, or ontology. Additionally, thenew functionality enables mash-ups of automated data imports combined with manually created data presentations.The application of the suite of tools is demonstrated by importing drug discovery related data about rare diseasesfrom Orphanet and acid dissociation constants from Wikidata. The RDFIO suite of tools is freely available for downloadvia pharmb.io/project/rdfio.Conclusions: Through a set of biomedical demonstrators, it is demonstrated how the new functionality enables anumber of usage scenarios where the interoperability of SMW and the wider Semantic Web is leveraged forbiomedical data sets, to create an easy to use and flexible platform for exploring and working with biomedical data.Keywords: Semantic MediaWiki, MediaWiki, Wiki, Semantic Web, RDF, SPARQL, WikidataBackgroundWhile much attention has been paid to the ever growingvolumes of biological data from recently emerging highthroughput technologies [1, 2], the biological sciences areimportantly also characterised by the extreme complexityof its data. This complexity stems both from the incredibleinherent complexity of biological systems, as well as fromthe vast number of data formats and assisting technolo-gies developed by the scientific community to describethese systems. In order to provide a coherent descriptionof biological systems making use of the data sources avail-able, data integration is of central importance [3]. Also,*Correspondence: samuel.lampa@farmbio.uu.se1Department of Pharmaceutical Biosciences, Uppsala University, SE-751 24,Uppsala, SwedenFull list of author information is available at the end of the articlewhile there are vast amounts of biological data publiclyavailable, for many problems the necessary data to be inte-grated is still comparably small, however complex, and inneed of organization before integration.Biological data integration is an active field of researchand a number of strategies have been presented foraddressing the data integration problem [4, 5]. Data inte-gration involves a wide range of considerations, includingdata governance, data licensing issues and technology. Interms of technical solutions, the most central solution fordata integration proposed so far is a set of flexible andinteroperable data formats and technologies commonlyreferred to as the Semantic Web [6, 7], with its mainunderlying data format and technology, the ResourceDescription Framework (RDF) [8, 9], accompanied bytechnologies such as the SPARQL Protocol and RDF© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 2 of 13Query Language (SPARQL) [10] and the Web OntologyLanguage (OWL) [11].The power of these data formats and technologies lie intheir ability to capture data, ontologies and linking infor-mation between multiple ontologies in a single underlyingserialisation format. This enables disparate user commu-nities to create data sets adhering to different ontologiesand adding linking information between datasets after-wards. It furthermore enables generic tools to leveragethe ontology and linking information to present datafrom multiple sources in a coherent, integrated fashion,on-demand.While most biological data today is not available inRDF format, initiatives such as the Bio2RDF project [12]are tackling this by providing a way to convert publiclyavailable datasets in non-RDF formats to RDF, by writ-ing so called rdfizers for each dataset, and using a URInormalisation scheme developed as part of the projectto ensure that URIs referring to the same object areencoded in the same way [12]. More recent examples ofwell supported RDF-ization efforts of biological data arethe Open PHACTS project and platform [13, 14], pro-viding an integrated environment for working with dataand tools related to drug discovery, and the EBI RDF [15]platform, which provides data from multiple of EBIs bio-logical data sources in an integrated semantic data layerwhere connections between multiple data sources caneasily be made, e.g. at the time of querying the data via theSPARQL endpoint made available.The heterogeneous nature of biological data alsomeans that the task of managing, annotating, curat-ing and verifying it is prohibitively complex for a sin-gle researcher to carry out because of the knowledgeneeded to understand the many biological systems, dataformats and experimental methods involved. This high-lights the importance of effective collaborative tools inbiology, to allow experts from multiple sub-fields withinbiology to work together to build integrated biologi-cal data sources. For example, in the chemicals andnanomaterials safety science field, semantically annotateddatabases with domain-specific ontologies are being usedto standardise collaborative community data entry andcuration [16, 17].One successful approach to enable flexible collaborationon biological data is wiki systems [18, 19]. Wikis facilitatecollaboration by removing technological complexity fromthe editing process, allowing anyone with access to thewiki to edit any part of it. Instead of complicated authen-tication controls, it generally manages trust in the contentby saving every change in the system as a new revision,not allowing deletion of content, and logging which userdid the change. This way, other users can review changesmade andmake any corrections needed or simply roll backchanges that do not fulfil the criteria set up for the datasource, resulting in a simple and friendly environment forediting content for any user.Plain-text wiki systems have a large drawback though:They only allow plain text to be stored while lacking sup-port for structured, machine-readable, data. To solve thisproblem a solution proposed by a number of groups isto combine a wiki system with support for storing struc-tured data in the form of semantic facts, consisting of apropertyvalue pair, closely mapping to the predicate andobject in RDF triples, and resulting in a combination ofthe ease-of-use, and flexibility of wikis, with the abilityto create structured, machine-readable data. A review ofnumerous Semantic Wiki implementations is available in[20]. A recent wiki approach for databases was introducedwith the Wikibase software used by the Wikidata project[21] and is already used in the life sciences [22, 23]Semantic MediaWiki (SMW) [24] is currently one of themost known and widely used semantic wikis. One of thefactors for its success is that it is based onMediaWiki [25],the software powering Wikipedia and thousands of otherwikis. SMW allows to combine the unstructured contentof typical MediaWiki wikis, with structural semantic con-tent, encoded using a dedicated syntax that extends theMediaWiki syntax.SMW has found a number of uses in biomedical con-texts. Apart from often being used as an internal wikisystem at many labs, it has also been used in publiclyavailable resources, including MetaBase [26], a wiki-database of biological databases, SNPedia [27], a wiki-database focusing on medically and personally relevantShort Nucleotide Polymorphisms (SNPs), the Gene Wikiportal onWikipedia [28], and a catalog of a transcriptomebased cellular state information in mammalian genomesin the FANTOM5 project [29].SMW has many features to make it interoperable withthe rest of the Semantic Web, such as export of normalwiki pages and the facts that relate them, as RDF/XML,export of Categories as OWL classes and so called Con-cepts [30] as OWL class descriptions [31]. Also, integra-tion with third party semantic data stores is possible viathird party plugins. It also has a feature to enable so calledVocabulary import, which is a way to link properties inthe wiki to predicates of external Semantic Web ontolo-gies, by manually creating special articles that define theselinks [32].A notable limitation of SMW is the lack of a general RDFdata import function. That is, the ability to do automaticbatch import of RDF datasets into the wiki. Note thatsuch a functionality is distinct from the so called vocabu-lary import feature described earlier, which only enablesmanual linking of properties to ontology items, but noautomatic import of data, and no support for import-ing plain RDF triples (OWL individuals), regardless ofwhether an ontology is used or not.Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 3 of 13This lack of a general RDF import function meansthat usage scenarios such as bootstrapping new wikisfrom existing data sources, or round-tripping between theSMW data structure and the RDF data format used in thewider Semantic Web, are not possible without externaltools. This has important consequences, since for exam-ple round-tripping between SMW and RDF could provideimportant benefits for data integration. As already men-tioned, wiki systems have proven to be excellent platformsfor collaborative editing. Thus, by storing RDF data in atext format closely resembling normal wiki syntax, it ispossible to leverage the benefits of a proven wiki platformto lower the barrier to entry for new users to start edit-ing semantic data. In other words, allowing full round-tripbetween SMW and RDF data sets would allow to presentRDF data in a format more apt to collaborative editing andcuration, after which it can be exported again into the RDFformat for use in the wider Semantic Web.Additionally, import of RDF data sets into SMWwould allow creating mash-ups, combining automati-cally imported data sets of moderately large size withmanually created presentations of this data using thequerying and visualisation tools available in SMW or itseco-system of third-party libraries. Based on these pos-sibilities it can be concluded that RDF import in SMWis an enabler of a number of usage scenarios useful indata integration, including making working with seman-tic data easier for users without deep knowledge of theSemantic Web.There exist a few solutions for semantic data import inSMW, developed as third-party extensions. Among these,Fresnel Forms [33] is focused on the import of an ontologystructure rather than plain RDF triples (OWL individ-uals), and also requires running the Protégé softwareoutside of the wiki installation. Furthermore, the LinkedWiki Extension [34] allows import of plain RDF triples butdoes this by importing the triples into an external triplestore rather than inserting the data as SMW facts insidethe wiki source text, which is required for being able tofurther modify the data in the wiki format.To solve this lack of plain triples RDF data import intoSMW facts in the wiki text, a set of tools and SMW exten-sions commonly named as the RDFIO suite was devel-oped. These tools and extensions are presented belowtogether with biomedical demonstrators of the benefits ofthe methodology.ImplementationThe RDFIO suite consists of the following parts:1. A web form for importing RDF data via manual entryor copy-and-paste.2. A SPARQL endpoint allowing both querying andcreation of RDF triples via an INSERT INTOstatement, as well as RDF export by runningCONSTRUCT queries.3. A SPARQL endpoint replicator, which can importsemantic data from an external SPARQL endpoint(in essence creating a mirror of the data set).4. A command-line import script for import of RDFdata stored in a file.5. A command-line export script for export for RDFdata into a file.6. A standalone command-line tool for converting RDFtriples into a MediaWiki XML file, for further importusing MediaWikis built-in XML import function,named rdf2smw (referred to as rdf2smw below).Tools 1-5 above were developed in the PHP program-ming language, as modules of a common MediaWikiextension called RDFIO. An overview picture of how theseparts are related to each other is available in Fig. 1. Tool 6above, which is a standalone tool, was developed in the Goprogramming language to provide shorter execution timesfor the RDF-to-wiki page conversion of large data sets.Tools 1-3 are implemented as MediaWiki Special-pages,each providing a page with a web form related to theirtask. Tools 1-5 all rely on the PHP based RDF libraryARC2 [35]. ARC2 provides its own MySQL-based datastore which is used for all its functions and which isinstalled in the same database as the MediaWiki instal-lation when installing RDFIO. To enable the ARC2 datastore to capture the data written as facts in the wiki a cus-tom SMW data store was developed. It hooks into eachpage write and converts the SMW facts of the page intothe RDF format used in the ARC2 store.The most resource demanding part of the import pro-cess is the creation of wiki pages in the MediaWiki soft-ware. Thus, to enable previewing the structure of the wikipages, most importantly the wiki page titles chosen, beforerunning the actual import, the standalone tool in 6 abovewas developed. By generating a MediaWiki XML file asan intermediate step before the import, the user has theoption to view the wiki page content and titles in theMediaWiki XML file in a text editor before running thefile through MediaWikis built-in import function. Whilethis is not a mandatory step, it can be useful for quicklyidentifying whether any configuration settings should bechanged to get more useful wiki page titles, before themore time-consuming MediaWiki import step is initiated.The limitation of using the standalone tools is that anymanual changes would be overwritten by re-running theimport (although an old revision with the manual changewill be kept, like always in MediaWiki). We thus antici-pate that the external tool will only be used for the ini-tial bootstrapping of the wiki content, while any importsdone after manual changes have been made, will be doneusing the PHP based import tool mentioned above, whichLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 4 of 13Fig. 1 Overview of the intended usage for the different parts of the RDFIO suite. The figure shows how RDF data can be retrieved from a set ofdifferent sources, as well as being exported again. The parts belonging to the RDFIO SMW extension and the rdf2smw tool are marked with dashedlines. The newly developed functionality in this paper is drawn in black while already existing functionality in MW and SMW is drawn in grey color.Red arrows indicate data going into (being imported into) the wiki, while blue arrows indicate data going out of (being exported from) the wiki.From top left, the figure shows: i) how RDF data files can be batch imported into SMW either by using the rdf2smw tool to convert them toMediaWiki XML for further import using MediaWikis built-in XML import function, or via the importRdf.php commandline script in the RDFIOSMW extension, ii) how plain triples (OWL individuals) can be imported from text files, or from web pages via copy and paste into a web form, iii)how a remote triple store exposed via a SPARQL endpoint can be replicated by entering the SPARQL endpoint URL in a web form, iv) how new RDFdata can be created manually or dynamically in the SPARQL endpoint via SPARQL INSERT INTO statements supported by the SPARQL+extension [44] in the ARC2 library, and finally, v) how data can also be exported via the SPARQL endpoint, using CONSTRUCT queries, or vi) by usingthe dedicated exportRdf.php commandline scriptsupports updating facts in place without overwritingman-ual changes.Results and discussionTo solve the lack of RDF import in SMW, the RDFIO suitewas developed, including the RDFIO SMW extensionand the standalone rdf2smw tool. The SMW extensionconsists of a set of functional modules, each consist-ing of a MediaWiki Special page with a web form, or acommandline script. A description of the features andintended use of each of these parts follows. See also Fig. 1for a graphical overview of how the different parts fittogether.RDF import web formThe RDF import web form allows the user to import RDFdata in Turtle format either from a publicly accessible URLon the internet, by manually entering or copy-and-pastingthe data into a web form. This allows users to import smallto moderate amounts of RDF data without the need forcommand-line access to the computer where the wiki isstored, as is often required for batch import operations.The drawback of this method is that since the importoperation is run as part of the web server process, it is notsuited for large amounts of data. This is because it wouldthen risk using up too much computational resourcesfrom the web server and making the website unresponsivefor other users for a single-server setting, which is oftenused in the biomedical domain.SPARQL import web formThe SPARQL import web form allows importing alldata from an external triple store exposed by a publiclyaccessible SPARQL endpoint. Based on an URL pointingLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 5 of 13to an endpoint it will in principle create a mirror of it,since the data imported into the wiki will in turn beexposed as a SPARQL endpoint (see the correspondingsection below). The import is done with a query thatmatches all triples in the external triple store (In technicalterms, a SPARQL clause of the form: WHERE { ?s ?p?o } ). In order not to put too much load on the webserver, the number of triples imported per execution isby default limited by a pre-configured limit. This enablesperforming the import in multiple batches. The user canmanually control the limit and offset values, but the off-set value will also be automatically increased after eachimport, so that the user can simply click the import but-ton multiple times, to import a number of batches withthe selected limit of triples per batch.SPARQL endpointThe SPARQL endpoint (see Fig. 2) exposes all the seman-tic data in the wiki as a web form where the data canbe queried using the SPARQL query language. The end-point also allows external services to query it via theGET or POST protocols. It can output either a formattedHTML table for quick previews and debugging of queries,a machine-readable XML result set, or full RDF triplesin RDF/XML format. The RDF/XML format requires theuse of the CONSTRUCT keyword in the SPARQL queryto define the RDF structure to use for the output. UsingCONSTRUCT to output RDF/XML basically amounts to aweb based RDF export feature, which is why a separateRDF export web form was not deemed necessary.The SPARQL endpoint also allows adding new data tothe wiki using the INSERT INTO statement available inthe SPARQL+ extension supported by ARC2.RDF import batch scriptThe batch RDF import batch script (importRdf.php) isexecuted on the command-line, and allows robust importof large data sets. By being executed using the standalonePHP or HHVM (PHP virtual machine) [36, 37] executableand not the web server process, it will not interfere withthe web server process as much as the web form basedimport. It will also not run into the various execution timelimits that are configured for the PHP process or the webserver. While a batch-import could also be implementedusing the web form by using a page reload feature, or anAJAX-based JavaScript solution, this is a more complexsolution that has not yet been addressed due to time con-straints. Executing the batch RDF import script in theterminal can look like in Fig. 3.Stand-alone RDF-to-MediaWiki-XML conversion tool(rdf2smw)The rdf2smw tool uses the same strategy for conversionfrom RDF data to a wiki page structure as the RDFIOFig. 2 A screenshot of the SPARQL endpoint web form in RDFIO. A key feature of the SPARQL endpoint is the ability to output the original RDFresource URIs of wiki pages, that were used in the original data imported. This can be seen by the checkbox option named Query by EquivalentURIs and Output Equivalent URIs, named so because the original URIs are stored using the Equivalent URI special property, on each page createdin the importLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 6 of 13Fig. 3 Usage of the command-line import tool in RDFIO. The figure shows examples of shell commands to use to import an RDF dataset, in this casein N-triples format, saved in a file named dataset.nt. The steps are: i) Change directory into the RDFIO/maintenance folder, and then ii)execute the importRdf.php script. One can set the variables --chunksize to determine how many triples will be imported at a time, and--offset to determine how many triples to skip in the beginning of the file, which can be useful if restarting an interrupted import session. The$WIKIDIR variable represents the MediaWiki base folderextension but differs in the following way: Whereas theRDFIO extension converts RDF to wiki pages and writesthese pages to the wiki database in one go, the standalonetool first converts the full RDF dataset to a wiki pagestructure and writes it to an XML file in MediaWikisXML import format, as illustrated in Fig. 1. This for-mat is very straightforward, storing the wiki page data asplain text, which allows to manually inspect the file beforeimporting it.Programs written in Go are generally orders of magni-tude faster than similar programs written in PHP. Thisperformance difference together with the fact that the exe-cution of the standalone rdf2smw tool is separate fromthe web server running the wiki is crucial when import-ing large data sets (consisting of more than a few hundredtriples) since the import requires demanding data opera-tions in memory such as sorting and aggregation of triplesper subjects. This is themain reason why this external toolwas developed.The usage of the tool together with MediaWikis built-inXML import script is illustrated in Fig. 4.RDF export batch scriptThe RDF export batch script (exportRdf.php) is acomplement to the RDF export functionality available inthe SPARQL endpoint, which analogously to the importbatch script allows robust export of large data sets with-out the risk for time-outs and other interruptions thatmight happen to the web server process or the users webbrowser.Executing the batch RDF export script in the terminalcan look like in Fig. 5.An overview of the RDF import processAs can be seen in Fig. 1, all of the import functions runthrough the same RDF-to-wiki conversion code except forthe rdf2smw tool which has a separate implementation ofroughly the same logic in the Go programming language.The process is illustrated in some detail in Fig. 6 andcan be briefly be described with the following processingsteps: All triples in the imported chunk (number of triplesper chunk can be configured for the commandlineimport script while the web form imports a singlechunk) are aggregated per subject resource. This isdone since each subject resource will be turned into awiki page where predicate-object pairs will be addedas SMW fact statements consisting of acorresponding property-value pair. WikiPage objects are created for each subjectresource. The title for this page is determined fromthe Uniform Resource Identifier (URI) of the subject,or from some of the predicates linked to this subject,according to a scheme described in more detail below. All triples with the same subject, which have nowbeen aggregated together, are turned into SMW facts(property-value pairs), to be added to the wiki page.Predicate and object URIs are converted into wikipage titles in the process, so that the correspondingFig. 4 Command-line usage of the rdf2smw tool. The figure shows the intended usage of the rdf2smw command line tool. The steps are, one perline in the code example: i) Execute the rdf2smw tool to convert the RDF data into a MediaWiki XML file. ii) Change directory into the MediaWikimaintenance folder. iii) Execute the importDump.php script, with the newly created MediaWiki XML file as first argument. The $WIKIDIRvariable represents the MediaWiki base folderLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 7 of 13Fig. 5 Usage of the command-line export tool in RDFIO. The figure shows examples of shell commands to use to export an RDF dataset, in this casein N-triples format, into a file named dataset.nt. The steps are: i) Change directory into the RDFIO/maintenance folder, and then ii) executethe exportRdf.php script, selecting the export format using the --format parameter. The --origuris flag tells RDFIO to convert SMWsinternal URI format back to the URIs used when originally importing the data, using the linking information added via SMWs Equivalent URI propertyFig. 6 A simplified overview of the RDF to wiki page conversionprocess. The figure shows in a somewhat simplified manner, theprocess used to convert from RDF data to a wiki page structure. Codecomponents are drawn as grey boxes with cog wheels in the righttop corner, while data are drawn as icons without a surrounding box.From top to bottom, the figure shows how RDF triples are firstaggregated per subject, then converted into one wiki page persubject, while converting all URIs to wiki titles, for new pages and linksto pages, where-after the pages are either written directly to the wikidatabase (the RDFIO SMW extension), or converted to XML andwritten to files (the standalone rdf2smw tool)property and value will be pointing to valid wiki pagenames. Naturally, if the object is a literal rather thanan URI, no transformation will be done to it. Duringthis process the pages corresponding to the createdproperty titles are also annotated with SMW datatype information, based on XML Schema typeinformation in the RDF source data. Optionally, the facts can be converted into aMediaWiki template call, if there is a templateavailable that will write the corresponding fact, by theuse of its parameter values. In the rdf2smw tool only, the wiki page content isthen wrapped in MediaWiki XML containing metadata about the page, such as title and creation date. In the RDFIO SMW extension only, the wiki pageobjects are now written to the MediaWiki database.Converting URIs to user friendly wiki page titlesThe primary challenge in the described process is to figureout user friendly wiki titles for the resources representedby URIs in the RDF data. This is done by trying out adefined set of strategies, stopping as soon as a title couldbe determined. The strategies start with checking if thereis already a page available connected to the URI via anEquivalent URI fact in the wiki text. If this is the case, thisexisting title (and page) will be used for this triple. If that isnot the case, the following strategies are tried in the statedorder: 1) If there are any properties commonly used toprovide a title or label for a resource, such as dc:titlefrom the Dublin Core ontology [38], the value of thatproperty is used. 2) If a title is still not found, the base part,or namespace of the URI is shortened according to anabbreviation scheme provided in the RDF dataset in theform of namespace abbreviations. 3) Finally, if none of theabove strategies could provide an accepted title, the localpart of the URI (The part after the last / or # characterin the URL) is used.PerformanceTable 1 provides information about the time neededto import a given number of triples (100, 1000, 10000or 100000) drawn as subsets from a test dataset (theLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 8 of 13Table 1 Execution times for importing RDF data into SMW usingthe importRdf.php script in the RDFIO extension (column 2) andconverting to MediaWiki XML files using the rdf2smw tool andthen importing the generated XML files with MediaWikis built-inXML import tool respectively (column 3 and 4), for a few differentdataset sizes (column 1)Number of Import RDF Convert to XML Import XMLTriples (RDFIO extension) (rdf2smw tool) (MediaWiki XML import)100 24 s 0.00 s 17 s1000 179 s (2m59s) 0.02 s 81 s (1m21s)10000 1652 s (27m32s) 0.3 s 683 s (11m23s)100000 16627 s (4h37m7s) 18 s 7063 s (1h57m43s)Comparative Toxicogenomics Database [39], convertedto RDF by the Bio2RDF project), using the RDF SMWextension directly via the importRdf.php command-line script, as well as by alternatively converting the datato MediaWiki XML files with the rdf2smw tool and thenimporting them using MediaWikis importDump.phpscript. Note that when importing using the rdf2smw toolthe import is thus performed in two phases.The tests were performed in a VirtualBox virtualmachine running Ubuntu 15.10 64bit, on a laptop runningUbuntu 16.04 64bit. The laptop used was a 2013 LenovoThinkpad Yoga 12 with a 2-core Intel i5-4210U CPU, withbase and max clock frequencies of 1.7 GHz and 2.7 GHzrespectively, and with 8 GB of RAM. The PHP versionused was PHP 5.6.11. Time is given in seconds and whereapplicable also in minutes and seconds, or hours, minutesand seconds.Manual testing by the authors show that the perfor-mance of an SMWwiki is not noticeably affected bymulti-ple users reading or browsing the wiki. An import processof many triples can temporarily slow down the brows-ing performance for other users because of table lockingin the database, though. This is a characteristic commonto MediaWiki wikis, when a large import operation is inprogress, or if multiple article updates are done at thesame time, unless special measures are taken, such as hav-ing separate, replicated, database instances for reading, toalleviate the load on the primary database instance.Continuous integration and testingThe fact that RDFIO is an extension to a larger software(SMW), which itself is an extension of MediaWiki andthat much of their functionality depends on state in arelational database, has added complexity to the testingprocess. Recently though, continuous integration systemsas well as improved test tooling for MediaWiki and SMWhas enabled better automated testing also for RDFIO.We use CircleCI as continuous integration system andresults from this and other services are added as indicatorbuttons on the README file on the respective GitHubrepositories.As part of the build process, system tests are run for theRDF import function and for the RDF export function,verifying that the exported content matches the data thatwas imported. In addition, work has been started to addunit tests. User experience testing has been carried out inreal-world projects mentioned in the introduction, wheresome of the authors were involved [16, 17].Round-trippingAs mentioned above, a system test for the round-trippingof data via the RDF and import and export functions isrun, to ensure that no data is corrupted in the process. Itis worth noting though that the RDF export will generallyoutput more information than what is imported. This isbecause SMWdoes store certainmeta data about all pagescreated, such as modification date etc. In the system test,these data are filtered out so that the test checks only con-sistency of the triples that were imported using RDFIO.An example of the difference between the imported andexported data can be seen in Fig. 7.Known limitationsAt the time of writing this, we are aware of the followinglimitations in the RDFIO suite of tools: The rdf2smw tool supports only N-Triples format asinput. There is currently no support for importing triplesinto separate named graphs, such that e.g. importedand manually added facts could be separated andexported separately. There is no functionality to detect triples for removal,if updating the wiki with a new version of a previouslyimported dataset, containing deprecated or havingsome triples simply removed. Cases with thousands of triples for a single subjectleading to thousands of fact statements on a singlewiki page  while technically possible  could lead tocumbersome manual editing.These limitations are planned to be addressed in futureversions of the tool suite.DemonstratorsDemonstrator I: Orphanet - rare diseases linked to genesAn important usage scenario for RDFIO is to visualiseand enable easy navigation of RDF data by bootstrap-ping an SMW instance from an existing data source.To demonstrate this, the open part of the Orphanetdataset [40] was imported into SMW. Orphanet con-sists of data on rare disorders, including associated genes.The dataset was already available in RDF format throughLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 9 of 13Fig. 7 A comparison between data before and after an import/export round-trip. This figure shows to the left a dataset containing one single triplein turtle format. To the right is shown the data resulting from performing an import/export round-trip  that is, importing the initial data into avirtually blank wiki (The wiki front page Main Page being the only page in the wiki) and then running an export again. It can be seen in theexported data how i) The Main Page adds a certain amount of extra data, and ii) how there is a substantial amount of extra metadata about eachresource added by SMW. The subject, predicate and value of the initial triple is color-coded with the same colours in both code examples (bothbefore and after) to make it easier to findthe Bio2RDF project [12], from where the dataset wasaccessed and imported into SMW. This dataset consistedof 29059 triples and was first converted to MediaWikiXML using the standalone rdf2smw tool, which was thenimported using MediaWikis built-in XML import script.This presented an easy to use platform for navigating theOrphanet data, including creating listings of genes anddisorders. Some of these listings are created automaticallyby SMW but additional listings can also be created onany page in the wiki, including on the wiki pages repre-senting RDF resources, by using the template feature inMediaWiki in combination with the inline query languagein SMW [41].An example of a useful user-created listing on an RDFnode, was to create a listing of all the disorder-geneassociations linking to a particular gene and the corre-sponding disorder, on the templates for the correspondinggene pages (For an example, see Fig. 8). In the same way,a listing of the disorder-gene association linking to partic-ular disorders and the corresponding genes, was createdon the templates for the corresponding disorder pages.This example shows how it is possible, on a wiki pagerepresenting an RDF resource, to list not only informationdirectly linked to this particular resource, but also infor-mation connected via intermediate linking nodes. Con-cretely, in the example shown in Fig. 8 we list a resourcetype (diseases) on a page representing a gene even thoughin the RDF data diseases are not directly linked to genes.Instead they are linked via an intermediate gene-disorderassociation node.Demonstrator II: DrugMet - cheminformatics/metabolomicsThe DrugMet dataset is an effort at collecting experi-mental pKa values extracted from the literature, linkedto the publication from which it was extracted, and tothe chemical compounds for which it was measured. TheDrugMet dataset was initially created by manually addingthe details in a self-hosted Semantic MediaWiki. The datawas later transferred to the Wikidata platform [21] forfuture-proofing and enabling access to the data for thewider community.This demonstrator highlights how this data could be fur-ther curated by extracting the data again from Wikidatainto a locally hosted SMW for further local curation.The data was exported fromWikidata using its publiclyavailable SPARQLREST interface [42]. The extraction wasdone using a CONSTRUCT query in SPARQL allowing tocreate a custom RDF format specifically designed for thedemonstrator. For example, in addition to the publicationand compound data, the query was modified to includerdf:type information for all the compounds, which isused by the RDFIO command line tool to generate aMedi-aWiki template call and corresponding template, for allitems of this type.After the data was imported into a local SMW wiki, itallowed to create a page with an SMW inline query dis-playing a dynamically sorted list of all the compounds,their respective pKa values, and links to the publicationsfrom where the pKa values were originally extracted. Thequery for this extraction is shown in Fig. 9, and the list isshown in Fig. 10.Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 10 of 13Fig. 8 Screenshot of a wiki page for a gene in the Orphanet dataset. In the middle of the page, the listing of gene disorder associations and thecorresponding disorders is shown. Note that these details are not entered on this page itself, but are queried using SMWs inline query languageand dynamically displayed. To the right are details entered directly on the pageImplications of the developed functionalityThe demonstrators above show that the RDFIO suite oftools is successfully bridging the worlds of the easy-to-usewiki systems and the somewhatmore technically demand-ing wider Semantic Web. This bridging has opened upa number of useful scenarios for working with seman-tic data in a flexible way, where existing data in semanticformats can easily and flexibly be combined by using thetemplating and querying features in SMW. This leads toa powerful experimentation platform for exploring andsummarising biomedical data, which earlier was not read-ily accessible.Availability Complete information about the RDFIO project canbe found at pharmb.io/project/rdfio A canonical location for information about theRDFIO SMW extension is available at MediaWiki.orgat www.mediawiki.org/wiki/Extension:RDFIOFig. 9 The SPARQL query for extracting DrugMet data. This screenshot shows the SPARQL query for extracting DrugMet data in Wikidatas SPARQLendpoint web form. This query can be accessed in the Wikidata SPARQL endpoint via the URL: goo.gl/C4k4gxLampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 11 of 13Fig. 10 A dynamic listing of DrugMet data. The listing shows a locally hosted SMWwiki with a list of compounds and related information. The list is acustom, dynamically generated listing of Compound name, pKa value and a link to the publication from which each pKa value was extracted,created using SMWs inline query language All the software in the RDFIO suite is available fordownload on GitHub, under the RDFIO GitHuborganisation, at github.com/rdfio where the RDFIOSMW extension is available at github.com/rdfio/rdfio,the rdf2smw tool at github.com/rdfio/rdf2smw andan automated setup of a virtual machine with a fullyconfigured SMW wiki with RDFIO installed isavailable at github.com/rdfio/rdfio-vagrantbox.OutlookPlanned future developments include enhancing therdf2smw tool with support formore RDF formats as input.Further envisioned development areas are:iv) Separating the ARC2 data store and SPARQLendpoint into a separate extension, so that the coreRDFIO SMW extension does not depend on it. Thiscould potentially improve performance of data importand querying, as well as make the core RDFIO exten-sion easier to integrate with external triple stores viaSMWs triple store connector. v) Exposing the RDF importfunctionality as a module via MediaWikis action API[43]. This would allow external tools to talk to SMWvia an established web interface. vi) Allowing to storedomain specific queries tied to certain properties thatcan, on demand, pull in related data for entities of acertain ontology such as gene info from Wikidata, forgenes.Availability and requirementsProject name: RDFIOProject home page: https://pharmb.io/project/rdfioOperating system(s): Platform-independent (Linux,Windows, Mac)Programming language: PHP (The RDFIO SMW exten-tion), Go (The rdf2smw tool)Other requirements: A webserver (Apache or Nginx), AMySQL compatible database,MediaWiki, SemanticMedi-aWiki, ARC2 (RDF library)License: GPL2 (The RDFIO SMW extention), MIT (Therdf2smw tool)ConclusionsThe RDFIO suite of tools for importing RDF data intoSMW and exporting it again in the same RDF format(expressed in the same ontology) has been presented. Ithas been shown how the developed functionality enablesa number of usage scenarios where the interoperabil-ity of SMW and the wider Semantic Web is leveraged.The enabled usage scenarios include; i) Bootstrappinga non-trivial wiki structure from existing RDF data, ii)Round-tripping of semantic data between SMW and theRDF data format, for community collaboration of thedata while stored in SMW, and iii) Creating mash-ups ofexisting, automatically imported data and manually cre-ated presentations of this data. Being able to combinethe powerful querying and templating features of SMWwith the increasing amounts of biomedical datasets avail-able as RDF has enabled a new, easy to use platformfor exploring and working with biomedical datasets. Thiswas demonstrated with two case studies utilising link-ing data between genes and diseases as well as data fromcheminformatics/metabolomics.Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 12 of 13AbbreviationsAJAX: Asynchronous Javascript and XML. A technology to access a web servicefrom Javascript, for receiving content or performing actions; OWL: Webontology language; RAM: Random-access memory; RDF: Resource descriptionframework; SMW: Semantic MediaWiki; SPARQL: SPARQL protocol and RDFquery language; URI: Uniform resource identifierAcknowledgementsThe authors thank Joel Sachs for mentoring AK during the Gnome FOSS OPW2014 project.FundingThe work was supported by the Google Summer of Code program of 2010granted to WikiMedia Foundation, the Gnome FOSS OPW program for 2014granted to WikiMedia foundation, the Swedish strategic research programmeeSSENCE, the Swedish e-Science Research Centre (SeRC), and theeNanoMapper project EU FP7, technological development and demonstration(FP7-NMP-2013-SMALL-7) under grant agreement no. 604134.Authors contributionsDV, SL: original concept; SL, OS, EW: planning and design; SL, AK:implementation; SL, PK, EW, RG: applications. All authors read and approvedthe manuscript.Availability of data andmaterialThe source code of the published software is available at GitHub, http://github.com/rdfio.The data used in Demonstrator I in this study are available from the Bio2RDFwebsite, http://download.bio2rdf.org/release/3/orphanet/orphanet.html.The data used in Demonstrator II in this study are available from a customquery in the Wikidata SPARQL Endpoint, https://query.wikidata.org.The query used in the Wikidata SPARQL endpoint is available on GitHub,together with a direct link to the Wikidata SPARQL Endpoint with the queryprefilled, https://gist.github.com/samuell/45559ad961d367b5d6a26269260dc29a.The authors declare that all other data supporting the findings of this studyare available within the article.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Pharmaceutical Biosciences, Uppsala University, SE-751 24,Uppsala, Sweden. 2Department of Bioinformatics - BiGCaT, NUTRIM, MaastrichtUniversity, P.O. Box 616, UNS50 Box 19, NL-6200 MD Maastricht, TheNetherlands. 3Institute of Environmental Medicine, Karolinska Institutet, SE-17177 Stockholm, Sweden. 4Division of Toxicology, Misvik Biology Oy, Turku,Finland. 5FanDuel Inc, Edinburgh, UK. 6Google Inc., 345 Spear Street, SanFrancisco, USA.Received: 2 May 2017 Accepted: 1 August 2017RESEARCH Open AccessMeSH Now: automatic MeSH indexing atPubMed scale via learning to rankYuqing Mao1,2 and Zhiyong Lu2*AbstractBackground: MeSH indexing is the task of assigning relevant MeSH terms based on a manual reading of scholarlypublications by human indexers. The task is highly important for improving literature retrieval and many otherscientific investigations in biomedical research. Unfortunately, given its manual nature, the process of MeSHindexing is both time-consuming (new articles are not immediately indexed until 2 or 3 months later) and costly(approximately ten dollars per article). In response, automatic indexing by computers has been previously proposedand attempted but remains challenging. In order to advance the state of the art in automatic MeSH indexing, acommunity-wide shared task called BioASQ was recently organized.Methods: We propose MeSH Now, an integrated approach that first uses multiple strategies to generate acombined list of candidate MeSH terms for a target article. Through a novel learning-to-rank framework, MeSH Nowthen ranks the list of candidate terms based on their relevance to the target article. Finally, MeSH Now selects thehighest-ranked MeSH terms via a post-processing module.Results: We assessed MeSH Now on two separate benchmarking datasets using traditional precision, recall and F1-score metrics. In both evaluations, MeSH Now consistently achieved over 0.60 in F-score, ranging from 0.610 to 0.612. Furthermore, additional experiments show that MeSH Now can be optimized by parallel computing in order toprocess MEDLINE documents on a large scale.Conclusions: We conclude that MeSH Now is a robust approach with state-of-the-art performance for automaticMeSH indexing and that MeSH Now is capable of processing PubMed scale documents within a reasonable timeframe. Availability: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.BackgroundThe rapid growth of scholar publications in biomedicinemakes the search of relevant information in literature in-creasingly more difficult, even for specialists [1, 2]. Todate, PubMedthe U.S. National Library of Medicine(NLM) premier bibliographic databasecontains over 24million articles from over 5,600 biomedical journals withmore than a million records added each year. To facilitatesearching these articles in PubMed, a controlled vocabu-lary called Medical Subject Headings (MeSH)1 was createdand updated annually by the NLM since 1960s. Currently,MeSH 2015 consists of over 27,000 terms representing awide spectrum of key biomedical concepts (e.g. Humans,Parkinson Disease) in a hierarchical structure. MeSHterms are primarily used to index articles in PubMed forimproving literature retrieval: The practice of manuallyassigning relevant MeSH terms to new publications inPubMed by the NLM human indexers is known as MeSHindexing [3]. Assigned MeSH terms can then be used im-plicitly (e.g., automatic query expansion using MeSH) orexplicitly in PubMed searches [4]. Compared with thecommonly used keyword-based PubMed searches, MeSHindexing allows for semantic searching (using the relation-ship between the subject headings) and searching againstconcepts not necessarily present in the PubMed abstract.In addition to its use in PubMed, MeSH indexing re-sults have also been used creatively in many other scien-tific investigation areas, including information retrieval,text mining, citation analysis, education, and traditionalbioinformatics research (see Fig. 1). When applied to in-formation retrieval, MeSH and its indexing results havebeen used to build tag clouds for improving the* Correspondence: zhiyong.lu@nih.gov2National Center for Biotechnology Information (NCBI), 8600 Rockville Pike,Bethesda, MD 20894, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 DOI 10.1186/s13326-017-0123-3visualization of search results [5, 6] and to help distin-guish between publication authors with identical names[7, 8]. Another major use of MeSH indexing is in bio-medical text mining, where it has been applied to prob-lems such as document summarization [9], documentclustering [10], and word sense disambiguation [11].MeSH indexing also serves several key roles in cit-ation analysis, from identifying emerging researchtrends [12, 13] to measuring similar journals [14] andcharacterizing research profiles for an individual re-searcher, institute or journal [15]. In the era ofevidence-based practice, MeSH becomes increasinglyimportant in assessing and training the literaturesearch skills of healthcare professionals [16, 17], aswell as in assisting undergraduate education in bio-logical sciences [18]. Finally, much bioinformatics re-search, such as gene expression data analysis [19, 20],greatly benefits from MeSH indexing [2125].Like many manual annotation projects [2630],MeSH indexing is a labour-intensive process. Asshown in [3, 31], it can take an average of 2 to3 months for an article to be manually indexed withrelevant MeSH terms after it first enters PubMed. Inresponse, many automated systems for assistingMeSH indexing have been previously proposed. Ingeneral, most existing methods are based on the followingtechniques: i) pattern matching, ii) text classification, iii)k-Nearest Neighbours, iv) learning-to-rank, or v) combin-ation of multiple techniques. Pattern-matching methods[32] search for exact or approximate matches of MeSHterms in free text. Automatic MeSH indexing can also beregarded as a multi-class text classification problem whereeach MeSH term represents a distinct class label. Thusmany multi-label text classification methods have beenproposed, such as neural networks [33], Support VectorMachines (SVM) [34, 35], Inductive Logic Programming[36], naïve Bayes with optimal training set [37], StochasticGradient Descent [38], and meta-learning [39]. While thepattern matching and text classification methods use onlythe information in the MeSH thesaurus and document it-self, the k-Nearest Neighbours (k-NN) approach takes ad-vantage of the manual annotations of documents similarto the target document, e.g. [40, 41]. Additional informa-tion, such as citations, can also be utilized for auto-matic MeSH indexing. For example, Delbecque andZweigenbaum [42] investigated computing neighbourdocuments based on the cited articles and cited au-thors. More recently, Huang et al. [3] reported anovel approach based on learning-to-rank algorithms[43]. This approach has been shown to be highly suc-cessful in the recent BioASQ2 challenge evaluations[4446] and has also been adopted by many others[47, 48]. Finally, many methods attempt to combineresults of different approaches [49, 50]. For instance,the current production system in MeSH indexing atthe NLM is called Medical Text Indexer (MTI),which is a hybrid system that combines both patternmatching and k-NN results [51] via manually-developed rules and continues to be improved overthe years [52, 53]. The proposed method in this workis also a hybrid system but unlike MTI, which onlyuses machine learning to predict a small set of MeSHterms, it combines individual results and ranks theentire set of recommendations through machinelearning instead of heuristic rules.Despite these efforts, automatic MeSH indexing re-mains a challenging task: the current state-of-the-artperformance remains at about 0.6 in F-measure [54].Several factors contribute to this performance bottle-neck: First, since each PubMed article can be assignedwith multiple MeSH terms, i.e. class labels, the task ofautomatic MeSH indexing can be seen as a multi-classFig. 1 Applications of MeSHMao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 2 of 9classification problem. In this regard, the size of theMeSH vocabulary makes automatic classification chal-lenging: 2014 MeSH includes more than 27,000 mainsubject headings and they are not equally used in index-ing [31]. Second, MeSH indexing is a highly complexcognitive task. It has been reported that the consistencybetween human indexers is only 48.2% for main headingassignment [55]. Lastly, both the MeSH vocabulary andindexing principles keep evolving over time. For in-stance, in response to emerging new concepts in the bio-medical research, MeSH 2014 includes almost five timesmore concepts than the edition of MeSH in 1963 thatonly contains 5,700 descriptors. On the other hand, thearticles in PubMed are not re-indexed when MeSH getsupdated. Thus, it is not always obvious in selectingbenchmarking data sets for system development andcomparison.In this paper, we propose a new method, MeSH Now,to the automatic MeSH indexing task. MeSH Now isbuilt on our previous research [3] but has a number ofsignificant advancements: First, MeSH Now combinesdifferent methods through machine learning. Second,new post-processing and list-pruning steps are nowadded in MeSH Now for improved performance. Third,from a technical perspective, MeSH Now is optimizedusing the latest MeSH lexicon and recent indexed arti-cles for system training and development. Finally, MeSHNow is implemented to operate in a parallel computingenvironment, making it possible for large-scale process-ing needs (e.g., providing computer results of newPubMed articles for assisting human indexing). Forevaluation, we first test MeSH Now on a previous data-set that was widely used in benchmarking. Furthermore,we created a new benchmarking dataset based on the re-cent BioASQ 2014 challenge task data. Our experimentalresults show that MeSH Now achieves state-of-the-artperformance on both data sets.MethodsApproach overviewOur approach reformulates the MeSH indexing task as aranking problem. Figure 2 shows the three main steps:First, given a target article, we obtain an initial list ofcandidate MeSH terms from three unique sources. Next,we apply a learning-to-rank algorithm to sort the candi-date MeSH terms based on the learned associations be-tween the document text and each candidate MeSHterm. Finally, we prune the ranked list and return anumber of top candidates as the final system output.Prior to these steps, some standard text processing wasperformed such as removing stop words and applying aword-stemming algorithm.Input source I: K-nearest neighboursWe first adapt the PubMed Related Articles algorithm[56] to retrieve k-nearest neighbours for each newPubMed article. The assumption is that documents simi-lar in content would share similar MeSH term annota-tions. Previous work [3] has supported this assumptionby showing that over 85% of the gold-standard MeSHannotations for a target document are present in itsnearest 20 neighbours.Furthermore, we found that retrieving neighboursfrom the whole MEDLINE database performed worsethan only retrieving neighbours from a subset of thedatabase (e.g., articles in the BioASQ Journal List, ornewly published articles). In particular, the results ofour approach are best when limiting the neighbourdocuments to articles indexed in the last 5 years (i.e.the articles were assigned with MeSH terms after2009). As mentioned before, MeSH terms evolveevery year but the articles already indexed will neverbe re-indexed. The same article would likely beassigned with different MeSH terms in 2014 versus20 years ago. Thus there are many outdated MeSHFig. 2 System overviewMao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 3 of 9terms in those neighbour documents, which can beharmful to the accuracy of our approach. Moreover,the word frequencies are also different in the older andmore recent articles, which are closely related to the simi-larity score for two articles. Therefore, we built our indexwith only articles that were assigned with MeSH termsafter 2009, and retrieved the neighbour documents usingsuch a new index instead of retrieving similar documentsfrom the whole PubMed. When building our documentindex for the PubMed Related Articles algorithm3, we alsomake sure that all annotated MeSH terms are removedsuch that they are not used in the computation of theneighbour documents. In other words, the similarity be-tween two documents is solely based on the words theyhave in common.The parameter k was fixed (k = 20) in [3], whichmeans the same number of neighbours will be in-cluded for all target articles. However, we observedthat some articles may only have a few very similardocuments. We therefore adjust the parameter k dy-namically between 10 to 40 in this work according tothe similarity scores of the neighbours: the smallerthe average similarity score of the neighbours, thefewer neighbours will be used. Once those k-nearestneighbour documents are retrieved, we collect all ofthe unique MeSH terms associated with those neigh-bour documents. Note that we only considered themain headings and removed subheadings attached tothe main headings.Input source #2: multi-label text classificationMotivated by [57], we implemented a multi-label textclassification approach where we treat each MeSH con-cept as a label and build a binary classifier accordingly.More specifically, we first train individual classificationmodels for each of the most frequently indexed 20,000MeSH terms, as the remaining ones are rarely used inindexing. Then we apply these models to the new articleand add those positively classified MeSH concepts ascandidates to the initial list. We also keep those associ-ated numerical prediction scores and use them as fea-tures in the next step.Our implementation is based on the cost-sensitiveSVM classifiers [58] with Huber loss function [59]. Cost-sensitive SVMs have been shown to be a good solutionfor dealing with imbalanced and noisy data in biomed-ical documents [60]. Let C+ denote the higher misclassi-fication cost of the positive class and C? denote thelower misclassification cost of the negative class, the costfunction is formulated as:?2wk k2 þ CþXi:yi¼1h yi ? þ w?xið Þð Þ þ C?Xi:yi¼?1h yi ? þ w?xið Þð Þwhere MeSH terms are treated as class labels C in theclassification, xi is a document of a given class (ieassigned with a specific MeSH term), ? is aregularization parameter, w is a vector of featureweights, and ? is a threshold. The function h is themodified Huber loss function and has the form:h zð Þ ¼?4?z;1?zð Þ2;0;8<:z??1?1 < z < 11?zWe can choose C+ to be greater than C? to overcomethe dominance of negative points in the decision process(here we set C+ = rC? and the ratio r to be 1.5). To trainthese 20,000 classifiers, we used the MEDLINE articlesthat were indexed with MeSH terms between January2009 and March 2014.Input source #3: MTI resultsMTI is used as one of the baselines in the BioASQ Task,which primarily uses MetaMap to map the phrases inthe text to UMLS (Unified Medical Language System)concepts [61]. We thus add all MeSH terms predictedby MTI as candidates, and obtained the feature vectorsfor those MeSH terms. This is useful since the MTI re-sults can return correct MeSH terms not found by theother two methods.Learning to rankOnce an initial list of candidate MeSH terms fromall three sources are obtained, we approached thetask of MeSH indexing as a ranking problem. In ourprevious work, we trained the ranking function withListNet [62], which sorts the results based on a listof scores. In this work we evaluated several otherlearning-to-rank algorithms [43] on the BioASQ testdataset, including MART [63], RankNet [64], Coord-inate Ascent [65], AdaRank [66], and LambdaMART,which are available in RankLib v2.24, and found thatLambdaMART achieved the best performance.LambdaMART [67] is a combination of MART andLambdaRank, where the MART algorithm can beviewed as generalizations of logistic regression [63]and LambdaRank is a method for learning arbitraryinformation retrieval measures [68]. To train such amodel, LambdaMART uses gradient boosting tooptimize a ranking cost function where the baselearners are limited-depth regression trees. New treesare added to an ensemble sequentially that best ac-count for the remaining regression error of the train-ing samples, i.e., each new tree greedily minimizesthe cost function. LambdaMART uses MART withspecified gradients and Newtons approximation.LambdaMART is briefly presented as follows [67]:Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 4 of 9First, we obtained a training set consisting of biomedicalarticles with human assigned MeSH terms from MED-LINE. For each article, we obtain an initial list of MeSHterms from its neighbour documents. Each MeSH term isthen represented as a feature vector. For the list of MeSHterms from its neighbour documents, denoted by {M1,M2, , MN}, where N is the number of feature vectors andMi is the ith feature vector, we obtain a corresponding list{y1, y2, , yN}, where yi?{0,1} is the ith class label. yi = 1 ifthe MeSH term was manually assigned to the target articleby expert indexers of the NLM, otherwise yi =0.BioASQ provided approximately 12.6 million PubMeddocuments for system development. Since all PubMeddocuments can be used as training data, we randomlyselected a set of 5,000 MEDLINE documents from thelist of the journals provided by BioASQ for training andoptimizing our learning-to-rank algorithm.FeaturesWe reused many features developed previously: neighbour-hood features, word unigram/bigram overlap features,translation probability features [69], query-likelihood fea-tures [70, 71], and synonym features.For neighbourhood features, we calculate both neigh-bourhood frequency  the number of times the MeSHterm appears in the neighbours, and neighbourhoodsimilarity  the sum of similarity scores for theseneighbours.For translation probability features, we use the IBMtranslation model [69], which uses title and abstract assource language, and MeSH terms as target language.We then utilize an EM-based algorithm to train thetranslation probabilities.For query-likelihood features, we treat each MeSHterm as Query (Q), title and abstract as document, anduse two genres of query models: classic BM25 model[70] and translation-based query model [71], to calculatethe probability of whether a MeSH term should beassigned to the article.In this work, we added a new domain-specific know-ledge feature. We used a binary feature indicatingwhether a candidate term is observed by MTI, whichMao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 5 of 9relies heavily on the domain-specific UMLS Meta-thesaurus [72], for generating its results.To compute the average length of documents and thedocument frequency for each word, a set of approxi-mately 60,000 PubMed documents is assembled. Thesedocuments are sampled from recent publications in theBioASQ Select Journal List. The translation model andthe background language model were built throughtraining with this data set accordingly.Post-processing and list pruningWe further improve our results with some post-processing steps.First, we observed that the Check tags (a special set ofMeSH Headings that are mentioned in almost every art-icle such as human, animal, male, female, child, etc.5) es-pecially the tags for the age factor are most difficult forour approach. The reason is that the Check tags are fre-quently present in the neighbour documents, e.g., an art-icle describing a disease in children might have manysimilar documents discussing about the same disease inadults, which will result in assigning the undesirableCheck tag Adult to the new article. On the other hand,it is improper to simply exclude the tag Adult ifChild already exists, because many articles in PubMedindeed include both Adult and Child as MeSH terms.More importantly, many Check tags related to age infor-mation are added according to the full text article. InBioASQ, we add the age check tags identified from theabstract text. We first find the numbers near the explicitage in the abstract, then predict the correct Age CheckTag according to those numbers and the rules for agecheck tags.Second, to improve the precision, we remove the par-ental MeSH terms when a more specific term is alsopredicted. This heuristic is based on the principle thatindexers should prefer the most specific term applicableinstead of more general terms. Therefore in the candi-date list, if a child term is ranked higher than its parentterm, we will remove the latter accordingly.Finally, after each MeSH term in the initial list isassigned a score by the ranking algorithm describedabove, the top N ranked MeSH terms will be consideredrelevant to the target article. N was set to be a fixednumber (N = 25) previously. We found, however, thatthe average number of MeSH terms per article in theBioASQ training data was only 12.7. Thus, we used anautomatic cut-off method to further prune the resultsfrom the top ranked MeSH terms as follows:Siþ1 < Si? log ið Þ??where Si is the score of the predicted MeSH term at pos-ition i in the top ranking list. The rationale for Formula(1) is that if the (i + 1)th MeSH term was assigned with ascore much smaller than the ith MeSH term, the MeSHterms ranked lower than i would not be considered rele-vant to the target article. Formula (1) also accounts forthe fact that the difference between lower-ranked MeSHterms is subtler than the difference between higher-ranked MeSH terms. The parameter ? was empiricallyset to be 0.3 in this research, and it can be tuned to gen-erate predictions favouring either recall or precision.ResultsBenchmarking datasetsTo demonstrate the progress of our development overtime and compare with other systems, we report oursystem performance on two separate data sets. One ofthem was widely used in previous studies: NLM2007 [3].The NLM2007 dataset contains 200 PubMed documentsobtained from the NLM indexing initiative6. The otheris created from the BioASQ 2014 test datasets:BioASQ5000.In 2014, the BioASQ challenge task [45] ran for sixconsecutive periods (batches) of 5 weeks each. For eachweek, the BioASQ organizers distributed new unclassi-fied PubMed documents, and participants have a limitedresponse time (less than 1 day) to return their predictedMeSH terms. As new manual annotations become avail-able, they were used to evaluate the classification per-formance of participating systems. To be more general(each BioASQ test set contains continuous PMIDs whichmay belong to a limited set of journals), we randomly se-lected 5,000 PubMed documents from the latest 9BioASQ test sets (start from Batch 2 Week 2 in order toavoid overlap with our system training data) to createBioASQ5000, with their corresponding MeSH termsalready assigned by December 6, 2014. Compared toNLM2007, BioASQ5000 is much larger in size and con-tains more recent articles in 2014.Comparison of different methodsHere we present our results when evaluated on the twodatasets. We first show results on the previously re-ported benchmarking dataset, NLM2007 [3] in Table 1.For comparison, we show the results of our previouswork as Huang et al., [3], and the results of theTable 1 Evaluation results on NLM 2007 test setMethods Precision Recall F1MTI  2011 0.318 0.574 0.409Huang et al. 2011 [3] 0.390 0.712 0.504Text Classification 0.655 0.355 0.461MTI  2014 0.568 0.525 0.545MeSH Now 0.622 0.602 0.612Bold data are the best valueMao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 6 of 9previous and current versions of MTI (MTI 2011 andMTI 2014). It should be noted that here we usedMeSH 2010 and retrieved neighbour documents pub-lished before the articles in NLM2007, and our learning-to-rank model was trained with documents publishedbefore the articles in NLM2007, because the newly pub-lished articles are assigned with new MeSH terms whichare not available in NLM2007. We can see that MeSHNow makes significant improvement over our previousmethod. We also notice that the results of MTI-2014 aremuch better than those of its previous version. BothMTI-2014 and text classification results (results of inputsource #2) contribute to the MeSH Now performancewith better results generated by MTI than textclassification.Table 2 shows the results on the BioASQ5000 dataset.For comparison, we added the results of MTI First Line(MTIFL_2014) and MTI Default (MTIDEF_2014), bothof which were used as baselines of the BioASQ chal-lenge. This further verifies that our new approach out-performs existing methods.System throughputThe time complexity of large-scale automatic indexing iscrucial to real-world systems but rarely discussed in thepast. In Table 3, we present the average processing timeof each step of our method based on BioASQ5000 on asingle computer. We can see that text classification ap-pears to be a bottleneck given the large size of the classi-fiers (20,000). However, this step can be performed inparallel so that the overall time can be greatly reduced.For example, our current system takes approximately9 h to process 700,000 articles via a computer clusterwhere 500 jobs can run concurrently.Discussion and conclusionsTo better understand the differences between thecomputer-predicted and human-indexed results, weconducted an error analysis based on the results ofMeSH Now on BioASQ5000 dataset. First, we foundthat the predicted MeSH terms with the lowest per-formance belong to MeSH Category E: Analytical,Diagnostic and Therapeutic Techniques and Equip-ment, especially the Statistics as Topic subcategory,such as Chi-Square Distribution, Survival Analysis,etc. This is most likely due to the lack of sufficientpositive instances in the training set (i.e. the numbersof these indexed terms in the gold standard are rela-tively small). On the other hand, the most incorrectlypredicted MeSH terms are Check Tags (e.g. Male,Female, Adult, Young Adult, etc.) despite thatthe F1 scores of these individual Check Tags are rea-sonably high (most are above the average). Because oftheir prevalence in the indexing results, however, im-proving their prediction is critical for increasing theoverall performance.As mentioned before, MeSH Now was developed in2014 based on the learning-to-rank framework we firstproposed in 2010 [3] for automatic MeSH indexing. Atthe same time, our ranking framework was adopted byseveral other state-of-the-art systems such as MeSHLa-beler [73] and DeepMeSH [74]. MeSHLabeler is verysimilar to MeSH Now with the major difference in usinga machine learning model to predict the number ofMeSH terms instead of heuristics. DeepMeSH furtherincorporates deep semantic representation into MeSH-Labeler for improved performance (0.63 in the latestBioASQ challenge in 2016).There are some limitations and remaining chal-lenges in this work for the automatic MeSH indexingtask. First, our previous work revealed that 85% ofthe gold-standard MeSH annotations should bepresent in the candidate list based on the nearest 20neighbours. However, our current best recall is below65%, suggesting there is still room for improving thelearning-to-rank algorithm to promote the relevantMeSH terms higher in the ranked list. Second, ourcurrent binary text classification results are lowerthan previously reported [35], partly because for allclassifiers we simply used the same training data,which is quite imbalanced. We believe that the per-formance of MeSH Now could be further improvedif better text classification results are available to beintegrated. Finally, we are interested in exploring theopportunities of using MeSH Now in practicalapplications.Table 2 Evaluation results on BioASQ5000 test setMethods Precision Recall F1Huang et al. 2011 [3] 0.357 0.701 0.473Text Classification 0.689 0.400 0.506MTIFL  2014 0.621 0.517 0.564MTI  2014 0.587 0.559 0.573MeSH Now 0.612 0.608 0.610Bold data are the best valueTable 3 Processing time analysis for different stepsKey steps in MeSH Now Average time perdocument (ms)Obtaining candidate terms via k-NN 1890.82Obtaining candidate terms via MTI 570.33Obtaining classification results fromeach binary text classifier25.63Learning to Ranking 103.86Post-Processing and List Pruning 1.85Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 7 of 9Endnotes1http://www.ncbi.nlm.nih.gov/mesh/2http://www.bioasq.org/3http://www.ncbi.nlm.nih.gov/books/NBK3827/4http://sourceforge.net/p/lemur/wiki/RankLib/5http://www.nlm.nih.gov/bsd/indexing/training/CHK_010.html6http://ii.nlm.nih.gov/DataSets/AcknowledgementsWe would like to thank the MTI authors and the BioASQ organizers. We alsothank Dr. Robert Leaman for his proofreading of this manuscript. Thisresearch is supported by the NIH Intramural Research Program, NationalLibrary of Medicine, the National Natural Science Foundation of China(81674099, 81603498), the Six Talent Peaks Project of Jiangsu Province, China(XYDXXJS-047), the Qing Lan Project of Jiangsu Province, China (2016), andthe Priority Academic Program Development of Jiangsu Higher EducationInstitutions (PAPD).Availability of data and materialsThe datasets supporting the conclusions of this article are available in http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.Authors contributionsZL conceived the study. YM and ZL participated in its design, analyzed theresults and wrote the manuscript. YM collected the data, implemented themethods and performed the experiments. Both authors read and approvedthe final manuscript.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Nanjing University of Chinese Medicine, 138 Xianlin Avenue, Nanjing,Jiangsu 210023, China. 2National Center for Biotechnology Information(NCBI), 8600 Rockville Pike, Bethesda, MD 20894, USA.Received: 30 June 2016 Accepted: 16 March 2017SOFTWARE Open AccessLarge-scale adverse effects related totreatment evidence standardization(LAERTES): an open scalable system forlinking pharmacovigilance evidencesources with clinical dataThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeAbstractBackground: Integrating multiple sources of pharmacovigilance evidence has the potential to advance the scienceof safety signal detection and evaluation. In this regard, there is a need for more research on how to integratemultiple disparate evidence sources while making the evidence computable from a knowledge representationperspective (i.e., semantic enrichment). Existing frameworks suggest well-promising outcomes for such integrationbut employ a rather limited number of sources. In particular, none have been specifically designed to support bothregulatory and clinical use cases, nor have any been designed to add new resources and use cases through anopen architecture. This paper discusses the architecture and functionality of a system called Large-scale AdverseEffects Related to Treatment Evidence Standardization (LAERTES) that aims to address these shortcomings.Results: LAERTES provides a standardized, open, and scalable architecture for linking evidence sources relevant tothe association of drugs with health outcomes of interest (HOIs). Standard terminologies are used to representdifferent entities. For example, drugs and HOIs are represented in RxNorm and Systematized Nomenclature ofMedicine  Clinical Terms respectively. At the time of this writing, six evidence sources have been loaded into theLAERTES evidence base and are accessible through prototype evidence exploration user interface and a set of Webapplication programming interface services. This system operates within a larger software stack provided by theObservational Health Data Sciences and Informatics clinical research framework, including the relational CommonData Model for observational patient data created by the Observational Medical Outcomes Partnership. Elements ofthe Linked Data paradigm facilitate the systematic and scalable integration of relevant evidence sources.Conclusions: The prototype LAERTES system provides useful functionality while creating opportunities for furtherresearch. Future work will involve improving the method for normalizing drug and HOI concepts across theintegrated sources, aggregated evidence at different levels of a hierarchy of HOI concepts, and developing moreadvanced user interface for drug-HOI investigations.Keywords: Pharmacovigilance, Post-market drug safety, Clinical terminologies, Linked-data* Correspondence:Suite 419, 5607 Baum Blvd, Pittsburgh, PA 15202, USA© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Knowledge Base workgroup of the Observational Health Data Sciences andInformatics (OHDSI) collaborative Journal of Biomedical Semantics  (2017) 8:11 DOI 10.1186/s13326-017-0115-3BackgroundA recent report from the United States Department ofHealth and Human Services noted that, while medicationshelp millions of people live longer and healthier lives, theyare also the cause of approximately 280,000 hospital admis-sions each year and an estimated one-third of all hospitaladverse events [1]. The field of post-market drug safety sur-veillance focuses on applying the most current methodo-logical advances to help identify undesired effects of drugsand biologics. One of the major opportunities and chal-lenges to safety investigators is that there are many disparateevidence sources from which a safety concern might eitherbe identified or evaluated. These may include spontaneousreporting systems, electronic health records, the literature,Web search logs, and social media. [27]. Safety concernscan also be predicted from the knowledge about the chem-ical structure and pharmacological properties of drugs [8].Combining multiple sources of biomedical evidence hasbeen shown to have value for improving the precision of au-tomated signal identification [9], and for identifying bothestablished [10] and new safety concerns [11].Consistent with these results, there has been a recentcall for more research on combinatorial signal detectionthat is defined as integrating multiple disparate evidencesources while making the evidence computable from aknowledge representation perspective (i.e., semantic en-richment) [12]. Examples of such features include: The use of formal (i.e., logically defined andcomputable) definitions for the meaning of entitiesrepresented in the database such as drugs and HOIs. Formally defined relationships between the entitiesrepresented in each integrated evidence source. Computational methods for inferring newknowledge from evidence, for example using rule-based or machine learning methods.Existing frameworks that have integrated varioussources in a way that provide some of these features in-clude ADEPedia [13, 14], MetaADEDB [15], CATTLE[16], and Bio2RDF [17]. Fig. 1 shows the evidence sourcesintegrated into these systems. As the figure indicates,there are several alternate sources that could be integratedincluding VigiBase®, pharmacovigilance signals from mul-tiple sources (or using alternative methods) [18], elec-tronic health records signals from multiple sources (orusing alternative methods) [19], alternate approaches toextracting safety concerns from unstructured text usingnatural language processing [20], and various sources ofdrug-drug interaction evidence [21].With the exception of Bio2RDF, none of the aforemen-tioned systems have an open architecture that would en-able the integration at large-scale, systematically, whilefacilitating the integration of new sources. Bio2rdf doeshave an open architecture. The code for loading a datasource into Bio2RDF is open source enabling motivatedscientists to create a local version of Bio2RDF and then in-tegrate a new source by writing code that translates thesources data into a Resource Description Framework(RDF) graph according to Bio2RDF conventions [22].They can also edit the translation code for existing sourcesto alter decisions that are made during the integrationprocess. This ability is important, because there are a var-iety of decisions made on how these sources are integratedthat can influence downstream analyses. Table 1 showssome of the decisions to be made with respect the generalFig. 1 The information sources of existing knowledge-based systems for pharmacovigilance. Citations to the sources mentioned can be found inthe Background section. EHR: electronic health record, AE: adverse event, EU: European Union, FAERS: Food and Drug Administration AdverseEvent Reporting System, CTD: Comparative Toxicogenomics DatabaseThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 2 of 15Extract, Translate, and Load (ETL) process for pharma-covigilance (i.e., not specific only to Bio2RDF).In a previous paper [23], we presented the vision of estab-lishing an open-source community effort to develop a globalknowledge base of known associations between drugs andHOIs: one that brings together and standardizes all availableinformation for all drugs and all HOIs from all electronicsources pertinent to drug safety. To make this vision a reality,a workgroup within the Observational Health Data Sciencesand Informatics (OHDSI) collaborative [24] was organizedfor the purpose of developing a standardized knowledge basefor the effects of medical products, and an efficient proced-ure for maintaining and expanding it. The main purpose ofthe knowledge base is to make it simpler to access, retrieve,and synthesize evidence so that users can develop an assess-ment of causal relationships between a given drug and HOIas accurate as current evidence provides.This paper discusses the results of this workgroupthus far. Specifically, the paper discusses the architec-ture and functionality of a prototype system calledLarge-scale Adverse Effects Related to Treatment EvidenceStandardization (LAERTES). LAERTES provides open andscalable architecture for linking evidence sources relevantto investigating the association of drugs with HOIs. Theremaining sections of this paper will discuss the motivatinguser story, the systems architecture, implementationdetails, and the current beta release.ImplementationMotivating user story and goalSafety physicians and risk management analysts investigatenew adverse drug event reports and emerging drug safetysignals. A typical way to express the requirements imposedon a software system by a specific user group is via the so-called user stories. The user story which drove the devel-opment of the LAERTES platform is defined as follows:As a safety physician or risk management analystmonitoring the safety of a marketed drug, I want to do acomprehensive search across known or emerging drug-HOI evidence so I can thoroughly and expeditiouslytriage emerging potential safety signals and assess theirpotential impact.In order to do that, a number of tasks have to be car-ried out including:1) quickly determining if a specific adverse event hasbeen previously reported for a given drug;2) gauging if a potential safety concern is at the clinicaldrug (e.g. simvastatin 20 mg oral tablet), ingredient(e.g., simvastatin), or class (e.g., statins) level;3) assessing the credibility of the sources reporting theassociation, and4) deciding what priority an adverse event signal mightwarrant for further investigation.LAERTES system and data architectureThe LAERTES system architecture (Fig. 2) includes threemain components that operate within a larger software eco-system provided by the OHDSI clinical research framework.The three components are 1) a Resource DescriptionFramework (RDF) data store that represents all includedevidence sources as Open Annotation Data (OA) model[25], 2) a relational data store that enables both summaryqueries providing an overview of evidence across all in-cluded sources, and drill down queries that examine import-ant information on specific evidence items; and 3) a webservices layer that hides the details of how to query the RDFand relational component so that client programs can moreeasily benefit from their combined functionality. The nextfew sub-sections discuss these components in more detail.RDF data and the drill down use caseRDF is a standard developed through the World WideWeb Consortium (W3C) that uses Uniform ResourceIdentifiers (URIs) and a graph-based data model to repre-sent any kind of connected data [26]. Since the introduc-tion of RDF as a key component of the Semantic Web, thestandard has become widely used, especially in the bio-medical sciences [27]. In comparison with the relationaldata model, the underlying graph model of RDF makesquerying across heterogeneous data sets simple. The datarepresented in RDF data are computable and semanticallynon-ambiguous through the use of URIs and ontologies.RDF Linked Data provides a convention to ensure thatall data items across multiple connected graphs are easilyaccessible using standard web technology.In LAERTES, a specific piece of evidence in favor oragainst an association between a drug and an HOI fromany integrated data source is represented using the OpenAnnotation Data (OA) model. OA is a standard forrepresenting human and computer annotations that isgaining broad adoption among many publishing com-munities. In LAERTES, every item of evidence about adrug-HOI pair is represented as an OA resource presentin the RDF data store (Fig. 2). The reasons for this ap-proach are to 1) use a single standard approach to repre-senting evidence items regardless of the source and 2)harness the aforementioned benefits of Linked Data.Each OA resource provides the data about the sourceof a specific evidence item (the target) and the seman-tic tags used to identify the record as a relevant evidenceitem (the body or bodies). The body of each OA hasresources for drug, drug group, and HOI concepts thatare represented using standard vocabularies (MedicalSubject Headings (MeSH), Medical Dictionary for Regu-latory Activities (MedDRA), Systematized Nomenclatureof Medicine - Clinical Terms (SNOMED-CT), andRxNorm). To enable integration with other data sourcesin the OHDSI clinical research framework and facilitateThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 3 of 15Table 1 Decisions that are made during the process of integrating sources that can influence downstream pharmacovigilanceanalysesData Type Feature Option for variability Performance questionsProduct labels Product label outcomementionNamed entity performance(PPV and sensitivity)Do improvements in entity recognitionperformance improve system recalland precision?Section location (e.g., anywherevs specific sections)Does identifying which sections are moreinformative than others reduce noise?Frequency information Threshold variation Does incorporation of ADE frequency improveperformance? What cut-off should be used?Pharmacovigilance DBs (e.g.FAERS, MedEffect, VigiBase)Minimum detectable relativeriskThreshold variation What is the appropriate cut-off for MDRR?Is it HOI specific?Database (s) chosen Does the database influence the value ofMDRR for this task?Risk identification method Disproportionality metric What metric (e.g. PRR, EBGM, IC) leads tothe best performance? Is it HOI specific?Number of cases in FAERS Threshold variation What is the appropriate cut-off for numberof case reports?Drug Indication DB Indication listings in FDB Yes/no and when mentioned Does using on-label and off-label indicationknowledge improve performance?Indexed literature Number of relevantpublications from the indexedliteratureThreshold variation Is there an appropriate cut-off for numberof publications? What is its variability relativeto specific HOIs and drugs?Source of relevantpublications from the indexedliteratureVarying the combination of sources Should we be selective about the sourcesused or chose all of them?Drug and outcome mentionin relevant indexed literatureNamed entity performance Do improvements in entity recognitionperformance improve system recalland precision?Main MeSH terms vs supplemental What is the value of MeSH supplementalterms relative to the primary index terms?Scientific discourse tag of the locationof mention (e.g., intro, methods, results,conclusions)Does limiting identification of drug-HOIco-mention to specifically tagged textexcerpts improve performance?Publication type label (randomized trial,case report, etc.)Should the publication type of thedrug-HOI co-mention be tracked andpossibly weighted to improve performance?Source of publication type label(Embase, MeSH)Is one publication type indexing systembetter than the other for the questionanswering task, or should theybe combined?Topic of the source publication basedon latent semantic indexingDoes the use of tags assigned to textsources by latent semantic indexingimprove system performance if usedas a feature?Observational health data(claims + EHR)Minimum detectable relativeriskThreshold variation What is the appropriate cut-off for MDRR?Is it HOI specific?Database (s) chosen Does the database influence the value ofMDRR for this task?Risk identification method Analytic method What method (e.g. disproportionalityanalysis, self-controlled case series, ICtemporal pattern discovery,high-dimensional propensity score)leads to the best performance?Is it HOI specific?The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 4 of 15the reuse of the LAERTES platform in the context of theOHDSI analytical tools, the source codes from the rele-vant terminology are replaced with the equivalent con-cept_id from the concept table which is part of theObservational Medical Outcomes Partnership (OMOP)Standard Vocabulary [28]. Fig. 3 shows an entity rela-tionship diagram for the OA resources created for ad-verse drug reactions extracted from US drug productlabeling. Graphs with the same basic structure (an an-notation resource linked to a target and a body) arecreated for other evidence sources but given adifferent type and selectors that are appropriate tothe source. For example, an OA resource that repre-sents a drug-HOI evidence from MEDLINE MeSHtag assignment would be given the type ohdsi:Pub-MedDrugHOIAnnotation and a selector with theexact text of the title and abstract.Relational data and the summary use caseAs the system diagram in Fig. 2 shows, aggregated evidenceexists for LAERTES in a relational data store. Within thedata store, there are linkouts to OA resources (describedbelow). A web application programming interface (API) isable to interact with both the relational data store as well asthe RDF linkouts. This interoperable representational statetransfer (REST) API can be leveraged for user interaction ei-ther directly or via third-party applications.The schema for the primary tables used in the relationaldata store is shown in Fig. 4. The evidence_sources tableholds metadata on each data source that has been loadedinto LAERTES. The table drug_hoi_relationship is used tohold the concept identifiers for the drug and HOI pairs usedin the drug_hoi_evidence table. Drug and HOI concepts inthis table have been converted from the source terminologies(e.g., MeSH, MedDRA) to RxNorm and SNOMED-CTTable 1 Decisions that are made during the process of integrating sources that can influence downstream pharmacovigilanceanalyses (Continued)Cohort selection Patient ethnicity, age, sex, co-morbidities, concurrent medicationsDoes cohort selection using thesefeatures affect model performance?What is the appropriate size anddiversity of the cohort to reducenoise and bias?Drug exposure conditions Length of exposure, dosage Does selecting minimum exposureduration criteria and/ or drug dosageinformation improve performance?Study replicability Number of locations for confirmingresultsHow many replicates of the studyshould be performed at differentinstitutions?Observation period Observation duration threshold Does setting minimum observation perioddurations improve performance?PPV: positive predictive value, OMOP: Observational Medical Outcomes Partnership, ADE: adverse drug event, MDRR: minimal detectable reporting ratio, HOI:health outcome of interest, DB: database, FAERS: Food and Drug Administration Adverse Event Reporting System, EBGM: empirical Bayes geometric mean. IC:information component, FDB: First Data Bank (commercial drug knowledge base), EHR: electronic health recordFig. 2 The overall architecture of LAERTES within the OHDSI clinical research software environment. REST: representational state transfer, OHDSI:Observational Health Data Sciences and Informatics, API: application programming interface, DBMS: database management system, CDM:common data model, OA: Open Annotation Data, RDF: Resource Description FrameworkThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 5 of 15concepts using relationships present in the Standard Vocabu-lary provided by the OHDSI clinical research framework(OMOP Vocabulary in Fig. 4). Natural language processing(NLP) is applied to sources that do not use a specific termin-ology. For example, the validated NLP tool SPLICER [29] isused to process United States product labeling from unstruc-tured text to RxNorm drug and MedDRA HOI mentions. Akey point is that clinical datasets represented in the OHDSIclinical research framework will use the standardized con-cepts from RxNorm and SNOMED making it possible toFig. 3 An entity relationship diagram showing how data from US product labeling is represented as a semantically enriched Open AnnotationData graphFig. 4 The data architecture of LAERTES. The system leverages the OMOP Vocabularies to describe drugs and health outcomes of interest viastandardized vocabulary concepts (concept table). LAERTES stores aggregated evidence in a summary table (drug_hoi_evidence) that provides alinkout (evidence_linkout) to an Open Annotation Data representation of the source data. In the relational database, the linkout functions as aforeign key to the adr_annotation table through a table (not shown) that maps the linkout to annotation identifiers (adr_annotation_uid). Clientprograms can also use the linkout as a URL to retrieve JSON data from an RDF store that has a linked data version of the source openannotation dataThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 6 of 15create queries that join the merged evidence in LAERTESwith clinical data.The drug_hoi_evidence table provides aggregate sum-mary statistics for every drug-HOI pair from a givensource, noting, wherever possible, if the evidence sup-ports or refutes an association. Aggregation across thesources is possible because all drug and HOI conceptsare translated to RxNorm and SNOMED-CT, respect-ively. The aggregation is based on a number of differ-ent scores and coefficients. For example, a specificdrug-HOI association might have evidence from spon-taneous reporting in the form of adverse event countsas well as the results of disproportionality analysesover the reporting database (i.e., proportional report-ing rates and other signal statistics [30]). If so, thedrug_hoi_evidence table would hold a distinct recordfor both statistics while indicating each records typein the statistic_value field.Evidence linkouts  the bridge between the summaryand drill down use casesAn important data element in the drug_hoi_evidencetable is the evidence_linkout column. This holds a URLthat functions as a foreign key to the adr_annotationtable through a table (not shown) that maps the linkoutto annotation identifiers. This enables analysts using therelational database to examine an OA represention ofthe source records used to create the summary data lo-cated in drug_hoi_evidence. Client programs can alsouse the linkout as a URL to retrieve JSON data fromthe RDF store that has a linked source open annota-tion data for the purpose of displaying this evidence.An example will help clarify the functionality. First,the following SPARQL script (executable on the pub-lic OHDSI RDF store [31]) shows how to query for asource document in the RDF store shown in the sys-tem diagram (Fig. 2):# The URI to the source document fromwhich the anonymous PubMed drug-HOI OA# resource represented by ?s is returnedin the ?sourceDocument variablePREFIX oa: <http://www.w3.org/ns/oa#>PREFIX ohdsi: <http://purl.org/net/ohdsi#>SELECT ?sourceDocumentWHERE {?s a ohdsi:PubMedDrugHOIAnnotation;oa:hasTarget ?target.?target oa:hasSource ?sourceDocument.} LIMIT 10This query can be used to retrieve the evidence item for thespecific drug-HOI pair Simvastatin 20 MG Oral Tablet(identifier:1539411) and HOI muscle weakness (identifier:36516876) from a specific source (in this case the a MEDLINErecord). The important changes are shown in bold font:# The URI to the source document thatprovides evidence for an associationbetween simvastatin# and Rhabdomyolysis is returned in the?sourceDocument variablePREFIX oa: <http://www.w3.org/ns/oa#>PREFIX ohdsi: <http://purl.org/net/ohdsi#>SELECT ?sourceDocumentWHERE {?s a ohdsi:PubMedDrugHOIAnnotation;oa:hasTarget ?target;oa:hasBody ?body.# simvastatin?body ohdsi:ImedsDrug ohdsi: 1539403.# Rhabdomyolysis?body ohdsi:ImedsHoi ohdsi: 45619309.?target oa:hasSource ?sourceDocument.}SPARQL queries like this one can be sent to an RDFendpoint, in order to facilitate the reuse of the annota-tions through produced other Linked Data applications.Returning our focus to the relational data model (Fig. 4),each of the entries in the evidence_linkout column holdsa URL that encodes the specific SPARQL query neededto retrieve OA resources for a given drug, HOI, and evi-dence source. Testing revealed that the needs and pref-erences of various users required the ability to access theopen annotation data as either RDF or relational data (forexample, a pharmaceutical companys IT infrastructuremight be more amenable to working with relational datarather than RDF). At the same time, other users morefamiliar with the interoperability and inference strengths ofRDF Linked Data will benefit from the RDF representation.To accommodate this dual functionality, the exact sameencoded URLs are used as foreign keys to the adr_annota-tion table through another table that maps the linkout toannotation identifiers. The target and adr_body tables holda copy of the OA target and hasBody data (Fig. 3).The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 7 of 15Evidence rollupsOne of the key features of the proposed platform is thatsafety evidence may be linked to the drug at differentlevels of granularity: at the clinical drug product level,comprising the active ingredients, strength, formulation,and brand name for the product, but may also be morecoarsely defined simply as evidence for a particular ac-tive ingredient. For example, an adverse event might bementioned in the drug product label for only one clinicaldrug containing a specific active ingredient. However, apublished case report might discuss an adverse eventthat appears to be associated with all the drugs contain-ing the active ingredient. LAERTES supports queryingthe evidence at four different rollup levels: (1) byRxNorm drug ingredient, (2) by RxNorm drug ingredi-ent and SNOMED-CT HOI, (3) by RxNorm drug ingre-dient and RxNorm clinical drug, and (4) by full detailwhich was across the RxNorm drug ingredient, RxNormclinical drug, and SNOMED_CT HOI. These rollupqueries are supported by a table called laertes_summary(Fig. 5). Data are aggregated from the evidence itemsand inserted into this table during the evidence loadprocess using queries against the tables shown in Fig. 4.ResultsTechnical implementationAt the time of this writing, six evidence sources havebeen loaded into LAERTES representing three literaturesources, two drug product label sources, and onespontaneous reporting source. Table 2 provides a briefsummary of each source, the methods used to normalizedrug and HOIs to RxNorm and SNOMED-CT respectively,and the number of drug-HOI pairs that were available be-fore and after mapping. The specific code used to performnormalization is available from the projects GitHub site[32]. In general, custom Python scripts execute queries thatidentify OHDSI concept identifiers for the source drug andHOI concepts, and then use OHDSI Standard Vocabularymappings to translate from source concepts to RxNormand SNOMED-CT. Table 3 provides the overlap of distinctdrug-HOI pairs at the drug ingredient level across the threebroad categories of evidence (drug product labeling, pub-lished literature, and spontaneous reporting).Each evidence source was processed using a customExtract, Translate, and Load (ETL) module developed inPython. All ETL modules follow a similar pattern involv-ing 1) transforming the source data to an RDF OA graphand 2) loading the graph into an RDF endpoint, and 3)executing a query that generates statistics (e.g. countdata) and linkouts. Each linkout is URL-encoded andthen shortened using a custom implementation of theHarryJerry Linx URL shortener [33]. Python scriptsmerge the count and linkout data from each source intodata files that are loaded into the relational database. Allof the code used to create the current implementation isavailable from the open source OHDSI/KnowlegeBaseproject [32]. Evidence source updates currently occurevery 3 to 6 months and follow the same workflow.Fig. 5 The drug roll-up table and example reports by order identifierThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 8 of 15Accessing and using data in LAERTESInterested persons can currently access the data inLAERTES in a few different ways. The RDF database ishosted on a public-facing server [31] and the authorscan provide direct access to the relational database uponrequest (e.g., via direct email or a request posted to for-ums.ohdsi.org). A proof-of-concept user interface hasbeen developed [34] and is also hosted on a publiclyaccessible server (Fig. 6) [35]. This simple user inter-face allows users to query LAERTES using OMOPconcept identifiers or concept names for drug ingre-dients, drug products, or HOIs. The system presentsa summary of query results in a simple tabular for-mat. Links are provided so that users explore drillTable 2 Distinct drug-Health Outcome of Interest pairs by sourceSource description Drug and HOI mapping method Distinct drug-HOIpairs in sourceDistinct drug-HOIpairs in LAERTES (%)Adverse drug reactions mined from USdrug product labels using a validatednatural language processing tool calledSPLICER [29]Drugs were coded using RxNorm andHOIs using MedDRA. The OMOPStandard Vocabulary was used to mapMedDRA to SNOMED-CT.272 436a 254 738 (93%)Adverse drug events extracted from EUSummary of Product Characteristics bythe PROTECT projectDrugs were mentioned by name andHOIs using MedDRA codes. Drugnames were mapped to RxNorm usinga combination of simple string matchingand Bioportal ontology searches. Manycombination products and someindividual drugs were not mappable. Allmappings were manually reviewed foraccuracy.26 989 24 537 (91%)FDA Adverse Event Reporting Systemcounts and Proportional ReportingRatio from [45]The OHDSI Usagi tool [46] was used tomap drug and HOI mentions to RxNormand MedDRA. The OMOP StandardVocabulary was used to map MedDRAcoded HOIs to SNOMED-CT. A paperdescribing the database and mappingmethod has been published [47].3 766 382 2 753 078 (73%)Abstracts from titles and abstractsindexed in MEDLINE that describedrug-HOI evidence according toMeSH indexing [48]Drug and HOI concepts were both codedusing MeSH. The OMOP StandardVocabulary was used to map from MeSHdrug concepts to RxNorm and MeSH HOIconcepts to SNOMED-CT.79 119b 77 395 (97.8%)Sentence spans from titles andabstracts indexed in MEDLINEthat describe drug-HOI evidenceaccording to queries against theSemantic Medline databaseDrug and HOI concepts were both codedusing UMLS concept identifiers. The UMLSMetathesaurus MRCONSO table was usedto map concepts to RxNorm, MeSH,MedDRA, and SNOMED-CT. The OMOPstandard vocabulary was then used to mapdrug concepts only available as MeSH toRxNorm and HOI concepts only available asMedDRA or MeSH concepts to SNOMED-CT.5 023b 2 813 (56%)Chemical disease associationsfrom the ComparativeToxicogenomics DatabaseDrug and HOI concepts were both codedusing MeSH. The OMOP Standard Vocabularywas used to map from MeSH drug conceptsto RxNorm and MeSH HOI concepts toSNOMED-CT.503 835 432 850 (86%)aSPLICER drug-hoi pairs are at the clinical drug level. All other sources are at the ingredient level. bDoes not include drug-HOI evidence where the source refers tothe drug by its MeSH pharmacologic group name.EU: European Union, FDA: Food and Drug Administration, HOI: Health outcome of Interest, OMOP: Observational Medical Outcomes Partnership, US: United States,MedDRA: Medical Dictionary for Regulatory Activities, MeSH: Medical Subject HeadingsTable 3 Overlap of distinct drug-HOI pairs at the drug ingredient level after mapping drugs to RxNorm and HOIs to SNOMED-CTLiterature (MEDLINE and CTD)vs spontaneous reporting(n = 3 049 743)Product labeling (US and EU)vs spontaneous reporting(n = 2 702 577)Literature (MEDLINE and CTD)vs product labeling (US and EU)(n = 566 379)All three(n = 3 057 406)119 293 (3.9%) 87 279 (3.2%) 14 838 (2.6%) 14 295 (0.5%)The counts and percentages shown contrast the sum of the union (shown in the heading) and intersection of the distinct drug-HOI pairs from bothsources mentioned.CTD: Comparative Toxicogenomics Database, EU: European Union, US: United StatesThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 9 of 15down information. This system uses some of theseveral REST API calls that are documented on theOHDSI Wiki [36].Progress has been made on integrating the cross-platform web services provided by the current versionof LAERTES with other applications in the OHDSIclinical research framework. Specifically, the vocabu-lary browsing component of the OHDSI ATLAS webbased tool [37] can use the LAERTES API to retrievethe available evidence of drug-HOI associations thatit displays to users. Furthermore, a new extension toATLAS is under development that will enable search-ing for drugs-HOI pairs with no evidence in any in-cluded source. The outputs of this program are callednegative controls and can be used for investigatingdrug-HOI associations using observational data tocalibrate the confidence intervals of statistical esti-mates to address hidden biases within the observa-tional dataset [38].How LAERTES can support the user scenarioThe current version of LAERTES is a prototype that cansupport some of the requirements of the safety physicianand risk management analyst whose user story is men-tioned at the beginning of this paper:1) Quickly determining if a specific adverse event hasbeen previously been reported for a given drug:LAERTES currently brings together three maintypes of information where drug-HOI associationsare reported (spontaneous reports, labeling, andpublished literature). The systems open architecturemakes it possible to add additional sources such asdata from clinical trials. Because drugs and HOIs arenormalized from the source terminologies to RxNormand SNOMED-CT, a single query accomplishes thetask of identifying existing evidence from any of theincluded sources. This is possible via SPARQL andSQL queries as well as through the Web API.2) Identifying if a potential safety concern is at theclinical drug, ingredient or class level: LAERTESallows searches specifically at the clinical drug oringredient levels while also providing evidencerollups which aggregate evidence at the ingredientlevel (see Section Evidence Rollups). However, therecurrently is only one source integrated into LAERTESthat provides evidence at the clinical drug level (USdrug product labeling). This is not likely to be alimitation since, in the OHDSI clinical researchframework, all clinical drug data is loaded into theCDM drug_exposure table (not shown) and then alsorepresented at the ingredient level in the CDMdrug_era table (also, not shown).3) Identifying the credibility of the sources reportingthe association: Both the relational and RDFcomponents of LAERTES explicitly note the sourceof an evidence item and, if relevant, the particulartype of evidence. For example, an evidence itemfrom a literature source would be explicitly taggedwith the method used to identify the evidenceFig. 6 Experimental user interface to the LAERTES evidence baseThe Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 10 of 15(MeSH tags or natural language processing) and thestudy type of the article from which the item wasfound (clinical trial, case report, or other). Similarly,drug-HOI evidence from product labeling is taggedwith the specific method used to identify it and thesection from which it was pulled. These tags areintended to be useful for filtering or prioritizing evi-dence based on a users perception of the relativecredibility of the sources or evidence type. However,further research is necessary to test this assumptionand identify the requirements for other ways to helpusers more rapidly assess evidence credibility.4) Deciding what priority an adverse event signal mightwarrant for further investigation: At present,LAERTES provides only an experimental graphicaluser interface (Fig. 6) but the workgroup is activelyworking on a new user interface that will fullysupport prioritizing an adverse event signal forfurther investigation. The new user interface is beingdesigned to help users take full advantage of the newpossibilities created by bringing together the multiplesources of drug-HOI evidence into the OHDSI clinicalresearch framework. As Listing 1 shows, LAERTES isdesigned to work seamlessly with patient data that hasbeen loaded into the OMOP CDM. As a result, userswould be able to directly generate new drug-HOIevidence from one or more clinical datasets (claims,electronic health records, or registries) using OHDSIpopulation-level effect estimation methods [39]. Thesemethods, which are in development, promise rapidlarge-scale exploration of a suspected drug-HOIassociation using causal considerations which includestrength of association, consistency, temporality,experiment, plausibility, coherence, biologic gradient,specificity, and analogy [40].Listing 1An example of querying patient data on the OHDSICDM using drug HOI pairs present in the LAERTESevidence base. The query counts the number of casespresent in the clinical dataset where a patient condi-tion is recorded within 30 days of the start of a drug(as indicated by data in the CDM drug_era table).The results shown are a subset of the results thatwere generated when the query was ran on a simu-lated population available to the OHDSI researchcommunity and the general public [41]. The resultsprovide summary information and Web links thatpoint to a summary of each source evidence item inthe LAERTES RDF store. These links could be usedby a third-party application to help the user furtherdrill down into the evidence that associated thedrug with the HOI. retrieve the count of distinct patientsexposed drug-HOI combination for whichthere is evidence in LAERTES from MEDLINE orEuropean product labeling using RxNormdrug identifier, SNOMED-CT drug identifier, evidencetype, evidence linkout, andselect rxnorm_drug, snomed_hoi,evidence_type, evidence_linkout,count(distinct person_id) pcountfrom(select sub1.person_id,drug_hoi_relationship.rxnorm_drug,sub1.drug_era_start_date,sub1.drug_era_end_date,drug_hoi_relationship.hoi,drug_hoi_relationship.snomed_hoi,sub1.condition_era_start_date,drug_hoi_evidence.evidence_type,drug_hoi_evidence.evidence_linkoutfromdrug_hoi_evidence inner joindrug_hoi_relationshipon drug_hoi_evidence.drug_hoi_relationship = drug_hoi_relationship.idinner join(select drug_era.person_id,drug_era_start_date,drug_era_end_date,drug_concept_id,condition_era_start_date,condition_concept_idfrom drug_erainner join condition_era on drug_era.person_id = condition_era.person_idwhere condition_era.condition_era_start_date > drug_era.drug_era_start_dateand condition_era.condition_era_start_date - drug_era.drug_era_start_date <= 30) sub1on sub1.drug_concept_id = drug_hoi_relationship.drug and sub1.condition_concept_id = drug_hoi_relationship.hoiwhere evidence_type in('MEDLINE_MeSH_CR','MEDLINE_MeSH_ClinTrial','MEDLINE_SemMedDB_CR','MEDLINE_SemMedDB_ClinTrial','SPL_EU_SPC')) sub2group by rxnorm_drug, snomed_hoi,evidence_type, evidence_linkoutorder by pcount desc;The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 11 of 15 RESULTSrxnorm_drug | snomed_hoi | evidence_type| evidence_linkout | pcountAcetaminophen| Edema| MEDLINE_SemMedDB_CR| https://goo.gl/ikKucQ | 14Lisinopril| Abdominal pain| MEDLINE_MeSH_CR| https://goo.gl/49EvSE | 14Albuterol| Atrial fibrillation| MEDLINE_MeSH_CR| https://goo.gl/IVPdzx | 13Chlorthalidone| Hyperlipidemia|MEDLINE_MeSH_ClinTrial| https://goo.gl/gMTfzk | 13Metformin| Edema| MEDLINE_MeSH_ClinTrial| https://goo.gl/kL3GIz | 13Enalapril| Anemia| MEDLINE_MeSH_CR|https://goo.gl/AB1Lue |13Captopril| Anemia| MEDLINE_MeSH_CR|https://goo.gl/Cfhkzt | 13The system satisfies non-functional requirementsThe new system is entirely open source so that anyinterested researcher can download, run, modify,and extend the code to fit their purposes. For ex-ample, the system currently does not have a datasource that provides adverse event data from socialmedia sources. Such new evidence sources could beintegrated in the platform by developing additionalPython ETL modules [42]. The system also providessystematic evidence to facilitate OHDSIs methodo-logical research efforts to enable the design, devel-opment and evaluation of new analytical approachesto observational research, and provides the basis forestimating systematic error and performing empir-ical calibration in all population-level estimationroutines [43].DiscussionDrugs on the market need to be monitored for publicsafety. A safety physician or risk management analysthas to review all the available information for drug safetyissues, following what is currently a highly manual,time-intensive, and error-prone process. Improvement ofthe automation of bringing all the relevant evidence to-gether in a consumable format will help such individualsbetter achieve their goals. LAERTES provides an opensource framework that uses OHDSI technology to bringtogether evidence from multiple sources in a way thatwill enable the development of software to meet the usergoals mentioned at the beginning of this paper.While the current version of LAERTES provides usefulfunctionality, it also leaves opportunities for further re-search. In order to integrate the evidence sources, drugand HOI concepts have to be converted into RxNormand SNOMED-CT concepts, respectively. Table 2 showsthat there are many cases where this conversion is in-complete and some parts of the source data are not inte-grated. Future work will examine ways to improve thetranslation and mapping process.Another opportunity for research is on how to appro-priately aggregate evidence at different levels of a hier-archy of HOI concepts. For example, we have observedthat some evidence sources map the concept myocardialinfarction (concept identifier 4329847) directly to theSNOMED equivalent, whereas others map directly to amore specific concept like acute myocardial infarction(concept identifier 312327). The OMOP Vocabulariescan be used to address this issue using the hierarchyprovided. However, unlike drugs, it is not always clearthe appropriate level to rollup HOI concepts. Futurework will examine this issue in more detail and explorethe use of alternate definitions of concept similarity [44].LimitationsEvidence sources that do not use standardized termin-ologies have to be processed to map the source conceptnames to SNOMED-CT and RxNorm concept codes.Even for evidence sources that use controlled terminolo-gies, there is often a conversion process required to inte-grate them with all of the included LAERTES sources.One limitation is that, because of the prototype natureof the current version of LAERTES, we currently do nothave precise precision/recall metrics for each of themethods we used (Table 2).Another limitation is that some terminologies areincompatible and result in imperfect mappings. Incases where mapping is incomplete, some parts of thesource data will not be integrated. We mentionedabove the example of the MEDLINE source usingMeSH to code drugs at the ingredient level, whiledrugs in US product labels are coded at the clinicaldrug level. These cases can be addressed with thedrug rollup queries described above. Table 3 repre-sents overlap between the general categories ofsources at the drug ingredient level using the drugconcept rollup strategy. However, unlike drugs, theappropriate level to rollup HOI concepts is not al-ways clear. Some examples seem straightforward suchas the myocardial infarction example mentionedabove which can be addressed using ancestor/descen-dant relationships in the OMOP vocabulary. However,it is less clear how to apply the strategy to the uni-verse of HOI concepts because there exists manydifferent hierarchies depending on the disease and thelevel of detail present in SNOMED. We did notattempt to address this issue in the LAERTESprototype.The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 12 of 15ConclusionPost-marketing drug safety surveillance is an import-ant, continuous, and demanding process. Accurateand timely identification and verification of safety sig-nals remains a major challenge. DrugHOI evidenceexists in many sources, which are disjointed, and vari-able in their representation of drugs and HOIs. Thecurrent practice of reviewing drug-HOI evidence is ahighly manual time intensive process that is wroughtwith opportunity for failure. Improvement of theautomation of bringing this information together in aconsumable format will greatly improve the pharma-covigilance field. LAERTES provides a framework thataccepts data from multiple sources and leverages theOMOP Vocabulary to translate those sources to oneterminology for drugs and one for conditions, whilealso enabling integration with clinical data stored inthe OMOP CDM. In addition, LAERTES is open-source facilitating domain experts participation in itsdevelopment. As the breadth of evidence available ondrug-HOI associationsis too wide for any individualto be expert, an open-source model allows for nichedomain experts to contribute their knowledge improv-ing the usefulness of LAERTES for the entire drugsafety community.This paper started with a motivating safety physicianand risk management analyst user story to help guideLAERTES use cases. LAERTES has already collated sev-eral of the evidence sources individuals in this rolewould traditionally use for investigating drug-HOI sig-nals. The LAERTES workgroup believes that the frame-work is in place to address the motivating example butrealizes there is more work to be done. The workgroupfully expects and welcomes feedback from the commu-nity; for example, on new data sources, improvements tothe Web API, and user interface design.AbbreviationsADE: Adverse drug event; AE: Adverse event; API: Application programminginterface; CDM: Common data model; CTD: Comparative ToxicogenomicsDatabase; DB: Database; DBMS: Database management system;EBGM: Empirical Bayes Geometric Mean; EHR: Electronic health record;ETL: Extract, translate, and load; EU: European Union; FAERS: Food and DrugAdministration Adverse Event Reporting System; FDB: First data bank (R);HOIs: Health outcomes of interest; IC: Information component;LAERTES: Large-scale adverse effects related to treatment evidencestandardization; MDRR: Minimal detectable reporting ratio; MedDRA: Medicaldictionary for regulatory activities; MeSH: Medical subject headings;NLP: Natural language processing; OA: Open annotation data model;OHDSI: Observational Health Data Sciences and Informatics;OMOP: Observational Medical Outcomes Partnership; PPV: Positive predictivevalue; RDF: Resource Description Framework; REST: Representational statetransfer; SNOMED-CT: Systematized Nomenclature of Medicine - ClinicalTerms; URIs: Uniform Resource Identifiers; US: United States; W3C: WorldWide Web ConsortiumAcknowledgementsThe following collaborating authors from the Knowledge Base workgroup ofthe Observational Health Data Sciences and Informatics (OHDSI) collaborativeparticipated in creating LAERTES and authoring this manuscript:Richard D. Boyce, University of Pittsburgh, Pittsburgh, PA, rdb20@pitt.eduErica A. Voss, Janssen Research & Development, LLC, Titusville, NJ,EVoss3@its.jnj.comVojtech Huser, National Institutes of Health, Bethesda, MD,vojtech.huser@nih.govLee Evans, LTS Computing LLC, West Chester, PA,levans@ltscomputingllc.comChristian Reich, QuintilesIMS, Burlington MA, reich@ohdsi.orgJon D. Duke, Georgia Tech Research Institute, Atlanta, GA,Jon.Duke@gatech.eduNicholas P. Tatonetti, Herbert Irving Assistant Professor of BiomedicalInformatics, Columbia University, New York, NY, nick.tatonetti@columbia.eduTal Lorberbaum, Columbia University, New York, NY,tal.lorberbaum@columbia.eduMichel Dumontier, Stanford University, Stanford, CA,michel.dumontier@gmail.comManfred Hauben, MD. MPH, Pfizer Inc, New York University Medical Center,New York, NY, manfred.hauben@pfizer.comMagnus Wallberg, Uppsala Monitoring Centre, Uppsala, Sweden,Magnus.Wallberg@who-umc.orgLili Peng, AstraZeneca R&D, Boston, MA, Lili.Peng@astrazeneca.comSara Dempster, AstraZeneca R&D, Boston, MA,sara.dempster@astrazeneca.comYongqun He, University of Michigan Medical School, Ann Arbor, Michigan,yongqunh@med.umich.eduAnthony G. Sena, Janssen Research & Development, LLC, Titusville, NJ,asena5@its.jnj.comVassilis Koutkias, Institute of Applied Biosciences, Center for Research &Technology Hellas, Thermi, Thessaloniki, Greece, vkoutkias@certh.grPantelis Natsiavas, Institute of Applied Biosciences, Center for Research &Technology Hellas, Thermi, Thessaloniki, Greece, pnatsiavas@certh.grPatrick B. Ryan, Janssen Research & Development, LLC, Titusville, NJ,ryan@ohdsi.orgFundingThis research was funded in part by the US National Institute on Aging(K01AG044433), and the National Library of Medicine (R01LM011838).Availability of data and materialsThe datasets used and/or analyzed during the current study available fromthe corresponding author on reasonable request. Links are provided withinthis published article to web services and an experimental user interfaceproviding data from the LAERTES evidence base.Authors contributionsThis was a collaborative project. Please see the list of collaborating authors inthe Acknowledgements sections. RDB was the lead developer of LAERTESand the main author on the manuscript. Other major contributors to writingthe manuscript and the design and implementation of LAERTES were EAV,VH, LE, CR, JDD, NT, MD, MW, AGS, and PR. MH made significantcontributions to clarifying medication safety investigation use cases forLAERTES. LP and SD contributed to data quality assurance and drugdevelopment use cases. YH provided input on the representation of healthoutcomes of interest. All authors read and approved the final manuscript.Authors informationNot applicable.Competing interestsThe authors have no competing interests to report. The opinions expressedare those of the authors and do not necessarily represent those of theiremployers.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Received: 23 November 2016 Accepted: 13 January 2017The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborativeJournal of Biomedical Semantics  (2017) 8:11 Page 13 of 15Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 DOI 10.1186/s13326-017-0155-8RESEARCH Open AccessQuerying clinical data in HL7 RIM basedrelational model with morph-RDBFreddy Priyatna1* , Raul Alonso-Calvo2, Sergio Paraiso-Medina2 and Oscar Corcho1AbstractBackground: Semantic interoperability is essential when carrying out post-genomic clinical trials where severalinstitutions collaborate, since researchers and developers need to have an integrated view and access toheterogeneous data sources. One possible approach to accommodate this need is to use RDB2RDF systems thatprovide RDF datasets as the unified view. These RDF datasets may be materialized and stored in a triple store, ortransformed into RDF in real time, as virtual RDF data sources. Our previous efforts involved materialized RDF datasets,hence losing data freshness.Results: In this paper we present a solution that uses an ontology based on the HL7 v3 Reference Information Modeland a set of R2RML mappings that relate this ontology to an underlying relational database implementation, andwhere morph-RDB is used to expose a virtual, non-materialized SPARQL endpoint over the data.Conclusions: By applying a set of optimization techniques on the SPARQL-to-SQL query translation algorithm, wecan now issue SPARQL queries to the underlying relational data with generally acceptable performance.Keywords: Clinical data, R2RML, SPARQLIntroductionIn the last years, clinical trials have started introducinggenomic variables [1]. This requires performing patientstratification when selecting the patient population toapply the clinical trials to. It involves the use of biomarkersto create subsets within a patient population that pro-vide more detailed information about how the patient willrespond to a given drug. Several datasets, commonly pro-duced by different institutions and hence rather heteroge-neous in general, need to be used for patient stratification[2]. Interoperability among those datasets is made easierby the use of biomedical standards and terminologies [3].However, achieving such interoperability poses relevanttechnological challenges [4]. In this work, we focus on asemantic interoperability approach to homogenize differ-ent data models into one Common Data Model (CDM).For this task several projects such as HL7 Reference Infor-mation Model (RIM) [5], i2b2 [6], OMOP [7] or CaGRID*Correspondence: fpriyatna@fi.upm.es1Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid,SpainFull list of author information is available at the end of the article[8] have defined their own CDM capable of storing het-erogeneous data coming from different sources. The basisof the work presented in this paper is founded on thesemantic interoperability layer developed in the EURECAproject [9], which has been deployed and tested in severalhealthcare institutions, such as the Institut Jules Bordet[10], the MAASTRO Clinic [11], and the German BreastGroup [12].In previous works [13] we already presented a HL7RIM [5] relational database implementation used as aCDM in the EURECA semantic interoperability layer. Thisdatabase aims to facilitate the interconnection with otherdata sources wheremedical ontologies are also being used,and has already been used for providing some form ofinteroperability among real data sources [13] from theaforementioned institutions. We are currently developingontology-based support to data access to facilitate suchintegration and allow incorporating other datasets moreeasily. This is the reason why we were looking into using aRelational Database to RDF (RDB2RDF) solution. We alsoprovide a SPARQL endpoint to a virtual view so that usersare relieved from knowing the underlying schema of theimplemented database.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 2 of 12RDB2RDF mappings are used to expose data from rela-tional databases as RDF datasets. Two major types of dataaccess mechanisms are normally provided by RDB2RDFtools: i) data translation (a specific case of ETL - Extract,Transform, Load -), where data are materialized into RDFdatasets and stored in a triple store (e.g., Virtuoso), whichprovides a SPARQL endpoint; and ii) query translation,where SPARQL queries are directly translated into SQLaccording to the specified RDB2RDF mappings, and eval-uated against the relational database, and where resultsare translated back using the mappings to conform withthe SPARQL query. In our case, we are interested in usingRDB2RDF mappings to make the data stored in our SQLimplementation available according to an ontology thatreflects the HL7 version 3 RIM. Furthermore, we have astrong requirement to use a query translation approach,given the importance of having fresh results, which cannotalways be ensured in the data translation approach.Our first attempt [14] at applying RDB2RDF-basedquery translation was with D2R server and mappings [15].This approach was not applicable since the evaluation ofthe SQL queries resulting from query translation was notefficient enough. Moreover, in some cases, queries couldnot be executed by the database management system (e.g.,their length was excessive). This was already mentionedin [16] which describes the experience of using RDB2RDFtools in the domain of astronomy. The conclusion therewas that RDB2RDF tools were not feasible to be used insuch a context, and this conclusion was consistent withour first attempt.Later, we started using morph-RDB [17] with R2RMLmappings [18] for this purpose. We have obtained betterresults that make this approach applicable in our context.In this paper we describe our experience, which showsthat it is possible to use efficient RDB2RDF tools in themedical domain.This paper is organized as follows. In theBackground section we discuss our current modelfor storing medical data, the HL7 RIM ontology, theR2RML mapping language, and our query translationengine morph-RDB. In the Methods section we dis-cuss our methodology for mapping legacy data intothe HL7 RIM ontology, selection of SPARQL queriesfor that ontology, and some optimization techniquesthat have been implemented in morph-RDB. In theResults and discussion section, we present our evalu-ation. Finally in the Conclusions section, we providesome conclusions and describe some of our future workin this area, including our deployment plans in theaforementioned healthcare institutions.BackgroundIn this section we will review the main foundationsof the work that we present in the paper, namelyHL7 and the HL7 RIM, the R2RML language, andmorph-RDB.HL7 RIMRecent years have witnessed a huge increase of biomedicaldatabases [19]. This increased availability opens up newopportunities, while setting some new important chal-lenges, especially with respects to their integration, whichis crucial to obtain a proportional increment of knowledgein the biomedical area. In this context, it is common toestablish a CDM for the representation of biomedical datawhich allow exploiting multiple established terminologiesto build a core concept dataset as the common medicalvocabulary of the platformAmong the many Detailed Clinical Models that havebeen reviewed for the integration of biomedical datasets[20], the HL7 v3 is one of the most relevant, since mainrequirement for the CDM is that any data coming fromclinical institutions can be represented without loss ofinformation. The HL7 RIM offers a wide coverage forrepresenting clinical data and has proven useful for clin-ical information exchange. The HL7 v3 standard definesthe RIM at its core. This definition consists of a UMLclass diagram (it does not define a data structure or adatabase model). Besides, issues such as the managementof data types are not trivially translatable into a databasemodel. As a consequence, we previously defined a rela-tional model for it, which can be seen in Fig. 1 anddescribed in [13].The HL7 RIM backbone contains three main classes:Act, Role and Entity, which are linked togetherby three association classes (Act-Relationship,Participation and RoleLink). The core of the HL7RIM is the Act class. An Act is defined as a record ofan event that has happened or may happen. Any health-care situation and all information concerning it shouldbe describable using the RIM by including the type ofact (what happens), the actor who performs the deedand the objects or subjects Entity that the act affectsto Role. Some additional information may be providedto indicate location (where), time (when), manner (how),together with reasons (why) or motives (what for). Actand Entity classes have some specializations that addsome attributes, such as Observation (a subclass ofAct), or Person (a subclass of Entity).This standard is able to represent almost any healthcaresituations and a wide variety of information associatedwith it [21]. Based on this idea, we have defined a subsetof the HL7 RIM schema where we implement the classesand attributes that are necessary to represent the scenariofor sharing clinical breast cancer clinical trials data: Act, with the subclasses Observation,Procedure, SubstanceAdministration, andExposure.Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 3 of 12Fig. 1 Relational model of H7RIM. Our database schema implementing the HL7RIM model [13] Role. Entity, with the sub-classes LivingSubject,Person, and Device. The classes;i) ActProcedureApproachSiteCode, ii)ActMethodCode,iii) ActTargetSiteCode, iv)ActObservationInterpretationCode, andv) ActObservationValues related to Act.Attribute data types are rather complex on theRIM, so they are changed according to the men-tioned scenario, following HL7 datatype specifications[22]. Therefore some attributes were simplified in therelational model compared to those defined by HL7v3 standard. To improve performance and understand-ing of the HL7 RIM schema, it is defined a set ofviews. These views cover the access retrieval require-ments for the clinical scenario. We defined a viewfor each clinical contexts (Observation, Procedure,SubstanceAdministration, and Exposure).Therefore, the defined HL7 RIM-based CDM above ful-fills the requirements needed for breast cancer clinicaltrials scenario. Furthermore, we have created an ontologythat reflects the HL7RIM model [23], which is availablefor others to reuse.Figure 2 depicts a simplified schema of the implementeddatabase following the HL7 v3 RIM definition. However,typically relationships among Entity and Role instancesare one-to-one. Moreover, the Act table is the backbonebut data is classified as one of its descendants (Obser-vation, Procedure, Substance Administration, Exposure,etc.). Thus the logical schema for querying an Act descen-dant (i.e. Observation) from our database looks like theschema represented in Fig. 3.Therefore, every Act subclass in the HL7 v3 RIM dataschema can be represented as a star diagram  typi-cally used in data warehouse definition. Our database canbe visualized as a snowflake diagram similar to the i2b2star model [6]. Each event record will be a subclass ofAct (similarly to the i2b2 fact table). Entities and Roles(patient, location, care provider, etc.) are lookup tablescalled Dimensions.Conversely to other works in literature that usequery translation [8], since Act tables contain thebiggest amount of data in the model, we have adoptedthe approach of dividing complex queries into atomicqueries. Consequently, in order to efficiently executequeries involving several instances of acts and rela-tionships (e.g. temporal dependencies), these queriesare divided and results are later combined using setoperators [13].Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 4 of 12Fig. 2 Simplified HL7RIM model. Our simplified logical database schema implementing the HL7RIM modelR2RMLR2RML [18] is a W3C recommendation for the defini-tion of a mapping language from relational databases toRDF. An R2RML mapping document consists of a setof Triples Maps rr:TriplesMap, used to specify therules to generate RDF triples from database rows/values.A TriplesMap consists of: A logical table rr:LogicalTable that is either abase table or SQL view, used to provide the rows tobe mapped as RDF triples. A subject map rr:SubjectMap that is used tospecify the rules to generate the subject componentof RDF triples. A set of predicate object mapsrr:PredicateObjectMap that is composed by aset of predicate maps rr:PredicateMap andobject maps rr:ObjectMap (to generate thepredicate and object components of RDF triples,respectively). If a join with another triples map isneeded, a reference object map rr:RefObjectMapcan be used. The other triples map to be joined isspecified in rr:parentTriplesMap and the joincondition is specified via rr:JoinFigure 4 illustrates an overview of an R2RMLTriplesMap class.Subject maps, predicate maps, and object maps are termmaps, which are used to specify rules to generate the cor-responding RDF triples element, and those rules can bespecified as a constant rr:constant, a database col-umn rr:column, or a template rr:template. Figure 5illustrates an overview of an R2RML TermMap class.morph-RDBmorph-RDB is part of the morph suite [24]. It receives asan input the connection details to a relational database,an R2RML mapping document and a SPARQL query. Ittranslates the SPARQL query into the underlying rela-tional database and translates the results back into aformat appropriate for the SPARQL query. The querytranslator component in morph-RDB implements thealgorithm described in [17], which extends previous workin [25] that defined a set of mappings and functionsin order to translate SPARQL queries posed againstRDB-backed triples stores into SQL queries, prove thecorrectness of the query translation using the notionsemantic-preserving. In other words, the SPARQL queryrealized as an SQL query returns the same answersas the same SPARQL query executed over an R2RMLmaterialization. We extend their work by relating thosemappings and functions with the R2RML mappingelements.Fig. 3 Logical view of HL7RIM model. Logical view of observation data in the HL7RIM modelPriyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 5 of 12Fig. 4 R2RML TriplesMap overview. An overview of R2RML TriplesMap, taken from [18]For an in-depth explanation of the query rewritingREVIEW Open AccessSemantic annotation in biomedicine: thecurrent landscapeJelena Jovanovi?1 and Ebrahim Bagheri2*AbstractThe abundance and unstructured nature of biomedical texts, be it clinical or research content, impose significantchallenges for the effective and efficient use of information and knowledge stored in such texts. Annotation ofbiomedical documents with machine intelligible semantics facilitates advanced, semantics-based text management,curation, indexing, and search. This paper focuses on annotation of biomedical entity mentions with concepts fromrelevant biomedical knowledge bases such as UMLS. As a result, the meaning of those mentions is unambiguously andexplicitly defined, and thus made readily available for automated processing. This process is widely known as semanticannotation, and the tools that perform it are known as semantic annotators.Over the last dozen years, the biomedical research community has invested significant efforts in the development ofbiomedical semantic annotation technology. Aiming to establish grounds for further developments in this area,we review a selected set of state of the art biomedical semantic annotators, focusing particularly on general purposeannotators, that is, semantic annotation tools that can be customized to work with texts from any area of biomedicine.We also examine potential directions for further improvements of todays annotators which could make them evenmore capable of meeting the needs of real-world applications. To motivate and encourage further developments inthis area, along the suggested and/or related directions, we review existing and potential practical applications andbenefits of semantic annotators.Keywords: Natural language processing (NLP), Biomedical ontologies, Semantic technologies, Biomedical text mining,Semantic annotationBackgroundOver the last few decades, huge volume of digital un-structured textual content have been generated in bio-medical research and practice, including a range ofcontent types such as scientific papers, medical reports,and physician notes. This has resulted in massive andcontinuously growing collections of textual content thatneed to be organized, curated and managed in order tobe effectively used for both clinical and research pur-poses. Clearly, manual curation and management ofsuch big corpora are infeasible, and hence, the biome-dical community has long been examining and makinguse of various kinds of Natural Language Processing(NLP) methods and techniques to, at least partially, fa-cilitate their use.In this paper, we focus on a specific NLP task, namelythe extraction and disambiguation of entities mentionedin biomedical textual content. Early efforts in biomedicalinformation extraction were devoted to Named EntityRecognition (NER), the task of recognizing specific typesof biomedical entities mentioned in text [1]. For in-stance, in the sentence The patient was diagnosed withupper respiratory tract infection, a NER tool wouldrecognize that the phrase respiratory tract infectiondenotes a disease, but would not be able to determinewhat particular disease it is. Semantic annotation, theNLP task of interest to this paper, makes a significantadvance, by not only recognizing the type of an entity,but also uniquely linking it to its appropriate cor-responding entry in a well-established knowledge base.In the given example, a semantic annotator would notonly recognize that the phrase respiratory tract infec-tion represents a disease, but would also identify whatdisease it is by connecting the phrase with the concept* Correspondence: bagheri@ryerson.ca2Department of Electrical Engineering, Ryerson University, 245 Church Street,Toronto, CanadaFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jovanovi? and Bagheri Journal of Biomedical Semantics  (2017) 8:44 DOI 10.1186/s13326-017-0153-xC0035243 denoting Respiratory Tract Infections fromthe UMLS Metathesaurus (see Table 1). This way, the se-mantics of biomedical texts is made accessible to soft-ware programs so that they can facilitate variouslaborious and time consuming tasks such as search, clas-sification, or organization of biomedical content.While a suite of biomedical semantic annotation toolsis available for practical use, the biomedical communityis yet to heavily engage in and leverage the benefits ofsuch tools. The goal of this paper is to introduce (i)some of the benefits and application use cases of bio-medical semantic annotation technology, (ii) a selectionTable 1 An overview of ontologies, thesauri and knowledge bases used by biomedical semantic annotation tools discussed in the paperBioPortal (http://bioportal.bioontology.org/) A major repository of biomedical ontologies, currently hosting over 500ontologies, controlled vocabularies and terminologies. Its Resource Indexprovides an ontology-based unified index of and access to multipleheterogeneous biomedical resources (annotated with BioPortal ontologies).DBpedia (http://wiki.dbpedia.org/) Wikipedia for machines, that is, a huge KB developed through a communityeffort of extracting information from Wikipedia and representing it in astructured format suitable for automated machine processing. It is the centralhub of the Linked Open Data Cloud.LLD - Linked Life Data (https://datahub.io/dataset/linked-life-data/) LLD platform provides access to a huge KB that includes and semanticallyinterlinks knowledge about genes, proteins, molecular interactions, pathways,drugs, diseases, clinical trials and other related types of biomedical entities. Itis part of the Linked Open Data Cloud (http://lod-cloud.net/)NCBI Biosystems Database (https://www.ncbi.nlm.nih.gov/biosystems) Repository providing integrated access to structured data and knowledgeabout biological systems and their components: genes, proteins, and smallmolecules.The NCBI Taxonomy contains the names and phylogenetic lineages of all theorganisms that have molecular data in the NCBI databases.OBO - Open Biomedical Ontologies (http://www.obofoundry.org/) Community of ontology developers devoted to the development of a familyof interoperable and scientifically accurate biomedical ontologies. Well knownOBO ontologies include: Chemical Entities of Biological Interest (ChEBI) - focused on molecular entities,molecular parts, atoms, subatomic particles, and biochemical roles andapplications Gene Ontology (GO) - aims to standardize the representation of gene andgene product attributes; consists of 3 distinct sub-ontologies: MolecularFunction, Biological Process, and Cellular Component Protein Ontology (PRO) - provides a structural representation of protein-related entitiesSNOMED CT (http://www.ihtsdo.org/snomed-ct) SNOMED CT is considered the worlds most comprehensive and precise,multilingual health terminology. It is used for the electronic exchange ofclinical health information. It consists of concepts, concept descriptions(i.e., several terms that are used to refer to the concept), and conceptrelationships.UMLS (Unified Medical Language System) Metathesaurus (https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/)The most well-known and widely used knowledge source in the biomedicaldomain. It assigns a unique identifier (CUI) to each medical concept andconnects concepts to each other thus forming a graph-like structure; eachconcept (i.e. CUI) is associated with its semantic type, a broad category suchas Gene, Disease or Syndrome; each concept is also associated with severalterms used to refer to that concept in biomedical texts; these terms are pulledfrom nearly 200 biomedical vocabularies. Some well-known vocabularies thathave been used by biomedical semantic annotators include: Human Phenotype Ontology (HPO) contains terms that describe phenotypicabnormalities encountered in human disease, and is used for large-scalecomputational analysis of the human phenome. Logical Observation Identifiers Names and Codes (LOINC) provides standardizedvocabulary for laboratory and other clinical observations, and is used forexchange and/or integration of clinical results from several disparate sources. Medical Subject Headings (MeSH) is a controlled vocabulary thesaurus createdand maintained by U.S. National Library of Medicine (NLM), and has beenprimarily used for indexing articles in PubMed RxNorm provides normalized names for clinical drugs and links between manyof the drug vocabularies commonly used in pharmacy management and druginteraction software.UniProtKb/Swiss-Prot (http://www.uniprot.org/uniprot/) Part of UniProtKB, a comprehensive protein sequence KB, which containsmanually annotated entries. The entries are curated by biologists, regularlyupdated and cross-linked to numerous external databases, with the ultimateobjective of providing all known relevant information about a particular protein.Jovanovi? and Bagheri Journal of Biomedical Semantics  (2017) 8:44 Page 2 of 18of the publicly available general purpose semantic an-notation tools for the biomedical domain, i.e., semanticannotators that are not specialized for a particular bio-medical entity type, but can detect and normalize en-tities of multiple types in one pass, and (iii) potentialareas where the work in the biomedical semantic anno-tation domain can be strengthened or expanded. Whilethe overview of application cases and state of the arttools can be of relevance to practitioners in the biome-dical domain, with the summary of potential areas forfurther research, we are also targeting researchers whoare familiar with NLP, semantic technologies, and se-mantic annotation in general, but have not been dealingwith the biomedical domain, as well as those who arewell aware of biomedical semantic technologies, buthave not been working on semantic annotation. By pro-viding researchers with an insight into the current stateof the art in biomedical semantic annotation in terms ofthe approaches and tools, as well as the research chal-lenges, we aim to offer them a basis for engagement withsemantic annotation technology within the biomedicaldomain and thus support even further developments inthe field.The following section provides several examples ofpractical benefits achievable through semantic anno-tation of biomedical texts (see also Table 2). The paperthen examines the available tool support, focusingprimarily on general purpose biomedical annotators(Tables 3 and 4). Still, considering the relevance andlarge presence of entity-specific biomedical annotators,i.e., tools developed specifically for semantic annotationof a particular type of biomedical entities such as genesor chemicals, we provide an overview of these tools, aswell. While examining the available tool support, we alsoconsider biomedical knowledge resources required forsemantic annotation (Table 1), as well as resources usedfor evaluating the tools performance (Table 5). This isfollowed by a discussion of the challenges that are pre-venting current semantic annotators from achievingtheir full potential.Benefits and use casesBetter use of electronic medical record (EMR) in clinicalpracticeElectronic medical records (EMRs) are considered valu-able source of clinical information, ensuring effectiveand reliable information exchange among physicians anddepartments participating in patient care, and support-ing clinical decision making. However, EMRs largelyconsist of unstructured, free-form textual content thatrequire manual curation and analysis performed by do-main experts. A recent study examining the allocation ofphysician time in ambulatory practice [2] confirmed thefindings of previous similar studies (e.g. [3]), namely thatphysicians spend almost twice as much time on themanagement of EMRs and related desk work than ondirect clinical face time with patients. Considering theinefficiency of manual curation of EMRs, automation ofthe process is required if the potentials of EMRs are tobe exploited in clinical practice [4].Semantic annotators provide the grounds for the re-quired automation by extracting clinical terms fromfree-form text of EMRs, and disambiguating the ex-tracted terms with concepts of a structured vocabulary,such as UMLS Metathesaurus. The identified conceptscan be subsequently used to search a repository of bio-medical literature or evidence-based clinical resources,Table 2 Example application cases of biomedical semantic annotation toolsApplication Case (AC) The role of semantic annotation tool in the AC Biomedical resources relevant for the AC(or representative examples, if multiple)Semantic search of biomedicaltools and services [6]Sematic search of biomedical tools and services enabledby semantic annotation of users (free-form) queries withconcepts from UMLS MetathesaurusCatalogs of and social spaces created aroundbiomedical tools and services, e.g.:- myExperiment (http://www.myexperiment.org/)- BioCatalogue (https://www.biocatalogue.org/)Semantic search of domainspecific scientific literature [74]Semantic annotation of PubMed entries with ontologicalconcepts related to genes and proteinsOntologies used for the annotation of biomedicalGonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 DOI 10.1186/s13326-017-0133-1SOFTWARE Open AccessAn ontology-driven tool for structureddata acquisition using Web formsRafael S. Gonçalves* , Samson W. Tu, Csongor I. Nyulas, Michael J. Tierney and Mark A. MusenAbstractBackground: Structured data acquisition is a common task that is widely performed in biomedicine. However,current solutions for this task are far from providing a means to structure data in such a way that it can beautomatically employed in decision making (e.g., in our example application domain of clinical functional assessment,for determining eligibility for disability benefits) based on conclusions derived from acquired data (e.g., assessment ofimpaired motor function). To use data in these settings, we need it structured in a way that can be exploited byautomated reasoning systems, for instance, in the Web Ontology Language (OWL); the de facto ontology language forthe Web.Results: We tackle the problem of generating Web-based assessment forms from OWL ontologies, and aggregatinginput gathered through these forms as an ontology of semantically-enriched form data that can be queried using anRDF query language, such as SPARQL. We developed an ontology-based structured data acquisition system, which wepresent through its specific application to the clinical functional assessment domain. We found that data gatheredthrough our system is highly amenable to automatic analysis using queries.Conclusions: We demonstrated how ontologies can be used to help structuring Web-based forms and tosemantically enrich the data elements of the acquired structured data. The ontologies associated with the enricheddata elements enable automated inferences and provide a rich vocabulary for performing queries.Keywords: OWL, Ontology, Structured data, Data acquisition, Form generationBackgroundOntology-based form generation and structured dataacquisition was first pioneered almost 30 years ago. Inthe early 1990s, Protégé-Frames used definitions of classesin an ontology to generate knowledge-acquisition forms,which could be used to acquire instances of ontologyclasses [1, 2]. The rise of the Web Ontology Language(OWL) [3, 4], standardized by the World Wide Web Con-sortium (W3C) in 2004, caused a paradigm shift in knowl-edge representation from frame-based to axiom-based.Because of its axiom-based nature, it is more difficult toacquire instance data based on OWL than it was based onframes.WithOWL as the preferredmodeling language forontologies, class definitions are collections of descriptionlogic (DL) axioms, and can no longer be seen as templates*Correspondence: rafael.goncalves@stanford.eduStanford Center for Biomedical Informatics Research, Stanford University,Stanford, CA, USAfor forms [5]. Unlike template-based knowledge represen-tations, where what can be said about a class is definedby the slots of the class template, axiom-based representa-tions do not have this kind of locally scoped specification,and allow any axiom describing the same class to be addedto the ontology, as long as the axiom does not lead toinconsistencies. Template-based knowledge representa-tion systems use closed-world reasoning and have localconstraints (e.g., cardinality of a slot for a particular class)that can be validated easily, while in an axiom-based sys-temwith the open-world assumption such local constraintchecking is much more problematic. Furthermore, in ourchosen application domain, assessment instruments havespecific formats that do not lend themselves to be seen asrepresenting instances of domain ontology classes. Itemsin the instruments have potentially complex descriptionsof information to be collected, such as the severity ofpain with a particular quality, and at a specific anatomical© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Gonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 2 of 14location. The challenge is to model the assessment instru-ments and relate the assessed data to a domain ontologywith which one can formulate meaningful queries.In this paper, we describe a system that we developedfor representing, acquiring, and querying assessment datathat uses: (1) an information model of assessment instru-ments to drive the generation of data-acquisition Webforms, (2) domain ontologies and standard terminolo-gies to give formal descriptions of entities in our chosendomain, and (3) a data model for the acquired informa-tion that links the data to the domain ontologies andstandard terminologies. Such linkage makes it possible toquery and aggregate the data using the logical representa-tion of the domain concepts in the ontologies. The choiceof Web forms as a method for acquiring data is due totheir widespread use and simplicity for data acquisition.The form generation software we present here works withforms modeled in OWL so long as these replicate ourdesign pattern for the form specification ontology. Thepaper describes requirements on the underlying ontolo-gies and informationmodels, and the steps for configuringthe software to generate forms and to acquire data usingclinical functional assessment as an exemplar.Related workIn addition to the comparison with the Protégé-Framestemplate-based instance acquisition method (in theBackground Section), we briefly contrast our work withother systems that use ontologies in the construction offorms for acquiring structured data.Girardi et al. [6] describe an ontology-based data-acquisition and data-analysis system where the structureof the data depends on the ontology classes, in such away that the GUI structures (tables, headers, filter dialogs,etc.) can be created at runtime based on the ontologyinformation. This system, like the earlier Protégé-Framessystem, assumes that classes in the ontology provide data-acquisition templates that directly define user-interfacefeatures. Our system is designed to work with OWLontologies where ontology axioms do not provide thestructural templates required in the system described byGirardi et al. Instead, the structure of the data-acquisitioninstrument has to be defined separately.ObTiMA, described by Stenzhorn et al. [7], is anotherontology-based data-acquisition system. It is a clinicaltrial-management application featuring a Trial Buildermodule that a clinical researcher can use to build casereport forms (CRFs). Items in a CRF are constructedby selecting concepts from a master ontology. A PatientData Management System provides a graphical user inter-face that allows clinicians to fill in the CRFs relevant tothe patients current treatment situation. The design ofObTiMA is very similar to the system we are proposing.The main differences, aside from ObTiMAs specific focuson clinical trial management, include (1) our use of OWLtomodel not only the domain concepts, but also the struc-tures of forms and data model, and (2) ObTiMAs use ofa tree view to represent concepts that can be selected todefine data items. It is understandable that, from the per-spective of supporting a clinical researchers use of themaster ontology to construct CRFs, a tree view provides anecessary simplification of the master ontology, althoughit nevertheless constrains what can be expressed. It isdifficult to see how some of the complex concepts (e.g.,constant pain caused by radiculopathy in the lower leftextremity) modeled in our work can be represented aspart of a tree structure.The clinical documentation system developed by Hor-ridge et al. [8] uses a template schema to allow atechnology-savvy clinician to create documentation tem-plates that include the local structure of subforms, andpotentially complex clinical descriptions consisting of fea-tures and their values. The features and values are mappedto a medical ontology, and the system automatically gen-erates ontological descriptions of the data elements basedon the mappings. Constrained by our goal to replicateexisting forms, we took the opposite approach where westart with ontological descriptions of the data elements,specify how they are used in assessment instruments aspart of the description of instruments, and generate formsfor the acquisition of data. Having the freedom to designtheir documentation system, Horridge et al. avoidedthe laborious work of manually modeling the domainconcepts.Bona et al. developed a work that is similar to ours[9]. They modeled the specifications of forms, questiongroups, questions, and answers as extensions of the Infor-mation Artifact Ontology (IAO),1 and the answers as theresult of the patient-history taking process. In their work,the questions are just strings that have associated accept-able answers, whereas we attempt to formalize much ofthe information content in our assessment instruments interms of a domain ontology. Furthermore, it is not clearthat the system automatically generates data-acquisitionforms from the ontology-based form specifications.Outside the domain of biomedicine, semantic wiki isa generic Web-based technology from which one candraw examples on how to arrive at a domain-independentsolution. Semantic wikis extend regular wikis with seman-tic technologies, wherein each wiki article is an RDFresource, and an instance of some resource such as a classdefined in the schema,2 which can be asserted to haverelations with other RDF resources. These relations aredefined by the authors of wiki articles, which could be achallenging task to perform without previous knowledgeof the domain or the modeling. In a survey of semanticwikis featuring OWL reasoning and SPARQL3 queryingfacilities [10], a user evaluation of a chosen semanticGonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 3 of 14wiki implementationIkeWiki [11]concluded thatauthoring instance data in such a way is cumbersome,even with users that are familiar with ontologies. A goodsolution to this would be exploiting the relations definedin the schema to provide wiki article templates whoseform input fields derive from those relations, thus makingit easier to create semantic wiki articles  essentiallythe user would only have to fill in the values of thoserelations, without having to understand the underlyingrepresentation.Another system that is very close to what we presenthere is K-Forms [12]. This tool allows users to constructforms using a graphical user interface, and then the result-ing form structure is seamlessly encoded as an OWLontology. However, unlike our work, K-Forms does nothave a mechanism to associate form data (whether ques-tions or answers) to user-specified domain ontologies,meaning that the queryability will be constrained to thesemantics provided by the system, rather than the moreflexible approach that we aim for.ImplementationIn this sectionwe describe the software, informationmod-els, and ontologies that we developed for OWL-based dataacquisition.The architecture of the form generation and data acqui-sition system we implemented is depicted in Fig. 1. Thetool takes as inputs an XML configuration file that speci-fies the form layout, and a form-specification OWL ontol-ogy that defines the content of the form (i.e., the actualquestions, answer options, etc.). The tool then generatesa form, and outputs answers to form questions in CSV,RDF and OWL formats. We implemented our tool in Java,using the OWL API v4.0.1 [13],4 and its source code ispublicly available on GitHub.5 A Web server is necessaryto deploy the application, so the project ships with anembedded instance of Jetty.6 The requirements to run theapplication are Java (v1.7 or above) and Apache Ant.7To try out the software, we supply executable scriptsfor Windows and UNIX-based operating systems, whichbuild and deploy the tool on the included JettyWeb server.These scripts are hosted in our GitHub repository. First,a user would clone the repository to their computer, andthen from a command line execute the appropriate scriptfor the operating system; use run-generator.sh on UNIX-based systems and run-generator.bat onWindows. Alter-natively, one can build the form-generator using ApacheAnt, and then deploy it onto the provided instance of Jetty.The application will then be available to browse on thedesignated localhost port. In addition to the tool itself, weprovide in the same GitHub repository 3 example formconfigurations, the ontologies that we developed, some ofthe data that we gathered via our tool, example SPARQLqueries over that data, and finally the results of executingthose queries on our data. Users can open the output dataand query it with the example SPARQL queries we supply,using, for example, the Protégé ontology editor [2].The two major stages in the application workflow are:form generation and form input handling, as describedbelow.(1) Form generation  Steps to produce a form:(a) Process XML configuration file, gatheringform layout information, IRIs and bindingsto ontology entitiesFig. 1 Architecture of the system. The form-generation and data-acquisition software takes an XML configuration file and a form specification asinputs. A form specification uses terms from the datamodel ontology to create question instances and to specify possible answers. It annotatesquestions and answers with concepts from domain ontologiesGonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 4 of 14(b) Extract from the input form specificationontology all relevant informationpertaining to each form element:(b.1) Text to be displayed (e.g., sectionheader, question text)(b.2) Options and their correspondingtext, where applicable(b.3) The focus of each question(c) Generate the appropriate HTML andJavaScript code(2) Form input handling  Once the form is filled inand submitted:(a) Process answer data and create appropriateindividuals(b) Produce a partonomy of the individualscreated in (2.a) that mirrors the layoutstructure given in the configuration(c) Return the (structured) answers to the userin a chosen formatAn application can combine the data with the OWLontologies to make description logic queries that inter-pret the data in terms of the semantics defined in theontologies.In order to use our tool, a user will have to model ques-tions and their descriptions in OWL, and then specify thelayout and content of the resulting form in an XML file.In the following subsections, we will describe the OWLmodeling and configuration components in detail.ModelingOur goal was to develop a set of light-weight ontologiesand models with minimal ontological commitments, andpostponing alignment with possible upper-level ontolo-gies to the future. Existing ontologies, such as the Infor-mation Artifact Ontology, do not provide a modelingof forms and questions that we could reuse. Further-more, what we need is an information model that states,for example, that the structure of a question on a formincludes a specific text string, not an ontology that char-acterizes parts of information artifacts in terms of logicaldescriptions (e.g., modeling the text of a question as aninstance of textual entity" class).The modeling component of our software consists of(1) a datamodel that specifies the structure of data-acquisition forms and of the resultant data, (2) formspecifications that define specific data-acquisition formsin terms of the datamodel structures and concepts andrelations in the domain ontologies, and (3) one or moredomain ontologies that define the concepts and relationsin an application domain. The domain ontologies thatwe developed are hosted and maintained in our GitHubrepository.8DatamodelThe datamodel, represented as an OWL ontology, is ageneric, context-free description of the information struc-tures of a form. It models form elements such as sectionsand questions, and the data elements generated froma form (e.g., a string value from a text area, or val-ues from an enumerated value set). Figure 2 summarizeskey aspects of our modeling: elements of a form areasserted as subclasses of FormStructure, such as Form,Section and Question. Each kind of FormStructure gen-erates some kind of Data; every form submission gen-RESEARCH Open AccessOptimization on machine learning basedapproaches for sentiment analysis on HPVvaccines related tweetsJingcheng Du1, Jun Xu1, Hsingyi Song1, Xiangyu Liu2 and Cui Tao1*AbstractBackground: Analysing public opinions on HPV vaccines on social media using machine learning basedapproaches will help us understand the reasons behind the low vaccine coverage and come up withcorresponding strategies to improve vaccine uptake.Objective: To propose a machine learning system that is able to extract comprehensive public sentiment on HPVvaccines on Twitter with satisfying performance.Method: We collected and manually annotated 6,000 HPV vaccines related tweets as a gold standard. SVM modelwas chosen and a hierarchical classification method was proposed and evaluated. Additional feature sets evaluationand model parameters optimization was done to maximize the machine learning model performance.Results: A hierarchical classification scheme that contains 10 categories was built to access public opinions towardHPV vaccines comprehensively. A 6,000 annotated tweets gold corpus with Kappa annotation agreement at 0.851was created and made public available. The hierarchical classification model with optimized feature sets and modelparameters has increased the micro-averaging and macro-averaging F score from 0.6732 and 0.3967 to 0.7442 and0.5883 respectively, compared with baseline model.Conclusions: Our work provides a systematical way to improve the machine learning model performance on thehighly unbalanced HPV vaccines related tweets corpus. Our system can be further applied on a large tweets corpusto extract large-scale public opinion towards HPV vaccines.Keywords: Twitter, Social media, Sentiment analysis, Support vector machines, Hierarchical classification, GoldstandardBackgroundHuman papillomavirus (HPV) is thought to be respon-sible for more than 90% of anal and cervical cancers,70% of vaginal and vulvar cancers, and more than 60%of penile cancers [1]. FDA approved HPV vaccines (Gar-dasil, Cervarix and Gardasil 9) for the protection frommost of the cancers caused by HPV infections. However,the HPV vaccines coverage in USA is still quite low es-pecially for the adolescents. Only 39.7% of girls and21.6% of boys have received all three required doses [2].Analysis of public opinions over the HPV vaccines couldreveal the reasons behind the low coverage rate and canhelp us provide new directions on improving futureHPV vaccines uptake and adherence.As one of the most popular social media in the world,Twitter attracts millions of users to share opinions onvarious topics every day. On average, around 6,000tweets are tweeted every second and 500 million tweetsare tweeted per day [3]. Besides, Twitter allows a limit of140 characters on one post to its users. This restrictionpushes the users to be very concise to share their opin-ions [4]. The huge number of concise tweets makesTwitter a precious and rich data source to analyze publicopinions [5].Due to the adaptability and accuracy, machine learningbased approach is one of the most prominent techniques* Correspondence: cui.tao@uth.tmc.edu1The University of Texas School of Biomedical Informatics, 7000 Fannin StSuite 600, Houston, TX 77030, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Du et al. Journal of Biomedical Semantics  (2017) 8:9 DOI 10.1186/s13326-017-0120-6gaining interest in sentiment analysis (SA) on microblog-ging posts [4]. However, few efforts have been done onTwitter to explore public opinions towards vaccinesusing machine learning based SA tools. Surian et al. ap-plied unsupervised topic modeling to group semanticallysimilar topics and communities from HPV vaccines re-lated tweets [6]. However, those topics are not closely re-lated to sentiments towards vaccination. Salathé et al.leveraged several supervised algorithms to mine publicsentiments toward the new vaccines [7]. Zhou and Dunnet al. utilized connection information on social networkto improve opinion mining on identifying negative senti-ment about HPV vaccines [8, 9]. However, those workonly covered limited coarse sentiment classifications(positive, negative, neutral, etc.). In the HPV vaccinationdomain, sentiment analysis at a more granular level isnecessary in addition to the current limited classifica-tions. To serve as a feedback to public health profes-sionals to examine and adjust their HPV vaccinespromotion strategies, the system not only needs to knowwhether people have negative opinions towards HPVvaccines but also should be able to extract the reasonsbehind the negative opinions.Thus, to access public opinions towards HPV vaccineson Twitter in a more comprehensive way, a finer classifi-cation scheme to HPV vaccination sentiment is needed.In this paper, we introduced our efforts on using ma-chine learning algorithms to access HPV vaccinationsentiment at a more granular level on Twitter. We builta hierarchical classification scheme including 10 categor-ies. To train the machine learning model, we manuallyannotated 6,000 tweets as the gold standard accordingto the classification scheme. We chose Support VectorMachines (SVM) as the algorithm due to the perform-ance in our pre-experiments. Due to the challenges ofmachine learning approaches on the highly unbalancedtweets corpus, we further did a series of optimizationsteps to maximize the system performance. Standardmetrics including precision, recall, and F measure werecalculated to evaluate our results.MethodsData source and annotationData collectionEnglish tweets containing HPV vaccines related key-words were collected from July 15, 2015 to August 17,2015. We used combinations of keywords (HPV, humanpapillomavirus, Gardasil, and Cervarix) to collect publictweets using the official Twitter application program-ming interface (API) [10]. During the study period, wehave collected 33,228 tweets in total. After removing theURLs and duplicate tweets, we randomly selected 6,000tweets for annotation.Annotation schema designAs were more interested in the concerns over HPV vac-cination, we did a literature review to find out the com-mon non-vaccination reasons of HPV vaccines [1114].The most common barriers found for vaccination arethe worries about side effects, efficacy, cost, and culture-related issues. We also went through a sample of tweetsand kept track of the major concerns on Twitter. Basedon our findings, a hierarchical classification scheme wasthen built for the classifications of different HPV vaccin-ation sentiments, see Fig. 1. Detailed definitions of eachcategory were provided in Table 1.Gold standard annotationWe annotated each tweet based on its content. Threeannotators (part time) were employed in this annotationprocess. Two of them have a public health backgroundand the other has health informatics background. Theannotators annotate the tweets according to the classifi-cation scheme. The annotator first decides whether thetweet is related to HPV vaccines or not. If it is related,the annotator further decides if it is positive, negative, orneutral. If it is negative, the annotator assigns one of thecategories under Negative to the tweet.All tweets have been annotated by at least two annota-tors in the first round. The third annotator was involvedwhen the two annotators have different annotations andmade the final decision in the second round. The firstround took up to one month. The second round took upto two weeks. We applied the brat rapid annotation toolfor this process [15]. After the annotation, the Kappavalue was calculated from the annotators to evaluate thequality [16].The example tweets annotated in our gold standardcan be seen in the Additional file 1: Table S1A.Fig. 1 Sentiment classification scheme for HPV vaccines relatedtweets: The categories in colored rectangles (other than black) areall possible sentiment labels that can be assigned to the tweetsDu et al. Journal of Biomedical Semantics  (2017) 8:9 Page 2 of 7Machine learning system optimizationOur system is a modularized machine learning systemthat consists different pre-processors and feature extrac-tors. A detailed overview of the system can be seen inFig. 2a.Tweets Pre-processing Text Normalizer. All upper-case letters were con-verted to lower case ones. All hashtags and Twitteruser names (e.g. @twitter) were excluded. All URLswere exchanged with string url (e.g. http://exam-ple.com to url). We also replaced any letter occur-ring more than two times in a row with twooccurrences (e.g. convert huungry, huuuungry tohuungry), proposed by Go A et al. [17]. POS Tagger. We used TweeboParser [18, 19]developed by Carnegie Mellon University to extractPOS tags for tweets. TweeboParser is trained on asubset of new labeled corpus for 929 tweets (12,318tokens) [19]. It provided a fast and robust Java-basedtokenizer and POS tagger for tweets.Features extractionConsidering the characteristics of HPV vaccine relatedtweets, we extracted the following features: Word n-grams. Contiguous 1 and 2 g of words areextracted from a given tweet. Clusters. Previous work found that word cluster canbe used to improve the performance of supervisedNLP models [20]. We mapped tweets tokens toTwitterWord Clusters developed by ARK group ofCarnegie Mellon University (the group is currentlyin University of Washington). This largest clusteringmapped 847,372,038 tokens from approximately 56million tweets into 1000 clusters. (e.g. tehy", thry,theey, they et al. belong to a same cluster) POS tags. Part of speech tags were extracted byTweeboParser as one of the features.Machine learning algorithmIn our pre-experiment, we leveraged the basic n-gramsfeature and applied Weka [21] to test and compare differ-ent machine learning algorithms: Naïve Bayes, RandomForest and Support Vector Machines (SVMs). As SVMsoutperformed the other two algorithms and it has knownperformance on pervious sentiment analysis tasks [22], weleveraged SVMs as the algorithms. SVMs are supervisedlearning models with associated learning algorithms thatanalyze data used for classification and regression analysis.We implemented LibSVM package as the library for ourtask. Default RBF kernel was used.Table 1 Detailed definition of different sentiment categories forHPV vaccines related tweetsSentiment DescriptionPositive Show positive opinion or prompt the uptake ofHPV vaccineNegative Safety Concerns or doubt on the safety issues of HPVvaccine or present vaccine injuriesEfficacy Concerns or doubt on the effectiveness of HPVvaccineCost Concerns on the cost of HPV vaccine (e.g.: moneyor time)Resistant Resistance to HPV vaccines due to cultural oremotional issuesOthers Other concernsNeutral Related to HPV vaccine topic but contains nosentiment or sentiment is unclear or contains bothnegative and positive sentimentUnrelated Not related to HPV vaccine topicFig. 2 Overview of the machine learning based system and optimization approach: (a) modularized machine learning system framework; (b)machine learning optimization stepsDu et al. Journal of Biomedical Semantics  (2017) 8:9 Page 3 of 7Machine learning system optimization Baseline model. To create a baseline sentiment analysismodel, we applied plain classification, used word-ngramsas the feature and chose default SVMs parameters. Hierarchical classification VS plain classification.Traditional multi-labels classification methods thattreat each category equally do not take into accountthe hierarchical information. The highly imbalancedstructure of our gold standard could have a dramaticeffect on the system performance [18]. In order toalleviate the effect of the imbalanced structure, wetested the hierarchical classification and comparedthe performance with the plain one. Three SVMsmodels were trained independently. The first SVMmodel categorized the tweets into Related andUnrelated groups; the second one then categorizedthe Related tweets into Positive, Negative andNeutral groups; the third model further categorizedthe Negative tweets into the five finest categories. Feature combinations. We tested the differentcombinations of word n-grams, clusters and POStags features and evaluated their impact on the sys-tem performance. Parameters optimization. For SVMs model with RBFkernel, there are two major parameters needed to bechosen beforehand for a given problem: C is thecost of misclassification; ? is the parameter of thekernel function [19]. The C parameter trades offmisclassification of training examples againstsimplicity of the decision surface, while the ? defineshow far the influence of a single training examplereaches, with low values meaning far and highvalues meaning close [23].An overview of the optimization steps can be seen inFig. 2b.EvaluationTo evaluate the performance of the machine learning algo-rithms, we used 10-fold cross-validation. Standard metricswere applied and the average score were calculated (includ-ing precision, recall and F measure for each category andMicro F measure and Macro F measure for overall per-formance). For micro-averaged score, we summed up allthe individual true positives, false positives, and false nega-tives of the system. For macro-averaged score, we took theaverage of the F score of different classes.ResultsAnnotation resultsThe Kappa value among the annotators was 0.851, whichindicated the high quality of this gold standard. Amongthe human annotated corpus, 3,984 (66.4%) tweets wererelated to HPV vaccine sentiments. Among the relatedtweets, 1,445 (36.3%) of them showed negative opinions,which is larger than both positive (1,153, 28.9%) andneutral tweets (1,386, 34.8%). The major concern in goldstandard is safety issues (63.1% in Negative group). De-tailed results can be seen in Fig. 3. The download linkfor annotation results can be found in section Availabil-ity of data and material.Machine learning system optimization resultsBaseline model performanceChoosing word-ngrams as the feature and default SVMsparameters (C = 256 and ? = 2e-5), we applied the trad-itional plain classification to create the baseline model.Hierarchical VS PlainThe performance comparison between baseline model(plain classification) and hierarchical classification canbe seen in Table 2. The hierarchical classificationmethod outperformed the plain method in each cat-egory. For the micro-averaging and macro averaging Fscore, hierarchical way significantly increased the per-formance to 0.7208 and 0.4841 from 0.6732 and 0.3967respectively. Specifically, for the category NegOthersand NegEfficacy, the hierarchical method increased0.3095 and 0.2593 on F score respectively.Results for the evaluation on feature setsSince the hierarchical method outperformed the plainmethod significantly, we chose this way as default in ourfollowing optimization steps. Default SVMs parameters(C = 256 and ? = 2e-5) were used in this step. The 10-Fig. 3 Sentiment distribution in 6,000 tweets gold standard.(Neg: Negative)Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 4 of 7fold evaluation results for different feature sets combina-tions can be seen in Table 3.The highest micro-averaging and macro-averaging Fscore were 0.73 and 0.4986, achieved by using the com-bination of n-grams, POS, and word clusters features.Adding POS and cluster feature set can both lead tonearly 0.5% increase in micro-averaging F -score com-pared with using word n-grams feature only (POS: from0.7208 to 0.7263; Cluster: from 0.7208 to 0.7255). AddingPOS feature only achieved the highest performance forUnrelated category, whereas adding cluster feature out-performed on Neutral category. Except for Unrelatedand Neutral category, Adding POS and cluster featuresets together achieved the highest performance.Results for the Evaluation on Parameters OptimizationAs adding POS and cluster feature sets togetherachieved the best performance. The ideal way to find thebest parameters C and ? should be grid search method.However, as we chose the hierarchical classificationmethods, we need to train three SVMs models inde-pendently. The grid search method will be muchcomputation-costly. To reduce the computation burden,we decided to optimize the parameters in two steps: 1)use the default C and grid search best ? combinationsfor three SVMs models; 2) use the ? combinations thatachieved the best performance in step 1 and grid searchbest C combinations for three SVMs models.The default C and ? are 256 and 2e-5 respectively. Forthe step one, we fix C to 256 for all the three modelsand gave ? a range of {2e-7, 2e-6, 2e-5, 2e-4, 2e-3} forthe grid search. Since we have three models, we totallytested 125 models in this step. The best ? combinationis: 2e-5 for the first SVMs model, 2e-4 for the secondone and 2e-4 for the third one. For the step two, wechose the found ? combination in the step one and gaveC a range of {64, 128, 256, 512, 1024} for the grid search.Due to the three models we have, 125 models weretested in this step. The best C combination found is: 512for the first SVMs model, 128 for the second one and512 for the third one. The performance comparison be-tween the best performing models after parameteroptimization and the model using default parameterscan be seen in Table 4. We can observe that by doingTable 2 10-fold cross validation performance on the baselinemodel and hierarchical classification model. (F: F-1 score; P: pre-cision; R: recall; for the categories that do not indicate themetric, F-1 score are used)ClassificationModelPlain Classification (Baselinemodel)HierarchicalClassificationMicro-averagingF 0.6732 0.7208Macro-averagingP 0.4455 0.5402R 0.3574 0.4386F 0.3967 0.4841Unrelated 0.8044 0.8599Neutral 0.5792 0.6181Positive 0.6528 0.7021NegSafety 0.7006 0.7277NegEfficacy 0 0.2593NegCost 0 0NegResistant 0 0NegOthers 0.155 0.4645Table 3 10-fold cross validation performance on differentfeature sets combinations. (Feature sets: (a) Word n-grams; (b)POS tags; (c) Clusters; F: F-1 score; P: precision; R: recall; for thecategories that do not indicate the metric, F-1 score are used)Feature sets (a) (a) + (b) (a) + (c) (a) + (b) + (c)Micro-averaging F 0.7208 0.7263 0.7255 0.73Macro-averaging P 0.5402 0.5438 0.5396 0.5477R 0.4386 0.4468 0.4442 0.4576F 0.4841 0.4905 0.4872 0.4986Unrelated 0.8599 0.864 0.859 0.8618Neutral 0.6181 0.6226 0.625 0.6231Positive 0.7021 0.7098 0.7123 0.7136NegSafety 0.7277 0.734 0.7357 0.7542NegEfficacy 0.2593 0.3214 0.2593 0.3793NegCost 0 0 0 0NegResistant 0 0 0 0NegOthers 0.4645 0.4614 0.4724 0.4753Table 4 10-fold cross validation performance among the bestperforming model after C and ? optimization and the modelusing default C and ?. (F: F-1 score; P: precision; R: recall; for thecategories that do not indicate the metric, F-1 score are used)Model Model usingdefault C and ?Best model usingoptimized ? onlyBest model usingoptimized C and ?Micro-averagingF 0.73 0.7352 0.7442Macro-averagingP 0.5477 0.6889 0.6873R 0.4576 0.5095 0.5142F 0.4986 0.5858 0.5883Unrelated 0.8044 0.8538 0.8633Neutral 0.5792 0.6330 0.6470Positive 0.6528 0.7239 0.7255NegSafety 0.7006 0.7641 0.7617NegEfficacy 0 0.4138 0.4068NegCost 0 0.5 0.5NegResistant 0 0 0NegOthers 0.155 0.5144 0.5403Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 5 of 7parameters optimization, our machine learning modelhas increased 1.442% and 8.97% on micro-averaging andmacro-averaging F score respectively. The optimizedmodel leads to significant increase on nearly all categor-ies except for NegResistant category.DiscussionsAnnotation results showed that there were still manyconcerns over the HPV vaccine on Twitter during thestudy period. The number of tweets holding negativeopinions on HPV vaccines exceeded the tweets holdingpositive opinions. The major concern found was aboutsafety issues. As it is a relative small corpus, in the fu-ture, we plan to apply this system on a large-scale tweetscorpus. We can leverage further analysis tool to trackthe changes and to identify the patterns of different sen-timents toward HPV vaccines over the time.As the gold standard has a highly imbalanced structure(highly uneven distribution of different categories), trad-itional plain classification method cant take advantageof the hierarchical classification information. The pro-posed hierarchical classification method outperformedthe plain method significantly on overall performanceand on each category as well. Adding POS tags and wordclusters as a feature has already shown its effect on im-proving performance on previous NLP tasks. Our ex-periment further demonstrated its power in the multi-classification tasks on tweets corpus for accessing vac-cination purpose. Parameter optimization is very neces-sary according to our results. It can greatly influence thesystem performance, especially on some categories withvery limited number.There are still several limitations of the work reportedhere. A serious issue for our Twitter corpus is that it ishighly unbalanced, which means that the distribution ofdifferent classes is highly diverse. It is very challengingfor machine learning system to handle classes with verylimited number. In the future, we plan to collect incorp-orate more tweets of minority classes to the gold stand-ard. In this work, we only used three feature sets. Morefeature sets can be included to improve the performance,including character n-grams, word dependency, struc-ture feature, and sentiment lexicons feature. Rule-basedapproaches might be more effective for classification onminority classes. A hybrid system consisting of both ma-chine learning and rule-based approach is supposed tobe very helpful.ConclusionsWe designed and conducted a study to classify HPV vac-cine related tweets by the sentiment polarity using ma-chine learning methods. A hierarchical scheme wasproposed for different sentiment classifications of HPVvaccines. Ten different categories were included to covermost types of public opinions for HPV vaccines. A goldstandard that is consisted of 6,000 randomly selectedtweets were manually annotated as the training dataset.Different classification methods were evaluated. Differ-ent combinations of feature sets and parameters weretested to optimize the performance of the machinelearning model. Compared with the baseline model, thehierarchical classification model with optimized featuresets and model parameters has increased the micro-averaging and macro-averaging F score from 0.6732 and0.3967 to 0.7442 and 0.5883 respectively.Our work provides a systematical way to improve themachine learning model performance on the highly un-balanced HPV vaccine related tweets corpus. Our systemcan be further applied on a large tweets corpus to ex-tract large-scale public opinion towards HPV vaccines.Similar systems can be developed to explore other publichealth related issues.Additional fileAdditional file 1: Table A. Sample tweets annotated in the goldstandard for each sentiment category (DOCX 43 kb)AcknowledgementsN/A.FundingThis research is partially supported by the National Library of Medicine of theNational Institutes of Health under Award Number R01LM011829. Theauthors also gratefully acknowledge the support from the UTHealthInnovation for Cancer Prevention Research Training Program Pre-doctoralFellowship (Cancer Prevention and Research Institute of Texas grant #RP160015).Availability of data and materialsThe annotations of gold corpus can be found at: https://sbmi.uth.edu/ontology/files/TweetsAnnotationResults.zip.Authors contributionsJD collected the data, wrote the initial draft and revised subsequent draft.JD, JX, XL and HS developed the method and performed the evaluation. HSand XL provided expertise in classification scheme. CT provided institutionalsupport, and contributed to research design. All authors read and aprovedthe final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationN/A.Ethics approval and consent to participateThis study received IRB approval from Committee for the Protection ofHuman Subjects at The University of Texas Health Science Center atHouston. The reference number is HSC-SBMI-16-0291.Author details1The University of Texas School of Biomedical Informatics, 7000 Fannin StSuite 600, Houston, TX 77030, USA. 2The University of Texas School of PublicHealth, 1200 Pressler Street, Houston, TX 77030, USA.Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 6 of 7Received: 20 November 2016 Accepted: 7 February 2017Papanikolaou et al. Journal of Biomedical Semantics  (2017) 8:43 DOI 10.1186/s13326-017-0150-0RESEARCH Open AccessLarge-scale online semantic indexing ofbiomedical articles via an ensemble ofmulti-label classification modelsYannis Papanikolaou1* , Grigorios Tsoumakas1, Manos Laliotis2, Nikos Markantonatos3and Ioannis Vlahavas1AbstractBackground: In this paper we present the approach that we employed to deal with large scale multi-label semanticindexing of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge(20132017), a challenge concerned with biomedical semantic indexing and question answering.Methods: Our main contribution is a MUlti-Label Ensemble method (MULE) that incorporates a McNemar statisticalsignificance test in order to validate the combination of the constituent machine learning algorithms. Somesecondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to theBioASQs super-set, the PubMed articles collection) and the proper parametrization of the algorithms used to dealwith this challenging classification task.Results: The ensemble method that we developed is compared to other approaches in experimental scenarios withsubsets of the BioASQ corpus giving positive results. In our participation in the BioASQ challenge we obtained the firstplace in 2013 and the second place in the four following years, steadily outperforming MTI, the indexing system of theNational Library of Medicine (NLM).Conclusions: The results of our experimental comparisons, suggest that employing a statistical significance test tovalidate the ensemble methods choices, is the optimal approach for ensembling multi-label classifiers, especially incontexts with many rare labels.Keywords: Semantic indexing, Multi-label ensemble, Machine learning, BioASQ, Supervised learning, Multi-labellearningBackgroundIntroductionMEDLINE is the premier bibliographic database of theNational Library of Medicine (NLM) of the United States.In June 2017 MEDLINE contained over 27 million ref-erences to articles in life sciences with a focus onbiomedicine. Each of these articles is manually indexedby human experts with concepts of the MeSH (MedicalSubject Headings) ontology (also curated by NLM), suchas Neoplasms, Female and Newborn. This manual index-ing process entails significant costs in time and money.*Correspondence: ypapanik@csd.auth.gr1Department of Computer Science, Aristotle University, 54124 Thessaloniki,GreeceFull list of author information is available at the end of the articleHuman annotators need on average 90 days to complete75% of the citation assignment for new articles [1]. Fora publication with novel and important scientific results,the first period of its lifetime is quite important, yet itis in this period that the publication remains semanti-cally invisible. For instance, if a researcher is searchingfor a particular MeSH term (e.g. Myopathy), he/she willnot be able to retrieve the latest non-indexed articles thatare related to this term, if they do not contain it liter-ally. Moreover, the average indexing cost for an articleis $9.401.MEDLINEs demand in manual indexing is steadilyincreasing as evident from Fig. 1, which plots the numberof articles being added to MEDLINE each year from 1950to 2017. At the same time, the available indexing budget at© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Papanikolaou et al. Journal of Biomedical Semantics  (2017) 8:43 Page 2 of 13Fig. 1 Number of articles being added to MEDLINE each year from1950 to 2017NLM is flat or declining. This highlights the importanceof tools for automatic semantic indexing of biomedicalarticles. Such tools can help increase the productivity ofhuman indexers by recommending them a ranked list ofMeSH descriptors relevant to the article they are currentlyexamining. In addition, such tools could replace juniorindexers (not senior revisers) for journals where thesetools achieve a high level of accuracy. Both usages of suchtools are currently adopted by NLM.From a machine learning perspective, constructing anautomatic semantic indexing tool for MEDLINE poses anumber of important challenges. First of all, there is alarge number of training documents and associated con-cepts. In 2017, MeSH contained 28,489 descriptors, whilePubMed contained over 27 million annotated abstracts.Efficient yet accurate learning and inference with suchlarge ontologies and training sets is non-trivial. Addition-ally, MEDLINE is growing at a non-trivial rate of morethan one million articles per year, i.e. more than 100 arti-cles per hour. This calls for learning algorithms that canwork in an online fashion both in the sense of handlingadditional training data as well as in the sense of being effi-cient enough during prediction in order to cope with thefast rate that new articles arrive. Furthermore, MEDLINEcontains abstracts from about 5000 journals covering verydifferent topics. This increases the complexity of the tar-get function to be learned, as concepts may be associatedwith different patterns of word distributions in differentbiomedical areas.MeSH concepts are hierarchically structured as adirected acyclic graph indicating subsumption relationsamong parent and child concepts. This structure is quitecomplex, as it comprises 16 main hierarchies with depthsup to 12 levels and many children nodes belong to morethan one ancestors and to more than one of the main hier-archies. While some progress has been recently achievedon exploiting such relationships, it is not entirely clearwhen and how these relationships help accuracy. AsMeSH evolves yearly on par with the medical knowledgeit describes, automatic indexing models must deal withsuch changes, both explicit (i.e. addition, deletion, merg-ing of concepts) and implicit (i.e. altered semantics ofconcepts) ones. Also, each scientific document is typi-cally annotated with several MeSH concepts. Such dataare known as multi-label [2] and present the additionalchallenge of exploiting label dependencies to improveaccuracy. Figure 2 shows the distribution of the numberof labels per document which is Gaussian with a mean ofabout 13 labels per document and a heavy tail on the right.The distribution of positive and negative examples formost of the MeSH concepts is very imbalanced [3].Figure 3 plots the frequencies of labels (x-axis) versusthe number of labels having such frequency (y-axis) for aBlfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 DOI 10.1186/s13326-017-0159-4RESEARCH Open AccessA document-centric approach fordeveloping the tolAPC ontologyAisha Blfgeh1,2*, Jennifer Warrender1, Catharien M. U. Hilkens3 and Phillip Lord1AbstractBackground: There are many challenges associated with ontology building, as the process often touches on manydifferent subject areas; it needs knowledge of the problem domain, an understanding of the ontology formalism,software in use and, sometimes, an understanding of the philosophical background. In practice, it is very rare that anontology can be completed by a single person, as they are unlikely to combine all of these skills. So people with theseskills must collaborate. One solution to this is to use face-to-face meetings, but these can be expensive andtime-consuming for teams that are not co-located. Remote collaboration is possible, of course, but one difficulty hereis that domain specialists use a wide-variety of different formalisms to represent and share their data  by the farmost common, however, is the office file either in the form of a word-processor document or a spreadsheet.Here we describe the development of an ontology of immunological cell types; this was initially developed by domainspecialists using an Excel spreadsheet for collaboration. We have transformed this spreadsheet into an ontology usinghighly-programmatic and pattern-driven ontology development. Critically, the spreadsheet remains part of the sourcefor the ontology; the domain specialists are free to update it, and changes will percolate to the end ontology.Results: We have developed a new ontology describing immunological cell lines built by instantiating ontologydesign patterns written programmatically, using values from a spreadsheet catalogue.Conclusions: This method employs a spreadsheet that was developed by domain experts. The spreadsheet isunconstrained in its usage and can be freely updated resulting in a new ontology. This provides a generalmethodology for ontology development using data generated by domain specialists.Keywords: Tawny-OWL, Document-centric, Ontology, Excel workflowIntroductionOntologies have been used extensively to describe manyparts of biology. They have two key features which maketheir usage attractive: first, they can provide a mecha-nism for standardising and sharing the terms used indescriptions; and, second, they provide a computation-ally amenable semantics to these descriptions, makingit possible to draw conclusions which are not explicitlystated.Ontologies are increasingly used to facilitate the man-agement of knowledge and the integration of information*Correspondence: a.blfgeh1@newcastle.ac.uk; abelfaqeeh@kau.edu.sa1School of Computing Science, Newcastle University, NE1 7RU NewcastleUpon Tyne, UK2Faculty of Computing and Information Technology, King AbdulazizUniversity, 21589 Jeddah, Saudi ArabiaFull list of author information is available at the end of the articleas in the SemanticWeb [1]. Biological data is not only het-erogeneous but requires special knowledge to deal withand can be large [2]. Ontologies are good for representingcomplex and, potentially, changeable knowledge. There-fore, they are widely used in biomedicine with examplessuch as the Gene Ontology [3], ICD-10 (InternationalClassification of Diseases) [4] or SNOMED (SystematizedNomenclature of Medicine) [5] being the best known.However, building an ontology is a challenging task [6].Ontologies often use languages with a complex underly-ing formalism (such as OWL1 -Web Ontology Language-for instance) especially when modelling complex domainarea such as biology or medicine. Moreover, ontologybuilding is normally a collaboration between domain spe-cialists and ontology developers. However, any form ofmulti-disciplinary collaboration is difficult. In the case,for example, of the Gene Ontology, these challenges were© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 2 of 9addressed through explicit community involvement usingmeetings, focus groups and the like [7]. Other methodolo-gies have adopted a more distributed approach [8].It is, perhaps, because of these challenges that, despitethe computational advantages of ontologies, the oldestand most common form of description in biology is freetext, or a semi-structured representation through the useof a standardised fill-in form. These representations havenumerous advantages compared to ontologies: they arerichly expressive, widely supported by tooling and whilethe form of language used in science (Bad English [9])may not be easy to use, understand or learn, it is widelytaught and most scientists are familiar with it. Similarly,most biologists are familiar with the tools used for pro-ducing free-text and forms, either a word-processor doc-ument or a spreadsheet. Tools for producing this formof knowledge are wide-spread, richly functional both inapplication and cloud-delivered form, and support highlycollaborative development.The ontology community, conversely, has largely builtits own tool-chain for development. Tools such as Protégé[10] are highly functional in their own right, but have auser interface which is far removed from those that biol-ogists are used to. There have been several responses tothis problem. First, it is possible to take existing ontol-ogy tools and customise them for use within a specificcommunity, so that they have a familiar look and feel;this is the approach taken by iCAT (Collaborative Author-ing Tool)  a version of WebProtégé [11] built explicitlyfor the ICD-11 community [12]. A second approach is toenable existing ontology tools to ingest office documents;for example, Cellfie [13] is a Protégé plugin which cantransform a spreadsheet into an OWL ontology, whichcan then be developed further; however this is a one-off process  once ingested, the data in the spreadsheetis converted into OWL; further updates cannot be madeusing the original spreadsheet formalism. Finally, toolssuch as RightField [14] and Populous [15] add ontologi-cal features to office documents, by allowing selection ofspreadsheet cells from a controlled vocabulary, followedby export to OWL using OPPL (Ontology Pre-ProcessorLanguage) [16] to express the patterns used in the trans-formation [17].These tools, however much they support the use ofoffice software, at some point require leaving this soft-ware and moving into an ontology specific environment.We have developed a new, highly-programmatic environ-ment for ontology development called Tawny-OWL [6].With this approach the ontology is developed as program-matic source code, which is then evaluated to generate thefinal ontology, either in memory or as an OWL file. Thisoffers a new methodology. In this research, we developeda document-centric workflow centred on the use of officetooling to construct the ontology; biologists generate andmaintain their dataset in an unconstrained Excel spread-sheet; we then use this spreadsheet directly as part of oursource code2, driven by Tawny-OWL. In this model, wecan apply arbitrary validation and transformation of thedata held in the spreadsheet, into an ontological form. Asthe spreadsheet is now part of the source code, rather thanbeing used as knowledge capture interface, it can be freelyupdated and the final ontology regenerated.In this paper, we describe the application of thismethodology to the generation of a catalogue of immuno-logical cell types, called the tolAPC (tolerogenic antigen-presenting cells) catalogue. We discuss the backgroundtechnology, the design decisions that we have faced andthe general implications that this approach has for ontol-ogy development.BackgroundThe tolAPC catalogue is a list of immunological celltypes. It has been captured as part of the EU CostAction BM1305 A-FACTT (Action to Focus and Accel-erate Cell-based Tolerance-inducing Therapies)3 which isaimed at increasing data sharing and collaborative work-ing across the community [18]. These cell types have beentolerised  that is treated so that they suppress theimmune response  and have been created with the inten-tion that they will be used therapeutically in a variety ofsituations including: the treatment of auto-immune dis-ease such as rheumatoid arthritis; or to reduce rejectionfollowing transplantation [19]. Information about thesecells is, therefore, high value. The tolAPC catalogue con-tains extensive details about these cell lines, including 9sheets of data. The catalogue has been created as anExcel spreadsheet, although it uses the spreadsheet onlyto represent tabular information (i.e. there is no use ofequations or calculation in the spreadsheet). The spread-sheet has been created by individual scientists freely; thatis, there is no formal constraint on the legal set of valuesin each cell, just the social convention of copying previ-ous cells. Figure 1 shows the structure of the spreadsheetfilled with false information due to the confidentiality ofthe tolAPC catalogue.Next, we describe Tawny-OWL; it is a fully program-matic development environment for OWL. It has beenimplemented in Clojure, which is a dialect of lisp, run-ning on the Java Virtual Machine. It wraps the OWL-API[20] which performs much of the actual work, includ-ing interaction with reasoners, serialisation and so forth.Tawny-OWL has a simple syntax which was originallymodelled on the Manchester OWL notation [21], modi-fied to conform to standard Clojure syntax and to increaseregularity [22]. For example, we can create a new classwith an existential restriction as follows:(defclass A :super (some r B))Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 3 of 9Fig. 1 A mock sample of tolAPC catalogue to show the structure of the Excel spreadsheetOr, we can define a new individual with a propertyassertion:{(defindividual i :fact (is r j))}As a domain specific language embedded in a full pro-gramming language, we also gain all the features of thatenvironment; for instance, we can create arbitrary pat-terns simply by using a Clojure function. Consider forexample:(defn some-only [property & classes](list (some property classes)(only property(or classes))))Here defn introduces a new function, property& classes are the arguments, and list packages thereturn values as a list. some, only and or4 are definedby Tawny-OWL as the appropriate OWL class construc-tors. This allows a definition specifying an existentialrelationship with a closure axiom as follows:(defclass D :super (some-only r A B))We also gain access to the full Clojure infrastructure: wecan edit and evaluate terms in a power editor or IDE (Inte-grated Development Environment)5; write unit tests andrun them through a build tool [23], publish and versionusing git, and continuously integrate our work with otherontologies.We have previously used this functionality to create thekaryotype ontology which is generated from a series ofinterlocking sub-patterns [24], parameterised using literaldata structures in the source code. The karyotype ontol-ogy is highly patternised, with almost all of the classescoming from a single large pattern.As a full programming environment, Clojure can alsoread and parse arbitrary data formats, which can operateas additional source during the generation of the ontology.We have previously used this to scaffold a mitochondrialontology from a varied set of input files [25], or to addmulti-lingual annotation using key=value propertiesfiles to the pizza ontology. We have also used this technol-ogy with a spreadsheet to specify a set of ontological unittests for the karyotype ontology [23]. In this case, the val-ues in the spreadsheet are used to generate a set of OWLclasses which are then checked for correct subsumptionusing a reasoner. In this case, however, these ontologicalstatements are used only as part of a test suite, rather thanintended for downstream usage, and the spreadsheet wascreated specifically for this purpose.MethodsThe data for the tolAPC catalogue was captured directlyin a spreadsheet largely co-ordinated through email. Asa pre-existing resource, it made little sense to rewritedirectly in OWL either using Protégé or Tawny-OWL to do so would have resulted in transcription errors,and made updates more complex. However, as describedin the Background section, we have all the compo-nents that we need to build an ontology directly from aspreadsheet.Therefore, we started the development process usingour new document-centric workflow that incorporatesExcel spreadsheet during development as described inFig. 2. We read the spreadsheet directly and extract allvalues we need to instantiate the ontology patterns wehave already designed using the programming facilitiesof Tawny-OWL. The final ontology can be saved as anOWL file to be browsed using Protégé software or a webbrowser.Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 4 of 9Fig. 2Workflow using Excel spreadsheet and Tawny-OWL PatternsBuilding the tolAPC ontologyIn this section, we describe the issues that have arisen dur-ing the process which can conceptually be split into threephases6:1. Extraction2. Validation3. OntologisationThe extraction phase is straight-forward. Clojure offersa number of libraries capable of reading a spreadsheet.In the case of the tolAPC catalogue, we read the spread-sheet using the Docjure library7, accessed directly fromthe file system. It would also be simple and straight-forward to read from a network which would supportbuilding ontologies from cloud-hosted spreadsheets. Pre-viously, for performance reasons, we have read and thencached the results of tests generated from a spreadsheet[23]; however, for the tolAPC catalogue performance issuch that the spreadsheet can be read in full every timethe environment is initialised, significantly simplifying thedevelopment.In the second phase, values extracted are validatedagainst a set of constraints specifying those which arelegal. For many of the fields, values are highly stereo-typed having only a few different options; for example,cells can either be Autologous or Allogeneic, whileexpression levels can either be + or -. Currently, valida-tion is performed through the use of ad hoc testing we expect to move to a more formal data constraint lan-guage in future. The choice of validation depends on therequirements and modelling choices made, which will bediscussed later.In the third phase, values are ontologised. The toplevel of the ontology which provides what we describe asschema terms is written by hand using Tawny-OWL. Inthe case of the tolAPC catalogue, this includes terms suchas CellType, Species and AntigenLoad. Next, a setof patterns is defined using these schema terms. Finally,these patterns are instantiated using the values from thesecond phase, generating entities that we call patternisedterms.During the development process, both reasoning andmanual inspection of the created ontology is used toensure that the process is happening as expected; for thelatter process, the ontology is saved to file and examined,either in the Clojure development environment or withinProtégé, as shown in Fig. 3.We next discuss the modelling issues that have arisen.Modelling in the tolAPC ontologyAll entities in the ontology need to be represented byan IRI (Internationalized Resource Identifier). Two broadschemes are used to generate IRIs: semantics free identi-fiers which are generally numeric; and semantically mean-ingful identifiers which are normally derived from thecommon name for the entities. Generally, the latter areeasier to work with, while the former are easier to keepstable over releases.Currently, for the tolAPC ontology, schema terms haveIRIs which reflect their names (CellType uses an IRI witha fragment of CellType), while patternised terms use anad hoc schema based on several of their properties (a sin-gle property is not enough to ensure uniqueness). If wewish to re-evaluate this situation at a later date, however,Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 5 of 9Fig. 3 tolAPC ontology displayed from Protégé screenTawny-OWL simplifies the situation; we can easily allo-cate IRIs to entities according to any scheme that wechoose, by changing a single function.A recurrent issue in ontology modelling is whether touse classes or individuals; within the tolAPC ontology,we faced this question for cell types. There are a num-ber of different criteria for making this decision [26]. Weconsidered briefly a realist perspective: modelled as asingle entity, cell types are probably best represented asa metaclass, akin to a taxonomic species [27]; modellingas multiple entities (differentiating between the protocoland the cell type produced) would also be possible. How-ever, there appears to be no clear principle to distinguishbetween these options. Similar problems also arise forproteins/cell-surface markers which are described in theontology. As an additional problem, these representationsintroduce considerable unnecessary complexity [28].We considered therefore the needs of our application:it seems unlikely that we will ever need subclasses of acell type, but might reasonably wish for cell types to beunique  to state that two cell types are necessarily thesame (or different) individual. For these reasons, wemodelcell types as individuals. An example from the ontologystructure is shown in Fig. 4.The tolAPC ontology largely models a set of cell types,with the rest of the ontology designed to support theseFig. 4 Class Structure in tolAPC ontologydescriptions. The ontology, as a result, contains very littlehierarchy, and is at the extreme end of a normalised ontol-ogy [29]. Cell types are defined as individuals with a largeset of different property assertions, as can be seen fromthe following definition:(individual cell-name:fact (is fromGroup group)(is hasLocation loc)(is fromClinicalDisease clinic-disease)(is fromSpecies from-species)(is hasStatus stat)(is hasType c-type)(is hasDescription desc)(is hasActivation active)(is hasAntigenLoad anti-load)(is itsOrigin cell-org)(is withStartMaterial start-material)(is hasIsolation isol))Here, cell-org, group, loc and others are variables,therefore, this definition describes a pattern. fromGroup,hasLocation and others are specific object propertiesfrom the schema terms of the ontology. individual,:fact and is are part of Tawny-OWL syntax. The wholedefinition defines a new cell type, and its association witha set of individuals.The values of the property assertions fall into one ofthree main categories.Open but Limited:Many properties support a very lim-ited, but nonetheless open, range of values. Examples ofthese are withStartMaterial which describes the tis-sue or part of the tissue from which the cells are derived.These values are modelled as disjoint classes, explicitlystated in the ontology. Although, we could have used anexternal ontology at this point, as only a few options areactually used, we have not imported one.Constrained Partition: Many properties support anexact number of options. These are modelled using aValue Partition [30]. Fortunately, Tawny-OWL providesexplicit support for this design pattern, which allowsa relatively succinct definition. An example of this isCellOrigin which is defined as follows:Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 6 of 9(deftier CellOrigin[[Allogeneic:comment "Allogeneic stem cell transplantuses a donor blood"][Autologous:comment "Autologous stem cell transplantuses a patient own blood"]])Unconstrained Values: Some properties have uncon-strained values such as Location, Group (i.e. the peopleresponsible for the cell type) or AntigenLoad. These arecurrently modelled as individuals, created on-demand.In some cases, these values also reuse terms from exter-nal ontologies; currently, our Species term refers to theNCBI (National Centre for Biotechnology Information)taxonomy, although we do not import the full semanticsof this ontology as it would cause a considerable increasein reasoning time, for relatively low reward.In addition to these threemain categories, we are addingphenotype descriptors to the cell types, in terms of raisedor lowered expression levels. For these, we are modellingthe expression levels as a value partition, while the over-all phenotype is modelled using the N-ary relationshippattern [31], as shown in Fig. 5.ResultsWe have developed a new ontology describing immuno-logical cell lines built by instantiating ontology designpatterns written programmatically, using values from aspreadsheet catalogue. The development of the tolAPContology is a work in progress. As can be seen fromTable 1, while parts of the tolAPC catalogue have beenrecast, there are significantly more spreadsheet cellswhich need to be converted.DiscussionIn this paper, we have described the development of thetolAPC ontology, describing data about immunologicalcell types. This ontology is unusual in that it is deriveddirectly from another data resource, the tolAPC cata-logue which is maintained as an Excel spreadsheet. Essen-tially, the ontology provides context and semantics to datawhich is available in another form.The value of recasting a spreadsheet into a form withprecise machine interpretable semantics is obvious, butthere are less apparent virtues arising from the process.Table 1 Current statistics of excel sheet and tolAPC ontologytolAPC catalogue Number of sheets 9Number of cells 1181Number of cell types 15tolAPC ontology Number of classes 21Number of individuals 101Number of object properties 13In the initial validation step, for example, we have hadto clarify parts of the tolAPC catalogue which are oth-erwise unclear. For example, one cell-line is describedas Autologous/Allogeneic. The original author intenthere is unclear: this could be intended to mean eitherautologous or allogeneic (possible), both (probably incon-sistent) or just the absence of knowledge. Similarly theprocess of ontologisation forces us to clarify some areasof the biology; including questions about whether celltypes produced by the same protocol at different timesare the same or otherwise, which touches on issues ofreproducibility. Where these issues have arisen, either theontology schema, patterns or the spreadsheet can bemod-ified accordingly. As shown in Fig. 2, information flowsin both directions between the spreadsheet and ontol-ogy. Currently, validation is performed by hand specify-ing constraints as enumerations of strings. In future, wewould like to move this to a more declarative approach;fortunately, because Tawny-OWL is implemented over afull programming language, there are a number of dif-ferent data constraint languages, such as Prismatic [32],or clojure.spec8. We expect richer validation will help toenhance the ontology development process further.The development of the tolAPC ontology is an ongoingwork where some parts of the tolAPC catalogue have beenadapted into the ontology, but there are other spread-sheet cells which still need to be imported. Additionally,while adding machine interpretable semantics is useful inits own right, we have only started to address the issueof interoperability with other ontologies. Currently, childterms of Species re-use IRIs from the NCBI taxonomy;the mapping between the free text used in the tolAPCcatalogue and the NCBI taxonomy is stored in a literalFig. 5 N-ary Relation in tolAPC ontologyBlfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 7 of 9data structure in source, but could also be stored in a flat-file or subsidiary spreadsheet. We do not import the fullontology for reasons of performance, a process known as asoft import [33]. Developing a programmatically definedontology allows us to switch easily between soft, hardand MIREOT-style semi imports [34]. Conversely, childterms of ClinicalDisease do not currently relate toother ontologies. At the current time, we have not pri-oritised this process because confidentiality restrictionson the tolAPC catalogue limit our ability to share theresults anyway. Adding this form of interoperability is notcomplex though as we have already demonstrated withSpecies and by using the scaffolding process describedpreviously [25].This work is a further demonstration of the value ofprogrammatic and pattern-driven ontology developmentusing the Tawny-OWL library; it builds on earlier workwith: a karyotype ontology where patterns are instantiatedusing in-code literal data structures; the mitochondrialontology which is scaffolded using a variety of differentinput formats; or our reworking of SIO which patternisesa pre-existing ontology [35]. Patternisation allows thedevelopment of an ontology to be performed rapidly andrepeatedly.The fully programmatic environment also demonstratesits value, as we have been able to add a new input format,even a very complex format such as an Excel spreadsheetwith relative ease, building on tools provided by others.This replicates our earlier experiences with Tawny-OWL;we can reuse and repurpose existing tools not specificallyintended for use in ontology development, also adapt acomplete software development environment to the task.The use of Excel spreadsheets to drive ontology pat-terns is not new of course; it is directly supported withProtégé plugins as well as with tools such as RightFieldand Populous. The key addition of our methodology isto incorporate the spreadsheet as a part of the ontologysource code. The spreadsheet can be updated, changedand consulted by the domain specialists who created it,and still remain part of the ontology development pro-cess. The importance of the right format should not beunder-estimated; for example, early versions of the GeneOntology were developed in their own bespoke syntax(later to evolve into OBO -Open Biomedical Ontologies-Format), something which persisted for a considerabletime after the development and release of OWL. Thereasons for this were simple: OBO Format behaved wellin a version control system, and could be easily created,edited and manipulated in a text editor, something nottrue of RDF (Resource Description Framework)9 serial-isation of OWL available at the time. We wish to buildon these lessons: ontologists should seek to interact andbuild on the tools that domain specialists already use, ifthey hope to describe the knowledge that these specialistshave. It is also for this reason, that we have not designedan Excel template. Rather, we let the experts design andcreate a suitable spreadsheet that matches their needs. So,domain users will be happy and comfortable in using theirusual tool (Excel spreadsheet, designed according to theirneeds) and ontology developers can conveniently programthe ontology using Tawny-OWL. Conversely, one disad-vantage of this approach is that domain users normallyonly interact with one part of the ontology source; thespreadsheet may be correct with respect to the domain,but the ontology wrong. We are, therefore, also investigat-ing techniques for making the Tawny-OWL section of theontology more readable [36].In future, we may consider designing a general templatefor particular domain experts who do not have a clearstructure for their data, so that gives them the opportunityto start organising their data in a semi-structured way;there are a number of pre-existing schemas that we couldusing, including MAGE-TAB [37] and later ISA-TAB [38].The tolAPC ontology and the document-centricapproach it embodies is a first step toward establishinga richer methodology, where we interact with domainspecialists using their own tool chain to capture knowl-edge. In the future, we aim to combine other formats likeWord documents in the ontology development pipelineand design a comprehensive template to communicateeffectively with domain specialists in order to build anaccurate and well designed ontology.ConclusionsIn this paper, we have successfully developed tolAPContology based on the tolAPC catalogue using an Excelspreadsheet as a source of information. Critically, thespreadsheet is unconstrained by the ontology developershaving been freely developed by the domain users. More-over, we have not converted the spreadsheet in a one-offprocess; the spreadsheet is part of the source code for theontology and can be freely updated. Taken together thisdemonstrates a newmethodology for building an ontologywhich enable us to interact with domain specialists usingtheir preferred tools.Endnotes1 https://www.w3.org/TR/owl2-overview/2 By source code, we mean the spreadsheet is notimported but remains the preferred form for editing.3 http://www.cost.eu/COST_Actions/bmbs/BM13054We have elided namespaces: or and some are alsocore Clojure functions.5We use Emacs but there is rich support in Vim, Eclipse,IntelliJ, or LightTable6 In practice, the tolAPC ontology is developediteratively.Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 8 of 97 https://github.com/mjul/docjure8 https://clojure.org/news/2016/05/23/introducing-clojure-spec9 https://www.w3.org/TR/1998/WD-rdf-schema-19980409/AbbreviationsA-FACTT: Action to focus and accelerate cell-based tolerance-inducingtherapies; iCAT: Collaborative authoring tool; ICD: International classification ofdiseases; IDE: Integrated development environment; IRI: Internationalizedresource identifier; NCBI National centre for biotechnology information; OBO:Open biomedical ontologies; OPPL: Ontology pre-processor language; OWL:Web ontology language; RDF: Resource description framework; SNOMED:Systematized nomenclature of medicine; tolAPC: Tolerogenicantigen-presenting cellsAcknowledgementsWe thank Dr Paloma Riquelme (University Hospital Regensburg) for help ingenerating the tolAPC catalogue and participants of the A-FACTT network forproviding data for the tolAPC catalogue. This work has been presented inODLS 2016 in Halle (Saale), Germany. Therefore, we would like to thank ODLS2016 reviewers and organisers for their effort and support.FundingCOST is part of the EU Framework Programme Horizon 2020 (action to focusand accelerate cell-based tolerance-inducing therapies, BM1305,http://www.afactt.eu). Aisha Blfgeh is funded by a scholarship from KingAbdulaziz University, Jeddah, Saudia Arabia.Availability of data andmaterialsThe software tool, Tawny-OWL, is available from http://github.com/phillord/tawny-owl. The tolAPC ontology is currently not available.Authors contributionsAB implemented and designed the tolAPC ontology. JW implemented thespreadsheet importer. CH conceived the tolAPC catalogue. PL conceived thedocument-centric approach. All authors read and approved the finalmanuscript.Ethics approval and consent to participateNewcastle University procedures determined that full ethical approval was notrequired for this research.Consent for publicationThe Authors consent to publish this Work in the thematic series BiomedicalOntologies (BIOONT series).Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1School of Computing Science, Newcastle University, NE1 7RU NewcastleUpon Tyne, UK. 2Faculty of Computing and Information Technology, KingAbdulaziz University, 21589 Jeddah, Saudi Arabia. 3Institute of CellularMedicine, Newcastle University, NE1 7RU Newcastle Upon Tyne, UK.Received: 13 February 2017 Accepted: 15 October 2017Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 DOI 10.1186/s13326-017-0167-4RESEARCH Open AccessIntegrating phenotype ontologies withPhenomeNETMiguel Ángel Rodríguez-García1,2, Georgios V. Gkoutos3,4,5, Paul N. Schofield6 and Robert Hoehndorf1,2*AbstractBackground: Integration and analysis of phenotype data from humans and model organisms is a key challenge inbuilding our understanding of normal biology and pathophysiology. However, the range of phenotypes andanatomical details being captured in clinical and model organism databases presents complex problems whenattempting to match classes across species and across phenotypes as diverse as behaviour and neoplasia. We havepreviously developed PhenomeNET, a system for disease gene prioritization that includes as one of its components anontology designed to integrate phenotype ontologies. While not applicable to matching arbitrary ontologies,PhenomeNET can be used to identify related phenotypes in different species, including human, mouse, zebrafish,nematode worm, fruit fly, and yeast.Results: Here, we apply the PhenomeNET to identify related classes from two phenotype and two disease ontologiesusing automated reasoning. We demonstrate that we can identify a large number of mappings, some of whichrequire automated reasoning and cannot easily be identified through lexical approaches alone. Combiningautomated reasoning with lexical matching further improves results in aligning ontologies.Conclusions: PhenomeNET can be used to align and integrate phenotype ontologies. The results can be utilized forbiomedical analyses in which phenomena observed in model organisms are used to identify causative genes andmutations underlying human disease.Keywords: Phenotype, PhenomeNET, Disease gene prioritization, OWL, Automated reasoningBackgroundUnderstanding the functions of genes and gene prod-ucts is vital for our understanding of normal biology andpathophysiology. In recent years the amount of geno-type and phenotype data available for species as distinctas man and model organisms such as nematode wormshas increased dramatically and continues to accelerate.Insights from non-human species have an important roleto play in our understanding of human biology [1] and thechallenge is to mobilise this data in a way in which it canbe used to give meaningful insights into human physiol-ogy and disease. While much data is now being captured*Correspondence: robert.hoehndorf@kaust.edu.sa1Computational Bioscience Research Center (CBRC), King Abdullah Universityof Science and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia2Computer, Electrical and Mathematical Sciences & Engineering Division(CEMSE), King Abdullah University of Science and Technology, 4700 KAUST, POBox 2882, 23955-6900 Thuwal, Saudi ArabiaFull list of author information is available at the end of the articleformally using ontologies, data integration and compari-son across species presents a major informatics challenge[2]. This task requires that related phenotypes which spanlevels of granularity as well as domains of knowledge, forexample behaviour or neoplasia, in organisms as anatom-ically distinct as zebrafish and man, can be matched andcompared so as to allow findings in one species to berelated to others.In response to this challenge we developed Phe-nomeNET. PhenomeNET [3] was built in 2011 as asystem for disease gene discovery and prioritization.PhenomeNET consists of an ontology integrating species-specific phenotype ontologies based on the PATO ontol-ogy [4] and relations between anatomical structures andphysiological processes, a database of gene-to-phenotypeassociations, and a measure of similarity between setsof phenotypes. Within PhenomeNET, species-specificphenotype ontologies are combined so that phenotypesobserved in different species can be compared directly.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 2 of 11The main application of PhenomeNET is the prioritiza-tion of candidate genes for human diseases by comparinghuman disease phenotypes to existing gene-phenotypeassociations derived from model organisms. In particular,human phenotypes associated with a disease can be com-pared to phenotypes observed in mouse or other modelorganisms using the integrated PhenomeNET ontology,and similarity between phenotypes can then be used toindicate the genetic basis of a disease. PhenomeNET hasbeen successfully used to find candidate genes for diseases[3, 5], identify novel pathways [6], and repurpose drugsusing mouse model phenotypes [7, 8].The PhenomeNET ontology was originally built byformally integrating species-specific phenotype ontolo-gies, permitting the relationship between classes of dif-ferent phenotype ontologies to be determined throughdeductive inference. For this purpose, PhenomeNETrelies on the UBERON [9] ontology that identifiesequivalences between anatomy ontologies of differentspecies, the Gene Ontology (GO) [10] as a meansto identify equivalent or related processes and func-tions, and the PATO ontology [11] to identify the qual-ities associated with anatomical entities or biologicalprocesses.Here, we use the PhenomeNET ontology to identifyalignments between phenotypes in different species. Wepresent our results based on three versions of the Phe-nomeNET ontology: the first version consists of the plainontology using only the axioms provided in the HumanPhenotype Ontology (HPO) [12] and the MammalianPhenotype Ontology (MP) [13]; in the second version, weextend our original ontology by adding additional lexicaland structural mappings generated with the Agreement-MakerLight [14] system and represent them as equiv-alent class axioms in our ontology; and in the thirdversion, we further generate mappings between classesin the PhenomeNET ontology, the Disease Ontology(DO) [15] and the Orphanet Rare Disease Ontology(ORDO) [16].We find that our axiomatic approach can identify alarge number of relations between classes that are notcurrently identified by other systems that do not uti-lize similar formal methods. However, our evaluation alsoshows that a large number of mappings can still be iden-tified through lexical and structural approaches, and thata purely axiomatic approach will miss many mappingsthat cannot currently be identified axiomatically due toincomplete and underspecified formalization of pheno-type classes. We illustrate how a combination of formal,lexical and structural approaches generates the most com-plete and comprehensive mappings between (phenotype)ontologies, and these mappings improve the applicationof phenotype ontologies in data analysis and translationalresearch.MethodsData sources and ontologiesIn our experiments, we use the Human PhenotypeOntology (HPO) [12], Mammalian Phenotype Ontology(MP) [13], Human Disease Ontology (DO) [15], andOrphanet Rare Disease Ontology (ORDO) [17] providedas part of the Ontology Alignment Evaluation Initiative2016 competition.The HPO is an ontology of human phenotypes and con-sists of 11,787 classes that provide a standarized vocabu-lary for describing phenotypic abnormalities which havebeen commonly encountered in human monogenic dis-eases [18]. The MP is mainly used to characterize mousephenotypes, but can also be applied to other organisms. Itconsists of 11,720 classes that have been organized into adirected acyclic graph (DAG) and can be used to describeabnormal phenotypes of physiological and anatomical sys-tems, behavior, and survival [19].DO provides a classification of human diseases accord-ing to multiple axes related to genetic disorders, infectiousdiseases, metabolic disorders. It consists 9247 classes thataim at unifying the representation of human diseasesdefined across a variety of developed biomedical vocabu-laries [20].ORDO is derived from the Orphanet database of rareand orphan diseases and used to represent and catego-rize the diseases within Orphanet. It consists of 12,960classes which provides a structured vocabulary to repre-sent relationships between phenomes, diseases, genes andrelevant features such genetic inheritance for analyzingrare diseases [17].Lexical mappingsWe use the AgreementMakerLight (AML) [21], releasedon 5 April 2016, to generate lexical mappings betweenontologies. We used the automatic match mode of theAML with the default settings to generate the lexical map-pings that were used to extend the PhenomeNet ontology.The default settings of AML include use of the UBERONontology, DO, and Wordnet as background knowledge,a lexical matcher, a word-based matcher that evaluatesoccurrence of the same words in class labels and syno-myms, and a string similarity measure (ISub).In addition to mappings generated by the AML, we alsoincorporate mappings between the ontologies obtainedfrom BioPortal [22]. For each mapping between classesfrom two ontologies, we add an equivalent class axiom tothe PhenomeNET ontology.Semantic similarity and evaluation dataFor additional external evaluation of our generated map-pings, we apply the PhenomeNET ontology to the priori-tization of candidate genes of human disease [3]. We usethe phenotypes associated with knockout mice availableRodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 3 of 11from theMouse Genome Informatics (MGI) database [23]and the phenotypes associated with human diseases fromthe Human Phenotype Ontology database [12]. We applyResniks semantic similarity measure [24] together withthe Best Matching Average strategy [25] to combine classsimilarities.We evaluate the results using a list of genediseaseassociations provided by the Human Phenotype Ontologydatabase [12] as well as a set of mouse models of humandisease provided by the MGI database [23].Source code and experimentsSource code for the PhenomeNET matching system,including parameter files, and the generated alignments,are available at http://github.com/bio-ontology-research-group/OAEI2016. Code to generate the PhenomeNETontology is available at https://github.com/bio-ontology-research-group/phenomeblast/tree/master/fixphenotypes.ResultsCombining knowledge-based and lexical approaches forontology integrationWe developed and extended the PhenomeNET ontologyto integrate several species-specific phenotype ontolo-gies and identify mappings between phenotype classes.Here, we consider a mapping between two classes (intwo ontologies) a formal relation between them, i.e., anaxiomatic relation such as equivalence, sub- or super-class, or disjointness. An alignment between two ontolo-gies is created by a set of mappings. Ontology matchingis the process of finding mappings between classes in twoontologies. Ontology integration goes beyond identifica-tion of an ontology alignment in that two or more ontolo-gies are merged into a single ontology that encompassesall classes in the original ontologies [14].Phenotype classes in the HP and MP ontologiesare formally defined using the Entity-Quality (EQ)pattern [4, 26]. Based on the EQ patterns, a phe-notype is decomposed into an affected entity anda quality that specifies how the entity is affected.The Entity will usually be a class taken either from ananatomy ontology or a physiology ontology. For exam-ple, the phenotype class macroglossia (HP:0000158)describes an anatomical abnormality and is defined asequivalent to has part some (increasedsize and (inheres in some tongue)and(has modifier some abnormal)), relying onthe entity tongue (from the UBERON anatomy ontol-ogy [9]) and the quality increased size (from PATO)in its definition. The class abnormality of salivation(HP:0100755) is a physiological abnormality and isdefined as equivalent to has part some (qualityand (inheres in some saliva secretion)and (has modifier some abnormal)), wheresaliva secretion is a class from the biological processbranch of the Gene Ontology (GO) [10].The general pattern for defining a phenotype class inboth the HP and MP ontologies, given Entity E and Qual-ity Q, is to declare them equivalent to has partsome (Q and inheres in some E). In somecases, the Entity E is further constrained, e.g., by a loca-tion in which a certain process may happen. The Eclasses are generally taken either from the UBERONcross-species anatomy ontology [9] or from the GO. Asthe use of anatomy and physiology ontologies (UBERONand GO) is shared between MP and HP, it is possible tointegrate both ontologies directly, based on the axiom pat-terns used to constrain their classes. However, the type ofaxiom pattern used in both ontologies results in a clas-sification that is primarily based on the PATO ontology,as the Quality Q is the main feature that distinguishesdifferent classes.In the PhenomeNET ontology, we rewrite all axioms inHP andMP using a pattern-based approach that allows usto utilize axioms from anatomy and physiology ontologiesand enrich the classification of phenotype classes [11, 27].In general, we declare phenotype classes defined using anEntity E and Quality Q as equivalent to has partsome (E and has-quality some Q) and we fur-ther add grouping classes that are defined as equivalentto has part some ((part of some E)andhas-quality some Quality). For example, basedon the axiom that defines macroglossia (HP:0000158)as equivalent to has part some (increasedsize and (inheres in some tongue) and(has modifier some abnormal)), we gener-ate two new axioms: macroglossia EquivalentTo: has part some (tongue and has-qualitysome increased size) as well as tongueabnormality EquivalentTo: has partsome((part of some tongue) and has-qualitysome Quality). These two axioms, together with thetransitivity and reflexivity of the part of relation,ensure that macroglossia becomes a subclass of tongueabnormality, and that all phenotypes affecting the tongueor a part of the tongue also become a subclass of tongueabnormality. The aim of rewriting the axioms is tobase the classification of phenotype classes primarily onanatomical or physiological entities instead of the quality,and to utilize the axioms involving parthood in anatomyand physiology ontologies [11, 28]. Crucially, all axiomswe generate fall in the OWL 2 EL profile [29, 30] and allowefficient automated reasoning using optimized OWL 2 ELreasoners such as ELK [31]. The first version of the Phe-nomeNET ontology (PhenomeNET-Plain) consists only ofthese axioms and no additional mappings.In addition to this knowledge-based approach to linkingthe HP and MP ontologies, we also add lexical mappings,Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 4 of 11Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 DOI 10.1186/s13326-017-0166-5REVIEW Open AccessExperiences from the anatomy track inthe ontology alignment evaluation initiativeZlatan Dragisic, Valentina Ivanova, Huanyu Li and Patrick Lambrix*AbstractBackground: One of the longest running tracks in the Ontology Alignment Evaluation Initiative is the Anatomy trackwhich focuses on aligning two anatomy ontologies. The Anatomy track was started in 2005. In 2005 and 2006 the taskin this track was to align the Foundational Model of Anatomy and the OpenGalen Anatomy Model. Since 2007 theontologies used in the track are the Adult Mouse Anatomy and a part of the NCI Thesaurus. Since 2015 the data in theAnatomy track is also used in the Interactive track of the Ontology Alignment Evaluation Initiative.Results: In this paper we focus on the Anatomy track in the years 20072016 and the Anatomy part of the Interactivetrack in 20152016. We describe the data set and the changes it went through during the years as well as thechallenges it poses for ontology alignment systems. Further, we give an overview of all systems that participated inthe track and the techniques they have used. We discuss the performance results of the systems and summarize thegeneral trends.Conclusions: About 50 systems have participated in the Anatomy track. Many different techniques were used. Themost popular matching techniques are string-based strategies and structure-based techniques. Many systems alsouse auxiliary information. The quality of the alignment has increased for the best performing systems since thebeginning of the track and more and more systems check the coherence of the proposed alignment and implementa repair strategy. Further, interacting with an oracle is beneficial.Keywords: Ontology alignment, Biomedical ontologies, Ontology alignment evaluation initiativeBackgroundIn recent years many ontologies have been developed andmany of those contain overlapping information. Knowl-edge of the inter-ontology relationships is important inmany cases. One example case is when we want to usemultiple ontologies, e.g., companies may want to use com-munity standard ontologies and use them together withcompany-specific ontologies. Other example cases areintegration, search and analysis of data in an environ-ment where different data sources in the same domainhave been annotated with different but similar ontolo-gies. It has been realized that this is a major issue andmuch research has been performed on ontology align-ment, i.e., finding mappings or correspondences betweenconcepts and relations in different ontologies [42]. Theresearch field of ontology alignment is very active with its*Correspondence: patrick.lambrix@liu.seDepartment of Computer and Information Science and Swedish e-ScienceResearch Centre, Linköping University, Linköping, Swedenown yearly workshop as well as a yearly event, the Ontol-ogy Alignment Evaluation Initiative (OAEI, http://oaei.ontologymatching.org/, e.g., [41]), that focuses on evalu-ating systems that automatically generate correspondencesuggestions. Many systems have been built and overviewsare found in [87, 99, 123, 144, 145] and at the ontol-ogy matching web site http://www.ontologymatching.org.The proceedings of the yearly Ontology Matching work-shop contain descriptions of the systems participatingin the OAEI as well as summary papers discussing theperformance results for these systems in the OAEI.One of the longest running tracks in the OAEI is theAnatomy track which focuses on two ontologies from thebiomedical domain. This domain is one of the earliestadopters of ontologies and a number of large ontologieshave been developed and are maintained. This domainmanages large volumes of high-complexity data with intri-cate relationships. Focusing on a particular domain allowsthe tools to exploit its inherent properties (for instance,© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 2 of 28it limits the possible meanings of concept labels) and toexploit existing resources as background knowledge. TheAnatomy track was started in 2005. In 2005 and 2006 thetask in this track was to align the Foundational Modelof Anatomy and the OpenGalen Anatomy Model. Since2007 the ontologies used in the track are the Adult MouseAnatomy and a part of the NCI Thesaurus. Since 2015 thedata in the Anatomy track is also used in the Interactivetrack of the OAEI.In this paper we focus on the Anatomy track in theyears 20072016 and the Anatomy part of the Interactivetrack in 20152016. We describe the data set (ontolo-gies and reference alignment) and the changes it wentthrough during the years as well as the challenges itposes in OAEI anatomy data and tasks Section. Fur-ther, we give an overview of all systems that participatedduring these years in the Anatomy track and the tech-niques they have used (Participating systems Section).We discuss the performance results of all systems that par-ticipated during these years in the Anatomy track task 1(Results in the OAEI anatomy track - task 1 Section),tasks 2 and 3 (Results in the OAEI anatomy track - task2 and 3 Section), task 4 (Results in the OAEI anatomytrack - task 4 Section) as well as in the Anatomy partof the Interactive track (Results in the OAEI interac-tive track - anatomy Section). We note that we do notshow all the performance results of the individual sys-tems over the years, but instead summarize the generaltrends. Our paper focuses on the whole period that thetrack was organized and deals with trends and overviewsand multiple systems over the years rather than withindividual systems. For results of the individual systemswe refer to http://oaei.ontologymatching.org/ as well asthe OAEI summary papers1 in the proceedings of theOntology Matching workshops. Further, we summarizeour observations2 and discuss some possible improve-ments and changes for the Anatomy track in ConclusionSection. We start however with some general informationabout ontology alignment and the evaluation of ontologyalignments.Ontology alignment and ontology alignmentevaluationIn this section we give some background on ontologyalignment. We describe a framework for such systems aswell as the measures that are usually used for measuringthe performance of ontology alignment systems.Ontology alignmentMany ontology alignment systems, although not all, arebased on the computation of similarity values betweenentities in different ontologies and can be described asinstantiations of the general framework in Fig. 1. Theframework consists of two parts. The first part (I in Fig. 1)computes correspondence suggestions (sometimes calledmapping suggestions or candidate mappings). The secondalignmentontologiesgeneraldictionaryinstancecorpusdomainthesaurusmatchermatchermatcherPreprocessingcheckerconflictuserIIIaccepted andsuggestionsrejectedfiltercombinationsuggestionsmappingFig. 1 Ontology alignment framework (e.g., [95])Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 3 of 28part (II) interacts with the user to decide on the finalalignment (partly evaluated in the Interactive track). Analignment algorithm receives as input two source ontolo-gies. Part I typically contains different components. Apreprocessing component can be used to modify the orig-inal ontologies, e.g., to partition the ontologies into map-pable parts thereby reducing the search space for findingcorrespondence suggestions. The algorithm can includeseveral matchers that calculate similarities between theentities from the different source ontologies or mappableparts of the ontologies. They often implement string-based, structure-based, constraint-based and instance-based strategies, as well as strategies that use auxiliaryinformation or a combination of these. Correspondencesuggestions are then determined by combining and fil-tering the results generated by one or more matchers.Common combination strategies are the weighted-sumand the maximum-based strategies. The most commonfiltering strategy is the (single) threshold filtering. By usingdifferent preprocessing, matching, combining and filter-ing techniques, we obtain different alignment strategies.The result of part I is a set of correspondence suggestions.In part II the suggestions are then presented to the user, adomain expert, who accepts or rejects them. The acceptedsuggestions are part of the final alignment. In an inter-active system the acceptance and rejection of suggestionsmay also influence further suggestions. Further, in parts I(not in the figure) and II reasoning may be used to checkfor conflicts and incoherence (see below) and the sug-gested alignment (and ontologies) may be repaired. Therecan be several iterations of parts I and II. The output of thealignment algorithm is a set of correspondences betweenentities from the source ontologies.Performance measuresThe performance of the systems in the OAEI has typicallybeen evaluated using measures related to the quality ofthe alignment suggested by the systems (precision, recalland F-measure with respect to a reference alignment) aswell as the run time of the systems. The precision of asystem is the ratio of the number of correctly suggestedcorrespondences by the system to the number of sug-gested correspondences by the system. The recall of asystem is the ratio of the number of correctly suggestedcorrespondences by the system to the number of correctcorrespondences according to the reference alignment. F-measure is a harmonic mean between precision and recalland is defined as:F? = (1 + ?) precision · recall? · precision + recallIn addition to these measures the Anatomy track hasalso computed the recall+ of the systems. As anatomyontologies often contain similar names, even for differentspecies [64], it is expected that a matcher based on stringsimilarity should dowell. Therefore, such amatcher, calledStringEquiv, that combines a normalization step and exactstring matching, was implemented. The resulting correctsuggestions of this matcher were called trivial correspon-dences and used as a baseline for recall+. In the mostrecent reference alignment there are 946 such correspon-dences out of a total of 1516 correspondences. The recall+of a system is the recall of the system on the part of the ref-erence alignment that was not found by StringEquiv andmeasures thus how well the system finds non-trivial cor-respondences. According to this definition the recall+ ofStringEquiv is equal to 03.Anothermeasure is the coherence of the suggested align-ment. An alignment is said to be coherent if the mergedontology containing the original ontologies (in this caseAMA andNCI-A) and the alignment is coherent, i.e., doesnot contain unsatisfiable4 concepts.The data from the Anatomy track is also used in theOAEI Interactive track where a user is simulated using anoracle. In addition to the performance measures above,also the number of requests to the oracle is used.OAEI anatomy data and tasksIn this section we describe the data sets (ontologies andreference alignment) and their histories as well as the tasksin the Anatomy and Interactive tracks of the OAEI, theparticular challenges that this track poses to the alignmentsystems and the evaluation procedure.Ontologies and reference alignmentOntologiesThe Adult Mouse Anatomy ontology (AMA) is a part ofthe Gene Expression Database5 and provides a spatial andfunctional organization of adult mouse anatomical struc-tures6. The National Cancer Institute (NCI) Thesaurus7contains more than 100 000 concepts and covers a broadrange of topics in cancer research and clinical care. In theOAEI we use a fragment of the NCI Thesaurus containinginformation about the human anatomy (NCI-A).In Table 1 we show the evolution of the ontologiesused in the Anatomy track. The 2007 version of AMAcontained 2744 concepts and 3 object properties. It con-tained around 4500 subsumption axioms (is-a relations).NCI-A contained 3304 concepts and 2 object proper-ties. There were around 5500 subsumption axioms. Theknowledge representation language used for both ontolo-gies was ALE . Both ontologies contained a large numberof annotation axioms (AMA - ca 3500, NCI-A - ca 15000).Annotation axioms provide additional information suchas provenance information (e.g., creator and owner). Inthe case of AMA and NCI-A these annotation axiomsincluded properties such as hasSynonym, hasRelatedIDand hasDefinition.Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 4 of 28Table 1 Evolution of AMA and NCI-A and the reference alignmentAMA NCI-A Reference alignment2007 2744 concepts, 3304 concepts, 1544 equivalence relations3 object properties, 2 object properties,ca 4500 subsumption axioms ca 5500 subsumption axioms2008 Same as earlier Same as earlier Removed 20 correspondences2010 Added 12 subsumption axioms Added 3 subsumption axioms Weakened 2 correspondencesRemoved 6 subsumption axioms Removed 3 subsumption axioms Removed 1 correspondenceAdded 17 disjointness axioms2011 Same as earlier Same as earlier Added 28 correspondencesRemoved 24 correspondencesThe ontologies were changed in 2010. In AMA 12new subsumption axioms were added and 6 subsump-tion axioms were removed while in NCI-A 3 subsump-tion axioms were added and 3 subsumption axioms weredeleted. In addition, 17 disjointness axioms were addedto the NCI ontology. This required the more expressiveknowledge representation languageALC for NCI-A.Being developed by different teams and with differentpurposes inmindAMA andNCI-A exhibit different prop-erties with respect to their structure. Table 2 comparesthe 2016 versions of the ontologies used in the Anatomytrack. The ontologies are comparable in number of con-cepts but exhibit a large difference in terms of maximumTable 2 Comparison between AMA and NCI-AAMA NCI-A# of concepts 2744 3304# of direct subconcepts ofowl:Thing1056 7Maximum depth of the is-ahierarchy9 13# equivalent concepts 0 0# of inner concepts 483 674# of leaf concepts 2261 2631Maximum number of directsubconcepts129 125# of concepts with one subconcept 74 125# of concepts with multiplesuperconcepts110 277Average leaves depth(= (sum leaf concepts depth)/(# leaf concepts)):3 6Average depth (= (sum conceptsdepth)/(# concepts)):3 6Average number of subconcepts(only concepts with subconcepts):5 5Average number of subconcepts(all concepts):1 1and average depth of leaf concepts. The AMA structureis flatter and approximately a third of the concepts aredirectly under owl:Thing. NCI-A has a deeper organiza-tion and the average depth of concepts for NCI-A is twiceas large as for AMA. These two ontologies share a largenumber of lexically similar labels.Reference alignmentThe alignment between AMA andNCI-A was undertakenas part of a project to enable linking data between them.The alignment was developed by using automatic toolsas well as a manual approach. As a first step a simplelexical comparison, a preliminary manual comparison bydomain experts as well as an approach combining lexi-cal and structural similarity were used [64]. The lexicalcomponent in the latter approach uses normalization ofterms, exact matching and synonyms from the UnifiedMedical Language System (UMLS)8 Metathesaurus, whilethe structural component is used as a verification stepwhere only correspondence suggestions whichmake sensewith respect to the structure of the ontologies are retained[6]. The results of the first step were manually validatedby domain experts and resulted in 830 correspondences.Further, a number of tools (DAG-OBO-edit [26], Protégé-OWL [124] and COBrA [3]) were selected and used fora further comparative analysis of AMA and NCI-A. Itwas found that most differences between the ontologiescame from design decisions of the hierarchical organiza-tion, the coverage of the ontologies and the granularity ofthe ontologies. Based on this analysis a certain harmoniza-tion and extending of the ontologies was performed. Thisresulted in the versions of the ontologies that were used inthe OAEI, and the initial OAEI reference alignment9 thatcontained 1544 equivalence relations (see Table 1).The reference alignment was modified in 2008 toremove 20 correspondences between concepts whichwere not part of the ontologies. In 2010, the referencealignment was slightly modified by weakening 2 corre-spondences (transforming them into subsumption rela-tions) and removing 1 correspondence. The changes wereDragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 5 of 28done mostly to produce a coherent alignment as with thepre-2010 versions of the ontologies and the pre-2010 ref-erence alignment the merged ontology containing AMA,NCI-A and the reference alignment was incoherent. Thesubsumption correspondences were never used in theevaluations. The latest changes in the reference alignmentwere made in 2011 - 28 correspondences were removedfrom the reference alignment and 24 new correspon-dences were added.In recent years, there have been a number of works,e.g., [5, 64, 71, 97, 98, 102], as well as some personal cor-respondence10 which suggested the existence of missingand wrong is-a relations in the ontologies and missingand wrong correspondences in the reference alignment.However, the evaluation of suchmistakes requires domainexpertise and so far there has not been such an effort afterthe latest changes in 2011.TasksDuring the years different tasks were introduced in thetrack: Task 1: Align AMA and NCI-A and optimizeF-measure. Task 2: Align AMA and NCI-A and optimizeF-measure with a focus on precision. Task 3: Align AMA and NCI-A and optimizeF-measure with a focus on recall. Task 4: Given a partial reference alignment consistingof all trivial correspondences and 50 non-trivialcorrespondences, align AMA and NCI-A andoptimize F-measure. Interactive track: Using an oracle (which may makemistakes), align AMA and NCI-A and optimizeF-measure.In the definition of F-measure, tasks 1, 4 and the inter-active track use ? = 1, while task 2 uses ? = 5 and task 3uses ? = 0.2.Task 1 has been used in all editions of the OAEIAnatomy track (20072016). Tasks 2 and 3 were part ofthe track during 20072010, while task 4 was included in2008201011. Since 2011 the coherence of the suggestedalignment is checked. Tasks 14 deal mainly with the non-interactive part of an ontology alignment system (part I inFig. 1).Since 2015 the data from the Anatomy track is usedin the OAEI Interactive track (run since 2013) whichaim is to evaluate the influence of user involvement forinteractive alignment tools. It is a first12 step towards anevaluation of part II in Fig. 1. In the track users are repre-sented by an oracle and tools can ask the oracle about thecorrectness of correspondence suggestions and use thisinformation in the generation of other correspondencesuggestions.ChallengesIn the early years the Anatomy track contained the largestontologies and was therefore the track that evaluated scal-ability of the systems. Nowadays, these ontologies areconsidered to be medium-sized.As the two ontologies share a large number of lexi-cally similar labels, string matching-based algorithms doquite well. Therefore, most systems use such algorithms.The challenge is, however, to combine these kinds ofmatchers with other types of matchers to improve theresults. Therefore, StringEquiv was used as a baselinematcher to measure the influence of the other types ofmatchers. Combining matchers in an effective way isnot easy and several systems did perform worse thanStringEquiv.As shown in Table 2 the is-a structure of the twoontologies is quite different. One challenge is, therefore,to develop structure-based approaches that can deal withdifferent is-a structure and granularity.The track allows the use of background information.Systems need to find appropriate external sources and usethem effectively. These external sources may be domainspecific or contain general information. The sources mayalso be incomplete and contain errors.Task 4 was the only task in any of the OAEI tracksthat evaluated the use of a given partial reference align-ment in the computation of new correspondence sug-gestions. The partial reference alignment could be usedin the preprocessing, computation or filtering compo-nents of the systems and new strategies needed to bedeveloped. Task 4 was, however, a difficult task. As thetrivial correspondences are given, string-based match-ing does not give an improvement. Further, given thefact that the partial reference alignment contains onlya few non-trivial correspondences, machine learning-based matchers are likely to fail. As the is-a struc-ture of AMA and NCI-A is not complete, structure-based approaches can also not be used to their fullpotential.In the Interactive track there are several challenges.The first challenge is to develop strategies for decidingwhich correspondence suggestions to show to the oracle.These questions should be important for the quality of thefinal alignment. However, there should not be so manyquestions as to overload the oracle. There should also benot too much waiting time between the questions. Thenstrategies for using the validation decisions of the oracleshould be developed. This is similar to task 4, but in thiscase the system has decided which correspondences couldbe part of the partial reference alignment and addition-ally, there are also validation decisions about non-correctcorrespondences. A further challenge in this track is thatthe systems need to deal with an oracle that may makemistakes.Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 6 of 28Evaluation procedureIn the period 20072010 the full reference alignment wasnot publicly available and all tests were done blind. Theauthors of the tools were provided with the ontologiesand were asked to produce an alignment which was thensent to the organizers of the track. The organizers wouldthen evaluate and compare the performance of the tools.In 2010 the SEALS platform13 was introduced in the eval-uation process for the Anatomy track. SEALS providesan evaluation framework where participants register andupload their tools to the portal. While the reference align-ment was still not available, the tools could be run throughSEALS and the results for the tool would be directly avail-able. The use of SEALS also meant that the organizerscould publish certain tests while keeping other tests blind.In addition to receiving the results directly, the fact thatthe tools were required to be uploaded made it possible torun all tools on a single hardware which made the com-parison of run times possible. Since 2011 the referencealignment has been publicly available.Initially, the authors of the tools could decide in whichtrack to participate, which made it possible to have spe-cialized tools for certain type of task, e.g., matchingbiomedical ontologies. However, from 2011 all tools areevaluated in all tracks.Participating systemsIn this section we give an overview of the participation inthe Anatomy track and discuss the techniques used by thedifferent systems.ParticipationIn total 50 different tools (not including different versionsof the tools) have been evaluated from 2007 to 2016 in theAnatomy track. The numbers of participants for specificyears is given in Table 3. During 20072011 around 10tools participated each year. During 20122016 the num-ber of participants has varied from 20 tools in 2013 to 10tools in 2015.Tables 4 and 5 show the participants and the years inwhich they participated. The table lists only the participa-tions in the Anatomy track. During the years that systemswere allowed to choose tracks, some systems may havechosen to participate in Anatomy during some years, andnot during other years. The latter are not taken up in thetable. Further, we only mark a participation in the caseof a successful evaluation, i.e, the system returned resultswithin the for that year predefined time frame.Half of the systems has participated more than once.The tools with the most participations (6) are Lilyand LogMap. Seven tools have participated 4 times,6 tools 3 times and 10 tools twice. In the recentinstances of the track we can observe an increase intools which participate with different versions, such asTable 3 Number of participating systems in the OAEI Anatomytrack during 20072016Year Number of distinct tools Number of tools includingdifferent versions2007 11 112008 8 92009 10 102010 10 102011 10 112012 14 172013 16 202014 5 102015 11 152016 10 13lightweight versions or versions which use backgroundknowledge.Alignment techniquesFor the overview of the systems in this section we usedthe papers describing the systems in the OAEI parts of theproceedings of the yearly Ontology Matching workshop.In the case we needed some clarifications we have alsolooked at the papers referenced in the OAEI papers. Forthe overview of string-based matchers we also used [10].We note that some of the participants in the earlier years,may have newer versions of the systems that have featuresthat are not discussed in this paper.In Table 6 we show the different components of theparticipating systems. All systems implement part I whilesome also implement part II and allow iterations. Manysystems do some kind of preprocessing. In most of thecases the preprocessing step deals with preparing datafor the matchers. In other cases the systems partitionthe ontologies to reduce the search space for the match-ers. All systems have a matching component and these arediscussed shortly. The combination strategies are usu-ally weighted sum (most common) or maximum-basedapproaches. Some systems use a more advanced approachwhere the weights for the weighted sum are selected usinga neural network (CIDER-CL, X-SOM, XMAP) or a geneticalgorithm (XMAPGen), using the overlap between theresults of the different matchers (CroMatcher), or using aclustering algorithm (CSA). Most filtering is performedusing a single threshold. SAMBOdtf and X-SOM use adouble threshold filtering approach where the correspon-dences with similarity values between the thresholds arechecked with respect to the structure of the ontologies,or are requested to be validated by a user, respectively.Lily uses a maximum entropy approach to calculate aDragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 7 of 28Table 4 Participating systems (with different versions) in the OAEI Anatomy track 20072016 (part 1)System 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016AgreementMaker [14] [147] [15] [13] [16]ALIN [56] [19]AML, AML_bk2013 [50, 51] [49] [47] [46] [48]Anchor-Flood [143] [141] [142]AOAS [6, 169] [168]AOT, [NA]AOTL [91]AROMA [22] [23] [24] [25] ASMOV [77] [74] [75] [76] [78]BLOOMS [NA] [129]CIDER-CL [155] [53]CODI [121] [122] [69] COMMAND [113] CroMatcher [58] [57] [59]CSA [NA] [154]DKP-AOM, [45]DKP-AOM-Lite [43] [44]DSSim [114] [115] [117] [116]Eff2Match [NA] [12]Falcon-AO[67] [68]FCA_Map[174] [173]GeRoMeSuite+SMB [89] [130]GMap [104] [105]GOMMA, [92]GOMMA-bk  [55] Hertuda [NA] [65] HotMatch [NA] [21] IAMA [NA] [172]Burek et al. Journal of Biomedical Semantics  (2017) 8:48 DOI 10.1186/s13326-017-0152-yRESEARCH Open AccessTowards refactoring the MolecularFunction Ontology with a UML profile forfunction modelingPatryk Burek1, Frank Loebe2* and Heinrich Herre1AbstractBackground: Gene Ontology (GO) is the largest resource for cataloging gene products. This resource grows steadilyand, naturally, this growth raises issues regarding the structure of the ontology. Moreover, modeling and refactoringlarge ontologies such as GO is generally far from being simple, as a whole as well as when focusing on certain aspectsor fragments. It seems that human-friendly graphical modeling languages such as the Unified Modeling Language(UML) could be helpful in connection with these tasks.Results: We investigate the use of UML for making the structural organization of the Molecular Function Ontology(MFO), a sub-ontology of GO, more explicit. More precisely, we present a UML dialect, called the Function ModelingLanguage (FueL), which is suited for capturing functions in an ontologically founded way. FueL is equipped, amongother features, with language elements that arise from studying patterns of subsumption between functions. Weshow how to use this UML dialect for capturing the structure of molecular functions. Furthermore, we propose anddiscuss some refactoring options concerning fragments of MFO.Conclusions: FueL enables the systematic, graphical representation of functions and their interrelations, includingmaking information explicit that is currently either implicit in MFO or is mainly captured in textual descriptions.Moreover, the considered subsumption patterns lend themselves to the methodical analysis of refactoring optionswith respect to MFO. On this basis we argue that the approach can increase the comprehensibility of the structure ofMFO for humans and can support communication, for example, during revision and further development.Keywords: Gene Ontology, Molecular Function Ontology, Unified Modeling Language, Ontology, Functiondecomposition, Intensional subsumptionBackgroundGene Ontology (GO) [1, 2] is an important, widely used,very large and continuously growing resource for cata-loging gene products. In 2000 GO contained less than5000 terms, which increased to circa 13,000 in 2003[1], exceeded 30,000 in 2010 [3] and is close to 45,000terms in July 2017 [2]. The Molecular Function Ontol-ogy (MFO) is a sub-ontology of GO of more than 11,000terms in 2017. This growth of the ontology leads to a sub-optimal structure [3]. Clearly, the GO Consortium itselfis constantly improving and evolving GO. In addition,*Correspondence: frank.loebe@informatik.uni-leipzig.de2Computer Science Institute, University of Leipzig, Augustusplatz 10, 04109Leipzig, GermanyFull list of author information is available at the end of the articlesize and importance of the ontology and the recogni-tion of problems have motivated refactoring initiatives,see [4, 5], for example. Overall, it turns out that modelingand refactoring large ontologies such as GO are diffi-cult tasks, which should be supported by human-friendlyrepresentations. Serialization formats used for machineprocessing of ontologies, such as the OBO flat file for-mat [6] or the Web Ontology Language (OWL) [7], arenot the easiest to be used by humans. This motivatesproposing the adoption of human-friendly graphical nota-tions for certain purposes, like languages used in softwareengineering, already employed for the task of ontologyrepresentation [8, 9].The Unified Modeling Language (UML) [10, 11], devel-oped and maintained by the Object Management Group© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 2 of 11(OMG) [12], is the de facto standard for graphical con-ceptual modeling of software systems. Moreover, UMLhas a high potential for various applications that gobeyond software engineering, among them modelingbiological knowledge and biological ontologies [4, 13],for several reasons. First, there is a rich infrastruc-ture. Numerous tools for UML modeling are availableon the market and can be used out of the box forvisualizing biological ontologies as a whole or in part.Another advantage is its adaptability. UML is equippedwith extension mechanisms such as stereotypes and pro-files, which support the easy construction of domain-or task-specific UML dialects. For example, a UMLprofile for the OBO relations ontology is proposedin [4].In the present paper we investigate if UML, moreprecisely, a dedicated dialect, can be utilized for mak-ing the structure of the Molecular Function Ontol-ogy more explicit and if it can support the refactor-ing of MFO. The focus on MFO within GO resultsfrom having dealt with the notion of function from ageneral point of view in earlier work, e.g. [14]. TheMethods section establishes foundations by sketchingsome features of functions in MFO, describing an inten-sional understanding of subsumption, and introducing theFunction Modeling Language (FueL) as a UML dialectthat is suited for function modeling. Concerning results,section Modeling molecular functions with FueL intro-duces core elements of FueL that are required to analyzethe subsumption patterns that section Patterns of func-tion subsumption defines based on FueL. Then we areprepared to illustrate the application of FueL to MFOin the Application section by modeling the structureof molecular functions and proposing some refactoringoptions. The Discussion section is mainly devoted torelated work and to the applicability of FueL at this stage.It further indicates directions of future work, before thepaper ends with section Conclusions.MethodsMolecular Function OntologyLike all GO terms, functions in MFO are specified byid, name, natural language definition and an optionallist of synonyms. For instance, the function of catalyz-ing carbohydrate transmembrane transport is specifiedby id: GO:0015144; name: carbohydrate transmembranetransporter activity; definition: catalysis of the transfer ofcarbohydrate from one side of the membrane to the other;synonym: sugar transporter. Additionally, for each func-tion its relations with other concepts can be captured. Thesemantics of the relations that are used for this purposeis provided by serialization languages such as the OBOflat file format or OWL, and/or by the OBO relationsontology (RO) [15]. In particular, functions in MFO areorganized into a hierarchy by means of the is_a link fromRO; furthermore, they are linked with processes by thepart_of relationship from RO; and in some cases they haverelations with concepts of other ontologies such as ChEBI[16]. For instance, GO:0015144 is linked, by means of theRO is_a relation, to its parent functions GO:1901476 car-bohydrate transporter activity and GO:0022891 substrate-specific transmembrane transporter activity, by meansof the RO part_of relation to the process GO:0034219:carbohydrate transmembrane transport, and by meansof the RO transports_or_maintains_localization_of toCHEBI:16646: carbohydrate.From the above we see that the semantics of functionsin MFO is provided to a large extent by informal naturallanguage expressions and partially by relations with otherconcepts.Intensional subsumptionWe propose defining the notion of function subsump-tion, which is a backbone of MFO, upon an intensionalinterpretation of the is_a relation. Typically, in the fieldof ontology engineering the extensional aspect of the is_arelation is stressed; in OWL, for instance, A is a sub-class of B if every instance of A is an instance of B. Thesame interpretation is used in RO, where is_a is definedby the reference to the sets of all instances (extensions)of the concepts. According to this understanding the is_arelation is often called extensional subsumption, in con-trast to its intensional counterpart(s), where we focus onstructural subsumption [17].Instead of referring to instances, structural subsumptionis defined based on the structure of a concept. The lattercan be understood as a composition of conceptual partsby means of various composing relations. For illustrationwithin GO itself, GO:0005215: transporter activity is jus-tified to intensionally subsume GO:0022857: transmem-brane transporter activity, because, following [17], bothare activities and they are (partially) defined by part_ofrelations to GO:0006810: transport and to GO:0055085:transmembrane transport, resp., and the latter is sub-sumed by the former. Overall, the main assumption is thatconcepts are complex structures which can be organizedinto a subsumption hierarchy. The reading of intensionalsubsumption is similar to inheritance in object-orientedlanguages, where one class inherits its structure fromanother. That enables the structuring of classes into hier-archies. Note that extensional and intensional subsump-tion need not be seen to be in conflict with each other,but they can be understood as different facets of thehierarchical organization of classes.UML, UML profiles and FueLThe Unified Modeling Language (UML) [10, 11] is arich graphical modeling language developed originallyBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 3 of 11for the support of software engineering. Currently, itsapplications go beyond software engineering, coveringa broad spectrum of domains, including systems andenterprise modeling, as well as biological systems mod-eling. The language is founded on an explicit distinctionbetween the static and the dynamic views of a system.It introduces thirteen diagram types, grouped into twosets: structural modeling diagrams and behavioral model-ing diagrams. UML lacks constructs dedicated to functionmodeling as such [18], but it provides several built-in mechanisms that allow for an easy extension of thelanguage.Among these extension mechanisms there are UMLprofiles. A profile is a light-weight UML mechanism,typically used for extending the language for par-ticular platforms, domains or tasks [11, ch. 12]. Itspecifies a set of extensions of the UML standardmetamodel which include, among others, stereotypes.With stereotypes it is possible to extend the stan-dard UML vocabulary with new, specialized model ele-ments. A stereotype can be graphically representedby a dedicated icon, though in the most straightfor-ward form it is represented simply by a stereotypename, surrounded by guillemets and placed above thename of the stereotyped UML element, cf. «Function»in Fig. 1.We used the profile mechanism for developing a UMLextension, called Function Modeling Language (FueL)1,aimed at supporting the modeling of functions, func-tion ascription, and function decomposition. FueL defines15 stereotypes for representing functions and functionstructure, as well as 8 stereotypes for modeling functiondecomposition, subsumption and function dependencies.The full specification of FueL stereotypes is available in[19]. Burek et al. [18] provides a detailed introductionto FueL, based on requirements for function model-ing derived from an elaborate review of correspondingliterature, in general, as well as of UML modeling con-structs related to functions, in particular. In additionto the profile, [18] comprises an axiomatic characteri-zation of the core elements of FueL and discusses itssuitability for function modeling with respect to therequirements identified.In the remainder of the current paper we analyze towhich extent FueL can be used for modeling and refactor-ingMFO. As a prerequisite for this analysis, we begin witha condensed account of FueL.ResultsModeling molecular functions with FueLFueL enables the graphical modeling of functions both ina compact and in an extended form. The compact formis particularly suited for large models containing manyfunctions, whereas the extended form is designed for visu-alizing the dependencies within the structure of a singlefunction or between several functions. Figures 1 and 2present an exemplary FueL model, depicting the structureof MFO function GO:0015144: carbohydrate transmem-brane transporter activity. Figure 1 presents the compactnotation, whereas the extended notation is shown in Fig. 2.The stereotypes utilized in the figures are discussed in theremainder of this section.FunctionsA function in FueL is understood as a role that an entityplays in the context of some goal achievement, e.g. ina teleological process. Put differently, a role in virtueof which the transition to a goal situation is achieved,or which contributes to such achievement, constitutes afunction. An entity, like putative glucose uptake protein inFig. 2, that plays such a role has that role as its function.This account of functions is similar to [20], where a bio-logical function of a molecule is described as the role thatthe molecule plays in a biological process. In this sense,the function GO:0015144: carbohydrate transmembranetransporter activity, defined in GO as catalysis of thetransfer of carbohydrate from one side of the membraneto the other, depicts the catalyst role in the teleologicalprocess of transferring carbohydrate from one side of themembrane to the other.In terms of the structure we can therefore say that afunction specification contains as its part a specificationof a goal achievement, understood as a teleological entitywhich is specified in terms of a transformation from aninput situation to an output situation. As presented inFig. 1 A FueL model of a molecular function, displayed in the compact notationBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 4 of 11Fig. 2 A FueL model of a molecular function, displayed in the extended notationFigs. 1 and 2, a function is depicted by a UML classi-fier with a stereotype «Function». It connects to its goalachievement by an association with a stereotype «has-goal-achievement» in the extended notation, whereas thecompact notation utilizes the attribute goal_achievement.Goal achievementsIn FueL, a goal achievement (GA) is defined as a teleo-logical transition, i.e., as a transition to a certain outputsituation (the goal). Note that transitions further exhibitan input situation. The GA characterization applies atboth the individual and categorial level. With respect tothe latter, input and output are defined as follows: The input category x of goal achievement y is asituation category such that every instance of y is atransition starting from a situation instantiating x. The output x of goal achievement y is a situationcategory specifying the situations in which instancesof y result by transition. Every instance of y is atransition resulting in a situation instantiating x.For example, the goal achievement (category) carbo-hydrate transmembrane transport establishes the inputcategory, the instances of which are situations of carbo-hydrate being on one of the two sides of the membrane,and the output category, the instances of which are sit-uations of carbohydrate being on the other side of themembrane. This means that every instance of carbohy-drate transmembrane transport exhibits a transition froman instance of the input category to an instance of theoutput category, i.e. from individual situations of carbohy-drate located on one side of the membrane, to individualsituations of carbohydrate located on the other side of themembrane.In the compact notation, the input is captured by theinput attribute of a function, see Fig. 1. In contrast, Fig. 2illustrates that an association with stereotype «has-input»is used for connecting a function with its input in theextended notation. The representation of outputs is anal-ogous in both variants.Typically, a transformation from an input to an outputsituation is a process. At the categorial level, the GA canthen be understood as a process category. In the runningexample, the GA is a teleological process category, namelyof carbohydrate transfer from one side of the membraneto the other. This process exhibits the causal transitionfrom the situation of carbohydrate being on one side ofthe membrane to the situation where carbohydrate is onthe other side of the membrane.Mode of goal achievementIn some cases the specification of a function is notreduced to a mere input-output pair, but it defines con-straints on the method of function realization. For exam-ple, the molecular functions GO:0015399: primary activetransmembrane transporter activity and GO:0015291: sec-ondary active transmembrane transporter activity sharethe same input: solute is on one side of the membrane,and the same output: solute is on the other side of themembrane. Therefore, the pure input-output views ofthe functions are equal. However, they are distinct due tothe way in which they achieve the goal. The former func-tion is realized by means of some primary energy source,for instance, a chemical, electrical or solar source, whereasthe latter relies on a uniporter, symporter or antiporterprotein. Thus we see that the functions provide the sameanswer to the question on what is to be achieved, how-ever they provide different answers on how that is realized.Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 5 of 11In order to represent this distinction, in FueL we intro-duce another component of function structure, calledMode of Goal Achievement (or Mode of Realization). Themode x of the goal achievement y specifies the way inwhich y transforms the input to the output situation. ForGO:0015399 the mode is: by some primary energy source,for instance chemical, electrical or solar source, and forGO:0015291 it is: by uniporter, symporter or antiporterprotein. The mode is a constraint on the function realiza-tion, which does not affect the input or the output. Forexample, if one adds to the function of transmembranetransport the constraint that the transport should be real-ized by the uniporter protein, then the input and theoutput remain unchanged. However, the function as suchchanges in that not every transportation process realizesit, but only those that are driven by a uniporter protein.ParticipantsOften goal achievements are expressed by action sen-tences of natural language and thus the results of linguisticanalysis of action sentences can be applied to the analysisof the structure of goal achievements. In linguistics, therole that a noun phrase plays with respect to the actionor state described by the verb of a sentence is called athematic role [21]. The specifications of molecular func-tions in MFO often contain two thematic roles  a patient(called an operand in FueL) and an actor (called a doerin FueL). An operand indicates the entity undergoing theeffect of the action. At the categorial level we say thatan operand y of the goal achievement x specifies a cate-gory y such that instances of x operate on instances of y.GO:0015144 operates on (transports) carbohydrate.A doer is not as common in MFO as an operand. Forexample, in the discussed carbohydrate transmembranetransport function no doer is indicated. Typically, a doeris a part of the GA in cases where the mode of realizationis provided. For instance, the functions GO:0015292: uni-porter activity and GO:0015293: symporter activity bothspecify the mode of realization and each indicates its doer,namely the respective protein.Patterns of function subsumptionBehind function subsumption various distinct relationsare actually implicitly hidden [14]. In this section weintroduce three patterns for function subsumption thatcan be indicated by FueL stereotypes [19]. The subse-quent Application section demonstrates the applicationof those patterns to the modeling of MFO.In FueL the notion of function subsumption is foundedon the subsumption of goal achievements. We say thatthe function x is subsumed by the function y if the goalachievement of x is subsumed by the goal achievement ofy. Since goal achievements are quite complex entities, itis not trivial to answer the question of what it means thatone goal achievement subsumes another. Here, however,the analysis of GA structure is helpful, which pertains tothe intensional aspects of the corresponding GA category,as discussed in previous sections. Based on this approachone can detect various patterns of function subsumption.Operand specializationSince function specifications often contain operands, it isvery common to construct a hierarchy of functions on thebasis of the taxonomic hierarchy of their operands. In fact,this pattern is applied frequently in MFO. Consider, forinstance, the functions GO:0015075: ion transmembranetransporter activity and GO:0008324: cation transmem-brane transporter activity, linked by the is_a relation inGO. As presented in Fig. 3 the relation between those twofunctions is based on the relation of their operands, ascation is subsumed by ion.Function subsumption by operand specialization isdepicted in FueL with a specialization link with the stereo-type «operand-spec». The supplier of the link is thesubsumed function, the client is the subsumer.Mode additionAnother pattern of function subsumption, frequently metin MFO, is based on modes of goal achievement. Considertwo functions presented in Fig. 4, GO:0022857: trans-membrane transporter activity and GO:0022804: activetransmembrane transporter activity. Both share the sameoperand, namely substance, as well as the same input-output pair  operand is on one side of the membraneand operand is on the other side of the membrane. Inthis sense those functions are equal. However, they dif-fer in that the former does not define any mode ofrealization, whereas the latter has the following modedefined: the transporter binding the solute undergoes aFig. 3 An example of operand specializationBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 6 of 11Fig. 4 An example of specialization by mode additionseries of conformational changes. Therefore, one can saythat GO:0022804 specializes GO:0022857 by addition of amode. We say that function x is subsumed by the functiony by mode addition if x is subsumed by y and x has somemode, whereas y has no mode assigned. Function sub-sumption by mode addition is depicted in FueL by meansof a specialization link with stereotype «mode-added».The subsumed function is the supplier of the link and thesubsuming function is a client.Mode specializationSubsumption of functions can be based on the modeof realization also in cases where a parent function hasalready a mode assigned. Consider, for instance, thefunction GO:0022804: active transmembrane transporteractivity having the mode: transporter binds the soluteand undergoes a series of conformational changes and thefunction GO:0015291: secondary active transmembranetransporter activity with the mode: transporter binds thesolute and undergoes a series of conformational changesdriven by chemiosmotic energy sources, including uni-port, symport or antiport. The latter clearly character-izes particular modes of active transmembrane transport.Consequently, it seems intuitive to say that GO:0015291specializes GO:0022804 (as is the case in GO).We call thistype of function subsumption the subsumption by modespecialization and define it as follows: The function x issubsumed by the function y by mode specialization if x issubsumed by y and mode r of x specializes mode s of y.In FueL function subsumption by mode specialization isdepicted with a specialization link with stereotype «mode-spec». The subsumed function is the supplier of the linkand the specialized function is a client.ApplicationObjectives of applying FueLIn general, graphical modeling languages like UML arebroadly applied in connection with diverse tasks, such asbrainstorming, collaborative design, and the modeling ofkey principles of systems and subject matters. Anotherbroad area of application concerns standardized visualiza-tion, for example, for documentation purposes.Regarding FueL more specifically, its application toGO and MFO, in particular, pursues three objectives.The first objective is the use of FueL for establish-ing a semantic basis for molecular functions that sup-ports the representation of functions in a systematicway, beyond their textual description. Moreover, the dis-cussed patterns represent basic knowledge of the inter-relations between biological processes and molecularfunctions. The part_of relation between biological pro-cesses and molecular functions can be mapped to the has-goal-achievement association between functions and goalachievements. Figure 2 comprises a corresponding exam-ple, where the process GO:0034219: carbohydrate trans-membrane transport is modeled as a goal achievement ofthe function GO:0015144: carbohydrate transmembranetransporter activity.The second and the main objective of applying FueLto MFO is to explicitly document design choices and thesubsumption patterns utilized implicitly in MFO. Figure 5presents such a documentation of a fragment of MFO interms of FueL. The patterns are indicated by the FueLstereotypes, which enables an easy-to-grasp visualizationof the structure of MFO as well as of the underlyingdesign choices. Stereotypes further allow for displayingmultiple facets of function subsumption, as in the case ofGO:0022804, which can be understood to involve modeaddition as well as operand specialization. The explicitspecification of design choices makes the ontology muchmore intelligible for human users, which is a major benefitof this approach.Thirdly, the application of FueL reveals potential forthe refactoring and revision of GO. Contributing tothe latter is another important objective of our work.For instance, the application of FueL in modeling thefunctions GO:0022857: transmembrane transporter activ-ity and GO:0022891: substrate-specific transmembraneBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 7 of 11Fig. 5 A segment of MFO modeled with FueLtransporter activity shows that both share similar goalachievements: transfer of an operand from one side of amembrane to the other, with input: operand is on oneside of the membrane, and output: operand is on theother side of the membrane. Consequently and follow-ing FueL, a potential difference between GO:0022857 andGO:0022891 can be searched for in their operands. ForGO:0022857 that is a substance, whereas for GO:0022891it is a specific substance or group of substances.Analysis of refactoring optionsLet us consider the previous case in greater detail, therebyidentifying three possibilities of analyzing and refactor-ing MFO elements based on FueL. A first FueL viewon a selected set of functions that includes the two justnamed is depicted in Fig. 5. It rests on the assumptionthat a specific substance or group of substances canbe considered as a subclass of a substance. Accordingly,Fig. 5 documents explicitly the pattern of subsumptionbetween GO:0022857 and GO:0022891, namely as a caseof operand specialization. The same aspect applies toGO:0022804, the operand of which is also a specificsubstance or group of substances.This straightforward approach, however, may be recon-sidered, especially the question of what the actual rela-tion between a substance and a specific substance orgroup of substances is. One indication may be derivedfrom GO:0022892: substrate-specific transporter activity(not displayed in Fig. 5), which is another parent func-tion of GO:0022891 in MFO. An operand of GO:0022892is exemplified by macromolecules, small molecules orions. If we thus interpret a specific substance or groupof substances as macromolecules, small molecules orions, this seems to suggest that further functions such asGO:0090482: vitamin transmembrane transporter activityand GO:0015238: drug transmembrane transporter activ-ity should also be considered as subclasses of substrate-specific transmembrane transporter activity. The latteris currently not the case in MFO, such that positioningthose functions under GO:0022891 is a refactoring option,independently of adopting FueL as a representation lan-guage. If FueL is employed, these considerations yieldBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 8 of 11an alternative to Fig. 5 (not shown in a separate figure),where, for instance, GO:0090482 is an operand specializa-tion of GO:0022891 instead of GO:0022857. GO:0022804,based on its operand identical to that of GO:0022891,would turn into a specialization of the latter by modeaddition.Another possible refactoring originates from an analysisof the subclasses of GO:0022891: substrate-specific trans-membrane transporter activity. Examining those sub-classes we find that they differ only in their operands.Each of those functions specifies the transport of a spe-cific kind of substance, for example, ion (GO:0015075)or carbohydrate (GO:0015144). This suggests that thedistinction between the operands of GO:0022857 andGO:0022891 is only superficial. According to this inter-pretation, GO:0022891 is merely used for the organizationof the function taxonomy, i.e., for grouping all functionsthat are distinguished by their operands. GO:0022891would then be a duplication of GO:0022857, which is onlyintroduced into MFO for structuring purposes, but whichcaptures no distinct specification of a biological function.The introduction of such grouping artifacts is a designchoice that is clearly not desirable, especially in com-plex ontologies like MFO or GO overall. One reason foravoiding them is that in many cases of using them sub-classes occur after several steps of specialization that donot or not exactly match the grouping specification. Forexample, GO:0005402: cation:sugar symporter activity inFig. 5 may be questioned to be a (pure) substrate-specifictransmembrane transporter activity, given the subsump-tion path via GO:0022804 involving mode addition andmode specialization.Concerning the purpose of better organization of thetaxonomy, we argue that FueL proves beneficial, not atleast due to its stereotyped links. As illustrated in Fig. 6,the application of FueL allows for dropping GO:0022891(if interpreted as a grouping artifact), on the one hand,while on the other hand, FueL enables the explicit spec-ification of design choices by stereotyped specializationlinks. Note that this supports the local grouping of theimmediate, explicit subclasses of a given function basedon the link stereotypes.The decision on such refactoring options, as in anymodeling enterprise, is the responsibility of the mod-eler(s), i.e., GO developers in our case. Regarding refactor-ing means and methods, however, we argue that the aboveanalysis demonstrates how graphical languages such asFueL, similarly as in software and systems engineering,can drive and support the revision of biological ontologieslike MFO. Although graphical modeling may not be effi-cient for representing the complete content of large andcomplex ontologies, we defend the position that graphi-cal languages can still be extremely helpful, for example,for depicting ontology fragments that exhibit problems.Moreover, in view of ontology development as a col-laborative enterprise, graphical modeling formalisms likeFueL help to conduct community based analysis in struc-tured ways.Fig. 6 A refactoring of the segment of MFO in Fig. 5Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 9 of 11DiscussionThe ideas underlying the structure of functions, intro-duced in FueL, are the result of an analysis of the currentstate of the art of function modeling in software, systemsand ontological engineering. For instance, the interpreta-tion of a function in terms of a role is common not only inbiological systems [20], but also in function modeling inmechanical engineering [2224].The notion of goal achievement grasps the teleologicalcharacter of a function, its orientation towards some goal.This aspect is stressed in many approaches to functionrepresentation, e.g. [2527]. In particular, defining a func-tion in terms of input-output pairs is present in modelingtechnical artifacts [28, 29]. The mode of realization, alsocalled the way-of-function-achievement, which specifiesconstraints on the method of how a function is realized,can be found in [30], among others.To the best of our knowledge, the presented patterns offunction decomposition are not collected and integratedinto any other single modeling framework, though thetechniques themselves are commonly used, especially insoftware and systems engineering, e.g. see the function-means-context link in [31] or the decomposition withzig-zaging in [32].Another aspect worth of discussion is the practicalapplicability of the proposed approach, in particular, withrespect to GO and its Molecular Function Ontology. Inthis connection it appears realistic to admit that the mereexistence of FueL as a UML profile does not render theapproach ready for an immediate, production-level adop-tion in the day-to-day curation of function terms in MFO.The tool set capable of handling MFO (and of GO over-all), for example, in terms of its size and in accordancewith its recent turn to its representation in OWL exhibitsbasically no connection to the world of UML and corre-sponding modeling tools. Insofar the direct application ofFueL involves bridging this gapmanually, which is limitingto small-scale, focused case analyses at the present stage.Nevertheless, we think that the detailed discussion ofrefactoring options in the previous section illustrates theutility of such analyses. There is a significant potential inview of the fact that, clearly, many more exemplary orspecific cases can and should be made based on MFO.For instance, analyzing the terms GO:0016209: antioxi-dant activity and GO:0003824: catalytic activity togetherwith their subclasses systematically, some of which theyshare, one may raise the question of why GO:0004601:peroxidase activity specializes antioxidant activity, but isnot subsumed by catalytic activity, despite the definitionof GO:0004601, which starts with Catalysis of reaction:donor + [...]. Moreover, there are various groups of termsof the form X regulator activity, X activator activityand X inhibitor activity, at different levels of generality(e.g., cf. receptor vs. acetylcholine receptor for X). Suchgroups may justify a novel, common pattern of functionsubsumption, namely based on the output of the corre-sponding goal achievement. One further finds that goalachievements are not yet present in GO in a numberof cases, i.e., there are no processes corresponding toavailable functions.Further analysis of MFO terms on the basis of FueLconstituents such as operands and modes leads to theidentification of functions that are specialized (1) almostexclusively by mode additions or mode specializations,whereas the subclasses of others (2) primarily relyon operand specialization. GO:0009055: electron car-rier activity may serve as an example of the formercase. At least seven out of its eight direct is_a childrenclearly arise through mode addition or specialization,e.g., GO:0045154: electron transporter, transferring elec-trons within cytochrome c oxidase complex activity andGO:0045156: electron transporter, transferring electronswithin the cyclic electron transport pathway of photosyn-thesis activity. In contrast, GO:0004872: receptor activ-ity has seven direct subclasses (apolipoprotein, cargo,GO:0005055: laminin, pattern recognition, GO:0038023:signaling, GO:0099600: transmembrane and virus receptoractivity), the subsumption links to which involve operandspecialization (and only cargo receptor activity a modeaddition, as well).Besides such distinctions of the way in which a termrelates to its overall set of direct subclasses, we observethat FueL-guided analysis can generally contribute tocomparing terms and their definitions more easily. Thisapplies in particular cases, e.g., when wondering about the(in)difference between the operand signal of GO:0038023and operand extracellular or intracellular signal ofGO:0099600. A decision on this question supports thecomparison of the overall definitions of both terms. Fur-ther considerationsmay be concerned with amore generalperspective. Looking at the operands identified in ouranalyses, we find that some operands are named by roleterms such as messenger (w.r.t. GO:0004872), others havenon-role names, e.g. laminin (w.r.t. GO:0005055), and yetothers mix both aspects, like hydrogen or electron acceptorin GO:0016491: oxidoreductase activity. This yields a con-necting factor to the field of roles and role analysis, cf. e.g.[3335], which may lead to novel refactoring considera-tions for MFO as well as to future refinements of functionsubsumption patterns.Overall, on the one hand we do see significant poten-tial based on inspecting MFO manually in a systematicand structured way, using FueL. On the other hand, thepurely manual approach is a limitation at the present stageand hampers an extensive evaluation, which would ideallyinvolve direct participation by GO developers.Despite the shortcoming regarding validation in prac-tice, we argue that presenting and demonstrating ourBurek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 10 of 11approach in a biological context is already beneficial. Theaspect of applying it systematically to specific functionterms, which may also be conducted merely on the con-ceptual basis of FueL, almost independently of the UMLlanguage aspects, is elaborated above. But more can besaid. First, although we consider MFO as a major caseof interest, FueL is applicable to functions in arbitrarydomains and contexts, cf. [18]. The approach presentedmay therefore be of interest concerning functions coveredin other biomedical ontologies. Secondly, we see manyroutes of future work that can be pursued, possibly incollaboration with other groups. In the context of MFO,there are at least the ideas (1) to provide tools that sup-port the use of FueL by ontology developers and augmentan established ontology lifecycle, as well as (2) to develop(semi-)automated approaches and software that can beapplied to the existing MFO structure, for example, inorder to determine instances of subsumption patterns.This leads to a final point here, though of no less impor-tance, where subsumption patterns are a natural candidateto deal with. Given OWL as the current basis of devel-opment and reasoning of many biomedical ontologies, away to bridge between OWL and FueL is highly desir-able, or  at the very least  the transfer of FueL-basedfunction analysis and representation to a correspondinguse of OWL. We expect either task to be ambitious.FueL is equipped with a formalization in first-order logic[18], which must be respected and related to clearly ifan OWL formalization or translation is derived fromFueL. Another issue along similar lines is the treatmentof UML stereotypes in OWL, as these are meta-classes inUML. There are a number of conceivable options to tackletheir treatment in OWL, ranging from not making themexplicit over the use of punning or annotations [36] tousing multiple OWL ontologies for one FueLmodel. Iden-tifying pros and cons of such options with respect to par-ticular purposes in the context of biomedical ontologiesremains an interesting future effort.ConclusionsIn the current paper we present and discuss applica-tions of UML and patterns of function subsumption tothe modeling and refactoring of biological ontologies. Inparticular, we developed a UML profile for function mod-eling, called the FunctionModeling Language (FueL) [19],and apply it to the modeling and refactoring of segmentsof the Molecular Function Ontology.The application of FueL enables the systematic, graphi-cal representation of functions and thereby of informationthat is currently available in MFO mainly in the form oftextual descriptions. We elaborate that behind the exten-sional is_a relation, which is used for the construction ofMFO, several different patterns of intensional subsump-tion can be determined. Modeling MFO via FueL helpsin identifying pattern instances that occur implicitly inMFO. Moreover, FueL provides the means of referringto those patterns directly in the hierarchy of molecu-lar functions. We argue that this can help in makingthe ontology structure more comprehensible for humanusers and that it supports communication. The claim isdemonstrated by an analysis and a model of an MFO frag-ment with FueL, from which we derive several refactoringoptions.Besides proposing the adoption of FueL and the par-ticular refactoring options in this paper, for future workwe consider first the continued analysis of MFO. Extend-ing this to a larger scale may require establishing soft-ware support, e.g., for identifying subsumption patterninstances within MFO (semi-)automatically. Moreover,FueL and its methods may also be transferred to or mayyield new methods for common languages of biomedicalontologies, nowadays including OWL.Endnote1 In contrast to FuML in a preceding publication[37] (cf. also the Acknowledgments section below), theacronym FueL has been adopted for a better termino-logical distinction from other efforts, like fUML [38] byOMG.AcknowledgementsThis paper is an extended version of a submission [37] presented at theInternational Conference on Biomedical Ontology (ICBO) 2015. We are gratefulto the involved ICBO reviewers and participants for valuable criticism. Likewisethe journal reviewers deserve our thanks for insightful and stimulatingcomments.FundingWe acknowledge support from the German Research Foundation (DFG) andthe University of Leipzig within the program of Open Access Publishing.Availability of data andmaterialsData sharing not applicable to this article as no datasets were generated oranalyzed during the current study.Authors contributionsPrimarily PB developed the UML profile that constitutes FueL [19], incollaboration with HH. FueL is based on pursuing ontological analysis of thenotion of function, with contributions by PB, FL, and HH. PB conceived of theidea of utilizing structural subsumption for the development of functionsubsumption patterns. All three authors discussed the latter as well as theapplication of FueL to MFO. Mainly PB and FL prepared the present paper,supported by HH. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 11 of 11Author details1Institute of Medical Informatics, Statistics and Epidemiology, University ofLeipzig, Haertelstrasse 16-18, 04107 Leipzig, Germany. 2Computer ScienceInstitute, University of Leipzig, Augustusplatz 10, 04109 Leipzig, Germany.Received: 1 March 2016 Accepted: 15 September 2017RESEARCH Open AccessPredicting activities of daily living forcancer patients using an ontology-guidedmachine learning methodologyHua Min* , Hedyeh Mobahi, Katherine Irvin, Sanja Avramovic and Janusz WojtusiakAbstractBackground: Bio-ontologies are becoming increasingly important in knowledge representation and in the machinelearning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to theSEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities ofdaily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to understandbiomedical data.Results: This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods wereapplied to create predictive models for ADL disabilities for the first year after a patients cancer diagnosis. The firstmethod is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods forreasoning with ontologies. The models showed that a patients race, ethnicity, smoking preference, treatment plan andtumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performancelevels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADLperformance levels (P < 0.1) than methods without ontologies.Conclusions: This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for MLalgorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rulelearning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. Theontology-guided ML method achieved better performance than the method without ontologies. The presented methodcan also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledgeand consistency with existing clinical expertise is critical.Keywords: Machine learning, Bio-ontologies, Quality of life, Activities of daily living, SEER-MHOSBackgroundPrecision medicine is an emerging approach for dis-ease prevention and treatment that takes into accountindividualized patient information including genomics,environment, and lifestyle [1]. This new era in medi-cine and health requires advanced methodologies foranalyzing, synthesizing, and disseminating heteroge-neous data, as well as the ability to harness existingknowledge in order to discover relationships and cre-ate computational models for improving care andquality of life. The focus on big data analysis in thebiomedical field creates an even greater need for ad-vanced computational methodologies that can translate datainto computer-interpretable knowledge and produce com-prehensible models that can then be used to advancepatient-centric healthcare. Machine learning (ML) is alreadywidely used in creating predictive models within a variety ofarenas of big data analysis, and is gaining popularity in med-ical and health applications [2].One major challenge in ML is communicating themeaning of data attributes and their significance tothe learning algorithm. Biomedical data are extremelycomplex, heterogeneous, and characterized by intri-cate semantics. Very few ML algorithms are capableof interpreting data beyond the mechanical fitting ofinput data/matrix of numbers into a given model.* Correspondence: hmin3@gmu.eduDepartment of Health Administration and Policy, College of Health andHuman Services, George Mason University, MS: 1J3, 4400 University Drive,Fairfax, VA 22030-4444, USA© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Min et al. Journal of Biomedical Semantics  (2017) 8:39 DOI 10.1186/s13326-017-0149-6The majority of ML methods (including the mostpopular Support Vector Machines, Random Forests,Logistic Regression, etc.) work with and almost exclu-sively focus on numeric data stored in flat tableswhile ignoring the semantic relationships (meaning)of data elements. Two existing ML disciplines thataddress complex data are statistical relational learning[3, 4] and inductive logic programming [5]. Both dis-ciplines are concerned with the more general problemof learning from datasets with complicated structures(relational databases or predicates). However, whilehealthcare data are particularly rich in knowledge, theuse of standard ML methods does not allow for theencoding of attribute types, hierarchies, ontologies,and other coding systems. Typically, in order to useany background knowledge or ontological relation-ships when applying ML methods, one needs toencode these in problem representations, i.e., by cre-ating additional dimensions that correspond to inter-actions between existing ones. This is because theML method for input is a matrix of numerical data.One example of using ontology in conjunction withML is the work by Kassahun et al. [6], in which theresearchers classified types of epilepsy patients andtheir localization using an ontology-based classifica-tion (OBC) methods that classified patients (slightly)more accurately than clinicians.An ontology formally represents domain knowledge asa set of concepts and relationships between those con-cepts. In artificial intelligence (AI), ontologies have beenapplied as artifacts to represent human knowledge. Theyare also critical components of knowledge management,e.g. Semantic Web, business-to-business applications,and natural language processing [710]. In biomedicine,ontologies have been widely adopted and used in know-ledge management, data integration, and decision sup-port and reasoning [11, 12]. Bio-ontologies are slowlyemerging in data-driven science, including data miningand ML, although mainly in the capacity and context ofnatural language processing applications [13].There are many existing bio-ontologies, each with ascope, purpose, and role of its own with no industrystandard. Consequently, there are communication bar-riers between the various information systems or appli-cations when different vocabularies are used. In order toaddress these barriers, the Unified Medical LanguageSystem (UMLS) was developed by the National Libraryof Medicine (NLM) in 1986 [14, 15]. The 2016 ABversion of the UMLS contains more than 3 millionconcepts (CUIs) and 13 million unique concept names(AUIs) from 199 source vocabularies [16]. The UMLSestablishes mappings between bio-ontologies by assign-ing a concept unique identifier (CUI) to names fromvarious vocabularies that have the same meaning. Thevocabulary mappings allow computer systems to trans-late data among the diverse information systems. Richrelationships (22 million) between concepts in theUMLS also provide a solid foundation for reasoning inmedical knowledge [11].Thus, given the advantages of bio-ontologies know-ledge, UMLS mappings, and the ability of ML to developand learn from predictive models, this paper aims todescribe and apply an ontology-guided ML method (em-phasis on rule learning) by incorporating hierarchicalrelationships from the UMLS. The UMLS is used to pro-vide medical domain knowledge for the ML method tounderstand the meaning and significance of the bio-medical data, with regards to the existence of specifichierarchical relationships between concepts. By applyingthe ontology-guided ML method to SEER-MHOS data,the technique is able to predict the cancer patients abil-ity to perform activities of daily living (ADLs). The out-come of which is generated rules that are highlytransparent and easy to understand. Thus, the rules canbe interpreted by the non-technical end users. Thisproof of concept study suggests that the combination ofbio-ontologies and ML methods provides an advancedcomputational and quantitative technique for analyzingbiomedical data.MethodsAQ21 rule learningAQ21 is a multi-task ML and data mining system forattributional rule learning and rule testing that can beapplied to a wide range of classification problems [17]. Itwas developed in the Machine Learning and InferenceLaboratory (MLI) at George Mason University. The sys-tem has been recently extended to include features spe-cific for processing biomedical data [18]. AQ21 is a typeof natural induction system that seeks to identifypatterns represented as attributional rules [19] that areeasily interpretable to end users. The basic form of anattributional rule is: CONSEQUENT < = PREMISEwhere both CONSEQUENT and PREMISE are conjunc-tions of attributional conditions. Each attributional con-dition involves attributes present in the data orconstructed by the program. Additionally, AQ21 canlearn rules with exceptions given by the formula CON-SEQUENT < = PREMISE |_ EXCEPTION. The AQ21system can also handle inconsistencies in data. Thesystem learns standard rules and generates exceptionphrases that represent covered negative examples. EX-CEPTION can be either an attributional conjunctivedescription or a list of examples constituting exceptionsto the rule. In the medical datasets, the exceptions arealways negative examples such as cancer recurrence anddisease progression.Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 2 of 8Learning rules generated by AQ21 consist of severalsteps, which can be classified as input preprocessing,rule generation, and rule optimization. The steps aregenerally executed in this order, although AQ21s learn-ing process is iterative in several ways. Input preprocess-ing includes rearranging data into classes, removingambiguous examples, and modifying representationspace through simple preprocessing methods (i.e.,discretization, attribute selection) or more advancedones that employ constructive induction algorithms [20].At its core, rule learning implements modification of asimplified version of the algorithm quasi-optimal (Aq)for constructing rules, which is a well-known sequentialcovering algorithm [21]. The algorithm starts with a ran-domly selected positive example, called the seed, andgenerates all possible (high quality) rules that cover theseed and do not cover (or approximately do not cover)any of the negative examples. The best quality top rulesare then selected and stored. Among positive examplesnot covered by these selected rules, another randomseed is selected and the operation is repeated. Thisprocess results in a number of very general rules (typic-ally more than needed) that need to be optimized andprepared for output. Optimization of rules includes theirtrimming, adjusting of generality through following hier-archies, selection, and mapping of attributes. The overallgoal of AQ21 is to produce rules that maximize user-defined quality criteria that typically provide tradeoffbetween accuracy (precision/recall) and their simplicityand transparency. Finally, the program employs a num-ber of methods designed to provide output in human-oriented forms, including the generation of the rules intoa natural language representation (layman terms) [22].AQ21 is the latest development from a series of AQrule learners that dates back to the 1970s [23]. A num-ber of well-known rule learners have been developedover the last decades [2426], but many are not utilizedin mainstream research at the present time. In the pastfew years the ML field has been dominated by statisticalmethods that focused primarily on providing highlyaccurate models. However, the community has begun toslowly transition back to understandability and transpar-ency of models produced, which is particularly import-ant in biomedical applications.Ontology-guided AQ21 (AQ21-OG)AQ21-OG is an extension of the AQ21 rule learningsystem. It applies hierarchical reasoning methods [27] toinclude UMLS and other ontologies when analyzing data.Currently, the program allows for mapping IS-A relation-ships. The implementation of the AQ21-OG includes:Step 1: Mapping data to the UMLS CUIs. This step isused to identify the base CUIs. The candidate CUIs areidentified automatically (SQL) and then reviewed byexperts for the problematic mappings.Step 2: Extracting complete sub-hierarchies by followingIS-A relationships using base CUIs. This is done byfollowing IS-A relationships in the UMLS for eachconcept until the complete parent, child, and siblingsub-hierarchy is extracted. The complete sub-hierarchy isdefined as the path from base CUI (furthest child(ren) inthe hierarchy) to the root (super parent, i.e. a parentthat is not also a child). This extraction is the basis forthe input file (in Step 4) that AQ21 will use to find thefarthest common ancestors for base CUIs (in Step 5).Step 3: Resolving inconsistencies in the hierarchy. Due tonature of the UMLS, a number of inconsistencies (e.g.,cycles, duplicates) may happen when due to beingconstructed from multiple source terminologies [2831].Cycles are not permitted in AQ21, so they are resolvedby breaking links that connect back to concepts higher inthe hierarchy, as measured by distance from the root.Other types of inconsistencies are removed from thefinal hierarchy.Step 4: Encoding extracted hierarchies intoML-software readable format. AQ21 requires a list ofparent-child pairs for all relationships that form thehierarchy. The data is read from text files that includeall semantic information required to correctly reasonwith the data. Specifically, in the AQ21, hierarchicalrelationships are part of the definition of attributesdomains (set of possible values) that describe data.Step 5: Optimizing the rules by using the extractedUMLS hierarchies from Step 2. AQ21-OG finds thehighest level of generalization in the hierarchy, which iseither consistent with data or maximizes the rulequality measures. This is particularly valuable whenanalyzing coded medical data with potentially hundredsof thousands of binary attributes. For example, ICD-9-CM diagnosis codes can result in the need to createclose to 10,000 binary attributes. Therefore, the need togeneralize those codes to reduce the number of featuresis a necessity.Study populationSEER-MHOS (Surveillance, Epidemiology, and EndResults - Medicare Health Outcomes Survey) data from1998 to 2011 (1,849,311 records) were used to extractcomorbidities and activities of daily living (ADLs), aswell as cancer characteristics. This dataset links twolarge population-based data that provide detailed infor-mation about Medicare beneficiaries with cancer [32].The SEER data extracted from the cancer registrycontains clinical, demographic and cause of death in-formation for persons with cancer, while the MHOSdata is extracted from survey responses and providesMin et al. Journal of Biomedical Semantics  (2017) 8:39 Page 3 of 8information about the health-related quality of life(HRQOL) of Medicare Advantage Organization (MAO)enrollees.A number of steps were followed to create the studypopulation dataset. First, the study population was lim-ited to those who completed at least one survey beforetheir cancer diagnosis and one survey roughly one yearafter the diagnosis. If a patient completed multiple sur-veys, the surveys closest to before the cancer diagnosisand the 1-year follow-up were used. These very strictcriteria significantly reduced the sample size and re-sulted in a cohort of 723 cancer patients.Dependent/Output Variables: the primary outcomeswere six ADLs (walking, dressing, bathing, moving in/out of chair, toileting, and eating) reported in a patientsurvey taken one year after the cancer diagnosis.Independent/Input Variables: the potential predictorswere selected based on the prior research [3337] andare as follows:(1)Patient demographics: age, race and marital status(2)Six ADLs reported in a patient survey taken beforethe cancer diagnosis(3)Thirteen self-reported comorbidities extracted froma patient survey taken before the cancer diagnosis:Angina Pectoris/Coronary Artery Disease, Arthritisof Hand/Wrist, Arthritis of Hip/Knee, Back pain,Congestive heart failure, Emphysema/Asthma/Chronic obstructive pulmonary disease, Diabetes,Crohns Disease/Ulcerative Colitis/InflammatoryBowel Disease, Hypertension, Myocardial Infarction,Other Heart Conditions, Sciatica and Stroke(4)Six cancer characteristics namely grade, staging,tumor size, histology, tumor extension, and behaviorextracted from the SEER registry(5)Cancer radiation and surgery treatment indicatorsextracted from the SEER registryAnalysis of the SEER-MHOS data with AQ21 and AQ21-OGThe dataset was randomly divided into training (80%)and testing (20%) sets. The training set was used to cre-ate predictive models and the testing set was used to as-sess the model discrimination. Models were first createdin order to find the predictor or set of predictors thatcould be used to predict the outcome (the six ADLs postcancer diagnosis). Two ML methods were used to createmodels: AQ21 and AQ21OG as previously describedabove. The quality of the two methods were assessedusing the number of positive (p), negative (n) cases cov-ered by the generated rules and the quality of the rulesQ(w). The rule R quality, Q(R,w) with weight w, or justQ(w) (denoted by q in the rule), is calculated using thefollowing formula described by Michalski and Kaufman[38]. P and N indicate total numbers of positive andnegative examples in data (here, disabled vs. functionallyindependent in terms of ADLs).Q R;wð Þ ¼ compl Rð Þw consig Rð Þ1?wwherecompl Rð Þ ¼ p=Pconsig Rð Þ ¼ p= pþ nð Þð Þ P= PþNð Þð Þð Þ PþNð Þ=NThe w is a weight (from 0 to 1) that represents the tra-deoff between completeness and consistency gain. Thelower the w is, the more consistent the rules need to be(fewer negative examples covered). The higher the w is,the more complete the rules need to be (more positiveexamples covered). Based on experimental evaluation ofthe rules, we decided to select w = 0.3 which indicatesslightly higher weight for more consistent rules. Thisvalue was used in both cases, with and without ontology.Completeness is frequently referred to as recall inmachine learning. Consistency gain can be viewed asnormalized precision that measures how much precisionwe gain over a random guess.Table 1 Characteristics of Patients in the final dataset (n = 723)Number %Age< 65 23 3%6574 353 49%7584 293 41%> =85 54 7%Top 5 ComorbiditiesHypertension 432 60%Arthritis of Hip 274 38%Arthritis of Hand 256 35%Other Heart 181 25%Sciatic 166 23%Cancer TypeBladder 57 8%Breast 181 25%Colorectal 105 15%Head Neck 22 3%Lung 87 12%Melanoma 58 8%Pancreas 11 2%Prostate 166 23%Stomach 11 2%Uterus 25 3%Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 4 of 8ResultsPatient cohortThis retrospective SEER-MHOS study included 723 can-cer patients. The average age was 74.7 +/? 6.63 years. Asummary of the dataset is shown in Table 1. Table 2shows the number of patients who reported ADL limita-tions before and after cancer diagnosis. The increasednumber of patients reporting disabilities after diagnosisshow that cancer has an impact on ADLs. Walking andchairing-in/out were the most affected ADLs amongthese Medicare recipients with cancer.Rule induction from the SEER-MHOSAQ21 methods generated a number of models (rulesets)for describing and predicting patients deficiencies inperforming ADLs from the SEER-MHOS dataset. Belowis an excerpt of two sample rules, one from each AQmethod, from a model for predicting a decline in theability to perform bathing independently.Sample 1: AQ21[Bathingimpairment] < ==[Race = Black, White, Chinese: 70, 245, 22%][Hispanic = No: 64, 241, 20%][Smoking = Some days, Not at all: 68, 238, 22%][Surgery = 51,40,27,0,45: 45, 113, 28%][Histology = Squamous cell neoplasm, Transitionalcell papillomas and carcinomas, Adenomas andadenocarcinomas, Nevi and melanomas, Cystic,mucinous and serous neoplasm, Ductal andlobular neoplasm, Epithelial neoplasms, NOS: 74,252, 22%][Stage = In situ, Localized only, Regional by directextension only: 69, 244, 22%][Primary site andmorphology = C0153458,C0153492,C0153532,C0242787, C0949022,C0235653,C0153483,C0153611,C0153555,C0153435,C0346782,C0153491,C0153612:30, 34, 46%]: p = 22, n = 2, q = 0.642Sample 2: AQ21-OG[Bathingimpairment] < ==[Race = White, Chinese: 64, 219, 22%][Hispanic = No: 64, 241, 20%][Smoking = Some days, Not at all: 68, 238, 22%][Surgery = 32,51,40,0,45: 40, 95, 29%](Continued)[Histology = Squamous cell neoplasm, Adenomasand adenocarcinomas, Nevi and melanomas,Cystic, mucinous and serous neoplasm, Ductal andlobular neoplasm, Epithelial neoplasms, NOS: 68,229, 22%][Cancer site = Lung and Bronchus, Melanoma,Descending Colon, Rectum, Pancreas, UrinaryBladder, Breast, Larynx: 61, 169, 26%][Primary site and morphology = C0154077,C0007102, C0153532, C0005684, C0153555,C0024624, C0006142, C0235652, C0864875,C0346647, C0345921, C0242379, C0346629,C0345865, C0242788, C0034885, C0007107,C0345713, C0587060, C1263771: 38, 49, 43%]: p = 23, n = 2, q = 0.653The predictors of bathing disability include patientdemographic (race and ethnicity), smoking history,tumor characteristics (histology, stage, and cancersites) and treatment (surgery). The interpretation ofthe first two lines of the first rule is: a patient islikely to have bathing impairment if the patients raceis White, Black or Chinese and the ethnicity is non-Hispanic. The surgery codes (treatments) in thefourth line can be found from https://seer.cancer.gov/manuals/2016/appendixc.html. The meaning of theCUIs in the last line is presented in Appendix. Thefirst two numbers, following the colon, within eachcondition (attribute) describe the number of patientswho have the bathing impairment and who do nothave the bathing impairment that satisfy the specificcondition. For example, among the White, Black orChinese patients, 70 of them have the bathing impair-ment while the remaining245 patients do not havethe bathing problem. The last number is prevalenceof the positive class that indicates the ratio of thenumber of positive (p) examples over the number ofpositive and negative (n) examples, p/(p + n). Therule outputs are similar using AQ21 and AQ21-OG.However, the quality of the rule, as measured byQ(w), generated by the second method (AQ21-OG) isslightly more accurate. The last line in the rule setdescribes the numbers of positive examples (p), nega-tive example (n) covered by the rule, and the rulequality. While the numbers dont appear to make alarge difference, the rules are simply an illustration ofthe type of improvement made by the method. Table 3Table 2 Number of patients reported ADL disabilities beforeand after cancer diagnosisADLs No. of patients beforecancer diagnosis% No. of patients aftercancer diagnosis%Bathing 39 5% 85 12%Dressing 27 4% 61 8%Eating 10 1% 32 4%Chairing 65 9% 113 16%Walking 98 14% 146 20%Toileting 21 3% 50 7%Table 3 Quality metrics for the AQ21 and AQ21-OG for thesample rulesSample 1 (AQ21) Sample 2 (AQ21-OG)Precision 0.91 0.92Recall 0.29 0.31F1-score 0.44 0.46Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 5 of 8shows the precision, recall and F1-Score of bothAQ21 and AQ21-OG for the above two sample rules.Although the recall in the Table 3 seems low, this isthe number for one example rule out of a set ofrules.Note that the rules presented above correspond toeach other; however, AQ21 and AQ21-OG are notguaranteed to generate similar rules. The ability togeneralize available data differentially within the hier-archies derived from an ontology may steer theprocess in a different direction causing the rules todiffer. Consequently, the quality of rules improves.Table 4 shows the quality of the rules generated by thetwo methods for each of the six ADLs. In all cases, theQ(R, w) improved after including UMLS, except fortoileting which remained unchanged. A paired t-testwas performed to compare the sample means for thequality of rules (Table 5). Although the sample size wassmall, after adding ontology, the mean of Q(R, w) valuesincreased by 6% (P = 0.05). There was a statistically sig-nificant difference (P < 0.1) between the effectiveness ofthose two methods.DiscussionAQ21 and its ontology-guided version, AQ21-OG, arehighly configurable and robust systems with features es-pecially valuable for: learning from biomedical data suchas individual patient data, learning from aggregated data,and using medical knowledge. One major advantage isthat AQ21-OG can optimize attributional rules with theassistance of medical knowledge from the UMLS, for thepurposes of rule generalization based on the hierarchicalrelationships. In this research, the rule generalizationprocedure continued until negative data against medicalknowledge was found. This was done automatically bythe AQ21 to increase accuracy of the predictive models.One big challenge for the ontology-guided MLmethod is performance. Performance is impacted by:(1) the extreme size and complexity of UMLS andother medical ontologies which limit the applicationof standard search methods and (2) the size of theSEER-Medicare dataset. Although this study onlyworked with a small subset of SEER-MHOS data, thehierarchical structure from the UMLS was alreadylarge and complicated. As previously discussed manyconcepts in the UMLS contain more than one parent,thus the generalized rules may contain more CUIsdue to the complexity of the UMLS.One limitation of this study was that the method wastested and validated based on a small sample of SEER-MHOS patients (n = 723). Additionally, survey data aretypically not best suited for ML applications because oftheir biases and subjectivity and limited potential use inreal decision support applications. Future work will in-clude increasing the sample size by using the entire SEER-MHOS data as an opposed to a subset of a 5% sample. Onthe methodological side, AQ21 will be extended to handleother types of semantic relationships in the UMLS. Fur-ther, more experimental evaluation is needed to improveaccuracy of the generated rules in order to match the ac-curacy of state-of-the-art statistical methods.ConclusionsThis paper presents how AQ21 and its ontology-guidedML version AQ21-OG were successfully applied to theSEER-MHOS data set and generated a set of models fordescribing and predicting cancer patients deficiencies inperforming six ADLs. These models are highly transpar-ent and relatively easy to understand. The results showthat the AQ21-OG outperforms the original AQ21 sinceAQ21-OG can optimize attributional rules with theassistance of medical knowledge from the UMLS. Thisresearch further demonstrates that bio-ontologies can beused to promote the effectiveness and efficiency of MLin healthcare.Table 4 Q(R, w*), precision, recall and F1-score calculated for a selected rule for each ADLADL AQ21 Q(R, w) AQ21-OG Q(R, w) AQ21 precision AQ21OG precision AQ21 recall AQ21-OG recall AQ21 F1-score AQ21-OG F1-scoreBathing 0.642 0.653 0.91 0.92 0.29 0.31 0.44 0.46Chairing 0.489 0.546 0.92 1.0 0.12 0.13 0.21 0.23Dressing 0.451 0.633 0.86 1.0 0.1 0.21 0.17 0.35Eating 0.617 0.697 1.0 1.0 0.2 0.3 0.33 0.46Toileting 0.584 0.584 1.0 1.0 0.16 0.16 0.28 0.28Walking 0.457 0.472 1.0 1.0 0.07 0.08 0.13 0.15*w = 0.3Table 5 T-test results for comparison of two methodsWithout ontology With ontology PMean 0.54 0.60 0.05Variance 0.0071 0.0066Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 6 of 8AppendixAcknowledgementsWe thank our undergraduates Sava Vukomanovic and Ilirjeta Krasniqi forpreparing earlier version of this work as a conference paper.FundingThe study was supported by the Thomas F. and Kate Miller Jeffress MemorialTrust, Bank of America, Trustee. Publication of this article was funded in partby the George Mason University Libraries Open Access Publishing Fund.Availability of data and materialsThe datasets generated and/or analyzed during the current study are notpublicly available due to the SEER-MEDICARE HEALTH OUTCOMES SURVEYDATA USE AGREEMENT (DUA).Authors contributionsHMin and JW conceived and designed the analysis. HM, KI, and SA ran theanalysis. All authors contributed to the interpretation of the data, revisions ofthe manuscript and read and approved the final manuscript.Ethics approval and consent to participateIt was a secondary data analysis of the SEER-MHOS.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 23 October 2016 Accepted: 6 September 2017Giraldo et al. Journal of Biomedical Semantics  (2017) 8:52 DOI 10.1186/s13326-017-0160-yRESEARCH Open AccessUsing semantics for representingexperimental protocolsOlga Giraldo1* , Alexander García1, Federico López2 and Oscar Corcho1AbstractBackground: An experimental protocol is a sequence of tasks and operations executed to perform experimentalresearch in biological and biomedical areas, e.g. biology, genetics, immunology, neurosciences, virology. ProtocolsJha et al. Journal of Biomedical Semantics  (2017) 8:40 DOI 10.1186/s13326-017-0146-9RESEARCH Open AccessTowards precision medicine: discoveringnovel gynecological cancer biomarkers andpathways using linked dataAlokkumar Jha, Yasar Khan, Muntazir Mehdi, Md Rezaul Karim, Qaiser Mehmood, Achille Zappa,Dietrich Rebholz-Schuhmann and Ratnesh Sahay*AbstractBackground: Next Generation Sequencing (NGS) is playing a key role in therapeutic decision making for the cancerprognosis and treatment. The NGS technologies are producing a massive amount of sequencing datasets. Often,these datasets are published from the isolated and different sequencing facilities. Consequently, the process ofsharing and aggregating multisite sequencing datasets are thwarted by issues such as the need to discover relevantdata from different sources, built scalable repositories, the automation of data linkage, the volume of the data,efficient querying mechanism, and information rich intuitive visualisation.Results: We present an approach to link and query different sequencing datasets (TCGA, COSMIC, REACTOME, KEGGand GO) to indicate risks for four cancer types  Ovarian Serous Cystadenocarcinoma (OV), Uterine CorpusEndometrial Carcinoma (UCEC), Uterine Carcinosarcoma (UCS), Cervical Squamous Cell Carcinoma and EndocervicalAdenocarcinoma (CESC)  covering the 16 healthy tissue-specific genes from Illumina Human Body Map 2.0. Thedifferentially expressed genes from Illumina Human Body Map 2.0 are analysed together with the gene expressionsreported in COSMIC and TCGA repositories leading to the discover of potential biomarkers for a tissue-specific cancer.Conclusion: We analyse the tissue expression of genes, copy number variation (CNV), somatic mutation, andpromoter methylation to identify associated pathways and find novel biomarkers. We discovered twenty (20) mutatedgenes and three (3) potential pathways causing promoter changes in different gynaecological cancer types. Wepropose a data-interlinked platform called BIOOPENER that glues together heterogeneous cancer and biomedicalrepositories. The key approach is to find correspondences (or data links) among genetic, cellular and molecularfeatures across isolated cancer datasets giving insight into cancer progression from normal to diseased tissues. Theproposed BIOOPENER platform enriches mutations by filling in missing links from TCGA, COSMIC, REACTOME, KEGGand GO datasets and provides an interlinking mechanism to understand cancer progression from normal to diseasedtissues with pathway components, which in turn helped to map mutations, associated phenotypes, pathways, andmechanism.Keywords: Cancer genomics, Biomarkers, Multi-Omics, Pathways, Gynecological cancer, Linked data,Semantic technologiesBackgroundNext Generation Sequencing (NGS) technologies opennew diagnostic and therapeutic ways for cancer research.The resulting high-throughput sequencing data has to beprocessed in complex data analytics pipelines includingannotation services. Unfortunately, there is not yet a*Correspondence: ratnesh.sahay@insight-centre.orgInsight Centre for Data Analytics, NUIG, Galway, Irelandwell-integrated platform available for both clinical andtranslational [15] research to fulfill these annotationand analytical tasks. In addition, the large volumes andgrowing variety of NGS data sources pose another chal-lenge, since the computational infrastructure for thebiological interpretation will have to cope with verylarge quantities and heterogeneities of data originatingfrom sequencing facilities [68]. More importantly, the© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 2 of 16functional annotation of genomics data for cancer hasto take tissue-specificity into consideration and thus hasto avoid ambiguity while consolidating and aggregatingclinical outcomes from disparate resources. Similarly, acomputational platform that can consolidate variety ofdata derived from electronic health records (EHRs), omicstechnologies, imaging, and mobile health is a funda-mental requirement to accelerate the recent precisionmedicine initiative1 [9]. In our initial work [10] we pre-sented an approach to link and query three large repos-itories  TCGA2, COSMIC3, and Illumina Human BodyMap 2.04  to analyse the expression of specific genes indifferent tissues and its variants by:? Linking of gene expression, copy number variation(CNV), somatic mutation data from two disjointresources (i.e., COSMIC and TCGA).? Identifying sets of genes using the Illumina HumanBody Map 2.0 with relevance for ovarian cancer witha comprehensive set of mutations.In order to analyse the tumorigenesis of female gyneco-logical cancer types, in this article we extend our previouswork [10] by including:? Ovarian Serous Cystadenocarcinoma (OV), UterineCorpus Endometrial Carcinoma (UCEC), UterineCarcinosarcoma (UCS), Cervical Squamous CellCarcinoma and Endocervical Adenocarcinoma(CESC) datasets.? Methylation data to further understand potentialpromoter genes based on methylation change andbiomarkers.? REACTOME, KEGG and GO biological processesdatasets to understand cancer causing gene regulationthrough associated pathways and biological processes.To further understand the epigenetics, we retrieved thegenomic positions (loci), mutation frequency, change inpromotormethylation for each gene in the above four can-cer types (OV, UCS, UCEC, & CESC). These are furtherclassified by biological processes involved in understand-ing the mechanism and associated pathways. By doing thiswe explore the variant and mutation prioritization using16 different tissue types reported in the Illumina BodyMap 2.0. The differential expressed genes derived fromIllumina Human BodyMap 2.0  using the procedure sug-gested by Trapnell, C. et al. [11]  are linked with differenttissue types and gene expressions in COSMIC and TCGAdatasets leading to a potential biomarker for a particulartissue-specific cancer.The proposed approach enriches mutations and methy-lation by filling in missing links from COSMIC, TCGA,REACTOME, KEGG and GO datasets providing a mech-anism to analyse cancer progression from normal todiseased tissues with key pathway components. Our keyobjective is to understand the tumorigenesis of these fourgynecological cancer types (OV, UCS, UCEC, & CESC). Inorder to retrieve the patterns of genes and tissue-specificinformation from various cancer mutations reported inmultiple repositories; we encountered three computa-tional challenges for linking and querying these multipledistributed repositories: (i) transform heterogeneous datarepositories and their storage formats into standard RDF;(ii) discovering links by finding specific patterns, i.e., cor-relations for a gene with regards to CNV, mutation, geneexpression, and methylation datasets; and (iii) scalablequerying over the large volume datasets covering 16 dif-ferent tissue types and the gene expression data fromdifferent repositories. We propose a data-interlinked plat-form called BIOOPENER5 that enables automated dis-covery of data linkages and querying of information fromlarge-scale cancer and biomedical repositories.The experiments conducted in this paper is aligned tothe transcriptome and epigenetics studies based on theHuman Body Map 2.0 (HBM) from Illumina which cov-ers the following tissues: adrenal, adipose, brain, breast,colon, heart, kidney, liver, lung, lymph, ovary, prostate,skeletal muscle, testes, thyroid, and white blood cells. TheHBM provides gene-specific information across one ormore tissue types and intends to support the identifica-tion of potential biomarkers for a targeted therapy. In thisstudy, our results not only discover novel biological out-comes but also provides a linked datasets that assimilatesclinical outcomes from related data repositories.The rest of the paper is structured as follows:Motivation section motivates our working scenariobased on Illumina Human Body Map (HBM) 2.0,cancer and biomedical databases (COSMIC, TCGA,REACTOME, KEGG and GO); Methods sectionpresents the BIOOPENER methodology and architec-ture; Results section discusses the results obtainedfrom the BIOOPENER platform; Related work sectionpresents the related work in linking and querying cancergenomics repositories; and Conclusion section drawsthe conclusion from our work.MotivationIn order to understand the tumorigenesis, it is oneapproach to compare normal and diseased tissue sam-ples to interpret the changes in the expression patterns ofthe genes with regards to the observed disease status. Inour case, Illumina Human Body Map (HBM) 2.0 servesthe purpose to identify similarities in gene expressionpatterns using the studies across different tissue types,where HBM discloses the similarities between humantissues on the molecular and genetic level. Due to over-laps between cancer behaviors, progression, and mutatedgenes, we have selected top 1006 genes by a filteringJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 3 of 16criteria based on the Reads Per Kilobase of transcript perMillion mapped reads (RPKM) values. Further, these top100 genes identified are linked using the genetic featuressuch as genomic loci (start, end), beta value, cell cycle etc.from previously observed studies in COSMIC and TCGArepositories. The work presented in this article covers onlynon-synonymous (NS) mutations. Since many somaticmutations are passenger  synonymous mutations  anddo not impact tumorigenesis, we first select those genesthat are more likely to be drivers. The selection of drivergenes is based on the mutations frequency (RPKM value).Illumina Human Body Map (HBM) 2.0: HBM coversdata from transcriptome studies for 16 tissue types. Sam-ples for these 16 tissue types have been processed, alignedand finally expression level have been determined [12].Sequencing has been performed to provide both paired-end and single-end libraries (read-length of 50bp and75bp). A list of differentially expressed genes are extractedusing the step 2 (assemble expressed genes and tran-scripts) of procedure suggested by Trapnell, C. et al. [11].The gene expression data extracted from HBM samplesreturns a very large list of more than 52000 genes. Fordata processing reasons we chose to reduce the list andtherefore defined the cut-off for each RPKM value accord-ing to the method suggested by Sandberg et.al [13]. Asa result, the data for each tissue type includes both thecoverages and the RPKM values as the correspondingexpression level. The RNA seq dataset provides addi-tional relevant data such as CNV, fusion genes, structuralvariation, differentially expressed genes, novel mutations,splice junctions and transcriptome variations [14].Annotation Databases (COSMIC & TCGA): The mainfocus of this work is the identification of patternsfor cancer mutations and globally known mutationsand their types for selected differentially expressedgenes across different tissue types. Figure 1 shows thecorrespondences, i.e., the associations or links that havebeen established between the TCGA and COSMICdatabases for this purpose. For this task, our primary con-cern has been the associations between the CNV, theknown mutations, and the gene expression data.As part of our initial work [10], we have identifiedinstances to link in the COSMIC and TCGA datasets(see Fig. 1). For example, GENE_NAME is used toestablish links between COMPLETE_MUTATION andGENE_EXPRESSION datasets between both the repos-itories. Similarly, GENE_NAME and HUGO_SYMBOLhas been used to link COMPLETE_MUTATION fromboth the datasets. Further, CNV datasets from COS-MIC and TCGA have been linked based on chr:start_endposition. From the computational perspective, the links(owl:sameAs) between COMPLETE_MUTATION andGENE_EXPRESSION datasets using the GENE_NAMEproperty allow to create a subset of driver genes from alarger complete set of mutations.Annotation Databases (REACTOME, KEGG, & GOprocesses): Weobserve a set of prospective links throughthe DNA methylation datasets  from COSMIC andTCGA  to GO proliferation Ids. These links broaden ourunderstanding of the cell proliferation (with frequentlymutated genes) where changes in methylation level regu-late the gene expression. In order to target certain genes,it is important to find the affected cancer types and thecommon pathways associated with the cell proliferation.The KEGG and REACTOME datasets provide additionallinks to identify genetic profiles from already identifiedmutations in COSMIC and TCGA datasets. Clinical vari-ations of any mutation from the REACTOME dataset willhelp to explore clinical relevant targets, effects of down-regulation of each pathway and alternate pathways for thecell.Figure 2 shows a set of prospective owl:sameAslinks between COSMIC, TCGA, REACTOME, KEGG,Fig. 1 Links between COSMIC and TCGA datasetsJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 4 of 16Fig. 2 Links between COSMIC, TCGA, REACTOME, KEGG, and GO datasetsGO datasets. For example: (i) if Gene Symbol usedin the TCGA gene expression gets linked (throughowl:sameAs) with the Gene Symbol of COSMIC methy-lation datasets, then a simple query can fetch result aboutthe changes in a promotor region associated with muta-tions already identified in TCGA and COSMIC datasets;(ii) similarly, ENSEMBL ID used in COMSIC, TCGA,and Gene Ontology datasets can be linked to obtain thetranscript level changes with mutated gene in order tounderstand the disease progression; (iii) finally, by linkingCOSMIC and TCGA Methylation datasets provides usthe measure of beta value changes, the responders, andnon-responders based on hyper and hypomethylation. Inour initial work [10], we have identified MYH7 as one ofthe potential biomarker based on copy number variation(CNV) frequencies. In this article, we are aiming to linkthe identified mutations (from COSMIC & TCGA) acrossKEGG, REACTOME, and GO datasets to understand themetabolic process of each reaction and the localizationof each component of a reaction further connecting themetabolic process to pathways described in the KEGGdataset.MethodsThe BIOOPENER approach is fundamentality similar tothe Bio2RDF7[15, 16] framework that created a mashup oflinked data connected through various linking properties(e.g., xRef, owl:sameAs, x-relation) [17]. BIOOPENERfocus is specifically around discovering and exploitingthe owl:sameAs links for constructing complex feder-ated queries  due to the precise owl:sameAs seman-tics [18]  across multiple datasets. We now presentthe BIOOPENERs architectural, linking, and queryingmethodology.BIOOPENER architectureThe BIOOPENER architecture is summarized in Fig. 3showing all three major components. First, the RDFiza-tion component that generates Linked Data from theCOSMIC, TCGA, REACTOME, KEGG, GO databasesresults into several SPARQL endpoints. It is importantto note that, the two datasets (COSMIC and TCGA)are converted from the raw format to RDF; further, welinked COSMIC and TCGA to REACTOME8, KEGG9,and GO10 datasets hosted at the Bio2RDF11. Second, thelinking component searches and discovers links betweenselected datasets. The links discovered by this componenthave an effect on the efficiency of the source selection, onthe query planning, and on the overall query executionover distributed SPARQL endpoints. Third, the scalablequery federation component: it a single-point-of-accessthrough which distributed data sources can be queried inthe concerto.The scalable query federation is based on the SPARQLquery federation engine called SAFE [19], which hasbeen developed for accessing distributed clinical trialrepositories. SAFE provides a single-point-of-accessthrough which distributed data sources can be queriedin unison. SAFE has been adapted to improve the effi-cient integration of data from the different COSMIC,TCGA, REACTOME, KEGG, GO SPARQL endpoints.More specifically, SAFE makes use of a favorable distribu-tion of data to reduce the number of sources required forprocessing federated SPARQL queries (without compro-mising recall). SAFE retrieves results from the large-scalerepositories by (i) efficient source selection as per thecapabilities of genomics repositories; (ii) query planningmechanism to decompose a query and build resultant dataset from several sub-queries; (iii) query optimisation toJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 5 of 16Fig. 3 BIOOPENER: Linking & Querying Cancer Genomic Resourcesexecute the sub-queries; and (iv) query execution mech-anism retrieve and integrate results. This approach isbased on the principle that integrated data sources allowquerying of multiple data sources in a single search,independently of their status being distributed or cen-tralized, whereas traditional methods of data integra-tion rather map the data models to a single unifiedmodel.RDFizationThe raw data files  of COSMIC and TCGA repos-itories  are available in the tab separated text (tsv)format, which are transformed into the RDF formatusing our in-house RDFizer tool that generates the N3triples. The transformed RDF data from each cancertype are hosted as different SPARQL endpoints. The fourtypes of data have been included from COSMIC, i.e.,gene expression, gene mutation, CNV, and methylation.From TCGA we have RDFized three types of data, i.e.,CNV, gene expression and methylation for four cancertypes, namely Ovarian Serous Cystadenocarcinoma (OV),Cervical Squamous Cell Carcinoma and EndocervicalAdenocarcinoma (CESC), Uterine Corpus EndometrioidCarcinoma (UCEC) and Uterine Carcinosarcoma (UCS).Table 1 shows the overall statistics of the RDF datasets:row 1 represents for the COSMIC gene expression datathe corresponding triples generated (column 3), the num-ber of subjects (column 4), the number of predicates(column 5), the number of objects (column 6) and its RDFdata size (column 7). Rows 2-4 represent the same typeof data for the COSMIC gene mutation, CNV and methy-lation data, respectively. A total of 154 million recordshas been RDFized, producing approximately 1.2 billiontriples, for COSMIC datasets. Row 5-8 represents thestatistics for the RDF version of TCGA-OV, TCGA-CESC,TCGA-UCEC, and TCGA-UCS, respectively. Rows 9-10represent the RDF data statistics for KEGG, REACTOMEand GOA datasets, respectively. These three datasets areexternal as we have not transformed them into the RDFformat but instead used the already available RDF versionsfrom Bio2RDF.LinkingWe propose a linked data based approach to create corre-spondences (links) between dispersed cancer and biomed-ical datasets. These datasets contain rich information andhelpful in answering the biological questions targeted inthis article. These links, once identified and established,will sustain and support the query federation over dis-tributed repositories (discussed in the Scalable queryfederation section).COSMIC and TCGA linking: we perform linking of theCOSMIC and TCGA datasets. We have employed theowl:sameAs construct to establish links across entitiesbased on the semantic properties highlighted in Fig. 1.For example, the entities that contain information aboutGene Symbol, TCGA_ID, ENSEMBL ID have been linkedusing owl:sameAs. An example link between COSMICand TCGA is shown in the Listing 1, where two COSMICsample ids have been identified as being identical to twoTCGA patient bar code ids.Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 6 of 16Table 1 RDF Data StatisticsNo. Data Triples Subjects Predicates Objects Size (MB)1 COSMIC GE 1184971624 148121454 18 148240680 100002 COSMIC GM 83275111 3620658 23 9004153 14003 COSMIC CNV 8633104 863332 10 921690 1224 COSMIC Methylation 170300300 8292057 22 603135 28005 TCGA-OV 81188714 10974200 15 4774584 37746 TCGA-CESC 3763470 627652 43 481227 495577 TCGA-UCEC 553271744 19233824 91 68370614 846878 TCGA-UCS 1120873 183602 36 188970 100189 KEGG 50197150 6533307 141 6792319 430210 REACTOME 12471494 2465218 237 4218300 95711 GOA 28058541 5950074 36 6575678 5858<Link?1><Source>COSMIC</Source><Target>TCGA?OV</Target><link>cosmic:TCGA?13?0920<sameAs>tcga:TCGA?13?0920</link></Link?1><Link?2><Source>COSMIC</Source><Target>TCGA?OV</Target><link>cosmic:TCGA?24?1850<sameAs>tcga:TCGA?24?1850</link></Link?2>Listing 1 COSMIC and TCGA Linking ExampleThe example links generated in our use-case are shownin the Fig. 4. The COSMIC and TCGA datasets havebeen integrated using the owl:sameAs construct. Forinstance, MYH7 (which is an RDF resource of type GeneSymbol) in both COSMIC and TCGA datasets is linkedusing owl:sameAs. To understand the promotor genesand their deviation, the methylation datasets of COS-MIC and TCGA are linked to retrieve beta values for agiven set of CNVs. For instance, cg00000292 which is anRDF resource of type Composite Element REF in bothCOSMIC and TCGA datasets have been linked usingowl:sameAs. Similarly, Fig. 4 shows the owl:sameAslinks between COSMIC and TCGA datasets for TCGA-13-0920 and TCGA-24-1850 (RDF resources of typeSample_ID).Linking COSMIC and TCGA with REACTOME,KEGG, & GO: We link COSMIC and TCGA with GeneOntology (GO) datasets to understand the biological pro-cessed involved with each mutation or CNVs and theunderlying impact of these mutations on cancer andhealthy cells. From the Fig. 4, it is evident that wehave linked ENSMUSP00000018795  which is an RDFresource of type Ensemble ID  in COSMIC datasetwith the similar resource in GO dataset. This will helpin retrieving the gene behavior of healthy cells (fromIllumina Body Map) compared to the diseased TCGAsamples by tracking the GO process involved in the onco-genesis. By enabling links between COSMIC and GOdatasets, we are now able to find links across Reactomeand KEGG datasets. This will allow tracking the changesin healthy cells based on their pathway activities to iden-tify the disease and biological process related pathways.For instance, the Ensemble ID from COSMIC is linkedwith the Ensemble ID in GO dataset providing us theGO processes and the GO IDs associated with theseprocesses. These are further linked with their respec-tive KEGG and Reactome IDs. The linking across thesedatasets are shown in Fig. 4.The number of links generated in case of COSMICand TCGA datasets, and the number of identified linksbetween KEGG, GO, and Reactome datasets are shown inthe Fig. 5. For instance, a total number of 121916 links aregenerated in COSMIC to link them with TCGA. Similarly,46112 links are generated to integrate TCGA with TCGAMethylation datasets, 891612 links are generated to linkTCGA Methylation dataset with GOA (Gene OntologyAnnotation) dataset, and 41424 links are generated tointegrate TCGAMethylation dataset with Reactome.On the other hand, we identified a total of 1049858existing links  within Bio2RDF  between GOA and GOdatasets. A total of 1810 outgoing links to KEGG fromGOand 7359 incoming links to GO from KEGG were identi-fied. A total of 28808 links were discovered between GOand Reactome datasets.Scalable query federationWe have developed a query federation engine  calledSAFE  for accessing sensitive clinical data at differentJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 7 of 16Fig. 4 Example Links between COSMIC, TCGA, KEGG, REACTOME, and GO Datasetslocations [19]. Two main changes have been intro-duced to SAFE for efficiently querying the COSMIC,TCGA, KEGG, Reactome, and GO SPARQL endpoints.First, standardise RDF query representation: in the ini-tial version [19], SAFE issues queries for statistical clin-ical information stored within distinct names graphsfor RDF data cubes [20]. Therefore, the internal queryprocessing (i.e., source selection, query planning, queryexecution) had to be adapted to query the regularRDFized versions of the COSMIC, TCGA, KEGG, Reac-tome, and GO datasets. Second, access control hadto be disabled: SAFE imposes restrictions for data-access as a feature (defined as Access Policy Model[19]) while federating queries over multiple clinical sites,i.e., imposing the data restrictions for different datarepositories. Since experiments conducted in this papermainly involve public repositories this feature has beendisabled.Fig. 5 Link StatisticsJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 8 of 16Figure 3 shows SAFEs three main components withinthe BIOOPENER platform: (i) Source Selection: performsmultilevel source selection based on the capabilities ofdata sources; (ii) Query Planning: filters the selecteddata sources based on access rights defined for eachuser; and (ii) Query Execution: performs the execution ofsub-queries against the selected sources and merges theresults returned.Source Selection: SAFE performs a tree-based two-levelsource selection as shown in Fig. 6. At Level 1, like otherquery federation engines [2123], we do triple-pattern-wise endpoint selection, i.e., we identify the set of relevantendpoints that will return non-empty results for the indi-vidual triple pattern in a query. At Level 2 (unlike otherquery federation engines), SAFE performs triple-pattern-wise named graph selection, i.e., we identify a set of rel-evant named graphs for all relevant endpoints alreadyidentified at Level 1. SAFE relies on data summaries toidentify relevant named graphs.Query Execution: The Listing 2 shows an SPARQLquery, which federates across COSMIC and TCGA dataasking for genomic loci of a mutated gene by chro-mosome start points which then returns the diseasemetastasis information along with the mutation type.Answering such a query requires the integration ofCOSMIC with TCGA and merging results from bothTCGA and COSMIC, and thus has to make use ofquery federation. The results for the first four triplepatterns in the given query (i.e., cosmic:sample,cosmic:gene, cosmic:start) are fetched fromCOSMIC and the results for the next four triple patterns(i.e., tcga:hybrid_ref, tcga:gene, tcga:start)are fetched from TCGA. Further, both results aremerged on the basis of the last triple pattern (gene_cowl:sameAs gene_t) which integrates COSMIC withTCGA. Sample results for this query can be seenin Fig. 9.?cosmic_meth a cosmic:Methylation;cosmic:sample ?sample;cosmic:gene ?gene_c;cosmic:start ?start_c.?tcga_meth a tcga:Methylation;tcga:hybrid_ref ?tcga_id;tcga:gene ?gene_t;tcga:start ?start_t.?gene_c owl:sameAs ?gene_t.}Listing 2 SPARQL Query Federation: Genomic loci of a mutatedgene by chromosome start pointsIn our initial work [10] we queried mutations and CNVdata to identify the novel mutations and their somaticbehavior from healthy to cancer cells. The Listing 3 showsa SPARQL query, which extracts promoter level changesoccurred due to mutations extracted from query shownin the Listing 2. This requires linking across the COSMICandTCGAMethylation datasets. The first three triple pat-terns fetch data from COSMIC and the next three triplepatterns fetch data from TCGA. The last triple patternprovides a link  owl:sameAs between genes  for mergingdata from both the data sources.?cosmic_meth a cosmic:Methylation;cosmic:gene ?gene_c;cosmic:beta_value ?beta_value_c.?tcga_meth a tcga:Methylation;tcga:gene ?gene_t;tcga:beta_value ?beta_value_t.?gene_c owl:sameAs ?gene_t.}Listing 3 SPARQL Query Federation: Mutations causingpromoter level changesThe SPARQL query listed in Listing 4 have covered 3distinct sources, i.e., methylation from TCGA and COS-MIC datasets with associatedGeneOntology AnnotationsFig. 6 Tree-based two level source selectionJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 9 of 16(GOA). TCGA provides the changes in methylation percomposite element, whereas in COSMIC we have suchchanges on the gene level. To retrieve both the gene andpromoter level information, we have queried genes fromboth data sources and extracted all the promoter regions.Once the promoter regions are identified, it is essential tounderstand the processes involved in these regions. Thishelped us to query GOA for extracting the processes onthe promoter and gene levels. If a gene level change donot comply with promoter level changes, it is an indica-tion of what processes of the gene have mutated them.Such results can be obtained through a federated querywith three data sources, i.e. COSMIC, TCGA, and GOA.The Listing 4 provides an example federated query wherethe first three triple patterns are answered fromCOSMIC,the next three triple patterns are answered from TCGAand the seventh triple pattern merges result obtainedfrom COSMIC and TCGA through gene. The eighthand ninth triple patterns fetch data from GOA which isfinally merged with COSMIC and TCGA datasets usingthe gene information.?cosmic_meth a cosmic:Methylation;cosmic:gene ?gene_c;cosmic:beta_value ?beta_value_c.?tcga_meth a tcga:Methylation;tcga:gene ?gene_t;tcga:beta_value ?beta_value_t.?gene_c owl:sameAs ?gene_t.?go_gene go?vocab:process ?process.?process dcterms:title ?cell_cycle.?gene_t owl:sameAs ?go_gene.}Listing 4 SPARQL Query Federation: Methylation changesThe SPARQL query shown in Listing 5 finds associ-ations between the genes, pathways and biological pro-cesses. We queried the healthy genes from Illumina BodyMap against all mutations obtained from TCGA andCOSMIC to find their DNA and promoter level methyla-tion changes. In order to explore the gain and loss on adisease at the phenotype level, we have included KEGGand REACTOME sources which map each discoveredgene with its biological process for phenotype and processdriven pathways. The Listing 5 shows a federated SPARQLquery, where the first three triple patterns are answeredfrom TCGA; and the next five triple patterns fetch andmerge data from REACTOME and GOA. The last fivetriple patterns obtain results from KEGG and merge themwith the rest of results.?tcga_meth a tcga:Methylation;tcga:gene ?gene_t;tcga:beta_value ?beta_value_t.?go_gene go?vocab:process ?process.?process dcterms:title ?cell_cycle.?gene_t owl:sameAs ?go_gene.?pathway a biopax:Pathway;biopax:displayName ?display_name;biopax:organism ?organism;biopax:xref ?id. ?id biopax:id ?go_gene.?kegg_res a kegg:Resource;rdfs:label ?label;dcterms:title ?title;kegg?vocab:reference ?ref;kegg?vocab:x?go ?process.}Listing 5 SPARQL Query Federation: Genes, pathways, andbiological processesThe Listing 6 retrieves the methylated promotorregions. The query shown in Listing 6 extracts the locationof methylation based on the input genes, composite ele-ment REF (promotor region) and chromosome number.For instance, we have queried MYH7 (gene) for promo-tor region cg05744229 at the chromosome 14 (region ofmethylation) and extracted two promotor regions fromTCGA and COSMIC with the start value of DNA pro-motor range such as 23904678 (TCGA) and 23435469(COSMIC).SELECT ?promoter_region ?start_c ?start_tWHERE {?cosmic_meth a cosmic:Methylation .?cosmic_meth cosmic:chromosome ?chr.?cosmic_meth cosmic:gene ?promoter_region .?cosmic_meth cosmic:start ?start_c . FILTER (?promoter_region = <http://sels.insight.org/cancer?genomics/gene/cg05744229>)?tcga_meth a tcga:Methylation .?tcga_meth tcga:gene <http://sels.insight.org/genomics/gene/MYH7>.?tcga_meth tcga:chr ?chr.?tcga_meth tcga:start ?start_t. FILTER (?chr = <http://sels.insight.org/genomics/chrom/14>)}}Listing 6 SPARQL Query Federation: Methylated promotorregionsListing 7 shows an example federated SPARQL queryderived from the Listing 2 for a specific gene, namelyMYH7. Similarly, we have executed the federated queriesshown in the Listings [2-6] for each of the hundred (100)genes extracted from the Illumina Body Map, mentionedabove.?cosmic_meth a cosmic:Methylation; cosmic:sample ?sample; cosmic:gene ?gene_c; cosmic:start ?start_c.?tcga_meth a tcga:Methylation; tcga:hybrid_ref ?tcga_id;tcga:gene tcga:MYH7; tcga:start ?start_t.?gene_c owl:sameAs tcga:MYH7.}Listing 7 SPARQL Query Federation: Genomic loci of MYH7 geneby chromosome start pointsThe query execution time for these gene-specificqueries is shown in the Table 2. The Query columnJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 10 of 16Table 2 Query Execution Time (QE=Query Execution)Query QE Time (msec) Results (No. of Triples) DatasetsListing 2 2110 21390 (TCGA)(COSMIC)Listing 3 5732 33264 (TCGA)(COSMIC)Listing 4 43092 63765 (TCGA)(COSMIC)(GOA)Listing 5 263463 232848 (TCGA)(GOA)(REACTOME)(KEGG)Listing 6 3481 25669 (TCGA)(COSMIC)lists individual queries (e.g., listings [2-6]), QE Time,Results (No. of Triples) and Datasets columns showthe query execution time in millisecond (msec), numberof triples returned as a result and the datasets required forexecuting individual queries.ResultsWe analyse the genes having RPKM value > 0.3747 anddifferentially expressed in all tissue types. Figure 7 shows alist of 100 genes retrieved from the HBM datasets, whichare highly expressed in 16 different tissues. We have iden-tified potential cancer types based on the gene patternsfor different tissues that helped further to understand thebehavior of most amplified cancer types. The overall goalof this study is to understand the relevance and associa-tion of mutation, genes expression, and promoter regionby:? Analysing the normal tissues expression levels,enriched and affected pathways along with theirassociated expression levels and changes obtainedfrom the HBM 2.0 datasets.? Analysing the normal tissues expression levels againstthe somatic mutations linked and retrieved from theCOSMIC and TCGA datasets.thyroidovaryprostatetestisbrainbreastcolonadiposekidneyadrenalleukocytelunglymph nodeskeletal muscleheartliverAPCSAMBPALBFGAAPOC3APOHGCFGL1APOA2CRPORM1ORM2HPFGBFGGSAA2SERPINA1APOA1AGTSAA1RBP4C3SCDTGPRM1PRM2TNP1DESPDK4MT?TPKLHL41MYBPC1CKMGAPDHACTA1MT?RNR1MT?RNR2MT?ND1MT?ND2MT?CO2MT?ND4MT?CO3MT?ND3MT?CYBMT?ATP6PLNMYL3ACTC1MYH7MYL2FABP3MBMT?ND5MT?CO1MT?ND4LMT?ATP8MTATP6P1MT?ND6MALAT1GPX3SPP1HBBSCGB1A1HLA?EFTLB2MTMSB4XSRGNTMSB10HLA?DRACD74LYZS100A9S100A8RPS12ACTBEEF1A1FABP4TXNIPSEMG1MYL9RPS27RPS11IGFBP4IGLC3IGLC2IGLV3?19IGKCIGHV3?23IGHG2IGKV4?1IGHG1IGLV3?25IGKV3?20IGHV1?2IGKV1?5IGHMIGHA1JCHAINCCL21Fig. 7 HBM: List of genes expressed in all tissues and highly expressedJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 11 of 16? Classifying the mutations obtained from above twosteps in terms of biological processed and pathwaysfrom GO, KEGG, and REACTOMEWe now discuss and analyse the results obtained fromthe BIOOPENER platform through linking and queryingthe cancer and biomedical repositories.Analysis: HBM, COSMIC, and TCGAInitially, we have selected top 100 genes that are highlyexpressed in all 16 tissues as shown in the Fig. 7 to(i) retrieve their CNV, mutation, gene expression andmethylation annotations from cBioPortal12; (ii) retrievemethylation from CNV annotator13 and UCSC CancerGenomics Browser14; and (iii) retrieve mutation datasetsfrom TCGA [24]. The results from TCGA (Fig. 8) clearlyindicate a mutation frequency elevated distribution ofthese genes in UCS, CESC, UCEC and OV cancers. InFig. 8 TCGA query output from cBIO Portal (Blue:Deletion,Red:Amplification, Green:Mutation, Brown:Multiple Alterations) [43]Fig. 8 we observe average percentage case mutations inthe UCS, UCEC, CESC and OV cancers are 87.5% ,58.3%,57.6%, 81.4% respectively. This outcome justifies the selec-tion of UCS, UCEC, CESC and OV as good candidates forfurther investigation due to its elevated amplification rateand its multiple repetition in different experiments.This study targets genes based on their contribution inmutations15, the listing 8 shows the highly relevant drivergenes transforming healthy human tissues into diseasedones for respective cancer types.OV: TG, MRPS12, GAPDH, TXNIP, S100A9, S100A8, RPS27, ALB, CRP, LYZ, and MYH7CESC: ND5, TG, AGXT, MYH7, FGA, APOC3, APOA1, C3, APCS,FBF1, SERPINA1, S100A9, and TXNIPUCS:MRPS12, TG, SEMG1, ND5, DLC1, CKM, ND4, ND1, FGL1,and RPS27UCES: TG, MYH7, DLC1, C3, TXNIP, FGA, AGT, S100A8, CRP,S100A9, APCS, and GCListing 8 Highly relevant driver genes for the OV, CESC, UCS, andUCES cancer typesThe overlap and frequency among these four cancertypes results into the discovery of top 20 biomarkersshown in the listing 9). Table 3 shows the potentialchromosome locations chr14,chr5,chr6,chr19 and genesTG,TXNIP,GC,MYH7 with high relevance in the progres-sion of four gynecological cancer types.TG, MRPS12, MYH7, DLC1, GAPDH, TXNIP, C3, ND5, S100A8,RPS27, FGA, AGT, CRP, ALB, LYZ, APCS, GC, APOA2, MYBPC1,ACTA1Listing 9 Top 20 Biomarkers for the OV, CESC, UCS, and UCEScancer typesFigure 9 shows the COSMIC and TCGA annotations.The CNV datasets doesnt use Gene symbol property(or predicate) and it is important to map (or link) genomeregions with gene symbols to retrieve CNV informationfrom different datasets. We implemented a linking rulebased on the chr_no,chr_start and char_end properties(or predicates) to retrieve the CNV information acrossdatasets to identify genes within the extracted loci. Resultof this annotation are shown in the Table 3. It is evi-dent that the MYH7 gene has many copies reported inthe COSMIC datasets as well as in the TCGA datasetssuggesting it a potential biomarker for four gynecolog-ical cancer types. The TG and MYH7 genes are highlymutated as they are repetitively appearing on multiplechromosomes. For instance, MYH7 primarily carried theLOSS type of a mutation for chr14 which is a dominantmutation with all its regulation of over, under and nor-mally expressed. Translational researchers may want torepeat and re-validate the study for Pubmed ID:1398522with the beta value  as a measure of methylation  of0.041999536. The scaled estimation (Tumour purity) ofJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 12 of 16Table 3 loci information for highly expressed gene in ovarian cancer from HBM 2.0Chr Star-End Mutation Type Genes PMID19 90910 -715430 GAIN FGF22, RNF126, TG 20668451206684509 4069657-4684967591967-60865911090336-110988918009428-80155968109010-81212571373387-138372511090336-1109889110547511-105479233113846 -31347388115293 -81214879269903 -929441546587-5107005106680-5106800LOSS/GAIN LKB1,P16INK4A,TRAF2,XPA,PTCH1,FANCC,DMRT3,WNK2,C9orf89,SYK,CKS2,CTSL1,NTRK2,KIF27,PTPRD,TLE4,CEP78,GNAQ,PRKACG21062161173116761658517020668451217813076 149661-384546 LOSS TAP1,NOL7,CD83,POUF3,MYH7,PLN,PKIB,PDSS2OSTM1,NUS1,TG,NT5DC1,NR2E1,NKAIN221062161206684512178130720668451217203655 15532-24132 GAIN TRIP13, TRIO,TARS,SUB1,SLC12A7,SKP2,SDHA,RPL37,MYH7,RNASEN,RAI14,RAD1,POLS,PDCD6,PAIP1,OSMR,NNT185590932106216114 23857092-2388648623857082-23886607LOSS MYH6, MYH7, TG, ACTA1 1855909321062161773.555 supports this gene (MYH7) from the methylationaspect to detect promoter level changes in the four cancertypes. Further multiple genomic locations will help clin-ical practitioners to find a potential CNV for a targetedstudy ultimately helping towards a better prognosis.Figure 10 shows the annotation of twenty (20)discovered biomarkers (genes) where promoter levelchanges are occurring on the extreme changes of -veor +ve beta-values in all the four cancer types stud-ied in this article. The most affected genes due tothese promotor level changes are: MYH7, TG, DLC1,S100A8. As reported in our initial work [10] majorchanges are occurring nearby -0.773 beta-value andtheir corresponding composite element reference idsare cg01429391, cg05744229, cg26670875, cg18205205,cg21242212, cg08240074, cg13785779, cg05744229. Mostof these changes are occurring around chromosome 1 and14 and 5 UTR. Next section discusses the mechanismbehind these changes and their pathway analysis.Analysis: GO, KEGG and REACTOMEWe have identified twenty (20) genes in terms of mutationfrequencies and CNV together with the promoter levelchanges in methylation data. However, we are unawareof the mechanism involved in combined effects of thesetwenty (20) genes. We have queried linked pathwaysand coalitions over the GO, KEGG, and REACTOMEdatasets. Figure 11  snippet generated from ClueGO[25, 26]  shows the muscle filament sliding pathway asa key in rare cancer types such as retinoblastoma whereeffective actin filament formation with Myosin (MYH4)is a prime regulator [27]. Our approach has identifiedactin (ACTA1) and myosin (MYH7) combination withMYBPC1 as the potential pathways causing promoterFig. 9 Linked annotations for MYH7 - COSMICJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 13 of 16Fig. 10 Promotor level methylation changes in biomaker geneschanges in gynecological cancers. Its evident that alter-ations in the activity and/or expression patterns of actin-bundling proteins could be linked to the cancer initiationor progression [28]. Haitian Lu, et al. suggests that theacute inflammatory response is associated with cancerdevelopment because inflammatory micro-environmentinhabits various inflammatory cells [29]. A network ofsignaling molecules are indispensable for the malignantprogression transformed cells attributed to the mutagenicpredisposition of persistent infection-fighting agents atFig. 11 Three pathways causing promoter changes in four gynecological cancer typesJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 14 of 16the sites of chronic inflammation causing cancer devel-opment in various tissues [29]. In our case, the rea-son behind significant methylation changes associateswith the pathway peptidyl-cysteine s-nitrosylation. Thedysregulation of s-nitrosylation in severe pathologicalevents including cancer onset, progression, and treatmentresistance leads to controlled epigenetic and treatmentresponse [30]. Figure 10 explains the gene associated witheach pathway and their contribution for OV, CESC, UCS,and UCSC cancer types. In this article, we demonstratedthat well-connected datasets allow to construct complexbiomedical queries (e.g., listings 2-6) covering variety ofgenetic and biological features (cnv, gene symbol, methy-lation, cell cycle, protein, pathway, etc.) that can spanthrough broad range of multiple repositories.Related workKandoth et al. [31] performed a cancer study with 12cancer types to enable logical classifications for the largeamount of data generated by TCGA and ICGC. Saleemet. al. [32] have covered TCGA database with few cancertypes and for a limited number of patient data. Simi-larly, a reduced version of the COSMIC database has beenRDFized to explore on the mechanism of TP53 [33]. Thefederation platform [34] called TopFed is being devel-oped to measure the query execution time on TCGAdata set, which then has been further extended to coverthe biological outcomes identified fromMedline abstracts[35]. A similar platform such as FIREBROWSE16, Web-TCGA [36], and PCAWG17 have been built for TCGAdataset covering a wide range of genomic signaturesand pan-cancer analysis. Gene and methylation annota-tion platforms such as omics4tb18 and Genevisible [37]help to decipher individual genes and their associationannotated from TCGA. From the computational perspec-tive, our goal is not to create yet another repository (ordatabase), but to link the already existing ones for use invarious analytical methods. We demonstrated that well-connected datasets allow to construct complex biomed-ical queries (e.g., listings 2-6) covering variety of geneticand biological features (cnv, gene symbol, methylation,cell cycle, protein, pathway, etc.) that can span throughbroad range of multiple repositories. The enrichmen-t/linkage between COSMIC and TCGA datasets had beencrucial to identify novel mutations. The approaches takenin DoCM [38], ICGC [39], and DIRECT [40] are comple-mentary to our work in the sense that, discoveries sug-gested by the BIOOPENER platform are the most likelymutations/genes/pathways which can be further validatedthrough creating links with the well-curated reposito-ries (DoCM, ICGS, and DIRECT ). Such validation isoutside the scope of this article; however, we do planto include well-curated databases in the next phase ofBIOOPENER project. Similarly, we plan to extend linkingwith the ICGC [39] datasets that contains primary andblood samples providing further insight into the metas-tasis of primary tissues. Our current work covers copynumber variation (CNV), genes, somatic mutation, andpromotormethylation which targets highlymutated genes(on different tissues) and associated pathways. As far aswe know, the work presented in this article is one of thefirst initiatives in discovering biomarkers and pathwaysfor female gynecological cancer types covering five large-scale cancer and biomedical repositories.DiscussionAs discussed above, the NGS technologies are produc-ing a massive amount of sequencing datasets [5, 8]. Atop-up of approximately 40 petabytes of genomic infor-mation every year is foreseen from a wide variety ofdata sources published by human genome research cen-ters worldwide [41]. Often, these datasets are publishedfrom isolated and different sequencing facilities. In cancergenomics, description of biological and genetic entitiesare available in several overlapping and complementarydata sources containing complex genomic features, stud-ies, and associations of such features [17, 42]. In orderto understand the tumorigenesis, it is often the case thatseveral genetic features, diseases, medical history, etc. arestudied together, therefore, one of the key challenge incancer genomics  a cornerstone of precision medicine is to discover gene-disease-drug data links and associ-ations which may provide novel insight into new drugdevelopment techniques tailored specific for an individ-ual patient (or a group of patients) targeting prevention,diagnosis and treatment of the diseases.In cancer genomics field massive amount of data existwith complex associations. To understand these complexassociations, it requires to fetch all possible gene-disease-drug combinations, for instance:? Multiple pathways are involved to translate aparticular gene? A single disease can be treated by eliminating effectof the combination of multiple drugs? Selection of these drugs is majorally based on theinhibitors (i.e., combination of gene-pathways)? Effect of one pathway alteration can change themodification of single gene and yields into multiplegenesIn this article, we aimed to understand the associationsbetween genetic, cellular and molecular features acrossisolated cancer datasets giving insight into cancer pro-gression from normal to diseased tissues. Correlation ofgenes in OV, UCS, UCEC, & CESC clearly indicates thatgynecologically induced cancers do have common mech-anism and overlapping pathways. Which means, a drugJha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 15 of 16created for one cancer type has a higher probability to beeffective for other associated cancer types.ConclusionIn this paper, we have presented a data-interlinked plat-form called BIOOPENER which enables querying dif-ferent types of mutations and genomic alterations tocontribute to molecular and clinical insights of cancerby defining most relevant variants and their prioritiza-tion. This knowledge could be highly advantageous for atargeted therapy and precision medicine based on geneexpression data. The presented experiments are based onCOSMIC, TCGA, REACTOME, KEGG, GO and HBM2.0 datasets and have been used to identify sets ofgenes with relevance for four female gynecological cancertypes - Ovarian (OV), Uterine Corpus Endometrial Carci-noma (UCS), Uterine Carcinosarcoma (UCEC), CervicalSquamous Cell Carcinoma and Endocervical Adenocar-cinoma (UCES) - covering the 16 healthy tissue-specificgenes from Illumina Human Body Map 2.0. We discov-ered 20 biomarkers (genes) in terms of mutation frequen-cies and CNV along with the promoter level changes inmethylation data. We discovered three potential pathwayscausing promoter changes in gynecological cancers. Infuture, we plan to extend by covering the breast cancertype including additional genomic signatures, e.g., fusiongene, structural variations.Endnotes1 http://www.nature.com/nature/journal/v537/n7619_supp/full/537S49a.html.2 https://tcga-data.nci.nih.gov/tcga/.3 http://cancer.sanger.ac.uk/cosmic.4 https://www.ebi.ac.uk/gxa/experiments/E-MTAB-513.5 http://bioopenerproject.insight-centre.org.6 https://github.com/yasarkhangithub/BioOpener/blob/master/Top_100_Gene_List.txt.7 http://bio2rdf.org/.8 ftp://ftp.ebi.ac.uk/pub/databases/RDF/reactome.9 http://download.bio2rdf.org/release/3/kegg/.10 http://download.bio2rdf.org/release/3/goa/.11 http://bio2rdf.org/.12 http://www.cbioportal.org/.13 https://omictools.com/cnv-annotation-category.14 https://genome-cancer.ucsc.edu/.15 https://github.com/yasarkhangithub/BioOpener/blob/master/Mutation_Key_Genes_Cancerwise.xlsx.16 http://firebrowse.org/.17 http://pancancer.info/.18 http://www.omics4tb.org/.AcknowledgmentThis article is based on a conference paper discussed at the SWAT4LS 2015,Cambridge, UK [10].FundingThis publication has emanated from research supported by the research grantfrom Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289.Availability of data andmaterialsThe BIOOPENER online demonstration website http://bioopenerproject.insight-centre.org/ is available for the scientific uses and the relevant datasets(in RDF) shown in the Table 1 are available at http://bioopenerfiles.insight-centre.org/.Authors contributionsAJ designed the study and helped in RDF data conversion, analysis andconcluding domain results. YK designed and implemented the queryfederation and RDF conversion. MM and QM discovered the links acrosscancer repositories. RK contributed to RDF data conversion and raw dataprocessing. AZ critically revised the manuscript. DR and RS have jointlysupervised the article. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 12 July 2016 Accepted: 30 August 2017Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 DOI 10.1186/s13326-017-0154-9RESEARCH Open AccessAnalysis and visualization of diseasecourses in a semantically-enabled cancerregistryAngel Esteban-Gil1, Jesualdo Tomás Fernández-Breis2* and Martin Boeker3AbstractBackground: Regional and epidemiological cancer registries are important for cancer research and the qualitymanagement of cancer treatment. Many technological solutions are available to collect and analyse data for cancerregistries nowadays. However, the lack of a well-defined common semantic model is a problem when user-definedanalyses and data linking to external resources are required. The objectives of this study are: (1) design of a semanticmodel for local cancer registries; (2) development of a semantically-enabled cancer registry based on this model; and(3) semantic exploitation of the cancer registry for analysing and visualising disease courses.Results: Our proposal is based on our previous results and experience working with semantic technologies. Datastored in a cancer registry database were transformed into RDF employing a process driven by OWL ontologies. Thesemantic representation of the data was then processed to extract semantic patient profiles, which were exploited bymeans of SPARQL queries to identify groups of similar patients and to analyse the disease timelines of patients.Based on the requirements analysis, we have produced a draft of an ontology that models the semantics of a localcancer registry in a pragmatic extensible way. We have implemented a Semantic Web platform that allowstransforming and storing data from cancer registries in RDF. This platform also permits users to formulate incrementaluser-defined queries through a graphical user interface. The query results can be displayed in several customisableways. The complex disease timelines of individual patients can be clearly represented. Different events, e.g. differenttherapies and disease courses, are presented according to their temporal and causal relations.Conclusion: The presented platform is an example of the parallel development of ontologies and applications thattake advantage of semantic web technologies in the medical field. The semantic structure of the representationrenders it easy to analyse key figures of the patients and their evolution at different granularity levels.Keywords: Biomedical informatics, Semantic web, Cancer registry, OntologyIntroductionCancer registries are an important part of the healthinformation systems in local and regional health orga-nizations. Regional and epidemiological cancer registriesare the foundation for cancer research and the qual-ity management of cancer treatment. In most devel-oped countries, the operation and the sampling of datain cancer registries are statutory. Cancer registries are*Correspondence: jfernand@um.es2Dpto. Informática y Sistemas, Facultad de Informática, Universidad de Murcia,IMIB-Arrixaca, Facultad de Informática, Campus de Espinardo, 30100 Murcia,SpainFull list of author information is available at the end of the articlecomplex structures for the documentation and analysisof data from patients diagnosed with cancer [1, 2]. Dif-ferent types of cancer registries collect patient data frominstitutions (institutional), regions (regional) or completelarger areas (epidemiological). Whereas epidemiologicalregistries provide mainly population-based informationonmorbidity and mortality, institutional and regional reg-istries can provide fine-grained information on treatmentand conditional survival.The information of regional cancer registries serves dif-ferent requirements such as the quality control of patientcare, the comparison of patient-related outcome param-eters and research support. Institutional and regional© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 2 of 16registries are also the main data source for epidemiolog-ical cancer registries. Regional cancer registries collectinformation about diagnosis, therapies and course of thedisease [3], the most important being the histopathologyof the primary tumor, including tumor staging and grad-ing. The long-term follow-up of the patients vital statusis one of the resource-intensive tasks of tumor registriesproviding the basis for survival analysis.Different software cancer registries solutions are cur-rently available, such as METRIQ1, OncoLog Registry2 orCNEXT3. The standardisation of the cancer registry soft-ware is difficult because of a large set of rapidly changinglegal and scientific requirements. Most of these softwaresolutions suffer from two main limitations. The interop-erability with other health applications such as ElectronicMedical Records (EMRs) is limited, which is a typicalproblem of clinical information systems [4]. The hetero-geneity of the underlying data models is a consequence ofthe difference between data models in current cancer reg-istry software [5, 6]. This imposes severe limitations onresearch and on the progress of cancer studies when clini-cal research activities need to integrate data from differentcancer registries of several regions.There have been proposals to overcome the afore men-tioned problems. In [7] the authors use the Unified Mod-elling Language for modeling cancer registry processesin a hospital. In [8] the authors propose a set of indica-tors to evaluate specific quality measures in cancer care,and [9] attempts to optimise cancer registries by means ofknowledge-based systems for monitoring patient records.Unfortunately, these approaches do not guarantee thegeneration of standard models and do not provide sat-isfactory solutions to scenarios which require customis-able, comparative analyses and data linking to externalresources [5].On the technical side, the Semantic Web stack canbe employed to provide information with given well-defined meaning, better enabling computers and peopleto work in cooperation [10]. Ontologies [11] constitute thestandard knowledge representation mechanism for theSemanticWeb, in which languages such as theWebOntol-ogy Language (OWL) enable a formal representation ofthe domain of interest. Important international initiatives[12, 13] strive to ensure that the Semantic Web becomesa fundamental system to achieve consistent and mean-ingful representation, access, interpretation and exchangeof clinical data. These semantic web technologies havealready been used to represent cancer diseases, e.g. in [14],an ontology models clinic-genomic cancer trials. Ontolo-gies were also proposed to represent certain types ofcancer disease [15, 16].The main objective of this study is the development ofa Semantic Web platform that facilitates the analysis andvisualisation of data from cancer registries including (1)the representation of the disease course of a patient, (2)the representation of the aggregated disease courses of agroup of patients, and (3) the definition of customisabledashboards for patient selection and visualisation of thedata. The use of simulated data demonstrates the viabilityof incorporating a local cancer registry into this model. Acomparative performance analysis of relational databasesand semantic repositories demonstrates excellent perfor-mance measures for the semantic repository.BackgroundStandards and classification systems in cancer registriesMost information contained in cancer registries is derivedfrom primary care interactions. For the purpose of struc-tured secondary documentation, tumor documentariescarefully reprocess primary documentation. In manycountries, a standardised common dataset has been devel-oped to better support exhaustive data exchange withthe epidemiological cancer registries, proposing the clas-sification of diagnostic and treatment information withclinical coding systems.The most important clinical classification systemapplied in cancer registries is the International Classifica-tion of Diseases version 10 (ICD-10) [17]. This classifica-tion system is divided in chapters, with blocks of diseases.For example, chapter II includes the classification for neo-plasms between the blocks C00 and D48. These blocks aresubdivided in hierarchies that further specify the diagno-sis. The ICD-O is a domain-specific extension of ICD forcancer diseases. ICD-O is a dual classification allowing thecoding of topography (tumor site) and tumor morphol-ogy. SNOMED CT [18] has adopted ICD-O codes for theclassification of tumor morphology.Several staging systems for cancer have evolved overtime and continue to evolve with scientific progress. Themost important classification system is the Classificationof Malignant Tumours (TNM) [19], which is related tothe description of the anatomical extent of the disease.This system is under constant development by the Unionfor International Cancer Control and the American JointCommittee on Cancer. The TNM staging is based on thesize or the extent of the primary tumor, the metastases inregional lymph nodes, and the presence of metastasis orsecondary tumors formed by the spread of cancer cells toother parts of the body.Clinical procedures are also encoded with coding sys-tems such as the ICD10-PCS (Procedure Coding System)[20] denoting aspects such as the clinical classification ofthe procedure, the surgical section or the body system.Visualisation of clinical recordsFrom the emergence of the electronic medical record(EMR), the amount of data has increased exponentially[21, 22]. The main objective of the EMR is representingEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 3 of 16the clinical characteristics of a patient from several per-spectives. For a variety of reasons [23] this objective hasnot yet been achieved.Visualisation methods are one way of facilitating therepresentation and flexible exploitation of EMR data.According to [24] there are two types of visualisation ofEMR data: Multimedia visualisation includes video, audio,graphical plots, rich text, hyperlinks and othermultimedia contents [25, 26]. Temporal visualisation depicts clinical timelines ofthe health state of the patient [27, 28]. Some of theserepresentations are able to generate a prospective ofthe future clinical characteristics of the patient usingdata mining techniques over all the EMR [29, 30].TheTimeLine project [24] combines the two approacheswith four key aspects of the user interface: demographicsand encounter information, medical problem list, graph-ical timelines and the data viewer that allows the naviga-tion over all data of the patient as bone scan, laboratorydata, etc. The main advantage of this project is that theclinician can visualise all patient data without switchingbetween various information systems.Semantic exploitation of dataSemantic representationThe methods for the transformation and semantic repre-sentation of information follow similar approaches. Theycan be classified in (1) those which generate a represen-tation of the datasets in semantic formats being the resultof the application of mappings between the entry datasource and the ontology that provides the meaning forthe content; and (2) those which permit ontology-baseddata access using data in traditional formats but queryingwith semantic web query languages. Next, we describe themost popular approaches and tools from both categories: D2RQ (Accessing Relational Databases as VirtualRDF Graphs) allows to query data stored inrelational databases using SPARQL on virtual RDFgraphs [31]. This tool is totally automatic. Triplify allows to publish [32] the content ofrelational databases as Linked Data [33] based on apartially automatic transformation process. Linked Data Views (Virtuoso). OpenLink Virtuoso[34] is a database management system that handlesseveral persistence models (relational, XML,object-relational, virtual and RDF). Persistencemodels stored in Virtuoso can be queried withSPARQL based on the automatic representation asLinked Data Views [35]. XS2OWL (Representation of XML Schemas inOWL syntax). XML schemas can be transformedinto OWL [36]. XML databases can be automaticallytransformed and queried with SPARQL. RDB2OWL (A Database-to-Ontology MappingLanguage and Tool). Approach to transform thedata stored in relational databases into RDF or OWL[37]. The user manually defines mappings betweenthe entries and the outputs. The transforming oflarge ontologies can be tedious. Karma. It links a source model to ontologies togenerate a semantic representation of the data source[38]. This process is partially automatic. Populous. Assistant for building ontologies [39], theprocess being guided by patterns. Populous is able toimport CSV data. SWIT (Semantic Web Integration Tool). Semantictransformation engine capable of generating RDF andOWL repositories from both relational and XMLdatabases [40]. Besides transforming the data, SWITprevents the generation of logically inconsistent datawith the support of DL reasoners. The transformationmethod has three main steps: (1) definition of themapping rules between the fields of the database andthe ontology; (2) generation of the OWL data; and (3)importing the OWL data into the semantic data store.Most approaches are based on the mappings betweenthe relational and semantic primitives of the correspond-ingmodels languages. Performing only a syntactical trans-formation, the meaning of the content is not reallyexploited. In this work we use the SWIT transformationapproach, which preserves the meaning of the contentbased on the specification of mappings between the enti-ties of the source relational schema and the entities of thetarget domain ontology.Semantic queryingThe amount of RDF data, and the development of applica-tions that use semantic web technologies for storing, pub-lishing and querying data has increased constantly in thelast decade [41]. Semantic endpoints in which the userscan exploit the data without any knowledge of SPARQLhave been developed. For example, Natural Language Pro-cessing has been used to develop a question answeringsystem [42]. In other works, the authors use parametrisedqueries to answer questions based on a template [43]. Infaceted search over RDF repositories, the user can refinethe filters over the results of each SPARQL query [41].In the biomedical field, the use of semantic querying islimited to the generation of semantic searchers or dash-boards. BioDash is an example of semantic dashboard thatexploits heterogeneous data sources for drug discovery[44]. Chem2Bio2RDF provides dashboards automaticallycollecting associations within the systems chemical biol-ogy space [45]. In this work, our goal is to go beyond theEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 4 of 16state of the art by allowing users to dynamically definetheir semantic dashboards.MethodsOntology constructionBest practices in ontology engineering recommend toreuse existing content and to create modular ontologies[46]. These recommendations are implemented reusingconcepts from different ontologies so that the resultingontology infrastructure is likely to be a networked ontol-ogy. TheOBOFoundry has also developed a series of prin-ciples for ontology construction which propose principlesfor modularity, orthogonality and reusability [47].The method for constructing the domain ontology usedin this work consisted in identifying the main entities thatshould be represented, searching BioPortal for existingontologies containing classes representing these entities,selecting the most appropriate ones (by our subjectivecriteria), and extending them when necessary. The finalontology has been implemented using Protégé4 in OWL-DL, which is the OWL subset based on DescriptionLogics.Data generation and representationIn this work, we have generated a simulated cancer reg-istry dataset using the statistical distribution of a realregistry dataset, following the method proposed in [48].Data provided by the National Cancer Registry of Ireland5were used to obtain a patient distribution by age. The can-cer registry was accessed on 10-05-2016 and we included533409 cases diagnosed from 1994 to 2013. The patientswere generated in groups classified by gender and 5-yearsage ranges (0-4, 5-9, 10-14, etc.). The last group of patientscontains people older than 85 years old.For each group of patients we have calculated the proba-bility distribution of diagnosing a concrete type of cancer,and the probability distribution of receiving a particulartherapy (surgery, chemotherapy, radiotherapy, hormonaltherapy, ...) for a concrete diagnosis. These probabilitieswere used to assign weights to every type of cancer withits therapies for each group of patients. For example, forpatients between 60 and 64 years old, the probabilities fordifferent types of cancer are breast cancer (0.23), lung can-cer (0.17), prostate cancer (0.17), and colorectal cancer(0.08). For patients within this age range and diagnosedwith colorectal cancer the probabilities of the therapieswould then be: teletherapy (0.44), chemotherapy (0.44)and surgical treatment (0.12). Figure 1 shows the stack ofdistributions. When the random number is between 0.57and 0.64 we assign colorectal cancer as the patients diag-nosis. Then, we generate a new random number to assignthe first therapy and so on.Furthermore, survival and mortality data were used forextracting the evolution of the disease. Finally, we ensuredFig. 1 Schema of probability distribution of diagnoses and therapiesthat the amount of patients with more than one cancerdiagnosis meets the distribution of the real dataset.Our simulated dataset consists in randomised cases. Foreach case, we establish the gender and age of the patient.Then, we apply a partially random distribution algorithmfor getting the patient characteristics. This algorithm usesthe weights assigned to each type of cancer, therapyor course to generate distributions similar to the origi-nal database. This algorithm is able to generate patientswith one or more diagnoses with various therapies andcourses following the probability distribution previouslycalculated.Such data have been represented in RDF by apply-ing SWIT, whose transformation method has three mainsteps: (1) definition of the mapping rules between thedatabase schema and the ontology; (2) generation of theRDF data; and (3) importing the RDF data into the seman-tic data store. We use a semantic repository to store thedata, which integrates two types of data sources: (1) anOWL files server with the formal representation of thedomain, and (2) an RDF repository which stores the data.Virtuoso6 is used as data store [49].Exploitation modelOur approach includes a set of methods for exploiting theinformation model in the semantic repository.Ontology-driven search (ODS)SPARQL is the language used for querying the data store.We use our ontology-guided input text subsystem [50]to make it easier for clinicians to exploit the data ware-house. The main objective is to allow users to design andexecute SPARQL queries without knowing SPARQL. Thistool is an editor for SPARQL queries supported by anOWL ontology. The OWL ontology provides the classesand properties that can be used for creating the SPARQLquery that will be executed on the RDF repository. TheEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 5 of 16construction of the queries begins with the selection ofa main class of the ontology. For example, if we wish tofind patients, then the ODS begins with the selection ofthe ontology class Patient. The user can define filters overthis class by using the data properties or object proper-ties of the ontology. The use of owl:ObjectProperty permitsto include other concepts in the query. For example, ifwe wish to find patients whose diagnosis is lung cancer,the user can select the owl:ObjectProperty hasDiagnosis,which is associated with the class Patient, which permitsto use the owl:ObjectProperty Pathological structure ofthe class Diagnosis to select the class representing lungcancer. The ODS is able to generate SPARQL queries inwhich the subject is an ontology class, the predicate is aproperty and the object can be either a value or other con-cept. By selecting an owl:ObjectProperty, the user can addother properties of this concept to the query. This servicefollows the approach of template-based searches [43].With this tool, the data store can be searched using theproperties defined in the ontology. Moreover, it allowsthe generation of aggregated queries for the elaborationof representative charts of the data store. The generatedqueries can be stored for parameterisation and reuse.Aggregate functions such as count, average, min or maxcan be used.The results of these queries can be linked with otherresources. The filters used can also be stored for laterreuse. The semantic search engine not only allows for dataretrieval but also for creating new classes in the semanticmodel, which can be assimilated to OWL defined classes.For example, the query for patients with colon cancercould be defining the class Patient with colon cancer.The members of this class are obtained by executing thecorresponding query.Semantic profilesConceptually speaking, the semantic profile is defined asthe set of relations and properties of an individual. Seman-tic profiles permit to identify groups of patients that sharethe same properties and are therefore useful for compar-ing and studying such groups. Ontologies are of specialinterest for creating profiles because they allow to selectand aggregate individuals from a conceptual perspective.Our approach can also generate the semantic profile of agroup of patients by applying one or more criteria.Hence, we define a semantic profile as the subset ofsemantic information of an individual that is interestingfor a particular analysis. The profile of the individual i iscalculated as shown in Eq. 1.SP(i) = S(d) ? S(SP(o)) (1)where S(d) represents a subset of the selectedowl:datatypeProperty and S(SP(o)) represents a functionthat retrieves the individuals linked through owl:object-Property axioms to i. The semantic profile is built by theapplication of the ODS by using the entities defined in adomain ontology. The ODS permits to select the proper-ties of interest and to define the filtering and aggregationconditions. The user can define the SPARQL queriesthat will return the subset of properties and relationshipsthat provides the best description of the individual forthe specific case. This information is obtained for eachindividual, and the results can be viewed as a cache ofthe most important semantic information describing theindividuals.Semantic profiles can be seen as a purpose-specificapplication of the semantic search engine. Two types ofsemantic profiles are of special relevance in the context ofthis work, namely, the timeline representation of a patientand the aggregated disease timeline representation of apatient group with some common properties. Both aredescribed in the next sections.Disease timeline of a cancer patientThe disease timeline of a patient contains informationabout various health-related events (e.g. diagnosis, patientconditions, therapies and the disease courses). Retrievingthese events for a patient requires data normalisation forthe representation of therapies by month. Figure 2 showsthat every diagnosis has an associated timeline whichincludes therapies and the disease course, both orderedby month. For example, we can show the timeline for abreast cancer patient that includes the applied therapies(surgical treatment, chemotherapy, etc.) for every period.Furthermore, we can show the course of the disease andits relation with changes in therapies. It also includes thedate of the diagnosis and the date of the last encounter.Finally, the profile contains all the patients diagnoses anda list of her conditions.Aggregated disease timeline of a group of patientsThe aggregated timeline of a patient group (see Fig. 3)includes all the events of the selected patients who havethe same selection criteria for a given period and for a con-crete diagnosis. The groups of patients are defined usingthe ODS, which permits to define groups of patients withthe same diagnosis, staging, grading and age range. Thispermits to obtain the semantic profile of each member ofthe group. Then, the semantic profiles of the members ofthe group are globally analysed, so obtaining a matrix thatcontains the disease courses of the included patients forevery month of the disease. Using this method, the user isable to generate, for example, a group of patients with lungcancer with ages between 60 and 70 years old. In this case,our service could represent which therapies are applied inchronological order and which are the most likely courses.At the same time, these graphical representations can beEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 6 of 16Fig. 2 Schema of semantic profile of a cancer patientused as new filters to recalculate the corresponding vari-ables. For example, if the user selects to apply chemother-apy as first therapy, the representation changes to reflectthe new scenario.Enrichment analysisEnrichment analysis is a type of statistical analysis that isfrequently used in biomedical domains [51]. Our enrich-ment analysis method is based on the hypergeometric dis-tribution method established for the GO:TermFinder todetermine the significance of a Gene Ontology annotationto a list of genes [52], and the hypergeometric distributionwas developed using Apache Commons Math7.This type of analysis is useful to compare several sub-sets of patients with the same diagnosis. We perform astatistical analysis of the ICD-10 codes to support theusers in the definition of diagnosis-based groups. We cal-culate the P-value for each group as shown in Eq. 2.P = 1 ?k?1?i=0(Mi)(N?Mn?i)(Ni) (2)where N is the total number of ICD10 codes used in thecancer registry, M is the number of diagnoses annotatedwith each ICD10 code, n is the number of ICD10 codes ofinterest for a concrete patient group and k is the numberof ICD10 codes used for annotating each diagnosis.Semantic dashboardA semantic dashboard is a graphical representation of theresults of one or more queries. Semantic dashboards arerepresented as ??L, V ?, isDashboard, U? where ?L, V ? areFig. 3 Overview of the generation of aggregated disease timeline of a patient groupEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 7 of 16the results of the SPARQL as key-value pairs ?L, V ?, andU is who defined the dashboard. Each user can define andcustomise her dashboards.The semantic dashboard is implemented using the ODSand permits to create aggregated data. The results canbe represented graphically and in tabular format. Basedon the persistence model of SPARQL queries, the repre-sentations can be used for accessing the data instancescontained in each representation. Consequently, aggrega-tion control boxes can be regarded as search filters of thesemantic search engine.Figure 4 shows the query generated with the ODS forsearching patients over 70 years old and classified bycancer type. In the left side we show the graphical repre-sentation and in the right side the data in tabular format.The semantic dashboards can also include multipleaggregated queries and display comparative graphics.Finally, dashboards can also be persisted, parameterisedby users and reused.RecommendationWe have developed an algorithm based on Bayesian net-works to suggest the most appropriate treatment fora patient. This algorithm is based on the generationof probabilistic models using semantic nodes profiles.Bayes networks cannot have cycles [53], but our seman-tic dataset might contain cycles. The semantic profilesmight have cycles due to, e.g., the repetitive application ofa given treatment to the patient. To solve this problem atree network is generated for each profile.In case of being interested in knowing which treatmentis likely to be the most appropriate for a patient given anumber of features, the model would first retrieve all thepatients with such features, and then use their semanticprofile to generate the map of Bayesian networks with thepossible treatments by period (month, term, etc.). Oncea treatment is selected, the network is re-calculated toimprove the next recommendation. Given this dynamicaspect of the network, the method requires that the userindicates which characteristics might generate a cycle inthe network to prevent the algorithm from falling in aninfinite loop.ResultsThe approach described in the previous section has beenapplied in a scenario that simulates an institutional cancerregistry. An ontology modeling the semantics of an insti-tutional cancer registry has been developed. This ontologyhas driven the transformation of the simulated datasetinto RDF and its storage in the semantic data store. WeFig. 4 Example of semantic dashboardEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 8 of 16have implemented a Semantic Web platform that permitsusers to exploit the cancer registry dataset by formulat-ing incremental, customisable queries using a graphicaluser interface based on the ODS and by generating dash-boards on demand. The complex timelines of the diseaseof individual and aggregated patients can also be exploredand analysed. Next, more details about these results areprovided.The ontologyWe have built a preliminary cancer registry ontology8based on the existing ontologies and fulfilling the require-ments of a local cancer registry. This first draft ontologyrepresents some aspects of cancer diseases and their treat-ment pragmatically. The ontology reuses the Semantic-science Integrated Ontology (SIO) [54] and the Ontologyfor Biomedical Investigations (OBI) [55]. The ontologyincorporates concepts from clinical standards used in can-cer such as ICD10, ICD-O-3, TNM staging, Karnofskyindex [56] and ASA index [57]. The ontology has beendefined in OWL-DL. The metrics of the ontology are asfollows (numbers in brackets represent the number ofentities added by our work). The ontology contains a totalof 20,551 classes (335), 28 properties (18) and 342 objectproperties (29), with 152,529 logical axioms (2581). Theontology defines the following classes: Patient represents a person with any type of cancerdisease. Properties: gender, birth date, diagnosis,therapies and disease courses. This class is equivalentto the class Patient in SIO. Patient condition represents the health condition of apatient at a given time. Properties: reference date,age, weight, height, Karnofsky index, ASA index andthe menopause status. Diagnosis represents the patient diagnosis at a giventime. Properties: ICD10 code, grading, staging,therapies, date, pathological structure, anatomicalstructure and tumor type. This class is equivalent tothe class Diagnosis in SIO. Therapy represents the patient therapies of adiagnosis at a given time. Different kinds of therapysuch as Chemotherapy, Surgical Treatment, NuclearMedicine and others have been modeled in theontology as subclasses of Therapy. Properties:medication, start date and end date. Disease course represents the development in time(process) of a tumor disease of a certain type(diagnosis) over a time interval at a given time point.Different kinds of course such as Complete remission(tumor is not detectable any longer), Progression(tumor mass increases to a certain amount),Recurrence (after complete or partial remission,tumor mass increases again), and others have beenmodeled in the ontology as subclasses of Diseasecourse. Properties of disease course are diagnosis,patient conditions, stage, order and date. Theproperties date and order are the key to sort thecourses of the patient for a concrete diagnosis. Thisclass is equivalent to the class Disease course in OBI. The ontology also includes some classes to representthe TNM classification system of malignant tumors.They include anatomical entities for cancer gradingand staging, e.g. Primary tumor, Regional LymphNodes and Distant Metastasis hierarchies. Health Classification System is the superclass of allclasses representing coding artifacts of health relatedclassification systems. To build the taxonomies ofclassifications for a cancer registry, we tried to reuseother ontologies. For the ICD10 code we use theontology built in [58].We have evaluated the quality of our ontology usingthe Ontology Quality Evaluation Framework (OQuaRE)[59]. OQuaRE is a framework for evaluating the qual-ity of ontologies based on standards of software quality.OQuaRE automatically calculates quality scores in therange [1,5] for a series of characteristics and subchar-acteristics. A score 1 indicates that it does not fulfillthe minimal requirements, 3 indicates that the ontologymeets the requirements, and 5 indicates that the ontologyexceeds the requirements. Table 1 shows the results forour ontology. The scores for Functional Adequacy, Main-tainability, Operability, Structural and Transferability areover 4. The lowest results are achieved for Compatibilityand Reliability, although they are over 3. The results showthat our ontology has a high level of cohesion, consistency,formalisation, modularity and reusability, which are themost relevant aspects for the present work.The semantic cancer registry systemWe have implemented a prototype system9 based on themethods described in previous sections. Figure 5 showsthe three main parts of this system. All the componentsof our system have been developed from scratch exceptSWIT, which is a previous result of our research group.The upper part of the figure shows the data transforma-tion module, which uses SWIT for transforming the orig-inal data in semantic information stored into the semanticdata store.The cancer registry ontology is the core of the system,allowing for the computational management of the infor-mation related to the cancer patients. All the servicesoffered by the prototype are implemented on top of thiscore. The data transformation requires to map the sourcedata schema to the cancer registry ontology.The lower part of the figure shows the other two mod-ules of the system. The right one shows the module forEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 9 of 16Table 1 OQuaRE Metrics for the Cancer Registry ontologySubcharacteristic ValueCompatibility (3.25)Replaceability 3.5Functional Adequacy (4.69)Clustering and Similarity 4.5Consistent Search and query 4.8Controlled Vocabulary 5.0Guidance and Decision Trees 5.0Indexing and Linking 4.67Infering 5.0Knowledge Acquisition 4.67Knowledge Reuse 4.875Reference Ontology 4.5Results Representation 3.5Schema and Value Reconciliation 4.75Text Analysis 5.0Maintainability (4.34)Analysability 4.33Changeability 3.86Modification Stability 4.0Modularity 5.0Reusability 4.5Testeability 4.33Operability (4.83)Learneability 4.83Reliability (3.0)Availability 4.0Recoverability 2.0Structural (4.67)Cohesion 4.0Consistency 5.0Formal Relation Support 4.0Formalisation 5.0Redundancy 5.0Tagledness 5.0Transferability (4.25)Adaptability 4.25the analysis of individual patients, that is, extraction ofsemantic profile and timeline analysis. The left one showsthe module for the analysis of groups of patients, whichalso includes the graphical access to the disease coursesof those groups. The ODS permits to create groups ofpatients that share some semantic properties. This per-mits to generate charts and tables with accumulated dataof the semantic repository. In this case, the system pro-vides an option for adding the grouping class or property,so that it can be considered as a customisable dash-board designer. The dashboard permits users to select andaggregate the information on every class of the seman-tic model. This module is the base for the constructionof other services such as the graphical representation ofthe aggregated timelines of a group of patients or thecustomisable dashboards.The dashboard visualises the concepts of the model incharted and grouped forms, and multiple, on-demand,incremental dashboards can be built. For instance, a usercan generate a pie chart selecting patients by their firsttherapy. The user can save any dashboard for querying theresults without needing to generate it again.Application to the simulated datasetWe have performed an initial evaluation of the system.We have generated a simulated database with 207.190patients10. By the application of SWIT, the generateddataset meets the constraints defined in our ontology,whose entities are used for creating the RDF dataset.The time for the transformation of the dataset from therelational database to the semantic datastore has beenthirty-two minutes (Main features of the server: IntelCoreTM i7-3770T Processor (8M Cache, up to 3.70 GHz),8GB RAM, SATA2).We have carried out some tests basedon the execution of different types of queries to comparethe performance of the relational and semantic stores.Table 211 shows that the time performance of thesemantic datastore is slower than the relational one forbasic queries that do not require joins. However, thesemantic datastore performs better than the relationalmodel, even with indexes, on this dataset for more com-plex queries. The semantic datastore is also faster whenfiltering by a single property of the class or the tablecolumn.Semantic dashboardThis tool permits users to formulate incremental, user-defined queries with a graphical user interface based onthe ODS. Figure 6 shows a comparative graphic over thetherapies applied to patients diagnosed with colorectalcancer in different age ranges. Table 3 shows the generatedquery for this case. The query results can be displayed inseveral customisable ways, allowing for the generation ofon-demand dashboards.Graphical representation of the disease timeline of a patientThis service permits users to observe the main propertiesof the timeline of a patient with a cancer disease. Figure 7shows an excerpt of the therapy and course timeline of aEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 10 of 16Fig. 5 Overview of the systempatient with pharynx cancer. In this view, users can seethe details of the diagnosis and of every therapy applied ineach period. Besides, users are provided with two evolu-tion charts, which are based on the patient course and onthe Karnofsky index.Graphical representation of the aggregated disease timelineof a patient groupFigure 8 shows the selection and the aggregation ofpatients using the following criteria: male patients agedbetween 50 and 70, diagnosed with colorectal cancer,Table 2 Results of the migration of the relational database to thesemantic data storeQuery SQL SQL SPARQL SPARQLcount time count timeresult resultRecovery all Patients 207.190 0,060s 207.190 0,189sRecovery all Therapies 400.290 0,132s 400.290 0,317sRecovery all Diagnosis 240.088 0,070s 240.088 0,220sRecovery all Courses 108.297 0,030s 108.297 0,155sRecovery patientswith diagnosis,therapies and courses207.190 1,048s 207.190 0,204sRecovery all femalePatients105.714 0,231s 105.714 0,189sRecovery all femalePatients with more of60 years old62.603 0,245s 62.603 0,192sand who have received Chemotherapy. Table 4 shows thequery generated for this case.After the selection and the aggregation of patients, thesystem generates charts that contain the therapies andthe disease courses of the patients. This service can beemployed as an exploratory therapy simulator. Optionally,the entire time matrix can be recalculated by selecting acertain therapy. This can help the user to estimate whichtherapy is likely to be themost appropriate. Figure 9 showsan excerpt of the panel for analysing the first two monthsof the therapies of a group of 60 patients.The enrichment analysisTerm enrichment was performed on several patientgroups using the hypergeometric distribution method forthe ICD10 code annotations on each diagnosis. First,we used a sample of cancer cases related to over 300patients. Our design requirement for this sample was toinclude patients of both genders, so we discarded breastand prostate cancer for this analysis. The sample con-tained three main cohorts: diagnosis of lung cancer (469),diagnosis of melanoma (338) and diagnosis of colorectalcancer (311).Table 5 shows the results associated with lung cancer formales and females. The results show that the differencebetween both groups is not significant for lung cancer but,as shown in Table 6, it is significant for colorectal can-cer. For example, Malignant neoplasm of rectum is clearlyover-represented in the gender male, which permits toEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 11 of 16Fig. 6 Dashboard viewconclude that this diagnosis is much more commonin men.Target usersThe target users of our platform are described next: Physicians can use our platform to extract knowledgefrom the cancer registry in aggregated form filteringon the risk of patients by applying clinical criteria.Furthermore, they can obtain a graphicalrepresentation of the disease course of a concretepatient or a group of patients. Health managers can use our platform to generatecustomisable dashboards to prepare a follow-up ofthe clinical services involved in the diagnosis ortherapies for cancer. Tumor documentaries can use the platform to detectcases with incomplete or inconsistent documentationfor data curation.DiscussionCancer registries have become a basic tool for dis-ease research and treatment. Nowadays, there are sev-eral technological solutions able to manage and analysethe information of patients with a determined diagnosis.However, the lack of formal semantic models is a prob-lem when personalised analyses or external data linksare required. In this paper we have presented a seman-tic platform for the analysis and visualisation of records inan institutional cancer registry. Based on the analysis ofrequirements, we have developed an ontology that modelsthe semantics of a regional cancer registry. We have usedthis model and SWIT for transforming and storing simu-lated data from a cancer registry in a semantic data store.Our approach permits users to formulate incremental,user-defined queries with a graphical user interface basedon the ODS. The results of the queries can be displayed inseveral customisable ways, allowing for the generation ofon-demand dashboards. The complex timelines of the dis-ease of individuals and aggregated patients can be clearlyrepresented.Rule-based systems and logic-based models have beensemantic approaches applied to cancer registries, such asanalysis of cancer registry processes [7], quality assur-ance [8] and decision support [9]. Our approach innovatesby combining traditional technologies such as relationaldatabases and semantic web technologies. We have cre-ated an OWL ontology for representing some aspects ofan institutional, local cancer registry. We have developedan RDF repository whose structure is driven by the OWLontology and permits to work by exploiting the seman-tics of the content. In this way retrieval is semanticallyenabled, so that queries are independent of the relationaldata structures of conventional databases. Our technolog-ical infrastructure has permitted us to develop a semanticsearcher for navigating through the complete cancer reg-istry, to extract semantic profiles of the patients, and toanalyse the structure of disease courses.Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 12 of 16Table 3 SPARQL query generated by ODS for a dashboardPREFIX ods:?http://www.imib.es/ontologies/disease-times?SELECT count(DISTINCT ?s), ?t WHERE{{?s rdf:type ?t FILTER (?t IN (ods:DrugTherapy, ods:Anti-hormoneTherapy,ods:Anti-hormonal_anti-androgens, ods:Anti-hormonal_anti-estrogens,ods:Anti-hormone_therapy_aromatase, ods:Other_Anti-hormoneTherapy,ods:Chemotherapy, ods:Immunotherapy, ods:OtherdrugTherapy,ods:Bisphosphonates, ods:Other_med_therapy,ods:NuclearMedicineTherapy, ods:OpenRadionuclides,ods:Other_nuclear_medicine_therapy, ods:RadioiodineTherapy,ods:OtherTherapy, ods:Hyperthermia, ods:Locoregional_hyperthermia,ods:Part-body_hyperthermia, ods:LightTherapy, ods:OtherLightTherapy,ods:Selective_ultraviolet_phototherapy, ods:Wait_and_see,ods:Radiotherapy, ods:Brachytherapy, ods:Interstitial_brachytherapy,ods:Other_brachytherapy, ods:OtherHigh-voltageRadiotherapy,ods:High-voltage_radiotherapy_n.n.bez.,ods:Other_high-voltage_radiotherapy, ods:Whole-body_irradiation,ods:Teletherapy, ods:OtherTeletherapy, ods:Teletherapy_n.n.bez.,ods:Teletherapy_with_linear_accelerator, ods:StemCellTransplantation,ods:AllogeneicSCT, ods:AutologousSCT, ods:SurgicalTreatment,ods:Therapy ))} .{{?s ods:hasDiagnosis ?a0.{?a0 rdf:type ?ta0 FILTER (?ta0 IN (ods:Diagnosis))} }.{?a0 ods:hasPathologicalStructure ?a01 .{?a01 rdf:type ?ta01 FILTER (?ta01 IN (ods:Colorectal_cancer))} } .{?s ods:hasPatient ?a1 . {?a1 rdf:type ?ta1 FILTER (?ta1 IN (ods:Patient))} } .{?a1 ods:age ?a12 . FILTER (?a12 ?= 60)} }} group by ?t}Our approach provides powerful and precise searchcapabilities assisted by a customisable dashboard adapt-able to the requirements of each user. This proposal isvery similar to the tools presented in [43], but we inno-vate by permitting users to generate re-usable templates.Furthermore, the templates do not only allow the gen-eration of search forms but also of parameterised user-customisable dashboards. The platform permits to usethe entities defined in the OWL ontology for creatingthe queries in a more intuitive way than using a tradi-tional relational model. Furthermore, the use of a NoSQLdatabase (e.g. RDF repository) allows to use a robust andscalable architecture for large clinical data warehouses[49]. Another important advantage of using semanticknowledge modelling is the possibility of sharing informa-tion and comparing clinical cases and processes.The semantic profiles enable the generation of time-lines for different patient records. Our approach combinesmultimedia and temporal visualisations [24] which canbe customised by the users. The semantic profiles canbe aggregated, hence enabling the generation of time-lines of a patient group with similar characteristics. Thisvisualisation can be used as a graphical representationof a Bayesian network. Clinicians can interact with thevisualisation to discover likely courses of patients dis-eases. The platform offers data analysis based on termenrichment to support clinicians to generate groups ofpatients.LimitationsOne limitation of this work is the application of a pre-liminary version of an ontology of cancer registry data.This ontology needs to be reviewed and extended. How-ever, we believe that the OQuaRE quality scores of theontology permit to use it for proof-of-concept implemen-tations and experiments such as the one presented in thiswork.Another limitation is the use of simulated data, becausereal data would enable a more reliable (1) validation of thecorrectness and completeness of the system, (2) testing ofthe performance of the system, and (3) evaluation of theimpact of missing data in the performance [8].In this work, we have been able to evaluate onlysome components of the platform. A complete evaluationFig. 7 Excerpt of the timeline representationEsteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 13 of 16Fig. 8 Ontology-driven searcher viewwould mean to measure the following metrics: efficiency,usability, usefulness of the graphical representations ofthe analysis of the disease courses or patients or groupor patients and the capacity to develop new customizabledashboards by the users.Future workThe results of this work were shown in a clinical sessionof the epidemiological service of our largest regional hos-pital, the Virgen de la Arrixaca Hospital in Murcia, Spain.The physicians showed their interest in applying the samemethodology to the Colorectal Cancer Prevention Pro-gram of the Region of Murcia (Spain). This use case willinclude real data from 322,869 patients recruited sinceTable 4 SPARQL query generated by ODS for a filterPREFIX ods:?http://www.imib.es/ontologies/disease-times?SELECT DISTINCT ?s WHERE {{?s rdf:type ?t FILTER (?t IN (ods:Patient))} .{{?s ods:hasDiagnosis ?a0 .{?a0 rdf:type ?ta0 FILTER (?ta0 IN (ods:Diagnosis))}} . {?a0 ods:hasPathologicalStructure ?a01 .{?a01 rdf:type ?ta01 FILTER (?ta01 IN (ods:Colorectal_cancer))}} . {?s ods:gender ?a1 . FILTER (str(?a1) = M)} .{?s ods:age ?a2 . FILTER (?a2 ?= 50)} .{?s ods:age ?a3 . FILTER (?a3 ?= 70)} .{?s ods:hasTherapy ?a4 .{?a4 rdf:type ?ta4 FILTER (?ta4 IN (ods:Chemotherapy))}}}}2006. Nowadays, the physicians can generate customis-able dashboards12 and they are interested in a predictionof their future level of risk of patients.In addition, a study combining real data from theDepartment of Epidemiology of Murcia Regional HealthCouncil (Spain) and the cancer registry of the Compre-hensive Cancer Center Freiburg (Germany) is planned. Onthe clinical side, this would permit to perform studies withdata originating in different registries as well as to performcomparative studies on the characteristics and evolutionof cancer patients in different populations or on clinicaloncology practice in these regions. On the technical side,this would permit to exploit the fact that ontology-basedapproaches facilitate data integration. Although data inte-gration has not been investigated in this work, we believethat sharing the same ontology for different registrieswould enable interoperability, and the data could be jointlyexploited by means of distributed SPARQL queries. Bythe same means, they could also be used to create anintegrated data warehouse. The decision between bothimplementation options depends on the requirements ofthe use case, is due to the time cost of executing the dis-tributed queries and the effort needed to maintain thedata warehouse. However, this effort does not imply majorchanges in the RDF data representation. Such a studycould also test how the ontology copes with different reg-istries, which we believe it is a relevant quality indicatorfor our ontology.We plan to extend the platform with studies of otherchronic pathologies, which might also include a clinicalvalidation. In this way, we plan to apply the platform formonitoring clinical trials thanks to the flexibility of theODS and the customisable dashboards. Furthermore, weplan to useD3SPARQL [60] to enrich the dashboard plots.Finally, we would like to use this model to generate rulesthat serve to automatically generate patient groups or forquality assurance of the data.Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 14 of 16Fig. 9 Excerpt of the aggregated disease timeline of a patient groupConclusionThis work has demonstrated that ontologies and the RDFrepositories can be effectively combined for exploiting alocal cancer registry. On the one hand, we constructed anontology that models the knowledge of local cancer reg-istry. On the other hand, we have used semantic web tech-nologies for building a platform to analyse the complextimelines of a patients with cancer. Besides, our seman-tic structure has allowed for representing the aggregateddisease timelines of patient groups.The semantic infrastructure has also permitted thegeneration of graphical representations of the storedknowledge in the cancer registry with the generation ofcustomisable dashboards.The work is an example of how ontologies can guidethe entire life cycle of a analysis platform: data trans-formation, exploitation and knowledge generation. Thesetechnologies allow users to configure advanced searches,Table 5 Term enrichment for ICD10 cores of Lung cancerICD 10 Code P-value P-valueMale Female(308) (152)C34.0 (Main bronchus) 0.77 0.35C34.1 (Upper lobe, bronchus or lung) 0.42 0.77C34.2 (Middle lobe, bronchus or lung) 0.48 0.58C34.3 (Lower lobe, bronchus or lung) 0.71 0.45C34.8 (Overlapping lesion of bronchus and lung) 0.51 0.63C34.9 (Bronchus or lung, unspecified) 0.55 0.40Table 6 Term enrichment for ICD10 cores of colorectal cancerICD 10 Code P-value P-valueMale Female(175) (127)C17.0 (Duodenum) 0.21 0.90C17.1 (Jejunum) 0.31 0C17.2 (Ileum) 0.94 0.47C17.8 (Overlapping lesion of small intestine) 0 0.41C17.9 (Small intestine, unspecified) 0.81 0.65C18.0 (Caecum) 0.99 0.03C18.1 (Appendix) 0.86 0.28C18.2 (Ascending colon) 0.90 0.26C18.3 (Hepatic flexure) 0.11 0.96C18.4 (Transverse colon) 0.15 0.93C18.5 (Splenic flexure) 0.59 0.79C18.6 (Descending colon) 0.86 0.30C18.7 (Sigmoid colon) 0.94 0.18C18.9 (Colon, unspecified) 0.37 0.75C19 (Malignant neoplasm of rectosigmoidjunction)0.96 0.18C20 (Malignant neoplasm of rectum) 6.39E-4 0.99C21.0 (Anus, unspecified) 0.98 0.18C21.1 (Anal canal) 0.87 0.30C21.8 (Overlapping lesion of rectum, anus andanal canal)0 0.07D01.0 (Colon) 0.81 0.65D01.2 (Rectum) 0.56 0Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 15 of 16build custom dashboards and establish complex analysisfrom semantic profiles. Furthermore, semantic technolo-gies establishes the bases to link to external data sourcesand comparative analysis with other organizations. Webelieve that this work provides new insights about howsemantic technologies can be applied to the exploitationof clinical data in general, and to clinical registries inparticular.Endnotes1 http://www.elekta.com/healthcare-professionals/products/elekta-software/cancer-registry.html2 http://www.oncolog.com/?cid=73 http://www.askcnet.org/4 http://protege.stanford.edu/5 http://www.ncri.ie/6 http://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/7 http://commons.apache.org/proper/commons-math/8 http://sele.inf.um.es/ontologies/cancer-registry2.owl9 http://sele.inf.um.es/SECARE/10 http://sele.inf.um.es/ontologies/individuals.zip11The test has been carried out in a local machine withMySQL 5 as relational database and Virtuoso 7 as RDFrepository.12 http://sele.inf.um.es/SECOLON/AbbreviationsASA: American society of anesthesiologists; DL: Description logics; EMR:Electronic medical record; ICD: International classification of diseases; OBO:Open biomedical ontologies; ODS: Ontology-driven search; OQuaRE: Ontologyquality evaluation framework; OWL: Web ontology language; PCS: Procedurecoding system; RDF: Resource description framework; SNOMED CT: Systematicnomenclature of medicine - clinical terms; SPARQL: SPARQL protocol and RDFquery language; SWIT: Semantic web integration tool; TNM: Classification ofmalignant tumours; RDFS: Resource description framework schemaAcknowledgementsNot applicableFundingThis project has been possible thanks to the Spanish Ministry of Economy,Industry and Competitiveness and the FEDER Programme through grantsTIN2014-53749-C2-2-R, and by the Fundación Séneca (15295/PI/10,19371/PI/14).Availability of data andmaterialsThe Cancer Registry ontology is freely available at http://sele.inf.um.es/ontologies/cancer-registry.owl. The semantic Web Platform for the analysisand visualisation of a cancer registry is available with the use case data athttp://sele.inf.um.es/SECARE/. The user and password to sign in is änonymous¨.The RDF dataset is available at http://sele.inf.um.es/SECARE/individuals.zip.Authors contributionsConceived and designed the approach: AEG, JTFB, MB. Implemented theapproach and performed the experiments: AEG, JTFB, MB. Analysed the results:AEG, JTFB, MB. Contributed to the writing of the manuscript: AEG, JTFB, MB. Allthe authors have approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Fundación para la Formación e Investigación Sanitarias de la Región deMurcia, Biomedical Informatics & Bioinformatics Platform, IMIB-Arrixaca, C/ LuisFontes Pagán, no 9, 30003 Murcia, Spain. 2Dpto. Informática y Sistemas,Facultad de Informática, Universidad de Murcia, IMIB-Arrixaca, Facultad deInformática, Campus de Espinardo, 30100 Murcia, Spain. 3Institute for MedicalBiometry and Statistics, Medical Center  University of Freiburg, Faculty ofMedicine, University of Freiburg, Stefan-Meier-Str. 26, 79104 Freiburg, Germany.Received: 7 June 2016 Accepted: 19 September 2017RESEARCH Open AccessOntology-based literature mining of E. colivaccine-associated gene interactionnetworksJunguk Hur1* , Arzucan Özgür2 and Yongqun He3,4,5,6*AbstractBackground: Pathogenic Escherichia coli infections cause various diseases in humans and many animal species.However, with extensive E. coli vaccine research, we are still unable to fully protect ourselves against E. coli infections.To more rational development of effective and safe E. coli vaccine, it is important to better understand E. colivaccine-associated gene interaction networks.Methods: In this study, we first extended the Vaccine Ontology (VO) to semantically represent various E. colivaccines and genes used in the vaccine development. We also normalized E. coli gene names compiled fromthe annotations of various E. coli strains using a pan-genome-based annotation strategy. The Interaction NetworkOntology (INO) includes a hierarchy of various interaction-related keywords useful for literature mining. Using VO,INO, and normalized E. coli gene names, we applied an ontology-based SciMiner literature mining strategy tomine all PubMed abstracts and retrieve E. coli vaccine-associated E. coli gene interactions. Four centrality metrics(i.e., degree, eigenvector, closeness, and betweenness) were calculated for identifying highly ranked genes andinteraction types.Results: Using vaccine-related PubMed abstracts, our study identified 11,350 sentences that contain 88 uniqueINO interactions types and 1,781 unique E. coli genes. Each sentence contained at least one interaction type andtwo unique E. coli genes. An E. coli gene interaction network of genes and INO interaction types was created.From this big network, a sub-network consisting of 5 E. coli vaccine genes, including carA, carB, fimH, fepA, andvat, and 62 other E. coli genes, and 25 INO interaction types was identified. While many interaction types representdirect interactions between two indicated genes, our study has also shown that many of these retrieved interactiontypes are indirect in that the two genes participated in the specified interaction process in a required but indirectprocess. Our centrality analysis of these gene interaction networks identified top ranked E. coli genes and 6 INOinteraction types (e.g., regulation and gene expression).Conclusions: Vaccine-related E. coli gene-gene interaction network was constructed using ontology-based literaturemining strategy, which identified important E. coli vaccine genes and their interactions with other genes throughspecific interaction types.* Correspondence: junguk.hur@med.und.edu; yongqunh@med.umich.edu1Department of Biomedical Sciences, University of North Dakota School ofMedicine and Health Sciences, Grand Forks, ND 58202, USA3Department of Microbiology and Immunology, Unit for Laboratory AnimalMedicine, University of Michigan Medical School, Ann Arbor, MI 48109, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hur et al. Journal of Biomedical Semantics  (2017) 8:12 DOI 10.1186/s13326-017-0122-4BackgroundIn addition to be harmless commensal strains, the versa-tile E. coli bacterial species includes many pathogenic vari-ants [1]. Depending on the site of infection, pathogenic E.coli strains are divided into intestinal pathogenic E. coli(IPEC) and extraintestinal pathogenic E. coli (ExPEC).Example IPEC pathotypes include enteroaggregative E.coli (EAEC), enterohaemorrhagic E. coli (EHEC), entero-pathogenic E. coli (EPEC), and enterotoxigenic E. coli(ETEC). The most common ExPEC pathotypes includeuropathogenic E. coli (UPEC), meningitis-associated E.coli (MNEC), and avian pathogenic E. coli (APEC) [2].These virulent E. coli strains cause various diseases (e.g.,gastroenteritis and urinary tract infections) with bigdamages worldwide. For example, ETEC is estimated tocause 300,000 to 500,000 deaths per year, mostly inyoung children [3].To prevent diseases caused by pathogenic E. coli infec-tions, extensive vaccine research has been conducted[47]. The Vaccine Investigation and Online InformationNetwork (VIOLIN; http://www.violinet.org/) [8, 9], acomprehensive web-based central resource for integratingvaccine research data curation and literature mining ana-lysis, currently includes over 40 manually annotated E. colivaccines. Among these vaccines, Dukoral, originallyintended for protection against Vibrio cholerae, provides amoderate protection against ETEC infections in human[10]. However, there is no other licensed human E. colivaccine available on the market, putting humans at risk ofE. coli infections. Therefore, more active research isneeded to develop new E. coli vaccines.For rational pathogenic E. coli vaccine design, it iscritical to understand E. coli gene functions and E. coli-host interaction mechanisms. With over 35,000 E. coli-related articles published in PubMed, it is impossible toread all these articles manually. Therefore, literaturemining becomes critical. In addition to pathogenicstrains, many E. coli strains are nonpathogenic. E. coliis also widely used as a model organism in microbiologystudies and as a commonly used tool in recombinant bio-logical engineering and industrial microbiology. Given somany E. coli strains and different E. coli usages, it has beena challenge in mining vaccine-related E. coli gene interac-tions from the large pool of literature reports. In thisstudy, we use the commonly applied GENETAG-stylenamed entity annotation [11], where a gene interactioncan involve genes or gene products such as proteins.While human gene names are well normalized based onthe HUGO Gene Nomenclature Committee (HGNC;http://www.genenames.org/), a similar gene nomenclaturestrategy for bacterial gene names has not been formed.However, it is possible to normalize bacterial gene namesusing the strategy of pan-genome. Specifically, a bacterialspecies can be described by its pan-genome, which iscomposed of core genes present in all strains, and dis-pensable (or accessory) genes present in two or morestrains or unique to single strain [12, 13]. After a pan-genome is generated, the gene/protein names of the pan-genome of a bacterial species can be obtained by gene/protein name merging and cleanup from the annotationsof all strains belonging to the bacteria species.Integration of biomedical ontology with literaturemining can significantly improve its performance. Anontology is a human- and computer-interpretable set ofterms and relations that represent entities in a specificbiomedical domain and how they relate to each other.Previously, we applied the community-based VaccineOntology (VO) [14] to enhance our literature mining ofinterferon-gamma related [15], Brucella-related [16],and fever-related [17] gene interaction networks withinthe context of vaccines and vaccinations. Recently, wehave developed the Interaction Network Ontology(INO) and successfully applied it to the studies of vac-cine gene interactions [18] and host-Brucella geneinteractions [19]. In these studies, we used and ex-panded SciMiner [20], a natural language processingand literature mining program with a focus on scientificarticle mining. SciMiner uses both dictionary- and rule-based strategies for literature mining [20].To better study gene interaction networks, we havealso developed a literature mining strategy CONDL,standing for Centrality and Ontology-based NetworkDiscovery using Literature data [17]. The centrality ana-lysis here refers to the application of different centralitymeasures to calculate the most important genes (i.e.,hub genes) of the resulting gene-gene interaction net-work out of biomedical literature mining. Four types ofcentrality measures have been studied: degree, eigen-vector, closeness, and betweenness [17, 21]. The CONDLstrategy was applied to extract and analyze IFN-? andvaccine-related gene interaction network [21] and vac-cine and fever-related gene interaction network [17], andour results showed that the centrality analyses couldidentify important genes and raise novel hypothesesbased on literature mined gene interaction networks. Inthis study, we applied this approach, together with thepan-genome E. coli gene collection, to E. coli gene inter-action networks using VO and INO to identify the cru-cial E. coli genes and interaction types.MethodsPan-genome based E. coli gene name normalizationE. coli gene names from E. coli K12 genome have beencollected in EcoGene (http://www.ecogene.org/) [22],which were used as the basis for our E. coli gene namenormalization. To integrate E. coli gene names from dif-ferent E. coli genome annotations, we applied the pan-genome strategy [12, 13]. Specifically, out of 75 E. coliHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 2 of 10strains, we used the Vaxign program [23], which includesthe OrthoMCL ortholog searching program [24], togenerate an E. coli pan-genome that includes core E.coli genes shared by all strains, and dispensable genespresent in two or more strains or unique to singlestrain. After the E. coli pan-genome was generated, thegene names of the pan-genome were reannotated bymerging together different gene names from these E.coli strains when these gene names belong to the samegenes of the pan-genome. The reannotated gene nameswere then used for next step literature mining.VO modeling of E. coli vaccines and genes used in E. colivaccine developmentE. coli VO ontology terms were obtained from the VIO-LIN vaccines website (http://www.violinet.org/vaxquery/vaccine_query_process.php?c_pathogen_id[]=25) that con-tained 44 manually annotated E. coli vaccines. In additionto specific E. coli vaccine representations (terms), we alsomodeled and represented E. coli vaccine genes. Here, avaccine gene is defined as a microbial gene that has beenused as a gene targeted or genetically engineered in atleast one experimentally verified vaccine. For example, avaccine gene may encode for a protective protein antigen,which can be expressed, purified, and used as the vaccineantigen component in a subunit vaccine. Some vaccinegenes encode for virulence factors, and their mutationsresult in the generation of live attenuated vaccines [25].VO/INO-SciMiner tagging of genes/interaction terms andvaccine termsOur current study relies on the use of SciMiner (and itsvariant VO-SciMiner). The original SciMiner achieved87% recall, 71% precision and 76% F-measure onBioCreAtIvE II Gene Normalization Task data [20]. Interms of identifying vaccine ontology terms, VO-SciMiner demonstrated 91% recall and 99% precision inthe domain of Brucella vaccines [16]. In the current study,VO-SciMiner was further modified to be able to handlethe compiled pan-genome-based E. coli genes with a morestringent name identification matching strategy.The abstracts and titles of all PubMed records pub-lished by the end of 2014 were used for the present lit-erature mining study. Figure 1 illustrates our overallworkflow. SciMiner [20] and its variations, specializedfor specific ontologies (INO-SciMiner [18] and VO-SciMiner [16]) were used to process sentences fromPubMed literature and to identify entities (E. coli VOterms, and INO terms). VO-SciMiner was modified tobe able to handle the compiled pan-genome-based E.coli gene. In order to focus on the genes related to E. colivaccine, the analysis was limited to the entities identifiedfrom the articles in E. coli and vaccine context, definedby a PubMed search of Escherichia coli [MeSH] andvaccines [MeSH]. Figure 1 illustrates the overall work-flow of our approach.Co-occurrence analysisThe tagged genes were used to study the co-occurrenceof genes and vaccines in the same sentences. First, an E.coli gene-gene interaction network was generated basedon the sentence-level co-occurrence of E. coli genes. TheE. coli gene-gene interactions were defined for any pos-sible pairs of E. coli genes, two or more of which wereidentified from same sentence. The VIOLIN vaccine data-base [8, 9] includes 25 E. coli vaccine genes as shown onthe VIOLIN website: http://www.violinet.org/vaxquery/query_detail.php?c_pathogen_id=25. These vaccine geneshave also been represented in the VO. These E. coli vac-cine genes were used in our ontology-based literaturemining study, which aims to identify other E. coli genesthat co-occur with these vaccine genes in the same sen-tences from peer-reviewed article abstracts.This E. coli gene-gene interaction network was ex-tended by INO to create a comprehensive vaccine-centered E. coli gene-gene interaction network. In thisstudy, these additional entities were limited only to thosein the same sentences, where two or more E. coli geneswere mentioned.Centrality analysisThe collected gene-interaction networks were subject tocentrality analysis. Four different centrality metrics wereFig. 1 Project workflow. The presented study was limited to theliterature in the vaccine domain. Representative E. coli genes, obtainedthrough a pan-genome orthologue analysis, host genes as well as twoestablished biomedical ontologies of interactions (INO) and vaccines(VO) were identified from the literature by SciMiner. Based on theco-occurrence among these identified entities, vaccine-associatedE. coli gene-gene interaction network was generated and furtheranalyzed to identify the central genes and enriched biological functionsin this networkHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 3 of 10computed to identify the most important nodes (i.e.,genes, vaccine genes, and INO terms) in the createdinteraction networks using the Cytoscape plug-inCentiScaPe [26]. The degree centrality of a node is thenumber of nodes that are its first neighbors (i.e., directlyconnected to the given node). The more connections anode has, the more central it is based on degree centrality.In degree centrality, all neighbors contribute equally tothe importance of a node. In eigenvector centrality, a nodecontributes to the centrality of another node proportion-ally to its own centrality. A node is more central, if it isconnected to many central nodes. The well-known PageR-ank algorithm for ranking web pages is also based oneigenvector centrality. Closeness and betweenness cen-tralities depend on the position of a node in the net-work. Closeness centrality is based on the distance of anode to the other nodes in the network. The closer anode is to the other nodes, the more important it isconsidered to be. Betweenness centrality is based onthe number of shortest paths connecting two nodesthat pass over the given node. A node is more central,if it acts like a bridge in the network, i.e., lies on manyshortest paths.Ontology-based hierarchical classification of interactiontermsAll the interaction keywords identified in our literaturemining were mapped to INO terms. The OntoFox tool[27] was used to extract these INO terms and additionalterms related to these INO terms. The Protégé OWLeditor [28] was used to visualize the hierarchical struc-ture of these extracted terms.ResultsPan-genome-based E. coli gene name normalizationAlthough EcoGene provides very good E. coli gene nameannotations, it mainly covers the E. coli strain K12.However, many other E. coli strains are available and E.coli gene names are very complicated with differentnames across various strains. For example, the genenames iroN and fepA are synonyms, and E. coli iroNencodes for an outer membrane receptor FepA (http://www.ncbi.nlm.nih.gov/gene/7324526). Similarly, E. colistrain CFT073 gene C0393 (hemoglobin protease) has100% sequence identity with the vacuolating autotran-sporter toxin (vat) gene from many other E. coli strainssuch as strain PAB48 (GenBank Accession ID:KR094946.1). Another example is the E. coli gene rfaJ,which has several synonyms such as waaJ (http://ecoli-wiki.net/colipedia/index.php/rfaJ:Quickview). Such syno-nym information is often not reported in EcoGene.Therefore, we applied the pan-genome based strategy asdetailed in the Methods section in order to get a morecomplete set of normalized E. coli gene names.VO modeling of vaccines and related vaccine genesThe newest VIOLIN vaccine database includes 44 E. colivaccines. Only approximately half of these vaccines existedin the initial release of VO back in 2012. In this study, weupdated VO by including all these vaccines in VO, and wealso added intermediate layer terms to better represent andorganize the relations among these terms. VO also repre-sents 25 E. coli vaccine genes and how these vaccine genesare used in E. coli vaccine formulations. Figure 2 providesan example of E. coli subunit vaccine E. coli FimH withCFA and then IFA. A subunit vaccine uses a subunit (typ-ically a protein) of a pathogen organism as vaccine antigen.This vaccine uses the E. coli protein FimH (an E. coli fim-brial subunit and D-mannose specific adhesin) as the pro-tective vaccine antigen, and it uses the complete Freundsadjuvant (CFA) in the first vaccination and the incompleteFreunds adjuvant (IFA) in the boost vaccination [29].Some E. coli vaccines are live attenuated vaccines. Onemethod to make a live attenuated vaccine is to knock out avirulence factor gene(s) in a wild-type virulent strain tomake it less virulent (i.e., attenuated) but keep the antige-nicity. For example, the carA and carB genes, which forma carAB operon, are virulent E. coli genes. Their mutationsFig. 2 VO hierarchical structure and axioms of E. coli vaccines. aVaccine hierarchy that shows the E. coli vaccines. b Axioms of the E.coli vaccine E. coli FimH with CFA and then IFA (VO_0001168). Thecircled term FimH is the E. coli protein FimH. These are screenshotswith the Protégé OWL editorHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 4 of 10in an E. coli strain led the development of the mutantvaccine E. coli carAB mutant vaccine [30]. Such a viru-lence factor gene whose mutation leads to the generationof an experimental verified vaccine is named virmugen[25]. In VO, an ontological axiom is used to represent therelation between the vaccine and the mutated genes:E. coli carAB mutant vaccine: not has_part some(carA or carB)In this ontological axiom, the relation not has partmeans that the mutant vaccine strain does not have carAand carB genes in the mutated bacterial genome.The VO representation of the vaccine-gene relationsprovides rationale for us to identify specific vaccinegenes and study how these vaccine genes are related toother E. coli genes.Literature mining statistics and interaction networkThe complete abstracts and titles from PubMed,published before December 31, 2014, were processed bySciMiner to identify E. coli genes, INO and VO terms.SciMiner identified 2,037 E. coli genes from 53,925 sen-tences in articles indexed with Escherichia coli [MeSH].The study was further limited to the articles in the vaccinecontext (defined by vaccines [MeSH]), where SciMineridentified a total of 1,781 unique E. coli genes that wereco-cited with at least one other E. coli genes at thesentence level. A total of 16,887 INO terms (mapped to88 unique INOs) were also identified in 11,350 sentences.An interaction network of these E. coli genes and INOterms within the vaccine context was visualized in Fig. 3a.A subnetwork focused on known genes used in E. coli vac-cines was generated as illustrated in Fig. 3b, which include5 vaccine-genes (nodes in cyan), 62 E. coli non-vaccinegenes (nodes in red), and 25 INO terms (nodes in purple).As seen in the carA and carB sub-network (Fig. 3c),carA and carB were found in our literature mining tointeract with each other through different interactiontypes including gene expression, gene fusion, dominantregulation, and protein translation. For example, the re-trieved sentence corresponding to the gene fusion inter-action (INO_0000106) between these two genes is:A construct was made in which the intergenic regionbetween the contiguous carA and carB genes wasdeleted and the sequences encoding the carbamyl-phosphate synthetase subunits were fused in frame [31].In this case, after deletion of the intergenic region be-tween these two genes, a fused carA-carB gene formed,and the resulting fusion protein was activated 10-foldrelative to the native protein [31].Meanwhile, our literature mining also found that carAor carB interacts with other genes. For example, carBinteracts with pyrB through the induction interactiontype (INO_0000122) as shown in the following sentence:In addition, however, exogenous uracil triggers cellu-lose production, particularly in strains defective in eithercarB or pyrB genes, which encode enzymes catalyzingthe first steps of de novo UMP biosynthesis. [32].This sentence represents a complex interactionprocess. Specifically, the direct induction interaction isthat exogenous uracil triggers cellulose production, andsuch interaction occurs when the carB or pyrB gene wasdefective. In this case, carB and pyrB genes are related,since both encode enzymes that catalyze the frist stepsof de novo UMP biosynthesis [32]. In this case, the twogenes do not directly interact through the inductiontype, i.e., it is not that carB (or pyrB) triggers pyrB (orcarB). Instead, the two genes are involved in providing acondition to another induction interaction. Our studyfound that such cases occur frequently.Other sub-networks centered on the other vaccinegenes are available in Additional file 1. A Cytoscape filecontaining the E. coli gene-vaccine interaction networkas well as the sub-networks centered on each vaccine-gene is available in Additional file 2.Centrality analysisOur centrality analysis using the Fig 3b subnetworkidentified the centralities of three types of nodes (E. colivaccine genes, other E. coli genes, and INO terms) in theliterature mined network as shown in Fig. 3b. By identi-fying top 10 nodes based on either of the four types ofcentrality scores, 19 central nodes were identified(Table 1). Out of the 19 central nodes, all the 5 E. colivaccine genes are in the list. The result is reasonablesince all the genes in Fig. 3b subnetwork are expected tointeract with at least one of these five E. coli genes. Eightother E. coli genes are also found central in the list.Besides identifying the central E. coli genes, we alsotargeted the identification of central types of interactionsamong these genes in the created vaccine associated E.coli gene interaction network. Therefore, INO terms(interaction types) were represented as nodes in the net-work. Six INO terms were identified in the top node list(Table 1). These terms (e.g., gene expression and regula-tion) represent the most commonly identified interactiontypes in vaccine-related E. coli gene interaction studies.Different centrality measures provide different aspectsof the network (Table 1), since they define centrality indifferent ways and capture central nodes based on differ-ent aspects. While some node are central based on allfour centrality metrics, some are identified as central byonly one or two of the centrality metrics. Overall, degreecentrality and eigenvector centrality results are similar.Interestingly, three out of the five vaccine genes wereranked in the top 10 only by the betweenness centralitymetric, suggesting that these three vaccine genes arecritical to link together different sections in the network.Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 5 of 10A node may be considered as important, even if it isidentified as central based on only one centrality metric.Therefore, to summarize the importance of a node, theminimum (i.e., top) rank of each node based on any ofthe four centrality metrics is shown in Table 1.INO ontology-based analysis of interaction typesHere is one example sentence identified from our study:Complementation experiments indicated that both themajor fimbrial subunit gene, fimA, and the fimH gene incombination with either the fimF or the fimG gene wererequired for mannose-specific adhesion. [33].This sentence represents the INO interaction typeregulation (INO_0000157). Specifically, the four genesfimA, fimH, and fimF (or fimG) were found to regulate(were required for) the mannose-specific adhesin [33].Note that in our literature mining, the regulation rela-tion does not have to be one gene regulating anothergene; it is also allowable for both genes regulating for aspecific phenotype.For the INO interaction type detection, we used theliterature mining keywords collected in the INO. Spe-cifically, in INO, we used the annotation property hasliterature mining keywords (INO_0000006) to assignmany keywords used to represent the interaction type.ABCFig. 3 The interaction network among E coli genes and INO terms. a Interaction network among all E. coli genes co-cited at a sentence-level withINO terms in the vaccine context. b a sub-network focused on five E. coli genes (in cyan nodes) that are known to be used in E. coli vaccines. c asub-network of two vaccine genes, carA and carB, and their immediate neighbors in (b). Gene names with additional synonyms were representedwith the sign |. For example, iroN|fepA represents that this gene has two gene symbols iroN and fepA. Nodes in red represent E. coli genes,except cyan nodes, and nodes in purple are INO terms identified in the same sentences of these E. coli genes. The pink dashed lines representinteraction between E. coli gene and INO terms, while the black solid lines represent the interaction between E. coli genesHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 6 of 10For example, required is a keyword assigned for theINO interaction type regulation.From our literature mining study, 25 specific INOinteraction types were identified. The hierarchical struc-ture of these 25 INO interactions types is shown in Fig. 4.As shown in this figure, the most common interactiontype is various types of regulation, including positive,negative, and dominant regulation types. Other inter-action types such as direct physical interactions andgene expression types (including transcription and trans-lations) are also included. Such an INO hierarchical ana-lysis clearly illustrates how different genes interactedwith each other based on the reported literature papers.DiscussionThe contributions of this study are multiple. First, thisstudy for the first time applied ontology-based literaturemining method to analyze vaccine-related E. coli geneinteraction network using all PubMed abstracts. Con-sidering the status of E. coli in microbiology, infectiousdiseases, and the whole biology, such a study is important.Second, our study employed pan-genome-based approachto normalize E. coli gene names across various strains.Third, this study represents the first-time application ofapplying both VO and INO in supporting literaturemining of pathogen and vaccine-related gene-geneinteractions. Fourth, we further demonstrated that thecentrality-based analysis enhanced our ability in identi-fying hub or critical genes or nodes in the E. coli gene-vaccine intearction network.The identification of those other E. coli genes thatinteract with known E. coli vaccine genes from our studyprovides scientific insights on E. coli vaccine researchand development. These genes as a whole provide anexplanation on the functions and biological processes ofthese genes preferred for vaccine development. Thesegenes also provide new candidates for future vaccine de-velopment. It should be noted that not all E. coli vaccinegenes were identified in our literature mining process,since our analysis focuses on retrieving gene-gene inter-actions instead of individual genes.Compared to our previous vaccine-related Brucellagene interaction literature mining study [16], the currentstudy includes the more challenging E. coli species andTable 1 The most central nodes in the network. The top 10nodes based on Degree (D), Eigenvector (E), Closeness (C), andBetweenness (B) centrality metrics. The minimum (i.e., top) rankof each node based on any of the four centrality metrics is shownin the Min columnType Name D E C B MinVaccine gene fimH 1 1 2 1 1Vaccine gene fepA 2 2 1 2 1E. coli fimA 3 7 9 6 3E. coli ompT 4 4 3 4 3E. coli hlyA 5 3 4 - 3INO Inclusion 6 5 3 7 3Vaccine gene vat - - - 3 3Vaccine gene carA - - - 5 5INO protein translation - - 5 - 5E. coli yfcU 8 6 - - 6INO gene expression 9 - 6 10 6E. coli entF 7 - - - 7E. coli chuA 9 8 7 - 7E. coli tonB - - - 8 8INO dominant regulation - - 8 - 8INO association 9 9 9 - 9INO regulation 9 - 9 - 9Vaccine gene carB - - - 9 9E. coli hlyD - 10 - - 10The rankings of the terms are shown. Terms with the same centrality scoreshave the same ranking. Abbreviations here: E. coli - E. coli gene, Vaccinegene - E. coli vaccine gene; INO  INO termFig. 4 INO hierarchy of 25 interaction keywords identified in thevaccine-related E. coli gene interaction network. OntoFox [27] was usedto extract the hierarchical structure among the 25 identified INO types.The OntoFox option of includeAllIntermediates was used in theprocess. The Protégé OWL editor was used for structure visualizationHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 7 of 10also for the first time employed a new INO-based inter-action type analysis approach. In general, our studyfound many commonly reported interaction types (e.g.,expression and regulation) from the E. coli vaccine-geneinteraction network. We also found that different typesof regulation often are not about the direct regulatoryinteractions between two genes (e.g. gene A regulatesgene B). Instead, they are often related to regulatoryinteractions between the genes and another interactionprocess or phenotype. For example, as shown in themannose-specific adhesion sentence described in theResults section, the gene fimA and the gene fimH wereboth required for a phenotype: mannose-specific adhe-sion [33], rather than they had a direct interaction.Another example is the carB vs pyrB interaction, whichwas also shown in the Results section, where the twogenes participate in a pathway and a defective pathwayprocess results in the occurrence of an induction inter-action [32]. These two examples represent quite com-plex interactions that involve multiple components andrelationships that are represented by multiple literaturekeywords as shown in our previous studies [18, 34].Further research is required to automatically identifysuch specific and complex patterns from the biomedicalliterature.It is possible that tagged E. coli genes from our litera-ture mining and their associated ortholog genes in otherbacteria may likely co-occur with most vaccines for vari-ous bacteria (instead of only E. coli). This aspect of studyis out of our scope for this study since we only focus onE. coli in this study. However, our previous INO-basedstudy found that many genes co-occur in sentences withvaccines, and we even developed an INO-based Fishersexact test to perform enrichment analysis of taggedgenes in the scope of INO [18]. It is noted that the pre-vious INO-based study focused on human genes [18]while our current study focuses on bacterial genes. How-ever, we envision that bacterial genes would performsimilarly. Our previous VO-based Brucella gene-vaccineinteraction study identified many interesting patternsamong the Brucella genes as well [16]. Furthermore,many studies have found that the collection of bacterialgenes, proven to be useful in vaccine development, oftenshare common characteristics [25, 35, 36]. For example,systematic analysis of a collection of experimentallyverified protective bacterial genes revealed multipleconserved domains (or called motifs) and preferred sub-cellular localizations among protective antigens [35, 36].The collection and analysis of a set of virulence factors(i.e., virmugens) whose mutations led to experimentallyverified live attenuated vaccines also discovered manyenriched virmugens patterns, for example, the frequentusage of bacterial aroA genes as virmugens, and virmu-gens often involving metabolism of nutrients (e.g., aminoacids, carbohydrates, and nucleotides) and cell mem-brane formation [25]. These results out of systematicalanalyses facilitate rational vaccine design. More re-searches are warrantied to apply literature mining toidentify more specific vaccine-associated gene/proteinpatterns and underlying biological and immunologicalmechanisms.Our literature mining method identifies gene-geneinteractions based on sentence-level co-citation analysis.The directionality of the extracted gene-gene interac-tions is not detected by the current SciMiner. Therefore,the generated gene-gene interaction network is undir-ected and the centrality scores are computed on thisundirected network. For example, if a sentence statesthat Gene A activates Gene B, an undirected edge be-tween Gene A and Gene B is included in the gene-geneinteraction network. The information that the direction-ality of the interaction is from Gene A to Gene B is lost.In our future work, we will develop new text mining andstatistical methods to identify the directionality informa-tion regarding gene-gene interactions. With the direc-tionality of extracted gene-gene interactions, it would beeasier to find provider or consumer roles for differ-ent genes. We will investigate how centrality analysis isaffected when directionality information is incorporated.A direction-based importance metric, such as SimRank[37], can be measured to provide direction-basedweights to network nodes and generate more interestingresults.Our future directions will be multiple. First, we plan toimprove our pan-genome-based gene name normalizationmethod to cover other pathogens and to include such astrategy automatically in our SciMiner pipeline to studyother pathogens (including bacteria, viruses, and para-sites). The performance of our SciMiner pipeline in host-pathogen interaction literature mining will be thoroughlyevaluated using manually curated documents. Second, wealso plan to apply our methods to study host-pathogen/vaccine interactions. In addition, we will extend the INOmodeling to better support ontology-based literaturemining. Furthermore, statistical and machine learningmethods [38, 39] will be explored to improve our litera-ture mining and downstream analysis.ConclusionsIn this study, we first used a pan-genome-basedapproach to collect and normalize E. coli genes and cor-responding gene names, relied on the Vaccine Ontologyto obtain E. coli vaccines and vaccine genes, and appliedthe Interaction Network Ontology to obtain possibleinteraction keywords. These E. coli gene names, vaccinenames, vaccine genes, and interaction keywords werethen combinatorially used by SciMiner to process allPubMed abstracts to construct a vaccine-related E. coliHur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 8 of 10gene-vaccine interaction network. From the contructedinteraction nework, our centrality analysis further identi-fied hub or critical E. coli genes and the types of theinteractions involved in the network. New insights havebeen identified using our systematic analysis. To ourknowledge, this is the first study of applying pan-genome and ontology-based literature mining strategy toconstruct E. coli gene interaction network and performsystematic centrality analysis.Additional filesAdditional file 1: A PDF file containing three other gene-vaccinesub-networks. (DOCX 932 kb)Additional file 2: A Cytoscape session file containing the E. coligene-vaccine interaction network and its sub-networks. (CYS 76 kb)AbbreviationsAPEC: Avian pathogenic E. coli; CONDL: Centrality and ontology-basednetwork discovery using literature data; EAEC: Enteroaggregative E. coli;EHEC: Enterohaemorrhagic E. coli; ExPEC: Extraintestinal pathogenic E. coli;HGNC: HUGO gene nomenclature committee; INO: Interaction networkontology; IPEC: Intestinal pathogen E. coli; MNEC: Meningitis-Associated E.coli; UPEC: Uropathogenic E. coli; VIOLIN: Vaccine investigation and onlineinformation network; VO: Vaccine ontologyAcknowledgementsThe authors thank the participants of the 5th International Workshop onVaccine and Drug Ontology Studies (VDOS) 2016 for their valuable feedback.FundingThis research was supported by grant R01AI081062 from the US NIH NationalInstitute of Allergy and Infectious Diseases (to YH) and the BAGEP Award ofthe Science Academy (to AO).Availability of data and materialsAll data generated or analysed during this study are included in thispublished article and Additional files.Authors contributionsJH developed the ontology-based literature mining pipeline and generateddata with the vaccine domain use case. AO performed the centrality-basedanalysis of the vaccine associated E. coli gene interaction network. YH devel-oped the VO and INO and served as an E. coli vaccine domain expert. JH,AO, and YH all participated in the project design, result interpretation, andmanuscript writing. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Biomedical Sciences, University of North Dakota School ofMedicine and Health Sciences, Grand Forks, ND 58202, USA. 2Department ofComputer Engineering, Bogazici University, Istanbul 34342, Turkey.3Department of Microbiology and Immunology, Unit for Laboratory AnimalMedicine, University of Michigan Medical School, Ann Arbor, MI 48109, USA.4Department of Microbiology and Immunology, University of MichiganMedical School, Ann Arbor, MI 48109, USA. 5Center for ComputationalMedicine and Bioinformatics, University of Michigan Medical School, AnnArbor, MI 48109, USA. 6Comprehensive Cancer Center, University of MichiganMedical School, Ann Arbor, MI 48109, USA.Received: 24 December 2016 Accepted: 3 March 2017The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33DOI 10.1186/s13326-017-0145-xRESEARCH Open AccessAn automatic approach for constructing aknowledge base of symptoms in ChineseTong Ruan1*, Mengjie Wang1, Jian Sun1, Ting Wang1, Lu Zeng1, Yichao Yin2 and Ju Gao2From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: While a large number of well-known knowledge bases (KBs) in life science have been published asLinked Open Data, there are few KBs in Chinese. However, KBs in Chinese are necessary when we want toautomatically process and analyze electronic medical records (EMRs) in Chinese. Of all, the symptom KB in Chinese isthe most seriously in need, since symptoms are the starting point of clinical diagnosis.Results: We publish a public KB of symptoms in Chinese, including symptoms, departments, diseases, medicines,and examinations as well as relations between symptoms and the above related entities. To the best of ourknowledge, there is no such KB focusing on symptoms in Chinese, and the KB is an important supplement to existingmedical resources. Our KB is constructed by fusing data automatically extracted from eight mainstream healthcarewebsites, three Chinese encyclopedia sites, and symptoms extracted from a larger number of EMRs as supplements.Methods: Firstly, we design data schema manually by reference to the Unified Medical Language System (UMLS).Secondly, we extract entities from eight mainstream healthcare websites, which are fed as seeds to train a multi-classclassifier and classify entities from encyclopedia sites and train a Conditional Random Field (CRF) model to extractsymptoms from EMRs. Thirdly, we fuse data to solve the large-scale duplication between different data sourcesaccording to entity type alignment, entity mapping, and attribute mapping. Finally, we link our KB to UMLS toinvestigate similarities and differences between symptoms in Chinese and English.Conclusions: As a result, the KB has more than 26,000 distinct symptoms in Chinese including 3968 symptoms intraditional Chinese medicine and 1029 synonym pairs for symptoms. The KB also includes concepts such as diseasesand medicines as well as relations between symptoms and the above related entities. We also link our KB to theUnified Medical Language System and analyze the differences between symptoms in the two KBs. We released the KBas Linked Open Data and a demo at https://datahub.io/dataset/symptoms-in-chinese.Keywords: Knowledge base, Symptoms in Chinese, Linked data, Information extractionBackgroundMedical knowledge bases (KBs) play an important role inhealthcare research. Existing KBs vary from coding sys-tems such as ICD10 [1], terminology systems such asUMLS [2], clinical ontology systems such as SNOMEDCT [3] to medical databases such as DrugBank [4]. Themajor objectives for these KBs are to provide knowl-edge to medical workers and to promote standardization*Correspondence: ruantong@ecust.edu.cn1East China University of Science and Technology, Shanghai, ChinaFull list of author information is available at the end of the articleand interoperability for biomedical information systemsand services. Besides, there exist many different types ofbiomedical KBs. For example, SIDER [5], and AMDD [6]contain drug-related information. Diseasome [7], ParkDB[8], and ChemProt [9] describe disease and disease-related gene information. These KBs are necessary inautomatically processing and analyzing electronic medi-cal records (EMRs) and then form the basis of the upperinformation applications such as clinical decision supportsystems.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 72 of 79Currently there are many general-purpose KBs builtby automatic approaches. The DBpedia project [10]extracted structured information from Wikipedia andpublished them on the Web. YAGO [11] was derivedfrom Wikipedia, WordNet, and GeoNames. NELL [12],SOFIE [13], and PROSPERA [14] extracted data fromthe Web. The input data for NELL consisted of an ini-tial ontology as well as a small number of instances.SOFIE extracted ontological facts from natural languagetexts and linked the facts into an ontology. PROSPERArelied on the iterative harvesting of n-gram-itemset pat-terns to generalize natural language patterns found intexts.There are also some studies in the medical field whichconstruct KBs automatically. Ayvaz et al. [15] built adataset of drug-drug interaction information from exist-ing datasets including DrugBank, KEGG, NDF-RT, andso on. Ernst et al. [16] constructed a knowledge graphfor biomedical science which extracted and fused datafrom scientific publications, encyclopedic healthcare por-tals and online communities. They used distant supervi-sion in the extraction step, and used logical reasoning forconsistency checking.However, most KBs are in English. The KBs in Chineseare necessary in order to process the large amount ofEMRs in Chinese, which has been accumulated since thewide adoption of hospital information systems a decadeago. Of all KBs, the symptom KB in Chinese is mostlyrequired, since symptoms are the starting point of clin-ical diagnosis and reflect the evolution of diseases. Wefocus on symptoms and symptom-related entity extrac-tion. Data sources and methods in the construction ofour KB are different from previous work, and the exper-iments show that our KB gains roughly a higher preci-sion than similar results in [16]. Our KB is constructedby fusing data automatically extracted from eight main-stream healthcare websites, three Chinese encyclopediasites, and symptoms extracted from a lager number ofEMRs as supplement. This automatic approach not onlyavoids a large amount of manual work, but also keeps upwith changes when new entities and relations appear.MethodsData schemaThe schema of our KB can be regarded as a simplified ver-sion of UMLS. UMLS contains complex taxonomy includ-ing physical objects, events, or even intellectual products.Since our KB focuses on symptoms, we only choose partsof UMLS related to our work, such as, Finding, Sign orSymptom, and Disease or Syndrome.We have summarized five concepts for our KBdriven by the requirements of processing and analyz-ing EMRs. Besides Symptom, we add four conceptsdirectly related to symptom, namely, Disease, Medicine,Department, and Examination. Traditional Chinesemedicine (TCM) describes symptoms that is differentfrom Western medicine. For example, Yin_deficiencyandQi_stagnation are TCM symptoms which have nodirect connection with Western medicine. In addition,TCM diagnosis and Western medicine diagnosis are twoindependent parts in EMR systems. TCM symptoms areincluded in the part of TCM diagnosis. Taking the abovefactors into consideration, Symptom is further dividedinto TCM Symptom and Symptom of Western Medicine,and Medicine is divided into TCM andWestern Medicinesimilarly.The schema graph is shown in Fig. 1. The center ofthe graph is the concept Symptom. Symptom links toother concepts with relations such as relevant_disease,and has datatype properties such as location. TheFig. 1 Schema Graph for Knowledge Base of Symptoms in Chinese. Each rectangle represents a concept and the bottom of each rectangle is aninstanceThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 73 of 79instances in Fig. 1 form a virtual scenario in clini-cal practice. A person has Headache, Sneezing andAversion_to_cold which is a TCM symptom. Perhapshe goes to the Internal_medicine_department, andhas the Blood_test and Temperature_measurement.Finally, he is diagnosed with Summer_cold complicat-ing with Tonsillitis and is suggested to take some Aspirinand Bupleurum_tenue. Chinese usually take TCM andWestern medicine together.Data extractionFigure 2 presents the overall workflow of constructing ourKB. In the data extraction step, we first use specific HTMLwrappers [17] to extract entities and attributes from semi-structured information in eight mainstream healthcarewebsites. Then, we extract entities from three Chineseencyclopedia sites. Entities obtained from healthcare web-sites are fed as seeds to train a multi-class classifier andclassify entities from encyclopedia sites. Finally, we train aConditional Random Field (CRF) model to extract symp-toms from EMRs. The details are described below.Data extraction from healthcare websitesWe collect eight mainstream healthcare websites (See inTable 1). There are normally two kinds of web pages foreach website. One is the list page containing a list of enti-ties. The other is the detail page containing the detaildescription of a particular entity. All the above websitescontain list pages of symptoms, diseases and medicines.However, list pages of department do not exist in JIANKE,PCbaby, or fh21, and list pages of examinations onlyappear in JIANKE, 120ask, and 39Health.We take the symptom extraction as an example. Allthe detail pages of symptoms in a website share similarlayouts. There is a portion called property box in thedetail page containing attribute-value pairs of an entity.We map the property box to properties in schema graphTable 1 Basic information of eight healthcare websitesWebsite name URLFamilydoctor http://www.familydoctor.com.cn/JIANKE http://www.jianke.com/120ask http://www.120ask.com/QQYY http://www.qqyy.com/39Health http://www.39.net/99Health http://www.99.com.cn/Fh21 http://www.fh21.com.cn/Pcbaby http://www.pcbaby.com.cn/manually and extract attribute-value pairs with HTMLwrappers. Since symptom may be a TCM Symptom orSymptom of Western Medicine, we use the relevantdepartment attribute to classify. Specifically, if a symp-tom is related with a department containing TCM (e.g,Traditional Chinese Orthopedics, it will be labeled asTCM Symptom. The symptom will be tagged as Symp-tom ofWesternMedicine if it is related with a departmentwithout TCM. An entity can be labeled as both TCMSymptom and Symptom of Western Medicine. Finally,synonymous relations are crawled from the abstracts ofentity pages with Hearst patterns described in [18]. Forexample, when applying a pattern [entity1] is known as[entity2] to sentence hyperpyrexia is known as fever, asameAs relation between hyperpyrexia and fever isextracted.We apply similar steps to other types of entities. Weuse heuristic information in the detail page of a medicineto distinguish TCM and Western medicine. If its descrip-tion in detail page contains keywords such as Chinesepatent medicine and herbal medicine, and it is Westernmedicine if its description contains pharmaceuticals,chemicals, and so on.Fig. 2Workflow of Constructing the Knowledge Base of Symptoms in Chinese. It contains three steps: (1) Extract data from healthcare websites,encyclopedia sites and EMRs, respectively. (2) Align the extraction results. (3) Link our symptoms to concepts in UMLSThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 74 of 79Data extraction from Chinese encyclopedia sitesWe extract and classify entities from three largest Chineseencyclopedia sites (i.e. Baidu Baike, Hudong Baike, andChinese Wikipedia).Algorithm 1 shows the extraction process. First, wetake entities from healthcare websites as seeds (denoteas EntityList), and extract their categories in Chineseencyclopedia sites (denote as SeedCateSet).Algorithm 1 Extract Entities from Encyclopedia SitesInput: EntityList, Baike EntitySet, kOutput: target EntitySet1: for each eli ? EntityList2: if eli exists in Baike EntitySet then3: SeedCateSet ? eli.getcategory()4: SeedCateList ? eli.getcategory()5: for each catei ? SeedCateSet6: frei ? the number of catei in SeedCateList7: if frei < k then8: low-confidence CS ? catei9: for each ei ? Baike EntitySet10: if ei.getcategory() ? SeedCateSet = ? &&11: ei.getcategory() ? low-confidence CS =? then12: target EntitySet ? ei13: return target EntitySetThen we crawl entities belonging to SeedCateSet. Sec-ond, we classify SeedCateSet into low-confidence andhigh-confidence by calculating the ratio of seeds in thesecategories. Thirdly, for each entity in encyclopedia site,if its categories contain low-confidence categories, it willbe regarded as noise and removed. We eventually col-lect 62,013 entities from Baidu Baike, 59,704 entitiesfrom Hudong Baike, and 2220 entities from ChineseWikipedia.In the classification step, we take entities extracted fromhealthcare websites as positive examples and entities fromlist pages of health preservation, facial and psychol-ogy in healthcare websites as negative examples. Thentrain a Decision Tree [19] classifier with seven labels:department, TCM, western medicine, symptom, disease,examination and other.The classifier uses two types of features, namely,word formation and word distribution. The features areobtained from five fields, namely, entity name, abstract,content, full-text, and category of entity page, as shown inFig. 3 and Table 2. If an entity is classified as Symptom, wewill use heuristic rules to further determine whether it isa TCM Symptom or Symptom of Western Medicine.Data extraction from EMRsDue to variations of symptoms in clinical practice, weincorporate clinical vocabularies into our KB by extractingFig. 3 Five Fields of Entity Page in Encyclopedia Sitessymptoms from a large number of EMRs. In order to learnsymptoms of Western medicine, we extract texts fromthe Physical Examination and Antidiastole_Westernmedicine fields of EMRs. Texts within Disease Analy-sis and Antidiastole TCM fields are selected for TCMsymptoms. Since data duplication takes place frequentlyin the texts of EMRs, we remove the sentences appearingmore than once in the same record. We manually anno-tate TCM symptoms and symptoms of Western medicinein EMRs. Then we train a CRF [20] classifier to recog-nize new symptoms. The features are classified as literalfeatures , position features, and part-of-speech (POS) fea-tures as shown in Table 3. The literal features and positionfeatures are adopted from [21].We set the context windowsize to 3 since it achieves best results in [21].Data fusionData fusion consists of three steps: entity type alignment,entity mapping, and attribute mapping.Table 2 Classification features for six entity typesFields of page Classification features for six entity typesEntity name Ends with any words in (department, disease,inflammation, tumour, syndrome, examination)Abstract Contains any words in (symptom, syndrome,symptoms of illness, disease name of TCM)ContentHas more than 3 words in (function, specification,adverse reaction, side effect, component, usage,dosage),Has more than 3 words in (cause, examination,antidiastole, diagnosis, mitigation, pathogenesis,clinical manifestation)Full-text Contains any words in (Chinese patent medicine,Chinese herbal medicine)Category Contains any words in (medicine, disease, TCM,drug, Chinese patent medicine, symptom)The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 75 of 79Table 3 Features of CRFFeature type Feature contentsLiteral features Unigram Xi?3, Xi?2, Xi?1, Xi , Xi+1,Xi+2, Xi+3Bigram Xi?3Xi?2, Xi?2Xi?1, Xi?1Xi ,XiXi+1, Xi+1Xi+2, Xi+2Xi+3Trigram Xi?3Xi?2Xi?1, Xi?2Xi?1Xi ,Xi?1XiXi+1, XiXi+1Xi+2,Xi+1Xi+2Xi+3Position features IndexiPOS features Unigram Pi?3, Pi?2, Pi?1, Pi , Pi+1,Pi+2, Pi+3Bigram Pi?3Pi?2, Pi?2Pi?1, Pi?1Pi ,PiPi+1, Pi+1Pi+2, Pi+2Pi+3Trigram Pi?3Pi?2Pi?1, Pi?2Pi?1Pi ,Pi?1PiPi+1, PiPi+1Pi+2,Pi+1Pi+2Pi+3Firstly, We align entity types with a voting method,i.e., the entity type receiving the most votes wins. Whentwo types have the same top votes,the entity type withthe higher priority wins. Priorities are determined by theresources rankings in Alexa.Secondly, we use the idea of Wang et al. [22] to mapentities. They used two variables, namely, commonnessand relatedness in combination to calculate similari-ties between entities. In this paper, the commonness isobtained by calculating string similarities between entitynames, while the relatedness is calculated with string sim-ilarities of attribute values. For example, entity EA and EBhave the same type and share two attributes A1 and A2.The commonness is defined asStringSimilarity(EA,EB) = |LCS(EA,EB)|Max(|EA| , |EB|) (1)where |LCS(EA,EB)| is the length of the longest commonsubsequence between EA and EB. The relatedness is theratio of the number of similar facts. If the product of com-monness and relatedness is higher than a threshold, we willmap EA to EB.Finally, we map attribute from the extraction resultsto our ontology. Since property boxes of each healthcarewebsite share the same attribute names, we manually mapattribute names in healthcare websites to our ontology.However, in Chinese encyclopedia sites, the infoboxes ofentity pages exist lots of attribute names that are similarin semantics but different in names. We map attributesto our ontology according to type information of entitiesand attribute values. For example, attribute symptomof triple <vertigo of heat stroke, symptom, thirsty> ismapped into symptom related symptom, because entityvertigo of heat stroke and attribute value  thirsty areboth symptoms.Link to UMLSTo investigate similarities and differences between symp-toms in Chinese and English, we link our KB to UMLS, amedical KB widely used in clinical practice and medicalinformatics research. Xu et al. [23] used context simi-larities to link phrases in English discharge summaryto Chinese discharge summary. However, there are nosuch contexts in our situation. We first call the API ofBaidu Translate [24] to translate symptoms in our KB intoEnglish phrases. Second, each phrase is transformed intoa bag of words (denote as BWCS), and the same opera-tion is done to concepts in UMLS (denote as BWUMLS).Third, the JaccardSimilarity [25] between elements inBWCS and BWUMLS is calculated. Only when the value ofJaccardSimilarity is 1, do we make a linkage.Results and discussionClassification resultsWe apply Decision Tree algorithm to train a multi-class classifier. Twenty-six features from five fields ofentity pages in encyclopedia sites are utilized in thispaper. We use ten fold cross-validation. Figure 4 showsthe results of our classifier in three encyclopedia sites,and indicates that our classifier has high accuracy andrecall.EMR data extraction resultsWe have collected 250,000 EMRs from ShanghaiShuguang Hospital as our corpora. Two TCM expertsannotate symptoms mentioned in 1000 EMRs, which arerandomly divided into two parts. One containing 660EMRs form the training set, and the rest form the test set.Then we train a CRF model. The precision of TCM symp-tom is 93.26%, and the precision of symptom of Westernmedicine is 95.37%. Finally, we use the model to recognizenew symptoms from 249,000 EMRs. We have extracted2376 symptoms, 387 of which are TCM symptoms and1989 of which are symptom of Western medicine.StatisticOur KB has 135,485 distinct entities and 617,499 facts.More precisely, there are 26,821 symptoms, 32,956 dis-eases, 67,712 medicines, 292 departments, and 7704examinations, shown in Fig. 5. Table 4 shows the dis-tribution of entities from each source and the ulti-mate results of our KB. We find: (1) The number ofsymptoms from all sources is 41,020. It is far largerthan the number of distinct symptoms (i.e. 26,821)in our KB, which shows the large-scale duplicationbetween different data sources. (2) The website thatcontributes most to symptoms in our KB is fh21. Itcontains 9780 symptoms and accounts for only 36.5%of our KB, which shows the advantage of collectingdata from different sources. (3) The EMRs also addThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 76 of 79Fig. 4 Classification Results for Encyclopedia Sites. The result for Examination is obviously lower than those for the other classifications, becausesome entity pages in encyclopedia sites are irregular, leading to few features being used. Besides, Chinese Wikipedia has a few seed entities ofexamination, so its result is worse than those for other encyclopedias2376 new symptoms to our KB, showing the differ-ences between symptoms used in websites and in clinicalpractice.For correctness evaluation, we use correctness ratio offacts [26] to evaluate symptom-related facts. Each sam-pled triple is evaluated by seven persons according to theirknowledge. We integrate the evaluating results by voting.Since our KB is large, we use simple random samplingmethod to draw samples.Based on [26], we sampled 417 triples from 26,821rdf:type triples whose objects are Symptom to calculatethe correctness of symptoms. Its correctness ratio is 98.1%.Then, we sampled 423 triples from 295,946 symptomrelated triples. The correctness ratio of symptom-relatedfacts is 95.9%.We have collected 4298 links between symptoms inour KB and concepts in UMLS (abbreviate as symp-tom links). We sampled 385 triples to evaluate thecorrectness of the links, and the correctness ratio is92.0%. We calculate the semantic type distribution onthe symptom links in UMLS, shown in Fig. 6a. Nor-mally, symptoms in our KB are expected to link toconcepts in Sign or Symptom or Finding. But 53.3% ofsymptoms are linked to other semantic types in UMLS.For example, Icterus (C0022346) and infectious jaun-dice (C0241954) are common symptoms in Chinese,while the semantic type of the former one is Patho-logic Function and the latter one is Disease or Syn-drome. This shows the range of symptom in life is muchbroader than that in medical science. Attribute distri-bution on symptom links in the two KBs are shownin Fig. 6b and c. We define six attributes for symp-toms in our KB, and UMLS defines 13 attributes forthe linked concepts. However, most properties in UMLSdo not have fixed domains and ranges. Thus, theseattributes can not be interpreted uniformly. For example,RO in UMLS refers to an uncertain relation. In con-trast, our KB provides relations with definite syntax andsemantics.Conclusions and future workThe KB is constructed by fusing data automaticallyextracted from eight mainstream healthcare websites,three Chinese encyclopedia sites, and symptoms extractedfrom a lager number of EMRs as supplement. Finally, weobtain 135,484 entities to our KB, among them, the num-ber of symptom entities is 26,821. The KB can be usedto annotate symptoms in EMRs. It can also be embed-ded into EMR systems to help therapists with symptomrecommendations. In the future, we will try to constructa whole KB of commonly used medical vocabularies inChinese, linking them to UMLS concepts.a bFig. 5 Data Distribution on our KB. From (a), symptom-related facts account for 49.23% in all facts of our KB, which is the result of symptoms beingthe focus of our KB. From (b), we find that medicine entities account for 50% of all entities, and 64.4% of them are TCMThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 77 of 79Table4EntityevaluationofdifferentdatasourcesEntityTypePrecisionHarvestedentitiesFamilydoctorJIANKE120askQQYY39Health99Healthfh21PCbabyBaiduBaikeHudongBaikeChineseWikipediaEMRsResultSymptom0.981477577487610412366595745978017872932997393237626821Disease0.96799988004713825467778344399113895688418458732956Medicine0.983893128132712175632514235121879221522746936567712Department0.976315512373119212553292Examination0.78314033022909230301397704Aggregateda0.9381569718436183768856237087543188924055311943307614372376135485a PrecisionvaluesareaveragedandnumbersofharvestedentitiesaresummedThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 78 of 79a b cFig. 6 Data Distribution on the Linked Results. a Semantic Type Distribution on Linked Concepts in UMLS, b Property Distribution on LinkedSymptom in our KB, c Property Distribution on Linked Concepts in UMLSAbbreviationsCRF: Conditional random field; EMR: Electronic medical record; KB: Knowledgebase; POS: Part-of-speech; TCM: Traditional Chinese medicine; UMLS: Unifiedmedical language systemAcknowledgementsWe would like to thank three medical experts from Shanghai ShuguangHospital for helping evaluate correctness of our data.FundingThis work and the publication cost of this paper was supported by the 863plan of China Ministry of Science and Technology (project No: 2015AA020107),Action Plan for Innovation on Science and Technology Projects ofShanghai(project No:16511101000), Research on the Construction Technologyof the Healthy and Aged Knowledge Base Based on the Combination ofMedical and Care (project No: 2015BAH12F01-05), and Research on EfficientQuery Algorithm for Large Scale Annotated Semantic Knowledge (project No:61402173).Availability of data andmaterialsWe released the KB as Linked Open Data and a demo at https://datahub.io/dataset/symptoms-in-chinese.About this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsTR designed the schema of our KB and gave professional technical guidanceat each step, namely, data extraction, data fusion, and linkage. LZ used specificHTML wrappers to extract entities and attributes from semi-structuredinformation in eight mainstream healthcare websites. JS extracted andclassified entities and attributes from three encyclopedia sites. TW trained aCRF model to extract symptoms from ERMs. MW fused data from differentsources, including entity type alignment, entity mapping, and attributemapping. And she linked our KB to UMLS. As experts in healthcare, JG and YYprovided help and guidance in the process of data evaluation. All authors readand approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Author details1East China University of Science and Technology, Shanghai, China. 2ShanghaiShuguang Hospital, 200025 Shanghai, China.Published: 20 September 2017Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 DOI 10.1186/s13326-017-0117-1RESEARCH Open AccessDeveloping a knowledge base to supportthe annotation of ultrasound images ofectopic pregnancyFerdinand Dhombres1*, Paul Maurice1, Stéphanie Friszer1, Lucie Guilbaud1, Nathalie Lelong2, Babak Khoshnood2,Jean Charlet3, Nicolas Perrot4, Eric Jauniaux5, Davor Jurkovic6 and Jean-Marie Jouannic1AbstractBackground: Ectopic pregnancy is a frequent early complication of pregnancy associated with significant rates ofmorbidly and mortality. The positive diagnosis of this condition is established through transvaginal ultrasoundscanning. The timing of diagnosis depends on the operator expertise in identifying the signs of ectopic pregnancy,which varies dramatically among medical staff with heterogeneous training. Developing decision support systemsin this context is expected to improve the identification of these signs and subsequently improve the quality ofcare. In this article, we present a new knowledge base for ectopic pregnancy, and we demonstrate its use on theannotation of clinical images.Results: The knowledge base is supported by an application ontology, which provides the taxonomy, the vocabularyand definitions for 24 types and 81 signs of ectopic pregnancy, 484 anatomical structures and 32 technical elementsfor image acquisition. The knowledge base provides a sign-centric model of the domain, with the relations of signs toectopic pregnancy types, anatomical structures and the technical elements. The evaluation of the ontology andknowledge base demonstrated a positive feedback from a panel of 17 medical users. Leveraging these semanticresources, we developed an application for the annotation of ultrasound images. Using this application, 6 operatorsachieved a precision of 0.83 for the identification of signs in 208 ultrasound images corresponding to 35 clinical casesof ectopic pregnancy.Conclusions: We developed a new ectopic pregnancy knowledge base for the annotation of ultrasound images. Theuse of this knowledge base for the annotation of ultrasound images of ectopic pregnancy showed promising resultsfrom the perspective of clinical decision support system development. Other gynecological disorders and fetalanomalies may benefit from our approach.Keywords: Application ontology, Knowledge base, Ectopic pregnancyBackgroundEctopic pregnancy is a common early pregnancycomplicationEctopic pregnancy occurs in 1 to 2% of pregnancies in de-veloped countries and is defined by the implantation of agestational sac outside the endometrial cavity of the uterus[1, 2]. The direct mortality rate from ectopic pregnancy is* Correspondence: ferdinand.dhombres@aphp.fr;ferdinand.dhombres@inserm.fr1UPMC Medical Faculty (Paris 6), Department of Fetal Medicine in ArmandTrousseau Hospital (APHP), INSERM U1142 (LIMICS), 26 Avenue du Dr ArnoldNetter, 75012 Paris, UE, FranceFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This articInternational License (http://creativecommonsreproduction in any medium, provided you gthe Creative Commons license, and indicate if(http://creativecommons.org/publicdomain/zeestimated to be 16.9 per 100,000 ectopic pregnancies [2],and is responsible for 4 to 10% of pregnancy-relateddeaths around the world [3]. Fallopian tubes are the mostcommon site for ectopics to implant (tubal ectopics) withabout 95% of ectopic pregnancies located there. For therest, the implantation occurs within the uterine wall, butoutside the endometrial cavity. Non-tubal ectopics aremore difficult to diagnose than tubal ectopics and are as-sociated with a higher mortality and morbidity [4]. De-layed diagnosis is the main factor for ectopic pregnancyassociated with maternal death [2] and also affects thesuccess rate of future pregnancies [5].le is distributed under the terms of the Creative Commons Attribution 4.0.org/licenses/by/4.0/), which permits unrestricted use, distribution, andive appropriate credit to the original author(s) and the source, provide a link tochanges were made. The Creative Commons Public Domain Dedication waiverro/1.0/) applies to the data made available in this article, unless otherwise stated.Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 2 of 13Ectopic pregnancy diagnosis relies on ultrasoundexpertiseThe positive diagnosis of ectopic pregnancy is establishedthrough ultrasound scanning. More specifically, transvagi-nal scanning has been demonstrated to be superior totransabdominal ultrasound [4]. Consistent with continu-ous improvement in imaging quality and expertise, it hasbeen recently suggested that a skilled operator couldachieve a definite diagnosis at the very first scan [6]. How-ever, most hospitals still rely on a heterogeneous staff tomanage patients at risk for ectopic pregnancy, includingemergency physicians, sonographers, radiologists and/ordoctors in training [2, 7], with different levels of trainingand expertise. Thus, three or more visits are needed for50% of these patients [8].A shared representation for ectopic pregnancy imagingExisting repositories of medical terminologies and ontol-ogies, namely the Open Biomedical Ontologies (OBO)Foundry [9], the National Center for Biomedical Ontology(NCBO) BioPortal [10] and the Unified Medical LanguageSystem (UMLS) Metathesaurus [11] do not include acomprehensive set of resources to represent ultrasoundsigns. None of the resources reviewed in a recent surveyof biomedical imaging ontologies was suitable for ectopicpregnancy [12]. This domain involves concepts from vari-ous medical domains, namely medical imaging, humananatomy and obstetrics/gynecology (OB/GYN). Whileexisting standard terminologies may support a formal andshared representation for parts of our domain, as do theFoundational Model of Anatomy (FMA) [13] and theRadiology Lexicon (RadLex) [14], none of them providesthe appropriate granularity for ectopic pregnancy imaging.More precisely, RadLex supports the representation ofsigns from various imaging modalities (including 50 ultra-sound imaging signs [15]), as well as their relations tovarious medical conditions (including ectopic pregnancy),which makes it the best resource for our domain. How-ever, RadLex is insufficient, because there are no sub-classes for ectopic pregnancy [RadLex:RID4942] andFig. 1 Graphical view of the "ectopic pregnancy" concept from the Radiologyneighborhood for "ectopic pregnancy"[RadLex:RID4942]RadLex only provides two related signs (ring of fire sign[RadLex:RID35495] and interstitial line sign [RadLe-x:RID35308]), as illustrated in Fig. 1.ObjectivesIn this article, we present a new ectopic pregnancy know-ledge base and its application to ultrasound image annota-tion. In this knowledge base, the signs of ectopic pregnancyare linked to specific types of ectopic pregnancy, the ana-tomical structures involved and the technical elements ofimaging. We also developed an ontology to provide the vo-cabulary used in the knowledge base, as well as an applica-tion for annotating ultrasound images, which leverages theknowledge base. We demonstrate the use of the knowledgebase on the annotation of clinical images.MethodsIn this section, we describe our approach to developinga knowledge base for ectopic pregnancy imaging. Westart by describing the underlying ontology. We presentthe knowledge base. Finally, we describe the applicationdeveloped to support the annotation of ectopic preg-nancy ultrasound images. The overview of the ontologyand knowledge base development is presented in Fig. 2.Ontology developmentTo build the ectopic pregnancy ontology (EPO), we ac-quired concepts from a medical corpus. We also reusedconcepts from existing terminologies. We organizedthese concepts into hierarchies.Acquiring concepts from textWe extracted terms from a medical corpus and orga-nized them into concepts.Extracting terms from a medical corpusIn order to cover the terms for the features to be anno-tated on EP images (i.e., types of ectopic pregnancy image,imaging signs, anatomical locations and technical ele-ments for ultrasound image acquisition), we used NaturalLexicon (RadLex, version 3.13.1), with a full expansion of the conceptsFig. 2 Overview of the design of the knowledge system for ectopic pregnancy: ontology design, reference image collection and application forimage annotationDhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 3 of 13Language Processing (NLP) techniques [16] to extract andselect medical terms from a collection of medical textsfrom two sources, namely the medical literature and de-identified reports of ultrasound examinations. More spe-cifically, we searched PubMed for all medical publicationsindexed with the MeSH term "Pregnancy, Ectopic" fromJanuary 2000 to December 2014 for which an abstract wasavailable, resulting in a collection of 2795 abstracts. Add-itionally, we extracted 4260 de-identified ultrasound re-ports form the Early Pregnancy Clinic database at theUniversity College London Hospital (UCLH), restricted toectopic pregnancy cases from October 2006 to April 2014.The lexico-syntactic analysis of these texts was performedusing the part-of-speech tagger TreeTagger [17] and theDhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 4 of 13term extractor YaTeA (http://search.cpan.org/dist/Lingua-YaTeA/). A total of 40,237 single/multi-word candidateterms were extracted.Organizing extracted terms into conceptsThe appropriate vocabulary for ectopic pregnancy wasdeveloped from these candidate terms, using the plat-form for ontology development from text Terminae/DAFOE [16]. Two experts reviewed and selected candi-date terms, and defined the relevant concepts for ectopicpregnancy image description. The experts followed gen-eral principles for ontology design (clarity, coherence,extensibility, minimal encoding bias, minimal ontologicalcommitment) [1821].Acquiring concepts from existing terminologiesWhenever possible, the experts reused elements fromexisting terminologies, following previously describedmethods [22, 23]. For example, fine-grained concepts forthe description of the pelvic anatomy in the FMA (e.g.,the uterus [FMAID:17558] and all its parts) were addedto the ontology.Organizing concepts into hierarchiesWe organized the resulting concepts into a subsumptionhierarchy and we added annotations and logical defini-tions to these concepts.Organizing concepts into a subsumption hierarchyWe used a core ontology for the medical domaindeveloped in our academic center (ontoMénélas) to sup-port the interoperability with other resources in ourorganization [2429]. The subsumption hierarchy (i.e., is-a or subClassOf relations) was developed in a top-downapproach [19, 30] leveraging expert knowledge in medicalimaging and OB/GYN, and by reusing existing resources.In particular, we reused some of the subsumption relationsfrom the FMA (among the FMA concepts that wereadded to the ontology) as previously described by theRadLex group [31].AnnotationsAll concepts for ultrasound signs of ectopic pregnancywere manually annotated. The minimal set of annota-tions included (i) one English label (ii) one textual defin-ition in English (iii) one PubMed identifier (PMID) forthe concepts extracted from the PubMed corpus. Otherannotations were optional (e.g., synonyms and Frenchversion of the annotations). The mappings of anatomicalconcepts to FMA concepts were stored as annotationsin the ontology. The FMA labels and definitions forthese concepts were also added as annotations. All text-ual annotations were based on SKOS predicates (prefLa-bel, altLabel, definition) [32]. We used the biomedicalontology editor Protégé version 5 (http://protege.stanfor-d.edu/) for editing the annotations.Logical definitionsGeneral concepts for categories of signs were defined inintension as opposed to extension. These concepts corres-pond to defined classes in the ontology. For example, theconcept color Doppler sign denotes an imaging sign, vis-ible during an ultrasound examination, using the colorDoppler mode. Therefore, this concept is formalized witha logical definition leveraging the property requiresModeand the concept color Doppler mode. As a result, signswhose definition contains requiresMode some color Dop-pler mode would automatically be classified as subclassesof color Doppler sign.Implementation in OWLThe ontology was represented using the Web OntologyLanguage, OWL [33]. The hierarchy was inferred with anOWL-DL reasoner (Hermit 1.3.8), which also checked theconsistency of the ontology.Knowledge base developmentThe ontology provides the vocabulary for describing ultra-sound images of ectopic pregnancy, which we used for de-veloping a sign-centric knowledge base to represent therelations of each sign to ectopic pregnancy types, anatom-ical structures and technical elements for the acquisitionof ultrasound images. Technical elements include theexamination route, the examination mode, and theechographic view. For example, the ring of fire signconcept is represented in Fig. 3 with its relations to a typeof ectopic pregnancy (tubal pregnancy through the rela-tion epo:suggests), to anatomical structures (ampulla,tubal isthmus, frimbrial portion through the relationepo:hasLocation) and to technical elements (vaginalroute, color Doppler mode (2D), adnexal area viewthrough the relations epo:requiresRoute, epo:requires-Mode, epo:requiresView, respectively).This knowledge was asserted at the most general leveland propagated through the subsumption hierarchies ofthe ontology. For example, although the concept ring offire sign is not explicitly linked to tubal pregnancy signin the knowledge base, this relation can be inferred fromring of fire sign epo:suggests tubal pregnancy and tubalpregnancy sign owl:equivalentClass (epo:suggests sometubal pregnancy).We assessed the domain and scope of the knowledgebase using competency questions, the answers to whichmust be represented with relations (asserted or inferred)from the knowledge base [34]. Such questions includedwhat are the different implantation sites of ectopic preg-nancies?, what are the imaging signs of cesarean sectionscar pregnancy?, which ultrasound mode is required toFig. 3 Simplified representation of the sign "ring of fire" in the knowledge baseDhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 5 of 13depict a ring-of-fire sign?, and what are the anatomicalstructures visible in an adnexal area view?Beside the development of the sign-centric knowledgebase, we selected from the medical literature ultrasoundimages of ectopic pregnancies illustrating the signs rep-resented in the knowledge base. We restricted the 2795PubMed citations used for the text corpus to articles inEnglish, indexed with the MeSH term Ultrasonographyand for which the article was freely available. One of theauthors (PM) selected relevant images from the articles,in which the ultrasound signs were precisely describedand illustrated. He annotated the signs in the knowledgebase with the PMID of the article. For example, the con-cept ring of fire sign is annotated with PMID 18936028in reference to an article describing this sign [35].Application developmentWe developed an application for the annotation of ultra-sound images of ectopic pregnancy. This application le-verages both definitional knowledge from the ontologyand assertional knowledge from the knowledge base.The main features of this application include:i) searching for image annotations using terms fromthe ontology,ii) suggesting relevant signs based on the knowledgebase, andiii) accessing reference images for a given sign.The user interface was developed as a Java 7 web ap-plication based on open-source elements. The ontologyand the knowledge base were stored in an RDF triplestore (Apache Jena 3.0). We used queries against aSPARQL endpoint (Apache Fuseki) to access the know-ledge base. Simple subsumption reasoning was sufficientto access all the asserted and inferred knowledge fromthe knowledge base. We established a set of SPARQLrules to suggest the signs, anatomical structures andtechnical elements associated with a given type of ec-topic pregnancy selected by the user. For example, thefollowing SPARQL query retrieves all ectopic pregnancytypes having at least one sign from a given set of signs.${selectedSigns} is a variable containing the URIs ofthis set of signs, ${inferredGraph} is the inferred ontol-ogy graph in the triplestore and "${language}" is the lan-guage used for label display in the system:PREFIX owl:<http://www.w3.org/2002/07/owl#>PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX epo:<http://www.semanticweb.org/ontologies/epo.owl#>PREFIX skos:<http://www.w3.org/2004/02/skos/core#>PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>SELECT DISTINCT ?disorder?disorder_label ?disorder_definitionFROM ${inferredGraph}WHERE {VALUES ?sign {${selectedSigns}}Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 6 of 13?sign skos:hiddenLabel ?sign_id;rdfs:subClassOf* epo:OPPIO_0000189 .?disorder skos:prefLabel?disorder_label;rdfs:subClassOf epo:OPPIO_c000016 .OPTIONAL {?disorder skos:definition?disorder_definition .}?sign rdfs:subClassOf* ?restr .?restr owl:onProperty epo:suggests .?restr owl:someValuesFrom/rdfs:subClassOf* ?disorder .FILTER(lang(?disorder_label)= "${language}")}ORDER BY ?disorder_labelThe result from this query is a list of ectopic preg-nancy types (URI, label and definition) and can be usedin subsequent queries to suggest new signs associatedwith these ectopic pregnancy types.EvaluationWe conducted an evaluation of the ontology, the know-ledge base and the application. The ontology and the know-ledge base were evaluated through a questionnaire andusers evaluated the application based on clinical cases.Evaluation of the ontology: Does the ontology containthe appropriate vocabulary for ectopic pregnancyultrasound imaging?The vocabulary provided by the ontology was presentedto a group of potential users with different levels of ex-pertise. After a demonstration of the application followedby a brief hands-on session to search for terms, we col-lected feedback from each user by anonymous question-naire. Questions assessed whether the terms for signs,anatomical structures, types of ectopic pregnancy andtechnical elements were consistent with their clinical prac-tice and if they were able to find the signs they were look-ing for in the application.Evaluation of the knowledge base: Are the suggestedsigns and images useful?The signs and images suggested by the knowledge basewere assessed by the same panel of users through an-other questionnaire. We asked users if they learned newsigns for some types of ectopic pregnancy and whetherthe reference images provided were helpful for analyzingultrasound images. Here we distinguished between jun-ior and senior users, because our intuition was that thejuniors are more likely than seasoned physicians to learnfrom our system.Evaluation of the application based on clinical casesUsing our application, users annotated ultrasound imagesof ectopic pregnancy scans. This study was approved bythe ethic committee of the French National College ofObstetrics and Gynecology (No CEROG 2015-GYN-1002). The ultrasound scans (reports and images) wererandomly selected from ectopic pregnancy cases managedat the Pyramids Medical Imaging Center in Paris and theEarly Pregnancy Unit at UCLH. All personally identifyinginformation was removed from the text of the reports,from the content of the images and from the image meta-data. Each observer was assigned a subset of 10 cases foranalysis, of which 5 were common to all observers and 5were specific. For each case, the observers were asked toannotate the images with the application. They were blindto the content of the ultrasound report.Our motivation for this preliminary evaluation was notso much to assess whether all relevant signs had been an-notated, but rather to ensure that the signs suggested byour application were appropriate. In other words, we focuson precision, not recall. Additionally, we evaluated the re-producibility of the annotations among the observers.PrecisionThe gold standard for the presence of signs on eachimage was derived from the ultrasound reports providedby the specialist centers. We measured the precision ofsign annotations provided by the observers (observedsigns) against the signs from the gold standard (relevantsigns). We used the usual definition for precision in in-formation retrieval [36]:precision ¼ relevant signsf g? observed signsf gj jj observed signsf gjReproducibilityThe measure for assessing the reproducibility of the anno-tations was the proportion of agreement for categorical as-sessment across multiple observers [37]. The proportionof agreement pa for a given sign in a given image was theratio of the number of agreements between the observers(i.e., the number of pairs of observers who agree) for thepresence of the sign, over the number n of trials of agree-ment (i.e., the total number of pairs of observers). For ex-ample, considering a group of x = 6 observers, of whom 5observers annotated one of the images with a given sign,the number of trials of agreement is n = 1 + 2 +? + (x ?1) = 15, and the number of agreements among the 5 ob-servers is 10. Thus, pa is 10/15 = 66%. The 95% confidenceinterval (CI) for pa was calculated from the Standard Errorof the proportion: SE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffipa 1?pað Þ=np. Considering astandard normal distribution for pa, the 95% CI is pa ±Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 7 of 131.96 × SE. Statistical computations were performed usingR version 3.2 and STATA version 14.ResultsEctopic pregnancy ontologyAs of June 2016, the ectopic pregnancy ontology (version1.1) contains 1388 concepts to describe ectopic pregnancyultrasound images, organized into several subsumptionhierarchies for types of ectopic pregnancies and the signs,anatomical structures and technical elements of imagingassociated with ectopic pregnancy. The usual metrics forontology description are presented in Table 1. There are24 classes for the types of ectopic pregnancy, as illustratedin Fig. 4. The 90 concepts for ultrasound signs includeendometrial trilaminar pattern, tubal ring sign, andring of fire sign. While most sign concepts are repre-sented as primitive classes, some of them are defined clas-ses. For example, the concept color Doppler sign is adefined class equivalent to [rdfs:subClassOf imaging signand epo:requiresMode some color Doppler mode]. Ingeneral, we created defined classes for the categories ofsigns by technical element (e.g., by the examination mode(e.g., 2D ultrasound sign, color Doppler sign) and byimplantation site of ectopics (e.g., tubal pregnancy sign,c-section scar pregnancy sign).There are 484 concepts for anatomical structures ofthe female pelvic anatomy (e.g., uterus, uterine tube,zone of uterine tube and ampulla) and early gesta-tional structures (e.g., gestational sac, trophoblast).General anatomical concepts from the FMA were usedto seed the hierarchy (e.g., organ zone and non gesta-tional anatomical structure). Specialized concepts (e.g.,gestational sac) were added to extend the FMA hier-archy as necessary for our application.The technical element concepts were organized intothree hierarchies for examination route, examinationmode and echographic view. There are 3 examinationroute subclasses (e.g., vaginal route), 9 examination modeTable 1 Ectopic Pregnancy Ontology (v1.1) metricsClass count 1399Object property count 44Individual count 0SubClassOf axioms count 2707EquivalentClasses axioms count 50DisjointClasses axioms count 39AnnotationAssertion axioms count- skos:prefLabel 1749- skos:altLabel 298- skos:definition 489- epo:FMAID (FMA class UI) 295subclasses (e.g., color Doppler mode), and 17 echographicview subclasses (e.g., longitudinal view of the uterus).The asserted subsumption hierarchy of the ontologyinvolved 2707 relations. The domain and range of 44 re-lations (e.g., hasLocation, suggests, requiresMode)are defined in the ontology. Finally, this ontology in-cludes no individuals, because instances of signs are theactual signs observed on images from a clinical case.Knowledge base for image annotationIn the knowledge base, the 81 signs defined in the ontol-ogy are related to ectopic pregnancy types, anatomicalstructures and the three categories of technical elements(the echographic view, the examination mode and theexamination route) as illustrated in Fig. 3. There are 169asserted relations between these signs and the differenttypes of ectopic pregnancy, as some signs can be associ-ated with several types of ectopic pregnancy. Similarly, thesigns can be related to multiple anatomical structures(with 239 asserted relations), as well as multiple technicalelements (with 356 asserted relations). The asserted know-ledge from the sign-centric knowledge base characteristicsis summarized in Table 2. After inference in the know-ledge base, 618 inferred relations between signs and typesof ectopic pregnancy were produced, as well as 1503 in-ferred relations between signs and technical elements.The signs in the knowledge base are associated withreference images and PubMed citations. One hundredand six articles from 33 medical journals werereviewed for establishing the collection of referenceimages, resulting in the selection of 80 images depict-ing relevant ultrasound signs. A total of 77 PMID an-notations and 98 image annotations illustrate the signsin the knowledge base.Application for ultrasound image annotationsAn overview of the user interface of the application ispresented in Fig. 5. The image to annotate is displayedin the top left corner of the screen. The annotationsearch field at the bottom of the screen supports auto-completion for terms related to ectopic pregnancy types,anatomical locations, technical elements and ultrasoundsigns. The results are displayed in a sliding panel andthe user can select the relevant terms, which are thenadded as image annotations in the top right corner ofthe screen as image annotation. As the user selects an-notations, the system provides a selection of referenceimages from the collection in the bottom left corner toillustrate the selected annotations. Finally, in the bottomright corner, the system suggests other signs of interestbased on the type of pregnancy, anatomical structureand technical elements selected.Fig. 4 Taxonomy of ectopic pregnancy by implantation sites (view of the Ectopic Pregnancy Ontology)Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 8 of 13EvaluationEvaluation of the ontology: Does the ontology contain theappropriate vocabulary for ectopic pregnancy ultrasoundimaging?A total of 17 users (junior and senior OB/GYN practi-tioners and radiologists, and sonographers from Franceand the UK) were presented with the application. Theirfeedback on the terms available in the ontology was gen-erally favorable. More specifically, 100% of the usersfound the vocabulary for the ectopic pregnancy signs tobe consistent with their clinical practice, 94,1% for theanatomical structures and 82,4% for the terms describingtechnical elements of imaging. Moreover, 82.4% wereable to find the signs they were looking for in the appli-cation, without further assistance.Evaluation of the knowledge base: are the suggestedsigns and images useful?Overall, half of the users (52.9%), including all five juniorusers, learned about new signs associated with ectopicpregnancy types. The reference images suggested by theTable 2 Characteristics of the ectopic pregnancy knowledgebase for imaging signsObject property Axioms (n)Relations- Ultrasound Sign ® Ectopic Pregnancy <epo:suggests> 169- Ultrasound Sign ® Anatomical Structure <epo:hasLocation> 239- Ultrasound Sign ® Technical Element <epo:requires> 356Annotations- PubMed citations <epo:PMID> 77- Image from reference collection <epo:ImagePath> 98application were always of often useful for 14 users(82.4%). One user considered that the suggested imageswere sometimes useful and two users considered thesuggested images rarely useful. As expected, the useful-ness of the application depended on the expertise of theuser, with junior users benefitting most.Evaluation of the application based on clinical casesSix independent observers, all OB/GYN practitionerswith different level of training in ultrasound imaging(three seniors, two senior registrars and one registrar)annotated 206 ultrasound images from 35 clinical casesof ectopic pregnancy (five common cases and five add-itional cases for each user). The cases are presented inTable 3. The observers provided 1486 annotations withan overall precision of 0.83. The precision for each signis presented in Fig. 6.For the five common cases, the observers used 46 dis-tinct signs to create 841 annotations. For 783 annota-tions (covering 26 distinct signs), the annotation wasmade by at least two observers. The 58 remaining anno-tations (6.9%) were created by only one of the six ob-servers and involved 20 distinct signs. The totalproportion of agreement for the presence of signs in im-ages was 40.35% [38.64%-42.05%]95%CI. The reproduci-bility for each sign annotation is presented in Table 4.DiscussionWe have developed an application ontology, a know-ledge base and an application for the annotation ofultrasound images of ectopic pregnancy. This was thefirst attempt to build semantic resources in this domain.We discuss the significance of our findings, as well asFig. 5 Overview of the interface of the web application for ectopic pregnancy ultrasound image annotations. This graphical user interface wasdeveloped using the AngularJS (https://github.com/angular/angular.js) and Bootstrap (https://github.com/twbs/bootstrap)Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 9 of 13the limitations and perspectives of this resource fromthe perspective of clinical application development.SignificanceUsing Sematic Web technologies [38] and ontologies[18], we successfully developed a comprehensive, unam-biguous, shared and computable representation of theectopic pregnancy ultrasound signs, for which existingresources were insufficient.The ontology and the knowledge base received positivefeedback from a panel of medical users (including mixedmedical staff and sonographers). This preliminary evalu-ation demonstrates that they were able to identify mor-phological ultrasound features for a particular diagnosisand to associate them with pre-defined terms. The useof a large and diverse corpus as our source of vocabularywas critical for reaching a shared and fine-grained repre-sentation of the domain [20]. As expected, the signs de-scribed in the ontology are consistent with the mostTable 3 Types of ectopic pregnancy among the 35 ultrasoundcases used for the annotation evaluationType of ectopic pregnancy Cases in common Other casesTubal pregnancy 3 (20 images) 12 (67 images)Cesarean section scar pregnancy 2 (13 images) 12 (74 images)Cervical pregnancy - 4 (24 images)Interstitial pregnancy - 2 (8 images)Total 5 (33 images) 30 (173 images)important signs for tubal pregnancy diagnosis identifiedin the recent meta-analysis by Richardson et al. [39].The relevance of this application ontology is illustratedby a high precision rate of 83%, which reflects the pro-portion of correct sign annotations made by the ob-servers. This result is especially encouraging at a timewhen we are considering using this knowledge base in aclinical decision support system.The global proportion of agreement was 40.35%, whichis satisfactory considering the number of images (33) andsigns (26) involved. In comparison, a proportion of agree-ment of 50% was reported for the binary assessment ofthe abnormality of fetal heart rate in 20 cardiotocogramsby 5 observers [37]. Interestingly, some signs with moder-ate proportions of agreement (e.g., tubal ring sign, pa =27.3% [20.5-34.1]95%CI), had good precision rates (e.g., pre-cision = .77 for tubal ring sign). Moreover, in these im-ages, a more general sign with a higher agreement waspresent (e.g., adnexal mass distinct from ovary, pa =79.4% [73.5-85.3]95%CI). This was an expected effect ofsign suggestions in the application.LimitationsLimitations of the ontologyMany biomedical ontologies developed recently have usedthe basic formal ontology (BFO) [40] as their top-levelontology for interoperability with other OBO ontologies.Instead, we used our local core ontology for medicine(Ménélas), because interoperability with other projects inFig. 6 Precision of ectopic pregnancy sign annotations in ultrasound imagesDhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 10 of 13our institution was more important. Moreover, the use of atop-level ontology was not a primary requirement in thedesign of our application ontology. Similarly, we did notuse the popular MIREOT [41] strategy for referencing ex-ternal resources in our ectopic pregnancy ontology. Be-cause it was crucial for this application ontology to ensurethe stability of our application, we decided to restrict to aminimum the ontological commitment that comes with thereuse of external, evolving ontologies. However, we kept themapping to reference resources, such as the FMA.Preliminary evaluationIn its current state, the application we developed onlysupports the annotation of clinical images, not the diag-nosis of the conditions represented on these images. Forour evaluation, most of the signs from the ontology usedin annotations were tubal pregnancy signs, cesarean-section scar pregnancy signs and some signs that werenot specific of a location. While sufficient for evaluatingthe precision and reproducibility of the annotations, thisskewed dataset would be insufficient for the evaluationof a diagnostic system.Toward a clinical decision support system (CDSS) forectopic pregnancy diagnosisThere is a need for CDSS in the domain of ultrasound signsfor early pregnancy. Except in specialist centers, manywomen with ectopic pregnancy will not be diagnosed bytransvaginal ultrasound at their first visit. However,adequate management necessitates detailed ultrasound dif-ferential diagnosis of the different early pregnancy compli-cations [4, 42] which requires advanced training [43]. Inpractice, only some of the initial transvaginal scans are per-formed by experts, thus delaying the appropriate diagnosisand treatment, increasing adverse outcomes and also gener-ating a significant number of visits [8, 44, 45]. In this con-text, a CDSS for early identification of relevant ectopicpregnancy signs will likely benefit non-expert operators.Table 4 Proportion of agreement on the presence of ultrasound signs in the 5 common cases of ectopic pregnancySign annotations form the Ectopic Pregnancy Ontology Agreement (n) Lack ofagreement (n)Images (n) Proportion of agreement(% and [95% CI])Endometrial trilaminar pattern 48 12 4 80.00 [69.88 - 90.12]Adnexal mass distinct from ovary 143 37 12 79.44 [73.54 - 85.34]Embryo visible outside the uterine cavity 45 15 4 75.00 [64.04 - 85.96]Gestational sac outside the uterine cavity 226 104 22 68.48 [63.47 - 73.49]Adnexal mass adjacent to ovary 122 58 12 67.78 [60.95 - 74.61]Gestational sac or trophoblast in a myometrial defect in previous caesareansection scar pregnancy site119 76 13 61.03 [54.18 - 67.88]Adnexal mass and corpus luteum at the same side 82 53 9 60.74 [52.50 - 68.98]Adnexal rounded hyperechoic mass 63 57 8 52.50 [43.57 - 61.43]Ring of fire sign 39 36 5 52.00 [40.69 - 63.31]Yolk sac visible outside the uterine cavity 64 71 9 47.41 [38.99 - 55.83]Adnexal mass as gestational sac with yolk sac 22 38 4 36.67 [24.48 - 48.86]Caesarean section scar pregnancy peritrophoblastic blood flow 19 41 4 31.67 [19.90 - 43.44]Intact endometrial midline echo 14 31 3 31.11 [17.58 - 44.64]Tubal ring sign 45 120 11 27.27 [20.47 - 34.07]Tubal ring without central identifying feature 24 66 6 26.67 [17.53 - 35.81]Trophoblast visible outside the uterine cavity 110 325 29 25.29 [21.21 - 29.37]Anterior distortion of uterus serosa 24 96 8 20.00 [12.84 - 27.16]Smaller trophoblastic border distance to the anterior uterine serosa 33 162 13 16.92 [11.66 - 22.18]Ectopic pregnancy wall more echogenic than corpus luteum wall 21 114 9 15.56 [9.45 - 21.67]Fluid collection located centrally within the uterine cavity 4 41 2 13.33 [1.17 - 25.50]Non intact endometrial midline echo 4 41 3 8.89 [0.57 - 17.21]Gestational sac or trophoblast located at the level of internal cervical os 10 125 9 7.41 [2.99 - 11.83]Gestational sac inside anterior myometrium and uterine cavity 1 44 3 2.22 [0.00 - 6.52]Gestational sac located eccentricaly from uterine cavity 1 149 10 0.67 [0.00 - 1.98]Total 40.35 [38.64 - 42.05]Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 11 of 13However, developing a CDSS for early pregnancy is challen-ging for several reasons. Potential users have heterogeneousexpertise; there is no standard terminology describing therelevant ultrasound signs; and the quality of ultrasound im-ages varies significantly among operators.We consider this ectopic pregnancy image annota-tion application, with its underlying ontology andknowledge base, a step toward a clinical decision sys-tem for ectopic pregnancy diagnosis. Research inCDSS based on ontologies has demonstrated differen-tial diagnosis assistance in Human Genetics [46] or inconventional Radiology [47].The precision of the annotations derived from ourknowledge base is promising for developing a CDSSfor ectopic pregnancy ultrasound. The prospectiveevaluation of a clinical decision support system(CDSS) based on our knowledge base should demon-strate improvement in clinical care. For example, theexpectation would be that, junior operators guided bythe signs suggested by the system achieve a betteranalysis of ultrasound images, and therefore reach thecorrect diagnosis more often than without the system.A specific challenge for such clinical evaluation is itsintegration in the clinical workflow.Finally, the knowledge base we developed could beextended from ectopic pregnancy to early pregnancy(i.e., including molar pregnancy, miscarriage and mul-tiple pregnancy at early stages of development), andmore generally to the next stages of fetal develop-ment (i.e., to represent ultrasound signs associatedwith fetal disorders).ConclusionsWe have developed a new ectopic pregnancy know-ledge base for the annotation of ultrasound images.The elements of this knowledge base (signs and typesof ectopic pregnancy, anatomical structures involvedand technical elements of imaging) are organized intoan ontology. We have demonstrated the use of thisDhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 12 of 13knowledge base for the annotation of ultrasound im-ages of ectopic pregnancy, with promising resultsfrom the perspective of clinical decision support sys-tem development. Other gynecological disorders andfetal abnormalities may benefit from our approach.AbbreviationsCDSS: Clinical decision support system; FMA: Foundational model ofanatomy; NCBO: National center for biomedical ontology; NLP: Naturallanguage processing; OBO: Open biomedical ontologies; OWL: Ontologyweb language; PMID: PubMed identification number; RadLex: Radiologylexicon; SKOS: Simple knowledge organization system; UCLH: UniversityCollege London Hospital; UMLS: Unified Medical Language SystemAcknowledgementsThis work was conducted using the Protégé resource, which is supported bygrant GM10331601 from the National Institute of General Medical Sciences of theUnited States National Institutes of Health. (website: http://protege.stanford.edu)The lexicalization plugin Archonte 4.2 for Protégé 5, developed by LaurentMazuel and supported by INSERM U1142 LIMICS, was used to edit the SKOSmultilingual annotations in the ontology. (website: http://github.com/ics-upmc/archonte)The authors would like to thank Olivier Bodenreider for his expert advice andsupport throughout the final editing of this manuscript.FundingThe development of the web-application was founded by the SATT-Lutechfor the Pierre and Marie Curie University, Paris, France.Availability of data and materialThe ontology for ectopic pregnancy imaging is available on BioPortal:http://bioportal.bioontology.org/ontologies/EPOAuthors contributionsAll the authors were involved in the drafting or the revising at different stagesof the manuscript. FD and PM developed the ontology and knowledge base incollaboration with JC, JMJ, EJ and DJ. FD lead the development of the webapplication and the knowledge base integration. FD, PM, JMJ, and EJestablished the evaluation protocols (questionnaires and clinical cases). PMpresented the application and collected the questionnaires. DJ and NP, ectopicpregnancy experts, provided the de-identified clinical material (text, images).FD, NL, and BK did the statistical analysis. SF, LG tested the application at everystage of development. All authors read and approved the final manuscript.Competing interestsThe authors declared that they have no competing interest.Consent for publicationNot applicable.Ethics approval and consent to participateThis study was approved by the institutional review board of the FrenchCollege of Obstetrics and Gynecology (No CEROG 2015-GYN-1002) for theuse of de-identified human data.Author details1UPMC Medical Faculty (Paris 6), Department of Fetal Medicine in ArmandTrousseau Hospital (APHP), INSERM U1142 (LIMICS), 26 Avenue du Dr ArnoldNetter, 75012 Paris, UE, France. 2INSERM U1153 (Obstetrical, Perinatal andPediatric Epidemiology Research Team, Center for Biostatistics andEpidemiology), Maternité Port Royal, 53 Avenue de lObservatoire, 75014Paris, UE, France. 3APHP DSI, INSERM U1142 (LIMICS), 15, rue de lÉcole deMédecine, 75006 Paris, UE, France. 4Pyramides Medical Imaging Center, 13av. de lOpéra, 75001 Paris, UE, France. 5University College Hospital (UCLH)Department of Obstetrics and Gynaecology, Academic Department ofObstetrics and Gynaecology, University College London (UCL) Institute forWomens Health, 86-96 Chenies Mews, London WC1E 6HX, UE, UK.6Department of Obstetrics and Gynaecology, Gynaecology Diagnostic andOutpatient Treatment Unit, University College Hospital (UCLH), 235 EustonRESEARCH Open AccessAn ontological analysis of medical Bayesianindicators of performanceAdrien Barton1,5*, Jean-François Ethier1,4,5, Régis Duvauferrier2,3 and Anita Burgun4AbstractBackground: Biomedical ontologies aim at providing the most exhaustive and rigorous representation of reality asdescribed by biomedical sciences. A large part of medical reasoning deals with diagnosis and is essentiallyprobabilistic. It would be an asset for biomedical ontologies to be able to support such a probabilistic reasoningand formalize Bayesian indicators of performance: sensitivity, specificity, positive predictive value and negativepredictive value. In doing so, one has to consider that not only the positive and negative predictive values, but alsosensitivity and specificity depend upon the group under consideration: this is the spectrum effect.Methods: The sensitivity value of an index test IT for a disease M in a group g is identified with the proportion ofpeople in g who have M who would get a positive result to IT if the test IT was realized on them. This value can beestimated by selecting a reference test RT for M and a sample s of g, and measuring the proportion, amongmembers of s having a positive result to RT, of those who got a positive result to IT. Similar approximationstrategies hold for prevalence, specificity, PPV and NPV. Indicators of diagnostic performances and their estimationsare formalized in the context of the OBO Foundry, built on the realist upper ontology Basic Formal Ontology (BFO).Results: Entities and relations from the Ontology for Biomedical investigations (OBI) and the Information ArtifactOntology (IAO) are used and complemented to represent reference tests and index tests, tests executions, testsresults and the relations involving those entities, as well as the values of indicators of performance and theirestimates. The computations taking as input several estimates of an indicator of performance to produce a finerestimate are also represented. The value of e.g. sensitivity estimates should be dissociated from the real sensitivityvalue  which involves possible, non-actual conditions, namely the result a person would get if a medical testwould be performed on her. Such conditions could not be directly represented in a realist ontology, but arepresentation is proposed that introduces only actual entities by considering a disposition whose probability valueis the real sensitivity value. A sensitivity estimate is a data item which is about such a disposition.Conclusions: This model provides theoretical basis for the representation of entities supporting Bayesian reasoningin ontologies.Keywords: Sensitivity, Specificity, Medical test, Spectrum effect, Disposition, Realist ontology, Informational entityBackgroundDefinition of indicators of performanceBiomedical ontologies aim at providing the most exhaust-ive and rigorous representation of reality as described bybiomedical sciences. A large part of medical reasoningdeals with diagnosis and is essentially probabilistic. Itwould be an asset for biomedical ontologies to be able tosupport such a probabilistic reasoning.Ledley and Lusteds seminal article [1] on Bayesian rea-soning in medicine defines different kinds of probabilisticentities. Consider for example the simple case of an in-stance of test of type IT (for index test  a test whose ac-curacy is being measured) aiming at detecting if a patient ina group g has an instance of disease of type M.1 The per-formance of test IT in diagnosing M can be quantified bythe positive predictive value of this test, hereafter abbrevi-ated PPV, defined by the Oxford Handbook of Medical* Correspondence: adrien.barton@gmail.comEqual contributors1Département de médecine, Université de Sherbrooke, Sherbrooke, Québec, Canada5Centre de recherche du CHUS, CIUSSS de lEstrie-CHUS, Sherbrooke,Québec, CanadaFull list of author information is available at the end of the article© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Barton et al. Journal of Biomedical Semantics  (2017) 8:1 DOI 10.1186/s13326-016-0099-4Statistics [2] as the proportion of tested positives who aretrue positives and by the negative predictive value, here-after abbreviated NPV, defined as the proportion of testednegatives who are true negatives. These values provide theprobability that a patient has or not the disease, dependingupon the result (positive or negative) to the test.However, such values depend on some characteristicsof the patient. If a patient received a positive test, theprobability that he has the disease can for example dependupon his sex, his status of smoker or non-smoker, andother biological or environmental parameters. In particular,it depends on the prevalence of the disease among thegroup of persons with those characteristics.Therefore, the statistical data communicated in themedical literature for a test are generally not the positiveand negative predictive values, but the so-called sen-sitivity and specificity. The Oxford Handbook ofMedical Statistics defines sensitivity as the propor-tion of those who have the disease who are correctlyidentified by the test as positive ([2], p. 340) and spe-cificity as the proportion of those who do not havethe disease who are correctly identified by the test asnegative. The PPV and NPV can be computed on thebasis of the prevalence Prev, sensitivity Se and specifi-city Sp thanks to the following Bayesian equations:PPV ¼ Prev:SePrev:Seþ 1?Prevð Þ 1?Spð ÞNPV ¼ 1?Prevð Þ:SpPrev: 1?Seð Þ þ 1?Prevð Þ:SpIn the remainder of the article, sensitivity, specificity,PPV and NPV will be called (Bayesian) indicators ofperformance and abbreviated IPs.In the wake of Ledley and Lusted [1] the sensitivityand specificity values have often been considered as de-pending only on the pathophysiological characteristics ofthe disease and of the test, and thus as being independentof the group of people under consideration. However, sen-sitivity and specificity values do in fact depend upon thegroup under consideration: this is the spectrum effect[3].The spectrum effectIf IT is an index test and M is a disease, lets introducef1(IT,M) as the proportion of individuals who get a posi-tive result to IT, among individuals who have M, whichfits with the usual definition of sensitivity (as providedby [2]). The main problem with this definition is that itdoes not specify the reference population. "The individ-uals who have M are part of which population: thepopulation in a given sample? The population of a spe-cific country? The whole human population? Ledley andLusted [1] considered that sensitivity and specificitydepend upon pathophysiological characteristics of thedisease, but not upon the population in consideration. Ifthis was the case, the proportion of people tested posi-tive among the diseased would be the same in any groupunder consideration  abstracting from statistical fluctua-tions due to randomness. However, as has been recog-nized by the medical literature, but regularly omitted, thishypothesis is false for at least two reasons. First, most testsare not inherently dichotomous but rely on acategorization of individuals based on continuous traits[3]. Second, various populations can express various dis-ease characteristics (such as various degrees of severity[4]) that will influence the chance to get a positive resultto a test.The latter can be illustrated with the following ex-ample. Suppose that around 80 % of people havingrheumatoid arthritis have a rheumatoid factor (RF), andwould with certainty receive a positive result to a testthat would perfectly2 detect this factor; and that theremaining 20 % do not have a rheumatoid factor, andwould receive a negative result (yet do have the disease).The diseased population is then composed of two sub-groups: a subgroup sg1 whose members would all getfor sure a positive result to IT, and a subgroup sg2whose members would all get for sure a negative result(see Fig. 1). The sensitivity calculated in this examplewould be 80 %.Nevertheless, in reality, those proportions varybased upon various characteristics of the patients. Forexample, RF presence increases with age at onset ofdisease in juvenile arthritis [5]. As a result, the sensi-tivity of a test for RF will increase according to theage of the individuals of the population being tested.Its sensitivity will be lower in younger patients andhigher in older patients.Fig. 1 Variation of sensitivity depending on the groupunder considerationBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 2 of 13Therefore, f1 is not a well-defined function: the valueof the proportion does not depend only upon IT and M,but also upon the population g under consideration(which could be, for example, the whole human popula-tion, the Canadian smoker population, the female popula-tion, etc.). This is the spectrum effect, which can also bemanifested, for example, as a dependence of sensitivityand specificity on the degree of severity of the disease inthe group under consideration [4].The sensitivity can therefore depend on the group gunder consideration. A better candidate than f1(IT,M) tothe definition of the sensitivity value would be the func-tion f2(g,IT,M) defined as the proportion3 amongpeople in g who have M of those who would get a posi-tive result to IT if the test IT was realized on them the mention in italic is necessary, as a test IT will not berealized on all individuals who have M, but on a sampleonly. The next part will distinguish three related entities:the real sensitivity4 value, its estimates, and the measure-ments of proportion in samples. It will also explain howsuch entities should be distinguished in an ontology of IPs.MethodsProportion measurement in a sampleIt is impossible to know f2(g,IT,M) with certainty inpractice, for two reasons. The first reason is that it isoften not possible to determine with certainty, throughreasonable means, whether a given person has the dis-ease M or not; in some cases, the only way to be certainwould be to perform an autopsy on the deceased patient.Therefore, one needs to use a reference test, which isthe best diagnostic test that is reasonable to perform inthe present context (for more on the distinction betweena reference test and the associated disease, see sectionThe challenge of representing indicators of performancein an ontology below).If the patient receives a positive result to this referencetest, it will be concluded that he has the disease; if he re-ceives a negative result, it will be concluded that he doesnot have it. But those inferences can be wrong: the refer-ence test might lead to a positive result for a non-diseased person, or a negative result for a diseased per-son. If RT is a reference test for M and IT is an indextest (of unknown accuracy) for M, then one can definethe function f3(g,IT,RT) as the proportion, among indi-viduals of g who would get a positive result to RT if thetest RT had been performed on them, of people whowould get a positive result to IT if the test IT was real-ized on them. Since RT is a reference test for M,f3(g,IT,RT) approximates f2(g,IT,M). Both values can dif-fer though: this is a first epistemic limit to the know-ledge of f2(g,IT,M).On top of this, f3(g,IT,RT) is not directly measurable.As a matter of fact, a test IT is never realized on apopulation as large as e.g., the whole population ofsmokers, or the whole male population. It is howeverpossible to approximate f3(g,IT,RT) by performing bothtests IT and RT on individuals in a sample s judged asbeing representative of the population g. Lets definef4(s,IT,RT) as the proportion, among members of s whogot a positive result to RT, of those who got a positive re-sult to IT. If s is a representative sample of g, thenf4(s,IT,RT) does approximate f3(g,IT,RT)  and thus, bytransitivity, does approximate f2(g,IT,M). Note that aslong as the sample s is not perfectly representative of g,f4(s,IT,RT) will differ at least slightly from f3(g,IT,RT)(which also differs from f2(g,IT,M)): this is a secondlimit to the knowledge of f2(g,IT,M).Lets illustrate those two limits of estimations with astudy [4] which analyzes the quality of the Neer test(here written IT) for diagnosing the shoulder impinge-ment syndrome (written M), a syndrome that is charac-terized by rotator cuff muscles inflammation near thesub-acromial space. In this study, the Neer test IT is re-alized on a sample (written s) of 552 patients, judged asrepresentative of the target population (g). Park et al.[4] take as reference test (RT) the surgical observation.Here, f4(s,IT,RT) is the proportion of people in thesample who have received a positive result to the Neertest, among those diagnosed as positive by surgical op-eration. f4(s,IT,RT) approximates f3(g,IT,RT), namelythe proportion of individuals in the target population gwho would get a positive result to the Neer test amongthose who would get a positive result by surgical observa-tion, if those tests were performed on them. Finally,f3(g,IT,RT) itself approximates f2(g,IT,M), which is theproportion of individuals in g who would receive a posi-tive Neer test result among those who have an im-pingement syndrome. Thus, f4(s,IT,RT) approximatesf2(g,IT,M).Note that similar approximation strategies hold forprevalence, specificity, PPV and NPV. Concerning e.g.specificity, one could thus define f 2(g,IT,M) as the pro-portion5 among people in g who dont have M of thosewho would get a negative result to IT if the test IT wasperformed on them; and f 4(s,IT,RT) as the proportion,among members of s who got a negative result to RT,of those who got a negative result to IT. Thus,f 4(s,IT,RT) approximates f 2(g,IT,M).Sensitivity value and sensitivity estimatesNow that those definitions have been given, we can de-termine which entity the word sensitivity refers to inthe medical literature. At first sight, this term might ap-pear polysemic. To illustrate this, lets consider a studywhich evaluates the quality of an exercise test in thediagnosis of coronary artery disease, and claims: Thesensitivity varied substantially according to sex (womenBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 3 of 1330 % and men 64 %) [6]. On one hand, the statementsensitivity varies substantially according to the sex sug-gests that sensitivity depends on the target population gin consideration, and that there is a sensitivity value forthe female population, and another one for the malepopulation. This formulation thus suggests that sensitivityvalue is given by the function f2(g,IT,M). However, thevalue 30 % assigned to the sensitivity of the test for womenrefers to a proportion which has been measured by the au-thors in a sample of 37 women, using coronary angiog-raphy as a reference test. This might thus suggest that thesensitivity value is in fact given by the function f4(s,IT,RT)However, two arguments suggest that the sensitivityvalue should be interpreted as f2(g,IT,M) rather thanf4(s,IT,RT). First, the value which is ultimately relevantfor medical practice is f2(g,IT,M): if s is a sample of gand RT is a reference test for M, f4(s,IT,RT) is of interestfor the medical practitioner only insofar as it providesan information on the diseaseM and the target populationg from which the sample is taken  that is, insofar as itprovides an estimate of f2(g,IT,M). Indeed, the fact that afew people who got a positive result to RT in a given sam-ple have got a positive or negative result to a test IT hasmedical relevance only insofar as it teaches us somethingabout how diseased people in the target population (notonly in the sample) will react to this test IT.Second, the sensitivity value is usually given with a95 % confidence interval (see e.g., [7] or [8]), which esti-mates the likely range of error in determining the sensitivityvalue. But f4(s,IT,RT) can be measured with certainty,6 andthus the confidence interval cannot characterize the uncer-tainty on our knowledge of f4. On the other hand, there issome uncertainty on the knowledge of f2(g,IT,M) andf3(g,IT,RT), as they are estimated on the basis of f4(s,IT,RT).Therefore, the 95 % confidence interval would characterizethe uncertainty on the knowledge of f3(g,IT,RT), which istaken as a proxy for f2(g,IT,M).7Thus, those two arguments suggest that the term sensi-tivity should refer to f2(g,IT,M)  which is relative to adisease and a target population  rather than to f4(s,IT,RT) which is relative to a reference test and a sample.8 Asfor f4(s,IT,RT), it can be interpreted as the value of a meas-urement of proportion in a sample, which provides an es-timate of the sensitivity value.Therefore, a sentence such as The sensitivity variedsubstantially according to sex (women 30 % and men64 %) should, more rigorously, be formulated as: Thesensitivity varies substantially depending on the sex:through measurement of proportions in samples, itsvalue was estimated to be 30 % for the women, and 64 %for the men. We could prefer the first formulation, morecompact, for practical reasons; but it is important toremember that it is only a shortcut for the secondformulation.Accordingly, we will need to dissociate three differentkinds of entities. First, tests execution on a sample s, re-ferring more precisely to the process of performing testsIT and RT and measuring the numbers of true positive,false positive, true negative and false negative as opera-tionalized by IT and RT - for example, the false positiveare people who are tested positive by the index test ITbut negative by the reference test RT in the sample s.Second, the proportion of true positives among positives(as given by the reference test) is relative to the indextest, the reference test and the sample, and its value isgiven by the function f4(s,IT,RT); as such, it provides anestimate of the sensitivity value. Third, the real sensitiv-ity, which is relative to an index test, a disease and apopulation g, and whose value f2(g,IT,M) is given by theproportion of people in the group who would have apositive result to the test IT among those who are dis-eased. The real sensitivity would provide a better infor-mation than a sensitivity estimate on the probability thata random member of the group g would get a positivetest result, in case he has the disease. However, its valuef2(g,IT,M) cannot be known with certainty, contrarily tothe value of the sensitivity estimate f4(s,IT,RT).More generally, those considerations can be adaptedto other indicators of performance (specificity, PPV andNPV), as well as the prevalence. In particular, f 2(g,IT,M)should refer to the real specificity value, whereasf 4(s,IT,RT) can be interpreted as the value of a measuredproportion in a sample that provides an estimate of thereal specificity value. In particular, real sensitivity, speci-ficity, PPV and NPV, as we have defined them above, de-pend neither on the sample nor on the reference test.However, they are estimated on the basis of proportionmeasurements which depend both on the sample andthe reference test. Accordingly, when a study [9] men-tions cadaveric prevalence of the rotator cuff tears, thisexpression should be understood as a linguistical shortcutdenoting a proportion measurement in a sample when thecadaverical analysis is adopted as reference test; and theradiological prevalence should be understood as a pro-portion measurement when the radiological analysis isadopted as reference test. The real prevalence, how-ever, does not depend on the reference test.Aggregation of sensitivity estimatesFinally, we need to add a last layer to this model. Ap-proximations of sensitivity taken in different samples,with different index tests, can be combined in order tobuild a finer estimate of sensitivity for a more encom-passing category of index tests. Consider for example themeta-analysis [7] which assess the quality of peripheralthermometers in detecting fever. They use as referencetest a pulmonary artery catheter, and consider 29 studiesBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 4 of 13assessing the sensitivity and specificity of those devices.Combining those values, they come up with an estimateof 0.64 for the sensitivity and of 0.96 for the specificity.The challenge of representing indicators of performancein an ontologyTo the extent that they aim at representing biomedicalknowledge and enabling medical reasoning, biomedicalontologies should provide a formalization of IPs as wellas the prevalence, by dissociating e.g. the real sensitivityfrom the sensitivity estimates, and the process leading tothose estimates. This article will introduce such aformalization in the context of the OBO Foundry [10],one of the most massive set of interoperable ontologies inthe biomedical domain, built on the upper ontology BasicFormal Ontology (BFO) 1.1 [11].BFO endorses a realist methodology, which carefullydissociates material entities (such as disorders) frominformational entities (such as diagnosis). In commonmedical practice, a disease may be diagnosed in idealcircumstances by a given gold standard test, which canbe defined as the most accurate reference test; but the dis-ease, the diagnosis, and the result to a gold standard testare three different entities that should be distinguished. Asa matter of fact, many human diseases already existed afew thousands of years ago, much before they could be di-agnosed. Moreover, a diagnosis can be wrong or imprecise.Finally, a given gold standard can be later replaced by abetter one: this shows that the disease cannot be definedby a positive result to a gold standard - otherwise, therecould not be, by definition, a better gold standard. Thus,while a diagnosis of a disease represents the best know-ledge by some health or research professional of the pres-ence of the disease in a particular patient, a diagnosis isnot equivalent to a disease: it is rather about a disease.This formalization is compatible with IAO (InformationArtifact Ontology [16]) and OGMS (Ontology for GeneralMedical Sciences).The question of how probabilistic notions can be rep-resented in ontologies has been tackled from differentperspectives in the past. For example, [12] has proposedthe alternative PR-OWL format that extends the clas-sical OWL format; we take here a different approach,which does not aim at changing the OWL format. Solda-tova and colleagues [13] have described a model inwhich probabilities can be assigned to research state-ments. We build here upon an alternative approach [14],in which probabilities can be assigned to dispositions.Sensitivity and specificity have been recently introducedin the Ontology of Biological and Clinical Statistics (OBCS[15]) as subclasses of Data item. We will partly endorseand refine this classification, by considering estimates ofsensitivity and specificity as subclasses of Data Item, andextend this classification to PPV and NPV. A data item, asdefined by the Information Artifact Ontology (IAO) [16],is intended to be a truthful statement about something. Inorder to formalize IPs, one should thus clarify which en-tities in the real world they are about.Proportion measurements are data items that are ob-tained from some processes named "proportion mea-sures", which involve performing two kinds of tests (theindex test and the reference test) in a sample. On theother hand, we have defined a real sensitivity valuef2(g,IT,M) as the proportion of people who would get apositive result by IT among those who have the diseaseM. But note here the conditional structure: what is re-ferred to is the proportion of true positives among dis-eased if IT was performed on them. In realisticsituations, however, as explained above, the sensitivityvalue will be estimated by performing the test on a sam-ple of the population only  not the entire population g;thus, f2(g,IT,M) is the value of a non-actual proportion.9However, possible-but-non-actual situations cannot bestraightforwardly represented in a realist ontology likeBFO. To solve this problem, we will formalize the real IPvalue as the probability assigned to a disposition borne byan instance of group of individuals; and estimates of IPs asdata items which are about such a disposition. This willprovide a formal characterization of IPs and their esti-mates based on proportion measurements.ResultsThe formalization that will be presented here can be visu-alized on Fig. 2 and Fig. 3, in which classes are in rectan-gles, instances in boxes with rounded edges, and thenumerical value assigned by datatype properties in ellipses.Unless specified otherwise, all the relations used here be-long to BFO 1.1 [11].Test results and sensitivity estimateLet us first start with the formalization of test results andthe IP estimates they lead to (see Fig. 1).10 A Medical_testwill be here considered as a subclass of Planned_process(as defined by OBI, the Ontology for Biomedical Investiga-tions [17]) which consists in the observation of a givenfeature to infer the presence of another feature  in thecase of interest, a pathological entity such as a disease.Consider a medical test11 IT1 and a disease M:Medical_test is_a Planned_processIT1 is_a Medical_testM is_a DiseaseSuppose that we are interested in the sensitivity andspecificity of test IT1 for diagnosing M in a group g1.This group g1 will be formalized as a collection ofhumans (for more on collections, see [18]). To estimate thissensitivity and specificity, one can select a sample s1Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 5 of 13considered to be representative of g1 (which will be calledthe reference class). Thus:g1 instance_of Collection_of_humanss1 instance_of Sample_of_humansSample_of_humans is_a Collection_of_humanss1 part_of g1Lets now introduce the class of tests RT1 which arereference tests for M:RT1 is_a Medical_tests1 is composed of n humans, named p1, p2,,pn.Two12 tests will be performed on each pi: an instance ofRT1, named thereafter rt1,i, and an instance of IT1,named it1,i; thus, for every i between 1 and n:pi instance_of Humanpi part_of s1pi participates_in rt1,ipi participates_in it1,iWe introduce tests_executions1,IT1,RT1 which has aspart all the tests rt1,i and it1,i for i between 1 and n andthe recording of which members of the sample are truepositives (those who have been tested positive both byIT1 and RT1), true negatives (those who have been testednegative both by IT1 and RT1), false positives (those whohave been tested positive by IT1 but negative by RT1)Fig. 2 Real sensitivity and specificity values and their estimatesFig. 3 Aggregation of several sensitivity estimatesBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 6 of 13and false negatives (those who have been tested negativeby IT1 but positive by RT1). This recording leads (OBI:-has_specified_output) to the creation of the instance ofData_set named tests_resultss1,IT1,RT1:tests_executions1,IT1,RT1 instance_of Planned_processrt1,i part_of tests_executions1,IT1,RT1it1,i part_of tests_executions1,IT1,RT1tests_resultss1,IT1,RT1 instance_of Data_settests_executions1,IT1,RT1 has_specified_outputtests_resultss1,IT1,RT1The tests_resultss1,IT1,RT1 will then serve as input(OBI:has_specified_input) to a planned process notedcomputationSe1 which computes a sensitivity estimatesnoted estimateSe1 , by calculating the proportion of truepositives among positives:13computationSe1 is_a Planned_processestimateSe1 is_a Data_itemcomputationSe1 has_specified_inputtests_resultss1,IT1,RT1computationSe1 has_specified_output estimateSe1Finally, we can use the datatype property OBI:has_-specified_value to relate estimateSe1 with its numericalvalue f4(s1,IT1,RT1):estimateSe1 has_specified_value f4(s1,IT1,RT1)Similar strategies can hold for representing Specificity,PPV and NPV and their estimates.14Aggregation of sensitivity estimatesWe will now show how various sensitivity estimates canbe aggregated for a finer sensitivity estimate (cf. Fig. 3).Suppose that we have another sample s2 (also a part_of g),composed of n humans named q1, q2, ..., qn'. We can per-form another measure of sensitivity for a related (possiblyidentical to IT1) index test IT2 for M in g on this sample,using a related (possibly identical to RT1) reference testRT2, by performing instances of RT2 named rt2,j (for jbetween 1 and n) and instances of IT2 named it2,j on eachmember qj of s2. One can then define the entity tests_exe-cutions2,IT2,RT2 as a planned process which has as partthose tests rt2,j and it2,j, and which has as output tests_re-sultss2,IT2,RT2; the latter serves as input to another compu-tation of sensitivity computationSe2 , which has as outputanother estimate of sensitivity estimateSe2 , to which thevalue f4(s2,IT2,RT2) can be assigned (the latter being theproportion, among people who have been tested positive byRT2 in s2, of people who had a positive result to IT2).As explained earlier, various sensitivity estimates canbe combined to estimate the value of the sensitivity of atest for M in g. If IT1 and IT2 on one hand, and RT1 andRT2 on the other hand, are similar enough (in particular,if they are identical), those results might be gathered tocome up with a finer estimate of the sensitivity value.More specifically, if IT1 and IT2 can be subsumed undera common index test class IT0, and RT1 and RT2 can alsobe subsumed under a common reference test class RT0,then their values can be compiled mathematically (for ex-ample by meta-analysis methods) to come up with thevalue of a (hopefully finer) estimate named estimateSe1,2,whose value is given by a function h(s1,IT1,RT1,s2,IT2,RT2).This can be generalized to the aggregation of more thantwo former estimates.We can here introduce a planned process of computa-tion of sensitivity named computationSe1,2, which takes asinput both estimateSe1 and estimateSe2 , and the output ofsuch a process, a data item named estimateSe1,2:computationSe1,2 instance_of Planned_processestimateSe1,2 instance_of Data_itemcomputationSe1,2 has_specified_input estimateSe1computationSe1,2 has_specified_input estimateSe2computationSe1,2 has_specified_output estimateSe1,2estimateSe1,2 has_specified_value h(s1,IT1,RT1,s2,IT2,RT2)We will not aim at giving the details of this function h,which is the responsibility of the statistician, not the on-tologist  who focuses on how to represent such values.Finally, since estimateSe1 or estimateSe1,2 are informationalentities, they must be about some entities. To determinewhat those entities are about, we will need to formalize theentity to which is assigned the real sensitivity value.Real sensitivity valueAs said earlier, estimates of sensitivity of IT for M in gaim at estimating the real sensitivity value, which is givenby the proportion of members of g who would get a posi-tive result to IT among those who have M. However, thecondition of performing the test IT on the members of gis never realized, because the test is performed (at best) onone or several samples of the population, not on the wholepopulation g: the performance of test IT on the membersof g is a possible (leaving aside practical difficulties), non-actual condition. Interpreting specificity, PPV, and NPValong the former lines would also imply such possible,non-actual conditions.BFOs realist methodology [19] implies that all instancesshould be actual entities. Thus, one cannot representdirectly such a possible-but-not-actual condition in anontology based on BFO. In order to solve this difficulty,we will introduce a strategy named randomization,which will clarify the nature of the real sensitivity valueas a probability assigned to an actual entity, namely adisposition. This will also clarify what an estimate ofBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 7 of 13sensitivity is about, namely about this disposition. Thus, itwill enable to represent IPs in a realist fashion, compliantwith BFOs methodology.From proportions to objective probabilities: therandomization strategyWe will explain first how the proportion of a subgroupin a group can be formalized as a probability valueassigned to a disposition; this will help explaining laterhow the proportion of a subgroup in a group undergoinga possible, non-actual condition can be formalized alongsimilar lines.Dispositions are entities that can exist without beingmanifested; an example of disposition is the fragility of aglass, which can exist even when the glass does notbreak. We will use Röhl & Jansen's model of disposition[20] in BFO, which associates to every instance of dis-position one or several instances of realizations, and oneor several instances of triggers (a trigger is the specificprocess that can lead to a realization occurring). In thismodel, the fragility of a glass is a disposition of the glassto break (the breaking process is the realization) when itundergoes some kind of stress (the process of undergo-ing such a stress is the trigger); this disposition inheresin the glass. Starting with the definition of these entitiesand their relations at the instance level, Röhl & Jansenproceed to formalize them at the universal level. Previ-ous work [14] has shown how to adapt this model toprobabilistic dispositions. Thus, an instance of balancedcoin is the bearer of an instance of disposition to fall onheads (the realization process) when it is tossed (thetrigger process), to which an objective probability 1/2can be assigned.We will now extend the scope of this model to thesituation at hand. Consider the prevalence Prev(g,M),which was defined above as the proportion of personshaving M in the actual population g. We can define thedisposition dPrevg,M , borne by the group g, that a personrandomly drawn in g has M. More specifically, lets writeTg the process randomly drawing a person in g, andRg,M the process drawing by Tg someone who has M:the triggers of dPrevg,M are instances of Tg and its realizationsare instances of Rg,M. Following the lines of previous work[14], one can thus define the probability assigned to thedisposition15 dPrevg,M , which is the probability of drawingrandomly someone who has M in g. This probability isequal to the proportion of individuals who have M in g,that is, to Prev(g,M): if there are e.g., 10 % diseasedpeople in g, then the probability of drawing randomly adiseased person in g is 10 %. Thus, the prevalence valuecan be identified to the objective probability assignedto the disposition dPrevg,M . We name this strategy therandomization of the proportion of persons havingM in g.The randomization strategy may not be necessary toformalize a proportion in an actual group, such as theprevalence. But this strategy can also be applied to pro-portions of people in groups which are subject to a pos-sible, non-actual condition  and thus, be relevant toformalize sensitivity and other IPs, and their estimates.As a matter of fact, the real sensitivity value f2(g,IT,M)was defined as the proportion of people who would geta positive result to IT among Ms bearers in g. This valuecan be randomized as follows. We can define dSeg,IT,M asthe disposition16 to draw randomly, among the individ-uals of g who have M, someone who is tested positive byIT. More specifically, lets define the process TSeg,IT,M asthe performance of test IT on the individuals in g, andrandom draw of an individual among those who havethe disease M;17 and the process RSeg,IT,M as the drawingby TSeg,IT,M of someone who got a positive result to IT.The triggers of dSeg,IT,M are instances of TSeg,IT,M, and its re-alizations are instances of RSeg,IT,M . As it happens, the realsensitivity value f2(g,IT,M) is the objective probabilityassigned to this disposition dSeg,IT,M,: indeed, if there aree.g., 15 % of the diseased people in g who would get apositive result by IT, then the probability of randomlydrawing someone who got a positive test result by ITamong diseased people in g if test IT would be per-formed on them is equal to 15 %.Specificity value can be defined along similar lines, asprobabilities assigned to actual dispositions borne by thegroup g noted dSpg,IT,M (and similarly for the PPV andNPV). Although both dSeg,IT,M and dSpg,IT,M are dispositionsinhering in g, they have different triggers and differentrealizations; the process TSpg,IT,M is the performance oftest IT on the individuals in g, and random draw of anindividual among those who do not have the disease Mand the process RSpg,IT,M is the drawing by TSpg,IT,M ofsomeone who got a negative result to IT.Assignment of real sensitivity values to dispositionsLet us now consider how to formalize these probabilityvalues in ontologies. dSeg,IT,M is a disposition individual in-hering in the group g; and a probability value can beassigned to this disposition using a datatype propertyhas_probability_value [15]. This probability value iswhat we called the real sensitivity value:18dSeg,IT,M has_probability_value f2(g,IT,M)Thanks to our analysis above, we can now answer ouroriginal question, and state what sensitivity estimatessuch as estimateSe1 or estimateSe2 are about19 - namely,about this disposition:estimateSe1 is_about dSeg1,IT1,MestimateSe2 is_about dSeg2,IT2,MBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 8 of 13Also, if the samples s1 and s2 are considered by thestatistician as representative enough of a general popula-tion g0 encompassing g1 and g2, if RT1 and RT2 are consid-ered as similar enough to be representative in the sameway of the disease M, and if IT1 and IT2 are considered assimilar enough to be representative of a more generalindex test IT0, then:estimateSe1,2 is_about dSeg0,IT0,MAs dSeg,IT,M is an individual, it cannot be related directlyto the classes IT and M, but only indirectly, through thefollowing formalization. First, dSeg,IT,M can be seen as aninstance of a disposition class written DSeIT,M, which hasas trigger the process class TSeIT,M: performance of testIT on the members of a group, and random draw of aperson among those who have the disease M; and asrealization the process class RSeIT,M defined as drawing byTSeIT,M of someone who got a positive result to IT. Wecan then introduce two new relations sensitivity_dispositio-n_of_test and sensitivity_disposition_for (abreviated asse_of_test and se_for_disease) relating DSeIT,M with IT and M:dSeg,IT,M instance_of DSeIT,MDSeIT,M is_a DispositionDSeIT,M se_of_test ITDSeIT,M se_for_disease MThese two relations se_of_test and se_for_disease areintroduced for pragmatic reasons of facility of use: on afoundational level, DSeIT,M and M (resp. IT) could be re-lated through a complex array of relations and entitiesthat involve the relation has_trigger between DSeIT,M andTSeIT,M, as well as a sequence of relations between TSeIT,Mand M (resp. IT). Such an analysis would raise interest-ing theoretical questions, as instances of DSeIT,M can existeven if no instance of M or IT do exist - we thereforeface here issues similar to the ones addressed by [20]and [21].Figure 2 represents classes and particulars involved informalizing tests execution and results, sensitivity estimates,the disposition this estimate is about, and the real sensitivityvalue. Figure 3 represents the classes and particulars in-volved in formalizing aggregation of sensitivity estimatesinto a finer estimate. Specificity, PPV and NPV can be for-malized along similar lines, as data items about dispositionsrelated to tests and diseases through relations that could belabeled sp_of_test, sp_for_disease, ppv_of_test, ppv_for_-disease, npv_of_test, and npv_for_disease.Example of applicationAn example will now illustrate this formalization.McTaggart and colleagues [8] have performed a meta-analysis to determine the accuracy of point-of-care testsfor detecting albuminuria (lets call IT0 the class of suchindex tests), using as reference test a laboratory testalbumin-creatinine ratio-ACR (lets call RT0 the class ofsuch reference tests).They take into account ten studies in their article.Consider for example Lloyd et al. [22], which measuresthe accuracy of semiquantitative Clinitek® microalbuminurine dipstick with a cutoff value indicating albumineriaat 3.4 mg/mmol (lets call IT1 the class of such indextests), with a laboratory ACR test with the same cutoffvalue as a reference (lets call RT1 the class of such refer-ence tests). A sample s1 of 204 diabetic patients (labelledhere p1,1, p1,2,, p1,204) was considered. On each ofthose patients, one measurement of IT1 called a1,i,1 andone of RT1 called rt1,i,1 is performed. The 2x204 = 408processual entities are all part of a general tests executionprocess labelled tests_executions1,IT1,RT1, which leads aftercomputation to the informational entity estimateSe1 , givingthe proportion of measure pairs in which IT1 led to a posi-tive result among those in which RT1 led to a positiveresult. This proportion is 83.8 %, and therefore, thevalue f4(s1,IT1,RT1) of the informational entity estima-teSe1 is 0.838.Writing g the human population, we have s1 part_of g;also, RT1 is_a RT0 and IT1 is_a IT0. Therefore,f4(s1,IT1,RT1) provides an estimate of f2(g,IT0,RT0), whichis the sensitivity value of a point-of-care test in detecting al-buminuria in the general population. However, other stud-ies are pooled with this one by McTaggart and colleagues[8] to provide a better estimate of f2(g,IT0,RT0). All together,they lead to the value h(s1,IT1,RT1,,s10,IT10,RT10) whichprovides an estimate of the value of f2(g,IT0,RT0).Note that the ten studies taken into account in thismeta-analysis include different kinds of patients. Sevenstudies involve each a different sample of patients (letscall them s1, s2, ., s7) with diabetes mellitus, one ofthem (s7) involving young patients with type 1 diabetes.Two studies consider samples of patients (s8 and s9)with kidney disease, diabetes mellitus, or both. Finally,one study includes a sample (s10) of patients treated foradvanced chronic kidney disease in a renal outpatientclinic. Lets call g the human population, g1 the mem-bers of g who have diabetes mellitus, g2 the members ofg who have a kidney disease and g0 the members of gwho have either diabetes mellitus or a kidney disease(that is, g0 is the mereological sum of g1 and g2). All siare part of g, the human population. Thus, the meta-analysis made by McTaggart and colleagues [8] providesan estimation of f2(g,IT0,RT0) or f2(g0,IT0,RT0). If themeta-analysis had been performed on s1-s7 only, then itwould have provided an estimation of f2(g1,IT0,RT0); andif it had been performed on samples of patients withkidney disease only, then it would have provided an es-timation of f2(g2,IT0,RT0).Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 9 of 13Note also that various cutoff values can be used to de-fine the presence of albuminuria, varying between2.65 mg/mmol to 3.4 mg/mmol, and those values arechosen by the medical sub-community who is con-ducting the study (the same cutoff value is taken forboth IT0 and RT0 in each study). Therefore, the clas-ses IT0 and RT0, which mention detecting albumin-uria without specifying a cutoff value, are notscientifically defined: those classes are not universals,but rather collection of particulars [19] whose natureis partly social ([8] acknowledge this limitation intheir meta-analysis).Alternative meta-analysis could use a subset of thosestudies to estimate various sensitivities, for example thesensitivity f2(g1,IT1,RT1) of point-of-care test with areference of laboratory ACR test, with albuminuria de-fined as ACR greater than 3.4 mg/mmol, in the refer-ence class of patients with diabetes mellitus; or thesensitivity f2(g2,IT2,RT2) of point-of-care test, with areference of laboratory ACR test, with albuminuria de-fined as ACR greater than 2.65 mg/mmol, in the refer-ence class of patients with kidney disease; etc. A well-founded semantic representation of sensitivity shouldthus make clear what is the reference class, as well asthe class of index test and reference test.Discussion and conclusionsWe have thus provided a practically tractable formalizationof IPs in a realist ontology, which clearly dissociates IPs realvalues, their estimates and the related proportion measure-ments. It has defined the central entities that are concernedby an IP estimation in a way that is compliant withOBO Foundry. In particular, it addresses the difficultyof considering possible, non-actual conditions in a realistontology based on BFO by introducing dispositions.This model could then be extended in three directions.A first step would be to clarify the ontological status ofthe two following entities: sample sizes on one hand;and 95 % confidence interval for sensitivity and specifi-city values on the other hand. A second step would beto clarify the relations se_of_test and se_for_disease,which could be reduced to basic relations and entitiesalready accepted in the OBO Foundry. A third stepwould be to use this model in an ontology-based diag-nostic system that would compute positive predictivevalues or negative predictive values from the prevalence,sensitivity and specificity values. More generally, it couldbe articulated with medical Bayesian networks. As amatter of fact, the notion of medical test used here couldbe generalized to a very general notion of test consistingin inferring the presence of an entity on the basis of theknowledge of the presence of another entity; as such, itcould serve as a foundation for the integration of Bayes-ian reasoning into ontologies.This model could be used in two kinds of computerapplications targeted at two different kinds of audiences.First, clinicians could determine more easily which kindof sensitivity and specificity (or PPV and NPV) estimatesthey could use when diagnosing a disease for a given pa-tient, by having a clearer view of the subjects characteris-tics in each samples on which those IP estimates arebased. As a matter of fact, section 3.4 illustrates how anontological analysis can make explicit what are the indextest, the reference test and the sample associated with asensitivity estimation. Universal qualities that are instanti-ated by all members of the sample - such as having dia-betes mellitus, being a man, being more than 65 years old,etc. - would enable to determine what could be the refer-ence class g associated with a sensitivity estimate. This en-ables to determine, when applying some given IP values toa specific patient with given characteristics, whether thisapplication is warranted or not.Second, statisticians could determine more easilywhich kind of sensitivity estimates they could aggregatetogether. If several estimations of IPs are representedontologically according to the structure shown above,one could use this ontological structure to determinewhich estimations of IPs could be combined to obtain afiner estimate. First, one would have to find a group g0that would encompass the reference classes (such as g1and g2) associated with those studies. Second, one wouldhave to analyze whether there exists some general indextest class such as IT0 (resp. some general reference testclass such as RT0) which would subsume the variousindex tests classes such as IT1 and IT2 (resp. referencetests such as RT1 and RT2) that are used in those studies.Once those are found, one could use meta-analyticmethods to derive a value for f2(g0,IT0,RT0) from theother studies. Future work will aim at building an ontol-ogy of medical tests to facilitate finding such encompass-ing index and reference test classes.As it takes into account the dependence of IPs uponthe group of people considered, it has the potential tocontribute to the development of precision medicine[23] in context of learning health systems [24, 25], anemerging approach that takes into consideration patientscharacteristics and dispositions, including individualvariability in genes, to offer more personalized prevent-ive, diagnostic and therapeutic strategies.Endnotes1These will be abbreviated in the following as a testIT and the patient has M. Note that a test may aim atdiagnosing a disease, in which case it can be called indi-cator of diagnostic performance. However, it may alsoaim at evaluating the presence of a disorder, a patho-logical process [26], a predisposition to a disease, a sign,a symptom, or other various medically relevant entitiesBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 10 of 13(such as a glycemia higher than 1.26 g/l). Several testsresults can then be considered to draw a diagnosticconclusion for a disease. Therefore, in the general case,indicators of performance are indicators of assay perform-ance rather than indicators of diagnostic performance (wethank an anonymous reviewer for this suggestion of ter-minology). Also, a test does not need to be performed ona human  it can be performed on a non-human animal.In the following, we will consider tests aiming at diagnos-ing a disease on a human, but our considerations canbe straightforwardly adapted to tests aiming at evaluat-ing another medically relevant entity on a human ornon-human animal.2In practice, such a test is not perfect; thus, it could beanalyzed as a chain of two tests: one that detects therheumatoid factor on the basis of e.g., some chemical re-action, and another one that detects rheumatoid arthritison the basis of the presence of the rheumatoid factor.3More specifically, it should be interpreted as the ex-pected value of such a proportion  but we will ignorehere this additional subtlety.4The article will concentrate on the case of sensitivity,but it can be similarly adapted to other IPs.5Here again (see footnote 3), this should be interpretedas the expected value of such a proportion.6At least for all practical purposes: from a theoreticalpoint of view, every measurement can be wrong, evenpure observations.7If one assumes that the sample is representative ofthe target population, there should be no selectionbias (which occurs when proper randomization is notachieved). However, the sensitivity values that wouldbe obtained using two different samples could beslightly different since randomness at the selectionprocess will yield slightly different samples. That iswhy statisticians use confidence interval for character-izing sensitivity and specificity.8We might also speak of a sensitivity in a sample forthe function f2(s,IT,M), that is, the proportion of peoplewho are tested positive by IT among the diseased personin the sample s. But it might be confusing to speak ofboth the sensitivity in a target population and the sen-sitivity in a sample; and the first and the second argu-ments above may justify keeping the label sensitivityfor this proportion in a target population g  that is, forf2(g,IT,M).9Let us summarize. On one hand, f2(g,IT,M) is thevalue of a non-actual proportion (because the test IT isnot performed on all members of g), which cannot beknown with certainty, but only estimated. On the otherhand, both f4(s,IT,RT) and f2(s,IT,M) (see footnote 8) arevalues of actual proportions (because the tests IT and RTare performed on all members of s); and althoughf2(s,IT,M) cannot be known with certainty (because wecannot know with certainty who has the disease: we canonly use a reference test  at best the gold standard  todetermine who are those individuals), f4(s,IT,RT) can beknown with certainty for all practical purposes (becausewe can know with certainty who got a positive resultto RT).10We have created an ontology according the lines ofwhat is described below, built on OBI, called BIPO(Bayesian Indicator of Performance Ontology). It can befound at https://github.com/OpenLHS/BIPO. It contains24 classes, 12 object properties, 2 data properties and 42logical axioms.11We will not take a stance on whether Medical_testshould be interpreted as identical to OBI:Assay, as pro-posed by [27].12Note that in some cases, several pairs of tests will beperformed on a person. See e.g., Kimberger et al. (2007),which measures the accuracy of a temporal arterythermometer in detecting fever (defined as a temperaturegreater than 37.8 °C), with respect to a reference standardgiven by a bladder thermometer: four measurement pairsof temporal artery temperature and bladder temperatureare performed on each of the seventy patients of the sam-ple considered by the authors. To represent such a case,one can introduce for every human pi a sequence of fourreference tests rt1,i,1, rt1,i,2, rt1,i,3 and rt1,i,4 .and four indextests it1,i,1, it1,i,2, it1,i,3 and it1,i,4; but the formalization thatis described below remains similar.13See e.g., http://vassarstats.net/clin1.html for an ex-ample of webpage supporting this kind of computation.14As a reminder, not only the values of PPV and NPVbut also the values of sensitivity and specificity dependon the group under consideration (this is the spectrumeffect), and it is not the task of the ontologist to deter-mine which ones should be idealized as constant (for allpractical matters) across groups and which ones shouldbe considered as variable: the task of the ontologist is torepresent those values and the entities those values de-pend upon.15[15] assigned a probability to a triplet (d,T,R) ratherthan to a disposition d, because it had to take into ac-count dispositions that may have several classes of trig-gers or realizations (that is, multi-trigger and multi-trackdispositions [20]). However, in the present situation,dSeg,M is simple-trigger and simple-track: all its triggersare instances of TSeg , and all its realizations are instancesof RSeg,M. Therefore, the probability value assigned to(dSeg,M,TSeg ,RSeg,M) can be, for practical matters, assigned dir-ectly to dSeg,M.16Such dispositions should not be confused with otherdispositions in the medical domain. First, diseases havebeen formalized as dispositions by the Ontology forGeneral Medical Sciences (OGMS) [26]. Second, therecan be predispositions to diseases that could beBarton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 11 of 13formalized as disposition. However, the disposition todraw randomly, among the individuals of g who have M,someone who is tested positive by IT, exists independentlyof whether the disease (or a predisposition to this disease)is formalized or not as a disposition. Note also that thisdisposition inheres in a group of people, whereas a diseaseas a disposition (as formalized by OGMS), or a predispos-ition to a disease, inheres in a single person.17In general, we cannot determine in practice with cer-tainty which individuals of g have M, and which do not(see the discussion about gold standard tests above); butthe practical impossibility to realize this trigger does notpreclude to define this entity.18We could also introduce the entity real_sensitivity-g,IT,M instance of Data_item, as a sibling of estimateSe1 suchthat real_sensitivityg,IT,M has_specified_value f2(g,IT,M)(cf. [14], in which real_sensitivityg,IT,M was denotedseg,IT,M). However, the value f2(g,IT,M) assigned to such anentity will never be known with certainty. We could substi-tute to this value the best estimate of the sensitivity value,as was proposed in [14]; however, such a model could notrepresent in a single ontology various estimates of thesame sensitivity  whereas it is possible in the presentframework, which also makes unnecessary the introduc-tion of the informational entity real_sensitivityg,IT,M.19It is important to differentiate what a sensitivity esti-mate is about (namely a disposition) from how it hasbeen mathematically obtained (for example, by weight-ing different proportion measurements)  as explainedearlier, the latter will not be represented in the ontology,as various mathematical methods can be used.AbbreviationsGeneral abbreviations for indicators of performanceIP: (Bayesian) Indicators of performance; NPV: Negative predictive value;PPV: Positive predictive value; Prev: Prevalence; Se: Sensitivity; Sp: SpecificityOther general abbreviationsACR: Albumin-creatinine ratio; RF: Rheumatoid factorClasses and instances abbreviations for disposition-related entitiesdPrevg,M : Disposition (borne by the group g) that a person randomly drawnamong the individuals in g would have M; dSeg,IT,M: Disposition (borne by thegroup g) that a person randomly drawn among the individuals of g whohave M would have a positive result to IT; this is an instance of DSeIT,M; DSeIT,M: Asubclass of Sensitivity disposition such that DSeIT,M se_for_disease M and DSeIT,Mse_of_test IT; DSpIT,M: A subclass of Specificity disposition such that DSpIT,Msp_for_disease M and DSpIT,M sp_of_test IT; Tg: The process of drawing randomlya person in g; the triggers of dPrevg,M are instances of Tg; TSeg,IT,M: The process ofperforming test IT on the individuals in g, and then drawing randomly anindividual among those who have the disease M; the triggers of dSeg,IT,M areinstances of TSeg,IT,M; Rg,M: The process of drawing by Tg someone who has M;the realizations of dPrevg,M are instances of Rg,M; RSeg,IT,M: The process of drawingby TSeg,IT,M someone who got a positive result to IT; the realizations of dSeg,IT,Mare instances of RSeg,IT,M;Other classes abbreviationsIT / IT0 / IT1 / IT2: A subclass of Medical test which is an index test (test whoseindicator of performance is being estimated); M: A subclass of Disease; RT /RT0 / RT1 / RT2: A subclass of Medical test which is a reference testOther instances abbreviationsg / g0 / g1 / g2: An instance of Collection of humans which is a generalhuman population; pi / qj: An instance of Human; it1,i (resp. it2,j): An instanceof (index) Medical test performed on person pi (resp. qj); rt1,i (resp. rt2,j): Aninstance of (reference) Medical test performed on person pi (resp. qj); s / s1 /s2: An instance of Sample of humans;Functions abbreviationsf1(IT,M): Proportion of individuals who get a positive result to IT, amongindividuals who have M; f2(g,IT,M): Proportion, among members of g whohave M, of those who would get a positive result to IT if the test IT wasrealized on them; this is the real sensitivity value of IT for M;f3(g,IT,RT): Proportion, among members of g who would get a positive resultto RT if the test RT was realized on them, of those who would get a positiveresult to IT if the test IT was realized on them; f4(s,IT,RT): Proportion, amongmembers of sample s who had a positive result to RT, of those who got apositive result to IT; this is an estimate of the real sensitivity value of IT for M,performed on a sample s, with RT as a reference test; f2(g,IT,M): Proportion,among members of g who dont have M, of those who would get anegative result to IT if the test IT was realized on them; this is the realspecificity value of IT for M; f4(s,IT,RT): Proportion, among members of samples who had a negative result to RT, of those who got a negative result to IT;this is an estimate of the real specificity value of IT for M, performed on asample s, with RT as a reference test; h(s1,IT1,RT1,s2,IT2,RT2): Estimate of thesensitivity value obtained by aggregating the estimate on sample s1 and theestimate on sample s2AcknowledgementsWe would like to thank two anonymous reviewers for their comments thatled to significant improvements in our model and in the manuscript, as well asassistance during various presentations of this work for their suggestions. ABawould like to thank the bourse de fellowship of the department ofmedicine of Sherbrooke University for financial support. This manuscript isan extended version of work presented at ICBO (International Conferenceon Biomedical Ontology) 2015.Authors contributionsABa conceived the formalization of the real indicator of performance values,JFE and ABa conceived the formalization of the estimation of indicators ofperformances, and ABa and JFE developped the BIPO ontology, in light of inputsfrom ABu and RD. JFE and RD provided the medical examples supporting theformalization.ABa drafted the manuscript with important feedbacks from JFE, RD and ABu. Allauthors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Département de médecine, Université de Sherbrooke, Sherbrooke, Québec,Canada. 2INSERM UMR 1099, LSTI, Rennes, France. 3CHU de Martinique, UniversitéAntilles-Guyane, Fort-de-France, France. 4INSERM UMR_S 1138 Eq 22, Université ParisDescartes, Hôpital européen Georges Pompidou, AP-HP, Paris, France. 5Centre derecherche du CHUS, CIUSSS de lEstrie-CHUS, Sherbrooke, Québec, Canada.Received: 2 February 2016 Accepted: 6 September 2016Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 DOI 10.1186/s13326-017-0126-0SOFTWARE Open AccessDead simple OWL design patternsDavid Osumi-Sutherland1* , Melanie Courtot1, James P. Balhoff2 and Christopher Mungall3AbstractBackground: Bio-ontologies typically require multiple axes of classification to support the needs of their users.Development of such ontologies can only be made scalable and sustainable by the use of inference to automateclassification via consistent patterns of axiomatization. Many bio-ontologies originating in OBO or OWL follow thisapproach. These patterns need to be documented in a form that requires minimal expertise to understand and editand that can be validated and applied using any of the various programmatic approaches to working with OWLontologies.Results: Here we describe a system, Dead Simple OWL Design Patterns (DOS-DPs), which fulfills these requirements,illustrating the system with examples from the Gene Ontology.Conclusions: The rapid adoption of DOS-DPs by multiple ontology development projects illustrates both the ease-ofuse and the pressing need for the simple design pattern system we have developed.Keywords: OWL, OBO, Design patternBackgroundBiologists classify biological entities in many differentways. A single neuron may be classified by structure(pseudo-bipolar), electrophysiology (spiking), neurotrans-mitter (glutamatergic), sensorymodality (secondary olfac-tory neuron), location(s) within the brain (antennal lobeprojection neuron,mushroom body extrinsic neuron), etc.A transport process occurring in a cell may be classi-fied by the type of chemical transported, where transportstarts and ends, and by what membranes are crossed.Bio-ontologies provide a widely used method for doc-umenting such classifications and the relationships thatapply between members of classes, such as partonomy.These classifications and relationships are central to thesuccessful use of bio-ontologies in helping biologists makesense of the ever increasing volumes of data they workwith. They are critical to the use of the Gene Ontology(GO) [1] and its associated annotations in interpretinggenomic data via its application in enrichment analysis [2].They are critical to the functioning of Virtual Fly Brain ingrouping and querying neuroanatomical data [3].*Correspondence: davidos@ebi.ac.uk1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust GenomeCampus, CB10 1SD Cambridge, UKFull list of author information is available at the end of the articleTo be successful in this role, bio-ontologies need tocapture all of the many forms of classification that areimportant to biologists; but maintaining this manuallybecomes impractical as ontologies grow. Without formal-ization, the reasons for existing classifications are oftenopaque. The larger an ontology, the harder it is for humaneditors to find all valid classifications when adding a term,or to work out how to re-arrange the hierarchy when newintermediate classes are added.The alternative to manually asserting classification isto use OWL inference to automate it. OWL equivalenceaxioms can be used to specify necessary and sufficientconditions for class membership. Standard reasoning soft-ware can then build a class hierarchy by finding classesthat fulfill these conditions.Many bio-ontologies now follow this approach,including the Uber Anatomy Ontology (Uberon) [4], theGO [5], the Ontology of Biomedical Investigations (OBI)[6], the Drosophila Anatomy Ontology (DAO) [7], theCell Ontology (CL) [8] and the Ontology of BiologicalAttributes (Ontology of Biological Attributes (OBA) [9].In the GO, over 52% of the classification is automated.Much of this classification leverages the structure ofimported ontologies; for example, classification of trans-port processes in the GO relies on a classification ofchemicals provided by the chemical ontology ChEBI [10]© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 2 of 7and on object property axioms specified in the OBOrelations ontology.A critical requirement for ongoing development of theseontologies is the specification of design patterns to guidethe consistent OWL axiomatization required for auto-mated classification. In many of these ontologies, classesare annotated with textual descriptions that follow stan-dard patterns which also need to be documented. Whereformal, machine-readable design patterns are sufficientlydetailed, they can be used to quickly generate new classes,update old ones when a pattern changes, and automati-cally generate user-facing documentation.OWL design pattern systemsThere is an extensive literature on ontology design pat-terns in OWL [11, 12]. Much of this is based on anapproach known as Content Ontology Design Patterns(CODPs; see [12]) for an overview). CODPs are small,autonomous ontologies that specify multiple classes andproperties. CODPs are typically re-used by one of twomethods. Either the pattern is imported and new sub-classes and sub-properties of pattern entities are instanti-ated in the target ontology, or it is used as a template, withentities in the pattern being given new identifiers in thenamespace of the target ontology.The GO and several other ontologies including CL andOBA already use standard patterns to generate new classterms via the TermGenie tool [13]. In GO, around 80% ofnew class terms are added via this route. This tool allowsnew terms to be added by specifying a desgin patternand a set of fillers for variable slots. Unlike CODPs, thesedesign patterns are not autonomous: they import classesand object properties from various ontologies. Thismeansthat their semantics are dependent on those of the ontolo-gies they import from. This is by design: the patternsare intended to leverage classification and axiomatiza-tion from external ontologies to drive classification in thetarget ontology.Design patterns in TermGenie are specified directly inJavascript. This specification is opaque to most humaneditors and is not easily reusable outside the context ofTermGenie. The other major mechanisms for specifyingdesign patterns for programmatic use are the languagesTawny OWL [14] and Ontology PreProcessing Language(OPPL) [15]. These are very powerful tools for gener-ating and manipulating ontologies, but are not easy forontology editors without strong technical backgroundsto write. They are also tied to specific languages andimplementations, limiting their use.Many editors of bio-ontologies are biologists withlimited computational expertise beyond a basic under-standing of some subset of OWL (typically limited to thesubset of OWL that can be encoded in OBO 1.4 [16]),which they interact with via Manchester Syntax renderingand graphs in graphical editing tools such such as Pro-tégé [17]. A simple, lightweight standard for specifyingdesign patterns is needed in order to make their devel-opment and use accessible to these editors. This standardshould be readable and editable by anyone with a basicknowledge of OWL. It must also be easy to use pro-grammatically without the need for custom parsers  i.e.it should follow some existing data exchange standardthat can be consumed by any modern programming lan-guage. Based on these requirements, we have defineda lightweight, YAML Aint Markup Language (YAML)-based syntax for specifying design patterns, called DeadSimple OWL Design Patterns, or DOS-DPs (inversion oftwo letters is an homage to the Web Ontology Language,OWL, on which it is based).ImplementationWe have developed a formal specification of DOS-DPsusing JSON-schema [18] draft 4 for use in validation anddocumentation. This is available from the DOS-DP repos-itory [19], which also lists recommendations for additionalvalidation steps. Description fields in the schema doc-ument intended usage. Where appropriate, the schemadocument also includes fields that document mappingsto relevant OWL entities. We use the Python jsonschemapackage to validate the schema and test it against exam-ple patterns. Table 1 contains a summary of schema fieldtypes and how they are used.ApproachDOS-DPs are designed to be easy to read, edit and parse.We chose YAML because it is relatively easy to read andwrite compared to other common data exchange formatssuch as JSON and XML, and can be consumed by a widerange of programming languages. In order to take advan-tage of JSON-Schema for specification and validation,DOS-DPs are restricted to the JSON compatible subset ofYAML [20].Each design pattern can have an arbitrary number ofvariables. For ease of reading, writing and parsing, vari-able interpolation uses printf, a standard part of mostmodern programming languages.OWL is expressed using Manchester Syntax [21], themost human-readable and editable of the OWL syntaxes,and the one most editors with a basic knowledge ofOWL are likely to have encountered. For ease of read-ing and editing, quoted, human-readable identifiers areused for OWL entities throughout the pattern. These areassumed to be sufficient to uniquely identify any OWLentity within a pattern. Dictionaries are used to map read-able identifiers to compact URIs (CURIEs)  prefixedshort form identifiers. A JSON-LD context is used to mapthese to full IRIs. The entity IRIs recorded in this waycan be used to check reference ontologies to find theOsumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 3 of 7Table 1 DOSDP JSON schema fieldsField type Used to Mandatory subfields Optional subfields Used inPrintf_owl Specify a logical OWL axiomusing printf to substitutevariable valuesaxiom_type, text, vars annotations logical axiomsPrintf_annotation Specify an annotation usingprintf to substitute variablevaluesannotationProperty, text,varsannotations annotationsList annotation Specify a list of annotationproperty axioms of a singletype using a list of valuesspecified by a data listvariableannotationProperty, value - annotationsPrintf_owl_convenience Specify a logical OWL axiomof a prespecified type, usingprintf to substitute variablevalues.text, vars annotations equivalentTo, subClassOf,disjointWith, GCIPrintf annotation obo Specify an annotationaxiom of a prespecifiedtype using a list of valuesspecified by a data listvariabletext, vars annotations, xrefs def, name, comment,List_annotation_obo Specify a list of annotationproperty axioms of asingle type, pre-specifiedtype. using a list of valuesspecified by a data listvariablevalue - xrefs, exact_synonyms, . . .Field type: Name of schema field type (JSON schema definition). Used to: Description of field usage. Used in: Schema Fields in which this field type is usedcurrent validity and status of all entities referenced ina pattern.While the full specification of DOS-DPs is intended tobe generic and expressive, a major aim is to hide com-plexity from editors wherever possible. To this end, wedefine convenience fields that are suitable for use in com-mon, simple design patterns. We also allow extensionsthat import and extend the core JSON schema and thatspecify default values for high level fields. For example, wedefine an extension to support the OBO standard. Thisdefines convenience fields for expressing OBO standardannotations and specifies a default annotation propertyfor readable identifiers and an OBO standard base URIpattern.Figure 1 shows an example design pattern for gener-ating classes of transport across a membrane defined bycargo type and membrane type. Figure 1a shows a patternfollowing the OBO extension. Figure 1b shows the samepattern expressed using the more verbose DOSDP core-specification. Figure 2 shows an example class generatedusing this pattern.DetailsPatternmetadataEach pattern is identified by an IRI. The short formof this IRI is recorded in a pattern_name field, and,by convention, is used for the file name. Each patternoptionally includes an extension specification, indicat-ing the extension to be used in interpreting the patterndocument. In 1a this is set to OBO.DictionariesIn both versions of the pattern, the fields classes andrelations serve as dictionaries for the OWL classes andobject properties respectively used in the pattern, map-ping human readable identifiers (keys) to short_formidentifiers (values). The core pattern specifies an anno-tation property to use as a source of readable identifiersvia the readable_identifier field. This is not required inthe OBO extension version, as the extension specifies adefault value of rdfs:label for this. The full patternalso contains an additional dictionary of OWL annotationproperties. These are not required in the OBO extension,which specifies dedicated fields for annotation propertiesused in the OBO standard. The core DOSDP specificationalso defines a dictionary field for OWL data properties.Input fieldsAll patterns contain one or more variable specificationfields. These are simple objects in which the keys are vari-able names and the values specify variable range. Thevars field specifies variables that range over OWL classes,specified as Manchester syntax expressions. For example,the value of the cargo variable in Fig. 1 is specified byOsumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 4 of 7Fig. 1 DOS-DP for defining classes of transmembrane import (basedon an example from the GO.) Panel A shows the DOS-DP using theOBO extension. Panel B shows the same pattern expressed using thecore specification (classes, relations and vars fields omitted from panelB for brevity). In Panel A, annotations are specified using dedicatedfields (def, name, xrefs). The mapping from these to OWLannotation properties is specified in the OBO extension schema. Thismapping is made explicit in Panel B, using an annotation_propertydictionary and the annotationProperty field in axiom specificationsunder annotations. Throughout both versions of the pattern, pairedfields text and vars specify printf text and fillers respectively. Thevalue field is used with the data_list_var def_xrefs to specify a listdatabase_cross_reference annotations on the definitionthe class expression: chemical entity or transcript. Thequoted OWL entity names in this expression are spec-ified in the dictionaries. Both patterns also include anexample of a variable that takes a data type as an input.The data_list_vars field specifies variables whose valuesare lists in which all elements share an OWL data type,specified in the value of the variable field. For exampledef_dbxref in Fig. 1 is specified to be a list of (XSD) strings.Output fieldsThe core schema has just two output fields: annotationsfor annotation property axioms and logical_axioms forlogical owl axioms. The value of both of these fields is a listof axiom specifications. Each axiom specification includesa specification of axiom type (logical type or annotationproperty). Content is either specified using printf sub-stitution of variable values into a text string (field typeprintf_annotation or printf_owl in Table 1 or by specify-ing a list of values to be used to generate multiple axiomsFig. 2 Example pattern implementation. An example of a term,leucine transport across the plasma membrane, generated using thepattern in Fig. 1. Note the automated classification under amino acidtransport across the plasma membrane, specified using the samepatternof the same type (e.g. field type list_annotation in Table 1.Where OWL entities (specified as vars) are used to specifyPrintf substitution, the readable label of the entity isused. Axiom specifications can also be used to specifyannotations of the specified axiom.In our example, the annotations field is used to specifyan rdfs:label axiom and a definition axiom. In bothcases a text output is specified using a text field to specifya printf statement and a vars field to specify an orderedlist of fillers. The definition axiom specification specifies aset of axiom annotations using a database_cross_referenceannotation property. These axioms will be generatedusing a list of strings provided in the data_list_vardef_dbxref. The results can be seen in Fig. 2.The OBO version (1) encodes the same informationusing named fields: name, def, and xrefs. These fields fol-low the tag names used in OBO format [16]. The fieldspecifications (in the OBO JSON schema doc) map thesefields to the relevant OWL annotation properties, remov-ing the need for ontology pattern developers to specifythese mappings in an annotation property dictionary.The logical_axioms field in Fig. 1b specifies just oneequivalence axiom. This is a very common pattern fordefining classes. To make specifying this type of pat-tern easier, we define convenience fields that can be usedwhenever there is only one axiom of a given type perpattern. The pattern in 1a uses the convenience field forequivalentTo to concisely capture the single logical axiomin this pattern.Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 5 of 7DiscussionLimitationsDOS-DPs are designed to be simple and clear. There area number of obvious ways that they could be made morepowerful but which we have avoided in order to retainsimplicity and clarity.By design, DOS-DPs lack a mechanism for relating pat-terns to each other via inheritance or composition. Suchmechanisms would add a technical burden to their, userequiring additional tooling, and so be a barrier to theiradoption. Manual maintenance of design pattern hierar-chies also risks re-creating the maintenance problem thatthese patterns are meant to solve.For the sake of simplicity, DOS-DPs also lack a sys-tem for specifying optional clauses. This places someburden on the development of patterns that naturallyform a subsumption hierarchy. However, the relationshipsbetween patterns can easily be derived by generating aset of OWL classes using default fillers (variable ranges)and classifying the results using a reasoner. This clas-sification can then be used as a way of testing sets ofDOS-DPs and to generate a browsable hierarchy of relatedpatterns.AdoptionDOS-DPs are used both as formal documentation, andas part of the ontology-engineering pipelines in the GO,OBA, the Environmental Ontology (ENVO) [22], thePlant Trait Ontology [23], the Plant Stress and DiseaseOntology [24], the Agriculture Ontology, and the Envi-ronmental Conditions and Exposures Ontology [25]; thecentral DOS-DP GitHub repo has a list of all adopters. SeeFigs. 1 and 2 for an example of a pattern used extensivelyin the GO.One heavy user of (OPPL) patterns is Webulous, anapplication that allows specification of OWL classes usingtemplates loaded into Google spreadsheets. It should bestraightforward to develop a version of Webulous thatsupports design patterns specified as DOS-DPs, removingthe need for expertise in OPPL to specify new patterns.Similarly, it should be possible to extend Tawny-OWL tosupport DOS-DPs. This could prove to be a very effec-tive combination of accessible design pattern specificationwith a computationally powerful language for writing andmanipulating OWL ontologies.Patterns inevitably evolve as use-cases evolve. Changingall uses of an existing pattern by hand is impracticalunless the number of uses is relatively low. For branchesof ontologies where all terms follow a completely stereo-typed pattern, we can specify whole branches simply byspecifying a DOS-DP together with a URI and set ofvariable fillers for each term. We plan to use this to pro-grammatically generate suitable branches of the GO ateach release.Where more flexibility is required, DOS-DPs could beused to update existing terms that are part of a human-edited ontology file. A system of tagging terms by thepattern they implement would allow all relevant terms tobe identified. DOSDP-scala [26] can be used to identifyexisting classes within an ontology that follow a speci-fied pattern, returning the fillers populating each vari-able in the pattern. If an ontology pattern changes thenDOSDP-scala can also be used to test whether taggedterms conform to the old pattern, flagging those that dofor automated update and those that do not for manualinspection.ConclusionsAs can be seen from Fig. 1, which shows a pattern fordefining terms in the GO, DOS-DPs are easy to read andwrite. The choice of YAML limits the need for balancingbrackets and commas. The use of printf, Manchestersyntax, and labels for OWL entities makes the pattern easyto read. Figure 2, which shows an application of the pat-tern specified in Fig. 1, illustrates how similar the patternis to the way human editors interact with ontology classesin a GUI editor like Protégé [17]. As well as ease of readingand writing, our other aim is language independence. Cur-rently there are partial (OBO-specific) implementationsin Python [27] and Jython [28, 29], along with the Scala-based pattern matcher [26]. TermGenie is being extendedto consume DOS-DPs. These implementations cover pat-tern validation and the addition of new classes. They alsoallow for generation of markdown format documentationfrom design patterns.Availability and requirementsProject name:Dead Simple OWLDesign Patterns (DOS-DP). The specification and recommendations for valida-tion are available from [29] under the GNUGeneral PublicLicense v3.0.Programming language and requirements: The schemais specified using JSON-schema [18]. This specificationcan be consumed by any language for which a schemachecker exists (see [18]).AbbreviationsChEBI: Chemical entities of biological interest; CL: Cell ontology; CODP:content ontology design pattern; CURIE: Compact URI; DOS-DP: Dead simpleOWL design pattern; GO: Gene ontology; GUI: Graphical user interface; IRI:Internationalized resource identifier; JSON: JavaScript object notation; OBA:Ontology of biological attributes; OBO: Open biomedical ontologies; OPPL:Ontology preprocessing language; OWL: Web ontology language; XML:Extensible markup language; XSD: XML schema description; YAML: YAML aintmarkup languageAcknowledgementsWe sincerely thank Helen Parkinson, the Gene Ontology Consortium and theSample, Phenotypes and Ontologies Group at the EBI for providing a fertileintellectual environment for the development of DOS-DPs.Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 6 of 7FundingThis work was funded by the Gene Ontology Consortium P41 grant from theNational HumanGenome Research Institute (NHGRI) [grant 5U41HG002273- 14]and by BD2K [grant U01 HG009453]. CJM acknowledges the support of theDirector, Office of Science, Office of Basic Energy Sciences, of the USDepartment of Energy (DE-AC02- 05CH11231). MC was funded by the GeneOntology Consortium P41 grant from the National Human Genome ResearchInstitute (NHGRI) [grant 5U41HG002273- 14] and EMBL-EBI core funds.Availability of data andmaterialsThe DOS-DP specification and recommendations for validation are availablefrom [29].Authors contributionsDOS-DPs were developed by DOS in with suggestions from CJM and JPB.Many of the uses of DOS-DPs described here were implemented by CJM. MCsupervised some of this work and contributed to drafting this paper. JPBdeveloped DOSDP-scala. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust GenomeCampus, CB10 1SD Cambridge, UK. 2RTI International, 27709 Research TrianglePark, NC, USA. 3Genomics Division, Lawrence Berkeley National Laboratory,94720 Berkeley, CA, USA.Received: 23 November 2016 Accepted: 29 March 2017Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 DOI 10.1186/s13326-017-0157-6RESEARCH Open AccessEntity recognition in the biomedicaldomain using a hybrid approachMarco Basaldella1, Lenz Furrer2, Carlo Tasso1 and Fabio Rinaldi2*AbstractBackground: This article describes a high-recall, high-precision approach for the extraction of biomedical entitiesfrom scientific articles.Method: The approach uses a two-stage pipeline, combining a dictionary-based entity recognizer with amachine-learning classifier. First, the OGER entity recognizer, which has a bias towards high recall, annotates the termsthat appear in selected domain ontologies. Subsequently, the Distiller framework uses this information as a feature fora machine learning algorithm to select the relevant entities only. For this step, we compare two different supervisedmachine-learning algorithms: Conditional Random Fields and Neural Networks.Results: In an in-domain evaluation using the CRAFT corpus, we test the performance of the combined systemswhen recognizing chemicals, cell types, cellular components, biological processes, molecular functions, organisms,proteins, and biological sequences. Our best system combines dictionary-based candidate generation withNeural-Network-based filtering. It achieves an overall precision of 86% at a recall of 60% on the named entityrecognition task, and a precision of 51% at a recall of 49% on the concept recognition task.Conclusion: These results are to our knowledge the best reported so far in this particular task.Keywords: Named entity recognition, Text mining, Machine learning, Natural language processingBackgroundThe scientific community in the biomedical domain is avibrant community, producing a large amount of scientificfindings in the form of data, publications, reports, and soon, each year, making it difficult for scholars to find theright information in this large sea of knowledge.To tackle this problem, researchers have developed dif-ferent text mining techniques with the goal of detect-ing the relevant information for the intended purpose.This papers focus is the technique called Named EntityRecognition (herein NER), which solves the problem ofdetecting terms belonging to a limited set of predefinedentity types.NER can be performed on both generic documents,to recognize concepts like person, date or location, oron technical documents, to recognize concepts like cells,*Correspondence: rinaldi@cl.uzh.chEqual contributors2University of Zurich, Institute of Computational Linguistics and Swiss Instituteof Bioinformatics, Andreasstrasse 15, CH-8050 Zürich, SwitzerlandFull list of author information is available at the end of the articlediseases or proteins. NER can be used by itself, with thegoal of recognizing themere presence of a term in a certainportion of the text, or as a preliminary stage for ConceptRecognition (CR), also known as Entity Linking or Nor-malization, where the term is not only recognized but alsolinked to a terminological resource, such as an ontology,through the use of a unique identifier [1].NER can be solved using several techniques: Using manual, hand-written rules. A group of expertsdevelops these rules using domain knowledge. Therules typically rely on orthographic patterns, such asparticular use of capitalization or punctuation. Eventhough rule-based systems can perform well ifsufficient expert time is available for creating therules, their maintenance requires repeated manualefforts, since the rules need to be revised or evenentirely replaced whenever the system is adapted tonew data (different entity types, another textcollection). In the biomedical domain, plain rule-based approaches like [2] have become rare; however,© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 2 of 14they continue to be used in combination with othertechniques, such as dictionary look-up [3, 4]. Using dictionaries to recognize known entities. Anautomatic process looks for all the possible entities(possibly, all the words in a document) in one ormore dictionaries (or ontologies, or databases, orgazetteers) of known entities. This method has theobvious drawback that new entities cannot berecognized, because they are not yet present in thedictionary. Pafilis et al. [5] and [6] use adictionary-based approach for NER of species andchemicals, respectively. Using machine learning techniques. A machinelearning method, like Support Vector Machines orConditional Random Fields (herein CRF), can betrained to recognize entities in a fragment of textusing a large set of pre-annotated documents asexamples. If trained properly, a machine learningmodel can potentially recognize entities that are notyet inserted in dictionaries or ontologies. Thedrawback of this approach is the fact that trainingmaterial is not always available for a certain domain,or if present, it may be unsatisfactory in terms ofquality or size. Examples for a CRF-based approachare presented in [7, 8]. Using a hybrid approach. Two or more of thepreviously mentioned approaches are used togetherto combine their strengths and, hopefully, overcometheir weaknesses. For example, [9] and [10]successfully use a hybrid dictionary-machine learningapproach.This paper presents an extension of the hybrid solu-tion introduced in [11]. In that paper, we present a hybriddictionary-machine learning approach, where the dictio-nary stage, performed by OntoGenes Entity Recognizer(OGER) [12, 13], generates a high recall, low precision setof all the possible entities that can be found in a docu-ment and then the machine learning stage, performed byDistiller [14], filters these entities trying to select only therelevant ones.The aim of this work is to improve the systempresented in [11] by exploring new techniques bothfor the dictionary and the machine-learning stage, inparticular by replacing the original machine learn-ing approach with one based on CRFs. We presentthese techniques, analyzing the new methods we intro-duced, and we evaluate them on the CRAFT corpus[15], a set of documents from the biomedical domainwhere the relevant concepts have been linked to sev-eral ontologies. Then, we compare the results obtainedwith the ones found in the literature, exploring thepotential of using the system as a concept recognitionpipeline.MethodsCRAFT corpusThe Colorado Richly Annotated Full Text (CRAFT) cor-pus is a set of articles from the PubMed Central OpenAccess Subset [16], a part of the PubMed Central archivelicensed under Creative Commons licenses, annotatedwith concepts pointing to several ontologies.The corpus is composed of 67 annotated articles avail-able in the public domain, plus 30 articles that have beenannotated but are reserved for future competitions andhave to date not been released.The ontologies used in the corpus are: ChEBI: Chemical entities of Biological Interest [17],containing chemical names CL: Cell Ontology [18], containing cell type names Entrez Gene [19], containing gene names GO: Gene Ontology [20]. CRAFT provides twosub-ontologies, one for physical entities (cellularcomponents, CC) and one for non-physical entities(biological processes andmolecular functions,BPMF). NCBI Taxonomy: the US National Center forBiotechnology Information Taxonomy [21],containing names of species and other taxonomicranks PR: Protein Ontology [22], containing protein names SO: Sequence Ontology [23], containing names ofbiological sequence features and attributesIn total, the available articles are annotated with over100,000 concepts. Moreover, each of the 67 articles con-tains linguistic information, such as tokenized sentences,part-of-speech information, and dependency parse trees.For our experiments, we used all terminology resourcesexcept for NCBI Entrez Gene. We decided to omit EntrezGene from the evaluation against CRAFT for a numberof reasons. For one, the distribution of the CRAFT cor-pus does not include a reference version (unlike all otherterminologies); this means that we would have to usean up-to-date version of Entrez Gene, which potentiallydiffers significantly from the version used in the anno-tation process. Secondly, Entrez Gene contains a largenumber of terms that overlap with frequent words ofthe general vocabulary (such as was, and, this), tak-ing care of which requires considerable additional effort,such as manually creating blacklists. Furthermore, omit-ting Entrez Gene has been suggested earlier by otherscholars (e.g. ([24], p. 8)).We associated each (sub-)ontology with a single entitytype. For NCBI Taxonomy, we regarded species andhigher taxonomic ranks (genus, order, phylum etc.) fromboth cellular organisms and viruses to a common entitytype organism. For the Gene Ontology, we followedBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 3 of 14CRAFTs division into physical and non-physical entities,i.e. we distinguished cellular components from biologi-cal processes/molecular functions.OGERThe OntoGene group has developed an approach forbiomedical entity recognition based on dictionary lookupand flexible matching. Their approach has been used inseveral competitive evaluations of biomedical text miningtechnologies, often obtaining top-ranked results [2528].Recently, the core parts of the pipeline have been imple-mented in a more efficient framework using Python [29]and are now developed under the name OGER (Onto-Genes Entity Recognizer). These improvements showedto be effective in the BioCreative V.5 shared task [30]: inthe technical interoperability and performance of annota-tion servers (TIPS) task, our system achieved best resultsin four out of six evaluation metrics [31]. In the TIPS task,participants were asked to provide an on-line service foron-the-fly annotation of biomedical entities in given doc-uments. The tasks goal was to investigate the feasibilityof installing an inter-institutional annotation cluster (con-trolled by a biomedical annotation metaserver), thereforethe evaluation was based on processing time and avail-ability of the participating systems [32]. OGER achievedsingle first place in the speed measures (average responsetime, mean time per document volume) and shared firstplace in the stability measures (mean time between fail-ures, mean time to repair).OGER offers a flexible interface for performingdictionary-based NER. It accepts a range of input formats,e.g. PubMed Central full-text XML, gzip-compressedchunks of Medline abstracts as made available for down-load by PubMed, BioC XML [33], or simply plain text.It provides the annotated terms along with the corre-sponding identifiers either in a simple tab-separated textfile, in brats standoff format [34], in BioC XML, or ina number of other, less common formats. It allows foreasily plugging in additional components, such as alterna-tive NLP preprocessing methods or postfiltering routines.We run an instance of OGER as a permanent web ser-vice which is accessible through an API and a web userinterface [35].For term matching, we used the terminology resourcesincluded in the CRAFT corpus. We extracted the relevantinformation from the various sources and converted itinto a unified, non-hierarchical format, in order for it to beaccepted by the annotation pipeline. For the format con-version, we used the back-end software of the Bio TermHub [36], which is a meta-resource for biomedical ter-minological resources. Through a web interface [37], anyuser can obtain a customized dictionary, which is com-piled on the fly from a number of curated, openly availableterminology databases.By concatenating the selected seven terminologies, weobtained a dictionary with 1.26 million terms pointing to864,000 concept identifiers. Based on preliminary tests,we removed all entries with terms shorter than three char-acters or terms consisting of digits only; this reduced thenumber of entries by less than 0.1%. In OGER, the entriesof the term dictionary were then preprocessed in the sameway as the documents with respect to tokenization, stem-ming, and case sensitivity, as described below. Finally, theinput documents were compared to the dictionary with anexact-match strategy.OGER was configured to have a moderate bias towardsrecall, at the cost of precision. We chose this strategy, tai-lored to a greater number of false positives (i.e. lower pre-cision) but less false negatives (i.e. greater recall), becausein the overall architecture OGERs output is filtered orused as a feature among many in the subsequent step,so we let the subsequent ML step decide which of theannotations produced by the dictionary step are actuallyuseful.After sentence splitting, the input documents were tok-enized with a simple method based on character class:any contiguous sequence of either alphabetical or numer-ical characters was considered a token, whereas any othercharacters (punctuation and whitespace) were consideredtoken boundaries and were ignored during the dictionarylook-up. This lossy tokenization already has a normal-izing effect, in that it collapses spelling variants whicharise from inconsistent use of punctuation symbols. Forexample, the variants SRC 1, SRC-1, and SRC1 wereall conflated to the two-token sequence SRC, 1. Ina small evaluation on the training set, we verified thatthis results in a moderate improvement of overall recall(+ 2.7 percentage points) and worked particularly well forsequences (+ 3.5) and proteins (+ 8.4), while the effecton precision was negative, but smaller (? 2.1). A simi-lar approach is described in [38], where the authors referto it as regularization. All tokens were then convertedto lowercase, except for acronyms that collide with aword from general language (e.g. WAS). We enforceda case-sensitive match in these cases by using a list ofthe most frequent English words. As a further normal-ization step, Greek letters were expanded to their lettername in Latin spelling, e.g. ? ? alpha. Since bothspellings are common in the biomedical literature, con-verting all occurrences to a canonical form allowed usto increase the number of matches. Finally, we appliedstemming to all tokens except for the acronyms, usingNLTKs [39] implementation of the Lancaster stemmer[40]. We favored this algorithm over the more widely usedPorter stemmer because of its greater strength, i.e. itshigher amount of conflations produced, which increasesthe overlap between the dictionary and the documents, inline with our aim for higher recall.Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 4 of 14As a tweaking step, we fine-tuned the above default con-figuration for the individual entity types. Based on theirrespective coverage in the training set, we adjusted theparameters as follows:proteins no stemmingsequences less strict acronym filter (more cases of case-insensitive matching)cells always case-insensitive matching (even foracronyms)cellular components always case-insensitive matchingDistillerThe Distiller framework [41] is an open source projectwhich aims to build a flexible, extensible system for avariety of natural language processing tasks [14].The main focus of the Distiller framework is the taskof Automatic Keyphrase Extraction (herein AKE), whichis the process of extracting relevant phrases from a docu-ment [42]. AKE is quite different from NER, as while theformer is interested in finding the small set of the most rel-evant terms in a document, the latter is focused on findingall the terms of some selected types.AKE can be performed as both an unsupervised andsupervised task, and Distiller actually has its roots in anunsupervised approach [43]. However, current state-of-the-art systems use mostly a supervised approach [44],so the framework offers the possibility to use such tech-niques as well.Supervised AKE is performed using a standard super-vised machine-learning pipeline. The first step is generat-ing the candidate keyphrases, using their part-of-speechtags to select certain phrasal patterns. Then, the candidatekeyphrases are assigned some features, using statistical[42], linguistic [45], or semantic [46] knowledge. Finally, amachine learning algorithm is trained and then evaluatedover a set of documents associated with human-assignedkeyphrases.Ensemble systemIn order to integrate Distiller with OGER together andbuild an effective NER system, the candidate generationphase of the former system has been replaced by OGERsoutput. In fact, the original candidate generation phaseof the Distiller has to be completely discarded, because itis tailored to recognizing generic noun phrases, whichmight not even be technical terms.For this reason, in this work we follow and extend thesame process we presented in [11], so the entity extractionpipeline is structured as follows:1. Given an input document, OGER matches all thebiomedical terms that appear in at least one of theselected ontologies;2. Distiller receives the terms selected by OGER andassigns them some features, preparing them to beprocessed by a machine learning system;3. A machine learning system, trained on the CRAFTcorpus, selects the relevant entities in the documentusing the information generated in the previous steps.The machine learning algorithms used are neural net-works (NN), as they were the best performing algorithmin [11], and Conditional Random Fields (CRF), as theyare currently considered the state-of-the-art algorithm,as pointed out in [24]. The architecture is slightly dif-ferent for the two algorithms: In the NN case, Distilleracts as a filter on OGERs output, i.e. it performs abinary accept/reject classification for each entity candi-date. In contrast, the CRF-based version considers anytoken sequence in the text as an entity candidate, usingOGERs annotations only as a feature amongmany. Hence,the output of the NN pipeline is always a subset of OGERsoutput, whereas this restriction does not hold for the CRFpipeline.For both algorithms, training is performed using 10-foldcross validation. As in [11], in the present paper we splitthe corpus for training/testing purposes, so the evalua-tion is performed on 20 documents only. However, thereare two crucial differences from [11]. Fist, in our previ-ous work we trained a binary classifier, i.e. a single modelto detect all entity types, while here we train a separatemodel for each entity type present in the CRAFT cor-pus. Second, here we evaluate our system not only on thenamed entity recognition task, i.e. considering the spansand entity types produced by our ensemble system, butwe also evaluate our system on concept recognition, i.e.taking into account the concept identifiers produced byOGER.FeaturesDue to the differences of the algorithms, we used slightlydifferent features to train NNs and CRFs. In fact, themain difference between neural networks and condi-tional random fields features is that the former workswell with both n-ary features and continuous-valued fea-tures, while the latter works better when using n-aryfeatures (labels) only. For this reason, some featuresare implemented as continuous valued in NN and asbinary labels in CRF, to adapt them to the algorithmused. For example, while with NN we used a counterto determine how many uppercase characters are con-tained in a term, the corresponding CRF feature wouldbe a binary label indicating the presence of uppercasecharacters in the token. In any case, since the libraryused to train CRF supports the use of numerical valuedfeatures as well, we also tried to use the exact same fea-tures used in NNs training for the CRFs training, but theBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 5 of 14resulting performance was lower than using just binarylabels.The features and the configuration used to train ouralgorithms are listed in Table 1. The features we selectedare derived from [11] where, after a process of featureselection, the best performing feature set used informa-tion about the shape of the token, i.e. the number of capitalletters, the number of uppercase characters, and so on,plus some domain knowledge, i.e. features about the affixesof the words and the presence of Greek letters insidethem.In detail, affixes (i.e. prefixes and suffixes) are partic-ularly useful in the biomedical domain because they areoften associated with a particular meaning. For example,chemical compounds often end with -ide, like sodiumchloride (the common table salt), diseases often end with-itis or -pathy (like arthritis or cardiopathy), andso on.In order to implement this feature, we used the Bio TermHub resource [36], and we generated four affixes lists, oneeach for two- and three-character prefixes and suffixesappearing in the following ontologies: Cellosaurus [47], developed by the Swiss Institute ofBioinformatics; Chemical compounds and diseases found in theComparative Toxicogenomics Database (CTD) [48],developed by the North Carolina State University; Entrez Gene [19], developed by the US NationalCenter for Biotechnology Information; Medical Subject Headings (MeSH) [49], developedalso by the US National Center for BiotechnologyInformation (restricted to the subtrees organisms,diseases, and chemicals and drugs); reviewed records from the Universal ProteinResource (Swiss-Prot) [50], developed by the jointUSA-EU-Switzerland consortium UniProt.Table 1 Feature sets: features used by the NN and CRF (see the Features section for details)Neural network Conditional random fieldsImplementationSoftware R [67], nnet library CRFSuite [68]Model parameters 1 hidden layer of size2 × (number of input features), softmax out-put layerTraining algorithm: averagedperceptron, default epsilon, 2 words windowInput n-grams selected by OGER Single tokensFeaturesCandidate character count Count Candidate is all uppercase Label yes/no Label yes/noCandidate is all lowercase Label yes/no Label yes/noCandidate contains Greek (i.e. alpha, ? ) Label yes/no Label yes/noCandidate contains dashes (-) Count Label yes/noCandidate contains numbers Count Label yes/noCandidate ends with a number Label yes/no Label yes/noCandidate contains capital letter not in first position Label yes/no Label yes/noCandidate contains lowercase characters Count Label yes/noCandidate contains uppercase characters Count Label yes/noCandidate contains spaces Count Label yes/noCandidate contains symbols Count Label yes/no2-3 character affixes appearing in an ontology in [36] Normalized frequency Label yes/noCandidate is symbol  Label yes/noCandidates part-of-speech  Yes, using [69]Candidates stem  Yes, using [70]Candidate pre-selected by OGER  Yes (see the Features section)Total features 36 About 2.8 millionTagging speed (on an Intel 4720HQ CPU) 1286 tokens/sec 632 tokens/secBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 6 of 14To weigh affixes based on their frequency, each affix afrom a terminological resourceD is assigned a normalizedscore s ?[0, 1] computed in this way:s(a,D) = freq(a,D)max({freq(a1,D) . . . freq(a|D|,D)})where freq(a,D) is the frequency of an affix a in D. Twoweights (one prefix and one suffix) for each ontology con-tained in BioTermHub are used as input for the neural net-works, while for CRF we use only a binary flag indicatingwhether the affix can be found in the selected ontology. Asmentioned before, we chose to use affix information dif-ferently for CRF and NN, since after testing both binaryfeatures and weighted features on both algorithms, wedetermined that the former approach performed better onCRF and the latter on NN.For CRF, we also tried to add prefixes and suffixes ofeach token as features, similarly to what we found in[7]. We trained several models by adding affixes of two,three, four, and five characters to each feature set, withthe model increasing from ?2.8 million features to ?11.6million features, but we found no significant improve-ment in performance with respect to using our dictionaryapproach only.Unfortunately, this approach would not have been fea-sible when using neural networks, because it would havegenerated a very large set of additional features, makingthe network practically impossible to train.Finally, it is also worth noting another fundamental dif-ference between the NN and the CRF approach.While theNN receives as input only the tokens selected by OGER,the input of the CRF is composed by the whole tokenizeddocument, and the selection of a token as a potential entityby OGER is used as a feature, as pointed out in Table 1.For this reason, CRFs are able to recognize entities that arenot recognized by OGER, while NNs cannot do this, sincethey know only the portions of the document selected bythe dictionary-based step, and therefore act simply as afilter on OGERs output.Test hardwareWe ran the OGER and Distiller systems on a computerequipped with an Intel i7 4720HQ quad core processorrunning at 2,6 GHz, 16 GB RAM and a Crucial M.2 M550SSD. The operating system was Ubuntu 16.04 LTS.OGER obtained a performance of 5994 tokens/secondwhen running in single thread mode, while the Distillersystem processed 1286 tokens/second when using NNand 632 tokens/second when using CRF. If necessary, theOGER system can be parallelized in a straightforwardmanner. Its efficiency is demonstrated also by the excel-lent result obtained in the recent TIPS challenge [31] (seethe OGER section).ResultsWe examined the performance of our systems in two sep-arate evaluations. First we evaluated the performance ofNER proper, i.e. we regarded only offset spans and the(coarse) entity type of each annotation produced by eachsystem, ignoring concept identifiers. This is a direct con-tinuation of the work presented in [11]. Subsequently,we describe the results of a preliminary concept recog-nition (CR) evaluation. To this end, we augmented theML-based output with concept identifiers taken from thedictionary-based pre-annotations, which enabled us todraw a fair comparison to previous work in CR on theCRAFT corpus.Named entity recognitionWe present the results obtained in Table 2. They arecompared with the previous version of our system, asdescribed in [11].The best recall is obtained by the new version of theOGER system, which obtains an overall performancehigher than the previous OGER/Distiller pipeline. The66% recall score obtained by the system offers an 11%improvement over the previous version but, perhaps moreimportantly, the precision of the annotations is muchhigher, almost doubling the precision from 34 to 59%.The higher quality of the annotations is reflected bythe fact that the neural network pipeline, which receivedonly minor improvements from the previous version, nowdisplays a less significant drop in recall, with a score of60%, just six point less than OGER. In the version pre-sented in [11], the recall drop when adding the machinelearning filtering stage was much higher (18%). The neu-ral network version improves the precision score as well,with a 1% increase. This result brings the overall F1-Scoreto 0.70, which makes this version of the system the bestperforming one.On the other hand, the combined OGER-CRF pipelineobtains a somewhat underwhelming performance, withlower precision, recall and thus F1-score when comparedTable 2 Comparison of the NER performance obtained in thispaper with the previous version of the system [11]System Precision Recall F1OGER 2016 0.34 0.55 0.42OGER+Distiller 2016 0.85 0.37 0.51OGER 0.59 0.66 0.62OGER+Distiller NN 0.86 0.60 0.70OGER+Distiller CRF 0.69 0.49 0.58OGER+Distiller Mixed 0.87 0.63 0.73Distiller CRF 0.71 0.47 0.58The best values are highlighted in boldfaceBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 7 of 14to the NN. Still, bothmodels perform better than the olderversion, corroborating the validity of our approach.Running the Distiller with a CRF-trained model withoutOGERs preprocessing step, i.e. without instructing theCRF which tokens have been marked as entities accord-ing to OGER, leads to an acceptable result, with higherprecision but lower recall (see Table 2, this pipeline iscalled Distiller CRF). This is unsurprising, since CRFsare known to be good systems for named entity recogni-tion; still, using the CRF without using OGERs dictionarywould imply the important caveat that the system wouldnot be able to associate terms with concept identifiers, butonly to recognize their presence and type.In Tables 3, 4, and 5 we analyze the performance ofour system for the individual entity types. Here we con-sider both a strict evaluation, which considers correct onlyannotations where reference and system spans match per-fectly, and a more lenient evaluation scheme, where weconsider a system annotation partially correct if it over-laps just partially with a CRAFT annotation. To be pre-cise, we used the average measure as defined by GATEsAnnotation Diff Tool [51], which is the mean of preci-sion/recall/F1 in the strict and lenient measures. Whena predicted annotation overlaps partially with a referenceannotation, it counts both as a false positive (FP) and afalse negative (FN) in the strict measure. In the lenientmeasure, this corresponds to a true positive (TP). Conse-quently, in the average measure, a partial match is countedas 12 TP,12 FP, and12 FN, such that the denominatorsin precision and recall (TP+FP and TP+FN, respectively)remain constant across all three measures. If more thanone predicted annotation overlaps with the same refer-ence annotation, it is counted as a partial match only once;additional predictions are counted as false positives. Thesame holds for a predicted annotation overlapping withmultiple reference annotations, which contribute to thefalse-negative count.It is interesting to see that the NN model obtains verygood F1-Scores (> 70%) on almost all entity types, buthas some problems to identify biological processes andmolecular functions. Nevertheless, the problems on thiscategory do not hinder the performance of the generalmodel, which is able to obtain the best precision on all cat-egories except for cells. Unfortunately, an almost optimalprecision is not always followed by a good recall, like inthe previously mentioned case of biological processes andmolecular functions, where we have 78% precision andonly 22% recall.The CRF model achieves good or acceptable F1-Scoresfor the majority of the entity types, but appears tohave trouble with correctly identifying chemicals and severely  sequences. In fact, the scores for these twoentity types are so low that the overall score for the CRFpipeline is lower than OGER, even though CRF clearlybeats OGER in five out of seven entity types. This par-tially explains the counterintuitive finding that a plaindictionary-based system achieves better results than theCRF-based system.Using these results, we built a mixed system com-posed of the best performing models, i.e. using CRFs forcells, biological processes and molecular functions, andcellular components, and NNs for the other entity types.This model, labeled OGER+Distiller Mixed in Table 2, isobviously the best possible system, with a 3% increase inF1-score when compared to the NN approach; however,this is a purely academic exercise, since in practice it isvery difficult to combine the NN and the CRF models dueto their very different nature.Finally, the assumption that CRFs are able to recognizeentities which are not detected by OGER is evident bylooking at the recall figures in Table 4. In fact, consid-ering cells, biological processes and molecular functions,and cellular components, we see that the CRF pipelineimproved OGERs recall figures by 11%, 20% and 7%,respectively. This happens because for CRF we adopt atoken-by-token process, where a correct item can be atoken annotated by the CRAFT annotators but not byOGER, while in the NN pipeline the correct samples areTable 3 Per-entity-type breakdown of the precision scores obtained by the different pipelinesEvaluation method: strict Evaluation method: averageEntity type OG OG+NN OG+CRF OG OG+NN OG+CRFAll 0.59 0.86 0.69 0.61 0.89 0.80Chemicals 0.44 0.89 0.48 0.45 0.89 0.50Cells 0.88 0.88 0.95 0.93 0.94 0.96Biological processes/molecular functions 0.39 0.78 0.68 0.45 0.88 0.73Cellular components 0.51 0.91 0.87 0.52 0.92 0.90Organisms 0.29 0.98 0.82 0.29 0.98 0.83Proteins 0.49 0.86 0.74 0.50 0.87 0.80Sequences 0.46 0.89 0.23 0.48 0.91 0.27The best values are highlighted in boldfaceBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 8 of 14Table 4 Per-entity-type breakdown of the recall scores obtained by the different pipelinesEvaluation method: strict Evaluation method: averageEntity type OG OG+NN OG+CRF OG OG+NN OG+CRFAll 0.66 0.60 0.50 0.69 0.61 0.58Chemicals 0.73 0.68 0.26 0.75 0.68 0.27Cells 0.77 0.67 0.88 0.77 0.71 0.89Biological processes/molecular functions 0.25 0.22 0.45 0.29 0.25 0.49Cellular components 0.60 0.56 0.67 0.61 0.58 0.69Organisms 0.92 0.91 0.91 0.92 0.91 0.92Proteins 0.84 0.75 0.66 0.85 0.75 0.72Sequences 0.67 0.64 0.08 0.69 0.65 0.09The best values are highlighted in boldfacen-grams annotated both by OGER and the CRAFT cor-pus annotators. Unsurprisingly, these three entity typesare the ones where the CRF obtains an overall F1-scorehigher than NN.Concept recognitionThe NER pipelines presented in this work are designedto perform entity annotation in terms of identifying rel-evant text regions without assigning identifiers to therecognized terms. Nonetheless, OGER performs conceptrecognition by default, and in the experiments reportedabove, the identifiers produced were simply ignored inthe downstream processing and evaluation steps. For thatreason, we decided to verify the potential of using thepipelines in a CR setting by carrying out an additionalexperiment.We chose a simple strategy to reintroduce the conceptidentifiers provided by OGER into the output of the MLsystems. This step was as straightforward as joining thecorresponding annotations in OGERs and Distillers out-put. For the combined OGER-CRF pipeline, this meantthat CRF annotations were removed if no matching entrywas found in OGERs output.We did not resolve ambiguous annotations; instead,multiple identifiers could be returned for the same span.While having no disambiguation at all is arguably a defi-ciency for a CR system, it is not imperative that eachand every ambiguity is reduced to a single choice. This isparticularly true when evaluating against CRAFT, whichcontains a number of reference annotations with multi-ple concept identifiers. For example, in PMID: 16504143,PMCID: 1420314, the term fish (occurring in the lastparagraph of the Discussion section) is assigned sixdifferent taxonomic ranks.Table 6 shows the performance of the described sys-tems in a CR evaluation against CRAFT, as well as theresults for a number of other systems as reported byTseytlin et al. [24], who carried out a series of experimentsusing the same dataset. All figures reflect the averageevaluation method as described in the previous section;however, at least for our systems, the difference betweenstrict and average evaluation is so small that the numbersare the same at the given level of precision. Please notethat the results reported by [24] are not perfectly com-parable to the ones we obtained, since the former weretested on the whole CRAFT corpus, while our approachwas evaluated on 20 documents only (since we used theremaining documents to train our system), as described inthe Ensemble system section. Still, the comparisonshows that even a relatively simple approach is sufficientTable 5 Per-entity-type breakdown of the F1 scores obtained by the different pipelinesEvaluation method: strict Evaluation method: averageEntity type OG OG+NN OG+CRF OG OG+NN OG+CRFAll 0.62 0.70 0.58 0.65 0.72 0.67Chemicals 0.55 0.77 0.34 0.56 0.77 0.35Cells 0.80 0.76 0.91 0.84 0.81 0.92Biological processes/molecular functions 0.30 0.35 0.54 0.35 0.39 0.58Cellular components 0.55 0.70 0.75 0.56 0.71 0.78Organisms 0.44 0.94 0.87 0.45 0.94 0.88Proteins 0.62 0.80 0.70 0.63 0.80 0.76Sequences 0.54 0.75 0.12 0.57 0.76 0.13The best values are highlighted in boldfaceBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 9 of 14Table 6 Performance of the presented systems in a CR evaluation, compared to results reported in [24]System Precision Recall F1OGER 0.32 0.52 0.40OGER+Distiller NN 0.51 0.49 0.50OGER+Distiller CRF 0.49 0.29 0.37MMTx 0.43 0.40 0.42MGrep 0.48 0.12 0.19Concept Mapper 0.48 0.34 0.40cTakes Dictionary Lookup 0.51 0.43 0.47cTakes Fast Lookup 0.41 0.40 0.41NOBLE Coder 0.44 0.43 0.43Please note that, as stated in the Concept recognition section, the systems described in [24] are evaluated on the whole corpus, while we use 20 documents for testing andthe remainder for training. The best values are highlighted in boldfaceto transform our NER pipeline into a CR system withreasonable quality. This is particularly true for the OGER-NN configuration, where both precision and recall are asgood as or better than the figures for all the reportedsystems.Table 7 shows the CR performance on all the consideredentity types and for all the configurations of the system.Here we see that applying NN filtering after the dictio-nary matching results in an increment of precision in allcases except for cells, where the drop is negligible. As forthe choice of ML algorithm, the NN pipeline is almostalways the best performing model, winning in all entitytypes except for cells in terms of F1-score. However, themain difference between NNs and CRFs is that the for-mer retains a good recall, with the worst drop of just 9%for proteins, while the latter shows a considerable dropin recall in many categories. In particular, while the CRFprecision scores are generally good, if not almost opti-mal for some categories, the results for chemicals andsequences are very bad in terms of recall, hindering thegeneral performance of the system.DiscussionIn this section, we analyze the results of the experimentspresented in the Results section and we contextualizethem with related work.Error analysisNN pipelineSince the NN output depends heavily on OGERs input,many of its mistakes are caused by the quality of the dic-tionary matching. While the precision reached by thismodel in filtering out non-interesting terms is quite highat 85%, the majority of the errors of the NN pipelines con-sist in generic terms, like verbs, adverbs, and so on, thatthe neural network is not able to filter out. For example,in three documents, the word error itself is erroneouslymarked as an entity. Moreover, just using regular expres-sions to detect common suffixes and manually inspectingthe results, we see that about 5% of the errors are adjec-tives, about 5% are adverbs, and about 9% are verbs in-ing form. Another typical error is constituted by commonsubstantives marked wrongly as entities. For example, theTable 7 Per-entity-type breakdown of Precision, Recall, and F1 obtained by the different pipelines in the CR evaluationPrecision Recall F1Entity type OG OG+ OG+ OG OG+ OG+ OG OG+ OG+NN CRF NN CRF NN CRFAll 0.32 0.51 0.49 0.52 0.49 0.29 0.40 0.50 0.37Chemicals 0.28 0.59 0.93 0.61 0.57 0.19 0.39 0.58 0.32Cells 0.88 0.87 0.98 0.72 0.66 0.68 0.79 0.75 0.81Biological processes/molecular functions 0.35 0.72 0.73 0.19 0.17 0.05 0.25 0.27 0.10Cellular comp. 0.49 0.87 0.89 0.59 0.56 0.52 0.54 0.68 0.65Organisms 0.16 0.49 0.47 0.71 0.70 0.67 0.26 0.58 0.55Proteins 0.45 0.84 0.91 0.83 0.74 0.64 0.59 0.79 0.75Sequences 0.27 0.59 0.37 0.53 0.51 0.06 0.36 0.54 0.10The best values are highlighted in boldfaceBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 10 of 14word region by itself makes 4% of the errors. Theseerrors can probably be easily eliminated by using dedi-cated part-of-speech features, and will eventually be tack-led in future versions of the system.The other most common category of errors are anno-tations that are found by OGER but not present inthe CRAFT corpus. For example, in document PMID:15917436, PMCID: 1140370, the genes M13mp18 andM13mp19 are detected by OGER but not in the CRAFTcorpus. These errors are hard to catch with dedicatedfeatures and, on the other hand, can potentially deliveruseful information, since the annotation is conceptuallycorrect, so we do not consider them as a highly criticalproblem of our system. Nevertheless, these errors havea high impact when evaluating the model. For example,in document PMID: 16870721, PMCID: 1540739, the NNpipeline selects the protein p53, which is not presentin the ontologies of the CRAFT corpus, but since thisparticular protein is central in the paper, it is repeatedmany times throughout the document, and its selection bythe NN pipelines accounts for about 4% of the total falsepositives of the pipeline.The false negatives of the NN pipeline depend largelyon the false negatives of OGER, since it cannot selectanything that has not been selected by OGER. Still, theNN can theoretically remove correct OGER selections,and analyzing the results of our system we see that thishappens in practice, too. In fact, the number of the falsenegatives of the NN increases of about 20%, as expectedby the lower recall of the NN pipeline when comparedto OGERs output. These false negatives are mostly shortstrings, like BLM, CD21, p34, with no apparent con-nection with the category of the word: 43% of the falsenegatives introduced by the NN are in fact words of 3 or4 characters (shorter terms had been removed from thedictionary initially, as described in the OGER section).CRF pipelineOne of the strengths of the model, i.e. detecting enti-ties that are not annotated as such, is also a potentialweakness while evaluating its precision. For example, indocument PMID: 17696610, PMCID: 1941754, the modelproduces the annotation Mei1, which is the name of agene/protein. This word is not annotated in the CRAFTcorpus with respect to the Protein Ontology, because atthe time of CRAFTs annotation, the Protein Ontologydid not include Mei1 (recent versions of the ontology doinclude it). CRAFT has annotations for Mei1 with respectto Entrez Gene, but this resource was ignored in our eval-uation, as is described in the CRAFT corpus section.Differently from the false positives produced by the neu-ral network pipeline described in the previous section, thistime the concept is not annotated by OGER either. How-ever, the CRF identifies that Mei1 is a protein: this is againconceptually correct, but still a mistake for the sake ofevaluating the performance of the system on the CRAFTcorpus.We argue that, while these annotations hinder theevaluation performance of the models, they are actuallydesirable. This way, in fact, we are able to annotate whatOGER is not able to recognize due to shortcomings inthe dictionary used, thus providing information which wethink is valuable for the user. Moreover, the ability of themodel to select entities that are not yet present in a knowl-edge base is one of the desirable aspects of using a MLapproach, as pointed out in the Background section.In our example, Mei1 could have been a recently dis-covered protein which then is not yet present in theontologies used; we argue that the ability of recognizing itas an entity and to classify its type, even without the abil-ity of linking it to an actual knowledge base, would be apositive feature of our system.On the other hand, the CRF model makes many errorswhere it includes punctuation in the annotation, like com-mas, parentheses, and so on. The relatively high frequencyof this error is evident when we consider that the CRFpipeline benefits from the average evaluation (which con-siders partial matches as partially correct) by improvingthe precision score by 20%, while the NN pipeline shows amere 3% improvement.This model also fails to recognize certain words or theirderivatives. For example, about 5% of the false negativesof the CRF pipeline consist in the failure of recognizingthe word gene or words with the same root (like genes,genetic, genome, etc.); another 2% is due to the failureof recognizing the word expression or similar. More-over, many acronyms are not recognized: in the documentPMID: 15061865, PMCID: 400732, the model fails to rec-ognize every reference to the acronym D2R (DopamineD2 receptor), weighing about 2% of the total false nega-tives. The same holds for the acronyms or short names(MLH1 or MLH3, PPAR?, PPAR?, etc.), with about15% of the false negatives being acronyms.A particular case are annotations where the annota-tion span is wrong. For example, in document PMID:17425782, PMCID: 1858683, the pygopus genes Pygo1and Pygo2 often appear as Pygo1/Pygo2; in theCRAFT corpus, the single gene names are annotated sepa-rately, while the CRF annotated the whole string. The geneis also often mentioned as Pygo1 gene, and here againin the CRAFT corpus it is annotated as Pygo1, while theCRF selects the whole phrase.OGERFalse negatives in the OGER output have an impact onthe entire system which is much higher than the one offalse positives. This is particularly true in combinationwith the NN postfilter, since annotations missing fromBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 11 of 14the beginning remain unreachable in any subsequent step.In combination with the CRF system, where the OGERannotations are seen as one feature among many, theeffect is less pronounced, since initial false positives mightbe corrected eventually; however, OGERs false negativesstill have a strong influence on the CRFs decision. There-fore, this error analysis focuses on annotations that weremissed by the OGER system.A frequent cause for missing annotations are synonymsthat are not covered by the dictionaries. For example,antibody is used in document PMID: 12925238, PMCID:194730 for the more specific term immunoglobulin com-plex, while the Gene Ontology does not list antibodyas a name for this (or any other) concept. Missing syn-onyms occurred with all entity types, and they were thepredominant source of error for proteins (estimated to bemore than one third of all misses). Sometimes, terms areabbreviated in text, such as olfactory receptor in docu-ment PMID: 14611657, PMCID: 329117, which should beolfactory receptor activity according to the dictionary.This was seen frequently with cellular components (morethan 25%), organisms, biological processes and molecu-lar functions (less than 10%), and proteins (around 5%).Another cause of misses are splitting and reordering ofmulti-word terms, as is both illustrated with gene expres-sion, which is rephrased as expression of [. . . ] gene inthe same document. Along with hyperonymy (see theantibody example above), splitting and reordering con-tributed considerably to the low recall we obtained forbiological processes and molecular functions. Linguisticvariation at the level of morphology was another sourceof mismatch between the terminologies and the texts.Derivation (changes in part-of-speech, such as mam-mal/mammalian, nucleus/nuclear, gene/genetic)was the most common problem for organisms (more than25%), and also occurred among cellular components andsequences (less than 10%). Likewise, plural forms were notalways mapped correctly to their singular forms listed inthe dictionaries, especially in the case of acronyms (suchas cDNAs), where stemming was disabled.Some instances are very close misses. For example,given the dictionary spelling PPAR-delta, OGER wasunable to capture PPAR? in document PMID: 15328533,PMCID: 509410. Even though the matching strategy wasdesigned to be robust against this kind of variation interms of punctuation and transliteration, this particularcase fell through the net. In order for two variants to beconsidered equivalent, a hyphen may only be dropped ifit connects two strings with a different character class(such as alphabetic and numeric, e.g. CRB-1 matchesCRB1). However, a transition from one script to another(Latin?Greek) does not qualify as a token boundary ona par with a hyphen  a design decision which should bereconsidered.Related workThe field of named entity recognition has decades of his-tory, with early work focusing on extracting a single entitytype, such as protein names, from scientific papers [52].Later on, some scholars started to introduce the use of ter-minological resources as a starting point for solving thisproblem [53].The most recent state-of-the-art performance isobtained by using supervised machine-learning basedsystems. For extracting chemical names, [7] describeshow two CRF classifiers are trained on a corpus of journalabstracts, using different features and model parameters.The output of the two classifiers is merged in differ-ent ways, attempting to combine the strengths of eachmethod, using a-posteriori knowledge (performance on atest set) or the models own confidence. The approach in[8] also tackles chemical name extraction with CRF, partlyusing the same software basis as the previous one. Thesystem is trained in a semi-supervised setting by adding alarge collection of unlabeled abstracts and full-text doc-uments. For tagging gene names, [54] describes anothersupervised sequence-labeling approach. The output of aCRF classifier is post-processed through graph propaga-tion in order to account for unseen data occurring in thetest set.There is growing interest in hybrid machine learningand dictionary systems such as the one described in [10],which obtains interesting performance on chemical entityrecognition in patent texts. The authors of [55] use differ-ent approaches for different entity types (machine learn-ing for chemical names, dictionary-based for organismand assay entities); given the complementary application,this is not a hybrid approach in the strict sense. A con-trastive overview that also covers rule-based approachesis given in [56]. While focusing on chemical entity recog-nition, their findings are equally applicable to other entitytypes.In the field of entity linking, dictionary-based methodsare predominant, since the prediction of arbitrary identi-fiers cannot be modeled in a generalized way. In [57], theauthors explore ways to improve established informationretrieval techniques formatching protein names and otherbiochemical entities against ontological resources. Usingthe CRAFT corpus, they measure the impact of case sen-sitivity and the information gain of individual tokens inmulti-word terms. Another strategy borrowed from infor-mation retrieval for increasing the coverage of recognizedentities is term expansion, i.e. indexing additional termsynonyms drawn from another source of knowledge. Forexample, known orthologous relations can be exploited bysubstituting a mentioned protein with an evolutionarilyand functionally equivalent protein from another species.This is applied to the detection of protein interactions infull text in [58]. The TaggerOne system [59] uses a jointBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 12 of 14model for tackling NER and linking at the same time yet another example of a hybrid system that combinesmachine learning and dictionaries. Using an annotatedcorpus for training, the NER task is learned through asemi-Markov model, which is an adaptation of Markovmodels well-suited for detecting multi-word terms. Forlinking, the extracted terms and the dictionary entries areprojected into the same vector space. Machine learningalso often plays an important role when it comes to entitydisambiguation. As an example, in [60] disambiguationis addressed with word embeddings, which are used forcomparing the context of annotated terms with dictionarydefinitions of the candidate concepts.The Colorado Richly Annotated Full Text (CRAFT) cor-pus [15, 61] has been built specifically for evaluatingthese kinds of systems. In [62] (and, with some moredetail, in [63]), the authors used the corpus to evaluateseveral concept recognition tools, showing how they per-form on the individual entity types in the corpus. Later,Tseytlin et al. [24] compared their ownNOBLE coder soft-ware against other concept recognition algorithms, show-ing a top F1-score of 0.44. Another system that makesuse of CRAFT for evaluation purposes is described in[64]. In a series of experiments including all entity typesexcept for sequences, the authors were able to outperformexisting systems in terms of F1-score, achieving approx-imately 93%, 75%, 78%, 60%, 54%, 44%, and 50% in anexact-match evaluation (NER, no identifiers) for species,cells, cellular components, chemicals, genes and proteins,genes, and biological processes and molecular functions,respectively.ConclusionsIn this paper, we have presented an efficient, high-qualitysystem for biomedical NER. We have shown that it canbe easily extended to produce concept identifiers, achiev-ing state-of-the-art results in a Concept Recognition (CR)evaluation. The presented system is a two-stage pipelinewith a dictionary-based pre-annotator (OGER) and amachine-learning classifier (Distiller). In a contrastiveevaluation, we examined the respective quality of the pre-annotations and two different classification approaches.We evaluated both processing speed and annotationquality in a series of in-domain experiments using theCRAFT corpus. OGERs scalability and efficiency wasalso demonstrated in the recently held TIPS task of theBioCreative V.5 challenge. For the NER performance, wecompared a NN classifier, which acted as a postfilter ofthe dictionary annotations, to a CRF classifier that usedOGERs output as a feature among many. While the CRFpipeline showed interesting behavior by predicting termsthat were missing from OGERs dictionary, it was beatenby the NN system for the majority of the entity typesand in the global evaluation, where the latter achieved aprecision of 86% at a recall of 60% (F1: 70%). By augment-ing the classifier output with concept identifiers from thepre-annotations, we were able to perform a CR evaluation.Again, the NN system outperformed the CRF approachwith a precision of 51% at a recall of 49%, which is well inline with scores reported in related literature.As future work, we will consider to tackle specific ter-minological categories where the classifiers fail to obtaingood performance, like biological processes and molecu-lar functions. Moreover, OGER performance can still beimproved in terms of recall by allowing multi-word termsto be reordered or shortened, or even split apart. Also,recall can be increased bymeans of term expansion, e.g. bycollecting additional synonyms from other sources (termi-nologies or corpora). In particular for biological processesand molecular functions, these two strategies can be com-bined to generate new term variants, as is shown in [65].Furthermore, we consider improving the classificationperformance of our system on the entity types where wefail to obtain satisfactory results and, more importantly,to develop a concept disambiguation stage that is ableto choose between the many concept IDs suggested byOGER. Finally, in order to better compare our work toother state-of-the-art systems, we will consider to extendthe evaluation to the full CRAFT corpus by using otherresources to train our system, or to other corpora, like theShARe corpus [66] used by [24].AbbreviationsAKE: Automatic keyphrase extraction; API: Application programming interface;CR: Concept recognition; CRAFT: Colorado richly annotated full text corpus;CRF: Conditional random fields; ML: Machine learning; NCBI: the US NationalCenter for Biotechnology Information; NER: Named entity recognition; NLP:Natural language processing; NN: Neural network; PMCID: PubMed Central ID;PMID: PubMed ID; TIPS: Technical interoperability and performance ofannotation servers (track at the BioCreative V.5 challenge)AcknowledgementsThe authors wish to express their gratitude, for their helpful suggestions, tothe anonymous reviewers. We are also grateful to the organizers of the SMBM2016 conference and the journal editors for giving us the opportunity toexpand that work in the present paper.FundingThe research activity of the OntoGene/BioMeXT group at the University ofZurich are supported by the Swiss National Science Foundation (grantCR30I1_162758).Availability of data andmaterialsThe CRAFT corpus, used for testing purposes in this paper, is available fromhttp://bionlp-corpora.sourceforge.net/CRAFT/. The OGER system is availableupon request. A web service (web interface and REST API) providingannotations of arbitrary text in multiple formats is available from https://pub.cl.uzh.ch/projects/ontogene/oger/. The Distiller framework is an open sourcesoftware developed by the Artificial Intelligence Laboratory at the University ofUdine and it is available at https://github.com/ailab-uniud/distiller-CORE/.Authors contributionsThe work described in this paper was initiated during an academic visit by MBat the OntoGene/BioMeXT group. LF adapted the OGER system to the needsof the experiments described in this paper, and provided all the data required.MB adapted the Distiller tool, integrating and testing machine learningBasaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 13 of 14algorithms which had not yet been considered in that tool. CT and FR providedadvice, guidance, and support. All authors read and approved the paper.Authors informationMarco Basaldella is a PhD student at the University of Udine. He is a member ofthe Artificial Intelligence Laboratory and his research area is informationextraction, focusing in particular on keyphrase extraction and entityrecognition and on the applications of machine learning in theaforementioned areas.Lenz Furrer is a PhD student at the University of Zurich. Member of theOntoGene/BioMeXT group, his research area is biomedical text mining, with afocus on entity and relation extraction at a large scale.Carlo Tasso is a full professor in Computer Science at the University of Udine.He founded the Artificial Intelligence Laboratory in 1984 and has ever sinceworked in the fields of artificial intelligence, information retrieval, contentpersonalization and intelligent knowledge-based systems.Fabio Rinaldi is a senior researcher and principal investigator at the Universityof Zurich and at the Swiss Institute of Bioinformatics. He is the leader of theOntoGene/BioMeXT group (http://www.biomext.org/) which specializes inbiomedical text mining.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Università degli Studi di Udine, Via delle Scienze 208, 33100 Udine, Italy.2University of Zurich, Institute of Computational Linguistics and Swiss Instituteof Bioinformatics, Andreasstrasse 15, CH-8050 Zürich, Switzerland.Received: 17 February 2017 Accepted: 19 September 2017Khan et al. Journal of Biomedical Semantics  (2017) 8:5 DOI 10.1186/s13326-017-0112-6RESEARCH Open AccessSAFE: SPARQL Federation over RDF DataCubes with Access ControlYasar Khan1, Muhammad Saleem2, Muntazir Mehdi1, Aidan Hogan3, Qaiser Mehmood1,Dietrich Rebholz-Schuhmann1 and Ratnesh Sahay1*AbstractBackground: Several query federation engines have been proposed for accessing public Linked Open Data sources.However, in many domains, resources are sensitive and access to these resources is tightly controlled by stakeholders;consequently, privacy is a major concern when federating queries over such datasets. In the Healthcare and LifeSciences (HCLS) domain real-world datasets contain sensitive statistical information: strict ownership is granted toindividuals working in hospitals, research labs, clinical trial organisers, etc. Therefore, the legal and ethical concerns on(i) preserving the anonymity of patients (or clinical subjects); and (ii) respecting data ownership through accesscontrol; are key challenges faced by the data analytics community working within the HCLS domain. Likewisestatistical data play a key role in the domain, where the RDF Data Cube Vocabulary has been proposed as a standardformat to enable the exchange of such data. However, to the best of our knowledge, no existing approach has lookedto optimise federated queries over such statistical data.Results: We present SAFE: a query federation engine that enables policy-aware access to sensitive statistical datasetsrepresented as RDF data cubes. SAFE is designed specifically to query statistical RDF data cubes in a distributedsetting, where access control is coupled with source selection, user profiles and their access rights. SAFE proposes ajoin-aware source selection method that avoids wasteful requests to irrelevant and unauthorised data sources. Inorder to preserve anonymity and enforce stricter access control, SAFEs indexing system does not hold any datainstancesit stores only predicates and endpoints. The resulting data summary has a significantly lower indexgeneration time and size compared to existing engines, which allows for faster updates when sources change.Conclusions: We validate the performance of the system with experiments over real-world datasets provided bythree clinical organisations as well as legacy linked datasets. We show that SAFE enables granular graph-level accesscontrol over distributed clinical RDF data cubes and efficiently reduces the source selection and overall queryexecution time when compared with general-purpose SPARQL query federation engines in the targeted setting.Keywords: SPARQL query federation, Data access policy, Linked Data, Healthcare and life sciencesBackgroundInspired by the publication of hundreds of LinkedDatasets on the Web, researchers have been investigat-ing federated querying techniques to enable access tothis decentralised content. Query federation aims to offerclients a single-point-of-access through which distributeddata sources can be queried in unison. In the context ofLinked Data, various optimised query federation engines*Correspondence: ratnesh.sahay@insight-centre.org1Insight Centre for Data Analytics, NUIG, Galway, IrelandFull list of author information is available at the end of the articlehave been proposed that can federate multiple SPARQLinterfaces [17].However, in the context of the Healthcare and LifeSciences (HCLS) domain  where data-integration isoften likewise vital  the requirements for a federatedquery engine can be rather more specialised. First, real-world HCLS datasets contain sensitive information: strictownership is granted to individuals working in hospi-tals, research labs, clinical trial organisers, etc. There-fore, the legal and ethical concerns on (i) preservingthe anonymity of patients (or clinical subjects); and (ii)respecting data ownership through access control; are© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 2 of 22key challenges faced by the data analytics communityworking within the HCLS domain [811]. Second, manyclinical datasets within the HCLS domain are composedof numerical observations that form multi-dimensionalcorpora of statistical information; for example, clinicaltrial data are often composed of various dimensions ofpatient attributes observed numerically along a temporaldimension. Thus to draw conclusions about biomarkers,side-effects of drugs, correlations within patient groups,etc., requires applying statistical analyses to custom slicesof multi-dimensional data.The current research is then motivated by the needs ofthree clinical organisations: University Hospital Lausanne(CHUV)1, Cyprus Institute of Neurology and Genetics(CING)2, and ZEINCRO3. These organisations wish todevelop a platform for analysing clinical data across mul-tiple clinical sites, which would allow for increasing thetotal number of patients that are included in each analysis,thus increasing the statistical power of conclusions relatedto biomarkers, effectiveness and/or side-effects of drugsor combinations of drugs, correlations between patientgroups, etc. The ultimate goal is to enable the collaborativeidentification of new drugs and treatments while reducingthe high costs associated with clinical trials. With respectto this motivating scenario, the aforementioned generalrequirements apply: strict access control and adequatemethods to represent and query statistical data acrossdifferent sites are crucial aspects of this use-case.Thus while query federation approaches enable the inte-gration of data from multiple independent sources  asrequired by our motivating use-case  traditional federa-tion approaches have not considered methods to enforcepolicy-based access control nor to deal specifically withstatistical data  as also required by our use-case and inmany other HCLS use-cases. Hence the central questiontackled by this paper is how existing federated approachescan be modified to take both access control and statisti-cal data into account while maintaining good performancecharacteristics.The key challenges for federated querying (in gener-alised settings) are efficient source selection [7, 12] (i.e.,determining which sources are (ir)relevant) and queryplanning (i.e., determining an efficient query executionstrategy). These challenges change significantly whenaccess control and statistical data are taken into account.More specifically, our hypothesis in this paper is thataccess control can facilitate more selective source selec-tion by reducing the amount of data to be processed at anearly stage of federated query processing, while statisticaldata represented as data cubes follow certain principlesof locality that can be exploited to restrict the sourcesselected, reducing overall query time.With respect to integrating policy-based access control,query federation engines often apply source selection atthe level of endpoints, whereas in a controlled environ-ment, a user may only have access to certain informa-tion within an endpoint. Adding an access control layerto existing SPARQL query federation engines thus addsunique challenges: (i) source selection should be granularenough to enable effective access control, and (ii) it shouldbe policy-aware to avoid wasteful requests to unautho-rised resources. However these challenges also presentan opportunity to optimise the source selection processby increasing the granularity (in line with the access-control policies) and the selectivity (by quickly filteringresources to which users do not have access) of sourceselection.On the other hand, with respect to statistical data, acommon approach to representing such data is to usedata cubes. In fact, the RDF Data Cube Vocabulary [13]has been standardised by the W3C precisely to enablethe integration of multi-dimensional (e.g., statistical) datafrom multiple sites, as required by our HCLS use-case.Thus the statistical data from our use-case can be rep-resented using this vocabulary, where existing federationengines could then be applied over the resulting sources(like any RDF dataset). However, our hypothesis is that thefixed structure of RDF data cubes implies certain local-ity conditions that are exploitable by the federated engineto optimise execution; in particular, we propose that spe-cialised query planning for RDF Data Cubes can achievebetter performance than a general federated query engineby taking such locality restrictions into account whenselecting the sources to which individual triple patternsshould be sent.In this paper, we present SAFE: a SPARQL QueryFederation Engine that supports policy-based accessto sensitive statistical data in a federated setting. Aspreviously discussed, SAFE is motivated by the needs ofthree clinical organisations in the context of an EU projectwho wish to enable controlled federation over statisticalclinical data  such as data from clinical trials  ownedand hosted in situ by multiple clinical sites, representedin the form of data cubes. However, the methods pro-posed by SAFE can be used in other settings involvingdata cubes outside of the HCLS domain (even for opendata).Given that a wide variety of work has already beenconducted on SPARQL query federation engines [17],it is important to highlight that our focus is on ahigher level than such works: our core hypothesis doesnot relate to low-level join algorithms nor to commu-nication protocols, for example, but rather focuses onthe level of specialised source-selection and query plan-ning algorithms for access-controlled federated over datacubes. Hence rather than propose and implement a fed-erated query approach from scratch, we adapt a general-purpose query federation engine (FedX [1]), which alreadyKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 3 of 22implements the low-level federation primitives that SAFErequires.More specifically, SAFE extends upon the FedX engine[1] with two high-level novel contributions: (i) graph-level source selection, which is required to implementmore granular access-control, and which is enabled by anovel data-summary generation technique and associatedalgorithm, (ii) optimisations for federated query process-ing over statistical data that are represented using theRDF Data Cube Vocabulary. With these modifications,we show that when compared with FedX, HiBISCuS [14]and SPLENDID [2], in such settings, SAFE can (i) supportmore granular graph-level access control than possibleby simply layering access control on top of an existingengine that uses endpoint-level source selection, and can(ii) efficiently reduce the query execution time, the datasummary generation time, and the overall data summarysize, when federating specifically over RDF data cubes.We perform experiments with datasets and queries takenfrom our motivating use-case; to justify the claim thatSAFE can be applied in other statistical scenarios, we addi-tionally perform experiments over RDF data cubes takenfrom other domains.In our initial work [15] SAFE has been evaluated againstthe FedX engine [1] which it extends. In this article, weextend our previous work by (i) improving the sourceselection algorithm and providing extended analysisthereof; (ii) developing an automated technique to gener-ate data summaries (i.e., indexes) with lower relative sizescompared to the raw data and faster generation time; (iii)evaluating against two additional query federation engines(HiBISCuS and SPLENDID); (iv) increasing the number ofqueries and datasets for evaluation experiments. We high-light that contribution (ii) is particularly important whenone considers updates to a source: while caching datasummaries in the federated query engine has the advan-tage of enabling much more targeted source selectionwhile minimising runtime queries to (potentially) remotesources, a disadvantage of using such summaries is theadditional overhead of having to keep them up to dateas the underlying sources change. This latter disadvan-tage can be partially mitigated by using lightweight sum-maries that are efficient to recompute over the updatedsources.The rest of the paper is structured as follows:Motivating scenario section discusses our motivationalscenario where data from different clinical locations needto be queried and aggregated. Related work sectiondiscusses background and related work. Methodssection presents the three main components of SAFEquery planning. Results section presents evaluationof SAFE with respect to queries over various sta-tistical datasets and Conclusions section concludesour work.Motivating scenarioAs previously discussed, our work stems from the ambi-tions of three clinical organisations  University HospitalLausanne (CHUV)4, Cyprus Institute of Neurology andGenetics (CING)5, and ZEINCRO6  who are in the pro-cess of developing a platform for analysing clinical dataacross multiple clinical sites, allowing the reuse of remotedata in a controlled manner. We now discuss importantaspects of this use-case in the context of our research andin the context of the requirements it places on the SAFEfederated engine.Use of Linked DataWith their stated goal of integrating clinical data in a con-trolled manner in mind, the three clinical organisationsmentioned are partners in the Linked2Safety EU project7.The two main goals of the Linked2Safety project are (i)the discovery of data about eligible patients  also knownas subject selection criteria  that can be recruited forclinical trials from multiple clinical sites; and (ii) enablingmulti-centre epidemiological studies to facilitate betterunderstanding of relationships between pathological pro-cesses, risk factors, adverse events, and between genotypeand phenotype. Although Linked Data technologies canhelp enable multi-site interoperability and integration, thecommunity largely focuses on datasets that can be madeopen to the public. In contrast, clinical data is often ofan extremely sensitive nature and there is often strictlegislation in place protecting the privacy of patients.Legal and ethical implications of patient privacyAccording to EU Data Protection Directive 95/46/EC8,clinical studies that involve patient-specific informationmust adhere to data-access restrictions that preservepatient anonymity. More specifically, a data access mech-anism must ensure that patient identity cannot be discov-ered by direct or indirect means using the dataset [16].Similar legislation exists in other jurisdictions. To avoidsharing of individual patient records, the Linked2Safetyconsortium has developed a data mining approach fortransforming raw clinical data into statistical summariesthat may aggregate (or indeed redact) multiple dimensionsof raw data as required for a particular application.The result is a set of anonymised data cubes whosedimensions correspond to insensitive clinical parameterswithout personal information [17]. The resulting mul-tidimensional output contains sufficient granularity toquickly decide if the dataset is relevant for a given analy-sis  e.g., to understand the scale and dimensions of thedata  and to perform high-level meta-analysis of aggre-gated data. These data cubes are represented in a standardformat  namely RDF Data Cube vocabulary per therecent W3C recommendation [13]  to enable interoper-ability (e.g., use of controlled vocabularies for dimensions)Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 4 of 22and to allow the later use of Linked Data publishing/accessmethods.Although the data considered are aggregated and do notcontain personal information about patients, deanonymi-sationmay still be possible [18]: one cannot open a datasetand fully guarantee that it will not (indirectly) compro-mise patient anonymity [19]. Likewise, if a (bio)medicaldataset necessarily involves genetic data, there existidentifying markers by which patients can be directlydeanonymised; thus genetic data can only be pseu-doanonymised [16]. Given such issues, in practice, sharingclinical datasets  even aggregated statistics  is oftenconducted under a strict legal framework between parties.Running exampleIn order to employ stricter data access restrictionson the anonymised multi-dimensional RDF data cubes,we require an access-controlbased query-federationapproach that enforces and optimises for restricted useraccess over these RDF data cubes. Likewise we wish to beable to optimise for certain locality conditions present inRDF representations of statistical data. To further illus-trate and motivate, we now provide a walkthrough ofan example that is representative of the main use-casescenario.Figure 1 shows four sample data cubes published bythree different clinical sites. Each observation repre-sents the total number of patients exhibiting a particu-lar adverse event. For example, the CHUV-S1 observa-tions describe the total number of patients (in the Casescolumn) that exhibit a particular combination of threeadverse events: Diabetes, (Abnormal) BMI_Abnormal(Body Mass Index) and/or Hypertension. The value 0 or1 indicates if the condition is present or not. For example,the second row in CHUV-S1 indicates that there are 26cases presenting with both Diabetes and Hypertensionbut without BMI_Abnormal.These data cubes can be represented in the RDFData Cube vocabulary [13], whose goal is to enablethe interlinking and integration of statistical data cubesover the Web. The RDF resulting from representing theaforementioned data cube in this vocabulary is shownin Fig. 2.9 Individual data cubes are assigned to separatenamed graphs [20], where, e.g., :CHUV-S1, :CHUV-S4are names for two RDF graphs from the same :CHUVsource, whereas :CING-S2 is a another named graphpublished in a different source (:CING). Though notshown for brevity, each such graph is associated with itsown provenance information. Each such named graphrepresents an independent data cube with disjoint sets ofobservations, encoded as RDF resources (e.g., :obs_7);this locality of data on information about observationssuggests the possibility of optimising the source selectionprocess to not only consider individual triple patterns, butrather observations as a whole when answering queries.However, it is important to note that the values of thedimensions (e.g., 1) and the properties used to capture thedimensions (e.g., sehr:Cases) are shared across datacubes, and thus across graphs and sources: locality doesnot apply in such cases.As suggested by these example data, while the RDFData Cube vocabulary provides terms to capture thestructure of a data cube (which we discuss in more detailin RDF data cubes section), additional vocabulary isrequired to capture the clinical terminology needed by theclinical partners. Hence the Linked2Safety consortiumhas developed the Semantic EHR Model [21] (using theprefix sehr in Fig. 2) to capture a unified clinical termi-nology that covers the needs of the three clinical partners[21]; terms such as sehr:Cases, sehr:Diabetes,sehr:HIV, etc., constitute this vocabulary, cap-turing statistical dimensions of the clinical datacubes.Once the data cubes are published by clinical sites,query functionality must be made accessible to clinicalresearchers in a manner that can abstract the details ofthe underlying sources. Given that the underlying dataare in RDF, a natural candidate is to use SPARQL [22]:the standard query language for RDF supported by awide variety of tools. Figure 3 shows a sample SPARQLquery specifying subject-selection criteria, asking for thecounts of cases that involve some combination of diabetes,Fig. 1 Example (2D) of data cubes published by CHUV, CING and ZEINCROKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 5 of 22Fig. 2 Datacubes represented using the RDF data cube vocabularyabnormal BMI, and hypertension. An answer returned bythe query, i.e., number of cases, will play a major rolein deciding the resources (i.e., number of subjects, loca-tion, etc.) required for conducting a clinical trial. How-ever, answering such a query requires integrating RDFdata cubes with three dimensions  Diabetes, Hyper-tension, BMI_Abnormal  and the respective countsoriginating from multiple clinical sites. For this, queryfederation techniques  that allow for answering queriesover multiple independent data sources  will be required.Referring back to Fig. 1, only three of the data cubes(:CHUV-S1, :CING-S2 and :ZEINCRO-S3) contain allrequired dimensions. An answer returned by the query(Fig. 3) should list counts (i.e., cases) from these three RDFdata cubes.Fig. 3 Example of subject selection criteria for clinical trialsHowever, as mentioned previously, these data cubescannot be published openly on the Web, but are rathersubject to strict access control policies. An importantquestion then relates to how such a policy mechanismcan be formulated over the RDF data cubes given inFig. 2. Towards answering this question, the consortiumhas proposed the Access Policy Model (prefix lmds),which describes the users profiles (their activity, location,organisation, position and role) and their respective accessrights (e.g., read, write) [23]. An example is provided inFig. 4, where we see this model used to state that theuser :James has read-level access to two named graphrepresenting two independent data cubes residing in twolocations: :CHUV-S1 and :CHUV-S2; by default, usersdo not have any privileges. In our scenario, the most com-mon form of access-control policies are applied at the levelof named graphs (i.e., data cubes).Now when answering the SPARQL query in Fig. 3,we must take into account these data-access policies.For example, assuming the query is executed by theuser :James, we know that he has access tothe :CHUV-S1 and :CING-S2 RDF data cubes only.Therefore, the query federation engine should retrieveresults only from :CHUV-S1 and :CING-S2 and shouldnot consider :ZEINCRO-S3 for querying.Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 6 of 22Fig. 4 Snippets of user profile, access policy and data cube source information. a User profile, bData cubes stored within named grap, c Access policyProblem statementIn summary, from prior works we have a vocabularyto represent data cubes as RDF [13], a vocabulary todescribe the clinical terminology of the partners in theproject [21], a wide variety of proposals on how to executeSPARQL queries in federated settings [17], and a vocab-ulary for describing data-access policies over these datacubes [23]. However, we have no work looking at puttingall these aspects together. Thus the motivation for ourresearch is to investigate how to enable efficient access-controlled query federation over statistical data cubes. Asargued in the introduction, we cannot use existing fed-eration engines off-the-shelf since they do not providesource-selection with the granularity needed to imple-ment graph-level access control. Likewise the regularstructure of data cubes suggests optimisations that wouldnot be possible in a more general RDF/SPARQL sce-nario. Hence the core research questions we tackle in thispaper are: How can we efficiently implement source-selection ina federated scenario on the level of graphs (as neededto efficiently support graph-level access control)? Can we optimise the query federation processspecifically for querying federated collections of RDFdata cubes in a manner that allows us to outperformoff-the-shelf engines?Towards tackling these questions  questions that arekey to realising the ambitions of the Linked2Safety project we will later propose the SAFE query federation engine.Related workThe scenario described in the previous section touchesupon three main areas: query federation, access controland data cubes. We now discuss related literature in thesethree areas, focusing on those works that deal in particularwith the Semantic Web standards (e.g., with RDF andSPARQL) as relevant in our scenario.SPARQL query federationMany query federation engines have been proposed forSPARQL (e.g., [17, 14, 2427]). Such engines accept aninput query, decompose it into sub-queries, decide rel-evance of individual data sources (typically consideringsources at the level of endpoints) for sub-queries, forwardthe sub-queries to the individual endpoints accordinglyandmerge the final results for the query. Such engines aimto find and execute optimised query plans that minimiseinitial latency and total runtimes. This can be achievedby (i) using accurate source selection to minimise irrele-vant messages, (ii) implementing efficient join algorithms,(iii) and using caching techniques to avoid repeated sub-queries.Source selection is typically enabled using a localindex/catalogue and/or probing sources with queries atruntime. The former approach assumes some knowl-edge of the content of the underlying endpoints andrequires update/synchronisation strategies. However, thelatter approach incurs a higher runtime cost, having tosend endpoints queries to determine their relevance forvarious sub-queries. Thus, many engines support a hybridof index and query-based source selection.Table 1 gives an overview of existing SPARQL queryfederation engines with respect to source selection type,physical join operators, use of caching and explicit sup-port for updates. We also remark on whether code isavailable for the system. In this setting, our work buildsupon an existing federated engine  FedX  with supportfor an access-control layer over statistical data representedas RDF data cubes.Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 7 of 22Table 1 Overview of existing SPARQL query federation enginesSystems Source selection Join type Code Policy Cache UpdateADERIS [27] Index Nested loop ? ? ? ?ANAPSID [26] Query & index Adaptive ? ? ? ?Avalanche [24] Query & index Distributed, merge ? ? ? ?DARQ [4] Index Nested loop, bind ? ? ? ?DAW [25] Query & index Based on underlying system ? ? ? ?FedSearch [3] Query & index Bind, pull-based rank ? ? ? ?FedX [1] Query Nested loop, bind ? ? ? ?LHD [5] Query & index Hash, bind ? ? ? ?SPLENDID [2] Query & index Hash, bind ? ? ? ?HiBISCuS [14] Query & index Nested loop, bind ? ? ? ?FEDRA [6] Query & index Nested loop, bind ? ? ? ?SAFE [15] Query & index Nested loop, bind ? ? ? ?Access control for SPARQLVarious authors have explored access control models forSPARQL query engines. Gabillon and Letouzey [28] pro-pose applying access control over named graphs andviews, which are defined as graphs dynamically gener-ated using SPARQL CONSTRUCT or DESCRIBE queries.Costabello et al. [29] propose SHI3LD: an access controlframework for SPARQL 1.1 query engines that operateson the level of named graphs where permissions are basedon the context of the user in the setting of a mobile device;permissions are checked using SPARQL ASK queries. Kir-rane et al. [30] propose using stratified Datalog rulesto enforce an access control model that operates overquad patterns, thus offering higher granularity of con-trol. Bonatti et al. [31], propose reactive policies thatcan model, for example, access-control settings throughan EventConditionAction paradigm. Daga et al. [32], onthe other hand, propose to use the Datanode ontology  fordescribing data flows  to model policy propagation rules.SAFE is designed specifically to query statistical RDFdata cubes in a distributed setting, where access controlis coupled with source selection and both operate on thesame level of granularity: named graphs. Access control deny or allow access  is based on user profiles and theiraccess rights, which are described in the Access PolicyModel created for the purposes of the Linked2Safetyproject [23].RDF data cubesThe RDF Data Cube Vocabulary (QB) [13] is a stan-dard for describing data cubes as RDF, providing termsto represent the structure of such cubes in an agreed-upon manner, facilitating interoperability in the exchangeand interlinkage of data cubes on the Web. We use qb:as a prefix to refer to this vocabulary. The core classesin the vocabulary are: qb:DataSet, whose instancesrepresent individual data cubes; and qb:Observation,whose instances represent a coherent tuple of measure-ment values (as per the example in Fig. 2 where eachobservation refers to a tuple of values in the originaldata cubes of Fig. 1). In terms of describing individualobservations  such as the type of measure (area, dura-tion, volume, location, etc.), units of measure (m2, s, m3,lat/long, etc.), and so forth  QB recommends use ofthe Statistical Data and Metadata eXchange (SDMX)10standard, which supports expressing such features in aninteroperable manner. QB also allows for expressing fur-ther features of data cubes, with a prominent examplebeing slices, where each slice is a group of observationswith certain values on given dimensions that can, forexample, be annotated with further meta-data or linkedto/from other data; as a brief example, in Fig. 2, we coulduse QB to define a slice of of the cube :CING-S2 to rep-resent statistics on patients with diabetes (with the value 1for sehr:Diabetes), which would include :obs_4 butnot :obs_3.Although QB has been standardised relatively recently,there have been a few research works looking at exploit-ing data in this format. For example, a number of toolshave been proposed to help users to create, publish andsubsequently analyse RDF data cubes [33, 34]. On theother hand, Kämpgen et al. [35] look at methods to derivemappings to integrate disparate data cubes together intoa global view. Relating more specifically to querying datacubes, Kämpgen and Harth [36] look at the performanceof using traditional relational tools (MySQL/Mondrian)for OLAP-style queries with a platform using QB andan off-the-shelf SPARQL store (Virtuoso), including theeffects of materialising views on query runtimes. In gen-eral however, while there have been some initial workslooking to exploit RDF data cubes, to the best of ourknowledge, no work has tackled the scenario of processingKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 8 of 22queries over a federation of data cubes that are access-restricted in various remote locations.MethodsSAFEs architecture is summarised in Fig. 5, which showsits three main components: (i) Source Selection: per-forms multilevel source selection based on the capabil-ities of data sources; (ii) Policy Aware Query Planning:filters the selected data sources based on access rightsdefined for each user; and (iii) Query Execution: performsthe execution of sub-queries against the selected sourcesand merges the results returned. In the following, wedescribe these components in detail. The first two compo-nents (Source Selection and Policy Aware Query Planning)are described in Algorithm 2 whereas the third compo-nent (Query Execution) is delegated to the FedX queryengine [1].Source selectionSAFE performs a tree-based two-level source selection asshown in Fig. 6. At Level 1, like other query federationengines [1, 2, 5, 14, 26], we perform triple-pattern-wiseendpoint selection, i.e., we identify the set of relevantendpoints that will return non-empty results for the indi-vidual triple patterns in a query. At Level 2 (unlike otherquery federation engines), SAFE performs triple-pattern-wise named graph selection, i.e., we identify a set of relevantnamed graphs containing RDF data cubes for all relevantendpoints already identified at Level 1. SAFE relies on datasummaries to identify relevant named graphs.Data summariesSAFEs data-summary generation algorithm is shown inAlgorithm 1. The algorithm takes the set of all datasets(for example, CHUV, CING and ZEINCRO) as input andgenerates a concise data summary that enables graph-level source selection (as needed for the coupling withgraph-level access control). By proposing a specialisedalgorithm for this setting, we claim that SAFE has signif-icantly improved data-summary generation times whencompared to other index-based approaches for generalsettings; this allows for faster recomputation of sum-maries when the underlying sources change. The datasummaries themselves and the algorithm used to generatethem are explained in the following.We assume a set of datasetsDwhere each datasetD ? Dis a RDF dataset: D := {(u1,G1), . . . (un,Gn)}, where each(ui,Gi) is a named graph with (unique) URI ui. In our case,named graphs refer to individual RDF data cubes as we donot consider a default graph. We denote all graph namesby names(D), the set of all graphs by graphs(D), and aparticular graph in the dataset by D(u) := G. We denoteby preds(G) := {p | ?s, o : (s, p, o) ? G} the set of allpredicates in G and, overloading notation, by preds(D) :=?(u,G)?D preds(G), we denote the set of all predicates inD. Finally, we assume that each dataset is published as anendpoint at a specific location, where loc(D) denotes thelocation (endpoint URL) of the dataset D.For each dataset D ? D, where each graph in Dcontains an RDF data cube, SAFE stores the follow-ing as a data summary: (i) the endpoint URL loc(D)(lmds:endpointUrl) (line 4 of Algorithm 1); (ii) theset of all graph names (lmds:cube/lmds:graph) (line6 of Algorithm 1); and (iii) a map for each pred-icate appearing in the dataset to the set of namescorresponding to the graphs in which it appears(lmds:cubeProperties) (lines 78 of Algorithm 1).Fig. 5 SAFE architectureKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 9 of 22Fig. 6 Tree-based two level source selectionAlgorithm 1: SAFE data summaries generationalgorithmData:D = {D1, . . . ,Dn}, loc(·)/* Set of all data sources and theirmapping to endpoint URLs */1 S ? {} ; /* SAFE summaries to be retrievedas output */2 for each D ? D do /* for each data source inD */3 initialise SD ; /* initialise summary fordataset D */4 SD.setURL(loc(D)) ;5 for each (u,G) ? D do /* for each graph inthe dataset */6 SD.addGraphName(u) ;7 for each p ? preds(G) do /* for eachpredicate in the graph */8 SD.mapPut(p, SD.mapGet(p) ? {u}) ; /* mappred. to gr. name */9 S ? S ? {SD} ;10 return S ; /* Data summaries of all sources*/We thus denote the set of all data summaries for D as Sand the data summary for a particular source as S(D). Asnippet of a data summary generated for the sample RDFdata cubes published by three clinical sites (CHUV, CING,ZEINCRO) of Fig. 1 is shown in Fig. 7, where CHUV containstwo RDF data cubes (:CHUV-S1, :CHUV-S4), CING con-tains one RDF data cube (:CING-S2), and ZEINCRO alsocontains only one RDF data cube (:ZEINCRO-S3).From the collection of raw data summaries S , we thencompute some indexes that will be used to accelerate thesource-selection process:1. The set of all predicates in a given dataset D:preds(D). For instance, for the dataset CHUV fromthe running example (Fig. 1, Fig. 7), we have thatpreds(CHUV) = {sehr:Diabetes,sehr:BMI_Abnormal, sehr:Hypertension,sehr:Smoking, sehr:Gender, sehr:Cases }.2. The set of properties unique to a graph with name uin the dataset D, where, overloading notation:upreds(u,D) := {p ? preds(D(u)) | u? : u? =u ? p ? preds(D(u?))}. For instance, from therunning example, we have that upreds(:CHUV-S1,CHUV) ={sehr:BMI_Abnormal,sehr:Hypertension}and upreds(:CHUV-s4,CHUV) ={sehr:Smoking,sehr:Gender}.3. The set of graph names in D that have at least oneunique property:unames(D) := {u ? names(D) | upreds(u,D) = ?}.For instance, from the running example,unames(CHUV) = {:CHUV-S1,:CHUV-S4}.These indexes will be used in the following source selec-tion algorithm.Source selection algorithmSAFEs source selection maps individual triple patterns tographs within sources that contain data relevant for thequery. However, in doing so, SAFE exploits certain localityproperties exhibited by RDF data cubes: more specifi-cally, we assume that subjectsubject joins (henceforth:ss joins) can only be satisfied by data local to a given RDFdata cube. This locality assumption is based on the ideathat data cubes stand as self-contained data structureswithin their respective named graph.More specifically, weassume that datasets, observations, slices, measures, etc.,are not split over multiple data cubes/named graphs. Forexample, in Fig. 2, this assumption restricts the possibilityof an observation instance, such as :obs_1, appearing asa subject in two distinct named graphs: in other words, theobservation is assumed to be a local resource unique toa given data cube. This locality restriction applies equallyKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 10 of 22Fig. 7 SAFE data summarieswithin a datasetD and across all datasetsD.11 Hence whendeciding the named graphs that may be relevant for agiven triple pattern, we also consider other triple patternsthat share the same subject; for example, in Fig. 3, triplepatterns 27 (lines 58) form an ss join and will be con-sidered in unison, where although all sources will matchthe second and third triple pattern, these sources will notbe considered relevant unless they are relevant for othertriple patterns with the same subject.The source selection process is detailed in Algorithm 1.The algorithm takes the set of all available datasets D,their data summaries S , and a a set of Basic Graph Pat-terns12 BGP as input. These BGPs correspond to all BGPsappearing in the input query, where each such BGP willbe processed separately since it may correspond to, forexample, optional or union patterns in the input query,rather than a standard join. The algorithm also accepts anaccess policy P and a user profile U ; for the moment, wefocus on the source selection, where we will provide moredetails of the user-level policies and access control in thesection that follows. As output, the algorithm returns theset of relevant sources and corresponding named graphsidentified for individual triple patterns.We now discuss in detail the operation of the algorithm:Line 1: The source selection algorithm will return a setof candidate graphs for each triple pattern; these willbe stored in R, which is initialised on Line 1. Thesources relevant for different BGPs will be kept sep-arate in the results since different sources may beselected for the same triple pattern in two differentBGPs.Line 2: The source selection algorithm will begin pro-cessing all BGPs in the query one-by-one. Each suchBGP may refer to different parts of the query thatmay require a certain operation, such as a UNION orOPTIONAL clause, etc.; these will later be processedand combined by the FedX engine.Lines 35: Within each BGP, the algorithm proceeds bygrouping triple patterns according to their subjectand processing each subject-group separately. Thealgorithm first takes all triple patterns for a givensubject and then extracts all (bound) predicates forthat subject.Lines 67: For each dataset, if it contains all the predi-cate IRIs in the subject group, then it may containrelevant graphs (otherwise the algorithm continuesto the next dataset).Lines 818: The algorithm uses information aboutgraphs that contain unique predicates in a givendataset to potentially filter sources. If the subject-group contains such a predicate, then only thatgraph can be relevant. However, if the subject-groupcontains two (or more) predicates that are unique fordifferent graphs, then no graph can contain relevantdata and the algorithm proceeds to the next dataset.If no such predicates are found in the subject-group,potentially all graphs in the dataset are consideredrelevant.Lines 1924: After all subject groups for the current BGPhave been processed, for triple patterns with vari-ables as predicates, some may be restricted to thesources of their subject group, while others may bematched to all possible graphs. In the latter case, toincrease the selectivity of source selection, for sucha triple pattern where at least the subject or objectis bound, we will send an ASK query to each datasetto see if it may be relevant or not, i.e., to see if thedataset contains data for that subject and/or object.If so, all graphs from that dataset are added.Line 25: The sources selected for the current BGP areadded to the results.Lines 2628: Wewill then check each graph selected (forsome triple pattern in some BGP) against the poli-cies and user profile, removing any graphs for whichthe user does not have access. We describe this pro-cess in more detail in Security and authenticationsection.Source selection exampleFrom our running example, let us consider the queryshown in Fig. 3, which contains one BGP with sevenKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 11 of 22Algorithm 2: Access-policybased triple-patternwise source selection methodData:D,BGP,S ,P,U/* Datasets, BGPs of a SPARQL query, Data Summaries, Access Policies, User Profile */1 R ? {} ; /* initialise relevance set *//* process each BGP from the query independently ... */2 for each bgp ? BGP do /* for each BGP *//* process source selection for local subject groups separately ... */3 for each s such that ?p, o : (s, p, o) ? bgp do /* for each unique subject in bgp */4 bgps ? {t ? bgp | subj(t) = s} ; /* get all patterns with that subject */5 Preds ? {p | ?o : (s, p, o) ? bgps ? p is bound } ; /* get all bound predicates */6 for each D ? D do /* for each dataset */7 if Preds ? S .preds(D) then /* if dataset covers predicates */8 us ? ?; /* name of a unique graph for bgps (initially null) */9 for each u ? S .unames(D) do /* for each gr. name with unique pred. */10 if Preds ? S .upreds(u,D) = {} then /* if (u,G) unique for bgps */11 if us = ? then /* if (u,G) first such unique graph */12 us ? u ; /* store the name */13 else /* multiple unique graphs imply no results for bgps in D */14 goto 6 ; /* continue to next dataset */15 if us = ? then /* precisely one unique graph found */16 R ? R ? (bgps × {us} × loc(D)) ; /* add for all patterns in bgps */17 else /* no unique graph found */18 R ? R ? (bgps × S .names(D) × loc(D)) ; /* add all graphs in D *//* ask queries for patterns with unbound predicates matching all graphs thus far... */19 for each (s, p, o) ? bgp ? BGP such that p is unbound ? (s is bound ? o is bound) do20 if R contains all possible graphs for (s, p, o) then /* no locality restriction found */21 R ? R ? {(t,u, d) ? R | t = (s, p, o)} ;22 for each D ? D do /* for each dataset */23 if ASK((s, p, o),D) = true then /* use ASK query to check relevance */24 R ? R ? ({(s, p, o)} × S .names(D) × loc(D)) ; /* add all graphs */25 R ? R ? {R} ; /* store sources selected for the current BGP *//* do policy-based graph filtering to prune selected graphs ... */26 for each (u, d) such that ?t : (t,u, d) ? R ? R do /* for each graph selected as relevant */27 if ¬authorise(d,u,P,U) then /* if user not permitted to access that graph */28 remove(R,u, d) ; /* remove unauthorised graph for all patterns and BGPs */29 returnR ; /* return relevant and permitted sources for all triple patterns and BGPs */triple patterns. The first step is to group the BGP intosubject groups, which will result in two sub-BGPs, asfollows:?dataset a qb:DataSet?observation qb:dataSet ?dataset?observation a qb:Observation?observation sehr:Diabetes ?diabetes?observation sehr:BMI_Abnormal ?bmi?observation sehr:Hypertension ?hypertension?observation sehr:Cases ?casesThe first subject group will be matched to all graphscontaining such a triple. For the second subject group, theset of all predicates is:Preds = {qb:dataSet,rdf:type,sehr:Diabetes,sehr:BMI_Abnormal,sehr:Hypertension,sehr:Cases}For each dataset, the algorithm checks to ensure that allpredicates in the subject group are covered by the predi-cates in that dataset; this is the case for all three datasetsin Fig. 2 (:CHUV, :CING, and :ZEINCRO). Again, giventhe locality restrictions, ss joins are answerable only overa given dataset/graph, and hence we can safely filter otherdatasets from consideration for all triple patterns in thatsubject group.Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 12 of 22Next, for each such dataset, the algorithm analy-ses graphs that uniquely contain predicates withinthat dataset. For example, for the :CHUV dataset, thefirst graph :CHUV-S1 contains the unique predicatessehr:BMI_Abnormal and sehr:Hypertension,while :CHUV-S4 contains the unique predicatessehr:Smoking and sehr:Gender. Since bothsehr:BMI_Abnormal and sehr:Hypertensionappear in the current subject group, only :CHUV-S1 willbe selected as relevant for all triple patterns from thedataset :CHUV. This means, for example, that :CHUV-S4will not be selected for the triple patterns referring tosehr:Diabetes, sehr:Cases, etc., even thoughdata exists to match those patterns; due to the localityrestrictions, such triple patterns cannot form a join withthe sehr:BMI_Abnormal and sehr:Hypertensiontriple patterns.Since no triple patterns contain unbound predicates,no ASK queries are sent. Then the selected sources areadded for the current BGP. For the first triple pattern,all graphs in all datasets will be matched. For all triplepatterns in the second subject-group, the following threedatasetgraph pairs will be selected: (:CHUV,:CHUV-S1),(:CING,:CING-S2), (:ZEINCRO,:ZEINCRO-S3). Finally,user authorisation is checked for all graphs, where ifauthorisation is not available, the graph is filtered; wewill discuss this access-control process in more detail inSecurity and authentication section.Endpoints with selected named graphs will then bequeried using standard federation techniques. For this,we use the FedX query engine [1], amending the queryrewriter to append the relevant graph information foreach endpoint.Source selection correctnessThe source selection algorithm assumes certain localityrestrictions that must hold in the data for the algorithmto be correct. In particular, for a given set of datasets D,we assume that if there exists a dataset D, a named graphname (u,G) ? D, and a triple (s, p, o) ? G, then theredoes not exist a dataset D?, a named graph (u?,G?) anda triple (s, p?, o?) ? G? such that D = D? or u? = u. Inother words, we assume that subjects are unique to a givennamed graph (considering all named graphs and datasetsinD).With this locality restriction, we can then begin to con-sider the correctness of Algorithm 2. The goal of the algo-rithm is to ensure that all possible answers for each BGPin the input (i.e., the answers possible for each BGP overa local merge of all data in D) can be generated by joiningthe union of the results of individual triple patterns eval-uated over the sources selected for those patterns. Oncethis process is assured, we delegate the processing of thequery to FedX, which can use standard query executionmethods to execute, for example, joins, left joins, unions,filters, etc., over the results of each BGP, producing thefinal query results for the user. First wemust highlight thata known and non-trivial obstacle to completeness in fed-erated scenarios is presented by blank nodes [37]; hence,per the running examples, we assume that no blank nodesare used in the data.More formally, let D denote the result of merging alldatasets in D into a single global dataset13, let bgp ={t1, . . . , tn} denote a BGP, and let [[D]]bgp denote the resultof evaluating bgp over D [38]. Next let R denote thesources selected for bgp by Algorithm 2, let R(t) denotea dataset composed of the named graphs selected for thetriple pattern t in R, and let [[R(t)]]t denote the evalua-tion of triple pattern t over that dataset. The correctnesscondition we wish to satisfy is then as follows: [[D]]bgp =[[R(t1)]]t1  . . . [[R(tn)]]tn .Let us start with a base case where the BGP has only sin-gleton subject groups, meaning that no two triple patternsshare a subject, and where all predicates are bound. In thiscase, the algorithm will select all datasets and (at least) allgraphs with matching predicates. In this case, it is not dif-ficult to show that [[R(t)]]t =[[D]]t for all t ? bgp, and thuswe have that [[D]]bgp =[[D]]t1  . . . [[D]]tn , which is thedefinition of the evaluation of BGPs [38]. Likewise, if weconsider only singleton subject groups, but where somepredicates are not bound, again the ASK queries will fil-ter only irrelevant graphs, where it is again not difficultto show that [[R(t)]]t =[[D]]t for all t ? bgp in this gener-alised case. In fact, these two base cases refer to standardtechniques in federated SPARQL query processing.What is left then is to verify the correctness of selectingsources by subject group. For the moment, we can assumethat bgp forms one subject group and that all predicatesare bound. Assume for the purposes of contradiction thatas a result of Algorithm 2, [[D]]bgp =[[R(t1)]]t1  . . . [[R(tn)]]tn . First off, given that [[R(ti)]]ti ?[[D]]ti for 1 ? i ? n(since R(ti) is a slice of data from D and BGPs are mono-tonic), we have that [[D]] bgp ?[[D]]t1  . . . [[D]]tn ,i.e., that we return only correct answers. Thus we are leftwith the case that [[D]]bgp[[R(t1)]]t1  . . . [[R(tn)]]tn .When could such a case occur? First of all, joins withina named graph between the triple patterns in bgp arenot restricted by the algorithm, which only applies a triv-ial condition that all predicates in the subject-group bematched in the relevant datasets and that unique predi-cates in the graph are also satisfied. Hence for such a caseto occur  for the algorithm to miss results  we wouldneed a join to occur across graphs on (at least) the sub-ject position. However, such a join would clearly break ourlocality restriction, and hence we have a contradiction.Wehighlight that it does not matter if the subject term is avariable or a constant here, nor does it matter if the con-stituent triple patterns in the subject group additionallyKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 13 of 22have joins in other positions: such subject-groups can stillonly generate results with terms sourced from a singlegraph.Finally, combining everything, we know that each sub-BGP pertaining to a subject group must return completeresults, and hence the join of these sub-BGPs must also becomplete. Thus the correctness condition (soundness andcompleteness) holds.Source selection complexity analysisWith respect to the complexity of Algorithm 2, we analyseworst-case asymptotic runtime complexity consideringRDF terms as symbols of constant length: i.e., we do notconsider the length of IRIs or literals since they are typi-cally bounded by some small constant factor. To keep theanalysis concise, we will consider q to be the size of thequery encoding the number of triple patterns in the unionof BGP; note that with this factor, we can abstract away thepresence of multiple BGPs, the number of predicate in thequery, etc., since these are bounded by q. Likewise we willconsider g to be the total number of graphs in all datasets,and d to be the number of datasets available (note that dis bounded by g). Finally we denote by p the number ofunique predicates in all graphs (more specifically, this isthe cardinality of the set of all terms appearing in the pred-icate position of any graph) and by t the total number oftriples in all graphs.Creating the subject groups for a BGP and extractingthe predicates for those groups can be done by sorting thetriple patterns in the query, which is feasible in O(q log(q))in the worst-case with, e.g., a Merge sort implementation.For each subject group, we must check for each datasetthat all predicates in that subject group are contained inthe predicates for each dataset. This is feasible by a Mergesort over all predicates in the subject group (whose cardi-nality we denote by ps) with all predicates in the dataset,resulting in O((ps + p) log(ps + p)) complexity for a givendataset and subject-group, or O(d((ps+p) log(ps+p))) forall datasets and a given subject group. When consideringall subject groups, we can more coarsely (but concisely)represent the complexity as O(qd((q + p) log(q + p))),replacing both ps and the number of subject groups withq by which both are bounded.Next, for each graph with unique predicates, we need tocheck if any such predicate appears in the subject group.This is bounded byO(qg(q+p) log(q+p)) since we need tocheck each graph once (again, the first q bounds the num-ber of subject groups, and the second and third q boundthe number of predicates in each subject group).Since the complexity O(qg(q+ p) log(q+ p)) asymptot-ically bounds the other factors, it represents the overallcomplexity up until Line 18 and thus the complexityof the analysis assuming all triple patterns have boundpredicates and no access control is in place.On Lines 1924, the algorithm performs ASK queriesto each dataset for each triple pattern matching the givencriteria (bounded by q). In the general case, resolving ASKqueries is NP-complete in combined complexity (consid-ering both the size of the query and the data), even in thecase that the query only contains a BGP and no featureslike optionals and filters; this is because each such queryrequires finding a homomorphism from the query graphto the data graph, where the graph homomorphism prob-lem is NP-complete. However, since we only issue ASKqueries with one triple pattern, and since the arity of thetriple pattern is bounded, this step is feasible in (at least)time linear in the size of the data (for example, running asimple scan over all data), and so for all patterns, we havea resulting worst-case complexity of O(qt) from this partof the algorithm.Hence before considering access control on Lines2628, we canmerge these two factors to give a worst-casecomplexity of O(qg(q+ t) log(q+ p)). We emphasise thatthis is a coarse upper-bound in that it encapsulates dis-joint cases whereby, e.g., a querys subject groups repeatall predicates in the query and a query has no boundpredicates, which is an impossible case to occur in onequery; however, the complexity needs to bound all suchcases since they affect different complexity parametersin different ways. In summary, as we will show, for real-world cases  where queries are small, predicates are few,and ASK queries are accelerated with pre-computed indexschemes  the algorithm is much more efficient than thisworse-case complexity bound may suggest.Finally it is worth mentioning that the combinedcomplexity for evaluating SPARQL queries is PSpace-complete [38], meaning that in a worst-case analysis, theactual evaluation of the query will dominate the sourceselection process. However again in practice, evaluatingSPARQL queries can often be done efficiently in spite ofsuch worst-case analyses: in particular, queries are oftenquite simple, having, for example, low treewidth (an indi-cator of how cyclical the interconnection of patterns inthe query are), where queries with bounded treewidth are(oftendepending on the exact query features permitted)fixed-parameter tractable [39]. The core conclusion is thatworst-case analyses do not paint a complete picture: it isimportant to consider empirical performance results aswell.Security and authenticationThe proposed policy aware SPARQL federation functionson top of a security and authentication mechanismemployed within the Linked2Safety software platform[40]. The security of user profiles, access policy, index(data summaries), and data cubes residing behinddifferent SPARQL endpoints is based on a Public KeyInfrastructure (PKI), which binds public keys withKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 14 of 22respective user identities by means of a CertificateAuthority (CA). The user related information (profile andpolicy), data cubes, and data summaries are stored andhosted by the data owners, i.e., by the healthcare organ-isation that gather the data. Each user identity must beunique within each CA domain maintained across organi-sations. For each user (profile), the user identity, the publickey, their binding, validity conditions and other attributesare made un-forgeable through public key certificatesissued by the CA. A log auditing mechanism keeps trackof the query, the user and the data-cubes returned.The process of allowing a user to access RDF data-cubes(and data summaries) is based on two axes: the first oneis to authenticate the user, which allows the system toverify that the user is who he/she claims to be. This isalso a prerequisite in order to know the role that an expertuser has in the Linked2Safety system since after verifyingthe user, we can extract his/her role and correspondingdata-access privileges. The second axis is to authorise theexpert user to access the requested RDF data cubes if cer-tain criteria based on his/her profile information are met,including role, working area, origin, etc. Each encrypteddata-cube is sent (along with the signed hash-digest)with the public key of the client who requested the data(from his/her certificate). Upon successful verification,the expert user (profile) is authorised to perform a par-ticular query over the SPARQL endpoint. The results aresigned and encrypted by the clinical partners before beingreturned to the expert user, using the certificates. TheLinked2Safety platform automatically verifies the originof the data (non-repudiation), the sources they were sentfrom (authentication) and decrypts them (data integrity),before providing the query results.Again, the security and authentication platform for theLinked2Safety project is the subject of existing work [40],which SAFE considers as a black box. Integration with thisplatform is represented in Lines 2628 of Algorithm 2,whereby applying source selection on the level of graphsallows the query federation process to be directly inte-grated with the graph-level access-control policies in placefor the given stakeholders. An example of such access poli-cies is given in Fig. 4 where we see that the user :Jamesis permitted access only to two graphs: :CHUV-S1 and:CING-S2. In Fig. 8, we provide a SPARQL querythat asks if the user :James has access to the graph:CHUV-S1; in the running example, this will return true.Let us consider again the example discussed for sourceselection where SAFE is executing the query in Fig. 3 overthe federation of RDF data cubes outlined in Fig. 2. With-out access control  prior to Line 26 in Algorithm 2  thesource selection algorithm will select the three datasetgraph pairs: (:CHUV,:CHUV-S1), (:CING,:CING-S2),(:ZEINCRO,:ZEINCRO-S3). However, the authenti-cated user does not have access to the latter source, andhence this graph will be filtered as a source, thus ensur-ing the user does not (attempt to) break the access policiesof the stakeholders; if the source were not filtered, therequest to access :ZEINCRO-S3would rather be rejectedat remote query-execution time.Note that in the following evaluation section, we focuson the performance of the SAFE engine for executingfederated queries and do not directly measure the per-formance of access control for the following two mainreasons: (i) SAFE makes a sequence of calls to an externalframework described in previous work [40] where suchan evaluation would relate more to the external frame-work than to SAFEs design; (ii) it is unclear to what wecould compare the results since other federated queryengines do not implement such access control. Instead,we highlight that the main contribution of SAFE foraccess control is implementing graph-level source selec-tion, which enables tight integration with such an accesscontrol system; we will compare the performance of thismore granular source selection in the following sectionwith existing engines that offer dataset-level sourceselection.ResultsIn Motivating scenario section, based on our motivatingscenario, we introduced two core research questions: How can we efficiently implement source-selection ina federated scenario on the level of graphs (as neededto efficiently support graph-level access control)?Fig. 8 SPARQL query authenticating a user against a data cube/named graphKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 15 of 22 Can we optimise the query federation processspecifically for querying federated collections of RDFdata cubes in a manner that allows us to outperformoff-the-shelf engines?In terms of the first question, we have proposed theSAFE engine, which performs graph-level source selec-tion that allows it to be integrated with graph-level accesscontrol; however, we have yet to see how efficient thisalternative form of source selection when executing fed-erated queries. In terms of the second question, we haveproposed certain locality restrictions applicable in thecontext of RDF data cubes to refine the source selectionprocess; however, we have yet to see how this optimisationcompares to existing engines.Along these lines, in this section we present the resultsof our evaluation comparing SAFE with three existingSPARQL query federation engines  FedX, HiBISCuS andSPLENDID  for a variety of queries and datasets along aseries of metrics and aspects.Experimental setupThe experimental setup (i.e., datasets, setting, queries andmetrics) for evaluation are described in this section. Notethat the experimental material discussed in the follow-ing section and an implementation of SAFE are avail-able online at https://github.com/yasarkhangithub/SAFE,which we will refer to in the following as the homepage.Datasets We use two groups of datasets exploring twodifferent use cases.The first group of datasets (internal) are collectedfrom the three clinical partners involved in our primaryuse case as described in Motivating scenario section.These datasets contain aggregated clinical data repre-sented as RDF data cubes and are privately owned/restricted.The second group of datasets (external) are col-lected from legacy Linked Data containing sociopoliti-cal and economical statistics (in the form of RDF datacubes) from theWorld Bank, IMF (International MonetaryFund), Eurostat, FAO (Food and Agriculture Organizationof the United Nations) and Transparency International.The World Bank data contains a comprehensive set ofinformation about countries around the globe, such asobservations on development indicators, financial state-ments, climate change, research projects, etc. The IMFdata provides a range of time series data on lending,exchange rates and other economic and financial indica-tors. The Eurostat data provides statistical indicators thatenable comparison between countries and regions acrossEurope. The Transparency International data includes aCorruption Perceptions Index (CPI), which ranks coun-tries and territories based on how corrupt their publicsector is perceived to be. The FAO data covers the areas ofagriculture, forestry and fisheries. The Linked Data cubesspace (Fig. 9) shows how these legacy datasets are inter-linked with each other. These datasets provide links toeach other using skos:exactMatch and owl:sameAsproperties. The circles represent datasets while edges rep-resent unidirectional or bidirectional links between anytwo datasets. These external datasets are available on thehomepage.Table 2 gives an overview of the experimental datasets,where we see that the largest dataset is IMF with 44 mil-lion triples describing 4 million observations with a totalraw data size of 3.5 GB. On the other hand, the datasetwith the highest dimensionality is FAO, with 247 uniqueproperties. The largest INTERNAL dataset is CHUV with0.8 million triples and 96 thousand observations; it alsohas the highest dimensionality, evidenced by 36 uniquepredicate terms.Setting Each dataset was loaded into a different SPARQLendpoint on separate physical machines. All experimentsare carried out on a local network, so that networkcost remains negligible. The machines used for exper-iments have a 2.60 GHz Core i7 processor, 8 GB ofRAM and 500 GB hard disk running a 64-bit Win-dows 7 OS. Each dataset is hosted as a Virtuoso (OpenSource v.7.2.4.2) SPARQL endpoint hosted physically onseparate machines. Each instance of Virtuoso is config-ured with NumberOfBuffers = 680000, MaxDirtyBuffers = 500000 and MaxQueryMem = 8G. Fur-ther parameters used to configure Virtuoso are availableon the homepage.Queries A total of 15 queries are designed to evaluateand compare the query federation performance of SAFEagainst FedX, HiBISCuS and SPLENDID based on themetrics defined. We define five queries for the federa-tion of internal datasets (QL-*) and ten for the federationof external datasets (QE-*). The list of external queries ismade available on the homepage. The queries are of vary-ing complexity and have varying type of characteristics asnoted in Table 3 where we summarise the characteristicsof these queries following similar dimensions to that usedin the Berlin SPARQL benchmark [41]. The row for thenumber of sources in this table indicates those matchedby at least one triple pattern in the query.Metrics For each query type we measured (i) the numberof sources selected; (ii) the average source selection time;(iii) the average query execution time; and (iv) the num-ber of ASK requests issued to sources. The performanceof SAFE, FedX, HiBISCuS and SPLENDID was comparedbased on these metrics. The query results produced byKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 16 of 22Fig. 9 Linked Data cubes spaceSAFE, FedX, HiBISCuS and SPLENDID are the same forall queries.Experimental resultsIn this section, we present the experimental results gath-ered for the given datasets, setting, queries and metricsdiscussed previously.Index Generation Time and Compression Ratio:SAFEs index/data summaries generation approach iscompared with various state-of-the-art approaches andthe comparison results are shown in Table 4. The compar-ison metrics used are index generation time (time), indexsize (size) and the index reduction (ratio: computed as1 ? index sizetotal dump size ).As can be seen from the results, the index sizes forall approaches are much smaller than the relative sizeof the raw data dump. In the case of FedX, no indexesare created since relevant sources are determined on-the-fly at runtime. Aside from FedX, SAFE produces thesmallest indexes by focusing only on meta-data aboutpredicates and named graphs: for external datasets hav-ing a raw dump size of 8 GB, SAFE generates an index ofsize 23 KB, achieving 99.99% reduction, while for internaldatasets, with a raw size of 51MB, SAFE achieves a 99.98%(8 KB) index reduction. It should be noted however thatalthough in relative terms HiBISCuS and SPLENDID pro-duce indexes that are 211 times larger, the absolute sizesof the indexes are relatively small across all engines, whereTable 2 Overview of experimental datasetsDataset Type ? trip ? obsv ? sub ? pred ? obj DataCHUV INT 0.8 M 96 K 96 K 36 88 31 MBCING INT 0.1 M 17 K 17 K 21 51 5 MBZEINCRO INT 0.4 M 49 K 49 K 24 59 15 MBTotal INT 1.3 M 162 K 162 K 81 198 51 MBWorld Bank EXT 15 M 1.7 M 1.7 M 240 2.1 M 1.9 GBIMF EXT 44 M 4 M 4 M 181 1.4 M 3.5 GBEurostat EXT 0.3 M 38 K 38 K 64 31 K 125 MBTrans. Int. EXT 43 K 3928 4198 49 11 K 121 MBFAO EXT 11 M 1.4 M 1.4 M 247 0.2 M 1.93 GBTotal EXT 72 M 7 M 7 M 923 4 M 8 GBKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 17 of 22Table 3 Summary of query characteristicsCharacteristics QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5? of Patterns 9 8 10 16 11 10 12 9 7 11 6 6 3 7 5? of Sources 3 4 4 3 4 3 3 4 3 3 3 3 3 3 3? of Results 41 1 10 41 64 22208 5 10 108779 4380 1701 17199 760 39312 10Filters ? ? ?>9 pattens ? ? ? ? ? ? ? ?Negation ?LIMITmodifier ? ? ? ?ORDER BYmodifier ? ?DISTINCTmodifier ? ? ? ? ? ? ? ? ? ? ? ?REGEX operator ? ?UNION operator ? ?reduction rates remain above 99.9% for all three enginesover all datasets.As far as index generation time is concerned, aside fromFedX which incurs no index generation costs, SAFE hasa significant gain over all the approaches. In particular,with respect to the EXTERNAL datasets, SAFEs index gen-eration time is 102 s as compared to 1,772 s and 369 sfor HiBISCuS and SPLENDID, respectively. SAFE has thelowest time for INTERNAL dataset as well though in over-all terms, these times are quite small. Hence we see that,for example, upon updates to the INTERNAL federationof datasets, SAFE could recompute indexes from scratchin around 10 seconds. Of course for larger datasets, this(re-)indexing grows to the order of minutes.While FedX incurs no index generation nor mainte-nance costs, we propose that SAFEs indexes will reducethe load on remote endpoints and ultimately the overallquery-execution time, and thus presents a good trade-off in a federated setting. These claims will be exploredTable 4 Index generation time and compression ratioSystem Time Size RatioExternal datasetsSAFE 102 s 23 KB 99.998%HiBISCuS 1772 s 112 KB 99.994%SPLENDID 369 s 252 KB 99.988%FedX   Internal datasetsSAFE 10 s 8 KB 99.984%HiBISCuS 26 s 20 KB 99.961%SPLENDID 74 s 21 KB 99.959%FedX   in the context of subsequent metrics (namely number ofASK queries, source selection time and query executiontime).Triple pattern-wise sources selected: Table 5 shows thetotal number of triple pattern-wise (TP) sources selectedby SAFE, FedX, HiBISCuS and SPLENDID for all thequeries. For the purposes of comparability, we count thenumber of datasets selected as relevant sources since onlySAFE additionally selects relevant graphs. The last col-umn in Table 5 shows the average number of TP sourcesselected by each approach across all queries.FedX performs source selection at the triple-pattern-level using ASK queries for each triple pattern to findout precisely which sources can answer an individualtriple pattern. Thus, on the level of individual triplepatterns, FedX selects all and only the actual contribut-ing sources for those patterns. However, these sourcesmight not be relevant after performing a join betweentwo triple patterns, i.e., results from some sources mightbe excluded after join. HiBISCuS uses a hybrid sourceselection approach by using both ASK queries and datasummaries to prune the number of relevant sources.SPLENDID uses VOID descriptions of sources to identifythe relevant sources for each triple pattern of the query.SPLENDID also make use of ASK queries for source selec-tion in cases where the query has bound variables that arenot covered in the VOID descriptions.The results show that on average HiBISCuS and SAFEhave better source selection algorithms in terms of theaverage number of sources selected (10 and 10, respec-tively). The results in Table 5 show that FedX andSPLENDID overestimate the set of sources that contributeto the final query results. In the query execution timessection, we will see that source overestimation leads tohigher query execution times. Thus both HiBISCuS andKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 18 of 22Table 5 Sum of triple-pattern-wise sources selected for each querySystem QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5 AvgSAFE 7 11 13 16 14 12 14 12 10 11 6 6 3 9 5 10FedX 8 18 20 24 24 15 19 19 11 22 14 16 7 15 13 16HiBISCuS 9 6 10 16 12 5 12 9 3 11 14 16 7 15 13 10SPLENDID 8 13 20 24 18 16 20 19 12 22 14 16 7 15 13 16SAFE outperform FedX and SPLENDID (in the averagecase) by not only considering sources relevant to a giventriple-pattern, but also the other triple patterns in thequery. For example, by using join-aware source selectiondesigned for RDF data cubes, SAFE manages to filter fur-ther potential sources that do not contribute to the endresults.Although SAFE does not have a clear advantage overHiBISCuS in terms of the number of datasets selectedas sources, SAFE does have an extra advantage overthe other engines not illustrated by Table 5: the SAFEsource selection algorithm prunes sources at the granular-ity of graphs, further restricting the data to be consideredbeyond datasets.Number of SPARQL ASK requests: Table 6 shows thetotal number of SPARQL ASK requests used to performsource selection for each query. FedX is an index-freeapproach and performs runtime SPARQL ASK requestsduring source selection for each triple pattern in query:hence without any indexes, FedX must run many moresuch queries than the other engines that do support indexinformation. HiBISCuS uses a hybrid approach that usesboth runtime SPARQL ASK requests and pre-computeddata summaries during source selection for each triplepattern in a query. SPLENDID uses VOID descriptionsas well as ASK requests in case of bound variables in thequery for which VOID does not offer relevant informa-tion. Hence, by considering indexes, both HiBISCuS andSPLENDID greatly reduce the number of ASK queriesused during source selection. On the other hand, SAFEuses data summaries for source selection, reverting toSPARQL ASK requests only when there is an unboundpredicate in a triple pattern and no locality restrictionsare found to apply on the subject group to which that pat-tern belongs. None of our evaluation queries have such atriple pattern; hence there are no SPARQL ASK requestsfor SAFE.Though flexible in the generic case  particularly in thecase of frequent updates to underlying sources  index-free approaches can incur a large cost in terms of SPARQLASK requests used for source selection, which can in turnincrease overall query execution time.Source selection time: Figure 10 compares the sourceselection time for SAFE, FedX, HiBISCuS and SPLENDIDfor each query, where the y-axis is presented in log-scale.The rightmost set of bars compares the average sourceselection time over all queries. Given that the indexes ofHiBISCuS, SPLENDID and SAFE remain quite small rel-ative to total data sizes, they can easily be loaded intomemory, where lookups can be performed in millisec-onds. On the other hand, executing remote ASK queriesare orders of magnitude more costly. Hence we see thatthe source selection time for SAFE is lower than the otherapproaches since SAFE uses ASK queries more sparingly,as previously discussed.Query execution time: For each query, a mean queryexecution time was calculated for SAFE, FedX, HiBISCuSand SPLENDID by running each query ten times.Figure 11 then compares the mean query execution timesof SAFE, FedX, HiBISCuS and SPLENDID for all queries.Again, the y-axis is log-scale. We set a timeout of 25minutes on query execution; with these settings, FedXtimes-out in the case of four queries, HiBISCuS in threequeries and SPLENDID in twelve queries (note that we donot show average execution times across all queries sinceit would be unclear what value to assign to queries thattime-out). On the other hand, SAFE does not time outin any such case. Looking at query response times, SAFEoutperforms the other engines in all queries. The fastestTable 6 Number of SPARQL ASK requests used for source selectionSystem QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5 AvgSAFE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0FedX 54 48 60 96 72 60 72 54 48 66 18 18 9 21 15 47HiBISCuS 12 12 6 24 12 18 12 12 12 12 0 0 0 0 0 9SPLENDID 17 16 25 12 24 25 23 14 27 32 14 16 7 15 13 19Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 19 of 22Fig. 10 Comparison of source selection timequery (QE-1) is executed by SAFE within 100 ms, whilethe slowest query (QL-4) takes approximately 2 minutes.In total, SAFE executes 7 of the 15 queries in less than asecond.DiscussionThere are a number of factors that can influence the over-all query execution time of a query federation engine,such as join type, join order selection, block and buffersize, etc. However, given that SAFE is based on theFedX architecture, we can attribute the observed runtimeimprovements to three main factors: (i) source selec-tion time is reduced (as we have seen in the previoussets of results); (ii) fewer sources are queried meaningless time spent waiting for responses; (iii) source prun-ing at graph level within a source leads to queryingfewer triples and (iv) triple patterns are more selective inSAFE, where, for example, our join-awareness makes itunlikely that all rdf:type triple patterns will need tobe retrieved/queried for all sources but rather only fromsources where such a triple pattern joins with a moreselective one. Taken together, these four main observa-tions explain the time saving observed for our presenteduse-cases, where the third and fourth observation in par-ticular  the locality conditions on ss joins designed forRDF data cubes combined with a more granular graph-level selection  play a significant role for restricting theamount of data generated for low-selectivity triple pat-terns. By making specific locality-based optimisations forthe case of RDF data cubes, and combining this withfiner-grained source selection on the level of graphs,SAFE can perform beyond what would is possible in off-the-shelf SPARQL federation engines designed for thegeneral case.Fig. 11 Comparison of query execution timeKhan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 20 of 22Aside from query execution times, there are also a num-ber of other important factors to consider, such as theload on the remove servers, and also the ability to copewith updates to the individual sources. In terms of loadon the remove servers, we argue that by generating fewerASK queries during the source-selection process, and ingeneral by applying a more granular source-selection thatrequires processing fewer data, SAFE generates less loadon remove servers, reducing the costs associated withhosting such services. With respect to updates, SAFE isless flexible than the index-free FedX approach, whichrequires no special action upon source updates; however,SAFEs index is rather lightweight and can be recomputedfrom scratch (over an entire federation of sources) in theorder of seconds for smaller datasets, and minutes forlarger datasets, which should be acceptable except in thecase that updates are very frequent and strong notions ofdistributed consistency are required. On the other hand,the benefit of SAFEs indexing approach has been demon-strated in terms of source selection times, load on servers,query execution times, and so forth. Hence there is a cleartrade-off, where we argue that in the case of SAFE, formost settings, the inflexibility with respect to updates ispaid off in terms of overall efficiency.Finally, the SAFE source selection algorithm has theadditional advantage of allowing tight integration with agraph-level access-control framework. While this access-control requirement was the original motivation behindthe design of SAFE, and in particular its graph-levelgranularity, as these experiments have shown, select-ing sources on the level of graphs, in combination withjoin-aware optimisations, also gives general performancebenefits even in the case that access-control is not needed.ConclusionsIn this paper, we have presented SAFE: a query federationengine that enables policy-based access to sensitive sta-tistical datasets represented as RDF data cubes. The workis motivated in particular by the needs of three clinicalorganisations who wish to develop a platform for collabo-ratively analysing clinical data that spans multiple clinicalsites, thus improving the statistical power of conclusionsthat can be drawn (versus one source alone). Clinicaldata  even in aggregated form  is of a highly sensi-tive nature, and thus query federation engines must takeaccess policies into account.In our initial work [15] SAFE has been evaluated againstthe FedX engine, in this article, we extend our previouswork by (i) evaluating against two additional query feder-ation engines (HiBISCuS and SPLENDID); (ii) increasingthe number of queries and datasets for evaluation exper-iments; (iii) presenting a variety of improvements andextended analyses for the data summary computation andsource selection procedures.SAFE is developed as an extension on top of theFedX federation engine to support two main features: (i)optimisations tailored for federating queries over RDFdata cubes; and (ii) source selection using highly com-pressed data summaries on the level of named graphsthat allows for integration with an existing access con-trol layer. We evaluated these extensions based on ourinternal data sets (private data owned by clinical organi-sations) as well as external data sets (public data availablefrom the LOD cloud) in order to measure the effi-ciency of SAFE against FedX, HiBISCuS and SPLENDID.Our evaluation results show that, for our use-case(s),SAFE outperforms FedX, HiBISCuS and SPLENDIDin terms of source selection and query executiontimes.In terms of future work, there are still a number ofpossible routes to explore with respect to improving theperformance of SAFE. For example, in considering RDFdata cubes described in individual named graphs, we cur-rently only include locality restrictions on subject groups(ss joins); however, there is the possibility to enforcesuch restrictions on other types of joins when they involveterms from the QB vocabulary, such as, for example,so joins on predicates like qb:dataSet. Furthermore,the structure of such data also suggests a closer lookat the underlying join operator implementations: ratherthan relying on the general FedX query processor, SAFEcould instead benefit from, for example, the ability topush joins to remove sources following available localityguarantees.Endnotes1 http://www.chuv.ch/2 http://www.cing.ac.cy/3 http://www.zeincro.com/4 http://www.chuv.ch/5 http://www.cing.ac.cy/6 http://www.zeincro.com/7 http://www.linked2safety-project.eu/8 http://www.dataprotection.ie/docs/EU-Directive-95-46-EC/89.htm9We omit definitions of prefixes for brevity since theyare inessential to the discussion.10 http://www.iso.org/iso/catalogue_detail.htm?csnumber=5250011Currently we assume this locality of ss joins occursin all cases since we deal purely with RDF data cubes;however, our algorithm could be trivially extended to dropor relax this locality principle in the presence of certainpredicates or on instances of certain classes that may bedescribed in multiple named graphs.Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 21 of 2212 http://www.w3.org/TR/sparql11-query/#BasicGraphPatterns13A possible corner-case occurs here if graphs withthe same name appear in multiple datasets, but we willassume that such a case does not occur.AcknowledgementsThis article is based on a conference paper discussed at the SWAT4LS 2014,Berlin, Germany [15]. We thank the anonymous reviewers for their usefulcomments.FundingThis publication has emanated from research supported in part by theresearch grant from Science Foundation Ireland (SFI) under Grant NumberSFI/12/RC/2289, EU FP7 project Linked2Safety (contract number 288328), EUFP7 project GeoKnow (contract number 318159), the Millennium NucleusCenter for Semantic Web Research under Grant NC120004, and FondecytGrant No. 11140900.Availability of data andmaterialsThe authors have made all the data and materials used in the manuscriptavailable. The source-code (AGPL License) of SAFE and queries and datasets usedin the experiments can be found at https://github.com/yasarkhangithub/SAFE.Authors contributionsYK and MS have lead the entire design and execution of the study and leaddrafting the manuscript. MM has helped in formalising algorithms in themanuscript, and assisted in designing the experiments. AH has provided thebackground information and related work of the study. QM helped inpreparing the datasets for performing experiments. DRS and RS have equallycontributed in supervising the entire study. All authors have equally reviewedand agreed upon the submission of the manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Author details1Insight Centre for Data Analytics, NUIG, Galway, Ireland. 2AKSW, University ofLeipzig, Leipzig, Germany. 3Centre for Semantic Web Research, DCC,University of Chile, Santiago, Chile.Received: 13 July 2016 Accepted: 10 January 2017Segura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 DOI 10.1186/s13326-017-0156-7RESEARCH Open AccessSimplifying drug package leaflets writtenin Spanish by using word embeddingIsabel Segura-Bedmar* and Paloma MartínezAbstractBackground: Drug Package Leaflets (DPLs) provide information for patients on how to safely use medicines.Pharmaceutical companies are responsible for producing these documents. However, several studies have shownthat patients usually have problems in understanding sections describing posology (dosage quantity andprescription), contraindications and adverse drug reactions. An ultimate goal of this work is to provide an automaticapproach that helps these companies to write drug package leaflets in an easy-to-understand language. Naturallanguage processing has become a powerful tool for improving patient care and advancing medicine because itleads to automatically process the large amount of unstructured information needed for patient care. However, to thebest of our knowledge, no research has been done on the automatic simplification of drug package leaflets. In aprevious work, we proposed to use domain terminological resources for gathering a set of synonyms for a giventarget term. A potential drawback of this approach is that it depends heavily on the existence of dictionaries, howeverthese are not always available for any domain and language or if they exist, their coverage is very scarce. To overcomethis limitation, we propose the use of word embeddings to identify the simplest synonym for a given term. Wordembedding models represent each word in a corpus with a vector in a semantic space. Our approach is based onassumption that synonyms should have close vectors because they occur in similar contexts.Results: In our evaluation, we used the corpus EasyDPL (Easy Drug Package Leaflets), a collection of 306 leafletswritten in Spanish and manually annotated with 1400 adverse drug effects and their simplest synonyms. We focus onleaflets written in Spanish because it is the second most widely spoken language on the world, but as for theexistence of terminological resources, the Spanish language is usually less prolific than the English language. Ourexperiments show an accuracy of 38.5% using word embeddings.Conclusions: This work provides a promising approach to simplify DPLs without using terminological resources orparallel corpora. Moreover, it could be easily adapted to different domains and languages. However, more researchefforts are needed to improve our approach based on word embedding because it does not overcome our previouswork using dictionaries yet.Keywords: Text simplification, Lexical simplification, Word embeddings, Drug package leafletsBackgroundSince 2001, according to a directive of the European Par-liament (Directive 2001/83/EC) [1], every drug productmust be accompanied by a package leaflet before beingplaced on the market. This document provides informa-tive details about a medicine, including its appearance,actions, side effects and drug interactions, contraindica-tions, special warnings, among others. This directive also*Correspondence: isegura@inf.uc3m.esComputer Science Departament, Universidad Carlos III de Madrid, Avenida dela Universidad, 30, Madrid, Spainrequired that drug package leaflets (DPLs) must be writ-ten in order to provide clear and comprehensible infor-mation for patients because their misunderstanding couldbe a potential source of drug related problems, such asmedication errors and adverse drug reactions.In 2009, the European Commission published a guide-line [2] with recommendations and advices in order toissue package leaflets with accessible and understandableinformation for patients. However, recent studies [3, 4]show that the readability and understandability of thesedocuments have not been improved during the last years.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Segura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 Page 2 of 9In particular, a recent work [5] about readability of DPLscorresponding to 36 drugs downloaded from EuropeanMedicines Agency website in 2007, 2010 and 2013 years,concluded that there was no improvement in the read-ability of the package leaflets studied between 2007 and2013, despite the European Commissions 2009 guidelineon the readability of these leaflets that established differ-ent rules to guarantee that patients can easily understandthem. Therefore, further efforts must be made to improvethe readability and understandability of DPLs in orderto ensure the proper use of medicines and to increasepatient safety.One of the main reasons why the understandability hasnot been improved is that these documents still con-tain a considerable number of technical terms describingadverse drug reactions, diseases and other medical con-cepts. Posology (dosage quantity and prescription), con-traindications and adverse drug reactions seem to be thesections most difficult to understand [6]. To help solvingthis problem, we propose an automatic system to sim-plify those terms describing adverse drug effects in DPLs.Text simplification is a natural language processing (NLP)task that aims to rewrite text into an equivalent one withless complexity for readers. Text simplification techniqueshave been applied to simplify texts from different domainssuch as crisis management [7], health information [810],aphasic readers [11], language learners [12].To the best of our knowledge, our previous work [13]is the only research about the automatic simplificationof DPLs. We focus on lexical simplification, that is, thesubstitution of complex concepts with simpler synonyms.Moreover, we focus on leaflets written in Spanish becauseit is the second most widely spoken language on theworld1. Our first approach consisted of choosing themost frequent synonym as the simplest one. To do this,we used specialized dictionaries for medicine for obtain-ing the set of synonym candidates for a given term,and then, we calculated their frequencies in a large col-lection of documents. In this new work, we focus ourefforts on exploring a domain-independent and language-independent approach, such as the use of word embed-ding. A word embedding is a function that transformswords into real-value vectors. This representation ensuresthat similar words have similar vectors, that is, their vec-tors are close together. At this time there is a explosionof research based on word embeddings applied to a widevariety of NLP tasks with very successful results. Althoughseveral works have already exploited the use of wordembeddings for detecting complex words [14], buildingparallel corpus for text simplification [15] or substitutionof complex words [16], the lexical simplification of DPLsis still an unexplored field. In addition, our work is one ofthe few studies that addresses the simplification of textswritten in Spanish.Related workThere are two main subtasks of text simplification: lexicaland syntactic simplification. Lexical simplification basi-cally consists of replacing complex concepts with simplersynonyms, while syntactic simplification aims to reducethe grammatical complexity of a text while preserving itsmeaning. Comprehensive surveys of the text simplifica-tion field can be found in [17, 18]. We have to distinguishbetween readability and understandability because theseconcepts capture different aspects of the complexity ofthe text. Readability concerns the length and structureof sentences (syntax) and consequently requires syntac-tic simplification approaches to split sentences in shorterunits with simpler structure. On the other side, under-standability is about the difficulty to interpret a word[19] and it requires lexical simplification approaches. Ourwork here focuses on improving the understandabilityof DPLs by replacing terms describing drug effects bysimpler synonyms.The main challenges of lexical simplification are (a)the difficulty of recognizing if a word is a complex termand (b) identifying the correct synonym for a particu-lar context in which the word appears (this is crucial,especially when the word is polysemous). For the firstissue (a), a common heuristic used is to select as com-plex words those that have a low frequency in a corpus(complex words tend to be rarer), but also to combine fre-quency with word length (words withmore than a numberof syllables/characters could be considered complicatedwords). In Semeval 2012 English Lexical Simplificationchallenge2 with ten participant systems, the evaluationresults showed that proposals based on frequency gavegood results comparing to other sophisticated systems.Similarly, the complex word identification task of SemEval2016 [20] showed that though decision trees and ensemblemethods achieved satisfactory performance, word fre-quency is still the most efficient predictor of word com-plexity. Decision trees and ensemble methods performbetter than neural networks because the small size of thetraining dataset. The only system exploiting word embed-dings was developed by Sanjay et al. [14], who trained asupport vector machine (SVM) algorithm to identify com-plex words. In addition to word embeddings, the featureset also included orthographic word features, similarityfeatures and Part-of-Speech (POS) tag features.Parallel corpora of original and simplified texts can beused for automatic text simplification. Biran et al. [21]used English Wikipedia and Simple English Wikipedia[22] (which was developed applying Easy-to-Read (E2R)guidelines3) to calculate the complexity of a word as theratio of its frequencies in each corpus.For the issue (b), there are two main approaches: usinglexical resources or using context words and n-gramsmodels. Lexical resources (such asWordNet [23]) are usedSegura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 Page 3 of 9to propose synonyms as candidates in order to replacecomplex wordS. Lexical resources are also combined withprobabilistic models, as has been tried in [24]. In the sec-ond approach, word contexts are used in [25] and [21],where a vector space model is used to capture lexicalJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 DOI 10.1186/s13326-017-0114-4RESEARCH Open AccessBuilding a model for disease classificationintegration in oncology, an approach basedon the national cancer institute thesaurusVianney Jouhet1,2*, Fleur Mougin2, Bérénice Bréchat1,2 and Frantz Thiessard1,2AbstractBackground: Identifying incident cancer cases within a population remains essential for scientific research inoncology. Data produced within electronic health records can be useful for this purpose. Due to the multiplicity ofproviders, heterogeneous terminologies such as ICD-10 and ICD-O-3 are used for oncology diagnosis recordingpurpose. To enable disease identification based on these diagnoses, there is a need for integrating diseaseclassifications in oncology. Our aim was to build a model integrating concepts involved in two disease classifications,namely ICD-10 (diagnosis) and ICD-O-3 (topography and morphology), despite their structural heterogeneity. Basedon the NCIt, a derivative model for linking diagnosis and topography-morphology combinations was defined andbuilt. ICD-O-3 and ICD-10 codes were then used to instantiate classes of the derivative model. Links betweenterminologies obtained through the model were then compared to mappings provided by the Surveillance,Epidemiology, and End Results (SEER) program.Results: The model integrated 42% of neoplasm ICD-10 codes (excluding metastasis), 98% of ICD-O-3 morphologycodes (excluding metastasis) and 68% of ICD-O-3 topography codes. For every codes instantiating at least a class inthe derivative model, comparison with SEER mappings reveals that all mappings were actually available in themodel as a link between the corresponding codes.Conclusions: We have proposed a method to automatically build a model for integrating ICD-10 and ICD-O-3 basedon the NCIt. The resulting derivative model is a machine understandable resource that enables an integrated view ofthese heterogeneous terminologies. The NCIt structure and the available relationships can help to bridge diseaseclassifications taking into account their structural and granular heterogeneities. However, (i) inconsistencies existwithin the NCIt leading to misclassifications in the derivative model, (ii) the derivative model only integrates a partof ICD-10 and ICD-O-3. The NCIt is not sufficient for integration purpose and further work based on othertermino-ontological resources is needed in order to enrich the model and avoid identified inconsistencies.Keywords: NCI thesaurus, Oncology, Terminology, Semantic integration, ICD-10, ICD-O-3BackgroundWith the increasing adoption of electronic health records(EHRs), the amount of data produced at the patientbedside is rapidly increasing. These data provide newperspectives to: create and disseminate new knowledge;consider the implementation of personalized medicine*Correspondence: vianney.jouhet@isped.u-bordeaux.fr1CHU de Bordeaux, Pole de sante publique, Service dinformation medicale,unit IAM, F-33000 Bordeaux, France2Univ. Bordeaux, Inserm, Bordeaux Population Health Research Center, teamERIAS, UMR 1219, F-33000 Bordeaux, Franceand offer to patients the opportunity to be involved inthe management of their own medical data [1]. Secondaryuse of biomedical data produced throughout patient careis an essential issue [2] and is the subject of numerousstudies over several years [16]. Since 2007, the AmericanMedical Informatics Association emphasized the value ofsecondary use of medical data: Secondary use of healthdata can enhance healthcare experiences for individuals,expand knowledge about disease and appropriate treat-ments, strengthen understanding about the effectivenessand efficiency of our healthcare systems, support public© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 2 of 12health and security goals, and aid businesses inmeeting theneeds of their customers [4].In the oncology field, it is necessary to identify anddescribe incident cancer cases within a population inorder to facilitate research and public health monitor-ing. For instance, cancer registries exhaustively recordincident cases of cancer in a given territory (which cor-respond to all new cancer cases occurring over a geo-graphical territory). This task remains time consumingif it is performed manually. As early as 1998, a techni-cal report was drawn up by the International Agency forResearch on Cancer describing the methods used by dif-ferent registries for establishing automated procedures toidentify new cases using available data [7]. Methods havebeen proposed for automatically identifying and register-ing cancers using structured data indexed with standardterminologies [812].However, many different medical specialties are con-tributing to record information in EHRs. As a result,within EHRs, data describing diseases are recordedaccording to multiple heterogeneous terminologies evenfor a single disease occurring in a single patient. Forinstance, in France, reimbursement data use the 10threvision of the International Statistical Classification ofDiseases and Related Health Problems (ICD-10) [13] todescribe diseases, whereas pathology data use either ADI-CAP (a French pathology terminology) or the 3rd edi-tion of the International Classification of Diseases forOncology (ICD-O-3) [14] and data frommultidisciplinarymeetings in oncology use ICD-O-3. Providing an inte-grated access to these disease classifications may improveautomated cancer identification.Although ICD-10 and ICD-O-3 both describe cancerdiseases, they exhibit differences in terms of structureand granularity. Thus, it is necessary to identify or tobuild a resource that allows the integration of cancerdisease classifications, taking into account these hetero-geneities. To achieve this goal, relations must be definedbetween the involved concepts, such as a neoplasm is adisease and has a specified morphology as well as a speci-fied topography. The National Cancer Institute thesaurus(NCIt)provides reference terminology covering vocabularyfor clinical care, translational and basic research, and pub-lic information activities(cited from http://ncit.nci.nih.gov/, visited 2015-01-22). It is described as a controlledterminology which exhibits ontology-like properties in itsconstruction and use [15]. These characteristics openup the possibility [...] in linking together heterogeneousresources created by institutions external to the NCI [16].Thus, the NCIt could be used as a resource to bridge thegap between disease classifications, which are structurallyheterogeneous.However, since 2005, it has been shown on many occa-sions that the NCIt remains flawed [1618] and especiallythat logic-based reasoning over the NCIt should be usedcautiously. On the other hand, re-building a model fromscratch would be time consuming and comes with noguarantee of avoiding inconsistencies. Despite the lim-itations described above, the NCIt contains knowledgethat could be useful for our integration purpose. In thismanuscript, we propose an approach to build a resourcebased on a subset of the NCIt, linking the three axes thatrefer to diseases as described in ICD-10 and ICD-O-3, i.e.,the diagnosis as well as its morphology and its topography.ICD-10Within ICD-10, chapter 2 corresponds to neoplasms.It is divided into four axes depending on the behav-ior of the tumor (namely Malignant neoplasms, In situneoplasms, Benign neoplasms and Neoplasms of uncer-tain or unknown behavior). Within the Malignant neo-plasms block, ICD-10 categories differentiate primarytumors from metastatic secondary tumors. In the sameway as for ICD-O-3, a neoplasm cannot have multi-ple behaviors. ICD-10 describes each neoplastic diseaseas a whole concept represented by a unique code. Forinstance, C50.2: Malignant neoplasm upper-inner quad-rant of breast describes two characteristics of the cancerdisease: The behavior (Malignant) which is part of themorphology description. The site of origin (upper-inner quadrant of breast)which corresponds to the topography.ICD-O-3ICD-O-3 is a multi-axial classification used in cancer reg-istries in order to record the anatomic site (topography)and the morphology of a neoplasm. The morphology iscoded with five digits. The first four digits represent thehistological description and the fifth digit indicates thebehavior (i.e. whether benign ormalignant) of a neoplasm.As a result, it is not possible for a morphology to havemultiple behaviors. The topography code indicates the siteof origin of a neoplasm; in other words, where the tumorarose [14]. From the ICD-O-3 point of view, any mor-phology code can be associated with any topography code.Some tumor morphologies have a usual primary sitebut it is expressly stated that these associations are pro-vided only to help coders and should not be consideredas systematic (and unique) topography-morphology com-binations. An example is given in [14]: An unusual, butpossible, example would be the diagnosis osteo-sarcomaof kidney, for which the kidney topography code (C64.9)would be used instead of bone, NOS (C41.9) [. . . ]. Thus,ICD-O-3 describes a disease by combining the morphol-ogy of the tumor and the topography from where thetumor arises. As a result, each neoplastic disease is notJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 3 of 12described as a whole concept entailed by a unique codewithin ICD-O-3.Concepts involved in ICD-10 and/or ICD-O-3Even if they are called disease classifications, ICD-10 andICD-O-3 are in fact used within EHR for recording diag-noses. The diagnosis is a way for the physician to describethe disease, which corresponds to an evolving process but,in fact, it is not the disease itself. A single disease mayhave multiple diagnoses all along its clinical course (forinstance, an in situ neoplasm may evolve and become amalignant invasive neoplasm) but the disease (process)remains the same. Thus, when used in this context, diseaseclassifications are, in fact, kinds of diagnoses which canbe viewed as opinions about the undergoing disease. Thisassertion is in accordance with the definition proposed byScheuermann et al. in [19] who claim that a diagnosis isa conclusion of an interpretive process that has as input aclinical picture of a given patient and as output an asser-tion to the effect that the patient has a disease of such andsuch a type. A diagnosis is a continuant entity that, oncemade, will survive through time, and is often supplanted byfurther diagnoses. The diagnostic process is thus iterative:the clinician is forming hypotheses during history taking,testing these during physical exam, forming new hypothesesas a result, and so on.In the oncology field, a diagnosis describes two majorfacts about the disease: (i) the type of tumoral cells (Mor-phology) and (ii) its site of origin (Topography). Thus,ICD-10 and ICD-O-3 both allow to record diagnoses buttheir structure differs slightly. As a result, three differentkinds of concepts are involved when considering these twoterminologies: The morphology of the tumor, which is arepresentation of the pathological description of thetumor reported at a given time. Morphology isrepresented within the ICD-O-3 morphology axis. The topography of the tumor, which is arepresentation of the anatomical site of origin of thetumor reported at a given time. Topography isrepresented within the ICD-O-3 topography axis. The diagnosis, which is a representation of thereported description of the tumor and encompassesinformation about both the topography and themorphology of the tumor. Diagnosis is represented assuch within ICD-10 and can be built by combining anICD-O-3 topography and an ICD-O-3 morphology.Because it is not possible to state that a diagnosis isequivalent to either a topography or a morphology, itis obviously not possible to find equivalences betweenconcepts represented within these two terminologies.The unique correspondences that can be found betweenICD-10 and ICD-O-3 concepts are thus a diagnosis (i.e.,an ICD-10 code) mapped to a topography-morphologycombination (i.e., a pair of an ICD-O-3 topography codeand an ICD-O-3 morphology code).The national cancer institute thesaurus (NCIt)NCI Thesaurus (NCIt) is NCIs reference terminology.NCIt provides the concepts used in caCORE and caBIG toestablish data semantics. It covers terminology for clinicalcare, translational and basic research, and public infor-mation and administrative activities. NCIt is also a widelyrecognized standard for biomedical coding and reference,used by a broad variety of public and private partners bothnationally and internationally [20].In the NCIt, topographies are described in theAnatomicstructure, system, or substance axis. Morphologies anddiagnoses are represented within the same hierarchy, sub-sumed by Neoplasm. Thus, no specific axis for tumormorphologies is defined and diagnoses are modeled asanatomic specializations of morphologies. For example,Breast adenocarcinoma is_a Adenocarcinoma is stated in:Breast adenocarcinoma ? Adenocarcinoma? Breast carcinomaSome NCIt concepts are annotated as being mappedto some ICD-O-3 morphologies. For example, Invasiveductal carcinoma, not otherwise specified is annotated asbeingmapped to two ICD-O-3morphology codes (8500/3Infiltrating duct carcinoma, NOS and 8521/3 Infiltrat-ing ductular carcinoma). The semantics of this mappingannotation are not defined (i.e., exact match or anothertype of relationship). In the NCIt, even if the term diseaseis employed, it is not clear whether Neoplasm repre-sents the disease or the diagnosis. For instance, in theNCI term Browser (https://ncit.nci.nih.gov/ncitbrowser/pages/home.jsf?version=16.10e), Neoplasm is defined asA benign or malignant tissue growth. . .  and An abnor-mal mass of tissue. . . . Disease classifications are mainlyused in EHR for diagnoses recording. In the remain-ing part of this manuscript, we use the NCIt conceptNeoplasm as a kind of diagnosis describing the disease.An OWL-DL representation of the NCIt is freely avail-able in the Web ontology Language (OWL) format onthe NCI website (https://cbiit.nci.nih.gov/evs-download/thesaurus-downloads). Although logic-based reasoningcan be made with this OWL-DL representation, someinconsistencies have been identified and it has beenshown that the NCIt should be used cautiously for thispurpose [1618].NCI Metathesaurus [21]The NCI Metathesaurus (NCIm) is a biomedical termi-nology database that covers most terminologies used byNCI for clinical care, translational and basic research,Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 4 of 12and public information and administrative activities [21],including ICD-10 and ICD-O-3. It has been built and ismaintained by the NCI. Its structure and a significant partof its concepts is based on the UMLS Metathesaurus [22].Inside the NCIm, elements coming from different termi-nologies but representing the same biomedical notion aregrouped into the same Concept Unique Identifier (CUI).MethodsWe focused our study on primary tumor descriptions,ignoring metastases and uncertain behaviors. ICD-10 andICD-O-3 do not have a formal representation. In [23],authors recommend to use SKOS to describe the knowl-edge of such resources. In order to bridge these two termi-nologies, it is necessary to identify how concepts that arerepresented within them (diagnosis, morphology, topog-raphy) are related. These relationships should thereforebe represented at the conceptual level so that they couldbe machine readable. Moreover, concepts represented byterminologies should be conceptually defined and relatedto corresponding codes. As a result, the targeted modelremains independent from terminologies to be integrated,thus enabling the integration of other disease classifica-tions. Our approach was to follow the W3C recommen-dations to define formal and semi-formal hybrid models[24] in order to build a model combining SKOS for thedescription of terminologies and OWL for representinginvolved concepts and for defining relationships betweenthese concepts, as proposed in [23]. Figure 1 presents theorganization of the proposed model using Graffoo [25].The methods are composed of three steps: Defining a formal pattern for linking diagnosis,topography and morphology Building a model based on the NCIt corresponding tothe formal pattern Instantiating the model with terminologiesDefining a formal pattern for linking diagnosis,topography andmorphologyIn order to link ICD-10 and ICD-O-3 concepts, it isnecessary to determine which relationships are involvedand how these relationships associate concepts with eachother. A topography-morphology combination in ICD-O-3 leads to a diagnosis description. ICD-O-3 axes canbe viewed as descriptors that, when combined, providenecessary and sufficient information to represent a diag-nosis. For instance, the diagnosis Malignant neoplasm oflower-outer quadrant of breast in ICD-10 can be definedas a malignant neoplasm arising from the lower-outerquadrant of breast (because it is defined as a presumedor stated primary malignant tumor within ICD-10). Asstated above, the topography of a tumor (and more pre-cisely, its primary site) is the anatomical site from whichFig. 1 Graffoo [25] representation of the proposed model. The model is formal and semi-formal hybrid. Terminologies (ICD-10 and ICD-O-3) arerepresented in SKOS. Above them, a formal model is represented in OWL. Every OWL class of the formal model are subclasses of skos:Concept sothat they can be instanciated by terminological artifactsJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 5 of 12a tumor arises. As a result, a diagnosis has a specificrelationship with the topography. The mention of aris-ing from is ambiguous because this relationship impliesthat the tumor arises from the topography as a whole(lower-outer quadrant of breast) or from a part of thistopography (a part of lower-outer quadrant of breast).Indeed, if a digestive systems tumor is reported, it mayrefer to a tumor that originates from a part of the diges-tive system and not from the whole digestive system. Inorder to capture the fact that a primary tumor refers toa primary site as a whole and all its parts, we need spe-cific topography classes. In [26], the W3C describes a wayto represent those reflexive parts (e.g., Class(CarPart_-reflexive complete unionOf(Car CarPart))). This pattern(called S-node) has been proposed for the biomedicaldomain in [27, 28]. Formally, we can define the Malig-nant neoplasm of the lower-outer quadrant of breast as adiagnosis whosemorphology is a malignant neoplasm andwhose primary site is the reflexive part of the lower-outerquadrant of breast. For describing the link between a diag-nosis and its morphology as well as its anatomical site, weneed to introduce the two following relationships (objectproperties in OWL parliance): has_morphology: for modeling the relation between adiagnosis and the type of cells (morphology) that arestated to be involved in the tumor described by thediagnosis. has_primary_site: for modeling the relation betweena diagnosis and an anatomical site (topography) thatis stated to be the origin of the tumor described bythe diagnosis.In description logics, the definition of the Malignantneoplasm of the lower-outer quadrant of breast diagnosiscan be stated as follows:Malignant neoplasm of lower outer quadrant ofbreast ?Diagnosis?? has_morphology.Malignant neoplasm?? has_primary_site.Lower outer quadrant ofbreast Reflexive partIn addition, because of its expressivity, ICD-O-3 pro-vides finer-grained information about the morphology ofdiagnoses than ICD-10 does. For instance, an adenocar-cinoma arising from the lower-outer quadrant of breastcan be reported using ICD-O-3. In ICD-10, there is nocode corresponding to this diagnosis. However, an ade-nocarcinoma being a type of malignant neoplasm, anadenocarcinoma arising from the lower-outer quadrant ofbreast can be defined as a type of malignant neoplasmarising from the lower outer quadrant of breast (which is acoarser grained concept that exists in ICD-10). Formally,this can be expressed in description logics as follows:(Diagnosis)?? has_morphology.Adenocarcinoma?? has_primary_site.Lower outer quadrant of breastReflexive part) ?Malignant neoplasm of lower outer quadrantof breastBuilding a model based on the NCIt corresponding to theformal patternBuilding a part-whole latticeIn order to address the integration of diagnoses (ICD-10)with topographies andmorphologies (ICD-O-3), the NCItrelationship disease_has_primary_anatomic_site is of par-ticular interest. The NCIts definition of this relationshipis: A role used to relate a disease to the anatomical sitewhere the originating pathological process is located. Thedomain and the range for this role are Disease, Disorder orFinding and Anatomic Structure, System, or Substance.This relationship is equivalent to the has_primary_siterelationship defined in the previous subsection. As dis-cussed on page 4, we consider that the primary anatomicsite of a tumor encompasses the site itself and all its parts(this definition is in accordance with is located, whichis mentioned in the NCIt definition of the disease_has_-primary_anatomic_site relationship). In order to makethis description possible, we have built a subsumptionlattice composed of classes defined as the reflexive partof each Anatomic Structure, System, or Substance. Forinstance, the Lower outer quadrant of breast Reflexive partwas defined as follows:Lower outer quadrant of breast Reflexive part ?Lower outer quadrant of breast?? part_of.Lower outer quadrant of breastThe lattice was built with DL-reasoning over classesdefined using two part-whole relationships available inthe NCIt (namely anatomic_structure_is_physical_part_-of and anatomic_structure_has_location).IsolatingmorphologiesIn contrast, no morphology axis is distinguished as suchwithin the NCIt, and it is not possible to find a relation-ship equivalent to the aforementioned has_morphology.However, theNCIt provides amapping between diagnosesand ICD-O-3 morphology codes. We have added classescorresponding to ICD-O-3 morphologies as types of theNCIt concept Findings and, based on the NCIt mappings,Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 6 of 12we have built new refined diagnosis concepts definedaccording to the following model:[NCIt diagnosis concept (refined)] ?[NCIt concept]?? disease_has_finding.[Morphology mapped tothe NCIt concept]For instance, in the NCIt, Adenocarcinoma is mappedto the 8140/3 ICD-O-3 morphology (Adenocarcinoma,NOS) and we defined the corresponding refined NCItconcept as follows:Adenocarcinoma (refined) ?Adenocarcinoma?? disease_has_finding.Adenocarinoma, NOS(ICD-O-3 morhology)Each of the morphologies were classified dependingon their tumoral behavior as described in ICD-O-3(i.e., benign, malignant primary, in situ, malignantmetastatic, unknown whether benign or malignant,unknown whether primary or metastatic).Building themodelUsing reflexive part anatomic concepts and morpholo-gies and adapting the description logics expressions pro-posed in page 4, we have defined a formal pattern todescribe relationships between diagnoses, morphologiesand topographies within the derivative NCIt. In descrip-tion logics, the pattern is the following:Diagnosis ??disease_has_finding.Morphology?? disease_has_primary_anatomic_site.Topography_Reflexive_PartBased on the defined pattern, we implemented andexecuted the following algorithm:+ For each (Morphology identified ?[Morphology])+ For each (Topography_Reflexive_Part identified? [Topography])+ Build [expression] of the form:? disease_has_finding.[Morphology]?? disease_has_primary_anatomic_site.[Topography]+ If at least one subclass of [expression] existsin the NCIt then+ Build the Diagnosis class defined asequivalent to [expression]Finally, we have implemented the model presented inFig. 1 containing:1. Morphologies,2. Reflexive part topographies,3. Diagnoses identified by the aforementionedalgorithm.Instantiating themodel with disease classificationsICD-O3 ICD-O3 morphologies were represented asinstances of the built-in morphology classes. ICD-O-3topographies were represented as instances of the built-inreflexive part topographies. Reflexive part topographies tobe instantiated by ICD-O-3 topographies were identifiedas follows:1. Identify mappings between ICD-O-3 topographiesand NCIt concepts having the same CUI within theNCIm,2. Define these codes as instances of the correspondingNCIt concept,3. Retrieve the corresponding reflexive part topographyafter DL-reasoning.ICD-10 ICD-10 codes were represented as instances ofbuilt-in diagnoses classes. Diagnoses to be instantiated byICD-10 codes were identified as follows:1. Identify mappings between ICD-10 codes and NCItconcepts having the same CUI within the NCIm,2. Define concepts corresponding to ICD-10 codesbased on the NCIt definition (by adding a restrictionfor the primary site based on the NCIt conceptformal definition) and ICD-10 (by adding arestriction for the behavior) semantics. For instance : Breast, Unspecified (C50.9) is a malignantprimary neoplasm within the ICD-10classification. Breast, Unspecified (C50.9) has the same CUI asMalignant Breast Neoplasm (C9335) within theNCIm. Malignant Breast Neoplasm (C9335) has asassociated primary site Breast (C12971) withinthe NCIt. The built expression describing Breast,Unspecified (C50.9) was then:Malignant Breast Neoplasm?? disease_has_finding.Malignant primaryneoplasm?? disease_has_primary_anatomic_site.Breast3. Retrieve the corresponding diagnosis afterDL-reasoning.Evaluation of the modelThe National Cancer Institute provides, within theSurveillance, Epidemiology, and End Results (SEER) Pro-gram, a set of tools for ICD conversions [29]. We usedJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 7 of 12the 2014-05-08 conversion file of ICD-O-3 to ICD-9-CM, to ICD-10 (Causes of Death) and to ICD-10-CM(available at http://seer.cancer.gov/tools/conversion/) as agold standard for evaluating how ICD-O-3 and ICD-10could be related. Based on this file, we have rebuilt ICD-O-3 topography-morphology combinations mapped toICD-10 codes. A 2-step evaluation was then performed: For each combination, we queried the proposedmodel in order to evaluate how many branches of thediagnosis lattice were instantiated by both the ICD-10code and the topography-morphology combination. We tried to build mappings based on the proposedmodel with a simple algorithm (ICD-10 codes andtopography-morphology combinations with theminimum hierarchical edge-based distance wereconsidered as mapped) and compared it with the goldstandard.ResultsAll the analyses were processed over the OWL-DL ver-sion of the NCIt (14.11d) available at http://evs.nci.nih.gov/ftp1/NCI_Thesaurus/.Built model based on the NCItA total of 6720 topographies involved in at least onediagnosis definition was identified and the correspondingtopographies reflexive parts were introduced in the NCItin order to build the topography reflexive parts lattice(Section: Building a part-whole lattice). A total of 1120NCIt codes was identified as being related to 1094 ICD-O-3 morphology codes. The 1094 corresponding morphol-ogy classes were added to the model and automaticallyclassified under six general morphology classes dependingon their behavior leading to a set of 1100 possible mor-phologies. Combining the 1100 morphology classes withthe 6720 reflexive part topographies, 7392000 expressionswere built. A total of 20133 (0.27%) expressions sub-suming at least one NCIt code were identified and thecorresponding classes were introduced in the model asdiagnoses.Instantiating the model with disease classificationsTable 1 presents the part of each terminology that wascovered by the final model. The numbers of codes to beintegrated were: 409 ICD-O-3 topographies 873 ICD-O-3 morphologies (excluding /6 Malignantneoplasms, stated or presumed to be secondary and/1 Neoplasms of uncertain and unknown behavior) 727 ICD-10 neoplasms (excluding C81-C96Malignant neoplasms, stated or presumed to beTable 1 Part of ICD-O-3 and ICD-10 terminologies integratedwithin the final modelTotal number Instantiating the final modelICD-O-3 Topographies 409 278 (68.0%)ICD-O-3 Morphologies 873 860 (98.5%)ICD-10 727 302 (41.5%)ICD-10 Benign 180 73 (40.5%)ICD-10 In situ 66 22 (33.3%)ICD-10 Malignant 481 207 (43.0%)secondary and D37-D48 Neoplasms of uncertain orunknown behavior)Using the NCIm, 298 ICD-O-3 topography codes werelinked to 540 NCIt codes. Within these NCIt codes, 29were not subclasses of Anatomic Structure, System, orSubstance. Among the 298 ICD-O-3 topography codes,20 were related only to these 29 codes and were thenexcluded (e.g., C05.1 Soft palate, NOS was erroneouslymapped to Malignant Soft Palate Neoplasm). Thus, 278topography codes were finally included within the modelas instances of the corresponding NCIt codes and clas-sified as instances of topography reflexive parts afterDL-reasoning. Using the NCIm, 302 ICD-10 codes werelinked to NCIt codes. Building the corresponding expres-sions and after DL-reasoning, we were able to add 302ICD-10 codes as instances of 380 diagnoses.Characteristics of the final modelThe resulting model is constituted of 113643 axioms,including 27953 classes (6720 topographies, 1100 mor-phologies and 20133 diagnoses). A total of 1440 codeswere instantiated (278 ICD-O-3 topographies, 860 ICD-O-3 morphologies and 302 ICD-10 codes).Within the model, a significant part of ICD-10 (51%)and ICD-O-3 topography codes (28%) are instances ofmultiple classes (Table 2). This situation arises when thehierarchy of diagnoses within the NCIt is not in accor-dance with the topography or the morphology that weused to describe them. For example Colon CavernousHemangioma is a direct subclass of the following expres-sions: ? disease_has_finding.Cavernous hemangioma? ? disease_has_primary_anatomic_site.ColorectalRegion Reflexive part ? disease_has_finding.Cavernous hemangioma? ? disease_has_primary_anatomic_site.ColonReflexive part ? disease_has_finding.Hemangioma,NOS? ? disease_has_primary_anatomic_site.ColorectalRegion Reflexive partJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 8 of 12Table 2 Number of codes instantiating multiple classes in themodelN Instances of multiple classesICD-O-3 Topographies 278 79 (28.4%)ICD-O-3 Morphologies 860 0 ( - %)ICD-10 302 153 (50.7%)ICD-10 Benign 73 26 (35.6%)ICD-10 In situ 22 22 ( 100%)ICD-10 Malignant 207 105 (50.7%) ? disease_has_finding.Hemangioma,NOS? ? disease_has_primary_anatomic_site.ColonReflexive partThe explanation for this situation is twofold: (1) there isneither an anatomic_structure_has_location relationship,nor an anatomic_structure_is_physical_part_of relation-ship between Colon and Colorectal Region within theNCIt; (2) Colon Cavernous Hemangioma is describedas having these two anatomic structures as a primarysite. On the other hand, through the NCIt diagnosis lat-tice, Colon Cavernous Hemangioma is described as beinga subclass of the concepts Cavernous hemangioma andHemangioma, NOS.Comparison with the SEER conversion fileBased on the SEER conversion file, excluding metastaticand uncertain behaviors from ICD-10 and ICD-O-3morphologies, we were able to build 103950 mappingsbetween an ICD-10 code and an ICD-O-3 topography-morphology combination. Due to the absence of somecorrespondences within the NCIm between the NCIt andICD10 and between the NCIt and ICD-O-3, some ICD-10 and ICD-O-3 codes do not instantiate any class inthe derivative model. As a result, 59% of these map-pings could not be evaluated (because the ICD-10 code,the ICD-O-3 topography or the ICD-O-3 morphologywas missing). Table 3 presents the results of the evalu-ation over the 42260 mappings combining codes whichinstantiate classes within the resulting model. The modelrelates 100% of the mappings through at least a diagnosis.A significant part of these mappings (36%) are related tomore than one branch of the diagnosis lattice, especiallyfor hematopoietic tumors (70%). Using the simple algo-rithm described above, the model was able to identify 42%of the mappings of the SEER file (61% for solid tumorsand 5% for hematopoietic tumors). A quarter of thesetopography-morphology combinations were also mappedto another ICD-10 code, which is not consistent with theSEER file.DiscussionImplementedmethods to build the modelWe achieved to automatically build a model based onthe NCIt, describing topographies, morphologies anddiagnoses that can be instantiated by both ICD-O-3 andICD-10 codes. As no morphological axis is availablewithin the NCIt, we have adapted the NCIt by addingconcepts corresponding to ICD-O-3morphologies, whichwe related to the corresponding diagnoses (based on theICD-O-3 annotation of the NCIt).For the description of topographies, we have built anorgan reflexive part lattice that enables the description ofa primary site as encompassing the site itself and all itsparts. These reflexive parts have been proposed for thebiomedical domain in [27].The diagnoses lattice was then automatically generatedby DL-reasonners based on the topography and morphol-ogy lattices avoiding is_a overloading [17]. As a result, theobtained diagnoses subsumption lattice is a valid formalrepresentation of diagnoses hierarchy in respect to thedefinitions of classes contributing to their descriptionIn order to instantiate the model, we have used theNCIm to identify links between the NCIt and the termi-nologies (namely, ICD-10 and the ICD-O-3 topographyaxis). For ICD-10, we had to add a restriction based onthe semantics available within the ICD-10 classificationin order to ensure that primary tumors were describedaccording to a primary site. The resulting model could notTable 3 Comparison with the SEER conversion program according to the tumor type (hematopoietic and solid tumors) and thenumber of branches of the diagnosis lattice that are identified for an ICD-10 code / ICD-O-3 combinationAll Hematopoietic tumors Solid tumorsN = 42260 (%) N = 14213 (%) N = 28047 (%)Related in the model* 42260 (100.0) 14213 (100.0) 28047 (100.0)More than 1 brancha 15234 (36.1) 9910 (69.7) 5324 (18.9)Mappings rebuilt from the model** 17766 (42.0) 739 (5.2) 17027 (60.7)Non unique mappingsb 4886 (27.5) 333 (45.1) 4553 (26.7)*Related in the model means that there is at least a common diagnosis inside the model that is instantiated by both the ICD-10 code and the ICD-O-3 combination**Mappings rebuilt from the model corresponds to the mappings that we were able to rebuild automatically from the modelaMore than 1 branch means that there is more than one branch of the diagnosis lattice that was instantiated by both the ICD-10 code and the ICD-O-3 combinationbNon unique mappings means that the topography-morphology combination was also mapped to another ICD-10 code (inconsistent with the SEER file)Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 9 of 12be instantiated completely by ICD-10 and ICD-O-3 codesfor different reasons: The NCIt completeness for describing diagnoses. Asour method relies on NCIt diagnoses, the resultingclasses which were built depend on their existencewithin the NCIt (e.g., C00.1 External Lower Lipmalignant neoplasm is not available within the NCIt). The NCIm provides a way to identify commonconcepts using the CUI but it can be incomplete orwrong (e.g., 20 ICD-O-3 topographies were mappederroneously to non-anatomic concepts).However, when the codes were found, we were ableto identify a common diagnosis for all cases describedin the SEER conversion program and, using a simplealgorithm, 42% of the SEER mappings (corresponding tocodes instantiating the model) could be rebuilt from themodel. Our aim is not to enable conversion between codesbut to provide a machine usable and semantically inte-grated view over them. From this perspective, the modelis consistent because it provides links between ICD-10diagnoses and ICD-O-3 topography-morphology com-binations when they exist within the SEER conversionfile. Moreover, the model describes many more possiblerelationships between diagnoses than the SEER conver-sion program does. For instance, in the latter, there isno relationship between Adenocarcinoma, NOS  Colon,NOS (C18.9  M8140/3) and Malignant neoplasm of rec-tosigmoid junction (C19.9) whereas our model identifiessuccessfully that they are both instances of Malignant,primary site - Large Intestine Reflexive part.Choice of the NCItIn the biomedical field, other description logics-based ter-minologies exist. Specifically, SNOMED-CT® provides notonly topography, morphology and diagnosis dimensionsbut also implements relationships between these con-cepts. However, the NCIt is specific to the oncology fieldand provides useful knowledge related to neoplasm diag-noses. The NCIt is freely and easily accessible.In contrast,SNOMED-CT has amuchmore restrictive affiliate licenseagreement and it is not easily accessible for countrieswhich are notmembers of the International Health Termi-nology Standards Development Organisation (IHTSDO).In addition, it has been shown that SNOMED-CTs for-mal representation suffers from the same flaws [30, 31]as the NCIt and has to be used cautiously while needinglogic-based reasoning. Thus, a similar evaluation could becarried out on SNOMED-CT in order to estimate whetherit could be useful for integrating disease classifications inoncology and to compare the results with what was foundwhen using the NCIt.Limitations of the NCIt for integration purposesIn [18], Schultz et al. discussed that the OWL-DL ver-sion of the NCIt may lead to unexpected results whichwere not visible due to the lack of use cases needing logic-based reasoning over the OWL-DL version of the NCIt.The integration of heterogeneous disease classificationscorresponds to such a use case. We have identified somelimitations due to inconsistencies.On the one hand, the NCIt provides concepts describ-ing cancer diagnoses and, on the other hand, conceptsdescribing the tumor topography. It also provides rela-tionships which are involved in topography-morphologycombinations, themselves expected to be equivalences ofdiagnoses. Its formal representation and the availabilityof an OWL version enable reasoning and the imple-mentation of DL-queries. However, some intrinsic char-acteristics prevent its direct use for the integration ofcancer disease classifications: (i) the absence of distinctionbetween morphologies and diagnoses; (ii) diagnosis con-cepts described as having a specific primary site but notits parts. We have proposed a method to address theseissues and to automatically build a consistent model basedon the NCIt and the intrinsic semantics available withinICD-O-3 and ICD-10.The obtained model representing diagnosis was clas-sified using DL-reasoning ensuring concistency of thesubsumption lattice. Linking these derivative consistentclasses with diagnosis as represented within the NCIt canbe used as an auditing tool. During the building pro-cess, a significant part of NCIt concepts was retrieved assubclasses of multiple diagnosis classes. As a result, thecorresponding ICD-10 codes were defined as instancesof multiple diagnosis classes and 36% of SEER mappingsevaluated were retrieved as being related to more thanone branch in the diagnosis lattice. For instance, the ICD-10 code C18.0Malignant neoplasm: Caecum was mappedto the NCIt concept C9329 Malignant Cecum Neoplasm,which is related to multiple anatomic sites: Gastrointesti-nal System,Cecum,Colon, Intestine andColorectal Region.As there is no relationship between Cecum, ColorectalRegion and Colon within the NCIt (except that they arepart of the large intestine), C18.0 instantiates the followingclasses: Malignant, primary site  Cecum Reflexive part Malignant, primary site  Colon Reflexive part Malignant, primary site  Colorectal Region ReflexivepartTwo issues can be identified: (i) Malignant Cecum Neo-plasm should not have Colon as an associated anatomicsite within the NCIt because Cecum is neither a part,nor a subclass of Colon, (ii) Cecum and Colon should berelated to Colorectal Region. The former is due to is_aJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 10 of 12overloading and has been discussed in [17]. The latterissue is due to the lack of part_of relationships withinthe NCIt. Another important issue can be identified forin situ neoplasms. 100% of the in situ ICD-10 codes areinstances of more than one diagnosis within the model.The NCIt asserts that a Carcinoma In situ is a Carcinoma,which seems to be true. However, in the NCIt, Carcinomais related to the Carcinoma, NOS ICD-O-3 morphology(having an invasive behavior) and Carcinoma In situ isrelated to the Intraepithelial carcinoma, NOS ICD-O-3morphology (having an in situ behavior). Consequently,the subsumption relationship between Carcinoma In situand Carcinoma is not consistent because a tumor cannotbe both invasive and in situ at the same time. For instance,D05 Carcinoma in situ of breast is mapped to the NCItconcept C3641 Stage 0 Breast Cancer, which is related tothe Intraepithelial carcinoma, NOS and Epithelioma, NOSICD-O-3 concepts through the NCIt lattice. As a result,D05 instantiates the following classes: Intraepithelial carcinoma, NOS - Breast Reflexive part Epithelioma, NOS - Breast Reflexive partBecause Intraepithelial carcinoma, NOS has an in situbehavior and Epithelioma, NOS has a malignant, invasivebehavior, it is not consistent to be an instance of thesetwo diagnoses. This issue emphasizes erroneous map-pings that may exist between ICD-O-3 and the NCIt dueto ambiguous labels. A simple solution to this problemwould be to add a concept representing the Carcinomacategory of which both Carcinoma a Carcinoma In situshould be subclasses.It is noteworthy that these patterns, which are mainlydue to is_a overloading, can easily be retrieved by search-ing for codes which are instances of multiple diagnoses.By linking ICD-O-3 and ICD-10 terminologies to theNCIt and adding some restrictions based on their ownsemantics, ourmethodmay provide a useful auditing solu-tion. Identifying those codes within the resulting modelmay enable discovery within the NCIt of: (i) structuralinconsistencies (e.g., Malignant cecum neoplasm relatedto Colon), (ii) missing concepts (e.g., Carcinoma invasivethat can be related to the ICD-O-3 concept Carcinoma,NOS) and (iii) missing relationships between concepts(e.g., Cecum which should be defined as a part of Colorec-tal Region).In order to build the organ reflexive part lattice, partsof anatomical concepts were identified using transitivepart-whole properties available within the NCIt (namelyanatomic_structure_is_physical_part_of and anatomic_-structure_has_location). This results in including cellparts as (indirect) subclasses of topographies (e.g. BirbeckGranule part_of Langerhans Cell part_of Epidermis part_-of Skin). This would suggest that we allow a neoplasmto have Birkbeck Granule as primary site. Since therange of the disease_has_primary_anatomic_site propertyincludes cells parts, such an assertion is allowed in theNCIt. Thereby, the built hierarchy is in accordance withthe NICt representation of primary sites. As discussed in[28], the transitivity of the part_of property remains con-troversial. For instance, in [32], Rescher stated that Apart (i.e., a biological sub-unit) of a cell is not said to bea part of the organ of which that cell is a part, which isin contradiction with that stated within the NCIt. How-ever, diagnoses retained in the derivative model wherethose subsuming at least an NCIt concept so that diag-nosis definitions remain realistic (because NCIt describesonly existing, even if sometimes rare, tumors). Neverthe-less, further work should be done in order to address thisissue. In this context, patterns proposed by Schulz andHahn in [28] are to be investigated.PerspectivesThe NCIt is known to contain some inconsistencies.Thus, the OWL-DL version of the NCIt should be usedcautiously. However, this resource is helpful in order tobuild a formal model for integrating heterogeneous can-cer disease classifications. Indeed, the NCIt remains a richknowledge resource and, as shown in this work, it is pos-sible to extract parts of this resource and reorganize themso as to correct some of these inconsistencies (such as is_aoverloading). Even if classes introduced in the derivativemodel are consistent (they have been classified based ontheir formal definition), instantiating them with terminol-ogy codes can lead to misclassifications. Our approachfor classes instantiation is based on the classification ofNCIt concepts within the derivative model, which inturn depends on the formal definition of NCIt concepts.We have proposed an approach, which identified possiblemisclassifications thanks to multiple classes instantiationby a single code. While this approach enables to findinconsistencies, there is a need for methods capable ofselecting the class that should be instantiated ultimately inthis situation.Using the NCIm CUI to map the NCIt to ICD-O-3 andICD-10 can be useful but is not enough because mappingsare missing and some are inconsistent. We are currentlyworking on a method based on the NCIm to identifyadditional mappings.SNOMED-CT® exposes comparable structural charac-teristics with diagnosis, anatomic and even morpho-logical concepts as well as relationships between them.Future work will explore SNOMED-CT as a resourcefor integration purpose. As SNOMED-CT is known tohave the same inconsistencies as NCIt, we will studythe feasibility of using both SNOMED-CT and the NCItto build a consistent model addressing semantic andJouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 11 of 12structural heterogeneities between disease classificationsin oncology.Topographies representation needs to be refined inorder to avoid inconsistencies and define consistent lev-els of granularity for the propagation of the disease_-has_primary_anatomic_site property. The FoundationalModel of Anatomy (FMA) Ontology [33] is a domainontology that represents a coherent body of explicit declar-ative knowledge about human anatomy [34]. Furtherwork will explore the ability to define these topographiesbased on the FMA.The main goal of this work is to provide a consistentresource for the integration of heterogeneous disease clas-sifications in oncology. While our approach based on theNCIt seems promising, two main limitations have beendiscussed (misclassification of codes within the deriva-tive model and incomplete coverage due to NCIm map-ping methods). The proposed pattern for the integrationof disease classifications in oncology enables to extractknowledge from available resources. We will apply thesame approach to other resources (e.g., SNOMED-CTand FMA) so as to enrich the derivative model. Thisfuture work will allow a better coverage and we will takeadvantage of existing links between concepts within theseresources (i.e., anatomical descriptions from the FMA)and between resources (i.e., mappings available within theNCIm).In addition, based on this derivative model, algorithmsfor disease identification can be built. This resourcecan manage heterogeneity by providing an integratedview of diagnoses recorded in EHRs. As a result, algo-rithms based on classes of the derivative model can usetransparently available data coded with disease classifica-tions and focus on building consistent rules for diseaseidentification.ConclusionWe have proposed a method to automatically build amodel for integrating ICD-10 and ICD-O-3 based onthe NCIt. The resulting derivative model is a con-sistent machine understandable resource that enablesan integrated view of these heterogeneous terminolo-gies. The NCIt structure and the available relationshipscan help to bridge disease classifications taking intoaccount their structural and granular heterogeneity. How-ever, (i) inconsistencies exist within the NCIt leadingto misclassifications when instantiating the derivativemodel with terminologies, (ii) the derivative modelonly integrates a part of ICD-10 and ICD-O-3. TheNCit is not sufficient for integration purpose and fur-ther work based on other termino-ontological resources isneeded in order to enrich the model and avoid identifiedinconsistencies.Additional fileAdditional file 1: The derivative model is available at: https://github.com/vianneyJouhet/ModelTerminologyIntegrationOncology/tree/master/data. (OWL 31932 kb)FundingNot applicable.Availability of data andmaterialsData sharing not applicable to this article as no datasets were generated oranalysed during the current study.Authors contributionsVJ developed and implemented methods, conducted the analysis, interpretedresults and wrote the manuscript. FM developed methods, interpreted resultsand contributed to the manuscript. BB contributed to methods, interpretedresults and contributed to the manuscript. FT contributed to methods,interpreted results and contributed to the manuscript. All authors read andapproved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Received: 12 August 2016 Accepted: 11 January 2017The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38DOI 10.1186/s13326-017-0138-9RESEARCH Open AccessMultiple kernels learning-based biologicalentity relationship extraction methodXu Dongliang1, Pan Jingchang1* and Wang Bailing2From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: Automatic extracting protein entity interaction information from biomedical literature can help tobuild protein relation network and design new drugs. There are more than 20 million literature abstracts included inMEDLINE, which is the most authoritative textual database in the field of biomedicine, and follow an exponentialgrowth over time. This frantic expansion of the biomedical literature can often be difficult to absorb or manuallyanalyze. Thus efficient and automated search engines are necessary to efficiently explore the biomedical literatureusing text mining techniques.Results: The P, R, and F value of tag graph method in Aimed corpus are 50.82, 69.76, and 58.61%, respectively. The P,R, and F value of tag graph kernel method in other four evaluation corpuses are 25% higher than that of all-pathsgraph kernel. And The P, R and F value of feature kernel and tag graph kernel fuse methods is 53.43, 71.62 and 61.30%,respectively. The P, R and F value of feature kernel and tag graph kernel fuse methods is 55.47, 70.29 and 60.37%,respectively. It indicated that the performance of the two kinds of kernel fusion methods is better than that of simplekernel.Conclusion: In comparison with the all-paths graph kernel method, the tag graph kernel method is superior in termsof overall performance. Experiments show that the performance of the multi-kernels method is better than that of thethree separate single-kernel method and the dual-mutually fused kernel method used hereof in five corpus sets.Keywords: Tag-graph kernel, Entity relationship extraction, Multi-kernels learingBackgroundThere are more than 20 million literature abstractsincluded in MEDLINE, which is the most authoritativetextual database in the field of biomedicine.The biomed-ical literature is difficult to detect manually because ofgrowing number of papers. Thus biomedical entity rela-tionship extraction is necessary to analysis biomedicalliterature.Biomedical entity relationship extraction is theextraction of inter-entity specific semantic relationships intext [1, 2]. Besides, it is benefit for semantic similarity [3],biological network construction [4, 5] and ontology termprediction [6, 7].*Correspondence: pjc@sdu.edu.cn1School of Mechanical, Electrical and Information Engineering, ShanDongUniversity, WenHua West Road, 264209 WeiHai, ChinaFull list of author information is available at the end of the articleIn the biomedical texts, the entity relationships containgene-disease association [810], drug-drug interaction[1113], protein-protein interaction. Biomedical relationextraction aiming to automatically discover relations fromthese biomedical articles with high efficiency and accu-racy, is becoming an increasingly well understood alterna-tive to manual knowledge discovery. In this article, entityrelationship extraction refers to the extraction of entityrelationship that appears in the same sentence. Consider-ing the extraction of protein interaction relationships asan example, as shown in Fig. 1. Sentence is a sentencecomprising a natural language in the biological literature,i.e., an object to be extracted; Protein means a biologi-cal entity named protein, which is present in the sentenceto be extracted, and three proteins coexist in the sen-tence in the figure, namely,IL-8,CXCR1 and CXCR2,respectively. Candidate Named Entity Pair refers to the© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 2 of 79Fig. 1 Sampling example of protein interaction (The PMID of the literature where the sentence is found is 23041326, and PMID refers to the retrievalnumber biological literature coded by PubMed)candidate relationship pairs comprising two proteins andthree candidate entity relationship pairs contained in thesentence, as shown in the figure, two of which are cor-rect protein relationship pairs. These relationship pairsare marked by two actual performance arrows in thefigures. The entity relationship extraction is the accurateextraction of the two correct entity relationship pairs.A knowledge network of biological entity can be pre-dicted and established by extracting biological entity rela-tionship [14]. A heavily studied area in biological textmining concerns the relationships known as protein-protein interactions (PPI). Massive PPI have accumulatedcontinuously with the exponential growth of biomedicalliterature.The remainder of the paper is organized as fol-lows: Section II reviews the related work. Section III isoverview of our approach, which contains introductionof our approach (A type of tag graph kernel method),Characteristics-based kernels, extension dependency pathtree kernel and fused kernel method. In section IV, weconstruct an experiment to evaluate our approach andfused kernel method. Section V is our conclusion.Biological entity relationship extraction methods canbe categorized into three categories statistical machinelearning method [15, 16], co-occurrence-based [17, 18]and pattern-based method [19, 20].The co-occurrence-based method is a graphical repre-sentation of relationships between terms [21, 22]. Antonoet al. [23] proposed new method known as WeMine-P2P based on WeMine Aligned Pattern Clustering algo-rithm which discovers and identifies the localized andco-occurring conserved patterns and regions allowingvariable length and pattern variations.Although the co-occurrence-based method is simpleand easy to use, the hypothesis depended on by thismethod fails to completely reflect the actual situationof massive and complicated biological texts, thereforeleading to a relatively poor accuracy. Therefore, the co-occurrence-based method is usually applied to the crudeextraction stage, indicating that all candidate relationshippairs are extracted. The more accurate extraction of entityrelationships requires fusing other information to filterthe extracted candidate relationship pairs.The patterns defined are used to match the labeledsequence in the pattern-based methods.The pattern-based method contains two methods: the method basedon extraction-pattern [24] and the method based on tem-plate [25]. The extraction-pattern-based method summa-rizes entity relationship to obtain several extraction rulesin the texts by using the natural language processing tool.The template-base method explores the entity relation-ships from the aspect of syntax or part of speech tosummarize a series of templates by utilizing the naturallanguage processing. Peng et al. [26] proposed a pattern-based biomedical relation extraction system with a newframework. There are three characteristics: 1) generatingpatterns by adjusting syntactic variations, 2) improvingthe coverage of patterns by using sentence simplification,3) the referential relations can be identified. Some sys-tems which are implemented by the pattern-base methoddepend on pre-defined patterns at the surface textuallevel [2729].Other parsers are used with hand-craftedpatterns [3032].Compared with the above two methods, machinelearning-based approaches which are driven by data andset of annotated corpora are effective [3336]. But thequality and the number of annotated corpora are signifi-cant effort to the performance of systems.Machine learning-based approaches include the fol-lowing two ways: supervised-machine-learning-basedmethod [37] and semi-supervised-machine-learning-based method [38, 39]. Supervised machine learningmethods have been employed with great success in PPIextraction. However, they usually require a large amountof annotated data for training which are expensiveto obtain in practical applications. Kamada et al. [37]The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 3 of 79proposed a method to predict strengths of PPIs byemploying protein domain information. Jiang et al. [38]proposed a multi-label correlated semi-supervisedmachine learning method. It can effectively solve theproblem of labeled data by exploring the intrinsicrelationship between related classes.The semi-supervised-machine-learning-based methodincludes the method based on characteristic [40, 41] andthe method based on kernel [42, 43].In this paper, a type of tag graph kernel method forextracting protein relationship was proposed and com-bined with feature-based kernel and extension path graphkernel into a fused kernel learning method.MethodsIn this article, the kernel method is used as a functionto calculate the similarity between two objects. We usedthree kernels to calculate the inter-entity relationshipsfrom three aspects, which can avoid losing importantfeatures and strengthen similarity measurement.Characteristics-based kernelsCharacteristic selection is the main work of usingcharacteristic-based kernel function for extracting theprotein interaction relationships, where lexical item fea-ture, entity distance and keyword are regarded to features.1) Item featureIn this work, we used the following three types of key-word item features: the keyword items included in the twoprotein entity names, the keyword items between the twoprotein entity names, and the keyword items around thetwo protein entity names.One protein name may contain multiple words, such asthe sentence in Fig. 1, where the bold part indicates a pro-tein entity name, and its characteristic value in the char-acteristic vector can be denoted as a1_(IL)-8, a2_CXCR1,and a3_CXCR2.In case that lexical item between two protein entitynames is absent, then the characteristics are considereddull. Such as, in the sentence in Fig. 1, the word and"between protein CXCR1 and protein CXCR2 is expressedas b1_and in the characteristic value in the characteristicvector.Given the two proteins, CXCR1 and CXCR2, in the sen-tence in Fig. 1, the three words at the left side of CXCR1are through, their and receptors and their character-istic values in the characteristic vector can be expressed asl1_through, l2_their, l3_receptors. Lexical item is absent atthe right side of CXCR2, and this feature item is set to dull.2) Keyword fetureMany words (keywords) around or between two proteinentities can designate the protein relationship, includ-ing has and receptors. In this paper, when a keywordemerges around or between two proteins, the keywordis inserted to the keyword form (there are about 600keywords in the keyword form). As for the sentence inFig. 1, the corresponding key word, receptors are foundin the key word form, and its characteristic value in thecharacteristic vector is expressed as k_receptors.3) Entity distance entityThe number of interval words between two proteins iscalled distance. The shorter the distance, the closer therelationship. Therefore, a shorter distance between twoproteins demonstrates a higher possibility of their inter-action. If the inter-entity distance is equal to or less thanthree words, then the corresponding characteristic valueis expressed as d_3; if the inter-entity distance is greaterthan three words but equal to or less than eight words,then the corresponding characteristic value is expressedas d_8; if the inter-entity distance is greater than eightwords but equal to or less than 15 words, then the corre-sponding characteristic value is expressed as d_15; if theinter-entity distance is greater than 15 words, then thecorresponding characteristic value is expressed as d_16.The characteristics of two protein entities (IL)-8 andCXCR1 extraction characteristics in the sentence in Fig. 1are expressed in Table 1.In this work, we employed the radial-based functionas the kernel function for calculating the feature vector(Formula (4)), in which s indicates the covariance matrix.K(x, y) = exp[?||x ? y||22s2](1)Extension dependency path tree kernelFormula (5) is the definition of extension path dependencypath tree kernel which is one of convolution tree kernel(c which is in the lower right corner is convolution). For-mula (5) shows that the tree structure is the representationof the protein entity. And the similarity of semantemebetween syntax analysis tree T1 and T2 is calculated by thesame number of structural subtree. Calculation process isas follows: first, the big tree is broken down into manydifferent sub-trees; second, calculating the similarities ofthese sub-trees; third, the similarity of the big tree is got byTable 1 (IL)-8 and CXCR1 characteristicsCharacteristic name Characteristic valueLexical item in the two a1_(IL)-8, a2_CXCR1Protein namesLexical item between the b1_has, b2_an, b3_important,. . .Two protein names b17_their, b18_receptorsLexical item around the l1_Interleukin, r1_andTwo protein namesKey word feature k_receptorsEntity distance entity d_16The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 4 of 79summing the similarity of the sub-trees. The dependencepath tree kernel [44] and the shortest path tree kernel [45]is two of classical convolution tree.Kc(T1,T2) =?n1?N1?n2?N2(n1, n2) (2)In this article, original dependency path tree ker-nels are selected for the extension to form the tensiondependency path tree kernels. A dependence relationshipanalysis is conducted (the analysis process is shown inFig. 2) using The expression of rsfA is under the con-trol of both ENTITY1 and ENTITY2. as example. Thepath tree between ENTITY1 and ENTITY2 is (DEPEN-DENCY(CONJ(ENTITY1,ENTITY2))). Apparently, theinformation of this tree is insufficient for the judgment ofthe inter-entity relationship. The solution provided herebyis used to extend the length of the dependency path whenthe path length is less than three. The path betweenENTITY1 and ENTITY2 in the above example canbe extended into (DEPENDENCY(PREP(control, of ))POBJ((of, ENTITY1)) (CONJ(ENTITY1, ENTITY2))).The algorithm is shown in Algorithm 1.Algorithm 1 Ext_Dep_Path_Sim(n1,n2)Input: n1,n2Output: Similarity of T1 and T21: if (the generator between n1 and n2 is defferent)2: (n1, n2) = 03: else if(n1 and n2 is marked as pre-terminal)4: (n1,n2) = 1 × ?5: else recursively calculate the following formula6: (n1,n2) = ?Nl(n1)?k=1(1+(cl(n1,k),cl(n2,k)))7: End ifWhere, n1 and n2 is root node of T1 and T2;?(0< ? < 1)is the attenuation factor;Nl(n1) at line 06 is the numberof child nodes of n1; n1 and n2 have the same genera-tive, so Nl(n1) = Nl(n2); In which cl(n,k) is the kth childnode of node n; (cl(n1,k),cl(n2,k)) represents calculat-ing the number of same subtrees between tree T1 and T2by a recursive algorithm. Hence, the time complexity ofalgorithm is O(n1log(min(n1,n2))).The function value between the same trees is muchlarger than that of different trees when the scale of thetree is very large. We adopted two ways to stop the func-tion value become too much large: a) The function valueis normalized by formula(6); b) In order to reducing theimpact of subtree scale, we imported the attenuation fac-tor ? to multiple the similarity contribution of the subtreeon its father node.K ?(T1,T2) = K(T1,T2)?K(T1,T1)K(T2,T2) (3)Tag Graph kernelDefinition 1 Graph kernel: set G as a finite or infinitegraph set, and function ? : G×G ?R is called one graphkernel. In the presence of one Hilbert space (which is prob-ably infinitely dimensional) F and one mapping ? : G?Fthus, all the points g, g? ?G, ?(g,g?)=< ?(g), ?(g?)> and< ·, · > represents the dot product of Hilbert space F.The current graph kernel methods are mainly dividedinto three categories: diffuse graph kernel, volume graphkernel, and path graph kernel. The authors of this articlepropose the tag graph kernel method. The core is used tocompare the quantity of public channels of the two graphsthrough hashtag to measure their similarity.Definition 2 Directed tag graph: given v is one node set,? is one directed edge set and ? ? ?×?, ? is a tag set, and m? ? × ? is a mapping from ? to ? , then graph G = (?, ?,m)is a directed tag graph.Definition 3 Adjacency matrix: given [E]ij = 1 ? (?i, ?j)? ?, and [E]ij = 1 ? (?i, ?j) /? ?, then matrix E is anadjacency matrix of directed tag graph G.Definition 4 Tag matrix: given tag set ? = {?1, ?2, · · ·}, if[L]ri = 1 ? ?r = label(?i), and [L]ri = 0 ? ?r = label(?i),then matrix L is the tag matrix of directed tag graph G.Definition 5 Matrix inner product: matrix A andmatrix B are the matrices of two m×n, and the innerproduct of matrix A and matrix B is defined as ?A,B? =m?i=0n?j=0AijBij.Fig. 2 Demonstration of extension dependency path tree kernelThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 5 of 79Given G and G? as two directed tag graphs, on the basisof hashtag, the all-paths hashtag graph kernel function isshown as Formula (7):K(G,G?)=r??r=0?r?Lr( ??i=0? iEi)LTr , L?r( ??i=0? iEi)L?Tr?=r??r=0|k|?m=0|k|?n=0?r[Lr( ??i=0? iEi)LTr]mn[L?r( ??i=0? iEi)L?Tr]mn(4)where, E and E? are the adjacency matrices of G and G?,respectively, and L0,L1, · · ·,Lr , and L?0, L?1, · · ·, L?r are thehashtags of G and G?, respectively. Matrix [ En]i j repre-sents the number of all paths in directed tag graph G witha length of n from node ?i to node ?j.??i=0?iEi can fuse allpaths with different lengths between different nodes intograph G. K is the set consisting of all hashtags, r? is theupper limit of hashtag top class, and ? (0 < ? < 1) is thepath weight parameter of adjacency matrix. ?r(?r > 0) isthe top class of hashtags, and the setting of ?0,?1, · · ·,?rcan effectively distinguish the effects of the hashtag atdifferent top classes on the different categories of tasks.Kernel fusionThe three kernel methods used in this article have theirown advantages and disadvantages. The feature-basedkernel is simple and effective but cannot obtain the sen-tence structural information. Extension dependency pathcan obtain the sentence structural information but ignoresthe deep grammar information. Tag graph kernels canutilize both the results of the grammar analysis and thecharacteristics of words but ignores the words with a rel-atively long distance and the path similarity of over threewords. To sum up, the authors of this article propose amethod based on the multi-kernel fusion to extract bio-logical entity relationships. For each kernel, the similarityis measured according to its field, as shown in Formula (8).K(x, y) =m?i=1Ki(x, y) (5)where i represents the quantity of kernels, m=3. Toachieve the kernel fusion of different analysis structures,the feature weight ? is imported, and ?i > 0,?i?i = 1.However, the kernel weighted sum is used to replace thesimple multi-kernel summing, as shown in Formula (9):K(x, y) =m?i=1?iKi(x, y) (6)At this point, the single-kernel target function is turnedinto as follows:Ld =?t?t ? 12?t?s?t?srtrs?i?iKi(xt , xs)(7)The multi-kernel combination also appears in Discrim-inant (11):g(x) =?t?trt?i?iKi(xt , xs)(8)The value of ?i is used through training, and the valuedetermines the role of the corresponding kernels in thediscriminant.Results and discussionTo evaluate the multiple-kernel-learning-based methodproposed herein, we conducted computational experi-ments and compared with the existing method.Experimental evaluation indexIn the biomedical entity relationship extraction research,there are three evaluation indices which are the following:(Precision, P), (Recall, R) and (F-score, F).P = TPTP + FP (9)R = TPTP + EN (10)F = 2 ? P ? RP + R (11)Where TP represents the number of correctly catego-rized positive examples, TN represents the number ofcorrectly categorized negative examples, FP representsthe number of wrongly categorized positive examples, andFN represents the number of wrongly categorized nega-tive examples. P refers to the precision of the algorithm,and R refers to the integrity of reaction algorithm. F valueTable 2 Statistical form of corpus informationCorpus set Number of texts Number of sentences Number of positive examples Number of negative examples Total number of examplesAimed 225 1955 1000 4834 5834IEPA 50 145 335 482 817BioInfer 863 1100 2534 7132 9666HPRD50 200 486 163 270 433LLL 45 77 164 166 330The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 6 of 79Table 3 Comparison between tag graph kernel and all-paths graph kernel in terms of their performanceTag graph kernel method All-paths graph kernelCorpus set P R F P R FBioInfer 51.64 68.92 59.73 46.89 62.13 57.25Aimed 50.82 69.76 58.61 44.97 65.82 55.46HPRD50 55.64 67.81 70.01 49.76 64.38 68.21IEPA 61.58 76.91 74.23 56.48 72.36 70.65LLL 71.92 70.84 77.43 67.19 66.95 72.68is the harmonic mean of the two evaluation indices of Pand R and is currently the main evaluation index for thecurrent biomedical entity relationship extraction study.Experimental corpusIn this section, we used five evaluation corpuses [46]which are authoritative evaluation corpuses in thebiomedical entity relationship extraction research. Sta-tistical information on the five experimental corpuses,Aimed, IEPA, BioInfer, HPRD50, and LLL, are shown inTable 2.Experimental resultsAll-paths graph kernel method [43] is one of the most typ-ical methods in the protein relationship extraction study.Table 3 shows the comparison of tag graph kernel methodand all-paths graph kernel method in terms of their per-formance in the five corpus sets. Evidently, the perfor-mance of the tag graph kernel method in five corpus setsis superior to that of the all-paths graph kernel method.The P, R, and F value of tag graph method in Aimedcorpus are 50.82, 69.76, and 58.61%, respectively. The cor-responding values of all-paths graph kernel method are44.97, 65.82, and 55.46%, respectively. The P, R, and Fvalue of tag graph kernel method in other four evaluationcorpuses are 2-5% higher than that of all-paths graph ker-nel. The results indicate that the overallperformance of taggraph kernel method is superior to that of all-paths graphkernel.In order to compare two kinds of kernel fusion meth-ods with the three simple kernel methods, we conductedexperiments on the BioInfer corpus which is moderatescale. The results are shown in Table 4. In the threeseparate kernel methods, the tag graph kernel methodproposed herein has the best performance followed by theextension dependency path tree kernel. The three kernelmethods have a better performance than the single kernelmethods. Furthermore, two kernels fuse methods whichone is tag graph kernel method obtained the better per-formance. The P, R and F value of feature kernel and taggraph kernel fuse methods is 53.43, 71.62 and 61.30%,respectively. The P, R and F value of feature kernel andtag graph kernel fuse methods is 55.47, 70.29 and 60.37%,respectively. Experiment results have indicated that theperformance of the two kinds of kernel fusion methods isbetter than that of simple kernel. Hence, the fussed ker-nel methods indeed improve the performance of proteinrelationship extraction method.As shown in Table 5, the three-kernel-fused methodsand fused kernel methods remain relatively stable in thefive kinds of corpus sets. The fused kernel method hasthe best performance in all aspects, and the proposed taggraph kernel method has the second best performance.The parameters in the tag graph are the parameters withthe best results after r? and Br have gone through a largeamount of training. Compared with P and R, the F valuein the five corpuses sets changes greatly. For example,the F value of the four methods in the BioInfer corpusranges from 52 to 62%, whereas the F-value in the LLLcorpus ranges from 68 to 91%. Such result is mainly dueto the changes in the distribution of positive and nega-tive changes of corpus, which greatly affect the F value,whereas other evaluation parameters are insensitive tothe changes in the positive and negative example ratio incorpus. The negative examples in Aimed and Bioinfer cor-puses far outnumber the positive examples. Thus, the Fvalue of the two corpuses is significantly lower than thatof other corpuses, such as LLL.ConclusionIn this paper, a tag graph kernel method used hashtagwas proposed, which is combined with extension-path-tree-kernel-basedmethod and characteristic-kernel-basedmethod, a fused kernel learningmethod was furtherTable 4 Performance of different kernel methods in BioInfercorpusMethod P R FCharacteristics-based kernels 45.61 63.57 56.24Extension dependency path tree kernel 41.32 69.76 52.58Tag graph kernel 51.64 68.92 59.73Feature kernel + path tree kernel 49.86 70.12 60.25Feature kernel + tag graph kernel 55.43 71.62 61.30Path tree kernel + tag graph kernel 55.47 70.29 60.37The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 7 of 79Table 5 Performance of different kernel methods in five types of corpusesCorpus set Evaluation parameters Characteristics- based kernels Extension path dependency kernel Tag graph kernel Kernels fromthree-kernel fusionAimed P 45.34 42.31 50.82 57.45R 61.25 68.54 69.76 72.31F 55.36 52.63 58.61 60.98IEPA P 56.84 52.48 61.58 73.82R 72.92 69.35 76.91 81.06F 87.15 63.79 74.23 79.57BioInfer P 45.61 41.32 51.64 91.69R 63.57 69.76 68.92 71.62F 56.24 52.58 59.73 62.35HPRD P 50.26 49.96 55.64 61.87R 67.59 66.31 67.81 72.35F 75.38 69.78 70.01 85.48LLL P 53.59 83.34 71.92 75.69R 70.12 69.78 70.84 78.37F 68.43 88.03 77.43 90.12proposed. Experimental results indicate that the P, R andF value of the tag graph kernel method is higher on fiveevaluation corpuses in comparison with the all-paths-graph kernel method. And the performance of multi-kernel fusion methods proposed herein is the best of allof methods used in this article. Obviously, multi-kernelfusion methods can make up for the defect in simple ker-nel and improve the performance of protein relationshipextraction method.AcknowledgementsThis research was partially supported by the National Natural Science of Chinaunder grant No. 61371177,No. U1431102, Science and Technology MajorProject in ShanDong under grant 2015ZDXX0201B04, Science and technologydevelopment Program in Shandong Province under grant 2014GGX101053,Science and technology development Program in Weihai Province undergrant 2014GGX101053.FundingThe publication costs for this article were funded by Shandong University.Availability of data andmaterialsIf you need data and material in the paper,please contact with xudongliang.Email: xudongliang@sdu.edu.cnAbout this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsXD and PJ designed and implemented the tag graph kernel method. WB andXZ combine the tag graph kernel methods with Characteristics-based kerneland extension path graph kernel into a fused kernel learning method. Allauthors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Author details1School of Mechanical, Electrical and Information Engineering, ShanDongUniversity, WenHua West Road, 264209 WeiHai, China. 2School of ComputerScience and Technology, Harbin Institute of Technology, WenHua West Road,264209 WeiHai, China.Published: 20 September 2017Sharp Journal of Biomedical Semantics  (2017) 8:2 DOI 10.1186/s13326-016-0110-0RESEARCH Open AccessToward a comprehensive drug ontology:extraction of drug-indication relations fromdiverse information sourcesMark E SharpAbstractBackground: Drug ontologies could help pharmaceutical researchers overcome information overload and speedthe pace of drug discovery, thus benefiting the industry and patients alike. Drug-disease relations, specificallydrug-indication relations, are a prime candidate for representation in ontologies. There is a wealth of availabledrug-indication information, but structuring and integrating it is challenging.Results: We created a drug-indication database (DID) of data from 12 openly available, commercially available, andproprietary information sources, integrated by terminological normalization to UMLS and other authorities. Acrosssources, there are 29,964 unique raw drug/chemical names, 10,938 unique raw indication target terms, and192,008 unique raw drug-indication pairs. Drug/chemical name normalization to CAS numbers or UMLS conceptsreduced the unique name count to 91 or 85% of the raw count, respectively, 84% if combined. Indication targetnormalization to UMLS phenotypic-type concepts reduced the unique term count to 57% of the raw count. The 12sources of raw data varied widely in coverage (numbers of unique drug/chemical and indication concepts andrelations) generally consistent with the idiosyncrasies of each source, but had strikingly little overlap, suggesting thatwe successfully achieved source/raw data diversity.Conclusions: The DID is a database of structured drug-indication relations intended to facilitate building practical,comprehensive, integrated drug ontologies. The DID itself is not an ontology, but could be converted to one moreeasily than the contributing raw data. Our methodology could be adapted to the creation of other structureddrug-disease databases such as for contraindications, precautions, warnings, and side effects.Keywords: Drug indications, Drug-disease relations, Drug ontologies, Drug information integration, UMLS, WHO-ATCBackgroundBiomedical information overload and the potential offormal ontologies to help overcome it are well recog-nized [13]. Information overload is but one threat tothe viability of the traditional pharmaceutical industry.Others include the rising costs of laboratory research,clinical trials, litigation over anomalous harmful sideeffects, and increasing times to market [4]. The successof the Gene Ontology (GO) as an in silico molecularbiology research tool [5] suggests that drug ontologiescould have a similar impact on drug research. Theadvance of practical ontologies into the pharmaceuticalCorrespondence: sharp@merck.comScientific Information Management, Merck Research Laboratories, 770Sumneytown Pike, West Point, Philadelphia, PA 19486, USA© The Author(s). 2017 Open Access This articInternational License (http://creativecommonsreproduction in any medium, provided you gthe Creative Commons license, and indicate if(http://creativecommons.org/publicdomain/zedomain has been much anticipated [68], and is becom-ing evident [9, 10].Pioneering reports on ontology-based, in silico drugdiscovery have emerged [1113]. The basic goal isontology-assisted inference of surprising and/or more-likely-to-succeed new drug candidate compounds forknown uses, thus cutting costs and time to market. Drugontology-assisted inference could also be applied to find-ing new uses for known compounds (drug repurposing)[14], or personalized genome-dependent safety/efficacyprofiling (pharmacogenomics) [1518]. These ontologiesinclude drug relations to chemically similar compounds,diseases (therapeutic classifications, indications, sideeffects), and biological pathways (mechanisms of action,molecular target proteins or their genes, secondary disease-gene and protein-protein interactions). In principle, suchle is distributed under the terms of the Creative Commons Attribution 4.0.org/licenses/by/4.0/), which permits unrestricted use, distribution, andive appropriate credit to the original author(s) and the source, provide a link tochanges were made. The Creative Commons Public Domain Dedication waiverro/1.0/) applies to the data made available in this article, unless otherwise stated.Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 2 of 10ontologies could be expanded to encompass many more di-mensions of drug information [19, 20]; that is, they can bemade more comprehensive.For further progress in building comprehensive drugontologies, rich and well-structured knowledge (content)about biological pathways and chemically similarcompounds is readily available from resources such asGO, GenBank [21], DrugBank [22], PubChem [23], andChemIDplus [24]. Rich drug-disease knowledge also isreadily available, but usually as unstructured (free) text;e.g., DailyMed [25]. Thus the well-structured butrelatively shallow WHO-ATC drug classification [26]has been utilized as a source for drug-diseaseknowledge [12, 13].It is important to distinguish between diseases, indica-tions, contraindications, side effects, and other suchdimensions of drug information. A drug indication canbe a disease1 that the drug is used for (i.e., to treat,prevent, manage, diagnose, etc.). An important subsetare approved indications which have been through aformal, country-specific regulatory vetting process. Butdrugs can also be indicated for medical conditions whichmay not be considered diseases, such as pregnancy.Drugs can also be indicated for procedures, such ascontrast media for radiology. In ontological terms,medical conditions (of which diseases are a subclass)and medical procedures constitute the range of drugindications. They also constitute the range of very differ-ent, even orthogonal, drug relations such as contraindi-cations, precautions, and warnings. The range for sideeffects, on the other hand, is arguably limited todiseases. Thus it is important to specify which ofthese relations is being addressed. This paper ad-dresses indications, but much of it is extensible toother drug-disease relations.MethodsWe created a drug-indication database (DID) using con-tent from openly available, commercially available, andMerck proprietary information resources. To integratethe data, we attempted to identify distinct triples of adrug, indication, and indication subtype (treat, prevent,manage, diagnose, etc.), and then normalize eachcomponent to a standard terminology or code. The rawdata varied widely in format, from well-structured,vocabulary-controlled triples to hierarchical classifica-tions to free text. While the DID itself is not anontology, it could be converted to one more easily thanthe contributing raw data.SourcesRaw data on drug/chemical-indication relations werecollected from the following resources.DailyMedDailyMed [25] is a free drug information resource pro-vided by the U.S. National Library of Medicine (NLM)that consists of digitized versions of drug labels (alsocalled package inserts) as submitted to the U.S. Foodand Drug Administration (FDA). The information formatof the labels is mostly free text but with standard sectionheadings, including Indications & Usage. DailyMed wasof special interest because of its comprehensive coverage,open availability, and the package inserts combination offormat consistency, rich detail, and provenance (manufac-turer-written, scientifically vetted, and FDA-approved).DrugBankDrugBank [22] is a unique bioinformatics and chemin-formatics resource that combines detailed drug (i.e.chemical, pharmacological and pharmaceutical) datawith comprehensive drug target (i.e. sequence, structure,and pathway) information provided by the University ofAlberta. Many records include an explicit Indicationfield populated with free text values, and leveragingthese was of special interest due to DrugBanks richcoverage of molecular target information.MeSH PAMeSH (Medical Subject Headings) [27] is NLMs con-trolled vocabulary used to index Medline/Pubmed [28]articles by scientific topics including drugs, chemicals,diseases, and other biomedical conditions, processes,and procedures. MeSH has ontology-like hierarchicaland other relationships between concepts, but it doesnot consistently link drugs to diseases/conditions/pro-cesses explicitly (e.g., Aspirin to Fever). It does,however, have a special Pharmacological Action (PA)relationship which links drugs and other chemicals totherapeutic classes (e.g., Aspirin to Antipyretics)which could be mapped to diseases/conditions/processes(e.g., Antipyretics to Fever).NDFRTNDFRT (National Drug File Reference Terminology) [29]is produced by the U.S. Veterans Health Administrationand is openly available from several resources includingNLMs UMLS (Unified Medical Language System) [30].Like MeSH PA, NDFRT consists of controlled vocabularyterms connected by specific relationship names, five ofwhich could be considered pointers to indications or PAs:may_treat, may_prevent, may_diagnose, has_mecha-nism_of_action, has_physiological_effect.PDRPDR (Physicians Desk Reference) is a commerciallypublished compilation of manufacturers prescribing in-formation (package insert) on prescription drugs,Fig. 1 Example USAN raw dataSharp Journal of Biomedical Semantics  (2017) 8:2 Page 3 of 10updated annually. [31] Its long history (65 editions) andubiquitous hardcopy availability give PDR a certainprovenance. Section 3 - Product Category Index classi-fies drugs (trade names) by disease (e.g., ALCOHOLDEPENDENCE) and/or PA (e.g., ANALGESICS).ChEBIChEBI (Chemical Entities of Biological Interest) [32]consists of a database and ontology supplied by theEuropean Bioinformatics Institute. The has_role rela-tionship of the ontology connects drugs and chemicalsto functions, including PAs (e.g., antibacterial drug;anti-ulcer drug; proton pump inhibitor).CTDCTD (Comparative Toxicogenomics Database) [33, 34]is supplied by North Carolina State University andMount Desert Island Biological Laboratory, SalisburyCove, Maine. The Chemical-Disease Associations fileconsists of pairs of MeSH terms connected by the rela-tionships therapeutic and/or marker/mechanism andannotated by evidence type; we used the direct evi-dence subset.USAN TCUSANs (United States Adopted Names) are the officialU.S. generic names chosen for drugs by the USANCouncil in consultation with the drugs sponsoring com-pany [35]. Each name has a variety of structured (butnot necessarily vocabulary controlled) relations signify-ing proprietary, chemical, and therapeutic information.This information is published annually in the USPDictionary of United States Adopted Names (USAN) andInternational Drug Names [36] and monthly by theAmerican Medical Association (AMA) [37] (Fig. 1), andMerck encodes it in our internal vocabulary system(eVOC). The Therapeutic Claim (TC) values includedisease names, PAs, and indication subtypes such astreatment of and prevention of.WHO-ATC resourcesWHO-ATC (World Health Organization Anatomic-Therapeutic-Chemical) is a five-level drug classifica-tion hierarchy specifying (typically, from top tobottom) the anatomical system acted upon, thera-peutic action, and chemical nature of the drug. Thehierarchy can convey multiple indications/PAs for agiven drug. WHO-ATC is widely accepted as a stand-ard for drug classification, including in the MerckeVOC system. We obtained WHO-ATC data fromtwo WHO datasets purchased by Merck andadditional mappings in eVOC; these are referred to asWHO_ATC [38], WHO_DD [39], evoc_ATC, and evoc_-eProj in the rest of this document. (All evoc_eProj andsome evoc_ATC data represent Merck proprietary infor-mation and therefore have been removed from the at-tached DID subset, Additional file 1.)Parsing and filteringThese resources and their contributions to our databaseare summarized in Table 1. Parsed refers to convertingthe raw data to triples of a drug, indication, and indica-tion subtype. In the process of parsing, some raw datawas found to be irrelevant, redundant, and/or intract-able, and therefore was removed from further processing(filtered). Differences in contribution counts from fil-tered to parsed correlate inversely with how well-structured and vocabulary-controlled were the rawsource data, from low (ChEBI, CTD, MeSH PA, NDFRT)to high (DailyMed, DrugBank).Filtering is not qualitatively different from initialsubsetting (Table 1, column 3). For example, ChEBIs,CTDs, and MeSH PAs relatively large initialcontributions can be attributed to their higher cover-age of non-drug chemicals and non-therapeutic quasi-indications (e.g., Carcinogens; Mutagens). These couldbe considered irrelevant to pharmacy/prescription appli-cations of the DID, but were left in for drug discoveryapplications. ChEBIs contribution was reduced 48% byfiltering out irrelevant (non-indication) has_role objects(e.g., metabolite; prodrug; epitope), but CTDsmarker/mechanism subset (63%) was not removed due toits potential use in future analysis. DailyMeds filtering re-duction was even larger but aimed at very different targets:Table 1 Source contributions of drug-indication datasource abbrev source name or description subset if any version/date number of drug-indication pairsinitial filtered parsedChEBI Chemicals of Biological InterestOntologyhas_role relations 104/June 1, 2013 16,415 8,598 8,598CTD Comparative ToxicogenomicsDatabaseChemicals-Diseases Associations,direct evidence subsetMay 2, 2014 82,000 81,214 81,214DailyMed NLMs database of FDA packageinsertssingle component title (productname) & Indications sections withtractable text length (<540)March 20, 2011 15,834 1,612 3,840DrugBank U. Alberta open access DB of drugtarget and other infotitle (drug name) & Indicationssections3.0/2011 1,599 1,595 6,004MeSH PA Medical Subject Headings PharmacologicAction relations2013/Dec. 3, 2012 26,293 25,847 25,908NDFRT National Drug Formulary ReferenceTerminologymay_treat & may_prevent relations 2009AA (UMLS) 50,775 5,294 5,294PDR Physicians Desk Reference Section 3 - Product Category Index 2006 3,150 1,204 2,169USAN_TC United States Adopted NamesTherapeutic ClaimsMarch 31, 2014(eVOC)6,569 5,954 7,234WHO_ATC World Health Organization Anatomic-Therapeutic-Chemical, Defined DailyDose index2005 16,276 7,807 9,004WHO_DD World Health Organization DrugDictionarysingle generic compounds with ATCcodes (minus 2005 WHO-ATC overlapand herbals BNA = 9)Sept. 2013 40,736 21,764 25,674evoc_ATC WHO-ATC codes in Mercks eVOCgeneric names dictionarysingle generic compounds with ATCcodes (minus WHO-ATC & WHO-DDoverlap)May 6, 2014 65,552 16,269 19,093The numbers refer to candidate drug-indication pairs in the initial raw data extract (initial), after filtering for internal redundancy, relevance, and/or tractability(filtered), and after parsing of free text into single concepts (parsed) as described in the main text. The filtered count is the number of unique pairs of raw drugname (DID column D) and indication entire value/string (column AQ), while the parsed count is the number of unique pairs of raw drug name and indication target/substring (column AR). evoc_eProj data are not shownSharp Journal of Biomedical Semantics  (2017) 8:2 Page 4 of 10combination products (20%), intractably long (>539 charac-ters) Indication & Usage texts (37%), and redundantIndication & Usage values paired with the same drug gen-eric name differing only by dosage, formulation, tradename, or supplier (33%). NDFRTs (90%) and PDRs (62%)filtering reductions were also due primarily to conflatingvarious forms (trade names, in PDRs case) of the same gen-eric name.2Initial counts from the WHO-ATC resources are basedon viewing each level of the WHO-ATC hierarchy as aseparate indication, rather than combining them into asingle raw term. Filtering resulted in reductions of 52%(WHO_ATC), 47% (WHO_DD), and 75% (evoc_ATC)reflecting removal of combination and ill-formed drugnames, and non-indication and redundant classificationterms (e.g., Antithrombotic Agents at nested hierarchicallevels [B01 and B01A]).It must be emphasized that the parsing, filtering,and normalizing (see below) done in this workemployed a wide variety of ad hoc methods andmanual curation commensurate with the raw data/source diversity.Normalizing drug namesVarious types of drug identifiers are exemplified in Fig. 1,including a generic name (in this case a USAN, aftobetinhydrochloride), chemical names, a structural formula,sponsor code designations, and a CAS (ChemicalAbstracts Service) Registry Number. Other types notshown in Fig. 1 include trade names (e.g., Tylenol corre-sponding to the generic name acetaminophen), FDAsUNII (Unique Ingredient Identifier; e.g., A1FCZ940WAfor aftobetin hydrochloride), and InChI (InternationalChemical Identifier) Key (e.g., GMWHTUNMFTUKHH-NDUABGMUSA-N for aftobetin hydrochloride) [40].The equivalence of such terms for the exact same chem-ical entity can sometimes be debated due to details suchas isomerism, salt forms, hydration, formulation, and dos-age, but they are commonly considered synonyms, withthe generic name as the preferred term (PT).Thus, to parse out the drug identifier in each rawdrug-indication record, we looked for source databasefields or elements containing these types of terms, andattempted to normalize them to generic names using thesources own and/or other synonym dictionaries. TheseSharp Journal of Biomedical Semantics  (2017) 8:2 Page 5 of 10dictionaries included those available from ChemIDplus,ChEBI, CTD, DrugBank, UMLS, and Mercks eVOC. Inaddition, to resolve conflicts among these dictionaries,we attempted to derive a preferred PT via CASnumber mapping and ranking the dictionaries in theorder ChemIDplus > ChEBI > DrugBank > eVOC > CTD.For example, in ChemIDplus the PT for CAS number103-90-2 is acetaminophen but in ChEBI it is para-cetamol. Thus the DID enables ChEBI drug-indicationdata for paracetamol to be grouped with other sourcesdrug-indication data for acetaminophen. UMLS is nota rich source of CAS numbers, but supplies an equallylanguage-neutral CUI (Concept Unique Identifier).Normalizing indicationsIn the DID and its non-proprietary subset (Additionalfile 1), indications associated with each drug name areencoded at four basic levels of granularity. Raw entire value/string (column AQ): the rawsources term/text, including entire DailyMedIndications & Usage sections converted to single-line sentences. Raw target/substring (column AR): a term/phrasewithin or based on the entire value/string, denotinga distinct indication concept. If the target/substringis the same as the entire value/string, it is flaggedwith Y in column AS. UMLS entry term (column AU) that best matchesthe target/substring and conforms to our semantictype preference for phenotypes (diseases and otherbiological conditions, processes, and functions; seebelow). UMLS mapping was done using ad hoc perlscripts designed to work with UMLS flat files(2013AA version), MetaMap [41], and/or NLMsonline UMLS browser [42]. Each UMLS entryterm is tagged for whether it is preferred (P)or a non-preferred synonym (S) (column AX).For readability in the DID, all P terms wereconverted to proper case and all S terms wereconverted to lower case using Excel stringfunctions. UMLS preferred term (column AV) andcorresponding CUI (column AW) were computedfrom UMLS 2013AA flat files to unify all encodingat this level, even if raw values consisted of UMLSterms or CUIs (MeSH PA and NDFRT), except formappings only available in more recent UMLSversions via NLMs online browser.Indication semantic typesFor UMLS encoding of indication concepts, we had apreference for UMLS concept terms classified underUMLS semantic types signifying phenotypes (diseasesand other biological conditions, processes, and func-tions). The goal of this was to reduce encoding scatter.For example, the raw term antibacterial agent exactlymatches a UMLS synonym under Anti-BacterialAgents (CUI C0279516) classified under semantic typeAntibiotic (A1.4.1.1.1.1). But calling a drug an anti-bacterial agent is equivalent to saying that its indicationis Bacterial Infections (C0004623, classified underDisease or Syndrome B2.2.1.2.1). By mapping Anti-Bacterial Agents/C0279516 to Bacterial Infections/C0004623, raw data that encode to either are unified.This is tantamount to trading lexical match precision forincreased terminological reduction (explained below).In the DID and Additional file 1, initial indicationmappings to non-phenotypic semantic type UMLS termsare encoded in columns BD-BL with their remapping tophenotypic type CUIs in AT-BC. If the initial non-phenotypic type mapping could not be mapped to a pheno-typic type CUI, it is encoded in AT-BC. For example,Cephalosporins (a WHO-ATC category, among other in-stances) maps to C2266959/Antibiotic/A1.4.1.1.1.1, but isstuck there because UMLS had no phenotypic type termsuch as cephalosporin activity; cephalosporin effect; orcephalosporin-sensitive infection.Indication subtypesIn prior work [19, 20] we observed that drug indicationsare often classified or annotated by subtypes such asapproved vs. non-approved, or treatment vs. prevention.The current works expanded raw data scope brought tolight additional types with lexical cues such as thera-peutic/pharmacologic class prefixes (Antidiabetic),suffixes (Anxiolytic), and head nouns (beta-adrenergicagonist; Lipoprotein Lipase Activators; smoking ces-sation adjunct). Some of these distinctions are likely tobe even more substantial than treatment vs. prevention;e.g., Antineoplastics and Carcinogens both map tocancer but in opposite ways, one inhibitory or negative,the other causative or positive. This suggests an indicationsubtype hierarchy representing a gradient of granularitywith raw terms like treatment and prevention at thebottom/leaf level and negative and positive at the top.In between would be lexical root forms such as treatrepresenting treats; treating; treatment; etc. If soencoded in the DID, users could select the mostappropriate indication subtypes and level of granularityfor their use case. We identified indication subtypes basedon Excel string searches (treat; anti; inhibit; etc.) inthe raw entire value/string (column AQ).Terminological reductionThe inherent value of terminological normalization isthe core principle of controlled vocabularies that havebeen used to organize, search, and represent informationSharp Journal of Biomedical Semantics  (2017) 8:2 Page 6 of 10for over a century [43]. To measure the success of ourterminological normalization efforts, we definedterminological reduction (TR) as TR = (N + X)/U, whereN = number of unique normalized names, X = number ofunique raw names which remain unnormalized, andU = number of unique original raw names.ResultsDatabase overviewThe Merck in-house version of the DID (January 2015release) contains 198,415 rows of data representingunique quadruplets of source, raw drug/chemical name,raw indication target term, and indication UMLS CUI.Across sources, there are 29,964 unique raw drug/chem-ical names, 10,938 raw indication target terms, and192,008 unique raw drug/indication pairs. Additional file1 is a copy of this spreadsheet minus 5,557 rows (3%)containing Merck proprietary information. Therefore re-producing these counts and the following analyses onAdditional file 1 would yield slightly different quantita-tive results, but not substantially alter our qualitativeconclusions. Additional file 1s schema worksheetshows the DID schema and two example records.Drug name normalizationDrug name mapping to CAS numbers is encoded inDID columns E-H. CAS numbers were assigned to 87%of the DID rows and 71% of the unique raw drug names,providing TR of the unique names to 91%. The preferredauthority ChemIDplus alone covered 84% of the rowsand 68% of the unique raw drug names. Almost all(98%) of these CAS number mappings are based onexact (case-insensitive) matches to the ChemIDplus orother standards PT for that CAS number, or to asource-specified synonym (<syn per source>). Thesynonym matches were manually curated and obviousbroader term (BT) and narrower term (NT) matcheswere reclassified as such. For BT and NT matches thedirectionality is raw-to-standard; e.g., raw arformoterolfumarate is a NT (a salt, derivative, analog, or formula-tion of) the closest ChemIDplus term which has a CASnumber, Arformoterol. Also distinguished are quasi-synonym matches such as cidofovir anhydrous: Cido-fovir. The intent is to offer users multiple match qualitylevels as options for filtering. The individual drug namemappings to ChEBI, ChemIDplus, and CTD are encodedin DID columns I-AC.Drug name mapping to UMLS is encoded in DIDcolumns AD-AM. UMLS CUI mapping, compared toCAS number mapping, produced superior coverage ofDID rows (96% vs. 87%) and unique raw DB drug names(89% vs. 71%), and superior TR (85% vs 91%). The differ-ence is at least partly due to the higher numbers ofsynonym and narrower UMLS matches, which may bean artefact of unequal curation effort or UMLS coverageof broad classes (e.g.,antiseptics) which by nature donot have CAS numbers.Indication normalizationNinety-nine percent of DID rows represent uniquetriplets of raw data source (column B), drug name(column D), and indication target/substring (columnAR), the other 1% representing compound matcheswhere more than one UMLS term was needed to coverthe indication concept completely. There are 10,938unique values of the target/substring, of which 28 (0.3%)could not be mapped to UMLS. The rest mapped to7,522 UMLS entry terms and thence to 6,227 UMLSPT/CUIs of the preferred semantic type (columnsAT-BC), yielding a TR of 57%.Indication semantic type normalizationUnlike the drug name normalization mappings, theindication UMLS mappings have a sizable prevalenceof quasi-synonym match types (column AT; 46% ofrows, 30% of unique target/substrings). This is attrib-utable to our preference for indication normalizationto phenotypic-type UMLS terms, operationalized in thesemantic type normalization step. Non-phenotypic-typeterms were thus reduced from 29% of DID rows amonginitial UMLS mappings (columns BD-BL) to 3% amongfinal (AT-BC), primarily terms of type PharmacologicSubstance/A1.4.1.1.1 (25% initial, 1% final). The preva-lence rank of Pharmacologic Substance/A1.4.1.1.1 chan-ged from first to 13th, reflecting the large contributionsfrom ChEBI, CTD, MeSH, PDR, USAN, and WHO-ATCconsisting or raw therapeutic/pharmacologic class terms(e.g., Analgesics; Antineoplastics; Carcinogens).Indication subtypesIndication subtype data are contained in DID columnsAN-AP. These data are very preliminary and incomplete.Supplementing and refining it is one of our ongoingextensions of this work.Comparison of sourcesCoverageTable 2 summarizes how much of the data was coveredby each of the 12 sources after normalization. CTD cov-ered by far the largest number of unique drug-indicationrelations (49%), followed by MeSH_PA, WHO_DD, andeVOC_ATC (1014%), followed by the others (15%).With the exception of USAN_TC, this rank-orderpattern also held for drug/chemical names alone. Forindications alone, CTD also covered 49%, followed byDrugBank (34%), DailyMed (23%), USAN_TC (18%),NDFRT (16%), and the others (58%).Table 2 Comparison of sources coverage of unique drugnames, indication terms, and drug-indication relations afternormalizationSource %normalizeddrug indication drug-indication pairsCTD 33 49 49MeSH_PA 27 6 14WHO_DD 28 5 14evoc_ATC 26 5 11ChEBI 17 8 5WHO_ATC 11 5 5DrugBank 6 34 4USAN_TC 23 18 4NDFRT 6 16 3DailyMed 4 23 2evoc_eProj 3 6 1PDR 3 5 1Percentages are relative to total counts of 25,278 unique normalized drug names,6,228 unique normalized indication terms, and 167,087 unique normalizeddrug-indication relationsSharp Journal of Biomedical Semantics  (2017) 8:2 Page 7 of 10OverlapTable 3 summarizes overlap, a measure of the unique-ness of each sources contribution to the DID, definedas the number of sources that contributed eachunique drug and indication (target) term and drug-indication pair, before (raw) and after normalization,Table 3 Comparison of sources overlapping coverage of unique drand after normalizationsource raw normalizdrug indic (target) drug-indic pairs drugAll 1.64 1.30 1.02 1.87evoc_ATC 1.62 3.40 1.03 2.09WHO_ATC 3.81 3.37 1.07 4.66WHO_DD 1.91 3.29 1.03 2.59MeSH_PA 2.81 1.33 1.03 3.08PDR 5.56 1.60 1.17 6.14evoc_eProj 1.00 1.80 1.00 2.21ChEBI 2.51 1.14 1.01 3.11USAN_TC 3.03 1.47 1.02 3.34DailyMed 4.68 1.60 1.15 5.18NDFRT 5.46 2.45 1.41 6.24DrugBank 5.63 1.49 1.20 6.24CTD 2.41 1.53 1.03 2.62Numbers represent the average number of sources sharing each term or term pair,drug name score of 1.00 for evoc_eProj means that system only shares its raw drugterms in its internal data systems. When these are normalized, as much as possible,names representing evoc_eProj content are shared with enough other DID normalieven though some company codes do not yet have public domain generic names. Dataindic) for the individual sourcesand the difference. Consistent with overall TR, thebiggest effect of normalization was seen in theincrease in shared indication terms with the descend-ing rank-order following the tendency of each sourceto express indications in other-than-phenotypic-typeterms (Table 3, column 9).The pooled (all sources) shared term data can also beviewed as a Zipf distribution [44] (Fig. 2) showing, again,the larger effect of normalization on indication than drugterms or drug-indication pairs. Strikingly, no raw drugnames were shared by more than 10 of our 12 resources,and only four normalized drug names were shared by all12 (Dexamethasone; Hydrocortisone; Methyldopa;Nitroglycerin). The most-shared (by 11 sources) norma-lized drug-indication pairs were Aspirin:Pain andMethyldopa:Hypertensive Disease (the UMLS PT forhypertension).RichnessEach sources average numbers of indications per drugname and drug names per indication, before and afternormalization, measure what might be called the rich-ness of their drug-indication information. CTD had byfar the highest (10) average raw indication targets perdrug/chemical name, consistent with its low overlap andhigh coverage. Following CTD was a cluster in the rangeof 3.54 indication targets/drug that included DailyMed,MeSH_PA, DrugBank, and NDFRT, then a cluster in the2.73.3 range that included WHO_DD, WHO_ATC,ug names, indication terms, and drug-indication relations beforeed changeindic drug-indic pairs drug indic drug-indic pairs1.80 1.14 0.23 0.50 0.126.77 1.38 0.47 3.37 0.356.67 1.96 0.85 3.30 0.896.54 1.40 0.68 3.25 0.374.54 1.43 0.27 3.21 0.404.68 2.32 0.58 3.08 1.154.17 1.44 1.21 2.37 0.443.40 1.72 0.60 2.26 0.713.30 1.81 0.31 1.83 0.792.79 1.48 0.50 1.19 0.333.61 1.76 0.78 1.16 0.352.62 1.70 0.61 1.13 0.502.06 1.08 0.21 0.53 0.05computed within each sources coverage. For example, the low outlier rawnames with itself, reflecting the use of Merck company codes as preferredto public domain generic names, the score rises to 2.21; that is, these genericzed content to push the non-self average from zero up to 1.21 (=2.211.00)are sorted in descending order of the change indication scores (column 9; change/Fig. 2 Zipf distributions of sources overlapping coverage of uniquedrug names, indication terms, and drug-indication relations beforeand after normalizationSharp Journal of Biomedical Semantics  (2017) 8:2 Page 8 of 10evoc_eProj, PDR, and evoc_ATC, and finally ChEBI (1.8)and USAN_TC (1.2). These numbers were little changedby normalization. The biggest changes were actuallynegative (0.4 more raw than normalized indications/drugfor MeSH_PA and evoc_eProj).The highest average numbers of drug names perraw indication target were provided by WHO_DD (69),evoc_ATC (56), and MeSH_PA (55). This same clusteralso showed the biggest effect of normalization. At thelow end, DailyMed and DrugBank data showed the mostdramatic effect of processing, their average indications/drug increasing from approximately 1 (raw entire values)to 2 (raw targets) to 3 (normalized indications).DiscussionOur DID is intended to facilitate building practical,comprehensive, integrated drug ontologies. As for com-prehensiveness, we achieved high source/data diversityas evidenced by a low overall degree of coverage overlapconsistent with the idiosyncrasies of each source (non-drug chemicals, free text, hierarchical terms, etc.).Diversity is not equivalent to comprehensiveness, but isindicative of it. As for integration, indication normalizationto phenotypic-type UMLS concepts provided substantialTR (57%). However, drug/chemical name normalization(TR 84%) was poor by comparison; therefore there wasalmost no effect of overall normalization on the averagenumber of indications per drug.WHO_DDs, WHO_ATCs, evoc_eProjs, PDRs, andevoc_ATCs richness may be somewhat artificial in thatit may be mainly due to WHO-ATCs and PDRs verygeneral higher hierarchical categories. However, this fea-ture may facilitate clustering of drug-indication relationsand so explain WHO-ATCs wide acceptance as a stand-ard for drug classification and discovery research.Because its true richness was not captured, DailyMedraises major issues for further development of the DID.These include the cost of dealing with the current (differ-ent) downloading, subsetting, and sectional parsingoptions, and developing better, less manual, free text-to-UMLS mapping methods. On the benefit side, methodsapplicable to DailyMeds Indications & Usage sectionsare expected to be adaptable/re-usable for contraindica-tions, side effects, and other dimensions of drug informa-tion. Relevance to clinical use cases is recognized [45] butDailyMeds fit to early-stage drug discovery has been ques-tioned [46]. NDFRT presents the opposite conundrum. Ina spot check of two drugs, we [20] found major discrepan-cies between NDFRTs may_prevent and may_treat rela-tions and the approved clinical indications. Thereforethese relations may be a poor fit to clinical drug ontologyuse cases. However, as a representation of possible drugindications conveyed by co-occurrence of MeSH terms inMedline, they may be ideal for early-stage drug discovery.Also, NDFRTs may_diagnose, has_mechanism_of_action,and has_physiological_effect relations will be examined forfuture inclusion in the DID.Finally, CTDs high-coverage, low-overlap outlier sta-tus raises suspicion that its marker/mechanism subset(63%) may not be relevant to drug indications and there-fore should be examined and possibly excluded fromfuture DID releases.ConclusionsThe DID is a database of structured drug-indication re-lations created using openly available, commerciallyavailable, and Merck proprietary information resourcesand terminological normalization tools. It is intended tofacilitate building practical, comprehensive, integrateddrug ontologies. The DID has good source/raw datadiversity as measured by low coverage overlap, andsignificant integration/normalization as measured byterminological reduction. Numerous opportunities existfor data cleaning, addition, and other improvements.Our methodology could be adapted to the creation ofother structured drug-disease databases such as forcontraindications, precautions, warnings, and side effects.Endnotes1Following UMLS, we take diseases to be synonym-ous with disorders. We also mean diseases to conveythe larger sense of pathological or aversive states thatmight otherwise be distinguished as signs, symptoms,abnormalities, deficiencies, injuries, etc.2Although different forms of the same generic name canin principle be specific to different indications, our confla-tion of NDFRT is not lossy because NDFRT appears toSharp Journal of Biomedical Semantics  (2017) 8:2 Page 9 of 10cross-generalize them regardless. For example, finasterideis marketed as a 1 mg tablet indicated to treat male-pattern baldness and a 5 mg tablet indicated to treatbenign prostatic hyperplasia. But NDFRT has may_treatrelations to both Alopecia and Prostatic Hyperplasia(the corresponding MeSH PTs) for all three: Finasteride1 mg Tab; Finasteride 5 mg Tab; and Finasteride. Inanother example, Bismuth and all of its salt variants haverelations to Escherichia Coli Infections; Virus Diseases;Helicobacter Infections; and Dysentery, Bacillary pre-sumably related to bismuth subsalicylates gastrointestinaleffects but definitely inappropriate for Bismuth Hydro-xide which is a hazardous industrial chemical. In anotherexample, radioactive and hazardous Iodine, I-125inappropriately shares the Iodine relations to Burns;Leg Ulcer; Radiation Injuries; Staphylococcal Infec-tions; and Surgical Wound Infection.Additional fileAdditional file 1: Drug-Indication Database non-proprietary subset.(XLSX 61806 kb)AbbreviationsAMA: American Medical Association; BAN: British Approved Name;BT: broader term; CAS number or CAS#: Chemical Abstracts Service RegistryNumber; ChEBI: Chemical Entities of Biological Interest; CTD: ComparativeToxicogenomics Database; CUI: Concept Unique Identifier [UMLS];DB: database; DID: Drug-Indication Database; eVOC: electronic VOCabularies[Merck internal system]; FDA: U.S. Food and Drug Administration; GN: generic[drug] name; GO: Gene Ontology; InChI: International Chemical Identifier;MedDRA: Medical Dictionary for Reporting Activities; MeSH PA: MedicalSubject Headings Pharmacological Action [relations]; MeSH: Medical SubjectHeadings; NDFRT: U.S. National Drug Formulary Reference Terminology;NLM: U.S. National Library of Medicine; NLP: natural language processing;NT: narrower term; OBO: Open Biological & Biomedical Ontologies;OTC: over-the-counter [drugs]; PDR: Physicians Desk Reference; PT: preferredterm; SNOMEDCT: Systematized NOmenclature of MEDicine ClinicalTerminology; TR: terminological reduction; UMLS: Unified Medical LanguageSystem; UNII: UNique Ingredient Identifier; USAN TC: United States AdoptedNames Therapeutic Claim; USAN: United States Adopted Names; USP: UnitedStates Pharmacopeia; UTS: UMLS Terminology Services; WHO-ATC: WorldHealth Organization Anatomic-Therapeutic-Chemical [classification];WHO-DD: World Health Organization Drug DictionaryAcknowledgementsI would like to thank my Merck colleagues, especially Jyoti Shah, KarenMarakoff, and Carol Rohl, for their contributions to this work. Also I wouldlike to thank Olivier Bodenreider at NLM, Nicholas Belkin at RutgersUniversity, and the Merck Educational Assistance Program for theircontributions to my Ph.D. thesis [20], of which this work is an extension.Availability of data and materialsThe non-proprietary subset of the DID is included with this paper(Additional file 1).Authors informationThe author holds a M.A. in Biochemistry and a Ph.D. in Information Science.He has been involved in biomedical vocabularies, ontologies, andinformation systems at NIH, NLM, and Merck since 1988.Competing interestsThe author has been employed by Merck & Co., Inc., since 1994.Received: 14 July 2015 Accepted: 16 December 2016RESEARCH Open AccessTherapeutic indications and other use-case-driven updates in the drug ontology:anti-malarials, anti-hypertensives, opioidanalgesics, and a large term requestWilliam R. Hogan*, Josh Hanna, Amanda Hicks, Samira Amirova, Baxter Bramblett, Matthew Diller, Rodel Enderez,Timothy Modzelewski, Mirela Vasconcelos and Chris DelcherAbstractBackground: The Drug Ontology (DrOn) is an OWL2-based representation of drug products and their ingredients,mechanisms of action, strengths, and dose forms. We originally created DrOn for use cases in comparative effectivenessresearch, primarily to identify historically complete sets of United States National Drug Codes (NDCs) that representpackaged drug products, by the ingredient(s), mechanism(s) of action, and so on contained in those products. Althoughwe had designed DrOn from the outset to carefully distinguish those entities that have a therapeutic indication fromthose entities that have a molecular mechanism of action, we had not previously represented in DrOn any particulartherapeutic indication.Results: In this work, we add therapeutic indications for three research use cases: resistant hypertension, malaria, andopioid abuse research. We also added mechanisms of action for opioid analgesics and added 108 classes representingdrug products in response to a large term request from the Program for Resistance, Immunology, Surveillance andModeling of Malaria in Uganda (PRISM) project. The net result is a new version of DrOn, current to May 2016, thatrepresents three major therapeutic classes of drugs and six new mechanisms of action.Conclusions: A therapeutic indication of a drug product is represented as a therapeutic function in DrOn. Adverseeffects of drug products, as well as other therapeutic uses for which the drug product was not designed are dispositions.Our work provides a framework for representing additional therapeutic indications, adverse effects, and uses of drugproducts beyond their design. Our work also validated our past modeling decisions for specific types of mechanisms ofaction, namely effects mediated via receptor and/or enzyme binding. DrOn is available at: http://purl.obolibrary.org/obo/dron.owl. A smaller version without NDCs is available at: http://purl.obolibrary.org/obo/dron/dron-lite.owlKeywords: Biomedical ontology, Drug product, Therapeutic indication, Mechanism of action, Patient centered outcomesresearch, Anti-hypertensive, Anti-malarial, Opioid analgesic, Function, Disposition* Correspondence: hoganwr@ufl.eduDepartment of Health Outcomes and Policy, University of Florida, Clinicaland Translational Research Building, 2004 Mowry Road, P.O. Box 100219,Gainesville, FL 32610, USA© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 DOI 10.1186/s13326-017-0121-5BackgroundThe Drug Ontology (DrOn) is a Web Ontology Languageversion 2 (OWL2) based representation of drug productsand their ingredients, mechanisms of action, strengths,and dose forms, as well as packaged drug products as rep-resented by United States National Drug Codes (NDCs)[13]. The primary goal of DrOn is to support analyses oflarge, drug-related datasets such as pharmacy claims andelectronic health record (EHR) data. Pharmacy claimsdatasets have traditionally been available to researchersfrom public and private payers (or third-parties that serveas the gateway to those payers data). In addition, state-wide prescription drug monitoring programs to combatopioid abuse are increasingly objects of study and captureNDCs of packaged drug products. These datasets useNDCs to identify the specific drug product that was dis-pensed to the patient as well as the drug products pack-aging and manufacturer. With the advent of researchconsortia and networks such as the Observational HealthData Sciences (OHDSI) collaborative [4] and the NationalPatient Centered Clinical Research Network (PCORnet)[5], massive EHR data sets with prescribing records nor-malized to the RxNorm terminology are increasingly avail-able to researchers. RxNorm is a standard medicationterminology built by the National Library of Medicine tostandardize prescribing data [6].Research using these increasingly available and growingclaims and EHR datasets is facilitated by the ability to querytheir drug data based on various characteristics of the drugproduct prescribed or dispensed, such as therapeutic indi-cation (e.g. hypertension) or mechanism of action of an ac-tive ingredient (e.g. beta blocker), rather than on the drugproduct itself. But the prototypical prescribing record in anEHR dataset identifies the drug product at the level of in-gredient, dosage, and dose formfor example, furosemide20 mg oral tabletusing a concept unique identifier fromRxNorm (i.e., RxCui). The prototypical dispensing recordin a claims database and prescription drug monitoring data-base uses an NDC to identify the drug product and itsmanufacturer and packaging: for example, a bottle of 100furosemide 20 mg oral tablets from Sanofi-Aventis brandedas Lasix. In the former case, the prescribing record will typ-ically have an RxCui of 310429 for the furosemide 20 mgtablet. In the latter case, the dispensing record will typicallyhave an NDC of 00039006710 for a bottle of 100 tablets.To query prescribing and/or dispensing records for all anti-hypertensivesor for all products with a beta-blocker as aningredientthe researcher needs an easy way to generatelists of all RxCuis and/or NDCs, respectively, that representdrug products with these characteristics. Our goal in creat-ing DrOn was to create this ability.Artifacts that pre-existed DrOn were not sufficientfor our purposes for various reasons. For example,any given version of RxNorm only contains the NDCsthat were active at the time the version was released,but DrOn contains a historical record of NDCs.RxNorm, therefore cannot support querying historicalEHR and claims datasets spanning multiple years andsometimes even decades without significant process-ing of past versions. This is not to criticize RxNorm:its primary use case was to support prescribing andordering of medications in a clinical setting (andhence its central focus is the so-called Semantic Clin-ical Drug). In addition, RxNorm is not available as anOWL2 artifact to support integration with other on-tologies such as those available from the Open Bio-medical and Biological Ontology Foundry [7]. UnlikeRxNorm and all other OWL2 artifacts of which weare aware, DrOn also represents the binding of ingre-dient compounds to cytochrome P450 (CYP) isoen-zymes as either substrate, inhibitor, or both tosupport research on potential drug-drug interactions[1]. Lastly, although the National Drug File ReferenceTerminology (NDF-RT) is another OWL2 artifactwith drug information such as therapeutic indicationsand mechanisms of action, it (a) makes basic scien-tific mistakes such as saying that ophthalmic timololmay treat systemic hypertension and that oral vanco-mycin may treat bacterial endocarditis and (b) hasnot been updated in its OWL2 form since 2013.The increasing applicability and relevance of DrOnwas the motivation for the work described here. Specific-ally, there were three use-cases from three majorresearch-focused projects that included representinganti-hypertensive, anti-malarial, and analgesic thera-peutic indications of certain drug products.Although we had designed DrOn from the outset toavoid the scientific mistakes about therapeutic uses ofdrugs made by NDF-RT and other artifacts, prior to thiswork DrOn did not represent any particular therapeuticindication(s) of drug products. Furthermore, the use casemotivating analgesic indications also required represent-ing the mechanisms of action of opioid analgesics andantagonists, which we describe here. Also, the projectmotivating inclusion of information about which drugproducts are anti-malarials submitted a request for alarge number of terms that resulted in additional signifi-cant development of DrOn that we report here. Lastly,we illustrate for the first time a method for queryingDrOn to generate sets of RxCuis and/or NDCs to meetthe use cases driving DrOn development.MethodsWe first present the three research projects and the usecases that they presented. Then we describe our meth-odology for addressing the use cases. Lastly, we discuss asoftware tool that we developed and used to query setsof RxCuis or NDCs from DrOn.Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 2 of 11Use casesOneFlorida clinical data research networkThe OneFlorida Clinical Research Consortium is a state-wide infrastructure in Florida for implementation science,comparative effectiveness research, and pragmatic clinicaltrials [8]. It applied for and was awarded Phase II fundingas a Clinical Data Research Network (CDRN) in the Na-tional Patient Centered Clinical Research Network (PCOR-net). As part of PCORnet, OneFlorida is required todevelop a patient cohort around a common condition, forwhich it chose hypertension [8].Computable phenotypes are commonly used to identify asubpopulation of interest [9]. In particular, researchers inOneFlorida are studying resistant hypertension. Develop-ment of the resistant hypertension cohort requires ex-tremely accurate counting of how many anti-hypertensivesa patient is taking. To support the development of accuratecomputable phenotypes for hypertension that, for example,do not categorize patients as having hypertension who arereceiving ophthalmic timolol but no anti-hypertensive drug,it was necessary to represent hypertension in DrOn as atherapeutic indication of drug products rather than of mo-lecular compounds. Because the OneFlorida data warehou-secalled the OneFlorida Data Trustincorporates bothclaims and EHR data, it is necessary to query for lists ofboth NDCs and RxCuis which are used to standardize EHRdata to identify the drug product prescribed (at the pre-scription stage, which manufacturer and packaging are notknown or even typically specified). In DrOn, a drug productis a tablet, capsule, portion of solution, portion of cream,etc. (prescription) whereas a packaged drug product is abottle of tablets or capsules, a tube of cream, a vial of solu-tion, etc. for sale, distribution, delivery to the hospital wardfrom the pharmacy, etc. (dispensing). Figure 1 shows the re-lationships among packaged drug products, drug products,and ingredients as captured in DrOn.Program for Resistance, Immunology, Surveillance andModeling of Malaria (PRISM)The Program for Resistance, Immunology, Surveillance andModeling of Malaria in Uganda (PRISM) is an InternationalCenter of Excellence for Malaria Research (ICEMR) thatserves East Africa. It is a collaboration between MakerereUniversity in Uganda and the University of California SanFrancisco. The National Institute for Allergy and InfectiousDisease (NIAID) created the ICEMR program in 2010 toestablish a worldwide network of research centers inmalaria-endemic settings to develop infrastructure forresearchers and practitioners working in various settings,especially governments and healthcare institutions, tocombat malaria.PRISM required representing anti-malarial indications ofdrug products. It also submitted a request for a large num-ber of terms to support matching drug codes in variousdata sets to DrOn. Upon cursory review, it appeared thatmany of these requests matched classes already in DrOn.However, after discussion it became apparent that PRISMrequired semantics for their requests that differed from theclasses that we created in DrOn to match RxNorm. Specif-ically, RxNorm semantic clinical drugs (e.g. amoxicillin50 mg/mL oral suspension) and semantic clinical drugforms (e.g. amoxicillin oral suspension) list all active ingre-dients exhaustively in a drug productthat is, they pre-clude the possibility of having additional active ingredients.For example, the class amoxicillin oral suspension does notsubsume the class amoxicillin/clavulanate oral suspension,because it cannot have (by definition) additional active in-gredients besides amoxicillin. The consequence is that inRxNorm (and therefore in DrOn) amoxicillin oral suspen-sion is a siblingnot a parentof amoxicillin/clavulanateoral suspension. Other combination drug suspensions withamoxicillin are also siblings. In addition, there is no com-mon parent in RxNorm or DrOn for these drug products(i.e., there is no class amoxicillin-containing suspension as acommon parent to the numerous siblings). However,PRISM required classes that had the semantics of amoxicil-lin-containing suspension that subsumes all kinds of sus-pensions containing amoxicillin (both with and withoutother active ingredients).Prescription drug monitoring program researchPrescription Drug Monitoring Programs (PDMPs) such asFloridas Electronic-Florida Online Reporting of ControlledSubstance Evaluation Program (E-FORCSE®) [10] utilizeFig. 1 Representation of packaged drug products, drug products, ingredients, and the relationships among them in DrOnHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 3 of 11databases of prescribed, controlled medications that includelimited patient information, prescriber information, andNDCs for the dispensed medication. These databases maybe accessed by prescribers (to view their prescribing histor-ies), pharmacists, and law enforcement. Health researchersmay also utilize this information to monitor (1) the intro-duction of newly controlled substances on the market, (2)medical morphine exposure in high-risk patient popula-tions, (3) multi-drug prescribing associated with overdose(e.g. overlapping prescriptions of opioids and benzodiaze-pines), and (4) the impact of opioid prescribing policies(e.g. on reducing prescribing of short-acting formulationsas the first option) on real-world prescribing behavior.Opioid research with the PDMPs required the ability toquery opioid analgesics based on (1) a combination oftherapeutic indication (pain, medication assisted therapyfor opioid dependence) and mechanism of action (bindingto opioid receptors), (2) different mechanisms of action,specifically binding to mu, delta, and/or kappa opioid re-ceptors in either antagonistic or synergistic ways, and (3)whether a drug is short- or long-acting. These queries areimportant to pharmacoepidemiologists for understandingthe abuse potential and addictive properties of drugproducts.Methodology of DrOn developmentHogan et al. [3] and Hanna et al. [1, 2] describe ourmethods of developing DrOn. DrOn development occursin two major parallel processes. The first process is trad-itional, manual editing using the Protégé ontology editor.We add to DrOn all information about the therapeuticindications and mechanisms of action of drug productsand their ingredients through this manual process: wedo not automatically import it from any other source.As described below, we search numerous sources of in-formation to develop a comprehensive list of drug prod-ucts with a particular indication or ingredients with aparticular mechanism of action, and we then curate theinformation manually using Protégé. The second processinvolved in building DrOn is automated construction ofclasses from RxNorm.DrOn is an OBO ontology and follows the OBO Foun-drys realist methods and principles of ontology develop-ment [7]. The ability to avoid confusing ophthalmic timololas an anti-hypertensive and oral vancomycin as having anyefficacy whatsoever against bacterial endocarditis was a keyvalidation of the realist approach we took in [3]. This abilitywas the result of a key insight from our realist analysis thata therapeutic indication is a property of a drug product(tablet, ointment, cream, solution, etc.) with one or moreactive ingredients; whereas the mechanism of action is typ-ically the property of a chemical compound (Fig. 2). For ex-ample, a molecule of timolol has the capability tocompetitively bind a beta-adrenergic receptor, but onemolecule by itself has no ability to treat hypertension; it re-quires instead a number of timolol molecules on the orderof Avogadros number to lower blood pressure. The drugproduct, therefore, has a sufficient number of molecules, aswell as a formulation, that can be targeted towards a spe-cific indication or indications. Thus, a timolol oral tabletdrug product has the indication of hypertension (but notglaucoma); a portion of timolol ophthalmic solution has theindication of glaucoma (but not hypertension); and onetimolol molecule by itself has no indication.Besides dose form and intended route(s) of administra-tion, the strength (quantity of active ingredient(s)) of adrug product also affects its therapeutic indication. Forexample, finasteride is prescribed in 5 mg dosages totreat benign prostatic hyperplasia but in 1 mg dosages totreat androgenetic alopecia. This example further sup-ports our claim that the drug product, not the molecule,is the bearer of a therapeutic indication.DrOn is available as both the full ontology, includingNDCs, and a version without NDCs that includes all themanually edited content as well as content derived fromRxNorm and imported from the Chemical Entities of Bio-logical Interest ontology (ChEBI), the Protein Ontology(PRO), and other ontologies. This lite version of DrOn isavailable at [11]. The reader who is interested in reprodu-cing our results on reasoning (methods and results dis-cussed below) will find this version most amenable to thetask. We recommend using the Fact++ reasoner in Pro-tégé version 5.1, and adjusting the Java heap size to 2GBor more (4GB is recommended).Methods for representing therapeutic indicationsWe analyzed therapeutic indications according to the realistperspective that we have maintained throughout the devel-opment of DrOn. Based on Hogan et al. we had already de-termined that a therapeutic indication is a property of adrug product and thus is some subtype of specificallydependent continuant per Basic Formal Ontology (BFO)[3]. Our analysis in this work focused further on whether itis more specifically a quality, role, or disposition, and if thelatter, whether it is more specifically a function.Methods for representing opioid analgesic mechanisms ofactionOpioid-acting compounds primarily act by binding ? (mu),? (kappa), and ? (delta) opioid receptors (there are as manyas 17 kinds of opioid receptors in total). These receptorsare located in the cellular membranes of peripheral andcentral nervous system neurons. The major effect of thisbinding is to prevent the release of neurotransmitters at thepresynaptic nerve terminal, although opioid receptors alsoexist at the postsynaptic neuron with inhibitory effects. Theconsequent reduction in neurotransmission is what causesanalgesic effects as well as side effects (or sometimes evenHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 4 of 11desired effects) such as decreased bowel motility leading toconstipation.Authors SA, BB, RE, TM, and MV looked for lists ofcompounds with opioid activity in various resources, eachone focusing on a particular resource. The resources wesearched included DrugBank, NDF-RT, BioPortal, Onto-bee, ChEBI, Wikipedia, and Goodman and Gilmans Man-ual of Pharmacology and Therapeutics [12], as well asgeneral Internet searches using Google. Each person alsodetermined whether each compound on the list is used indrug products for analgesic activity. Once each personhad generated as complete a list as she or he could fromher or his assigned resource, we deduplicated the lists toproduce a master list [13].Once we had obtained a master list, the same authorssubsequently searched the same kinds of resources forwhether each compound was a mu agonist/antagonist,kappa agonist/antagonist, and/or a delta agonist/antagon-ist. Note that some compounds have agonist activity atone receptor but antagonist activity at another receptor.For example, fentanyl is a mu agonist and delta antagon-ist, although in bulk its mu agonist effects dominate suchthat it produces analgesia.Methods for creating new classesAuthors SA, BB, MD, RE, TM, and MV each createdone subset of the requested OWL classes for the PRISMproject such that the union of these subsets covered allthe requests for which we could identify the active ingre-dients. These six class creators created about 20 classeseach. To avoid conflicts in the DrOn git repository onBitBucket, author JH created a fork of the repositorycalled dron-workspace. Authors WRH and AH then setup a separate OWL file for each class creator. Thisallowed each one to check in his or her OWL file with-out any need for a merger that could break the RDF/XML in any OWL file. If another class creator hadchecked in his or her file before, synchronization waseasily achieved by doing a git pull command before is-suing a git push to the centralized repository. Finally,author JH merged the results of all six OWL files into anew dron-hand.owl module of DrOn.For one group of term requests, route of administra-tion was applicable. This group of terms involved drugsolutions (i.e., one or more active ingredients dissolvedin a liquid medium). Some drug solutions are designedfor intravenous administration; some are designed forboth ophthalmic and otic adminstration (i.e., one formu-lation can be administered either way); and some areformulated only for ophthalmic administration. We havenot yet represented routes of administration of drugproducts in DrOn; this task is out of the scope of thispaper and remains for future work. For drug solutionterms, we created a generic solution class with anequivalent class definition. As an example, we definegentamicin solution as:drug solution and (has_proper_partsome (scattered molecular aggregateand (is bearer of some activeingredient role)and (has granular part somegentamicin)))Then we created classes gentamicin intravenous solu-tion and gentamicin ophthalmic solution as primitivechildren of the gentamicin solution class.Software tool for querying DrOnTo query sets of RxCuis or NDCs from DrOn for use inresearch with EHR and claims datasets, we developeddron-query, an open-source, Java-based, command-linesoftware application that uses the pre-existing OWL-API(Web Ontology Language Application ProgrammingInterface) library. The dron-query application is availableon GitHub at [14], and there is a getting started Wikipage at [15]. Essentially, dron-query (1) takes as input adescription logic (DL) query formatted in ManchesterSyntax (a standard syntax for writing description logicaxioms in Protégé among others), (2) executes it using adescription-logic reasoner, and (3) outputs for everyclass that meets the criteria specified in the query its (a)IRI, (b) rdfs:label, and (c) RxCui annotation value. If theFig. 2 Representation of molecular dispositions to capture mechanisms of action in DrOnHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 5 of 11query is for packaged drug products, the NDC is therdfs:label output for the class, and the RxCui field in theoutput is null. But if the query is for drug products, theRxCui field is populated in addition to the IRI andrdfs:label.We developed and executed DL queries using dron-query for the three use cases, and report the results.ResultsWe represented (1) anti-malarial function and asserted itas a function of 57 drug products (the reasoner infers itto be a function of an additional 203 drug products), (2)anti-hypertensive function and asserted it as a functionof 326 drug products (inferred for 2419 additional prod-ucts), (3) analgesic function and asserted it as a functionof 413 drug products (inferred for 2779 additional prod-ucts), and (4) six opioid mechanisms of action andasserted them as dispositions of 59 chemical com-pounds. We created 108 new classes in response to thePRISM term request.Therapeutic indicationsA drug product has the potential to treat a disease orsymptom or other condition, and this potential is onlyrealized upon administration of the product toandsubsequent action onan organism. Note that here weare using the word potential in the sense of an abilityor capability and not in the sense of probability of theability or capability of being realized.Administering a drug product does not guaranteerealization of this potential; for example, one dose mightbe insufficient, or edema of the bowel wall might inhibitabsorption of the drug, or the patient might have agenotype that results in excessive metabolism of the ac-tive ingredient into an inactive metabolite.According to Basic Formal Ontology, this situationmeans that a therapeutic indication of a drug product isa realizable entity or one of its subtypes. BFO definesrealizable entity as a specifically dependent continuantthat has at least one independent continuant entity as itsbearer, and whose instances can be realized (manifested,actualized, executed) in associated processes of specificcorrelated types in which the bearer participates [16].Furthermore, because the physical makeup of thedrug product confers this potential, and becausephysical changes to the drug product can cause it tolose its ability to treat instances of a particular typeof symptom or disease, a therapeutic indication is adisposition (a subtype of realizable entity). BFO de-fines disposition as a realizable entity that is suchthat, if it ceases to exist, then its bearer is physicallychanged [16]. Drug products can only lose theirtherapeutic potential through physical change. For ex-ample, a portion of epinephrine solution contained ina self-injection device degrades upon exposure to airand light, diminishing and ultimately removing withtime its disposition to treat hypersensitivity reactions.Lastly, for a therapeutic indication of a drug product forwhich the product was intentionally manufactured, atherapeutic indication is a function, which per BFO is akind of disposition. BFO defines function as dispositionthat exists in virtue of the bearers physical make-up, andthis physical make-up is something the bearer possesses be-cause of how it came into beingeither through natural se-lection (in the case of biological entities) or throughintentional design (in the case of artifacts) [16]. Becausedrug products are extensively designed and planned tohave certain therapeutic indications, these indications arefunctions. We also note that this usage is consistent withthe most recent exposition of functions as they are repre-sented by BFO [17]. Figure 3 illustrates the relationship ofdrug products to therapeutic functions.Note that this means that given the definitions of func-tion and disposition in BFO, the therapeutic potential(s)of a drug product for which the product was manufac-tured are functions, whereas therapeutic uses that are dis-covered later (for example, off label uses of a drug perthe Food and Drug Administration) are initially disposi-tions that are not functions. Should the manufacturer sub-sequently manufacture the product with the intention ofincluding these additional therapeutic indications, thenthese potentials are functions.Note, however, that no particular instance of dispositionbecomes an instance of function. For example, consider adrug product that is a tablet with an indication (function)to treat X. Suppose that later researchers discover thatthe tablet also has the disposition to treat Y. Then all thetablets manufactured prior to the addition of this indicationbear a function to treat X and a disposition to treat Y,whereas all the tablets manufactured after the change beartwo functions: to treat X and to treat Y. It is possiblethat the exact same physical basis for the disposition existsfor the function. This is analogous to the chopsticks ex-ample of Spear et al. where there are two identical sticks ofwood (in terms of structure and form) one with the func-tion to handle food because it was designed and manufac-tured to handle food, and one with a disposition but not afunction to handle food because it came to have its struc-ture incidentally [17].Therefore, a therapeutic indication (function) is a sub-type of therapeutic potential (disposition), where theFig. 3 A drug product is the bearer of a therapeutic functionHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 6 of 11former is the result of an intensively planned and exe-cuted manufacturing process and the latter is broaderand includes effects of drug products that are discoveredafter they are manufactured. For example, bupropiontablets were originally designed and manufactured fordepression. Later, researchers discovered that they alsohelped with smoking cessation, so companies startedmanufacturing these tablets for smoking cessation inaddition to depression, ergo a new therapeutic function.Bupropion tablets manufactured prior to the changehave one function; bupropion tablets manufactured afterthe change have two functions.This distinction also helps to differentiate thera-peutic indications from adverse effects of drug prod-ucts. The realization of a disposition of an oxycodonetablet to cause constipation is an undesired effect. Itis not a therapeutic indication (function). The dispos-ition of oxycodone tablets to create a dependence (oraddiction) is also an adverse event. We would repre-sent in DrOn the dispositions to adverse events. Perthe Ontology of Adverse Events (OAE), an adverseevent is a process [18]. This process is typically therealization of certain dispositions of the drug product.Thus, we have also identified a key way to link DrOnto OAE: a disposition of a drug product (DrOn) is re-alized by an adverse event process (OAE).In some cases, a type of disposition can have someinstances whose realizations are therapeutic and otherinstances whose realizations are adverse events. Forexample, erythromycin when used to treat infectioncan have the adverse event of diarrhea caused by itsdisposition to increase gastric motility. However, thesame disposition to increase gastric motility is some-times used therapeutically to treat gastroparesiscaused by diabetes mellitus. In other cases, the typeof disposition can have some instances whose realiza-tions are both therapeutic effects and adverse events,such as when the therapeutic effect is taken to an ex-treme (e.g., bleeding from anti-coagulant therapy andhypotension from anti-hypertensive therapy).Anti-hypertensive therapeutic functionDef: a therapeutic function of a drug product that is re-alized by administration of the drug product resulting ina decrease of systemic arterial pressure.Hypertension is a sustained elevation in the pressureexerted by blood on the systemic arteries (as opposed topulmonary arteries) of an organism. This condition iswell known to be a risk for multiple morbidities includ-ing coronary artery disease, stroke, and kidney disease.Drug products manufactured to treat hypertension allhave the disposition of lowering this pressure when ad-ministered in the proper form and according to theproper route of administration.Anti-malarial therapeutic functionDef: a therapeutic function of a drug product that is re-alized by administration of the drug product resulting increation of a material basis of a resistance to malaria in-fection disposition.In other words, a drug product like a choloroquinetablet confers upon administration a protective resist-ance to an infection of a certain kind, namely protectiveresistance to individuals of one of four Plasmodium spe-cies. The Infectious Disease Ontology defines protectiveresistance as: A disposition that inheres in a materialentity in virtue of the fact that the entity has a part (e.g.a gene product), which itself has a disposition to mitigatedamage to the entity [19]. Resistance to an infectiousagent then is a type of protective resistance. This kind ofresistance can be either acquired or innate (e.g., an ex-treme but common case of innate resistance is the resist-ance of one species to the infectious agents thatcommonly infect another species). Acquired resistancecan occur through acquired immunity, through adminis-tration of anti-infective drug products, and throughother mechanisms.An anti-malarial is thus a drug product that, when ad-ministered, confers a protective resistance to humansagainst the four species of Plasmodium that cause mal-aria in humans. DrOn imports resistance to malaria in-fection, which is itself defined as A resistance to infectionby P. vivax, P. ovale, P. malariae, and/or P. falciparum.Analgesic therapeutic functionDef: a therapeutic function of a drug product that is re-alized by administration of the drug product resulting inblocked realization of a disposition to pain.We follow the definition of pain by Smith and Ceus-ters as a type of process: an unpleasant experience onthe part of a human subject that is both sensory andemotional and that is of a type that is either canonicalpain  or phenomenologically indistinguishable from ca-nonical pain [20]. The physical basis of the pain canrange from activation of the nociceptive system (canon-ical pain) to damage to the nociceptive system (neuro-pathic pain) to changes in the cognitive system (e.g.,pain behavior without nociception).The net result is that certain physical changes result ina disposition to experience pain, and the ultimate effectof analgesics is to block the realization of this dispos-ition. Restated, they confer a blocking disposition to paindispositions.Adding mechanisms of action for opioid analgesicsWe defined six dispositions, all of which inhere in mole-cules. There are two each (agonistic and antagonistic)for each of the three major opioid receptors that wereHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 7 of 11relevant to the E-FORCSE use case. These definitions allfollow the template:A disposition of a molecule to bind an instance of <kind of receptor > in a manner that < activates,inhibits > the realization of the biological function ofthe receptor.We then link the function to the molecule usingOWL2 axioms of the form:<molecule> subClassOf (is bearer ofsome <disposition>)(where is bearer of  is an object property, <molecule >is a class, and < disposition > is a class).Note that for compounds in DrOn that are repre-sented by ChEBI class internationalized resource iden-tifiers (IRIs), we are asserting this axiom directly onChEBI classes (which is true of nearly all the opioidcompounds). Furthermore, we note that we do notyet represent in DrOn the processes that realize thesedispositions, nor the other participants in these pro-cesses (e.g., the mu receptor itself ). The reason is thatour use cases have not yet required it. However, weacknowledge that it might be useful to query for alldrug products whose ingredients act on mu receptors,regardless of whether the action inhibits or activatesthe receptor. It therefore remains future work toenhance the representation in DrOn to include theprocesses that realize molecular dispositions and theirparticipants. We note that the Gene Ontology [21]has terms for the processes (e.g., GO:0031698 beta-2adrenergic receptor binding), and the ProteinOntology [22] has terms for the receptors (e.g.,PR:000001193 beta-2 adrenergic receptor) that shouldbe reused for this purpose. For more on DrOns rep-resentation of drug ingredients (molecules and aggre-gates of them) and their dispositions and roles, seeHanna et al. [1].All told, we added at least one of the six dispositionsto 59 molecule types (Table 1).We note that not all drug products with (an ingredientthat has) an opioid agonist mechanism have an analgesicindication. For example, loperamide tablets are used totreat diarrhea and are not indicated for pain relief (be-cause they bind opioid receptors in the nerve cells of thegut almost exclusively).PRISM term requestWe added 108 classes in response to the PRISM termrequest: 89 classes have an equivalent class axiom, and19 drug solution classes have necessary axioms only.Figure 4 shows the class amoxicillin suspension and thatthe Fact++ reasoner has inferred that the 10 amoxicillinsuspension classes (that derive from RxNorm) are sub-sumed under it.Using dron-query tool for the use casesAnti-hypertensive queryWe used dron-query to query several lists of RxCuis forthe anti-hypertensive use case. Specifically, we queriedall the RxCuis for anti-hypertensive products whose in-gredients had one of the following mechanisms of ac-tion, with one set of RxCuis per mechanism of action:beta-adrenergic blockade, calcium channel blockade,NKCC2 inhibition (loop diuretics), sodium-chloride co-transporter inhibition (thiazide and thiazide-like di-uretics), angiotensin converting enzyme inhibition, andangiotensin receptor blockade.The template for each query in Manchester syntax wasthe following:'drug product' and ('is bearer of' some'anti-hypertensive function') and(has_proper_part some ('has granularpart' some ('is bearer of' some<disposition to bind some enzyme/receptor>)))Table 1 Counts of molecules with various opioid mechanismsof actionAction Type of receptormu kappa deltaAgonist 28 11 9Antagonist 4 3 3Fig. 4 The new class amoxicillin suspension and its inferred childrenin DrOnHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 8 of 11For beta blockers, the exact query is:'drug product' and ('is bearer of' some'anti-hypertensive function') and(has_proper_part some ('has granularpart' some ('is bearer of' some 'non-activating competitive beta-adrenergicreceptor binding disposition')))This query returns 612 classes representing drug prod-ucts at multiple levels of granularity (i.e., metoprolol oraltablet, metoprolol 50 mg oral tablet, Lopressor 50 mgoral tablet).To get a set of NDCs for anti-hypertensive drug prod-ucts with beta blocker ingredients, the query is:packaged drug product and(has_proper_part some ('is bearer of'some 'anti-hypertensive function') and(has_proper_part some ('has granularpart' some ('is bearer of' some 'non-activating competitive beta-adrenergicreceptor binding disposition'))))This query returns 4,831 classes and their NDCs.Anti-malarial queryFor all drug products with an anti-malarial function, thequery is:'drug product' and ('is bearer of' someanti-malarial function)This query returns 260 drug products and theirRxCuis.Opioid analgesic queryFor all analgesics that have a mu agonist as an ingredi-ent, the query is:'drug product' and ('is bearer of' someanalgesic) and (has_proper_part some('has granular part' some ('is bearer of'some 'mu agonist')))This query returns 3034 drug products and theirRxCuis. The analogous query for packaged drug prod-ucts (not shown) returns 8142 packaged drug productsand their NDCs.DiscussionBased on use cases from three research projects, we ex-tended DrOn with three therapeutic functions and sixmechanisms of action of drug products. We also added108 classes in response to term requests, including amoxi-cillin suspension and chloroquine tablet. These classeshave different semantics than RxNorm classes becausethey do not exhaustively list all active ingredients. For ex-ample, the new class amoxicillin suspension subsumes allsuspensions that have amoxicillin as one ingredient, in-cluding compound drug products such as amoxicillin/cla-vulanate oral suspension. Lastly, we illustrated the use ofthe dron-query software tool for the three use cases.The three therapeutic functionsanti-hypertensive,anti-malarial, and analgesicare fairly diverse, providingpreliminary evidence that our approach is general. Thereare no common biological pathways, or even organ sys-tems, to hypertension, malaria, and pain. Although theblood is involved with both hypertension and malaria,the anti-hypertensive drugs all have a mechanism of ac-tion that take place elsewhere (especially the kidneys,where they bind cellular receptors).We have laid an ontological basis for representing add-itional therapeutic indications, off-label usages, and adverseeffects of drug products. And per the original design ofDrOn, we capture the therapeutic indication on the appro-priate entity (drug product and not molecule). In so doing,we do not mistake ophthalmic timolol for an anti-hypertensive or oral timolol for a glaucoma drug. Repre-senting actual dispositions of drug products towards ad-verse events, and thereby linking DrOn and OAE, remainsfuture work.DrOn is extensible in the manner described for repre-sentation of additional therapeutic functions. Ongoingwork at present includes representing the therapeuticfunction to treat asthma.In addition, our past approach to representing mecha-nisms of action was in this work easily adapted to opiatesand opioids. However, we note that all the mechanisms ofaction represented in DrOn to date involve drug moleculesbinding to molecular entities in the cellular membrane (e.g.,receptors, enzymes, and transporters). Representing mecha-nisms of action that involve other kinds of processes mightnot follow this pattern. For example, because anti-infectivesact on a symbiont of the organism to which they are ad-ministered, as opposed to the organism itself, representingthem could be more complex. Nevertheless, the space ofcompounds that exert a biological effect through receptorbinding and enzyme inhibition is large and diverse.We do not at present relate the molecular mechanism(s)of action of a molecule to the relevant therapeutic func-tions of a drug product that incorporates the molecule, orvice versa. Thus, for atenolol oral tablet there is no con-nection between its function to treat hypertension and thedispositions of its atenolol molecules to bind beta-adrenergic receptors. This task is future work. Becauseour representation does not require capturing the rela-tionship between mechanisms of action and therapeuticHogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 9 of 11functions however, we can represent in DrOn the thera-peutic functions of drug products for which no mechan-ism of action at the molecular level is yet known.For the PDMP research, we have begun but not com-pleted work on whether particular opiate and opioid-containing drug products are short vs. long actingformulations. We merely note here that duration of actionis a property of not just the half-life of the active ingredi-ent(s) but also the particular way a dose is manufactured(e.g., normal vs. delayed vs. extended release tablets andcapsules) and the genotype of the individual (e.g., singlenucleotide polymorphisms that result in poor vs. ultra-rapid metabolism of drugs and prodrugs).This work was limited to the extensions necessary tomeet specific research use cases. We have thus not stud-ied clinical uses. However, we note that were a systemsuch as an EHR to recommend oral vancomycin to treatendocarditis, or ophthalmic timolol to treat hyperten-sion, a clinician might begin to doubt the system. Inaddition, for our research use cases, we have not yetcompared counting anti-hypertensives, hypertensive pa-tients, or patients with resistant hypertension usingDrOn to counting them using NDF-RT or other arti-facts. This task remains as future work.We also did not represent the intended routes of ad-ministration of drug products, which prevented inclu-sion of an equivalent class axiom on all classes added inresponse to PRISM term requests. This task also re-mains future work. We note that the Vaccine Ontologyhas a set of classes representing routes of administrationthat will likely be applicable [23].We also note that none of the use cases discussed hererelate yet to pharmacogenomics or analyzing pharmaco-dynamics or pharmacokinetics of drugs. However DrOn isbeing used in other projects in this manner: we refer the in-terested reader to Brochhausen et al. [24], which discussesontological representations of potential drug-drug interac-tions and their pharmacodynamics and pharmacokinetics,and how these representations reuse DrOn classes.ConclusionsWe successfully captured therapeutic indications of drugproducts in the Drug Ontology as functions and othertherapeutic uses of drugs as dispositions, in keeping withthe definitions of disposition and function in Basic For-mal Ontology. We represented the anti-hypertensive, anti-malarial, and analgesic indications of numerous drugproducts in DrOn in this manner. We also representedthe mechanisms of action of opioid analgesics (and otheropioid drug products), and we included over 100 newclasses in response to a term request from the PRISM pro-ject. We also demonstrated how to use the dron-querytool to extract from DrOn subsets of drug-product andpackaged-drug-product classes and their annotations forvarious use cases. To date, our results show promise thatour methods are applicable to other therapeutic indica-tions and mechanisms of action of drug products andtheir ingredients, respectively.AbbreviationsBFO: Basic formal ontology; CDRN: Clinical data research network;CYP: Cytochrome P450; DrOn: Drug ontology; E-FORCSE: Floridas electronic-Florida online reporting of controlled substance evaluation program;EHR: Electronic health record; ICEMR: International center of excellence formalaria research; NDC: National drug code; NDF-RT: National drug file referenceterminology; NIAID: National institute for allergy and infectious disease;OAE: Ontology of adverse events; OBO: Open biological and biomedicalontologies; OWL2: Web ontology language version 2; PCORnet: National patient-centered clinical research network; PDMP: Prescription drug monitoring program;PRISM: Program for resistance, immunology, surveillance and modeling ofmalaria in uganda; RDF: Resource description framework; XML: eXtensiblemarkup languageAcknowledgementsWe would like to thank Werner Ceusters for discussions on BFO definitionsduring which he also pointed out the existence of the recent publicationclarifying functions and dispositions as defined by Basic Formal Ontology.This acknowledgement does not imply that he agrees with all thestatements in this manuscript.FundingThis work was supported in part by the NIH/NCATS Clinical and TranslationalScience Awards to the University of Florida UL1 TR000064/UL1 TR001427 andby award CDRN-1501-26692 from the Patient Centered Outcomes ResearchInstitute (PCORI). The content is solely the responsibility of the authors anddoes not necessarily represent the official views of NIH/NCATS or PCORI.Availability of data and materialsThe Drug Ontology itself is available at the following permanent URL: http://purl.obolibrary.org/obo/dron.owl. Also, the BitBucket repository where wemanage DrOn development is located at: https://bitbucket.org/uamsdbmi/dron. The various spreadsheets we created in the process of this work arepublicly available on Google Docs at the URLs cited in the manuscript.Authors contributionsAuthor WRH conceived the work and drafted the initial version of themanuscript, a revised manuscript in response to comments of reviewers forthe International Conference on Biomedical Ontology, and a manuscript inresponse to the invitation to submit to the Journal. Author AH assisted withrefining the ontological analysis of therapeutic indications and creatingdefinitions of anti-hypertensive, anti-malarial, and analgesic. Authors AS,BB, MD, RE, TM, and MV performed work as described in the Methodssection, and also helped with the definitions just mentioned. Author CDhelped define the opioid analgesic use cases and identified additionalsource materials for creating comprehensive lists of opioid compounds andtheir receptor binding. All authors reviewed, provided feedback on, andapproved the manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationThis research does not describe any individuals personal information andthus this consent is also not applicable.Ethics approval and consent to participateThis research is not human subjects research and thus approval and consentare not applicable.Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 10 of 11Received: 11 November 2016 Accepted: 24 February 2017Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 DOI 10.1186/s13326-017-0162-9RESEARCH Open AccessMatching disease and phenotypeontologies in the ontology alignmentevaluation initiativeIan Harrow1*, Ernesto Jiménez-Ruiz2, Andrea Splendiani3, Martin Romacker4, Peter Woollard5,Scott Markel6, Yasmin Alam-Faruque7, Martin Koch8, James Malone9 and Arild Waaler2AbstractBackground: The disease and phenotype track was designed to evaluate the relative performance of ontologymatching systems that generate mappings between source ontologies. Disease and phenotype ontologies areimportant for applications such as data mining, data integration and knowledge management to supporttranslational science in drug discovery and understanding the genetics of disease.Results: Eleven systems (out of 21 OAEI participating systems) were able to cope with at least one of the tasks in theDisease and Phenotype track. AML, FCA-Map, LogMap(Bio) and PhenoMF systems produced the top results forontology matching in comparison to consensus alignments. The results against manually curated mappings provedto be more difficult most likely because these mapping sets comprised mostly subsumption relationships rather thanequivalence. Manual assessment of unique equivalence mappings showed that AML, LogMap(Bio) and PhenoMFsystems have the highest precision results.Conclusions: Four systems gave the highest performance for matching disease and phenotype ontologies. Thesesystems coped well with the detection of equivalence matches, but struggled to detect semantic similarity. Thisdeserves more attention in the future development of ontology matching systems. The findings of this evaluationshow that such systems could help to automate equivalence matching in the workflow of curators, who maintainontology mapping services in numerous domains such as disease and phenotype.Keywords: Biomedical ontology, Ontology alignment, OAE, Evaluation, Phenotype, DiseaseBackgroundThe Pistoia Alliance Ontologies Mapping project1 was setup to find or create better tools and services for mappingbetween ontologies (including controlled vocabularies) inthe same domain and to establish best practices for ontol-ogy management in the Life Sciences. The project hasdeveloped a formal process to define and submit a requestfor information (RFI) from ontology matching systemproviders to enable their evaluation.2 A critical compo-nent of any ontology alignment system is the embeddedmatching algorithm, therefore the Ontologies Mapping*Correspondence: ian.harrow@pistoiaalliance.orgEqual contributors1Pistoia Alliance Ontologies Mapping Project, Pistoia Alliance Inc, USAFull list of author information is available at the end of the articleproject is supporting their development and evaluationthrough sponsorship and organisation of the Disease andPhenotype track (added in 2016) for the OAEI campaign[1]. In this paper we describe the experiences and resultsin the OAEI 2016 Disease and Phenotype track.3The Disease and Phenotype track is based on a realuse case where it is required to find two pairwise align-ments between disease and phenotype ontologies: (i)Human Phenotype Ontology [2] (HP) to MammalianPhenotype Ontology [3] (MP), and (ii) Human DiseaseOntology [4] (DOID) to Orphanet Rare Disease Ontol-ogy4 (ORDO). The first task maps between human andthe more general mammalian phenotype ontologies. Thisis important for translational science in drug discovery,since mammalian models such as mice are widely used© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 2 of 13to study human diseases and their underlying genetics.Mapping human phenotypes to other mammalian phe-notypes greatly facilitates the extrapolation from modelanimals to humans. The second task maps between twodisease ontologies: the more generic DOID and the morespecific ORDO, in the context of rare human diseases.These ontologies can support investigative studies tounderstand how genetic variation can cause or contributeto disease.Currently, mappings between the aforementionedontologies within the disease and phenotype domain aremostly generated manually by bioinformaticians and dis-ease experts. Inclusion of automated ontology matchingsystems into such curation workflows is likely to improvethe efficiency and scalability of this process to expand thecoverage across many source ontologies. Automation ofmappings is also important because the source ontologiesare dynamic, often having more than ten versions per yearwhich means the mappings must be maintained to remainuseful and valid.PreliminariesIn this paper we assume that the ontologies are repre-sented using the OWL 2 Web Ontology Language [5],which is a World Wide Web Consortium (W3C) rec-ommendation.5 Description Logics (DL) are the formalunderpinning of OWL 2 [6].An ontology mapping (also called match or correspon-dence) between entities of two ontologies O1,O2 is typ-ically represented as a 4-tuple ?e, e?, r, c? where e and e?are entities of O1 and O2, respectively; r ? {,,?} isa semantic relation; and c is a confidence value, usually,a real number within the interval (0 . . . 1]. Mapping con-fidence intuitively reflects how reliable a mapping is (i.e.,1 = very reliable, 0 = not reliable).An ontology alignment M between two ontologies,namely O1,O2, is a set of mappings between O1 andO2. In the ontology matching community, mappings aretypically expressed using the RDF Alignment format [7].In addition, mappings can also be represented throughstandard OWL 2 axioms (e.g., [8]). This representationenables the use of the OWL 2 reasoning infrastructurethat is currently available.When mappings are translated into OWL 2 axioms, analigned ontology OM = O1 ? O2 ? M is the result ofmerging the input ontologies and an alignment betweenthem. The aligned ontology is also an OWL 2 ontology.An ontology matching system is a program6 that, giventwo input ontologies O1 and O2, generates an ontologyalignmentMS.An ontology matching task is typically composed byone or more pairs of ontologies with their correspondentreference alignments MRA. Reference alignments can beof different nature: gold standards, silver standards andbaselines. Gold standards are typically (almost) completemapping sets that have been manually curated by domainexperts, while silver standard mapping sets are not nec-essarily complete nor correct. Finally, baseline mappingstypically represent a highly incomplete set of the totalmappings. In this paper we use a type of silver standardthat has been created by voting the mappings produced byseveral matching systems. In the remainder of the paper,we refer to this (silver standard) mapping set as consensusalignments.The standard evaluation measures, for a system gen-erated alignment MS, are precision (P), recall (R) andf-measure (F) computed against a reference alignmentMRA as follows:P = |MS ? MRA||MS| , R =|MS ? MRA||MRA| , F = 2 ·P · RP + R(1)Standard precision and recall have, however, limitationswhen considering the (OWL 2) semantics of the inputontologies and the mappings. Hence a mapping m suchthat m ? MS and m ? MRA will penalise the stan-dard precision value even though OMRA |= m, that is,m is inferred or entailed (using OWL 2 reasoning) by theunion of the input ontologies O1 and O2 and the refer-ence mappingsMRA. Analogously, a mappingm such thatm ? MS andm ? MRA will penalise standard recall, eventhough the aligned ontology OMS can entail m. In thispaper we adopt the notion of semantic precision and recallas defined in Eqs. 2 and 3 to mitigate the limitations of thestandard measures (the interested reader please refer to[9, 10] for alternative definitions).Semantic precision and recall, as presented in this paper,may still suffer from some limitations [11]. In order toreduce the impact of these limitations, when computingsemantic precision and recall, equivalence mappings (?)are split into two subsumption mappings ( and ).Note that when evaluating the mappings produced by amatching system against (incomplete) baseline mappings,only semantic recall should be taken into account.P(sem) = |{m ? MS | OMRA |= m}||MS| (2)R(sem) = |{m ? MRA | OMS |= m}||MRA| (3)An ontology is incoherent [12] if it contains logicalerrors in the form of unsatisfiable concepts. If the unionof the input ontologies O1 and O2 and the referenceHarrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 3 of 13mappings MRA is incoherent, semantic precision andrecall, as defined in Eqs. 2 and 3, may lead to unex-pected results. In this case, mapping repair (e.g., [1214])techniques should be applied before computing semanticprecision and recall.MethodsThe Ontology Alignment Evaluation Initiative7 (OAEI)is an annual campaign for the systematic evaluation ofontologymatching systems [1, 1517]. Themain objectiveis the comparison of ontology matching systems on thesame basis and to enable the reproducibility of the results.The OAEI included 9 different tracks organised by dif-ferent research groups and involving different matchingtasks.The novel Disease and Phenotype8 track was one of thenew additions in the OAEI 2016 campaign. The track aimsat evaluating the performance of systems in a real-worlduse case where pairwise alignments between disease andphenotype ontologies are required.The Disease and Phenotype track closely followed theOAEI phases as summarised in Fig. 1.DatasetThe Disease and Phenotype track comprises two match-ing tasks that involve the alignment of the HumanPhenotype Ontology (HP), the Mammalian PhenotypeOntology (MP), the Human Disease Ontology (DOID),and the Orphanet Rare Disease Ontology (ORDO).Table 1 shows the metrics provided by BioPortal of theseontologies.Task 1: pairwise alignment of the HP and the MP ontolo-gies (HP-MP matching task).Task 2: pairwise alignment of the DOID and theORDO ontologies (DOID-ORDOmatching task).Preparation phaseAs specified by the OAEI the ontologies and (public)reference alignments were made available in advanceduring the first week of June 2016. The ontologiesand mappings were downloaded from BioPortal [18] onJune 2nd.The mappings were obtained using a script that, given apair of ontologies, uses BioPortals REST API9 to retrieveall mappings between those ontologies. We focused onlyon skos:closeMatch (BioPortal) mappings10 as suggested in[19], and we represented them as equivalencemappings.11The BioPortal-based alignment between HP and MP con-sisted in 639 equivalence mappings, while the alignmentbetween DOID and ORDO included 1,018 mappings.Mappings were made available in both RDF Alignmentand OWL 2 formats.The preparatory phase gives the opportunity to bothOAEI track organisers and participants to find and correctproblems in the datasets. During this phase we noticedthat the BioPortal mappings were highly incomplete.12Hence, the participants were notified that the BioPortal-based mappings were to be used as a baseline and not as agold standard reference alignment. Given the limitationsof the BioPortal mappings we were in need of creatinga (blind) consensus reference alignment to perform the(automatic) evaluation (see details in the Evaluation phasesection).All (open) OAEI datasets were released on July 15th,2016 and did not evolve after that.Execution phaseSystem developers had to implement a simple inter-face and to wrap their tools including all requiredlibraries and resources in order to use the SEALSinfrastructure.13 The use of the SEALS infrastructureensures that developers can perform a full evaluationlocally and eases the reproducibility and comparability ofthe results.This phase was conducted between July 15th andAugust 31st, 2016. During this time OAEI organisersattended technical issues reported by the developers. Wealso requested system developers to register their systemsand their intention to participate in the different OAEItracks by July 31st. Thirty systems were registered, fromwhich 14 seemed potential participants of theDisease andPhenotype track.Evaluation phaseParticipants were required to submit their wrapped toolsby August 31st, 2016. From the 30 registered systems only21 were finally submitted, and 13 were annotated (by thesystem developers) as participants of theDisease and Phe-notype track. The final results were published on theOAEIwebsite by October 15th.Fig. 1 Phases of the OAEI 2016 Disease and Phenotype track. Important Dates: D1 (publication of preliminary datasets), D2 (final datasets released), D3(system registration), D4 (system submission), D5 (publication of evaluation results and presentation in the Ontology Matching workshop [1])Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 4 of 13Table 1 Metrics of the track ontologies. Source: NCBI BioPortal on 2nd June 2016Ontology Number of axioms Number of classes Maximum depth Avg. number of childrenHP 137,289 11,786 15 3MP 129,036 11,721 15 3DOID 124,362 9248 12 3ORDO 188,991 12,936 11 16Note that the metric average number of children excludes the leaf nodesAlgorithm 1 Steps followed in the evaluationInput:O1,O2: ontologies in matching task;MRAm :manually generated alignment; Systems: ontologymatching systems participating in the task. Generation of system alignments with SEALS infras-tructure:1: for each Systemi in Systems do2: MSi ? Systemi(O1,O2)  Computes systemalignment3: end for Generation of consensus alignments:4: MRAc2 ? ConsensusAlignment(MS1 . . .MSn, 2) With vote 25: MRAc3 ? ConsensusAlignment(MS1 . . .MSn, 3) With vote 3 Aligned ontologies for consensus reference align-ments:6: OMRAc2 ? O1 ? O2 ? MRAc2  RepairMRAc2 if required7: OMRAc3 ? O1 ? O2 ? MRAc3  RepairMRAc3 if required Evaluation for each system generated alignments:8: for eachMSi inMS1 . . .MSn do Aligned ontology forMSi :9: OMSi ? O1 ? O2 ? MSi  RepairMSi if required Results against consensus alignment with vote 2:10: P2 ? SemanticPrecision(MSi ,OMRAc2)11: R2 ? SemanticRecall(MRAc2 ,OMSi) Results against consensus alignment with vote 3:12: P3 ? SemanticPrecision(MSi ,OMRAc3)13: R3 ? SemanticRecall(MRAc3 ,OMSi) Results against manually generated alignment:14: Rm ? SemanticRecall(MRAm ,OMSi) Manual assessment of unique system mappings:15: USi ? UniqueMappings(MSi ,OMRAc2)16: {Pm,PC,NC} ? ManualAssessment(USi)17: end forThe evaluation for the Disease and Phenotype track wassemi-automatic with support of the SEALS infrastruc-ture. Systems were evaluated according to the followingcriteria for each of the matching tasks of the Disease andPhenotype track: Semantic precision and recall with respect to theconsensus alignments. Semantic recall with respect to manually generatedmappings. Manual assessment of unique mappings produced bya participant system.Algorithm 1 formalizes the steps followed in the evalua-tion for each of theDisease and Phenotypematching tasks.The following subsections below comment on the mainpoints of the evaluation process.Consensus alignments. The consensus alignments areautomatically generated based on the alignments pro-duced by the participating systems in each of thematchingtasks of the track. For the evaluation we have selected theconsensus alignments of vote=2 (i.e., mappings suggestedby two or more systems) and vote=3 (i.e., mappings sug-gested by three or more systems). In the case where bothan equivalence and a subsumption mapping contribute tothe consensus, the equivalence relationship prevails overthe subsumption. The use of vote=2 and vote=3 wasmotivated by our experience in the creation of consen-sus alignments [20]. Consensus alignments with vote?4are typically highly precise but also very incomplete unlessthe number of contributing systems is significant.14 Notethat, when there are several systems of the same fam-ily (i.e., systems participating with several variants), their(voted) mappings are only counted once in order toreduce bias.15Note that consensus alignments have numerous lim-itations. It allows us to compare how the participatingsystems perform only in relation to each other. Some ofthe mappings in the consensus alignments may be erro-neous (false positives), as it only requires 2 or 3 systemsto agree on the erroneous mappings they find. Further-more, the consensus alignments may not be complete,as there will likely be correct mappings that no or onlyone system is able to find. Nevertheless, consensus align-ments help to provide some insights into the performanceof a matching system.Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 5 of 13Semantic precision and recall. As introduced in thePreliminaries section, the semantic precision and recalltake into account the implicit knowledge derived fromthe ontologies and the mappings via OWL 2 rea-soning.16 Hence, the methods SemanticPrecision andSemanticRecall in Algorithm 1 receive as input a set ofmappings M and a coherent ontology O. Both methodsreturn as output the value of |M?||M| whereM? is a subset ofM such that the mappingsm ? M? are entailed byO (i.e.,O |= m).Manually generated mappings. These reference map-pings were created through manual curation by eightdisease informatics experts, who are authors of this paper,all working within or for the pharmaceutical industry forthree areas of phenotype and disease; namely carbohy-drate and glucose metabolism, obesity and breast cancer.These sets of reference mappings comprised of 29 pair-wise mappings between HP and MP and 60 pairwisemappings between DOID and ORDO across the threeareas. They included some relationships of equivalence,but most of them represented subsumption relationships.The three areas were selected as representative sampleswhich were known already to be present across the foursource ontologies. Inclusion of these manually definedmappings enabled a real-world evaluation of recall for thetwo matching tasks. The future editions of the track willincrease the number of manual mappings through inclu-sion of additional areas relevant to the phenotype anddisease domain.Unique mappings and manual assessment. Uniquemappings are mappings generated by an ontology match-ing system that have not been (explicitly) suggestedby any of the other participating systems, nor entailedby the aligned ontology using the consensus alignmentwith vote=2(OMRAc2). The method UniqueMappings inAlgorithm 1 receives as input a set of mappings M andthe (coherent) ontology OMRAc2 and returns as output M?where M? ? M such that the mappings m ? M? are notentailed byOMRAc2(i.e.,OMRAc2 |= m).Manual assessment over unique mappings has beenperformed by an expert in disease informatics from thepharmaceutical industry. This assessment aims at comple-menting the evaluation against the consensus alignmentsof those mappings that, although being suggested or votedby only onematching system,may still be correct.We havefocused the assessment on unique equivalencemappingsand we have manually evaluated up to 30 mappings foreach system in order to (roughly) estimate the percentageof correct mappings (i.e., precision, Pm in Algorithm 1)and the positive/negative contribution to the total num-ber of unique mappings (PC and NC in Algorithm 1),that is, the weight of the correct (i.e., true positives) andincorrect (i.e., false positives) mappings. Intuitively, thepositive contribution (see Eq. 4) of a system producing asmall set of unique mappings will most likely be smallerthan a system producing a larger set of unique (andmostlycorrect) mappings. The negative contribution (see Eq. 5)will weight the number of incorrect uniquemappings withrespect to the total. Negative and positive contributions,for a set of unique mappings USi computed by a system i,are defined as follows:PositiveContribution(USi)=??USi?? · Precision (USi)?nj=1??USj?? (4)NegativeContribution(USi)=??USi?? · (1 ? Precision (USi))?nj=1??USj??(5)ResultsWe have run the evaluation of the Disease and Phenotypetrack in a Ubuntu Laptop with an Intel Core i7-4600UCPU @ 2.10 GHz x 4 and allocating 15 Gb of RAM. Fromthe 13 systems registered to the track (out of 21 OAEI par-ticipants), 11 systems have been able to cope with at leastone of the Disease and Phenotype matching tasks withina 24 h time frame. Results for all OAEI tracks have beenreported in [1].Participating systemsAML [21, 22] is an ontology matching system originallydeveloped to tackle the challenges of matching biomedicalontologies. While its scope has since expanded, biomed-ical ontologies have remained one of the main drivesbehind its continued development. AML relies on the useof background knowledge and it also includes mappingrepair capabilities.DiSMatch [23] estimates the similarity among conceptsthrough textual semantic relatedness. DiSMatch relies ona biomedical domain-adapted variant of a state-of-the-art semantic relatedness measure [24], which is based onExplicit Semantic Analysis.FCA-Map [25] is an ontology matching system based onFormal Concept Analysis (FCA). FCA-Map attempts topush the envelope of the FCA to cluster the commonalitiesamong classes at various levels.LogMap [26, 27] relies on lexical and structural indexesto enhance scalability. It also incorporates approximatereasoning and repair techniques to minimise the numberof logical errors in the aligned ontology.Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 6 of 13LogMapBio [28] extends LogMap to use BioPortal [18]as a (dynamic) provider of mediating ontologies, insteadof relying on a few preselected ontologies. LogMap-Bio retrieves the most suitable top-10 ontologies for thematching task.LogMapLt is a lightweight variant of LogMap, whichessentially only applies (efficient) string matching tech-niques.LYAM++ [29] is a fully automatic ontology matching sys-tem based on the use of external sources. LYAM++ appliesa novel orchestration of the components of the matchingworkflow [30].PhenomeNET [31] alignment system comes in threeflavours, which rely on three different versions of thePhenomeNET ontology [32]. PhenomeNET-Plain (Phe-noMP) relies on a plain ontology which only uses theaxioms provided by the HP ontology and the MP ontol-ogy. PhenomeNET-Map (PhenoMM) utilizes additionallexical equivalence axioms between HP and MP pro-vided by BioPortal. Finally, PhenomeNET-Full (PhenoMF)relies on an extended version of the PhenomeNET ontol-ogy with equivalence mappings to the DOID and ORDOontologies obtained via BioPortal and the AML matchingsystem [21].XMap [33] is a scalable matcher that implements parallelprocessing techniques to enable the composition of basicontology matchers. It also relies on the use of externalresources such as the UMLS Metathesarus [34].Use of specialised background knowledgeThe use of (specialised) background knowledge is allowedin the OAEI, but participants are required to spec-ify which sources their systems rely on to enhance thematching process. AML has three sources of backgroundknowledge which can be used as mediators betweenthe input ontologies: the Uber Anatomy Ontology [35](Uberon), the Human Disease Ontology [4] (DOID)and the Medical Subject Headings17 (MeSH). LYAM++also makes use of the Uberon ontology [35]. LogMap-Bio uses BioPortal [18] as dynamic mediating ontologyprovider, while LogMap uses normalisations and spellingvariants from the general (biomedical) purpose UMLSLexicon.18 XMAP uses synonyms provided by theUMLS Metathesaurus [34]. Finally, PhenoMM, PhenoMFand PhenoMP rely on different versions of the Phe-nomeNET19 ontology [32] with variable complexity asdescribed above.Evaluation against BioPortal (baseline) mappingsTable 2 shows the results in terms of semantic recallagainst the baseline mappings extracted from BioPortal asdescribed in the Methods Section (Preparation phase).In the DOID-ORDO task, LYAM++ failed to completethe task while PhenoMM and PhenoMP produced emptymapping sets.BioPortal mappings mostly represent correspondenceswith a high degree of lexical similarity and, as expected,most of the systems managed to produce alignments witha very high recall. DiSMatch, LYAM++, PhenoMM (inthe DOID-ORDO task) and PhenoMP were the exceptionand produced very low results with respect to the base-line mappings. As mentioned in the Methods Section,since the BioPortal mappings were highly incomplete, theresults in terms of (semantic) precision were not signif-icant. For this reason, we needed to create consensusalignments for each task.Creation of consensus alignmentsIn the MP-HP matching task 11 systems were able toproduce mappings. Mappings voted by LogMap and Phe-nomeNET families were only counted once, and hencethere were 7 independent system groups contributing tothe consensus alignment. In the DOID-ORDO match-ing task 8 systems generated mappings and there were 6independent system groups contributing to the consensusalignment.Table 3 (resp. Table 4) shows the size of the differ-ent consensus alignments from vote=1, i.e., mappingssuggested by one or more system groups, to vote=7(resp. vote=6), i.e., mappings suggested by all systemgroups, in the HP-MP matching task (resp. DOID-ORDOtask). It is noticeable that in the HP-MP task therewere 0 mappings where all systems agreed, while inthe DOID-ORDO task there were only 36. The num-ber of mappings suggested by one system or more isspecially large because PhenomeNET systems produce alarge number of subsumption mappings. If only equiv-alence mappings of PhenomeNET systems are takeninto account, the number of mappings with vote=1would be 3433 in the HP-MP task and 2708 in theDOID-ORDO task.Table 2 Recall against BioPortal (baseline) mappingsSystem AML DiSMatch FCA-Map LYAM++ LogMap LogMapBio LogMapLt PhenoMF PhenoMM PhenoMP XMapHP-MP 1.0 0.25 0.998 0.014 0.997 1.0 0.994 1.0 1.0 0.412 0.995DOID-ORDO 0.993 0.048 0.984 - 0.942 0.950 0.943 0.994 0.0 0.0 0.967Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 7 of 13Table 3 Consensus alignments for the HP-MP matching taskMin. Votes 1 2 3 4 5 6 7Mappings 217039 2308 1588 1287 677 152 0Seven (family) system groups contributingAs described in the Methods Section we have selectedthe consensus alignments of vote=2 and vote=3. Theseconsensus alignments for HP-MP contain 2308 and1588 mappings, respectively; while for DOID-ORDOthey include 1883 and 1617 mappings, respectively.Table 5 shows some examples of mappings included withthe consensus alignments of vote=2 and vote=3. Alsoshown are some examples of manually created mappingsand (correct/incorrect) unique mappings from ontologymatching systems.Results against consensus alignmentsThe union of the input ontologies together with the con-sensus alignments or the mappings computed by each ofthe systems was coherent and thus, we did not require torepair any of the mapping sets to calculate the semanticprecision and recall. Note that the downloaded ontol-ogy versions from BioPortal did not contain any explicitor implicit disjointness. Tables 6 and 7 show the resultsachieved by each of the participating systems against theconsensus alignments with vote=2 and vote=3. In theDOID-ORDO task, LYAM++, PhenoMM and PhenoMPfailed to produce mappings and they were not included inTable 7.We deliberately did not rank the systems since, as men-tioned in the Methods section, the consensus align-ments may be incorrect or incomplete. We have simplyhighlighted the systems producing results relatively closeto the consensus alignments. For example, in the HP-MPtask, LogMap is the system producing an alignment thatis closer to the mappings voted by at least 2 systems, whileFCA-MAP produces results very close to the consensusalignments with vote=3.The use of semantic precision and recall allowed us toprovide a fair comparison for the systems PhenoMF, Phe-noMM and PhenoMP. These systems discover a large setof subsumption mappings that are not explicit in the ref-erence alignments, but they are still valid (i.e., they areentailed by the aligned ontology using the reference align-ment). For example, the standard precision of PhenoMFTable 4 Consensus alignments for the DOID-ORDO matchingtaskMin. Votes 1 2 3 4 5 6Mappings 50,998 1883 1617 1447 991 36Six (family) system groups contributingin the HP-MP task is 0.01 while the semantic precisionreaches the value of 0.76.Tables 6 and 7 also include the results of BioPortal map-pings against the consensus alignments. Precision valuesare perfect, but recall is very low, which confirms ourintuitions (recall Preparation phase section) about theincompleteness of BioPortal mappings.It is striking howXMap and LogMapLt produced resultsvery similar to the ones obtained by the BioPortal map-pings. Closer scrutiny of these results showed us thatthe computed mappings were indeed very similar to theBioPortal mappings (i.e., the F-measure of XMap andLogMapLt against the baseline mappings provided byBioPortal is ? 0.95 in both tasks).This could be expected for LogMapLt, since it onlyrelies on simple string matching techniques as the match-ing system underlying BioPortal [36]. However, the resultsfor XMap are unexpected since it produced top-resultsin the other biomedical-themed tracks of the OAEI2016 [1].Results against manually created mappingsTable 8 shows the results in terms of semantic recallagainst the manually created alignments. The resultsobtained in the HP-MP are relatively large positive valuesin general, especially for PhenoMF and PhenoMM thatachieve a semantic recall of 0.90. The numbers for theDOID-ORDO, however, are much smaller values and onlyLogMap, LogMapBio and DisMatch are able to discovera few of the manually curated mappings. LogMapBioobtained the best semantic recall value with 0.17, whichis far from the top results in the HP-MP task. The afore-mentioned results are also reflected when considering theconsensus alignments. In the HP-MP task, both the con-sensus alignments with vote 2 and 3 obtained reasonablygood results. However the picture changes dramaticallyin the DOID-ORDO task where none of the manuallycurated mappings are covered by the mappings agreed by2 or more systems. The most likely explanation for thisresult is that the manual mappings for DOID-ORDO rep-resent more complex subsumption mappings which werenot possible to (semantically) derive for the other map-pings. Table 8 also shows the results for the BioPortalmappings, which, as expected, have a coverage of curatedmappings very similar to the obtained by LogMapLt andXMap systems.The use of semantic recall together with the standardmeasure, as in previous section, allowed us to providemore realistic results and a fair comparison with the Phe-nomeNET family systems. As it can be observed in theHP-MP task (Table 8), the standard recall, unlike thesemantic recall, obtained by the other participants wasvery low and not comparable to the PhenomeNET familysystems.Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 8 of 13Table 5 Example mappings in the Disease and Phenotype trackEntity 1 Entity 2 Rel. Sourcex-linked chondrodysplasia punctata (DOID_0060292) Chondrodysplasia punctata (Orphanet_93442) ? (only) consensus alignment vote=2Meningeal melanomatosis (DOID_8243) Diffuse leptomeningeal melanocytosis ? Consensus alignment vote=3(Orphanet_252031)Reactive arthritis (DOID_6196) Reactive arthritis (Orphanet_29207) ? Consensus alignment vote=3Hypoplastic scapulae (HP_0000882) Short scapula (MP_0004340) ? (only) consensus alignment vote=2Macrocytic anemia (HP_0001972) Macrocytic anemia (MP_0002811) ? Consensus alignment vote=3Unerupted tooth (HP_0000706) Failure of tooth eruption (MP_0000121) ? Consensus alignment vote=3Breast leiomyosarcoma (DOID_5285) Rare malignant breast tumor  Manually created(Orphanet_180257)Abnormality of body weight (HP_0004323) Abnormal body weight (MP_0001259) ? Manually createdMicrocephaly (HP_0000252) Decreased brain size (MP_0000774) ? AML unique mapping (correct)Skeletal dysplasia (HP_0002652) Abnormal skeletal muscle morphology ? AML unique mapping (incorrect)(MP_0000759)Carbohydrate metabolism disease (DOID_0050013) Disorder of carbohydrate metabolism ? LogMapBio unique mapping (correct)(Orphanet_79161)Spinocerebellar ataxia type 35 (DOID_0050982) Transglutaminase 6 (Orphanet_279644) ? LogMapBio unique mapping (incorrect)Female hypogonadism (HP_0000134) Small ovary (MP_0001127) ? PhenoMF unique mapping (correct)While the top performing algorithms were able to detectequivalence matches across whole source ontologies forthe two mapping tasks giving high F-measures (Tables 6and 7), it is clear from detection of the curated align-ments that these proved much more difficult with a trendfor lower semantic recall across both tasks (Table 8).This result was not surprising because the curated align-ments mostly comprised of subsumption relationshipsrather than equivalence. Table 5 shows two examplesof curated mappings; the equivalence mapping betweenabnormality of body weight and abnormal body weightwas suggested by at least one the systems, while the sub-sumption mapping between breast leiomyosarcoma andrare malignant breast tumor was not discovered by any ofthe systems.Results for manual assessment of unique mappingsTables 9 and 10 show the results of the manualassessment of the unique mappings generated by theparticipating systems. As mentioned in the Methodssection we manually analysed up to 30 unique equiva-lence mappings for each system to estimate the precisionof the generated mappings not agreed with other sys-tems. Table 5 shows examples of unique mappings com-puted by AML, LogMapBio and PhenoMF. Note that,we focus on equivalence mappings since PhenomeNETTable 6 Results against consensus alignments with vote=2 and vote=3 in the HP-MP taskSystem-mappings Mappings Precision-2 F-Measure-2 Recall-2 Precision-3 F-Measure-3 Recall-3BioPortal (baseline) 639 1.00 0.50 0.33 1.00 0.60 0.43AML 1755 0.93 0.86 0.80 0.85 0.90 0.94DiSMatch 644 0.55 0.30 0.21 0.45 0.28 0.20FCA ? Map 1590 0.98 0.85 0.75 0.94 0.93 0.92LYAM + + 381 0.41 0.12 0.07 0.17 0.06 0.04LogMap 2011 0.94 0.92 0.91 0.77 0.86 0.97LogMapBio 2151 0.92 0.92 0.93 0.75 0.85 0.98LogMapLt 667 1.00 0.51 0.34 1.00 0.62 0.45PhenoMF 204,089 0.76 0.83 0.92 0.63 0.76 0.95PhenoMM 198,149 0.77 0.83 0.91 0.64 0.76 0.94PhenoMP 169,660 0.78 0.67 0.58 0.64 0.57 0.51XMap 650 1.00 0.50 0.33 1.00 0.61 0.44Precision and Recall represent their semantic variantsHarrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 9 of 13Table 7 Results against consensus alignments with vote=2 and vote=3 in the DOID-ORDO taskSystem-mappings Mappings Precision-2 F-Measure-2 Recall-2 Precision-3 F-Measure-3 Recall-3BioPortal (baseline) 1018 0.99 0.71 0.55 0.99 0.76 0.62AML 2098 0.85 0.91 0.97 0.78 0.87 1.00DiSMatch 335 0.23 0.08 0.05 0.19 0.07 0.04FCA ? Map 1803 0.97 0.96 0.96 0.89 0.94 0.99LogMap 1667 0.95 0.91 0.88 0.91 0.92 0.94LogMapBio 1804 0.92 0.91 0.90 0.86 0.90 0.95LogMapLt 1000 0.99 0.72 0.56 0.99 0.76 0.62PhenoMF 40,612 0.95 0.89 0.83 0.95 0.94 0.92XMap 1030 0.98 0.72 0.57 0.98 0.77 0.63Precision and Recall represent their semantic variantssystems produce a large amount of (unique) subsumptionmappings.BioPortal mappings, as expected, contains a very lownumber of uniquemappings in the DOID-ORDO task andno unique mappings in the HP-MP task.It is noticeable in the HP-MP task that, although DiS-Match and LYAM++ produced very low results withrespect to the consensus alignments (see Table 3), the pos-itive contribution of their unique mappings is one of thehighest. Nevertheless, their negative contribution has alsoan important weight. PhenomeNET systems producedthe most precise set of unique mappings although theirpositive contribution was lower than other systems.In the DOID-ORDO matching task, AMLs uniquemappings contains the higher number of true positiveswith a reasonable number of false positives. LogMapBioprovided the best trade-off between positive and negativecontribution.The last row in Tables 9 and 10 shows (excluding Bio-Portal mappings) the total number of unique mappings,its (average) precision, and the total (aggregated) positiveand negative contribution.Results in the OAEI interactive matching trackThe OAEI interactive track20 aims at offering a systematicand automated evaluation of matching systems with userinteraction to compare the quality of interactive match-ing approaches in terms of F-measure and number ofrequired interactions. The interactive track relies on thedatasets of the OAEI tracks: Conference, Anatomy, Large-bio, and Disease and Phenotype; and it uses the referencealignments of each track as oracle in order to simulateTable 8 Results against curated alignmentsSystem-mappingsHP-MP task DOID-ORDO taskStandard recall Semantic recall Standard recall Semantic recallBioPortal (baseline) 0.17 0.52 0.00 0.00AML 0.28 0.76 0.00 0.00DiSMatch 0.07 0.14 0.02 0.03FCA ? Map 0.21 0.62 0.00 0.00LYAM + + 0.00 0.00 - -LogMap 0.24 0.66 0.02 0.12LogMapBio 0.28 0.69 0.03 0.17LogMapLt 0.17 0.52 0.00 0.00PhenoMF 0.90 0.90 0.00 0.00PhenoMM 0.90 0.90 - -PhenoMP 0.83 0.83 - -XMap 0.17 0.52 0.00 0.00Consensus vote = 1 0.90 0.90 0.05 0.20Consensus vote = 2 0.31 0.79 0.00 0.00Consensusvote = 3 0.24 0.66 0.00 0.00Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 10 of 13Table 9 Manual assessment of unique mappings and estimated positive and negative contribution in the HP-MP taskSystem-mappings Unique mappings Precision Positive contrib. Negative contrib.BioPortal (baseline) 0 -AML 122 0.87 8.63% 1.33%DiSMatch 291 0.83 19.80% 3.96%FCA ? Map 26 0.96 2.04% 0.08%LYAM + + 226 0.70 12.91% 5.53%LogMap 130 0.93 9.90% 0.71%LogMapBio 176 0.93 13.40% 0.96%LogMapLt 0 - - -PhenoMF 89 1.00 7.27% 0.00%PhenoMM 85 1.00 6.94% 0.00%PhenoMP 80 1.00 6.53% 0.00%XMap 0 - - -Total 1225 0.91 87.42% 12.58%the interaction with a domain expert with variable errorrate [1].In this section we briefly present the results withthe Disease and Phenotype datasets in the OAEI 2016interactive track, which represents a side contribu-tion of the work presented in this paper. For moredetails and results, the interested reader please refer tostate-of-the-art papers on interactive ontology alignment[1, 3739].The consensus alignment with vote=3 was used asoracle in the Disease and Phenotype interactive track.Table 11 shows the obtained F-measure by AML andLogMap when simulating an interaction with a perfectuser (i.e., always gives the correct answer when askedabout the validity of a mapping).21 Both systems increasethe F-measure with respect to the non-interactive results(see Tables 6 and 7) with a gain between 0.03 and 0.11.It is noticeable that the number of required requests byLogMap is around 4-5 times larger than AML.DiscussionThe OAEI has been proven to be an effective campaignto improve ontology matching systems. As a result, avail-able techniques are more mature and robust. Neverthe-less, despite the impressive state-of-the-art technology inontology alignment, new matching tasks like those pre-sented in this paper are very important for the OAEIcampaign since they introduce new challenges to ontol-ogy alignment systems. For example, our preliminary testswith the Disease and Phenotype dataset revealed that onlythe 2015 versions of AML and LogMap, among the sys-tems participating in the OAEI 2015, were able to copewith the track ontologies.In the OAEI 2016 campaign there were 11 systems thatwere able to produce results in at least one of the Diseaseand Phenotype matching tasks. The four systems: AML,FCA-Map, LogMap (and its Bio variant) and PhenoMFproduced alignments relatively close to the consensusalignments for theDisease and Phenotype evaluation tasksTable 10 Manual assessment of unique mappings and estimated positive and negative contribution in the DOID-ORDO taskSystem-mappings Unique mappings Precision Positive contrib. Negative contrib.BioPortal (baseline) 5 0.40AML 308 0.87 30.40% 4.68%DiSMatch 259 0.40 11.80% 17.70%FCA ? Map 61 0.83 5.79% 1.16%LogMap 80 0.90 8.20% 0.91%LogMapBio 144 0.97 15.85% 0.55%LogMapLt 7 0.50 0.40% 0.40%PhenoMF 3 1.00 0.34% 0.00%XMap 16 0.56 1.03% 0.80%Total 878 0.75 73.81% 26.19%Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 11 of 13Table 11 Results in the OAEI interactive trackTask System F-measure Gain RequestsHP-MP AML 0.93 0.03 388LogMap 0.97 0.11 1928DOID-ORDO AML 0.96 0.09 413LogMap 0.99 0.07 1602as described in this paper. The results against curatedalignments proved to be more challenging since they gobeyond equivalent matches to include matches of seman-tic similarity, especially subsumption relationships. Thisfinding suggests that while the systems performed wellenough for detection of equivalent mappings, in future itwould be good to improve their performance for detec-tion of semantic similarity matches. For example, Phe-nomeNET systems showed potential advantage thoughexploiting a specialised background knowledge embed-ded within the system. LYAM++ is also specialised in theuse of background knowledge, but it did not perform wellin the Disease and Phenotype track, unlike in the OAEIAnatomy track, probably due to the lack of a suitablesource of background knowledge for this track.The OAEI also includes two biomedical-themed tracks,namely Anatomy and Largebio [1]. The complexity of thematching tasks is similar to the Anatomy track in terms ofontology size and expressiveness, while the Largebio tasksrepresent a significant leap in complexity with respectto the other OAEI test cases. The main differences withrespect to the evaluation in the Disease and Phenotypetrack are the following: (i) we constructed two consen-sus reference alignments, unlike the Anatomy track wherethere exist a curated reference alignment [40] and theLargebio track where the reference alignment has beenextracted from the UMLS Metathesaurus [8]; (ii) we per-formed an evaluation with respect to manually createdmappings and a manual assessment of unique mappingsproduced by participating systems; and (iii) we usedsemantic precision and recall together with the standardmeasures.The findings of the Disease and Phenotype evaluationshow the potential of the top performing ontology match-ing systems that could help to automate the workflowof curators, who maintain ontology mapping services innumerous domains such as the disease and phenotypedomain. Furthermore, the constructed consensus align-ments substantially improve available mapping sets pro-vided by BioPortal.ConclusionsWe have presented the methodology followed in the novelDisease and Phenotype track and the results in the OAEI2016. The top systems in the track coped well with thedetection of equivalence matches, but struggled to detectsubsumption matches. This deserves more attention inthe future development of ontology matching systems.The Pistoia Alliance Ontologies Mapping project hasgained much value from participation in the 2016 OAEIcampaign through sponsorship and design of this newtrack on Disease and Phenotype. We believe that there isa real need for ontology matching algorithm developersto collaborate with ontology curators to improve the scaleand quality of workflows necessary to build and maintainontology mapping resources.We are in an exploding information age with increasingamounts of human biology and genetics data in particularfrom sequencing technology improvements, biobanks andsmart portable devices. This drives the need for strongerontological standards, tools and services for ontologymapping to enable more efficient application of all thisinformation. We expect that the Disease and Phenotypetrack will evolve in future campaigns as a strong use casewhich is widely applicable in the life sciences and beyond.Evolution of the trackThe OAEI 2017 will include a new edition of the track,which will be composed by the same tasks as in 2016(with updated ontology versions) and two additional tasksrequiring the pairwise alignment of: HP and MESH (Medical Subject Headings)ontologies; and HP and OMIM (Online Mendelian Inheritance inMan) ontologies.The alignment between HP andMESH is a new require-ment of the Pistoia Alliance Ontologies Mapping project,while the mapping between HP and OMIM is placedwithin the scope of the Research Council of Norwayproject BigMed to improve the suggested genes associ-ated to a given phenotype in state of the art tools likePhenoTips [41].In the future editions of the Disease and Phenotypetrack, apart from including new datasets and updated ver-sions, we aim to enhance the evaluation in a numberof ways. We will consider new metrics like the map-ping incoherence [12], the functional coherence [42] orthe redundancy (minimality) [43] to evaluate the com-puted alignments. We also intend to redefine the notion ofsemantic precision and recall, using the using the seman-tic closure of the (aligned) ontologies, in order to includethe cases where the aligned ontology is incoherence (i.e.,contains unsatisfiable classes).We plan to increase the number of manually gener-ated mappings considering additional areas relevant tothe phenotype and disease domain. In addition, we willalso work towards the semi-automatic creation of goldHarrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 12 of 13standard reference alignments for the tasks by combin-ing the consensus alignments and the manually generatedmappings.Endnotes1 http://www.pistoiaalliance.org/projects/ontologies-mapping2 https://pistoiaalliance.atlassian.net/wiki/display/PUB/Ontologies+Mapping+Resources3The contents of this paper have been partially reportedin the OAEI 2016 annual report [1], published withinthe informal proceedings of the Ontology Matchingworkshop [44].4 http://www.orphadata.org/cgi-bin/inc/ordo_orphanet.inc.php5 https://www.w3.org/TR/owl2-overview/6Typically automatic, although there are systems thatalso allow human interaction7 http://oaei.ontologymatching.org/8 http://oaei.ontologymatching.org/2016/phenotype/9 http://data.bioontology.org/documentation#Mapping10 https://www.bioontology.org/wiki/index.php/BioPortal_Mappings11We did not consider mappings labelled asskos:exactMatch since they represent correspondencesbetween entities with the same URI, and thus thesemappings are redundant if translated into OWL 2 axioms.12Our tests with last year participants revealed a largeamount of missing valid mappings. The Results sectionquantifies this degree of incompleteness.13 http://oaei.ontologymatching.org/2016/seals-eval.html14We may consider vote ? 4 in future editions of theDisease and Phenotype track as the contributing partici-pants increase.15 There could still be some bias through systemsexploiting the same resource, e.g., UMLS.16We rely on the OWL 2 reasoner HermiT [45].17 http://bioportal.bioontology.org/ontologies/MESH18 https://www.nlm.nih.gov/pubs/factsheets/umlslex.html19 http://aber-owl.net/ontology/PhenomeNET20 http://oaei.ontologymatching.org/2016/interactive/21 From the Disease and Phenotype track participatingsystems only AML, LogMap and XMap implement aninteractive algorithm. We have discarded XMap from theresults since its number of oracle/user requests was verylow in the Disease and Phenotype track.AbbreviationsDL: Description logics; DOID: Disease ontology; F: F-measure; HP: Humanphenotype ontology; M: Mappings; MP: Mammalian phenotype ontology; O:Ontology; OA: Ontology alignment; OAEI: Ontology alignment evaluationinitiative; ORDO: Orphanet rare disease ontology; OWL: Web ontologylanguage; P: Precision; R: Recall; RA: Reference alignment; RDF: Resourcedescription frameworkAcknowledgementsWe would like to thank the organisers and participants of the OAEI campaign.We also thank the anonymous reviewers for their comments and suggestionsto improve the paper.FundingThis work was partially funded by the Pistoia Alliance Ontology Mappingsproject, the BIGMED project (IKT 259055), the SIRIUS Centre for Scalable DataAccess (Research Council of Norway, project no.: 237889), the EU projectOptique (FP7-ICT-318338), and the EPSRC projects ED3 and DBOnto.Availability of data andmaterialsOAEI 2016 datasets available from: http://oaei.ontologymatching.org/2016/phenotype/.OAEI 2017 datasets available from: http://oaei.ontologymatching.org/2017/phenotype/.Main entry point for the Disease and Phenotype track: http://sws.ifi.uio.no/oaei/phenotype/.Authors contributionsIH, EJR and AS organised and designed the experiments of the track. EJRconducted the automatic evaluation. IH prepared the manually curatedmappings and performed the manual assessment which was checked by AS,MR, PW, SM, YAF and JM. All authors contributed to the writing of themanuscript. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Pistoia Alliance Ontologies Mapping Project, Pistoia Alliance Inc, USA.2Department of Informatics, University of Oslo, Oslo, Norway. 3Novartis, Basel,Switzerland. 4Roche Pharma Research and Early Development, pREDInformatics, Roche Innovation Center, Basel, Switzerland. 5GlaxoSmithKlineR&D, Stevenage, UK. 6BIOVIA 3DS, San Diego, USA. 7Eagle Genomics,Cambridge, UK. 8OSTHUS, Aachen, Germany. 9FactBio, Cambridge, UK.Received: 13 April 2017 Accepted: 27 October 2017RESEARCH Open AccessNCBO Ontology Recommender 2.0: anenhanced approach for biomedicalontology recommendationMarcos Martínez-Romero1* , Clement Jonquet1,3, Martin J. OConnor1, John Graybeal1, Alejandro Pazos2and Mark A. Musen1AbstractBackground: Ontologies and controlled terminologies have become increasingly important in biomedical research.Researchers use ontologies to annotate their data with ontology terms, enabling better data integration andinteroperability across disparate datasets. However, the number, variety and complexity of current biomedicalontologies make it cumbersome for researchers to determine which ones to reuse for their specific needs. Toovercome this problem, in 2010 the National Center for Biomedical Ontology (NCBO) released the OntologyRecommender, which is a service that receives a biomedical text corpus or a list of keywords and suggestsontologies appropriate for referencing the indicated terms.Methods: We developed a new version of the NCBO Ontology Recommender. Called Ontology Recommender 2.0,it uses a novel recommendation approach that evaluates the relevance of an ontology to biomedical text dataaccording to four different criteria: (1) the extent to which the ontology covers the input data; (2) the acceptanceof the ontology in the biomedical community; (3) the level of detail of the ontology classes that cover the inputdata; and (4) the specialization of the ontology to the domain of the input data.Results: Our evaluation shows that the enhanced recommender provides higher quality suggestions than theoriginal approach, providing better coverage of the input data, more detailed information about their concepts,increased specialization for the domain of the input data, and greater acceptance and use in the community.In addition, it provides users with more explanatory information, along with suggestions of not only individualontologies but also groups of ontologies to use together. It also can be customized to fit the needs of differentontology recommendation scenarios.Conclusions: Ontology Recommender 2.0 suggests relevant ontologies for annotating biomedical text data. Itcombines the strengths of its predecessor with a range of adjustments and new features that improve its reliabilityand usefulness. Ontology Recommender 2.0 recommends over 500 biomedical ontologies from the NCBO BioPortalplatform, where it is openly available (both via the user interface at http://bioportal.bioontology.org/recommender,and via a Web service API).Keywords: Ontology selection, Ontology recommendation, Ontology evaluation, Semantic Web, Biomedicalontologies, NCBO BioPortal* Correspondence: marcosmr@stanford.edu1Stanford Center for Biomedical Informatics Research, 1265 Welch Road,Stanford University School of Medicine, Stanford, CA 94305-5479, USAFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 DOI 10.1186/s13326-017-0128-yBackgroundDuring the last two decades, the biomedical communityhas grown progressively more interested in ontologies.Ontologies provide the common terminology necessaryfor biomedical researchers to describe their datasets, en-abling better data integration and interoperability, andtherefore facilitating translational discoveries [1, 2].BioPortal [3, 4], developed by the National Center forBiomedical Ontology (NCBO) [5], is a highly used plat-form1 for hosting and sharing biomedical ontologies. Bio-Portal users can publish their ontologies as well as submitnew versions. They can browse, search, review, and com-ment on ontologies, both interactively through a Webinterface, and programmatically via Web services. In 2008,BioPortal2 contained 72 ontologies and 300,000 ontologyclasses. As of 2017, the number of ontologies exceeds 500,with more than 7.8 million classes, making it one of thelargest public repositories of biomedical ontologies.The great number, complexity, and variety of ontologiesin the biomedical field present a challenge for researchers:how to identify those ontologies that are most relevant forannotating, mining or indexing particular datasets. Toaddress this problem, in 2010 the NCBO released thefirst version of its Ontology Recommender (henceforthOntology Recommender 1.0 or original Ontology Rec-ommender) [6], which informed the user of the most ap-propriate ontologies in BioPortal to annotate textual data.It was, to the best of our knowledge, the first biomedicalontology recommendation service, and it became widelyknown and used by the community.3 However, the servicehas some limitations, and a significant amount of work hasbeen done in the field of ontology recommendation sinceits release. This motivated us to analyze its weaknesses andto design a new recommendation approach.The main contributions of this paper are the following:1. A state-of-the-art approach for recommendingbiomedical ontologies. Our approach is based onevaluating the relevance of an ontology to biomedicaltext data according to four different criteria, namely:ontology coverage, ontology acceptance, ontologydetail, and ontology specialization.2. A new ontology recommendation system, the NCBOOntology Recommender 2.0 (henceforth OntologyRecommender 2.0 or new OntologyRecommender). This system has been implementedbased on our approach, and it is openly available atBioPortal.Our research is particularly relevant both for re-searchers and developers who need to identify the mostappropriate ontologies for annotating textual data ofbiomedical nature (e.g., journal articles, clinical trial de-scriptions, metadata about microarray experiments,information on small molecules, electronic health re-cords, etc.). Our ontology recommendation approachcan be easily adapted to other domains, as it will be il-lustrated in the Discussion section. Overall, this workadvances prior research in the fields of ontology evalu-ation and recommendation, and provides the commu-nity with a useful service which is, to the best of ourknowledge, the only ontology recommendation systemcurrently available to the public.Related workMuch theoretical work has been done over the pasttwo decades in the fields of ontology evaluation, selec-tion, search, and recommendation. Ontology evaluationhas been defined as the problem of assessing a givenontology from the point of view of a particular criter-ion, typically in order to determine which of several on-tologies would best suit a particular purpose [7]. As aconsequence, ontology recommendation is fundamen-tally an ontology evaluation task because it addressesthe problem of evaluating and consequently selectingthe most appropriate ontologies for a specific contextor goal [8, 9].Early contributions in the field of ontology evaluationdate back to the early 1990s and were motivated by thenecessity of having evaluation strategies to guide andimprove the ontology engineering process [1012].Some years later, with the birth of the Semantic Web[13], the need for reusing ontologies across the Webmotivated the development of the first ontology searchengines [1416], which made it possible to retrieve allontologies satisfying some basic requirements. Theseengines usually returned only the ontologies that hadthe query term itself in their class or property names[17]. However, the process of recommending ontologiesinvolves more than that. It is a complex process thatcomprises evaluating all candidate ontologies accordingto a variety of criteria, such as coverage, richness of theontology structure [1820], correctness, frequency ofuse [21], connectivity [18], formality, user ratings [22],and their suitability for the task at hand.In biomedicine, the great number, size, and complexityof ontologies have motivated strategies to help re-searchers find the best ontologies to describe theirdatasets. Tan and Lambrix [23] proposed a theoreticalframework for selecting the best ontology for a particu-lar text-mining application and manually applied it to agene-normalization task. Alani et al. [17] developed anontology-search strategy that uses query-expansiontechniques to find ontologies related to a particular do-main (e.g., Anatomy). Maiga and Williams [24] con-ceived a semi-automatic tool that makes it possible tofind the ontologies that best match a list of user-defined task requirements.Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 2 of 22The most relevant alternative to the NCBO OntologyRecommender is BiOSS [21, 25], which was released in2011 by some of the authors of this paper. BiOSS evalu-ates each candidate ontology according to three criteria:(1) the input coverage; (2) the semantic richness of theontology for the input; and (3) the acceptance of theontology. However, this system has some weaknesses thatmake it insufficient to satisfy many ontology reuse needsin biomedicine. BiOSS ontology repository is not updatedregularly, so it does not take into account the most recentrevisions to biomedical ontologies. Also, BiOSS evaluatesontology acceptance by counting the number of mentionsof the ontology name in Web 2.0 resources, such as Twit-ter and Wikipedia. However, this method is not always ap-propriate because a large number of mentions do notalways correspond to a high level of acceptance by thecommunity (e.g., an ontology may be popular on Twitterbecause of a high number of negative comments about it).Another drawback is that the input to BiOSS is limited tocomma-delimited keywords; it is not possible to suggestontologies to annotate raw text, which is a very commonuse case in biomedical informatics.In this work, we have applied our previous experiencein the development of the original Ontology Recom-mender and the BiOSS system to conceive a new ap-proach for biomedical ontology recommendation. Thenew approach has been used to design and implement theOntology Recommender 2.0. The new system combinesthe strengths of previous methods with a range of en-hancements, including new recommendation strategiesand the ability to handle new use cases. Because it is inte-grated within the NCBO BioPortal, this system works witha large corpus of current biomedical ontologies and cantherefore be considered the most comprehensive biomed-ical ontology recommendation system developed to date.Our recommendations for the choice of appropriateontologies center around the use of ontologies to per-form annotation of textual data. We define annotationas a correspondence or relationship between a term andan ontology class that specifies the semantics of thatterm. For instance, an annotation might relate leucocytein some text to a particular ontology class leucocyte inthe Cell Ontology. The annotation process will also re-late textual data such as white blood cell and lymphocyteto the class leucocyte in the Cell Ontology, via synonymand subsumption relationships, respectively.Description of the original approachThe original NCBO Ontology Recommender supportedtwo primary use cases: (1) corpus-based recommenda-tion, and (2) keyword-based recommendation. In thesescenarios, the system recommended appropriate ontol-ogies from the BioPortal ontology repository to annotatea text corpus or a list of keywords, respectively.The NCBO Ontology Recommender invoked theNCBO Annotator [26] to identify all annotations for theinput data. The NCBO Annotator is a BioPortal servicethat annotates textual data with ontology classes. Then,the Ontology Recommender scored all BioPortal ontol-ogies as a function of the number and relevance of theannotations found, and ranked the ontologies accordingto those scores. The first ontology in the ranking wouldbe suggested as the most appropriate for the input data.The score for each ontology was calculated according tothe following formula4:scoreðo; tÞ ¼XðannotationScoreðaÞ þ 2  hierarchyLevelðaÞÞlog10ðjojÞ?a ? annotationsðo; tÞsuch that:score o; tð Þ?? : score o; tð Þ?0annotationScoreðaÞ ¼ 10 if annotationType ¼ PREF8 if annotationType ¼ SYN(hierarchyLevelðaÞ ?Z : hierarchyLevelðaÞ ? 0Here o is the ontology that is being evaluated; t is theinput text; score(o, t) represents the relevance of theontology o for t; annotationScore(a) is the score for theannotation a; hierarchyLevel(a) is the position of thematched class in the ontology tree, such that 0 representsthe root level; |o| is the number of classes in o; and anno-tations(o,t) is the list of annotations (a) performed with ofor t, returned by the NCBO Annotator.The annotationScore(a) would depend on whether theannotation was achieved with a class preferred name(PREF) or with a class synonym (SYN). A preferredname is the human readable label that the authors of theontology suggested to be used when referring to theclass (e.g., vertebral column), whereas synonyms are alter-nate names for the class (e.g., spinal column, backbone,spine). Each class in BioPortal has a single preferred nameand it may have any number of synonyms. Because syno-nyms can be imprecise, this approach favored matches onpreferred names.The normalization by ontology size was intended todiscriminate between large ontologies that offer goodcoverage of the input data, and small ontologies withboth correct coverage and better specialization for theinput datas domain. The granularities of the matchedclasses (i.e., hierarchyLevel(a)) were also considered, sothat annotations performed with granular classes (e.g.,epithelial cell proliferation) would receive higher scoresthan those performed with more abstract classes (e.g.,biological process).Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 3 of 22For example, Table 1 shows the top five suggestions ofthe original Ontology Recommender for the text Melan-oma is a malignant tumor of melanocytes which arefound predominantly in skin but also in the bowel andthe eye. In this example, the system considered that thebest ontology for the input data is the National CancerInstitute Thesaurus (NCIT).In the following sections, we summarize the most rele-vant shortcomings of the original approach, addressinginput coverage, coverage of multi-word terms, inputtypes and output information.Input coverageInput coverage refers to the fraction of input data that isannotated with ontology classes. Given that the goal isto find the best ontologies to annotate the users data,high input coverage is the main requirement for ontology-recommendation systems. One of the shortcomings of theoriginal approach is that it did not ensure that ontologiesthat provide high input coverage were ranked higherthan ontologies with lower coverage. The approach wasstrongly based on the total number of annotationsreturned by the NCBO Annotator. However, a large num-ber of annotations does not always imply high coverage.Ontologies with low input coverage can contain a greatmany classes that match only a few input terms, or matchmany repeated terms in a large text corpus.In the previous example (see Table 1), EHDA (HumanDevelopmental Anatomy Ontology) was ranked at thesecond position. However, it covers only two inputterms: skin and eye. Clearly, it is not an appropriateontology to annotate the input when compared withLOINC or EFO, which have almost three times moreterms covered. The reason that EHDA was assigned ahigh score is that it contains 11 different eye classes (e.g.,EHDA:4732, EHDA:3808, EHDA:5701) and 4 differentskin classes (e.g., EHDA:6531, EHDA:6530, EHDA:7501),which provide a total of 15 annotations. Since the recom-mendation score computed using the original approach isdirectly influenced by the number of annotations, EHDAobtains a high relevance score and thus the second pos-ition in the ranking. This issue was also identified byLópez-García et al. in their study of the efficiency ofautomatic summarization techniques [27]. These au-thors noticed that EHDA was the most recommendedontology for a broad range of topics that the ontologyactually did not cover well.Multi-word termsBiomedical texts frequently contain terms composed ofseveral words, such as distinctive arrangement of micro-tubules, or dental disclosing preparation. Annotating amulti-word phrase or multi-word keyword with an onto-logical class that completely represents its semantics is amuch better choice than annotating each word separ-ately. The original recommendation approach was notdesigned to select the longest matches and consequentlythe results were affected.As an example, Table 2 shows the top 5 ontologiessuggested by the original Ontology Recommender forthe phrase embryonic cardiac structure. Ideally, the firstontology in the ranking (SWEET) would contain theclass embryonic cardiac structure. However, the SWEETontology covers only the term structure. This ontologywas ranked at the first position because it contains 3classes matching the term structure and also because itis a small ontology (4549 classes).Furthermore, SNOMEDCT, which does contain a classthat provides a precise representation of the input, wasranked in the 5th position. There are 3 other ontologies inBioPortal that contain the class embryonic cardiac struc-ture: EP, BIOMODELS and FMA. However, they wereranked 8, 11 and 32, respectively. The recommendationalgorithm should assign a higher score to an annotationthat covers all words in a multi-word term than it does todifferent annotations that cover all words separately.Input typesRelated work in ontology recommendation highlights theimportance of addressing two different input types: textcorpora and lists of keywords [28]. The original OntologyRecommender, while offering users the possibility ofselecting among these two recommendation scenarios,would treat the input data in the same manner. To satisfyTable 1 Ontologies suggested by the original Ontology Recommender for the sample input text Melanoma is a malignant tumor ofmelanocytes which are found predominantly in skin but also in the bowel and the eyeRank Ontology No. annotations Terms annotated Score1 NCIT 21 melanoma, malignant tumor, melanocytes, found, skin, bowel, eye 55.22 EHDA 15 skin, eye 38.33 EFO 10 melanoma, malignant tumor, skin, bowel, eye 35.94 LOINC 18 melanoma, malignant, tumor, skin, bowel, eye 35.95 MP 9 melanoma, skin, bowel, eye 34.8*See "List of abbreviations"For each ontology, the table shows its position in the ranking, the acronym of the ontology in BioPortal*, the number of annotations returned by the NCBOAnnotator for the sample input, the terms annotated (or covered) by those annotations and the ontology scoreMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 4 of 22users expectations, the system should process these twoinput types differently, to better reflect the informationcoded in the input about multi-word boundaries.Output informationThe output provided by the original Ontology Recom-mender consisted of a list of ontologies ranked by rele-vance score. For each ontology, the Web-based userinterface displayed the number of classes matched andthe size of each recommended ontology. In contrast, theWeb service could additionally return the particularclasses matched in each ontology. This informationproved insufficient to assure users that a recommendedontology was appropriate and better than the alternatives.For example, it was not possible to know what specific in-put terms were covered by each class. The system shouldprovide enough detail both to reassure users, and to givethem information about alternative ontologies.In this section we have described the fundamentallimitations of the original Ontology Recommender andsuggested methods to address them. The strategy forevaluating input coverage must be improved. Addition-ally, there is a diversity of other recently-proposed evalu-ation techniques [8, 19, 25] that could enhance theoriginal approach. Particularly, there are two evaluationcriteria that could substantially improve the output pro-vided by the system: (1) ontology acceptance, which rep-resents the degree of acceptance of the ontology by thecommunity; and (2) ontology detail, which refers to thelevel of detail of the classes that cover the input data.Description of the new approachIn this section, we present our new approach to biomed-ical ontology recommendation. First, we describe ourontology evaluation criteria and explain how the recom-mendation process works. We then provide some imple-mentation details and discuss improvements to the userinterface.The execution starts from the input data and a set ofconfiguration settings. The NCBO Annotator [26] isthen used to obtain all annotations for the input usingBioPortal ontologies. Those ontologies that do not pro-vide annotations for the input data are consideredirrelevant and are ignored in further processing. Theontologies that provide annotations are evaluated oneby one according to four evaluation criteria that ad-dress the following questions:1. Coverage: To what extent does the ontologyrepresent the input data?2. Acceptance: How well-known and trusted is theontology by the biomedical community?3. Detail: How rich is the ontology representation forthe input data?4. Specialization: How specialized is the ontology tothe domain of the input data?According to our analysis of related work, these arethe most relevant criteria for ontology recommenda-tion. Note that other authors have referred to thecoverage criterion as term matching [6], class matchmeasure [19] and topic coverage [28]. Acceptance is re-lated to popularity [21, 25, 28], because it measuresthe level of support provided to the ontology by thepeople in the community. Other criteria to measureontology acceptance are connectivity [6], and connect-edness [18], which assess the relevance of an ontologybased on the number and quality of connections to anontology by other ontologies. Detail is similar to struc-ture measure [6], semantic richness [21, 25], structure[18], and granularity [24].For each of these evaluation criteria, a score in theinterval [0,1] is typically obtained. Then, all the scoresfor a given ontology are aggregated into a compositerelevance score, also in the interval [0,1]. This scorerepresents the appropriateness of that ontology to de-scribe the input data. The individual scores are com-bined in accordance with the following expression:scoreðo; tÞ ¼ wc  coverageðo; tÞ þ wa  acceptanceðoÞþwd  detailðo; tÞ þ ws  specializationðo; tÞwhere o is the ontology that is being evaluated, t rep-resents the input data, and {wc, wa, wd, ws} are a setof predefined weights that are used to give more orless importance to each evaluation criterion, such thatwc + wa + wd + ws = 1. Note that acceptance is the onlycriterion independent from the input data. Ultimately,the system returns a list of ontologies ranked accord-ing to their relevance scores.Ontology evaluation criteriaThe relevance score of each candidate ontology iscalculated based on coverage, acceptance, detail, andTable 2 Top 5 ontologies suggested by Ontology Recommender1.0 for the sample input text embryonic cardiac structureRank Ontology No. annotations Terms covered Score1 SWEET 3 structure 13.72 NCIT 4 embryonic, cardiac, structure 10.53 HUPSON 2 cardiac, structure 10.14 VSO 1 structure 9.85 SNOMEDCT 2 embryonic cardiac structure 8.9For each ontology, the table shows its position in the ranking, the acronym ofthe ontology in BioPortal, the number of annotations returned by the NCBOAnnotator for the sample input, the terms annotated (or covered) bythose annotations and the ontology scoreMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 5 of 22specialization. We now describe these criteria inmore detail.Ontology coverageIt is crucial that ontology recommendation systems sug-gest ontologies that provide high coverage of the inputdata. As with the original approach, the new recommen-dation process is driven by the annotations provided bythe NCBO Annotator, but the method used to evaluatethe candidate ontologies is different. In the new algo-rithm, each annotation is assigned a score computed inaccordance with the following expression5:annotationScore2 að Þ ¼ annotationTypeScore að ÞðþmultiWordScore að ÞÞ  annotatedWords að Þwith:annotationTypeScore að Þ ¼ 10 if annotationType ¼ PREF5 if annotationType ¼ SYNmultiWordScore að Þ ¼ 3 if annotatedWords að Þ > 10otherwiseIn this expression, annotationTypeScore(a) is a scorebased on the annotation type which, as with the originalapproach, can be either PREF, if the annotation hasbeen performed with a class preferred name, or SYN, ifit has been performed with a class synonym. Ourmethod assigns higher relevance to scores done withclass preferred names than to those made with classsynonyms because we have seen that many BioPortalontologies contain synonyms that are not reliable (e.g.,Other variants as a synonym of Other Variants of Basa-loid Follicular Neoplasm of the Mouse Skin in the NCIThesaurus).The multiWordScore(a) score rewards multi-word an-notations. It gives more importance to classes that anno-tate multi-word terms than to classes that annotateindividual words separately (e.g., blood cell versus bloodand cell). Such classes better reflect the input data thando classes that represent isolated words.The annotatedWords(a) function represents the num-ber of words matched by the annotation (e.g., 2 for theterm blood cell).Sometimes, an ontology provides overlapping annota-tions for the same input data. For instance, the textwhite blood cell may be covered by two different classes,white blood cell and blood cell. In the original approach,ontologies with low input coverage were sometimesranked among the top positions because they had mul-tiple classes matching a few input terms, and all thoseannotations contributed to the final score. Our newapproach addresses this issue. If an ontology providesseveral annotations for the same text fragment, only theannotation with the highest score is selected to contrib-ute to the coverage score.The coverage score for each ontology is computed asthe sum of all the annotation scores, as follows:coverage o; tð Þ ¼ normXannotationScore2 að Þ ?a?selectedAnnotations Að Þwhere A is the set of annotations performed with theontology o for the input t, selectedAnnotations(A) is theset of annotations that are left after discarding overlap-ping annotations, and norm is a function that normalizesthe coverage score to the interval [0,1].As an example, Table 3 shows the annotations per-formed with SNOMEDCT for the input A thrombocyteis a kind of blood cell. This example shows how our ap-proach prioritizes (i.e., assigns a higher score to) annota-tions performed with preferred names over synonyms(e.g., cell over entire cell), and annotations performedwith multi-word terms over single-word terms (e.g.,blood cell over blood plus cell). The coverage score forSNOMEDCT would be calculated as 5 + 26 = 31, whichwould be normalized to the interval [0,1] by dividing itby the maximum coverage score. The maximum cover-age score is obtained by adding the scores of all the an-notations performed with all BioPortal ontologies, afterdiscarding overlapping annotations.It is important to note that this evaluation of ontologycoverage takes into account term frequency. That is,matched terms with several occurrences are consideredmore relevant to the input data than terms that occurless frequently. If an ontology covers a term that appearsseveral times in the input, its corresponding annotationscore will be counted each time and the coverage scorefor the ontology accordingly will be higher. In addition,because we select only the matches with the highestscore, the frequencies are not distorted by terms embed-ded in one another (e.g., white blood cell and blood cell).Our approach accepts two input types: free text andcomma-delimited keywords. For the keyword input type,only those annotations that cover all the words in aTable 3 SNOMEDCT annotations for the input A thrombocyte isa kind of blood cellText Matched class (type) Annotation score Selectedthrombocyte platelet (SYN) 5 Yesblood cell blood cell (PREF) (10 + 3)*2 = 26 Yesblood blood (PREF) 10 Nocell cell structure (SYN) 5 Nocell cell (PREF) 10 Nocell entire cell (SYN) 5 NoThe table shows the text fragment covered by each annotation, the name andtype of the matched class, the annotation score, and the annotations selectedto compute the relevance score for SNOMEDCTMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 6 of 22multi-word term are considered. Partial annotations areimmediately discarded.Ontology acceptanceIn biomedicine, some ontologies have been developedand maintained by widely known institutions or researchprojects. The content of these ontologies is periodicallycurated, extensively used, and accepted by the commu-nity. Examples of broadly accepted ontologies are SNO-MEDCT [29] and Gene Ontology [30]. Some ontologiesuploaded to BioPortal may be relatively less reliable,however. They may contain incorrect or poor qualitycontent or simply be insufficiently up to date. It is import-ant that an ontology recommender be able to distinguishbetween ontologies that are accepted as trustworthy andthose that are less so.Our approach proposes to estimate the degree ofacceptance of each ontology based of informationextracted from ontology repositories or terminologysystems. Widely used examples of these systems in bio-medicine include BioPortal, the Unified Medical LanguageSystem (UMLS) [31], the OBO Foundry [32], Ontobee[33], the Ontology Lookup Service (OLS) [34], and Aber-OWL [35]. The calculation of ontology acceptance isbased on two factors: (1) The presence or absence of theontology in ontology repositories; and (2) the number ofvisits (pageviews) to the ontology in ontology repositoriesin a recent period of time (e.g., the last 6 months). Thismethod takes into account changes in ontology accept-ance over time. The acceptance score for each ontology iscalculated as follows:acceptance oð Þ ¼ wpresence  presenceScore oð Þþwvisits  visitsScore oð Þwhere: presenceScore(o) is a value in the interval [0,1] thatrepresents the presence of the ontology in apredefined list of ontology repositories. It iscalculated as follows:presenceScoreðoÞ ¼Xni¼1wpi  presenceiðoÞwhere wpi represents the weight assigned to the presenceof the ontology in the repository i, withXni?1wpi ¼ 1, and:presencei oð Þ ¼ 1 if o is present in repository i0 otherwise visitsScore(o) represents the number of visits to theontology on a given list of ontology repositories ina recent period of time. Note that this score cantypically be calculated only for those repositoriesthat are available on the Web and that have anindependent page for each provided ontology. Thisscore is calculated as follows:visitsScoreðoÞ ¼Xni¼1wvi  visitsiðoÞwhere wvi is the weight assigned to the ontology visits onthe repository i, withXni?1wvi ¼ 1 ; visitsi(o) representsthe number of visits to the ontology in the repository i,normalized to the interval [0,1]. wpresence and wvisits are weights that are used togive more or less importance each factor, withwpresence + wvisits = 1.Figure 1 shows the top 20 accepted BioPortal ontol-ogies according to our approach at the time of writingthis paper. Estimating the acceptance of an ontology bythe community is inherently subjective, but the aboveranking shows that our approach provides reasonableresults. All ontologies in the ranking are widely knownand accepted biomedical ontologies that are used in avariety of projects and applications.Ontology detailOntologies containing a richer representation for a specificinput are potentially more useful to describe the input thanless detailed ontologies. As an example, the class melanomain the Human Disease Ontology contains a definition, twosynonyms, and twelve properties. However, the class mel-anoma from the GALEN ontology does not contain anydefinition, synonyms, or properties. If a user needs anontology to represent that concept, the Human DiseaseOntology would probably be more useful than the GALENontology because of this additional information. An ontol-ogy recommender should be able to analyze the level of de-tail of the classes that cover the input data and to givemore or less weight to the ontology according to the degreeto which its classes have been specified.We evaluate the richness of the ontology representa-tion for the input data based on a simplification of thesemantic richness metric used by BiOSS [25]. For eachannotation selected during the coverage evaluation step,we calculate the detail score as follows:detailScore að Þ ¼ definitionScore að Þ þ synonymsScore að Þ þ propertiesScore að Þ3where detailScore(a) is a value in the interval [0,1] thatrepresents the level of detail provided by the annotation a.This score is based on three functions that evaluate thedetail of the knowledge representation according to thenumber of definitions, synonyms, and other properties ofthe matched class:Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 7 of 22definitionScoreðaÞ ¼ 1 if jDj?kdjDj=kd otherwise(synonymsScoreðaÞ ¼ 1 if jSj?ksjSj=ks otherwise(propertiesScoreðaÞ ¼ 1 if jPj?kpjPj=kp otherwise(where |D|, |S| and |P| are the number of definitions, syno-nyms, and other properties of the matched class, and kd,ks and kp are predefined constants that represent thenumber of definitions, synonyms, and other properties, re-spectively, necessary to get the maximum detail score. Forexample, using ks = 4 means that, if the class has 4 ormore synonyms, then it will be assigned the maximumsynonyms score, which would be 1. If it has fewer than 4synonyms, for example 3, the synonyms score will becomputed proportionally according to the expressionabove (i.e., 3/4). Finally, the detail for the ontology wouldbe calculated as the sum of the detail scores of the annota-tions done with the ontology, normalized to [0,1]:detail o; tð Þ ¼XdetailScore að ÞAj j ?a?selectedAnnotations Að ÞExample: Suppose that, for the input t = Penicillin is anantibiotic used to treat tonsillitis, there are two ontologiesO1 and O2 with the classes shown in Table 4.Assuming that kd = 1, ks = 4 and kp = 10, the detailscore for O1 and O2 would be calculated as follows:detail O1;tð Þ ¼21þ 2 4= þ 7 10=3þ31þ 1þ 1¼ 0:87detail O2;tð Þ ¼20þ 1 4= þ 3 10=3þ30þ 0þ 2 10=¼ 0:13Given that O1 annotates the input with two classesthat provide more detailed information than the classesfrom O2, the detail score for O1 is higher.Ontology specializationSome biomedical ontologies aim to represent detailedinformation about specific subdomains or particulartasks. Examples include the Ontology for BiomedicalFig. 1 Top 20 BioPortal ontologies according to their acceptance scores. The x-axis shows the acceptance score in the interval [0, 100]. The y-axisshows the ontology acronyms. These acceptance scores were obtained by using UMLS to calculate the presenceScore(o), BioPortal to computethe visitsScore(o), and assigning the same weight to pageviewsScore(o) and reposScore(o) (wpv = 0.5, wrepos = 0.5)Table 4 Example of ontology classes for the input Penicillinis an antibiotic used to treat tonsillitisOntology Class No. definitions No. synonyms No. propertiesO1 penicillin 1 2 7antibiotic 1 7 16O2 penicillin 0 1 3tonsillitis 0 0 2The table shows the ontology name, the class name, and the number of classdefinitions, synonyms and propertiesMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 8 of 22Investigations [36], the Human Disease Ontology [37]and the Biomedical Resource Ontology [38]. These on-tologies are usually much smaller than more generalones, with only several hundred or a few thousandclasses, but they provide comprehensive knowledge fortheir fields.To evaluate ontology specialization, an ontology rec-ommender needs to quantify the extent to which a can-didate ontology fits the specialized nature of the inputdata. To do that, we reused the evaluation approach ap-plied by the original Ontology Recommender, andadapted it to the new annotation scoring strategy. Thespecialization score for each candidate ontology is calcu-lated according to the following expression:specialization o; tð Þ ¼ normXannotationScore2 að Þ þ 2  hierarchyLevel að Þð Þlog10 oj jð Þ !?a?Awhere o is the ontology being evaluated, t is the inputtext, annotationScore2(a) is the function that calculatesthe relevance score of an annotation (see Section Ontologycoverage), hierarchyLevel(a) returns the level of thematched class in the ontology hierarchy, and A is the setof all the annotations done with the ontology o for theinput t. Unlike the coverage and detail criteria, which con-sider only selectedAnnotations(A), the specialization criter-ion takes into account all the annotations returned by theAnnotator (i.e., A). This is generally appropriate becausean ontology that provides multiple annotations for a spe-cific text fragment is likely to be more specialized for thattext than an ontology that provides only one annotationfor it. The normalization by ontology size aims to assign ahigher score to smaller, more specialized ontologies. Ap-plying a logarithmic function decreases the impact of on-tologies with a very large size. Finally, the norm functionnormalizes the score to the interval [0,1].Using the same hypothetical ontologies, input, andannotations from the previous example, and taking intoaccount the size and annotation details shown in Table 5,the specialization score for O1 and O2 would becalculated as follows:specializationðO1; tÞ ¼ norm ð10þ 2  5Þ þ ð5þ 2  3Þlog10ð120000Þ ¼ norm 315:08 ¼ normð6:10ÞspecializationðO2; tÞ ¼ norm ð5þ 2  6Þ þ ð10þ 2  12Þlog10ð800Þ ¼ norm 512:90 ¼ normð17:59ÞIt is possible to see that the classes from O2 are lo-cated deeper in the hierarchy than are those from O1.Also, O2 is a much smaller ontology than O1. As aconsequence, according to our ontology-specializationmethod, O2 would be considered more specialized forthe input than O1, and would be assigned a higherspecialization score.Evaluation of ontology setsWhen annotating a biomedical text corpus or a list ofbiomedical keywords, it is often difficult to identify asingle ontology that covers all terms. In practice, it ismore likely that several ontologies will jointly cover theinput [8]. Suppose that a researcher needs to find thebest ontologies for a list of biomedical terms. If there isnot a single ontology that provides an acceptable cover-age it should then evaluate different combinations of on-tologies and return a ranked list of ontology sets that,together, provide higher coverage. For instance, in ourprevious example (Penicillin is an antibiotic used to treattonsillitis), O1 covers the terms penicillin and antibioticand O2 covers penicillin and tonsillitis. None of thoseontologies provides full coverage of all the relevant inputterms. However, by using O1 and O2 together, it is pos-sible to cover penicillin, antibiotic, and tonsillitis.Our method to evaluate ontology sets is based on theontology combinations approach used by the BiOSSsystem [21]. The system generates all possible sets of 2and 3 candidate ontologies (3 being the default maximum,though users may modify this limit according to their spe-cific needs) and it evaluates them using the criteria pre-sented previously. To improve performance, we use someheuristic optimizations to discard certain ontology setswithout performing the full evaluation process for them.Table 5 Ontology size and annotation details for the ontologies in Table 4Ontology Size Class Annotation type Annotation score Hierarchy levelO1 120,000 penicillin PREF 10 5antibiotic SYN 5 3O2 800 penicillin SYN 5 6tonsillitis PREF 10 12This table shows the number of classes (size) of each ontology, the class names, the annotation types, the annotation scores, and the level of each class in theontology hierarchy, such that 1 corresponds to the root (or top) level, 2 correspond to the level below the root classes, 3 to the next level, and so onMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 9 of 22For example, a set containing two ontologies that coverexactly the same terms will be immediately discardedbecause that sets coverage will not be higher than thatprovided by each ontology individually.The relevance score for each set of ontologies is calcu-lated using the same approach as for single ontologies,in accordance with the following expression:scoreSetðO; tÞ ¼ wc  coverageSetðO; tÞ þ wa acceptanceSetðOÞ þ wd detailSetðO; tÞ þ ws specializationSetðO; tÞwhere O = {o | o is an ontology} and |O| > 1. Thescores for the different evaluation criteria are calcu-lated as follows: coverageSet: It is computed the same way as for asingle ontology, but takes into account all theannotations performed with all the ontologies in theontology set. The system selects the bestannotations, and the sets input coverage iscomputed based on them. acceptanceSet, detailSet, and specializationSet:For each ontology, the system calculates its coveragecontribution (as a percentage) to the sets coveragescore. The recommender then uses this contributionto calculate all the other scores proportionally. Byusing this method, the impact (in terms ofacceptance, detail and specialization) of a particularontology on the set score will vary according to thecoverage provided by such ontology.Implementation detailsOntology Recommender 2.0 implements the ontologyrecommendation approach previously described in thispaper. Figure 2 shows the architecture of OntologyRecommender 2.0. Like its predecessor, it has two in-terfaces: a Web service API,6 which makes it possibleto invoke the recommender programmatically, and aWeb-based user interface, which is included in theNCBO BioPortal.7The Web-based user interface was developed usingthe Ruby-on-Rails Web framework and the Javascriptlanguage. Server side components were implementedusing the Ruby language. These components interact withother BioPortal services to retrieve all the informationneeded to achieve the recommendation process.The typical workflow is as follows. First, the OntologyRecommender calls the Annotator service to obtain allthe annotations performed for the input data using allBioPortal ontologies. Second, for each ontology, it in-vokes other BioPortal services to obtain the number ofclasses in the ontology, the number of visits to eachontology in a recent period of time, and to check thepresence of the ontology in UMLS. Third, for each an-notation performed with the ontology, it makes severalcalls to retrieve the number of definitions, synonymsand properties of the ontology class involved in the an-notation. The system has four independent evaluationmodules that use all this information to assess eachcandidate ontology according to the four evaluation cri-teria proposed in our approach: coverage, acceptance,detail, and specialization. Because of the system'smodular design, new ontology evaluation modules canbe easily plugged in.NCBO provides a Virtual Appliance for communitiesthat want to use the Ontology Recommender locally.This appliance is a pre-installed copy of the NCBO soft-ware that users can run and maintain. More informationabout obtaining and installing the NCBO Virtual Appli-ance is available at the NCBO Wiki.8The system uses a set of predefined parameters tocontrol how the different evaluation scores are calcu-lated, weighted and aggregated. Given that high inputcoverage is the main requirement for ontology recom-mendation systems, the weight assigned by default toontology coverage (0.55) is considerably higher than theweight assigned to ontology acceptance, detail andspecialization (0.15). Our system uses the same cover-age weight than the BiOSS system [21]. The defaultconfiguration provides appropriate results for generalontology recommendation scenarios. However, both theweb interface and the REST service allow users to adaptthe system to their specific needs by modifying theweights given to coverage, acceptance, knowledge de-tail, and specialization. The predefined values for all de-fault parameters used by Ontology Recommender 2.0are provided as an additional file [see Additional file 2].Some Ontology Recommender users may need to ob-tain repeatable results over time. Currently, however,any changes in the BioPortal ontology repository, suchas submitting a new ontology or removing an existingone, may change the suggestions returned by theOntology Recommender for the same inputs. BioPortalservices do not provide version-based ontology access,so services such as the Ontology Recommender and theAnnotator always run against the latest versions of theontologies. A possible way of dealing with this short-coming would be to install the NCBO Virtual Appli-ance with a particular set of ontologies and keep themlocally unaltered.The Ontology Recommender 2.0 was released inAugust 2015, as part of BioPortal 4.20.9 The traffic datafor 2016 reflects the great interest of the community onthe new system, with an average of 45.2 K calls permonth to the Ontology Recommender API, and 1.2 Kviews per month on the Ontology RecommenderMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 10 of 22webpage. These numbers represent an increase of morethan 600% in the number of calls to the API over 2015,and more than 30% in the number of pageviews over2015. Other widely used BioPortal services are Search,with an average of 873.9 K calls per month to theAPI, and 72.9 K pageviews per month in 2016; andthe Annotator, with an average of 484.8 K calls permonth to the API, and 3 K pageviews per month in2016. Detailed traffic data for the Ontology Recom-mender and other top used BioPortal services for theperiod 20142016 is provided as an additional file[see Additional file 1]. The source code is available inGitHub10 under a BSD License.User interfaceFigure 3 shows the Ontology Recommender 2.0 userinterface. The system supports two input types: plaintext and comma-separated keywords. It also providestwo kinds of output: ranked ontologies and rankedontology sets. The advanced options section, which isinitially hidden, allows the user to customize (1) theweights applied to the evaluation criteria, (2) the max-imum number of ontologies in each set (when usingthe ontology sets output), and (3) the list of candidateontologies to be evaluated.Figure 4 shows an example of the system's outputwhen selecting keywords as input and ontologies asoutput. For each ontology in the output, the user inter-face shows its final score, the scores for the four evalu-ation criteria used, and the number of annotationsperformed with the ontology on the input. For instance,the most highly recommended ontology in Fig. 4 is theSymptom Ontology (SYMP), which covers 17 of the 21input keywords. By clicking on the different rows of thecolumn highlight annotations, the user can select anyof the suggested ontologies and see which specific inputterms are covered. Also, clicking on a particular term inthe input reveals the details of the matched class in Bio-Portal. All scores are translated from the interval [0, 1]to [0, 100] for better readability. A score of '0' for a givenFig. 2 An overview of the architecture and workflow of Ontology Recommender 2.0. (1) The input data and parameter settings are receivedthrough any of the system interfaces (i.e., Web service or Web UI), and are sent to the system's backend. (2) The evaluation process starts. TheNCBO Annotator is invoked to retrieve all annotations for the input data. The system uses these annotations to evaluate BioPortal ontologies, oneby one, according to four criteria: coverage, acceptance, detail and specialization. Because of the system's modular design, additional evaluationcriteria can be easily added. The system uses BioPortal services to retrieve any additional information required by the evaluation process. Forexample, evaluation of ontology acceptance requires the number of visits to the ontology in BioPortal (pageviews), and checking whether theontology is present in the Unified Medical Language System (UMLS) or not. Four independent evaluation scores are returned for each ontology(one per evaluation criterion). (3) The scores obtained are combined into a relevance score for the ontology. (4) The relevance scores are used togenerate a ranked list of ontologies or ontology sets, which (5) is returned via the corresponding system's interfaceMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 11 of 22ontology and evaluation criterion means that the ontol-ogy has obtained the lowest score compared to the restof candidate ontologies. A score of '1' means that theontology has obtained the highest score, in relation to allthe other candidate ontologies.Figure 5 shows the Ontology sets output for the samekeywords displayed in Fig. 4. The output shows that usingthree ontologies (SYMP, SNOMEDCT and MEDDRA) itis possible to cover all the input keywords. Different colorsfor the input terms and for the recommended ontologiesin Fig. 5 distinguish the specific terms covered by eachontology in the selected set.LimitationsOne of the shortcomings of the current implementationis that the acceptance score is calculated using data fromFig. 3 Ontology Recommender 2.0 user interface. The user interface has buttons to select the input type (i.e., text or keywords) and outputtype (i.e., ontologies and ontology sets). A text area enables the user to enter the input data. The Get Recommendations button triggers theexecution. The advanced options button shows additional settings to customize the recommendation processFig. 4 Example of the Ontologies output. The user interface shows the top recommended ontologies. For each ontology, it shows the positionof the ontology in the ranking, the ontology acronym, the final recommendation score, the scores for each evaluation criteria (i.e., coverage,acceptance, detail, and specialization), and the number of annotations performed with the ontology. The highlight annotations buttonhighlights the input terms covered by the ontologyMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 12 of 22only two platforms. BioPortal is used to calculate thevisits score, and UMLS is used to calculate the presencescore. There are other widely known ontology repositor-ies that should be considered too. We believe that thereliability of the current implementation would be in-creased by taking into account visits and presence infor-mation from additional platforms, such as the OBOFoundry and the Ontology Lookup Service (OLS). Ex-tending our implementation to make use of additionalplatforms would require us to have a consistent mechan-ism to check the presence of each candidate ontologyinto other platforms, as well as a way to access updatedtraffic data from them.Another limitation is related to the ability to identifydifferent variations of a particular term. The coverageevaluation metric is dependent on the annotations iden-tified by the Annotator for the input data. The Annota-tor deals with synonyms and term inflections (e.g.,leukocyte, leukocytes, white blood cell) by using the syno-nyms contained in the ontology for a particular term.For example, Medical Subject Headings (MeSH) pro-vides 11 synonyms for the term leukocytes, includingleukocyte and white blood cells. As a consequence, theAnnotator would be able to perform an annotation be-tween the input term white blood cells and the MESHterm leukocytes. However, not all ontologies providesuch level of detail for their classes, and therefore theAnnotator may not be able to appropriately perform an-notations with them. The NCBO, in collaboration withUniversity of Montpellier, is currently investigatingseveral NLP approaches to improve the Annotator ser-vice. Applying lemmatization to both the input termsand the dictionary used by the Annotator is one of themethods currently being tested. As soon as these newfeatures will be made available in the Annotator, theywill automatically be used by Ontology Recommender.EvaluationTo evaluate our approach, we compared the performanceof Ontology Recommender 2.0 to Ontology Recom-mender 1.0 using data from a variety of well-known publicbiomedical databases. Examples of these databases arePubMed, which contains bibliographic information for thefields of biomedicine and health; the Gene ExpressionOmnibus (GEO), which is a repository of gene expressiondata; and ClinicalTrials.gov, which is a registry of clinicaltrials. We used the API provided by the NCBO ResourceIndex11 [39] to programmatically extract data fromthose databases.Experiment 1: input coverageWe selected 12 widely known biomedical databases andextracted 600 biomedical texts from them, with 127words on average, and 600 lists of biomedical keywords,with 17 keywords on average, producing a total of 1200inputs (100 inputs per database). The databases used arelisted in Table 6.Given the importance of input coverage, we first exe-cuted both systems for all inputs and compared thecoverage provided by the top-ranked ontology. WeFig. 5 Example of the Ontology sets output. The user interface shows the top recommended ontology sets. For each set, it shows its positionin the ranking, the acronyms of the ontologies that belong to it, the final recommendation score, the scores for each evaluation criteria(i.e., coverage, acceptance, detail, and specialization), and the number of annotations performed with all the ontologies in the ontologyset. The highlight annotations button highlights the input terms covered by the ontology setMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 13 of 22focused on the top-ranked ontology because the major-ity of users always select the first result obtained [40].The strategy we used to calculate the ontology coveragediffered depending on the input type: For texts, the coverage was computed as thepercentage of input words covered by the ontologywith respect to the total number of words thatcould be covered using all BioPortal ontologiestogether. For keywords, the coverage was computed as thepercentage of keywords covered by the ontologydivided by the total number of keywords.Figures 6 and 7 show a representation of the coverageprovided by both systems for each database and inputtype. Tables 7 and 8 provide a summary of the evalu-ation results.For some inputs, the first ontology suggested byOntology Recommender 1.0 provides very low coverage(under 20%). This results from one of the shortcomingspreviously described: Ontology Recommender 1.0 occa-sionally assigns a high score to ontologies that providelow coverage because they contain several classesmatching the input. The new recommendation ap-proach used by Ontology Recommender 2.0 addressesthis problem: Virtually none of its executions providesuch low coverage.For example, Table 9 shows the ontologies recom-mended if we input the following description of a dis-ease, extracted from the Integrated Disease View (IDV)database: Chronic fatigue syndrome refers to severe, con-tinued tiredness that is not relieved by rest and is notdirectly caused by other medical conditions. See also:Fatigue. The exact cause of chronic fatigue syndrome(CFS) is unknown. The following may also play a role inthe development of CFS: CFS most commonly occurs inwomen ages 30 to 50.Ontology Recommender 1.0 suggests the Bone Dyspla-sia Ontology (BDO), whereas Ontology Recommender2.0 suggests the NCI Thesaurus (NCIT). Because BDOcovers only 4 of the input terms, while NCIT covers 17,the recommendation provided by Ontology Recom-mender 2.0 is more appropriate than that of itspredecessor.Ontology Recommender 2.0 also provides bettermean coverage for both input types (i.e., text and key-words) across all the biomedical databases included inthe evaluation. Compared to Ontology Recommender1.0, the mean coverage reached using OntologyRecommender 2.0 was 14.9% higher for texts and19.3% higher for keywords. That increase was evengreater using the ontology sets output type providedby Ontology Recommender 2.0, which reached a meancoverage of 92.1% for texts (31.3% higher than theOntology Recommender 1.0 ratings) and 89.8% forkeywords (26.9% higher).For the selected texts, the average execution time ofOntology Recommender 2.0 for the "ontologies" outputis 15.4 s, 43.9% higher than the Ontology Recommender1.0 execution time (10.7 s). The ontology recommenda-tion process performed by Ontology Recommender 2.0is much more complex than the one performed by theoriginal version, and this is reflected by the executiontimes. The average execution time for keywords is simi-lar in both systems (9.5 s for Ontology Recommender1.0 and 9.4 s for Ontology Recommender 2.0). Whendealing with keywords, the complex process performedby Ontology Recommender 2.0 is compensated by itsability to discard unnecessary annotations before staringTable 6 Databases used for experiment 1Database name Acronym Topic Source field TypeARRS GoldMiner GM Biomedical images Image caption TextAutism Database (AutDB) AUTDB Autism spectrum disorders Phenotype profile TextGene Expression Omnibus GEO Gene expression Summary TextIntegrated Disease View IDV Disease and treatment Description TextPubMed PM Biomedicine Abstract TextPubMed Health Drugs PMH Drugs Why is this medication prescribed? TextAdverse Event Reporting System AERS Adverse events Adverse reactions KeywordsAgingGenesDB AGDB Aging related genes Keywords KeywordsClinicalTrials.gov CT Clinical trials Condition KeywordsDrugBank DBK Drugs Drug category KeywordsPharmGKB-Gene PGGE Relationships about drugs, diseases and genes Gene related diseases KeywordsUniProt KB UPKB Proteins Biological processes KeywordsThe table shows the database name, its acronym, the main topic of the database, the specific field from which the information was extracted, and the type oftextual data extracted (i.e., text or keywords)Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 14 of 22the ontology evaluation process. These execution timesare substantially better than those reported for similarsystems. For example, the BiOSS system [21] needed anaverage of 207 s to process 30 keywords with a repositoryof 200 candidate ontologies. Performance of OntologyRecommender 2.0 is reasonable for general scenarios,where the quality of the suggestions is typically more im-portant than the execution time.Experiment 2: refining recommendationsOur second experiment set out to examine whetherOntology Recommender 2.0 is effective at discerninghow to make meaningful recommendations when ontol-ogies exhibit similar coverage of the input text. Specific-ally, we were interested in analyzing how the new versionuses ontology acceptance, detail and specialization toprioritize the most appropriate ontologies.We started with the 1200 inputs (600 texts and 600lists of keywords) from the previous experiment, andselected those inputs for which the two versions ofOntology Recommender suggested different ontologieswith similar coverage. We considered two coveragevalues similar if the difference between them was lessthan 10%. This yielded a total of 284 inputs (32 inputtexts and 252 lists of keywords). We executed both sys-tems for those 284 inputs and analyzed the ontologiesobtained in terms of their acceptance, detail andspecialization scores.Figure 8 and Table 10 show the results obtained. Theontologies suggested by Ontology Recommender 2.0have higher acceptance (87.1) and detail scores (72.1)Fig. 6 Coverage distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0 (solid blue line), using theindividual ontologies output, for 600 texts extracted from 6 widely known databases (100 texts each). Vertical lines represent the mean coverageprovided by the first ontology returned by Ontology Recommender 1.0 (dotted red line) and 2.0 (dashed-dotted blue line). The X-axis indicates thepercentage of words covered by the ontology. The Y-axis displays the number of inputs for which a particular coverage percentage wasobtained. AUTDB: Autism Database; GEO: Gene Expression Omnibus; GM: ARRS GoldMiner; IDV: Integrated Disease View; PM: PubMed; PMH:PubMed Health DrugsMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 15 of 22Fig. 7 Coverage distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0 (solid blue line), usingthe individual ontologies output, for 600 lists of keywords extracted from 6 widely known databases (100 lists of keywords each). Vertical linesrepresent the mean coverage provided by the first ontology returned by Ontology Recommender 1.0 (dotted red line) and 2.0 (dashed-dotted blueline). The X-axis indicates the percentage of input keywords covered by the ontology. The Y-axis displays the number of inputs for which aparticular coverage percentage was obtained. AERS: Adverse Event Reporting System; AGDB: AgingGenesDB; CT: ClinicalTrials.gov; DBK: DrugBank;PGGE: PharmGKB-Gene; UPKB: UniProt KBTable 7 Summary of evaluation results for text inputsDatabase MeanlengthaExecutions with coverage < 20%b Mean coverage (top ranked ontology)c Execution time (seconds)1.0* 2.0** 1.0* 2.0** 2.0 (sets)*** 1.0* 2.0** 2.0 (sets)***AUTDB 128.8 12.0% 0.0% 66.8% 76.0% 90.3% 12.2 18.3 26.9GEO 146.4 8.0% 0.0% 70.7% 76.9% 92.9% 11.2 17.2 26.1GM 55.7 48.0% 0.0% 46.6% 82.6% 94.8% 9.6 12.3 15.2IDV 150.2 28.0% 0.0% 50.6% 71.8% 89.3% 9.5 13.1 21.1PM 208.9 7.0% 0.0% 69.1% 73.8% 93.1% 13.8 21.2 36.9PMH 77.4 13.0% 0.0% 61.1% 73.5% 91.9% 8.0 10.5 13.3Mean 127.9 19.3% 0.0% 60.8% 75.7% 92.1% 10.7 15.4 23.2aMean of the number of words for the inputs extracted from the databasebPercentage of executions where the coverage of the top recommended ontology was lower than 20%cMean coverage provided by the top ranked ontology*Ontology Recommender 1.0; **Ontology Recommender 2.0; ***Ontology Recommender 2.0 (ontology sets output)Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 16 of 22than those suggested by Ontology Recommender 1.0.Importantly, the graphs show peaks of low acceptance(<30%) and detail (<20%) for Ontology Recommender1.0 that are addressed by Ontology Recommender 2.0.The ontologies suggested by Ontology Recom-mender 2.0 have, on average, lower specializationscores (65.1) than those suggested by OntologyRecommender 1.0 (95.1). This is an expected result,given that the recommendation approach used byOntology Recommender 1.0 is based on the relationbetween the number of annotations provided by eachontology and its size, which is our measure forontology specialization.Ontology Recommender 1.0 is better than OntologyRecommender 2.0 at finding small ontologies thatprovide multiple annotations for the users input.However, those ontologies are not necessarily themost appropriate to describe the input data. As wehave seen (see Section 1.2.1), a large number of anno-tations does not always indicate a high input cover-age. Ontology Recommender 1.0 sometimes suggestsontologies with high specialization scores but withvery low input coverage, which makes the ontologiesinappropriate for the users input. The multi-criteriaevaluation approach used by Ontology Recommender2.0 has been designed to address this issue by evaluatingontology specialization in combination with other cri-teria, including ontology coverage.Experiment 3: high coverage and specialized ontologiesWe set out to evaluate how well Ontology Recommender2.0 prioritizes recommending small ontologies that pro-vide appropriate coverage for the input data. We created15 inputs, each of which contained keywords from a veryspecific domain (e.g., adverse reactions, dermatology, unitsof measurement), and executed both versions of theOntology Recommender for those inputs.Table 11 shows the particular domain for each of the15 inputs used, and the first ontology suggested by eachversion of Ontology Recommender, as well as the size ofeach ontology and the coverage provided.Analysis of the results reveals that Ontology Recom-mender 2.0 is more effective than Ontology Recom-mender 1.0 for suggesting specialized ontologies thatprovide high input coverage. In 9 out of 15 inputs (60%),the first ontology suggested by Ontology Recommender2.0 is more appropriate, in terms of its size and coverageprovided, than the ontology recommended by OntologyRecommender 1.0. Ontology Recommender 2.0 considersinput coverage in addition to ontology specialization,which Ontology Recommender 1.0 does not. In addition,Ontology Recommender 2.0 uses a different annotationscoring method (the function annotationScore2(a); seeSection 2.1.1) that gives more weight to annotationsthat cover multi-word terms. There is one input (no.13), for which the ontology suggested by Ontology Rec-ommender 2.0 provides higher coverage (88% versusTable 8 Summary of evaluation results for keyword inputsDatabase Mean lengtha Executions with coverage < 20%b Mean coverage (top ranked ontology)c Execution time (seconds)1.0* 2.0** 1.0* 2.0** 2.0 (sets)*** 1.0* 2.0** 2.0 (sets)***AERS 29.5 6.0% 0.0% 54.6% 97.8% 99.6% 10.2 9.1 10.5AGDB 8.2 5.0% 0.0% 53.4% 67.5% 82.9% 6.5 9.9 10.9CT 16.6 12.0% 2.0% 61.4% 76.5% 84.8% 9.9 8.4 10.2DBK 5.9 13.0% 1.0% 60.5% 74.7% 89.6% 4.3 6.8 7.3PGGE 15.4 2.0% 0.0% 73.1% 80.5% 83.0% 9.9 9.1 10.3UPKB 29.9 18.0% 0.0% 74.6% 96.3% 99.1% 16.5 13.1 16.9Mean 17.6 9.3% 0.5% 62.9% 82.2% 89.8% 9.5 9.4 11.0aMean of the number of words for the inputs extracted from the databasebPercentage of executions where the coverage of the top recommended ontology was lower than 20%cMean coverage provided by the top ranked ontology*Ontology Recommender 1.0; **Ontology Recommender 2.0; ***Ontology Recommender 2.0 (ontology sets output)Table 9 Comparison of the terms covered by Ontology Recommender 1.0 and Ontology Recommender 2.0 for the input textpreviously shownOntology Position Terms coveredOntology Recommender 1.0 Ontology Recommender 2.0BDO 1 23 Chronic, severe, chronic, unknownNCIT 2 1 Chronic fatigue syndrome, severe, continued, rest, directly, medical, Fatigue, exact, cause,chronic, fatigue syndrome, unknown, suggest, due to, following, role, development, agesMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 17 of 2280%), but it is bigger than the ontology recommendedby Ontology Recommender 1.0 (324 K classes versus119 K). In 5 out of 15 inputs (33%), both systems rec-ommended the same ontology.DiscussionRecommending biomedical ontologies is a challengingtask. The great number, size, and complexity of biomed-ical ontologies, as well as the diversity of user require-ments and expectations, make it difficult to identify themost appropriate ontologies to annotate biomedical data.The analysis of the results demonstrates that ontologiessuggested using our new recommendation approach aremore appropriate than those recommended using theoriginal method. Our acceptance evaluation method hasproved to be successful to rank ontologies, and it is cur-rently used not only by the Ontology Recommender, butalso by the BioPortal search engine. The classes returnedwhen searching in BioPortal are ordered according to thegeneral acceptance of the ontologies to which they belong.We note that, because the system is designed in amodular way, it will be easy to add new evaluation cri-teria to extend its functionality. As a first priority, weintend to improve and extend the evaluation criteriacurrently used. In addition, we will investigate the effectof extending the Ontology Recommender to includerelevant features not yet considered, such as the fre-quency of an ontologys updates, its levels of abstrac-tion, formality, granularity, and the language in whichthe ontology is expressed.Indeed, using metadata information is a simple butoften ignored approach to select ontologies. Coverage-based approaches often miss relevant results becausethey focus on the content of ontologies and ignore moreFig. 8 Acceptance, detail and specialization distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0(solid blue line), for the 284 inputs selected. Vertical lines represent the mean acceptance, detail and specialization scores provided by OntologyRecommender 1.0 (dotted red line) and 2.0 (dashed-dotted blue line). The X-axis indicates the acceptance, detail and specialization score providedby the top ranked ontology. The Y-axis displays the number of inputs for which a particular score was obtainedTable 10 Mean acceptance, detail and specialization scores provided by the two versions of Ontology Recommender forexperiment 2Text (32 inputs) Keywords (252 inputs) All (284 inputs)1.0* 2.0** 1.0* 2.0** 1.0* 2.0**Mean acceptance 91.3 99.2 39.8 85.2 45.7 87.1Mean detail 5.8 56.1 15.7 73.9 14.6 72.1Mean specialization 94.7 90.3 94.8 61.6 95.1 65.1*Ontology Recommender 1.0; **Ontology Recommender 2.0Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 18 of 22general information about the ontology. For example,applying the new Ontology Recommender to the Wiki-pedia definition of anatomy12 will return some widely-known ontologies that contain the terms anatomy, struc-ture, organism and biology, but the Foundational Modelof Anatomy (FMA), which is the reference ontologyabout human anatomy will not show up in the top 25 re-sults. Our specialization criterion uses the content of theontology and the ontology size to discriminate betweenlarge ontologies and small ontologies that have betterspecialization. However, ontologies that provide multipleannotations for the input data are not always specializedto deal with the input domain. Sometimes very special-ized ontologies for a domain may provide low coveragefor a particular text from the domain. In this scenario,metadata about the domain of the ontology (e.g., 'anat-omy' in the case of FMA) could be used to enhance ourontology specialization criterion by limiting the sugges-tions to those ontologies whose domain matches the in-put data domain. We are currently refining, incollaboration with the Center for Expanded Data Anno-tation and Retrieval (CEDAR) [41] and the AgroPortalontology repository [42], the way BioPortal handlesmetadata for ontologies in order to support even moreontology recommendation scenarios.Our coverage evaluation approach may be further en-hanced by complementing our annotation scoringmethod (i.e., annotationScore2) with term extractiontechniques. We plan to analyze the application of aterm extraction measure, called C-value [43], which isspecialized for multi-word term extraction, and thathas already been applied to the results of the NCBOAnnotator, leading to significant improvements [44].There are some possible avenues for enhancing ourassessment of ontology acceptance. These include con-sidering the number of projects that use a specificontology, the number of mappings created manuallythat point to a particular ontology, the number of usercontributions (e.g., mappings, notes, comments), themetadata available per ontology, and the number, publi-cation date and publication frequency of ontology ver-sions. There are other indicators external to BioPortalthat could be useful for performing a more comprehen-sive evaluation of ontology acceptance, such as thenumber of Google results when searching for the ontol-ogy name or the number of PubMed publications thatcontain the ontology name [21].Reusing existing ontologies instead of building newones from scratch has many benefits, including lower-ing the time and cost of development, and avoidingduplicate efforts [45]. As shown by a recent study [46],reuse is fairly low in BioPortal, but there are someontologies that are approaching complete reuse (e.g.,Mental Functioning Ontology). Our approach shouldbe able to identify these ontologies and assign them alower score than those ontologies where the knowledgewas first defined. We will study the inclusion of add-itional evaluation criteria to weigh the amount ofTable 11 Experiment 3 resultsInput Size Input domain Ontology Recommender 1.0 Ontology Recommender 2.0Result Size Coverage (%) Result Size Coverage (%)1 23 Adverse reactions SNOMEDCT 324,129 52.1 MEDDRA 68,261 91.32 8 Autism spectrum disorder ASDPTO 284 100.0 ASDPTO 284 100.03 14 Bioinformatics operations SWO 4,068 100.0 EDAM 3,240 100.04 30 Biomedical investigations NCIT 118,941 63.3 OBI 3,055 100.05 26 Cell types SYN 14,462 76.9 CL 6,532 88.46 14 Clinical research NCIT 118,941 78.6 OCRE 389 100.07 13 Dermatology DERMLEX 6,106 92.3 DERMLEX 6,106 92.38 14 Environmental features ENVO 2,307 100.0 ENVO 2,307 100.09 18 Enzyme sources NCIT 118,941 55.6 BTO 5,902 72.210 19 Fish anatomy NIFSTD 124,337 68.4 TAOcpr 3,428 79.011 44 Human diseases RH-MESH 305,349 52.3 DOID 11,280 95.512 13 Mathematical models in life sciences MAMO 100 100.0 MAMO 100 100.013 25 Primary care NCIT 118,941 80.0 SNOMEDCT 324,129 88.014 23 Signs and symptoms NCIT 118,941 65.2 SYMP 936 91.315 25 Units of Measurement TEO 687 92.0 TEO 687 92.0The size of each ontology (number of classes) and the coverage provided are also shown. The best results for each input (lowest ontology size and highest coverage),are highlighted in boldThe table shows the input size (number of keywords) and domain, as well as the first ontology suggested by Ontology Recommender 1.0 and OntologyRecommender 2.0Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 19 of 22original knowledge provided by a particular ontologyfor the input data.The current version of Ontology Recommender uses aset of default parameters to control how the differentevaluation scores are calculated, weighted and aggregated.These parameters provide acceptable results for generalontology recommendation scenarios, but some users mayneed to modify the default settings to match their needs.In the future, we would like the system to use an auto-matic weight adjustment approach. We will investigatewhether it is possible to develop methods of adjusting theweights dynamically for specific scenarios.Ontology Recommender helps to identify all the ontol-ogies that would be suitable for semantic annotation.However, given the number of ontologies in BioPortal, itwould be difficult, computationally expensive, and oftenuseless to annotate user inputs with all the ontologies inthe repository. Ontology Recommender could functionwithin BioPortal as a means to screen ontologies foruse with the NCBO Annotator. Note that the output ofthe Annotator is a ranked list of annotations performedwith multiple ontologies, while the output of the Ontol-ogy Recommender is a ranked list of ontologies. A usermight be offered the possibility to Run the OntologyRecommender first before actually calling the Annota-tor. Then only the top-ranked ontologies would be usedfor annotations.A user-based evaluation would help us understand thesystems utility in real-world settings. Our experienceevaluating the original Ontology Recommender andBiOSS showed us that obtaining a user-based evaluationof an ontology recommender system is a challengingtask. For example, the evaluators of BiOSS reported thatthey would need at least 50 min to perform a high-quality evaluation of the system for each test case. Weplan to investigate whether crowd-sourcing methods, asan alternative, can be useful to evaluate ontology recom-mendation systems from a user-centered perspective.Our approach for ontology recommendation was de-signed for the biomedical field, but it can be adaptedto work with ontologies from other domains so long asthey have a resource equivalent to the NCBO Annota-tor, an API to obtain basic information about all thecandidate ontologies, and their classes, and alternativeresources for extracting information about the accept-ance of each ontology. For example, AgroPortal [42] isan ontology repository based on NCBO BioPortal tech-nology. AgroPortal uses Ontology Recommender 2.0 inthe context of plant, agronomic and environmentalsciences.13ConclusionsBiomedical ontologies are crucial for representingknowledge and annotating data. However, the largenumber, complexity, and variety of biomedical ontol-ogies make it difficult for researchers to select themost appropriate ontologies for annotating their data.In this paper, we presented a novel approach forrecommending biomedical ontologies. This approachhas been implemented as release 2.0 of the NCBOOntology Recommender, a system that is able to findthe best ontologies for a biomedical text or set ofkeywords. Ontology Recommender 2.0 combines thestrengths of its predecessor with a range of adjust-ments and new features that improve its reliabilityand usefulness.Our evaluation shows that, on average, the new systemis able to suggest ontologies that provide better inputcoverage, contain more detailed information, are morespecialized, and are more widely accepted than thosesuggested by the original Ontology Recommender. Inaddition, the new version is able to evaluate not only in-dividual ontologies, but also different ontology sets, inorder to maximize input coverage. The new system canbe customized to specific user needs and it providesmore explanatory output information than its predeces-sor, helping users to understand the results returned.The new service, embedded into the NCBO BioPortal,will be a more valuable resource to the community ofresearchers, scientists, and developers working withontologies.Endnotes1The BioPortal API received 18.8 M calls/month onaverage in 2016. The BioPortal website received 306.9 Kpageviews/month on average in 2016 (see Additionalfile 1 for more detailed traffic data). The two main Bio-Portal papers [3, 4] accumulate 923 citations at the timeof writing this paper, with 145 citations received in2016.2http://bioportal.bioontology.org/3At the time of writing this paper, there are 63 cita-tions to the NCBO Ontology Recommender 1.0 paper[6]. The Ontology Recommender 1.0 API received 7.1 Kcalls/month on average in 2014. The Ontology Recom-mender webpage received 1.4 K pageviews/month onaverage in 2014. Detailed traffic data is provided inAdditional file 1.4This formula is slightly different from the scoringmethod presented in the paper describing the originalOntology Recommender Web service [6]. It correspondsto an upgrade done in the recommendation algorithm inDecember 2011, when BioPortal 3.5 was released, forwhich description and methodology was never pub-lished. The normalization strategy was improved byapplying a logarithmic transformation to the ontologysize to avoid a negative effect on very large ontologies.Mappings between ontologies, used to favor referenceMartínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 20 of 22ontologies, were discarded due to the small number ofmanually created and curated mappings that could beused for such a purpose. The hierarchy-based semanticexpansion was replaced by the position of the matchedclass in the ontology hierarchy.5The function is called annotationScore2 to differentiateit from the original annotationScore function.6The API documentation is available at http://data.bioontology.org/documentation#nav_recommender7The Web-based user interface is available at http://bioportal.bioontology.org/recommender8https://www.bioontology.org/wiki/index.php/Category:NCBO_Virtual_Appliance9BioPortal release notes: https://www.bioontology.org/wiki/index.php/BioPortal_Release_Notes10https://github.com/ncbo/ncbo_ontology_recommender11The NCBO Resource Index is an ontology-basedindex that provides access to over 30 million biomedicalrecords from 48 widely-known databases. It is availableat: http://bioportal.bioontology.org/.12https://en.wikipedia.org/wiki/Anatomy13http://agroportal.lirmm.fr/recommenderAdditional filesAdditional file 1: Ontology Recommender traffic summary. Summary oftraffic received by the Ontology Recommender for the period 20142016,compared to the other most used BioPortal services. (PDF 27 kb)Additional file 2: Default configuration settings. Default values used bythe NCBO Ontology Recommender 2.0 for the parameters that controlhow the different scores are calculated, weighted and aggregated.(PDF 9 kb)AbbreviationsBIOMODELS: BioModels ontology (BIOMODELS); COSTART: Coding Symbolsfor Thesaurus of Adverse Reaction Terms; CPT: Current ProceduralTerminology; CRISP: Computer Retrieval of Information on Scientific Projectsthesaurus; EFO: Experimental Factor Ontology; EHDA: Human DevelopmentalAnatomy Ontology, timed version; EP: Cardiac Electrophysiology Ontology;FMA: Foundational Model of Anatomy; GO: Gene Ontology;HUPSON: Human Physiology Simulation Ontology; ICD9CM: InternationalClassification of Diseases, version 9 - Clinical Modification; ICPC: InternationalClassification of Primary Care; LOINC: Logical Observation Identifier Namesand Codes; MEDDRA: Medical Dictionary for Regulatory Activities;MEDLINEPLUS: MedlinePlus Health Topics; MESH: Medical Subject Headings;MP: Mammalian Phenotype Ontology; NCIT: National Cancer InstituteThesaurus; NDDF: National Drug Data File; NDFRT: National Drug File -Reference Terminology; OMIM: Online Mendelian Inheritance in Man;PDQ: Physician Data Query; RCD: Read Codes, Clinical Terms version 3;RXNORM: RxNORM; SNOMEDCT: Systematized Nomenclature of Medicine -Clinical Terms; SWEET: Semantic Web for Earth and Environment TechnologyOntology; SYMP: Symptom Ontology; VANDF: Veterans Health AdministrationNational Drug File; VSO: Vital Sign OntologyAcknowledgmentsThe authors acknowledge the suggestions about the problem of recommendingontologies provided by the NCBO team, as well as their assistance and advice onintegrating Ontology Recommender 2.0 into BioPortal. The authors also thankSimon Walk for his report on the BioPortal traffic data. Natasha Noy and VanessaAguiar offered valuable feedback.FundingThis work was supported in part by the National Center for BiomedicalOntology as one of the National Centers for Biomedical Computing,supported by the NHGRI, the NHLBI, and the NIH Common Fund undergrant U54 HG004028 from the U.S. National Institutes of Health. Additionalsupport was provided by CEDAR, the Center for Expanded Data Annotationand Retrieval (U54 AI117925) awarded by the National Institute of Allergyand Infectious Diseases through funds provided by the trans-NIH Big Data toKnowledge (BD2K) initiative. This project has also received support from theEuropean Unions Horizon 2020 research and innovation programme underthe Marie Sklodowska-Curie grant agreement No 701771 and the FrenchNational Research Agency (grant ANR-12-JS02-01001).Availability of data and materials Project name: The Biomedical Ontology Recommender. Project home page: http://bioportal.bioontology.org/recommender. Project GitHub repository: https://github.com/ncbo/ncbo_ontology_recommender. REST service parameters: http://data.bioontology.org/documentation#nav_recommender. Operating system(s): Platform independent. Programming language: Ruby, Javascript, HTML. Other requirements: none. License: BSD (http://www.bioontology.org/BSD-license). Datasets used in our evaluation: https://git.io/vDIXV.Authors contributionsMMR conceived the approach, designed and implemented the system, anddrafted the initial manuscript. CJ participated in technical discussions andprovided ideas to refine the approach. MAM supervised the work and gaveadvice and feedback at all stages. CJ, MJO, JG, and AP provided criticalrevision and edited the manuscript. All authors gave the final approval of themanuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Stanford Center for Biomedical Informatics Research, 1265 Welch Road,Stanford University School of Medicine, Stanford, CA 94305-5479, USA.2Department of Information and Communication Technologies, ComputerScience Building, Elviña Campus, University of A Coruña, 15071 A Coruña,Spain. 3Laboratory of Informatics, Robotics and Microelectronics ofMontpellier (LIRMM), University of Montpellier, 161 rue Ada, 34095Montpellier, Cdx 5, France.Received: 28 October 2016 Accepted: 13 April 2017RESEARCH Open AccessDisSetSim: an online system for calculatingsimilarity between disease setsYang Hu1, Lingling Zhao2, Zhiyan Liu2, Hong Ju3, Hongbo Shi4, Peigang Xu2, Yadong Wang2* and Liang Cheng4*From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China. 16 December 2016AbstractBackground: Functional similarity between molecules results in similar phenotypes, such as diseases. Therefore, it isan effective way to reveal the function of molecules based on their induced diseases. However, the lack of a toolfor obtaining the similarity score of pair-wise disease sets (SSDS) limits this type of application.Results: Here, we introduce DisSetSim, an online system to solve this problem in this article. Five state-of-the-art methodsinvolving Resniks, Lins, Wangs, PSB, and SemFunSim methods were implemented to measure the similarity score of pair-wise diseases (SSD) first. And then pair-wise-best pairs-average (PWBPA) method was implemented to calculated theSSDS by the SSD. The system was applied for calculating the functional similarity of miRNAs based on their induceddisease sets. The results were further used to predict potential disease-miRNA relationships.Conclusions: The high area under the receiver operating characteristic curve AUC (0.9296) based on leave-one-out crossvalidation shows that the PWBPA method achieves a high true positive rate and a low false positive rate. The system canbe accessed from http://www.bio-annotation.cn:8080/DisSetSim/.Keywords: Functional similarity, Similarity score, Disease sets, Disease-miRNA relationshipsBackgroundThe similarity of pair-wise disease sets (SDS) has drawnmore and more attention in identifying functional similar-ity of the disease-caused molecules [1], predicting poten-tial relationships between diseases and molecules [28],and so on. In previous studies, Wang et al. utilized theSDS to construct a human miRNA functional similaritynetwork (MFSN) [1]. And Sun et al. used the SDS topredict novel disease lncRNA relationships [9].The performance of calculating the SDS is mainlybased on the method for computing the similarity ofpair-wise diseases (SD). Currently, seven state-of-artmethods involving Resniks [10], Lins [11], Wangs [12],process-similarity based (PSB) [13], SemFunSim [14],ILNCSIM [15], and FMLNCSIM [16] methods werefrequently used for computing the SD. Among thesemethods, Resniks [10], Lins [11], and Wangs methods[12] are designed earlier for Gene Ontology (GO) [8,17]. And these methods were introduced for calculatingthe SD by DOSim [18] and DisSim [19]. Resniks andLins methods [10, 11] are based on information content(IC) for computing similarity between terms of ontology.And Wangs method [12] is based on the hierarchicalstructure of the ontology. PSB and SemFunSim methodsare newly developed for Disease Ontology (DO) [20].PSB method [13] utilized the association of biologicalprocess between genes to calculate disease similarity. Incomparison, SemFunSim method [14] considered moretypes of the functional associations including protein-protein interaction [21], human mRNA co-expression[22], and so on.Resources for calculating the similarity score of pair-wise diseases (SSD) mainly includes the vocabularies ofdiseases and disease-related genes. The frequently useddisease vocabularies contain Online Mendelian Inherit-ance in Man (OMIM) [23], Medical Subject Headings* Correspondence: ydwang@hit.edu.cn; liangcheng@hrbmu.edu.cn2Department of Computer Science and Technology, Harbin Institute ofTechnology, Harbin 150001, Peoples Republic of China4College of Bioinformatics Science and Technology, Harbin MedicalUniversity, Harbin 150001, Peoples Republic of ChinaFull list of author information is available at the end of the article© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28DOI 10.1186/s13326-017-0140-2(MeSH) [24], and DO [20]. OMIM records the names ofgenetic disorders without providing semantic associationsbetween them. MeSH provides a hierarchy of terms inbiomedical domain. It contains 16 categories, of whichonly C and F03 involve disease names. In comparison withOMIM and MeSH, DO has been established around theconcept of disease, and it aims to provide a clear definitionfor each disease. The disease-related genes are scattered inthe databases, such as Gene Reference into Function(GeneRIF) [25], OMIM [23], Genetic AssociationDatabase (GAD) [26] and Comparative ToxicogenomicsDatabase (CTD) [27]. It is better to use relationships of allof these databases.pair-wise-all pairs-maximum (PWAPM) method andpair-wise-best pairs-average (PWBPA) method areoptional for calculating similarity of pair-wise term sets[28]. For comparing multiple aspects, the best measureis the PWBPA method, which is widely utilized in calcu-lating similarity of DO and GO term sets [1, 7, 9, 12].Although DOSim [18] and DisSim [19] implementedthe disease similarity methods in R package and webinterface, no tools provided the function to calculate thesimilarity score of pair-wise disease sets (SSDS)currently. In this article, we designed and implementedan online tool DisSetSim to calculate the SSDS. Fivestate-of-art disease similarity methods (Resniks, Lins,Wangs, PSB, and SemFunSim) and the PWBPA methodwas implemented in the tool. The system is freely avail-able at http://www.bio-annotation.cn:8080/DisSetSim/.MethodsDate sourcesData sets of DisSetSim are from open source databases, andthey are listed in Table 1. DO [20] records disease names. Itprovides terms for calculating disease similarity. GeneRIF[25], OMIM [23], GAD [26] and CTD [27] are manuallycurated databases of disease-related genes. All of diseases inthese databases are mapped to terms in DO based on SIDD[29]. GO annotation (GOA) [30] includes functional anno-tation of genes. HumanNet is the gene functional networkof human. In addition, HMDD v2.0 [31] contains disease-related miRNAs, diseases of which were manually mappedto terms in DO by OAHG [32].Methods for calculating similarity score of pair-wisediseasesFive state-of-art methods involving Resniks [10], Lins[11], Wangs [12], PSB [13], and SemFunSim methods[14] have been implemented for calculating the SSD.Resniks and Lins methods are based on IC. The IC ofa disease t is described as Eq. 1:IC tð Þ ¼ ?log2ntN; ð1Þwhere N is the total number of genes annotated bydiseases, and nt is the number of genes annotated by t.Assuming t1 and t2 are two diseases, the similarity ofthem is defined by Resnik as following [10]:SimResnik t1; ; t2ð Þ ¼ IC tMICAð Þ; ð2Þwhere tMICA is the most informative common ancestor(MICA) of t1 and t2. Lin defines the similarity of t1 andt2 as Eq. 3 [11]:SimLin t1; ; t2ð Þ ¼ 2?IC tMICAð ÞIC t1ð Þ þ IC t2ð Þ : ð3ÞAssuming T1 is the set involving t1 and all of its ances-tor terms of ontology. Semantic contribution of term tto t1 is represented as following:St1 tð Þ ¼1 t ¼ t1St1 tð Þ ¼ max w?St1 t0  j t 0?T 1 t?t1(;ð4Þwhere w is the contribution factor of each semanticrelationship. According to Wang et al. [1], w is definedas 0.5 for IS_A relationship of DO [20]. Then, all thesemantic contributions of T1 to t1 is SV(t1), which isdefined as following:SV t1ð Þ ¼Xt?T1St1 tð Þ: ð5ÞAssuming T2 is the set involving t2 and all of its ances-tor terms, the similarity between t1 and t2 is defined asfollowing by Wangs method [12]:SimWang t1; t2ð Þ ¼Xt?T 1?T 2St1 tð Þ þ St2 tð Þð ÞSV t1ð Þ þ SV t2ð Þ : ð6ÞAssuming t1 and t2 can be related with m and n bio-logical processes of GO based on hypergeometric test,respectively, the similarity of t1 and t2 is defined by thePSB method as following:Table 1 Data sourcesData source Web siteDO http://disease-ontology.org/CTD http://ctdbase.org/GeneRIF http://www.ncbi.nlm.nih.gov/gene/about-generifGAD https://geneticassociationdb.nih.gov/OMIM http://www.omim.org/GO & GOA http://www.geneontology.orgHumanNet http://www.functionalnet.org/humannet/OAHG bio-annotation.cn/OAHG/The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 20 of 79SimPSB t1; t2ð Þ ¼ 12Xmi¼1max1?j?nSim p1i; p2j  m0BBB@þXnj¼1max1?i?mSim p2j; p1i  n1CCCCAð7Þwhere p1i and p2j is the ith and jth significant relatedbiological process terms of t1 and t2, respectively.Sim(p1i, p2j) represents similarity between two processesp1i and p2j, which is defined as Eq. 8:Sim p1; p2ð Þ ¼12? ICGO p1ð Þ þ ICGO p2ð Þð Þ?n p1?p2ð Þn p1?p2ð Þ?ICGO p1ð ÞMax ICGOð Þ?ICDO p1ð ÞMax ICDOð Þ ?ICGO p2ð ÞMax ICGOð Þ ?ICDO p2ð ÞMax ICDOð Þ ;ð8Þwhere ICGO and ICDO represent IC based on GO andDO, respectively. n(p1 ?p2) and n(p1 ?p2) denote thenumber of common genes of p1 and p2, and the numberof total genes of p1 and p2, respectively.Assuming G1 and G2 represent related gene sets of t1and t2, respectively. Then, the similarity of t1 and t2 bythe SemFunSim method can be described as following:SimSemFunSim t1;t2ð Þ¼Xmi¼1max1?j?nSim g1i; g2j  þXnj¼1max1?i?mSim g2j; g1i  mþn?m?GMICA??n?GMICA?ð9Þwhere |GMICA| represents the number of genes inGMICA. m and n denote the number of genes in G1 andG2, respectively. Sim(g1i, g2j) is the functional similarityscore between genes g1i and g2j, which could be obtainedfrom HumanNet [33].Method for calculating similarity score of pair-wise dis-ease setsThe PWBPA method was utilized for calculating theSSDS. The similarity of two disease sets T1 and T2 is de-fined as following:PWBPA T1;T2ð Þ ¼PNi¼0max0<j?MSim ti; tj þPMj¼0max0<i?NSim tj; ti N þM ;ð10Þwhere T1 and T2 contains N and M diseases,respectively. ti and tj represents ith and jth terms of T1and T2, respectively.Predicting potential association between diseases andmiRNAsFunctional similarity between miRNAs could be calcu-lated based on their related disease sets. Similarities ofeach pair-wise miRNAs are utilized to establish a MFSN.Node of the network represents miRNA. Weight of edgeis the functional similarity score. Then, disease-relatedmiRNAs were prioritized using the network rankingalgorithm named random walk with restart (RWR) [7].The random walker starts on one or several seednodes and then randomly transits to neighboring nodesconsidering the probabilities of the edges between thetwo nodes. And the probability to return to the seednodes is supposed as ?. Then, RWR algorithm can bedefined as following:Ptþ1 ¼ ?P0 þ 1??ð ÞAPt ð11Þwhere P0 denotes the initial probability vector, Pt is avector in which the ith element represents the probabil-ity of finding the walker at node i and step t, A is thecolumn-normalized adjacency matrix of the network.The algorithm was performed until the differencebetween Pt and Pt+1 falling below 10?10, which means allthe nodes become stable.In this study, the known miRNAs of a disease wereconsidered as seed nodes. The unknown miRNAs of itcould be scored based on RWR on the MFSN. Afterranking the miRNAs based on the scores, disease-relatedmiRNAs could be prioritized.ImplementationDisSetSim has been implemented on a JavaEE frameworkand run on the web server (2-core (2.26 GHz) proces-sors) of Ucloud [34]. The four-layer architecture involv-ing DATABASE, ALGORITHM, TOOLS, and VIEWlayer is shown in Fig. 1 The detailed description of thearchitecture is fixed as following.(1) DATABASE layer. This layer stores DO, disease-related genes, and functional associations between genes.These are exploited by ALGORITHM layer for calculat-ing the similarity between disease sets.(2) ALGORITHM layer. Five algorithms of measuringthe similarity between DO terms have been imple-mented, which include Resniks, Lins, Wangs, PSB, andSemFunSim methods. And the method named PWBPAfor calculating the SSDS were also implemented.(3) TOOL layer. Two tools including PairSim andBatchSim have been provided for exploring the SSDS.PairSim calculates the similarity for a given pair ofThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 21 of 79disease sets, and BatchSim computes similarity betweeneach pair of multiple disease sets.(4) VIEW layer. Web pages are provided for viewing theresults. It shows the similarity of pair-wise disease sets.ResultsWeb interfaceDisSetSim provides two tools PairSim and BatchSim forquerying the SSDS. The details about the usage of thesetwo tools are described as follows.The usage of PairSimFigure 2a shows a case for searching the similarity scoreof a given pair of disease sets. The web page for input-ting disease sets is http://www.bio-annotation.cn:8080/DisSetSim/ basic-init. Each of these disease sets could beinputted in a textbox. A disease set is comprised byseveral diseases. And each disease is represented by theidentifier of term in DO. All the term identifiers couldbe downloaded from the hyperlink disease terms in theinputting page. Here, we click the example button touse our example. Then, we choose one of the fivemethods (Resniks, Lins, Wangs, PSB, and SemFunSim)for calculating the SSD. After submitting this pair ofdisease sets, the system could return the similarity scorebased on the PWBPA method.The usage of BatchSimFigure 2b shows a case for searching the similarity scoreof all the pairs based on the selected files. The web pagefor inputting disease sets is http://www.bio-annota-tion.cn:8080/DisSetSim/batch-init. Two files includingdisease sets should be selected before submitting. Thefile should be a plain text which contains several diseasesets. Each disease set must be in a newline, and each dis-ease set contains several disease IDs which are separatedby commas. The size of uploaded file must be <2 Mb.Here, we selected our example file in this page. Then,we choose one of the five methods for calculating theSSD. After clicking the submit button, the system couldreturn the similarity score of all the pairs of the selectedfiles based on the PWBPA method.miRNA functional similarity networkBy applying DisSetSim to the inputted disease sets ofmiRNAs, the similarity score of each miRNA pair couldbe obtained. Using miRNA as node and similar miRNAsas edge, the MFSN was constructed based on varioussimilarity cutoffs. As shown in Fig. 3a, the number of linksdramatically decreases when the cutoff increasesfrom low value to high value. When the cutoff isequal to or bigger than 0.7, the link numbers remainrelatively stable. Therefore, we use 0.7 as cutoff forthe MFSN. In total, 1042 miRNA-miRNA functionalassociations between 346 miRNAs were obtained asMFSN (Fig. 3c). Similar to the most of the reportedbiological networks, the degree of this MFSN also shows ascale-free distribution [5, 9, 3537]. It means that most ofthe miRNAs only have a few functionally similar miRNAs,and a few of miRNAs have a numerous functional similarmiRNA (Fig. 3b).Here, the PWBPA method was utilized for calculat-ing similarity between disease sets, and SemFunSimwas used as computing the similarity of pair-wise dis-eases. This is because that the SemFunSim methodwas proven to obtain the best performance [14]. Alterna-tively, other state-of-art methods could also be chosen toconstruct MFSN.ResnikALGORITHMDATABASETOOLDisease Ontology Disease-related genesVIEWLin Wang PSB SemFunSimPWBPAFunctional interaction between genesPairSim BatchSimFig. 1 System overview of DisSetSimThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 22 of 79Disease-related miRNAsBy applying the above similarity scores of miRNAs,novel disease-related miRNAs were predicted based onRWR algorithm (See Methods section). To evaluatethe performance of the similarity scores of miRNAs,leave-one-out cross validation of 5710 known experi-mentally confirmed miRNA-disease associations, in-cluding 265 diseases with at least two miRNAs, wereused for this assessment. For a disease of interest,each known miRNA of this disease was left out asthe testing case, and the remaining miRNAs of thisdisease were used as seed nodes. All the miRNAsexcept the miRNAs of this disease were considered ascandidate miRNAs. We then examined how well thetesting miRNA ranked relative to the candidate miR-NAs. If the ranking of this testing miRNA exceeded agiven cutoff, we regarded this miRNA-disease associ-ation as successfully predicted. As a result, an areaunder the ROC curve (AUC) of 0.9296 was achieved(Fig. 4), which demonstrated that our miRNA func-tional similarity was effective in recovering knownexperimentally confirmed disease-related miRNAs.DiscussionAs the best of our knowledge, non-coding RNAs(ncRNAs) attract more and more attentions becauseof their important regulation roles in molecular level.However, the lack of protein limits the identificationof their function. Here the application of our tool inconstructing MFSN and predicting miRNA-diseaseassociations provides a novel way to help for explor-ing the function of miRNAs especially for prioritizingABFig. 2 Schematic workflow of DisSetSim. a Schematic workflow of PairSim. b Schematic workflow of BatchSimThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 23 of 79miRNA-disease associations. This application can beextended to other ncRNAs, such as lncRNAs andcircRNAs. Although methods for calculating the SDShave been implemented by previous methods, it isnot easy to calculate the SSDS. Therefore, DisSetSimbenefits researchers for exploring the function ofdisease-related molecular.ConclusionsIn this article, we designed and developed a web systemDisSetSim to calculate the SSDS. Five state-of-artmethods were implemented (see METHODS section)for calculating disease similarity. And the PWBPAmethod was implemented for calculating the SSDS. Twotools involving PairSim and BatchSim provide thefunction to obtain the SSDS by inputting a pair-wisedisease sets and multiple disease sets, respectively.The functional similarity of miRNAs could be calcu-lated based on our system. Here, the similarity of eachpair-wise miRNAs was calculated. And then a MFSNwas constructed based on miRNA similarity. Thenetwork was further utilized to predicate disease-relatedmiRNAs based on RWR. The high AUC (0.9296) showsthe MFSN is very suitable for predicting potential rela-tionships between diseases and miRNAs.AbbreviationsCTD: Comparative Toxicogenomics Database; DO: Disease Ontology; GAD: GeneticAssociation Database; GeneRIF: Gene Reference into Function; GO: Gene Ontology;GOA: GO annotation; IC: information content; MFSN: miRNA functional similaritynetwork; MICA: the most informative common ancestor; ncRNA: non-coding RNAs;OMIM: Online Mendelian Inheritance in Man; PSB: process-similarity based;PWAPM: pair-wise-all pairs-maximum; PWBPA: pair-wise-best pairs-average;PWR: random walk with restart; SD: pair-wise diseases; SSD: the similarity score ofpair-wise diseases; SSDS: the similarity score of pair-wise disease sets.AcknowledgmentsYadong Wang and Liang Cheng are the corresponding authors. LinglingZhao and Zhiyan Liu are the co-first authors.FundingThis work was supported by the Major State Research Development Programof China [No. 2016YFC1202302], the National Natural Science Foundation ofChina (Grant No. 61502125, $2000), Heilongjiang Postdoctoral Fund (GrantNo. LBH-Z15179, $800), and China Postdoctoral Science Foundation (GrantNo. 2016 M590291, $1000).Availability of data and materialsThe system can be accessed from http://www.bio-annotation.cn:8080/DisSetSim/.Fig. 3 Construction and characteristics of the miRNA functional similarity network. a Cumulative distribution of the edges between miRNAs whenusing various similarity cutoffs. b Degree distribution for miRNA in the miRNA functional similarity network. c The miRNA functional similarity networkFig. 4 ROC curve of the PWBPA method based on leave-one-out crossvalidation on known experimentally verified miRNA-disease associationsThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 24 of 79About this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsYH, LZ and ZL implemented the first version of the online system. HJ, HS, PXupdated the system. YW and LC wrote the manuscript. All authors read andapproved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Harbin Institute of Technology, School of Life Science and Technology,Harbin 150001, Peoples Republic of China. 2Department of ComputerScience and Technology, Harbin Institute of Technology, Harbin 150001,Peoples Republic of China. 3Department of information engineering,Heilongjiang Biological Science and Technology Career Academy, Harbin150001, Peoples Republic of China. 4College of Bioinformatics Science andTechnology, Harbin Medical University, Harbin 150001, Peoples Republic ofChina.Published: 20 September 2017Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 DOI 10.1186/s13326-017-0163-8RESEARCH Open AccessIdentifying genotype-phenotyperelationships in biomedical textMaryam Khordad* and Robert E. MercerAbstractBackground: One important type of information contained in biomedical research literature is the newly discoveredrelationships between phenotypes and genotypes. Because of the large quantity of literature, a reliable automaticsystem to identify this information for future curation is essential. Such a system provides important and up to datedata for database construction and updating, and even text summarization. In this paper we present a machinelearning method to identify these genotype-phenotype relationships. No large human-annotated corpus ofgenotype-phenotype relationships currently exists. So, a semi-automatic approach has been used to annotate a smalllabelled training set and a self-training method is proposed to annotate more sentences and enlarge the training set.Results: The resulting machine-learned model was evaluated using a separate test set annotated by an expert. Theresults show that using only the small training set in a supervised learning method achieves good results (precision:76.47, recall: 77.61, F-measure: 77.03) which are improved by applying a self-training method (precision: 77.70, recall:77.84, F-measure: 77.77).Conclusions: Relationships between genotypes and phenotypes is biomedical information pivotal to theunderstanding of a patients situation. Our proposed method is the first attempt to make a specialized system toidentify genotype-phenotype relationships in biomedical literature. We achieve good results using a small training set.To improve the results other linguistic contexts need to be explored and an appropriately enlarged training set isrequired.Keywords: Genotypes, Phenotypes, Genotype-phenotype relationship, Semi-automatic corpus annotation,Self-training, Computational linguisticsBackgroundMany research experiments are being performed to dis-cover the role of DNA sequence variants in human healthand disease and the results of these experiments are pub-lished in the biomedical literature. An important categoryof information contained in this literature is the newlydiscovered relationships between phenotypes and geno-types. Experts want to know whether a disease is causedby a genotype or whether a certain genotype determinesparticular human characteristics. This information is veryvaluable for researchers, clinicians, and patients. Thereexist some manually curated resources such as OMIM [1]which are repositories for this information, but they donot provide complete coverage of all genotype-phenotype*Correspondence: mkhordad@alumni.uwo.caDepartment of Computer Science, University of Western Ontario, 1151Richmond Street, N6A 5B7 London, Canadarelationships. Because of the large quantity of literaturepossessing this information, a reliable automatic systemto identify these relationships for future curation is desir-able. Such a system provides important and up to date datafor database and ontology construction and updating, andeven for text summarization.Related workIdentifying relationships between biomedical entities byanalyzing only biomedical textFinding the relationships between entities from infor-mation contained in the biomedical literature has beenstudied extensively and many different methods toaccomplish these tasks have been proposed. Generally,current approaches can be divided into three types:Computational linguistics-based (e.g., [24]), rule-based(e.g., [5, 6]), and machine learning and statistical methods© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 2 of 16(e.g., [7, 8]). Furthermore some systems (e.g., [911]) havecombined these approaches and have proposed hybridmethods.RelEx [10] makes dependency parse trees from the textand applies a small number of simple rules to these treesto extract protein-protein interactions. Leroy et al. [12]develop a shallow parser to extract relations between enti-ties from abstracts. The type of these entities has not beenrestricted. They start from a syntactic perspective andextract relations between all noun phrases regardless oftheir type. SemGen [9] identifies and extracts causal inter-action of genes and diseases from MEDLINE citations.Texts are parsed using MetaMap. The semantic type ofeach noun phrase tagged by MetaMap is the basis of thismethod. Twenty verbs (and their nominalizations) plustwo prepositions, in and for, are recognized as indicatorsof a relation between a genetic phenomenon and a disor-der. Sekimizu et al. [2] use a shallow parser to find nounphrases in the text. The most frequently seen verbs in thecollection of abstracts are believed to express the relationsbetween genes and gene products. Based on these nounphrases and frequently seen verbs, the subject and objectof the interaction are recognized.Coulet et al. [4] propose a method to capture phar-macogenomics (PGx) relationships and build a semanticnetwork based on relations. They use lexicons of PGxkey entities (drugs, genes, and phenotypes) from Phar-mGKB [13] to find sentences mentioning pairs of keyentities. Using the Stanford parser [14] these sentencesare parsed and their dependency graphs1 are produced.According to the dependency graphs and two patterns,the subject, object, and the relationship between themare extracted. This research is probably the closest tothe work presented here, the differences being that themethod to find relationships is rule-based and the enti-ties of interest include drugs. Direct comparison with ourresults is difficult because the genotype-phenotype rela-tionships with their associated precision and recall valuesare not presented separately. Temkin and Gilder [3] usea lexical analyzer and a context free grammar to makean efficient parser to capture interactions between pro-teins, genes, and small molecules. Yakushiji et al. [15]propose a method based on full parsing with a large-scale,general-purpose grammar.The BioNLP module [5] is a rule-based module whichfinds protein names in text and extracts protein-proteininteractions using pattern matching. Huang et al. [6] pro-pose a method based on dynamic programming [16] todiscover patterns to extract protein interactions. Katrenkoand Adriaans [8] propose a representation based ondependency trees which takes into account the syntac-tic information and allows for using different machinelearning methods. Craven [7] describes two learningmethods (Naïve Bayes and relational learning) to findthe relations between proteins and sub-cellular struc-tures in which they are found. The Naïve Bayes methodis based on statistics of the co-occurrence of words.To apply the relational learning algorithm, text is firstparsed using a shallow parser. Marcotte et al. [17]describe a Bayesian approach to classify articles basedon 80 discriminating words, and to sort them accord-ing to their relevance to protein-protein interactions.Bui et al. [11] propose a hybrid method for extractingprotein-protein interactions. This method uses a set ofrules to filter out some PPI pairs. Then the remainingpairs go through a SVM classifier. Stephens et al. [18],Stapley and Benoit [19], and Jenssen et al. [20] discussextracting the relation between pairs of proteins usingprobability scores.Supervised learning approaches have been used to rec-ognize concepts of prevention, disease, and cure andrelations among these concepts. Work using a standard-ized annotated corpus beginning with Rosario and Hearst[21] and continuing with the work of Frunza and Inkpen[22, 23] and Abacha and Zweigenbaum [24, 25] has seengood performance progress.An approach to extract binary relationships betweenfood, disease, and gene named entities by Yang et al. [26]has similarities to the work presented here because it isverb-centric.Most of the biomedical relation extraction systemsfocus on finding relations between specific types of namedentities. Open Information Extraction (OIE) systems aimto extract all the relationships between different types ofnamed entities. TextRunner [27], ReVerb [28], and OLLIE[29] are examples of OIE systems. They first identifyphrases containing relations using part-of-speech patternsand syntactic and lexical constraints, and then with someheuristics detect related named entities and relation verbs.PASMED [30] extracts diverse types of binary relationsfrom biomedical literature using deep syntactic patterns.Advanced OIE systems [31, 32] have been proposed toextract nominal and n-ary relations.Increasing interest in neural network models, such asdeep [33], recurrent [34], and convolutional [35] net-works, and their applications to Natural Language Pro-cessing, such as word embeddings [36] have provideda new set of techniques for relationship identification,some which deal with relationships of a general nature,such as Miwa and Bansal [37], and some which dealwith biomedical relationships, such as Jiang et al. [38].Our method is a more traditional pipeline methodidentifying genotypes and phenotypes, and then usingsurface, syntactic, and dependency features to identifythe relationships. So, rather than developing an exten-sive overview of these neural network models, we insteadpoint the reader to Liu et al.s excellent summary of thesemethods [39].Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 3 of 16Identifying genotype-phenotype relationships usingbiomedical text and/or other curated resourcesThe research works mentioned in the previous sectionhave been highlighted because they are concerned withidentifying various relations among biomedical entities byanalyzing only the natural language context in whichmen-tions of these relations and entities are immersed. There isa vast literature presenting research focussed specificallyon the genotype-phenotype relation. Most of this researchpresents the discovery of novel genotype-phenotype rela-tions based on biomedical evidence and is beyond theintent of this paper and would be out of place to be sur-veyed here. Incidentally, it is this type of literature that weare interested in mining to extract genotype-phenotyperelationships.While not finding genotype-phenotype relationships,many research works are concerned with a relatedquestion: disease-gene relationships. One of the earli-est works in this area is that of Doughty et al. [40]which provides an automated method to find cancer-and other disease-related point mutations. The method ofSinghal et al. [41] to find disease-gene-variant tripletsin the biomedical literature makes strong use of a num-ber of modern natural language tools to analyze the textin which these triplets reside, but this method also usesinformation mined from all of the PubMed abstracts, theWeb, and sequence analysis which requires the use of amanually curated database. Another research work thatinvestigates gene variants and disease relationships is thatof Verspoor et al. [42]. Another work that investigatesmutation-disease associations is Mahmood et al. [43].A recent review of algorithms identifying gene-diseaseassociations using techniques based on genome variation,networks, text mining, and crowdsourcing is provided byOpap and Mulder [44].Other literature reports on techniques to extractgenotype-phenotype relationships combining biomedicaltext mining with a variety of other resources. An exam-ple of this type of technique is the pioneering work ofKorbel et al. [45]. Being the first to use evidence frombiomedical literature, it uses the correlation of gene andphenotype mentions in the text together with compar-ative genome analysis that depends on a database oforthologous groups of genes to provide gene-phenotyperelationship candidates. Novel relationships that were notmined directly from the text are reported. Another typeof technique, exemplified by the work of Goh et al. [46]is the integration of curated databases to find genotype-phenotype relationship candidates.A work by Bokharaeian et al. [47] which is very closeto the research presented here uses two types of Sup-port Vector Machines for their learning method and thetype of relationship being identified is between single-nucleotide polymorphisms (SNPs) and phenotypes. Thiswork presents three types of association (positive, nega-tive, and neutral) and three levels of confidence (weak,moderate, and strong).In each of the referred to works, either the presentationof the genotype-phenotype relationship is complicated bybeing part of a larger relationship, such as in the workof Coulet et al. [4], or the method to suggest the rela-tionship requires information found in manually curateddatabases, such as the works of Korbel et al. [45], Gohet al. [46], and Singhal et al. [41]. Our work then stands outby being different on each of these fronts: we identify onlythe genotype-phenotype relationships and we use only thetext in the PubMed abstract being analyzed. Also, we arenot attempting to find new relationships, rather we areonly mining those relationships that occur in the abstract.In addition, we are using a machine learning method thatrequires human annotated data. We view the method pro-vided in this paper as complementing these othermethodsin the ways just described.Briefly then, in this paper we discuss a semi-supervisedlearningmethod for identifying genotype-phenotype rela-tionships from biomedical literature.We start with a semi-automatic method for creating a small seed set of labelleddata by applying two named entity relationship tools [48]to an unlabelled genotype-phenotype relationship dataset.This initially labelled genotype-phenotype relationshipdataset is thenmanually cleaned. Then using this as a seedin a self-training framework, a machine learned model istrained. It is worth noting that throughout this paper wedo not take into account the phenotypes at the subcellularlevel. The evaluation results are reported using precision,recall and F-measure derived from a human-annotatedtest set. Precision (or positive predictive value) is the ratioof correct relationships in all relationships found and canbe seen as a measure of soundness. Recall (or sensitiv-ity) is the ratio of correct relationships found comparedto all correct relationships in the corpus and can be usedas a measure of completeness. F-measure combines pre-cision and recall as the harmonic mean of these twonumbers.Semi-supervised learningTo train machine learning systems, it is easier and cheaperto obtain unlabelled data than labelled data. Semi-supervised learning is a bootstrapping method whichincorporates a large amount of unlabelled data to improvethe performance of supervised learning methods whichlack sufficient labelled data.Much of the semi-supervised learning in ComputationalLinguistics uses the iterative bootstrapping approach, ini-tially proposed by Riloff and Shepherd [49] for buildingsemantic lexicons, which later evolved into the learningof multiple categories [50]. These methods have furthertransformed to the semi-supervised learning of multipleKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 4 of 16related categories and relations as a method to enhancethe learning process [51].Instead of using this category of semi-supervised learn-ing, we use a methodology called self-training. Ng andCardie [52] proposed this type of semi-supervised learn-ing to combat semantic drift [53, 54], a problem withthe bootstrapped learning of multiple categories. Theyused bagging and majority voting in their implementa-tion. A set of classifiers get trained on the labelled data,then they classify the unlabelled data independently. Onlythose predictions which have the same label by all clas-sifiers are added to the training set and the classifiersare trained again. This process continues until a stopcondition is met. For Clark et al. [55] a model is sim-ply retrained at each iteration on its labelled data whichis augmented with unlabelled data that is classified withthe previous iterations model. According to this sec-ond method, there is only one classifier which is trainedon labelled data. Then the resulting model is used toclassify the unlabelled data. The most confident predic-tions are added to the training set and the classifier isretrained on this new training set. This procedure repeatsfor several rounds. We adopt this latter methodology inour work.Rule-based andmachine learning-based named entityrelationship identification toolsIbn Faiz [48] proposed a general-purpose softwaretool for mining relationships between named entitiesdesigned to be used in both a rule-based and a machinelearning-based configuration. This tool was originallytailored to recognize pairs of interacting proteins andhas been reconfigured here for the purpose of iden-tifying genotype-phenotype relationships. Ibn Faiz [48]extended the rule-based method of RelEx [10] for iden-tifying protein-protein interactions. In this method thedependency tree of each sentence is traversed accordingto some rules and various candidate dependency paths areextracted.This extended method is able to detect the more generaltypes of relationships found between named entities inbiomedical text. For example the rule-based system is ableto find relationships with the following linguistic patterns,where PREP is any preposition, REL is any relationshipterm, and N is any noun: ENTITY1 REL ENTITY2; e.g., GENOTYPE causesPHENOTYPE Relations in which the entities are connected by oneor more prepositions: ENTITY1 REL (of | by | to | on | for | in |through | with) ENTITY2; e.g., PHENOTYPE isassociated with GENOTYPE (PREP | REL | N)+ (PREP)(REL | PREP | N)*ENTITY1 (REL | N | PREP)+ ENTITY2; e.g.,expression of PHENOTYPE by GENOTYPE REL (of | by | to | on | for | in | through | with |between) ENTITY1 and ENTITY2, e.g.,correlation between GENOTYPE andPHENOTYPE. ENTITY1 (/ | \ | ?) ENTITY2; e.g.,GENOTYPE/PHENOTYPE correlation.In addition to the linguistic patterns this method requiresa good set of relationship terms. To find protein-proteininteraction relationships, a list of interaction terms (acombination of lists from RelEx [10] and Bui et al. [11])was used by Ibn Faiz to elicit protein-protein interactions.In the work reported below an appropriate set of relation-ship terms for genotype-phenotype relationships has beendeveloped and used in the rule-based system to recognizethis type of relationship.Ibn Faiz [48] also used his general-purpose tool in amachine learning approach using a maximum entropyclassifier and a set of relationship terms appropriate foridentifying protein-protein interactions. This approachconsiders the relationship identification problem as abinary classification task. The Stanford dependencyparser produces a dependency tree for each sentence. Foreach pair of named entities in a sentence, proteins inthis case, the dependency path between them, the parsetree of the sentence, and other features are extracted.These features include: dependency features coming fromthe dependency representation of each sentence, syntacticfeatures, and surface features derived directly from the rawtext (the relationship terms and their relative position).The extracted features along with the existence of a rela-tionship between named entity pairs in a sentence make afeature vector. A machine learning model is trained basedon the positive (a relationship exists) and negative (a rela-tionship does not exist) examples. To avoid sparsity andoverfitting problems, feature selection is used. Becausethe maximum entropy classifier and the linguistic depen-dency and syntactic features are the common foundationfor this technique, only an appropriate set of relationshipterms need to be provided for genotype-phenotype rela-tionship identification. In the work reported below, thesame set of relationship terms as used in the rule-basedapproach are used in the machine-learning approach.MethodsA block diagram showing the complete workflow is pro-vided in Fig. 1. Details of this workflow are presented inthe following.Curating the dataAs mentioned before we did not have access to anydata prepared specifically for the genotype-phenotypeKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 5 of 16Fig. 1Workflowrelationship identification task, so our first task was tocollect a sufficient number of sentences containing phe-notype and genotype names that include both genotype-phenotype relationships and non-relationships. Threesources of data have been used in this project: Khordad et al. [56] generated a corpus for thephenotype name recognition task. This corpus iscomprised of 2971 sentences from 113 full papers. Itis designated as the MKH corpus henceforth. PubMed was queried for genotype and phenotypeand correlation and 5160 abstracts were collected. Collier et al. [57] generated and made available to usthe Phenominer corpus which contains 112 PubMedabstracts. Both phenotypes and genotypes areannotated in this corpus, but not their relationships.The annotation was carried out with the sameexperienced biomedical annotator who accomplishedthe GENIA corpus [58] tagging. Phenominercontains 1976 sentences with 1611 genotypes and 472phenotype candidates. However, there are two issueswith this corpus: The phenotypes at the cellular level arelabelled in the Phenominer corpus. Our workon genotype-phenotype relationships does notconsider this type of phenotype because thelinguistic context is different fromrelationships involving the non-cellular levelphenotypes.In all of the steps explained below, this type ofphenotype is included. We report precision,recall, and F-measure with and without thistype of phenotype involved ingenotype-phenotype relationships labelled inthe test set. Generic expressions (e.g., gene, protein,expression) referring to a genotype or aphenotype earlier in the text are tagged in thiscorpus as genotypes and phenotypes. Forexample locus is tagged as a genotype in thefollowing sentence: Our original associationstudy focused on the role of IBD5 in CD; wenext explored the potential contribution ofthis locus to UC susceptibility in 187German trios.The work reported here only considersexplicitly named genotypes and phenotypes.Thus, including these examples will have aslightly negative effect on the trained modeland any relationships that include entities thatare named implicitly will not be identified inthe test set, reducing the precision and recallslightly.Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 6 of 16Genotype and phenotype names were already anno-tated in the third resource and phenotypes were alreadyannotated in the first resource. So, we had to annotategenotypes in the first resource and genotypes and pheno-types in the second resource. BANNER [59], a biomedicalNER system, has been used to annotate the genotypenames and an NER system specialized in phenotype namerecognition [56] has been used to annotate the phenotypenames. Only sentences with both phenotype and genotypenames have been selected from the above resources tocomprise our data and the remaining sentences have beenignored. In this way, we have collected 460 sentences fromthe MKH corpus, 3590 sentences from the PubMed col-lection and 207 sentences from Phenominer. These 4257sentences comprise our initial set of sentences. All thesentences are represented by the IOB label model (Inside,Outside, Beginning). The phenotype names and genotypenames are tagged by their token offset from the beginningof each sentence because they can occur multiple times ina sentence.Training setAt the beginning of the project we did not have anylabelled data. Instead of using annotators knowledge-able in biomedicine to label a sufficiently large corpus ofbiomedical literature, we decided instead to use the previ-ously described relationship identification tools modifiedto work with our data and use their agreed upon out-puts, cleaned by a non-expert, as our labelled training set.This methodology has allowed us to partially evaluate thismethod of semi-automatic annotation.As mentioned previously, the rule-based and machinelearning-based systems for identifying biomedical rela-tionships have been appropriately tailored to this task bysupplying a set of genotype-phenotype relationship wordsthat are appropriate for identifying this type of biomed-ical relationship. This set of relationship words includesa list of 20 verbs and two prepositions (in and for) fromRindflesch et al. [9] which encode a relationship betweena genetic phenomenon and a disorder and the PPI rela-tionship terms from Ibn Faizs work [48] which we foundto apply also to genotype-phenotype relationships.2Our initial corpus is separately processed by therule-based and the machine learning-based relationshipidentification tools. Each of these tools find some rela-tionships in the input sentences. After the results arecompared, those sentences that contain at least one agreedupon relationship3 are initially considered as the train-ing set. From the original corpus, 519 sentences com-prised the initial training set as the result of this process.However, as these tools have been developed as generalnamed entity relationship identifiers, we could not becertain that even their similar results produce correctlylabelled examples. Therefore, the initial training set wasfurther processed manually. Some interesting issues wereobserved.1. Some sentences do not state any relationshipbetween the annotated phenotypes and genotypes.Instead, these sentences only explain the aim of aresearch project. However, these sentences arelabelled as containing a relationship by both tools;e.g., The present study was undertaken toinvestigate whether rare variants of TNFAIP3 andTREX1 are also associated with systemic sclerosis.2. The negative relationships stated with the word noare considered positive by both tools; e.g., With thegenotype/phenotype analysis , no correlation inpatients with ulcerative colitis with the MDR1 genewas found.3. Some sentences from the Phenominer corpus aresubstantially different compared to other sentences,because of the two issues we discussed earlier aboutthis corpus. The phenotypes below the cellular levelhave different relationships with genotypes. Forexample, they can change genotypes while thesupercellular-level phenotypes are affected bygenotypes and are not capable of causing any changeto them.4. Some cases have both tools making the samemistakes: suggesting incorrect relationships (i.e.,negative instances are suggested as positiveinstances) or missing relationships (i.e., positiveinstances are given as negative instances).After making corrections (see issues 2 and 4) and delet-ing sentences exhibiting issues 1 and 3, 430 sentencesremained in the training set. These corrections and dele-tions were made by the first author. To increase the train-ing set size, 39 additional sentences have been labelledmanually and have been added to the training set. Thedata set is skewed: there are few negative instances. Toaddress this imbalance, 40 sentences without any relation-ships have been selected manually and have been added tothe training set. As shown in Table 3, the final training sethas 509 sentences. There are 576 positive instances and269 negative instances.Test setTo ensure that the training set and the test set are inde-pendent, the test set is chosen from the initial set withthe training set sentences removed. To select the sen-tences to be included in the test set, the results fromprocessing our initial set with the two general purposerelationship identification tools have been used. In somecases both tools identify relationships from the same sen-tence but the relationships differ. For example in sentenceCommon esr1 gene alleles-4 are unlikely to contributeto obesity-10 in women, whereas a minor importance ofKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 7 of 16esr2-19 on obesity-21 cannot be excluded. the machinelearning-based tool finds a relationship between esr2-19and obesity-21 but the rule-based tool claims that there isalso a relationship between esr1 gene alleles-4 and obesity-10. Since we were confident that this type of sentencewould provide a rich set of positive and negative instances,this type of sentence is extracted to make our initial testset of 298 sentences.In order for the test set to provide a reasonable evalua-tion of the trained model, the sentences must be correctlylabelled. A biochemistry graduate student was hired toannotate the initial test set. Pairs of genotypes and pheno-types are extracted from each sentence and her task was toindicate whether there is any relationship between them.Issues 1 and 3 discussed in the previous section havebeen observed by the annotator in some of the sentences.Also, there are some cases where she is not sure if thereis a relationship or not. Furthermore, she disagreed withthe phenotypes and genotypes annotated in 54 sentences.After deleting these 54 problematic sentences the final testset comprises 244 sentences (which contain 536 positiveinstances and 287 negative instances). See Table 3.Unlabelled dataAfter choosing the training and testing sentences from theinitial set of sentences, the remaining sentences have beenused as unlabelled data. The unlabelled set contains 3440sentences. A subset of these (408 sentences containing 823instances which approximates the number found in theoriginal training set) are used in the self-training step4.Training a model with the machine learning methodNow that we have a labelled training set, it is pos-sible to train a model using a supervised machinelearning method to be evaluated on the test set. Wehave applied the maximum entropy classifier devel-oped for relationship identification (described above) [48]for our genotype-phenotype relationship identificationapplication. A genotype-phenotype pair is represented bya set of features derived from a sentence. Tables 1 and 2provide the list of features.Dependency parse trees can contain important infor-mation in the dependency path between two named enti-ties. Figure 2 shows the dependency tree produced bythe Stanford dependency parser5 for the sentence Theassociation of Genotype1 with Phenotype2 is confirmed..The dependency path between the phenotype and thegenotype is Genotype1-prep_of -association-prep_with-Phenotype2. Association is the relationship term in thispath and prep_of and prep_with are the dependency rela-tionships related to it. The presence of a relationship termcan be a signal for the existence of a relationship andits grammatical role along with its relative position givesvaluable information about the entities involved in therelationship. Sometimes two entities are surrounded bymore than one relationship term. Key term is introducedto find the relationship term which best describes theinteraction. Ibn Faiz [48] used the following steps to findthe key term: when one step fails the process continues tothe next step, but if the key term is found in one step thefollowing steps are ignored.Table 1 List of dependency featuresFeatures DescriptionRelationship term Root of the portion of the dependency tree connecting pheno-type and genotypeStemmed relationship term Stemmed by MALLETRelative position of relationship term Whether it is before the first entity, after the second entity orbetween themThe relationship term combinedwith the dependency relation-shipTo consider the grammatical role of the relationship term in thedependency path.The relationship term and its relative positionKey term Described in Ibn Faizs four step method [48]Key term and its relative positionCollapsed version of the dependency path All occurrences of nsubj/nsubjpass are replaced with subj,rcmod/partmod with mod, prep x with x and everything elsewith O, a placeholder to indicate that a dependency has beenignored.Second version of the collapsed dependency path Only the prep_* of dependency relationships are kept.Negative dependency relationship A binary feature that shows whether there is any node inthe path between the entities which dominates a neg depen-dency relationship. This feature is used to catch the negativerelationships.prep_between A binary feature that checks for the existence of two consecu-tive prep_between links in a dependency path.Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 8 of 16Table 2 List of syntactic and surface featuresFeatures DescriptionSyntactic featuresStemmed version of relationship term in the Least Common Ancestor(LCA) node of the two entitiesIf the head6 of the LCA node of the two entities in the syntax tree is arelationship term then this feature takes a stemmed version of the headword as its value, otherwise it takes a NULL value.The label of each of the constituents in the path between the LCA andeach entity combined with its distance from the LCA nodeSurface featuresRelationship terms and their relative positions The relationship terms between two entities or within a short distance (4tokens) from them.1. Any relationship term that occurs between theentities and dominates them both in the dependencyrepresentation is considered to be the key term.2. A word is found that appears between the entities,dominates the two entities, and has a child which is arelationship term. That child is considered to be thekey term.3. Any relationship term that occurs on the left of thefirst entity or on the right of the second entity anddominates them both in the dependencyrepresentation is considered to be the key term.4. A word appears on the left of the first entity or on theright of the second entity, dominates the two entities,and has a child which is a relationship term. Thatchild is considered to be the key term.Self-training algorithmThe first model is trained using the training set and themachine learning method described earlier. To improvethe performance of our model, a self-training processhas been applied. Figure 3 outlines this process. Thisprocess starts with the provided labelled data and unla-belled data. The labelled data is used to train a modelwhich is used to tag the unlabelled data. In most self-training algorithms the instances with the highest confi-dence level are selected to be added to the labelled data.However, as has been observed in some self-training algo-rithms, choosing the most confident unlabelled instancesand adding them to the labelled data can cause overfitting[60]. We encountered a similar overfitting when we addedthemost confident unlabelled instances. So we consideredthe following two measures to select the best unlabelledinstances. The confidence level must be in an interval. It mustbe more than a threshold ? and less than a specifiedvalue ? . The predicted value of the selected instances must bethe same as their predicted value by the rule-basedsystem.In each iteration an at most upper-bounded number ofinstances are selected and added to the labelled data toprevent adding lots of incorrectly labelled data to thetraining set in the first iterations when the model is notpowerful enough to make good predictions.We used relationship identification output from thePPI-tailored rule-based tool as an added level of conser-vatism in the decision to add an unlabelled instance tothe training set. It has only moderate performance ongenotype-phenotype relationship identification. So, usingthis tools advice along with the confidence level meansthat the relationship must be of a more general naturethan just genotype-phenotype relationships. However, atsome point this conservatism holds the system back fromlearning broader types of relationships in the genotype-phenotype category. Therefore this selection factor is usedonly for the first i iterations, and after i iterations the bestFig. 2 Dependency tree related to the sentence The association of Genotype1 with Phenotype2 is confirmedKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 9 of 16Fig. 3 The self training processunlabelled data is chosen based only on the confidencelevel. Again, here, the confidence level must be in aninterval.This proposed self-training algorithm has been triedwith various configurations and each variable in this pro-cess has been given several values. Each resulting modelhas been tried separately with our test set and the best sys-tem is selected based on its performance on the test set. Inour best configuration 15 unlabelled instances are addedto the labelled data in each iteration, in the first 5 itera-tions predictions made by the rule-based system are takeninto account, the least confidence level is 85%, the high-est confidence level is 92% and the process stops after 6iterations.Results and discussionThe proposed machine-learned model has been evalu-ated using the separate test set manually annotated bya biochemistry graduate student. The distribution of ourdata (number of sentences and number of genotype-phenotype pairs in each set) is illustrated in Table 3. Thenumbers of positive instances and negative instances inthe unlabelled data are not available.Table 4 shows the results obtained by the super-vised learning algorithm and the proposed self-trainingalgorithm. The results of testing Ibn Faizs rule-based andTable 3 Distribution of data in our different setsData set Sentences Instances PositiveinstancesNegativeinstancesTraining set 509 845 576 269Test set 244 823 536 287Unlabelled data 408 823 N/A N/Amachine learning-based relationship identification tools[48] originally configured to find protein-protein inter-actions have been included in the table for comparisonpurposes. Although these tools were not configured tobe used for our application, as can be seen in the table,the PPI-configured tools, especially the rule-based system,have good precisions. This performance by the rule-basedsystem led us to consider the rule-based predictions asone factor in choosing which unlabelled data to add to thelabelled data. The recalls of the PPI-configured tools arequite low as one would expect. The precision results meanthat there are some linguistic structures that are commonbetween protein-protein and genotype-phenotype rela-tionships and these structures are useful for distinguishingcorrect from incorrect relationship candidates.The lowrecall values indicate there are some genotype-phenotyperelationship contexts which are specific to this type ofrelationship and the relation terms used to configure thegeneral purpose relationship tools are key to finding theserelationships.As illustrated in Table 4, we get good performance byusing a small initial training set and then we are ableto gain a modest improvement by using our proposedself-training algorithm. The initial results with thesmall training set were: precision: 76.47, recall: 77.61,F-measure: 77.03. The self-training algorithm gave theTable 4 Evaluation resultsMethod Precision Recall F-measureSupervised learning method 76.47 77.61 77.03Self-training method 77.70 77.84 77.77PPI-configured ML-based tool 75.19 53.17 62.29PPI-configured rule-based tool 77.77 38.04 51.09Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 10 of 16following results: precision: 77.70, recall: 77.84, F-measure: 77.77. The self-training step provided onlyslightly more than 10% extra training examples (90 rela-tionship instances added to the original 845 instances), sothe modest performance improvement is not unexpected.The following details will help to better appreciate theseresults. First, we have not attempted to find the bestparameter settings by using the test set to determine thesesettings (this would lead to over-fitting to the test set).Rather, we have experimented with various parameter set-tings to understand how the semi-supervisedmethodmaywork.We are using themodified learnedmodel on the testset only to give precision and recall values to gauge theappropriateness of this technique. Second, instead of hav-ing a separate validation set and choosing the best modelbased on its performance with this set, every learnedmodel (682 models were developed using 22 parametersettings and 1 to 31 iterations of the semi-supervisedtraining step) has been tested with the test set. So, theresults can be interpreted as: if a particular parametersetting and number of iterations of the semi-supervisedalgorithm would have produced the best model basedon its performance on the validation set, this parametersetting and number of iterations of the semi-supervisedalgorithm would give the results based on its performanceon the test set. Rather than reporting the best F-measureover all parameter settings, the data was studied to seecertain trends. In particular, the reported values are forthe best performing model in the semi-supervised iter-ation that happens before a decline in precision that iswitnessed in almost all of the parameter settings. This wedetermined to be the sixth iteration. We chose this trendbecause the semi-supervised method at this point hadprovided the best ratio of true to false positives which weconsidered a worthwhile goal. Although some parametersettings performed better in terms of precision than thesereported results, it was felt that using this (almost) globaltrend in precision as a cutoff point would be a better markof the performance rather than looking solely at a singleparameter setting that might be seen to be over-fitted tothe test set.Graphs of the precision, recall, and F-measure val-ues for each parameter setting for the 31 iterations ofthe semi-supervised learning algorithm are presented inFigs. 4, 5, and 6, respectively. Table 5 highlights the max-imum values for each of these measures. The values foreach of these measures for all 682 parameter settings canbe found in https://github.com/mkhordad/Pheno-Geno-Fig. 4 Precision values on the test set for all 22 parameter settings for 31 semi-supervised learning iterationsKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 11 of 16Fig. 5 Recall values on the test set for all 22 parameter settings for 31 semi-supervised learning iterationsFig. 6 F-measure values on the test set for all 22 parameter settings for 31 semi-supervised learning iterationsKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 12 of 16Table 5 Maximum values for precision, recall, and F-measurePrecision Recall F-MeasureParameter setting Maximum value Iteration Maximum value Iteration Maximum value Iteration0.82 0.92 0.7699 4 0.8138 17 0.7880 190.83 0.92 0.7714 5 0.8287 31 0.7911 310.84 0.92 0.7709 7 0.8250 30 0.7889 260.85 0.92 0.7780 5 0.8268 31 0.7935 170.86 0.92 0.7709 5 0.8156 13 0.7870 120.87 0.92 0.7743 5 0.8063 20 0.7788 200.88 0.92 0.7698 5 0.8231 23 0.7907 230.85 0.93 0.7770 6 0.8268 24 0.7870 120.86 0.93 0.7757 5 0.8324 25 0.7856 250.87 0.93 0.7689 4 0.8287 15 0.7857 190.88 0.93 0.7704 7 0.8343 20 0.7946 170.89 0.93 0.7665 1 0.8399 27 0.7923 300.85 0.94 0.7755 9 0.8250 31 0.7836 140.86 0.94 0.7712 2 0.8250 31 0.7849 190.87 0.94 0.7741 5 0.8436 26 0.7961 260.88 0.94 0.7689 5 0.8194 25 0.7849 130.89 0.94 0.7715 6 0.8156 13 0.7892 130.85 0.95 0.7694 2 0.8156 20 0.7866 110.86 0.95 0.7688 2 0.8287 19 0.7896 150.87 0.95 0.7694 2 0.8268 31 0.7848 110.88 0.95 0.7705 7 0.8231 28 0.7875 140.89 0.95 0.7681 10 0.8212 21 0.7848 13Extraction. There are two general trends in all of theparameter settings that we tried. First, there is a shortincrease in precision followed by a slow decline in thismeasure. Second, a short decline in recall is followedby a general increase in this measure until the point(approximately iteration 15 to 17) when few new instancesare being added to the training set. See Fig. 7 for apresentation of the addition of instances to the train-ing set for each parameter setting. It should be notedthat shortly after iteration 15, few instances are avail-able to be added to the training set. The minimum andmaximum value range proves to be too narrow in someinstances, but eventually all experimental settings lackinstances to add. The precision and recall curves tendto flatten out at about this point. It would be inter-esting to see how an increase in unlabelled instanceswould affect the outcome of the semi-supervisedlearning.Recalling the work of Singhal et al. [41], they investi-gated disease-gene-variant triplets, which is close to thefocus of this paper, and they provided precision, recall,and F-measure values based on the performance of theirsystem on two datasets curated from human-annotatedPubMed articles concerning prostate and breast cancer.The precision, recall, and F-measure results were 0.82,0.77, and 0.794, and 0.742, 0.73, and 0.74, respectively forthe two datasets. Also recalling the work of Bokharaeianet al. [47], they investigated relationships between SNPsand phenotypes. Looking at their reported results that areclosest to what is reported here, they achieve precision upto 69.2, recall up to 68.7, and F-measure up to 71.3. Withthe understanding that the datasets are different and therelationships being identified are closely related but notexactly the same, we can say that the method presentedhere, which is based only on the natural language textsurrounding the genotype-phenotype relationship, com-pares favourably with the results obtained by these othermethods.Looking forward, some improvements to the currentmodel can be suggested. Some of these improvementsare typical of the machine-learning paradigm. First is thebalance of positive and negative examples in the train-ing set. While we tried to add some negative sentencesto our data to make it more balanced, Table 3 shows thatKhordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 13 of 16Fig. 7 Instances added for all 22 parameter settings for 31 semi-supervised learning iterations on the test setour data is still biased: the number of negative instancesis less than the number of positive instances. A morebalanced training set is likely to improve the performanceof the trained model. Second, the quality of the originalset of examples which forms the seed for the self-trainingalgorithm affects the ability of that algorithm to increasethe size of our training set. Because the best results werereached only after 6 iterations, the last training set hasonly 935 instances. Our suggestion is to add more man-ually annotated sentences to the original seed trainingset, so that the first model made by this set makes betterpredictions with a stronger level of confidence.In addition to these methodological improvements, thesimilarity of false positives and false negatives can indi-cate some aspects of the problem to focus on. For instance,our system incorrectly finds relationships in sentenceswhich address the main objective of the research beingdiscussed, i.e., those sentences suggesting the possibilityof a relationship rather than stating a relationship. Findingand ignoring such sentences would improve the results.As mentioned before, certain relationships containedin the Phenominer corpus are undetectable in the testset data because the relationship identification systemdoes not have the appropriate biological and linguisticknowledge to recognize them. Table 6 shows the resultsafter deleting the Phenominer sentences from our testset. The improved results (precision: 80.05, recall: 81.07,F-measure: 80.55) demonstrate the true performance ofthe relationship tool to identify relationships for which itwas constructed to find. Detecting these problematic rela-tionships would require some significant changes to thesystem.First, the current system does not recognize relation-ships that deal with sub-cellular phenotypes. To includethis type of phenotype, biomedical knowledge will needto be enhanced to identify these phenotypes in the text.Our system was built to consider only clinically observ-able phenotypes. Additionally, the linguistic knowledgewill need to be supplemented because the direction ofthis relationship is different. Second, the current sys-tem is not able to extract complicated relations wherea pronoun refers to a phenotype or a genotype in thesame sentence or the previous sentences (anaphora),or where a non-explicit noun phrase is used to referTable 6 Results after deleting Phenominer sentences from thetest setMethod Precision Recall F-measureSupervised learning method 80.20 79.79 80.00Self-training method 80.05 81.07 80.55Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 14 of 16(e.g., the gene), or where a part of or the whole geno-type or phenotype is omitted (ellipsis) in a sentence.For example in the following sentence Serum levels ofanti-gp70 Abs-7 were closely correlated with the pres-ence of renal disease-16, more so than anti-dsDNA Abs-24. only the relationship between anti-gp70 Abs-7 andrenal disease-16 is identified by our system but themore complicated relationship between renal disease-16 and anti-dsDNA Abs-24 is missed. Resolving theseproblems will require a more sophisticated linguisticmodel, the focus of computational linguistics researchgenerally.ConclusionsTo summarize, our contributions in this paper are thefollowing: Reconfiguring a generic relationship identificationmethod to perform genotype-phenotype relationshipidentification. Proposing a semi-automatic method for making asmall training set using two relationshipidentification tools. Developing a self-training algorithm to enlarge thetraining set and improve the genotype-phenotyperelationship identification results. Analysing the results and specifying the types ofsentences and relationships that our system has poorperformance finding and giving some suggestions onhow to improve the results.In conclusion, we have generated a machine-learnedmodel dedicated solely to the identification of genotype-phenotype relationships mentioned in biomedical textusing only the surrounding text. With a test corpus, wehave provided a baseline measure of precision, recall, andF-measure for future comparison. An analysis of the falsenegatives and false positives from this corpus have sug-gested some natural language processing enhancementsthat would decrease the false negative and false positiverates. From a biological perspective, determining the typeof relationship, e.g., does the relationship describe a directexpression of a gene or is the relationship indicative of apathway effect, would be an important aspect of the rela-tionship to mine from the text and is an interesting nextresearch direction to consider.Endnotes1A directed graph representing dependencies of wordsin a sentence.2 Seven verbs from [9] are not found in [48]. The approx-imately 270 relationship words (808 surface forms) canbe found in https://github.com/mkhordad/Pheno-Geno-Extraction. These words have a good overlap with thecurrent relations in the UMLS Semantic Network thatwere used in Sharma et al.s verb-centric approach [61].3Genotype-phenotype pairs that have a relationship arethe positive instances. Genotype-phenotype pairs that donot have a relationship are the negative instances. Thesentences mentioned have both positive and negativeinstances.4 Each self-training iteration requires each sentence tobe evaluated using the current model. Using the full unla-belled set proved to be too computationally expensive forthe experimental setting, so a subset was used instead.5 http://nlp.stanford.edu/software/stanford-dependencies.shtml6Collins head finding rule [62] has been used.AcknowledgementsSupport for this work was provided through a Natural Sciences andEngineering Research Council of Canada (NSERC) Discovery Grant to Robert E.Mercer. Interactions with Nigel Collier were greatly appreciated.FundingSupport for this work was provided through a Natural Sciences andEngineering Research Council of Canada (NSERC) Discovery Grant to Robert E.Mercer. The funding body played no role in the design of the study, nor in thecollection, analysis, and interpretation of data, nor in the writing of themanuscript.Availability of data andmaterialsThe software and the data are available at: https://github.com/mkhordad/Pheno-Geno-Extraction.Authors contributionsMKH carried out the literature survey, developed the approach described inthe paper, conceived the design of the study, performed the statisticalanalysis, and drafted the manuscript. RM participated in the design of thestudy, performed some of the analysis, and helped to draft the manuscript. Allauthors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 12 July 2016 Accepted: 28 October 2017Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 DOI 10.1186/s13326-017-0148-7RESEARCH Open AccessTowards achieving semanticinteroperability of clinical study data with FHIRHugo Leroux* , Alejandro Metke-Jimenez and Michael J. LawleyAbstractBackground: Observational clinical studies play a pivotal role in advancing medical knowledge and patienthealthcare. To lessen the prohibitive costs of conducting these studies and support evidence-based medicine, resultsemanating from these studies need to be shared and compared to one another. Current approaches for clinical studymanagement have limitations that prohibit the effective sharing of clinical research data.Methods: The objective of this paper is to present a proposal for a clinical study architecture to not only facilitate thecommunication of clinical study data but also its context so that the data that is being communicated can beunambiguously understood at the receiving end. Our approach is two-fold. First we outline our methodology to mapclinical data from Clinical Data Interchange Standards Consortium Operational Data Model (ODM) to the FastHealthcare Interoperable Resource (FHIR) and outline the strengths and weaknesses of this approach. Next, wepropose two FHIR-based models, to capture the metadata and data from the clinical study, that not only facilitate thesyntactic but also semantic interoperability of clinical study data.Conclusions: This work shows that our proposed FHIR resources provide a good fit to semantically enrich the ODMdata. By exploiting the rich information model in FHIR, we can organise clinical data in a manner that preserves itsorganisation but captures its context. Our implementations demonstrate that FHIR can natively manage clinical data.Furthermore, by providing links at several levels, it improves the traversal and querying of the data. The intendedbenefits of this approach is more efficient and effective data exchange that ultimately will allow clinicians to switchtheir focus back to decision-making and evidence-based medicines.Keywords: FHIR, CDISC ODM, Interoperability, Clinical research data, Longitudinal clinical studyBackgroundClinical research plays a vital role in advancing medicalknowledge and improving clinical outcome. It is becomingincreasingly clear that results from clinical studies needto be shared and compared to one another in order tosupport efficient evidence-based medicine [1] and reducethe costs of conducting these studies. By the same token,Hsu et al. [2] argue that to fulfil the goals of precisionmedicine requires the mining and aggregation of clinicaldata from multiple sources and entails novel approachesto obtaining contextual observations. Hume et al. [3] statethat: clinical research can no longer be considered an iso-lated venture and is increasingly conducted in network*Correspondence: hugo.leroux@csiro.auThe Australian E-Health Research Centre, CSIRO Health and Biosecurity, Level5, Health Sciences Building 901/16, Royal Brisbane and Womens Hospital,Herston 4029, Queensland, Australiastructures where seamless data exchange is critical to oper-ational efficiency and effectiveness. The challenge whencomparing results from different data sets is to ensure thatwe are comparing corresponding data sets.The Operational Data Model (ODM) [4] is an XML1-based standard from the Clinical Data Interchange Stan-dards Consortium (CDISC) that was originally developedto facilitate the exchange, archival and audit trail require-ments of clinical information but whose use has beenextended to cover cases not initially anticipated [3], suchas integrating health records within clinical research sys-tems. The Federal Drug Administration has mandated theuse of the CDISC standards for the electronic capture andreporting of clinical study data [5]. ODM is particularlywell-suited for a data capture context [3, 611]. It is amature data interchange standard that has proven usefulfor exchanging both document and message formats [3].© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 2 of 14Its strength is in its relative simplicity, adaptabilitythrough the use of extensions [3] and in its ability to sup-port the creation of a broad range of customisable ClinicalReport Forms (CRFs) [3, 12].ODM, however, lacks a rich-enough information modelto capture the innate contextual information of the clinicalstudy data [7, 13]. Its relative simplicity, has impacted onits ability to advance all aspects of interoperability, limit-ing its support for data mapping, data types, terminologyand semantic representation [3]. In spite of its efficacyas a data interchange, ODM has some shortcomings inthe mapping of semantically identical data elements dueto lack of support for semantics associated with the dataelements [3, 11].ODM can be considered to represent syntactic interop-erability (as defined by [14]) of clinical data as it providesa vehicle for clinical data to be shared using an XML-based model. However, our aim is to achieve semanticinteroperability. Semantic interoperability is the ability, forhealth information systems, to exchange information andautomatically interpret the information exchanged mean-ingfully and accurately in order to produce useful resultsas defined by the end users of both systems [14, 15]. Exten-sions to ODM, such as the Clinical Data Acquisition Stan-dards Harmonization (CDASH) [16] and the BiomedicalResearch Integrated Domain Group (BRIDG) [17] providea reference model, although as stated by [18]: studies thatuse CDASH CRFs achieve semantic alignment through ashared data standard, rather than through specific seman-tics. Furthermore, there is no requirement for the CDASHmodel to be used within ODM [7]. Moreover, uptake ofCDASH and BRIDG to provide data semantics has beenlimited [3]. As a result, ODM is ill-suited for advancingthe semantic interoperability solution that is required toachieve cross-study exploration of the clinical studies asthere is the potential for the data to be interpreted in a waythat was not originally intended by the study initiators.The ability to achieve cross-study analysis also neces-sitates clinical studies to adopt a more streamlineddata structure [7]. However, the monolithic natureof the ODM data model favours a one-dimensionaltraversal of the clinical data along its hierarchy ofStudy-Subject-StudyEvent-Form-ItemGroup-Item.More effective exploration and querying of the clinicaldata, especially when dealing with longitudinal studies,requires more direct access to the data, particularly at theStudy Event, Subject and Item levels [6, 7, 19].The Fast Healthcare Interoperable Resources [20](FHIR) framework, a HL72 standard that has been swiftlyadopted by the health-care community [2123], looks thelikely candidate for overcoming this challenge. It is gearedtowards communication of clinical data using HL7 mes-saging protocols but is also supported by a rich informa-tion model to achieve semantic interoperability of clinicaldata. This makes FHIR the natural match to complementthe ODM standard [8] as ODM shares several design prin-ciples, such as making use of extensions for edge casesand human readability, with FHIR [3]. Furthermore, FHIRhas the potential to incorporate existing electronic healthrecord (EHR) data to augment the findings of retrospectiveobservational studies. As intimated by Kubick [24], FHIRcanmake it possible to reach inside of EHRs not just to cap-ture data, but to monitor protocol progress, provide safetyalerts, and allow much greater visibility into trial conductand can lead to dramatic improvements in study efficiencyand drug safety.This research builds upon the approach [8] to integrateclinical data extracted in CDISC ODM format into severalFHIR resources with a view to achieving semantic inter-operability of clinical study data. In the next section, weoutline the approach taken to map the ODM-based dataand metadata onto eight FHIR resources. In particular, weoutline the suitability of the FHIR resources in support-ing the ODM model and on all the assumptions made toreintroduce the contextual information to the data. Wethen critique this approach. Consequently, we proposethe FHIR ClinicalStudyPlan resource to capture theclinical study metadata, including the potential to encodethe study protocol as part of the model. This is followedby a description of the FHIR ClinicalStudyDataresource that describes the clinical study data. Finally, itleads into a discussion on the design principles of thetwo proposed FHIR resources and on their suitability forrepresenting clinical study data.Integrating ODMwith FHIRThis section outlines the approach described in [8] tointegrate the ODM data model to a selection of eightFHIR3 resources to capture both the data and meta-data properties of the ODM data model. The CDISCODM data model [4] consists of two main hierarchies:a Clinical Data and a Metadata hierarchy, asdepicted in Fig. 1, that are referenced using the sameobject identifier (OID). These two parallel hier-archies ensure that the clinical study follows a prede-termined structure of subject, event, form, itemgroup and item. Figure 2 outlines the FHIR resourceschosen to model the ODM data. The entities in red (Care-Plan and Questionnaire) denote metadata concepts. Theremaining entities, in blue, model the clinical data at var-ious levels of the ODM hierarchy. Solid lines are used todenote the links between the entities.The approach taken to map the ODM data into FHIRresources is a semi-automatic process. As the ODM datamodel does not natively provide any mechanism to cap-ture the contextual information relating to the study, thedata semantics needs to be re-introduced during this pro-cess. This can only be achieved if the person doing theLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 3 of 14Fig. 1 The ODM data model. Illustrates the logical organisation of the ODMmodel into the data andmetadata hierarchiesmapping has access to all the conceptual informationdefining the study. Figure 3 illustrates how the hierarchicalODMmodel has been mapped to the FHIR resources.StudyA study defines static information about the structureof an individual study. We choose to model the Studycomponent from ODM using the CarePlan resourcebecause we want to model the activities planned for thepatient during the study in the context of the study pro-tocol. CarePlan provides a link to the study coordinatorthrough the participant attribute and study proto-col through the support attribute. Furthermore, theCarePlan resource offers a number of attributes, suchas context, category and description that canprovide additional context to the care plan.SubjectWhile the Subject represents a critical element of thestudy, its role is quite subdued in ODM. In particular,the specification provides no functionality to record thesubjects attributes such as gender or date of birth,recommending that these be modelled as clinical datawithin the forms. The logical mapping for the Sub-ject in FHIR is the Patient resource. Relevant con-textual information, such as the patients gender, dateof birth and care provider, can be encapsulated withinthe resource. The clinical data for each subject is con-tained within a ClinicalImpression resource thatis linked to the Patient resource. The care plan islinked to this resource using the plan attribute. TheClinicalImpression permits very pertinent infor-mation to be associated to the patients data through theFig. 2 The FHIR data model. Depicts themetadata (red) and data (blue) FHIR resources and their links that comprise the data model to transform theclinical data from ODM to FHIR. The CarePlan and Questionnaire resources are used to capture themetadata for the study. A Patientresource is used to represent the study participant while the clinical data for this participant is contained within a ClinicalImpressionresource. The study events are captured within the EpisodeOfCare resource and the Encounter resource represents one atomic event. TheQuestionnaireResponse resource captures the form responses and the Observation resource illustrates those responses that areanalogous to a patients observations. The QuestionnaireResponse resource is linked back to the Questionnaire resourceLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 4 of 14Fig. 3Mapping the ODM data model to the FHIR resources. Illustrates how the CDISC ODMmodel (depicted by unshaded rectangles) is overlaidwith the FHIR resources. The Metadata section, depicted on the right of the model with red rectangles to represent the FHIR resources, is mappedto the CarePlan resource at the Study and Study Event level, and to the Questionnaire resource to represent the form and itscomposition. The Data section is depicted on the left of the model with the FHIR resources depicted as blue rectangles. The Patient resourcerepresents the study participant. The ClinicalImpression resource captures the clinical data for this participant and they are both linked tothe ODMmodel at the Subject Data level. As both the EpisodeOfCare and Encounter resource correspond to study events, they aremapped at the StudyEventData level. The QuestionnaireResponse resource captures the form responses and is linked to the form dataand its composition. Finally, the Observation resource is used to capture those responses that are more analogous to a patients observationsuse of the trigger, investigations and summaryattributes.Study eventA study event comprises a StudyEventDef and aStudyEventData component that are referenced usinga common OID. The StudyEventDef manages the setof forms to be completed at this phase of the studyand represents an activity within the CarePlanresource. StudyEventDef entities define scheduled andunscheduled events and these are defined within thedetail.scheduled attribute of the activity. TheStudyEventData entity contains clinical data collectedduring a subjects visit. We chose the EpisodeOfCareresource for this entity because it provides details aboutthe group of activities and their purpose pertainingdirectly to a patient. A study event may result in manyvisits from a patient. Each individual visit is modelledas an Encounter and is linked to the episode of carethrough the episodeOfCare attribute. The patientattribute links the resource to the study subject whilethe assessor attribute provides a link to the clinicianconducting the clinical assessment.FormA form defines a collection of data items collectedduring the study and termed a case report form. Aform comprises a FormDef and a FormData compo-nent that are referenced using a common OID. Theform is linked to CarePlan through the activity.actionResulting attribute. The FormDef defines theform structure and its questions. The logical mapping offorms in FHIR is the Questionnaire resource. Thisresource contains the typical attributes for questionnaires,such as an identifier, version, publisher and status, butcan also be customised using the extension mechanismin FHIR. The FormData entity contains the clinical dataassociated with the form. The logical mapping for theFormData in FHIR is the QuestionnaireResponseresource. The benefits of using the QuestionnaireRe-sponse resource are that the order of the responses ismaintained and these can be linked and validated againstthe questions asked. Conversely, however, few mecha-nisms exist to standardise the generation of CRFs for clini-cal studies [8, 11]. This limits the reuse of CRFs unchangedacross protocols [11]. Furthermore, the tendency is toorganise data items, relevant to a research protocol, intoindividual CRFs based on considerations other than log-ical grouping [8, 11] but one that befits the data captureprocess [8]. Owing to the strong coupling between theform design and the ODM model, until such a time thatimplementations of ODM allow for a clear demarcationbetween the form design and its display, we advise againstmodelling the CRF per se as a FHIR resource [7].Item groupThe ItemGroupDef and ItemGroupData entitiesconstitute an item group referenced using a commonOID.Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 5 of 14The ItemGroupDef entity defines the optional group-ing of questions on a form. Groups are defined usingthe Questionnaire.group attribute. The FHIR spec-ification stipulates that a group attribute define either aquestion or a group but not both. The ItemGroupDatacontains the clinical data detailing the responses for theitem group. FHIR organises these grouped responseswithin the QuestionnaireResponse.group attribute.Similar to forms, items are often grouped to match thedata collection process and not necessarily because oftheir semantic similarity [8, 11].ItemAt the item level, the ItemDef and ItemData entitiesdefine each question and its subsequent response. TheItemDef entity defines the question asked during thestudy along with defining attributes such as the datatype,data size, measurement unit, permissible range andcode list. The Questionnaire.group.questionattribute is the most appropriate to define the ItemDefentity. The logical mapping for the ItemData entityis the QuestionnaireResponse.group.questionattribute. The response to the question is then con-tained within the question.answer sub-attribute.This model works best in a lifestyle study scenario usingquestionnaires in the traditional question-answer mode.In the case of longitudinal clinical studies where theresponses are analogous to a patients observations duringan episode of care, we believe the ItemData entity to bemore appropriately represented using the Observationresource. Furthermore, as outlined in the FHIR speci-fications, data captured in questionnaires can be diffi-cult to query after the fact. Individual items within aQuestionnaireResponse or an Observation aresubsequently linked back to the Encounter in whichthey occur.DiscussionAn implementation of the mapping between ODMand FHIR is available at http://healthinet.it.csiro.au/net/jbs/odmFhir. We have semantically enriched theoriginal ODM data with relevant domain informationfrom SNOMED CT4 and LOINC5. The implementationdemonstrates that the FHIR resources provide a goodfit to semantically enrich the extracted data from theCDISC ODM. In spite of its shortcomings in providingcontext to the clinical data, the CDISC ODM providesa sound hierarchical framework for capturing the clini-cal data. However, as outlined in [25], a mapping processinvariably leads to the loss of pertinent information. Onthe metadata side, for example, a study is modelled asa CarePlan. The CarePlan resource, however, is notused in its intended manner in that it does not relateto a particular individual. Similarly, despite being chosento capture the clinical data, the ClinicalImpressionresource has no capability to model the study hierarchy.As a result, it relies on several other FHIR resources, suchas EpisodeOfCare and Encounter, which are alsonot used as intended, to describe the hierarchy. As statedby Kubick [24], it is preferable to avoid data transforma-tions, if possible, especially when this involves massagingthe data to fit into different formats, as this opens upthe possibility of introducing errors and reducing the datareliability. Another issue relates to discrepancies betweenthe data types defined within the ODM and FHIR mod-els. In addition to the type, ODM allows the permissiblerange of the resulting data and, in the case of decimalvalues, the length of the permissible value to be defined.The answer attribute within the QuestionnaireResponseresource has no such capability. The Observationresource is the only one to allow such a definition.The main challenge of the mapping process, however,relates to the FHIR specifications. Being an emergingand evolving standard, FHIR is in a great state of flux.As such, FHIR resources are constantly being updatedbetween releases. The implications are that relationshipsdescribed using one version of the FHIR specificationsmay no longer be available in a subsequent version.The Questionnaire and ClinicalImpressionresources are two resources that have undergone severalchanges.Clinical study design using FHIRKubick [24] advocates (i) for the adoption of FHIR for clin-ical research; (ii) for clinical data to be captured directly atthe source; and, (iii) for data transformation to be avoidedwhenever possible. Similarly, Huser et al. [10] argue thatthe adoption of a single format for study protocols andstudy results decreases the development time required toimport studies into the repository or to exchange databetween systems. Besides, the FHIR model has the poten-tial to manage clinical data in its own right [8]. Conse-quently, we propose the introduction of two new FHIRresources to capture the data and metadata from theclinical study. These resources have been integrated ina data model, as illustrated in Fig. 4, which correspondsto the mapping, in FHIR, for a typical research study.The ClinicalStudyPlan resource, outlined in Fig. 5,defines the study and provides an overview of the plannedactivities. The ClinicalStudyData resource, outlinedin Fig. 6, describes the data captured as part of the studyorganised around the events and visits of the patient.ClinicalStudyPlanThe ClinicalStudyPlan resource comprises severalattributes to capture the fundamental concepts within thestudy. Thus the identifier attribute provides a uniqueidentifer for the resource. A title attribute capturesLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 6 of 14Fig. 4 The clinical research data model in FHIR. Illustrates themetadata (red) and data (blue) resources comprising the clinical data model fordescribing and capturing the research study natively in FHIR. The study plan can be described using either the ClinicalStudyPlan orPlanDefinition resource. The latter can be further defined using the ActivityDefinition resource. The Questionnaire resourceprovides the definition for forms within the study plan. A link to the study plan is contained within the ClinicalStudyData resource. TheClinicalStudyData resource encapsulates the clinical data comprising the research study. It facilitates links to the Patient resource, todescribe the study participant. It further describes investigations that can be a QuestionnaireResponse or a series of Observation orImagingManifest resources. The ImagingManifest resource further defines an ImagingStudy resource to describe the imaging studybeing conductedthe title under which the study is publicly known. AnofficialTitle attribute holds the scientific title of thestudy. The date of registration of the study is containedin the registrationDate field and the regulatoryagency effecting the registration is depicted within theauthoringBody field. A mandatory status attributespecifies the current state of the resource. The study spon-sors can be described in the sponsor field, which allowsa Group resource to be defined. The publicContactattribute specifies the contact details of the personresponsible for general enquiries about the study. Aninvestigator attribute discloses the principal investi-gator for the study; a person tasked at initiating the study,developing the study protocol and responding to scien-tific enquiries about the study. A textual description ofthe aims of the study is provided by the descriptionattribute. The actual or forecasted date of first participantenrolment is recorded in the dateFirstEnrolmentand the expected total number of participants enrolled iscaptured in the sampleSize attribute.The desired outcome of the study is captured withinthe goal attribute. Each goal is further divided intothree sub-attributes. The name of the outcome is con-tained within the outcome attribute. A metric attributedescribes the metric or method of measurement used toevaluate the outcome and finally a timepoint attributerecords the timepoints of interest in which to achieve thegoal.We then define the activities that constitute the study.In [8], we outlined how the Questionnaire resourceis insufficient to capture all activities from clinical stud-ies, especially longitudinal ones. By defining all aspects ofactions resulting from clinical studies within activityattributes, we facilitate the definition of both tradi-tional questions and more observational measurements.A scheduled attribute allows the timing of an activ-ity to be defined. We chose an actionResultingattribute to describe the questionnaire developed as partof the activity. We then define a detail attribute to pro-vide a detailed description of sub-activities that will ulti-mately lead to Observation and ImagingManifestresources in FHIR. This attribute thus provides threesub-attributes to document the category, type and ratio-nale for each sub-activity. We also chose to record thePractitioner or Organisation involved in theactivity through the performer attribute and providea reference to the activitys location using a locationattribute. Finally, a note attribute allows any commentsrelating to the clinical study plan to be recorded.We have started engaging with the HL7 FHIR-I6 [26]and RCRIM7 [27] working groups. The FHIR commu-nity, however, intends to release, as part of STU38, aPlanDefinition resource that captures many of thefunctionalities of the ClinicalStudyPlan resource.PlanDefinitionPlanDefinition9 is a resource proposed by the HL7community that is at the ballot phase and that they intendto release as part of STU3 in late 2016. We will only com-ment on the main concepts as this is a draft proposalLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 7 of 14Fig. 5 The ClinicalStudyPlan resource. Describes the elements comprising the ClinicalStudyPlan resource. This resource has beengenerated using the FHIR Build Process [45] based on the FHIR Guide to Designing Resources [46]. The build process buildsthe resource and generates the webpage that describes the resource, as depicted in this Figure. The table structure is defined in the ResourceDefinition page [47], which also provides a definition of the flags; ?! indicates that the element is a modifying element, while  indicatesthat this element is part of the summary set. The activity element allows either the definition of detailed items or a Questionnaire resourceto be specifiedthat is still subject to change at short notice. Unlike theClinicalStudyPlan, this resource has not been designed toaddress the planning of clinical research specifically butit is flexible enough to undertake this role. However, sim-ilar to the ClinicalStudyPlan resource, it containsattributes to represent the plans unique identifier, name,status, purpose and contributor. In addition, it defines theversion as well as attributes that capture the type of plandefined, the clinical usage foreseen for the plan, a naturallanguage description of the plan, dates of publicationand last review, the context of use (coverage) of the planas well as the topics described.Central to this resource is the definition of actions(actionDefinition) to occur as part of the plan.Each action has an identifier, a label, a title, a descrip-tion of the action both in natural language and asCodeable entities and a link to supporting documenta-tion for the action. Each action further defines a conditionfor whether as well as some triggers to specify whenthe action should occur. A description of the activitycomprising this action can be further defined within anActivityDefinition resource.The proposed ActivityDefinition10 resourceprovides a conceptual description of an action thatshould be undertaken. Similar to the PlanDefinitionresource, it is at the ballot phase and is intended to bereleased as part of STU3 in late 2016. It contains similarorganisational attributes as the PlanDefinition resource. Adetailed definition of the activity can be achieved usinga CodeableConcept element. A category attributedefines the type of activity undertaken and a timingattribute specifies when the activity should occur.Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 8 of 14Fig. 6 The ClinicalStudyData resource. Describes the elements comprising the ClinicalStudyData resource. This resource has alsobeen generated using the FHIR Build Process [45] based on the FHIR Guide to Designing Resources [46]. The table structureis defined in the Resource Definition page [47], which also provides a definition of the flags; ?! indicates that the element is a modifyingelement, while  indicates that this element is part of the summary set. The event element describes the events occurring throughout the study.An event can be further divided into visits. Each visit defines an investigation, which can be only one of the following: aQuestionnaireResponse resource or a series of Observation or ImagingManifest resourcesClinicalStudyDataThe ClinicalStudyData resource describes thedata captured during the study. An identifier isdefined to provide a link to the primary identifiersfor the study. For external identifiers, such as a hos-pital patient id, an externalIdentifier attributeis provided. A mandatory patient attribute pro-vides a reference to the patient being assessed. TheClinicalStudyData resource provides a link to eitherthe ClinicalStudyPlan or PlanDefinitionresource, through the plan attribute, to uniquely iden-tify the study that this clinical data instance represents.A status attribute defines the current state of theresource. We also chose to keep a record of past statusesin a statusHistory attribute that captures the paststatuses as well as the time that the event was in thespecified state. The time period during which the patientunderwent the clinical assessment is depicted in theperiod attribute.We then define the event occurring during the courseof the study. An event represents the execution of oneor more activities during the course of the study toassess the patient. Each event transitions through anumber of states and the state is contained within thestatus attribute. A type attribute describes the typeof the event. The clinicians involved in this event aredescribed within the participant attribute, which fur-ther defines their role and a reference to the involvedmember, be it a Practitioner or an Organization.A summary of the event is provided within the summaryattribute.In clinical study parlance, an event can last any-thing from a few seconds to several months or evenyears. Consequently, we define a visit attribute toLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 9 of 14describe one or more encounters between the clinicianand the patient. Visits can be planned or unplannedand this is defined within the scheduled attribute.The timing of the visit is described within the timingattribute. Finally, we define an investigationsattribute to capture one or more clinical investigationsduring the course of the visit. These take the formof a reference to either a QuestionnaireResponseor a series of Observation or ImagingManifestresources.Demonstrating the clinical study design with FHIRWe illustrate the fit of the FHIR data model by discussinga clinical study focussed on cardiovascular episodes. Ourfocus is to highlight the impact that the addition of con-textual information, and their relationships with the dataelements, have on the semantic relevance and interpre-tation of the clinical data. Typically, the output froma clinical study, in ODM XML format, is as depictedbelow:<ItemGroupData ItemGroupOID="IG_VITAL_READINGS_8016"ItemGroupRepeatKey="1"TransactionType="Insert"><ItemData ItemOID="I_VITAL_SYSTOLICBP_1220" Value="119"><MeasurementUnitRefMeasurementUnitOID="MU_MMHG"/></ItemData><ItemData ItemOID="I_VITAL_DIASTOLICBP_4036" Value="79"><MeasurementUnitRefMeasurementUnitOID="MU_MMHG"/></ItemData></ItemGroupData>This states that the study participant has a bloodpressure of 119/79 mmHg but provides no informa-tion on how the measurements were obtained. Handler[28] outlines nine factors that may affect the accuracyof blood pressure measurements. To assist the user inmaking informed decisions about the clinical data, rel-evant contextual information, such as illustrated below,should be provided with the data. This additional meta-data tells us that the readings were taken at 10:00 amby a nurse from the left upper arm and in a sittingposition. While it is important to standardise the dataand metadata, what is missing is the relationship to theinitial blood pressure measurements. When the mea-surements are presented as a series of unrelated dataelements, they cannot reliably be interpreted (Appendixin [5]).<ItemGroupData ItemGroupOID="IG_VITAL_READINGS_8016"ItemGroupRepeatKey="1"TransactionType="Insert"><ItemData ItemOID="I_VITAL_SYSTOLICBP_1220" Value="119"><MeasurementUnitRefMeasurementUnitOID="MU_MMHG"/></ItemData><ItemData ItemOID="I_VITAL_DIASTOLICBP_4036" Value="79"><MeasurementUnitRefMeasurementUnitOID="MU_MMHG"/></ItemData></ItemGroupData><ItemGroupData ItemGroupOID="IG_VITAL_CONTEXT_4378"ItemGroupRepeatKey="1"TransactionType="Insert"><ItemData ItemOID="I_VITAL_READINGSTIME_1720"Value="10:00"><MeasurementUnitRefMeasurementUnitOID="MU_HHMM"/></ItemData><ItemData ItemOID="I_VITAL_READINGSTAKENBY_4404"Value="Nurse"/><ItemData ItemOID="I_VITAL_READINGSBODYPART_8890"Value="Left upper arm"/><ItemData ItemOID="I_VITAL_READINGSBODYPOSITION_1915"Value="Sitting"/></ItemGroupData>The FHIR framework, in particular resources such asObservation, provides the means to accurately rep-resent the relationships between the data elements sothat they can be understood and interpreted more effec-tively. A representation of the blood pressure above,implemented using the Observation resource, is illus-trated in the Additional file 1. In addition to capturingthe blood pressure measurements described previously,the Observation resource provides a reference to thePatient resource to identify the study participant andto the Practitioner resource to identify the clini-cian performing the blood pressure measurement. Moreimportantly, it natively encapsulates the contextual infor-mation, such as the body part and body position, as well asthe ability to interpret the measurements.By taking advantage of resources that encapsulatesrich interrelated clinical data, as demonstrated by theObservation resource, the ClinicalStudyDataLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 10 of 14resource facilitates the definition of the entire researchstudy, in terms of the subjects enrolled; the clinical dataassociated with these subjects and the experiments under-taken. In addition, it provides a framework, through theplan element, for the study plan to be associated with theclinical data. An implementation of the ClinicalStudyDataand ClinicalStudyPlan resources is provided at http://healthinet.it.csiro.au/net/jbs/.DiscussionThe implementations of the ClinicalStudyPlan andClinicalStudyData resources demonstrate the fit ofthe FHIR standard in capturing andmanaging clinical datafrom research studies. The pertinence of this finding isthat the clinical data no longer need to be transformedfrom an arbitrary standard into FHIR resources, thusreducing the risk of introducing errors and losing fidelity.The proposed models achieve semantic interoperabilityby defining a set of common elements for describing theactions performed on the data as well as defining commonelements for describing the data and its context throughthe use of controlled terminologies and ontologies. This,then allows the resources to be shared and processedacross systems. The FHIR resources provide the meansto navigate and access the clinical data at numerous lev-els with the addition of several dimensions at the patient,event, activity and data item level, thereby negating thelimitations of the monolithic and rigid hierarchy of theODM data model.The World Health Organisation (WHO) has releaseda list of twenty mandatory items for the definition of astudy protocol [29] so that the given trial can be con-sidered fully registered. We present, in Table 1, a list-ing of the twenty items alongside the attributes fromthe ClinicalStudyPlan and PlanDefinitionresources. We have not provided a study type asthe ClinicalStudyPlan inherently suggests a clinical study.We have also chosen not to explicitly define thesource of monetary funds and countries ofrecruitment as these are primarily associated withclinical trials. However, it is our support for eligibilitycriteria that is particularly inadequate. In our defence,our focus here has been the definition of an alterna-tive structural representation to CDISC ODM for clinicalstudy design. Furthermore, we regard the formulation ofan effectual eligibility criteria as non-trivial and one thatwe deemed out of scope for this paper. We intend toengage with the HL7 community to embed computablestudy protocol criteria within our resource as adequaterepresentation of the study protocol is very useful andimportant [10]. Previous attempts, such as the CDISCProtocol Representational Model (PRM), have had lim-ited adoption by the clinical study community [10]. (PRM[30] is a UML11-based standard that developed a set ofTable 1 Listing of the 20 WHO items for clinical study protocolWHO trial registrationdata setPlanDefinition ClinicalStudyPlan1. Primary registry andtrial identifying numberIdentifier Identifier2. Date of registration inprimary registryRegistrationDate3. Secondary identifyingnumbersIdentifier ExternalIdentifier4. Source(s) of monetary ormaterial support5. Primary sponsor Sponsor6. Secondary sponsor(s) Sponsor7. Contact for publicqueriesPublisher PublicContact8. Contact for scientificqueriesInvestigator9. Public title Title Title10. Scientific title OfficialTitle11. Countries ofrecruitment12. Health condition(s) orproblem(s) studiedPurpose* Description13. Intervention(s) ActionDefinition Activity14. Key inclusion andexclusion criteria15. Study type Type* *16. Date of first enrolment DateFirstEnrolment17. Target sample size SampleSize18. Recruitment status Status Status19. Primary outcome(s) Coverage* Goal20. Key secondaryoutcomesCoverage* GoalAn asterisk (*) indicates that this data item is partially or indirectly addressed in themodelstandardised protocol concepts that was intended to beused alongside the other CDISC and HL7 standards.)The appeal in definining a visit as part of anevent in the ClinicalStudyData resource is tomore accurately describe protracted events within mul-timodal longitudinal clinical studies. It is often useful,in the case of lengthy events, to be able to definea sub-event and subsequently record the study par-ticipants attendance to the sub-event. Consequently,the outcome of those visits can be represented as aQuestionnaireResponse, ImagingManifest or anObservation through the investigations attribute.The QuestionnaireResponse, ImagingManifestand Observation resources suit different types of clin-ical studies [8]. The pertinence of the Observationresource is the ability to store important contextual infor-mation alongside the clinical data, the ability to interpretLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 11 of 14the observation in the context of a controlled vocabularyor ontology and the ability to provide some justification asto the absence of a measurement [8].While the PlanDefinition resource can be usedto describe the study plan, it still has some inadequaciesto overcome. As the PlanDefinition resource hasnot been specifically designed to address the planningof clinical research, it logically has to be more generic.Consequently, it is unclear how the PlanDefinitionresource relates to the FHIR resources designed to cap-ture the clinical data that it defines. Furthermore, it isalso unclear what mechanism is envisaged to ensure thatthe data capture resources conform to the plan defini-tion. Moreover, as the resource has not been designedfor a clinical research domain, the PlanDefinitionresource also lacks the necessary mechanisms to fullydefine the study protocol. In particular, it does not offerthe option of recording the date of registration,the sponsor(s), the date of enrolment,expected sample size, study type and studyoutcome(s). More importantly, in our view, is the lackof support for machine-processable inclusion and exclu-sion criteria to be embedded within the PlanDefinitionresource. While the PlanDefinition defines a triggerand a condition element, these relate to the executionof the PlanDefinition resource and do not constitute thedefinition of the conditions addressing the eligibility ofthe participants to participate in the study. We advocatefor the eligibility criteria to be designed in a manner toinfluence and advance the study design and form gen-eration as outlined in [3] in their five phases of clinicalresearch data lifecycle.While the ClinicalStudyPlan and PlanDefinitionresources are structurally similar, there are sub-tle differences between them. It is unclear how theQuestionnaire resource (indicated by dotted lines inFig. 4) fits within the PlanDefinition resource. This may, inour view, restrict its ability to be used for anthropologicalstudies or surveys.Related workPrior to FHIR, several information models have beenproposed to standardise the representation of clinicalinformation. The Clinical Element Model (CEM) is aninformation model designed to provide a consistent archi-tecture for representing clinical information in EHR sys-tems [31]. The ISO 13606 standard is an internationalstandard published by ISO that specifies the informationmodels and vocabularies needed for the interoperability ofEHR systems [32]. Both models aim to address the issueof semantic interoperability by standardising the data,metadata and their relationships similar to our approach.Numerous research have centred around the CDISCmodels recently. Dugas [25] describes two tools to convertforms between the CDISC ODM and HL7 CDA12 for-mats to facilitate the sharing of electronic health records(EHRs) and clinical data to address the problem of redun-dant documentation in both systems. His findings reflectour position that the conversion process is lossy becausethe CDISC and HL7 models serve different purposesand hence have different properties. Similarly, the SALUSproject [33, 34] is a former attempt to adapt CDISCstandards to build a semantic framework to improve theinteroperability between clinical research and clinical caredomains. More specifically, it looks at combining thestrengths of CRFs with those of EHRs to address adversedrug reactions. We envisage our proposed FHIR clini-cal study model to facilitate the incorporation of existingEHR data to augment the capabilities of retrospectiveobservational studies similar to their approach. Jiang et al.[35] have developed and evaluated a Semantic Web-basedapproach for the generation of domain-specific templatesfrom the integration of the BRIDG model and the ISO21090 data types, to support clinical study metadata stan-dards development. Vadakin and Hinkson [36] discuss theCDISC PRM and outline its importance in supportingresearch study design, registration, tracking and in pro-viding a single-source of protocol content electronically.They stress that typical protocol document is not usefulfor information management and re-use. PRM standard-ises the protocol content into a structured document thatis easier to understand and to exchange, in machine-readable format, across systems [36]. We are mindful oftheir findings in order to address the issue of the protocoldefinition within our research data model.A topical area of research has been the standardisationand structuring of clinical forms. Abler et al. [18] discussthe need for a language for forms that can effectivelyrecord the logical relationships between questions or setsof questions asked in the forms. Richesson and Nadkarni[11] provide a review of the electronic data capturestandards landscape and discuss their current limitations.Bruland et al. [9] discuss the standardisation of CRFsto achieve interoperability in clinical research. Theyoutline the difficulties of promoting the standardisingand structured representation of forms in the context ofdata exchange and propose a mapping model betweenthe National Cancer Institute forms and CDISC ODMfiles semantically annotated using the Alias element. Asstated in [8], the tendency would be to organise the formswithin a Questionnaire resource in FHIR. However,this understates the nature of the information capturedand the choice of a QuestionnaireResource,Observation and ImagingManifest resourceensures the optimal capture of the information.The Linked Clinical Data Cube (LCDC) [6, 7, 19]describes a semantic web approach to investigate theassociation of the semantic statistics vocabularies withLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 12 of 14clinical data exchange standards and demonstrate theirfit in achieving the semantic enrichment of clinical studydata with a view to fulfilling semantic interoperabil-ity. The LCDC defines a set of modularised data cubesthat helps manage the multi-dimensional and multi-disciplinary nature of clinical data. It requires mappingto the RDF Data Cube [37] and DDI13-RDF Discovery[38] vocabularies to organise the data and links to domainontologies to semantically enrich it. The LCDC repre-sents the precursor to our data model in FHIR. The HL7working group on Semantic Interoperability [39] has ini-tiated work on translating the XML and JSON version ofFHIR into FHIR RDF. Once completed, this should allowthe integration of the FHIR data model with the semanticstatistics vocabularies.Future workWe intend to engage with the FHIR community to addressthe full support for the definition of eligibility criteriawithin the FHIR resources. There is a need for the cur-rent text-based criteria to be formalised and provided inmachine-readable format to facilitate computerised deter-mination of eligibility [10]. Machine-processable defini-tion of eligibility criteria will not only mould the studydesign but can influence the patients recruitment processas outlined in [40].The FHIR specification provides the functionality,through the FHIR mapping language [41], to transformclinical data from one model to another. We intend totake advantage of this functionality to map the FHIRclinical data model back to the CDISC standards. As out-lined earlier, the regulatory bodies favour the use of theCDISC standards for the reporting of clinical studies [5].By using FHIR to model the clinical study data, we capturethe contextual information, and fulfil the requirements ofthe FDA by retrofitting the clinical data to the CDISCmodels.We also aim to support the formulation of tempo-ral constraints to assist in the scheduling of activitiesas outlined in [42], which describes a knowledge-basedapproach to specifying and monitoring temporal con-straints in relational databases.ConclusionThis paper has presented a proposal for a clinical studyarchitecture to support the semantic interoperability ofclinical data using the FHIR resources. We have shownhow the clinical research community is likely to benefitfrom the adoption of FHIR resources to capture and man-age clinical study data. In this regard, we have outlined amethod to link clinical data from the XML-based CDISCODMmodel to a selective group of FHIR resources.Whilewe have revealed a fit between the ODM model and theFHIR resources, we do not regard this as a long termsolution. First, owing to the evolving nature of the FHIRspecifications, this mapping is likely to change at a whim.Second, it is preferable to avoid data transformations butfor data to be captured directly at the source.We have thusproposed two FHIR models, a ClinicalStudyPlanand a ClinicalStudyData resource, and shown thatthey can natively manage clinical data. We have comparedour work to the proposed HL7 PlanDefinition FHIRresource and discussed their suitability in adequately rep-resenting the research study protocol definition. We havedemonstrated, with the help of a working example, the fitof our clinical data model in interpreting clinical researchdata. Our work has built the foundations to not only facil-itating the syntactic but also semantic interoperability ofclinical research data.Endnotes1 Extensible Markup Language [43]2Health Level Seven3Based on the Standard for Trial Use 3 September 2016version4 Systematized Nomenclature of Medicine Clinical Ter-minology5 Logical Observation Identifiers Names and Codes6 FHIR-Infrastructure7 Regulated Clinical Research InformationManagement8 Standard for Trial Use 39This resource can be found at http://hl7.org/fhir/2016Sep/plandefinition.html10This resource can be found at http://hl7.org/fhir/2016Sep/activitydefinition.html11Unified Modeling Language12Clinical Document Architecture13Data Documentation InitiativeAdditional fileAdditional file 1: The file observation_example.json lists the Observationresource as described in the Demonstrating the clinical study design withFHIR section in json format. (JSON 2 kb)AbbreviationsCDA: Clinical document architecture; CDASH: Clinical data acquisitionstandards harmonization; CDISC: Clinical data interchange standardsconsortium; CEM: Clinical element model; DSTU2: Draft standard for trial use 2;FHIR: Fast Healthcare interoperable resources; FHIR-I: FHIR-Infrastructure; HL7:Health level seven; ISO: International organization for standardization; LCDC:Linked clinical data cube; LOINC: Logical observation identifiers names andcodes; ODM: Operational data model; RCRIM: Regulated clinical researchinformation management; SNOMED CT: Systematized nomenclature ofmedicine clinical terminology; STU3: Standard for trial use 3; XML: Extensiblemarkup languageLeroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 13 of 14AcknowledgementsThe authors would like to thank Drs Marlien Varnfield and Parnesh Raniga forreviewing the paper.An initial version of this paper has been published on Researchgate at [44].FundingThis research has not been funded from an external grant.Availability of data andmaterialsThe mapping of the clinical data from CDISC ODM to FHIR can be accessed at:http://healthinet.it.csiro.au/net/jbs/odmFhir.The web resources and implementation of the new FHIR resources have beenincluded at: http://healthinet.it.csiro.au/net/fhir/.A working example showcasing the two FHIR resources can be found at:http://healthinet.it.csiro.au/net/jbs/implementation.Authors contributionsHL designed the FHIR resources that AM and ML reviewed. HL implementedthe FHIR resources and part of the clinical study examples and drafted themanuscript. AM implemented part of the clinical study examples. AM and MLreviewed the manuscript. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 2 June 2016 Accepted: 3 September 2017The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31DOI 10.1186/s13326-017-0142-0RESEARCH Open AccessDynamically analyzing cell interactions inbiological environments using multiagentsocial learning frameworkChengwei Zhang1, Xiaohong Li1*, Shuxin Li1 and Zhiyong Feng2From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016Shenzhen, China.16 December 2016AbstractBackground: Biological environment is uncertain and its dynamic is similar to the multiagent environment, thus theresearch results of the multiagent system area can provide valuable insights to the understanding of biology and areof great significance for the study of biology. Learning in a multiagent environment is highly dynamic since theenvironment is not stationary anymore and each agents behavior changes adaptively in response to other coexistinglearners, and vice versa. The dynamics becomes more unpredictable when we move from fixed-agent interactionenvironments to multiagent social learning framework. Analytical understanding of the underlying dynamics isimportant and challenging.Results: In this work, we present a social learning framework with homogeneous learners (e.g., Policy Hill Climbing(PHC) learners), and model the behavior of players in the social learning framework as a hybrid dynamical system. Byanalyzing the dynamical system, we obtain some conditions about convergence or non-convergence. Weexperimentally verify the predictive power of our model using a number of representative games. Experimentalresults confirm the theoretical analysis.Conclusion: Under multiagent social learning framework, we modeled the behavior of agent in biologicenvironment, and theoretically analyzed the dynamics of the model. We present some sufficient conditions aboutconvergence or non-convergence and prove them theoretically. It can be used to predict the convergence of thesystem.Keywords: Multiagent learning, Cell interaction, Nonlinear dynamicBackgroundAll living systems live in environments that are uncertainand dynamically-changing. However, it is remarkable thatthese systems survive and achieve their goals by exhibitingintelligent features such as adaption and robustness. Bio-logical system behaviors [1] and human diseases [2] areoften the outcome of complex interactions among a verylarge number of cells and their environments [3, 4].*Correspondence: xiaohongli@tju.edu.cn1School of Computer Science and Technology, Tianjin University, Peiyang ParkCampus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, ChinaFull list of author information is available at the end of the articleSimilarly, in the multiagent system [59], an importantability of an agent is to adjust its behavior adaptively tofacilitate efficient coordination among agents in unknownand dynamic environments. If we regard the cells in thebiological system as the agents in the multiagent system,we can analyse the cells behavior using the theory ofmultiagent system. So understanding collective decisionmade by such intelligent multiagent system is an inter-esting research topic not only for artificial intelligent butalso for biology. The conclusion of the theoretical analy-sis can be applied to the research of biology, for example,the results of convergence can be used for explaining thephenomenon of cells group behaviour.© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 44 of 79Now, computational methods have been widely used tosolve biological problems [10, 11]. Many researchers haveinvestigated biological systems which are composed ofcells and their environments via modeling and simulation[1, 12]. There are two principal approaches: populationbased modeling and discrete agent based modeling. Pop-ulation based modeling approximates the cells within anygrid box by a set of variables associated with the grid box[13, 14]. Discrete agent based modeling maps each cell toa discrete simulation entity [13, 15, 16].We use multiagent learning techniques to model thebehaviors of each cell agent, which is an important tech-nique to achieve efficient coordination in multiagent sys-tem area [9, 1719]. Until now, significant amount ofefforts have been devoted to develop effective learningtechniques for different multiagent interaction environ-ments [2023]. In the multiagent environments, eachagent interacts with the agent selected from its neigh-borhood randomly each round, and updates its strategybased on the feedback in the current round. To describethe behavior of an agent, one common line of researchesis to extend existing reinforcement learning techniquesin single-agent environment to multiple-agent interactionenvironment. However, due to the violation of Markovproperty, the existing theoretical guarantees do not holdany more in multiagent environment. It is important andchallenging for us to model the multi-agent environ-ment and analyse the learning dynamics of multiagentenvironments.This paper presents a social learning framework to sim-ulate the dynamics of multiagent system in biologicalenvironment and a theoretical analysis of the learningdynamics of this model is also given. The analysis resultsshed lights on how and when the consistent knowledge interms of equilibrium can be or not be evolved among thepopulation of agents. In the social learning framework, allagents play PHC strategy [24] for decisionmaking, and usea weighted graphmodel for neighbor selection. In the partof theoretical analysis, we present a theoretical model toanalyze the learning dynamics of the learning framework.The purpose of analysing the learning dynamics is to judgewhether the learning algorithm that the agent adopt canconverge or not. The intention behind is that convergenceto an equilibrium has been the most commonly acceptedgoal to pursue in multiagent learning literature. Firstly, wemodel the overall dynamics among agents as a system ofdifferential equations. Then, some conditions are provedto be the sufficient condition of convergence or non-convergence. It can be used to predict the convergenceof the system. Finally, we estimate the prediction throughsimulation experiment. The experimental results confirmthe predictive outcomes of our theoretical analysis.The remainder of the paper is organized as follows.Method section first reviews normal-form game and thebasic gradient ascent approach with a GA-based algo-rithm named PHC, and then introduces the multiagentlearning framework where all the agents are PHC learn-ers. In the Result and discussion section, we presentthe theoretical model of the learning dynamics of agents,and prove convergence and non-convergence conditionsby analyze geometrical behaviors of the hybrid dynamicsystem in the help of nonlinear dynamic theory. In theExperimental simulation section, we evaluate the pre-dictive ability of our theoretical model by comparing itwith the simulation results. Lastly we conclude the paperand point out future directions in Conclusion section.MethodNotation and definitionNormal-form gamesIn a two-player, two-action, general-sum normal-formgame, the payoff for each player i ? {k, l} can be specifiedby a matrix as follows,Ri =[r11i r12ir21i r22i](1)Each player i selects an action simultaneously from itsaction set Ai = {1, 2}, and the payoff of each player isdetermined by their joint actions. For example, if player kselects the pure strategy of action 1 while player l selectsthe pure strategy of action 2, then player k receives apayoff of r12k and player l receives the payoff of r21l .Apart from pure strategy, each player can also employ amixed strategy to make decisions. Amixed strategy can berepresented as a probability distribution over the actionset and a pure strategy is a special case of mixed strate-gies. Let pk ? [0, 1] and pl ? [0, 1] denote the probabilityof choosing action 1 by player k and player l, respec-tively. Given a joint mixed strategy profile (pk , pl), theexpected payoffs of player l and player k can be computedas follows,Vk (pk , pl) =r11k pkpl + r12k pk (1 ? pl) + r21k (1 ? pk) pl+ r22k (1 ? pk) (1 ? pl) (2)Vl (pk , pl) =r11l pkpl + r21l pk (1 ? pl) + r12l (1 ? pk) pl+ r22l (1 ? pk) (1 ? pl) (3)A strategy profile is a Nash Equilibrium (NE) if no playercan get a better expected payoff by changing its currentstrategy unilaterally. Formally,(p?k , p?l) ? [0, 1]2 is a NE, iffVk(p?k , p?l) ? Vk (pk , p?l ) and Vl (p?k , p?l ) ? Vl (p?k , pl) forany (pk , pl) ? [0, 1]2.Gradient ascent (GA) and PHC algorithmWhen a game is repeatedly played, an individually ratio-nal agent updates its strategy with the propose of max-imizing its expected payoff. We know that the gradientThe Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 45 of 79direction is the fastest increasing direction, thus it is awell-deserved way to model the behavior of agent usinggradient ascent algorithm. Agent i that employs GA-basedalgorithm updates its policy towards the direction of itsexpected reward gradient, which is shown in the followingequations.p(t+1)i ? ??Vi(p(t))?pi(4)p(t+1)i ? [0,1](p(t)i + p(t+1)i)(5)The parameter ? is the size of gradient step. [0,1] isthe projection function mapping the input value to thevalid probability range of [ 0, 1], which is used for prevent-ing the gradient from moving the strategy out of the validprobability space. Formally, we have[0,1](x) = argminz?[0,1] |x ? z| . (6)To simplify the notation, let us define ui = r11i + r22i ?r12i ? r21i and ci = r12i ? r22i . For the two-player case, theEqs. 4 and 5 can be represented as follows,p(t+1)k ? [0,1](p(t)k + ?(ukp(t)l + ck))(7)p(t+1)l ? [0,1](p(t)l + ?(ulp(t)k + cl)). (8)In the case of infinitesimal size of gradient step (? ? 0),the learning dynamics of the agent can be modeled as asystem of differential equations. Further, it can be ana-lyzed using dynamic system theory [25]. It is proved thatthe strategies of all agents will converge to a Nash equilib-rium, or if the strategies do not converge, agents averagepayoff will converge to the average payoff of Nash equilib-rium [26]. The policy hill-climbing algorithm (PHC) is acombination of gradient ascent algorithm and Q-learningwhere each agent i adjusts its policy p to follow the gra-dient of expected payoff (or the value function Q). It isshown in the Algorithm 1.Here, ? ? (0, 1] and ? ? (0, 1] are learning rate, andQ values are maintained just as in normal Q-learning.The policy is improved by increasing the probability ofselecting the highest valued action based on the learningrate ?.Modeling multiagent learningUnder the multiagent social learning framework with Nagents, each agent interacts with one of its neighborsselected randomly from its neighborhood each round.The neighborhood of each agent is determined by itsunderlying network topology. The interaction betweeneach pair of agents is modeled as a two-player normal-Algorithm 1 The policy hill-climbing algorithm (PHC)for agent i ? {r, c}1: Let ? ? (0, 1] and ? ? (0, 1] be learning rates.Initialize Qi (a) ? 0, pi (a) ? 1|Ai| .2: repeat3: Select action a ? Ai according to mixed strategy piwith suitable exploration.4: Observe reward r and update Q valueQi (a) ? (1 ? ?)Qi(a) + ?r5: Step p closer to the optimal policy w.r.t. Q,pi(a) ? pi(a) + awhile constrained to a legal probability distribution,a ={ ??a a = argmaxa?Qi(a?)?a? =a ?a? otherwise?a = min(pi(a), ?|Ai|?1)6: until the repeated game endsform game. During each interaction, each agent selectsits action following a specified learning strategy, whichis updated repeatedly based on the feedback from theenvironment at the end of interaction. The framework ispresented in Algorithm 2.Algorithm 2 Overall interaction protocol of the sociallearning framework1: repeat2: for each agent in the population do3: Chose one of its neighbors with a certain proba-bility.4: Play a two-player normal-form game with thisneighbor.5: Select a action according to its mixed strategywith suitable exploration.6: end for7: Environmental feedback.8: for each agent in the population do9: Observing reward r and update its policy basedon its past experience according to specific poli-cies.10: end for11: until the repeated game endsWe use graph G = (V ,E) to model the underlyingneighborhood network, which is composed by N = |V |agents. The edges E = {eij}, i, j ? V represent socialcontacts among agents, where eij denotes the probabil-ity that agent i chooses agent j to interact with. We have?j?V eij = 1?eii = 0. Here, we propose an adaptive strat-egy for agents to make their decisions in social learningframework with PHC learning strategy, which is shown inAlgorithm 3.The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 46 of 79Algorithm 3 Learning process in the multiagentframework for agent i ? V1: Let ? ? (0, 1] and ? ? (0, 1] be learning rates.Initialize Qi (a) ? 0, pi (a) ? 1|Ai| .2: repeat3: Select agent j ? V according to E with probabilityeij.4: Select action a ? Ai according to mixed strategy piwith suitable exploration.5: Observe reward r according to interaction betweeni and j.6: Update Q valueQi (a) ? (1 ? ?)Qi(a) + ?r7: Step p closer to the optimal policy w.r.t. Q,pi(a) ? pi(a) + awhile constrained to a legal probability distribution,a ={ ??a a = argmaxa?Qi(a?)?a? =a ?a? otherwise?a = min(pi(a), ?|Ai|?1)8: until the repeated game endsResult and discussionAnalysis of the multiagent Learning DynamicsIn this section, we present a theoretical model to estimateand analyze the learning dynamics of the abovemultiagentlearning framework in Algorithm 3. We extend notationsin section to the multiagent environment. Without loss ofgenerality, we consider the case with two-action only.Assume that the payoff that an agent receives onlydepends on the joint action, then the payoff for agenti ? V can be defined as a fixed matrix Ri,Ri =[r11i r12ir21i r22i](9)where rmni denotes the payoff received by agent i wheni selects action m and its neighbor selects n. Here, weuse the pi to denote the probability that the player iselects action 1. Then the mixed strategy (p1, p2, . . . , pN )in multiagent framework can be considered as a point inRN constrained to the unit square. The expected payoffVi (p1, p2, . . . , pN ) of player i can be computed as follows,Vi(p1, p2, . . . , pn)=?j?V eijVi,j(pi, pj)=uipi?j?V eijpj + cipi +(r21i ? r22i)pj + r22i(10)where ui = r11i + r22i ? r12i ? r21i , ci = r12i ? r22i ,Vi,j(pi, pj) = r11i pipj + r12i pi (1 ? pj) + r21i (1 ? pi) pj +r22i (1 ? pi)(1 ? pj), and eij is the probability that theagent i selects agent j to interact with.Each agent i updates its strategy in order to maximizethe value of Vi. Recall the Eqs. 4 and 5, we can obtainp(k+1)i =?[p(k)i + ??piVi(p1, p2, . . . , pN )]=?[p(k)i + ?(ui?j?V eijpj + ci)] (11)where parameter ? is the size of gradient step.As ?p ? 0, it is straightforward that the Eq. 11 becomesdifferential equation. Considering the step size to beinfinitesimal, the unconstrained dynamics of the all play-ers strategies can be modeled by the following differentialequations.p?i = ui?j?V eijpj + ci, i ? {1, 2, . . . ,N} (12)Equation 12 can be simplified as follows using somenotation,P? = UEP + C (13)where P = (p1, p2, . . . , pN )T , P? = (p?1, p?2, . . . , p?N )Tand C = (c1, c2, . . . , cN )T . The matrix U =diag(u1,u2, . . . ,uN ) is the diagonal matrix generated by(u1,u2, . . . ,uN ).For the constrained dynamics of the strategies, we canmodel it as the following equations,???p?i = 0 pi = 0 ? Gi ? 0p?i = 0 pi = 1 ? Gi ? 0p?i = Gi otherwise(14)where Gi = ui ?j?V eijpj + ci.Notice that Eq. 14 is a hybrid system composed of twoparts: a series of continuous linear differential dynamicsystems in the respective domain space and a switchmechanism between differential dynamic systems whendynamic touch the boundary. Generally, it is hard toobtain a complete conclusion by analyzing dynamics of ageneral hybrid system, even though the differential sys-tem is linear. But we can still find some convergence andnon-convergence conditions under certain instances(i.e.,Eq. 14).Non-convergence condition of the multiagent learningframeworkAccording to the above definition, we have the fol-lowing general result under which non-convergence isguaranteed.Theorem 1 In an N agent, two-action, integrated gen-eral sum game, every player follows the constraineddynamics of the strategy we defined in Eq. 14. If the follow-ing two conditions are met,1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,that UEP? + C = 0,The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 47 of 792. There exists a pair of pure imaginary eigenvalues ofmatrix UE,then there exists a set P ? [ 0, 1]N , that the solution ofthe initial value problem of Eq. 14 with P(0) ? P can notconverge.Proof Considering the complexity of the hybrid systemrepresented by Eq. 14, we begin with the unconstrainedones. Based on the theorems of differential equationsdynamical systems [25], we calculate the analytic solutionof Eq. 13. Homogenizing the in-homogeneous equation bysubstituting P with P = X + P?, where UEP? + C = 0, wegetX? = UEX.Here, UE is an N × N matrix, then there is a invertiblematrix T = (v1, . . . , vN ) that can transform UE into J,T?1UET = J =???J1 · · ·... . . ....· · · Jm???The Ji is a square matrix and its form is one of thefollowing two,(1)?????? 1 · · ·? 1... . . ....· · · ?????? (2)?????D2 I2 · · ·D2 I2... . . ....· · · D2?????where D2 =[? ??? ?], I2 =[1 00 1], ?,? , ? ? R and? = 0. Here, J is the Jordan normal form of matrixUE. Ji isthe Jordan block corresponding to ?i, which is a repeatedeigenvalue of UE with multiplicity ni. If eigenvalue ?i is areal number, then Ji is in the form (1), else Ji is in the form(2). Suppose that ?1, . . . , ?k are matrix UEs real eigenval-ues, and ?k+1, . . . , ?m is matrix UEs complex eigenvalues,then we have n1 + . . . + nk + 2(nk+1 + . . . nm) = N .Then the analytic solution of function X? = UEX withinitial value X(0) will beX(t) = exp (tUE)X(0) = T???etJ1. . .etJm???T?1X(0).Using the notation Y (t) = T?1X(t), we haveY (t) = exp (tJ)Y (0) =???etJ1. . .etJm???Y (0).Suppose that ?k = ?i is a pure imaginary eigenvalueof UE with multiplicity nk , so ?¯k = ??i is an eigenvalueof UE with multiplicity nk . Then J has a block Jk , Jk =?????D2 I2 · · ·D2 I2... . . ....· · · D2?????, where D2 =[0 ??? 0].Due to etD2 = exp(t[0 ??? 0])=[cos?t sin?t? sin?t cos?t],there must exist a pair of items about vector Y (t) asfollows.{yi(t) = yi(0) cos?t + yi+1(0) sin?tyi+1(t) = ?yi(0) cos?t + yi+1(0) sin?tIf yi(0) = 0 ? yi+1(0) = 0, then Eq. 14 has a peri-odic solution. Let vi and vi+1 to denote eigenvector ofT = (v1, . . . , vN ) corresponding to ?k and ?¯k , respectively.Note that X(t) = TY (t), then the solution of Eq. 13 withthe initial value P(0) ? S is cyclical, whereS = {P ? [ 0, 1]N |P = k1v1 + k2v2 + P?, k1, k2 ? R} .Because of P? ? (0, 1)N , there must exists a ? > 0 forthe deleted neighborhood B(P?; ?) ? (0, 1)N of P?,B(P?; ?) = {x ? RN |0 < ||x ? P?||2 < ?} ? (0, 1)NLet P denote S?B(P?; ?), the solution of the Eq. 14 withany initial value belongs to P is cyclical, which means thealgorithm corresponding to the Eq. 14 can not converge.Theorem 1 shows that there exist some situations inwhich the agents fail to converge under the multiagentsocial learning framework. Before giving the details ofthose situations, we need to introduce the following nota-tions first.According to the theorem 1, T is the transformationmatrix for T?1UET = J , T = (v1, v2, . . . , vN ). Letvj1, vj2, . . . , vjnj denote eigenvectors associated to eigen-value ?j, j = 1, 2, . . . ,m. According to properties of thematrix transformations [27], vj1, vj2, . . . , vjnj are linearlyindependent. Classify column vectors of the transfor-mation matrix T into three parts corresponding to ?,V1 = {vi|Re(?i) < 0}, V2 = {vi|Re(?i) = 0} andV3 = {vi|Re(?i) > 0}. Now we are ready to give the pre-cise description of the subspace where the agents fail toconverge, which is summarized in the following theorem.Theorem2 If Eq. 14meets both conditions of Theorem 1,and ?k = ?i, ?k = ??i are a pair of pure imaginary eigen-values of UE, then there exists a pair of vectors vk , v?k ? V2, > 0, and a set P = S ? B(P?; ?), whereS={P ? [ 0, 1]N |P=X + P?,X ? span(V1 ? {vk , v?k})},B(P?; ?) = {x ? RN |0 < ||x ? P?||2 < ?} ? [ 0, 1]N ,The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 48 of 79that the solution of the initial value problem of the Eq. 14with P(0) ? P cant convergence.Proof According to Theorem 1, we have the solution ofthe initial value problem that the Eq. 14 with P(0) ? S ?B(P?; ?) can not convergence. HereS = {P ? [ 0, 1]N |P = X + P?,X ? span({vk , v?k})}For the eigenvalue ?i associated to vector vi ? V1, thereare Re(?i) < 0. According to conclusions of bifurcationtheory [25], the subspace span(V1) is a stable submanifoldof the unconstrained dynamics (13), which means everytrajectory start from S? will eventually convergence to P?,whereS? = {P ? [ 0, 1]N |P = X + P?,X ? span(V1)} .Then trajectories start from S will eventually conver-gence to S, thus we got the final conclusion that thesolution of the initial value problem of the Eq. 14 withP(0) ? P cant convergence.Note that Theorem 1 and 2 are just sufficient conditionsof non-convergence.Convergence condition of the multiagent learningframeworkIn most cases, the conditions that guarantee the conver-gence of a algorithm are more valuable.Theorem 3 In an N agent, two-action, integrated gen-eral sum game, every player follows the constraineddynamics of the strategy we defined in Eq. 14. If the follow-ing two conditions are met,1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,that UEP? + C = 0,2. All of the eigenvalues of matrix UE has negative realpart,then all the solutions of the initial value problem of Eq. 14with P(0) ? [0, 1]N will converge eventually.Proof The conclusion is obvious. It is known that theconstruction of the linear dynamic system is stable. If alleigenvalues of matrix UE have negative real part, thenpoint P is a stable equilibrium point. It means that all thesolutions of the initial value problem of the Eq. 14 withP(0) ? [0, 1]N will converge to P.Theorem 3 proposes a sufficient condition to identifythe convergence of dynamic in Eq. 14. We know thatit is hard to calculate eigenvalues of a matrix with highdimensional. Here, we propose a more realistic conver-gence condition which is suitable for multiagent learningframework shown in Algorithm 3.Theorem 4 In an N agent, two-action, integrated gen-eral sum game, every player follows the constraineddynamics of the strategy we defined in Eq. 14. If matrixUE is symmetrical, then all the solution of the initialvalue problem of Eq. 14 with P(0) ? [0, 1]N will convergeeventually.Proof It is known that the eigenvalues of real symmetricmatrix are real numbers [27]. We analyze all the cases ofEq. 14 when all of the eigenvalues of matrix UE are real:1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,that UEP? + C = 0.2. There are no such a point, that UEP? + C = 0.For case 1), if all eigenvalues of matrix UE are negativenumber, then point P is a stable equilibrium points; oth-erwise, all the solutions of the initial value problem of thehybrid system with P(0) ? [0, 1]N will move away from Ptoward boundary of the hybrid system [25]. Because thedomain of hybrid system represented by 14 has bound-ary(i.e., P(t) ? [0, 1]N ), then there must exists a pointP? = (p?1, . . . , p?N)T in the boundary of the domain, where(p?i = 0 ? Gi ? 0) ? (p?i = 1 ? Gi ? 0) for all i ? V . Thedynamic P(t) will converge to P? eventually.Similarly, we can find a point P? = (p?1, . . . , p?N)T in theboundary of the hybrid system domain in case 2) and thedynamic P(t) will converge to P? eventually. The theoremmust hold.Based on conclusions of Subsections Non-convergencecondition of the multiagent learning framework andConvergence condition of the multiagent learning frame-work , we can determine the learning dynamics of anycases we defined in Eqs. 14 and 13. However, the com-putational complexity may be prohibitive when the modelsize becomes too large. In the next section, we considera special case under an interesting network structurewhich can be analyzed with relatively light computationalcomplexity for any network size.The simplest case whose underlying topology is a ringWe consider the case when the underlying topology is aring, and each agent only interacts with the neighbor onits right-hand side in each interaction. As defined in theprevious section, the adjacency matrix E isE = {eij}N×N , i, j ? {1, 2, . . . ,N},where eij ={1 j = (i + 1)modN0 else .According to Eq. 14, the constrained dynamics of thisspecial case can be modeled as follows:???p?i = 0 pi = 0 ? Gi ? 0p?i = 0 pi = 1 ? Gi ? 0p?i = Gi otherwise(15)The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 49 of 79where Gi = uipi+1 + ci, i = {1, 2, . . . ,N ? 1}, and GN =uNp1+cN . Through analyzing the dynamics of this model,we have the following conclusion.Theorem 5 In an N-player, two-action, integratedgeneral-sum game, every agent follows the constraineddynamics of the model in Eq. 15. If one of the agentsconverges to a strategy, then every agent will convergeseventually.Proof Suppose agent k converges at some time, accord-ing to the definition, its strategy pk will be a constant. InEq. 15, we have Gk?1 = uk?1pk + ck?1 be a constant,which means convergence of player k implies convergenceof player k ? 1. By induction, every agent will convergeeventually.According to the above theorem, we can easily obtainthe following proposition.Proposition 1 In Eq. 15, if there exists a dominantstrategy for some players, then their strategies will asymp-totically converge to a Nash equilibrium.According to the above conclusion, finally we presentthe following unconvergence result.Theorem 6 In an N agent, two-action, integrated gen-eral sum game, every player follows the constraineddynamics of the strategy we defined in Eq. 15. If everyplayer has no dominant strategy, and met one of thefollowing conditions,1. N = 4k, k ? N and ?Ni=1 ui > 0.2. N = 4k + 2, k ? N and ?Ni=1 ui < 0.then there exists a set P ? [0, 1]N that the solution of theinitial value problem of the Eq. 15 with P(0) ? P cantconverge.Proof According to the definitions above, the payoffmatrix of player i isRi =[r11i r12ir21i r22i], i ? {1, 2, . . . ,N},and ui = r11i + r22i ? r12i ? r21i , ci = r12i ? r22i . Then we haveuici= (r11i + r22i ? r12i ? r21i ) (r12i ? r22i )= (r11i ? r21i ) (r12i ? r22i ) ? (r12i ? r22i )2(16)Since every agent has no dominant strategy, we have(r11i ? r21i) (r12i ? r22i)< 0.Thus we have uici < 0, andciui= ?(r12i ? r22i)(r11i ? r21i) ? (r12i ? r22i ) =11 +(r21i ?r11i)(r12i ?r22i).Set p?i = ? ciui and P? =(p?1, p?2, . . . , p?N)T , then we haveP? ? (0, 1)N andUEP+C = 0. Considering the Eq. 15, bycalculating the eigenvalue of matrix UE, we have?N = u1u2 . . .uN =N?i=1ui.If N = 4k, k ? N and ?Ni=1 ui > 0, then matrix UEhas a pair of pure imaginary eigenvalue. Otherwise, ifN =4k+2, k ? N and?Ni=1 ui < 0, thenmatrixUE has a pair ofpure imaginary eigenvalue. According to Theorem 1, thereexists a set P ? [0, 1]N that the solution of the initial valueproblem of Eq. 15 with P(0) ? P can not convergence.Experimental simulationIn this section, we compare the empirical dynamics ofthe multiagent social learning framework composed byPHC learners with theoretical prediction of our hybriddynamic model. We perform two experiments that satisfythe Theorem 1 and 4, respectively.A non-convergence multiagent GameIn this subsection, we consider a 4-player, two-actiongame. The game is defined as follows,R1 =[1 00 1],R2 =[1 00 1],R3 =[1 00 1],R4 =[1 00 1]E =????0 1/2 0 1/21/2 0 1/2 00 1/2 0 1/21/2 0 1/2 0????Metrix Ri, i ? {1, 2, 3, 4} is the payoff matrix of agent i,and element eij of matrix E is the probability that player iselects player j in each interaction. In this game, we haveu1 = u3 = 2, u2 = u4 = ?2, c1 = c3 = ?1, and c2 = c4 =1. Then the unconstrained dynamic model of this game isP? = UEP + C, whereUE =????0 1 0 1?1 0 ?1 00 1 0 1?1 0 ?1 0???? ,C = (?1, 1,?1, 1)T .This game has a P? = (1/2, 1/2, 1/2, 1/2)T ? (0, 1)4,which satisfies UEP? + C = 0. Matrix UE has a pair ofpure imaginary eigenvalues which is ?1 = 2i and ?1 =2i. The eigenvectors are v1 = (0, 1/2, 0, 1/2)T and v2 =(1/2, 0, 1/2, 0)T corresponding to ?1 and ?2. Let P(0) =P?+k1v1+k2v2. As long as k1 and k2 are sufficiently small,The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 50 of 790 500 1000 1500 2000 2500 300000.20.40.60.81RoundPp1p2p3p4Fig. 1 Agent dynamics of game satisfying the conditions of Theorem 1according to Theorem 1, the solution of the initial valueproblem of game 1 with P(0) cant converge.In Fig. 1, the dynamic solution of the game with initialvalue P(0) is plotted, where k1 = k2 = 0.1. Each of thefour lines in Fig. 1 shows the strategys dynamic changingof each agent, respectively. We can see that the strategiesof those agents do not converge. Obviously, the simulationresults are consistent with the theoretical prediction.A convergence multi-agent GameIn this subsection, we consider a 4-player, two-actiongame. The game is defined as follows,Ri =[1 00 1], i ? {1, 2, 3, 4}E =????0 1/2 0 1/21/2 0 1/2 00 1/2 0 1/21/2 0 1/2 0????Metrix Ri, i ? {1, 2, 3, 4} is the payoff matrix of agent i,and element eij of matrix E is the probability that player iselects player j in each interaction. In this game, we haveui = 2 and ci = ?1, i ? {1, 2, 3, 4}. Then the uncon-strained dynamic model of this game is P? = UEP + C,whereUE =????0 1 0 11 0 1 00 1 0 11 0 1 0???? ,C = (?1,?1,?1,?1)T .Because matrix UE is symmetrical, according toTheorem 4, the solution of the initial value problem of thisgame with any P(0) ? [0, 1]4 will converge eventually.Figure 2 illustrates dynamics of the PHC learners strat-egy for the game with initial value initial value P(0) =(1/2, 1/2, 1/2, 1/2)T . Each of the four lines in Fig. 2 showsFig. 2 Agent dynamics of game satisfying the conditions of Theorem 4The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 51 of 79the strategys dynamic changing of each agent, respec-tively. We can see that the strategies of those agents con-verge eventually, which are consistent with the theoreticalprediction.ConclusionIn this work, we proposed a multiagent social learningframework to model the behavior of agent in biologicenvironment, and theoretically analyzed the dynamics ofmultiagent social learning framework using non-lineardynamic theories. We present some sufficient conditionsabout convergence or non-convergence and prove themby the theoretically analysis. It can be used to predict theconvergence of the system. Experimental results show thatthe predictions of our dynamic model are consistent withthe simulation results.As future work, more extensive study of the dynam-ics of multiagent social learning framework with PHClearners is needed. Other worthwhile directions includeto improve the PHC algorithm, to develop more realisticmultiagent social learning framework to model the realis-tic interactions among cells in biologic environments, andto achieve better convergence performance based on ourtheoretical findings.AcknowledgementsWe thank the reviewers valuable comments for improving the quality of thiswork.FundingThis work has partially been sponsored by the National Science Foundation ofChina (No. 61572349,61572355).Availability of data andmaterialsAll data generated or analysed during this study are included in this publishedarticle.About this supplementThis article has been published as part of Journal of Biomedical SemanticsVolume 8 Supplement 1, 2017: Selected articles from the Biological Ontologiesand Knowledge bases workshop. The full contents of the supplement areavailable online at https://jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.Authors contributionsCZ contributed to the algorithm design and theoretical analysis. SL had a mainrole in the editing of the manuscript. XL and ZF contributed equally to the thequality control and document reviewing. All authors read and approved thefinal manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Author details1School of Computer Science and Technology, Tianjin University, Peiyang ParkCampus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, China.2School of Computer Computer Software, Tianjin University, Peiyang ParkCampus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, China.Published: 20 September 2017