RESEARCH Open Access
A novel method to identify pre-microRNA
in various species knowledge base on
various species
Tianyi Zhao1, Ningyi Zhang1, Ying Zhang2, Jun Ren3, Peigang Xu1, Zhiyan Liu1, Liang Cheng4* and Yang Hu3*
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: More than 1/3 of human genes are regulated by microRNAs. The identification of microRNA (miRNA)
is the precondition of discovering the regulatory mechanism of miRNA and developing the cure for genetic
diseases. The traditional identification method is biological experiment, but it has the defects of long period,
high cost, and missing the miRNAs that but also many other algorithms only exist in a specific period or low
expression level. Therefore, to overcome these defects, machine learning method is applied to identify miRNAs.
Results: In this study, for identifying real and pseudo miRNAs and classifying different species, we extracted
98 dimensional features based on the primary and secondary structure, then we proposed the BP-Adaboost
method to figure out the overfitting phenomenon of BP neural network by constructing multiple BP neural
network classifiers and distributed weights to these classifiers. The novel method we proposed, from the 4
evaluation terms, have achieved greatly improvement on the effect of identifying true pre-RNA compared to
other methods. And from the respect of identifying species of pre-RNA, the novel method achieved more
accuracy than other algorithms.
Conclusions: The BP-Adaboost method has achieved more than 98% accuracy in identifying real and pseudo
miRNAs. It is much higher than not only BP but also many other algorithms. In the second experiment, restricted by
the data, the algorithm could not get high accuracy in identifying 7 species, but also better than other algorithms.
Keywords: Pre-miRNA identification, BP neural network, Adaboost
Background
MicroRNAs(miRNAs) are a class of small single-strand
and non-coding RNA molecules of approximately 22 nu-
cleotides in length, miRNAs play important roles in many
biological process including affecting stability, metabolism,
signal translation, disease development and translation of
mRNAs [1]. Meanwhile, miRNAs are also very important
in the treatment of diseases, such as: cancer [2], X
chromosomal defects [3], DiGeorge disease [4], etc. As the
development of science and technology, people pay more
and more attention to miRNA research, amount of novel
miRNAs are discovered, the number and functional fea-
tures are far beyond our imagination [5, 6]. The main
challenge of studying miRNAs is how to find miRNAs and
the action sites, at present the main methods for identi-
fying miRNAs are cDNA clone and sequencing and
computational prediction, the expression of cDNA se-
quencing method is low and costs amount of time and
funding [712]. Therefore, computational method are
more prevalent, several algorithms have been proposed
to detect pre-miRNAs, the main challenge is to dis-
criminate the real pre-miRNAs from the pseudo ones
and identify novel miRNAs.
* Correspondence: liangcheng@hrbmu.edu.cn; huyang@hit.edu.cn
4College of Bioinformatics Science and Technology, Harbin Medical
University, Harbin 150001, China
3School of Life Science and Technology, Harbin Institute of Technology,
Harbin 150001, Peoples Republic of China
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30
DOI 10.1186/s13326-017-0143-z
Recently, for miRNAs identification, machine learning
techniques have been widely used. Sequence composition
and structural conformation features are applied to train
the learning system, then the classifiers employ multiple
features to obtain the final prediction. Xue et al. discov-
ered the significant difference of local contiguous sub-
sequence between real and pseudo miRNAs. Therefore,
they applied the three-character group local structure-
sequence features to describe the samples, and based on
SVM they proposed the triplet-SVM to identify novel
miRNAs and miRNAs from specific species [13]. Zhao et
al. employed parallel triplet local structure-sequence fea-
ture, however they chose the first nucleotide of the con-
tiguous triplet group as the local structure-sequence
feature, and add two MFE related features and two
nucleotide pairing features, then apply the SVM classifier
PMirP [14]. Jiang et al. added MFE and P-value as the fea-
tures based on the feature set in [13], and proposed the
classifier Mipred based on RF method [15], Limin Jiang. et
[16] applied BP neural network to identify real and pseudo
pre-miRNAs, and proved the superiority of BP neural net-
work by comparing with triplet-SVM?RF methods.
Neural network [17] and other classifiers of data driv-
ing tend to occur overfitting phenomenon. BP neural
network is a widely used classification algorithm, it has
strong self-learning ability and is particularly suitable for
solving internal mechanism problems. However, the
algorithm tends to be overfitting and the output is un-
stable. The boosting algorithm [18] integrates multiple
weak classifiers to obtain a strong classifier and avoid
overfitting phenomenon. Freund [19] promoted the
boosting algorithm to Adaboost(adaptive boosting) so
that the new algorithm can be more suitable for practical
applications.
Therefore, in this study, we proposed BP-Adaboost
algorithm to establish multiple BP neural network classi-
fiers and distribute the weights of classifiers through
Adaboost framework. Eventually a strong classifier with
high accuracy is obtained.
Methods
Feature extraction
N-gram frequency
In the recent years, for pre-miRNA identifying, studies have
shown that the local primary sequence is crucial to the pre-
miRNA sequence [20]. Therefore, the n-gram frequency is
the most commonly used feature in the primary sequence
feature selecting [21, 22]. However, there is still no exact
criteria for choosing the value of n. Thus, n is often chosen
by comparing the effect of n-gram frequency with different
n-values. In this study, we chose n as 3. Thus, for a cer-
tain sequence, there are 64(43) combinations in a
triple-nucleotide group, then we computed the frequency
occurrence of these 64 combinations in the sequence.
Energy characteristics features
Some studies showed that the minimum free energy
(MFE) indicates the stability of a secondary structure. The
real pre-miRNA sequences have a lower minimum free
energy than that of the randomly generated pre-miRNA
sequences. Therefore, the minimum free energy of a pre-
miRNA sequence is also considered as a feature in distin-
guishing the pre-miRNA sequences. RNAfold is used to
compute the MFE value of a secondary structure.
Structural-diversity based features
The base-pair of nucleotide in the sequences is also a re-
markable characteristic in distinguishing real and pseudo
pre-miRNAs. The traditional nucleotide pairing are A-U
pairing and C-G pairing, but in pre-miRNA sequences
there are also other forms of nucleotide pairing, such as
the G-U pairing. Therefore, in this study, the G-U
pairing is also included as one of the features.
Triple structure sequence
To highly specify the primary sequence features, the sec-
ondary structure is also a significant feature. The software
RNAfold is employed to calculate the potential secondary
structure. In the predicted secondary structure, there are
two states for each nucleotide of the sequence, matched
or non-matched, indicated by brackets ( or ), and dots ..
In this study, the two brackets are not distinguished,
which means every ) is replaced by (. For any three nu-
cleotides groups, there are 8 (2×3) possible characters
combinations, including (((, ((., (.(, (.., .((, .(., ..( and .
Considering the first nucleotide of the three characters
group, then there are 32 (4 × 8) different combinations,
which are denoted as A(((, U(.., etc. For a given sequence,
the 32 dimensional feature vector is sufficient information
for miRNA identification. Then the calculated 32D feature
is employed to train the classifier.
Using the feature extraction methods which we
mentioned above, we can extract 98D features from any
pre-miRNA sequence in total.
First we integrate the obtained pre-sequence into pri-
mary sequence and secondary structure sequence. For
the primary sequence, we choose the n-gram parame-
ter(n = 3) and extracted 64 dimensional feature. In
addition to calculating the potential structure, the soft-
ware RNAfold is applied to predict the second structure
sequence. In this structure, we extracted 32 dimensional
features according to the triple structure sequence. We
also extract the energy characterization MFE as a feature
of pre-miRNA sequence. The possible nucleotide pairing
G-U is included as the last feature. Therefore, altogether
we extracted 98 features.
The Flow chart of Feature extraction is illustrated
in Fig. 1.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 54 of 79
Methods and framework
BP-Adaboost
Due to BP neural network tends to be caught in
overfitting phenomenon and unstable output, in this
study, we proposed a new method BP-Adaboost
based on BP neural network. We employed BP
neural network as a weak classifier to establish mul-
tiple classification model by training repeatedly. Fi-
nally, a strong classifier is obtained after adjusting
the weights through Adaboost.
The framework is shown in Fig. 2.
First, we establish N BP network classifiers by the
extracted features and their corresponding labels. While
training and establishing classifiers, each classifier will
get a corresponding weight. In the end, we obtained a
strong classifier by combining these N weight-distributed
classifiers.
The construction of BP neural network classifier
To accomplish the construction of classifier, we also
need to set the various parameters besides the ob-
tained features and the corresponding labels.
First, we need to choose the number of nodes in the
hidden layer, since there is no specific criterion at
present, we choose the number through the empirical
formula as followed.
M ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiN þ Lp þ a ð1Þ
The number of nodes(M) equals to a constant(a ?
[1, 10]) plus the square root of the number of fea-
ture dimensions(N) plus the output(L). In this study,
N = 98, L = 1. From the formula above, in this
study, we choose M = 12.
After setting the number of nodes (12) and hidden
layers(3) of the BP neural network, the structure of
BP network is 9812-1. Then set the paramaters
(Epochs, The learning rate, Error bounds) and func-
tions(Performance function, Transfer function of
hidden layer nodes, Transfer function of output
nodes,The training function) of BP network. The
parameters and functions of BP network are listed
in Table 1.
Method process
For a given set of multiple classification training data
T = {(x1, y1), ? , (xN, yN)}, the input data xN ? X R
n,
with an arbitrary integer label yN. First, initialize the
weight distribution, the initial weight of each sample
is 1/N. Then train the sample to get the first classi-
fier, and reduce the weight of the correct classifica-
tion samples while raising the weight of improper
classification samples. By the statistics of the weights
Fig. 1 Flow chart of Feature extraction
Fig. 2 Frame of BP-Adaboost
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 55 of 79
of improper classification samples, the weights of cor-
responding classifiers are obtained. Repeat the process
above, we can get multiple classifiers and the corre-
sponding weights, then the ultimate strong classifier
are obtained. The method process is as followed,
Results
Data description
The dataset of pre-miRNAs was downloaded from http://
bioinf.sce.carleton.ca/SMIRP [23]. There are 7 species
samples in the data set and each species has both a posi-
tive sample set and a negative sample set.
These 7 species are Anolis carolinensis?Arabidopsis
lyrata?Arabidopsis thaliana?Drosophila melanogas-
ter?Drosophila pseudoobscura?Epstein barrvirus?Xeno-
pus tropicalis. As each species has a negative sample set,
altogether we obtain 8 classes of pre-miRNAs, one of them
is pseudo pre-miRNAs. The total number of the gene se-
quences of the whole data set is 12,846, among them 9264
sequences are pseudo pre-miRNAs, and the rest 3582 of
them are true pre-miRNAs.
In this article, we distinguish the real pre-miRNAs from
the pseudo ones before classifying these 7 species. V-fold
cross-validation with moderate computational complexity
is widely used for model selection. Usually, a value of V
between 5 and 10 is selected based on experience. In this
study, V = 10. First, the 12,846 sequences are randomly
divided into 10 groups, and choose 9 of them to be train-
ing samples. The last one is tested as the testing set for a
total of 10 training times. The final statistical results are
averaged.
Evaluation criteria
The four kinds of prediction results are true positive
(TP), false positive (FP), true negative (TN), and false
negative (FN). Many evaluation indicators can be used
for the classification results. First, the accuracy rate
(ACC) is the proportion of the correct classification.
Precision and recall are common used evaluation criteria
in pattern recognition, precision represents the propor-
tion of true positive samples of the classified positive
samples, and recall represents the proportion of cor-
rectly classified positive samples of the whole positive
samples, specificity represents the proportion of the cor-
rectly classified negative samples of the whole negative
samples, the computational formula is as follows,
ACC ¼ TP þ TN
TP þ FP þ TN þ FN ð2Þ
precision ¼ TP
TP þ FP ð3Þ
recall ¼ TP
TP þ FN ð4Þ
specificity ¼ TN
TN þ FP ð5Þ
The authentic classification of pre-miRNAs
In this study, the label of pseudo pre-miRNAs is 0,
and the label of real pre-miRNAs is 1(for all
species).
Figure 3 shows the curves of the four error metrics for
10 experiments, blue dot-solid line is the ACC curve,
purple dotted line is precision curve, red solid line is
Table 1 Parameters and functions of BP neural network
Setting items The value set
Epochs 50
The learning rate 0.1
Performance function MSE
Error bounds 0.01
Transfer function of hidden layer nodes Tansig
Transfer function of output nodes Purelin
The training function Trainlm
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 56 of 79
recall curve, and black dash-dotted line is specificity
curve. It is observed that the fluctuation of the precision
curve is relatively large, and the rest three curves are
more stable.
The error statistics of the average results of 10 experi-
ments are shown in the Table 2. The table shows that
BP-Adaboost algorithm is superior to other 4 algorithms
in these 4 accuracy assessment, and Naïve Bayes is the
worst. The accuracy of BP-Adaboost algorithm reaches
98.22%, and this represents the superiority and effective-
ness of this novel method we proposed in distinguishing
real and pseudo mi-RNAs. The table also shows the ac-
curacy of BP neural network is only second to BP-
Adaboost algorithm, and thats the reason why we
choose the BP neural network combined with Adaboost
algorithm. Due to the randomness of BP network, by
weighting multiple classifiers to obtain the final results
effectively improves the classification accuracy and
stability.
Species classification of pre-miRNAs
The Fig. 4 shows the classification results from the
statistic of each classified real pre-miRNA sequence.
There are 7 species in the graph, red bar shows
the true number of the species, blue bar shows the
correctly classified number. From the graph we can
tell the accuracy of the classification of some species
are not ideal, the reason is the number of pseudo
pre-miRNA sequences is large. In the cross valid-
ation, real pre-miRNA sequences of the samples are
not enough, so the number of real pre-miRNA se-
quences is smaller after the classification. Therefore,
the samples of some kind of species are more likely
not enough.
The classification accuracy of each species is shown as
Table 3.
It can be seen from the table that the accuracy of BP-
Adaboost algorithm is superior to other algorithms, al-
though the accuracy of the rest 4 algorithms is higher in
some specific species, the accuracy of the algorithm in
this study is the highest in total.
Discussions
In this paper, we proposed a new method to identify the
pre-miRNAs. We use the Adabost algorithm to generate
ten BP classifiers to finish the identification. It can pro-
vide a new thinking of solving the problem of miRNAs
identification. We use several original algorithms to
compare with our method, and found that our method
can achieve better performance than them. Although the
method can fully play the generalization of BP, and make
the whole method hardly over-fit, it still has the prob-
lems such as: its performance is not ideal in solving
multi-classification problem with imbalanced samples,
its training time is longer than normal BP algorithm. In
the respect of experiment, the method should be tested
in many other data sets to verify the effectiveness of
BP-Ababoost.
Fig. 3 Results of 4 error standards of ten experiments
Table 2 Comparison of the BP-Adaboost with alternative
models
Algorithm ACC Precision Recall Specificity
BP-Adaboost 0.9822 0.9576 0.9797 0.9830
BP 0.9541 0.9429 0.9736 0.9800
Random Forest 0.9336 0.9270 0.9744 0.9772
Naïve Bayes 0.7026 0.4831 0.9721 0.5987
SVM 0.8811 1 0.5729 1
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 57 of 79
Conclusions
The identification of miRNAs is significant for human to
study its function and understand its network regulation
mechanism, discovering more novel miRNAs can also
promote the prediction of miRNA target genes and the
development of new drugs. In this study, we proposed a
method combined BP neural network with Adaboost al-
gorithm, it can effectively overcome the defects of un-
stable output and overfitting phenomenon, our method
obtained a strong classifier by integrating multiple week
classifiers (BP neural network classifiers) and distribut-
ing the weights to them. The data set of traditional clas-
sification of real and pseudo pre-miRNA sequences
combined the real and pseudo sequences of one species
together, in this study, we combined the real and pseudo
sequences of 7 different species together, which in-
creased the diversity and difficulty of the classification.
In the end, we obtained a high accuracy identification
result of real and pseudo pre-miRNAs. Beyond that, in
this study we also classified 7 different species from pre-
miRNAs which is the part that few people are paying
attention to. Due to that, the sample data is not enough,
though the accuracy of our classifier is higher than other
methods, but the overall classification result is still need
to be proved. However, the method we proposed is still
able to provide guidance for the miRNA identification.
Abbreviations
ACC: Accuracy rate; BP-Adaboost: Back Propagation neural network fused
with Adaboost algorithm; FN: False Negative; FP: False Positive; miRNA: microRNA;
MSE: Mean Square Error; RF: Random Forest algorithm; SVM: Support Vector
Machine algorithm; TN: True Negative; TP: True Positive
Acknowledgments
Yang Hu, Zhiyan Liu and Liang Cheng are the corresponding author. Tianyi
Zhao, Ningyi Zhang and Ying Zhang are the co-first author.
Funding
This work was supported by the National Natural Science Foundation of
China (No: 61,571,152 and 61,502,125, $2000), the National High-tech
R&D Program of China (863 Program, $2500) [Nos: 2014AA021505,
2015AA020101, 2015AA020108], the National Science and Technology
Major Project [Nos: 2013ZX03005012 and 2016YFC1202302, $1000],
Heilongjiang Postdoctoral Fund (Grant No. LBH-Z15179, $800), and China
Postdoctoral Science Foundation (Grant No. 2016 M590291, $500).
Availability of data and materials
The dataset of pre-miRNAs was downloaded from http://bioinf.sce.carleton.ca/
SMIRP
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Authors contributions
TZ and NZ implemented the first version of the BP-Adaboost. JR, PX, ZL, LC
updated the algorithm. YZ and YH wrote the manuscript. All authors read
and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Fig. 4 Results of 7 species classify by BP-Adaboost
Table 3 Accuracys comparison of the BP-Adaboost with
alternative models in 7 species
Species BP-Adaboost BP RF Naïve Bayes SVM
Anolis carolinensis 0.66 0.10 0.78 0.69 0.14
Arabidopsis lyrata 0.39 0.25 0.53 0.21 0
Arabidopsis thaliana 0.45 0.23 0.67 0.54 0
Drosophila melanogaster 0.61 0.20 0.51 0.75 0.21
Drosophila pseudoobscura 0.79 0.35 0.31 0.41 0.14
Epstein barrvirus 0.42 0.26 0.24 0.06 0.10
Xenopus tropicalis 0.68 0.43 0.45 0.07 0
Total 0.57 0.29 0.51 0.30 0.22
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):30 Page 58 of 79
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.
Author details
1Department of Computer Science and Technology, Harbin Institute of
Technology, Harbin 150001, Peoples Republic of China. 2Department of
Pharmacy, Heilongjiang Province Land Reclamation Headquarters General
Hospital, Harbin 150088, China. 3School of Life Science and Technology,
Harbin Institute of Technology, Harbin 150001, Peoples Republic of China.
4College of Bioinformatics Science and Technology, Harbin Medical
University, Harbin 150001, China.
Published: 20 September 2017
RESEARCH Open Access
Disease Compass a navigation system for
disease knowledge based on ontology and
linked data techniques
Kouji Kozaki1* , Yuki Yamagata2, Riichiro Mizoguchi3, Takeshi Imai4 and Kazuhiko Ohe4
Abstract
Background: Medical ontologies are expected to contribute to the effective use of medical information resources
that store considerable amount of data. In this study, we focused on disease ontology because the complicated
mechanisms of diseases are related to concepts across various medical domains. The authors developed a River
Flow Model (RFM) of diseases, which captures diseases as the causal chains of abnormal states. It represents causes
of diseases, disease progression, and downstream consequences of diseases, which is compliant with the intuition
of medical experts. In this paper, we discuss a fact repository for causal chains of disease based on the disease
ontology. It could be a valuable knowledge base for advanced medical information systems.
Methods: We developed the fact repository for causal chains of diseases based on our disease ontology and
abnormality ontology. This section summarizes these two ontologies. It is developed as linked data so that
information scientists can access it using SPARQL queries through an Resource Description Framework (RDF) model
for causal chain of diseases.
Results: We designed the RDF model as an implementation of the RFM for the fact repository based on the
ontological definitions of the RFM. 1554 diseases and 7080 abnormal states in six major clinical areas, which are
extracted from the disease ontology, are published as linked data (RDF) with SPARQL endpoint (accessible API).
Furthermore, the authors developed Disease Compass, a navigation system for disease knowledge. Disease Compass
can browse the causal chains of a disease and obtain related information, including abnormal states, through two
web services that provide general information from linked data, such as DBpedia, and 3D anatomical images.
Conclusions: Disease Compass can provide a complete picture of disease-associated processes in such a way that
fits with a clinicians understanding of diseases. Therefore, it supports user exploration of disease knowledge with
access to pertinent information from a variety of sources.
Keywords: Disease ontology, Definition of diseases, River flow model of disease, Linked data, Navigation system
Background
Recently, medical information resources that store consid-
erable amount of data have become available. Semantic
technologies are expected to contribute to the effective
use of such information resources, and medical ontologies
such as the Systematized Nomenclature of Medicine-
Clinical Terms (SNOMED-CT, http://www.nlm.nih.gov/
research/umls/Snomed/snomed_main.html main.html)
and the Ontology of General Medical Sciences (OGMS)
[1] have been developed to realize sophisticated medical
information systems. Although medical ontologies consist
of various domains, such as diseases, anatomies, drugs,
and clinical information, disease is a particularly import-
ant concept because diseases have complicated mecha-
nisms that are deeply related to concepts across many
medical domains. Therefore, in this study, we have fo-
cused on a disease ontology.
Although disease ontologies such as the Human Disease
Ontology (DOID) [2] and the Infectious Disease Ontology
(IDO) [3] exist, they primarily focus on the ontological
* Correspondence: kozaki@ei.sanken.osaka-u.ac.jp
1The Institute of Scientific and Industrial Research, Osaka University, 8-1
Mihogaoka, Ibaraki, Osaka 567-0047, Japan
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 
DOI 10.1186/s13326-017-0132-2
definition of a disease with related properties, i.e., static
aspects of diseases are the main concern. While the
OGMS provides an ontological representation model of
disease disposition, it does not capture a complete picture
of disease-associated processes.
In contrast, we proposed a definition of a disease that
captures the causal chain of the abnormal states in a
computational model known as the River Flow Model
(RFM) of diseases [4, 5]. Our disease ontology consists
of rich information about these causal chains, which
provides domain-specific knowledge about diseases and
answers questions such as What disorder/abnormal
state causes a disease? or How might the disease ad-
vance, and what symptoms may appear? Consequently,
we believe that the ontology could be a valuable know-
ledge base for advanced medical information systems.
In this paper, we discuss a fact repository for causal
chains of disease based on our disease ontology. It is de-
veloped as linked data so that information scientists
can access it using friendly SPARQL queries through
an RDF model for causal chain of diseases. We de-
signed the RDF model as an implementation of the
RFM for the fact repository while ontological defini-
tions of the RFM are discussed in our previous work
[4, 5]. It provides known knowledge about mechanism
of disease to support education for novice clinicians,
differential diagnosis, decision making for medical
treatment and so on.
In this paper, we also describe Disease Compass, a
navigation system for disease knowledge based on the
RDF model of our disease ontology. The system has two
special features. First, users can browse disease know-
ledge according to the causal chains of diseases defined
in the disease ontology. Second, users can obtain related
information about the selected disease from linked data
sources. Thus, Disease Compass helps users make sense
of disease knowledge from various relevant sources.
The remainder of this paper is organized as follows.
The methods used to develop our disease ontology
and navigation system are introduced in Methods sec-
tion. In Results section, we describe the disease ontol-
ogy in detail and discuss how it can be published as
linked data and Disease Compass. In Discussion sec-
tion, we discuss our contributions from the perspec-
tive of the ontological definition of disease and
medical information systems. Conclusions and sugges-
tions for future work are presented in Conclusions
section.
This paper is an extended version of the conference
paper presented in ICBO2015. It is mainly added the fol-
lowing 3 topics; 1) details of integration of biomedical
abnormal states discussed in Integration of biomedical
abnormal states section, 2) details of development of
Disease Compass, especially about its navigation
function for general causal chains, and 3) Some concrete
examples of how the system is used with discussions.
Methods
We developed the fact repository for causal chains of
diseases based on our disease ontology [4, 5] and abnor-
mality ontology [6, 7]. This section summarizes these
two ontologies.
Definition of a Disease
Basic definition
Based on the RFM, we define a disease as follows [4].
Definition 1
A disease is a dependent continuant constituted of one
or more causal chains of clinical disorders (abnormal
states) that appear in a human body and is initiated by
at least one disorder.
In this definition, by clinical disorders (abnormal
states), we mean states in a human body which consists
of causal chains of a disease. They are associated with
dysfunctional or otherwise pathological functioning in
organisms while they may not be abnormal in some
cases. They are defined in our Abnormality Ontology [7]
as mentioned in Abnormality Ontology section.
Then, what is a causal chain of disorders? Although it
looks like a process, it is a dependent continuant. It is
possible to compare a causal chain of disorders to a
waterfall, river flow, or a forest fire. Here, we show how
a disease is a dependent continuant rather than a
process. The following is an informal account of our
view. This topic is extensively discussed with ontological
definitions of related concepts in the literature [5].
A causal chain is composed of one or more pairs of
entities, such as a causal event and an effect event, in
which the latter has been caused by the former. In the
case of multiple-pair chains, the effect becomes another
cause that causes another effect. What makes clinical
causal chains special is that causal entities are usually
still active when the effect entity has been caused.
Therefore, the two entities overlap in temporal space. In
the case of continuant entities, by overlap we mean
that the intervals of active states of neighboring continu-
ants overlap, i.e., the causal continuant maintains its
state when the effect state has been caused.
Let us examine how well a flowing river matches a
causal chain of a disease. The river itself enacts branch-
ing, changing shape, extending, diminishing, etc. A river
could be created when a lake overflows, e.g., after a
heavy rainstorm. Initially, the flow is minimal and,
potentially, temporary. Here, overflow from a lake would
correspond to an etiological disorder in a clinical causal
chain. If the initial flow increases, the water extends in
length and is recognized as a river. After emerging as a
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 2 of 18
river (as a disease), it extends further to another lake or
to the sea. While extending, it branches (branching can
be the appearance of another disorder). Eventually, the
river may dry up due to climate change (cure). Thus, the
life of a river corresponds well to the life of a disease.
Note that a river is defined as an enactor of those pro-
cesses, and Definition 1 suggests that a disease is defined
as an enactor of its manifestation process. Thus, in con-
cordance with OGMS, both a river and a disease are
continuants; however, a river is an independent continuant
and a disease (causal chain) is a dependent continuant that
depends on a bearer, i.e., an organism.
This informal observation is supported by ontological
accounts of processes and objects [8]. Although we omit
details because of space limitations, we present the ana-
logy to support the definition of a disease as a dependent
entity of a new type that differs from both a disposition
and a process.
Granularity
We do not specify any particular granularity of disorder
and causal chains because we believe granularity should
be determined flexibly according to the necessity of de-
scription of each disease. That is, we define diseases
based on the most agreeable medical knowledge at this
time because the current medical knowledge changes
as time goes. However, with regard to the original
cause, we should trace the causal chain back to the
cell level rather than to the genome level. When we
define diseases generally, granularity is not an issue;
however, it matters when we define a particular dis-
ease in the ontology.
In addition, we do not impose specific time resolution
on the causal processes, so that, if necessary, we can in-
clude rapid processes, such as fractures. After receiving a
strong external pressure, a bone undergoes a very quick
destruction process resulting in fracture. The causal
process can be captured by much finer time resolution
than those involved in ordinary pathological processes
captured at the clinical level. Fractures can be handled by
the disease model discussed in the next section.
Related work
Here we compare the definitions of diseases in the
OGMS and RFM.
(1)Dispositions are introduced in the course of disease
development in the human body. A disposition is a
potentiality. In the current OGMS, realization of this
potentiality takes the form of chains of physical/
physiological changes. Thus, disease and disease
course are distinguished, that is, the former is
dependent continuant and the latter a process. We
believe this use of disease is counterintuitive to
clinicians; thus, we propose a disease definition that
allows the disease to be understood as a causal chain
of abnormal states.
(2)Consider how a particular disease is identified. For
example, when explaining diabetes, OGMS refers to an
elevated level of glucose in the blood. However, it
provides an insufficient account of why the explanation
of diabetes must include elevated level of glucose.
What role does this elevated level play in diabetes?
Why must elevated level of glucose in the blood be
included for diabetes but nothing else? It must be
something specific to the disease of interest, i.e., each
realization of the disease must involve an entity of this
sort. For OGMS, emphasis is placed on the disposition
and the disorder (a certain disordered body part) in
which this disposition inheres. We believe that the
reference to elevated glucose level suggests a need for
an additional entity, which is included in our disease
model. Thus, we introduce the notion of core causal
chain, which roughly corresponds to so-called main
pathological/etiological condition(s).
We know that defining such an entity type, i.e., causal
chains, discussed in (2) is difficult because such causal
chains are not always definite for each disease because
they vary from one patient to another. Hence, OGMS use
of disposition is a mere potentiality. In the case of latent
diabetes, for example, there is no elevated level of glucose
in the blood, although there is a disposition thereto. Ac-
cordingly, for latent diabetes, we follow OGMS in recog-
nizing the need for something other than just elevated
level of glucose in the blood. However, we think that
something more is requiredsomething that is essential
for each particular disease. In the case of diabetes, this
would be the deficiency of functioning of insulin, because
this must have occurred for all patients who suffer from
diabetes. To address this issue, we draw on OGMS notion
of homeostasis and introduce the term disturbance of
homeostasis to explain what we consider as the essential
core of each disease. Disturbance of homeostasis can be
caused through the concretization of a disposition, or it
can be caused by some outside trigger, e.g., an injury.
We agree with OGMS in that a disease is a dependent
continuant, and its definition is expected to address the fol-
lowing conditions: (1) the existence of its pre-clinical mani-
festation, (2) the fact that it can cause another disease, and
(3) variation in the disease course from patient to patient
[1]. We have attempted to find a disease definition that sat-
isfies these conditions using an RFM [4].
Abnormality Ontology
Three-Layer Ontological Model of Abnormal States
Here we discuss the abnormal states used in our disease
ontology to define diseases. The reliability and utility of
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 3 of 18
the disease definitions are considerably dependent on
the quality of the abnormal states. To develop abnormal
states consistently, we have developed an abnormality
ontology [6, 7] with a three-layer structure:
 Level 1: Generic abnormal states
Level 1 defines very fundamental (or generic)
concepts, which do not depend on any structural
entity, i.e., object independent states. They are
commonly found in several objects, and can be
usable in several domains besides medicine, such as
machinery, materials, and aviation.
 Level 2: Object-dependent abnormal states
Level 2 has been developed by identifying the target
object and specializing generic abnormal states at
Level 1 with consistency. The top level concepts at
Level 2 are dependent on generic structures, such as
wall-type structure, tubular structure, and
bursiform structure, which are common and are
used in several domains.
 Level 3: Specific context-dependent abnormal states
Level 3 consists of context-dependent abnormal
states, which refer to the Level 2 abnormal states to
define them, and are specialized into specific
disease-dependent ones.
Level 1 defines very fundamental and generic con-
cepts, e.g., small in area, hypofunction, etc., which
are commonly used in clinical medicine and other do-
mains. Therefore, they do not have a target entity which
has the abnormal state. Level 2 concepts are dependent
on objects. In the lower level of the tree, concepts are
designed to represent abnormalities at specific human
organ/tissue/cell levels. For example, by specifying small
in area at Level 1, tube narrowing, where the cross-
sectional area of a tubular structure has become nar-
rowed, is defined at Level 2. This is further specified in the
definitions vascular stenosis (blood vessel-dependent),
arterial stenosis, coronary artery stenosis (coronary
artery-dependent), and so on. Level 3 concepts are cap-
tured as specific disease-dependent (context-dependent)
abnormal states. For example, coronary artery stenosis
at Level 2 is defined as a constituent of ischemic heart
disease at Level 3. In the proposed ontological ap-
proach, common concepts can be kept distinct from
specific concepts and can be defined appropriately ac-
cording to their context.
Representation of Abnormal State
In medicine, abnormal states are interpreted from the di-
verse perspectives of specialists, such as clinicians, pathol-
ogists, biologists, and geneticists, and correspondingly a
variety of representations of abnormal states are used.
Therefore, we have classified abnormal states into three
categories: a property (e.g., hypertension), a qualitative
representation (e.g., blood pressure is high), and a quanti-
tative representation (e.g., blood pressure 180 mm Hg).
Their interdependence is formulated in a Property-
Attribute interoperable representation framework for
abnormal states we proposed in previous work [6, 7].
We capture all abnormal states as properties repre-
sented by a tuple: <Property (P), Property Value (Vp)>,
e.g., <stenosis, true>. Apparently, any state requires tem-
poral specification as well as its bearer. For simplicity,
these temporal indexes are omitted. However, the bearer
is specified to represent the fact that it is in a state, as dis-
cussed below. We specify the property by decomposing it
into a tuple: <Attribute (A), Attribute Value (V)>. The
Attribute Value can be either a Qualitative Value (Vql) or
a Quantitative Value (Vqt). For example, arterial stenosis
is decomposed into < cross-sectional area (A), small
(Vql) > as a qualitative representation, or < cross-sectional
area (A), 5 mm2 (Vqt) > as a quantitative representation.
Then, we introduce Object to identify the target
object, and we represent an abnormal state as a triple:
<Object (O), Attribute (A), Value (V)>. This is the basic
form of abnormalities in our representation model. In
addition, we introduce Sub-Object (SO) as an ad-
vanced representation of what will be focused on. For
example, in the case of hyperglycemia, since the glu-
cose concentration (A) means the ratio of the focused
object (SO) relative to the whole mixture (O), the repre-
sentation of hyperglycemia is a quadruple, <blood (O),
glucose (SO), concentration (A), high (V)>. In an ad-
vanced representation, colonic polyposis is described
as < colon (O), polyp (SO), number (A), many (V) > .
Our model can deal with both clinical test data and
abnormal states in disease definitions. Clinical test data
can be represented in the form <Object (O), Attribute
(A), Quantitative Value (Vqt) > (OAVqt), which can be
converted into a property representation form <Object
(O), Property (A), Property Value (Vp) > (OPVp) via a
qualitative representation form. For example, in terms of
the state of hypertension, our model ensures interoper-
ability among the forms < blood (O), pressure (A),
180 mmHg (Vqt)>, <blood pressure, high>, and < hyper-
tension, true>. Therefore, our model realizes interoper-
ability between test data and abnormal states in the
definition of diseases. The ontological foundation for the
concepts discussed thus far is given by an upper ontol-
ogy, i.e., YAMATO [9].
Ontology Editing
We used the Hozo (http://www.hozo.jp) [10] ontology
editing tool. Hozo is based on an ontological theory of
role [11] and has a sophisticated graphical user interface.
Although Hozo uses a proprietary ontology format based
on XML, it can export ontologies in Web Ontology
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 4 of 18
Language (OWL) [12]. An API for use the Hozo format
is also available at the website of Hozo. We also devel-
oped a graphical tool that allows clinicians to edit a dis-
ease definition intuitively without prior knowledge of
ontology construction. The tool can export disease defi-
nitions in the Hozo ontology format.
System Development based on Linked Data Technology
There are several approaches for system development
based on ontologies. A typical approach is to use APIs
for ontology processing. Because our disease ontology is
constructed using Hozo, we can develop application sys-
tems using APIs for Hozo ontologies. We can also use
the OWL API because Hozo has an OWL export func-
tion. However, linked data technology is particularly effi-
cient for developing applications across multiple datasets
on the web. Therefore, we adopted an alternative ap-
proach to publish the disease ontology as linked data so
that it can be used to develop an application system easily.
At the same time, the schema of RFM is published at
http://rfm.hozo.jp/ the Hozo format and the OWL format.
If the users are familiar with OWL, they can use the OWL
version in spite of our disease ontology is currently pub-
lished only as Linked Data through its SPARQL endpoint
(see Disease ontology as linked data section).
Results
Computational Model of Diseases
Core causal chain of a disease
Based on the disease ontology based on RFM, we build a
computational model of diseases to make it easier to
define particular diseases. In the following, we divide
diseases into (Type 1) those whose etiological and
pathological processes are well understood and (Type 2)
other diseases.
Type 1 diseases are identified by their inherent etio-
logical/pathological process(es). Type 2 diseases include
so-called syndromes and are typically represented in
terms of criteria for diagnosis. We deal with Type 1 dis-
eases first. Note that every Type 1 disease should have a
clue to identify the disease. In other words, we should
be able to find something similar to the so-called main
pathological/etiological condition(s) that theoretically
characterize(s) the disease. As stated above, this is what
OGMS should include. We know that Type 2 diseases
necessarily employ criteria for diagnosis to identify them
because of a lack of knowledge about their etiological/
pathological processes. However, this does not mean
Type 2 disease is excluded from our disease model as
discussed below, which we share with OGMS.
We also need a formulation to organize diseases in an
is-a hierarchy in a disease model. According to our def-
inition, a disease can be represented as a directed graph
consisting of disorders as nodes and causal links. An is-a
relation between diseases using an inclusion relationship
between causal chains can be described as follows.
Definition 2: Is-a relation between diseases
Disease A is a super class of disease B if all causal chains
at the class level of disease A are included in those of
disease B. The inclusion of nodes (disorders) is deter-
mined by considering an is-a relation between the
nodes, as well as the sameness of nodes.
Definition 3: Core causal chain of a disease
The causal chain of a disease included in the chains of
all its subclass diseases is called the core causal chain.
Definition 3 helps us capture the necessary and suffi-
cient conditions of a particular disease systematically,
which roughly corresponds to the so-called main patho-
logical/etiological conditions. Figure 1 shows one of the
main types of diabetes constituted by corresponding
types of causal chains. The most generic type in this ex-
ample is (non-latent) diabetes, which is constituted by
the following chain:
deficiency of insulin? elevated level of glucose in the
blood.
The next lower subclasses include Type-I diabetes,
which is constituted by:
destruction of pancreatic beta cells? lack of insulin I in
the blood? deficiency of insulin? elevated level of
glucose in the blood,
and steroid diabetes, which is constituted by:
long-term steroid treatment?? deficiency of
insulin? elevated level of glucose in the blood.
If a doctor wanted a hierarchy that represents diabetes-
caused blindness, it would be:
deficiency of insulin? elevated level of glucose in the
blood?? loss of sight.
Although we explain the disease model using Type 1
diseases, the model is also applicable to Type 2 diseases
because of the flexibility of granularity and degree of
being well-understood. These two kinds of flexibility
can be exploited according to each disease. In the case
of Type 2 diseases, we could employ an unknown
causal node linking to just a few of the symptoms that
are typically observed in the syndrome under consider-
ation. Note that this model can capture a seemingly iso-
lated symptom by combining it with an unknown cause
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 5 of 18
to form a causal network. It also captures diseases with
multiple causal chains.
In addition, the proposed model can distinguish, for ex-
ample, between diabetes with blindness and diabetes-
driven blindness by specifying the core causal chain of
focus. In summary, the disease model yielded by the pro-
posed definition of disease (Definition 1, Disease ontology
as linked data section) covers a wide range of diseases. In
fact, we have constructed models of 6051 diseases from 12
different divisions in our ontology, which shows the ex-
pressive power of the proposed disease model.
Types of causal chains in disease definitions
In theory, when we define a disease, we can consider
three types of causal chains that appear in the definition
of disease.
General Causal Chains are all possible causal chains
of (abnormal) states in a human body. They can be
referred to by any disease definition.
The Core Causal Chain is a causal chain that appears
in all patients that have the disease.
Derived Causal Chains are causal chains that are ob-
tained by tracing general disease chains upstream or
downstream from the core causal chain. Upstream chains
imply possible causes of the disease, whereas downstream
chains imply possible symptoms in a patient suffering
from the disease.
The core causal chain represents a stable definition of
the disease. That is, it defines only such causal chains
that appear in all patients. On the other hand, the gen-
eral causal chains and derived causal chains are not part
of the definition but possible causal chains which might
not appear in some patients. That is, the general causal
chains and derived causal chains represents possibilities
how causal chains could be extended.
Figure 1 shows how the main types of diabetes are
composed of corresponding types of causal chains. The
figure shows that diabetes subtypes are defined by
extending the diseases core causal chain according to its
derived causal chains (upstream or downstream).
Note that it is obviously difficult to define all general
causal chains in advance, because it is impossible to
know all possible states in the human body and their
causal relationships. To overcome this difficulty, we de-
fine general causal chains by generalizing the core/de-
rived causal chains of every disease defined by clinicians
using a bottom-up approach. We asked clinicians to
define only core causal chains and typical derived causal
chains of each disease according to their existing know-
ledge and information that can be found in textbooks.
General causal chains are then defined by generalizing
these definitions.
The scope of the model for disease
In this section, we discuss the scope of the proposed
model of diseases. This paper focuses on how to capture
diseases and the implementation of diseases as causal
chains based on the RFM. As mentioned in Definition of
a Disease section, we assume that ontological definitions
of causal chains, diseases, and abnormalities (disorders)
etc. are out of the scope of this paper since they are dis-
cussed in our previous papers [4, 5].
At first, we have to distinguish the following two prob-
lems when we represent a disease.
1) How we can represent causal chains of diseases as a
computational model
2) How far the current medical knowledge reveals the
causal mechanism of diseases?
Fig. 1 Types of diabetes composed of causal chains
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 6 of 18
This paper focuses on the former and the latter prob-
lem is out of scope. That is, we discuss how the model
capture the current medical knowledge about diseases
based on the RFM.
We here classify difficulties for defining diseases as
causal chains into the following two types;
1) Problems of grain size to describe causal chains.
2) How far we can follow causes of diseases.
For the former, we use the Abnormality Ontology we
proposed in our previous work [7]. It supports multi
levels representations of abnormal states according to
their grain size.
For the latter, we carefully investigated how we should
represent causal chains of diseases through discussions
with medical experts including clinicians. As the result,
we introduced a flexible representation model which we
can describe causal chains according to a range of given
knowledge even if some causes of the disease are un-
known. When causes of an abnormal state is unknown,
we represent it unknown node in the causal chain. It
means that the model can represent causal chains
according to how far we can follow causes of diseases.
On the other hand, when we know that an abnormal
state has several causes, we can also represent it using
multiple chains. Please note that multiple chains can be
represented using AND/OR graphs which is a simple
and basic knowledge representation. Although represen-
tations of AND/OR in OWL are somewhat complicated,
this is why we publish causal chains of diseases as not
OWL but simple RDF graph as discussed in Disease
ontology as linked data section.
Implementing the Disease Ontology
We developed the disease ontology using Hozo.
Although Hozo is based on an ontological theory of
roles and has its own ontology representation model, we
show an OWL representation of the ontology to aid
understandability. Note that we use a simplified OWL
representation of the disease ontology to provide an
overview; however, it does not support the full semantics
of Hozo. The detailed semantics of Hozo are discussed
in the literature [12].
Figure 2 shows an OWL representation of angina pec-
toris, whose causal chain is shown in Fig. 3. Abnormal
states that appear in the disease are listed using the
owl:Restriction properties on the hasCoreState/hasDerives-
State properties. The former represents abnormal states in
its core causal chain, and the latter represents those in its
derived causal chain as defined by a clinician. The causal
relationships among them are represented by hasCause/
hasResult properties. If the probability of the causal rela-
tionship is high, hasProbableCause/hasProbableResult
properties are used instead. However, how this probability
is determined is beyond the scope of this paper. Causal
chains (states and causal relationships among them) in
core causal chains are necessary (Definition 3); therefore,
the owl:someValuesFrom properties are used. On the other
hand, because causal chains in derived causal chains are
possible, owl:allValuesFrom properties are used to repre-
sent the possible causes/results. If there are more than
two possible causes/results, owl:unionOf is used to list
them. The definitions of diseases refer to the definitions of
abnormal states, which represent the possible causes
and results, as shown in Fig. 4. General disease chains
are represented as an aggregation of the definitions of
abnormal states.
Our disease ontology has been developed in collabor-
ation with clinicians from 13 fields, such as cardiology,
Fig. 2 Class definition of angina pectoris in OWL
Fig. 3 Causal chain of angina pectoris
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 7 of 18
neurosurgery, and allergology. As of May 11, 2013, it con-
tained approximately 6302 disease concepts and 21,669
disorder (abnormal state) concepts with the causal rela-
tionships that exist among them. We keep to revise them
mainly focusing on major diseases.
Disease ontology as linked data
Basic policy to publish the disease ontologies as linked data
The standard format for linked data is RDF; thus, one
might consider it easy to publish ontologies in RDF
formats using OWL or RDF(S) as linked data. However,
ontology languages, such as OWL, are primarily designed
for class descriptions, and there is an assumption that the
language will be used for reasoning based on logic. In
contrast, for linked data, finding and tracing connections
between instances is the primary task. Therefore, for con-
venience or efficiency, OWL and RDF(S) are not always
appropriate for linked data because of their complicated
graph structures.
For example, when we obtain a general disease chain,
which is probably caused by myocardial_ischemia, we
must repeat SPARQL queries to obtain RDF graphs,
which include blank nodes, such as those shown in Fig. 5.
Furthermore, when we obtain the definitions of a
disease, we must perform more complicated queries to
obtain graphs that correspond to OWL descriptions,
such as those shown in Fig. 3, with restrictions inherited
from the diseases super classes. These queries and graph
patterns are intuitively very different from the disease
chains we want to produce. It is due to restrictions from
the super classes.
This is problematic, especially when the conceptual
structures of the ontology are intended for use as a
knowledge base with rich semantics. Consequently, we
have designed an RDF data model in order to publish
our disease ontology as linked data [13].
RDF model for causal chains of diseases
Once the disease ontology was constructed, information
about the causal chains of diseases was extracted and
converted into RDF format as linked data. We call this
dataset Disease Chain-LD, and it consists of diseases,
abnormal states, and the relationships among them. Ab-
normal states are represented by instances of the Abnor-
mal_State type, and the causal relationships between
them are represented by hasCause and hasResult, which
are inverse properties. The abnormal states connected by
these properties are a possible cause/result; therefore, gen-
eral disease chains can be obtained by collecting all abnor-
mal states according to these connections.
Diseases are represented by instances of Disease type.
Abnormal states that constitute a core causal chain and
a derived causal chain of a disease are represented by
hasCoreState and hasDerivedState properties, respect-
ively. Is-a (sub-class-of ) relationships between diseases
and abnormal states are represented by subDiseaseOf/
subStateOf properties rather than rdfs:subClassOf be-
cause the diseases and abnormal states are represented
as RDF resources, whereas rdfs:subClassOf is a property
between rdfs:Classes.
Figure 6 shows an example of an RDF representation of
diseases. It represents disease A and its sub-disease disease
B, whose causal chains are shown in Fig. 7. Note that the
causal chains consist of abnormal states and the causal
relationships among them. Therefore, when we obtain a
diseases core causal chain or derived causal chain, we
must obtain both the abnormal states connected to the
disease by hasCoreState/hasDerivedState properties and
the causal relationships among them. Although causal re-
lationships are described without determining whether
they are included in the causal chains of certain diseases,
we can identify the difference of abnormal states which
diseases include by assessing whether the abnormal states
at both ends of hasCause/hasResult properties are con-
nected to the same disease by hasCoreState/hasDerived-
State properties. Furthermore, for a disease that has a
super disease, such as disease B in Fig. 7, in addition to
obtaining the causal chain directly connected with the
Fig. 4 Class definition of myocardial ischemia in OWL
Fig. 5 RDF graph of a general disease chain in OWL
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 8 of 18
disease, we must also obtain the causal chains of the super
disease, and these chains must be aggregated.
We have published the disease ontology as linked data
based on our RDF model. It includes the definitions of
1.554 diseases and 7080 abnormal states in six major clin-
ical areas, which were extracted from the disease ontology
on June 24, 2014. At this point, the dataset contains
61,473 triples. Although the disease ontology includes the
definitions of diseases in 13 clinical areas, we have pub-
lished only those parts that were well reviewed by clini-
cians. A SPARQL endpoint to access the disease ontology
is published at http://lodc.med-ontology.jp/. The users
can find concrete examples of causal chains of diseases in
RDF through this endpoint using SPARQL queries as dis-
cussed in the next section. Furthermore, we also provide
user friendly navigation system for causal chains of disease
as discussed in Development of Disease Compass section.
Example Queries
The processing is not complicated; it requires only sim-
ple procedural reasoning. We can obtain the causal
chains that define a disease through several SPARQL
queries to the dataset [13].
Figure 8 shows example queries to obtain an abnor-
mal state in the dataset. Because all abnormal states
are defined as individual resources of the Abnormal_State
type, we can obtain them using the query shown in (a1).
When we want to obtain the causes/result of a se-
lected abnormal state, we can follow the hasCause/
hasResult properties. For example, (a2) is a query to
obtain all causes of the selected abnormal state. Fur-
thermore, we can obtain a general disease chain that
includes the abnormal state using the query shown in
(a3). This query means to follow all of the hasCause/
hasResult properties recursively from the selected
abnormal state.
On the other hand, Fig. 9 shows example queries to
obtain the definitions of diseases. We can obtain all
diseases in the dataset using query (d1), which is
similar to query (a1). When we want to obtain all
super diseases (super class) of a selected disease, we
can use the query show in (d2). To obtain the core
causal chains or derived causal chains of a selected
disease, we can use query (d3) or (d4), respectively.
By combining queries (d2), (d3), and (d4), we can ob-
tain all causal chains that appear in the definitions of
the disease using the query shown in (d5). Further-
more, when we want to obtain a list of causal rela-
tionships that appear in the causal chain of the
definition of the diseases rather than a list of abnor-
mal states, we can use the query shown in (d6). This
query finds all properties among all abnormal states
that appear in the definition of the selected disease.
We believe that all of the above queries are easy to
understand and intuitive for many people. This is a
significant advantage of our RDF model compared to
our original OWL disease ontology.
Fig. 7 Causal chains of diseases shown in Fig. 2
Fig. 8 Example queries. Here, dont: represents a prefix of the Disease
Chain-LD and < abn_id > and < dis_id > represent the id of a selected
abnormal state and disease, respectively
Fig. 6 RDF representation of disease
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 9 of 18
Integration of biomedical abnormal states
Mapping to Other Resources As illustrated in the pre-
vious section, our ontology provides three levels of
abnormal states, from generic to disease-specific.
Level 1 in our ontology defines generic concepts that
correspond to the PATO concepts [14], and those con-
cepts can be mapped to related PATO concepts (Fig. 10).
The lower Level 2 concepts are human anatomical
structure-dependent abnormal states that correspond to
Human Phenotype Ontology (HPO) concepts [15]. By
creating links between Level 2 concepts and HPO con-
cepts, it becomes possible to navigate from the HPO
concepts to the upper generic PATO concepts. Level 3
provides disease-specific abnormal states, such as myo-
cardial ischemia in ischemic heart disease and chest
pain in angina pectoris. In the revised version 11 of the
International Classification of Diseases (ICD), diseases
contain causal properties information [16]; therefore,
we plan to map our Level 3 concepts to the correspond-
ing ICD concepts. Level 3 abnormal states are described
in the causal chains of diseases [4]. By mapping our dis-
ease concepts of disease ontology to the ICD, ICD users
can understand the causal relationships of the abnormal
states in diseases. Our ontology also allows users to navi-
gate related concepts in other resources, such as HPO
and PATO.
Mapping our ontology to other resources in order to
integrate various data related to abnormalities will also
provide benefits to the users of other resources. First,
one can find concepts from generic to specialized terms
easily by referring to the single is-a tree in our abnor-
mality ontology. For example, although HPO does not
consider consistent is-a relationships in terms of sten-
osis, by referring to arterial stenosis at Level 2 in our
ontology through mapping, HPO users can obtain the fol-
lowing is-a relationships: arterial stenosis is-a vascular
stenosis is-a narrowing tube is-a small in area.1 Since
small area is linked to a PATO concept, via our ontol-
ogy, users might find orthologous concepts of other spe-
cies. In particular, human phenotypes can be linked to the
phenotypes of model organisms, e.g., mouse and rat, if the
set composed of Attribute (A) and Value (V) are identical
and the Object (O) has structural similarity. PATO2YA-
MATO attempts to integrate phenotype descriptions res-
iding in differently structured comparison contexts [17].
By applying PATO2YAMATO, mapping concepts across
species and integrating knowledge from various species
may be possible.
We also plan to link the components of the <Object
(O), Attribute (A), Value (V) > representation of abnormal
states to other resources. For example, we suppose that
Object (O) can be linked to concepts in the Foundational
Model of Anatomy (FMA) ontology [18]. By mapping
medical terminologies, such as SNOMED-CT[19] and
MeSH terms [20], it will be possible to retrieve biomedical
articles related to abnormal states or diseases. It will be
also useful for the users of these terms to understand how
their research subjects are involved in various abnormal
states in the human body relative to diseases.
Trial integration In order to assess the feasibility of
the above approach, we conducted a trial integration of
some examples taken from the three-level ontology of
abnormal states, the disease ontology, and some typical
external resources, in which we used 386 abnormal
states consisting of 279 abnormal states that are super
classes of 107 states that are the bottom level classes
appearing in 12 typical diseases from three medical spe-
cializations, i.e., cardiovascular medicine, neurology, and
gastroenterology. Mapping an abnormal state defined in
the abnormal state ontology to related concepts in exter-
nal resources was performed manually after detecting
candidates using a perfect string match algorithm.
Fig. 9 Example queries to obtain definitions of diseases. In this
figure dont: represents a prefix of the Disease Chain LOD, and <
dis_id > represents the id of a selected disease
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 10 of 18
Table 1 shows the mapping results for PATO, HPO,
MeSH, and SNOMED-CT for each level of abnormal
states. As can be seen in the table, 52 abnormal states in
PATO are mapped to those among 134 states at Level 1
of the abnormal ontology, none in HPO, two in MeSH,
and none in SNOMED-CT. No abnormal state is found
among the 107 states at Level 3 in the four external
sources because they are disease-specific abnormal
states. It is interesting to find that abnormal states at Level
2 have more corresponding states in external resources
than those at other levels. This is because those at Level 1
are too abstract and those at Level 3 are too specific. Some
examples of mapping results are shown in Table 2.
Another interesting finding is that our abnormal
ontology can fill the conceptual gap between abstract
PATO concepts and organ-specific HPO concepts. For
example, mitral valve insufficiency in HPO, which means
an imperfect state of the closure function of the mitral
valve, corresponds to mitral incompetence at Level 2 of
our abnormal state ontology. On the other hand, PATOs
insufficient corresponds to dysfunction at Level 1 of our
abnormal state ontology. Then, these two are connected
via valve incompetence in our ontology. In addition, the
fact that valve incompetence subsumes tricuspid incom-
petence demonstrates that the concepts at Level 2 of our
abnormal state ontology can help find hidden subsump-
tion relations between concepts in PATO and HPO.
Abnormality Ontology as Linked Data
While causal chains of abnormal states are published as
linked data based on the RDF model for the causal
chains of diseases, we export the is-a hierarchy of abnor-
mal states in the abnormality ontology in OWL format
using the OWL export function in Hozo [12] and dir-
ectly publish it as linked data because it does not have
particularly complicated conceptual structures.
Development of Disease Compass
Disease Compass
To exploit the value of a disease ontology as a know-
ledge source for advanced medical information systems,
it is important that the users can navigate the ontology
easily and intuitively according to their interests. Med-
ical experts may not find it easy to use SPARQL queries
to obtain information about disease chains. Therefore,
we have developed Disease Compass as a navigation
system to explore the disease ontology. We designed
the system so that users without experience with
Fig. 10 Integration of abnormal states in biomedicine
Table 1 Mapping between abnormal state ontology and
external resources. Some examples of mapping results are
shown in Table 2
Concepts Our Ontology PATO HPO MeSH SNOMEDCT
Level 1 134 52 0 2 0
Level 2 145 2 27 28 17
Level 3 107 0 0 0 0
Total 386 54 27 30 17
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 11 of 18
ontologies or linked data can easily explore disease
knowledge and related information.
It is available at http://lodc.med-ontology.jp/ (demo
movies are also available as attached files).
System architecture
Figure 11 shows the Disease Compass system architec-
ture. The system obtains disease knowledge from Dis-
ease Chain-LD, which is converted from the disease
ontology. It also has mapping information with other
Linked Open Data (LOD) and web services, and it can
obtain related information through these mappings. Al-
though the system currently has mappings to only
DBpedia and BodyPart3D, it will be possible to extend
mappings to other LOD sources using existing ap-
proaches [21] to generate such linkages.
Technically, the system uses two methods to access
these mapped datasets, i.e., SPARQL queries for linked
data and an API for web services. If related resources
(ontologies and other datasets) are published as LOD,
the system can be extended easily to link such related
information using SPARQL, which is the major benefit
of using linked data techniques. In addition, many
linked datasets include links to other data. For example,
DBpedia includes links to major medical codes, such as
ICD10 and MeSH; thus, the system can follow these
links through mappings between Disease Chain-LD and
DBpedia.
Disease Compass is a web service that is supported on
PCs, tablets, and smartphones. It is implemented using
Virtuoso as its RDF database and HTML 5 for
visualization of disease chains and other information. All
modules of the system provide APIs for other web ser-
vices. This allows other web services to use all the func-
tions of Disease Compass so that their modules will
work with other related services.
Table 2 Some examples of mapping results between abnormal state ontology and external resources. Blank cells mean that
abnormal states defined in our ontology are not existent in other resources
Concepts Our Ontology PATO HPO MeSH SNOMEDCT
Level 1 structural abnormality
Level 1 material degeneration PATO:0002037
degeneration
Level 1 hardening PATO:0000386
hard
Level 1 size abnormality
Level 1 large in size PATO:0000586
increased size
Level 1 hyperfunction PATO:0001625
increased
functionality
Level 1 dysfunction
Level 1 movement abnormality D009069
Movement
Disorders
Level 2 narrowed cross-sectional area of tube
Level 2 hardening of wall
Level 2 cellular tissue necrosis PATO:0000647
necrotic
D009336
Necrosis
Level 2 conoronary artery stenosis HP:0005145
Coronary artery
stenosis
D023921
Coronary Stenosis
233970002
Coronary artery
stenosis
Level 2 arterial occlusion 2929001
Occlusion of artery
Level 2 coronary artery occlusion D054059
Coronary Occlusion
63739005
Coronary occlusion
Level 2 chest pain HP:0100749
Chest pain
D002637
Chest pain
29857009
Chest pain
Level 3 coronary artert stenosis in
arteriosclerosis
Level 3 esophagel stenosis in esophagitis
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 12 of 18
User interface for navigation
Disease Compass supports the following types of naviga-
tion of the disease ontology.
1. Navigation for definition of disease based on the
RFM of a disease: navigating definitions of diseases
based on causal chains with links to other systems/
datasets
2. Navigation for general causal chains in a human
body: browsing possible causal chains of abnormal
states (disorder) in the human body
3. Navigation for definition of abnormal state
(clinical disorder): browsing the is-a hierarchy of
abnormal state (clinical disorder) ontology with
mappings to other resources
The above are related, i.e., the user can freely access
other systems.
Navigation for definition of disease
Figure 12 shows the Disease Compass user interface for
navigating the definition of a disease. Users select a dis-
ease according to the is-a hierarchy of diseases, or they
search a disease chain by disease name or the abnormal
state included in the disease. The system visualizes the
disease chains of the selected diseases in a user-friendly
representation in the center of the window. The system
also obtains and displays information related to the
selected disease and abnormal state from the linked web
services, such as general information from linked data
(DBpedia) and 3D images of anatomies.
DBpedia is a linked open dataset extracted from Wiki-
pedia [22]. We use DBpedia English (http://dbpedia.org)
and Japanese (http://ja.dbpedia.org). DBpedia provides
general information about diseases; however, medical
experts have not approved this content. Nevertheless, we
suggest that the content is sufficiently valuable to pro-
vide an overview of various diseases. In addition,
DBpedia also provides links to major medical termin-
ology and codes, such as ICD10 and MeSH, which
allows users to gather specialized information about a
given disease. This technology, with which related infor-
mation from other web resources (e.g., ontologies, medical
codes, and datasets) can be obtained through mappings, is
easy to apply to other linked data. We plan to extend the
target linked data in the near future.
A web service named BodyPart3D/Anatomography [23]
is employed to generate 3D images of anatomies. The tar-
get area of the image is decided by Disease Compass,
which combines all targets of abnormal states appearing
in the definition (causal chains) of the selected disease
chain. Subsequently, the system highlights the part of the
3D image that is the target of the selected abnormal state
in the disease chain.
The functionality of Disease Compass is enabled because
of the successful combination of our disease ontology and
other web resources based on linked data technologies. As
a result, Disease Compass allows users to explore disease
knowledge and related information through various web
resources.
An additional movie file shows a demonstration of navi-
gation for definition of disease [see Additional file 1].
Navigation for general causal chains
When viewing the definition of a disease, the user selects
a target abnormal state and can use the click menu to
trace the causes and/or effects that form the selected
abnormal state. Then, a view for navigating general
causal chains is shown. In this view, users can browse
the possible causal chains of abnormal states (disorders)
in the human body through different diseases.
Figure 13 shows an example of navigation for general
causal chains whose starting point is heart failure. The
red node represents the starting point, purple nodes are
effects, and green nodes are causes. By right clicking,
diseases that include the selected abnormal state are
Fig. 11 Disease Compass system architecture
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 13 of 18
listed, and the system shows the definition of the
selected disease when its name is clicked in the list. Note
that some abnormal states shown in the view appear in
different diseases when they are linked to the same
chain. In particular, it is important that the user can ob-
tain both the derived causal chains defined by the clin-
ician directly and the causal chains derived by tracing
the general causal chains through all clinical areas.
An additional movie file shows a demonstration of navi-
gation for general causal chains [see Additional file 2].
Navigation for definition of abnormal state (clinical disorder)
As discussed in Development of Disease Compass sec-
tion, we investigated the differences in the hierarchical
structure of biomedical resources and conducted a trial
integration of our abnormality ontology and related
resources, such as PATO, HPO, and MeSH, based on
ontological theory [9]. As a result, we developed a proto-
type of the abnormality ontology as linked data with a
browsing system. By mapping information from other
resources, users can access disease knowledge through
our abnormality ontology and through other open
resources (Fig. 14).
For example, ID of PATO shown in the definition of
low force/decrease in force is used to jump into the
Ontobee browser, which allows users to see various re-
lated information defined by PATO. Similarly, clicking a
MeSH ID shown in the definition pane leads users to
Fig. 12 Disease Compass user interface
Fig. 13 View for navigating general causal chains
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 14 of 18
the corresponding terms defined in PubMed. For
example, myocardial ischemia in the ontology of abnor-
mal states is mapped to the corresponding MeSH term,
i.e., myocardial ischemia (MeSH ID: D17202), from
which users can retrieve all relevant information anno-
tated by myocardial ischemia in NCBI.
Discussion
The primary feature of the RFM of disease is a disease
model based on the causal chains of clinical disorders. It
is an appealing alternative representation of existing dis-
ease ontology, such as the DOID and the OGMS. The
model captures the possibilities of clinical disorders
(abnormal states) as causal chains and represents dis-
eases by overlapping them. It can intuitively represent
the causes of diseases, disease progression, and the
downstream consequences of diseases to medical ex-
perts. Through these representations, the RFM of a dis-
ease can provide a broad picture of disease-associated
processes in a way that fits well with the clinical under-
standings of diseases.
Publishing the disease ontology based on the RFM
as linked data allows users to access rich knowledge/
information in the disease ontology through a stand-
ard API. Furthermore, Disease Compass provides a
well-organized graphical navigation function for the
disease ontology as linked data with related web re-
sources by mapping information.
In fact, Disease Compass, which allows users to navi-
gate disease definitions with the help of abnormal states,
enables users to learn whether an abnormal state is a
disease cause or effect by identifying its position in the
causal chain of the disease. For example, in the case of
hypertension, users can easily find diseases that are
caused by hypertension, including hypertension disease in
cardiovascular medicine, and those that cause hyperten-
sion as a symptom. For example, the user may find that
chronic kidney disease, in which hypertension appears in
the upper stream of its causal chain, causes various
inflammation, whereas, for Liddle Syndrome, hyperten-
sion appears in the lower stream of its causal chain as a
result of hyperactivity of the epithelial Na channel of
amiloride-sensitive. Thus, users can learn that an abnor-
mal state can be a cause or an effect (symptom) of dis-
eases thanks to the causal chain model of diseases in
Disease Compass.
Disease Compass also allows users to compare multiple
diseases to find unexpected commonalities. For example,
in the case of ischemia, for myocardial infarction in car-
diovascular medicine and ischemic cerebrovascular disease
in neurology, although the locations where the corre-
sponding abnormal states occur differ, both causal chains
share a similar path up to ischemia (Fig. 15). In fact, both
causal chains have structural disorders, such as stenosis
and occlusion, or ischemia is caused by a spasm via
decreased blood flow, and eventually necrosis occurs in
either case. After necrosis occurs, succeeding symptoms
are quite different according to myocardial necrosis or ne-
crosis of brain cells, as shown in Fig. 15 (a)), in which
symptoms (e.g., ventricular wall motion abnormalities)
occur in the cardiovascular system, and in Fig. 15 (b), in
which different symptoms (e.g., paralysis) are caused in
locations governed by the nervous system, thereby reflect-
ing the differences between respective organs.
Disease Compass helps users uncover hidden relations
between different diseases across divisions (Fig. 16). It
Fig. 14 Browsing system for the abnormality ontology as linked data
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 15 of 18
can be done through general causal chains which include
common abnormal states between different diseases
across medical divisions. For example, while heart fail-
ure is a typical disease in cardiovascular internal medi-
cine, Disease Compass can show that it can be caused by
autoimmunity in systemic scleroderma in allergology and
rheumatology. Another disease that causes heart failure
in different medical fields is renal arteriovenous fistula
(aneurysmal type) whose abnormal state, i.e., renal
arteriovenous shunt, can also cause heart failure, which
would be informative for novices because not all text-
books mention this rare fact.
In summary, the benefits of abnormal states organized in
terms of the subsumption relation between states are signifi-
cant. This helps users fill conceptual gaps between concepts
in external resources and reveal hidden commonality be-
tween diseases in different medical fields. This can be real-
ized because all diseases are described in terms of the causal
chains of abnormal states and are organized in an ontology.
Conclusions
This paper has discussed a navigation system for disease
knowledge based on a disease ontology and linked data
technologies. Our ontology defines diseases based on the
causal chains of abnormal states (disorders), and a
browsing system allows users to explore the definitions
of diseases with related information obtained from
linked data. We believe that this system will allow users
to gain a broader understanding of diseases according to
their interests and intentions.
(a)
(b)
Fig. 15 Interaction between abnormal ontology and Disease Compass
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 16 of 18
The following list shows the summary of our contribution
to clinicians and information scientists;
1. For clinicians
 They can access the basic information of diseases
with causal chains through navigation using the
Disease Compass. That is, they can know possible
causes and/or effects of the disease of interest.
 They can find (retrieve) diseases according to
causes or effects.
 They can access related resources through
mapping information.
2. For information scientists
 6302 diseases and 21,699 abnormal states are
defined by clinicians as a fact repository causal
chains of diseases based on the RFM. It shows that
the RFM was applicable to a variety of diseases.
 1554 diseases and 7080 abnormal states in six
major clinical areas, which are extracted from the
above RFM-diseases, are published as linked data
(RDF) with SPARQL endpoint (accessible API).
We call the linked data Disease Chain LD. Infor-
mation scientists can access it using friendly
SPARQL queries through a RDF model we de-
signed for causal chain of diseases.
 A navigation system for disease knowledge using
the Disease Chain LD, called Disease Compass, is
developed. It provides navigating functions for
causal chain of diseases and related information
through links to other resources with GUI. It
shows how the Disease Chain LD can be used for
developing information systems.
The system was evaluated informally by medical ex-
perts in several meetings and workshops, and positive
comments were received. A full-scale user evaluation is
to be conducted in future.
Future work will also include extending the related
resources using linked data and development of more
practical applications using the Disease Chain-LD.
The system is also subject to continuous improve-
ment, including bug fixes and development of new
functions. There remain a few topics on diseases to
explore. One is a notion of an imbalance model that
models pre-clinical manifestation based on the dis-
turbance of homeostasis and roughly corresponds to
OGMS disposition [1]. Another topic is identity
tracking of a disease to capture its progression [24].
We must consider these significant topics and their
computational models.
The latest version of Disease Compass is available at
http://lodc.med-ontology.jp/.
Endnotes
1Though the type 1 and type 2 diseases are not med-
ical terms, we use these terms only in this paper to men-
tion two kinds of diseases.2We defined small in area as
a property that an area is smaller than a given threshold.
On the other hand, narrowing tube is defined that a
cross section of a tube is smaller than a given threshold.
In the same ways, definitions of arterial/vascular sten-
osis are the cross section of an artery/blood vessel is
smaller than a given threshold. Considering these defi-
nitions, we consider that arterial stenosis is-a vascular
stenosis is-a narrowing tube is-a small in area.
Additional files
Additional file 1: A demonstration of navigation for definition of
disease using Disease Compass. (MP4 28362 kb)
Additional file 2: A demonstration of navigation for general causal
chains of disease using Disease Compass. (MP4 45031 kb)
Fig. 16 Causal relationships of heart failure
Kozaki et al. Journal of Biomedical Semantics  (2017) 8:22 Page 17 of 18
Abbreviations
DOID: Human Disease Ontology; IDO: Infectious Disease Ontology;
LOD: Linked Open Data; OGMS: Ontology of General Medical Sciences;
OWL: Web Ontology Language; RDF: Resource Description Framework;
RFM: River Flow Model; SNOMED-CT: Systematized Nomenclature of
Medicine-Clinical Terms
Acknowledgements
This paper is an extended version of the paper presented at the 5th
International Conference on Biomedical Ontology (ICBO2015), July 2730,
2015, Lisbon, Portugal.
The authors are deeply grateful to Drs. Natsuko Ohtomo, Aki Hayashi,
Takayoshi Matsumura, Ryota Sakurai, Satomi Terada, Kayo Waki, and others at
the University of Tokyo Hospital for describing the disease ontology and
providing us with their broad clinical knowledge. We would also like to
thank other team members, Drs. Yoshimasa Kawazoe, Masayuki Kajino, and
Emiko Shinohara, from the University of Tokyo for their useful discussions
related to biomedicine.
The authors would like to thank Enago (www.enago.jp) for the English
language review.
Funding
This research is supported in part by the Japan Society for the Promotion of
Science (JSPS) through its FIRST Program and the Ministry of Health, Labour
and Welfare, Japan.
Availability of data and materials
Developed ontologies and software are available at http://lodc.med-ontology.jp/
Authors contributions
KK designed and implemented the Disease Compass. YY developed ontology
of abnormal states. RM defined most of the theory. TI and KO have led our
medical ontology project and developed disease ontology. All authors read
and approved the final manuscript.
Authors information
Kouji Kozaki, Associate professor at The Institute of Scientific and Industrial
Research, Osaka University.
Yuki Yamagata, Researcher at National Institute of Biomedical Innovation.
Riichiro Mizoguchi, Research Professor at Research Center for Service Science
School of Knowledge Science, Japan Advanced Institute of Science and
Technology.
Takeshi Imai, Assistant Professor at Department of Medical Informatics,
Graduate School of Medicine, The University of Tokyo
Kazuhiko Ohe, Professor at Department of Medical Informatics, Graduate
School of Medicine, The University of Tokyo.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1The Institute of Scientific and Industrial Research, Osaka University, 8-1
Mihogaoka, Ibaraki, Osaka 567-0047, Japan. 2National Institute of Biomedical
Innovation, Health and Nutrition, 7-6-8, Saito-Asagi, Ibaraki, Osaka 567-0085,
Japan. 3Research Center for Service Science, Japan Advanced Institute of
Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan.
4Graduate School of Medicine, The University of Tokyo, 7-3-1, Hongo,
Bunkyo-ku, Tokyo 113-0033, Japan.
Received: 15 February 2016 Accepted: 6 June 2017
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 
DOI 10.1186/s13326-017-0118-0
RESEARCH Open Access
BioFed: federated query processing over
life sciences linked open data
Ali Hasnain1*, Qaiser Mehmood1, Syeda Sana e Zainab1, Muhammad Saleem2, Claude Warren Jr3,
Durre Zehra1, Stefan Decker1 and Dietrich Rebholz-Schuhmann1
Abstract
Background: Biomedical data, e.g. from knowledge bases and ontologies, is increasingly made available following
open linked data principles, at best as RDF triple data. This is a necessary step towards unified access to biological data
sets, but this still requires solutions to query multiple endpoints for their heterogeneous data to eventually retrieve all
the meaningful information. Suggested solutions are based on query federation approaches, which require the
submission of SPARQL queries to endpoints. Due to the size and complexity of available data, these solutions have to
be optimised for efficient retrieval times and for users in life sciences research. Last but not least, over time, the
reliability of data resources in terms of access and quality have to be monitored. Our solution (BioFed) federates data
over 130 SPARQL endpoints in life sciences and tailors query submission according to the provenance information.
BioFed has been evaluated against the state of the art solution FedX and forms an important benchmark for the life
science domain.
Methods: The efficient cataloguing approach of the federated query processing system BioFed, the triple pattern
wise source selection and the semantic source normalisation forms the core to our solution. It gathers and integrates
data from newly identified public endpoints for federated access. Basic provenance information is linked to the
retrieved data. Last but not least, BioFed makes use of the latest SPARQL standard (i.e., 1.1) to leverage the full benefits
for query federation. The evaluation is based on 10 simple and 10 complex queries, which address data in 10 major
and very popular data sources (e.g., Dugbank, Sider).
Results: BioFed is a solution for a single-point-of-access for a large number of SPARQL endpoints providing life
science data. It facilitates efficient query generation for data access and provides basic provenance information in
combination with the retrieved data. BioFed fully supports SPARQL 1.1 and gives access to the endpoints availability
based on the EndpointData graph. Our evaluation of BioFed against FedX is based on 20 heterogeneous federated
SPARQL queries and shows competitive execution performance in comparison to FedX, which can be attributed to
the provision of provenance information for the source selection.
Conclusion: Developing and testing federated query engines for life sciences data is still a challenging task.
According to our findings, it is advantageous to optimise the source selection. The cataloguing of SPARQL endpoints,
including type and property indexing, leads to efficient querying of data resources over the Web of Data. This could
even be further improved through the use of ontologies, e.g., for abstract normalisation of query terms.
Keywords: Life sciences dataset, Linked open data, SPARQL query federation
*Correspondence: ali.hasnain@insight-centre.org
1Insight Centre for Data Analytics, National University of Ireland (NUIG),
Galway, Ireland
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 2 of 19
Background
The Web provides access to large-scale sets of interlinked
data from heterogeneous scientific domains, and  in par-
ticular for the life science researchers  develops into a
source of reference data from scientific experiments [1].
The comprehensive set of Linked Open Data (LOD)1 cov-
ers over 60 billion triples provided by more than 1000
different data sets. The small but important portion of the
Linked Open Data cloud is composed of the Life Science
Linked Open Data (LS-LOD), which results to 8% (83 data
sets) of the overall LOD cloud2. The life science data con-
tributes significantly to the ongoing research in semantic
Web technologies, since the life science research commu-
nity gathers and exposes their expertise in form of high
quality ontologies, which support innovative retrieval
methods across distributed SPARQL endpoint engines as
presented in this publication.
Significant contributions in terms of data integration
and data provision have been made available from the
Bio2RDF project3, the Linked Life Data initiative4, the
Neurocommons group5, through the Healthcare and Life
Sciences knowledge base6 (HCLS Kb), from the Linked
Cancer GenomeAtlas (Linked TCGA) [2, 3], and theW3C
HCLSIG Linking Open Drug Data (LODD) initiative7.
The outcomes from these initiatives develop themselves
into reference data sources that feed the existing life sci-
ence expertise back into the ongoing large-scale research,
for example into high-throughput gene sequencing
research with a need to access the full body of biomedical
data [4]. As a natural consequence, a single point of access
and reference to the life sciences (LS) data is an impor-
tant step forward in using the data and  eventually 
in mastering the data deluge.
It has already been an important step forward to
integrate and RDF-ize the existing biological knowledge
sources to make the data available according to semantic
Web and open data principles, but this is not sufficient
for efficient data access. Further improvements have to
deal with the access to the existing SPARQL endpoints,
access to the meta-data of the data repositories, balancing
access overheads against query efficiency and ultimately,
the efficient use of all technological advancements alto-
gether [5]. The resulting solution should cope with the
size and the complexity of the data, and should still pro-
vide full access to the data in a way that a researcher
can formulate and explore complex queries in refer-
ence to the full amount of integrated data without large
processing overheads, i.e. the the heterogeneity of the
data should not impair the identification of meaning-
ful results [68]. In particular, the exploitation of refer-
ence meta-data information and the use of state of the
art federation technologies form an important step for
the evaluation of such an engine in a real-life use case
scenario.
The integration of heterogeneous data sources takes
place in research teams that make the result available as a
SPARQL endpoint, leading to the challenge of combining
disparate SPARQL endpoints with the help of federation
engines, which a priori rely on the federation of queries
being delivered across the distributed resources [9]. The
latest SPARQL standard, i.e. SPARQL 1.1, is a key techno-
logical advancement to assemble federated queries (with
the help of the SERVICE query option), and is sup-
ported by SWobjects8, Apache Jena9 and dotNetRDF10.
This resulted into the development of different systems
[1015] capable of executing queries in a federated envi-
ronment and claiming that this approach is sufficiently
generic for processing federated queries over any other
data set. However, specific draw-backs have to be consid-
ered that will be addressed in the presented solution:
 First, the federation of queries does not enforce that
the queries deliver the expected results, i.e. access to
meta-data information from the SPARQL endpoints
should improve the outcomes.
 Second, preparing meaningful and productive
SPARQL queries remains to form a skillful task and
profits from domain expertise (e.g., from the domain
ontologies) as well as the meta-data information from
the data sources.
 Last, once the meta-data information has been used
to define the query (to be federated across
endpoints), optimisations solutions should apply to
enable efficient, i.e. speedy, response times.
BioFed is a federated query engine that makes use
of state of the art semantic Web technologies to query
large and heterogeneous data sets in the life sciences
domain: the federation covers 130 public SPARQL end-
points optimised for LS-LOD. It offers a single-point-
of-access for distributed LS data enabling scientists to
access the data from reliable sources without extensive
expertise in SPARQL query formulation (for SPARQL
1.1, online user interface with drop down menus). Its
autonomous resource discovery approach identifies rel-
evant triple patterns, matches types according to their
labels as a basic semantic normalisation approach, and
optimises the retrieval based on source selection strate-
gies for efficient response times. New public endpoints are
added through a cataloguing mechanism based on source
selection [16]. The provided provenance information cov-
ers the sources queried, the number of triples returned and
the retrieval time.
The remaining part of this paper is organised as fol-
lows: we present related work in Section Related work.
Then we present the methodologies covering the imple-
mentation details including discovery, source selection
and query re-writing (Section Methods). BioFed salient
features are presented in Section BioFed salient features.
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 3 of 19
The results and the evaluation against the query engine
FedX is given in Section Results and discussion. Section
Conclusions covers the conclusion, discussion and
future work.
Related work
Advances in federated query processingmethods form the
key achievement for federated query engines that auto-
matically access data from multiple endpoints. Each of
the suggested solutions follows slightly different principles
and even goals, and realises different trade-offs between
speed, completeness, and flexibility requirements, which
are partially imposed by the status of technological
advancements at that time the data sources ready for use.
Umbrich et al. [17, 18] proposed  in a straight forward
way  a Qtree-based index structure that summarises the
content of data sources for query execution over the Web
of Data. The index gives access to the data in the SPARQL
endpoint, but comes with significant draw-backs such as
a lack of access to relational information, e.g., from the
meta-data of the SPARQL endpoint, the overheads in pre-
processing the existing data, and the consequence of out-
of-date indexes and index rebuilding needs.
In terms of advanced index assisted approaches, the
SHARE project registry [19] stores the index informa-
tion as OWL class definitions and instances of the
myGrid ontology. Similarly, OpenLifeData [20], indexed
Bio2RDF using its semantically rich entity-relationships
and exposed it as SADI services after registering in the
SHARE registry. SHARE project stores the set of distinct
predicates for all the endpoints. The source selection is
performed by matching the predicate of the triple pat-
tern against the set of predicates of all indexed endpoints.
All the endpoints which contain matching predicates are
selected as relevant sources for that triple pattern.
Kaoudi et al. [21] propose a federated query technique
on top of distributed hash tables (DHT), which is a simi-
lar approach to the indexing techniques used by Umbrich
et al. The DHT-based optimiser makes use of three greedy
optimisation algorithms for best plan selection. Overall,
the authors achieve good query execution times, but suffer
from the same disadvantages as the previous solution.
Avalanche [22] gathers endpoint data sets statistics and
bandwidth availability on-the-fly before the query feder-
ation, which increases the overall query execution time.
Vandervalk et al. [23], presented two approaches for query
optimisation in a distributed environment, requiring basic
statistics regarding RDF predicates to query the remote
SPARQL endpoints. For one approach a static query
plan is computed in advance of query execution, using
graph algorithms for finding minimum spanning trees.
Whereas, in the second approach, the planning and exe-
cution of the query are evaluated to follow an independent
query plan.
Quilitz and Leser [24] have developed DARQ for the
federation of queries across SPARQL endpoints. It opti-
mises the selection of relevant data sources on the bases of
data descriptions, e.g., usage of predicates in the endpoint,
and statistical information, to optimise the routing of
queries to associated endpoints. This approach is straight
forward, but could exploit better the distribution of triples
given from a specific data source.
Langegger et al. in [25] describe a similar solution using
a mediator approach, which continuously monitors the
SPARQL endpoints for any changes in the data sets and
updates the service descriptions automatically. They solve
the problem of out-of-date descriptions, but unfortunately
the authors have introduced the restriction that all sub-
jects of triple statements must be variables for the bound
predicate requirement of DARQ.
Schwarte et al. [11] have build FedX, which is a query
federation engine for the Web of Data and which does
not require an index for accessing the distributed data.
FedXmakes use of SPARQL ASK queries to enquire about
the content and to determine the endpoints with relevant
information. This approach provides sufficiently fast data
retrieval as compared to other prior art techniques [26],
however, it under-exploits data provide from the endpoint
up front to optimise the query generation.
Saleem et al. [13] presented DAW, a duplicate-aware
federated query approach over the Web of Data. It
makes use of the min-wise independent permutations
[27] and compact data summaries to extend existing
SPARQL query federation engines in order to achieve
the same query recall values while querying fewer
SAPRQL endpoints, which is a very specific optimi-
sation solution for source selection. HiBISCuS [14] is
an efficient hypergraph based source selection approach
for SPARQL query federation over multiple SPARQL
endpoints.
SPLENDID [26] exploits Vocabulary of Interlinked
Datasets (VoID) descriptions that are provided from the
SPARQL endpoints, and makes use of SPARQL ASK
queries to determine relevant sources for the querying
of specific triple patterns. This leads to the result that
SPLENDID is able to federate more expressive queries in
comparison to the previous solutions, but has not been
tested on the very specific case of distributed SPARQL
endpoints for the life sciences with their high complexity
of data.
Other optimisation techniques have also been
attempted. Li and Heflin [28] have built a tree structure
that supports federated query processing over heteroge-
neous sources and uses a reasoner to answer queries over
the selected sources and their corresponding ontologies.
This approach offers new ways to use class specifications
for complex querying, but has not been tested against
challenging life science use cases either.
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 4 of 19
ELITE [29] is an entailment-based federated query
processing engine. It makes use of the ontology-based
data access, R-tree based indexing, query rewriting, and
DL-Lite formalism to retrieve more complete results
which other systems may miss due to no reasoning over
given query.
Ludwig and Tran [30] propose a mixed query engine
that assumes to encounter incomplete knowledge about
the sources to select and discover new sources during
run time, which would not scale sufficiently in the case
of complex data and larger numbers of SPARQL end-
points. Acosta et al. [31] present ANAPSID, an adaptive
query engine that adapts query execution schedulers
to SPARQL endpoints data availability and run-time
conditions, which would not scale to the life science
domain either.
In BioFed we exploit the potential of VoID descriptors 
the state of the art approach for describing any dataset in
order to catalogue the classes and properties from remote
SPARQL endpoints. This cataloguing mechanism facil-
itates query federation mechanism to access data from
multiple heterogeneous biological datasources and offers
the opportunity to support the user of the retrieval engine
with efficient query formulation tools: the queries are
build on the basis of existing data and then distributed
to the relevant endpoints through the source selection
approach. For this, BioFed adopts a hybrid source selec-
tion approach [1], i.e., we make use of both index and
SPARQL ask queries.
Moreover BioFed covers the full range of public
SPARQL endpoints in health care and life sciences
domain, including Bio2RDF, which is a significant scope in
terms of number of endpoints and complexity of data, and
will remain to form a significant challenge for the seman-
tic data integration of the near future. BioFed provides a
single point of access for LS data with other important
information e.g., provenance due to which some queries
may take longer when compared to the other tools like
FedX, whereas provenance is the key for the life sciences
domain targeted by BioFed. One smaller-scale alternative
approach is Topfed [3] which is a TCGA tailored federated
query engine.
Furthermore, the information in the captured catalogue
doesnt rely on semantically rich entity-relationships,
which would require complete knowledge of the defined
schema, which  in return  is difficult to access for most
of the used resources. Our focus is to cover a wide range of
large-scale SPARQL endpoints and to catalogue sufficient
information to achieve efficient querying of the federated
resources.
It is worth noticing that the current interface provided
by BioFed supports designing a basic set of SPARQL
queries using a set of Query Elements (Qe) [16, 32, 33].
Different concepts and properties from endpoints acts
as Qe in order to formulate SPARQL queries. Advanced
and state of the Art query builders e.g., KnowledgeEx-
plorer [34] and SPARQL Assist [35] make use of the
original ontologies/vocabularies and provide an auto-
complete mechanism to write a SPARQL query, but we
believe BioFed interface is a step towards building a basic
SPARQL query that queries over multiple LS SPARQL
endpoints as BioFed offers the set of concepts and proper-
ties in a particular context that can easily be selected from
drop-down menu in order to formulate SPARQL query.
Methods
General architecture
The general architecture of BioFed is given in Fig. 1. Given
a SPARQL query, the first step is to parse the query and
get the individual triple patterns (Step 1). The next step is
the triple-pattern-wise source selection (TPWSS).
Definition 1 (Total Triple Pattern-wise Sources
Selected) Let Q = {t1, . . . , tm} be a SPARQL query contain-
ing triple patterns t1, . . . , tm,R = {Rt1 , . . . ,Rtm} be the cor-
responding relevance set containing relevant data sources
sets Rt1 , . . . ,Rtm for triple patterns t1, . . . , tm, respectively.
We define TTPWSS = ?Rti?R
? |Rti | be the total triple
pattern-wise sources selected for query Q, i.e., the sum
of the magnitudes of relevant data sources sets over all
individual triple patterns Q.
The TPWSS identify relevant (also called capable)
sources against individual triple patterns of the query
(Step 2B). BioFed performs this step by using the dis-
covery approach presented in Hasnain et al. [16]. This
discovery enumerates the known endpoints and relates
each endpoint with one ormore graphs andmaps the local
vocabulary to the vocabulary of the graph (Step2A). Step
3 is to convert the given SPARQL 1.0 query into corre-
sponding SPARQL 1.1 query. This step is known as Query
Re-writing and further explained below. BioFed makes
use of the TPWSS information and the SPARQL SER-
VICE clause to rewrite the required SPARQL 1.1 query.
The resulting SPARQL 1.1 query is executed on top of
the Apache Jena query engine and the results are returned
back (Step 4). In the following, we will explain each of
these steps in detail.
BioFed is designed as a Domain Specific Query Engine
(DSQE) that transforms expressions between existing
vocabularies, i.e. for the vocabularies used by SPARQL
endpoints, and combines those expressions into a sin-
gle federated query using SPARQL Service calls. It
does occur, that a single node is translated into multi-
ple nodes (e.g., drug may become both a molecule and a
smallMolecule) leading to multiple triples being created
as a cross product from all possible node translations.
The resulting statement is executed and then returns the
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 5 of 19
Fig. 1 BioFed architecture. ARDI comes from previous work by Hasnain et al. [4, 16]
results to the user. The algebra rewriter examines each
segment of the BGP triples and attempts to expand the
terms based on the vocabulary mapping into terms of the
endpoint graphs and stores the result for each.
Autonomous Resource Discovery and Indexing (ARDI)
The ARDI comprises a catalogue of LS-LOD and a set of
functions to perform standard queries against it [4, 16].
The methodology for developing the ARDI consists
of two stages namely catalogue generation and link
generation. The methodology for catalogue generation
relies on retrieving all types (distinct concepts) from
each SPARQL endpoint and all associated properties
with corresponding instances. URI patterns and exam-
ple resources were also collected during this process.
Data was retrieved from more than 130 public SPARQL
endpoints11 and organised in an RDF document - the
LS-LOD catalogue. The list of SPARQL endpoints was
captured from publicly available Bio2RDF data sets and by
searching for data sets in CKAN12 tagged life science or
healthcare.
The methodology for link generation is presented in
[16] where naïve, named entity and domain matching
approaches for weaving the types together was dis-
cussed. We later extended our linking mechanism to
incorporate Regex Matching [16].
BioFed utilises the ARDI to perform TPWSS. The ARDI
is also used to determine the URLs of the SPARQL end-
points, and provides the base data for the endpoint selec-
tion logic. The ARDI examines each node in each triple in
the BGP. For each triple it determines if there is a match
in the ARDI or if the triple should be left unmatched (e.g.,
an RDF:type predicate). Each unmatched node is passed
through unchanged.
The RDF in Listing 1 is an illustrative example of
a portion of the catalogue generated for the KEGG
Hasnain et al. Journal of Biomedical Semantics  (2017) 8:13 Page 6 of 19
SPARQL endpoint13. VoID is used for describing the
data set and for linking it with the catalogue entries:
the void#Dataset being described in this catalogue
entry is KEGG SPARQL endpoint. In cases where
SPARQL endpoints were available through mirrors (e.g.,
most Bio2RDF endpoints are available through Car-
leton Mirror URLs) or mentioned using alternative URLs
(e.g., http://kegg.bio2rdf.org/sparql), these
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 
DOI 10.1186/s13326-017-0127-z
RESEARCH Open Access
Ontological interpretation of biomedical
database content
Filipe Santana da Silva1,2 , Ludger Jansen3 , Fred Freitas1 and Stefan Schulz4*
Abstract
Background: Biological databases store data about laboratory experiments, together with semantic annotations, in
order to support data aggregation and retrieval. The exact meaning of such annotations in the context of a database
record is often ambiguous. We address this problem by grounding implicit and explicit database content in a
formal-ontological framework.
Methods: By using a typical extract from the databases UniProt and Ensembl, annotated with content from GO, PR,
ChEBI and NCBI Taxonomy, we created four ontological models (in OWL), which generate explicit, distinct
interpretations under the BioTopLite2 (BTL2) upper-level ontology. The first three models interpret database entries as
individuals (IND), defined classes (SUBC), and classes with dispositions (DISP), respectively; the fourth model (HYBR) is a
combination of SUBC and DISP. For the evaluation of these four models, we consider (i) database content retrieval,
using ontologies as query vocabulary; (ii) information completeness; and, (iii) DL complexity and decidability. The
models were tested under these criteria against four competency questions (CQs).
Results: IND does not raise any ontological claim, besides asserting the existence of sample individuals and relations
among them. Modelling patterns have to be created for each type of annotation referent. SUBC is interpreted
regarding maximally fine-grained defined subclasses under the classes referred to by the data. DISP attempts to
extract truly ontological statements from the database records, claiming the existence of dispositions. HYBR is a hybrid
of SUBC and DISP and is more parsimonious regarding expressiveness and query answering complexity. For each of
the four models, the four CQs were submitted as DL queries. This shows the ability to retrieve individuals with IND,
and classes in SUBC and HYBR. DISP does not retrieve anything because the axioms with disposition are embedded in
General Class Inclusion (GCI) statements.
Conclusion: Ambiguity of biological database content is addressed by a method that identifies implicit knowledge
behind semantic annotations in biological databases and grounds it in an expressive upper-level ontology. The result
is a seamless representation of database structure, content and annotations as OWL models.
Keywords: Ontology, Interpretation, Biological database, OWL, Data semantics
Background
Biological databases store data about summarized results
from laboratory experiments. Apart from numeric and
unstructured text entries, they usually include seman-
tic annotations, characterized by identifiers from domain
ontologies, to enhance database entries with standardised
meaning. For instance, database records from the Uni-
fied Protein Resource (UniProt) [1] are annotated with
*Correspondence: stefan.schulz@medunigraz.at
4Institute for Medical Informatics, Statistics and Documentation, Medical
University of Graz, Auenbruggerplatz 2/V, 8036 Graz, Austria
Full list of author information is available at the end of the article
terms taken from the Protein Ontology (PRO) [2] and
the Gene Ontology (GO) [3]. It is mainly via their use as
annotation vocabularies that bio-ontologies have become
important resources for the management of biomedical
research data.
As much as these domain ontologies, in isolation, obey
formal principles and good practice guidelines [4, 5],
as little the meaning of the annotations themselves has
been formalized so far. The exact interpretation of what
it means when, e.g., in a UniProt record the protein
PRO:Methionine synthase is linked to the biological pro-
cess GO:Methylation, is left to the user, mainly due to
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 2 of 14
limited representation of UniProt Core [6]. UniProt Core
includes the description on database fields related to each
other, but without formalization and links to GO (for
example). This can constitute a source of misunderstand-
ing and hamper correct data interpretation, leading to
doubtful or wrong conclusions.
Although the meaning of semantic annotations in
database records may seem trivial for domain experts,
human interpretation of large numbers of records is
tedious and time-consuming. Laukens and colleagues [7],
among others, have highlighted the difficulty of interpret-
ing database content in the context of proteomics. The
reason for this is that there is still a divide between biolog-
ical databases and the semantic technologies developed
for biomedical ontologies. Scattered data need to be inte-
grated into a coherent picture, which is complicated by
ambiguity and lack of interoperability.
On the one hand, there are rich and well-curated
databases with highly structured tabular content but lim-
ited ontological explicitness. Like most content of tabular
data structures, these databases require implicit back-
ground assumptions for their correct interpretation.
Imagine, for example, a database table with three fields
Protein, Organism and Phenotype, filled with the symbols
Prot1, Org1, and Phen1. Such a table is open to multi-
ple interpretations, among which only one is the intended
one, viz. that organisms of the type Org1 in which protein
Prot1 is dysfunctional are at risk to develop the patholog-
ical phenotype Phen1. This interpretation is not formally
described anywhere, because it is assumed that database
curators and users would not succumb to erroneous inter-
pretations, such as that all proteins of Prot1 are included
in at least one organism of type Org1, or that organ-
isms of type Org1 have as part at least one protein of the
type Prot1 and exhibit specifically at least a Phen1. There-
fore, a formal description would be fundamental for the
correct interpretation of the database content in other
contexts.
On the other hand, there is an increasing number of
biomedical ontologies in which logic-based axioms pro-
vide precise descriptions, which indeed enable formal
reasoning. Such axioms are expressed in Description Log-
ics (DL) [8] using the Web Ontology Language OWL2
[9]. DL queries can be answered based on satisfiability
testing and class subsumption. For instance, such queries
enable to retrieve Parkinsons disease in a query when
searching for diseases that affect the extra-pyramidal
system, if Parkinsons disease has been formally char-
acterised as a disorder located in the basal ganglia of
the brain, and the latter as part of the extra-pyramidal
system.
This division between database content and structure
on the one hand (with its implicit meaning) and ontol-
ogy content on the other hand (with its explicit meaning)
is, currently, an obstacle towards querying both together.
Given this picture, several questions arise:
i. How can the implicit knowledge about entities and
relationships described in the structure of a
biological database be represented?
ii. How can the content of databases be interpreted, i.e.,
which domain entities are represented by the data
elements and their connections?
iii. Are structure and content of biological databases of
ontological nature?
iv. If this is the case, how can they be translated into
axioms or assertions in a commonly used ontology
language, and which representational patterns might
be considered?
v. Once database structure and content are expressed
by formal-ontological means, how can existing
bio-ontologies be plugged into this structure?
vi. Given a seamless integration among these
components, are there benefits for content retrieval,
regarding correctness, completeness, and
user-friendliness?
vi. Is such a system capable to accommodate large
amounts of data in biological databases, also
considering the size of a domain ontology?
Addressing questions i-iv, we hypothesise that there are
feasible ways to express implicit and explicit database
content by formal-ontological means and combine this
content with pre-existing domain ontologies.
Regarding question v, previous work has shown how
content of tables in scientific publications can be inter-
preted on formal grounds [10]. Question vi has been
addressed in [11], which introduced the reasoning capa-
bilities of querying highly axiomatised bio-ontologies.
Question vii needs to be addressed after answering ques-
tions i-iv, but is beyond the scope of the present paper.
We will demonstrate how entities referenced by a typ-
ical extract from a biomedical database can be inter-
preted under several ontological viewpoints, viz. regard-
ing the introduction of individuals (IND), the addition
of new axioms to existing classes (DISP) and the intro-
duction of additional defined classes (SUBC and HYBR).
The resulting OWL models are, then, tested under three
aspects:
i. Database content retrieval: classes or individuals are
retrieved by means of DL queries;
ii. Information completeness: is the interpretation
generated able to answer user queries?
iii. DL complexity and decidability: in order to solve DL
queries, there should be theoretical guarantees that
the machine performs under a reasonable cost and
finite time (complexity) and always finishes its task
(decidability).
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 3 of 14
Methods
This section describes the ontology engineering princi-
ples we subscribed to, as well as the data we gathered to
exemplify our approach.
Engineering principles
Firstly, we believe that ontology structure and content
should be driven by the underlying reality, rather than
by specific application needs. We subscribe to the prin-
ciples of the OBO Foundry [4], and emphasise the use
of a principled upper-level ontology, here BioTopLite2
(BTL2) [12], which offers a set of high-level classes,
together with constraining axioms, using a small number
of core relations. Classes like Organism, Mono molecular
entity, and Body part facilitate the alignment with other
ontologies like GO, PRO, SNOMEDCT and ChEBI. BTL2
can also be aligned with most of BFO [13] and the OBO
Relation Ontology [14]. BTL2 regards all instances of its
classes as implicitly time-indexed, thus solving the ambi-
guity problem of using binary relations for the cases
where BFO2 [13] requires ternary ones, which are not
expressible in OWL [15].
The fundamental role of Description Logics (DLs) [8]
is justified by the widespread use of the Web Ontology
Language OWL2 [9], supported by popular editors and
classifiers [16]. We use OWL-DL, which corresponds to
the language specification SROIQ [17], and which com-
bines expressiveness with complete and finite reasoning
power. OWL2 supports classes, binary relations (object
properties), and individuals, together with related axioms
and assertions, for which we will use the OWL2 Manch-
ester Syntax [18]. Important for DL is the distinction
between ABox and TBox. The TBox contains termi-
nological" class-level axioms, i.e. the ontological content
proper, whereas the ABox contains contingent asser-
tions" about individuals.
Dispositions
Real world entities are often described in terms of dis-
positions, i.e., tendencies of something to act in a certain
manner under given circumstances resulting from natu-
ral constitution, nature, quality, or orderly arrangement.
Saying that all animals are organisms is a universal state-
ment; stating that all humans are able to develop diabetes
mellitus type 2 is a dispositional statement. Several works
[12, 1921] have suggested to include dispositions in
biomedical ontologies; e.g., the disposition to pump blood
is present in all healthy organs of the type Heart.
Large parts of biomedical database content seem to be
dispositional: In biochemistry, a statement that a protein
A participates in a process B does probably not mean
that all instances of A constantly participate in a pro-
cess of type B, but rather that all instances of A have the
disposition to participate in such a process. Biomedical
observations yield statistical results, which indicate that
participants of an experiment are ascribed to certain capa-
bilities (e.g. to participate in B under certain experimental
conditions) [19, 22].
Information content entities
Finally, database content as such needs ontological
scrutiny, as highlighted in [7]. Database content is onto-
logically best characterised as information content. This
requires a strict distinction between (i) the database
content proper and (ii) the entities in the world ref-
erenced by the former. As well as the data in clinical
documents, biomedical database content is connected by
a specific relation (often named represents, isAbout,
or denotes) with biomedical entities. Such information
content entities do not necessarily denote particulars (i.e.,
instances) in the domain described. A myocardial infarc-
tion record entry about a patient recently admitted to the
emergency room may have the attribute probable, even
if the patient does (in fact) not have any heart problem.
Similarly, a database entry on, e.g., the relation between
protein Pk and phenotype Ti in an organism Om may be
affected by experimentation, reporting, or curation errors.
Running example
For the analysis reported in this paper, we selected a
typical biological database example (cf. Table 1), gener-
ated by joining data from UniProt [1] and Ensembl [23]
by standard database querying (Additional file 1). This
was performed in order to retrieve all related records
to the metabolism of homocysteine and other sulphu-
rated amino acids, like methionine and cysteine (see [24]
for more information regarding homocysteine metabolic
pathway).
From UniProt (release 2015_01), we retrieved 21,868
records, and (exactly) 1000 from Ensembl (release 78). All
sample data were retrieved on January 22nd, 2015. Data
from the NCBI Taxonomy (2015AA) were incorporated
at the end of the retrieval process, adding the taxonomy
identifiers of the organisms from which data are recorded
in UniProt and Ensembl.
Using the ontology editor Protégé v.5, supported by the
DL classifier HermiT [16] v.1.8.3, we created four OWL2
models, each of which followed a different strategy. They
were created according to the data organisation presented
in Table 1, based on a sample record (Table 2). Terms
for individuals were created according to the same orga-
nization, but identified by a bold lower-case letter and a
random number, like p1001 or m2001 as terms for an
individual protein and molecule (respectively).
The four OWLmodels uniformly represent all informa-
tion entities (database content) as individuals. The models
differ, however, in the way how referents of this informa-
tion are interpreted, viz. (i) as individuals (Additional file 2),
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 4 of 14
Table 1 Typical data record from the joined databases Uniprot and Ensembl. The abstraction introduces the symbols of the example
ontologies
Field Source Content Abstraction
Protein (PR) UniProt Cystathionine gamma-lyase Prot1
Organism (NCBI Taxonomy) NCBI Taxonomy via UniProt Rattus norvegicus (Rat) Org1
Processes (not distinguishing
between Biological process and
Molecular function in Gene
Ontology (GO))
GO via UniProt hydrogen sulfide biosynthetic
process; negative regulation of
apoptotic signaling pathway;
positive regulation of I-kappaB
kinase/NF-kappaB signaling;
protein homotetramerization;
protein sulfhydration
BProc1, BProc2, . . . , BProck
Cell components (GO_cc) GO via UniProt cytoplasm; nucleus;
extracellular vesicular exosome;
CComp1, CComp2, . . . , CCompx
Small molecules (ChEBI) ChEBI Homocysteine Mol1,Mol2, . . . ,Moly
Phenotypes Ensembl Amino acid metabolism errors;
cataract; Gamma-cystathionase
deficiency
Phen1, Phen2, . . . , Phenz
(ii) as fully defined subclasses (Additional files 3 and 4) (iii)
as disposition (Additional file 5) classes.
In the following, names of individuals are picked out
in bold face with lower case initials, in contrast to class
names in italics with leading upper case character. Sym-
bols that include white spaces are enclosed in single
quotes, e.g., has part.
In order to test the fitness of these models, four com-
petency questions (CQs) were formulated in natural lan-
guage and then reformulated as DL queries (cf. Table 3)
in order to emulate typical query operations over ontolo-
gies and databases, performed by biomedical researchers.
Q1 aims at retrieving biological processes in which cer-
tain proteins participate; Q2 retrieves the cellular com-
ponent(s) a given organism includes, together with the
proteins found in them. Q3 retrieves proteins recorded
as participant of biological processes in a given organism.
Finally, Q4 retrieves organisms able to exhibit a specific
phenotype.
Results
Table 1 represents the typical structure of the data ana-
lyzed in this work. It is categorized and organized by the
following structure:
 one protein term (e.g., CBS) ;
 one taxon term (e.g., Rattus norvegicus);
Table 2 Schematic view over UniProt, NCBI Taxonomy and
Ensembl data
Protein Organism Bio Process Cell component Molecule Phenotype
Prot1 Org1 BProc1 ; CComp1 ; Mol1 ; Phen1 ;
Bproc2 ; CComp2 ; Mol2 ; Phen2 ;
Bproc3 CComp3 Mol3 Phen3
 one to many terms for GO biological processes or GO
molecular function (e.g., Blood vessel remodelling );
 one to many terms for GO cellular components (e.g.,
Cytoplasm);
 zero to many terms for phenotypes (e.g., Endocrine
pancreas increased size );
 one to many terms for small molecules (e.g.,
Homocysteine)
This structure was imported from UniProt and
expanded with mappings to Ensembl via identifiers. Fol-
lowing [25], we treat terms from GO Molecular function
as referring to processes. This is supported by the fact
that the latter ones are named activities in GO; and
heuristically, by the fact that in experiments molecular
functions are always discovered through their realizations,
i.e., through the observation of processes or their results.
Table 3 Queries translated into DL queries
Q1  Which biological processes have proteins of the kind Prot1
as participant?
Biological process and (has participant some Prot1)
Q2  In which cellular locations is Prot1 active in organisms of the
type Org1?
Cellular component and (is included in some Org1) and
(includes some Proti)
Q3  Which proteins are involved in processes of the type BProc1 in
organisms of the type Org1?
Protein and (is participant in some BProc1) and (is included in
some Org1)
Q4  Which organisms are able to exhibit a specific phenotype Phen1?
Organism and (is bearer of some (Disposition and (has realization
only Phen1)))
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 5 of 14
Even if all terms from the database are understood, there
are still numerous open questions regarding the precise
meaning of such a database record. We fill this gap by
eliciting the necessary implicit knowledge from a domain
expert familiar with the process of database population,
performing an in-depth ontological analysis in the line
of Gangemi et al. [26]. This analysis begins with the for-
mal categorization of relations and basic classes, under
a suitable upper-level ontology. This was done by manu-
ally aligning the top-level classes of the domain ontolo-
gies GO, ChEBI and PR under the top-level ontology
BTL2 [12].
Once the entities are categorised, the following ques-
tions need to be answered:
 How are the structural elements of a database (i.e.
tables, fields) related to each other? Which
knowledge is missing that is required for correctly
understanding these relations?
 Which expressiveness is required to axiomatise the
content in a logic-based language in an appropriate
way to represent all implicit and explicit content?
 Which additional entities need to be included into
the ontology (e.g., Dysfunctionality and Disposition
in the above example)?
 Which compromises and simplifications may be
needed? Which propositions are categorical, which
ones are dispositional? [19] Do we have to include
ABox entities (individuals)?
When it comes to an ontology-based representation
of database content (as exemplified in Table 1), we face
three interpretation challenges: (i) the data points and col-
umn headers, (ii) the relation between the data points
and the column headers, and (iii) the relations among the
columns.
Task (i) is facilitated by the fact that many of the content
terms are already represented in biomedical ontologies
like GO. Besides, the natural language terms used as field
labels can easily be aligned to content from other ontolo-
gies. In our case, most field labels could be aligned with
BTL2.
Task (ii) will normally be accounted for by the subclass
or instantiation relation: the content terms denote classes
or instances of the class denoted by the field label. E.g.,
Cystathionine gamma-lyase subClassOf Protein, Rattus
norvegicus subclassOf Organism, etc.
Task (iii) requires reference to the implicit knowl-
edge a scientist is likely to have. For example, a
UniProt record that points to Methylation, Bos tau-
rus and Methionine synthase expresses that in a given
experiment with cattle tissue an instance of Methionine
synthase was observed to participate in a methylation
process.
In the following, we investigate four different
approaches for representing the meaning of the content
and structure of biological databases:
1. Representation as sample individuals (IND);
2. Representation as defined maximally fine-grained
subclasses, seeing as referents of the information
entities in the database (SUBC);
3. Representation with dispositional properties (DISP);
4. Hybrid representation with subclasses and
dispositions (HYBR).
Our sample ontologies include one Protein class (Prot1),
one Organism class (Org1), and three subclasses of each
of Cell Component (CComp1,...,3), Biological Process
(BProc1,...,3), Small Molecule (Mol1,...,3), and Phenotype
(Phen1,...,3), respectively (Table 2).
Representation as individuals (IND)
The first representation is motivated by the fact that
a database entry is about a concrete experiment, in
which individual entities in space and time are described,
e.g., a piece of biological material, a certain amount of
molecules, the phenotype of an individual rat, etc. This
view is agnostic with respect to whether the observed
phenomena are manifestations of natural laws or not.
In this perspective, our sample data report that individ-
ual protein molecules p1001, p1002, . . . of the type Prot1
exist in some particular cell components cc1001, cc2001,
. . . of the types CComp1,...,n of some organisms o1001,
o1002, . . . of the type Org1. Biomolecular process individ-
uals bp1001, bp2001, . . . that are members of the classes
BProc1,...,m include moleculesm1001,m2001, . . . of the type
Mol1,...,k (specific to Org1). Finally, the dysfunctions of the
proteins p1001, p1002, . . . cause the organisms o1001, o1002,
. . . to display one ormore phenotypes ph1001, ph2001, . . . of
the type Phen1,...,n (Table 2).
We are aware that only collections of molecules (and
never single molecules) and activities thereof are observed
[22]. However, assuming that the observation of the
behaviour of collective individuals allows us to deduce
what happens at the level of individuals (as done when
describing chemical reactions or biochemical pathways
with symbols denoting single molecular entities), we here
populate the ABox with single, non-collective, sample
entities and the relations among them. Index numbers are
aligned arbitrarily.
In the following we describe our interpretation
approach. For instance, individual protein molecules in
individual organisms are active in processes, e.g., within
cell components, like:
p1001 is included in cc1001
cc1001 is included in o1001
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 6 of 14
We also introduce instances for protein molecules that
participate in process instances within an organism:
p1004 is participant in bp1001
p1004 is included in o1004
Proteinmolecules participate, within a particular organ-
ism, in process instances (e.g., bp1001) that synthesise
specific molecules (e.g.,m1001):
p1010 is participant in bp1001
bp1001 has participant m1001
p1010 is included in o1010
Whenever the database fields for processes, molecules,
or cell components have more than one entry, the
database, unfortunately, leaves open which processes
involve which molecules and where they are located.
Ideally, this information might be retrieved from other
sources. Otherwise, a relation between an individual
processes and molecules participating in them can be
expressed by referring to an appropriate process individ-
ual bp1001 and an appropriate individual moleculem1001.
An analogous strategy is possible to express the participa-
tion of cell components in processes.
bp1001 includes m1001
There are organisms with specific phenotypes, in which
there is a protein of a certain type, which is however
dysfunctional. Dysfunctionalities can be represented as
qualities, here also expressed as the individual d1001.
p1013 is included in o1013
o1013 includes ph1001
p1013 is bearer of d001
For these data to be interpreted in a DL context, ABox
entities (in this scenario) are to be understood as arbi-
trary individuals that participate in a specific experiment.
For the sake of simplicity, for each assertion that can be
derived from the database, new terms for individuals are
created.
Another simplifying assumption of this approach is that
all database terms are non-empty, i.e., they actually refer to
some existing entity. Each information-content individual
in the database needs to represent an existing individual
involved in the experiment. This is, of course, problem-
atic if the data is wrong due to curation errors, or if the
biological processes recorded did not really happen.
Representation as multiple subclasses (SUBC)
The second approach interprets database terms as refer-
ring to maximally fine-grained defined classes. The nam-
ing of these new subclasses follows strict naming criteria
as exemplified below. This is important for extracting the
original class names from the subclass names, because
only the former ones are interesting for querying. For
instance, the database represents a protein class Prot1
that is connected with an organism class Org1 and a bio-
process class BProc1. Accordingly, we create the classes
Prot1_in_Org1_in_BProc1, Org1_with_Prot1_and_BProc1,
and BProc1_in_Org1_with_Prot1 with appropriate full def-
initions (Fig. 1).
We leave open whether these defined classes are empty.
In a way, defined classes are nothing more than logical
artefacts. For this reason, the creation of such defined
OWL classes has a modest ontological engagement. Nev-
ertheless, these defined classes can serve as the referents
of the data instances [27].
In order to fully incorporate the idea that database
entries are individuals that refer to classes by means of
annotations, we create the following description logic
formula for each database entity:
databaseEntryx type represents only
(DefinedClass1 or DefinedClass2 or . . . or DefinedClassN )
Bearing this representation in mind, querying can be
limited to the expression in parentheses, which brings
two advantages, viz. that neither individuals and nor
value restrictions would impact the performance of the
reasoner.
In the following, the modelling patterns are given for
proteins, organisms, small molecules, biological processes
and phenotypes. Here, the index variable i denotes a
record, in which field (e.g., for protein) is filled exactly
Fig. 1 Example of subclass creation and relations enabled to be used
in class definitions
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 7 of 14
once; hence the notation Proti1 . Accordingly, the notation
for organisms is Orgi1 , because there is exactly one organ-
ism type referred to by a record. The other fields may
be multiply filled; therefore the notation is, e.g., BProc1 ,
BProc2 , . . . , BProcm .
Proteins: We introduce classes for dysfunctional pro-
teins as well as for organism-specific proteins and their
combination:
Proti1_Dysf equivalentTo Proti1 and
is bearer of some Dysfunctional
Proti1_in_Orgi1 equivalentTo Proti1 and
is part of some Orgi1
Proti1_Dysf_in_Orgi1 equivalentTo
Proti1_Dysf and Proti1_in_Orgi1
Specifically, subclasses are created to represent the pos-
sible links among classes denoted by annotations within a
record. For instance, the subclass Proti1_in_Orgi1 is gener-
ated to express that we deal with a protein of an organism
of a certain type Orgi1 . In addition, subclasses are intro-
duced for phenotypes, processes, cell components and
molecules:
Proti1_Dysf _in_Orgi1_with_Phen1,...,o equivalentTo
Proti1_Dysf _in_Orgi1 and
is part of some (Orgi1 and
(includes some Phen1,...,o))
Proti1_in_Orgi1_in_BProci,...,m equivalentTo
Proti1_in_ Orgi1 and is participant in some BProc1,...,m
Proti1_in_Orgi1_in_CCompi1,...,n equivalentTo
Proti1_ in_Orgi1 and is included in some CComp1,...,n
Proti1_in_Orgi1_with_Mol1,...,k equivalentTo
Proti1_in_Orgi1 and (is participant in some
(Process and
(has participant someMol1,...,k )))
Organisms: Classes are introduced for organisms with
proteins in general, and for organisms with organism-
specific proteins in particular. The latter ones are also
specialized by phenotypes, processes and molecules:
Orgi1_with_Proti1 equivalentTo Orgi1 and
has part some Proti1
Orgi1_with_Proti1_Dysf equivalentTo Orgi1 and
(has part some Proti1_Dysf )
Orgi1_with_Phen1,...,o_and_Proti1_Dysf equivalentTo
Orgi1_with_Proti1_Dysf and includes some Phen1,...,o
Orgi1_with_Proti1_and_BProc1,...,m equivalentTo
Orgi1 and (has part some (Proti1 and
(is participant in some BProc1,...,m)))
Orgi1_with_Proti1_and_Mol1,...,k
equivalentTo Orgi1 and
(has part some Proti1 ) and
(is participant in some (Process and
(has participant someMol1,...,k)))
Small molecules: We introduce classes for small
molecules contained in organisms, and further specify
these classes by stating the type of the proteins with which
these small molecules interact, i.e., with which they are
related by participating in the same biological processes.
Mol1,...,k_in_Orgi1 equivalentToMol1,...,k and
is part of some Orgi1
Mol1,...,k_in_Orgi1_with_Proti1
equivalentToMol1,...,k_in_Orgi1 and
(is participant in some (Process and
(has participant some Proti1 )))
Processes: Subclasses are introduced for the partici-
pating proteins which are included in a certain type of
organism.
BProc1,...,m_in_Orgi1_with_Proti1
equivalentTo BProc1,...,m and
(has participant some Proti1 ) and
(is included in some Orgi1 )
Phenotypes: Subclasses are introduced for associated
dysfunctional proteins and their respective organisms.
Phen1,...,o_in_Orgi1_with_Proti1_Dysf ?
equivalentTo Phen1,...,o and
(is included in some Orgi1_with_Proti1_Dysf )
The querying strategy for this representation model
is to check whether specific subclasses are retrieved or
not. For instance, if we want to retrieve processes with
Proti1_in_Orgi1 , the corresponding DL query is
Process and (has participant some Proti1 ) and
(is included in some Orgi1 )
The automated reasoner delivers a list with the corre-
sponding defined subclasses, such as:
BProc1_in_Orgi1_with_Proti1 ,
BProc2_in_Orgi1_with_Proti1 or
BProc3_in_Orgi1_with_Proti1 .
A disadvantage of the SUBC interpretation is that it
requires the introduction of classes that are not to be
found in the ontologies used for annotation (such as GO
or PRO) and that these classes are retrieved by the above
query. For querying purposes, their superclasses must be
identified, viz. BProc1, BProc2, and BProc3. This requires
some post-processing of the results as explained below.
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 8 of 14
Thus, subclasses for all types of entities referred to in
a database are created, which is on the one hand highly
prolific, because every possible association of entries in
table fields must be combined into a new defined class.
On the other hand, the expressiveness power of the
DL dialect needed is reduced to the EL++ [28], cor-
responding to OWL2-EL, which is known for its good
scalability [28].
Representation with dispositions (DISP)
In the representational patterns IND and SUBC, database
entries were seen as observations about individuals, either
represented as existing ABox entities or as specific,
potentially empty, subclasses. Whereas INDmakes strong
existential claims, stating that the content of a field is
interpreted as representing an actually existing biological
individual, the ontological engagement of SUBC is more
modest, as it allows empty classes (although non-denoting
database entries are rather the exception than the norm).
Both IND and SUBC avoid to claim any universal state-
ment of the form For all A there is some B for any class
A referred to by database.
In contrast, the DISP pattern goes a step further, assum-
ing that the database content has been created to give
insights into scientific regularities in the sense that all
members of a class have a disposition to behave in a certain
way, thus exhibiting a law of nature.
To ascribe a disposition for a certain process P to an
object m does not imply that m actually and at all times
participates in an instance of P. It implies only that the
physical structure of m allows m to participate in pro-
cesses of the type P. The proposed modelling pattern in
DL is the following [29]:
Object1 and Object2 and . . . and Objectn subclassOf
is bearer of some (Disposition and
(has realization only Process1))
whereObject1 refers to a class; andObject2 toObjectn refer
to other classes, or to statements of the type ClassA and
relation some ClassB.
The bearers of dispositions are independent continu-
ants [19, 20]. Thus, possible bearers of dispositions, in
our case organisms, proteins, small molecules and cell
components.
For organisms and proteins, we create a series of gen-
eral class inclusions (GCIs) in OWL, with the class of
interest (e.g. Proti1 ) intersected with the constraining con-
ditions at the left hand side (e.g. is part of some Orgi1 ).
Dispositions are, then, ascribed to organism-specific pro-
teins within certain cellular components. We introduce
dispositions to perform biological processes that have cer-
tain kinds of molecules as output. Here is the general
pattern.
Proti1 and is part of some Orgi1 subClassOf
is bearer of some (Disposition and
has realization only BProc1,...,m) and
is bearer of some (Disposition and
has realization only (Process and
has participant someMol1,...,k))
In this and the next formula, the restriction
is included in some
(CComp1 or CComp2 or . . . or CCompx)
could be added. However, this restriction is rather weak
due to the disjunction, which may leave room for several
classes to be added.
As a rule, dispositions have realisation conditions. The
realisation of the disposition of a protein to participate
in a given biological process depends, among others, on
the chemical environment within the organism and the
cell component. Such dispositions are introduced for all
proteins of the type Proti1 , under the condition that they
are included in Orgi1 as well as in one or more cellular
components (CComp1,...,n). These dispositions are defined
in terms of the process types BProc1,...,m processes, or in
terms of unspecified processes in which one ormore small
molecules (Mol1,...,k ) participate.
Our interpretation of the example is that the ability
to exhibit a certain pathological phenotype is attributed
to organisms in virtue of having a dysfunctional protein.
Again, the table does not tell us which kind of dysfunc-
tion affects which kind of process that results in which
phenotype:
Orgi1 and (includes some (Proti1 and
(is bearer of some Dysfunctional))) subClassOf
is bearer of some (Disposition and
(has realization only Phen1,...,o))
Formally, we could characterize a class of small
molecules as bearing dispositions in the following way:
Mol1 orMol2 or . . . orMolk
subclassOf is bearer of some (Disposition and
(has realization only (Process and
(has participant some Proti1 ) and
(is included in some Orgi1 ) and
(is included in some
(CComp1 or CComp2 or . . . or CCompn)))))
As we said, dispositions could theoretically also be
ascribed to cell components, as these are also independent
continuants. However, according to the shared back-
ground assumptions of biologists, cellular components are
not participants but only the locations of the biomolecular
processes under scrutiny. That an entity bears a disposi-
tion of being the arena in which a process might take place
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 9 of 14
would require the extension of either the notion of dispo-
sition or the notion or participation. Therefore, we refrain
from ascribing dispositions to cell components.
The use of general class inclusions (GCIs), i.e. the use
of complex class expressions on the left hand side of the
axiom, is a straightforward application of the above pat-
tern. However, this strategy does not support retrieval
purposes, as DL queries only retrieve simple names of
classes or individuals, but not complex expressions.
Hybrid class-level representation (HYBR)
To avoid complex class expressions on the left hand side
of GCIs, a feasible approach that supports DL queries
on dispositions would require equivalence axioms as the
following:
Orgi1_with_Proti1_Dysf equivalentTo Orgi1 and
(has part some (Proti1 and
(is bearer of some Dysfunctional)))
Here,Dysfunctional is a class that qualifies a given Proti1
as being causally related to a pathological phenotype.
The class Orgi1_with_Proti1_Dysf can then be used on
the left hand side of an axiom that states the disposi-
tions of organisms of the type Orgi1 under the condition
of having dysfunctional proteins of the type Proti1 . This
corresponds to the modelling pattern SUBC.
In our example, this means that the SUBC model
requires n defined classes for organisms of the type Orgi1
that have dysfunctional proteins of the type Proti1 and
which include a phenotype Phen1,...,o, whereas the DISP
approach requires one axiom with organisms of the type
Orgi1 that have dysfunctional proteins of the type Proti1 
at the left hand side, with expressions on Phen1,...,o at the
right hand side:
Orgi1_with_Proti1_Dysf subClassOf
is bearer of some (Disposition and
(has realization only Phen1,...,o))
This leads to a hybrid approach in which subclass def-
initions are still needed. The hybrid representation may
be preferred as being more parsimonious, which however
has to be traded off against the increase in DL expressive-
ness, viz. from OWL-EL to OWL-DL, at least when DISP
(like proposed for SUBC) avoiding generation of a huge
number of very specific subclasses, as in SUBC.
Evaluating representation scenarios
We created four DL queries (Q1Q4) (cf. Table 3) to
evaluate (i) database content retrieval, using ontologies as
query vocabulary; (ii) information completeness; and (iii)
DL complexity and decidability. Q1 aims at retrieving bio-
logical processes in which certain proteins participate; Q2
aims at retrieving the cellular component(s) a given organ-
ism includes, together with the proteins found in them.Q3
aims at retrieving proteins recorded as participant of bio-
logical processes in a given organism. Finally, Q4 aims at
retrieving organisms able to exhibit a specific phenotype.
Queries on SUBC or HYBR models require further pro-
cessing, because they retrieve the subclasses introduced
in the models, e.g., Phen1,...,k_in_Orgi1_with Proti1_Dysf,
whereas the user is only interested in retrieving the classes
used in the annotation, such as Phen1,...,k in our case.
This is easily achieved by extracting the original
class names from the constructed names of each
retrieved class; e.g., Phen1,...,k is extracted from
Phen1,...,k_in_Orgi1_with Proti1_Dysf .
Results from Q1Q4 are displayed in Table 4. Apart
from the OWL profiles required, the result shows how
individuals can be retrieved with IND, and classes in two-
step queries for SUBC and HYBR. DISP does not retrieve
anything due to the use of GCIs without class definitions.
As expected, SUBC generates more classes and axioms
than DISP and HYBR. In IND, there are more axioms
than in SUBC, DISP and HYBR due to the large amount
of relationships created among the individuals while an
OWL model following the IND strategy may not include
any class definitions. IND and SUBC were not able to
retrieve Q4, which includes a disposition axiom and can
be answered only by HYBR.
In the context of an integrative framework, combining
ontologised databases and bio-ontologies, interesting
variations of these competency questions can be imag-
ined. These variations can exploit the axiomatic content
of the linked ontologies, such as subclass axioms or role
restrictions. Expressed in DL queries, these variations
would require none or minor syntactic variations:
Table 4 Query results together with characteristics of the four ontology implementations (without importing BTL2)
Model Q1 Q2 Q3 Q4 Classes Axioms Individuals OWL profile
IND bp1001, cc1001, p1004  24 207 51 OWL-DL
bp2001, cc2001,
bp3001 cc3001
SUBC BProc1 CComp1 Proti1  68 149 0 OWL-EL
DISP     29 70 0 OWL-DL
HYBR BProc1 CComp1 Proti1 Orgi1 48 129 0 OWL-DL
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 10 of 14
 In Q1, a query could target a number of biological
processes by a common ancestor process, or a phase
of a certain process provided by GO;
 In Q2 and Q3, the organism could be substituted by a
biological taxon or other groupings of organisms,
such as provided by the NCBI taxonomy or
SNOMED CT (organism branch);
 In Q1 and Q3, processes could be clustered by
querying for metabolite characteristics. This can be
(for instance) provided by GO extensions, like the
GO  ChEBI linkage.
 In Q4, phenotypes could be queried through how
they are characterised, for instance by certain body
locations. This can be achieved such as provided by
SNOMED CT body structure and disorder.
Users should choose an interpretation approach that
accounts for their respective requirements and fits to
the computational resources available. With IND, the
whole semantic expressivity belongs to the ontology the
individuals are imported into; there is no guarantee that
this ontology is expressive enough to support reasoning
and querying, whereas the patterns provided by SUBC
and HYBR come with axioms that fulfil this task.
Our results indicated that DISP and HYBR promise
better results when reasoning over biomedical databases.
However, limitations may arise for these approaches due
to the nontrivial use of dispositions and scalability prob-
lems, because the reasoning complexity increases with
higher expressivity. In these respects, SUBC might be the
most parsimonious solution, as it may be less problem-
atic for scaling when applying reasoning and performing
queries, with the expense of simulating relations to avoid
the complexity that comes with the use of dispositions.
Discussion
Recently, ontology-aided interpretation of databases has
emerged as a research topic in the biomedical domain,
e.g., for disambiguating the sense of free-text keywords
in query generation to access data repositories [30], or as
a means to interpret proteomics data [31]. As biomedi-
cal observation databases, (e.g.) for proteomics, are still
interpreted manually [7], led to the suggestion of annota-
tion tools that support data interpretation. In these works,
authors suggest a deeper use of ontologies to support
interpretation, which is something that goes beyond of
what is currently performed with functional annotations.
Aiming to attain this purpose, we have proposed four
representation strategies: IND, SUBC, DISP and HYBR.
Interpreting data as individuals (IND)
The representation pattern IND is completely based on
single individuals (ABox entities), present in the underly-
ing experimental assays the results of which are referred to
by the database content. This approach, similarly to ontol-
ogy population [32], refrains from raising any ontological
claim apart from asserting the existence of individuals
and relations among them. The ABox entities can then be
retrieved by DL queries, but the performance problems
of large ABoxes with expressive TBoxes are known [47]
and may therefore hamper the theoretical issue of scal-
ability. In addition, the assertion of existence is an esti-
mation, because data may exhibit errors, especially when
not manually curated and, e.g., extracted from literature
abstracts by natural language processing.
IND andOntology-based Data Access
Previously, OWL models have been created in which
OWL axioms and assertions were automatically gener-
ated from database schemes [33]. These models, how-
ever, represent (first of all) data (information entities)
and not the reality denoted by the data. Our approach,
in contrast, aims at representing the latter, e.g, to which
classes the information entities denotes and further rela-
tions among them. In addition, relations extracted from
databases are semantically idiosyncratic and shallow, e.g.,
neglecting the complexity of the underlying reality, of
which a database schema represents nothing more than a
customized view.
For instance, database integration following the
Ontology-Based Data Access [34] (OBDA) approach
relies on a limited set of ontological relations that are
provided by ontologies. In OBDA, integration relies on
connecting information present in databases with ontolo-
gies, without discussing which interpretation of the data
is more appropriate, i.e., whether the data refer to indi-
viduals, classes, or classes of disposition bearers (neither
of which is expressed in the database nor defined in the
ontology). In practice, OBDA enables the user to retrieve
individuals from a database virtually, e.g., by means of
an ontology used as query vocabulary and an engine to
convert queries in SPARQL [35] to its respective SQL
equivalent, or retrieve RDF triples such as in Bio2RDF
[36] or the UniProt SPARQL Endpoint [37]. Such inter-
pretation issues may be not so relevant for daily database
usage, e.g., accessing or retrieving queries; but for biolog-
ical databases, which include data from real experiments,
raising them is quite relevant.
Approaches that rely on SPARQL queries, like OBDA,
do not go further into how data are to be interpreted,
which is crucial for the biomedical domain. E.g., queries
created in SPARQL and ontologies formalized in OWL
employ different semantics, e.g., of which the latter
enables more complex reasoning tasks (e.g.,classification
and consistency checking) than the former. Reasoning
is crucial for validating content interpreted according to
the semantics provided by ontologies, which frequently
employ OWL.
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 11 of 14
Opposed to the stance that ontology artefacts should,
first, represent purpose-oriented data structures, where
different use cases might require different, partly incom-
patible design decisions [38], we reinforce the interop-
erability aspect of ontologies, which we consider to be
representational artefacts whose representational units
are intended to designate classes or types in reality and
to relate them to each other [39], which also requires
agreement on a set of high-level categories and relations.
Databases and temporal contexts
Ceusters and Smith [40] describe an approach called Ref-
erent Tracking, which is mainly devoted to the identifica-
tion of individuals from Electronic Health Records (EHR).
Referent tracking is based on the generation of triples in
order to record how individuals are related to each other
within a specific context. This approach is similar to our
IND strategy, but equally affected by the problems of non-
referring representational units [41], e.g., in case of false
diagnoses or abandoned care plans.
The domain upper-level ontology BTL2 had been cre-
ated with the purpose of enforcing temporal contexts
for continuant individuals [15]. Whereas in EHRs time
indexing is necessary to represent patients histories, the
biological annotation case described in this paper refrains
from temporal indexing, which may become relevant
when further describing the annotation process itself,
where temporal changes occur as data is automatically
annotated and later reviewed by human curators.
Interpreting data as subclasses (SUBC)
The inability to represent non-denoting database informa-
tion was addressed by the SUBCmodelling patterns which
created a defined subclass for each putative referent. Our
approach for this modelling is agnostic to whether such
classes are instantiated or empty, as their only rationale is
to act as referents of information entities in the database.
Therefore, this representation can (in a way) be considered
ontologically neutral in the sense that we only describe
potentially instantiated classes without being committed
to the actual existence of any instances. Instead, the
OWL model for SUBC exemplify a way to represent dis-
course, regardless of whether meaningful or nonsensical.
However, we have shown that an OWL-EL extract rep-
resented with SUBC successfully retrieves the desired
database content.
On many occasions, researchers already use ontology
terms in biological databases to express relations among
classes, such as that in certain types of organisms, cer-
tain biological processes are performed by or with the aid
of certain proteins. In such cases, the SUBC modelling is
more natural and will reflect the observed reality.
However, one has to deal with a problem that so often
appears in the area of knowledge representation, known
as the frame problem. When one ascribes a certain logi-
cal property to a class, it means that all members should
possess it. But in biology, there are always exceptions
and variations that arguably falsify universal statements
about classes. This all-or-nothing stance can be seen
as a drawback of the SUBC approach, which has been
extensively discussed. The usefulness of a SUBC approach
has been proven in practice in the realms of knowledge
representation applications; nevertheless, proposals to
accommodate exceptions [42], modal [43], and even prob-
abilistic, fuzzy solutions [44] have appeared both in KR
and DL [45, 46].
Interpreting data with dispositions (DISP) and the hybrid
representation (HYBR)
The DISP and HYBR representation strategies, attempts
to extract ontological statements in a stricter sense,
i.e. accounts of scientific laws expressed by universally
quantified statements about all members of a class. This
is possible by introducing dispositions, e.g., by stating
that all organisms with a certain dysfunctional protein are
predisposed to develop certain pathological phenotypes
under certain conditions only.
The DISP approach may be considered ontologically
problematic, as it is quite promiscuous in ascribing dispo-
sitions on class level. What is observed in an experiment is
the outcome of a particular process (which might be a col-
lective process). From the observation of the outcome, it
is inferred that particular process happened, which gives
support to the assumption that the participating partic-
ulars have had the disposition to participate in such a
process.
The problem lies in the extrapolation from the obser-
vation of a single case to all members of a certain class 
such inductive inferences are notoriously difficult. They
may be quite safe when describing the behaviour of small
molecules: knowing that one particular molecule has a
certain disposition, we can quite safely assume that other
molecules of the same kind share this disposition, as we
can think of no intrinsic property that could make a dif-
ference here. However, on the biological level, systems are
much more complex. If a gene defect in a certain individ-
ual organism increases the risk for, e.g., diabetes mellitus,
it does not exclude the possibility that in other organisms
with the same gene defect there is no such risk. We would,
that is, not be justified to ascribe an increased diabetes
risk to the latter population (though we were justified to
ascribe them a certain tendency to do so [19]).
There is no principled contradiction between SUBC and
DISP. The fact that the class inclusion axioms proposed
in DISP to introduce conditions are not suitable for DL
querying, approximates the second and the third mod-
elling approach in the sense that the latter also benefits
from fully defined subclasses. Therefore, the combination
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 12 of 14
of these two modelling styles (HYBR) proved to yield the
best retrieval results with all four competency questions.
General remarks
In this sense, the need for analysing and formalising
the reality behind the database schemes was confirmed
by our effort when creating and querying ontologically
founded interpretation models. Current use of biological
databases might indeed demonstrate that a flat tabular
structure with the fields Protein, Organism, Process,
Cellular component, Molecule and Phenotypemight work
for most standard queries. Its ontological interpretation
under a common upper-level representation aiming at
a formal description of the domain itself and not just
of a specific view thereof, creates added value for more
complex queries that require semantic and not only syn-
tactic integration of biomedical ontology resources.
Entries from biomedical databases derive mostly from
harvesting scientific literature or, otherwise, from the
results of experiments. The veracity of these reports
can be roughly assumed, but any precise representation
should take into account that experimental, measurement,
reporting, and curation errors might occur, so that a cer-
tain number of entries in biological databases may be false
or even contradictory. This requires accounting for the
underlying domain knowledge that does not surface in
the database schema. Examples for these missing links
are, in our examples, that the phenotypes listed in the
database record are at least partly conditioned by protein
dysfunctions.
We do not claim that our interpretation approach is the
only possible one, or that it is exhaustive. In any case, it
might be incomplete and should therefore require refine-
ment and extension by domain experts. For example, a
phenotype might not only be the result of the dysfunc-
tion of a protein, but may also be caused by the complete
absence of this protein in an organism.
The real world applicability of the proposed approaches
has to be assessed with large datasets in the light of
computational constraints.
Conclusion
Interpretations of biological database content tend to
be ambiguous. Accordingly, we formulated the following
questions:
i. How can the implicit knowledge about entities and
relationships described in the structure of a
biological database be represented?
ii. How can the content of databases be interpreted, i.e.,
which domain entities are represented by the data
elements and their connections?
iii. Are structure and content of biological databases of
ontological nature?
iv. If this is the case, how can they be translated into
axioms or assertions in a commonly used ontology
language, and which representational patterns might
be considered?
Answering (i), we presented a method that formalises the
implicit knowledge behind the schemas of databases like
UniProt and Ensembl. In order to account for (ii), we
grounded all classes in an expressive upper-level ontology.
The result is (iii) a seamless representation of database
structure, content and annotations as (iv) an OWLmodel.
Four different ontological interpretations of database
content were developed and compared. The first and
the second strategy represent data individuals denot-
ing either individual processes and their participants
(IND), or defined classes of such entities, using maximally
expressive OWL class terms (SUBC), respectively. The
third strategy (DISP) makes stronger claims by universally
ascribing dispositions to some of the continuant classes
involved. The fourth strategy (HYBR) combines elements
from SUBC and DISP.
The usefulness of the representations was assessed by a
series of competency questions formalised as DL queries,
for which the hybrid representation of database referents
as subclasses together with dispositions (HYBR) yielded
the most convincing result when considering expressiv-
ity and reasoning. However, the SUBC may be well suited
for automating interpretation, as its expressiveness scales
better for reasoning tasks over a large amount of data.
Adding dispositional properties may constitute a useful
add-on, although it is epistemically problematic to auto-
mate the ascription of dispositions to classes based on
cursory evidence on sample individuals gathered in lab
experiments.
Additional files
Additional file 1: Sample data with records retrieved from UniProt and
Ensembl. (XLSX 15.4 kb)
Additional file 2: IND representation example. (OWL 37.3 kb)
Additional file 3: SUBC representation example. (OWL 69.2 kb)
Additional file 4: HYBR representation example. (OWL 31.6 kb)
Additional file 5: DISP representation example. (OWL 14.7 kb)
Acknowledgements
This work was funded by Conselho Nacional de Aperfeiçoamento de Pessoal de
Nível Superior (CAPES) 3914/2014-03 and Conselho Nacional de
Desenvolvimento Científico e Tecnológico (CNPq) 140698/2012-4.
Authors contributions
All authors contributed equally to the manuscript. FSS wrote the document,
reviewed and managed comments from other authors. LJ has written and
contributed to the ontological basics of the manuscript, as well as reviewed
and commented on content and organization. FF and SS reviewed and
supervised the thesis from which the whole material of this paper is based in.
All authors read and approved the final manuscript.
Santana da Silva et al. Journal of Biomedical Semantics  (2017) 8:24 Page 13 of 14
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Centro de Informática, Universidade Federal de Pernambuco, Av. Jornalista
Anibal Fernandes, 50.740-560, Recife, Brazil. 2Núcleo de Telessaúde,
Universidade Federal de Pernambuco, Av. Prof. Moraes Rego, 50670-420,
Recife, Brazil. 3Institut für Philosophie, Universität Rostock, D-18051, Rostock,
Germany. 4Institute for Medical Informatics, Statistics and Documentation,
Medical University of Graz, Auenbruggerplatz 2/V, 8036 Graz, Austria.
Received: 1 November 2016 Accepted: 7 April 2017
Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 
DOI 10.1186/s13326-017-0125-1
RESEARCH Open Access
Learning from biomedical linked data to
suggest valid pharmacogenes
Kevin Dalleau1, Yassine Marzougui1,2, Sébastien Da Silva1, Patrice Ringot1, Ndeye Coumba Ndiaye3
and Adrien Coulet1*
Abstract
Background: A standard task in pharmacogenomics research is identifying genes that may be involved in drug
response variability, i.e., pharmacogenes. Because genomic experiments tended to generate many false positives,
computational approaches based on the use of background knowledge have been proposed. Until now, only
molecular networks or the biomedical literature were used, whereas many other resources are available.
Method: We propose here to consume a diverse and larger set of resources using linked data related either to genes,
drugs or diseases. One of the advantages of linked data is that they are built on a standard framework that facilitates
the joint use of various sources, and thus facilitates considering features of various origins. We propose a selection and
linkage of data sources relevant to pharmacogenomics, including for example DisGeNET and Clinvar. We use machine
learning to identify and prioritize pharmacogenes that are the most probably valid, considering the selected linked
data. This identification relies on the classification of genedrug pairs as either pharmacogenomically associated or
not and was experimented with two machine learning methods random forest and graph kernel, which results are
compared in this article.
Results: We assembled a set of linked data relative to pharmacogenomics, of 2,610,793 triples, coming from six
distinct resources. Learning from these data, random forest enables identifying valid pharmacogenes with a
F-measure of 0.73, on a 10 folds cross-validation, whereas graph kernel achieves a F-measure of 0.81. A list of top
candidates proposed by both approaches is provided and their obtention is discussed.
Keywords: Linked data, Pharmacogenomics, Data mining, Knowledge discovery from databases, Machine learning,
Valid pharmacogenes
Background
Pharmacogenomics (PGx) studies how individual gene
variations cause variability in drug responses [1]. Well
established knowledge in PGx constitutes a basis for
implementing personalized medicine, i.e., a medicine tai-
lored to each patient by considering in particular her/his
genomic context. The state of the art of this domain
lies both in the biomedical literature and in specialized
databases [2, 3], but a large part of it is controversial,
and not yet applicable to medicine. Indeed, this results
from studies difficult to reproduce and that do not fulfill
statistical validation standards for two main reasons: the
*Correspondence: adrien.coulet@loria.fr
Equal contributors
1LORIA (CNRS, Inria Nancy-Grand Est, University of Lorraine), Campus
Scientifique, Nancy, France
Full list of author information is available at the end of the article
small size of populations involved in studies because of
the rarity of gene variants studied and the potential coac-
tion of several variants [4, 5]. It is consequently of interest
to the PGx community to explore any source of evidence
that may contribute to confirming or moderating PGx
state of the art. So far, existing works used either molec-
ular network databases or the biomedical literature (see
Discovery of pharmacogenes subsection). We propose
in this work to explore how other resources, and partic-
ularly Linked Open Data (LOD) may be useful in this
domain.
Linked open data
LOD are constituting a large and growing collection of
datasets that present the main advantages of being rep-
resented in a standard format (based on both RDF and
URIs) and partially connected to each other and to domain
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 2 of 12
knowledge represented within semantic web ontologies
[6]. For these reasons, LOD offer novel opportunities
for the development of successful data integration and
knowledge discovery campaign, as required for the dis-
covery of novel pharmacogenes. LOD are part of a com-
munity effort to build a semantic web, where web and
data resources can be interpreted both by humans and
machines. The recent availability of LOD is particularly
beneficial to the life sciences, where relevant data are
spread over various data sources with no agreement on
a unique representation of biological entities [7]. Conse-
quently, data integration is an initial challenge one faces
if one wants to mine life science data considering sev-
eral data sources. Various initiatives such as Bio2RDF, the
EBI platform, PDBj and Linked Open Drug Data (LODD)
aim at pushing life sciences data into the LOD cloud with
the idea of facilitating their integration [811]. It results
from these initiatives a large collection of life-science data,
unequally connected but in a standard format and avail-
able for mining. Despite good will and emerging standard
practices for publishing data as LOD, several drawbacks
make their use still challenging [12, 13]. Among exist-
ing difficulties we can cite the limited amount of links
between datasets and the limits of implementations of
federated queries.
Pharmacogenomics data and linked data
PharmGKB is a comprehensive database about PGx that
includes manually annotated genedrug relationships [3].
Recently, annotations of PharmGKB have been completed
with a level of evidence going from 1 to 4, distinguish-
ing well validated genedrug relationships (level= 1 ? 2)
from insufficiently validated ones (34), thus point-
ing at knowledge in need for additional investigations
[14]. PharmGKB does not provide its data in RDF, but
parts of PharmGKB have been transformed and pub-
lished in RDF by contributors of the Bio2RDF project,
thus enabling SPARQL queries [15]. Clinical annotations
of PharmGKB are however not freely available. Their
usage is granted through a license agreement, prevent-
ing the data from being redistributed, thus published
as Linked Open Data. Many other databases provides
data that are indirectly relevant to PGx. For instances,
DrugBank [16] provides drugtarget relationships; Clin-
Var [17] provides gene variantphenotype relationships;
SIDER [18, 19] and Medi-Span provides drugphenotype
relationships such as drug adverse events or indications
[20]. Medi-Span is a proprietary database of Wolters
Kluwer Health (Indianapolis, IN) aiming at providing
drug clinical data to clinicians. DGIdb (The Drug Gene
Interaction database) is another interesting initiative that
integrates quasi-exhaustively data about genedrug rela-
tionships, considering 15 distinct sources [21]. DisGenet
is a data integration initiative that focuses on genedisease
relationships and provides data in RDF, including parts of
ClinVar and OMIM [22].
Data integration effort clearly oriented to PGx applica-
tions are less common, particularly if considering seman-
tic web approaches [23]. Hoehndorf et al. integrated and
made available a set of PGx related data that includes
PharmGKB, DrugBank and CTD (the Comparative Tox-
icogenomics Database), using semantic web technologies
[24]. They used the integrated dataset to identify pathways
that may be perturbed in PGx. In this effort of publish-
ing PGx data, Coulet et al. extracted about 40,000 PGx
relationships from the biomedical literature and published
them in the form of RDF statements [25].
Mining linked data
Suggesting valid pharmacogenes in this work is seen as
proposing novel genedrug relationships from an RDF
graph, which in turn can be described as a link prediction
problem. Many works have focused on the link prediction
problem, studying various approaches such as machine
learning [26, 27], graph mining [2830], identity resolu-
tion [31, 32] and data visualisation [33]. Some of these
methods obtain good results, but all are dependent from
the input graphs (its quality, topology, etc.) and are hard
to reuse for new applications. Recently, de Vries and de
Rooij proposed a complete framework for applying Graph
Kernel (GK) in an adaptive manner to RDF graphs [34].
GK are machine learning methods that have the ability to
deal directly with graph data, particularly by computing
kernel functions that evaluate similarity between graphs
or pieces of graphs [35]. The framework of de Vries and
de Rooij is implemented in an open source library named
Mustard [36]. It enables classifying RDF instances consid-
ering their neighborhood in the graph. This neighborhood
is encoded within features such as labels of edges or graph
substructures such as walks (i.e., linear paths) or sub-
graphs. In the work we present here, we reused Mustard
and fitted its capability of instance classification to the
case of link prediction.
In relation with PGx research, Percha et al. mined the
set of RDF statements extracted from text by Coulet et al.
with a Random Forest (RF) algorithm and successfully
predicted drugdrug interactions [37]. With the aim of
predicting pharmacogenes, we experimented as Percha
et al. with the RF algorithm in the preliminary stage of
this work [38]. First results we obtained with RF are here
updated and compared with GK approaches.
Discovery of pharmacogenes
Hansen et al. proposed a method based on a logistic clas-
sifier to generate candidate pharmacogenes, using data
from PharmGKB, DrugBank, and proteinprotein inter-
actions from InWeb [39]. An issue with this approach
is that PharmGKB and DrugBank are manually curated
Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 3 of 12
from the literature and are consequently expensive to
maintain and update. Garten et al. answered this issue
by proposing an automatic method that consider directly
(and only) the literature [40]. They improved the results
obtained by Hansen et al. by considering genedrug pairs
co-occurring in sentences of the PGx literature. Recently,
Funk et al. proposed also to use the biomedical literature,
plus GO annotations, to identify pharmacogenes [41].
They obtain a high F-measure and AUC-ROC (0.86 and
0.86), but proposed a coarse-grained classification that is
only binary (pharmacogene or not), avoiding any ranking
of the candidates.
Semantic web technologies have also been experi-
mented for PGx knowledge discovery. Dumontier and
Villanueva-Rosales proposed a knowledge representation
of the domain and benefit from reasoning mechanisms
to answer sophisticated queries related to depression
drugs [42]. Coulet et al. used patient data to instantiate a
description logics knowledge base, then extracted associ-
ation rules from it to identify gene variantdrug response
associations [43]. More generally, advantages that seman-
tic web technologies may offer to PGx and personalized
medicine are listed in [23].
We present here a method that consists in mining a set
of diverse linked data sources to help validating uncer-
tain genedrug relationships. This method can be divided
in three steps: first, selecting and connecting relevant
PGx linked data; second, formatting linked data to train
and compare two machine learning algorithms (RF and
GK); third, classify and rank candidate pharmacogenes
with these two approaches. The paper is organized as
follow: next section presents our methods for preparing,
then learning from the linked data; next, Results Section
presents the evaluation and the use of the two machine
learning approaches we considered and brings elements
of interpretation; the two last sections discuss our results
and conclude on this work.
Methods
Data preparation
Data selection Initial step is to select a set of data that
include relevant data about PGx genedrug relationships.
Figure 1 gives a general overview of the type of data we
consider for this study: three types of entities, gene, pheno-
type and drug; and relationships between them, i.e., gene
phenotype, phenotypedrug and genedrug relationships.
Fig. 1 Overview of the type of entities and relationships considered and their origin. Entities are of three distinct types: Gene, Phenotype and Drug.
GenePhenotype relationships are coming from ClinVar and DisGeNET, PhenotypeDrug relationships from SIDER and Medi-Span, GeneDrug
relationships from DrugBank. In addition, we included gene and drug entities from PharmGKB to enable building the training and test sets.
Equivalence mappings are defined between entities of the same type but of different origin. In addition to entityentity relationships, we consider
some attributes that are specific to entities, such as the ATC class of drug that is a drug attribute. Naming of different parts of the data (e.g., GP links,
gene attributes) is used later in the step of formatting of the linked data. The detailed schema of the data is provided Fig. 2
Dalleau et al. Journal of Biomedical Semantics  (2017) 8:16 Page 4 of 12
We selected data sources manually but oriented our selec-
tion to sources providing typed relationships and limited
ourselves to two sources per relationship. As a result,
we selected ClinVar and DisGeNET for genephenotype;
SIDER andMedi-Span for phenotypedrug; DrugBank for
genedrug relationships. PharmGKB completes the set of
data sources to enable building the training and test sets
(see Training and test sets subsection).
Data RDFization The second step is about turning
selected data in a standardized RDF graph, available at
https://pgxlod.loria.fr. We benefit from the fact that Dis-
GeNET [44], SIDER [45] and DrugBank [46] are already
available online in the form of LOD and reused them.
DisGeNET includes data from ClinVar, but because it
includes only a part of it, we made our own RDF
version of ClinVar following guidelines and scripts of
the Bio2RDF project. We completed the Bio2RDF ver-
sion of PharmGKB locally with genedrug relationships
manually annotated by PharmGKB but not openly dis-
tributed [15]. Similarly, we transformed drug indications
and side-effects from Medi-Span in the form of RDF
triples and loaded them into our SPARQL server. For
the management of RDF data, we rely on Blazegraph, a
graph database system that provides support for RDF and
SPARQL. Medi-Span data, as PharmGKB clinical anno-
tations are protected by a license agreement and can not
be redistributed. This explains why we are providing a
controlled access to our set of PGx linked data. We pro-
pose to open this dataset, on demand, with licensees.
Figure 2 presents the detailed schema (i.e., type of enti-
ties and relationships) of the linked data we selected and
consider for mining. Figure 3 presents an example of
data from the PGx linked data, instantiating the schema
presented Fig. 2. The SPARQL query returning data pre-
sented in Fig. 3 is provided in Additional file 1. Other
SPARQL queries, such as the one provided in Additional
file 2, may be built by considering the partial data schema
presented Fig. 2.
Mapping definition To define mappings, we first relied
on standard identifiers such as NCBI Gene ID found in
DisGeNET and ClinVar URIs and UMLS CUI found in
DisGeNET, ClinVar, SIDER and Medi-Span. We defined
regular expressions over URIs to isolate identifiers and
when twomatch, we define amapping. Figure 3 shows two
Fig. 2 Schema of the pharmacogenomic linked data selected for this study. Entities are related to either Genes, Phenotypes (or Diseases) or Drugs.
We artificially enriched the data with an additional type of entity: genedrug pairs. These entities link exactly one gene and one drug and are the
nodes of the graph we classify either as associated or not associated from a PGx point of view, to valid candidate pharmacogenes. For mapping
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 
DOI 10.1186/s13326-017-0137-x
RESEARCH Open Access
Discovering associations between
adverse drug events using pattern structures
and ontologies
Gabin Personeni1* , Emmanuel Bresso1, Marie-Dominique Devignes1, Michel Dumontier2,3,
Malika Smaïl-Tabbone1 and Adrien Coulet1,3
Abstract
Background: Patient data, such as electronic health records or adverse event reporting systems, constitute an
essential resource for studying Adverse Drug Events (ADEs). We explore an original approach to identify frequently
associated ADEs in subgroups of patients.
Results: Because ADEs have complex manifestations, we use formal concept analysis and its pattern structures, a
mathematical framework that allows generalization using domain knowledge formalized in medical ontologies.
Results obtained with three different settings and two different datasets show that this approach is flexible and allows
extraction of association rules at various levels of generalization.
Conclusions: The chosen approach permits an expressive representation of a patient ADEs. Extracted association
rules point to distinct ADEs that occur in a same group of patients, and could serve as a basis for a recommandation
system. The proposed representation is flexible and can be extended to make use of additional ontologies and various
patient records.
Keywords: Adverse drug event, Association rules, Ontologies, Patient data, Pattern structures, Pharmacovigilance
Background
Adverse Drug Events (ADEs) occur unevenly in differ-
ent groups of patients. Their causes are multiple: genetic,
metabolic, interactions with other substances, etc. Patient
data, in the form of either Electronic Health Records
(EHRs) or adverse effects reports have been successfully
used to detect ADEs [1, 2]. We hypothesize that mining
EHRs may reveal that subgroups of patients sensitive to
some drugs are also sensitive to others. In such a case, sev-
eral ADEs, each caused by different drugs, could be found
to occur frequently in a subgroup of patients. While this
is known to be true in certain classes of drugs, we further
hypothesize that such associations can be found across
different classes. We propose a method to identify these
frequently associated ADEs in patients subgroups.
*Correspondence: gabin.personeni@loria.fr
1LORIA (CNRS, Inria NGE, Université de Lorraine), Campus Scientifique, F-54506
Vanduvre-lès-Nancy, France
Full list of author information is available at the end of the article
The main issue to reach this goal is that ADE manifes-
tations are complex and that they are reported in variable
manners. Indeed, ADEs are not limited to the simple case
of one drug causing one phenotype but may be an asso-
ciation between several drugs and several phenotypes.
Furthermore, these drugs and phenotypes can be reported
using different vocabularies and with varying levels of
detail. For instance, two clinicians may report the same
ADE caused by warfarin, an anticoagulant drug, either as
warfarin toxicity or with amore precise description such
as ulcer bleeding caused by warfarin. As such, biomed-
ical ontologies provide helpful resources to consider the
semantic relationships between ADEs.
In [3], Roitmann et al. proposed a vector represen-
tation of patient ADE profiles: a patient is represented
by a feature vector in which each feature is one phe-
notype experienced by the patient. All phenotypes are
here considered as independent features. This representa-
tion is used with clustering algorithms to group patients
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 2 of 13
into clusters in which prevalent drugs and phenotypes
can be identified. This work could be expanded by con-
sidering biomedical ontologies coupled with a semantic
similarity measure such as the one described in Devignes
et al. [4], to cluster together patients taking distinct but
similar drugs and expressing distinct but similar pheno-
types. However, a limitation of a vector representation
is that it aggregates all ADEs of a patient in a single
object. In this paper, we propose a representation of the
ADEs of a patient that preserves the distinctness of these
events.
In [5], Winnenburg et al. extracted drug-phenotype
pairs from the litterature to explore the relationships
between drugs, drug classes and their adverse reactions.
Adverse event signals are computed both at the drug and
drug class levels. This work illustrates that some drug
classes can be associated with a given adverse effect, and
further investigates the association at the individual drug
level. In cases where the association with the adverse
effect is present for every drug in the class, it demonstrates
the existence of a class effect. Otherwise, the association
is present for only some drugs of the class, and cannot be
intrinsically attributed to the class itself. This result shows
that it is possible to consider ADEs either at the invidid-
ual drug level or at the drug class level. The approach
we propose in this paper addresses this possibility, both
at the level of ADE representation and inside the data
mining approach itself, which allows generalization with
biomedical ontologies. In addition, we are also capable
of detecting ADE associations involving different classes
of drugs.
For this purpose, we use an extension of Formal Concept
Analysis (FCA) [6] called pattern structures [7] in combi-
nation with ontologies to enable semantic comparison of
ADEs. FCA has been successfully used for signal detec-
tion in pharmacovigilance: in [8, 9], FCA is used to detect
signals in a dataset of ADEs described with several drugs
causing a phenotype. In this case, FCA permits to mine
for associations between a set of drugs and a phenotype.
In this article, pattern structures allow us to extend the
descriptions of ADEs with biomedical ontologies, and to
mine higher-order associations, i.e., associations between
ADEs.
We experimented with two types of datasets. A first
dataset was extracted from EHRs of patients diag-
nosed with Systemic Lupus Erythematosus (SLE), a
severe autoimmune disease. Such patients frequently
experience ADEs as they often take multiple and
diverse drugs indicated for SLE or derived patholo-
gies [10]. Our second dataset was extracted from
the U.S. Food & Drug Administration Adverse Event
Reporting System (FAERS). This dataset was linked
to biomedical ontologies thanks to a novel resource,
AEOLUS [11].
Methods
ADE definition
An ADE is a complex event in that it may often involve
several drugs, and manifest through several phenotypes.
An ADE can then be characterized by a set of drugs,
and a set of phenotypes. To facilitate comparison between
ADEs, we consider sets of active ingredients of drugs,
rather than sets of commercial drug names. In the rest of
this article, we use the term drug to denote an active
ingredient. In this study, we represent an ADE as a pair
(Di,Pi), where Di is a set of drugs, and Pi is a set of phe-
notypes. Table 1 presents examples of ADEs that could be
extracted from the EHRs, and will serve here as a run-
ning example. Table 2 provides the origin and label of each
ontology class code used in this article.
SLE EHR dataset from STRIDE
Our first dataset is a set of 6869 anonymized EHRs of
patients diagnosed with SLE, extracted from STRIDE, the
EHR data warehouse of Stanford Hospital and Clinics [12]
between 2008 and 2014. It documents about 451,000 hos-
pital visits with their relative dates, diagnoses encoded
as ICD-9-CM phenotype codes (International Classifica-
tion of Diseases, Ninth Revision, Clinical Modification)
and drug prescriptions as a list of their ingredients, repre-
sented by RxNorm identifiers.
We first establish a list of ADE candidates for each
patient EHR. From each two consecutive visits in the EHR,
we extract the set of drugs Di prescribed during the first
visit and the diagnoses Pi reported during the second. The
interval between the two consecutive visits must be less
than 14 days, as it is reasonable to think that a side effect
should be observed in such a time period after prescrip-
tion. Moreover, Table 3 shows that increasing this interval
does not significantly increase the number of patients in
our dataset. An ADE candidate Ci is thus a pair of sets
Ci = (Di,Pi). We retain in Pi only phenotypes reported
as a side effect for at least one drug of Di in the SIDER
4.1 database of drug indications and side effects [13]. We
remove candidates where Pi is empty. Furthermore, we
remove an ADE candidate (D1,P1) if there exists for the
same patient another ADE candidate (D2,P2) such that
Table 1 Example of a dataset containing 3 patients with 2 ADEs
each, in lexicographic order
Patient ADEs
P1 ({acetaminophen},{ICD 599.9}) ({prednisone},{ICD 599.8})
P2 ({prednisone},{ICD 599.8}) ({prednisone},{ICD 719.4})
P3 ({acetaminophen},{ICD 719.4}) ({acetaminophen, prednisone},
{ICD 599.9})
Class labels: ICD 599.8 is other specified disorders of the urethra and urinary tract,
ICD 599.9 is unspecified disorders of the urethra and urinary tract, ICD 719.4 is
pain in joint
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 3 of 13
Table 2 This table provides the origin and label of each ontology
class code used in this article
Ontology code Ontology Label
A02B ATC Drugs for peptic ulcer and
gastro-oesophageal reflux disease
A02BC ATC Proton pump inhibitors
A04A ATC Antiemetics and antinauseants
A06A ATC Drugs for constipation
A07A ATC Intestinal antiinfectives
B01A ATC Antithrombotic agents
B03X ATC Other antianemic preparations
B05X ATC I.V. solution additives
C01BB03 ATC Tocainide
C03C ATC High-ceiling diuretics
C05B ATC Antivaricose therapy
C07A ATC Beta blocking agents
C08D ATC Selective calcium channel blockers with
direct cardiac effects
C08DB ATC Benzothiazepine derivatives
C09A ATC Ace inhibitors, plain
C10A ATC Lipid modifying agents, plain
G04BE ATC Drugs used in erectile dysfunction
G04BE04 ATC Yohimbin
H02A ATC Corticosteroids for systemic use, plain
H02AA03 ATC Desoxycortone
H02AB ATC Glucocorticoids
H02AB07 ATC Prednisone
N02A ATC Opioids
N02B ATC Other analgesics and antipyretics
N02BE01 ATC Paracetamol / Acetaminophen
N05B ATC Anxiolytics
N05C ATC Hypnotics and sedatives
N06BC ATC Xanthine derivatives
N06BC01 ATC Caffeine
R05D ATC Cough suppressants, excl. combinations
with expectorants
R06A ATC Antihistamines for systemic use
R06AA ATC aminoalkyl ethers
R06AA09 ATC Doxylamine
S01A ATC Antiinfectives
S01AX ATC Other antiinfectives in ATC
280-289 ICD-9-CM Diseases of the blood and blood-forming
organs
280 ICD-9-CM Iron deficiency anemias
285.9 ICD-9-CM Anemia, unspecified
287.5 ICD-9-CM Thrombocytopenia, unspecified
427.31 ICD-9-CM Atrial fibrillation
428 ICD-9-CM Heart failure
Table 2 This table provides the origin and label of each ontology
class code used in this article (Continued)
428.0 ICD-9-CM Congestive heart failure, unspecified
428.9 ICD-9-CM Heart failure, unspecified
580-629 ICD-9-CM Diseases of the genitourinary system
580 ICD-9-CM Acute glomerulonephritis
586 ICD-9-CM Renal failure, unspecified
599.8 ICD-9-CM Other specified disorders of urethra and
urinary tract
599.9 ICD-9-CM Unspecified disorder of urethra and urinary tract
710-739 ICD-9-CM Diseases of the musculoskletal system and
connective tissue
710 ICD-9-CM Diffuse diseases of connective tissue
719.4 ICD-9-CM Pain in joint
The ontologies used in this article are described in the Medical Ontologies section
on page 4
D1 ? D2: indeed, reiterated prescriptions of drugs may
indicate that they are safe for this patient.
In such cases, where several ADEs have comparable sets
of drugs, we only retain the ADEwith themaximal set, i.e.,
themost specialized set of drugs. Indeed, as we aim to find
associations between different ADEs, we thus avoid con-
sidering multiple times such similar sets of drugs. Finally,
we keep only patients having experienced at least two
ADEs, as our goal is to mine frequently associated ADEs.
After filtering, we obtain a total of 3286 ADEs for 548
patients presenting at least two ADEs.
FAERS dataset
FAERS publishes a database gathering ADEs reported by
patients, healthcare professional and drug manufacturers
in the United States. It is used for postmarketing phar-
macovigilance by the U.S. Food & Drug Administration,
data mining of signals in pharmacovigilance [2] or of
adverse drug-drug interactions [14]. A recently-published
resource, AEOLUS [11] maps FAERS drugs and pheno-
types representations to RxNorm and SNOMED CT (Sys-
tematized Nomenclature of Medicine  Clinical Terms)
respectively. We used this tool to rebuild a database of
FAERS reports, linked to RxNorm and SNOMED CT,
from the fourth quarter of 2012 to the second quarter of
2016 included.
Table 3 Number of patients with at least 2 selected ADEs and
number of ADEs for these patients, for different maximum
inter-visit interval in days
Interval (days) 1 2 6 10 14 18 22 26 30
|Patients| 434 461 498 526 548 555 558 564 576
|ADEs| 2396 2587 2902 3110 3286 3388 3454 3501 3621
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 4 of 13
Each FAERS report lists a set of prescribed drugs Di
and the a of experienced phenotypes Pi. Thus, we can for-
malize each report as a pair of sets (Di,Pi). These reports
are grouped in cases, enabling us to identify additional
reports that follow up an initial ADE. We selected, in
the FAERS database, cases with multiple reported ADEs,
excluding ADEs where the set of drugs is included in
another ADE of the same case. With these constraints, we
extract 570 cases with two or more distinct ADEs, for a
total of 1148 ADEs.
Medical ontologies
We use three medical ontologies, considering only their
class hierarchy, to enable semantic comparisons of drugs
and phenotypes when comparing ADEs:
 ICD-9-CM describes classes of phenotypes, as it is
used in STRIDE to describe diagnoses;
 SNOMED CT is an ontology of medical terms, which
we use to describe the phenotypes of FAERS, using
the mappings provided by AEOLUS;
 The Anatomical Therapeutic Chemical Classification
System (ATC) describes classes of drugs. In this
work, we used only the three most specific levels of
ATC: pharmacological subgroups, chemical
subgroups and chemical substances.
Association rule mining
Assocation rule mining [15] is a method for discovering
frequently associated items in a dataset. Association rule
mining is performed on a set of transactions, represented
as sets of items. Association Rules (ARs) are composed
of two sets of items L and R, and are noted L ? R.
Such a rule is interpreted as when L occurs in a tran-
scation, R also occurs. Note that ARs do not express any
causal or temporal relationship between L and R. ARs
are qualified by several metrics, including confidence and
support. The confidence of a rule is the proportion of
transactions containing L that also contains R. The sup-
port of a rule is the number of transactions containing
both L and R. For instance, if a rule A,B ? C has a
confidence of 0.75 and a support of 5, then, C occurs in
3
4 of the transactions where A and B occur, and A,B,C
occur together in 5 transactions. Note that the support
may also be represented relatively to the total number of
transactions in the dataset, e.g., 5500 for a dataset of 500
transactions.
Several algorithms for association rule mining, such as
Apriori, have been proposed, based on frequent itemsets
[16]. Such frequent itemsets can be identified using an
itemset lattice [17]. FCA offers facilities for building lat-
tices, identifying frequent itemsets and association rule
mining [18]. In the following section, we present FCA and
its extension pattern structures, as a method to mine ARs.
Formal concept analysis and pattern structures
Formal Concept Analysis (FCA) [6] is a mathematical
framework for data analysis and knowledge discovery. In
FCA a dataset may be represented as a concept lattice,
i.e., a hierarchical structure in which a concept represents
a set of objects sharing a set of properties. In classical
FCA, a dataset is composed of a set of objects, where each
object is described by a set of binary attributes. Accord-
ingly, FCA permits describing patients with the ADEs
they experienced represented as binary attributes, as illus-
trated in Table 4. The AR ADE1 ? ADE3 that can be
extracted from this dataset has a support of 2 and a con-
fidence of 23 . This AR expresses that two thirds of the
patients that experienced ADE1 also experienced ADE3,
and that the rule was verified by 2 patients (P1 and P3)
in the dataset. However, FCA does not take into account
the similarity between attributes. For instance, both ADE3
and ADE4 could be caused by the same drugs, while pre-
senting slightly different phenotypes. In such a case, we
may want to extract a rule expressing that patients who
experienced ADE1 also experienced an ADE similar to
ADE3 or ADE4.
Accordingly, approaches extracting ARs from sets of
binary attributes are limited as the similarity of attributes
is not considered. This is the case of algorithms such as
Apriori, or classical FCA approaches.We propose to intro-
duce a more detailed representation of patients ADEs,
along with a fine-grained similarity operator.
Pattern structures generalize FCA in order to work with
a set of objects with descriptions not only binary but of
any nature such as sets, graphs, intervals [7, 19]. Par-
ticularly, pattern structures have been used to leverage
biomedical knowledge contained in ontology-annotated
data [20].
A pattern structure is a triple (G, (D,), ?), where:
 G is a set of objects, in our case, a set of patients,
 D is a set of descriptions, in our case, representations
of a patients ADEs,
 ? is a function that maps objects to their descriptions.
  is a meet operator such that for two descriptions X
and Y in D, X  Y is the similarity of X and Y : X  Y
is a description of what is common between
descriptions X and Y . It defines a partial order ? on
elements of D. X ? Y denotes that Y is a more
specific description than X, and is by definition
Table 4 Example of a binary table to be used for extraction of
associations between ADEs using Formal Concept Analysis (FCA)
Patient ADE1 ADE2 ADE3 ADE4
P1 × ×
P2 × ×
P3 × × ×
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 5 of 13
equivalent to X  Y = X. Generalization on object
descriptions is performed through the use of the
meet operator. In the following section, we define
three distinct meet operators (1, 2, 3) that enable
considering similarities between ADE descriptions at
different levels of granularity. This section also
illustrates the application of pattern structures.
In pattern structures, the derivation operator . defines
a Galois connection between sets of objects and descrip-
tions, as follows:
A = g?A?(g) for a set of objects A
d = {g ? G | d ? ?(g)} for a description d
Intuitively, A is the most precise description for the set
of objects A, and d is the set of objects described by a
description more specific than d. A pattern concept is a
pair (A, d) with A = d and d = A. Pattern structures
enable building a lattice of pattern concepts, which allow
associating a set of patients with a shared description of
their ADEs, based on their similarity.
In our study, G is the set of patients that are related
through ? to the description of their ADEs in D. We
have designed different experiments using pattern struc-
tures, each providing their own definition of the triple
(G, (D,), ?).
Experimental design
In this section, we describe three experiments to extract
ARs betweenADEs. Each one defines a different represen-
tation of patient ADEs and a different setting of pattern
structures, making increasing use of ontologies.
Experiment 1: Pattern structurewithout semantic comparison
Table 4 presents a naive representation of patient ADEs.
However, we want a representation that takes into account
similarity between ADEs, instead of considering ADEs
as independent attributes. Accordingly, we propose in
this first experiment a representation that groups ADEs
with high level phenotypes and we define an operator to
compare their sets of drugs.
We define here the pattern structure (G, (D1,1), ?1):
objects are patients, and a patient description of D1 is
a vector of sub-descriptions, with first-level ICD-9-CM
classes as dimensions. Each sub-description is a set of
drug prescriptions, i.e., a set of sets of drugs. For instance,
considering only the two ICD-9-CM classes of Table 5:
?1,ICD 580-629(P1) = {{prednisone}, {acetaminophen}}
?1,ICD 710-739(P1) = ?
Here, ADEs are decomposed w.r.t. their phenotypes.
Sub-descriptions are associated to a first-level ICD-9-CM
Table 5 Example of representation of patient ADEs for
(G, (D1,1), ?1), with two first-level ICD-9-CM classes: diseases of
the genitourinary system (580-629), and of the musculoskeletal
system and connective tissue (710-739)
Patient ICD 580-629 (genitourinary system) ICD 710-739
(musculoskeletal system)
P1 {{prednisone}, {acetaminophen}} ?
P2 {{prednisone}} {{prednisone}}
P3 {{prednisone, acetaminophen}} {{acetaminophen}}
class to represent ADEs: the patient presents a phe-
notype of that class after taking a prescription in that
sub-description. In the example presented in Table 5, the
patient P1 experienced an ADE with a phenotype from
the ICD-9-CM class 580-629 twice: once after prescrip-
tion of prednisone, and another time after prescription of
acetaminophen.
We define a sub-description as a set of prescriptions,
where none of the prescriptions are comparable to each
other by the partial order ?. We then define the meet
operator 1, such that, for every pair of descriptions
(X,Y ) ofD1:
X 1 Y = max
(?, {x ? y | (x, y) ? X × Y})
where max(?i, S) is the unique subset of maximal ele-
ments of a set S given any partial order ?i. Formally,
max(?i, S) = {s | x.(s ?i x)}. In the present case,
it retains only the most specific set of drugs prescribed
in the description. For instance, given four drugs d1
through d4:
{{d1, d2, d3}} 1 {{d1, d2}, {d2, d4}}
= max (?, {{d1, d2, d3} ? {d1, d2}, {d1, d2, d3} ? {d2, d4}})
= max (?, {{d1, d2}, {d2}})
= {{d1, d2}}
We only retain {d1, d2} since {d2} ? {d1, d2} and {d1, d2}
is the only ?-maximal element. Indeed, the semantic of
{d2}  a prescription that contains the drug d2  is more
general than the semantic of {d1, d2}  a prescription that
contains both the drugs d1 and d2.
Given that each patient has a description for each
first-level ICD-9-CM class, the meet operator defined
for a sub-description can be applied to a vector of sub-
descriptions:
?1(P1) 1 ?1(P2) = ??1,1(P1), . . . , ?1,n(P1)?1
??1,1(P2), . . . , ?1,n(P2)?
= ??1,1(P1) 1 ?1,1(P2), . . . ,
?1,n(P1) 1 ?1,n(P2)?
Figure 1 shows the semi-lattice associated with this pat-
tern structure and the data in Table 5. Nevertheless, this
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 6 of 13
Fig. 1 Semi-lattice representation of the data in Table 5 using the
pattern structure (G, (D1,1), ?1), where arrows denote the partial
order ?1
example shows that in the absence of semantics between
descriptions, generalization rapidly produces empty sets
devoid of information.
Experiment 2: Extending the pattern structure with a drug
ontology
Using a drug ontology permits to find associations
between ADEs related to classes of drugs rather than
individual drugs. Thus, we extend the pattern structure
described previously to take into account a drug ontol-
ogy: ATC. Each drug is replaced with its ATC class(es), as
shown in Table 6. We notice that the fact that one drug
can be associated with several ATC classes is handled by
our method as sets of drugs become represented as sets of
ATC classes.
We define this second pattern structure (G, (D2,2), ?2)
where descriptions of D2 are sets of prescriptions with
drugs represented as their ATC classes. In order to com-
pare sets of classes from an ontology O, we define an
intermediate meet operator O , for x and y any two sets
of classes ofO:
x O y = max
(
, {LCA (cx, cy
) | (cx, cy
) ? x × y})
where LCA(cx, cy) is the least common ancestor of cx and
cy in O, and 
 is the ordering defined by the class hierar-
chy ofO. For any set of classes S, max(
, S) is the subset of
most specific ontology classes of S (they have no descen-
dant in S). Thus, x O y is the subset of most specific
ancestors of classes in x and y. From O we define the
partial order ?O , which compares two sets of ontology
classes, x and y, such that x ?O y ? x O y = x and
x ?O y denotes that y is a more specific set of ontology
Table 6 Example of representation of patient ADEs for
(G, (D2,2), ?2)
Patient ICD 580-629 (genitourinary system) ICD 710-739
(musculoskeletal system)
P1 {{H02AB07},{N02BE01}} ?
P2 {{H02AB07}} {{H02AB07}}
P3 {{H02AB07, N02BE01}} {{N02BE01}}
P4 {{H02AA03}} ?
Class labels: H02AA03 is desoxycortone, H02AB07 is prednisone, N02BE01 is
acetaminophen
classes than x. We then define the meet operator 2 such
that for every pair of descriptions (X,Y ) ofD2:
X 2 Y = max
(?O ,
{
x O y | (x, y) ? X × Y
})
This pattern structure allows generalization of ADEs
involving different drugs that share a pharmacological
subgroup. For instance:
?(P1) 2 ?(P4) = ?{{H02AB07}, {N02BE01}} ,??2
?{{H02AA03}},??
= ?max(?O , {{H02AB07} O {H02AA03},
{N02BE01} O {H02AA03}}),??
= ?max(?O , {{H02A}, {}}),??
= ?{{H02A}},??
Here, we use O to compare sets of drugs. Com-
parison of {H02AA03} (desoxycortone) and {H02AB07}
(prednisone) yields their common ancestor in the
ontology: {H02A} (corticosteroids for systemic use,
plain). We observe that {N02BE01} (acetaminophen) and
{H02AA03} (desoxycortone) only have the root  of the
ontology in common, thus {N02BE01} O {H02AA03} =
{}. The max function excludes it from the final result,
as it is redundant with {H02A}, since {} ?O {H02A}.
The vector ?{{H02A}},?? represents the closest general-
ization of the descriptions of patients P1 and P4, and can
be read as: drugs of the class H02A (corticosteroids for
systemic use, plain) are associated with a phenotype in the
ICD-9-CM class diseases of the genitourinary system (580-
629), and no drugs are associated to the ICD-9-CM class
diseases of musculoskeletal system and connective tissue
(710-739).
Experiment 3: Extending the pattern structure with a drug
and a phenotype ontology
We define a third pattern structure that permits the use of
both ATC and a phenotype ontology for better specializa-
tion of phenotypes compared to the previous experiment.
As this experimental design can be applied to both the
EHR and FAERS datasets, we design a pattern structure
that can operate with any drug and phenotype ontologies.
We apply it to our EHR dataset with ATC and ICD-9-CM,
and to the FAERS dataset with ATC and SNOMED CT.
To avoid over-generalization, we excluded the twomost-
general levels of ICD-9-CM and the three most-general
levels of SNOMED CT. Table 7 illustrates the data repre-
sentation used with this pattern structure, using ATC and
ICD-9-CM. Here, ADEs are represented as vectors ?Di,Pi?
with two dimensions: the set of drugs Di associated with
the set of phenotypes Pi. A patient description is then a set
of such vectors.
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 7 of 13
Table 7 Example of representation of patient ADEs for
(G, (D3,3), ?3)
Patient Description
P1 {?{H02AB07},{ICD 599.8}?, ?{N02BE01},{ICD 599.9}?}
P2 {?{H02AB07},{ICD 599.9}?, ?{H02AB07},{ICD 719.4}?}
P3 {?{H02AB07,N02BE01},{ICD 599.9}?, ?{N02BE01},{ICD 719.4}?}
Class labels: H02AA03 is desoxycortone, H02AB07 is prednisone, N02BE01 is
acetaminophen, ICD 599.8 is other specified disorders of the urethra and urinary
tract, ICD 599.9 is unspecified disorders of the urethra and urinary tract, ICD 719.4
is pain in joint
We define the pattern structure (G, (D3,3), ?3), where
descriptions of D3 are sets of ADEs. We first define
an intermediate meet operator ADE on our ADEs
representations:
vx ADE vy = ?Dx,Px? ADE ?Dy,Py?
=
?
??
??
?Dx O Dy,Px O Py? if both dimensions contain
at least one non-root class
??,?? otherwise.
The operator ADE applies the ontology meet opera-
tor O on both dimensions of the vector representing the
ADE, using either ATC or ICD-9-CM as the ontology O.
Both dimensions of the resulting vector needs to contain
non-root ontology classes for it to constitute a represen-
tation of an ADE. If it is not the case, we set it to ??,?? to
ignore it in further generalizations.
We define the meet operator 3 such that for every pair
of descriptions (X,Y ) ofD3:
X 3 Y = max
(?ADE ,
{
vx ADE vy |
(
vx, vy
) ? X × Y})
Compared to 2, 3 introduces a supplementary level
of computation with ADE , which generalizes ADEs and
applies O to an additional ontology: ICD-9-CM.
Extraction and evaluation of associations rules
The pattern structures described previously can be used
to build concept lattices, where each concept associates a
set of patients with the similarity of their ADEs descrip-
tions. Such a concept lattice allows for identifying fre-
quent ADEs descriptions, which can be used for extract-
ing Association Rules (ARs). An AR is identified between
two related concepts in the lattice, with descriptions ?(l)
and ?(r) such that ?(l) < ?(r). Thus, such an AR com-
prises a left-hand side L = ?(l) and a right-hand side
R = ?(r) ? ?(l), where ? denotes set difference. Such a
rule is noted L ? R.
This process can be expected to generate a large amount
of rules, among which ARs serving our goal of detect-
ing associations between ADEs must be identified. We
therefore filter ARs according to the following conditions:
 The right-hand side R of the AR contains at least one
ADE, noted as (DR,PR) for which there is no ADE
(DL,PL) in the left-hand side L such that either DR
and DL are ?O comparable, or PR and PL are ?O
comparable. This condition ensures that the
right-hand side of the rule introduces new drugs and
phenotypes unrelated to those of the left-hand side,
i.e., the association between the ADEs of both sides is
not trivial.
 As patients in the EHR dataset are treated for
Systemic Lupus Erythematosus (SLE), rules must not
include related phenotypes (ICD-9-Cm class 710 and
descendants).
ARs extracted from the SLE patients EHR dataset
were evaluated by computing their support in the entire
STRIDE EHR dataset. Selected ARs with the largest sup-
port were transformed into SQL queries, in order to
retrieve matching patients from the STRIDE database.
Statistical analysis of the extracted ADE associations
Figures 2 and 3 show an overview of ATC drug classes
associated by the ARs extracted in the third EHR experi-
ment. We isolated every pair of ATC classes associated by
ARs, i.e., one ATC class or one of its subclass is present
in the left-hand side of the AR, and one is present in
its right-hand side. Figure 2 shows the frequency of such
associations and Fig. 3 shows, for the significant ones,
the difference to the frequency obtained if the association
would be random. For each pair (l, r) of ATC classes, we
search for the set of rules of the form L ? R, such that
l or one of its subclasses appears in L and r or one of its
subclasses appears in R and compute their combined sup-
port. The combined support of a set of rules is the number
of patients described by at least one of these rules. The
combined support of all rules having class l in L or class
r in R is also calculated and indicated at the beginning of
each row for l classes and at the top of each column for r
classes. Cells of the Fig. 2 indicate, for each (l, r), the ratio
between (i) the combined support of ARs where l appears
in L and r appears in R and (ii) the combined support of
ARs where l appears in L. This ratio denotes how often
the extracted rules associate an ADE where a drug from
l with an ADE where drug from r is involved. Note that
the total of all ratios is greater than 1 for each row as one
rule can associate more than two ATC classes, and one
patient can verify more than one rule. Fig. 3 shows sig-
nificant (p < 0.001, Z-test) deviations from the expected
values of these ratios. For each ATC class appearing in
right-hand sides of ARs, the expected ratio was computed
as the combined support of rules where that class appears
in the right-hand side divided by the combined support
of all rules. A Z-test was used to assess significance at
p < 0.001 of such deviations.
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 8 of 13
Fig. 2 Heatmap of the distribution of drug classes associations found in Experiment 3 within the EHR population. On the left, ATC classes appearing
in the left-hand side of Association Rules (ARs) and the combined support of the corresponding rules. At the top, ATC classes appearing in the
right-hand side of ARs and the combined support of the corresponding rules. Values in cells denote the ratio between (i) the combined support of
ARs where the left ATC class appears in the left-hand side and the top ATC class appears in right-hand side; and (ii) the combined support of ARs
where the left ATC class appears in the left-hand side. For instance, the combined support of rules where Beta-Blocking Agents (C07A) appears in
the left-hand side is 39, and the combined support of the subset of these rules where High-Ceiling Diuretics (C03C) appears in the right-hand side is
72% (0.72) of 39
Results
We present in this section the results of the experiments
described previously. As the first two experiments make
use of the tree structure of ICD-9-CM to simplify the
representation of ADEs (as specified in Methods, FAERS
phenotypes are mapped onto SNOMED CT rather than
ICD-9-CM), they were applied only to the EHR dataset.
The third experimental design offers a generalization of
the approach to any drug and phenotype ontologies, and
was applied to both the EHR and FAERS datasets.We thus
present the results of four experiments: three experiments
on our EHR dataset using all three experimental designs,
and a fourth one on the FAERS dataset using the third
experimental design.
Overview of results
The four experiments result in four concept lattices, from
which we extract Association Rules (ARs) of the form
L ? R. Empirically, we only retain ARs with a support of
at least 5, and a confidence of at least 0.75. Table 8 presents
some statistics about this process in our four experiments.
We observe that the third experiment generates a
much larger concept lattice from the EHR dataset than
from the FAERS dataset, despite their similar number
of patients. Nevertheless, we obtain after filtering only
twice as many rules from the EHR dataset in comparison
with the FAERS dataset. Moreover, rules extracted from
FAERS have generally larger support values. These results
can be explained by the differences between the two
datasets: the EHR dataset is built from ADEs extracted
from EHRs of patients diagnosed with SLE, while the
FAERS dataset gathers ADEs reported from the general
population. Futhermore, the higher number of ADEs per
patient in the EHR dataset tends to increase similarities
between patients, thus increasing the number of gener-
ated concepts.
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 9 of 13
Fig. 3 Statistical significance of the distribution of extracted ADE associations in Experiment 3 within the patient population. The ratio in each cell of
Fig. 2 was compared to its expected value assuming a proportional distribution of ATC classes in the right-hand side. Empty cells indicate that the
difference between the observed and expected ratios is not significant (p > 0.001, Z-test). Other cells show the difference between the observed
and expected ratios, and this difference is significant (p < 0.001, Z-test). p-values where computed using a standard normal table, assuming normal
distributions centered on expected ratios
Figures 2 and 3 show an overview of ATC drug classes
present in ADEs associated by the ARs extracted in the
third EHR experiment. Figure 2 shows the frequency
of such associations and Fig. 3 shows, for the signifi-
cant ones, the difference to the frequency obtained if the
association would be random. Figure 3 highlights a few
positive deviations from the expected association ratios.
For instance, we find that ADEs involving Beta-Blocking
Agents (C07A) are associated strongly with ADEs involv-
ing High-Ceiling Diuretics (C03C). Both classes of drugs
are involved in antihypertensive therapy, either separately
or in combination. Thus, it is likely that a certain num-
ber of patients are prescribed with these two classes of
drugs. Our results suggest that among these patients,
some could experience distinct ADEs involving each class.
We also observe that ADEs involving Antithrombotic
Table 8 Statistics about the processes of lattice building and Association Rule (AR) extraction, implemented in Java
Experiment 1 (EHR) 2 (EHR) 3 (EHR) 3 (FAERS)
Number of patients 548 548 548 570
Number of ADEs 3286 3286 3286 1148
Lattice size (number of concepts) 1.9 million 2.3 million 2.5 million 22,700
ARs extracted 5 million 7 million 9 million 18,500
ARs retained after filtering 772 1907 913 493
ARs with a support of at least 8 8 50 15 151
Maximum support 9 10 10 27
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 10 of 13
Agents (B01A) are significantly associated with other
ADEs involving the same class of drugs. Thus, it appears
that the proposed approach reveals significant associ-
ations of ADEs involving either the same or different
classes of drugs.
Examples of extracted association rules
Table 9 presents examples of ADE associations obtained
for the three experiments performed on EHRs. In fact,
nearly the same rule is found here with varying general-
ization levels across the three experiments. Note that for
readability and comparison purpose, all ARs are expressed
in the third experiment formalism. In this example, we
observe that the AR from experiment 2 is more general
than the AR from experiment 1 (R06A is a super-class of
doxylamine in ATC). In the third experiment, more spe-
cialized phenotypes are obtained (for instance ICD 586 is
a sub-class of ICD 580-629). For each experiment, ADEs
can involve a combination of two or more drugs or drug
classes. ARs may also associate a pair of ADEs on the left-
hand side with a single ADE on the right-hand side as in
our thrid experiment.
The complete set of filtered rules for each experiment
is available online at https://github.com/g-a-perso/ADE-
associations/.
An overview of the 11 ARs extracted from the third
experiment on EHR with support greater than or equal to
8 is presented in Table 10. For instance, we produce the
following AR, with support 10 and confidence 0.77:
{?{Benzothiazepine derivatives} , {Congestive heart failure}?}
? {?{Drugs for peptic ulcer and GORD} , {Atrial fibrillation}?}
This rule expresses that 1013 of patients who present
congestive heart failure (ICD 428.0) after prescription of
benzothiazepine derivatives (C08DB), also present atrial
fibrillation (ICD 427.31) after prescription of a drug for
peptic ulcer and gastro-esophageal reflux disease (A02B).
This rule holds for 10 patients.
Support of EHR rules in STRIDE
Our EHR dataset is only a small part of the total STRIDE
data warehouse that contains about 2 million EHRs. We
therefore evaluated the support of the 11 ARs listed in
Table 10 in the whole STRIDE data warehouse. Each
AR was transformed into an SQL query to retrieve the
patients verifying the rule. Table 10 reports the support in
the dataset of SLE-diagnosed patients as S1 and the sup-
port in the entire STRIDE database as S2. In all cases the
support raises from S1 to S2 and the increase ratio varies
from 2 to 36. This illustrates that the ARs extracted from
the SLE EHRs can be relevant to patients outside of the
initial dataset.
Discussion
ADE extraction
We observed a large quantitative difference between the
results of our experiments on EHRs and on FAERS. This is
explained by the different nature of the two datasets: while
the FAERS dataset gathers self-reported ADEs, we built
the EHR dataset from ADEs we extracted. As the extrac-
tion of ADEs from EHR is not the core of this work, we
used a simple method that we do not evaluate here.
This method has inherent limitations. Particularly, there
is uncertainty as whether the extracted events are actu-
ally caused by the concerned drugs. We acknowledge that
our method for ADE detection is not as robust as dispro-
portionality score algorithms [21]. In particular, we could
consider confounding factors such as age, sex, comor-
bidities or concomitant drugs. Nevertheless, we filtered
extracted ADEs using SIDER in order to retain only phe-
notypes that are known as side effects of the drugs listed
in that ADE.
Another limitation is that we are considering only drug
ingredients, whereas one ingredient may be prescribed in
various forms (for instance, eye drops or tablets). Not con-
sidering the form of the drug may result in imprecise ADE
definitions, as one phenotype may be caused by only some
forms of the ingredient. Using the unambiguous encod-
ing of prescriptions of the STRIDE EHR dataset would
address this limitation, but was not available in this study.
Table 9 Example of one extracted rule with varying generalization levels across the three experiments on EHRs
Experiment Rule Support
1 (EHR) {?{yohimbine, doxylamine, vancomycin, caffeine}, {ICD 580-629}?} ?{?{doxylamine, tocainide}, {ICD
280-289}?}
5
2 (EHR) {?{G04BE, N06BC}, {ICD 580-629}?} ?{?{R06A}, {ICD 280-289}?} 9
3 (EHR) {?{G04BE, N06BC}, {ICD 586}?, ?{A02B, N06BC}, {ICD 586}?} ?{?{R06AA}, {ICD 285.9}?} 5
Class labels: A02B is drugs for peptic ulcer and gastro-oesophagal disease, G04BE is drugs used in erectile dysfunction, N06BC is xanthine derivatives, R06A is
antihistamines for systemic use, R06AA is aminoalkyl ethers ICD 280-289 is diseases of the blood and blood-forming organs, ICD 285.9 is anemia, unspecified, ICD
580-629 is diseases of the genitourinary system, ICD 586 is renal failure, unspecified. Here, yohimbine belongs to the class G04BE (drugs used in erectile dysfunction),
caffeine belongs to the classe N06BC (xanthine derivatives) and doxylamine belongs to the class R06AA (aminoalkyl ethers)
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 11 of 13
Table 10 A selection of 11 Association Rules based on their
support in the SLE EHRs dataset
Rule S1 S2
{?{Anilides}, {Thrombocytopenia, unsp.}?, 9 326
?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}
? {?{Opioids}, {Anemia, unsp.}?}
{?{Serotonin (5HT3) antagonists}, {Thrombocytopenia,
unsp.}?,
8 256
?{Anilides}, {Thrombocytopenia, unsp.}?,
?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}
? {?{Opioids}, {Anemia, unsp.}?}
{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 9 176
?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}
? {?{Opioids}, {Anemia, unsp.}?,
?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?}
{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 8 157
?{Anilides}, {Thrombocytopenia, unsp.}?,
?{Antithrombotic agents}, {Thrombocytopenia, unsp.}?}
? {?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?,
?{Opioids}, {Anemia, unsp.}?}
{?{Benzothiazepine derivatives}, {Congestive heart failure,
unsp.}?}
10 129
? {?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?}
{?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?, 8 66
?{ACE inhibitors, plain}, {Atrial fibrillation}?,
?{Anilides}, {Atrial fibrillation}?}
? {?{Serotonin (5HT3) antagonists}, {Heart failure}?,
?{Drugs for peptic ulcer and GORD}, {Congestive heart
failure, unsp.}?}
{?{Serotonin (5HT3) antagonists}, {Atrial fibrillation}?, 8 64
?{Drugs for peptic ulcer and GORD}, {Atrial fibrillation}?,
?{ACE inhibitors, plain}, {Atrial fibrillation}?}
? {?{Electrolyte solutions}, {Congestive heart failure,
unsp.}?,
?{Osmotically acting laxatives}, {Heart failure}?}
{?{Proton pump inhibitors}, {Thrombocytopenia, unsp.}?, 10 49
?{Anilides}, {Thrombocytopenia, unsp.}?,
?{Glucocorticoids}, {Thrombocytopenia, unsp.}?}
? {?{Opioids}, {Anemia, unsp.}?,
?{Drugs for peptic ulcer and GORD}, {Anemia, unsp.}?}
{?{Proton pump inhibitors}, {Congestive heart failure,
unsp.}?,
9 37
?{Antithrombotic agents, Anilides, Opium alkaloids and
derivatives}, {Heart failure}?,
?{Anilides}, {Congestive heart failure, unsp.}?, ?{Anxiolytics},
{Heart failure}?,
?{Electrolyte solutions}, {Congestive heart failure, unsp.}?}
Table 10 A selection of 11 Association Rules based on their
support in the SLE EHRs dataset (Continued)
? {?{Opioids}, {Anemia, unsp.}?}
{?{Sulfonamides, plain}, {Congestive heart failure, unsp.}?, 8 33
?{Antithrombotic agents, Anilides, Opium alkaloids and
derivatives},
{Heart failure}?,
?{Proton pump inhibitors}, {Congestive heart failure,
unsp.}?,
?{Anxiolytics}, {Heart failure}?,
?{Anilides}, {Congestive heart failure, unsp.}?,
?{Electrolyte solutions}, {Congestive heart failure, unsp.}?,
?{Sulfonamides, plain, R05D}, {Heart failure}?}
? {?{Opioids}, {Anemia, unsp.}?}
{?{Anilides, Opium alkaloids and derivatives, Proton pump
inhibitors}, {Heart failure}?,
8 31
?{Anilides, Proton pump inhibitors}, {Congestive heart fail-
ure, unsp.}?,
?{Antithrombotic agents, Anilides, Opium alkaloids and
derivatives},
{Heart failure}?,
?{Anxiolytics}, {Congestive heart failure, unsp.}?,
?{Electrolyte solutions}, {Congestive heart failure, unsp.}?}
? {?{Opioids}, {Anemia, unsp.}?}
S1 denotes the support in the dataset used to extract the AR, and S2 denotes its
support in the entire STRIDE dataset
For these reasons, ADEs extracted from EHRs likely
present a relatively high rate of false positives. This is also
reflected in the size of the concept lattice we generated
from that dataset, as noise increase the number of possible
generalizations (see Table 8).
ADE representation
While pattern structures permit detailed descriptions of
ADEs, the algorithmic complexity of comparing those
descriptions and building the concept lattice needs to be
considered. In particular, the size of the concept lattice
that needs to be generated proves to be a limiting factor
to scale the approach on larger datasets. We observed that
the size of the lattice increases as we use more detailed
descriptions of ADEs.
One apparent limitation of this work is the absence of
temporal relationships between ADEs. We voluntarily did
not consider that aspect because the order of occurrence
of ADEs can vary between patients. However, in cases of
interest, this order can be checked in patient EHRs as pat-
tern structure concepts retain patient identifiers as well
as their description. Preliminary investigation for a given
subset of patient EHRs reveals that the ADEs of the left-
hand side of an AR can occur either before or after the
ADEs of the right-hand side of the rule.
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 12 of 13
In our experiments on EHRs, we only considered side
effect phenotypes occuring in a timeframe of 14 days
after a prescription, whereas an ADE may manifest much
later after the initial prescription. Thus, we only extracted
associations between rather short-term ADEs. The repre-
sentation of ADEs used in the different experiments could
be expanded with data about the actual delay between
the prescription and the observed phenotypes. This would
allow for mining associations in a dataset of both short-
term and long-term ADEs, while retaining the ability
to discriminate between these different manifestations.
In particular, this could permit extracting associations
between short-term and long-term ADEs, where short-
term toxicity to a given drug could be used as a predictor
of the long-term toxicity of another.
Associations between ADEs
We use association rule mining to extract associations
between frequently co-occuring ADEs. A limitation of
that approach is that we cannot infer any causal rela-
tionship between these ADEs. However, it appears more
meaningful to investigate potential common causes of
ADEs associated through an AR, rather than to search a
direct causal relationship between involved ADEs. Besides
concerns on the quality of the association itself, this lim-
its its interpretation and exploitation: without a proper
explanation of the relationship of the two ADEs, the rules
cannot be used to guide drug prescription. They can how-
ever raise vigilance towards the possible occurence of an
additionall ADE.
A large amount of ARs can be extracted from our
concept lattices. We automatically filtered a subset of
these ARs by excluding rules that do not fit the scope
of the study. While the approach we proposed is flex-
ible, it is difficult to compare ARs extracted from very
different datasets and expressed with different ontolo-
gies. Therefore, we tested selected rules obtained from
our SLE-oriented EHR dataset on the whole STRIDE
database. The results of these tests indicate that rules
extracted from a subset of EHRs (here patients diag-
nosed with SLE) can apply to a more general set of
patients (Table 10). Indeed, SLE patients are suscepti-
ble to multiple occurrences of ADEs caused by a wide
range of drugs. EHRs of such patients, used in conjunc-
tion with biomedical ontologies may then be used to
identify frequently associated ADEs. We now need to
prioritize these ARs with respect to their importance in
terms of cost and risk of the phenotypes present in their
right-hand side.
Conclusions
We explore in this paper an approach based on pattern
structures to mine EHRs and adverse event reporting sys-
tems for commonly associated ADEs. Pattern structures
permit to work with an expressive representation of ADEs,
which takes into account the multiplicity of drugs and
phenotypes that can be involved in a single event. Pattern
structures also allow to enhance this representation with
diverse biomedical ontologies, enabling semantic compar-
ison of ADEs. To our knowledge, this is the first approach
able to consider such detailed representations to mine
associations between frequently associated ADEs. The
proposed approach is also flexible and can be applied to
various EHRs and adverse event reporting systems, along
with any linked biomedical ontology. We demonstrated
the genericity of the approach on two different datasets,
each of them linked to two of three distinct biomedical
ontologies.
The kind of extracted ARs presented in this article
could serve as a basis for a recommandation system.
For instance, such a system could recommand vigilance
towards the possible occurence of an ADE based on the
ADE history of the patient. Drugs involved in ARs of inter-
est could be investigated, in light of the current knowledge
of their mechanisms, to look for possible common causes
between associated ADEs. Our chosen representation for
ADEs could be further extended to include additional
properties of drugs and phenotypes, such as drugs targets
annotated with Gene Ontology classes. This could permit
to search for association rules taking into account the drug
mechanisms.
Abbreviations
ADE: Adverse drug events; AR: Association rule; ATC: Anatomical therapeutic
chemical classification system; EHR: Electronic health record; FAERS: Food &
Drug Administration adverse event reporting system; FCA: Formal concept
analysis; ICD-9-CM: International classification of diseases, ninth revision,
clinical modification; SLE: Systemic lupus erythematosus; SNOMED CT:
Systematized nomenclature of medicineclinical terms
Acknowledgements
We acknowledge the participants of the Bio-Ontologies SIG conference for
their constructive feedback on the preliminary results of this work.
Funding
This project is supported by the PractiKPharma project, grant
ANR-15-CE23-0028, funded by the French National Research Agency and by
Snowflake an Inria associate team, and the France-Stanford Center for
Interdisciplinary Studies.
Availability of data andmaterials
The complete set of filtered association rules for each experiment is available
online at https://github.com/g-a-perso/ADE-associations/.
Authors contributions
GP, MDD, MS and AC designed the experiments and wrote the manuscript. GP
developed necessary code and ran the experiments. EB prepared the FAERS
dataset for mining and advised on technical aspects of the work. MD provided
access to the STRIDE database and advised in the manipulation of this data.
MDD and MD advised about biomedical use cases and interpreted the results.
All authors read and approved the final manuscript.
Ethics approval and consent to participate
The work was done with IRB approval (#24883) at Stanford University.
Consent for publication
Not applicable
Personeni et al. Journal of Biomedical Semantics  (2017) 8:29 Page 13 of 13
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1LORIA (CNRS, Inria NGE, Université de Lorraine), Campus Scientifique, F-54506
Vanduvre-lès-Nancy, France. 2Institute of Data Science, Maastricht
University, 6200 MD Maastricht, Netherlands. 3Stanford Center for Biomedical
Informatics Research, Stanford, USA.
Received: 3 November 2016 Accepted: 1 August 2017
SOFTWARE Open Access
Literature evidence in open targets - a
target validation platform
?enay Kafkas1,2*, Ian Dunham1,2 and Johanna McEntyre1,2
Abstract
Background: We present the Europe PMC literature component of Open Targets - a target validation platform that
integrates various evidence to aid drug target identification and validation. The component identifies target-disease
associations in documents and ranks the documents based on their confidence from the Europe PMC literature
database, by using rules utilising expert-provided heuristic information. The confidence score of a given document
represents how valuable the document is in the scope of target validation for a given target-disease association by
taking into account the credibility of the association based on the properties of the text. The component serves the
platform regularly with the up-to-date data since December, 2015.
Results: Currently, there are a total number of 1168365 distinct target-disease associations text mined from
>26 million PubMed abstracts and >1.2 million Open Access full text articles. Our comparative analyses on the
current available evidence data in the platform revealed that 850179 of these associations are exclusively
identified by literature mining.
Conclusions: This component helps the platforms users by providing the most relevant literature hits for a
given target and disease. The text mining evidence along with the other types of evidence can be explored
visually through https://www.targetvalidation.org and all the evidence data is available for download in json
format from https://www.targetvalidation.org/downloads/data.
Keywords: Target validation, Text mining, Target-disease associations, Document ranking, Information retrieval
Background
Understanding the underlying mechanisms of diseases is
crucial in translational research. Discovering the association
between drug target and disease has become a main focus
for scientists since it is key for developing new drugs or re-
purposing them. Scientists gather various evidence repre-
senting different aspects of target-disease associations such
as gene expression changes and the role of genetic varia-
tions to increase understanding. Such evidence can be
stored in structured databases and requires integration to
obtain complete and comprehensive knowledge in target
validation studies.
Motivated by this, the Target Validation Platform
(https://targetvalidation.org) [1] integrates different evi-
dence from various resources with the aim of assisting
scientists to identify and prioritise drug targets (proteins
and their genes) associated with diseases and phenotypes.
The evidence includes common disease genetic evidence
based on GWAS study results from GWAS Catalog [2],
rare Mendelian disease evidence based on ClinVar [3]
clinical variant information from EVA and text mined
target-disease associations from the Europe PMC
(https://europepmc.org/) literature database [4] (see Table 3
for a complete list of evidence types).
Europe PMC contains over 33 million records and ex-
pands at a rate of over a million articles per yearone
article every two minutes as scientists publish their find-
ings continuously. Text mining target-disease associa-
tions is crucial for an integrated platform like the Target
Validation Platform, since it provides a high volume of
complementary and up-to-date data to the other type of
evidences, otherwise the knowledge would stay hidden
in millions of documents.
In this study, we present the Europe PMC Open Tar-
gets literature component that identifies target-disease
associations in documents and ranks the documents
* Correspondence: kafkas@ebi.ac.uk
1European Molecular Biology Laboratory (EMBL-EBI), European Bioinformatics
Institute, Wellcome Genome Campus, Hinxton CB10 1SD, UK
2Open Targets, Wellcome Genome Campus, Hinxton CB10 1SD, UK
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Kafkas et al. Journal of Biomedical Semantics  (2017) 8:20 
DOI 10.1186/s13326-017-0131-3
according to their confidence based on rules utilising
expert-provided heuristic information. Our main aim is
to provide a scalable, robust and continuous text-mining
service to the community for a real-world and very
important applicationtarget validation. Many of the
previous studies focused on extracting gene-disease
association from the literature [57]. However, only a
few of them specifically focused on developing methods
for integrated resources; DisGeNET [8] and DISEASES [9]
for example cover various types of evidence for target
validation. These two systems provide confidence scores
for target-disease associations extracted from Medline
abstracts for a given disease or target and dont provide
very regular updates to the data. In DisGeNET, the tar-
get-disease text mining method is based on a machine
learning approach while in DISEASES, target-disease
associations are extracted based on scoring their co-
occurrences according to their confidence. In compari-
son to DisGeNET and DISEASES, our system operates
on full text articles in addition to abstracts, and ranks
documents according to the confidence for a given tar-
get-disease association rather than ranking the associa-
tions extracted from the whole set of Medline abstracts.
More specifically, we calculate a document confidence
score for each given (article, target, disease) triple which
represents how valuable the document is in the scope of
target validation for the given target-disease association
(see "Document scoring" section). However, the confi-
dence score of a given target-disease association is handled
at the platform level and calculated based on all the
evidence data in the platform by using a harmonic sum
approach (see [1] for the details). This confidence score
at the association level represents the overall credibility
of the evidence for a given target-disease association.
Our approach to target-disease extraction differs from
these systems, and probably many other traditional
text-mining studies, in that we rely on heuristic infor-
mation from experts/users for developing the system.
The platform was first launched in December, 2015 and
is publicly available at https://targetvalidation.org. Since
then, our system has served the platform regularly
(monthly) with up-to-date data.
Implementation
Resources used
The literature source that we used in the study is the
Europe PMC database. Europe PMC is one of the largest
biomedical literature databases in the world which pro-
vides public access to >30.4 million abstracts and >3.3
million full text articles from PubMed and PubMed
Central. In our analyses, we used the latest version of
the Open Access full text articles (http://europepmc.org/
ftp/archive/v.2016.06/) (~1.2 Million), and all of the
PubMed abstracts (~26 Million) from the database.
Two comprehensive resources, UniProt and the Ex-
perimental Factor Ontology (EFO) are used to identify
target and disease names in text, respectively. These two
resources are chosen as the reference resources by Open
Targets. The data providers of the platform are asked to
ground their target and disease entities in to these refer-
ence resources so as to integrate the evidence in the
platform. Therefore, two dictionaries are generated and
refined from the human part of the SwissProt Database
(the annotated part of UniProt, Release 2015_10) (http://
www.uniprot.org/) and disease and phenotype parts of
EFO (http://www.ebi.ac.uk/efo/) (Release 2.74) before
applying text mining. In the refining process, we filtered
out the terms that would introduce potentially very high
numbers of false positives. These are the terms having
character length < 3 (e.g. A is a gene name) and terms
that are ambiguous with common English words (e.g.
Large is a protein name as well). In addition, we gener-
ated term variations by replacing the widely used Greek
letters in gene/disease names with their symbols (e.g. re-
placing alpha with ?). The final target and disease dic-
tionaries consisted of a total of 104,434 and 29,846
terms respectively. These dictionaries are available from
ftp://ftp.ebi.ac.uk/pub/databases/pmc/otar/.
Target and disease name annotation
We used the Europe PMC text-mining pipeline, which is
based on Whatizit [10], to annotate target and disease
names in text with the two dictionaries described above.
Although we reduce a very high level of ambiguity by
applying the dictionary refinement process before text
mining the documents, some target and disease name
abbreviations could still be ambiguous with some other
names. For example, ALS which is an abbreviation used
for Amyotrophic Lateral Sclerosis, is ambiguous with
Advanced Life Support in some articles (e.g. see
PMID:26811420). Therefore, we implemented and used
a disease and target name abbreviation filter for screen-
ing out the potential false positive abbreviations intro-
duced during the annotation process. Our tool differs
from the available abbreviation finders, such as [11]
since it behaves rather as a filter specifically for potential
false positive target and disease name abbreviations an-
notated based on our dictionaries.
The abbreviation filter operates based on several rules
using heuristic information. Regular expressions are used
for identifying the text sequences in the form of X..
Y. Z. (XYZ). The text in parentheses (i.e. (XYZ)) is
identified as a gene/disease name abbreviation candidate
if it is in the uppercase form, has length <6 (the length
was decided by manually analysing a random subset of
the Uniprot and EFO dictionaries) and annotated by
the system either as a disease or a gene name, whereas,
the text located immediately before the parentheses is
Kafkas et al. Journal of Biomedical Semantics  (2017) 8:20 Page 2 of 9
identified as the potential long form. For example, in
the following sentence from the article having
PMID:26811420; The guidelines form the basis for all
levels of resuscitation training, now from first aid to
advanced life support (ALS), the italicised text
matches with our pattern defined above. ALS would
be the abbreviation candidate and advanced life sup-
port would be the potential long form. Documents
matching the pattern above are analysed manually by
an expert to come up with heuristics that we can apply
in filtering the ambiguous abbreviation. Abbreviation
candidates satisfying one of the following rules are
kept as true target/disease abbreviations, otherwise,
they are filtered out:
For disease name abbreviation candidates:
 If any of the EFO long forms of the abbreviation
candidate exists in the document
 If the long form extracted from the text contains
any of the keywords (disease, disorder, syndrome,
defect, etc.) that can be used to describe a disease
For gene or protein name abbreviation candidates:
 If (XYZ) appears more than 3 times in the
document body (this rule applies to OA full text
documents only)
 If the long form matches any of the terms from
SwissProt or Enzymes (http://enzyme.expasy.org/)
 If the long form ends with (-ase/-ases) OR it contains
any of the keywords (factor, receptor, gene, protein
etc.) that can be used to describe a target name
 If at least 3 sentences for full text and at least 2
sentences for abstracts contain one of the keywords:
mutation, SNP, variation, gene, inhibit, variation,
variant, polymorphism, mutant, isoform, protein,
enzyme, activate, antibody, transcription, tumor
suppressor, express, overexpress, regulator, receptor,
oncogene along with the protein name abbreviation
candidate and a disease name.
Target-disease association identification
Our association extraction method is based on identifi-
cation of target-disease co-occurrences at the sentence
level and applying several filtering rules to reduce noise
possibly introduced by the high sensitivity, low specifi-
city co-occurrence method. Our filtering rules utilise
heuristic information from a careful manual analysis of
the text data to filter out potential false positive associ-
ations. More specifically, the manual analyses are con-
ducted iteratively by analysing a randomly selected set
of results and identifying the reasons behind the false
positives in the results so that we could formulate them
as filtering rules to tune our system.
The system applies the following filtering rules:
1. Filter out all type of articles except Research
articles (e.g. Reviews, Case Reports).
2. Filter out target-disease associations appearing in the
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 
DOI 10.1186/s13326-017-0158-5
RESEARCH Open Access
A histological ontology of the human
cardiovascular system
Claudia Mazo1* , Liliana Salazar2, Oscar Corcho3, Maria Trujillo1 and Enrique Alegre4
Abstract
Background: In this paper, we describe a histological ontology of the human cardiovascular system developed in
collaboration among histology experts and computer scientists.
Results: The histological ontology is developed following an existing methodology using Conceptual Models (CMs)
and validated using OOPS!, expert evaluation with CMs, and how accurately the ontology can answer the
Competency Questions (CQ). It is publicly available at http://bioportal.bioontology.org/ontologies/HO and https://
w3id.org/def/System.
Conclusions: The histological ontology is developed to support complex tasks, such as supporting teaching
activities, medical practices, and bio-medical research or having natural language interactions.
Keywords: Ontology, Human histology, Fundamental tissues, Organs, Cardiovascular system
Background
Morphological sciences experts knowledge is an impor-
tant source in histology studies and practices for human
studies at cellular, tissue, organ and system levels. Asmany
other domains, histology domain also suffers from prob-
lems like vocabulary heterogeneity, the use of ambiguous
language, semantic differences and subjectivity that may
affect research, analysis and information retrieval pro-
cesses. Different terms are used to designate the same
concept or structure or the same term is used with
different meanings, in different texts.
Two main challenges are identified in the histology
domain [1]: (i) communicate specifically, clearly and pre-
cisely histology concepts and (ii) represent or model
knowledge from histology data sources in order to inter-
act and process it automatically. These challenges require
a profound analysis of the structure and the concepts
of histological terminologies. This analysis can be done
by constructing histological domain ontologies. The use
of ontologies for representing knowledge is common in
medical applications, such as anatomy, and histology
*Correspondence: claudia.mazo@correounivalle.edu.co
Equal contributors
1Computer and Systems Engineering School, Universidad del Valle, Cali,
Colombia
Full list of author information is available at the end of the article
among others. The union between ontologies and medi-
cal information is considered as a necessary alternative to
solve main problems regarding those sources of informa-
tion [24].
The term ontology has many definitions depending on
the author and the way an ontology is built and used by
computer systems. One of the most widespread definition
of ontology is: Ontology is an explicit and formal spec-
ification of a shared conceptualisation [5]. Ontologies
create models to formalise knowledge in the same way
that it is used. From a histology perspective, an ontology
would consist of concepts defined by histological knowl-
edge. Additionally, relations, attributes, rules and axioms
enrich and contribute to expand the vocabulary used to
formalise knowledge. On the other hand, a taxonomy
is a set of definitions that are organised by a hierarchy
that starts at the most general description and gets more
refined and specific terms as the hierarchy goes down.
Many ontologies and taxonomies are available in elec-
tronic form with Open Source licenses. Ones of the
best known medical taxonomies are: GALEN [6] (basic
clinical concepts  fracture, bone, and so on  con-
trolling combinations of related concepts  bone frac-
tures  and complex concepts  clavicle fracture),
UMLS (Unified Medical Language System) [7], MeSH
(Medical Subject Heading) [8], Kingsbury Center for
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 2 of 15
Cancer Care Glossary [9], MedicineNet Medical Dictio-
nary [10], Multilingual Glossary of Technical, and Pop-
ular Medical Terms in nine European Languages [11],
ICD (International Classification of Diseases) [12] among
others [13]. Some ontologies are used in web retrieval
systems [14], identification of relations between dis-
eases [15], and diagnosis [16], among others [13]. Some
ontologies are used in web retrieval systems [14], iden-
tification of relations between diseases [15], and diagno-
sis [16], among others. Uberon ontology is an anatomy
ontology, which is a common standard used by the
biomedical research community [17]. However, none
of these ontologies covers histological knowledge of
the human cardiovascular system without pathologies
in the same kind of guidance and organisation to our
research.
In this paper, we describe our work to build a histolog-
ical ontology of the human cardiovascular system. This
work is licensed under a Creative Commons Attribution-
NonCommercial-ShareAlike 4.0 Generic1 license. We
selected the cardiovascular system because it is one of
the most committed to the development of diseases asso-
ciated with modern life. To the best of our knowledge,
and after a careful search in the most relevant reposito-
ries, there is no a histological ontology in the literature,
thus we consider this one to be a relevant contribution
to the research community in the histology domain. We
left the histological ontology publicly available at http://
bioportal.bioontology.org/ontologies/HO, the documen-
tation at https://w3id.org/def/System and the OWL files
at https://github.com/claxima/HistologicalOntology.
The rest of the paper is structured as follows: the
methodology to build the histological ontology is pre-
sented in Methods section; the evaluation and the
results are presented and discussed in Results section; in
Discussion section we analyse the obtained results; and
some conclusions are presented in Conclusions section.
Methods
The NeOn methodology is one of the most used method-
ologies for ontology engineering [18]. This methodology
does not prescribe a rigid ontology development work-
flow, but instead it suggests nine scenarios for developing
ontologies. The methodology covers commonly occur-
ring situations which mostly focus on reusing, merging,
restructuring and re-engineering ontological resources.
Taking into account that we will create a histological
ontology without reusing ontological resources, accord-
ing to our analysis of the State-of-the-Art, we decided
to use the methodology proposed in [19]. This method-
ology consists of the following steps: (i) identification of
purpose, scope, CQs and scenarios, (ii) identification of
those ontologies we could reuse, (iii) domain analysis and
knowledge acquisition, (iv) iterative building of informal
ontology models, (v) formalisation and (vi) evaluation.We
modify minimally this methodology in steps i, iii and vi,
Fig. 1 presents the resulting steps. Firstly, wemerge step (i)
and (iii) which will be our first step called capturing expert
and histological knowledge. Secondly, we use three eval-
uation criteria  detecting pitfalls, expert evaluation and
answering CQs  while [19] uses two evaluation criteria
 CMs and the Protégé axiom language plug-in provided
by Protégé.
Capturing expert and histological knowledge
In this step, the aim is domain knowledge extraction using
a set of knowledge capture activities  meetings, dis-
cussions, histology classes, among others. We planned a
series of activities with the experts through which the
foundations of our ontology were built: purpose, scope,
CQs and scenarios. We hosted a series of meetings with
the group of histology experts conformed by members of
the research group Teblami2, from the Universidad del
Valle3, in which the domain experts discussed the termi-
nology and the structure used to describe the processes to
Fig. 1Methodology to develop ontologies
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 3 of 15
analyse a histological sample. The experts team was com-
prised by histology professors, with more than five years
of teaching and research experience, and biomedical grad-
uate students, with mayor on histology, all of them formed
the research histology area.
The questions to answer, at this stage, were the follow-
ing: (i) what is the ontology going to be used for?, (ii) what
do we want the ontology to be aware of?, (iii) what is the
scope of the knowledge that we want to have in the ontol-
ogy?, and (iv) how is the ontology going to be used?, the
answers are provided in the following subsections.
Purpose, Scope and Scenarios
Commonly, ontology development is not the final goal of
the process. Instead, an ontology becomes an artefact to
be used by other systems. Under this perspective, the pur-
pose is defined by the main reasons that can lead to creat-
ing an ontology [20]. Our Ontology was constructed for:
(i) sharing a common understanding of histology knowl-
edge between people and machines in processes such
as automatic recognition and identification of cells, tis-
sues and organs; (ii) allowing reuse of domain knowledge;
(iii) allowing change specifications of histology knowl-
edge, if changes occur in it. Therefore, our main target
community are both, medical professors and biomedical
researchers. In addition, explicit specifications of histol-
ogy knowledge are useful for users who should learn the
meaning of histological terms, to specialised users that
want to develop a semantic visual information retrieval
system, or to other users that want to label automati-
cally histological images or teach to students histologicals
structures and relations.
This work is focused on the human cardiovascular sys-
tem, which is one of the most committed to the devel-
opment of diseases associated with modern life. Three
scenarios are described to illustrate and motivate the use
for this histological ontology. These scenarios are later
used to develop a set of CQs and to indicate how the
ontology would be used in these cases.
Professor: a histology expert works as a professor in a
university teaching histology of the cardiovascular sys-
tem. The expert teaches different group levels, covering
histology of cells, tissues, organs and systems. The pro-
fessor should cover each topic considering components,
relations and organisations. Additionally, she/he may also
promote self-learning to on-campus students and facili-
tate on-line learning to external or remote students.
Biomedical research: a researcher is interested in work-
ing with a big data set of histological images, which
are not labelled. The researcher has to label each his-
tological image with cells, tissue and organs using a
controlled vocabulary, in short time, reducing subjectiv-
ity and increasing precision. Additionally, the researcher
should search and recover images according to present
structures to develop different steps in her or his
research.
Medical: a histology expert works in a hospital analysing
samples in the cardiovascular system context. When
receiving a sample, the histologist analyses, labels and
validates different characteristics of the sample.
Having defined the purpose, scope, and scenarios of
the ontology, we discussed the CQs with our histology
experts. These CQs were used at a later stage in order to
evaluate the resulting ontology.
Competency questions (CQs)
CQs are the kind of questions for which we want
the ontology to be able to provide support for repre-
sentation or reasoning processes. Additionally, those
questions are essential for evaluating ontologies [21].
Experts should express the CQs in natural language
without any constraint. Based on the above scenarios,
we have identified four categories of CQs: classifica-
tions, properties, constraints and inferences. Examples
of those CQs are presented in Table 1, and https://
github.com/claxima/HistologicalOntology/blob/master/
CompetencyQuestions.pdf contains the complete
document.
Table 1 Examples of CQs
Classification
What are the organs of the cardiovascular system?
What is the composition of the myocardium?
What are the muscular arteries?
Properties
What are the tunics in veins?
Which is the constitution of a media tunic?
What are the structures present in the large veins?
Constraints
A simple epithelial tissue cannot be stratified
A capillary is only composed of endothelium
An organ can have three tunics maximum
Inferences
If a set of cells is close to a light region, then the tissue is probably an
epithelial tissue
If an organ has a thin media tunic as well as a thick adventitia tunic and
a wide light region, it is probably a vein
If an organ has a thick media tunic and a small light region, it is probably
an artery
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 4 of 15
Classes and properties
In this step, we illustrate the construction of our ontol-
ogy and explain its primitive classes and properties. The
core classes of our histological ontology are: cells, tis-
sues, organs and systems. These are the main structures
to represent. Some examples of relevant properties are:
layers, cell morphology, ducts, specialisation, mechanism
of secretion, nature of secretion, valves and nodes. Some
examples of object properties of histology ontology are
included in Table 2.
A modular implementation taking into account tissues,
organs and systems was used in our ontology to facilitate
integration and/or reuse of histological data.
Two tasks were developed in this stage: (i) build the glos-
sary of terms with their definitions and synonyms, and
(ii) build the taxonomy of concepts. Figure 2 shows the
complete glossary of terms obtained for the human car-
diovascular system. Figures 3, 4, 5, 6, 7, 8 and 9 show
the CMs which represent the taxonomies for cells, tissues
and organs; these taxonomies are divided to show in more
detail the different components and relations.
Identifying reusable ontologies
Ontology research and analysis were carried out to assess
whether there were elements that could be reused in our
proposal [22]. For that, we took into account the histo-
logical and the anatomical perspectives. BioPortal [23]
contains some histological terms. However, this thesaurus
has different kind of guidance to our research due to
the fact that its organisation does not contain a specific
order and some terms are randomly located, for this rea-
son it cannot be reused. BioPortal [24] contains concepts
similar to those required in our ontology such as tissues
and cells. Nevertheless, this is a human histopathologi-
cal ontology which contain abnormal cell types which can
occur in either disease states or disease models, then this
ontology cannot be used in our research. Additionally,
this ontology does not contain the organs of the cardio-
vascular system nor the classification of tissues since it
is focused on retinal, mammary, urethral, among others.
Finally, some terms can be referenced as individual con-
cepts. BioPortal [23] and [24] have similar terms to those
required in our research, for instance terms related to the
epithelial tissue. Nevertheless, these concepts are linked
Table 2 Object properties in histology ontology
Property Domain class Range class Inverse property
isOrganOf Organ System hasOrgan
isTypeOf TypeOrgan Organ hasType
isCellOf Cell Tissue hasCell
isMorphologyOf Cell morphology Epihelial tissue hasMorphology
hasNumberLayer Epithellial tissue Number layer isNumberLayerOf
by a different route, tissues blood vessels. These ontolo-
gies contain many concepts but the hierarchical relations
among them are not detailed in depth. Under this con-
dition, if the hierarchy is represented as a tree, some of
its branches are left inconclusive. This case is seen, for
instance, for muscle tissue. Concepts are linked in one-
way allowing to connect from a large to a small structure
but not reverse. Due to the way the concepts are organ-
ised, the methods to search for a concept may not appear
logical nor intuitive. Hence, the user may need specialised
knowledge or spend more time and effort (e.g. exhaus-
tive search) in finding possible routes for these terms.
BioPortal [25] contains the cardiovascular system and its
organs. It is a complete ontology and close to what is
sought in our research. However, some terms are not in
this ontology such as the type of epithelium, connective
and muscle tissues, which has another classification 
cutaneous, corneal and lymphatic. Moreover, it is a fairly
complete cardiovascular system and organs ontology. It
has large shortcomings regarding the fundamental tissues
 epithelial tissue and muscle tissue can be referenced
as individual terms. Uberon, the Uber-anatomy ontol-
ogy, [17] is an anatomy ontology representing a variety
of entities classified according to traditional anatomical
criteria such as structure, function and developmental lin-
eage. Uberon ontology takes into account Cardiovascular
system. However, Uberon represents anatomical struc-
tures grouped in high-level categories and it is organised
according to traditional anatomical classification criteria,
being different to our histological classification criteria.
BioPortal [26] is a mouse ontology with an adult gross
anatomy focus, for this reason it does not contain micro-
scopic terms such as cells, fibres, and tissue with histo-
logical information. However, this ontology contains some
similar organ and system terms which can be referenced
as individual concepts in our ontology.
Finally, we did not find in the State-of-the-Art an ontol-
ogy of histology neither a similar organisation of hierar-
chies of histology terms that we may be able to reuse.
We followed a top-down approach [27] where histology
experts work together to identify requirements and create
the CMs. Finally, we did not reuse any available ontol-
ogy, nevertheless, there is an open door to include terms
which are related to existing ontologies by linking using
rdfs:sameAs and rdfs:seeAlso.
Iterative building of informal ontology models
We use CMs in each step of our methodology. CMs are
graphs comprised of nodes connected by arcs represent-
ing concepts and relations between them [28] (see Fig. 10).
CMs are useful to share and capture knowledge, to facil-
itate communication with experts as well as to formalise
use cases, and for evaluation purposes. Figure 10 illus-
trates the classification of the muscular tissue, in two
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 5 of 15
Fig. 2 Glossary of human cardiovascular system
ways: (i) muscular tissue is classified into smooth and
striated, (ii) striated muscular tissue is classified into
skeletal and cardiac.
Histology and expert knowledge are represented
using instances and relations with as much detail as
possible in CMs. Concept-predicate structures are
easily identified with this knowledge modelling. Sub-
jects are entities that perform or receive an action,
whereas the predicate is everything that may be
said about a subject. The subjects, predicates and
objects are extracted from histological knowledge
manually.
Fig. 3 Taxonomy of main cells observed in a sample of the circulatory system
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 6 of 15
Fig. 4 Taxonomy of the fundamental tissues. The epithelial tissue is not completely displayed here to improve visualisation
Classes and subclasses were identified using the CMs
representation; for example, epithelial tissue is_a funda-
mental tissue and simple flat epithelium is_an epithelial
tissue. Similarly, attributes were obtained. For instance,
has_attribute or is_attribute_of. An iterative process was
carried out to represent histological and expert knowl-
edge by providing a full narration of the instances,
specific properties, and relations. Experts did a valida-
tion process after obtaining our representation of the
knowledge.
Fig. 5 Taxonomy of the epithelial tissue
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 7 of 15
Fig. 6 Taxonomy of histological classification of the circulatory system
Formalisation
Informal models obtained, in the last step, with CMs
are converted into formal models which are computa-
tionally valid, using Web Ontology Language Overview
(OWL) [29]. Formal languages enable the encoding
of knowledge and often include reasoning rules. Our
histological ontology is expressed in OWL and imple-
mented using Protégé [30].
The transformation from CMs models into an OWL
model requires an interdisciplinary work. Domain
Fig. 7 Taxonomy of histological classification of layers: a layers of the heart. b layers of blood vessels
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 8 of 15
Fig. 8 Taxonomy of classification of anatomical regions present in the heart
experts develop part of the ontology by modelling their
knowledge, with the assistance of knowledge engineers.
Experts defined classes, properties and relations, with
as much detail as possible, to obtain a consistent OWL
model. Interdisciplinary work has advantages and
challenges. One of the most important advantages is
the possibility of covering topics in more depth, consid-
ering that there are many and varied perspectives for
exploring a topic, to develop important discoveries. Chal-
lenges include arranging time for meetings, developing
a common language and a knowledge baseline, dealing
proactively with expectations and misunderstandings,
focusing on a CM, and providing timely feedback.
Results
In this section we present the results obtained using
a three-fold approach to validate our ontology before
putting it into use. First of all, we detected some
of the most common pitfalls using OOPS!. Secondly,
we performed expert evaluation using conceptual mod-
els. Thirdly, we evaluated how accurately the ontology
answered our CQs.
Detecting Pitfalls
We used OOPS! [31], a web tool for detecting the most
common pitfalls in ontologies.OOPS! detects warnings in
cases such as: reasoning problems, naming conventions,
unconnected elements, modelling as well as reasoning
problems and many others described in the catalogue.
This evaluation enables to improve the maintainability,
the accessibility and the clarity of the ontology.
After executing OOPS! with the histological ontology,
we obtained a summary of the pitfalls encountered as pre-
senting in Figs. 11 and 12. Figures show two pitfalls being
Fig. 9 Taxonomy of classification of anatomical sectors present in the heart
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 9 of 15
Fig. 10 CM representation
detected as well as one suggestion and one warning in
each case.
Expert evaluation
We use CMs for evaluating the ontology taking into
account that CMs represent the conceptual scaffold of the
knowledge we are representing. Although several criteria
are used to validate ontologies, we are interested in the
formal correctness of the ontology, as described in [32]:
(i) completeness based on covering all terms related to the
cardiovascular system, (ii) duplication errors to eliminate
ontology elements which are redundant, (iii) disjunction
errors to define a class as a conjunction of distinct classes,
and (iv) consistency and coherence based on checking if
the current definitions have been accurately represented
 syntactically and semantically.
Abacha and Zweigenbaum [33] propose a validation
of medical ontologies through simple questions with
only two possible answers (Yes/No) and a textual feed-
back. This method makes the evaluation easier for
medical experts and they can interpret feedback eas-
ier. We used this method through the construction of
a survey. The elaboration of this survey was addressed
with four basic objectives: (i) identify elements that
need to be validated, (ii) organise the elements to be
validated, (iii) identify the characteristics to be val-
idated in these elements, and (iv)interpret the feed-
back and make the necessary updates. We have made
the complete survey publicly available at the follow-
ing URL http://survey-megaspace.rhcloud.com/survey/
index.php/656146?lang=es. The second step consists in
providing the survey to our group of experts. The third
step consists in interpreting experts feedback to vali-
date or modify the ontology. We applied two different
surveys. The first survey was applied in order to do
an initial evaluation on the first version of our ontol-
ogy, which was enhanced following the expert recom-
mendations. This survey was taken by 20 students in
the third year of Medicine and Surgery at Universi-
dad del Valle. The second survey was taken by 51
experts from Latin America with different specialties
(See Fig. 13), from which 32 have over 10 years of
experience. Additionally, the action fields are 22 pro-
fessor, 1 researcher and 28 both. The results of the
surveys are summarised in Figs. 14, 15 and 16. Taking
into account our criteria to evaluate, the experts eval-
uation tackles issues concerning concepts and logical
relations.
Fig. 11 Evaluation results for tissues
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 10 of 15
Fig. 12 Evaluation results for organs and system
Where possible, the first version of the ontology was
enhanced by following the students recommendations.
However, one of the drawbacks of the first survey was the
lack of experience of the participants. For this reason, their
answers were previously revalidated by an expert in order
to take them into account.
Each evaluated criterion increased, when it is com-
pared to the first survey, by (i) completeness 35, 196%,
(ii) duplication and disjunction 17, 156%, (iii) consistency
and coherence 20, 000%. The results confirm that the
new version had improved regarding the first one
using the experts suggestions. Additionally, our ontol-
ogy was designed in a modular way that enables an
easy integration or reuse. In this way, the integration
of other systems, such as the digestive and the respira-
tory, can be done without modifying the cardiovascular
system.
Answering CQs
We evaluate the capability of the ontology to answer
the CQs, using SPARQL [34]. SPARQL was used to
represent the CQs to retrieve data from the ontology
according to the query. SPARQL queries were created to
verify if the ontology gives a correct answer for each CQ,
https://github.com/claxima/HistologicalOntology/blob/
master/SPARQL_Queries.pdf contains the complete doc-
ument. CQ, SPARQL query and a figure with the result
obtained are presented in the following examples:
Fig. 13 a Experts by country of the second survey. b Experts by specialty of the second survey. Quantity represents the number of experts
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 11 of 15
a b
Fig. 14 Completeness: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage of experts per
question and Question represents the associated number to a question
CQ-0: What are the fundamental tissues? Figure 17
shows the obtained results.
PREFIX rdf: <http://www.w3.org/1999/02/22
-rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/
07/owl#>
PREFIX xsd: <http://www.w3.org/2001/
XMLSchema#>
PREFIX rdfs: <http://www.w3.org/2000/01/
rdf-schema#>
PREFIX tissue: <https://w3id.org/def/
Tissue#>
SELECT ?s ?name
WHERE { ?s rdfs:subClassOf tissue:
Tejido;
rdfs:label ?name .}
CQ-1: What are the types of connective proper tissue?
Figure 18 shows the obtained results.
PREFIX rdf: <http://www.w3.org/1999/02/22-
rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/
07/owl#>
PREFIX xsd: <http://www.w3.org/2001/
XMLSchema#>
PREFIX rdfs: <http://www.w3.org/2000/01/
rdf-schema#>
PREFIX tissue: <https://w3id.org/def/
Tissue#>
SELECT ?s ?name
WHERE { ?s rdfs:subClassOf tissue:
TejidoConectivoAdultoPropiamenteDicho;
rdfs:label ?name}
a b
Fig. 15 Duplication and disjunction: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage of
experts per question and Question represents the associated number to a question
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 12 of 15
a b
Fig. 16 Consistency and coherence: a Results from the first survey. b Results from the second survey. In the axes: Experts represents percentage of
experts per question and Question represents the associated number to a question
CQ-2: What are the layers present in the heart?
Figure 19 shows the obtained results.
PREFIX rdf: <http://www.w3.org/1999/02/22-
rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/07/
owl#>
PREFIX xsd: <http://www.w3.org/2001/
XMLSchema#>
PREFIX rdfs: <http://www.w3.org/2000/01/
rdf-schema#>
PREFIX organ: <https://w3id.org/def/
Organ#>
SELECT ?s ?name
WHERE { ?s rdfs:subClassOf
organ:TunicaCoraz\{o}n;
rdfs:label ?name .}
CQ-3: Which are the elastic arteries? Figure 20 shows the
obtained results.
PREFIX rdf: <http://www.w3.org/1999/02/22-
rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/
07/owl#>
PREFIX xsd: <http://www.w3.org/2001/
XMLSchema#>
PREFIX rdfs: <http://www.w3.org/2000/01/
rdf-schema#>
PREFIX organ: <https://w3id.org/def/
Organ#>
SELECT ?s ?name
WHERE { ?s rdfs:subClassOf organ:
OrganoArteriaElastica;
rdfs:label ?name}
Discussion
A three-fold approach to validate the histological ontology
was used  detecting pitfalls using OOPS!, expert eval-
uation using CMs, and how accurately the ontology can
answer the Competency Questions (CQ).
Regarding the detecting pitfalls, the results suggest
that the domain and range axioms are equal for two
object properties and a warning refers to the conven-
tion used. However, those are not pitfalls in our case and
do not affect the correctness of our ontology. It does
Fig. 17 Obtained results for CQ-0
Mazo et al. Journal of Biomedical Semantics  (2017) 8:47 Page 13 of 15
Fig. 18 Obtained results for CQ-1
not represent a problem, since it is about appearance or
style of the ontology and does not compromise the proper
ontology functioning.
The results shown that the experts agreed with the
following aspects of our ontology: completeness, dupli-
cation and disjunction, and consistency. Completeness
was tackled by the first question in each CM; some
relevant concepts were added to the ontology after
the first evaluation. Duplication and disjunction were
evaluated based on the second question at each CM
and we have also ensured that there were neither
duplication nor conflict in the concepts. Consistency
and coherence were covered in the third question at
each CM.
The obtained results in the experts survey were crucial
for us due to the feedback provided based on the large
experience in histology. This means that the feedback was
valuable for our research and the fact that we obtained
positive results makes it possible to put the ontology
into use.
The criteria for an ontology evaluation (consistency,
completeness, conciseness, expandability and sensitive-
ness) are used to addresses the possible types of errors
made and the future use. Exist reliable indications of
the quality of terms and definitions in ontologies and
taxonomies [31]. However, the results obtained can-
not be compared to other approaches in the state-of-
the-art because these other works addressed different
disciplines. Additionally, a key factor in the ontology
evaluation is to evaluate and compare the ideas within the
area [32].
Conclusions
In this paper, we presented a histological ontology of
the human cardiovascular system. The ontology enables
to represent histological knowledge with the purpose
of processing, inferring and obtaining new, and more
complete, knowledge. The histological ontology was
built from histological analysis perspective, potentiat-
ing its use in teaching, medical practices and biomed-
ical research. We believe that our ontology meets the
current need for teaching and learning the concepts
of the cardiovascular system, using tissues without
pathologies.
In the future, we will extend the ontology to other
systems using the same methodology adopted for this
ontology. Extending the ontology is possible taking into
account that the ontology was implemented in a mod-
ular way  tissues, organs and systems. Moreover, we
will use the ontology in four specific applications: (i)
SOFTWARE Open Access
PIBAS FedSPARQL: a web-based platform
for integration and exploration of
bioinformatics datasets
Marija Djokic-Petrovic1,2* , Vladimir Cvjetkovic2, Jeremy Yang3,4, Marko Zivanovic5 and David J. Wild3
Abstract
Background: There are a huge variety of data sources relevant to chemical, biological and pharmacological
research, but these data sources are highly siloed and cannot be queried together in a straightforward way.
Semantic technologies offer the ability to create links and mappings across datasets and manage them as a
single, linked network so that searching can be carried out across datasets, independently of the source. We
have developed an application called PIBAS FedSPARQL that uses semantic technologies to allow researchers
to carry out such searching across a vast array of data sources.
Results: PIBAS FedSPARQL is a web-based query builder and result set visualizer of bioinformatics data. As an
advanced feature, our system can detect similar data items identified by different Uniform Resource Identifiers
(URIs), using a text-mining algorithm based on the processing of named entities to be used in Vector Space Model
and Cosine Similarity Measures. According to our knowledge, PIBAS FedSPARQL was unique among the systems
that we found in that it allows detecting of similar data items. As a query builder, our system allows researchers
to intuitively construct and run Federated SPARQL queries across multiple data sources, including global initiatives,
such as Bio2RDF, Chem2Bio2RDF, EMBL-EBI, and one local initiative called CPCTAS, as well as additional user-specified
data source. From the input topic, subtopic, template and keyword, a corresponding initial Federated SPARQL query
is created and executed. Based on the data obtained, end users have the ability to choose the most appropriate data
sources in their area of interest and exploit their Resource Description Framework (RDF) structure, which allows users
to select certain properties of data to enhance query results.
Conclusions: The developed system is flexible and allows intuitive creation and execution of queries for an extensive
range of bioinformatics topics. Also, the novel similar data items detection algorithm can be particularly useful for
suggesting new data sources and cost optimization for new experiments. PIBAS FedSPARQL can be expanded with
new topics, subtopics and templates on demand, rendering information retrieval more robust.
Keywords: Federated SPARQL query, Bioinformatics, Data integration, Ontologies, Data mining and information retrieval
Background
Motivation
Nowadays, large amounts of bioinformatics data are pub-
licly available to researchers of the life science community.
These data and associated annotations are accessible
through heterogeneous databases hosted as part of many
independent and highly specialized resources and repre-
sented in different formats, conventions, vocabularies and
ontologies. Still, modern research in bioinformatics greatly
depends on the availability and efficient use of these data.
Scientific research often requires access to various data
points across scattered and highly distributed sources.
This makes finding relevant data for scientific research
projects a difficult and laborious task. With the rapid
accumulation of bioinformatics data, this issue has only
become more important and challenging.
* Correspondence: m.djokic@kg.ac.rs
1Virtual World Services GmbH, Asperner Heldenplatz 6, 1220 Wien, Austria
2Department of Mathematics and Informatics, Faculty of Science, University
of Kragujevac, Radoja Domanovica 12, Kragujevac 34000, Serbia
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 
DOI 10.1186/s13326-017-0151-z
The lack of integrated solutions that would contrib-
ute to better results and discovering of new knowledge
is a primary issue in the bioinformatics community [1].
Hence, the bioinformatics community has increasingly
taken to employing Semantic Web technologies for
better and easier data integration. The benefits of this
approach include aggregation of heterogeneous data
using explicit semantics, and simplified annotation and
expression of rich and well-defined models for data
aggregation and searching [2]. Therefore, the grand
vision and practical technologies of the Semantic Web
offer a possibility of solving longstanding problems of
data integration in bioinformatics [3].
Motivated and influenced by the ongoing needs of
supporting the research activities of the PIBAS (CPCTAS-
LCMB) Research Center (RC) [4], the authors have suc-
cessfully employed Semantic Web technologies, enabling
integration of external and internal bioinformatics data-
sets. RC is a laboratory for testing bioactive substances
which are candidates for use in pharmaceutical therapeu-
tics. Work at RC includes monitoring of in vitro effects of
active substances in cell lines of different origin (primarily
cancer cell lines) and primary cells isolated from other
types of tissue. Experiments carried out in RC include
measuring the effectiveness of a substance in inhibiting a
specific biological function (IC50) in human cancer cell
lines and quantifying the mechanisms of apoptosis, migra-
tion and angiogenesis. The experimental data obtained at
RC are varied and complex and represent intertwined re-
lationships among various terms and concepts used at RC.
This complex data structure is represented as an ontology
[5]. The ontology simplifies the search for experimental
data and comprises a formal, rigorous representation of
the conceptual model of the domain.
The main subjects that RC staff are interested in are
information about targets, bioassays and cell lines used
in earlier experiments. In addition to the PIBAS ontol-
ogy [5], which provides internal support to RC staff,
supplementary information can be extracted from glo-
bal initiatives such as Bio2RDF [6], Chem2Bio2RDF [7]
and the EMBL-EBI platform [8]. For example, information
about targets can be found in ChEMBL [9], BindingDB
[10] and Drugbank [11] datasets, form the EMBL-EBI,
Chem2Bio2RDF and Bio2RDF initiatives, respectively. The
necessary information for bioassays can be found in
ChEMBL and Pubchem [12] datasets form the EMBL-EBI
and Chem2Bio2RDF initiatives, respectively. Information
about cell lines can be found in ChEMBL and ChemBank
[13] datasets from the EMBL-EBI and Chem2Bio2RDF
initiatives, respectively. Another search requirement is in-
vestigation of actual research results in publications. For
example, information about publications can be found via
PubMed [14], from the Bio2RDF initiative, as well as in
the local Reference ontology [15] developed for internal
use at RC. In previous work [16], the authors focused on
integration of these initiatives. Based on manually entered
data, such as InChi, InChiKey, SMILES or molecular for-
mula, the system offers templates and generates static
Federated SPARQL queries [17] for retrieval of relevant
information. This system has been very helpful in discov-
ering new knowledge, but in the light of ever-increasing
volume of experimental data, the needs of RC mandated
the development of a new system. One of the main re-
quirements in this regard was the inclusion of relevant
and new datasets in predefined queries to make it possible
to find complementary information about data items (tar-
gets, bioassays and cell lines). An additional requirement
was the capability to detect similar data items to increase
the performance of experiments and lower processing
costs. This is one of the major challenges in the bioinfor-
matics community, as the data items are represented by
distinct URIs at different endpoints [18], which necessi-
tated a serious effort to discover and compare their com-
mon properties.
In order to meet the above-mentioned requirements of
RC, the authors developed PIBAS FedSPARQL,1 a plat-
form based on Semantic Web technologies that allows
end users to easily provide input data and run predefined
Federated SPARQL queries across multiple data sources
and detect similar data items, among data obtained from a
query. For the process of detecting similar data items, the
authors developed a text-mining algorithm based on the
processing of object values (strings) of the named entities
to be used in Vector Space Model (VSM) [19] and Cosine
Similarity Measures (CSM) [20]. Also, one of the features
of PIBAS FedSPARQL is the capability of filtering results
obtained by a query. Filtering is based on a projection of
RDF data sources included in the query. Searching and
sorting of results is also offered. Users can add additional
data source if they are interested in querying endpoint
that is not contained in the predefined query. The system
can also be extended with new topics, subtopics and tem-
plates on demand.
Features
Adhering to the philosophy of Arsic et al. [16], the au-
thors implemented the following SPARQL features:
 Federation: Federated SPARQL queries over remote
endpoints, gather novel and complementary data
about targets, bioassays and cell lines in real time.
This eliminates constant update monitoring.
 Scalability: Data integration with user-specified data
sources is possible. Furthermore, end users have the
ability to choose the most appropriate data sources
in their area of interest and exploit their RDF
structure. This allows them to select certain
properties of data sources to improve query results.
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 2 of 20
 Advancement: Detecting similar data items using a
method based on text-mining. This feature is helpful
for optimizing the costs of new experiments.
 Availability: Locally used RC data are now public
and available to the entire bioinformatics
community.
The rest of the paper is organized as follows: The next
subsection represents a survey on related works. In the
Implementation section, we present the architecture of
PIBAS FedSPARQL. In the Methods section, we describe
all features of PIBAS FedSPARQL and highlight our al-
gorithm for similar data items detection, explaining it in
detail and presenting a use case. In the Results section
we present the results obtained through an evaluation.
In the Conclusions and future work section, apart from
presenting the final remarks, we also outline a possible
approach for future work. The section Appendices con-
tains various definitions used in our study.
Related work
In modern biology and chemistry, exploiting the diverse
kinds of available data about a topic of interest is challen-
ging, as data are spread over many sources. Bioinformatics
datasets are highly distributed and heterogeneous, and this
heterogeneity exists at many levels including data formats,
conventions and meaning. Due to these factors, trad-
itional approaches for data searching often deliver un-
satisfactory results. The need for an integrated solution
has led many organizations to use the Semantic Web,
because of its wide range of possibilities. The Semantic
Web is recognized as a common framework that allows
data to be used and shared across applications and
database boundaries [21].
Initiatives such as Bio2RDF [6] and LODD [22] address
the problem of connecting biological and drug data.
Bio2RDF has transfigured and interrelated many biological
databases, offering a platform for constructing queries
across these data sources. The LODD initiative integrates
various sources of drug data, motivated by domain-aware
scientific questions. Chem2Bio2RDF [7] aggregates data
from various data sources that are contained in Bio2RDF
and LODD. It covers around 25 distinct datasets with con-
nected compounds, drugs, pathways, side effects, genes,
diseases and PubMed documents. Chem2Bio2RDF also
includes a tool to facilitate queries and a set of compre-
hensive functions to address specific research requests.
EMBL-EBI [8] contains a wide range of freely accessible
molecular data sources, such as UniProt [23], ChEMBL
and Reactome [24]. Open PHACTS [25] is a unique initia-
tive developed as a shared platform for integration and
knowledge discovery. It constitutes an approach based on
the Semantic Web to address bottlenecks in drug discov-
ery. The project mainly focuses on distinct information
sources, lack of standards and information overcharge as
major issues. Its goals are establishing open standards and
creating infrastructure for research cooperation. Projects
such as LinkHub [26], SWIT [27] and BioGateway [28]
also offer their solutions for the integration of bioinfor-
matics data.
All the solutions mentioned above have many datasets
in common and together they combine vast amounts of
bioinformatics data. Besides profound background know-
ledge about the underlying data sources, users also need
to have solid command of the SPARQL query language to
successfully access the data. SPARQL is an RDF query lan-
guage used to retrieve and control data stored in RDF
graphs [29]. SPARQL also allows executing queries that
are distributed over multiple endpoints, so-called Feder-
ated SPARQL queries [30]. Generally, SPARQL has a
complex syntax that is difficult to work with for inexperi-
enced users and, consequently, querying data is a problem
for many researchers. Therefore, a number of existing ap-
plications strive to provide a user-friendly interface for
browsing bioinformatics data or to allow users to perform
Federated SPARQL queries. Several of these solutions are
described below.
SPARQLGraph [31] is a web-based platform for the vis-
ual creation and execution of biological SPARQL queries.
The graphical query builder allows end users to create and
share query graphs in a simple way. Several template quer-
ies are provided, offering a great starting point for building
new graphs and assisting researchers in finding answers to
biological questions. In the SPARQLGraph the datasets
are integrated in the interface internally and no other
datasets are supported. In PIBAS FedSPARQL some data-
sets are integrated and end users can also add an outside
dataset if they want to query endpoints that are not in the
list of integrated datasets. Both interfaces provide template
queries in multiple datasets and enable end users to
choose from these datasets to facilitate direct querying.
QueryMed [18] allows queries relevant to a wide range
of biomedical topics. It runs federated queries across
multiple SPARQL endpoints. QueryMed is designed to
be accessible to users who are not familiar with the
underlying ontologies or the SPARQL query language.
The system allows users to select the data sources they
wish to use. Users can also add additional data sources.
After retrieval of the initial result set, query results can
be filtered to improve their relevance. As an advanced
search feature, the system also allows users to exploit
the underlying structure of the RDF data to improve
query results. This solution is the most similar to our
approach, but the main difference lies in the fact that
PIBAS FedSPARQL offers a feature for finding similar
data items in the retrieved result set.
Twinkle [32] provides a stand-alone graphical user
interface to load and edit SPARQL queries. In this case,
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 3 of 20
users are expected to know what is already available at
the SPARQL endpoints and to write the queries that can
be used to directly query remote SPARQL endpoints.
This approach is the opposite of ours: initial PIBAS Fed-
SPARQL queries are predefined, while conversance of
SPARQL is necessary for adding new datasets. Although
Twinkle was mostly designed as a general purpose
system, it only supports a small number of specific
SPARQL endpoints, while PIBAS FedSPARQL allows
users to add any new SPARQL endpoint.
GoWeb [33] was created for answering queries on bio-
medical data. It lets users run old-style keyword-based
web searches with ontology search features. After a key-
word search, documents can be filtered based on the
biomedical annotations they contain. Nevertheless, in
GoWeb the exact queried sources are not transparent
and cannot be selected or customized by end users as in
PIBAS FedSPARQL.
The SMART [34] query tool is a web-based application
that allows biology researchers to run SPARQL queries
over multiple data sources. Their queries are constructed
using a description logic written in the Manchester OWL
syntax [35]. In contrast, PIBAS FedSPARQL allows end
users to intuitively run predefined queries by selecting
topics, subtopics, templates and entering keywords with-
out requiring background knowledge about the SPARQL
syntax.
BioQueries [36] lets users to design and share SPARQL
queries that can simplify and reduce many common and
frequent bioinformatics data retrieval tasks. The BioQu-
eries interface provides context-specific anchoring for
queries via the use of placeholders. Queries are repre-
sented as a sentence with one or more gaps where a user
can enter context-specific information. In the PIBAS Fed-
SPARQL system, Federated SPARQL queries are displayed
as a corresponding virtual sentence based on the items se-
lected and keyword entered.
FedX [37] runs queries over either Sesame repositor-
ies2 or SPARQL endpoints. During the initial phase, it
loads the list of data sources without its statistical
information. The source selection is done by sending
SPARQL ASK queries. The size of intermediate result is
minimized by a rule-based join optimizer according to a
cost estimation. By contrast, PIBAS FedSPARQL pre-
serves intermediate results because it is very important
for RC staff to gain all relevant data.
To overcome the problem of querying multiple data
sources, which can vary in their RDF representations, pro-
ficiency in SPARQL is essential, but usually not sufficient,
for successful information retrieval from such data
sources. Identifying relevant data sources and discovering
their capabilities and the type of data they contain is a
process known as source discovery [38] and a necessary
pre-step for determining whether a particular data source
matches researchers demands. There are often many al-
ternative ways of carrying out source discovery [38], all of
varying efficiency, and SPARQL experts have to choose
from these options. Our approach for solving these
challenges is based on close co-operation with RC experts.
In order to fulfill the requirements of RC, we carried out a
source discovery process and arrived at Bio2RDF, Chem2-
Bio2RDF and EMBL-EBI as viable data sources (initia-
tives). Then, a series of small SPARQL queries were
created from pattern queries that were partly handpicked
from initiative examples and handcrafted. Furthermore,
we interoperated between data sources, tracking and link-
ing related instances, which we received as results from
executing the series of the SPARQL queries. Assessing the
results, we picked up suitable handcrafted pattern queries
and created the final SPARQL queries for each require-
ment. Thus, PIBAS FedSPARQL federates data by execut-
ing already predefined Federated SPARQL queries and
this is different from a federated query engine BioFed [39]
that is able to federate more than 130 public SPARQL
endpoints. In BioFed queries are built based on existing
data and then distributed to the relevant endpoints
through a source selection approach.
Although integrated approaches in the bioinformatics
domain are available, there are still a number of challenges
that must be addressed in order to make such resources
accessible to researchers. Data warehousing within bio-
informatics information infrastructures in order to enable
semantic interoperability between its various stakeholders,
is one of the main challenges [40]. A simple form of a data
warehouse that is focused on a single subject is called a
data mart [41]. Depending on the requirements and
complexity of the system, there are several types of imple-
mentation of data warehousing. For example, Open
PHACTS [25] uses a bottom-up approach, where the data
marts are created first and then combined into a single,
all-encompassing data warehouse. Generally, in data
management, semantic warehousing is a methodology of
digitalizing text data using similar functions as data
warehousing such as ETL (extract, transform, load) [40].
In PIBAS FedSPARQL authors do not use semantic
warehousing, although the VSM approach employed can
be seen as a data mart solution in the sense that extracted
semantic information (text) is transformed and prepared
for usage in CSM.
One of the most intriguing problems in the bioinformat-
ics community is finding similar data items across the
same or different initiatives [18]. PIBAS FedSPARQL
offers a flexible and interesting way to overcome this
challenge using a method based on text-mining. We apply
VSM on terms, which are actually words or phrases from
biological or chemical areas, and then compare the vec-
tors using CSM. This algorithm is described in detail in
the Methods section.
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 4 of 20
The study of semantic similarity between words has
long been an integral part of information retrieval,
natural language processing and the Semantic Web.
Semantic similarity between entities changes over time
and across domains. The rest of this paragraph outlines
some traditional approaches to identifying semantic
similarity. Given a taxonomy of concepts, a straightfor-
ward method to calculate similarity between two words
(concepts) is to find the length of the shortest path
connecting the two words in the taxonomy [42]. If a
word is polysemous, then multiple paths might exist
between the two words. In such cases, only the shortest
path between any two senses of the words is considered
for calculating similarity. A problem that is frequently
acknowledged in relation to this approach is that it re-
lies on the notion that all links in the taxonomy repre-
sent a uniform distance. Resnik [43] proposed a
similarity measure using information content. This ap-
proach defines the similarity between two concepts C1
and C2 in the taxonomy as the maximum of the infor-
mation content of all concepts C that subsume both C1
and C2. The similarity between two words, then, is
defined as the maximum of the similarity between any
concepts that the words belong to. Resnik used Word-
Net [44] as taxonomy and calculated information con-
tent using the Brown corpus [45]. Matsuo et al. [46]
used a similar approach to measure the similarity be-
tween words and apply their method in a graph-based
word-clustering algorithm.
Semantic similarity measures have been used in many
Semantic Web applications. Ehrig et al. [47] describes a
framework that aims at comparing concepts across ontol-
ogies, and not ontologies themselves. This is similar to
our solution, where we only compare object values
(concepts). David et al. [48] present a number of measures
for ontology matching and state that simple measures like
Cosine Similarity on a term-frequency vector give accur-
ate results. This is also the measure method we use in our
system.
In our previous work, we demonstrated the power of
ontology-based information system [5]. A new ontology
was developed for RC that contains encoded knowledge
about local experimental structure and an ontological
database was created that contains data from individual
experiments. Additionally, to make it possible to find
relevant information essential for the further perform-
ance of local experiments, a local approach for running
static Federated SPARQL queries over CPCTAS [5],
Bio2RDF, Chem2Bio2RDF and EMBL-EBI was created
[16]. Currently, RC wanted to expand the search and
discover complementary data by adding new dataset
and finding similar data items to potentially narrow
down the choice of materials and methods for future
experiments. In this paper, the PIBAS FedSPARQL sys-
tem is described, which implements these ontological,
database and strategic approaches.
Implementation
Architecture overview
The PIBAS FedSPARQL architecture is shown in Fig. 1.
The main components are user interface and query engine.
The user interface enables users to construct simple and
Fig. 1 PIBAS FedSPARQL architecture overview. The architecture consists of two main layers: query engine and user interface. The user interface
enables users to construct simple and advanced queries and view the results of their execution. The query engine preforms a series of
demanding processes that needs to be done before queries can be executed. The main query engine component, Data Source Manager, scans
the local DataSources ontology, reads the users input and passes the information through the Query preparation component to the SPARQL
query runner component, where the queries are executed. The Dataset projection component plays a role in the Dynamic query filter feature,
allowing users to easily discover the structure of underlying datasets included in Federated SPARQL queries. The Detecting Similar Data Items
component identifies similar data items from results retrieved after running predefined queries or queries extended with new datasets
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 5 of 20
advanced queries and view the results of their execu-
tion, while the query engine executes queries across re-
mote SPARQL endpoints. PIBAS FedSPARQL was
implemented in PHP and Python. The JQuery library3
was used to develop an interactive and user-friendly
interface, while sparqllib4 was used to run Federated
SPARQL queries. The list of available datasets used for
creating predefined Federated SPARQL queries is
placed in the local DataSources ontology [49] devel-
oped using Protégé 4.0.2 [50].
User interface
The user query interface was implemented in HTML,
JQuery and JavaScript. Its core components are:
Initial query interface Users can choose from prede-
fined topics, subtopics and templates. The selection of
subtopics is limited by of the topic selected. This also
applies to the relation between topics and templates. All
relations reflect the needs of the researchers at RC. Every
Fig. 2 Representations of basic relations in the DataSources ontology in the Protégé editor a) Topic Biology b) Subtopic BiologyTarget c)
Template Found targets for the drug and some of its properties d) PIBAS/CPCTAS dataset instance. This figure shows screenshots of the
local ontology DataSources in the Protégé ontology editor [50]. The ontology contains information about initiatives and datasets included
in predefined Federated SPARQL queries. Each dataset in the ontology is represented as an instance of a certain class. The object property
conectedWith connects dataset instances with template instances. Every Subtopic class instance is connected with a Template class instance
through the object property hasTemplate. Every Topic class instance is connected with a Subtopic class instance through the object
property hasSubTopic
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 6 of 20
template is based on a form of an underlying predefined
Federated SPARQL query.
Predefined query extension This component allows
end users to add new datasets to the predefined Feder-
ated SPARQL queries.
Dynamic query filter This component allows end users
to select the desired datasets, load the properties avail-
able for these datasets and dynamically expand Feder-
ated SPARQL queries with selected properties.
Result presentation This component allows end users
to view the results of predefined queries in table form.
One column shows retrieved results as URI or string,
while another column displays data source and initiative
name. End users can also apply a dynamic query filter to
view the results organized by source. In both cases, the
columns can be sorted and searched based on entered
text.
Query engine
PIBAS FedSPARQL runs Federated SPARQL queries on
our local JOSEKI endpoint.5 Before the queries can be
executed, a series of demanding processes need to be
performed. These tasks are carried out by the following
components:
Data source ontology This component implies the
DataSources ontology that contains the patterns of pre-
defined queries for all templates as well as information
about datasets that are initially included in queries.
Data source manager This component scans the data
source ontology and uses the corresponding datasets in-
formation to fulfill the user requirements. The data
source manager also keeps track of predefined datasets
and the datasets included in extended queries.
Table 1 Representation of current (sub)topics and templates in
the DataSources ontology
Topic Subtopic Template/Template label* Keyword
Biology Targets Find targets for the drug/1 InChiKey
Chemogenomic Assays  Find assays for the drug/2 SMILE
Cell lines  Find cell lines for the drug/3 InChiKey
Research Papers Find papers with a title for the
keyword/4
No restriction
aTemplate labels are used in Table 2 and Table 6
Fig. 3 Predefined query of Template2 for its pre-selected datasets. This figure shows the predefined Federated SPARQL query of the template
Find targets for the drug. This query covers the PIBAS/CPTAS, Drugbank/Bio2RDF, ChEMBL/EMBL-EBI and BindingDB/Chem2Bio2RDF datasets.
All predefined Federated SPARQL queries in the local DataSources ontology contain %s characters which represent objects values that will be
replaced with the keyword entered by the user. The last %s character will be replaced with a particular pattern query if a new dataset is added
using the Add new dataset feature
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 7 of 20
Dataset projection This component returns properties
for every dataset included in Federated SPARQL queries.
End users can choose from a number of properties based
on their description.
Query preparation This component is in charge of
translating and preparing the requirements of end users
into valid Federated SPARQL queries. Requirements in-
clude selecting options from the initial query interface,
adding new endpoints to predefined queries and dynamic
query filtering.
SPARQL query runner This component executes Fed-
erated SPARQL queries.
Detecting similar data items This component detects
similar data items (URIs) from results retrieved after
running predefined queries or queries extended with
new datasets. Similar data items are shown on a new
web page.
Table 2 List of RDF datasets integrated in PIBAS FedSPARQL
PIBAS FedSPARQL
Data source Triples Template
label
Reference or dataset link
CPCTAS
PIBAS dataset 437 1; 2; 3 [5]
Reference
dataset
42.089 4 [15]
EMBL-EBI
ChEMBL 425.304.329 1; 2; 3 https://www.ebi.ac.uk/chembl/
Chem2Bio2RDF
BindingDB 20.484 1 https://www.bindingdb.org/
bind/index.jsp
Pubchem 78.000.000 2 https://www.ncbi.nlm.nih.gov/
pcassay
Bio2RDF
Drugbank 3.672.531 1 http://www.drugbank.ca/
PubMed 5.005.343.905 4 http://www.ncbi.nlm.nih.gov/
pubmed/
Fig. 4 Running of predefined query in PIBAS FedSPARQL a) Initial user interface b) Results after running predefined query. The initial user
interface allows users to create queries in a very simple way by selecting a (sub)topic, template and entering a keyword. By clicking on the Run
query button, the predefined Federated SPARQL query is executed and users receive results in the form of a table. The first column shows the
retrieved results as URI or string. The second column displays the data source and initiative name. The icon in the top-right corner of the table
shows statistical information about the retrieved data, including data source name, initiative name and the number of obtained data items per
data source
Djokic-Petrovic et al. Journal of Biomedical Semantics  (2017) 8:42 Page 8 of 20
Methods
Running of predefined queries
Information about initiatives and datasets included in
predefined queries is placed in the local ontology Data-
Sources. Each dataset is represented as an instance,
while each template is connected to a dataset instance
using the object property connectedWith. With respect
to their purpose, the same dataset can be associated
with a variety of templates. Every template belongs to a
corresponding subtopic. Each subtopic has its own
topic. For example, the topic Biology has the subtopic
BiologyTarget while it is connected to Template2 (see
Fig. 2). Template2 is created based on the following
preselected datasets: PIBAS/CPCTAS, BindingDB/
Chem2Bio2RDF, Drugbank/Bio2RDF and ChEMBL/
EMBL-EBI.
Currently, the DataSources ontology contains topics
that are created in accordance with the requirements
of RC experts. Topics are divided into three areas:
Biology, Chemogenomic and Research. All (sub)topics
and templates are changeable and can easily be modi-
fied or added to. Templates can be modified in various
ways. For example, the template Find targets for the
drug, which requires the InChiKey value, can be trans-
formed into a template that requires another value,
such as SMILES. This change necessitates a manual
modification in the predefined query. Templates can
be expanded with one or more new datasets. Similarly,
datasets can also be excluded from templates. A repre-
sentation of all topics and their relations in PIBAS
FedSPARQL is shown in Table 1.
The property hasInitialQuery of each template repre-
sents a predefined Federated SPARQL query that runs
across preselected datasets. Pattern queries for every
dataset are collected from initiative examples and parts
of them are handcrafted. Figure 3 shows the predefined
query of Template2. All %s characters that represent
objects in the predefined query will be replaced with the
keyword entered by the end user, while the aftermost
character is reserved for an additional dataset.
At the moment, PIBAS FedSPARQL uses datasets
(Table 2) from the EMBL-EBI, Bio2RDF and Chem2-
Bio2RDF platforms. These are, used to establish the
predefined Federated SPARQL queries. CPCTAS, as union
of the PIBAS and Reference dataset, covers all the men-
tioned topics currently used for templates. Seeking to
meet the needs of RC staff and highlight the importance
of small laboratories, we have related the PIBAS dataset
with templates from the Biology and Chemogenomic
topics. The Reference dataset, as collection of ontologies,
RESEARCH Open Access
Revealing protein functions based on
relationships of interacting proteins and
GO terms
Zhixia Teng1,2*, Maozu Guo2*, Xiaoyan Liu2, Zhen Tian2 and Kai Che2
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: In recent years, numerous computational methods predicted protein function based on the protein-protein
interaction (PPI) network. These methods supposed that two proteins share the same function if they interact with each
other. However, it is reported by recent studies that the functions of two interacting proteins may be just related. It will
mislead the prediction of protein function. Therefore, there is a need for investigating the functional relationship between
interacting proteins.
Results: In this paper, the functional relationship between interacting proteins is studied and a novel method, called as
GoDIN, is advanced to annotate functions of interacting proteins in Gene Ontology (GO) context. It is assumed that the
functional difference between interacting proteins can be expressed by semantic difference between GO term and its
relatives. Thus, the method uses GO term and its relatives to annotate the interacting proteins separately according to
their functional roles in the PPI network. The method is validated by a series of experiments and compared with the
concerned method. The experimental results confirm the assumption and suggest that GoDIN is effective on
predicting functions of protein.
Conclusions: This study demonstrates that: (1) interacting proteins are not equal in the PPI network, and their function
may be same or similar, or just related; (2) functional difference between interacting proteins can be measured by their
degrees in the PPI network; (3) functional relationship between interacting proteins can be expressed by relationship
between GO term and its relatives.
Keywords: Protein function, Interacting protein, Gene ontology, Directed network
Background
Characterizing protein functions is critical to understand-
ing biological pathway, investigating disease and develop-
ing drugs [1, 2]. To elucidate protein functions, numerous
research efforts have been made based on techniques ran-
ging from sequence homology detection to text mining of
scientific literature. However, only some of proteins are
annotated with functional information for well-studied
model organisms so far. The situations would be even
worse for the other organisms.
Recently, biological network provides chance of study-
ing gene and its products (e.g protein, microRNA) at
system level [3, 4]. It is widely recognized that a protein
performs functions according to its partners in protein-
protein interaction (PPI) network. This recognition has
motivated the development of numerous network-based
methods for predicting protein function. These methods
are proposed on the principle of guilt-by-association
(GBA), that is, the closer the two proteins are in the net-
work the more similar are their functions [5]. These
network-based methods can be roughly grouped into
two major classes: direct annotation methods [611]
* Correspondence: tengzhixia@nefu.edu.cn; maozuguo@hit.edu.cn
1Department of Information Management and Information System,
Northeast Forestry University, Harbin 150040, China
2Department of Computer Science and Engineering, Harbin Institute of
Technology, Harbin 150001, China
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27
DOI 10.1186/s13326-017-0139-8
and model-assisted methods [1215]. The comprehen-
sive reviews of these methods can be found in [5, 16].
The direct annotation methods suppose that the inter-
acting proteins share the same function and inferred
protein functions by means of propagating the known
functional annotations of its neighbors along the net-
work edges. The model-assisted methods assume that
proteins in the same group perform the same function.
They firstly identify functional groups of proteins, and
then annotate each group with the known functional an-
notations of the groups members. In recent years, chi et
al. [17] proposed a method named CIA, which iteratively
updated annotations of a protein according to functional
similarity between the protein and its partners. Wang
[18] put forward a method named FCML to predict pro-
tein function by multi-label learning. The FCML took
functional association between Gene Ontology (GO)
terms [19] under consideration when it worked. Almost
all of these methods predicted protein functions using
PPI network and GO terms. In these methods, predict-
ing protein function is to associate term with protein ac-
cording to functional semantic information of the term.
The result of predicting is named as annotation of pro-
teins and an annotation is represented by a term. These
methods have promoted the development of the protein
functional predicting. However, most of them ignored
some crucial information which affect the quality of
prediction:
(1)The PPI network is usually supposed as non-
directional. In fact, it is commonplace in the PPI
network that regulation relationship, upstream-
downstream relations between interacting proteins
when they are involved in signal transduction, tran-
scriptional regulation, cell cycle or metabolism [20].
Moreover, it is reported by recent studies [2123]
that GBA is the exception rather than the rule in
the PPI network and protein functions are deter-
mined by specific and critical interactions. Hence
the relationship between interacting proteins may
affect their functions and should be considered in
the process of predicting protein functions.
(2)In GO context, a series of standard terms are
defined to describe characteristics of gene products
(i.e. protein), and the terms are arranged as directed
acyclic graph (DAG) hierarchy according to
functional associations of them. Therefore, the
functional information is not only expressed by
semantics of terms but also contained in the
hierarchy. Thus, the predictions of protein functions
may be misled if the functional associations of terms
are ignored. In fact, the information underlying in
GO hierarchy are crucial for functional predicting of
proteins.
In this paper, we mainly study two problems: (1) how
to measure the functional difference between interacting
proteins; (2) how to demonstrate functional difference
between the interacting proteins in GO context. To
solve above problems, we advance a novel method to
predict protein functions by diffusing GO terms in the
directed PPI network (GoDIN). Firstly, the relationship
between interacting proteins is generalized as functional
proactive-reactive. It is assumed that the proactive
protein performs fewer and more specific functions than
the reactive protein. And then a directed PPI network is
generated according to the functional proactive-reactive
relationships of interacting proteins. Secondly, a
coefficient variation is defined to measure functional
difference between interacting proteins. Finally, func-
tional associations of GO terms are taken into consid-
eration in the process of annotating interacting
proteins. By a proposed iterative algorithm, GO terms
are allocated to describe protein functions in the PPI
network under the control of coefficient variations.
The method will be illustrated in the following
section.
Methods
Functional relationship between interacting proteins
As reported, many proteins play functional roles that are
different from their neighbors in the PPI network. For
example, a protein annotated with terms: RNA trans-
port, RNA binding may involve in translation mech-
anism and bind with diverse functional unrelated
proteins [23]. For instance, the function of proteins
which help others fold correctly may be unrelated to
that of their partners. These proteins are more likely to
be hubs than others in the PPI network. The hubs often
have many partners and may involve in several different
biological activities. In general, a protein is multi-
functional if it takes part in many different biological ac-
tivities. As reported [22], the more multi-functionality of
a protein is, the less specific is its function. Besides, Gil-
lis et al. also found that the multi-functionality of a pro-
tein is highly correlated with its degree in the PPI
network. Specifically, a protein with high degree may
perform general function so that they could collaborate
with other proteins in diverse biological activities. It can
be considered that the low degree proteins are proactive
and the high degree proteins are reactive in biological
activities. Thus, the relationship between interacting
proteins can be generalized as functional proactive-
reactive according to their degrees in PPI network.
Let the PPI network be generalized as a digraph, in
which a node presents a protein and an arch links
two interacting proteins, oriented from the low degree
one to the high degree one. Note that, two interacting
proteins with equal degree are linked with a
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 10 of 79
bidirectional arch. Accordingly, as displayed in Fig. 1,
a novel directed PPI network is generated from the
original undirected PPI network.
As discussed above, the functional specificity may
descent on the direction from proactive protein to re-
active protein. Thus, the descent direction of func-
tional specificity between two interacting proteins is
defined as (1). In the formula, O(vi, vj) represents the
descent direction of functional specificity between the
two interacting proteins vi and vj; d(.) denotes the de-
gree of a protein in the PPI network. The formula
means that: vi plays more specific functions than vj if
O(vi, vj) =1; vi play general functions than vj if O(vi,
vj) = ?1; vi and vj are equal in the network and they
share the same function if O(vi, vj) =0.
O vi; ; vj
  ¼
1; d við Þ < d vj
 
;
0; d við Þ ¼ d vj
 
;
?1; d við Þ > d vj
 
:
8
><
>:
ð1Þ
Measuring functional difference between interacting
proteins
Here the functional difference between two interact-
ing proteins is measured. It is considered that a pro-
tein perform specific functions if the protein is
involved in few activities, vice versa. In the PPI net-
work, the number of connections of a protein can re-
flect the number of activities the protein involves in.
Thus, the functional specificity of a protein can be
measured by degree of the protein in the PPI net-
work. For two interacting proteins, their functional
difference may be determined by the specificity differ-
ence of the functions which are performed by their
interaction. Accordingly, a coefficient variation is de-
fined to measure the functional difference between
two interacting proteins. The functional coefficient
variation between two interacting proteins vi and vj is
marked as CV(vi, vj) and can be measured by (2).
CV vi; ; vj
  ¼ 1
d við Þ?
1
d vj
 

 ð2Þ
Annotate the interacting proteins with GO terms based
on their functional difference
In traditional methods, the known GO terms of a
protein were directly associated with interacting partners
of the protein. These methods ignored the functional
difference between the interacting proteins. In fact, the
functions of interacting proteins may be same or similar,
or related but different. Therefore, the relatives of
known terms of a protein are selected to annotate inter-
acting partners of the protein in our method.
To select relatives, it is supposed that an ideal term
can annotate functions of neighbors exactly. Semantic
value of the ideal term can be estimated based on those
of the known terms of interacting proteins and func-
tional coefficient variation and descent direction of func-
tional specificity between them. In our method, semantic
value of a term gm is marked as S(gm) and computed by
(3). In the formula, dep(gm) is the depth of gm, and
desc(gm) is number of descendants of gm, and Gtotal is
the total number of terms in GO hierarchy. Equation (3)
is proved to be effective on calculating semantic values
of terms in [24]. The semantic value of a term is big if
the term has few descendants or lies at deep level in GO
hierarchy. The bigger the semantic value of the term is,
the more specific is the function described by the term.
S gm
  ¼ dep gm
 
: 1?
log desc gm
 þ 1 
log Gtotalð Þ
 
ð3Þ
For interacting proteins vi and vj, gm is a known term
of the protein vi; an ideal term gm
* can be inferred from
gm to annotate protein vj; and semantic value of gm
* ,
Fig. 1 A simplified example of generating directed PPI network
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 11 of 79
S(gm
* ) can be computed by (4). Equation (4) is applicable
for propagating terms between interacting proteins no
matter which one of them is annotated.
S gm
  ¼ 1þ CV vi; ; vj
  ?O vi;;vjð Þ:S gm
  ð4Þ
Based on semantic value of the ideal term, one or
more relatives of the known term are selected to anno-
tate protein vj by (5). In the formula, R(gm) represents
the set of relatives of gm and gm
r is a relative of gm. To
do this, the relatives of the known term, which are the
most similar with the ideal term in term of the semantic
value, are selected to describe functions of the protein vj.
In practice, the selected relatives may be grandparents,
parents, siblings, children or grandchildren of the known
terms.
argmin
grm?R gmð Þ
S grm
 
?S gm
   ð5Þ
Generally speaking, this process provides three kinds
of predictions: (1) some ancestors of the known terms of
the proactive protein may be appropriate to describe the
reactive protein; (2) some descendants of the known
terms of the reactive protein can annotate the proactive
protein; (3) terms of two interacting proteins can be
shared directly by them if the proteins are equal in the
PPI network.
Diffusing functional information in the PPI network
To mine functional information as much as possible, an
iterative algorithm is designed to diffuse GO terms in
the whole PPI network. As described in Fig. 2, the algo-
rithm includes four steps as following.
Step 1: Select seed proteins from annotated proteins of
which proactive partners have not been annotated yet;
Step 2: Select relatives of known terms of seed pro-
teins to describe functions of their interacting partners
according to formulas (4) and (5);
Step 3: Update terms of seed proteins based on their
annotated reactive partners according to formulas (4)
and (5);
Step 4: Remove seed proteins from the annotated pro-
teins; the edges related to the seed proteins cannot me-
diate diffusing between interacting proteins; and go to
step 1 until all proteins in the PPI network are annotated
or there does not exist annotated partners for remained
unannotated proteins.
Time complexity analysis
Given a PPI network including n proteins, the time com-
plexity of determining functional relationship between
proteins is O(n2). Similarly, the time complexity of meas-
uring functional difference between proteins is O(n2) too.
If the proteins is at most annotated by p GO terms, and
the maximum degree of the proteins is k, the time com-
plexity of diffusing functional information between two
proteins is O(p × k). Accordingly, diffusing functional in-
formation in the whole PPI network is O(m × p × k) if
there are m proteins are annotated in the PPI network.
Based on these analysis, the time complexity of the
GoDIN should be O(n2) + O(n2) + O(m × p × k). Because
the maximal value of m is n and the maximal value of k is
n-1, the time complexity of the GoDIN is about O(n2).
A simple example of GoDIN
To make our method clearly, a simple example is illus-
trated in Fig. 3. In the Background of the Fig. 3, some
GO terms are organized as DAG, in which terms are
linked with arches oriented from child to parent. As
well, semantic values of the terms are all listed in the
Background. Initially, protein M is annotated with term
g7, L is annotated with term g4, and the other proteins
in the subnetwork are unannotated. The functional
proactive-reactive relationship between interacting pro-
teins has been marked by an arch oriented from the pro-
active protein to the reactive protein. To reveal
functions of the other proteins, annotations of M and L
are diffused between interacting proteins iteratively. In
Fig. 2 Algorithm for diffusing GO terms in the whole PPI network
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 12 of 79
our example, the diffusing process is finished through
six iterations. In each round, some functional inferences
are made and key information of inferences is displayed
in a table. The key information include the descent dir-
ection of functional specificity between two interacting
proteins (O(vi, vj)), functional coefficient variant between
seed protein and its neighbor (CV(vi, vj)), known term
(gm) and its semantic value (S(gm)), semantic value of
ideal term (S(gm
* )) and selected relative of the known
term (gr
*).
In the first iteration, M is regarded as a seed protein
and its neighbors include E, L and J. According to the
formula (1) and (2), O(M, E) is 1 and CV(M, E) is 1/6.
The known GO term of M is g7 and the semantic value
of g7, S(g7) is 0.35. By replacing parameters in the for-
mula (4) with these data, S(g7
* ) is estimated as 0.408. Ac-
cording to the formula (5), g8 is appropriate to annotate
protein E. Similarly, the annotations of L and J are
predicted by the same means. Note that, because L has
been annotated before diffusion, Ls term g4 should also
be diffused to the seed proteins M. According to True
Path Rule (TPR), g4 also annotates M if g4 is an ancestor
of g7. Thus, the annotations of M cannot be changed by
GO term g4. In addition, the protein M cannot be se-
lected as a seed protein again and arches M ? J,
M ? E, M?L cannot be used to diffuse GO terms
again.
In the second iteration, J, E and L are candidates for
seed proteins. Because the protein L has a proactive an-
notated partner E, L cannot be taken as a seed protein.
Therefore, J and E are selected as seed proteins. Accord-
ing to the formula (1), O(J, A) is 0, which means that
the protein J and A share the same function. Thus, pro-
tein A can be annotated with term g8, which is also can
be inferred though the formula (4) and (5). Different
from L, A has not been annotated at all before diffusion,
Fig. 3 A simple example of how to diffusing GO terms though directed PPI network
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 13 of 79
so it does not need to infer annotations of J from those
of A. As for the seed protein E and its partner L, O(A,
L) is ?1 and CV(A, L) = 1/6 in term of the formula (1)
and (2). Based on these parameters and S(g8),
S(g8
* ) = (1 + 1/6)-1 × 0.4 = 0.343. Therefore, term g7 is
selected to annotate L in term of the formula (5). After
that, protein J and E cannot be regarded as seed proteins
and arches J?A and E ? L cannot be used in the other
iterations.
The processes of the 3rd, 4th, 5th iterations are similar
to the previous iterations. Due to Space Limitations, the
details of these iterations are not described here. In the
6th iteration, it can be found that all proteins in the sub-
network have been annotated already and no arch which
can mediate diffusing between interacting proteins re-
mains. Thus, the iteration is terminated and the diffus-
ing of GO terms though the subnetwork is finished. The
result of inferences are collected and listed in the table.
Experiments and discussions
Experimental datasets
Three high reliable PPI networks of saccharoinyces cere-
visiae (Krogan, DIP, BioGRID) are used to study the per-
formance of the proposed method GoDIN. Krogan [25]
consists of interactions with probabilities above 0.273.
The latest version of DIP was downloaded from database
of interacting protein (http://dip.doe-mbi.ucla.edu/dip/)
[26] on July 7, 2013. BioGRID consists of the physical in-
teractions of saccharoinyces cerevisiae and it was down-
loaded from biological general repository for interaction
datasets (http://thebiogrid.org/download.php) [27] on
March 10, 2015. At the same time, the functional anno-
tations of proteins of saccharoinyces cerevisiae were
download from GO website and the annotations with
evidence code IEA (Inferred from Electronic Annota-
tion), NR (Not Recorded), ND (No biological Data
available), or IC (Inferred by Curator) were excluded.
The basic information of the three PPI networks are
listed in Table 1. In the table, #PPI is the number of in-
teractions in the network; #Proteins is the number of
proteins in the network; #Annotated proteins is the
number of the proteins with GO annotations; MF, BP
and CC represent the annotation aspects: molecular
function, biological process and cellular component
respectively.
Performance measures
Three widely-used measures: precision (P), recall (R)
and f-measure (F) are employed to measure performance
of GoDIN and other related methods. The measures are
consistent with the famous Critical Assessment of Func-
tional Annotations (CAFA) experiments [28]. P is the
average precision of predictions about proteins on which
at least one prediction was made. R is average recall of
predictions on all target proteins. F is a harmonic mean
between P and R, which gives an intuitive number for
comparisons of the concerned methods. Supposed that x
represents a target protein and K (x) is a set of known
terms of x, P can be calculated as Eq. (6). In Eq. (6), P(x)
is the set of predictive annotations; S is the target pro-
tein set for testing; m is the number of proteins which at
least have one predictive term. Similarly, R and F can be
computed by Eq. (7) and Eq. (8) respectively.
P ¼ 1
m
X
x?S
K xð Þ?P xð Þj j
P xð Þj j ð6Þ
R ¼ 1
Sj j
X
x?S
K xð Þ?P xð Þj j
K xð Þj j ð7Þ
F ¼ 2P:R
P þ R ð8Þ
Functional relationship between interacting proteins
To study functional relationships of interacting proteins,
the interactions of Krogan, DIP and BioGRID are ana-
lyzed thoroughly. Firstly, annotations of proteins in the
networks are processed and the terms with evidence
code IPI (Inferred from Physical Interaction), IGI (In-
ferred from Genetic Interaction) are excluded to avoid
circular judgement. Secondly, the interactions are com-
posed of two annotated proteins are selected for analysis.
Finally, the selected interactions are grouped into: (1)
the same annotation group, (2) the similar annotation
group and (3) the related annotation group. The same
annotation group consists of interactions which are
composed of proteins with the same term. The similar
annotation group consists of interactions which are
composed of proteins with different terms of the same
sub-ontology. Usually, the terms of the same sub-
ontology are similar. The related annotation group con-
sists of interactions which are composed of proteins only
with terms of different sub-ontologies. The results of
analysis are displayed in Table 2. In the table, #PPIt is
the number of interactions in the network; #PPI is the
number of interactions in the group; Pct(%) presents the
percentage of interactions in the group.
From Table 2, it can be seen that nearly 60% of inter-
actions in the three networks belong to the first group;
about 40% of interactions belong to the second group;
Table 1 Basic information of the three PPI networks
Network #PPI #Proteins #Annotated proteins
MF BP CC
Krogan 7123 2708 2109 2424 2570
DIP 22,613 5097 3415 3941 4207
BioGRID 59,748 5640 4106 4754 5100
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 14 of 79
only less than 1% of interactions belong to the third
group. As far as we know, none of methods relying on
PPI network could annotate the interacting protein cor-
rectly in the third group. The traditional methods sup-
posed that the interacting proteins share the same term.
Thus, about 40% of functional predictions may not be
correct. Meanwhile, the results suggest that the majority
of interacting proteins share the same or similar terms,
which is consistent with basic assumptions of GoDIN.
Functional difference between interacting proteins
To investigate the influence of the degree on functional
difference between proteins, the annotations and degrees
of interacting proteins are analyzed. The analysis is per-
formed on Krogan, DIP, BioGRID and the interactions of
the networks are grouped into: the same annotation group
and the similar annotation group. The former consists of
interactions which are composed of proteins with the
same term, and the latter includes interactions of which
the proteins are annotated by similar terms. The results
derived from the same annotation groups and the similar
annotation groups are illustrated in Table 3 and Table 4
separately. In the tables, #PPI is the number of interac-
tions in the group; #SameDeg is the number of interac-
tions in which the interacting proteins with the same
degree in the group; #DiffDeg is the number of interac-
tions in which the interacting proteins with different de-
grees in the group; Pct(%) presents the percentage of
interactions in the group. The results suggest that the ma-
jority of the interacting proteins have different degrees in
the three networks. Some of the interacting proteins with
different degrees are annotated by similar terms while the
others share the same term.
To explain this phenomenon, coefficient variation is
used to measure functional difference between the inter-
acting proteins. The coefficient variations of proteins
with different degrees in the same annotation group are
compared with those in the similar group. As shown in
Fig. 3, the box-whisker plots are used to display the dis-
tributions of coefficient variations of different groups. In
the figure, the distributions of the coefficient variations
in the same annotation groups are represented by
dashed boxes and lines. Meanwhile, the distributions of
coefficient variations in the similar annotation groups
are represented by solid boxes and lines. As known, the
bottom and top of the boxes are always the first and
third quartiles of coefficient variations, and the bands in-
side the boxes are the second quartiles (the median) of
coefficient variations, and the hollow spots inside the
boxes are the averages of coefficient variations. For clear,
the same annotation groups of the three networks:
Krogan, DIP and BioGRID are marked as SameKrogan,
SameDIP, SameBIO respectively. Accordingly, the similar
annotation groups of those networks are signed as Simi-
larKrogan, SimilarDIP and SimilarBIO.
As displayed in Fig. 4, coefficient variations in the two
different groups of the same network show obvious dif-
ferent distributions. According to the median, average,
the first and third quartiles, the coefficient variations in
the same annotation groups are higher than those in the
similar annotation groups. This may suggest that the
functional differences of interacting proteins in the same
annotation groups are smaller than those in the similar
annotation groups. Sometimes, although the degrees of
proteins are different, the functional coefficient variation
between the proteins is tiny small. Therefore, the func-
tional difference between the proteins may be negligible
and they share the same term. This just explains why
some of the interacting proteins with different degrees
share the same term. According to the results and ana-
lysis, the coefficient variation defined by GoDIN is ef-
fective to measure the functional difference between
interacting proteins. The functional coefficient variation
between interacting proteins can be considered as a
positive clue to predict protein function.
Table 2 Functional relationship of interacting proteins
Network #PPIt Same annotation Similar annotation #Related annotation
#PPI Pct(%) #PPI Pct(%) #PPI Pct(%)
Krogan 6931 4114 59.36 2794 40.31 23 0.33
DIP 20,050 9816 48.96 10,182 50.78 52 0.26
BioGRID 58,765 30,154 51.31 28,464 48.44 147 0.25
Table 3 Relationship between function and degree of
interacting proteins in the same annotation group
Network #PPI #SameDeg Pct(%) #DiffDeg Pct(%)
Krogan 4114 243 5.9 3871 94.1
DIP 9816 531 5.4 9285 94.6
BioGRID 30,154 406 1.3 29,748 98.7
Table 4 Relationship between function and degree of
interacting proteins in the similar annotation group
Network #PPI #SameDeg Pct(%) #DiffDeg Pct(%)
Krogan 2794 91 3.26 2703 96.74
DIP 10,182 159 1.56 10,023 98.44
BioGRID 28,464 188 0.6 28,276 99.4
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 15 of 79
Comparison with the related methods
To test the performances of GoDIN, we take FunFlow
[7], CIA [17], FCML [18] as comparing methods. These
comparing methods have been discussed in the intro-
duction. They are three typical methods of predicting
protein function based on PPI network and GO context
respectively. The comparisons are performed on Krogan,
DIP and BioGRID from three annotation aspects: mo-
lecular function (MF), biological process (BP) and cellu-
lar component (CC) respectively. Figures 4, 5 and 6
show the precision, recall and F-measure of these
methods on different networks and annotation aspects.
As shown in Fig. 5, the precision of GoDIN is compar-
able to the best methods: CIA and FCML on Krogan.
Meanwhile, GoDIN shows better precision than the other
methods on DIP and BioGRID. FunFlow performs better
than the others on DIP but it shows the lower precision
than other methods on Krogan and BioGRID. In GoDIN,
the functional differences of interacting proteins are con-
sidered and the differences of terms are used to demon-
strate the functional differences during predicting protein
function. This is why GoDIN shows better performances
than the others in term of the precision. The functional
relationships of terms are also considered thoroughly in
CIA and FCML, but they pay no attention to the func-
tional differences of interacting proteins. FunFlow ignores
the functional relationships of terms in the process of pre-
dicting protein function so that it performs not as well as
the others.
In addition, it is also found that all of the methods show
relatively low accuracy. This may be due to two issues: (1)
the large number of GO terms; (2) the dependency of GO
terms. The influence of the above issues will be more ob-
vious while the proteins are annotated by more terms.
This would be a place to start the future study.
As displayed in Fig. 6, FunFlow shows the best recall on
almost all of the networks while GoDIN performs better
on most of the networks and annotation aspects than
FCML and CIA. The performances of CIA are not better
than those of FCML. This may be attributed to global
characteristics and local characteristics of PPI network.
Specifically, CIA only takes local characteristics of PPI net-
work into consideration in predicting protein functions
while the other methods consider both global and local
characteristics of PPI network. This may be the reasons
why the recall of CIA is lower than those of the other
methods. Besides, some proteins in the datasets are anno-
tated by shallow terms, and the misjudgments on these
Fig. 4 Comparison of coefficient variants based on different
annotation groups
Fig. 5 Comparison of precision of the related methods
Fig. 6 Comparison of recall of the related methods
Fig. 7 Comparison of F-measure of the related methods
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):27 Page 16 of 79
proteins have obvious negative impact on the recall. This
would be a place to start our future study.
As shown in Fig. 7, GoDIN performs almost as well as
the best method FCML on Krogan while shows the best F-
measure on DIP and BioGRID. FunFlow performs not bet-
ter than the others on all of the networks. Overall, GoDIN
shows better performances than the three methods in
terms of metrics: precision, recall and F-measures when
they are applied to predict protein functions.
Conclusions
Predicting protein function based on PPI network is a
hotspot of biological research in recent years. In this
paper, the functional relationship between interacting
proteins is studied and a novel method of protein func-
tion prediction is proposed based on the relationship.
To validate the effectiveness of the method, a series of
analysis and experiments are performed on the three
high reliable networks from the different annotation as-
pects. The results suggest that: (1) interacting proteins
are not equal in the PPI network, and their function
may be same or similar, or just related; (2) functional
difference between interacting proteins can be measured
by their degrees in the PPI network; (3) functional rela-
tionship between interacting proteins can be expressed
by semantic relationship between GO term and its rela-
tives; (4) compared with the other concerned methods,
GoDIN has high precision and f-measure and it is effect-
ive on predicting protein function.
Acknowledgements
We would like to thank the editors and the anonymous reviewers for their
comments that led to significant improvements in our manuscript.
Funding
Publication of this article was funded by Fundamental Research Funds for
the Central Universities (DB13AB02, DL13AB02) and Natural Science
Foundation of China (61,671,189, 61,271,346, 61,571,163, 61,532,014 and
91,335,112).
Availability of data and materials
Not applicable.
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1,2017: Selected articles from the Biological
Ontologies and Knowledge bases workshop. The full contents of the
supplement are available online at https://jbiomedsem.biomedcentral.com/
articles/supplements/volume-8-supplement-1.
Authors contributions
Conceived and designed the approach: ZXT, MZG. Implemented the
approach and performed the experiments: ZXT,ZT, KC. Analyzed the results:
ZXT, MZG, XYL, ZT. Contributed to the writing of the manuscript: ZXT, MZG,
XYL. All the authors have approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Published: 20 September 2017
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 
DOI 10.1186/s13326-016-0111-z
RESEARCH Open Access
Consolidating drug data on a global scale
using Linked Data
Milos Jovanovik* and Dimitar Trajanov
Abstract
Background: Drug product data is available on the Web in a distributed fashion. The reasons lie within the
regulatory domains, which exist on a national level. As a consequence, the drug data available on the Web are
independently curated by national institutions from each country, leaving the data in varying languages, with a
varying structure, granularity level and format, on different locations on the Web. Therefore, one of the main
challenges in the realm of drug data is the consolidation and integration of large amounts of heterogeneous data into
a comprehensive dataspace, for the purpose of developing data-driven applications. In recent years, the adoption of
the Linked Data principles has enabled data publishers to provide structured data on the Web and contextually
interlink them with other public datasets, effectively de-siloing them. Defining methodological guidelines and
specialized tools for generating Linked Data in the drug domain, applicable on a global scale, is a crucial step to
achieving the necessary levels of data consolidation and alignment needed for the development of a global dataset
of drug product data. This dataset would then enable a myriad of new usage scenarios, which can, for instance,
provide insight into the global availability of different drug categories in different parts of the world.
Results: We developed a methodology and a set of tools which support the process of generating Linked Data in the
drug domain. Using them, we generated the LinkedDrugs dataset by seamlessly transforming, consolidating and
publishing high-quality, 5-star Linked Drug Data from twenty-three countries, containing over 248,000 drug products,
over 99,000,000 RDF triples and over 278,000 links to generic drugs from the LOD Cloud. Using the linked nature of
the dataset, we demonstrate its ability to support advanced usage scenarios in the drug domain.
Conclusions: The process of generating the LinkedDrugs dataset demonstrates the applicability of the
methodological guidelines and the supporting tools in transforming drug product data from various, independent
and distributed sources, into a comprehensive Linked Drug Data dataset. The presented user-centric and analytical
usage scenarios over the dataset show the advantages of having a de-siloed, consolidated and comprehensive
dataspace of drug data available via the existing infrastructure of the Web.
Keywords: Methodology, Drugs, Drug products, Healthcare, Linked data, Open data, Data consolidation, Tools
Background
Accessing comprehensive drug and healthcare data on
the Web can be a challenging task. The main reason
lies in the fact that the data is available in different for-
mats, at distributed Web locations. Additionally, most
of the drug and healthcare datasets are published for
specific purposes only, so consequentially, they contain
*Correspondence: milos.jovanovik@finki.ukim.mk
Faculty of Computer Science and Engineering, Ss. Cyril and Methodius
University in Skopje, Rugjer Boshkovikj 16, P.O. Box 393, 1000 Skopje,
Macedonia
limited data. For instance, national drug repositories con-
tain information about drug products which are approved
and sold in the country [17]; websites aimed for the
general public [813] contain descriptive drug use infor-
mation, such as target, dosage, packaging and warnings;
websites aimed for professionals [1416] contain more
specific drug data, such as active ingredients, chemical
formulas, drug-drug interactions, food-drug interactions,
toxicity, etc. Websites such as DrugBank [14, 17] con-
tain more comprehensive drug data. However, the drug
entries in their dataset are generic drugs, i.e. active ingre-
dients of drugs, and not actual drug products which can
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 2 of 24
be bought by patients. Various mobile and web appli-
cations [1822] contain information about drug prod-
ucts, but the data they use is country-specific and is
not available to the interested parties in an open for-
mat. Global drug product repositories exist as well [23
25], but they either do not provide means of getting the
data in an open format, or are locked behind a pay-wall.
Additionally, some of them provide data for drugs reg-
istered in countries from specific world regions only, or
solely provide the name of the drug product in a specific
country.
The Linked Data approach
According to [2629], the emergence of the Linked Data
principles has introduced new ways to integrate and con-
solidate data from various and distributed sources. The
Linked Data principles provide means and standards for
representing, storing and retrieving data over the exist-
ing infrastructure of theWeb. This enables publishing and
contextual linking of data on the Web and with it, cre-
ating a Web of data, as opposed to the current Web of
documents.
With this, the Web becomes a distributed network for
standards-based data access, usable by software agents
and machines. The interlinked nature of the distributed
datasets provides new use-cases for the end-users, which
are generally unavailable over isolated datasets. The
Linked Data approach solves the issue of having data
silos in traditional relational database systems, silos which
cannot link to other databases without a specific code
written for the task, or specific mapping created in a data
warehousing solution.
Additionally, the schema-flexibility of RDF provides
means of independent data management and evolution,
which is very well suited for the nature of theWeb, but also
for non-Web use in environments where data de-siloing
and consolidation is needed [30, 31].
So far, as a result of the adoption of Linked Data prin-
ciples by data publishers, the Linked Open Data (LOD)
Cloud [32] has been populated with 1104 interlinked
datasets [33]. These datasets, published on the Web fol-
lowing the Linked Data principles, belong to 8 different
domains: government, publications, life sciences, user-
generated content, cross-domain, media, geographic and
social web.
Motivation
Drug and healthcare data is already available on the Web,
both as Linked Data in the LOD Cloud and as regular
data on websites and in mobile applications, intended for
human consumption. However, the drug data available
currently as Linked Data is comprised of drug entities
which are generic drugs, i.e. active ingredients of drugs,
not actual drug products registered in a specific country,
under a specific name, with a specific dosage form,
strength and price, for which an end-user might be inter-
ested. Such end-users are the patients, the pharmacists,
the doctors, but also medical institutions, pharmaceuti-
cal companies, etc, which need access to drug products
from specific countries for a multitude of user-centric and
analytical scenarios [34]: accessing general information
about drugs which can be bought in a country, accessing
information about the availability of different drugs and
drug categories in different countries, accessing pricing
information, etc.
The national drug data are generally available on the
Web, but on regular webpages and not as Linked Data.
In order to transform the data from the national drug
registries into high-quality, 5-star Linked Data [35], we
propose a set of methodological guidelines which aim to
assist drug data publishers and other interested parties
into generating a global Linked Drug Data dataset, con-
sisting of official data about drugs registered for use in
different countries. One such global Linked Drug Dataset
would be accessible by using W3C standards via the exist-
ing Web infrastructure and would enable a myriad of new
drug-analysis scenarios, including the above-mentioned
ones. It would enable further data exploitations via devel-
opment of innovative applications and services which use
the data gathered from national drug registries of different
countries in the world.
Such methodological guidelines should include steps
aimed towards modeling and aligning the data, trans-
forming the data into 5-star Linked Data, publishing the
created datasets on the Web and defining use-cases or
developing applications on top of the dataset. They should
be aimed towards assisting drug data owners and pub-
lishers, such as the national governing bodies, medical
institutions, pharmaceutical companies, pharmacies, etc.,
in publishing their data in the same aligned, Linked Data
format. Their data, once transformed into Linked Drug
Data and interlinked with the drug data already pub-
lished using the same methodology, could be used for
data-based analytics (by the medical institutions, pharma-
ceutical companies and governing bodies) or for reaching
potential customers (by pharmacies).
In this paper, we propose a set of such methodolog-
ical guidelines for consolidating drug data on a global
scale, using Linked Data. In order to support our method-
ological guidelines, we have developed a set of open and
publicly available tools, which can be reused in the pro-
cess of applying the steps from themethodology over drug
data from any country.
Using the proposed methodological guidelines and
tools, we generated the LinkedDrugs dataset which con-
sists of drug product data from twenty-three countries,
and therefore validated the methodology. For this pur-
pose we developed an automated system which gathers
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 3 of 24
drug data from the official national drug registries of
twenty-three different countries, executes data cleaning,
aligns and transforms the data into 5-star Linked Data and
publishes them on the Web in a common, aligned and
consolidated Linked Data dataset. We then demonstrate a
set of user-centric and analytical usage scenarios over the
generated dataset, which are otherwise unavailable or very
time-consuming in a scenario where a user works with the
data available on the Web in HTML webpages.
Related work
Linked Data projects in the healthcare domain
Numerous projects and efforts have already worked on
transforming drug and healthcare data into Linked Data.
According to [33], there are currently 83 life science
datasets in the LODCloud. These datasets contain health-
care data from various subdomains, such as drugs, dis-
eases, genes, interactions, clinical trials, enzymes, etc. The
most notable are the Linking Open Drug Data (LODD)
project, Bio2RDF and the Semantic Web Health Care and
Life Sciences Interest Group at W3C.
The LODD project [36] focuses on interlinking data
about drugs already existing on the Web, as described in
[37, 38]. The data ranges from impact of the drugs on
gene expression to results of clinical trials. The aim of
the project is to enable answering of interesting scientific
and business questions by interlinking previously sepa-
rated data about drugs and healthcare. As part of their
work, they have collected datasets with over 8,000,000
RDF triples, interlinked with more than 370,000 RDF
links. However, it seems that the project has not been
updated for some time now, and Bio2RDF has taken over
the hosting and continual publication of the datasets.
Bio2RDF [39] is an open-source project which cre-
ates RDF datasets from various life science resources
and databases, and interconnects them following the
Linked Data principles into one comprehensive network
[4042]. The latest release of Bio2RDF contains around
11 billion triples which are part of 35 datasets [43].
These datasets hold various healthcare data: clinical trials
(ClinicalTrials), drugs (DrugBank, LinkedSPL, NDC), dis-
eases (Orphanet), bioactive compounds (ChEMBL), genes
(GenAge, GenDR, GOA, HGNC, HomoloGene, MGD,
NCBI Gene, OMIM, PharmGKB, SGD, WormBase), pro-
teins (InterPro, iProClass, iRefIndex), gene-protein inter-
actions (CTD), biomedical ontologies (BioPortal), side
effects (SIDER), terminology (Resource Registry, MeSH,
NCBI taxonomy), mathematical models of biological pro-
cesses (BioModels), publications (PubMed), etc.
One of the datasets, which is a part of the LODD
cloud and Bio2RDF, is the DrugBank dataset, described in
[17]. It provides RDF data about drugs, such as chemical,
pharmacological and pharmaceutical information, taken
from an the existing DrugBank database [14, 17] of drug
information. The DrugBank RDF dataset [44] contains
over 766,000 RDF triples for 4,770 drugs. These drugs are
generic drugs, i.e. active ingredients in drug products.
The World Wide Web Consortium (W3C) has estab-
lished the SemanticWeb for Health Care and Life Sciences
Interest Group (HCLS IG) [45] to help organizations from
the health domain in their adoption of the Semantic Web
technologies. It is comprised of experts from around 30
W3C member organizations: research centers, universi-
ties, companies, health institutions, etc. Its mission is
to develop and support the use of the technologies of
the Semantic Web in the fields of healthcare, life sci-
ences, clinical research and translational medicine [46]. It
is comprised of various subgroups, which are focused on
making the biomedical data available in RDF, developing
and maintaining biomedical ontologies, etc.
In recent years, our research team gained experience in
the drug and healthcare domain by applying the Linked
Data principles and the Semantic Web technologies in the
several different scenarios.We have transformed and pub-
lished the drug product data from the Health Insurance
Fund of Macedonia as 5-star Linked Data, by connecting
it to the LODD and LOD Cloud datasets via the Drug-
Bank dataset [47]. We have since extended this dataset
with 5-star Linked Data about the Macedonian medical
institutions and drug availability lists from pharmacies
[48]. We have also used Linked Data for an analysis of
the connections between drugs and their interactions with
food, and recipes from different national cuisines, result-
ing in findings that uncovered the ingredients and cuisines
most responsible for negative food-drug interactions in
different parts of the world [49]. These projects helped us
gain insight in the domain and identify the challenges of
applying Linked Data in the domain.
Linked Datamethodologies
There are a few methodologies defined in the Linked Data
domain, which deal with the process of generating, pub-
lishing and using Linked Data. They are mainly focused
on government data, but some are domain independent.
The W3C Government Linked Data Working Group has
created official guidelines for publishing and accessing
open (government) data using the Linked Data principles
[50], and with it they suggest three existing methodologies
which can be used with the Linked (Government) Data
lifecycle. These three methodologies are: (a) the method-
ology of Hyland et al., (b) the methodology of Hausenblas
et al. and (c) the methodology of Villazón-Terrazas et al.
Hyland et al. [51] define a methodology for Linked Gov-
ernment Data, which consists of six steps. Their method-
ology is based on the specifications and best practices by
the W3C, and consists of the following steps: (1) Identify,
(2) Model, (3) Name, (4) Describe, (5) Convert, (6) Pub-
lish, and (7) Maintain. The methodology contains most
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 4 of 24
steps which are part of the generally accepted Linked Data
lifecycle, but is missing guidelines on how to use the gen-
erated Linked Data. The authors believe that the usage of
the generated dataset should be left to the users and other
interested parties, and according to them, is not a task for
the Linked Data publisher.
Hausenblas et al. [52] state that the existing data man-
agement approaches - which assume control over the data,
the schema and the data generation - cannot be used in the
environment of the Web, due to its open and decentral-
ized nature. Their methodology consist of the following
steps: (1) Data awareness, (2) Modeling, (3) Publishing, (4)
Discovery, (5) Integration, and (6) Use-cases. It also cov-
ers most of the general Linked Data lifecycle steps, but
does not provide detailed guidelines for the process of
publishing the generated Linked Data dataset on theWeb.
Similarly to Hyland et al., based on their experience
in linked government data production, Villazón-Terrazas
et al. [53] define a set of methodological guidelines for
generating, publishing and exploiting Linked Govern-
ment Data. Their lifecycle consists of the following steps:
(1) Specify, (2) Model, (3) Generate, (4) Publish, and (5)
Exploit. This is the only existing methodology in the
Linked Data domain which covers all of the lifecycle steps,
but unfortunately is focused on government data.
In addition to these three methodologies selected by
the W3C Government Linked Data Working Group, the
LOD2 Project has developed an updated Linked Data
lifecycle for extracting, creating, enriching, linking and
maintaining Linked Data [54]. The Linked Data lifecycle
supported by the LOD2 integrated environment consists
of (1) Extraction, (2) Storage, (3) Authoring, (4) Interlink-
ing, (5) Classification, (6) Quality, (7) Evolution/Repair,
and (8) Search/Browsing/Exploration. Even though this is
the only methodology which provides software tools for
the denoted steps, and the number of steps here is larger
than in the other methodologies, it still misses some key
elements of the Linked Data lifecycle, such as the data
modeling, the definition of the URI format for the enti-
ties and the ways of publishing the generated dataset. The
provided tools are also general, and cannot be applied
in a specific domain without further work and domain
knowledge.
Results
We developed a methodology and a set of supporting
tools for the drug domain, which allowed us to streamline
the incremental process of generating high-quality Linked
Data of drug products from twenty-three countries. This
newly created dataset of drug product data, the Linked-
Drugs dataset [55], currently consists of over 248,000
drug products, interlinked with over 91,000,000 relations
denoting similarity between them, and with over 278,000
links to their corresponding active ingredients available
as Linked Data in the LOD Cloud. The LinkedDrugs
dataset enables a novel way of using drug product data, by
unlocking new user-centric and analytical usage scenar-
ios, previously unavailable over isolated and siloed drug
data repositories. These scenarios utilize the consolidated
and aligned nature of the dataset and its contextual links
to entities from the LOD Cloud. This is achieved by auto-
matically generating the additional relations in our dataset
which link the drugs between themselves, and link the
drugs with drug entities and active ingredients published
as part of other LOD Cloud datasets. Using W3C stan-
dards over the existing infrastructure of the Web, we are
then able to retrive data from these distributed datasets,
and present them to the end-users in a comprehensive
manner.
Here, we will provide a brief overview of the developed
methodological guidelines and their tools aimed at assist-
ing the data publishers during the specific steps in the
methodology, while a more in-depth description is pro-
vided in the Methods section. We will then present the
LinkedDrugs dataset, along with the process of develop-
ing the automated system which constructs the dataset.
Then, we will present an overview of newly enabled
usage scenarios over the consolidated drug product
data.
Themethodological guidelines
Our methodological guidelines for consolidating drug
data using the Linked Data approach improve upon the
existing Linked Data methodologies and contain steps,
activities and tools which are specific to the drug data
domain. We used our experience in the domain of Linked
Healthcare Data to develop guidelines which aim to guide
data publishers through the process of generating high
quality, 5-star Linked Data in order to interlink, align and
consolidate drug data from different national drug reg-
istries or other sources of drug data. The alignment and
relationship between the existing methodologies and our
guidelines is outlined in Table 5.
Our methodology consists of five steps (Fig. 1):
(1) Domain and Data Knowledge, (2) Data Modeling and
Alignment, (3) Transformation into 5-star Linked Data,
(4) Publishing the Linked Data Dataset on the Web, and
(5) Use-cases, Applications and Services. These steps have
been developed with reuse as a primary goal; therefore,
their main focus is the encouragement of data publishers
in the drug domain to develop, modify and use reusable
components during the steps of the methodology. This
makes the Linked Drug Data lifecycle modular, i.e. con-
structed of loosely-coupled components which can be
reused in the domain. Here, by loosely-coupled we mean
components which can be used separately when neces-
sary, but which also form a seamless workflow for gener-
ating a high-quality, 5-star Linked Drug Data dataset. The
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 5 of 24
Fig. 1 The methodology for consolidating drug data using the Linked Data approach
reuse of such components reduces development time and
increases productivity [56, 57].
Methodology supporting tools
As part of the methodological guidelines, we developed
a set of tools as reusable components which simplify the
execution of the specific steps of the methodology. Their
intent is to support the Linked Drug Data generation pro-
cess for both people from the drug domain which do not
have deeper knowledge of Linked Data, and Linked Data
publishers which do not have deeper knowledge of the
drug domain. The set consists of (a) the RDF schema,
(b) the CSV template, (c) the OpenRefine transforma-
tion script, (d) the SPARQL-based tool for extending
and interlinking the dataset and (e) the web-based tool
for automated transformation, interlinking and publish-
ing of the generated Linked Drug Data dataset. These
reusable components are open and publicly available on
GitHub [58].
The RDF schema is a common data schema for all
national drug data repositories, used for modeling of drug
products on a global scale (Fig. 2). Its goal is to provide
alignment of drug data from different sources, with dif-
ferent format and different levels of data granularity, in
order to enable simpler data exploitation. It is comprised
of classes and properties from the Schema.org vocabulary
[59], which is a novel approach in the drug data domain.
The CSV template represents the necessary formal tem-
plate for the data which is being prepared for transfor-
mation, i.e. the data which will be annotated with our
RDF schema. It contains 39 columns which represent
the different data fields needed from the source data for
the transformation process. They inlude the URI of the
drug, its brand name, generic name(s), manufacturer, ATC
code(s), active ingredient(s), strength, cost, license infor-
mation, etc. They are modelled to fit the RDF schema,
which encompasses all data necessary for high-quality
modeling of the domain.
The OpenRefine transformation script is a reusable
tool which helps automate the transformation process,
while ensuring compliance of the generated data with
the defined RDF schema and therefore provides aligned,
high-quality 5-star Linked Data for the drug domain. Its
intent is to lower the bounds of transforming data into
RDF and Linked Data for data publishers which are not
deeply involved and experienced in the Semantic Web
and Linked Data practices. Through several sub tasks,
this reusable script for the OpenRefine-based suite of
tools (e.g. LODRefine, BatchRefine), interlinks the drug
products between themselves based on the therapeutic,
pharmacological and chemical properties of the drugs,
links the drug products to generic drugs, i.e. active ingre-
dients from the LOD Cloud datasets and transforms the
data into RDF format.
The SPARQL-based tool for extending and interlink-
ing the dataset is comprised of two reusable SPARQL
queries. The first query extends the dataset by assigning
ATC codes to all drugs from the dataset which miss this
information. It does so by using the interlinked nature of
the dataset, i.e. the links the drug products have to generic
drugs and active ingredients from the LOD Cloud. The
second query detects all pairs of drugs from the dataset
which have the same ATC code, and then interlinks them
with properties from the RDF schema which denote the
therapeutic, pharmacological and chemical similarity of
the drugs.
The web-based tool for automated transformation,
interlinking and publishing of the generated Linked Drug
Data dataset is intended for data publishers which gen-
erate Linked Drug Data with the previous tools. The
data publishers can upload the generated Linked Data
dataset(s) on the LinkedDrugs project website [60], and
after a human-based quality assessment, the dataset will
be automatically published. For this we use a publicly
available Virtuoso instance [61], from which the new
dataset is available on the Web as Linked Data, via its
SPARQL endpoint [62].
LinkedDrugs: global linked drug products dataset
We applied the outlined steps of the methodology and
the set of tools as part of an automated system for
transforming and generating 5-star Linked Drug Data
from twenty-three countries: Austria, Azerbaijan, Costa
Rica, Cyprus, Egypt, Estonia, Ireland, Macedonia, Malta,
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 6 of 24
Fig. 2 The RDF vocabulary designed for the drug data domain, comprised of Schema.org classes and properties. For dataset interoperability, it also
uses the classes from the ATC Classification Ontology and properties from the DrugBank Ontology and RDFS
Netherlands, New Zealand, Nigeria, Norway, Romania,
Russia, Serbia, Slovakia, Slovenia, South African Republic,
Spain, Uganda, Ukraine and USA. The countries were
chosen to represent the global diversity and to show that
a holistic solution for generating Linked Drug Data on a
global scale is possible. Currently, the generated Linked-
Drugs dataset contains over 99,000,000 RDF triples, which
represent data for over 248,000 drug products from
the denoted countries. The dataset also contains over
91,000,000 schema:relatedDrug relations between
the drugs, and over 278,000 rdfs:seeAlso relations to
generic drugs from DrugBank and DBpedia.
This automated system and its workflow represent a
concrete example of applying the methodological guide-
lines and supporting tools presented in this paper, and
thus serve as their validation scenario.
Generating the LinkedDrugs dataset
The national drug registries of many countries around
the world are available online. We analyzed the national
drug registry websites of 31 countries in order to define a
common set of properties, i.e. a schema skeleton, for the
target dataset. These steps of domain analysis and RDF
schema definition correspond to the activities denoted in
Step I and Step II of the methodological guidelines, which
are already done and we directly use them for our specific
application.
In order to design, test and validate our automated sys-
tem for gathering 2-star drug data from the national drug
registries and generating 5-star Linked Data from the drug
domain on a global scale, we selected a subset of twenty-
three countries. We aimed for a diverse subset, which will
encompass different global regions.
The drug registries of these countries are available
online. Their websites are listed in the project page on
GitHub [58]. The drug data from most of the national
registries is available in a structured format in HTML
pages, intended for human consumption via searching
and browsing on the website itself. The data from a
smaller group of countries is available via structured files
in Microsoft Excel or PDF formats, available for direct
download.
The automated system gathers the data, performs data
cleaning, aligns the data with the predefined schema
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 7 of 24
skeleton, uses the transformation script and the SPARQL
query to transform the data to RDF, extend it with missing
ATC codes and add links to drugs from the same dataset
and drugs from DrugBank and DBpedia, thus turning
the dataset into a Linked Data dataset. The workflow of
these actions is depicted in Fig. 3. These steps represent
the activities defined in Step III of the methodological
guidelines.
Data gathering and staging. In order to create a sus-
tainable system for Linked Drug Data, we had to design a
way to collect the necessary data from the national drug
registries, on a scheduled basis. Therefore, we developed
a set of specialized web crawler applications which crawl
the designated drug registry websites, collect the neces-
sary data, clean it and store it in a predefined CSV format
(Fig. 3). We need to use a set of such crawlers as the target
websites differ in structure and available data. The out-
put CSV files from the crawlers use the predefined CSV
structure described above.
Like most of the data available on the Web, the drug
data available on the national drug registry websites is
not evenly structured, nor completely clean. We therefore
needed to extend our crawlers with functionalities which
perform data cleaning tasks and work to detect data from
all variations of the source webpages.
As per the suggestions in Step III of the guidelines,
we used their existing webpage URLs of the drugs as
their unique URIs, e.g. https://lekovi.zdravstvo.gov.mk/
drugsregister/detailview/53457. As many of the drug enti-
ties contain information about more than one generic
name, manufacturer, active ingredient and strength, the
crawlers are tasked with splitting them into the corre-
sponding columns in the CSV files. Additionally, infor-
mation such about the cost of the drug and its strength
are split into separate fields, to match the CSV template.
The crawlers also take care about the specific formats
needed for some of the columns, such as the dates, the
country codes, the currency codes, and the prescription
status.
The drug data from the several of the countries were
an exception, as they are available for download as
Microsoft Excel or PDF files. For these datasets, the
crawlers have to restructure the columns from the source
data and generate a CSV file with the correct column
names according to the CSV template. For these drugs,
we generate custom URIs as identifiers, which have the
format http://linkeddata.finki.ukim.mk/lod/data/loddw/
drugs/countryCode#drugID. Here, drugID is an ID gen-
erated by the crawler, countryCode is a three-letter
country code (according to [63]) of the country where the
drug was registered and the other parts of the URI identify
the project and the datatype on our Linked Data website:
/lod/data/loddw is the project and /drugs is the
categorization of the data.
The result of this stage in the workflow, in our case with
the twenty-three selected countries, is a set of twenty-
three separate CSV files which follow the CSV template.
The only difference is that some of the CSV files can have
no values in some columns, due to the data being unavail-
able online. When we get all twenty-three CSV files, the
first part of the workflow is done andwe can continue with
the second part.
Transformation to Linked Data. The CSV files can
be combined into one CSV file, or remain separate. The
only difference will be in the performance of the next
step which can be done as a single longer process, or
as twenty-three separate and shorter processes. To keep
the processing time per transformation shorter, we use
twenty-three separate CSV files, each representing the
drug dataset of a different country.
We send the twenty-three CSV files to a BatchRe-
fine instance [64], which represents a wrapper over an
Fig. 3Workflow: Transforming 2-star data from different national drug data registries to 5-star Linked Drug Data
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 8 of 24
OpenRefine instance with the RDF extension, that can
be used as a REST-based service. The HTTP POST calls
to the BatchRefine REST-based service are done with the
reusable BASH script developed as part of the methodol-
ogy supporting tools and contain (a) the CSV file which
needs to be transformed and (b) the OpenRefine trans-
formation script defined as a supporting tool for the
guidelines. The result of the call is a transformed RDF out-
put, which contains part of the generated Linked Drug
Data dataset.
The output of our BatchRefine transformations are
twenty-three RDF files in Turtle format. These RDF files
are a Linked Data dataset: they contain 5-star data about
the drugs from the twenty-three countries, along with
links to generic drugs from the LOD Cloud - more specif-
ically, links to generic drugs from both DBpedia and
DrugBank. As we will see later in the text, we can use these
links to fetch data about the drugs from our dataset which
we dont have on our end and which do not exist on the
source national drug registry websites, but can be found
in other datasets on the Web and can potentially prove to
be of interest for the end-users.
After the transformation with BatchRefine is done, we
load the RDF files into a Virtuoso instance [61] using a
BASH script. All RDF files are loaded into the same RDF
graph. The latest run of the workflow (Fig. 3) resulted in
248,746 distinct drugs in this step, represented with a total
of 7,245,294 RDF triples and with 278,542 outgoing links
to drugs from the LOD Cloud.
After the RDF data has been stored into an RDF graph
in Virtuoso, we use the SPARQL queries for extend-
ing the dataset with missing ATC codes and interlink-
ing related drugs. We execute the SPARQL queries over
our dataset stored in Virtuoso, using a BASH script.
In the latest run of the workflow (Fig. 3), 38,758 new
ATC codes were added for drug products which did
not have an ATC code in the source registry. Then,
91,782,500 schema:relatedDrug triples were added
between similar drug products, i.e. 45,891,250 pairs of
drugs from our dataset were identified to have the same
function, but exist under different brand names, are from
different countries, or are produced by different manufac-
turers, or maybe have a different packaging size, strength,
cost, etc. As we will see further in the text, we can utilize
these interlinkings for providing the users with alternative
drugs they may aquire for treating their condition, either
in the same of in a different country.
The workflow shown in Fig. 3 is activated on a sched-
uled period, currently set at one month. In order to handle
the data changes during updates, we backup the RDF
graph holding our dataset and replace it with the newly
created RDF graph. With this we employ versioning and
maintain the default graph identifier to always denote the
latest version of the LinkedDrugs dataset.
According to the recommendations in Step IV of
our guidelines, the dataset needs to be published on
the Web, where it will be publicly available. There-
fore, we published our LinkedDrugs dataset accord-
ing to the best practices for publishing Linked Data
[50], via a permanent, dereferenceable URI, which sup-
ports HTTP content negotiation: http://linkeddata.finki.
ukim.mk/lod/data/drugs# [65]. The dataset is hosted
at a live Virtuoso instance [61], in a named RDF
graph <http://linkeddata.finki.ukim.mk/lod/data/drugs#>
which holds the latest version of the dataset, publicly
available via a SPARQL endpoint [62] which serves as
a REST-based service. Additionally, data dumps of the
dataset are available on Datahub [55].
Usage scenarios
With the automated system and its workflow we start
with twenty-three different, distributed and browsable
datasets, available on the Web and intended for human
consumption via HTMLwebpages, and using themethod-
ological guidelines and tools we manage to create a con-
solidated dataset of interlinked and schema-aligned drug
products from different countries, additionally linked to
generic drugs and active ingredients from the LODCloud.
In order to demonstrate the advantages of having the drug
data in a 5-star data quality format, and therefore imple-
ment the recommendations from Step V of the method-
ological guidelines, we will show a few use-case scenarios
via SPARQL queries. The most basic scenario would be to
select all data about a single drug of interest, which is very
simple and straight-forward, and therefore omitted here.
Since the Virtuoso SPARQL endpoint at [62] can be used
as a REST-based service, these SPARQL queries could be
used from any type of application to always access and
exploit the latest data available.
Interlinked drug data. The interlinking cre-
ated between the drugs from the dataset, with the
schema:relatedDrug triples, can be utilized in a
use-case which allows the end-users to discover drugs
from the same country or another country which have
the same therapeutic, pharmacological and chemical
properties as the drug of his/her interest. This is a useful
feature when the drug of interest is not available or when
the user is traveling abroad. Getting information about
drugs with the same properties and their respective prices
can be useful for determining the drug from a specific
category which is affordable in the specific case. This can
be used by pharmacists, doctors and even patients for
gathering information and determining the appropriate
treatment.
An example SPARQL query which can be used to iden-
tify the drugs from all countries which have the same
therapeutic, pharmacological and chemical properties as
the drug of interest, is shown below (Query 1).
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 9 of 24
Query 1
1 prefix schema: <http://schema.org/>
2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>
3
4 SELECT ?drug ?name ?gtin ?strengthValue ?strengthUnit ?costPerUnit
5 ?costCurrency ?manufacturerName ?prescriptionStatus ?country
6 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>
7 WHERE {
8 <http://www.legemiddelverket.no//Legemiddelsoek/Sider/Legemiddelvisning.
9 aspx?pakningId=c5a02a30-d471-4ba0-8197-38955f384dd8>
10 schema:relatedDrug ?drug .
11 OPTIONAL { ?drug schema:name ?name }
12 OPTIONAL { ?drug schema:gtin13 ?gtin }
13 OPTIONAL { ?drug schema:prescriptionStatus ?prescriptionStatus }
14 OPTIONAL { ?drug schema:addressCountry ?country }
15 OPTIONAL {
16 ?drug schema:cost ?costEntity .
17 ?costEntity schema:costPerUnit ?costPerUnit ;
18 schema:costCurrency ?costCurrency .
19 }
20 OPTIONAL {
21 ?drug schema:availableStrength ?strengthEntity .
22 ?strengthEntity schema:strengthValue ?strengthValue ;
23 schema:strengthUnit ?strengthUnit .
24 }
25 OPTIONAL {
26 ?drug schema:manufacturer ?manufacturerEntity .
27 ?manufacturerEntity schema:legalName ?manufacturerName .
28 }
29 }
30 ORDER BY ?name
In the query, only a handful of data of interest of the
related drugs are selected, but depending on the specific
use-case, they can be expanded. Query 1 can also be mod-
ified to include the specific drug of interest, by modifying
the drug URI in line 8. In our example in Query 1, we
use the Norwegan drug Airomir as an example, and get
results for over 300 distinct and related drugs from many
different countries in the dataset. The query and its full
results can be viewed online on Seminant [66], at http://
seminant.com/queries/5803e77573656d19eb6c5d00. Par-
tial results are shown in Table 1.
Linked LOD drug data. The main advantage of hav-
ing links between data from different and distributed
datasets is the ability to query them from a sin-
gle point, over the existing infrastructure of the Web,
using W3C standards such as HTTP, SPARQL and
RDF. As we have rdfs:seeAlso links from drugs
in our dataset to corresponding generic drugs from
the DrugBank and DBpedia datasets, we can utilize
them to get additional information about the drugs
from our dataset whenever we are browsing them.
Such additional information will come from the Drug-
Bank and DBpedia datasets, and can include additional
drug description, the interactions the drug has with
other drugs or with certain foods, the drug mech-
anism of action, the drug pharmacology, absorption,
biotransformation and toxicity, the list of alternative
brand names and a list of webpages for the drug on other
locations on the Web. This data is not available on the
original national drug registry websites, which are the
source for our dataset; it is data retrieved directly from
the distributed DrugBank and DBpedia dataset, using
SPARQL federation [67].
An example of a federated SPARQL query which selects
information about a drug of interest from the DrugBank
and DBpedia datasets is shown below (Query 2).
Table 1 Partial results from Query 1
Drug product Manufacturer Country
Activent Sr Medical Union Pharmaceuticals -
Egypt
EG
Aerolin 100mcg/dose
Inhaler
EG
Aeroline 400 Inhaler EG
Aerotropa Pharco B International-egpyt EG
Agolin Agog Pharma Ltd UG
Airomir iNova Pharmaceuticals
(New Zealand)
NZ
Airomir Autohaler Teva Sweden AB NO
Airomir Autohaler iNova Pharmaceuticals
(New Zealand)
NZ
Airomir Autohaler 100
microgramos
Teva Pharma S.L.U. ES
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 10 of 24
Query 2
1 prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>
2 prefix schema: <http://schema.org/>
3 prefix drugbank: <http://wifo5-04.informatik.uni-mannheim.de/drugbank/
4 resource/drugbank/>
5 prefix dbo: <http://dbpedia.org/ontology/>
6 prefix dbp: <http://dbpedia.org/property/>
7
8 SELECT ?loddrug ?genericName
9 (group_concat(distinct ?brandName; separator = ", ") AS ?brandNames)
10 ?comment ?description ?biotransformation ?affectedOrganism ?absorption
11 ?chemicalFormula ?toxicity
12 (group_concat(distinct ?foodInteraction; separator = " ") AS ?foodInteractions)
13 (group_concat(distinct concat(?interactingDrugLabel, : , ?interactionStatus);
14 separator = ". ") AS ?drugInteractions)
15 (group_concat(distinct ?url; separator = ", ") AS ?urls)
16 WHERE {
17 GRAPH <http://linkeddata.finki.ukim.mk/lod/data/drugs#> {
18 <http://www.aemps.gob.es/cima/especialidad.do?metodo=
19 verPresentaciones&codigo=79539>
20 rdfs:seeAlso ?loddrug .
21 }
22 SERVICE <http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql> {
23 OPTIONAL { ?loddrug drugbank:description ?desc . }
24 OPTIONAL { ?loddrug drugbank:genericName ?gname . }
25 OPTIONAL { ?loddrug drugbank:brandName ?bname . }
26 OPTIONAL { ?loddrug drugbank:biotransformation ?biotransformation . }
27 OPTIONAL { ?loddrug drugbank:affectedOrganism ?affectedOrganism . }
28 OPTIONAL { ?loddrug drugbank:absorption ?absorption . }
29 OPTIONAL { ?loddrug drugbank:chemicalFormula ?chemicalFormula . }
30 OPTIONAL { ?loddrug drugbank:foodInteraction ?foodInteraction . }
31 OPTIONAL { ?loddrug foaf:page ?page . }
32 OPTIONAL { ?loddrug drugbank:toxicity ?toxicity . }
33 OPTIONAL {
34 ?drugInteractionEntity drugbank:interactionDrug1 ?loddrug ;
35 drugbank:interactionDrug2 ?interactingDrug ;
36 drugbank:text ?interactionStatus .
37 ?interactingDrug rdfs:label ?interactingDrugLabel .
38 }
39 }
40 SERVICE <http://dbpedia.org/sparql> {
41 OPTIONAL {
42 ?loddrug dbo:abstract ?abstract .
43 FILTER (langMatches(lang(?abstract), "en"))
44 }
45 OPTIONAL {
46 ?loddrug rdfs:label ?label .
47 FILTER (langMatches(lang(?label), "en"))
48 }
49 OPTIONAL {
50 ?loddrug rdfs:comment ?comment .
51 FILTER (langMatches(lang(?comment), "en"))
52 }
53 OPTIONAL { ?loddrug dbp:tradename ?tradename . }
54 OPTIONAL { ?loddrug dbo:wikiPageExternalLink ?externalLink . }
55 }
56 BIND(IF(bound(?abstract), ?abstract, ?desc) as ?description)
57 BIND(IF(bound(?bname), ?bname, ?tradename) as ?brandName)
58 BIND(IF(bound(?gname), ?gname, ?label) as ?genericName)
59 BIND(IF(bound(?page), ?page, ?externalLink) as ?url)
60 }
The query selects some very important data about the
drug of interest and its active ingredient from Drug-
Bank and DBpedia. The most significant are the chemical,
biological and pharmacological properties of the drug,
along with its interactions with food and with other drugs.
This data is not always available on the national drug data
registries, but is of high importance for the end-users,
especially the pharmacists and doctors who may require
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 11 of 24
Table 2 Partial results from Query 2
Description (from DBpedia)
Duloxetine (Cymbalta, and generics) is a serotonin-norepinephrine reuptake inhibitor (SNRI) created by Eli Lilly. It is mostly prescribed for major depres-
sive disorder, generalized anxiety disorder, fibromyalgia and neuropathic pain. Duloxetine failed to receive US approval for stress urinary incontinence
amid concerns over liver toxicity and suicidal events; however, it was approved for this indication in the UK, where it is recommended as an add-on
medication in stress urinary incontinence instead of surgery.
Food Interactions
Food does not affect maximum levels reached, but delays it (from 6 to 10 hours) and total product exposure appears to be reduced by only 10 percent.
People taking this product who drink large amounts of alcohol are exposed to a higher risk of liver toxicity. Take without regard to meals.
Drug Interactions
Amitriptyline: Possible increase in the levels of this agent when used with duloxetine.
Ciprofloxacin: Ciprofloxacin increases the effect/toxicity of duloxetine.
Desipramine: Possible increase in the levels of this agent when used with duloxetine.
Flecainide: Possible increase in the levels of this agent when used with duloxetine.
Fluvoxamine: Fluvoxamine increases the effect and toxicity of duloxetine.
Imipramine: Possible increase in the levels of this agent when used with duloxetine.
Isocarboxazid: Possible severe adverse reaction with this combination.
Nortriptyline: Possible increase in the levels of this agent when used with duloxetine.
Phenelzine: Possible severe adverse reaction with this combination.
Propafenone: Possible increase in the levels of this agent when used with duloxetine.
Rasagiline: Possible severe adverse reaction with this combination.
Thioridazine: Increased risk of cardiotoxicity and arrhythmias.
Tranylcypromine: Possible severe adverse reaction with this combination
them when determining treatment for acute conditions
of a patient who is already on a treatment of a chronic
medical condition.
An example run of Query 2, for the drug prod-
uct Duloxetina from Spain, results in details for
the generic drug Duloxetine from both Drug-
Bank and DBpedia: http://seminant.com/queries/
5803e9b973656d19eba65e00. Among other details, it also
shows the 3 specific food - drug interactions the drug
is involved in, along with the 13 drug - drug interac-
tions it has. Partial results from the query are shown in
Table 2.
Analytics. Aside from the use-case scenarios for end-
users, our LinkedDrugs dataset can be used for analytical
queries as well. These analytical queries allow interested
parties to gain insight into the drug markets of different
countries, allowing them to analyze the available consol-
idated data using a single entry point for querying and
using a single query language. The analytics could be built-
in in specific analytic applications, or can be executed with
separate and standalone SPARQL queries.
To get a better understanding of the analytical possibil-
ities over consolidated drug data from multiple countries,
we will look at an example query which identifies the most
common drug categories per country. This would allow
the user, e.g. pharmaceutical company, to gain a better
knowledge on the national drug markets and make an
informed decision about placing their drug in the country
of interest. A general SPARQL query for this analytical
scenario is given below (Query 3):
Query 3
1 prefix schema: <http://schema.org/>
2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>
3
4 SELECT count (distinct ?drug) as ?count ?atc ?country
5 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>
6 WHERE {
7 ?drug a schema:Drug ;
8 schema:addressCountry ?country ;
9 drugbank:atcCode ?atcCode .
10 FILTER (strlen(?atcCode) > 3)
11 BIND(SUBSTR(xsd:string(?atcCode), 1, 3) AS ?atc)
12 }
13 GROUP BY ?country ?atc
14 ORDER BY DESC (?count)
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 12 of 24
Table 3 Partial results from Query 3
Drugs ATC Prefix Country
5362 C09 RO
2152 C09 ES
1536 J01 RU
1488 C09 NL
1270 N05 US
976 J01 ZA
758 C09 IE
709 C09 SK
707 N02 NZ
A sample run of Query 3 shows that Romania, Spain,
Netherlands, Ireland and Slovakia have most drugs (5,362,
2,152, 1,488, 758 and 709, respectively) in the category
of agents acting on the renin-angiotensin system (ATC
C09), Russia and South African Republic have most drugs
(1,536 and 976, respectively) in the category of antibac-
terials for systemic use (ATC J01), while USA has 1,270
drugs in the psycholeptics category (ATC N05). These
partial results are shown in Table 3. The full results from
the query are available at http://seminant.com/queries/
5803ebc473656d19ebac5e00.
Another analytical scenario would be to assess the aver-
age drug price per drug category, per country. It could be
used by medical authorities in a country to determine the
cost situation per category in other countries and use the
information for local regulations. It could also be used by a
pharmaceutical company to determine the price range for
a new drug, before it goes to market. An example SPARQL
query which can be used for such an analysis is given
below (Query 4):
A sample run of Query 4 identifies that the ATC drug
categories with the highest average price in Norway are
drugs for disorders of the musculo-skeletal system (ATC
M09), respiratory system products (ATC R07) and ali-
mentary tract and metabolism products (ATC A16). In
Macedonia they are alimentary tract and metabolism
products (ATC A16), pituitary and hypothalamic hor-
mones and analogues (ATC H01) and antihemorrhagics
(ATC B02), while in Australia and Slovenia they are respi-
ratory system products (ATC R07) and in South African
Republic they are immunosuppressants (ATC L04), hema-
tological agents (ATC B06) and alimentary tract and
metabolism products (ATC A16). These partial results are
shown in Table 4, while the full results which include other
countries as well, are available at http://seminant.com/
queries/5803ed6e73656d19eb537e00.
In cases when an inter-country comparison of the pric-
ing is necessary, an application could use a currency
converter to transform the values to the same currency of
choice, and make the comparison.
Discussion
The generated LinkedDrugs dataset aims to be the first
comprehensive, consolidated and aligned dataset of drug
product data on a global scale. In its first version, the
dataset consisted of only seven countries; currently, that
number has grown to twenty-three. As we have the
methodology and the tools in place, the addition of new
countries is more or less straightforward. Even though
Linked Data datasets with drug data already exist in the
LOD Cloud, they consist solely of generic drugs, i.e. active
ingredients. Contrary to this, our LinkedDrugs dataset
consists of drug products which are registered in a specific
Query 4
1 prefix schema: <http://schema.org/>
2 prefix drugbank: <http://www4.wiwiss.fu-berlin.de/drugbank/resource/drugbank/>
3
4 SELECT (?totalCost / ?drugCount as ?averageCost) ?costCurrency ?atc
5 ?drugCount ?country
6 WHERE {
7 SELECT count (distinct ?drug) as ?drugCount
8 sum (xsd:float(?cost)) as ?totalCost
9 ?costCurrency ?atc ?country
10 FROM <http://linkeddata.finki.ukim.mk/lod/data/drugs#>
11 WHERE {
12 ?drug a schema:Drug ;
13 schema:addressCountry ?country ;
14 drugbank:atcCode ?atcCode ;
15 schema:cost ?costEntity .
16 ?costEntity schema:costPerUnit ?costPerUnit ;
17 schema:costCurrency ?costCurrency .
18 FILTER (strlen(?atcCode) > 3)
19 BIND(SUBSTR(xsd:string(?atcCode), 1, 3) AS ?atc)
20 FILTER (?costPerUnit != "0"^^xsd:double)
21 BIND(REPLACE(?costPerUnit, ",", ".") AS ?cost)
22 }
23 }
24 GROUP BY ?country ?atc ?costCurrency
25 ORDER BY DESC (?averageCost)
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 13 of 24
Table 4 Partial results from Query 4
Avg. price Currency ATC Prefix Country
93480.60 NOK M09 NO
47221.40 NOK R07 NO
39557.30 NOK A16 NO
32021.40 MKD A16 MK
28837.20 MKD H01 MK
27478.20 MKD B02 MK
22500.00 AUD R07 AU
17822.00 SVN R07 SI
13360.10 EUR V10 CY
10679.50 ZAR LO4 ZA
10127.10 ZAR B06 ZA
9880.81 ZAR A16 ZA
country, have a specific name, dosage form, amount,
strength, barcode, manufacturer, license, price, ATC code,
etc. Since these drug products from our dataset are linked
to the existing generic drugs and active ingredients from
the LOD Cloud, the novel usage scenarios greatly exceed
what is currently possible with Linked Drug Data on
the Web.
The developed methodological guidelines and support-
ing tools intend to encourage and lower the boundaries
for data publishers from the domain to contribute to
the LinkedDrugs dataset, further extending its potential
and value. Since the adoption of the developed tools can
present a potential hurdle, we believe that the transfor-
mation process presented in this paper can serve as an
additional guide.
Our methodological guidelines extend the existing
Linked Data methodologies, briefly presented in the
section Background . Here, we will make an explicit
comparison of the similarities and differences in the
approaches among the existing methodologies, and our
findings and proposal to group them into five gen-
eral steps. These five general steps have allowed us to
define our own methodological guidelines for the drug
data domain, by defining tasks and tools specific for the
domain and adding them to the corresponding general
steps. These specific tasks and tools have been the result
of our research in the domain, as well as our previous work
with generating, publishing and using healthcare Linked
Data datasets.
The existing Linked Data methodologies have a vary-
ing number of steps, but still generally cover the same
activities. The main difference in the methodologies is the
grouping of actions within different steps and on different
levels of granularity. Apart from some explicit differences,
which we will further examine, they cover the palette of
actions involved in the process of generating and publish-
ing a Linked Data dataset, and thus can be grouped into
five general steps.
From Table 5, we can see that all existing Linked
Data methodologies define the knowledge of the data, its
domain and the existing datasets from the LOD Cloud as
the first step(s). The actions they cover in the steps relate
to these tasks, and while somemethodologies focus on the
data domain, others recommend knowledge of existing
Linked Data datasets, as well.
The second step refers to the tasks of modeling the data,
locating and selecting the appropriate ontology or vocab-
ulary, extending existing ontologies and vocabularies to
Table 5 Aligning our methodological guidelines with existing Linked Data methodologies. Additionally, our approach focuses on
lifecycle reuse by step modularity
Our Methodology Hyland et al. Hausenblas et al. Villazón-Terrazas et al. LOD2
I. Domain and Data Knowledge 1. Identify 1. Data Awareness 1. Specify 1. Extraction
2. Storage
II. Data Modeling and Alignment 2. Model 2. Modeling 2. Model
3. Name
III. Transformation into 5-star Linked Data 4. Describe 3. Publishing 3. Generate 3. Authoring
5. Convert 4. Discovery 4. Interlinking
5. Integration 5. Classification
6. Quality
7. Evolution/Repair
IV. Publishing the dataset on the Web 6. Publish 4. Publish
V. Use-cases, Apps and Services 6. Use cases 5. Exploit 8. Search/
Browsing/
Exploration
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 14 of 24
match the original data schema, creating a custom ontol-
ogy or vocabulary from scratch and mapping it to existing
ontologies, and defining the URI naming scheme for the
ontology classes and properties. The methodologies of
Hyland et al. [51], Hausenblas et al. [52] and Villazón-
Terrazas et al. [53] explicitly define these tasks, whereas
the LOD2 methodology does not define them. For this
step, we defined an RDF schema which can be reused
when working with drug data from national drug reg-
istries or other sources. The provided CSV template
allows the data publisher to align the data in the cor-
rect format in a CSV file, and prepare it for the next
step.
The third step is focused on transforming the source
data into RDF, creating links and 5-star Linked Data
datasets, and creatingmetadata descriptions of the dataset
and its links to other datasets. All four methodologies
define these tasks, with the LOD2 methodology being the
most specific one. The LOD2 methodology, being the lat-
est one, understandably contains activities which include
classification, quality control, data evolution and version-
ing. The use of the VoID vocabulary is explicitly stated
in this phase in the methodologies of Hyland et al. and
Hausenblas et al. The methodology of Villazón-Terrazas
et al. defines this task in its next step, 4. Publish, but
as it contains other tasks which better fit in our next
step, we left it out of this one. Here, we developed an
OpenRefine transformation script which can be used with
any source drug data formatted with the CSV template
from the previous step, in order to get high quality, 5-star
Linked Drug Data. For the purpose of generating addi-
tional links between similar drugs in the dataset itself, we
developed a SPARQL-based tool which can be used over
any Linked Drug Dataset generated with the OpenRefine
transformation script.
The fourth step defines the tasks for publishing the
dataset on the Web. The exact actions which should be
undertaken are defined in the methodologies of Hyland
et al. and Villazón-Terrazas et al. The other two method-
ologies do not define such tasks and steps. In order to
simplify this step for data publishers, we provide a web-
based tool for automated publishing of the generated
Linked Drug Data dataset. The tool also interlinks the new
dataset with the consolidated LinkedDrugs dataset, which
already contains drug product data from twenty-three
national drug repositories.
The fifth step consists of tasks for defining use-case
scenarios or development of specific applications and ser-
vices which take advantage of the newly created Linked
Data dataset and its links to other datasets. Themethodol-
ogy of Hyland et al. does not specify such an activity, argu-
ing that the data publisher does not have to think about
potential use and reuse of the data while working on gen-
erating Linked Data from the source dataset. According to
them, the data publisher should just make sure he/she cor-
rectly transforms, describes and publishes the data, and let
the users and the interested community know about the
dataset. However, Hausenblas et al., Villazón-Terrazas et
al. and the LOD2 Project team explicitly state that the last
step of the Linked Data generation process should consist
of defining use-cases and/or developing applications and
services.
The tools we developed as support for the steps of
the methodology aim to help data publishers from the
domain. They can aid both data publishers from the drug
domain which do not necessarily have prolific experience
with Linked Data, as well as Linked Data publishers which
are not very familiar with the domain of drugs and health-
care. These open-source and reusable tools are supposed
to lower the bounds for interested parties to get involved
in the domain and include their datasets in the global
LODD and LOD Cloud.
Conclusions
The amounts of data available on the Web represent
a goldmine for data-driven applications and services
[68]. Unfortunately, the data is available in different for-
mats and is distributed over various locations. This is
a huge obstacle which blocks progress in data retrieval
in many domains. The healthcare domain is no differ-
ent: the national drug authorities publish their data on
the Web in different formats and granularity levels, and
there is no comprehensive method for retrieving and
using them.
The Linked Data concept provides new ways of pub-
lishing and connecting data from different distributed
sources, which allows data consolidation. It also pro-
vides a new spectrum of use-case scenarios which can
be useful for generating new business value for busi-
nesses and independent developers, by allowing them
to develop innovative applications and services in the
domain. The opportunities which lie within the creation
of new use-case scenarios from Open Data and Linked
Data are a field whose potential is becoming increasingly
recognized [69].
Motivated by this, we propose methodological guide-
lines for using Linked Data principles to consolidate
drug data, and provide a set of tools which intend to
aid the data publishers in the steps of the methodol-
ogy. Our methodological guidelines extend the existing
Linked Data methodologies, both general and aimed for
government data. We combine the steps from the exist-
ing methodologies into five steps which are necessary for
developing a sustainable Linked Data dataset. Each of
these steps is extended to include specific tasks, actions
and tools which are important for the drug data domain.
The aim of our methodological guidelines is to enable the
creation of Linked Drug Data datasets which are very well
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 15 of 24
aligned between themselves and which can then easily be
consolidated into a comprehensive system or dataspace.
We implemented our guidelines and tools as part of
an automated system for transformation and publish-
ing of the 2-star and 3-star data from twenty-three
national drug registries into consolidated and aligned
5-star Linked Drug Data. The system provides reg-
ular updates which make sure the created Linked-
Drugs dataset always contains new data. In order to
create the dataset, we use the defined common RDF
schema, the CSV template, the OpenRefine transforma-
tion script, as well as the SPARQL queries for extending
the dataset and interlinking the drugs from the dataset.
With this, we ensure data alignment with other LOD
and LODD datasets. Currently, our generated Linked-
Drugs dataset contains over 99,000,000 RDF triples, which
represent data for over 248,000 drug products from
twenty-three countries. The dataset also contains over
91,000,000 schema:relatedDrug relations between
the drugs, and over 278,000 rdfs:seeAlso relations to
generic drugs and active ingredients from DrugBank and
DBpedia.
We further present new usage scenarios enabled by
Linked Data, aimed both for end-users and analytical pur-
poses, which utilize the consolidated and aligned nature
of the dataset and its contextual links to entities from the
LOD Cloud. We achieve this by automatically generat-
ing the additional relations in our dataset which link the
drugs between themselves, and link the drugs with drug
entities published as part of other LOD Cloud datasets.
Using W3C standards and the Semantic Web technolo-
gies, we are then able to retrive data from these distributed
datasets, and present them to the end-users in a compre-
hensive manner.
As future work, we plan to extend the number of
national drug registries from the current twenty-three.
The design of the supporting system requires that we only
modify the data gathering and staging part of the work-
flow, while the transformation and publishing process
remains the same. We also intend to extend the reconcil-
iation tasks in OpenRefine, by employing reconciliation
for the company names. This would provide additional
links in the dataset between the drugs and the manu-
facturers, represented by company entities on the LOD
Cloud. Given that the names of the companies and their
details are sometimes on local languages, depending on
the drug registry, we would also add a translation ser-
vice in the workflow. We also plan to add an automated
VoID metadata generation task for the dataset, which
is a recommendation from Step III. Regarding applica-
tions built on top of the datasets, we have already started
developing user-focused and analytical applications which
provide the end-users with detailed information about a
drug product and its interaction with other drugs and
foods, but also provide insight into the drug product avail-
ability on a global scale. All of these additional features
would further add to the benefit of having consolidated
and aligned drug data from various countries in one place,
accessible via the existing infrastructure of the Web and
via existing W3C standards.
Methods
Themethodological guidelines
Based on our experience with applying the Linked Data
principles in the healthcare and drug domain and on
the existing Linked Data methodologies, we developed a
set of methodological guidelines for consolidating drug
data using the Linked Data approach. These guidelines
improve upon the existing Linked Data methodologies
and contain steps, activities and tools which are specific
to the drug data domain. Their purpose is to guide data
publishers through the process of generating high qual-
ity, 5-star Linked Data in order to interlink, align and
consolidate drug data from different national drug reg-
istries or other sources of drug data. The alignment and
relationship between the existing methodologies and our
guidelines is outlined in Table 5.
Along with the guidelines, we have developed a set
of tools which simplify the execution of the specific
steps of the methodology. Their intention is to support
the Linked Drug Data generation process for both peo-
ple from the drug domain which do not have deeper
knowledge of Linked Data, and Linked Data publish-
ers which do not have deeper knowledge of the drug
domain. These tools are open and publicly available on
GitHub [58].
Our methodological guidelines consist of the following
steps (Fig. 1):
I. Domain and Data Knowledge
II. Data Modeling and Alignment
III. Transformation into 5-star Linked Data
IV. Publishing the Linked Data dataset on the Web
V. Use-cases, Applications and Services
These steps have been developed with reuse as a pri-
mary goal; therefore, their main focus is the encourage-
ment of data publishers in the drug domain to develop,
modify and use reusable components during the steps
of the methodology. This makes the Linked Drug Data
lifecycle modular, i.e. constructed of loosely-coupled
components which can be reused in the domain. These
loosely-coupled components can be used separately when
necessary, but also form a seamless workflow for generat-
ing a high-quality, 5-star Linked Drug Data dataset. The
reuse of such components, like in other software devel-
opment cases, reduces development time and increases
productivity [56, 57].
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 16 of 24
Step I: Domain and data knowledge
The first step corresponds to the first steps from the exist-
ing methodologies: it is important for the data publisher
to know the domain and the data in it very well. This
understanding of the data source schema and semantics
is crucial for the following steps which will involve data
modeling, schema alignment and data transformation.
In the drug data domain, if this is the first time the data
publisher comes across Linked Data, our advice is to first
get familiar with the 5-star data system fromTim Berners-
Lee [35], the four principles of Linked Data [70], and the
LOD Cloud [32]. After that, a study of the LODD Project
[36], the Bio2RDF Project [43] and the DrugBank Linked
Data dataset [44] is recommended. This will help the data
publisher to get a better insight into the Linked Drug Data
and Linked Healthcare Data domains, the types of data
which exist in them, their schema, their similarities and
differences and their existing and potential links. It will
also help him/her determine the ontologies and vocabu-
laries already used in the domain, which can be important
for the next step.
In a general case, when working with any other domain,
it is important for the data publisher to get familiar with
the domain in question and with the meaning of the
dataset selected for transformation. For this, a consult
with a domain expert is usually necessary and there-
fore advised. Another approach is to explore the existing
Linked Data datasets which are similar to or from the
same domain as the one of interest. For this, the Datahub
portal [71] and the LOD Cloud cache instance [72] could
be used.
Step II: Datamodeling and alignment
In the next step, the data publisher should focus on data
modeling and alignment with other existing or future
datasets. The data publisher has to choose the correct
schema for the dataset, in order to annotate it correctly,
i.e. use the data fields which are necessary for the final
use-cases, annotate the fields unambiguously and with the
correct semantics and make the correct schema choices
which will allow seamless alignment with other datasets.
Additionally, the data publisher has to define the URI
naming scheme for the data entities, and optionally for the
ontology or vocabulary classes and properties.
Data schema. In the drug data domain, studying the
datasets from the LODD and Bio2RDF projects can help
get an insight into the ontologies and vocabularies used
in the domain. Some of the ontologies and vocabular-
ies which a data publisher needs to have in mind are:
Schema.org [59], DBpedia Ontology [73], UMBEL [74],
DICOM [75], the DrugBank Ontology used for the data at
[44], as well as other biomedical ontologies [76].
In order to support the data publishers from the
drug domain in this step, we designed a reusable RDF
schema for the data, shown in Fig. 2. The schema can
be used by data publishers working with drug data
from national drug registries or other sources. The
schema is comprised of classes and properties from
the Schema.org vocabulary [59]: the schema:Drug
class, along with a large set of properties which
instances of the class can have, such as name,
code, activeIngredient, nonProprietaryName,
availableStrength, cost, manufacturer,
relatedDrug, description, url, etc. Addition-
ally, in order to align the drug data with generic drugs
from DrugBank, we use the properties brandName,
genericName, atcCode and dosageForm from the
DrugBank Ontology. In order to annotate the links which
the drug product entities will have to generic drug enti-
ties from the LOD Cloud dataset, the rdfs:seeAlso
relation is used.
In general, when working with data from another
domain, the data schema is defined with the choice of
vocabularies or ontologies to be used. The principles of
ontology engineering and usage have been developed for
this purpose exactly: to maximise the chances of reuse,
and therefore allow better alignment between datasets
[77]. This means the agent should always try to reuse an
existing vocabulary or ontology, giving advantage to those
which are most widely used. A few tools for ontology and
vocabulary discovery exist, and the data publisher should
use them in this stage. The two most notable are Linked
Open Vocabularies (LOV) [78] and DERI Vocabularies
[79], which also provide usage statistics which can be used
to assess the impact of a given vocabulary or ontology in
a specific domain. Our choice of the Schema.org vocabu-
lary follows the reusability paradigm: it is the most widely
and generally used vocabulary across the Web.
However, datasets tend to have specific fields, which are
not covered by existing ontologies. In this cases, the exist-
ing ontology or vocabulary should be extended, or a new
one should be defined. However, each time a new ontol-
ogy is developed, it is important to define the mappings
between the new classes and properties and the classes
and properties from other ontologies, in order to enable
ontology matching and RDF-based reasoning, for schema
alignment. In order to avoid defining specific new prop-
erties, we reused some properties from the Schema.org
vocabulary which are currently not explicitly intended for
use with schema:Drug entities. An example of such
properties is the schema:addressCountry property
which should be used for an address, but we use it in
our schema to denote the country in which the drug is
registered.
Another important approach in this step is the use of
upper-level ontologies and vocabularies; they can pro-
vide a schema for many different and specific domains,
due to their generality. Having two or more datasets
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 17 of 24
annotated with the same upper-level ontology or vocab-
ulary allows interlinking and inference between them,
i.e. it improves the alignment which is crucial for data
consolidation.
Data alignment. For the data alignment task, the data
publisher needs to make sure that the generated dataset
will be well aligned with existing and future datasets from
the same domain. In order to aid the data publishers with
this, as well as help them in preparing the drug data for
the transformation step, we developed a CSV template
intended for the drug domain [58]. This CSV template can
be used with the drug data and is comprised of the fields
necessary for applying the RDF schema from Fig. 2.
The data publisher interested in publishing Linked Drug
Data should use this CSV template for the data, following
the specifics defined for each of the fields. These specifics
assure that the data will be of high quality and completely
aligned with other drug data generated using the same
methodological guidelines.
URI formats. From the URI naming scheme perspec-
tive, in the domain of drug data it is important to deter-
mine the types of entities which exist in the dataset. This
will help in defining the entity URIs for the Linked Data
dataset. According to the Linked Data principles, each
entity in the dataset - along with the classes and proper-
ties in the ontology - needs to have a unique indentifier in
the form of an HTTP URI. In order to provide better per-
formance when using the dataset in the future, our experi-
ence suggests using separate URL paths for different entity
types, e.g. http://example.com/drug/, http://
example.com/interaction/, http://example.
com/disease/, etc. An additional recommendation is
to use slash-based URIs, instead of hash-based ones. This
may result in using an additional HTTP request by the
machine accessing the URI, but it provides better perfor-
mance when accessing large datasets [80].
However, to simplify this step for the drug data publish-
ers, we advise the use of the existing webpage URLs of
the drugs from the national registry websites, which are
already unique. According to the Linked Data principles,
the entity URI should denote a Web location where the
end-users and agents can get more information about the
entity, so our approach satisfies the condition.
Step III: Transformation into 5-star Linked Data
During the third step, the source dataset should be trans-
formed into a 5-star Linked Data dataset. The process of
transformation can be executed in many different ways,
and with various software tools, e.g. OpenRefine [81],
LODRefine [82], D2R Server [83], Virtuoso [84], Silk
Framework [85], etc.
To help the data publishers from the drug domain and
to automate this step, we developed a reusable OpenRe-
fine transformation script [58]. This transformation script
is specifically designed for the drug data domain, and the
RDF schema and CSV template from Step II. It contains a
set of actions which generate RDF from the inputed CSV
file which contains drug data. In the process, it also locates
associated generic drugs from the DrugBank and DBpe-
dia datasets for each drug product in source dataset, and
extends the generated RDF with links between the drugs
from the dataset and the corresponding drugs from the
LOD Cloud.
The transformation script can be reused with anyOpen-
Refine instance which has the RDF extension. It can be
applied on any drug data dataset formatted with the CSV
template from Step II. As a result, it will generate a Linked
Drug Data dataset annotated with the RDF schema from
Step II (Fig. 2).
The RDF schema from Step II defines relations between
the drug products from the dataset as well. These relations
are denoted with the schema:relatedDrug relation
(Fig. 2). In order to provide means for generating RDF
triples which interconnect the drugs from the dataset, we
developed a SPARQL query [58] which can be executed
over the dataset generated with the OpenRefine trans-
formation script. The SPARQL query detects all pairs of
drugs from the dataset which have the same ATC code
- and therefore have the same therapeutic, pharmaco-
logical and chemical properties - and creates two triples
connecting the first drug to the second one with the
schema:relatedDrug property, and vice-versa.
In a general case, in order to make the correct choices
about the tools to be used for the transformation pro-
cess, it is important to distinguish the characteristics of
the dataset first. The nature of the dataset will determine
if (a) the transformation is a one-time task, a task which
will have to be executed on a given time interval (e.g. once
a month), or a continually running task; (b) old versions
of the transformed dataset are necessary for versioning
and as backup, if during future transformations only the
changes in the data are needed for transformation, i.e.
delta updates are performed, or if older data are no longer
necessary for the particular use-case; (c) manual or auto-
mated data cleansing is needed before the first transfor-
mation and/or subsequent transformations; (d) the source
dataset is always available at the same location and is
accessible via the same interfaces. These specifics of the
dataset in question can then help the data publisher deter-
mine if the transformation task can be fully or partially
automated, and identify the parts of the transformation
workflow which require human attention and input.
Adding metadata about the newly created Linked Data
dataset is significant from the data reuse perspective -
using vocabularies such as VoID [86] help ubiquitously
determine the characteristics of the dataset and the links
the dataset has to other Linked Data datasets, through
software agents. VoID metadata contains information
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 18 of 24
about the name, description and category of the dataset,
versioning information and update frequency, contact
information, the license under which the dataset is made
available, the links to the SPARQL endpoints and URI
lookup endpoints, used vocabularies and their properties
and classes. It also explicitly defines the links between the
dataset and other Linked Data sets, defined in the dataset
itself. The use of the VoID vocabulary is explicitly stated in
the corresponding steps in the methodologies of Hyland
et al., Hausenblas et al. and Villazón-Terrazas et al.
In the domain of drug data providing new and updated
data is very important - old data has no importance
to the end-user, except for analytics. This means that
the data publisher should anticipate the change rate of
the source dataset and correctly design the workflow of
refreshing data from the source dataset to the Linked Data
dataset. This would translate to creating a sustainabil-
ity plan which will transform new data and add it to the
LinkedData dataset, remove old data and provide version-
ing. Depending on the size of the source dataset, the data
publisher can choose to re-transform the source dataset
on each update, or to provide means for performing delta
updates. Providing versioning is also important, as new
transformations can sometimes result in errors, rendering
the dataset unusable.
Step IV: Publishing the Linked Data dataset on theWeb
In the forth step, the generated 5-star Linked Data dataset,
along with its VoID metadata, should be published on the
Web. This should be done following theW3C recommen-
dations for publishing Linked Data on theWeb [50], which
suggest enabling direct URI resolution, providing a REST-
ful API, providing a SPARQL endpoint, and/or providing
the dataset as a file for download.
There is a large palette of tools and software platforms
which allow seamless Linked Data publishing. Among
them are D2R Server [83] and Virtuoso [84], which allow
Linked Data publishing of datasets which are originally
in an RDF file (Turtle, N3, RDF/XML, JSON-LD, etc.), a
CSV file, or in a relational database. These platforms then
allow access to the Linked Data dataset via HTML pages,
via RDF file downloads and via a SPARQL endpoint which
can be used as a RESTful API as well.
When creating a Linked Drug Data dataset, we
recommend adding and interlinking it with the global
LinkedDrugs dataset which will consist of all such
datasets generated by different parties, using these
guidelines. To enable this, we have developed a web-
based tool for uploading the generated datasets [60],
which after a human-based quality assessment triggers
an automated process for interlinking the new dataset
with the existing LinkedDrugs datasets and publishing
it according to the Linked Data principles and best
practices.
We also recommend publishing the dataset at
Datahub.io [71] under the healthcare and drugs
categories, as well as adding the #linkeddrugs tag.
Additionally, we advise joining the LOD Cloud [87]. Both
these actions will enable higher visibility of the dataset.
Another important part of this step is the announce-
ment of the newly created Linked Data dataset to the pub-
lic. For this, information about the dataset along with its
VoID metadata should be published on popular data por-
tals, such as Datahub.io [71]. This announcement should
also be done via existing communication channels of the
data publisher and his/her organization. In order to facil-
itate further use and reuse of the dataset, it is important
to provide a form or a contact email address for inter-
ested parties to be able to report data or access issues, and
provide feedback. On the organization side, it is impor-
tant that these reports and requests are attended to in
a timely fashion; otherwise the usability of the dataset is
significantly lowered.
Step V: Use-cases, applications and services
The last step refers to defining use-case scenarios and/or
developing specific applications and services which will
use the data from the newly created Linked Data dataset,
to showcase the (re)usability of the dataset and its links to
other Linked Data datasets. This will present the poten-
tial of the contextually linked datasets to future interested
parties.
When creating a Linked Drug Data dataset, poten-
tial use-case scenarios, applications and services should
include contextually linked data from the LODD datasets
and the Bio2RDF datasets. The LODD datasets, especially
the DrugBank linked dataset, contain data about generic
drugs, i.e. active ingredients, along with their pharma-
ceutical and pharmacological properties, targets, brand
names, food interactions, drug interactions, etc. Since the
national drug data registries contain information about
drug products, one-to-one mappings between the enti-
ties from such datasets and the DrugBank and DBpedia
datasets are not possible. Instead, using our RDF schema
from Step II and the OpenRefine transformation script
from Step III, each entity from the dataset will be linked to
one or more generic drugs/active ingredients from Drug-
Bank and DBpedia, based on its ATC code [88]. This
way, the drugs from the dataset get a contextual link to
the generic drugs, and from there, to all of its proper-
ties and characteristics. Additionally, the existing links
from the DrugBank and DBpedia generic drugs to other
healthcare datasets can be further exploited, as they also
represent contextual links. Such links currently point to
LinkedCT and Bio2RDF. To demonstrate the usability of
the generated Linked Drug Data datasets, we provide
example use-cases on the project website [60] and on
GitHub [58].
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 19 of 24
In a general case, the use-cases can be text-based
scenarios, specific SPARQL queries, or prototype appli-
cations, describing the ways in which the data from
the new dataset can be browsed, retrieved and used.
Here, a specific focus should be given on how the
links to other Linked Data datasets can be exploited
to reach other data, not present in the original data
source, to extend its context. With this, the data pub-
lisher will show to interested parties that the original
dataset has more value when combined with datasets
from the same or similar context, instead of being used
in an isolated scenario. Besides such use-case, the same
effects of the Linked Data dataset can be showcased
with the development of applications and services. They
bring more visibility to the general (re)usability of the
Linked Data dataset, but generally require more time and
effort.
The created use-cases, applications and/or services,
should be shared and announced to the public, along with
the dataset itself and its VoID metadata. The use of the
same channels from the previous step is advised.
Methodology supporting tools
As part of the methodological guidelines, with the intent
to provide help to the data publishers working in the
drug data domain, we designed and developed a set
of tools. These tools consist of the RDF schema, the
CSV template, the OpenRefine transformation script, the
SPARQL-based tool for interlinking related drugs and the
web-based tool for automated transformation, interlink-
ing and publishing of the generated Linked Drug Data
dataset.
RDF schema
In order to model the domain of drug products on a global
scale, we needed to create one common schema for all
national drug data repositories, and then use it for anno-
tating the drug data. With it, the goal was to provide
alignment of drug data from different sources, with differ-
ent format and different levels of data granularity, in order
to enable simpler data exploitation.
First, we analyzed the national drug data repositories of
31 countries1 and the analysis helped us define a com-
mon set of properties which exist and which we want
to use in our dataset. This set consisted of 24 proper-
ties, including the brand name of the drug, the generic
name, the ATC code, the EAN code (barcode), the list
of active ingredients, the drug strength, dosage form,
cost, manufacturer, the country it was registered in, the
details about its license, etc. Not all national drug data
repositories provide all of the data and properties we
selected for our schema, but we did not want to decide
against using those properties - they are useful where
available.
Following the best practices in ontology and vocabulary
use [77], we started by considering reuse of classes and
properties from existing vocabularies. We used the com-
mon set of properties we defined in the previous step, and
found that the Schema.org vocabulary [59] was fully appli-
cable for our set. The Schema.org vocabulary, as part of its
Health and Lifesciences Extension [89], contains a defini-
tion of the class schema:Drug and contains a large set of
properties applicable to it [90]. As we can see on Fig. 2, the
RDF schema uses the DrugBank ontology and the RDFS
ontology, as well, for interoperability purposes.
Schema.org is a joint initiative of Google, Bing, Yahoo
and Yandex, as a common vocabulary intended for struc-
tured markup on web pages [9193]. It is used by these
search engines to introduce rich snippets about people,
events, products, movies, restaurants, books, tv shows,
etc. It is also used in Googles Knowledge Graph, in emails
confirming reservations and receipts (from restaurants,
hotels, airlines, etc.) both from Gmail and Microsofts
Cortana, it is used for rich pins on Pinterest, as well as
fromApples Siri [94]. Its use on theWeb has been increas-
ing in the past few years, more rapidly than the more rig-
orous general-purpose vocabularies and ontologies before
it [95]. Its success is mainly attributed to its simplicity:
it uses a generally flat hierarchy of classes, so that the
boundaries of implementation from data publishers and
webmasters is kept low.
The growing use of the Schema.org vocabulary, as well
as its domain generality, has put the vocabulary in a
position in which it is being used for aligning existing
ontologies and datasets. This is happening in the health-
care domain, as well [96]. With the release of Schema.org
version 3.0 [97], the medical and healthcare related terms
[98] have been moved to the Health and Livesciences
Extension [89], to enable and ensure future collabora-
tive development of the terms by the Healthcare Schema
Vocabulary community group at W3C [99, 100]. This
plan for a long-term support by the community from the
domain instills sufficient certainty for us to choose the
Schema.org vocabulary, instead of the domain specific
ontologies [76], to provide a common schema for drug
products on a global scale.
In order to provide some alignment between the gen-
erated datasets and the LODD and DrugBank datasets,
we use several properties from the DrugBank ontology
to describe the drug products. More specifically, we use
drugbank:brandName, drugbank:genericName,
drugbank:atcCode and drugbank:dosageForm
as additional properties for the same values denoted
by schema:name, schema:nonProprietaryName,
schema:code and schema:dosageForm, respec-
tively. We do this for simplifying the SPARQL federated
queries when working with data from our LinkedDrugs
dataset and the DrugBank dataset. Additionally, each drug
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 20 of 24
product from the LinkedDrugs dataset is an instance of a
specific class from the ATC Classification Ontology [101],
in order to classify the drug according to the ATC classi-
fication system, based on its ATC code(s). We also chose
rdfs:seeAlso as it is the most widely used relation for
interlinking similar entities in the LOD Cloud [33].
Just as any other RDF schema, vocabulary and ontol-
ogy, the RDF schema selected for our Linked Drug Data
datasets can be evolve in time; it can be extended and
modified in the future by us or third-parties, as the field of
drug data evolves.
CSV template
In order to enable data publishers to annotate their drug
data with the RDF schema from Fig. 2, we need a for-
mal template for the data which is being prepared for
transformation, and a formal transformation process. For
the former, we define a CSV template, available publicly
and as open-source [58]. The CSV template contains 39
columns which represent the different data fields needed
from the source data for the transformation process. They
inlude the URI of the drug, its brand name, generic
name(s), manufacturer(s), ATC code(s), active ingredi-
ent(s), strength, cost, etc. They are modelled to fit with
the RDF schema, which encompasses all data necessary
for high-quality modeling of the domain.
The data type of the different columns is usually a sim-
ple text value, except where we note otherwise. Some
important notes regarding the field data types include:
the strength value is divided into an integer-value col-
umn denoting the strength, while the unit is part of a
text-value column denoting the strenght unit; similarly,
the cost of the drug is separated into a float value and
a currency value; the several date columns need to be
formatted as YYYY-MM-DD; the prescription status
should be enumerated as either OTC or Prescrip-
tionOnly; the currency code needs to comply with the
ISO standard for denoting currencies [102]; the coun-
try where the drug is registered in needs to be denoted
using a country code accoding to an ISO standard [63]; if
there are multiple generic names, manufacturers or active
ingredients, they should be denoted one-per-column in
the available genericNameN, manufacturerN and
activeIngredientN columns, respectively, etc. The
details about the other column data types are available on
the project website [58].
The CSV template uses a vertical line character (|)
as a delimiter, since the regular CSV separators such
as a comma (,) and a semicolon (;) are very often
present in the cell values when working with drug data,
and can be misinterpreted. It is important to note that
the order of the columns in the CSV template is not
relevant, if used with our OpenRefine transformation
script.
As with the RDF schema, the CSV template is open and
publicly available, and therefore can be extended or modi-
fied in the future by both us and third-parties, as the drug
data field evolves and more Linked Drug Data dataset are
being created.
OpenRefine transformation script
Step III of the methodology contains the task of trans-
forming the source data into the RDF schema selected
in Step II. Since we defined an RDF schema which can
be applied in the drug data domain for drug products
which are registered in different countries, we also provide
a tool which can help automate the transformation pro-
cess, while ensuring compliance of the generated data with
the defined RDF schema and therefore providing aligned,
high-quality 5-star Linked Data for the drug domain. The
intent of this tool is to lower the bounds of transforming
data into RDF and Linked Data for data publishers which
are not deeply involved and experienced in the Semantic
Web and Linked Data practices.
We provide this Linked Data generation tool in the form
of an OpenRefine transformation script. OpenRefine [81]
is an open-source software for working with structured
data, usually CSV, TSV, XML, etc. It provides users with
functionalities for working with large datasets: the users
can record their action over a small set of example rows,
and then apply them over the entire source data. Here, the
actions can include data transformations, merging, data
cleaning tasks, manipulation of the columns, etc. It also
has an RDF extension which allows reconciliation of cell
values against RDF data from SPARQL endpoints. This
allows linking of cell values with entities from a SPARQL
endpoint, for unambiguous identification of entities. It
also allows mapping of the source data into RDF, by defin-
ing an RDF skeleton. The output of this action is an
RDF file generated from the source data, according to the
definitions in the RDF skeleton.
The ability of OpenRefine to save the user actions and
then export them in JSON format, allows reuse of certain
sets of actions for different datasets. This gives us the abil-
ity to define the data transformation which can be reused
over different source drug datasets, which have the same
columns. As we have a CSV template, we can use this as
part as our set of tools. The defined list of data transforma-
tion actions we created is what we have as our OpenRefine
transformation script [58].
Our OpenRefine transformation script is designed for
data complying with the CSV template, and its output is
a Linked Drug Data dataset which uses our RDF schema.
The transformation script contains three actions:
A. reconcile the columns genericName1,
genericName2, ..., genericName5 against
DBpedia,
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 21 of 24
B. reconcile the column atcCode against the
DrugBank dataset, and
C. create an RDF schema skeleton
Action A. uses the RDF extension feature of OpenRe-
fine which uses the cell value from a selected column
to find potential entities from a given SPARQL endpoint
which can be matched to the entity denoted by the row.
In our case, we use the five genericName columns -
which hold the generic name of the active ingredient of
the drug entity - and we try to match each of them to a
dbo:Drug entity from the DBpedia SPARQL endpoint,
using its rdfs:label value. If the reconciliation service
finds a matching candidate entity, we use it in step C. to
create an RDF triple which links the drug entity from our
CSV dataset with the matched candidates from DBpedia,
via an rdfs:seeAlso relation, for instance:
Example 1
@prefix dbp: <http://dbpedia.org/resource/>
@prefix mkd: <https://lekovi.zdravstvo.gov
.mk/drugsregister/detailview/>
mkd:55446 rdfs:seeAlso dbp:Clopidogrel .
Action B. does a similar reconciliation, but on the
atcCode column from the CSV dataset and against the
DrugBank endpoint. It tries to find matches between
the value of the atcCode column on our side and
the drugbank:atcCode value of drugbank:drugs
instances from the endpoint. Unlike the situation in A.,
here we can have more than one matching candidate
from DrugBank. The reason is that there can be multiple
drugbank:drugs instances which have the same ATC
code, i.e. share the same therapeutic, pharmacological and
chemical properties. Similar as in A., we use all matching
candidates from the reconciliation in step C. to create RDF
triples which link the drug entity from our CSV dataset to
the matched drug entities from DrugBank, such as:
Example 2
@prefix mkd: <https://lekovi.zdravstvo.gov
.mk/drugsregister/detailview/>
@prefix dbd: <http://wifo5-04...uni-
mannheim.de/drugbank/resource
/drugs/>
mkd:841690570 rdfs:seeAlso dbd:DB00201 ;
rdfs:seeAlso dbd:DB00316 .
Action C. creates the RDF schema skeleton, which con-
tains the rules for mapping the consolidated CSV file into
RDF. In the RDF schema skeleton (Fig. 2), we define map-
pings between the CSV columns and certain RDF triple
patterns. Some of the mappings are straight-forward, such
as the mappings of the brand name, the generic name,
the dosage form, the country, the url, the description, etc.
For them, we define the URI of the drug as a subject,
we denote a specific property for the triple, and then we
define the value of the column as a literal or an object
of the triple. For instance, the brand name of a drug is
mapped into RDF triples with the following format:
Mapping Example 1
<drug-URI> schema:name <value-of-brandName-
column> ;
drugbank:brandName <value-of-
brandName-column> .
However, other mappings are more complex. Mappings
of values such as the ATC code, the cost, the strength,
the manufacturer, the license details, etc., need new enti-
ties to be created, entities of different types. For instance,
in order to add the information about the ATC code to
the drug entity, we need to create a new blank node of
type schema:MedicalCode, which has two additional
triples: one with the schema:codeValue property and
one with the schema:codingSystem property. This
ATC code mapping can be represented with:
Mapping Example 2
<drug-URI> schema:code <blank-node-ID> .
<blank-node-ID> rdf:type schema:MedicalCode;
schema:codeValue <value-of-
atcCode-column> ;
schema:codingSystem ATC.
The license mappings were the most complex, which
we can see from Fig. 2. Aside from using OpenRefines
user interface for defining the RDF skeleton, we used
its GREL language for mapping the reconciliation results
from actions A. and B. into rdfs:seeAlso triples.
As a result of the transformation script, a Linked Data
dataset with links to the LOD Cloud is created. Similarly
as the other tools, the transformation script is available
as an open-source JSON file, which can be extended
and modified in the future. As a support for it, we
also developed a BASH script which sends the CSV file
with drug data, formatted according to our CSV tem-
plate, along with the OpenRefine transformation script
to a running BatchRefine service [58]. The result from
this call is the RDF output representing the transformed
dataset.
SPARQL-based tool for extending and interlinking the dataset
Once the drug dataset is transformed into a Linked Data
dataset with the other tools, an additional step is required
in Step III to create the internal links between drugs
which share the same functionality, i.e. share the same
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 22 of 24
therapeutic, pharmacological and chemical properties, in
order to create a better basis for use-cases.We need to cre-
ate links between drugs from the dataset which have the
same function, i.e. are aimed to treat the same condition.
To create these links, we use drugs ATC codes. Accord-
ing to theWorld Health Organization coding scheme [88],
if two drugs have the same ATC code, they share the
same function. For this purpose, we define a reusable
SPARQL query [58] which detects all pairs of drugs from
the dataset which have the same ATC code, and using
the schema:relatedDrug property creates a pair of
triples for them, for instance:
Example 3
@prefix rus: <http://www.vidal.ru/drugs/>
@prefix mkd: <https://lekovi.zdravstvo.gov.
mk/drugsregister/detailview/>
rus:trombopol__22439 schema:relatedDrug
mkd:51201 .
mkd:51201 schema:relatedDrug
rus:trombopol__22439 .
These two triples create a two-way link between the
drugs in the dataset, denoting their functional similarity.
The SPARQL query results with storing the newly created
RDF triples in the same RDF graph where the dataset is
already stored. These interlinkings can be utilizes for pro-
viding the users with alternative drugs they may require
for treating their condition, either in the same of in a
different country.
Since not all source registries contain the ATC code
information, and in order to increase the number of inter-
linked drug products from the dataset and support better
data analytics, we define an additional reusable SPARQL
query [58] which assignes ATC codes to all drugs from
the dataset which miss this information. The SPARQL
query detects drug products without an ATC code, finds
the generic drug from DBpedia which the drug is linked
to with the rdfs:seeAlso relation, gets the ATC code
of the DBpedia generic drug and assigns it to the drug
product in question. Since the SPARQL query for inter-
linking drugs from the dataset depends on the ATC code,
this SPARQL query for extending the dataset with missing
ATC code values should be executed first.
Both SPARQL queries are parametrized and should be
edited before execution. They can be executed over the
Linked Data storage used for storing the Linked Data
dataset generated with the other tools.
Web-based tool for automated transformation, interlinking
and publishing
The generated Linked Drug Data dataset needs to be pub-
lished on theWeb according to the Linked Data principles
and best practices, as advised in Step IV. In order to aid the
data publishers, this step can be automatically executed
by using a web-based tool we provide. The data publish-
ers can upload the generated Linked Data dataset(s) on
the LinkedDrugs project website [60], and after a human-
based quality assessment, the dataset will be automatically
published. For this we use a publicly available Virtuoso
instance [61], from which the new dataset is available on
the Web as Linked Data, via its SPARQL endpoint [62].
The RDF graph identifier is returned to the data publisher
after the successful upload process.
Besides publishing finished Linked Drug Data datasets,
the web-based tool and its automated process can also
execute the previous steps of the methodology for the
data publisher: (a) they can generate an interlinked
Linked Data dataset from an input CSV file, and (b)
they can interlink drugs with schema:relatedDrug
relations from an input RDF file. For the former, the
uploaded CSV file needs to be generated following our
CSV template, and based on it, the predefined RDF
schema and the OpenRefine transformation script, our
web-based tool and its server-side process will gener-
ate the Linked Data dataset. Using the SPARQL-based
tool from above, it will then generate links between
the drugs from the dataset, based on their ATC codes.
For the latter, the web-based tool directly creates the
schema:relatedDrug relations between similar drugs
from the uploaded Linked Drug Data dataset in RDF.
With this, we provide the convenience to move most
of the data processing from the methodological guide-
lines away from the data publishers, and simplify their
workflow.
When a data publisher uses our web-based tool at
[60] to publish a Linked Drug Data dataset, our sys-
tem also adds it to the global Linked Drug Data dataset
- the LinkedDrugs dataset - by storing it in another
RDF graph and generating schema:relatedDrug
triples for linking the drugs from the new dataset with
the drugs from the existing datasets in LinkedDrugs,
and vice-versa. The LinkedDrugs dataset then contains
data for drug products provided by different publish-
ers, including our team, and is available via a perma-
nent, dereferenceable URI, which supports HTTP content
negotiation [65].
Endnote
1Austria, Azerbaijan, Belgium, Canada, Costa Rica,
Croatia, Cyprus, Czech Republic, Egypt, Estonia, EUs
European Medicines Agency, France, Hungary, Ireland,
Italy, Macedonia, Malta, Netherlands, New Zealand,
Nigeria, Norway, Romania, Russia, Serbia, Slovakia,
Slovenia, South African Republic, Spain, Uganda, Ukraine
and USA.
Jovanovik and Trajanov Journal of Biomedical Semantics  (2017) 8:3 Page 23 of 24
Acknowledgements
The authors would like to thank Simona Micevska, Angjela Davitkova, Damjan
Gjurovski and Marjan Georgiev, who helped with the extensive data gathering
process. The work in this paper was partially financed by the Faculty of
Computer Science and Engineering, Ss. Cyril and Methodius University in
Skopje, as part of the SemBigData: Using Semantic Web Technologies to
Connect and Explore Big Data research project.
Authors contributions
MJ and DT designed the research. MJ developed the methodological
guidelines and the supporting tools. MJ developed the automated data
consolidation system and the LinkedDrugs dataset. MJ and DT analyzed the
data and designed the usage scenarios. MJ and DT contributed to the
manuscript. Both authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Received: 28 April 2016 Accepted: 17 December 2016
RESEARCH Open Access
Ontology-based specification, identification
and analysis of perioperative risks
Alexandr Uciteli1*, Juliane Neumann2*, Kais Tahar1, Kutaiba Saleh3*, Stephan Stucke4, Sebastian Faulbrück-Röhr4,
André Kaeding4, Martin Specht3, Tobias Schmidt3, Thomas Neumuth2, Andreas Besting5, Dominik Stegemann5,
Frank Portheine5 and Heinrich Herre1*
Abstract
Background: Medical personnel in hospitals often works under great physical and mental strain. In medical decision-
making, errors can never be completely ruled out. Several studies have shown that between 50 and 60% of adverse
events could have been avoided through better organization, more attention or more effective security procedures.
Critical situations especially arise during interdisciplinary collaboration and the use of complex medical technology, for
example during surgical interventions and in perioperative settings (the period of time before, during and after surgical
intervention).
Methods: In this paper, we present an ontology and an ontology-based software system, which can identify risks across
medical processes and supports the avoidance of errors in particular in the perioperative setting. We developed a practicable
definition of the risk notion, which is easily understandable by the medical staff and is usable for the software tools. Based on
this definition, we developed a Risk Identification Ontology (RIO) and used it for the specification and the identification of
perioperative risks.
Results: An agent system was developed, which gathers risk-relevant data during the whole perioperative treatment process
from various sources and provides it for risk identification and analysis in a centralized fashion. The results of such an analysis
are provided to the medical personnel in form of context-sensitive hints and alerts. For the identification of the ontologically
specified risks, we developed an ontology-based software module, called Ontology-based Risk Detector (OntoRiDe).
Conclusions: About 20 risks relating to cochlear implantation (CI) have already been implemented. Comprehensive testing
has indicated the correctness of the data acquisition, risk identification and analysis components, as well as the web-based
visualization of results.
Keywords: Perioperative risks, Ontology, Risk definition, Risk specification, Risk identification, Risk analysis, Agent system
Background
Patient safety is a quality objective and an important
factor of the quality of treatment in hospitals in general
[1]. Prevention of medical errors and risks is a significant
method to improve patient safety. Medical personnel
often work under great physical and mental strain. In
medical decision-making, errors can never be completely
ruled out [2]. In 2000, the report To Err is Human [3]
was published by the Institute of Medicine of the US
National Academy of Sciences (IOM). This attracted
great international attention and moved the topics of
medical risks, errors and patient safety into the focus of
the scientific interest. The IOM concluded in the report
that from 2.9 to 3.7% of all patients admitted to hospitals
in the USA sustain an adverse event. In 70% of these
cases, the patient retains no or only minor damage, 7%
lead to permanent damage and 14% cause the patients
death. The study also showed that between 50 and 60%
of these adverse events could have been avoided through
better organization, more attention or more effective
security procedures. Analyses show that the number of
* Correspondence: auciteli@imise.uni-leipzig.de;
juliane.neumann@medizin.uni-leipzig.de; kutaiba.saleh@med.uni-jena.de;
heinrich.herre@imise.uni-leipzig.de
1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),
University of Leipzig, Leipzig, Germany
2Innovation Center Computer Assisted Surgery (ICCAS), University of Leipzig,
Leipzig, Germany
3Jena University Hospital, Jena, Germany
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 
DOI 10.1186/s13326-017-0147-8
medical errors in Germany is also not negligible. Accord-
ing to a report by the Robert Koch Institute [4], the inci-
dence of suspected medical errors is approximately 40,000
cases across the country per year. Hence, the estimated
error recognition rate of 30% corresponds to the rate of
approximately 12,000 recognized medical errors per year.
Since the publication of To Err Is Human, risk
management and patient safety has consistently remained
a topic of interest for scientific studies as well as for sug-
gestions of goals for improvements [5]. Critical situations
arise especially during interdisciplinary collaboration and
the use of complex medical technology, for example
during surgical interventions and in perioperative settings.
Especially the oversight of medically relevant treatment
data or an incomplete medical history may lead to incor-
rect treatment [6].
We present an ontology and a conception for an
ontology-based software tool, which can identify and
analyze risks across medical processes. Furthermore, the
tool supports the avoidance of errors in the perioperative
setting. The results of the risk analysis are conveyed to
medical personnel in form of context sensitive hints and
alerts. The software architecture is designed to respond
not only to risks within a single treatment step, but to
also consider the patients entire stay in the hospital. For
a practical implementation in the clinical environment,
the cochlear implantation (CI) was selected as a surgical
use case at Jena University Hospital. For this purpose,
medical and technical treatment risks were analyzed and
medical guidelines and standards were taken into ac-
count. In addition, data and information sources were
defined based on an anonymized CI patient record.
Further sources of critical events were collected by
undertaking of qualitative interviews with technical,
nursing and medical personnel participating in a CI
treatment process. On this basis, risk situations were
defined and integrated into ontological models. This work
is a part of the OntoMedRisk project [7] funded by the
German Federal Ministry of Education and Research.
Methods
Introduction in General Formal Ontology (GFO)
The development of the intended ontologies and of the
needed ontological analyses are carried out within the
top-level ontology GFO [8, 9]. In GFO, the entities of
the world are classified into categories and individuals.
Categories can be instantiated, but individuals are not
instantiable. GFO allows for categories of higher order,
i.e. there are categories whose instances are themselves
categories, for example the category species. Spatio-
temporal individuals are classified along two axes, the
first one explicates the individuals relation to time and
space, and the second one describes the individuals
degree of existential independence.
Spatio-temporal individuals are classified into continu-
ants, presentials and processes. Continuants persist
through time and have a lifetime. A particular kind of con-
tinuant corresponds to ordinary objects such as cars, balls,
trees, etc. These are called material objects: they carry a
unity, consist of matter and occupy space. The lifetime of
a continuant is presented by a time interval of non-zero
duration; such time intervals are called chronoids in GFO
[10]. Continuants are individuals, which may change, for
example, an individual cat C crossing the street. Then, at
every point in time t of crossing, C exhibits a snapshot
C(t). These snapshots differ in their properties. Further,
the cat C may lose parts while crossing, though, remaining
the same entity. The entities C(t) are individuals of their
own, called presentials; they are wholly present at a
particular point in time, being a time boundary. If the
continuant is a material object M, the presentials exhibited
by M at point in time t, denoted by M(t), are called mater-
ial structures. Presentials cannot change, because any
change needs an extended time interval or two coinciding
time boundaries.
Processes are temporally extended entities that happen
in time, for example a run; they can never be wholly
present at a point in time. Processes have temporal
parts, being themselves processes. If a process P is
temporally restricted to a point in time then it yields a
presential M, which is called a process boundary of P
[10]. Hence, presentials have two different origins, they
may be snapshots of continuants or parts of process
boundaries [9]. There is a duality between processes and
presentials, the latter are wholly present at a point in
time, whereas this is never true for processes. The corre-
sponding classes/sets of individuals, denoted by the
predicates Cont(x), Pres(x), and Proc(x), are assumed to
be pair-wise disjoint. Processes are the most basic kind
of entity, because they form a ground for presentials and
continuants, and determine the coherence of spatiotem-
poral reality. A boundary of a process P is defined by the
restriction of this process to a point in time of its
temporal extension. We postulate that any presential is a
part of some process boundary.
The integration between material objects and pro-
cesses is proposed in the integration law in GFO, which
states that for every material object M, being a continu-
ant, there is a process Proc(M), the boundaries of which
coincide with the presentials exhibited by M. There are
several basic relations which canonically connect
processes, presentials, and continuants [8, 9].
Spatio-temporal individuals, according to the second
axis, are classified with respect to their complexity and
their degree of existential independency. Attributives
depend on bearers, which can be continuants, presen-
tials, and processes. Situations are parts of reality, which
can be comprehended as a coherent whole [11]. Material
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 2 of 14
situations are composed of material objects, which are
connected by relators, and relators are instances of rela-
tions. Situoids are processes, which satisfy principles of
coherence, comprehensibility, and continuity. A surgical
intervention is an example of a process or a situoid. A
snapshot of this situoid at a certain point in time is a
surgical presentic situation, which has a spatial location
and includes various entities such that a coherent whole
is established.
There is a variety of types of attributives, among them,
qualities, roles, functions, dispositions, and structural
features. Properties are categories, the instances of which
are attributives. According to the different types of
attributives (relational roles, qualities, structural features,
individual functions, dispositions, factual, etc.) we distin-
guish quality properties and role properties, and the role
properties are classified into relational role properties
(abr. relational properties) as well as social role proper-
ties (social properties).
Ontological definition of the risk notion
The solution of all philosophical problems related to the
notion of risk is out of the scope of this paper. Instead,
we focus on a practicable definition of the risk notion,
which can be easily understood by medical staff and is
usable for the software tools. Our definition of the risk
notion has been developed in close cooperation with do-
main experts (medical staff ). Based on this definition, it
should be possible for the medical staff to specify the
relevant risk types, and for the software to identify and
to analyze the risk in a particular treatment situation.
There are various definitions of the notion of risk.
One of the most known/popular definitions is presented
in [12]. The authors divide the notion of risk into three
components, which are associated to the following
questions:
1. What can happen, i.e., what can go wrong?
(scenario)
2. How likely is it that that will happen? (probability of
the scenario)
3. If it does happen, what are the consequences?
(consequence of the scenario)
A risk, then, is a triple which consists of a scenario,
the probability of that scenario, and consequence of that
scenario.
Furthermore, there are several standards investigating
the notion of risk. The ISO/IEC 27005:2008 [13] defines
the notion of risk (information security risk) as poten-
tial that a given treat will exploit vulnerabilities of an
asset or group of assets and thereby cause harm to the
organization; the OHSAS 18001:2007 [14] - as a com-
bination of the likelihood of an occurrence of a
hazardous event or exposure(s) and the severity of injury
or ill health that can be caused by the event or expo-
sure(s); and the ISO 31000 (Risk management) [15] - as
an effect of uncertainty on objectives.
In [16] the authors analyze 11 common definitions of
risk and characterize them based on three categories: (a)
risk as a concept based on events, consequences and un-
certainties; (b) risk as a modeled, quantitative concept
(reflecting the aleatory uncertainties); and (c) subjective
risk descriptions. Most definitions belong to category (a),
the rest can be interpreted both in the sense of (b) or (c).
The common ground of most risk definitions is that
all of them consider a risk as involving a possibility for
the occurrence of a particular event or situation. Most of
these definitions consider such events as adverse ones.
The ontological analysis of risk is carried out within
the framework of GFO and takes into account the avail-
able definitions of risk. The analysis is built upon the
ontology of situations and situation types, which partly
uses ideas presented in [11, 17]. Adverse situations are
situations that contain adverse events. In this paper we
use the notion of adverse event/situation not only in the
sense of Any untoward occurrence that may present
during treatment with a pharmaceutical product but
which does not necessarily have a causal relation to the
treatment [18], but we also include events/situations
that are not related to medical interventions.
The notion of a possible situation is established within
the framework of a particular actualist representationism,
which postulates that possible situations are abstract en-
tities, the existence of which is consistent with the currently
available knowledge about the actual world. This view is
partly influenced by [1921] and is subsequently explicated
for material situations. Material situations are composed of
material facts, which are constituted by material objects and
connecting relators. An example of a material fact is a
spatio-temporal entity that is denoted by the expression
Johns drinking a beer. Associated to this fact we may con-
struct the relational proposition John is drinking a beer.
There is a difference between a fact and the corresponding
proposition. A proposition is an abstract entity, which can
be satisfied by facts (which are parts of reality). Arbitrary ab-
stract situations are sets of relational propositions, which
are not necessarily abstracted from real, i.e. actual situations.
An abstract situation S is realized by an actual situation S? if
any relational proposition in S is satisfied in the situation S?.
An abstract situation S, related to a domain D, is said to be
possible if it is consistent with the currently available know-
ledge about D, the domain experts agreed on. Hence, a pos-
sible situation has the potential to be realized by an actual
situation. A (spatiotemporal) situation S is said to be a risk
situation if it satisfies certain conditions, which imply that
for one of its possible succeeding situations S? any of its
realizing situations is an adverse situation.
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 3 of 14
We hold that a risk exists in a situation, that it de-
pends on it, and, hence, that it can be considered as a
situations property. We distinguish between single (in
sense of gfo:Property [8]) and composite properties, the
latter being composed of single ones and which can be
disassembled by the relation gfo:has_part.
Definition 1. A composite property CP is a property
that has as parts several single properties SP1, ..., SPn.
Definition 2. A risk for an adverse situation of type
AST is a composite property CP such that every
situation S possessing the property CP has a possible
succeeding situation of type AST, which can be realized
with a certain probability.
Definition 3. A risk is a composite property CP for
which there exists an adverse situation AST such that CP
is a risk for the adverse situation AST (as defined by 2).
Definition 4. A risk situation is a situation having at
least one risk (Fig. 1). In this paper, we consider risk sit-
uations as situations with a risk recognized as relevant
by the medical community and non-risk situations as
situations with no risk recognized as relevant by the
medical community.
Example 1. The risk of a bacterial infection during
cochlear implantation in infants depends on various
parameters, such as the infants age, the corresponding
bone thickness of the skull and the inner ear structure.
If the child is younger than 5 months, the bone thick-
ness mostly remains below 2 mm. Thus, the risk of
penetrating the skull and injuring the dura mater during
surgery increases so that the risk of the bacterial dura
mater infection (meningitis) increases as well. The
ground-truth probability for the adverse event of dura
mater infection during CI is about 59% [22]. For men-
ingitis prevention, the patient has to be vaccinated
against pneumococcus, meningococcus and haemophilus
influenzae type b several weeks before the surgery (indi-
cation phase). In addition, an antibiotic prevention
should be performed right before the surgery. According
to our definition an increased risk for acquiring menin-
gitis can be represented as a composite property,
consisting of three single properties, namely, the young
age (< 5 month), the absence of a meningitis vaccination,
as well as the absence of an antibiotic prevention. This
example is used in this paper for further explanations.
Results
Risk Identification Ontology (RIO)
We developed a Risk Identification Ontology (RIO,
Fig. 2), which is built upon the ontological model of the
notion of risk. This ontology is used for the specification
and the identification of perioperative risks. The ontol-
ogy RIO is founded in the GFO. As the starting point we
consider the treatment process, which may consist of
various treatment phases (gfo:has_part). The complete
treatment as well as the phases are complex processes
(gfo:Situoid). The treatment has a particular temporal
extension, called the treatment time (gfo:Chronoid).
According to GFO processes are projected (gfo:project-
s_to) onto their time intervals. For every point in time
(gfo:Time_boundary) of the treatment exists (gfo:exist-
s_at) exactly one treatment situation (gfo:Situation). A
point in time of the treatment is according to GFO a
boundary of the treatment time (gfo:boundary_of),
whereas the corresponding treatment situation is a
boundary of the treatment itself.
For each treatment phase, particular points in time of
risk detection (PTRD) can be defined. The treatment sit-
uations, existing at these points in time, are analyzed
with respect to the existence of risks. Such situations are
called potential risk situations (PRS), because they do
Fig. 1 Definition of the risk notion (the white arrows represent the is-a relation)
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 4 of 14
not necessarily contain risks. Situations and in particular
treatment situations possess various properties (gfo:-
Property). These properties may belong to the situation,
but also to the participants, as, for example physicians
(doctors), medical instruments, and, most important, to
the patients. We consider these properties also as
properties of the current treatment situation (gfo:ha-
s_property). Properties of the potential risk situations
that are relevant for the estimation of the risk are called
KPIs (Key Performance Indicators) in this paper. Ac-
cording to Definitions 14 a particular combination of a
subset of the KPIs of a PRS (for example, age of
patient = 3 months, menginitis vaccination = false) is a
risk if the PRS may lead to an adverse situation at a later
point in time (rio:succeeding_situation).
A PRS may contain various risks, and risks of the same
type (the instances of the same risk class) may occur in
distinct PRS and may lead (rio:risk_for_adverse_situa-
tion) to distinct adverse situations (the instances of the
same adverse situation class). Each KPI is associated
with potential risk situations, whereas the risk situations
additionally possess the composite risk properties.
Furthermore, the risks can be related to those treatment
phases for which they are relevant (rio:risk_in_phase). A
risk is relevant in a particular phase, if all required KPI
values for the risk assessment need to be recorded (e.g.
according to external or internal hospital guidelines) and
need to be available in this phase in a respective
database to prevent the risk from being realized in an
adverse situation. Adverse situations may exhibit various
degrees of severity and risks may possess various prob-
abilities for the occurrence of adverse situations.
With help of the RIO the risks in a current potential
risk situation are identified by the software component
Fig. 2 Risk Identification Ontology (RIO)
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 5 of 14
OntoRiDe, and, hence the situation can be classified
either as a risk or as a non-risk situation.
Risk specification
Perioperative risk assessment
For the development of a perioperative risk identification
ontology the recognition and assessment of potential
medical, technical, organizational, and human risk
factors are an essential prerequisite. Therefore, an exten-
sive risk assessment was performed for an otorhinolar-
yngological use case. The insertion of cochlear implants
(CI) was chosen in order to demonstrate the features
and benefits of the ontology-based risk identification
system. The perioperative medical and technical risk
factors, procedure related complications and their com-
plication rates as well as prevention strategies were
extracted from peer-reviewed publications and evidence-
based best-practice guidelines of the German Society of
Oto-Rhino-Laryngology, Head and Neck Surgery [23]. In
addition, entries of the Critical Incident Reporting System
(CIRS) of the University Hospital Jena (Germany) and an
example of an anonymized patient record were analyzed
for organization and human-related risk assessment. The
derived risk characteristics, potential following adverse sit-
uations and their causes were used to describe relevant
perioperative and cross-process risks factors.
Perioperative process modeling
The information of risk factors and of potentially adverse
events has to be provided to the responsible medical
personnel at the right time by offering appropriate
context-sensitive hints and alerts. Therefore, the medical
and organizational processes have to be taken into ac-
count. The general perioperative workflow of the CI treat-
ment was modeled and visualized in a process diagram, as
event-driven process chain (EPC). In the following, both
generalized and use-case specific treatment phases were
defined in the formal process model. The generalized
treatment phases are depicted in Fig. 3. Besides the CI
treatment process, the defined phases are suitable for
representing various elective surgeries and interventions.
The treatment process was modeled by representing
the sequence of clinical activities, treatment decisions,
parallel processes and possible events, the involved per-
sons as well as resources, like data and documents, med-
ical devices, or IT systems. In addition, the identified
risk factors, complications, and prevention activities
were integrated in the process model.
By mapping the identified risk factors to the dedicated
activities and treatment phases, the process model was
then used subsequently for further risk assessment and
perioperative risk modeling. This enabled over 120 po-
tential perioperative risks to be identified and also
mapped to their related process step in the process
model.
Perioperative risks modeling
In the next step the identified potential risk factors, ad-
verse situations, and critical incidents, which are related
to cochlear implantation interventions, were examined
in an extensive risk analysis. Thereof, a risk classification
for formal risk specification was derived. The identified
risk factors were subsequently classified into different
categories of medical, organizational, technical, or
human-related risks. Thus, the treatment phases were
categorized into risk detection phases, in which the
corresponding risk is relevant and could potentially lead
to an adverse situation. Additionally, there is a category
for cross-process risks, which could lead anytime to an
adverse situation, e.g. the risk of dizziness and falls or
the high bleeding risk during surgery due to anticoagu-
lant medication.
For each treatment phase, different KPIs were defined,
which allow the identification of specific perioperative
risks. The KPIs are linked with operators and a certain
data range to a conditional expression of a possible risk
factor (e.g., c1: Age_in_months IN [0, 5), c4: Vaccina-
tion_status == no, Fig. 4, Example 1). The KPI data
type values could be for instance a Boolean value, text,
date, or number. A combination of these conditional ex-
pressions is formalized as a risk specification rule. If the
risk specification rule becomes true, due to the values of
their conditions and KPIs, there is a high occurrence
probability of adverse situations, which have to be also
specified for each risk. In addition, for each adverse situ-
ation an occurrence probability and a severity were de-
fined (the severity is defined on a separate spreadsheet).
In the risk specification, the KPIs were described along
with their possible acquisition sources. Therefore, the risk
specification defines both the required measurement
phases and the measurement sources, like patient-related
Fig. 3 Treatment phases
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 6 of 14
data and sensor data, e.g. data from the digital patient rec-
ord, the hospital information system, checklists, or situa-
tions in actual process execution. In Fig. 4 a risk
specification based on Example 1 is presented.
The tool RIOGen, developed within the project, gener-
ates ontological entities from the risk specification and
inserts them into RIO. For every risk condition, for
example, a subclass of the corresponding KPI is inserted.
Here the class names are automatically generated
according to certain rules. For every condition class an
anonymous equivalent class is created as property re-
striction, based on the property has_data_value (Fig. 5).
Then, for each risk a subclass of rio:Risk is created. The
name for the subclass is defined in the risk specification
(e.g., Risk Name: Infection_Risk_001, Fig. 4). For the risk
subclass, an equivalent anonymous class is also defined
which is based on the has_part property and on the cor-
responding condition classes; this anonymous class rep-
resents the risk specification rule (Fig. 6). Furthermore,
the treatment phases are created and connected with
those KPIs and risks which are relevant for them. Fi-
nally, we define the connections between risks and those
adverse situations, which possibly evolve from them, as
annotations (incl. probability and severity, Fig. 7). We
specified the probability as annotation (as_probability) of
the annotation relating to the adverse situation
(risk_for_adverse_situation).
Ontology-based Risk Detector (OntoRiDe)
We developed an ontology-based software module,
called Ontology-based Risk Detector (OntoRiDe), which
allows the identification of the ontologically specified
risks. This tool receives the KPIs of the current potential
risk situation as an input parameter, and carries out the
risk specification rule, which is contained in the ontol-
ogy; then it classifies the current situation as a risk or
non-risk situation and returns the results. If the current
KPIs satisfy one of the rules (i.e., at least one risk is rec-
ognized) then the considered situation is a risk situation,
otherwise it is a non-risk situation.
Further information, which the tool returns to the
user, includes the description of the existing risks, the
treatment phases, in which the risks are relevant, but
also the adverse situations, which may evolve from them
(with the probability of occurrence and degree of sever-
ity). The most important functionality is the possibility
to recognize the risks, but, furthermore, to determine
and provide for every recognized risk all combinations
of current KPIs that are responsible for every recognized
risk. Using this information the user is able to eliminate
all of the riskscauses.
In the following, we briefly sketch the functionalities of
the OntoRiDe. For every risk class the corresponding risk
specification rule, which is specified as an anonymous
equivalent class (Fig. 6), is interpreted and transformed
Fig. 4 Risk specification
Fig. 5 Risk conditions
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 7 of 14
into a disjunctive normal form (by stepwise execution of
the de Morgan rules and of the law of distributivity). Any
of the conjunctions presents a possible explanation for the
risk (e.g., c1 AND c4 AND c6 and c3 AND c5 AND
c6, Fig. 4). Then, the single conditions (Fig. 5) are
checked, i.e., it is determined whether the current KPI
value is included in the specified value range. If all condi-
tions of the conjunction are satisfied, then the correspond-
ing KPIs and further information are provided for the user
as explanation.
We did not use a standard DL reasoner. Instead, we
implemented suitable functions in OntoRiDe, which are
relevant for the specific risk identification problem.
Firstly, we want to apply rules, which cannot be easily
interpreted by standard reasoners, especially rules which
contain mathematical expressions or predefined con-
stants. Such special types of rules are implemented by
the OntoRiDe. Secondly, standard reasoners carry out
various tasks, such as checking the consistency, classifi-
cation, and realization. However, most of these standard
tasks are not relevant for the identification of risks. This
leads to a reduced efficiency of the overall system, if a
standard reasoner is utilized for the interpretation of risk
specification rules. Finally, OntoRiDe must provide the
user with all possible explanations about the existence of
a risk in the current situation in an understandable way.
The problem of detection and exploration of all possible
explanations or justifications of an entailment is a well--
known task, for the solution of which there exists several
methods and tools [2426]. Furthermore, there are vari-
ous investigations about the cognitive complexity and
the understanding of the considered justifications [27,
28]. In this context a justification of an entailment is
understood to be the minimal set of axioms sufficient to
produce an entailment [24]. In [27, 28] the understand-
ability of justifications and the corresponding reading
strategies of OWL users are analyzed. The details of
several user studies show that ontology developers find
certain justifications very difficult to understand and to
work with. We developed a very simple form of explan-
ation, which is understandable for the medical
personnel. The OntoRiDe translates the risk specifica-
tion rules into a disjunctive normal form and checks all
conditions of the respective conjunctions. By this
procedure all KPI combinations, verified by the rule as
true, and the corresponding conditions (value ranges),
can be provided for the user in form of understandable
explanations (e.g., age < 5 month and vaccination = no
and antibiotic prevention = false).
In this way, we identify all and only relevant risks in
the current situation, as well as provide all possible ex-
planations for them, so that all requirements have been
fulfilled. Although the OntoRiDe is not a reasoner, it is
sound and complete with regard to our problem.
Fig. 6 Risk specification rule
Fig. 7 Annotations of risk and adverse situation
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 8 of 14
Agent system
OntoRiDe is embedded into an agent system, which is
developed within the project OntoMedRisk. The purpose
of this system is to conveniently access data, that is dis-
tributed across various data sources within a hospital in
a unified manner. In this way, the agent system derives
elementary information for identifying risk situations.
The data has to be collected by the agent system and is
determined by a set of KPIs. They represent risk-
relevant parameters, which have to be monitored by the
agent system throughout the entire perioperative treat-
ment process. The collected KPI-related data is provided
for the risk identification and analysis in a centralized
fashion. The results of those analyses are then forwarded
to the medical staff as context-sensitive hints and alerts.
The goal of OntoMedRisk is to reduce the risks of ad-
verse situations and complications through early and ad-
equate interventions.
The functional architecture of the agent system is
shown in Fig. 8. The agent system is integrated into the
hospital information system from which it collects
patient and risk related data. Besides the data and agent
related components, the agent system also includes the
functional components OntoRiDe and OntoRA (Ontology-
based Risk Analysis). The software-based agent system has
been implemented using the Java Agent Development
Framework (JADE) [29]. JADE embodies a framework, a
platform and the middleware for a FIPA-standardized
(Foundation for Intelligent Physical Agents, [30]) develop-
ment of multiagent systems. The main functions of a
JADE-based agent system can be categorized into supplying
agent behavior and agent communication. The agents
communicate in an asynchronous, message-based fashion,
using the Agent Communication Language (ACL) [30].
The internal data storage (FHIRbase) of the agent system is
based upon the HL7-FHIR specification [31]. Therefore,
Fig. 8 Architecture of the agent system
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 9 of 14
the data within the agent system is represented as FHIR re-
sources. The agent system models, for example, the infor-
mation received from OntoRiDe as FHIR RiskAssessment
Resource and saves it in the FHIRbase for further analysis.
We have been able to map all relevant risk information to
FHIR. The input KPIs have been saved, for example, as Ris-
kAssessment.basis (indicates the source data considered as
part of the assessment (FamilyHistory, Observations, Proce-
dures, Conditions, etc.)), the possible adverse situations 
as RiskAssessment.prediction.outcome (one of the potential
outcomes for the patient (e.g. remission, death, a particular
condition)), the probability of an adverse situation  as Ris-
kAssessment.prediction.probability (how likely is the out-
come), and the explanations for a detected risk  as
RiskAssessment.prediction.rationale (additional information
explaining the basis for the prediction) [31].
The continuous patient-specific risk monitoring relates
to the treatment phases of the perioperative treatment
process. Based on the supplied phase information,
OntoRiDe provides a phase-specific KPI set to the Agent
Controller. Using this information, the Agent Controller
generates patient-specific Data Retrieval Agents, which
manage the KPI sets and periodically send requests to
the Data Access Agents. Those agents are specifically
tailored for each data source in order to fetch data cor-
rectly. The collected KPI data is sent back to the
requesting Data Retrieval Agents and stored in the
FHIRbase. Based on a trigger, the Risk Communication
Agent fetches the patient-specific KPI data from this
database and sends it to OntoRiDe for risk identification
purposes. The risk reports resulting from this identifica-
tion process are then forwarded to OntoRA for further
processing. The purpose of OntoRA is to analyze the
identified risk situations and to provide the results in a
web interface, which can be accessed by medical staff
within the hospital information system.
Therefore, OntoRA implements a responsive, web-based
user interface hosted on the Apache Tomcat platform [32],
which allows the development of a platform-independent
solution, lowering costs and increasing flexibility.
The server-sided component of the application consists
of two parts, a backend for the web content and a web ser-
vice to which the agent system can send data. The web
service stores the received data in a MongoDB database
[33] hosted within the hospital information system. If a
client requests data, the backend takes care of this request
by fetching the data from the database and sending it to
the client. The client-side uses a responsive approach,
which allows the usage of web interfaces on multiple de-
vices, such as desktop PCs, tablets, and phones. To
achieve this, a combination of HTML5 [34], JQuery [35]
and Bootstrap 3 [36] is used. The user interface consists of
two web pages, a patient overview and a page containing a
patients risks, which are displayed in the users web
browser. The user can select the patient of interest, whose
risks are to be displayed. In this view, the risks are ordered
by the severity of each risk-event combination. After
selecting a risk tile, detailed information like the risk de-
scription or risk parameters are displayed (Fig. 9).
The agent system is currently deployed at the Jena
University Hospital. Referring to Fig. 8, the hospital
information system in which the agent system is inte-
grated into is displayed in Fig. 10. The agent system has
to collect data from various data sources within the
same subnet (1) and from a FHIR server, which holds
patient-related data (2). Because of several linked sub-
nets, the agent system also has to request KPI data from
a communication server (3) in order to access data from
remote data sources in different subnets.
Related work
Several approaches towards the formal representation of
risks and adverse events through ontologies are de-
scribed in the literature. We analyzed these existing on-
tologies for their potential to detect perioperative risks
in hospitals, but we concluded that none of these ontol-
ogies and tools could be applied to our project.
Bouamrane et al. [3739] report on the development
of an ontology-based system to support clinical decision
making. The support is provided in a two-step process.
First, the developed system calculates risk scores using
numerical formulas. In this step, the system does not
use the developed ontology but computes numeric
values using an open-source Java-based rule engine
(JBoss Rules). After calculating the relevant risk scores,
the DL reasoner (Pellet) classifies the patient into several
predefined categories for risks, recommended tests and
precaution protocols, using the OWL-DL representation
of the patient medical history profile and the decision
support ontology. The decision support ontology is di-
vided into three domains: a risk assessment ontology, a
recommended test ontology and a precaution protocol
ontology. The aim of the risk assessment ontology is to
detect potential risks of intra-operative and post-
operative complications in a given formal representation
of a patient medical profile.
Similar to the system of Bouamrane, our approach also
provides two components of decision support namely
OntoRiDe and OntoRA (Fig. 8). They can perform simi-
lar tasks as those of Bouamranes system. In addition,
OntoRiDe will also use the self-developed RIO for risk
identification similarly to the usage of the risk assessment
ontology. However, there are also important differences
between the two ontologies and systems. The risk assess-
ment ontology focuses only on the patients risk related to
intra-operative and post-operative complications such as
cardio-vascular and respiratory risks, whereas RIO covers
various risk types such as special and general treatment
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 10 of 14
risks, technical risks, organizational risks etc. The
second significant difference is that our approach in-
tegrates the treatment process, its steps, and situa-
tions in the risk conceptualization. In this way, it is
possible to analyze and identify cross process risks or
risk situations so that errors, especially in the peri-
operative field, could be avoided.
In [40] Third et al. describe a model for representing
scientific knowledge of risk factors in medicine. This
model enables the clinical experts to encode the risk
associations between biological, demographic, lifestyle
and environmental elements and clinical outcomes in
accordance with evidence from the clinical literature.
The major advantage of our approach in comparison
Fig. 10 Integration of the agent system into the hospital information system of the Jena University Hospital
Fig. 9 Visualization of risk information in the web interface of OntoRA
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 11 of 14
with the model developed by Third is the formal repre-
sentation of cross process risks that can lead to potential
adverse situations during different treatment phases. An-
other added value of our approach is that it can also
cover risks related to human and environmental factors
such as technical or organizational risks. These types of
risks are not considered in Thirds model.
In [41] an ontology of the Open Process Task Model
(OPT-Model) is presented. This ontology is primary
intended as a generic knowledge base, which implements
the various influences of processes and their relations in
medical environments, for a prospective risk analysis.
The advantage of RIO over the OPT-model-ontology is
that it provides an accurate risk analysis. By using RIO,
OntoRiDe is able to perform risk classifications accord-
ing to the risk occurrence time. This process allows us
to identify the point in time and treatment phase on
which a risk arise. Another further benefit of RIO is the
implicitly embedded risk specification, which meets the
spirit of evidence-based medicine. This implicit domain
knowledge is encoded in OWL rules and can be inferred
automatically using ontological reasoning to assess
current perioperative risk situations.
In [42] the authors report a clinical decision support
system (CDSS) for undergoing surgery based on domain
ontology and rules reasoning in the setting of hospital-
ized diabetic patients. Similar to our approach this sys-
tem uses logical rules to complement the domain
knowledge with implicitly embedded risk specification
and clinical domain knowledge. The important upside of
our approach is that it does not make restrictions based
on certain diseases such as diabetes mellitus, whereas
CDSS focuses only on glycemic management of diabetic
patients undergoing surgery.
The Ontology of Adverse Events (OAE) [43] and the
Ontology of Vaccine Adverse Events (OVAE) [44]
(Marcos, Zhao, and He 2013), which was developed
based on OAE, describe data relating to adverse events.
The OAE was designed to standardize and integrate data
relating to adverse events that occur after medical inter-
vention. The OVAE is used for representing and analyz-
ing adverse events associated with US-licensed human
vaccines. In OAE the notion adverse event is defined as
a pathological bodily process that occurs after a medical
intervention (e.g., following a vaccination), while a risk is
represented by a factor associated with the occurrence
of an adverse event. The work presented here focuses in-
stead on the risk situations and proposes a generic
model for the risk specification in the perioperative area.
Thus, we do not restrict ourselves to risks that are caus-
ally and exclusively related to medical interventions.
Contrary to OAE, our approach also considers other risk
types such as technical and organizational risks. More-
over, we use the term adverse situation in order to
avoid excluding situations that are not related to medical
interventions.
We also analyzed several conversion tools such as
Excel2OWL, Mapping Master and Populus [4547] for
their potential to build an expressive formal ontology
from our risk specification spreadsheet, but we con-
cluded that none of these tools could be applied to our
project. In fact, our Excel spreadsheet contains domain
specific logical rules (see Figs. 4 and 6) that are not cov-
ered in these software solutions. We therefore decided
to develop RIOGen, a Java tool that enables us to
automatically generate RIO entities from the risk specifi-
cation template.
Discussion
We elaborated an ontological foundation of the notion
of risk, upon which we developed a Risk Identification
Ontology (RIO). With help of RIO perioperative risks
can be specified, whereas OntoRiDe can be used to iden-
tify risks in a given treatment situation. This allows the
recognition of risk situations and supports the avoidance
of possible adverse effects or consequences. Further-
more, we implemented an agent system to realize the
ontology-based approach. This agent system gathers
during the whole perioperative treatment process risk-
relevant data from various sources and provides it for
the risk identification respectively the risk analysis in a
centralized fashion. The results of those analyses are
transmitted to the medical personnel in form of context
sensitive hints and alerts.
None of the presented approaches (s. Related work)
can answer competency questions such as Which treat-
ment situation could be a potential risk situation?,
Which properties or KPIs are responsible for an actual
risk situation? and Which risk situation belongs to
which treatment phase?. The aim of RIO and OntoRiDe
is to solve this issue.
Our approach has the following limitations: 1. Only
known und specified risks can be identified by the
system; 2. All required data (KPIs) must be available in
the respective source systems in electronic form. There-
fore, the system can only react to known and correctly
specified risks to which the required data was electronic-
ally recorded.
Future work
Further development of the agent system will encompass
the implementation of interfaces for different 3rd party
data sources in collaboration with their original vendors.
To facilitate the expansion of the agent system, a devel-
oper package for Data Access Agents will be released,
providing interfaces for integrating additional data
sources in conformance to the given specifications.
Furthermore, it is intended to expand and to optimize
Uciteli et al. Journal of Biomedical Semantics  (2017) 8:36 Page 12 of 14
the application of the agent system to cater for
additional use cases and to better support mobile devices
in order to provide real-time feedback and improve
usability. Finally, future work could include a machine-
learning approach, where the agent system recognizes
adverse events by itself and derives risks, which subse-
quently will be monitored to prevent the repeated
occurrence of these adverse events.
The presented Risk Identification Ontology could be
used for the ontology-based analysis of clinical studies
for different medical applications and use cases. Future
work will include further analysis and clinical evaluation
studies.
Our present work raises the question of what are the
formal, ontological connections between a risk, its
adverse situation and its probability. This question will
also be examined and discussed in the future.
Conclusion
We developed the Risk Identification Ontology and an
ontology-based agent system, which can identify and
analyze risks across medical processes and supports the
avoidance of errors in the perioperative setting. About
20 risks relating to cochlear implantations have already
been implemented. Comprehensive testing has shown
that a stable and platform-independent deployment of
all components on different virtual machines was
successful. Further testing using the FHIR server as a
source for KPI data has illustrated the correctness of the
data collection, risk identification and risk analysis
components, as well as the web-based visual representa-
tion of results. The test system contains a web-based
form for entering the test data sets, which are then
stored on the FHIR server. The domain experts (medical
staff ) have tested the functionality and usability of the
system based on practice-relevant test data. According
to the interviews with domain experts, the system
currently meets all specified requirements.
Abbreviations
ACL: Agent Communication Language; CDSS: Clinical decision support system;
CI: Cochlear implantation; CIRS: Critical Incident Reporting System; EPC: Event-
driven process chain; FHIR: Fast Healthcare Interoperability Resources;
FIPA: Foundation for Intelligent Physical Agents; GFO: General Formal Ontology;
IOM: Institute of Medicine of the US National Academy of Sciences; JADE: Java
Agent Development Framework; KPI: Key Performance Indicator; OAE: Ontology
of Adverse Events; OntoRA: Ontology-based Risk Analysis; OntoRiDe: Ontology-
based Risk Detector; OPT-Model: Open Process Task Model; OVAE: Ontology of
Vaccine Adverse Events; PRS: Potential risk situation; PTRD: Point in time of risk
detection; RIO: Risk Identification Ontology
Acknowledgements
An earlier version of the paper has been presented at ODLS 2016
(Ontologies and Data in Life Sciences) in Halle (Saale).
This work is a part of the OntoMedRisk project funded by the German
Federal Ministry of Education and Research (reference number 01IS14022).
We acknowledge support from the German Research Foundation (DFG) and
Leipzig University within the program of Open Access Publishing.
Funding
This work has been funded by the German Federal Ministry of Education and
Research (BMBF) in the KMU-innovativ funding program under reference
number 01IS14022 as part of the OntoMedRisk project [7]. The aim of the
funding initiative is the strengthening of innovation capacity of small and
medium sized enterprises in Germany. Nevertheless, the funding body
played no role in the design of the study and collection, analysis, and
interpretation of data and in writing the manuscript.
Availability of data and materials
The datasets generated and/or analyzed during the current study are not
publicly available due to them containing information that could compromise
research participants consent and contrast with the projects funding objective
with the aim of strengthening the innovation capacity of small and medium
sized enterprises in Germany, but are available from the corresponding author
on reasonable request.
Authors contributions
AU designed and implemented RIO, RIOGen and OntoRiDe, made substantial
contributions to conception of the risk notion and developed the methodology
for risk specification. KT focused on the analysis and discussion of recent related
works and contributed to the development of RIOGen and OntoRiDe. HH was
responsible for project management, conception of the risk notion and
semantic foundation of RIO using GFO. JN and TN performed risk assessment
for the otorhinolaryngological use case, interpreted the clinical data regarding
the occurance of adverse situations and described the perioperative and
cross-process risks factors. MS provided expertise for selection of the use case,
medical/technical supervision, and usability assessment. TS has contributed by
inferring/defining the correct key performance indicators and risks in the
otorhinolaryngological medical domain. KS has carried out the definition of
associated risks in a clinical environment, the configuration of the FHIR server
as well as the agent-based system and the mapping of FHIR resources at Jena
University Hospital. AK specified the contents and required functionalities of the
agent system. SFR designed key-functionalities of the agent system and specified
related dependencies. SS designed the architecture, specified the functionalities in
terms of the implementation and implemented the agent system. FP was
responsible for project management and conception of OntoRA. AB developed
the communication between OntoRiDe and OntoRA and the data storage level
of the application. DS developed the user interface and the communication
between server and client. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),
University of Leipzig, Leipzig, Germany. 2Innovation Center Computer
Assisted Surgery (ICCAS), University of Leipzig, Leipzig, Germany. 3Jena
University Hospital, Jena, Germany. 4GMC Systems mbH, Ilmenau, Germany.
5SurgiTAIX AG, Herzogenrath, Germany.
Received: 8 February 2017 Accepted: 30 August 2017
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34
DOI 10.1186/s13326-017-0144-y
RESEARCH Open Access
Investigations on factors influencing
HPO-based semantic similarity calculation
Jiajie Peng, Qianqian Li and Xuequn Shang*
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China.16 December 2016
Abstract
Background: Although disease diagnosis has greatly benefited from next generation sequencing technologies, it is
still difficult to make the right diagnosis purely based on sequencing technologies for many diseases with complex
phenotypes and high genetic heterogeneity. Recently, calculating Human Phenotype Ontology (HPO)-based
phenotype semantic similarity has contributed a lot for completing disease diagnosis. However, factors which affect
the accuracy of HPO-based semantic similarity have not been evaluated systematically.
Results: In this study, we proposed a new framework called HPOFactor to evaluate these factors. Our model includes
four components: (1) the size of annotation set, (2) the evidence code of annotations, (3) the quality of annotations
and (4) the coverage of annotations respectively.
Conclusions: HPOFactor analyzes the four factors systematically based on two kinds of experiments: causative gene
prediction and disease prediction. Furthermore, semantic similarity measurement could be designed based on the
characteristic of these factors.
Keywords: Biological ontology, Semantic similarity, Human phenotype ontology
Introduction
In the last few years, disease diagnosis has greatly ben-
efited from the rapid development of next generation
sequencing (NGS) technologies [13]. However, it is
difficult to make the right diagnosis purely based on
sequencing technologies for many diseases with complex
phenotypes and high genetic heterogeneity. Because the
genetic variants always relate to the complex clinical phe-
notypic characteristics. This kind of relation is difficult to
understand [46].
Recently, tools to measure phenotypic characteristics
have received increasing attention. Patient phenotypes are
defined as the entire physical, biochemical and physiolog-
ical makeup of a patient which determined by both genet-
ically and environmentally [7]. Phenotype data can help
people to understand the relation between the genetic
*Correspondence: shang@nupu.edu.cn
Northwestern Polytechnical University, 127 West Youyi Road, 710072 Xian,
China
variances and biological process activities. Advanced phe-
notype data analysis have played an important role in
explaining gene function and understanding biological
mechanism in biomedical research [811]. One of the
key steps in phenotype data analysis is to precisely mea-
sure the similarity between phenotypes, and combine this
knowledge with the disease diagnosis process to improve
disease diagnosis efficiency. Therefore, a formal and con-
trolled vocabulary is required to unify the representation
of phenotypes and phenotype attributes.
It has been proved in many applications that ontology
is effective to represent biomedical information as terms
and their directed relationships with a directed acyclic
graph (DAG) [1218]. In order to meet the demand, an
ontology called Human Phenotype Ontology (HPO) was
constructed to describe the abnormal human phenotypes
encountered in human Mendelian disease by Robinson
et al. in 2008 [7]. Currently, HPO has been widely used
to provide the unified and structured vocabulary to rep-
resent the phenotypic features encountered in human
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 62 of 79
diseases [19]. HPO is always combined with next gen-
eration sequencing data analysis to support the human
disease diagnosis [20, 21].
In order to improve diagnostic efficiency, many compu-
tational methods have been proposed to measure the phe-
notypic similarity between patient and historical disease
data (or genes) [22, 23]. Among these computational mea-
surements, calculating HPO-based phenotype semantic
similarity has played an important role in completing
disease diagnosis process.
Recently, several measurements have been developed
to compute HPO-based phenotype semantic similarity
[2325]. Although ontology-based semantic similarity
measurement has been extensively studied in the last ten
years [2633], it is still a difficult task to measure the
phenotype similarity based on HPO structure and anno-
tations. The reason is that many factors could affect the
accuracy of HPO-based phenotype semantic similarity,
such as the number of annotations per gene/disease, the
evidence code of annotations, the coverage of annotations
and the quality of annotations [22].
To figure out how different factors affect the perfor-
mance of ontology-based semantic similarity measure-
ment, some methods have been proposed to evaluate
different involved factors. To test whether different edi-
tions of Gene Ontology (GO) would result in different
semantic similarities, Gillis et al. proposed an evaluation
framework based on protein interaction networks [34].
The result shows that 3 and 20% of genes are not semanti-
cally similar to themselves between monthly GO editions
and between biennially GO editions. The semantic sim-
ilarities are only stable over short-period GO editions.
Skunca et al. proposed a novel method to systematically
evaluate the quality of the computationally inferred GO
annotations [35]. The reliability of electronic GO anno-
tations is defined as the proportion of electronic annota-
tions confirmed by the experimental annotations in the
future release of GO. The coverage is defined as the pro-
portion of experimental annotations predicted by the elec-
tronic annotations in an older release of GO. The result
shows that the electronic GO annotations have high qual-
ity, which could lead to accurate semantic similarity. Both
of the aforementioned methods are based on the histori-
cal versions of ontology. These methods cannot be used to
evaluate the factors that affect the performance of HPO-
based semantic similarity measurement, since the histor-
ical versions of HPO are not available currently (personal
communication with the founder of HPO). Furthermore,
other factors may also affect the accuracy of HPO-based
semantic similarity. First, HPO contains large numbers of
annotations with different evidence code indicating the
different levels of evidences supporting the annotation.
Second, HPO is a growing data source. The coverage and
quality of annotations may vary with the updating of HPO
data source. Third, the number of HPO terms annotating
different diseases/genes may be different. These factors
are all related to the HPO-based semantic similarity calcu-
lation. It is difficult to evaluate each factor individually. It
is challenging and demanding to test whether these factors
would affect the accuracy of HPO-based semantic simi-
larity. The evaluation of different factors may guide the
design of HPO-based semantic similarity measurement
and support the human disease diagnosis. However, to the
best of our knowledge, no method has been proposed to
evaluate the factors that affect the accuracy of HPO-based
semantic similarity.
In this article, we proposed a new framework named
HPOFactor to evaluate the effect of four factors involved
in the HPO-based semantic similarity calculation sep-
arately. The contribution of our present study are as
follows.
 To the best of our knowledge, HPOFactor is the first
framework that is specially designed for evaluating
the factors involved in HPO-based semantic
similarity calculation;
 We develop a method to generate different versions
of the HPO annotations with different coverage and
quality levels;
 We test the minimal size of annotation set that does
not affect the accuracy of HPO-based semantic
similarity.
Methods
We proposed HPOFactor, a new framework to evalu-
ate the factors that affect the performance of phenotype
semantic similarity measurement based on human phe-
notype ontology (HPO). The proposed framework has
four parts. First, it tested whether changing the size
of phenotype annotations would affect the performance
of phenotype semantic similarity measurement. Second,
it tested whether using annotations with different evi-
dence codes would affect the performance of phenotype
semantic similarity measurement. Third, it tested whether
changing the annotation coverage would affect the per-
formance of phenotype semantic similarity measurement
by randomly deleting the HPO annotations. Last, it tested
whether varying the quality of HPO annotations would
affect the performance of phenotype semantic similarity
measurement by randomly swapping the existing annota-
tions of different HPO terms. The diagram of the whole
framework is shown in Fig. 1.
Calculating HPO-based semantic similarity
HPO provides a structured and controlled vocabulary to
describe the human phenotypes and the genes/diseases
associated with the phenotypes [7]. Using the unified
description from HPO, the semantic similarity between
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 63 of 79
Fig. 1 -The workflow of HPOFactor
gene and patient or between disease and patient can be
calculated. Based on the HPO-based semantic similar-
ity, we can predict whether a patient associates with a
gene or has certain disease. For example, we can rank
the candidate genes based on its similarity with the
patient to predict the patient-associated genes. The phe-
notypes of a patient can be observed in clinical treat-
ment and the gene/disease phenotype set can be obtained
from database like HPO. Since the phenotype sets of
patient, gene and disease are all able to be unified by
HPO terms, calculating the similarity between patient and
gene/disease is equal to calculating the similarity between
two sets of HPO terms.
Let P1 and P2 be two phenotype term sets correspond-
ing to a patient and a disease (or gene) respectively. P1
represents a set of phenotype terms of a patient observed
in clinical treatment. P2 represents a set of phenotype
terms of a disease (or gene) obtained from HPO database.
Adopting the approach in [22], the semantic similar-
ity between a patient and a gene (or disease) can be
calculated by aggregating the pair-wise phenotype simi-
larity between terms across P1 and P2. Given two phe-
notype sets, their HPO-based similarity is calculated as
follows.
sim (P1,P2) =12 × simset (P1 ? P2) +
1
2
× simset (P2 ? P1)
(1)
where simset(P1 ? P2) represents the similarity from
P1 to P2. For each phenotype p1 in P1, we calculate the
similarity between p1 and each phenotype in P2. Then
the highest similarity score is selected as the similar-
ity between p1 and phenotype set P2. The average of
all similarities between each phenotype in P1 and P2 is
defined as the similarity from P1 to P2. Mathematically,
simset(P1 ? P2) is defined as follows.
simset(P1 ? P2)=avg
?
? ?
p1?P1
maxp2?P2simterm(p1, p2)
?
?
(2)
where simterm(p1, p2) represents semantic similarity
between two phenotypes p1 and p2. It is noted that the
similarity from phenotype set P1 to P2 is different from
the similarity from phenotype set P2 to P1. Therefore,
Eq. 1 averages the two dissymmetric similarities as the
similarity between two phenotype sets.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 64 of 79
To calculate simterm(p1, p2), let S(p1, p2) be the set of all
common ancestors of p1 and p2. pmin is the term that has
the minimal annotations in S(p1, p2). Given two pheno-
types p1 and p2, their similarity simterm(p1, p2) is defined
as follows.
simterm(p1, p2) = ? log NpminN (3)
where Npmin is the number of annotations of pmin (includ-
ing annotations of itself and its descendants) and N is the
total number of annotations involved in HPO.
Based on this semantic similarity measurement, we will
evaluate the factors that affect HPO-based semantic simi-
larity in the following subsections.
Test the effect of the size of annotation set
In the process of calculating semantic similarity measure-
ment introduced in last subsection, one of the key factors
is the size of annotation set of compared genes or diseases.
The size is usually large in the HPO branches for those
well studied ones. Therefore, the size of annotation set is
not a stable factor in the semantic similarity calculation.
In this subsection, we proposed a method to test whether
the size of annotation set would affect the precision of
semantic similarity.
Given a set of query patients Q, each element q in Q
has an annotation set obtained from clinical treatment
saved as Pq. Given a set of genes/diseases H involved in
HPO database, each element h in H has an annotation set
obtained fromHPO database saved as Ph. We changed the
size threshold of annotations s and calculate the seman-
tic similarity at different thresholds. Given the threshold s,
the detail of the method is described as follows. For each
element h in H, we randomly selected s phenotypes from
Ph, saved as Psh. This step is represented mathematically in
Eq. 4.
Psh = RandomSelection(Ph, s) (4)
For each query patient q inQ, we calculate the similarity
between Pq and Psh for each h in H using Eq. 1. Then, we
ranked all elements in H based on the similarities with Pq
saved as Horder (see Eq. 5).
Horder = Rank
(
H , {y|y = sim(Pq,Psh), h ? H}
)
(5)
At last, we can test whether the known patient associ-
ated element (gene or disease) has a high rank in Horder .
The higher the rank is, the better the performance of the
semantic similarity measurement is.
Test the effect of annotationswith different evidence codes
In HPO, the annotations are supported by different evi-
dences. When the HPO project was initialed, most anno-
tations were extracted from the OMIM database [36]
by parsing the clinical features. These annotations are
labeled by the evidence code IEA representing inferred
from electronic annotation. There are also other evi-
dences, such as PCS representing inferred from public
clinical study and biomedical literature, ICE representing
inferred from individual clinical experience, ITM rep-
resenting inferred by text-mining technique and TAS
representing inferred from traceable author statement.
In this subsection, we test whether using different anno-
tations with different evidence codes would affect the
precision of HPO-based semantic similarity. First, the
annotations in HPO are grouped based on the evidence
codes. Given the annotation setA,Ae represents the anno-
tation set with evidence e. For each evidence code e, we
only use annotations contained in Ae to calculate the
semantic similarity between phenotypes. Given a set of
genes/diseases H, the annotation set of each element h in
H is obtained from Ae, saved as Phe. Similar with the pro-
cess described in last subsection, we rank all elements in
H based on the similarities with the phenotypes of query
patient.
Horder = Rank
(
H , {y|y = sim(Pq,Phe), h ? H}
)
(6)
Finally, we could see which evidence code can lead the best
performance.
Test the effect of annotation quality
To determine whether annotation quality was one of
the factors that control the performance of HPO-based
semantic similarity, we re-ran semantic similarity mea-
surement by varying the quality of HPO annotation. To
this end, we varied the HPO annotation quality by ran-
domly swapping the phenotype-annotation associations
in HPO. For example, assume that d1 ? p1 and d2 ?
p2 are two disease-phenotype pairs randomly selected
from HPO. After the swapping process, we get two new
pairs d1 ? p2 and d2 ? p1 to replace the original
two pairs. Given the original HPO annotation set A, we
can generate a low quality set Au by randomly swapping
the phenotype-annotation associations. To make sure
the quality be decreased, the new generated phenotype-
annotation associations should not be contained in the
set of original HPO phenotype-annotation associations.
u represents different quality levels, such as swapping
20% phenotype-annotation associations, 40% phenotype-
annotation associations.Au has the same size with A but
different quality level. For each low quality level u, we use
the low quality annotation setAu to calculate the semantic
similarity between phenotypes. The annotation set of each
element h in H is got from Au, saved as Phu. Comparison
of the performance of semantic similarity using annota-
tion sets with different quality level could test whether
the annotation quality was a key factor of the HPO-based
semantic similarity measurement.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 65 of 79
Test the effect of annotation coverage
Currently, HPO is not complete. Much unknown knowl-
edge and knowledge in the literature are not included
in the HPO database. Therefore, it is critical to test
whether annotation coverage was a key factor for HPO-
based semantic similarity measurement. To this end, we
randomly delete the annotations from annotation set A
to generate a low coverage annotation set Ac. c repre-
sents different coverage levels, such as randomly delet-
ing 20% of the annotations in A, deleting 40% of the
annotations in A. For each coverage level c, we use
the low coverage annotation set Ac to calculate the
semantic similarity between phenotypes. Given a set of
genes/diseases H, the annotation set of each element h
in H is obtained from Ac, saved as Phc. By comparing
the results on the annotation sets with different cover-
age levels, we can test whether the annotation cover-
age is a key factor for HPO-based semantic similarity
calculation.
Results
Data preparation
The Human Phenotype Ontology (HPO) data used in
our experiment was downloaded from the HPO official
website (http://human-phenotype-ontology.github.io/)
on April 1st, 2016. It includes 459,452 gene annota-
tions and 78,313 disease annotations. HPOFactor was
implemented with Python language.
We used the curated clinical phenotype features in
[22] to generate simulated patients for experiments. The
associated phenotypes, disease causative genes and pene-
trance of each phenotype of the diseases are available in
the dataset. For each disease, we simulated 100 patients.
The simulation process is described as follows. To con-
sider the gender-specificity of phenotypes, we first sim-
ulated the gender of each patient. A random number fg
was generated. Then, the patients gender is assigned as
follows:
{
fg > 0.5 ,male
fg ? 0.5 ,famale (7)
Second, given a phenotype p of a patient, a ran-
dom number rp was generated. Let fp be the pene-
trance of this phenotype associated with the assigned
disease. If rp < fp, the phenotype p was assigned
to the patient. It is noted that each simulated patient
must have at least one phenotype. Finally, 3300 patients
was generated. For each patient, we know its disease
causative gene and associated disease. Therefore, we
adopted the evaluation criterion from [22] to test whether
the causative gene or associated disease of a patient
can be identified based on the HPO-based semantic
similarity.
Evaluation for the size of annotation set
In this experiment, we compared the results of using dif-
ferent sizes of annotation set to identify the disease associ-
ated with the patient. The size threshold s used in Eq. 4 is
from 1 to 10. The result shows that the patient associated
diseases have low ranks when the number of annotations
is small, indicating low performance (see Fig. 2). Partic-
ularly, when s = 1 and s = 2, the ranks of most true
patient associated diseases are lower than the 450. Figure 2
shows that the performance improved with the increase
of the size of annotation set. Noted that the performance
become stable when s > 5.
We also compared the results of using different sizes
of annotation set to identify the causative gene. The
gene annotations in HPO are richer than the disease
annotations (see the Data preparation subsection). To
see the global distribution, we set the gene set thresh-
old s as {1, 5, 10, . . . , 45, 50}. Similar with the result of
identifying disease, the causative genes have low ranks
when the number of annotations is small (see Fig. 3).
When s = 1, the ranks of most causative genes are
lower than 500. It is shown that the performance of
HPO-based semantic similarity improved steadily with
the increase of the number of annotations. The perfor-
mance keeps stable when the size of annotations is larger
than 25.
The result shows an important guidance for the HPO-
based semantic similarity calculation that the result may
be more reliable when the number of annotations is large
enough.
Evaluation for the annotations with different evidence
codes
In this part, we test whether using annotations with dif-
ferent evidence codes would affect result of identifying
the disease associated with the patient. We do not test
the performance for causative gene identification since
the gene annotations in HPO do not have evidence codes
currently. We only compare three evidence codes: IEA,
TAS and PCS, since other evidence codes do not have
enough number of annotations. To avoid the bias result-
ing from the lack of annotation, we did the experiment
on the size of annotations sets which are larger than 5.
We choose the size threshold since the experiment in
last subsection shows that the performance become stable
when s > 5.
Figure 4 shows that using annotations with PCS evi-
dence code performs better than using the annotations
with IEA and TAS evidence code. Specifically, when
the ranking threshold is 5, the ratio of patients for
PCS is 0.993, which is higher than IEA and TAS (the
number is 0.901 and 0.580 respectively). The ratio of
patients for PCS is 0.997, when the ranking threshold
is 10. In comparison, the ratios of patients satisfying
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 66 of 79
Fig. 2 -The rank of disease by changing the size of phenotype annotation set. The x-axis is the number of HPO annotations. The y-axis is the rank of
disease associated with the query patient
Fig. 3 -The rank of causative gene by changing the size of phenotype annotation set. The x-axis is the number of HPO annotations. The y-axis is the
rank of causative gene of the query patient
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 67 of 79
Fig. 4 -The rank of disease by the phenotype with different evidence code. The x-axis is the ranking threshold for the disease. The y-axis is the ratio
of patients satisfying the ranking threshold
the threshold are 0.906 and 0.609 for IEA and TAS
respectively.
Evaluation for the annotation quality
To test the effect of annotation quality to the perfor-
mance of HPO-based semantic similarity, we compared
the results of using annotation sets with different quali-
ties to identify the patient associated diseases (Fig. 5(a)) or
causative genes (Fig. 5(b)). Overall, the result shows that
the performance goes downwith the decrease of the anno-
tation quality in both experiments. It is shows that the
performance decreases significantly when more than 40%
annotations become noise.
In the associated disease identification experiment,
when the ranking threshold is 10, the ratio of patients sat-
isfying the threshold is 0.967 for original annotation set.
In comparison, the ratios of patients satisfying the thresh-
old are 0.933, 0.862, 0.637 and 0.198 for annotation sets
with 20%, 40%, 60% and 80% noise respectively. Further-
more, the statistical test shows that the result for original
annotation set is significantly different with 40%, 60% and
80% set (Tukey test, p-value < 0.05).
Fig. 5 -The rank of disease (a) and causative gene (b) by varying the quality of phenotype annotations. The x-axis is the ranking threshold for the
disease/causative gene. The y-axis is the ratio of patients satisfying the ranking threshold
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 68 of 79
Fig. 6 -The rank of disease (a) and causative gene (b) by changing the coverage of phenotype annotations. The x-axis is the ranking threshold for
the disease/causative gene. The y-axis is the ratio of patients satisfying the ranking threshold
In the causative gene identification experiment, when
the ranking threshold is 10, the ratio of patients satisfy-
ing the threshold is 0.951 for original annotation set. In
comparison, the ratios of patients satisfying the threshold
are 0.909, 0.863, 0.496 and 0.005 for annotation sets with
20%, 40%, 60% and 80% noise respectively. Furthermore,
the statistical test shows that the result for original anno-
tation set is significantly different with 40%, 60% and 80%
set (Tukey test, p-value < 0.05).
Evaluation for the annotation coverage
To test the effect of annotation coverage to the perfor-
mance of HPO-based semantic similarity, we randomly
delete the annotations and use annotation sets with differ-
ent coverage levels to identify the associated disease and
causative genes. The result shows that the performance of
HPO-based semantic similarity decreased with the reduc-
tion of the annotations (Fig. 6(a) and (b)). However, there
was no significant difference when the deleted annota-
tions are less than 60% (Tukey test, p-value > 0.05). It
indicates that HPO-based semantic similarity is more sen-
sitive to the quality of annotations than the coverage of
annotations.
Discussion
In this article, we proposed a novel framework called
HPOFactor to evaluate the factors that may affect the
accuracy of HPO-based semantic similarity. HPOFactor
evaluates four factors involved in the HPO-based seman-
tic similarity: size of annotation set, evidence code of
annotations, quality of annotations and coverage of anno-
tations. Particularly, we found the performance of HPO-
based semantic similarity decreased steadily with the
reduction of coverage and quality of annotations. There
was no significant difference among different coverage
levels (p-value > 0.05), but there was significant difference
among different quality levels (p-value < 0.05), indicating
that quality is more important than coverage. This is
important because not all human diseases and genes are
annotated in current HPO, but existing annotations in
HPO have high quality.
Conclusion
Recently, the rapid development of next generation
sequencing techniques have significantly accelerated dis-
ease diagnosis. However, it remains challenging to make
the right diagnosis formany diseases with complex pheno-
types and high genetic heterogeneity. Hence, HPO-based
phenotype similarity become an important part of com-
pleting disease diagnosis.
The evaluation result can make the HPO-based seman-
tic similarity better used in phenotype-based causative
gene prediction and disease prediction. In the future, we
will evaluate the combination effects of different factors
on HPO-based semantic similarity. Furthermore, we will
design semantic similarity measurement based on the
characteristic of these factors.
Funding
This project has been funded by the National Natural Science Foundation of
China (Grant No. 61332014, 61272121); the Start Up Funding of the
Northwestern Polytechnical University (Grant No. G2016KY0301); the
Fundamental Research Funds for the Central Universities (Grant No.
3102016QD003). The publication costs for this article were funded by
Northwestern Polytechnical University.
Availability of data andmaterials
The datasets during and/or analysed during the current study available from
the corresponding author on reasonable request.
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Ethics approval and consent to participate
Not applicable.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):34 Page 69 of 79
Authors contributions
JP and XS conceived the project; JP and QL designed the algorithm and
experiments; JP and QL wrote this manuscript. All authors read and approved
the final manuscript.
Consent for publication
Not applicable
Competing interests
The authors declare that there are no competing interests.
Published: 20 September 2017
RESEARCH Open Access
Drug target ontology to classify and
integrate drug discovery data
Yu Lin1, Saurabh Mehta1,3, Hande Küçük-McGinty1,2, John Paul Turner5, Dusica Vidovic1,5, Michele Forlin1,5,
Amar Koleti1, Dac-Trung Nguyen7, Lars Juhl Jensen6, Rajarshi Guha7, Stephen L. Mathias4, Oleg Ursu4,
Vasileios Stathias5, Jianbin Duan1,2, Nooshin Nabizadeh1, Caty Chung1, Christopher Mader1, Ubbo Visser2,
Jeremy J. Yang4, Cristian G. Bologa4, Tudor I. Oprea4* and Stephan C. Schürer1,5*
Abstract
Background: One of the most successful approaches to develop new small molecule therapeutics has been to
start from a validated druggable protein target. However, only a small subset of potentially druggable targets has
attracted significant research and development resources. The Illuminating the Druggable Genome (IDG) project
develops resources to catalyze the development of likely targetable, yet currently understudied prospective drug
targets. A central component of the IDG program is a comprehensive knowledge resource of the druggable
genome.
Results: As part of that effort, we have developed a framework to integrate, navigate, and analyze drug discovery
data based on formalized and standardized classifications and annotations of druggable protein targets, the Drug
Target Ontology (DTO). DTO was constructed by extensive curation and consolidation of various resources. DTO
classifies the four major drug target protein families, GPCRs, kinases, ion channels and nuclear receptors, based on
phylogenecity, function, target development level, disease association, tissue expression, chemical ligand and substrate
characteristics, and target-family specific characteristics. The formal ontology was built using a new software tool to
auto-generate most axioms from a database while supporting manual knowledge acquisition. A modular, hierarchical
implementation facilitate ontology development and maintenance and makes use of various external ontologies, thus
integrating the DTO into the ecosystem of biomedical ontologies. As a formal OWL-DL ontology, DTO contains asserted
and inferred axioms. Modeling data from the Library of Integrated Network-based Cellular Signatures (LINCS) program
illustrates the potential of DTO for contextual data integration and nuanced definition of important drug target
characteristics. DTO has been implemented in the IDG user interface Portal, Pharos and the TIN-X explorer of protein
target disease relationships.
Conclusions: DTO was built based on the need for a formal semantic model for druggable targets including various
related information such as protein, gene, protein domain, protein structure, binding site, small molecule drug,
mechanism of action, protein tissue localization, disease association, and many other types of information. DTO
will further facilitate the otherwise challenging integration and formal linking to biological assays, phenotypes,
disease models, drug poly-pharmacology, binding kinetics and many other processes, functions and qualities
that are at the core of drug discovery. The first version of DTO is publically available via the website http://
drugtargetontology.org/, Github (http://github.com/DrugTargetOntology/DTO), and the NCBO Bioportal
(http://bioportal.bioontology.org/ontologies/DTO). The long-term goal of DTO is to provide such an
integrative framework and to populate the ontology with this information as a community resource.
* Correspondence: toprea@salud.unm.edu; sschurer@miami.edu
Equal contributors
4Department of Internal Medicine, Translational Informatics Division,
University of New Mexico School of Medicine, Albuquerque, NM, USA
1Center for Computational Science, University of Miami, Coral Gables, FL, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Lin et al. Journal of Biomedical Semantics  (2017) 8:50 
DOI 10.1186/s13326-017-0161-x
Background
The development and approval of novel small molecule
therapeutics (drugs) is highly complex and exceedingly
resource intensive, being estimated at over one billion
dollars for a new FDA approved drug. The primary reason
for attrition in clinical trials is the lack of efficacy, which
has been associated with poor or biased target selection [1].
Although the drug target mechanism of action is not re-
quired for FDA approval, a target-based mechanistic under-
standing of diseases and drug action is highly desirable and
a preferred approach of drug development in the pharma-
ceutical industry. Following the advent of the Human
Genome, several research groups in academia as well as
industry have focused on the druggable genome i.e. the
subsets of genes in the human genome that express pro-
teins that have the ability to bind drug-like small molecules
[2]. The researchers have estimated the number of drug-
gable targets ranging from few hundreds to several thou-
sands [3]. Furthermore, it has been suggested by several
analyses that only a small fraction of likely relevant drug-
gable targets are extensively studied, leaving a potentially
huge treasure trove of promising, yet understudied (dark)
drug targets to be explored by pharmaceutical companies
and academic drug discovery researchers. Not only is there
ambiguity about the number of the druggable targets, but
there is also a need of systematic characterization and an-
notation of the druggable genome. A few research groups
have made efforts to address these issues and have indeed
developed several useful resources, e.g. IUPHAR/BPS Guide
to PHARMACOLOGY (GtoPdb/IUPHAR) [4], PANTHER
[5], Therapeutic Target Database (TTD) [6], Potential Drug
Target Database (PDTD) [7], covering important aspects of
the drug targets. However, to the best of our knowledge, a
publically available structured knowledge resource of drug
target classifications and relevant annotations for the most
important protein families, one that facilitates querying, data
integration, re-use, and analysis does not currently exist.
Content in the above-mentioned databases is scattered and
in some cases inconsistent and duplicated, complicating data
integration and analysis.
The Illuminating the Druggable Genome (IDG) project
(http://targetcentral.ws/) has the goal to identify and
prioritize new prospective drug targets among likely tar-
getable, yet currently poorly or not at all annotated pro-
teins; and by doing so to catalyze the development of
novel drugs with new mechanisms of action. Data com-
piled and analyzed by the IDG Knowledge Management
Center (IDG-KMC) shows that the globally marketed
drugs stem from only 3% of the human proteome. These
results also suggest that the substantial knowledge deficit
for understudied drug targets may be due to an uneven
distribution of information and resources [8].
In the context of the IDG program we have been devel-
oping the Drug Target Ontology (DTO). Formal ontologies
have been quite useful to facilitate harmonization, integra-
tion, and analysis of diverse data in the biomedical and
other domains. DTO integrates and harmonizes knowledge
of the most important druggable protein families: kinases,
GPCRs, ion channels and nuclear hormone receptors.
DTO content was curated from several resources and the
literature, and includes detailed hierarchical classifications
of proteins and genes, tissue localization, disease associ-
ation, drug target development level, protein domain infor-
mation, ligands, substrates, and other types of relevant
information. DTO content sources were chosen by domain
experts based on relevance, coverage and completeness of
the information available through them. Most resources
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 
DOI 10.1186/s13326-017-0113-5
RESEARCH Open Access
12 years on  Is the NLMmedical text
indexer still useful and relevant?
James Mork* , Alan Aronson and Dina Demner-Fushman
Abstract
Background: Facing a growing workload and dwindling resources, the US National Library of Medicine (NLM)
created the Indexing Initiative project in 1996. This cross-library teams mission is to explore indexing methodologies
for ensuring quality and currency of NLM document collections. The NLM Medical Text Indexer (MTI) is the main
product of this project and has been providing automated indexing recommendations since 2002. After all of this
time, the questions arise whether MTI is still useful and relevant.
Methods: To answer the question about MTI usefulness, we track a wide variety of statistics related to how
frequently MEDLINE indexers refer to MTI recommendations, how well MTI performs against human indexing, and
how often MTI is used. To answer the question of MTI relevancy compared to other available tools, we have
participated in the 2013 and 2014 BioASQ Challenges. The BioASQ Challenges have provided us with an unbiased
comparison between the MTI system and other systems performing the same task.
Results: Indexers have continually increased their use of MTI recommendations over the years from 15.75% of the
articles they index in 2002 to 62.44% in 2014 showing that the indexers find MTI to be increasingly useful. The MTI
performance statistics show significant improvement in Precision (+0.2992) and F1 (+0.1997) with modest gains in
Recall (+0.0454) over the years. MTI consistency is comparable to the available indexer consistency studies. MTI
performed well in both of the BioASQ Challenges ranking within the top tier teams.
Conclusions: Based on our findings, yes, MTI is still relevant and useful, and needs to be improved and expanded.
The BioASQ Challenge results have shown that we need to incorporate more machine learning into MTI while still
retaining the indexing rules that have earned MTI the indexers trust over the years. We also need to expand MTI
through the use of full text, when and where it is available, to provide coverage of indexing terms that are typically
only found in the full text. The role of MTI at NLM is also expanding into new areas, further reinforcing the idea that
MTI is increasingly useful and relevant.
Keywords: Indexing methods, Text categorization, MeSH, MEDLINE, Machine learning, BioASQ
Background
For more than 150 years, the US National Library of
Medicine (NLM) has provided access to the biomedical
literature through the analytical efforts of human index-
ers. Since 1966, access has been provided in the form of
electronically searchable document surrogates consisting
of bibliographic citations, descriptors assigned by index-
ers from the Medical Subject Headings (MeSH®) [1] con-
trolled vocabulary and, since 1975, author abstracts for
many citations.
*Correspondence: jmork@mail.nlm.nih.gov
US National Library of Medicine, 8600 Rockville Pike, Bethesda, USA
The MEDLINE®/PubMed® database (MEDLINE) con-
tains over 23 million citations. It currently grows at the
rate of about 760,000 citations per year and covers over
5600 international biomedical journals in 36 languages.
Human indexing consists of reviewing the full text of
each article, rather than just the abstract or summary,
and assigning Descriptors from theMeSH vocabulary that
represent the central concepts as well as every other topic
that is discussed to a significant extent.
MeSH vocabulary
In the 2015 MeSH vocabulary, there are 27,455 Descrip-
tors, which are often referred to as MeSH Headings (e.g.,
© The Author(s). 2017 COPYRIGHT NOTICE. The article is a work of the United States Government; Title 17 U.S.C 105 provides that
copyright protection is not available for any work of the United States government in the United States. Additionally, this is an
open access article distributed under the terms of the Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0), which permits worldwide unrestricted use, distribution, and reproduction in any
medium for any lawful purpose.
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 2 of 10
Lung). The scope of main heading descriptors may be
refined further by selections from a collection of 83 top-
ical MeSH Subheadings which are also known as Qual-
ifiers (e.g., Lung/abnormalities means that the article is
about the abnormalities associated with the Lung more
than the Lung itself ). In addition the vocabulary con-
tains 225,067 Supplementary Concept Records (formerly
called Supplementary Chemicals) consisting of chemicals,
drugs, proteins, and diseases. Each Supplementary Con-
cept Record is linked to one or more MeSH Heading via
their Heading Mapped to entries (e.g., Achondroplastic
dwarfism is linked to MeSH Main Heading Achondropla-
sia). MeSH Check Tags are a special type of MeSH Head-
ing that are required to be included for each article and
cover species, sex, human age groups, and pregnancy (e.g.,
Male) [2].
Impact of MEDLINE indexing
Since 1990, there has been a steady and sizeable increase
in the number of articles indexed for MEDLINE, because
of both an increase in the number of in-scope articles in
journals that are already being indexed and, to a lesser
extent an increase in the number of indexed journals.
NLM expects to index over one million articles annually
within a few years.
MEDLINE Indexing has been used by librarians and
researchers from its inception in 1879 by John Shaw
Billings [3] and is currently used by an even larger com-
munity through PubMed [4]. PubMed uses the MED-
LINE Indexing as part of their Automatic Term Mapping
query expansion [5] and through their result filtering
which depends on MEDLINE Indexing for determining
species, sex, and ages [6]. Other recent examples of spe-
cific uses of MEDLINE Indexing include the results of
TREC Genomics track (2003  2007) [7] and TREC Clin-
ical Decision Support track (2014 - ongoing) [8] which
show that the judicial use of manual MEDLINE indexing
in faceted retrieval or for query expansion leads to at least
moderate, and in some cases to significant improvements
in Mean Average Precision (MAP). For example, fusion of
an implementation of Okapi BM25 ranking function with
Boolean searches for gene names in MeSH fields resulted
in 71.5% improvement in MAP over the Okapi ranking
function alone and placed third in the 2003 Genomics
track evaluation [9].
To cope with the workload growth that outpaces the
growth of resources, NLM started the Indexing Initiative
project in 1996. This cross-library team is tasked with
exploring and implementing indexing methodologies to
ensure that MEDLINE and other NLM document col-
lections maintain their quality and currency and thereby
contribute to NLMs mission of maintaining quality access
to the biomedical literature.
NLMmedical text indexer
The NLM Medical Text Indexer (MTI) is the main prod-
uct of the Indexing Initiative and has been providing
indexing recommendations based on the MeSH vocabu-
lary since 2002. In 2011, NLM expanded MTIs role by
designating a select set of journals where MTI performs
particularly well as MTI first-line (MTIFL) journals. The
initial list of 14 MTIFL journals has grown to include 230
journals in 2014. In 2014, MeSH on Demand [10] was
developed in collaboration with the NLM MeSH Section
providing a simplified user interface toMTI. In its first full
month of operation, the interface provided MeSH-based
key terms for 140,940 English text documents submitted
to it. MTI was also used on a regular basis between 2002
and 2012 to provide fully-automated keyword indexing for
NLMs Gateway [11] meeting abstract collection, which
was not manually indexed.
MTI produces semi-automated indexing recommenda-
tions based on the MeSH controlled vocabulary and is in
daily use to assist Indexers, Catalogers, and NLMs His-
tory of Medicine Division (HMD) in their subject analysis
efforts. Although mainly used in indexing efforts for pro-
cessing MEDLINE citations [12] consisting of identifier,
title, and abstract, MTI is also capable of processing arbi-
trary text, which is the primary mode of text processed
by the new MeSH on Demand interface. MTI provides
an ordered list of MeSH Main Headings, Subheadings
(MEDLINE processing only), and Check Tags as a final
result.
The NLM Medical Text Indexer (MTI) [13] combines
and ranks terms suggested by three modules depicted
in Fig. 1. Figure 1 also shows the logic flow as text is
processed through the various components of the MTI
system. Each of the major MTI components is very briefly
described below.
MetaMap indexing [14]
A method that applies a ranking function to UMLS
Metathesaurus concepts [15] identified by MetaMap [16].
The Restrict to MeSH [17] mapping algorithm which
finds the closest matching MeSH Heading(s) to a UMLS
Metathesaurus concept is used by MTI to map the UMLS
Metathesaurus concepts identified by MetaMap Indexing
to the required MeSH Descriptors.
PubMed related citations [18]
The related citations of a document are those documents
in theMEDLINE/PubMed database that are themost sim-
ilar to it. MTI simply requests a list of PubMed Unique
Identifiers (PMID) for these related citations that have
been indexed and then extracts the MeSH Descriptors
from each of the citations.
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 3 of 10
Fig. 1MTI processing flow diagram
Machine learning [1921]
Twelve of the 40 MeSH Terms listed in Table 1 that MTI
considers Check Tags (Adolescent; Adult; Aged; Aged, 80
and over; Child, Preschool; Female; Humans; Infant; Male;
Middle Aged; Swine; and Young Adult) are reliably (correct
80.62% of the time) identified using a machine learning
algorithm that is trained on citations in the MEDLINE
database that were indexed in the last three years. These
twelve terms used for Machine Learning are highlighted
in bold text in Table 1.
Once MTI has the set of ranked lists of MeSH Main
Headings produced by the methods described so far, the
various lists must be clustered into a single ranked list of
recommendations through our Clustering and Ranking
Module [22]. Once all of the recommendations are ranked
and selected, MTI has a post processing feature that val-
idates all of the recommendations and adds or removes
select terms based on the targeted end-user. Full end-
to-end processing of MEDLINE citations takes approxi-
mately 30 - 45 seconds depending on citation length and
complexity.
In addition to MEDLINE processing, current uses of
MTI where the filtering and results are specifically tuned
includeMTI First Line (MTIFL) andMeSHonDemand.
The human curation of MTIFL results is called MTIFL
Completion. MTIFL Completion starts with MTIFL pro-
viding the initial indexing for a citation and then a human
indexer completes the indexing process by adding any
missed terms and removing any incorrect terms provided
by MTIFL. The MTIFL Completion citation then goes
through the normal manual review process. MeSH on
Demand [10] is a new use of MTI added in 2014 in collab-
oration with the NLM MeSH Section. MeSH on Demand
is a very simplified interface to the MTI system. The
MeSH on Demand interface allows users to provide any
text (e.g., MEDLINE citation or free text) as input and
provides a list of relevant MeSH Descriptors and MeSH
Supplementary Concepts that summarizes the input text
and a list of the top ten citations related to the text in
PubMed as a result. These results are very heavily filtered
in favour of terms with high confidence. Although these
new uses of MTI are qualitative indicators of its potential
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 4 of 10
Table 1 MeSH terms MTI considers check tag
Adolescent History, 18th Century
Adult History, 19th Century
Aged History, 20th Century
Aged, 80 and over History, 21st Century
Animals History, Ancient
Bees History, Medieval
Cats Horses
Cattle Humans
Cercopithecus aethiops Infant
Chick Embryo Infant, Newborn
Child Male
Child, Preschool Mice
Cricetinae Middle Aged
Dogs Pregnancy
Female Rabbits
Guinea Pigs Rats
History of Medicine Sheep
History, 15th Century Swine
History, 16th Century United States
History, 17th Century Young Adult
All bolded check tags represent machine learning suggested check tags
usefulness, the goal of this work is to quantitatively esti-
mate the MTI use and evaluate the quality of its services
compared to other available tools. This paper presents
our internal log-based evaluation of MTI as well as the
results of evaluating MTI in the BioASQ Challenges. Each
BioASQ Challenge is a series of challenges on biomedical
semantic indexing and question answering with the aim of
advancing the state of the art accessibility for researchers
and clinicians to biomedical text [23].
Methods
To answer the questions of whether or notMTI is still use-
ful and relevant, we have used two different approaches
evaluating MTI from both an internal and an external
viewpoint. We track a large number of statistical mark-
ers for MTI on a monthly basis including how every
single MeSH Heading is performing, how MTI performs
for each journal, how each of the three input meth-
ods (MetaMap Indexing, PubMed Related Citations, and
Machine Learning) performs individually and in com-
binations with the two other methods, how often MTI
recommendations are referred to by the indexers, and how
muchMTI is used other than for providing NLM Indexing
recommendations.
We used the Hooper Measure of Indexing Consis-
tency [24] shown in Fig. 2, to calculate the consistency
percentages for MTI, MTIFL, and previously published
indexer consistency studies by Lancaster [25], Leonard
[26], Marcetich and Schuyler [27], and Funk and Reid [28].
For the purpose of computing the consistency percentages
forMTI andMTIFL, |N| is the human indexer and |M|
is either MTI or MTIFL.
We used the descriptions for the various study cate-
gories found in the Funk and Reid [28] paper to correlate
the appropriate MTI andMTIFL results to the proper his-
torical study categories. We have also used these descrip-
tions to identify equivalent categories from some of the
other historical studies to fill in the results. For exam-
ple: The definition of the Descriptors (DESC) category
from Funk and Reid is equivalent to the Checktags and
Main Headings Only category used in the Lancaster and
Leonard studies.
We do not track how well MTI and MTIFL perform
when identifying the Central-concept main headings, so
we were not able include that metric in our evaluation.
For an external evaluation, MTI participated in the
Large-scale online biomedical semantic indexing task of
the 2013 and 2014 BioASQ Challenges [23]. This task is
designed to parallel the human indexing currently being
done at NLM. During each of the BioASQ Challenges,
MTI was impartially and rigorously compared to systems
developed by a world-wide community of researchers and
industrial teams all performing the same task. We do not
consider evaluation of MTI using manual indexing biased
because we exclude citations that rely on MTI First Line
indexing (MTIFL) from the evaluation and for the cita-
tions included in the evaluation MTI recommendations
are used at the indexers discretion. BioASQ provided us
with solid data on how MTI performance compares to
Fig. 2 Hoopers measure of indexing consistency
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 5 of 10
other state of the art systems and contributes an outside
perspective on MTI. The BioASQ Challenges consisted of
three batches of six weekly sets of data to be processed
for a total of 18 sets each year. Each data set was pro-
cessed by the various systems and the results returned
to the BioASQ organizers within a 24 h period to make
sure none of the citations would have been indexed yet
by an indexer which may have biased the results. MTIFL
and later default MTI were used as baselines through-
out the BioASQ Challenges. A winner was picked for
each of the three batches based on the best performing
single run of the six possible runs for each batch. So,
each BioASQ Challenge had three identified winning sys-
tems, one for each of the three batches. Participants were
not required to participate in all of the runs during the
BioASQ Challenge.
Results
Is the NLMmedical text indexer used?
The contract indexers are paid by the article indexed; if
they did not feel MTI was useful, they would simply stop
referring to the recommendations made by MTI. A recent
quote from one of the indexers nicely illustrates the use-
fulness of MTI: . . . from our perspective, its not so much
that MTI is STILL useful to the task of indexing, its that
it is increasingly very useful to the task of indexing . . . there
has been a real shift in perspective on MTI. Indexers used
to view it as not helpful . . . now (most) view it as extremely
helpful and overall very accurate. Figures 3 and 4 illus-
trate how daily requests of MTI by the indexers have
continually increased from 15.75% of indexing production
(299.78 average daily requests) in 2002 to 62.44% of index-
ing production (2997.40 average daily requests) in 2014,
an almost 10-fold increase. This continued and steadily
increasing use of MTI by the indexers indicates that they
still consider MTI to be useful for their task of indexing.
Another measure of whether or not MTI is useful and
relevant is monitoring its use outside of the NLM index-
ing purposes. Table 2 details the number of MTI requests
for 2012, 2013, and 2014 excluding any of our usage. We
capture the total number of items: either free text or
MEDLINE citations that were processed by MTI; number
of MeSH on Demand requests (only available for 2014),
and the number of different domains that the web requests
come from. These numbers include web requests through
our Interactive MTI web page, Batch MTI web page, Web
API interface, and the new MeSH on Demand interface.
These numbers do not include the daily MTI and MTIFL
processing of MEDLINE citations, our BioASQ process-
ing, or the testing that is done for the NLM indexing
efforts.
A number of outside researchers, authors, and institu-
tions around the world use MTI and MeSH on Demand
for various reasons. We do not track who is using our
systems or what they are processing, so the only way we
know what people are doing with our tools is by inter-
acting with them when there are questions or they need
assistance. We know from these interactions that people
are using MTI, MTIFL, and MeSH on Demand to identify
MeSH keywords for biomedical related course materials,
MeSH keywords for their research papers, and to help
summarize text they are working with.
Is the NLMmedical text indexer relevant?
We only started tracking MTI performance statistics in
2007. In 2007, MTI statistics showed Precision of 0.3019,
Recall of 0.5163, and F1 of 0.3810. In 2014, the MTI
statistics show significant improvement in Precision and
F1 with modest gains in Recall reflecting our focus on
improving MTI Precision over the years: Precision of
0.6003 (+0.2992), Recall of 0.5617 (+0.0454), and F1 of
0.5807 (+0.1997). Figure 5 illustrates the performance
changes of MTI between 2007 and 2014 using Precision,
Recall, and F1 measures. Figure 5 also shows MTIFL F1
results between 2011 and 2014. It is clear from Fig. 5 that
journals added to the MTIFL program are some of the top
performers with the F1 score (0.7018) dramatically higher
than the overall MTI performance (0.5807).
MTI Referenced as a % of Indexing Production
Fig. 3 Percentage of indexing production referenced via MTI
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 6 of 10
Average Indexer Daily Use of MTI
Fig. 4 Average daily usage of MTI by indexers
The MTI statistics for 2014 also show that MTIs con-
sistency with the human indexers is comparable to the
available indexer consistency studies. Table 3 details how
both MTI and MTIFL compare with the previously pub-
lished indexer consistency studies. Table 3 includes infor-
mation on when each study was performed, how many
articles were involved in the study, and where available
what percentage of consistency was observed using the
Hooper Measure of Indexing Consistency [24]. Each of
the included study categories is described below using the
Funk and Reid [28] descriptions as a basis and updat-
ing the details to conform to todays MeSH and Indexing
practices:
 Checktags (CT): Checktags are a special type of
MeSH term required to be included for each article
and cover species, sex, human age groups, historical
periods, pregnancy, and various types of research
support (e.g., Male).
 Geographics (GEO): These are MeSH terms from
the Z (Geographicals) MeSH Tree (e.g., Paris, Indian
Ocean).
 Descriptors (DESC): All MeSH terms including
Geographicals and Checktags. These were called
Checktags & Main Headings Only in the Lancaster
[25] and Leonard [26] studies.
 Main headings (MH):MeSH terms which are not
Geographicals or Checktags (e.g., Lung).
 All main headings (no Checktags):MeSH terms
including Geographicals, excluding Checktags.
Table 2 MTI web usage statistics 2012  2014
2012 2013 2014
MTI Requests 44,970 42,919 87,549
# Items processed 3,148,431 7,963,477 11,294,998
MeSH on demand requests   225,750
# Different domains 118 124 147
The MTI and MTIFL sets in Table 3 include results for
all of the citations completed between November 2013
and November 2014 (one standard indexing year). The
MTIFL set of 27,068 documents is included in the MTI
superset of 673,125 documents.
We also have anecdotal evidence from the NLM Index-
ing staff stating their feeling is that new indexers are com-
ing up to speed and being more productive faster due in
part to MTIs recommendations. The MTI recommenda-
tions help new indexers who are not yet as familiar with
the entire set of 27,000+ terms in theMeSH Vocabulary as
more experienced indexers by providing suggestions they
may not be aware of and helping them to limit the scope
of terms they might be looking to use. We also have more
experienced indexers who rarely, if ever, use MTI recom-
mendations because they are able to index faster without
referring to the recommendations.
External evaluation
MTI was used as the baseline system in the 2013 and 2014
BioASQ Challenges. MTI performed well in both chal-
lenges ranking within the top tier teams. Tables 4 and 5
highlight the results of the 2013 and 2014 BioASQ Chal-
lenges respectively. The statistics shown in Tables 4 and
5 are unofficial results based on snapshots taken of the
BioASQ Results web page on the given dates identified
for each table. Tables 4 and 5 both contain the results for
the winning team, MTIFL, and MTI. We have included
the number of articles completed during each batch, the
System Name as provided by the competitors, Precision,
Recall, and F1 measure for each of the winning systems
and the results for both MTI and MTIFL. Please note that
the default MTI results were not included as a baseline
until the third batch of the 2013 BioASQ Challenge - up
to that point we only provided baseline results based on
MTIFL filtering.
In each of the BioASQ Challenges, MTI and MTIFL
were very competitive with the winning systems. In 2013,
the largest difference in F1 between the winning system
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 7 of 10
Fig. 5MTI and MTIFL performance 2007  2014
and MTI/MTIFL was 0.0256 (0.5793  0.5537 in batch 2).
In 2014, the difference in F1 between the winning system
and MTI/MTIFL was a little wider at 0.0453 (0.6317 
0.5864 in batch 3).
Discussion
The five-fold increase in MTI use by NLM Indexers and
the MTI Web Usage statistics detailed in Table 2 pro-
vide an indication of how relevant MTI is by showing
an increasingly high demand for MTI recommendations.
The important thing to note here is that the requests
for MTI processing come from researchers, authors, and
institutions around the world. For 2014, the data show a
significant increase in the number of requests for MTI
recommendations and a wider audience of users across
more domains. In 2014, we also added a new access point
to MTI with the MeSH on Demand interface which is
already showing high use. These usage statistics show a
sustained and increasing demand for MTI which is a very
strong indication that MTI is still relevant.
The MTIFL consistency results in Table 3 (described in
the Results section) echo the performance gains we see
in Fig. 5 when compared to MTI and reflect the fact that
only journals where MTI performs very well are added
to the MTIFL program. The MTIFL consistency results
come close to the Funk and Reid [28] consistency results
and the differences may simply reflect the large disparity
in the number of articles involved (760 vs 27,068).
MTI and MTIFL performance in the BioASQ Chal-
lenges and the fact that both were designated as baselines
for the Challenges show that MTI is still relevant.
The benefits of having a challenge like BioASQ pushing
systems to improve is evident by how much improvement
in performance the winning system, MTI, and MTIFL
show over the first BioASQ Challenge. The highest F1
measure for a winning system in 2013 was 0.5816 while in
2014 it was increased to 0.6317 (+0.0501) [23]. MTI and
MTIFL did not show improvement in F1, but, did have
improvements in Precision from a high of 0.6127 in 2013
to a high of 0.6400 (+0.0273) in 2014 reflecting our push
to focus on improving Precision over Recall the last few
years in both MTI and MTIFL.
The benefits of participating in the 2013 and 2014
BioASQ Challenges for MTI were two-fold:
1. MTI was rigorously and without bias compared to
systems developed by a world-wide community of
researchers and industrial teams all performing the
same task.
2. The challenges provided a forum for the free
exchange of methods and ideas allowing the MTI
team to incorporate the best practices explored by
Table 3 Inter-indexer consistency statistics - past and present studies
Marcetich & Schuyler
Lancaster Leonard Manual Computer Funk & Reid MTI MTIFL
Year of study 1968 1975 1981 1981 1983 2014 2014
Number of articles 16 100 50 50 760 673,125 27,068
Checktags (CT)     74.70% 62.01% 70.91%
Geographics (GEOG)     56.60% 41.52% 57.24%
Descriptors (DESC) 46.10% 48.20%   55.40% 40.85% 53.97%
Main headings (MH)     48.20% 35.17% 48.89%
All main headings (no Checktags)   39% 43%  35.29% 49.12%
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 8 of 10
Table 4 2013 BioASQ results as of October 21, 2013 for winning
system and MTI/MTIFL
Batch # Articles System name Precision Recall F1
1 10,681 System3 0.5602 0.5735 0.5668
MTIFL 0.5940 0.5196 0.5543
2 11,808 System1 0.5921 0.5670 0.5793
MTIFL 0.6127 0.5050 0.5537
3 9828 MTI 0.5610 0.6193 0.5887
MTIFL 0.6027 0.5653 0.5834
System1 0.5873 0.5760 0.5816
the participating teams. Incorporating some of these
approaches into the MTI workflow in 20132014
improved the Precision of MTI indexing suggestions
by 4.44% (Recall was improved by 0.08% and F1 by
2.23%) [29, 30].
Participating in the BioASQ Challenges also provided
us with a renewed interest in machine learning. The 2013
winning system developed by Tsoumakas, et al. [31] was a
purely machine learning system. In the past, we ran sev-
eral experiments [1921] to see if machine learning might
be able to assist MTI and found it to be successful for
a handful of MeSH Terms. During our experiments, we
ran into problems with unbalanced training sets due to
the infrequency of most of the MeSH Terms where we
have a very small set of positive examples in compari-
son to the set of negative examples. In the end, only the
results for some of the most frequently used MeSH Terms
were viable enough to incorporate into MTI. In the first
BioASQ Challenge, we learned that Tsoumakas et al. were
able to successfully overcome this problem and performed
slightly better than MTI in most of the weekly sets as
shown in Table 4 (described in the Results section).
Another interesting topic from the BioASQ Chal-
lenges that we had not pursued before with MTI but
Table 5 2014 BioASQ results as of August 5, 2014 for winning
system and MTI/MTIFL
Batch # Articles System name Precision Recall F1
1 17,061 Asclepius 0.5958 0.5923 0.5941
MTI 0.5908 0.5614 0.5757
MTIFL 0.6284 0.5199 0.5690
2 17,073 Antinomyra SYS1 0.6189 0.5863 0.6022
MTI 0.6012 0.5621 0.5810
MTIFL 0.6176 0.5367 0.5743
3 18,256 Antinomyra SYS1 0.6527 0.6120 0.6317
MTI 0.6099 0.5646 0.5864
MTIFL 0.6400 0.5257 0.5773
which proved beneficial in the BioASQ Challenges was a
learning-to-rank method used by Mao and Lu [32, 33].
Our analysis of the MTI recommendations not provided
to the indexers shows that MTI incorrectly assigns low
scores and removes many of the actual indexing terms
used by the human indexers. The learning-to-rank algo-
rithms seem to identify these abandoned and ignored
terms allowing the system to move them up higher in the
ranked list. In fact Mao and Lu used the MTI results as
one of their features in their approach.
The winning system in the second and third batches
of the 2014 BioASQ Challenge (Antinomyra) was devel-
oped by Liu et al. [34], their system combines the support
vector machines explored by Tsoumakas et al. [31] and
the learning-to-rank approach by Mao and Lu [32, 33]
into a system that outperformed either approach indi-
vidually as shown in Table 5 (described in the Results
section).
Competing in the BioASQ Challenges also provided the
impetus for us to explore why MTI was missing some of
the terms that the human indexers use. The main rea-
son we found for missing the most frequently occurring
MeSH Terms (Check Tags) was that the necessary infor-
mation was contained in the full text available to indexers,
but not in the Title or Abstract that MTI was using to
compute its recommendations. This specific information
tends to be found in the Methods section of the full
text where the authors describe how their experiments
were structured. Usually this is where we see informa-
tion on the type of experiment subjects (Animal, Humans,
or both), sex of the subjects (Male or Female), age of
the subjects (Infant, Newborn; Infant; Child, Preschool;
Child; Adolescent; Young Adult; Adult; Middle Aged; Aged;
and Aged, 80 and over), and if an Animal study, what
kind of animals (Mice, Rats, Hamsters, etc.). A simple
example of this can be seen in Fig. 6 where we have high-
lighted the descriptions of the experiment subjects in the
Title, Abstract, and Full Text. For PMID 24000132, Fig. 6
illustrates how the author provided only a very general
description of rats for the experiment subjects in the
Title and Abstract and nothing about what sex the rats
were, or what specific type of rats they were. The full text
on the other hand includes very specific information in the
Methods section of the paper letting us know the sub-
jects were Male Sprague-Daley rats in the experiment.
This information from the full text is critical to MTI
because recommending just Ratswould only provide one-
third of the correct answer. The human indexer would use
Male, Rats, and Rats, Sprague-Dawley.
Future work
We are currently looking at several ways to incorporate
machine learning and learning-to-rank either intoMTI, or
as a starting point for a next generation MTI.
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 9 of 10
Fig. 6 Title and abstract versus full text example (PMID: 24000132)
One very promising approach we are investigating is
to use Wilbur and Kims Stochastic Gradient Descent
approach [35] as a starting point for a next generationMTI
and then add in lookup lists, machine learning, indexing
rules, and filtering from the existingMTI system. The pre-
liminary indications are encouraging showing that the two
systems are in fact complementary.
Mao and Lu [32, 33] are also seeing very good results
with their learning-to-rank algorithm which uses MTI as
one of the features. We are currently working with them
to see if MTI can use their ranking results to try to salvage
some of the abandoned MTI recommendations.
We intend to start working with full text (e.g., from
PubMed Central) to see if we can improve MTI perfor-
mance with a focused look at the full text. Only 10% of
the articles MTI processes have XML full text in PubMed
Central, but it would provide us with data to explore
full text.
MTI is also being considered to possibly expand its role
by assisting with mapping OLDMEDLINE [36] terms to
the latest version of the MeSH Vocabulary for citations
originally printed in hardcopy indexes published prior
to 1966, and the possibility of providing keywords for
citations that normally would not be humanly indexed
to provide additional access points that would assist in
retrieval.
Conclusion
After twelve years and two BioASQ Challenges it was a
perfect time to look around and perform a reality check
to determine if MTI was indeed still useful and rele-
vant. In this paper we have presented several qualitative
and quantitative reasons why we think that MTI is in
fact still useful and relevant. The statistics on how much
MTI is used by the indexers and by people outside of the
US National Library of Medicine show that MTI usage
continues to grow. The unbiased external review of MTI
by the BioASQ Challenges where MTI provided two of
the baseline systems showed us that MTI is still one of
the benchmarks for biomedical semantic indexing; but it
also proved that we have room for improvement, and even
provided possible research avenues to make some of those
improvements to MTI. For the first time, the BioASQ
Challenges also provided us with a third-party mechanism
to compare MTI against other world-class systems in an
unbiased and principled manner.
Abbreviations
CT: MeSH check tag; MeSH: Medical Subject Headings; MH: MeSH main
headings, also described as descriptors; HMD: US National Library of Medicine
History of Medicine Division; MTI: NLM medical text indexer; MTIFL: NLM
Medical text indexer first line; NLM: US National Library of Medicine; PMID:
PubMed unique identifier; SH: MeSH subheadings, also described as qualifiers
Acknowledgements
This work was supported by the Intramural Research Program of the National
Institutes of Health and the National Library of Medicine. We would like to
thank our colleagues François Lang and Willie Rogers for providing direct and
indirect support of MTI. We would also like to extend special acknowledgment
to Hua Florence Chang who was the original creator of MTI. Florences
foresight has provided us with a robust and tunable program. We would also
like to take this opportunity to thank George Paliouras and the entire BioASQ
Team for organizing the BioASQ Challenges and providing the opportunity to
evaluate MTI. Finally, we would like to thank the NLM indexers and Indexing
staff for their continued support and collaboration they have provided over
the last twelve years teaching the MTI team how they index and ensuring that
MTI succeeds.
Authors contributions
All authors contributed to the design and implementation of the various
experiments included in this paper. All authors have participated in writing
and editing the manuscript. All authors discussed, read and approved the
manuscript. JGM is the lead developer for the MTI system.
Competing interests
The authors all declare that they have no competing interests.
Received: 29 June 2016 Accepted: 11 January 2017
Mork et al. Journal of Biomedical Semantics  (2017) 8:8 Page 10 of 10
RESEARCH Open Access
SNPPhenA: a corpus for extracting ranked
associations of single-nucleotide
polymorphisms and phenotypes from
literature
Behrouz Bokharaeian1*, Alberto Diaz1, Nasrin Taghizadeh2, Hamidreza Chitsaz3 and Ramyar Chavoshinejad4
Abstract
Background: Single Nucleotide Polymorphisms (SNPs) are among the most important types of genetic variations
influencing common diseases and phenotypes. Recently, some corpora and methods have been developed with
the purpose of extracting mutations and diseases from texts. However, there is no available corpus, for extracting
associations from texts, that is annotated with linguistic-based negation, modality markers, neutral candidates, and
confidence level of associations.
Method: In this research, different steps were presented so as to produce the SNPPhenA corpus. They include
automatic Named Entity Recognition (NER) followed by the manual annotation of SNP and phenotype names,
annotation of the SNP-phenotype associations and their level of confidence, as well as modality markers. Moreover,
the produced corpus was annotated with negation scopes and cues as well as neutral candidates that play crucial
role as far as negation and the modality phenomenon in relation to extraction tasks.
Result: The agreement between annotators was measured by Cohens Kappa coefficient where the resulting scores
indicated the reliability of the corpus. The Kappa score was 0.79 for annotating the associations and 0.80 for the
confidence degree of associations. Further presented were the basic statistics of the annotated features of the
corpus in addition to the results of our first experiments related to the extraction of ranked SNP-Phenotype associations.
The prepared guideline documents render the corpus more convenient and facile to use. The corpus, guidelines and
inter-annotator agreement analysis are available on the website of the corpus: http://nil.fdi.ucm.es/?q=node/639.
Conclusion: Specifying the confidence degree of SNP-phenotype associations from articles helps identify the strength of
associations that could in turn assist genomics scientists in determining phenotypic plasticity and the importance of
environmental factors. What is more, our first experiments with the corpus show that linguistic-based confidence
alongside other non-linguistic features can be utilized in order to estimate the strength of the observed SNP-phenotype
associations. Trial Registration: Not Applicable
Keywords: SNP, Phenotype, Relation extraction, Negation, Modality, Degree of confidence
* Correspondence: behrou.bo@usm.es
1Facultad informatica, Complutense University of Madrid, Calle Profesor José
García Santesmases, 9, 28040 Madrid, Spain
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 
DOI 10.1186/s13326-017-0116-2
Background
Background
An SNP is a single base mutation occurring at the DNA
level. Variations in DNA sequences can affect how humans
develop diseases and respond to pathogens, chemicals,
drugs, and other agents [1]. There exist an approximate ten
to thirty million SNPs in humans [2]. As a result of the in-
creasing number of related articles, the use of automatic as-
sociation extraction in determining the associations of
mutations (e.g. SNPs) and their consequences is increasing
in biological systems and genotype-phenotype studies.
In genetic epidemiology, GWA study refers to the
process of examining several common genetic variants
in different people so as to discover a possible correlation
between a variant and a phenotype trait. A phenotype is
an organisms recognizable characteristics or traits such as
its development, biochemical or physiological properties,
behavior, and the concomitant products of that behavior
[3]. The large amount of data generated from these studies
[4] necessitates the need to develop an automatic ap-
proach in order to facilitate the study of the extracted as-
sociations. Recently, a few corpora and methods have
been developed with the aim of extracting mutation and
disease associations from texts such as [5] and [6]. There
is, on the other hand, no available corpus for extracting
the association of SNP-phenotypes from texts annotated
with negation, modality, and the confidence degree of
such associations. The need for different levels of annota-
tion for biomedical associations has been considered in
certain biomedical resources such as PharmGKB [7]. It
collects information about the impact of human genetic
variations in drug responses that have been annotated
with four levels of evidence.
In this paper, we described and discussed the process
of constructing ranked SNP-phenotype association corpus
(SNPPhenA), inter-annotator agreement analyses and the
results of some utilized baseline methods during an initial
experiment. In most cases, implementing a biomedical
text-mining system is a difficult task as the basic scientific
communication components  i.e. journals and data-
bases  are designed to be read by humans, not ma-
chines or computers. In order to address this problem,
xml was selected as the main format for the produced cor-
pus. Furthermore, biomedical Natural Language Process-
ing (BioNLP) systems (e.g. relation extraction) have been
mostly applied to abstracts as, though concise, they are
more readily available. Also, abstracts are deemed as good
targets for information extraction (IE) because they are a
succinct and summarized version of an article [8], hence
the selection of abstracts in the present research.
Motivation
Several named entities have been investigated during the
biomedical relation extraction task, few of which are
suitable candidates for annotating with confidence de-
grees, which is the major aim of the research when
identifying the strength (severity) of associations or in-
teractions. The reason for this is that there are no ad-
equate biomedical agreements. For instance, Drug-drug
Interactions (DDI) or Protein-protein Interactions (PPI)
are two biomedical relations discussed by a myriad of
researchers. However, it is difficult even for a human
expert to reliably classify the strength or severity of
DDIs or PPIs according to confidence level, a problem
existing due to the variation in the types of related ex-
periments and the paucity associated with the methods
of quantifying and estimating the significance of both
the research method and the association. Most GWA
studies that report SNP-phenotype associations are
generally based on case-control researches [9] initially
tested for statistically significant differences between
the proportion of exposed subjects among cases and
controls. Accordingly, to gauge the research significance
of the result, researchers are encouraged to, more often
than not, report a level of evidence by considering p-
values and study size.
Both preparing a reliable corpus annotated with confi-
dence level in associations and developing an automated
tool for this purpose are evidently more difficult for a
host of other biomedical named entities that may require
different models of study [7]. For instance, comparing
and finding an acceptable agreement of confidence level
for an association reported in a case-control experiment
beside to a case study reported association would be
more difficult and challenging. In addition, it is difficult
to identify the strength and severity of associations (or
interactions) in a sentence explaining a biochemical
mechanism occurring in many corpora such as DDI and
Protein-related associations because every chemical reac-
tion may precipitate different sequences within the body.
Consequently, insofar as NLP, ranked SNP-phenotype
association extraction based on confidence level is con-
sidered to be a more feasible task in comparison with
many other biomedical association extraction tasks.
Additionally, it is worth mentioning that specifying neu-
tral candidates and the effects of negation annotated in
the corpus is influenced by measured confidence level of
association between two entities, elaborated in the follow-
ing sections. This shows how crucial it is to have reliable
annotations for confidence level in associations as well as
an automated method for identifying them.
Yet another objective of the present was to identify
the association of such phenotypes as quantitative traits
instead of diseases with SNPs, variously studied by re-
searchers. Such extension is significant because many
phenotypes can be detected during the sub-clinical phase
of a disease history, hence determining their association
with an SNP entails a more early diagnosis and treatment
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 2 of 13
of the disease. Certain phenotypes, it should be noted, are
important risk factors for the disease.
Related tasks and phenomena
One of the linguistic-based phenomena discussed in this
paper is negation. According to linguistics [10], neg-
ation refers to a morphosyntactic operation wherein a
lexical item or construction is denied or whose meaning
becomes inverted by another lexical item. Likewise, the
lexical item representing the negation is referred to as
the negator. Commonly used in clinical and biomedical
text documents, negation is a significant cause of low
precision in automated information retrieval systems. In
the prepared corpus, the marked sentences were anno-
tated with negation scopes and cues. A sample of a ne-
gated sentence can be found in Fig. 1, wherein the SNP
and phenotypes are written in bold font.
The other linguistically-driven phenomenon employed
here is linguistic modality. Generally, modal expressions
are words that state modality which is the expression of
the subjective attitudes and opinions of the presenter
about a possible fact or to control a probable action
including intentions, possibility, probability, necessity,
obligation [11]. In this research, linguistic-based modals
and speculation analyses were made use of in order to
determine the confidence level of the SNP-phenotype
association candidates in the corpus. The linguistic-based
confidence level of an extracted biomedical association
can provide an estimate for the reliability of the obtained
association and the strength of the biomedical association.
Figure 2 demonstrates the sample of a sentence in the
corpus with three modality markers. The modality ana-
lysis of a sentence and the linguistic-based confidence
level of associations can be utilized in addition to other
non-linguistic features so as to obtain more accurate
annotations.
Named Entity Recognition (NER) is the first step to-
wards extracting associations and relations as well as
making related corpora within biomedical texts [12]. It
is crucial to notice that the characteristics of NER in the
biomedical domain are different from those in the news-
wire domain [13]. Identifying mutations in texts is among
the most difficult NER tasks in BioNLP, investigated in a
myriad of studies such as [1416]. EMU is another muta-
tion tagger effective in reducing the annotation time of
articles candidate for mutation related associations [17]. It
should be noted that implementing a state-of-the-art
automated SNP and phenotype NER is not the objective
of this research. Rather, it is the first step toward produ-
cing an association extraction corpus, where, the product
of the automated algorithm is subsequently checked
manually.
The rest of the paper is organized as follows: The next
section reviews some of the related works; section three
presents the methodology of the paper; section four is
dedicated to the evaluation and results; and the last sec-
tion concludes the paper.
Related works
In this section, we are going to introduce some of the
relevant works about preparing the datasets used for
extracting mutation related entities including disease as
well as different methods of annotating negation and
levels of confidence in the biomedical domain.
Mutation association extraction methods and corpora
Besides classical relation extraction tasks in the BioNLP
domain such as protein-protein and gen-disease, certain
novel methods and corpora have been developed with
the aim of extracting mutation/polymorphism and dis-
ease associations, among which, mention can be made
of BRONCO [18] and Variome [19]. BRONCO contains
more than four hundred variants and their associations
with genes, diseases, drugs and cell lines in the context
of cancer, all extracted from 108 full-text articles. Var-
iome covers 12 types of relations annotated in 10 full-
text articles. While BRONCO includes more documents,
both corpora annotate several types of relations, such
as mutation-disease association, as binary relations on
a full-text level. On the other hand, the advantages of
abstract-level relation extraction over full-text were
mentioned in the introduction section. Therefore, the
prepared corpus in this research was provided on an
abstract level.
PKDE4J [5] and Dimex [6] are two methods for
extracting mutation and disease association, the latter
being a rule-based unsupervised mutation-disease asso-
ciation extraction working on the abstract level. The
PKDE4J, however, is a supervised method that employs
a rich set of rules to detect the used features. Both
methods work on usual binary relations that determine
whether or not there exist an association; neither method
considers the degree of certainty or confidence [20].
Fig. 1 A sample sentence in the corpus within a negation cue and scope
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 3 of 13
developed another related miner system that gathers
heterogeneous data from a variety of literature sources
in order to draw new inferences as to the target pro-
tein families. Likewise, Ravikumar and his colleagues
[21] developed an automated extraction tool in order to
obtain protein-specific residue associations from the
literature. Another similar automated approach was
proposed by [22], which extracts impacts and related
information from literature. In another recent study,
Klein et al. proposed the principal infrastructure for the
benchmarking of mutation text mining systems [23].
The corpus prepared in this research was annotated
with negation cues and scopes, modality markers, and
neutral association candidates. Such linguistic features
were conducive to the extraction of more accurate infor-
mation about the extracted SNP-phenotype associations.
Fig. 2 A sample of a sentence with three modality markers
Fig. 3 Different steps for producing the SNPPhenA corpus
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 4 of 13
Annotating the modality and degree of confidence
As mentioned earlier, modality indicates the degree to
which a certain observation is possible, probable, likely,
certain, permitted, or prohibited. A host of studies have
been conducted for the identification of modality and
speculation in NLP; very few, however, have been
employed for the classification of modality language in
bioscience texts.
Although several studies such as [24] have been con-
ducted within the linguistics community as to hedging
in scientific texts, in neither is there direct relevance to
the task of classifying from an NLP and machine learn-
ing perspective.
Light and his colleagues conducted one of the very few
direct studies [25], where the speculation identification
is introduced using examples from the biomedical domain.
They address the question of whether there is sufficient
agreement among researches as to what constitutes a
speculative assertion that renders the task viable from a
computational perspective. Despite the fact that Light at-
tempts to separate the two sides of speculation (strong
and weak), he fails to glean sufficient evidence for such a
reliable distinction. They conclude that having a reliable
distinction between speculative and non-speculative sen-
tences is feasible, and reliable automated methods might
also be developed.
It is noteworthy that in addition to the preponderance
of biomedical relation extraction annotations that merely
include usual binary association information, there exist
certain others containing extra-linguistic information in-
cluding POS, negation, and speculations information. As
an example, the Genia corpus [26], along with biological
events, contains annotations for three levels of uncer-
tainty. Nonetheless, to the best of our knowledge, all of
the mutation related corpora have only been annotated
with binary associations. In the current study, the corpus
was enriched through adding more linguistic information
such as the linguistic based confidence level of associations,
modality markers, and neutral association candidates.
Negation annotation
In general, two negation detection methods have been
developed to annotate the employed corpora: A linguistic-
based approach and an event-oriented approach. Among
other negation annotated corpora, one may refer to the
two most well-known: the linguistically-focused, scope-
based BioScope [27] and the event-oriented Genia [26]. In
BioScope, scopes recognize the position of the key negated
event within the sentence, with each argument of the key
events coming under the scope, as well. Genia, on the
contrary, independently deals with modality within the
events. In a Genia event, biological concepts (relations and
events) are annotated for negation, yet no linguistic cues
are annotated. In fact, the objective of the BioScope cor-
pus is to approach this language phenomenon in a general,
task-independent, and linguistically-oriented manner. It
can further automatically recognize negation scopes and
cues in sentences.
Fig. 4 A sample of SNP and phenotype named entity recognition in the corpus
Table 1 Some of the most occurred phenotypes in the corpus
Phenotype/phenotypic trait Num. of abstracts
health risk 40
smoking 33
Obesity 25
metabolic syndrome 16
hypertension 10
insulin sensitivity 9
hypertriglyceridemia 7
glucose metabolism 6
impaired glucose tolerance 5
longevity 4
body mass intake 4
cognitive performance 4
skin pigmentation 3
AIDS 3
Table 2 Eight of most occurred SNPs in the SNPPhenA corpus
and number of contained abstracts
SNP Number of abstracts
rs12255372 78
rs429358 55
rs7412 46
rs4680 38
rs1051730 25
rs662799 20
rs1799971 18
rs1800629 14
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 5 of 13
NegDDI-DrugBank is another corpus that was anno-
tated by the authors of the previous work with scopes of
negation and negation cues [28]. The automatic extrac-
tion of Drug-Drug interactions from the text is held to
be highly significant, as two corpus versions (in 2011
and 2013) were prepared in this regard. Concerning the
high rate of negated sentences in the DDI corpus, a
complete set of sentences within DDI 2011 (with a total
of 5806 sentences and 579 files) was automatically anno-
tated with negation scopes and cues. The results were,
then, manually checked by three experts to address pos-
sible mistakes within the course of the automated process
[29]. Adding a new XML negation-tag containing negation
cues and negation scopes, the NegDDI-DrugBank corpus
was established.
Corpus construction
In this section, the steps followed in the construction of
the SNPPhenA corpus are explained. The entire process
consists of three major steps of collecting documents,
automatically and manually recognizing the SNP and
phenotypes, and annotating the associations and the re-
lated information (Fig. 3). The last step entails annotat-
ing the association candidates, the confidence level of
associations, the modality markers and the negation
scopes and cues of the sentences.
In order to have consistent annotations, all annotators
were given the same instruction which includes a pellu-
cid definition of the entities and their relationships, rules
and conventions of annotating the confidence level of
associations and complete examples for each type of
tags. The annotation guideline also contains rules for
tackling linguistic phenomena such as negation cues and
modality markers. Moreover, this document presents
different types questions raised and retorted by the an-
notators during the annotation process. The annotation
guideline can be found on the website of the corpus.
In the end, 360 XML files were generated comprised
of the abstract texts, SNPs, Phenotypes, and the SNP-
phenotype associations in the selected sentences. The
Phenotypes, SNP names and the association candidates
were annotated as xml element tags for each nominated
sentence in the abstract. Next, the annotations and the
final product were manually checked. The produced
SNPPhenA corpus is available for public use 1. So as to
better fathom and employ the corpus, brat stand-off
annotation format of the files is also available at the
website of the corpus. The next subsection is dedicated
to the abstracts collection process 2.
Abstract retrieval
Information provided by the http://www.gopubme-
d.org/ search engine was used to collect genome-wide
association abstracts. GoPubMed is a webserver allowing
users to explore PubMed search results with Gene
Ontology [30]. Twenty popular SNPs were used as query
terms enumerated popular by http://www.snpedia.com/
website; the extracted list of abstracts was shortened via
selecting those comprised of popular disease names. The
list was finally truncated again through choosing those
that have candidate sentences consisting of both types of
entities. We collected a total of 360 abstracts (including
2625 sentences) with at least one candidate sentence
with an SNP and a phenotype name. There were 483 key
sentences containing at least one SNP and one pheno-
type name that were annotated with the xml element
Fig. 5 A sample of two annotated associations between two SNPs and a phenotype in the SNPPhenA corpus
Fig. 6 Samples of positive association candidate between highlighted two SNPs and a phenotype
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 6 of 13
SENTENCE. The total number of SNP names anno-
tated in the SNPPhenA corpus was 875. It is worth
mentioning the SNPPhenA is a sentence-level corpus
and sentences merely including SNP or Phenotype were
not annotated.
The next step was to perform an automatic Named
Entity Recognition, followed by a manual checking of
sentences with candidate relations for SNPs and pheno-
type names, as explained in the section below.
Named entity recognition (NER)
An essential part of biomedical NLP is to detect biomed-
ical named entities [31]. During the construction process,
two Named Entity Recognitions were done on SNPs and
Phenotypes. These two tasks are minutely explained in the
two following subsections. A sample of implemented
NERs is shown in Fig. 4.
Phenotype NER
A phenotype is the appearance of an organism in terms
of its morphology, development, physiology, behavior
and its concomitant products [3]. Although there are da-
tabases containing disease names and popular phenotype
names, no compendious database of phenotypes is yet
available.
In this regard, a dictionary-based NER task was imple-
mented by combing two more complete and pertinent
databases. The prepared dictionary includes a list from
the Comparative Toxicogenomics Database (CTD) for
disease names [32]. Also included is the phenotype
ontology prepared in the blast project [33]. The collected
list of phenotypes includes 65,530 phenotype names along
with more than twelve thousand disease names and their
synonyms.
The phenotype names were initially recognized auto-
matically by the prepared dataset. Manual checks were
subsequently made by two experts in order to identify
missed or inexact phenotypes.
A short list of the most frequent phenotypes is shown
in Table 1 where the top two phenotypes in the corpus
are health risk and smoking.
SNP NER
The inconsistent description of biological data elements
renders the relation extraction tasks challenging. Names
associated with polymorphism are particularly problematic
because historical or common names are, more often than
not, employed instead of standard nomenclature [34],
specifically in candidate gene association studies. What
is more, it is hard to find the links between historical
or common SNP names and refSNP [35]. To address
this issue, we implemented a database containing both
refSNP(rs) and historical names, matched with their
corresponding rsID numbers, while utilizing the Variant
Name Mapper(VNM) tool [36]. The VNM tool consists of
historical names matched with their corresponding rsID
numbers extracted from multiple open-access databases,
including SNP500Cancer [37], SNPedia [38], pharmGKB
[39]. The database was utilized for extracting the different
SNP names.
Similar to the phenotype NER process, SNP name an-
notations were initially checked manually by two biology
experts and verified by a third professional annotator. A
short list of the most frequent SNPs is shown in Table 2.
Annotating the candidate SNP-phenotype associations
This section deals with the process of annotating the
associated candidates which includes the annotation of
the SNP-phenotype associations, the confidence level of
associated candidates, modality markers, and negation
scopes and cues in the negated sentences.
Annotating the SNP-phenotype associations
Following the collection of abstracts and the determin-
ation of the SNP and phenotype candidate names, the
associations between SNP and phenotype were manually
annotated by three gurus in genetics (Fig. 5). The SNP-
Fig. 7 Samples of negative association candidate between highlighted six SNPs and a phenotype
Fig. 8 A sample of neutral association candidate with used highlighted entities
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 7 of 13
phenotype candidates were classified into three cat-
egories of positive, negative and neutral. The positive
SNP-phenotype relation candidates are those with
clearly indicated associations (Fig. 6). In contrast, negative
SNP-phenotype relation candidates are those in which a
lack of association is evident (Fig.7). In addition to the typ-
ical classes of relationships, a neutral class is defined for
those that fall between the two other classes, where the
presence or absence of association is not remarked in the
sentence (see Fig. 8).
As Fig. 8 shows, the presence or absence of association
is neither mentioned between rs4689 and anorexia
nervosa, nor can it be identified with a high level of
confidence, hence, the association between the SNP and
the phenotype was annotated as neutral.
In more precise terms, an SNP-Phenotype association
candidate is identified as neutral if:
(i) The absence or presence of association between
SNP-phenotype cannot be specified from the sentence
(or container clause) with a confidence level of more
than zero.
(ii) The status of presence or lack of association be-
tween the SNP and the phenotype does not change from
positive to negative or vice versa if the sentence (or con-
tainer clause) is negated and SNP and phenotype names
are located in the scope of the negation.
(iii) The confidence level of association between SNP
and the phenotype does not change if a modal marker is
utilized in the sentence and both entities are located in
the scope of modality.
The association in Fig. 9, for instance, is neutral and
the used negation cue (no) does not change the status
of the association between the SNP and the phenotypes.
It is worth mentioning that in most relation extraction
corpora, neutral candidates were considered to be part
of the negative (non-positive) class. Considering them as
a separate class of associations allows researchers to con-
duct different types of experiments. More details as to the
role of neutral candidates in biomedical relation extraction
tasks can be found in the authors other study [40].
Similar to the previous steps, the manual checking was
initially performed by two experts, and in order to sort
out the issue of contradictory confidence levels, the ver-
dict of a third expert annotator was taken into account.
Annotating the level of confidence of the SNP-Phenotype
associations
In spite of the fact that genetic components have the in-
structions for the growth and development of each individ-
ual, a persons phenotype is influenced by environmental
factors during embryonic development and throughout life.
Environmental factors can stem from a variety of influences
such as diet, climate, illness and level of stress. For instance,
the capability to taste food is a phenotype estimated, by sci-
entists, to be 85% influenced by genetic inheritance [41].
Nevertheless, environmental factors such as dry mouth or
recently eaten food could affect such ability.
Phenotypic plasticity is the ability of a genotype to
generate more than one phenotype due to various envi-
ronments [42]. The plasticity is considered to be high if
environmental factors have a strong influence. Con-
versely, if the phenotypic plasticity is low, the genotype
can be made use of so as to reliably predict the pheno-
type. The degree of influence environmental factors have
on a persons ultimate phenotype is, not infrequently, a
matter of heated scientific debate.
Fig. 9 A sample of neutral association candidate with a negation cue
Fig. 10 A sample of a strong association that has been mentioned to have a strong degree of confidence
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 8 of 13
Differing phenotypic plasticities alongside possible
unknown genetic components are the two reasons why
GWA study uses confidence level in order to describe
the strength of association. The linguistic-based confi-
dence level of the reported association ultimately yields
informative data leading to the determination of pheno-
typic plasticity.
However, there is no available data source or automated
method for extracting confidence level from the obtained
results. This is when the presence of such a tool and data
source is critical and conducive to reviewing literatures.
For this purpose, the confidence levels of positive asso-
ciation candidates in the corpus were annotated by a
guru in human genetics. Based on the strength of the
linguistic correlation between each individual phenotype
and the relevant SNP mentioned in the abstract, the
confidence level of associations was categorized into weak,
moderate, and strong. Moreover, when the association is
neutral (ASSOCIATION= neutral), the degree of confi-
dence is set to zero. The confidence levels were assorted
considering modality, adverbs and the reported statistical
results (p-value). Detailed information about the annota-
tion guidelines can be seen in the guidelines document,
available on the website of the corpus. The process, all the
same, is demonstrated here via some samples.
The sentence shown in Fig. 10, for example, is consid-
ered to have a high confidence level as it indicates found
a significant genotype effect.
The sample mentioned in Fig. 11, on the other hand,
is annotated as having a weak confidence level because
of the might be clause. However, there exist certain
cases that fall under both two categories such as the
sample below (see Fig. 12), annotated as moderate.
The annotation of confidence level was carried out by
two biology experts both of whom had the same opinion
regarding 86% of the association candidates in the whole
corpus. In order to sort out the issue of contradictory
confidence levels (14%), the opinion of a third guru an-
notator was considered.
Linguistic based negation detection and modality
markers
Identifying negative statements is essential in order to
obtain accurate information from the text data. The sen-
tence in Fig. 13 demonstrates the importance of consider-
ing negation where there is no association between APOE
(rs429358) and bvFTD; however, if the negation had
been neglected, an incorrect association might have been
identified.
A rule-based system, proposed by [43], was initially
utilized in order to annotate the negation scopes and
cues. During the process, a set of negation cues such as
not, lack, were detected making use of Bioscopes
guidelines. Negation cues indicate that a negation exists
in a sentence. Considering the syntactic context, the
scopes of negation and negation cues were subsequently
determined, a task already done in a previous work by the
authors [28] annotating the DrugDDI 2011 corpus. In
order to preclude any possible mistakes, manual checks
were made by an expert following the automated process.
In addition to the negation cue and scopes, modality
markers were annotated during the annotating process.
The employed modality markers obtained from the list
were already provided in [44], which is an extension of
the list provided by [45] for the biomedical domain. The
process includes an automated annotation, followed by
an expert performing the manual check. The five more
frequent annotated modal markers in the corpus are:
suggest, more, strong, observe, and show.
Evaluation and results
In this section, inter-annotator agreement analyses and
the calculated scores are initially presented; then some
of the basic statistics of the produced corpus will be
Fig. 11 A sample of a weak association that has been mentioned to have a weak degree of confidence
Fig. 12 A sample of moderate association that has been mentioned to have a moderate degree of confidence
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 9 of 13
demonstrated; and finally, the results obtained from our
first experiment using the corpus are presented.
Inter-annotator agreement
In order to evaluate the quality of the corpus and the re-
liability of the annotations, the inter-annotator agree-
ment score was measured for the task of classifying
candidate sentences into positive, negative and neutral
classes, and also for the task of determining the confi-
dence level of the association. As was mentioned before,
two annotators had independently tagged the corpus. In
the case of disagreement between two tags, a third anno-
tator was asked to decide on the correct one. For the
task of classifying candidate sentences, inter-annotator
agreement was 91%, which means that in 91% of cases,
the two annotators agreed. Additionally, we computed
Cohens Kappa coefficient [46] for the two annotators;
this coefficient takes into account the degree of agree-
ment that could be expected to occur by chance and is
computed as follows:
? ¼ po?pe
1?pe
Where Po is the relative observed agreement among
annotators, and pe is the hypothetical probability of
chance agreement. The Kappa value was 0.79 for the
two annotators. In general, ? = 1 indicates a complete
agreement. Furthermore, ? < 0 shows that there is no
agreement between annotators other than what would
be expected by chance (as given by pe).
As far as the task of annotating the confidence level of
the association with four categories (zero, weak, medium,
strong), annotators agreed in 87% of the occasions; yet the
Kappa value was 0.80 which is satisfactory.
Characteristics of the SNPPhenA corpus
This section provides detailed statistics as to the linguis-
tic and nonlinguistic properties of the corpus. The basic
properties of the corpus are presented in Table 3 which
includes the statistics of the produced corpus in terms of
test and training parts. As the table shows, the candi-
dates with a positive association comprise the largest
category while the negatively associated candidates con-
stitute the smallest category.
Table 4 provides the detailed analyses concerning the
different types of SNP-phenotype association candidates.
Additionally, as mentioned earlier, the key negated
sentences in the corpus were annotated with scopes of
negation and negation cues. As Table 4 shows, 16.8% of
the sentences have at least one negation cue. Further
analysis shows that not and no with respective occur-
rences of 35 and 38 were the most frequent negation
cues. According to the conducted analyses, each sen-
tence in the corpus had an average of 76.9 tokens, 1.7
SNPs, and 1.2 phenotypes.
As illustrated in Table 3, 76.3% of the samples are distin-
guished (i.e. they are positive and negative association can-
didates). It can, therefore, be concluded that the annotated
sentences were mostly expressed as a direct mechanism or
association between one or more SNPs and a phenotype.
Additionally, as Table 4 shows, 63.8% of the candidate
sentences have at least one clause connector, while
36.2% do not have one. The result of statistical analysis
on the clause connectors further indicates that 9.7%
(=87/895) of instances had concessive clauses.
Fig. 13 A sample of a negated sentence with negation cue and scope
Table 3 Basic statistics of the SNPPhenA corpus in terms of test
and train parts
Item Train Test Total
Files 270 90 360
Sentences 1940 685 2625
Key sentences 362 121 483
SNP 691 244 935
Phenotypes 496 158 654
SNP-Phenotype association candidates 935 365 1300
Neutral candidates 142 166 308
Negative candidates 91 29 120
Positive candidates 702 170 872
Table 4 Statistics of different types of SNP-phenotype association
candidates in the SNPPhenA corpus
Item Number Percentage (%)
Total SNP-phenotype association candidates 1300 100
Candidate with at least one negation cue 218 16.8
Candidates with only one negation cue 188 14.5
Candidates with clause connectors 823 63.8
Candidates without clause connector 470 36.2
Weak degree of confidence candidates 515 39.6
Moderate degree of confidence candidates 124 9.5
Strong degree of confidence positive
candidates
233 17.9
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 10 of 13
Experiment
The results of our first experiments with the corpus are
presented in this subsection. Although several mutation-
related association extraction methods have recently
been developed, automatically measuring the confidence
level in an association is a novel task. Consequently, our
first experiments were evaluated via certain baseline ker-
nel methods for the two subtasks.
In order to categorize the associations, we employed
the two kernel methods that have been expansively made
use of in the relation extraction task; the local context
kernel [47] and sub-tree kernel [48]. Additionally, the
binary Bag of Word (BOW) method was carried out on
the corpus so as to predict the degree of confidence for
the associations. In all the experiments, the training part
of the prepared corpus was used for training the classifier
and the test part was employed for testing the system
(Tables 5, 6 and 7).
Table 5 shows the performance of the two utilized
baseline methods, applied to all three types of candi-
dates. The reported f-score was measured for the detec-
tion of positive SNP-phenotype association candidates.
Table 6 further indicates the performance of the baseline
methods were only applied to the positive and negative
association candidates.
The results of the confidence level prediction of associa-
tions are presented in Table 7 where the best f-measure is
related to the candidate expressions of associations with a
weak confidence level, while the worst result is obtained
for the moderate confidence level.
The lower performance of identifying the confidence
level of association in comparison with the association
extraction method demonstrates that the simple features
used in the binary BOW may not have enough information
to surmount the task and more linguistic features are re-
quired. Moreover, the difficulty of the task might be precipi-
tated by the fact that during the annotation process, the
annotators employed the mentioned p-value number as a
complementary factor for identifying the confidence cat-
egory, which was the case with 20% of the candidate sen-
tences. It can, accordingly, be concluded that accurately
identifying ranked association from biomedical articles
requires more linguistic features including dependency
parsing, lemmatizing and features related to identifying
the significance degree of the biomedical statistical tests.
A simple version of the baseline method can be found
online 3. It is indispensible to mention that the online
system may have a worse performance in comparison
with the reported results in this section due to the ab-
sence of manual checking during the NER task as well
as the omission of the negation detection step.
All the kernel method experiments were carried out
by a support vector machine with SMO [49] implemen-
tation. Weka API [50] was used as the implementation
platform.
Conclusion and future work
In this research, a SNPPhenA corpus was developed in
order to extract the ranked associations of SNPs and
phenotypes from GWA studies. The process entailed
collecting relevant abstracts, Named Entity Recognition,
and annotating the associations, negation, modality
markers, and the confidence level of the associations.
As opposed to the previous biomedical relation extrac-
tion corpora containing true and false types of relations, the
annotated associations in the corpus were divided into
three classes: positive, negative and neutral candidates. The
neutral candidates were those SNP-phenotype candidates
that showed no clear evidence as to the presence or lack of
association between the SNPs and phenotypes. Identifying
neutral candidates is critical for the negation process as the
status of such candidates and their corresponding level of
confidence do not change when they are located in the
scope of negation terms; the status of distinguished associ-
ation candidates, on the other hand, change in such cases.
Similarly, the confidence level, certainty or uncertainty of a
neutral candidate, does not change if it is located in the
scope of a speculation or modality term. Hence, determin-
ing the effect of negation as well as modality terms requires
the identification of neutral candidates.
Table 5 Comparative f-score results for the test SNPPhenA part
for two kernel methods with all types of candidates (positive,
negative and neutral class)
Method LCK Subtree kernel
F1 71.3% 57.7%
Recall 68.7% 51.8%
Precision 69.2% 50.3%
Table 6 Obtained comparative results for the test SNPPhenA
corpus for the two investigated kernel methods with non-neutral
candidates (positive and negative class)
Method LCK Subtree kernel
F1 63.4% 45.7%
Recall 59.8% 41.3%
Precision 56.6% 40.1%
Table 7 Obtained results for the calculating confident interval
of the positive association of the test part of the SNPPhenA
corpus by bag of words method
Parameter Weak degree
of confidence
Moderate degree
of confidence
Strong degree
of confidence
F1 69.5% 32.6% 35.3%
Recall 66.4% 30.5% 34.2%
Precision 65.3% 31.6% 32.2%
Bokharaeian et al. Journal of Biomedical Semantics  (2017) 8:14 Page 11 of 13
Not to be forgotten is the fact that the SNPPhenA cor-
pus must be considered as an initial step in extracting
graded associations from literature, which could result
in the idea of a fuzzy relation extraction task that can be
employed so as to construct better biomedical ontologies.
Furthermore, it is important for future researches to
employ more linguistic-based and non-linguistic-based
factors that could be utilized to determine the confi-
dence of the reported associations. Credibility of the
genotyping techniques (such as MLPA or RFLP) and the
validity of the research through graph-based network
analyses can be employed in the process of identifying
the overall confidence level of the reported associations.
Endnotes
1https://figshare.com/s/b18f7ff4ed8812e265e8
2https://figshare.com/s/f19191317056d6835b38
3http://snpphenotypeext-nilg.rhcloud.com/
Additional file
Additional file 1: Abstract files of SNPPhenA corpus. (ZIP 651 kb)
Acknowledgement
The authors acknowledge Dr. Mariana Neves (Hasso-Plattner-Institute,
University of Potsdam, Germany) for her very helpful comments and for
advice regarding the usage of brat and pubannotaion tools.
The authors also acknowledge Dr. MT Pilehvar (Cambridge University, UK) for
her useful comments and suggestions for organizing the paper.
Funding
Not applicable.
Availability of data and materials
The prepared corpus (SNPPhenA) is available at this address: XML format:
https://figshare.com/s/b18f7ff4ed8812e265e8: BRAT format: https://
figshare.com/s/f19191317056d6835b38: Simple online version of the
association extractor is available here: (http://snpphenotypeext-
nilg.rhcloud.com/): Web site of the corpus: http://nil.fdi.ucm.es/?q=node/639:
Annotation guideline: http://nil.fdi.ucm.es/sites/default/files/guidline.pdf: DTD:
http://nil.fdi.ucm.es/sites/default/files/SNPPhenA_DTD.zip: Kappa calculation:
https://figshare.com/s/f1fe27ca17022fd4a698: Document Text Files (Additional
file 1): https://figshare.com/s/47886f335fb0beaf3099
Authors contribution
The constructing the corpus was managed by BB, preparing the files of the
corpus as well as carrying out the baseline methods was performed by the
first author. Moreover the annotating the negation scope and cues was
performed by BB. The basic structure of the paper and some details of the
experiments and presenting the results were performed by AD. All authors
read and approved the final manuscript. NT (University of Tehran, Tehran,
Iran) developed a program to optimize the corpus and helped in writing
the guideline document. HC (Colorado state university, Colorado, US)
helped in preparing the inter-annotator measurement and also preparing
and coordinating the two annotator. He also helped in structure of the
paper and figures. RC (External Collaborator, Royan Institute for Reproductive
Biomedicine, Tehran, Iran) helped in annotation of the corpus as well as give
some guidance in biological aspects of the study.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Author details
1Facultad informatica, Complutense University of Madrid, Calle Profesor José
García Santesmases, 9, 28040 Madrid, Spain. 2School of Electrical and
Computer Engineering, College of Engineering, University of Tehran, Tehran,
Iran. 3Department of Computer Science, Colorado State University, Fort
Collins, CO 80523, USA. 4External Collaborator, Reproductive Biomedicine
Research Center, Royan Institute for Reproductive Biomedicine, Tehran, Iran.
Received: 5 July 2016 Accepted: 13 January 2017
Park et al. Journal of Biomedical Semantics  (2017) 8:25 
DOI 10.1186/s13326-017-0134-0
RESEARCH Open Access
Towards a more molecular taxonomy of
disease
Jisoo Park1* , Benjamin J. Hescott1 and Donna K. Slonim1,2
Abstract
Background: Disease taxonomies have been designed for many applications, but they tend not to fully incorporate
the growing amount of molecular-level knowledge of disease processes, inhibiting research efforts. Understanding
the degree to which we can infer disease relationships from molecular data alone may yield insights into how to
ultimately construct more modern taxonomies that integrate both physiological and molecular information.
Results: We introduce a new technique we call Parent Promotion to infer hierarchical relationships between disease
terms using disease-gene data. We compare this technique with both an established ontology inference method
(CliXO) and a minimum weight spanning tree approach. Because there is no gold standard molecular disease
taxonomy available, we compare our inferred hierarchies to both the Medical Subject Headings (MeSH) category C
forest of diseases and to subnetworks of the Disease Ontology (DO). This comparison provides insights about the
inference algorithms, choices of evaluation metrics, and the existing molecular content of various subnetworks of
MeSH and the DO. Our results suggest that the Parent Promotion method performs well in most cases. Performance
across MeSH trees is also correlated between inference methods. Specifically, inferred relationships are more
consistent with those in smaller MeSH disease trees than larger ones, but there are some notable exceptions that may
correlate with higher molecular content in MeSH.
Conclusions: Our experiments provide insights about learning relationships between diseases from disease genes
alone. Future work should explore the prospect of disease term discovery from molecular data and how best to
integrate molecular data with anatomical and clinical knowledge. This study nonetheless suggests that disease gene
information has the potential to form an important part of the foundation for future representations of the disease
landscape.
Keywords: Disease Ontology inference, Disease tree inference, Pairwise disease similarity, Disease gene association,
Medical Subject Headings tree, Disease Ontology, Hierarchical clustering, Parent Promotion
Background
The recent growth in availability of genomic and clini-
cal data allows for the discovery of new molecular-level
mechanistic models of disease. However, existing dis-
ease taxonomies and ontologies are often focused on
either physiological characterizations of disease, some-
times using decades-old criteria, or on the organizational
and billing needs of hospitals. Automatically inferring
commonmolecular links between related diseases is made
more difficult by the limited molecular representation
*Correspondence: jisoo.park@tufts.edu
1Department of Computer Science, Tufts University, 161 College Avenue,
Medford, MA 02155, USA
Full list of author information is available at the end of the article
in current taxonomies [1], leading some researchers to
manually group related disorders for individual projects
(for example, PheWAS analysis [2] or network-based dis-
ease gene prioritization [3]). Yet such manual efforts limit
consistency and reproducibility. To further advance such
research and biomedical knowledge in the genomic era,
a recent National Academy of Sciences working group
has called for the development of new disease taxonomies
better suited to incorporate molecular information [4].
A truly modern taxonomy would presumably combine
clinical, physiological, and molecular data. The question
we address here is the degree to which we can infer a
meaningful disease taxonomy simply using disease gene
information. In this, we were inspired by efforts by Trey
Idekers group to infer a version of the Gene Ontology
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 2 of 11
using pairwise similarity scores between genes [5, 6].
Their CliXO algorithm, for example, sorts gene pairs by
a pairwise similarity score and incrementally uses these
scores to group together cliques of similar genes. The
resulting ontology forms a Directed Acyclic Graph (DAG)
of sets of genes. As in that work, here we are not argu-
ing that we should ultimately construct a disease hierarchy
automatically in this way. However, learning how we can
discover the relationships in existing disease taxonomies
from disease gene data is a first step towards developing
new hierarchies of disease that integrate the clinical infor-
mation used in todays taxonomies with genomic data.
Such integrated taxonomies are needed to better support
research in molecular medicine [7].
To infer a disease taxonomy, we would like to sim-
ply cluster diseases hierarchically based on associated
genes from a large gene-disease database. However, if
the items we are clustering are diseases, the internal
nodes of any hierarchical clustering method will corre-
spond to unnamed sets of diseases. While some of these
may be informative, identifying them is a challenge. We
therefore introduce here an algorithm called Parent Pro-
motion, based on hierarchical clustering, that addresses
this problem.
We acknowledge that we are deliberately blurring the
distinction here between an ontology of disease [8] and
a disease taxonomy [9]. In this manuscript, we focus on
learning a hierarchical characterization of disease using
existing disease terminology, yet incorporating molecu-
lar relationships. Such a description may be able to better
identify novel relationships between disorders that do not
appear clinically similar but that arise from similar under-
lying genotypes. Yet we are not expecting here to compre-
hensively infer disease relationships as in most ontologies,
in part because the current project ignores the clinical and
anatomical characteristics built into many existing tax-
onomies. Accordingly, we frequently use the term disease
hierarchy to encompass our inferred hierarchies as well
as those to which we compare.
One important question is how to evaluate our inferred
hierarchies of disease when there is no existing gold stan-
dard. However, there are a handful of existing taxonomies
and disease ontologies that are somewhat suitable for
molecular analyses and comparisons [4]. Medical Subject
Headings (MeSH) is a hierarchical structure of controlled
biological vocabularies used to index articles in MED-
LINE [10].MeSH includesmanymedical concepts beyond
diseases, but here we refer to MeSH category C, a com-
prehensive set of 26 trees that represent relationships
between diseases. SNOMED-CT provides an organized
terminology for clinical terms [11]; this is one of the most
detailed terminologies available, but there are restrictions
on its distribution. The Unified Medical Language Sys-
tem (UMLS) metathesaurus includes disease terms from
multiple taxonomies; while it is not intended to be an
ontology, its semantic network can identify some relation-
ships between terms [12]. The Disease Ontology (DO)
also integrates the knowledge and relationships from sev-
eral taxonomies, including MeSH, SNOMED-CT, and
ICD [13].
Initially, because of the high coverage and availability of
MeSH and its simple structure, we chose to compare our
inferred hierarchies to the MeSH forest of disease terms.
Although it is not necessarily a gold standard for the prob-
lem we are trying to solve, we can use such a comparison
to identify the strengths and limitations of different infer-
ence methods. In addition, identifying individual MeSH
disease trees that are more consistent with the hierar-
chies inferred from disease-gene data helps in assessing
the molecular content of existing domains in MeSH. We
have also extended our assessments by comparison to the
Disease Ontology, which is a more complex process for
reasons detailed below.
Even after fixing a reference hierarchy for comparison,
the question of how to assess correctness remains. Many
of the standard network and graph comparison metrics
are inappropriate for our problem. One that does make
sense is a strict variant of Edge Correctness [14] that
asks how many parent-child relationships we get right.
We therefore use Edge Correctness as one measure of
accuracy.
One limitation of Edge Correctness, however, is that the
distances between pairs of terms are not uniform [15].
That is, two diseases that are separated by more than
one taxonomic link may be more closely related to each
other than two other diseases in a direct parent-child
relationship. We therefore also introduce the notion of
Ancestor Correctness, a feature-based similaritymeasure-
ment [16] that assesses our ability to properly identify
ancestry without concern about distances.
Finally, neither Edge Correctness nor Ancestor Correct-
ness penalizes an algorithm for false positives (inferred
edges not in the reference hierarchy). This is fine for
inference methods like Parent Promotion that build trees,
which all have the same number of edges for a fixed set
of disease nodes, but not for comparison to ontology-
learning approaches that can add arbitrary numbers of
edges. Accordingly, we also compute a variation of hier-
archical precision and recall [17], analagous to Ancestor
Correctness, that accounts for both false positives and
false negatives.
Methods
Reference taxonomies
To quantify performance of various disease hierarchy
inference methods, we compare our inferred taxonomies
to the 2016 Medical Subject Headings (MeSH) disease trees
[10] and the Disease Ontology (DO) [18], downloaded
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 3 of 11
on August 5, 2016. From both datasets, we exclude
diseases for which we cannot find any associated genes,
because our methods would then have no way to learn
about how they relate to other diseases. However, exclud-
ing diseases can disconnect our reference hierarchies. To
reconnect them, we therefore add edges from a deleted
nodes parents to all of its closest descendants that do have
associated genes.
We note that the MeSH trees allow repeated disease
names, resulting in multiple nodes with the same name
in different parts of the tree. We treat these terms as
if they were the same node, effectively matching against
the corresponding DAG. However, given that the original
structure is a tree, most of these DAGs end up being fairly
tree-like.
Because the Disease Ontology is substantially larger
than any of the individual MeSH trees, we extracted
smaller DAGs from the full DO to facilitate algo-
rithm comparison. To find these smaller DAGs, we
searched through the DO starting at the most general
term. A term became a root of a DO subnetwork if
its name approximately corresponded to the name of
the root of one of the 26 MeSH trees and if it had
at least 100 DO terms as descendants. This approach
identified four new DAGs that can be described as
covering mostly Cardiovasular Disease, Gastrointesti-
nal Disease, Musculoskeletal Disease, and Nervous
System Disease.
Table 1 reports the sizes and topology of these four sub-
networks of the DO. All are fairly tree-like; only small
numbers of nodes have more than one parent, and the
total number of edges is not that much larger than the
number of nodes. We note that it is not necessarily the
case that all disease nodes in the DAG labeled Muscu-
loskeletal Disease, for example, actually correspond to
musculoskeletal disorders, because the Disease Ontology
and MeSH are organized according to different princi-
ples. We therefore acknowledge that each subnetwork
of the DO may contain terms that map to several dif-
ferent MeSH disease trees. Nonetheless, we use these
labels as shorthand ways to refer to the chosen DO
subnetworks.
Withheld MeSH subtrees for method development
We selected four small subtrees from MeSH that we used
for refining our computational methods. These are the
MeSH subtrees rooted at the terms Infant Premature
Diseases, Dementia, Respiration Disorders, and Eye
Diseases, giving us a range of subtrees of different sizes
and complexity (Table 2). Note that the MeSH tree rooted
at Eye Diseases includes 149 disease terms and 178
edges, indicating that several terms appear multiple times,
althoughwe allow a node with a given name to appear only
once in each inferred hierarchy.
Although we show the performance of the inference
methods on these subtrees separately in Additional file 1,
we did not think it fair to include them in our over-
all MeSH results because we used them to tune our
methods. Accordingly, we removed the subtrees rooted
at these nodes from the relevant disease trees in MeSH
before evaluating the different methods performance.
Only one whole disease tree, C11 (Eye Diseases), was
removed, because the entire C11 tree was used formethod
development.
There are two other MeSH disease trees that were also
removed before evaluation: C21, Diseases of Environ-
mental Origin, which included only 3 diseases with asso-
ciated genes, and C22, Animal Diseases, which contained
no diseases with associated genes. We therefore report
averaged MeSH results over the remaining 23 MeSH dis-
ease categories.
Disease genes
We use disease genes to calculate pairwise similarity
of diseases. For our comparison to MeSH, we gathered
disease-gene associations from the Online Mendelian
Inheritance in Man (OMIM) database [19] and the Geno-
pedia compendium in the HuGE database of Human
Genetic Epidemiology [20], both downloaded on February
3rd, 2016. OMIM contains human genes, phenotypes
(typically specific diseases), and information about rela-
tionships between them. In particular, OMIM phenotypes
include Mendelian disorders, whose associated genes are
either known or not yet known, as well as mutations that
increase susceptibility to infection, cancer, or drugs [21].
Table 1 Subnetworks of the Disease Ontology
Root disease #Diseases (nodes) #Edges #Nodes with 1 parent #Nodes with 2 parents #Nodes with 3 parents
Disease 2,039 2,095 1,982 55 1
Cardiovascular disease 141 141 139 1 0
Gastrointestinal disease 115 118 110 4 0
Musculoskeletal disease 133 135 129 3 0
Nervous System disease 308 324 291 15 1
The entire Disease Ontology (root = Disease) and four subnetworks of various sizes extracted from it. The original DO and its subnetworks are tree-like: 1) the numbers of
edges are close to n ? 1, where n is the number of nodes and 2) only a small fraction of nodes have 2 or more parents
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 4 of 11
Table 2 Four MeSH subtrees of various sizes used for method
development
Root disease #Diseases (nodes) #Edges
Infant, Premature, Diseases 6 5
Dementia 13 12
Respiration disorders 23 22
Eye diseases 149 178
Genopedia includes links to articles on epidemiological
studies that identify gene-disease interactions. The major-
ity of these are discovered through association stud-
ies; linkage mapping and animal studies are specifically
excluded [20]. We combined disease-gene associations
from the two databases as in our previous work [1],
using the MEDIC merged disease vocabulary (down-
loaded from the Comparative Toxicogenomics Database
[22] on February 3rd, 2016). This combined data set con-
tains 2755 diseases and 12,873 genes.
To infer hierarchies based on DO terms with this
disease-gene data, however, required converting the
MeSH disease terms to DO terms. The DO obo file pro-
vides synonym information for this conversion. However,
because not every MeSH term has a DO equivalent, nor
vice-versa, the mapped disease gene data set included
1790 DO terms with 12,230 associated genes. The Disease
Ontology actually includes 6932 disease nodes, so the
resulting DAG of diseases with associated genes was
largely disconnected.
For the DO analysis, we therefore augmented the dis-
ease gene data with disease-gene associations from the
DISEASES database [23] (downloaded on August 5th,
2016) which directly uses DO terms. We used the filtered
version of the DISEASES database which provides non-
redundant disease-gene association pairs, and selected
only associations derived from experiments or database
curation (knowledge), which we expect to be of rela-
tively high confidence. The DISEASES data included 772
disease terms and 13,059 genes. When combined with
the mapped data from the MeSH comparison, the total
yielded 2039 DO terms with 16,404 associated genes, pro-
ducing a sufficiently connected ontology for our purposes.
Although this number of disease genes seems high, note
that our genes are really referring to entities with distinct
HGNC official gene symbols, as reported in the NCBI
Gene database and associated with some disease term in
the databases described. Some HGNC symbols refer to
distinct subunits of genes, while a few (under 3.5%) refer
to non-coding sequences that have either been shown to
play a regulatory role in disease, or that are locations of
SNPs linked to disease in GWAS studies. At most 250
such non-coding entities are implicated in more than one
disease and might therefore potentially play a role in our
analyses.
Measuring pairwise similarity
For our inference algorithms we needmethods tomeasure
similarities both between pairs of diseases and between
pairs of genes. To calculate pairwise similarity between
diseases A and B, disease_sim(A,B), let GA be the set of
associated genes for diseaseA andGB the set of associated
genes for disease B. We then use the Jaccard Index [24] to
represent the similarity between the disease gene sets as
follows:
disease_sim(A,B) = Jaccard(GA,GB) = |GA ? GB||GA ? GB|
To calculate pairwise similarity between genes g1 and
g2, gene_sim(g1, g2), we do the opposite, as we are inter-
ested in measuring the similarity of diseases with respect
to their associated genes:
gene_sim(g1, g2) = Jaccard(Dg1 ,Dg2) =
|Dg1 ? Dg2 |
|Dg1 ? Dg2 |
whereDg1 is the set of diseases associated with gene g1 and
Dg2 is the set of diseases associated with gene g2.
Note that no information about the relationships
between diseases other than this measure of overlapping
disease genes is incorporated into this similarity matrix or
used by our inference algorithms.
Inference strategies
Clique Extracted Ontology (CliXO)
To use CliXO to generate disease ontologies, we begin by
creating a matrix containing the Jaccard similarity score
between genes as defined above. CliXO uses this similar-
ity matrix as input. It also relies on two parameters: ?,
which represents the amount of noise allowed in forming
cliques, and ? , which represents missing data. The algo-
rithm is demonstrated to be relatively robust to variation
in ? , so we set ? = 0.5 as done by the CliXO team [5].
Variation in ? has higher impact on the results, so tuning
it to the data set is suggested. We chose ? = 0.05 because
it produced reasonable-sized output graphs in our initial
experiments on the four MeSH subtrees in Table 2.
Initially, CliXO returns a DAG whose internal nodes
correspond to sets of genes, not to specific disease terms
in the reference ontology. We then used the ontology
alignment technique of [6] to align the resulting ontol-
ogy to the MeSH reference or to the Disease Ontology,
in order to identify disease terms in the output DAG.
Accordingly, some of the disease terms may not be repre-
sented in the CliXO output, because they fail to map to
any node. (Fig. 1 demonstrates the topological difference
for a small example; note that the CliXO output on the
right maps only 5 of the 6 disease nodes.)
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 5 of 11
a) b)
Fig. 1 Topological difference between MeSH and the corresponding inferred ontology using CliXO. a A MeSH subtree containing prematurity
complications. b Corresponding Disease Ontology inferred using CliXO and ontology alignment. Drawn in Cytoscape v. 3.3.0 [30]
Parent Promotion
We introduce a new technique we call Parent Promotion
that focuses on similarities in disease genes. The idea is
to group diseases by their similarity scores and use hierar-
chical clustering to form subgroups. Parent-child relations
are then created from these subgroups by counting cita-
tion frequency in PubMed.
Specifically, we transform the pairwise similarity score
into a distance by subtracting it from 1. We then perform
complete-linkage hierarchical clustering on the disease
terms using the hclust function in R with these dis-
tances. Internal nodes in this dendrogram correspond to
sets of diseases. To convert the resulting dendrogram to
a hierarchy with a single disease at each node, we iden-
tify the number of disease-related articles in PubMed
for each disease in a cluster using the NCBIs E-utilities
(http://www.ncbi.nlm.nih.gov/books/NBK25501/).
Working up from the bottom of the dendrogram, the
disease term with the most citations is promoted to
become the parent, with all other diseases in the clus-
ter left as its children. Once defined as a child, a disease
does not have another chance to be promoted. That is,
we only consider the most recently promoted disease and
its siblings in a cluster when deciding the next parent.
Figure 2 shows an example of how the dendrogram guides
the Parent Promotion process.
Notice that the inferred tree created by the Parent Pro-
motion technique always has the same number of diseases
(nodes) as the reference. However, the number of edges
may differ from that of the reference, which may be either
implicitly or explicitly a DAG. In either case, Parent Pro-
motion may therefore produce a result with fewer edges.
Minimumweight spanning tree
We also compared our new Parent Promotion method
to the standard technique of finding a Minimum Weight
Spanning Tree (MWST) [25] over the complete network
of disease terms, with pairwise similarity scores between
diseases as edge weights. The idea behind this is that a
representation of the relationships between diseases that
connects all the disease terms by their highest disease
gene similarity represents a minimum-length description
of the data that seems likely to capture real disease rela-
tionships. The MWST is unrooted, so we choose the
disease with themost related PubMED articles as the root.
Evalution metrics
Comparing the inference methods remains challenging
due to the topological differences of the output. In par-
ticular, both Parent Promotion and MWST produce trees
whose n nodes are exactly those of the reference hierar-
chy. In contrast, the DAG output by the CliXO method
a) b) c)
Fig. 2 How the Parent Promotion method transforms a dendrogram created by hierarchical clustering. a Dendrogram for diseases of infants born
preterm. Hierarchical clustering builds a tree whose internal nodes are hard to interpret. b Parent Promotion finds the most general disease term
from each cluster and promotes it as an internal node. An internal node becomes the parent of all other nodes in the same cluster. Disease term 3
has the most citations and keeps being selected for promotion until it becomes the root. Disease term 6 has more citations than 5 and is promoted
as the parent of 5. However, it later becomes a child of 3 because it has fewer citations than 3. c Final tree built by Parent Promotion
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 6 of 11
may be much larger (as in Fig. 1). We use multiple meth-
ods to quantify and compare performance despite these
differences.
Edge Correctness (EC)
Inspired by the notion of Edge Correctness (EC) used
in network alignment [14] we measure the number of
edges that are identical to those in the reference hierar-
chy. Unlike in the network alignment problem, which uses
Edge Correctness as a proxy for node correctness, for this
problem we know the node correctness and wish to mea-
sure correctly inferred edges. We count edges as correctly
matched if and only if the parent child relations (both
the edges and the directions of the edges) are preserved.
To create an overall score we calculate the percentage of
edges in the reference that also appear in the inferred
ontology.
Ancestor Correctness (AC)
While Edge Correctness (EC) can measure how well two
networks are aligned, it may not be the best method
for evaluating disease taxonomies. In particular, dis-
eases separated by multiple taxonomic links may still
be closely related to each other, so EC can underesti-
mate performance by ignoring the ancestor-descendant
relationship. EC also rewards successfully matched edges
with no penalty for incorrect ones. This property
may favor CliXO, which tends to produce DAGs with
many edges.
To address the first shortcoming, we introduce the
notion of Ancestor Correctness (AC). For a disease x,
let xref be a node representing x in the reference ontol-
ogy and xinf be a node representing x in our inferred
hierarchy. Also let A(x) be the set of all ancestors
of x in the appropriate hierarchy. Then for a specific
disease xinf in the inferred taxonomy we can mea-
sure how well it matches the reference by calculating
AncestorJaccard = Jaccard(A(xref ),A(xinf )). We can then
apply AncestorJaccard globally by averaging across all dis-
eases in the inferred network. We report this average as
our AC score for the inferred network. Note that we only
consider diseases existing in both hierarchies. However,
we exclude diseases that are roots in both because they do
not have any ancestors.
Ancestor Precision and Recall (AP and AR)
Ancestor Correctness (AC) provides a good estimate of
topological similarity in terms of the number of preserved
ancestors of mapped nodes. However, it still does not
penalize false positives.
To address this problem, we adapt the Hierarchical
Precision (HP) and Hierarchical Recall (HR) measure-
ments from Verspoor et al. [17]. These measurements
compare the sets of all ancestors of a disease in the
inferred hierarchy to the ancestors of the same term in the
reference. Informally, HP is the fraction of xs ancestors
in the inferred hierarchy that are correct, while HR is the
fraction of true ancestors of x that are also predicted by an
inference method to be ancestors of x.
More specifically, for a disease x, let xref be the node in
the reference and xinf be the node in the inferred ontology.
Then our HP and HR are calculated as follows:
HP(xref , xinf ) =
|A(xref ) ? A(xinf )|
|A(xinf )| (1)
HR(xref , xinf ) =
|A(xref ) ? A(xinf )|
|A(xref )| (2)
We also calculate an F score using HP and HR as:
F(x) = 2 × HP(x) × HR(x)HP(x) + HR(x) (3)
Finally, we define Ancestor Precision (AP) and Ances-
tor Recall (AR) to be the average of HP and HR across all
diseases in our reference hierarchy.
Results
Comparison to MeSH
We ran all three algorithms on the disease gene data
and disease terms from each of the 23 MeSH trees.
Table 3 reports the averaged performance across all 23
trees for each method and the different evaluation crite-
ria. Across this data set, we see that Parent Promotion
on average outperforms CliXO and MWST for almost
all evaluation measures. The only exception is Ancestor
Recall, for which MWST slightly edges out Parent Pro-
motion. Detailed performance on eachMeSH disease tree
is shown in Additional file 1; in most cases the meth-
ods relative performance is similar to that in Table 3.
The detailed table also shows that, for each evaluation
criterion, performance of the different methods is highly
correlated across the 23 disease trees, suggesting that
some trees are more consistent with the disease gene data
than others.
Comparison to the Disease Ontology
We first attempted to reconstruct all of the Disease Ontol-
ogy reflected in our disease-gene data set (2095 edges
connecting 2039 DO terms). However, we could not com-
pare the performance of all three inference methods on
this full data set because running CliXO, which has at
its core the computationally hard problem of finding
cliques, was infeasible on a data set this large and complex.
Nonetheless, we found that Parent Promotion consistently
outperformed MWST on this large data set. Specifically,
Parent Promotion had an EC of 0.07 compared toMWSTs
EC of 0.05, an AC of 0.23 compared to MWSTs AC of
0.04, and an F score of 0.40 compared to MWSTs 0.08.
We used the subnetworks of DO listed in Table 1 to
compare all three methods. Table 4 shows the results of
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 7 of 11
Table 3 Average performance of inference methods across the MeSH trees
Method EC (± stdev) AC (± stdev) AP (± stdev) AR (± stdev) F (± stdev)
Parent Promotion 0.13 (± 0.06) 0.30 (± 0.10) 0.46 (± 0.16) 0.47 (± 0.14) 0.47 (± 0.15)
CliXO 0.12 (± 0.10) 0.22 (± 0.12) 0.30 (± 0.14) 0.38 (± 0.17) 0.33 (± 0.15)
MWST 0.07 (± 0.04) 0.11 (± 0.07) 0.13 (± 0.08) 0.48 (± 0.18) 0.21 (± 0.11)
Average Edge Correctness (EC), Ancestor Correctness (AC), Ancestor Precision (AP), Ancestor Recall (AR) and F-score across the different trees in the MeSH forest. Standard
deviation is shown in parentheses. Best performance across different inference techniques is highlighted in italic
all three methods on these subnetworks of DO. We again
see that in most cases Parent Promotion outperforms
CliXO and MWST for each evaluation measure, with the
exception of Musculosketal Disease, where CliXO out-
performs Parent Promotion and MWST. Again, MWST
often has good Ancestor Recall despite unimpressive per-
formance on most other metrics.
Figure 3 shows an example of one of the larger con-
nected components inferred by Parent Promotion using
the DO data. All edges in the figure occur in both the
Disease Ontology and the inferred tree. Although the
inferred tree is relatively flat, the figure demonstrates that
inference method is capturing some logical relationships
between diseases.
Data sources and quantity matter
We investigated the influence of the type and amount
of data using Parent Promotion on the MeSH disease
trees. First, we tried using data from just OMIM or just
Genopedia. OMIM has a higher percentage of monogenic
diseases identified using classical methods such as posi-
tional cloning, while Genopedia has a higher percentage
of GWAS data. On the other hand, OMIM includes much
less data, containing just 2434 genes linked to 1173 dis-
orders, whereas Genopedia contains 12,527 genes impli-
cated in 2499 disorders. Therefore, it is not surprising
that performance on the Genopedia data exceeds that
on the OMIM data, nearly across the board. The excep-
tion, interestingly, is C16, Congenital, Hereditary, and
Neonatal Diseases and Abnormalities, where the OMIM-
only version outperforms Genopedia-only by the AC, AP,
and F measures. This seems likely to be because this
MeSH tree includes many hereditary disorders whose
genes are particularly likely to be included in OMIM.
Detailed results for this comparison appear in Additional
file 2. (EC is omitted because it is uninformative for many
of the smaller data sets.)
In most cases, furthermore, the combination of the
two data sources is better than either alone. There are
a few cases where performance declines slightly with
both compared to just Genopedia, but in those cases the
OMIM data actually adds just a handful of genes that
arent already in the Genopedia data, and the changes
in performance are small, consistent with small random
perturbations.
To further explore the hypothesis that more data pro-
duces better results, we also ran an experiment where we
randomly removed 25% or 50% of the disease-gene asso-
ciations from each MeSH tree, and again tried to infer
trees via Parent Promotion. On average, performance
on all measures improved with more data, although the
effects on most individual trees were modest (results are
in Additional file 3).
Discussion
Overall, these experiments have provided some impor-
tant insights into what can and cannot be learned about
disease relationships from disease genes alone.
Table 4 Evaluation results for four DO subnetworks
Edge Correctness Ancestor Correctness F-score (Ancestor precision, ancestor recall)
Parent Parent Parent
Root disease Promotion CliXO MWST Promotion CliXO MWST Promotion CliXO MWST
Cardiovascular disease 0.06 0.09 0.07 0.32 0.18 0.11 0.50 0.27 0.21
(0.57, 0.44) (0.24, 0.30) (0.13, 0.48)
Gastrointestinal disease 0.17 0.13 0.03 0.37 0.26 0.14 0.55 0.39 0.26
(0.56, 0.53) (0.36, 0.42) (0.18, 0.48)
Musculoskeletal disease 0.16 0.08 0.10 0.15 0.26 0.09 0.26 0.41 0.17
(0.44, 0.18) (0.42, 0.40) (0.16, 0.19)
Nervous System disease 0.13 0.07 0.09 0.29 0.17 0.10 0.46 0.30 0.19
(0.70, 0.34) (0.26, 0.34) (0.13, 0.34)
Average Edge Correctness (EC), Ancestor Correctness (AC), Ancestor Precision (AP), Ancestor Recall (AR) and F-score across four DO subnetworks. Standard deviation is shown
in parentheses. Best performance across different inference techniques is highlighted as italic
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 8 of 11
Fig. 3 Parent Promotion tree using DO data. Subtree of the disease tree built by Parent Promotion on DO musculoskeletal system disease data that
is an exact match to nodes and edges in the DO
The correlations observed across the MeSH trees sug-
gest that disease relationships in some MeSH categories
are easier to learn than others. Correctness appears to
be higher for smaller trees, perhaps simply because there
are fewer possibilities. However, there are some large
disease subtrees with higher AC and EC scores, espe-
cially Endocrine System Diseases (C19), Nutritional and
Metabolic Diseases (C18), and Respiratory Tract Dis-
eases (C08).
It is possible that the MeSH hierarchy in these areas is
better defined by molecular data, or that there are sim-
ply more disease genes known in these areas than in some
others. One observation is that these categories include
several well-studied complex diseases with high public
health impact. For example, C19 includes diabetes and
ovarian and pancreatic cancer; C18 also includes diabetes,
plus obesity and related conditions; and C08 features
asthma, COPD, and several types of lung cancer. Which
exact properties of a set of diseases contribute most to the
success of inference algorithms is an important question
for future work.
On the Musculoskeletal Disease DO subnetwork,
CliXO outperforms Parent Promotion by several criteria.
Parent Promotion struggles with this region of the Dis-
ease Ontology, in part because the term Musculosketal
Disease has fewer PubMed citations than the less gen-
eral term Bone Disease. The latter is therefore promoted
incorrectly to become the root, while the former remains
low in the inferred tree.
We also notice that despite its relatively poor per-
formance overall, MWST seems to have good Ancestor
Recall in many cases, sometimes even beating other meth-
ods. This may be because MWST tends to infer tall, thin
trees rather than short and broad ones. Figure 4 illustrates
this tendency. A node has more ancestors in tall, thin trees
than in broad trees, and as a result, is more likely to share
ancestors with the reference.
By attempting to infer relationships for each MeSH dis-
ease category separately, or within specific subnetworks
of the Disease Ontology, most of the work described here
has only a limited ability to detect novel molecular con-
nections across diseases currently thought to be unrelated.
However, we can begin to address the question of whether
such discovery is possible with these methods by looking
at the performance of Parent Promotion on data from the
full Disease Ontology, and by examining inferred edges
connecting pairs of disease terms that are not directly
connected in the DO.
We found 1900 such pairs. Most of these make unsur-
prising connections. For example, progressive muscular
atrophy was, in our inferred hierarchy, directly con-
nected to spinal muscular atrophy because they share 34
genes (all of those associated with the first disease term).
Other pairs may span different medical domains and tis-
sues yet have well-known commonalities that are already
described in existing hierarchies (e.g. rheumatoid arthri-
tis and type I diabetes mellitus, both of which are listed as
autoimmune disorders in MeSH).
However, there are other inferred edges whose rela-
tionships are plausible but not currently characterized.
For example, liver cirrhosis and pre-eclampsia share an
edge in our inferred hierarchy because they have large
and highly overlapping sets of associated genes. These
disorders initially appear to affect very different anotom-
ical systems and processes; both the Disease Ontology
andMeSH categorize pre-eclampsia under cardiovascular
disease/hypertension (MeSH also lists it as a pregnancy
complication), while cirrhosis is represented primarily
as a liver disease in both hierarchies. Yet there is evi-
dence that cirrhosis elevates the risk of pre-eclampsia
during pregnancy [26]. There are also specific cases
(e.g. HELLP syndrome, characterized by hemolysis, ele-
vated liver enzymes, and low platelet count) that link
liver dysfunction with increased pre-eclampsia risk [27].
As another example, fatty liver disease is also surpris-
ingly linked to pterygium or surfers eye, character-
ized by fleshy growths of the eye that are linked to
sunlight exposure. Molecular markers associated with
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 9 of 11
a)
b)
c)
Fig. 4 A MeSH tree rooted at Respiration Disorder and corresponding inferred disease trees. a The MeSH tree containing Respiration Disorder
and its descendants. b The disease tree inferred by Parent Promotion on data from the tree in a). c The disease tree inferred by MWST from the same
data. MWST builds a taller and slimmer tree. As a result, most diseases have more ancestors in c) than in a) or b). This leads MWST to have good
performance with respect to Ancestor Recall (AR)
pterygium appear to be associated with cell migration
or involved with epithelial-to-mesenchymal transition
(EMT) [28], a class of genes also thought to play a
role in how the liver responds to injury such as that
caused by fatty liver disease [29]. Future work explor-
ing the implication of such potential connections may be
warranted.
Conclusions
We have demonstrated that it is possible to recover much
of the structure of both MeSH disease trees and the
DO from molecular data alone. However, this work is a
preliminary analysis, and there is much more to learn.
Although our aim in this project has been only to
infer gene-based relationships between disease terms in
existing taxonomic systems, one ultimate goal for a 21st-
century disease taxonomy is the inference of new disease
terms based on molecular information [4, 7]. Classifica-
tion of cancer or autism subtypes based on underlying
genetic contributions, for example, might be possible in
such a system.
The examples in the previous section of discovering
links across apparently disparate disease types raise the
possibility that novel connections in the inferred hierar-
chies for the full Disease Ontology data may correspond
to novel disease subtypes with commonmolecular causes.
Thus the discovery of new disease terms could arise
from future work based on such analyses. Of the meth-
ods described here, CliXO is the only one that might
directly address this problem, by inferring internal nodes
corresponding to sets of genes and then by finding new
methods to map these gene sets into plausible disease
classes. Further exploration of its abilities to do so, or
extension of clustering-based methods analogous to Par-
ent Promotion to incorporate comparable possibilities, is
warranted.
Taxonomy inference using data from diseases across
organ systems and tissues, such as that in the full Dis-
ease Ontology data set, may also lead to improved
categorization of disease processes. Subgraphs of the
inferred hierarchies may represent disease groups spe-
cific to certain anatomical systems, and investigation of
disease genes associated with such a subgraph might
provide some insights into anatomical expression and rel-
evance of disease genes. However, to identify inferred
subgraphs representing specific anatomical systems we
would need a comprehensive mapping between DO terms
and these systems. The development of such a mapping
and further interpretation of the substructure in such
broad inferred hierarchies remains an interesting open
question.
Future work may also include exploring the incorpora-
tion of tissue specific gene expression to integrate relevant
tissues and organs with the molecular level data, and
to look more broadly at ways to combine clinical and
molecular data. We also have not yet fully explored the
range of relevant tree- and DAG-inference methods from
the machine-learning community. However, the current
results leave us optimistic that by including molecular
information, it will be possible to construct integrated dis-
ease taxonomies that better support medical research in
the genomic era.
Park et al. Journal of Biomedical Semantics  (2017) 8:25 Page 10 of 11
Additional files
Additional file 1: Performance of three disease hierarchy inference
algorithms (Parent Promotion, CliXO, MWST): Edge Correctness, Ancestor
Correctness, Ancestor Precision/Recall and F-score for 23 MeSH trees.
(PDF 78 kb)
Additional file 2: Performance of Parent Promotion using disease-gene
association information in OMIM, Genopedia and combination of two:
Ancestor Correctness, Ancestor Precision/Recall and F-score for 23 MeSH
trees. (PDF 61 kb)
Additional file 3: Change in performance of Parent Promotion
depending on the size of disease-gene association information: Edge
Correctness, Ancestor Correctness, Ancestor Precision/Recall and F-score
for 23 MeSH trees. (PDF 53 kb)
Abbreviations
AC: Ancestor Correctness; AP: Ancestor precision; AR: Ancestor recall; CliXO:
Clique Extracted Ontology; DAG: Directed acyclic graph; DO: Disease
Ontology; EC: Edge Correctness; HP: Hierarchical precision; HR: Hierarchical
recall; HuGE database: Human genome epidemiology database; ICD:
International classification of diseases; MeSH: Medical subject heading; MWST:
Minimum weight spanning tree; NCBI: National Center for Biotechnology
Information; OMIM: Online Mendelian inheritance in man; PheWAS: Phenome
wide association studies; SNOMED CT: Systematized nomenclature of
medicine, clinical terms; UMLS: Unified medical language system
Acknowledgements
We thank Karin Verspoor for suggesting hierarchical precision and recall,
Stephan Schürer for recommending the DISEASES database, Trey Ideker and
Michael Kramer for providing the CliXO code and thoughtful advice, Inbar
Fried for comments about measurement for network alignment, and
members of the Tufts Bioinformatics and Computational Biology group for
helpful feedback and comments.
Funding
Research reported in this publication was supported by NIH award
R01HD076140. The content is solely the responsibility of the authors and does
not necessarily represent the official views of the National Institutes of Health.
Availability of data andmaterials
Not applicable.
Authors contributions
DKS, BJH, and JP conceived and designed the overall research strategy,
analyzed the experimental results and wrote the manuscript. JP implemented
the Parent Promotion method and ran the experiments. All authors read and
approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
All authors read and approved the final version of the manuscript.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Computer Science, Tufts University, 161 College Avenue,
Medford, MA 02155, USA. 2Department of Integrative Physiology and
Pathobiology, Tufts University School of Medicine, 145 Harrison Avenue,
Boston, MA 02111, USA.
Received: 8 November 2016 Accepted: 17 July 2017
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 
DOI 10.1186/s13326-017-0119-z
RESEARCH Open Access
Evaluating the effect of annotation size
on measures of semantic similarity
Maxat Kulmanov1,2 and Robert Hoehndorf1,2*
Abstract
Background: Ontologies are widely used as metadata in biological and biomedical datasets. Measures of semantic
similarity utilize ontologies to determine how similar two entities annotated with classes from ontologies are, and
semantic similarity is increasingly applied in applications ranging from diagnosis of disease to investigation in gene
networks and functions of gene products.
Results: Here, we analyze a large number of semantic similarity measures and the sensitivity of similarity values to the
number of annotations of entities, difference in annotation size and to the depth or specificity of annotation classes.
We find that most similarity measures are sensitive to the number of annotations of entities, difference in annotation
size as well as to the depth of annotation classes; well-studied and richly annotated entities will usually show higher
similarity than entities with only few annotations even in the absence of any biological relation.
Conclusions: Our findings may have significant impact on the interpretation of results that rely on measures of
semantic similarity, and we demonstrate how the sensitivity to annotation size can lead to a bias when using semantic
similarity to predict protein-protein interactions.
Keywords: Semantic similarity, Ontology, Gene ontology
Background
Semantic similaritymeasures are widely used for datamin-
ing in biology and biomedicine to compare entities or
groups of entities in ontologies [1, 2], and a large number
of similarity measures has been developed [3]. The sim-
ilarity measures are based on information contained in
ontologies combined with statistical properties of a cor-
pus that is analyzed [1]. There are a variety of uses for
semantic similarity measures in bioinformatics, includ-
ing classification of chemicals [4], identifying interacting
proteins [5], finding candidate genes for a disease [6], or
diagnosing patients [7].
With the increasing use of semantic similarity measures
in biology, and the large number of measures that have
been developed, it is important to identify a method to
select an adequate similarity measure for a particular pur-
pose. In the past, several studies have been performed
*Correspondence: robert.hoehndorf@kaust.edu.sa
1Computational Bioscience Research Center, King Abdullah University of
Science and Technology, 23955-6900, Thuwal, Saudi Arabia
2Computer, Electrical and Mathematical Sciences and Engineering Division,
King Abdullah University of Science and Technology, 23955-6900, Thuwal,
Saudi Arabia
that evaluate semantic similarity measures with respect to
their performance on a particular task such as predicting
protein-protein interactions through measures of func-
tion similarity [810]. While such studies can provide
insights into the performance of semantic similarity mea-
sures for particular use cases, they do not serve to identify
the general properties of a similarity measure, and the
dataset to be analyzed, based on which the suitability of
a semantic similarity measure can be determined. Specif-
ically, when using semantic measures, it is often useful
to know how the annotation size of an entity affects
the resulting similarity, in particular when the corpus to
which the similarity measure is applied has a high vari-
ance in the number of annotations. For example, some
semantic similarity measures may always result in higher
similarity values when the entities that are compared have
more annotations and may therefore be more suitable
to compare entities with the same number of annota-
tions. Furthermore, the difference in annotation size can
have a significant effect on the similarity measure so
that comparing entities with the same number of anno-
tations may always lead to higher (or lower) similarity
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 2 of 10
values than comparing entities with a different number in
annotations.
Here, we investigate features of a corpus such as the
number of annotations to an entity and the variance (or
difference) in annotation size on the similarity measures
using a large number of similarity measures implemented
in the Semantic Measures Library (SML) [11]. We find
that different semantic similarity measures respond differ-
ently to annotation size, leading to higher or lower seman-
tic similarity values with increasing number of annota-
tions. Furthermore, the difference in the number of anno-
tations affects the similarity values as well. Our results
have an impact on the interpretation of studies that use
semantic similarity measures, and we demonstrate that
some biological results may be biased due to the choice
of the similarity measure. In particular, we show that the
application of semantic similarity measures for predicting
protein-protein interactions can result in a bias, similarly
to other guilt-by-association approaches [12], in which
the sensitivity of the similarity measure to the annotation
size confirms a bias present in protein-protein interaction
networks so that well-connected and well-annotated pro-
teins have, on average, a higher similarity by chance than
proteins that are less well studied.
Methods
Generation of test data
We perform all our experiments using the Gene Ontology
(GO) [13], downloaded on 22 December 2015 from http://
geneontology.org/page/download-ontology and Human
Phenotype Ontology (HPO) [14], download on 1 April
2016 from http://human-phenotype-ontology.github.io/
downloads.html in OBO Flatfile Format. The version of
GO we use consists of 44,048 classes (of which 1941 are
obsolete) and HPO consists of 11,785 classes (of which
112 are obsolete). We run our experiments on several dif-
ferent sets of entities annotated with different number
of GO or HPO classes and one set of entities anno-
tated with GO classes from specific depth of the graph
structure. The first set contains 5500 entities and we ran-
domly annotated 100 entities each with 1, 2, . . . , 54, 55 GO
classes. We generate our second set of entities annotated
with HPO classes in the same fashion. The third set is a
set of manually curated gene annotations from the yeast
genome database file (gene_associations.sgd.gz) down-
loaded on 26 March 2016 from http://www.yeastgenome.
org/download-data/curation. The dataset consists of 6108
genes with annotations sizes varying from 1 to 55, and
each group of the same size contains a different number of
gene products. We ignore annotations with GO evidence
code ND (No Data). The fourth set contains 1700 entities
which is composed of 17 groups. Each group have 100 ran-
domly annotated entities with GO classes from the same
depth of the ontology graph structure.
Computing semantic similarity
After the random annotations were assigned to the enti-
ties, we computed the semantic similarity between each
pair of entities using a large set of semantic similarity
measures. We include both groupwise measures and pair-
wise measures with different strategies of combining them
[1]. Groupwise similarity measures determine similarity
directly for two sets of classes. On the other hand, indirect
similarity measures first compute the pairwise similari-
ties for all pairs of nodes and then apply a strategy for
computing the overall similarity. Strategies for the latter
include computing the mean of all pairwise similarities,
computing the Best Match Average, and others [1].
Furthermore, most semantic similarity measures rely
on assigning a weight to each class in the ontology that
measures the specificity of that class. We performed our
experiments using an intrinsic information content mea-
sure (i.e., a measure that relies only on the structure
of the ontology, not on the distribution of annotations)
introduced by [15].
The semantic similarity measures we evaluated include
the complete set of measures available in the Semantic
Measures Library (SML) [11], and the full set of measures
can be found at http://www.semantic-measures-library.
org. The SML reduces an ontology to a graph structure
in which nodes represent classes and edges in the graph
represent axioms that hold between these classes [16, 17].
The similarity measures are then defined either between
nodes of this graph or between subgraphs.
The raw data and evaluation results for all similarity
measures are available as Additional file 1: Table S1. The
source code for all experiments is available on GitHub at
https://github.com/bio-ontology-research-group/pgsim.
Measuring correlation
In order to measure the sensitivity of the similarity mea-
sures to the number of annotations we calculated Spear-
man and Pearson correlation coefficients between set of
annotations sizes and the set of average similarity of one
size group to all the others. In other words, we first com-
puted the average similarities for each entity in a group
with fixed annotation size and computed the average sim-
ilarity to all entities in our corpus. For calculating the
correlation coefficients we used SciPy library [18].
Protein-protein interactions
We evaluate our results using protein-protein interac-
tion data from BioGRID [19] for yeast, downloaded on
26 March 2016 from http://downloads.yeastgenome.org/
curation/literature/interaction_data.tab. The file contains
340,350 interactions for 9868 unique genes. We filtered
these interactions using the set of 6108 genes from the
yeast genome database and our final interaction dataset
includes 224,997 interactions with 5804 unique genes.
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 3 of 10
Then we compute similarities between each pair of genes
using simGIC measure [1] and Resniks similarity mea-
sure [20] combined with Average and Best Match Average
(BMA) strategies and generate similarity matrices. Addi-
tionally, we create a dataset with random GO annotations
for the same number of genes, and the same number of
annotations for each gene. We also generate the similarity
matrices for this set using the same similarity measures.
To evaluate our results, we use the similarity values as
a prediction score, and compute the receiver operating
characteristic (ROC) curves (i.e., a plot of true positive
rate as function of false positive rate) [21] for each similar-
ity measure by treating pairs of genes that have a known
PPI as positive and all other pairs of proteins as negatives.
In order to determine if our results are valid for
protein-protein interaction data from other organisms,
we perform a similar evaluation with mouse and human
interactions. We downloaded manually curated gene
function annotations from http://www.geneontology.org/
gene-associations/ for mouse (gene_associations.mgi.gz)
and human (gene_associations.goa_human.gz) on 12
November 2016. The mouse annotations contain 19,256
genes with annotations size varying from 1 to 252 and
human annotations contain 19,256 genes with annota-
tions size varying from 1 to 213. We generate random
annotations with the same annotations sizes for both
datasets and compute similarity values using Resniks
similarity measure combined with BMA strategy. For
predicting protein-protein interactions we use BioGRID
interactions downloaded on 16 November 2016 from
https://thebiogrid.org/download.php. There are 38,513
gene interactions for mouse and 329,833 interactions for
human.
Gene-Disease associations
To evaluate our results with differnt ontologies, we aim
to predict genedisease associations using phenotypic
similarity between genes and diseases. We use mouse
phenotype annotations and mouse genedisease associ-
ations downloaded from http://www.informatics.jax.org/
downloads/reports/index.html (MGI_PhenoGenoMP.rpt
andMGI_Geno_Disease.rpt). The dataset contains 18,378
genes annotated with Mammalian Phenotype Ontology
(MPO) [22] classes with size varying from 1 to 1671, and
1424 of genes have 1770 associations with 1302Mendelian
diseases. We downloaded Mendelian disease phenotype
annotations from http://compbio.charite.de/jenkins/job/
hpo.annotations.monthly/lastStableBuild/ and generated
random annotations with the same sizes for both gene and
disease annotation datasets. We computed similarity of
each gene to each disease by computing the Resniks simi-
larity measure combined with BMA strategy between sets
of MPO terms and HPO terms based on PhenomeNET
Ontology [6]. Using this similarity value as a prediction
score we computed ROC curves for real and random
annotations.
Results and discussion
Our aim is to test threemain hypothesis. First, we evaluate
whether the annotation size has an effect on similar-
ity measures, and quantify that effect using measures of
correlation and statistics. We further evaluate whether
annotation size has an effect on the variance of similarity
values. Second, we evaluate whether the difference in the
number of annotations between the entities that are com-
pared has an effect on the similarity measure, and quan-
tify the effects through measures of correlation. Third,
we evaluate whether the depth of the annotation classes
has an effect on similarity measures. Finally, we classify
semantic similarity measures in different categories based
on how they behave with respect to annotation size, differ-
ences in annotation size and depth of annotation classes,
using the correlation coefficients between similarity value.
To measure the effects of annotation size, we fix the
number of annotations of entities in our test corpus, and
compare those with a certain number of annotations to all
other entities. As we have generated 100 entities for each
of the 55 annotation sizes in our corpus, we obtain a distri-
bution of 550,000 (100 × 5500) similarity values for each
annotation size. In the resulting distribution of similarity
values, we compute average (arithmetic mean) similarity
and variance. To determine if, and how much, the sim-
ilarity values increase with annotation size, we compute
Spearman and Pearson correlation coefficients for each
similarity measure. The results for a selected set of simi-
larity measures are shown in Table 1, and for Resniks sim-
ilarity measure [20] (with the Best Match Average strategy
for combining pairwise measures) and the simGIC mea-
sure [1] in Fig 1.We find that, in general and across almost
all similarity measures, similarity values increase with the
number of annotations associated with an entity. The vari-
ance in the average similarities, however, either increases
or decreases with the annotation size, depending on the
similarity measure.
To determine whether the results we obtain also hold for
a real biological dataset, we further evaluated the semantic
similarity between yeast proteins using a set of selected
semantic similarity measures. We find that the results in
our test corpus are also valid for the semantic similarly
of yeast proteins. Figure 1 shows the average similarity of
yeast proteins as a function of the annotation size for two
semantic similarity measures.
For example, the protein YGR237C has only a single
annotation, and the average similarly, using the simGIC
measure, is 0.035 across the set of all yeast proteins. On
the other hand, protein CDC28, a more richly annotated
protein with 55 annotations, has as average similarly 0.142
(more than 4-fold increase). These results suggest that
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 4 of 10
Table 1 Spearman and Pearson correlation coefficients between similarity value and absolute annotation size as well as between
variance in similarity value and annotation size
Similarity measure Spearman Pearson
Yeast Synthetic GO Synthetic HPO Yeast Synthetic GO Synthetic GO
Average Variance Average Variance Average Variance Average Variance Average Variance Average Variance
GIC (Graph
Information
Content)
0.929780 0.251586 0.970924 0.773449 0.953247 0.980159 0.861348 0.117734 0.831167 0.744321 0.802873 0.958817
NTO (Normalized
Term Overlap)
0.178345 0.860012 0.248990 0.976335 0.123304 0.988240 0.014072 0.682683 0.009088 0.574883 0.158914 0.593458
UI (Union
Intersection)
0.892631 0.298097 0.879582 0.934921 0.729942 0.995599 0.788675 0.030649 0.777515 0.914405 0.736711 0.935415
BMA with
Jiang, Conrath
1997
0.960133 0.892027 0.998773 0.993506 0.999351 0.996609 0.892576 0.812184 0.895020 0.629497 0.907974 0.692269
BMA with
Lin 1998
0.980519 0.800362 0.998918 0.994733 0.999134 0.998052 0.925181 0.772250 0.896497 0.638574 0.917599 0.677309
BMA with
Resnik 1995
0.980519 0.717457 0.998773 0.994228 0.998918 0.998124 0.939044 0.703981 0.895107 0.642652 0.917738 0.675426
BMA with
Schlicker 2006
0.980519 0.800362 0.998918 0.994733 0.999134 0.998052 0.925181 0.772250 0.896497 0.638574 0.917599 0.677309
some entities have, on average and while comparing simi-
larity to exactly the same set of entities, higher similarity,
proportional to the number of annotations they have.
As our second experiment, we evaluate whether the
difference in annotation size has an effect on the similar-
ity measure. We follow the same strategy as in our first
experiment: we have used the same datasets but measured
the average similarities as function of absolute difference
of compared entities. For the annotation sizes from 1 to
55 we get 55 groups of similarities with annotation size
difference from 0 to 54, and for each group we com-
puted average similarity and variance in similarity values.
Furthermore, we computed Pearsson and Spearman cor-
relation coefficients between annotation size difference
and average similarities to determine the sensitivity of the
similarity to annotation size difference. Figure 1 shows our
results using synthetic data as well as functional anno-
tations of yeast proteins for Resniks similarity measure
(using the Best Match Average strategy) and the simGIC
measure, and Table 2 summarizes the results. Full results
are available as supplementary material. We find that for
most measures, average similarity decreases as the dif-
ference in annotation size increases, while the variance
in similarity values behaves differently depending on the
similarity measure.
In our third experiment, we evaluate whether the depth
of the annotation classes has an effect on the similarity
measure. We use our fourth dataset which we randomly
generated based on the depth of classes in the GO. The
maximum depth in GO is 17, and we generate 17 groups
of random annotations. We then compute the average
similarity of the synthetic entities within one group to
all the other groups, and report Pearsson and Spearman
correlation coefficients between annotation class depth
and average similarities to determine the sensitivity of
the similarity to annotation class depth. Figure 1 shows
our results using synthetic data as well as functional
annotations of yeast proteins for Resniks similarity mea-
sure (using the Best Match Average strategy) and the
simGIC measure, and Table 2 summarizes the results. We
find that for most measures, average similarity increases
with the depth of the annotations, i.e., the more spe-
cific a class is the higher the average similarity to other
classes.
A classification of similarity measures
Our finding allows us to broadly group semantic similar-
ity measures into groups depending on their sensitivity to
annotation size and difference in annotation size. We dis-
tinguish positive correlation (Pearsson correlation > 0.5),
no correlation (Pearsson correlation between ?0.5 and
0.5), and negative correlation (Pearsson correlation< 0.5),
and classify the semantic similarity measures based on
whether they are correlated with annotation size, dif-
ference in annotation size, and depth. Additional file 1:
Table S1 provides a comprehensive summary of our
results.
By far the largest group of similarity measures has a
positive correlation between annotation size and simi-
larity value, and a negative correlation between variance
and annotation size. Popular similarity measures such as
Resniks measure [20] with the Best Match Average com-
bination strategy, and the simGIC similarity measure [23],
fall in this group. A second group of similarity measures
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 5 of 10
Fig. 1 The distribution of similarity values as a function of the annotation size (top), annotation size difference (middle) and annotation class depth
(bottom) for Resniks measure (using the Best Match Average strategy) and the simGIC measure
has no, or only small, correlation between annotation size
and similarity values, and might therefore be better suited
to compare entities with a large variance in annotation
sizes. The Normalized TermOverlap (NTO)measure [24]
falls into this group. Finally, a third group results in lower
similarity values with increasing annotation size.
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 6 of 10
Table 2 Spearman and Pearson correlation coefficients between similarity value and difference in annotation size as well as between
variance in similarity value and difference in annotation size
Similarity measure Spearman Pearson
Yeast Synthetic GO Synthetic HPO Yeast Synthetic GO Synthetic GO
Average Variance Average Variance Average Variance Average Variance Average Variance Average Variance
GIC (Graph
Information
Content)
0.895310 0.931818 0.999928 0.999784 0.999784 0.997835 0.875583 0.503795 0.964250 0.484246 0.963553 0.496135
NTO (Normalized
Term Overlap)
0.901443 0.233045 0.999784 0.961833 0.999784 0.959524 0.882986 0.192168 0.990210 0.848649 0.993038 0.849263
UI (Union
Intersection)
0.909524 0.924459 1.000000 0.658658 1.000000 0.518687 0.906605 0.596963 0.963476 0.547645 0.963569 0.508495
BMA with Jiang,
Conrath 1997
0.283838 0.925830 0.902597 0.521861 0.891486 0.770130 0.074788 0.850654 0.834208 0.495874 0.848264 0.735985
BMA with
Lin 1998
0.462843 0.674892 0.901587 0.552237 0.891126 0.731530 0.303157 0.707318 0.836486 0.517670 0.852998 0.693744
BMA with
Resnik 1995
0.578211 0.579149 0.901587 0.537807 0.891126 0.699856 0.442458 0.487544 0.835991 0.507179 0.854007 0.670199
BMA with
Schlicker 2006
0.462843 0.674892 0.901587 0.552237 0.891126 0.731530 0.303157 0.707318 -0.836486 0.517670 0.852998 0.693744
Impact on data analysis
In order to test our results on an established biological
use case involving computation of semantic similarity, we
conducted an experiment by predicting protein-protein
interactions using the similarity measures. Prediction of
protein-protein interactions is often used to evaluate
and test semantic similarity measures [810], but simi-
lar methods and underlying hypotheses are also used for
candidate gene prioritization [25] in guilt-by-association
approaches [12].
We use our manually curated set of yeast gene anno-
tations and then generated random GO annotations for
each protein in this set while maintaining the annotation
size fixed. Specifically, to generate a completely random
annotation dataset, we replace each GO annotation of
each protein in our yeast dataset by a random GO class.
Thereby, the number of annotations for each protein
remains constant, while the content of the annotation is
replaced by a random GO class. We then compute pair-
wise semantic similarity between the proteins, once using
the real annotations and additionally using the randomly
generated annotations, and we use the resulting ranking
as prediction of a protein-protein interaction. Using real
protein-protein interactions from the BioGRID database
[19], we compute the true positive rate and false positive
rate of the predictions for each rank and plot the receiver
operating characteristic (ROC) curves for both cases. The
ROC curves are shown in Fig. 2 for simGIC and Resnik
similarity measure. For example, for predicting PPIs using
Resniks similarity measure and the BMA strategy, the
area under the ROC curve (ROC AUC) using real biolog-
ical annotations is 0.69, while the ROC AUC for random
annotations is 0.65. Despite the complete randomization
of the annotations, ROC AUC is significantly (p ? 10?6,
one-sided Wilcoxon signed rank test) better than ran-
dom. We repeat this experiment with human and mouse
PPIs and Resniks similarity measure (Fig. 3, and find that
in each case, random annotations provide a predictive
signal. For mouse PPIs, ROC AUC with random annota-
tions is 0.63 while real GO annotations result in a ROC
AUC of 0.74, and for human PPIs, ROC AUC with ran-
dom annotations is 0.54 and 0.58 with real annotations.
In both cases, the ROC curves are significantly better
than random (p ? 10?6, one-sided Wilcoxon signed rank
test).
We further test if this phenomenon also holds for other
applications of semantic similarity, in particular disease
gene prioritization through phenotype similarity. For this
purpose, we use the PhenomeNET systems [6, 26] and
compare the semantic similarity associated with loss of
function mouse models and human disease phenotypes.
Using real annotations, ROC AUC is 0.90, while the ROC
AUC for random phenotype annotations is 0.73 (Fig. 4),
demonstrating that the phenomenon also holds for other
use cases besides predicting PPIs.
The good performance in predicting PPIs in the absence
of biological information is rather surprising. We hypoth-
esized that well-studied proteins generally have more
known functions and more known interactions, and also
that genes involved in several diseases have more phe-
notype annotations. The Pearson correlation coefficient
between the number of interactions and the number of
functions in our yeast dataset is 0.34, in the human dataset
0.23, and 0.36 in the mouse PPI dataset. Similarly, in our
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 7 of 10
Fig. 2 ROC Curves for protein-protein interaction prediction using random annotations and interaction data from BioGRID for yeast
dataset of genedisease associations, there is a correla-
tion between the number of phenotype annotations and
the number of genedisease associations (0.42 Pearson
correlation coefficient). While the correlations are rela-
tively small, there is nevertheless a bias that is confirmed
by selecting a similarity measure that follows the same
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 8 of 10
Fig. 3 ROC Curves for protein-protein interaction prediction using random annotations and interaction data from BioGRID for mouse and human
Fig. 4 ROC Curves for gene-disease association prediction using PhenomeNet Ontology with mouse phenotype from MGI and OMIM disease
phenotype annotations compared with random annotations
Kulmanov and Hoehndorf Journal of Biomedical Semantics  (2017) 8:7 Page 9 of 10
bias. We tested whether the same phenomenon occurs
with another similarity measure that is not sensitive to
the annotation size or difference in annotation size. Using
Resniks measure with the Average strategy for combin-
ing the similarity values, we obtain a ROC AUC of 0.52
when predicting yeast PPIs. Although this ROC AUC is
still significantly better than random (p ? 10?6, one-
sidedWilcoxon signed rank test), the effect is much lower
compared to other measures.
In the context of gene networks, prior research has
shown that the amount of functional annotation and net-
work connectivity may result in biased results for certain
types of analyses, leading the authors to conclude that
the guilt by association principle holds only in excep-
tional cases [12]. Our analysis suggests that similar biases
may be introduced in applications of semantic similarity
measures such that heavily annotated entities will have,
on average and without the presence of any biological
relation between entities, a higher similarity to other enti-
ties than entities with only few annotations. A similar
but inverse effect exists for differences in annotation size.
Consequently, comparing entities with many annotations
(e.g., well-studied gene products or diseases) to entities
with few annotations (e.g., novel or not well-studied gene
products) will result, on average, in the lowest similar-
ity values, while comparing well-studied entities to other
well-studied entities (both with high annotation size and
no or only small differences in annotation size) will result
in higher average similarity for most similarity measures
even in the absence of any biological relation.
Conclusions
We find that the annotation size of entities clearly plays
a role when comparing entities through measures of
semantic similarity, and additionally that the difference in
annotation size also plays a role. This has an impact on
the interpretation of semantic similarity values in several
applications that use semantic similarity as a proxy for
biological similarity, and the applications include priori-
tizing candidate genes [6], validating text mining results
[27], or identifying interacting proteins [10]. Similarly to
a previous study on protein-protein interaction networks
[12], we demonstrate that the sensitivity of similarity mea-
sures to annotation size can lead to a bias when predict-
ing protein-protein interactions. These results should be
taken into account when interpreting semantic similarity
values.
In the future, methods need to be identified to correct
for the effects of annotation size and difference in annota-
tion size. Adding richer axioms to ontologies or employing
similarity measures that can utilize axioms such as dis-
jointness between classes [28] does not on its own suffice
to remove the bias we identify, mainly because the rela-
tion between annotated entities (genes or gene products)
and the classes in the ontologies does not consider dis-
jointness axioms. It is very common for a gene product to
be annotated to two disjoint GO classes, because one gene
product may be involved in multiple functions (such as
vocalization behavior and transcription factor activity)
since gene products are not instances of GO classes but
rather are related by a has function relation (or similar) to
some instance of the GO class. A possible approach could
be to rely on the exact distribution of similarity values for
individual entities [29] and use a statistical tests to deter-
mine the significance of an observed similarity value. An
alternative strategy could rely on expected similarity val-
ues based on the distribution of annotations in the corpus
and the structure of the ontology and adjusting similar-
ity values accordingly so that only increase over expected
similarity values are taken into consideration.
Additional file
Additional file 1: Supplementary Table. (PDF 17 kb)
Abbreviations
AUC: Area under curve; BMA: Best match average; GO: Gene ontology; HPO:
Human phenotype ontology; NTO: Normalized term overlap; PPI:
Protein-protein interaction; ROC: Receiver operating characteristic; SML:
Semantic measures library
Funding
This research was supported by funding from the King Abdullah University of
Science and Technology.
Availability of data andmaterials
All source code developed for this study is available from https://github.com/
bio-ontology-research-group/pgsim.
Authors contributions
RH conveived of the study, MK performed the experiments and evaluation, all
authors interpreted the results and wrote the manuscript. Both authors have
read and approved the final version of the manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Received: 15 October 2016 Accepted: 1 February 2017
RESEARCH Open Access
Building a semantic web-based metadata
repository for facilitating detailed clinical
modeling in cancer genome studies
Deepak K. Sharma1, Harold R. Solbrig1, Cui Tao2, Chunhua Weng3, Christopher G. Chute4 and Guoqian Jiang1*
Abstract
Background: Detailed Clinical Models (DCMs) have been regarded as the basis for retaining computable meaning
when data are exchanged between heterogeneous computer systems. To better support clinical cancer data
capturing and reporting, there is an emerging need to develop informatics solutions for standards-based clinical
models in cancer study domains. The objective of the study is to develop and evaluate a cancer genome study
metadata management system that serves as a key infrastructure in supporting clinical information modeling in
cancer genome study domains.
Methods: We leveraged a Semantic Web-based metadata repository enhanced with both ISO11179 metadata
standard and Clinical Information Modeling Initiative (CIMI) Reference Model. We used the common data elements
(CDEs) defined in The Cancer Genome Atlas (TCGA) data dictionary, and extracted the metadata of the CDEs using
the NCI Cancer Data Standards Repository (caDSR) CDE dataset rendered in the Resource Description Framework
(RDF). The ITEM/ITEM_GROUP pattern defined in the latest CIMI Reference Model is used to represent reusable
model elements (mini-Archetypes).
Results: We produced a metadata repository with 38 clinical cancer genome study domains, comprising a rich
collection of mini-Archetype pattern instances. We performed a case study of the domain clinical pharmaceutical
in the TCGA data dictionary and demonstrated enriched data elements in the metadata repository are very useful
in support of building detailed clinical models.
Conclusion: Our informatics approach leveraging Semantic Web technologies provides an effective way to build a
CIMI-compliant metadata repository that would facilitate the detailed clinical modeling to support use cases
beyond TCGA in clinical cancer study domains.
Keywords: Detailed Clinical Models (DCMs), Clinical Information Modeling Initiative (CIMI), Common Data Elements
(CDEs), The Cancer Genome Atlas (TCGA), Cancer Studies, Semantic Web Technologies
Background
Detailed Clinical Models (DCMs) have been regarded as
the basis for retaining computable meaning when data are
exchanged between heterogeneous computer systems [1].
Several independent clinical information modeling initia-
tives have emerged, including Health Level 7 (HL7)
Detailed Clinical Models (DCM) [2], ISO/CEN EN13606/
Open-EHR Archetype [3], Intermountain Healthcare
Clinical Element Models (CEMs) [4], and the Clinical
Information Model in the Netherlands [5]. The collective
clinical information modeling community has recently ini-
tiated an international collaboration effort known as the
Clinical Information Modeling Initiative (CIMI) [6]. The
primary goal of CIMI is to provide a shared repository of
detailed clinical information models based on common
formalism.
While the primary focus of these modeling efforts has
been on interoperability between electronic health rec-
ord (EHR) systems, there are also emerging interests in
the use of detailed clinical models in the context of
* Correspondence: jiang.guoqian@mayo.edu
1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,
Rochester, MN 55905, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 
DOI 10.1186/s13326-017-0130-4
clinical research and broad secondary use of EHR data. A
typical use case is the Office of the National Coordinator
(ONC) Strategic Health IT Advanced Research Projects
Area 4 (SHARPn) [7, 8], in which the Intermountain
Healthcare CEMs have been adopted for normalizing pa-
tient data for the purpose of secondary use. In the context
of clinical research, for example, Clinical Data Interchange
Standards Consortium (CDISC) intends to build
reusable domain-specific templates under its SHARE
project [9, 10].
To better support clinical cancer data capturing and
reporting, there is an emerging need to develop inform-
atics solutions for standards-based clinical models in
clinical cancer study domains. For example, National
Cancer Institute (NCI) has implemented the Cancer
Data Standards Repository (caDSR) [11], together with a
controlled terminology service (known as Enterprise
Vocabulary Services  EVS), as the infrastructure to sup-
port a variety of use cases from different clinical cancer
study domains. NCI caDSR has adopted the ISO 11179
metadata standard that specifies a standard data struc-
ture for a common data element (CDE) [12, 13].
The use case in this study is based on The Cancer
Genome Atlas (TCGA) Biospecimen Core Resource
(BCR) data dictionary [14]. The data dictionary is used
to create clinical data collection forms for different clin-
ical cancer genome study domains. TCGA clinical data
include vital status at time of report, disease-specific
diagnostic information, initial treatment regiments and
participant follow-up information [15]. The data dic-
tionary groups a preferred set of CDEs per TCGA
cancer study domain and renders them as an XML
Schema document. All clinical data collected are vali-
dated against these schemas, which provides a layer of
standards-based data quality control. All the CDEs are
recorded in the NCI caDSR repository, the implementa-
tion of which is based on the ISO 11179 standard. We
envision that cataloging a preferred set of CDEs for
each clinical cancer study domain is analogous to iden-
tifying or creating preferred Detailed Clinical Models
for a given domain.
The objective of the study is to develop and evaluate a
cancer genome study metadata management system that
serves as a key infrastructure in supporting clinical
information modeling in cancer genome study domains.
We leveraged a Semantic Web-based metadata reposi-
tory enhanced with both the ISO11179 metadata
standard and the Clinical Information Modeling Initia-
tive (CIMI) Reference Model (RM). We used the
CIMI-compliant archetype patterns to represent pre-
ferred set of CDEs used in the TCGA data dictionary
and identified additional data elements from caDSR
for a given domain. And then we loaded a RDF-
metadata repository with data elements based on these
archetype patterns. We hypothesize that clinical infor-
mation modeling tools can leverage such metadata reposi-
tory to reuse data elements already widely adopted by
clinical genomic research studies (e.g., TCGA studies).
Methods
Materials
ISO 11179 and its OWL representations
ISO 11179 is an international standard known as the
ISO/IEC 11179 Metadata Registry (MDR) standard [12].
It consists of six parts. Part 3 of the standard uses a
meta-model to describe the information modeling of a
metadata registry, which provides a mechanism for un-
derstanding the precise structure and components of
domain-specific models.
Figure 1 shows a diagram illustrating the high-level
data description meta-model in the ISO 11179 specifica-
tion. The Data Element is one of the foundational con-
cepts in the specification. ISO 11179 also specifies the
relationships and interfaces between data elements,
value sets (i.e., enumerated value domains) and standard
terminologies.
Several Semantic Web-based representations of the
ISO 11179 Part 3 meta-model have been created for pro-
jects including the XMDR project [16], Semantic MDR
in a European SALUS project [17] and CDISC2RDF in
FDA PhUSE Semantic Technology project [18]. In the
present study, we utilize a meta-model schema in OWL/
RDF developed in the CDISC2RDF project, which is a
subset of ISO 11179 Part 3 meta-model.
Reference model in UML
The CIMI Reference Model (RM) is an information
model from which CIMIs clinical models (i.e., arche-
types) are derived [6]. The CIMI DCMs are expressed as
formal constraints on the underlying RM. The CIMI
Reference Model is represented in the Unified Modeling
Language (UML). The September 5, 2014 version of the
CIMI Reference Model (v2.0.1) had four packages: 1)
CIMI Core Model; 2) Data Value Types; 3) Primitive
Types and 4) Party. While the core CIMI Reference
Model Classes are defined in the CIMI Core Model
package, the Party package defines the generic concepts
of PARTY, ROLE and related details for describing po-
tential demographic attributes. Both of these packages
utilize the types declared in the Data Value Types and
Primitive Types packages.
Figure 2 shows partial view of UML Class diagram of
the CIMI Core Model. The classes ITEM, ITEM_GROUP,
and ELEMENT form very generic pattern (referred as
ITEM/ITEM_GROUP Pattern here onwards) that can be
used recursively to represent almost any clinical informa-
tion. The ITEM_GROUP class represents the grouping
variant of ITEM as an ordered list whereas the ELEMENT
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 2 of 11
class represents a leaf ITEM which carries no further re-
cursion. Figure 3 shows Archetype Definition Language
(ADL) [19] definition of a Body Temperature archetype,
which illustrates how ITEM_GROUP and ELEMENT can
be combined when representing a clinical concept.
The caDSR CDE dataset
NCI caDSR is part of the NCI Cancer Common Onto-
logical Representation Environment (caCORE) infra-
structure and uses caCORE resources to support data
standardization in cancer clinical research studies [11].
The system includes an administrator web interface for
overall system and CDE management activities. Inte-
grated with caCORE Enterprise Vocabulary Services
(EVS), the CDE Curation Tool aids developers in con-
sumption of NCI controlled vocabulary and standard
terminologies for naming and defining CDEs.
NCI caDSR provides the ability to download CDEs
in either Excel or XML format [20], which we used to
download an XML image of all non-retired production
CDEs (i.e., CDEs with Workflow status NOT = RE-
TIRED) as of August 7, 2014. Figure 4 shows an XML
rendering of the CDE Pharmacologic Substance Begin
Occurrence Month Number from the NCI caDSR.
The TCGA data dictionary
The Cancer Genome Atlas (TCGA), a joint venture sup-
ported by the NCI and the National Human Genome
Fig. 1 High-level data description meta-model in ISO 11179 specification
Fig. 2 CIMI Core Model in UML Diagram
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 3 of 11
Fig. 3 The definition section of an archetype for a CIMI Body temperature concept. The definition is rendered in archetype definition
language (ADL)
Fig. 4 The CDE Pharmacologic Substance Begin Occurrence Month Number in XML recorded in the NCI caDSR
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 4 of 11
Research Institute (NHGRI), is a comprehensive and co-
ordinated effort to accelerate the understanding of the
molecular basis of cancer through the application of
genome analysis technologies, including large-scale gen-
ome sequencing. Being a component of TCGA Research
Network, the Biospecimen Core Resource (BCR) serves
as the centralized tissue processing and clinical data col-
lection center. A BCR data dictionary has been produced
using the standard CDEs from NCI caDSR. The CDEs in
the data dictionary are publicly available in the XML for-
mat. In this project, we will download a snapshot of the
data dictionary from the TCGA website [14]. Figure 5
shows a TCGA data dictionary variable Month Of Drug
Therapy Start is annotated with the CDE Pharmaco-
logic Substance Begin Occurrence Month Number from
the NCI caDSR.
Methods
Figure 6 shows the system architecture of our proposed
approach. The system comprises four layers: a RDF
transformation layer; a RDF store-based persistence
layer; a semantic services layer and an authoring applica-
tion layer. This paper focuses on transformation layer
and persistence layer.
RDF transformation of caDSR and TCGA datasets
The XML2RDF tool, developed by the Redefer project
[21], was used to transform the XML-based TCGA data
dictionary and the XML-based caDSR production CDEs
into a corresponding RDF representation. We loaded the
resulting RDF datasets into a 4store instance, an open-
source RDF triple-store and exposed them via a
SPARQL endpoint, allowing us to use the SPARQL
query language to preform semantic queries across the
datasets.
OWL-based schema for CIMI Reference Model and ISO
11179
We used the latest version of CIMI Reference Model
(v2.0.1) in the XML Metadata Interchange (XMI) for-
mat. We then converted the CIMI Reference Model
from XMI to RDF format using the Redefer XML2RDF
transformation services [21]. We then defined the
SPARQL queries to retrieve the UML based elements of
the CIMI Reference Model such as classes, attributes
and associations. We created a JAVA program that pro-
duces an OWL rendering of the CIMI Reference Model
using the UML2OWL mappings specified by the Object
Management Group (OMG) Ontology Definition meta-
model (ODM) standard [22]. We finally harmonized and
Fig. 5 A TCGA data dictionary variable Month Of Drug Therapy Start annotated with the CDE Pharmacologic Substance Begin Occurrence
Month Number that is originally recorded in the NCI caDSR
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 5 of 11
created an OWL-based schema for CIMI Reference
Model and ISO11179.
Defining and populating reusable archetype patterns
We defined reusable archetype patterns that capture
the clinical cancer domains defined in the TCGA
data dictionary, their associated CDEs and the meta-
data structures (Object Class, Property, Value
Domain, etc.) recorded in the caDSR data repository.
We then defined a collection of SPARQL queries to
retrieve the metadata elements from both the TCGA
data dictionary and the caDSR CDE dataset. Figure 7
shows a SPARQL query example that retrieves all
CDEs of the domain clinical pharmaceutical
defined in the TCGA data dictionary and their meta-
data recorded in caDSR CDE dataset. We also devel-
oped a JAVA program that populates all reusable
archetype patterns in TCGA clinical cancer domains
into the instance data using the OWL-based schema
that we created.
Fig. 6 System architecture of our proposed approach
Fig. 7 A SPARQL query example that retrieves all CDEs of the domain clinical pharmaceutical
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 6 of 11
Evaluation of clinical utility
We performed a case study for the domain Clinical
Pharmaceutical to demonstrate clinical utility of our
approach. Specifically, we demonstrated how many
properties and enumerated value domains are enriched
for the domain through the ISO 11179-based data ele-
ments recorded in the NCI caDSR. We then evaluate
clinical utility of the enriched data elements using a
Medication template defined in CDISC Clinical Data
Acquisition Standards Harmonization (CDASH) stand-
ard [23]. We created the alignment between the CDISC
Medication template and the CDEs retrieved from the
domain Clinical Pharmaceutical and the alignment con-
sensus was achieved through a series of discussions
among the project team members.
Results
In total, the TCGA data dictionary contains 38 clinical
cancer domains and 775 CDEs, which covers 21 cancer
types. Table 1 shows a list of examples showing the clin-
ical cancer domains and the number of CDEs in each
domain.
We created an OWL rendering of CIMI Reference
Model and harmonized it with the ISO 11179 metadata
model schema, in which all classes defined in the CIMI
Reference Model are asserted as the subclasses of an
ISO 11179 class mms:AdministeredItem. Figure 8 shows
a screenshot of Protégé 4 environment illustrating the
class hierarchy of OWL-based schema for harmonized
CIMI Reference Model with ISO 11179 model.
We populated reusable archetype patterns against the
OWL-based schema and produced a metadata reposi-
tory based in RDF format. The repository covers all 38
clinical cancer study domains, comprising 316 distinct
object classes, 4719 distinct properties, 1015 non-
enumerated value domains and 1795 enumerated value
domains (i.e., value sets).
Table 2 shows two pattern examples extracted from
the TCGA domain clinical pharmaceutical. Pattern 1
captures a number of CDEs asserted in the TCGA data
dictionary; Pattern 2 captures equivalent metadata
structures (Object Class, Property, Value Domain, etc.)
recorded in the caDSR data repository. The 7 CDEs
captured in Pattern 1 have their Object Class in com-
mon that is Pharmacologic Substance. The Pharma-
cologic Substance is linked with three Property
instances: Begin Occurrence, End Occurrence and
Continue Occurrence. The properties are associated
with 4 Value Domains: Event Year Number, Event
Month Number, Event Day Number, and Yes No
Character Indicator.
Evaluation results
As a case study, we looked into the domain Clinical
Pharmaceutical that contains 18 CDEs. We retrieved the
object classes recorded in caDSR and identified 11 dis-
tinct object classes. And then, we retrieved globally in
the caSDR CDE datasets for all properties and value
domains associated with the 11 object classes. Figure 9
shows a bar graph illustrating the enrichment for the do-
main Clinical Pharmaceutical by data element, property,
value domain and enumerated value domain. The graph
indicated that the domain is greatly enriched with prop-
erties and value domains associated with those 11 object
classes, which forms a pool of data elements that could
be used to build detailed clinical models in this domain.
To evaluate clinical utility of our approach, we aligned
the data elements between CDASH Medication and
TCGA Clinical Pharmaceutical. Table 3 shows the align-
ment results. Out of 20 CDASH data elements with their
data collection questions, 9 of them aligned with the
CDEs asserted in the TCGA data dictionary whereas 10
of them aligned with those enriched data elements iden-
tified from our system. This shows that the addition of
the enriched data elements can not only guide us to
evaluate a data dictionary by identifying the gaps, but
also provide a pool of data elements to choose from to
help build clinical models. We believe that the results
demonstrated that enriched data elements are useful in
building a clinical model for the use cases beyond ori-
ginal TCGA data dictionary.
Discussion
In this study, we first transformed the TCGA data diction-
ary and the caDSR CDE dataset from their XML format
to the RDF-based representations. This transformation
makes it easier to query caDSR metadata elements that
correspond to the CDEs defined in the TCGA data dic-
tionary. The TCGA data dictionary terminology bindings
Table 1 A list of examples showing TCGA clinical cancer study
domains
Clinical Cancer
Domains
Number
of CDEs
Notes
clinical shared 98
clinical laml 49 Acute Myeloid Leukemia [LAML]
clinical cesc 47 Cervical squamous cell carcinoma and
endocervical adenocarcinoma [CESC]
clinical lgg 33 Brain Lower Grade Glioma [LGG]
clinical lihc 31 Liver hepatocellular carcinoma [LIHC]
clinical prad 25 Prostate adenocarcinoma [PRAD]
clinical paad 23 Pancreatic adenocarcinoma [PAAD]
clinical thca 20 Thyroid carcinoma [THCA]
clinical shared stage 19
clinical pharmaceutical 18
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 7 of 11
enable exploration of additional metadata associated
with CDEs that would otherwise be challenging to
associate programmatically. These newly discovered
elements help get better insight about the gaps in their
proper and efficient usage in the models that data
dictionaries intend to represent. Second, the CIMI
Reference Model offers a simple recursive pattern
(with its ITEM, ITEM_GROUP and ELEMENT clas-
ses) to represent CDEs in each TCGA cancer genome
study sub-domain, as instances. The CIMI Reference
Model is transformed from its UML format to a corre-
sponding OWL representation and harmonized it with
a subset of ISO 11179 metadata model. As indicated
above, the transformation of the TCGA data diction-
ary, caDSR CDEs, CIMI Reference Model, ISO 11179
into RDF normalizes their representation and makes it
easier to query the content using a standard SPARQL
end-point. Finally, we performed a case study in the
domain of Clinical Pharmaceutical and demonstrated
the clinical utility of our proposed approach. We con-
sider that this approach is novel as to our best know-
ledge this is the first attempt trying to reuse the CDEs
recorded in the caDSR for supporting creating clinical
information models based on the CIMI Reference
Model.
The metadata repository system proposed in this study
has the following three major implications. The first im-
plication is that the system would enable producing a
Fig. 8 A screenshot of Protégé 4 environment showing an OWL-based schema. The schema is for a CIMI Reference Model harmonized with ISO
11179 model
Table 2 Two pattern examples extracted from the TCGA
domain clinical pharmaceutical
Pattern 1 Pattern 2
clinical pharmaceutical [ITEM_GROUP] clinical pharmaceutical
[ITEM_GROUP]
Pharmacologic Substance
[ITEM_GROUP]
Year Of Drug Therapy Start [ELEMENT] Begin Occurrence
[ITEM_GROUP]
Month Of Drug Therapy Start [ELEMENT] Event Year Number
[ELEMENT]
Day Of Drug Therapy Start [ELEMENT] Event Month Number
[ELEMENT]
Event Day Number
[ELEMENT]
Year Of Drug Therapy End [ELEMENT] End Occurrence
[ITEM_GROUP]
Month Of Drug Therapy End [ELEMENT] Event Year Number
[ELEMENT]
Day Of Drug Therapy End [ELEMENT] Event Month Number
[ELEMENT]
Event Day Number
[ELEMENT]
Therapy Ongoing [ELEMENT] Continue Occurrence
[ITEM_GROUP]
Yes No Character Indicator
[ELEMENT]
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 8 of 11
Fig. 9 A bar graph showing the enrichment for the domain Clinical Pharmaceutical. The enrichment by data element, property, value domain
and enumerated value domain is illustrated
Table 3 Alignment results of data elements between CDASH Medication and TCGA Clinical Pharmaceutical
Question Text Prompt Data Element
Name
TCGA CDEs or Enriched Data Elements
Were any medications taken? Any meds Administered
What is the medication/treatment identifier? CM number Identifier; Unique Identifier
What was the term for the medication/therapy taken? Medication or Therapy Drug Name
Did the subject take < specific medication/treatment > ? <specific medication/
treatment>
Cytokine Administered; Placebo
Bevacizumab Administered; HER2/neu Administered
What were the active ingredients? Active Ingredients PubChem Compound Identifier
For what indication was the medication/therapy taken? Indication Indication
What was the ID for the adverse events(s) for which the
medication was taken?
AE ID Toxicity Description; Toxicity Grade
What was the ID of the medical history condition(s)
for which the medication was taken?
MH ID
What was the individual dose of the medical/therapy? Dose Prescribed Dose
What was the total daily dose of the medication therapy? Total Daily Dose Cumulative Agent Total Dose
What was the unit of the medical/therapy? Dose Unit Total Dose Units;Prescribed Dose Units
What was the dose form of the medication/therapy? Dose Form Pharmaceutical Dosage Form Code
What was the frequency of the medication/therapy? Frequency Number Cycles
What was the route of administration of the
medication/therapy?
Route Route Of Administration
What was the start date of the medication/therapy? Start Date Year Of Drug Therapy Start;Month Of Drug Therapy Start;
Day Of Drug Therapy Start
What was the start time of the medication/therapy? Start Time Agent Administered Begin Time
Was the medication/therapy taken prior to the study? Taken Prior to Study? Prior Therapy Treatment Regimen
What was the end date of the medication/therapy? End Date Year Of Drug Therapy End; Month Of Drug Therapy End;
Day Of Drug Therapy End
What was the end time of the medication/therapy? End Time Agent Administered End Time
Is the medication/therapy still ongoing? Ongoing Therapy Ongoing
Bold italic font indicates an enriched data element
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 9 of 11
profile of CIMI-compliant detailed clinical models for
TCGA clinical cancer study domains by leveraging the
best practice of detailed clinical modeling in CIMI com-
munity. Pattern 1 as shown in Table 2 is designed to
capture a preferred set of CDEs and metadata for each
domain asserted in the TCGA data dictionary. The se-
mantics captured in Pattern 1 should be equivalent to
those asserted in the TCGA XML Schemas. In other
words, Pattern 1 serves as the CIMI-compliant represen-
tation of a preferred set of CDEs in a TCGA cancer
study domain.
The second implication is that we gained new insights
on how the ISO 11179 standard could interact with the
CIMI Reference Model for supporting detailed clinical
modeling. The added value would ultimately be the abil-
ity to represent ISO 11179 based constructs as con-
straints on CIMI Reference Model. Pattern 2 is designed
to capture equivalent metadata structures (Object Class,
Property, Value Domain, etc.) of a CDE informed by ISO
11179. As shown in Table 2, Pattern 2 is represented in
a post-coordination manner following certain rules. The
approach used in Pattern 2 is similar to the dissection
approach that is a common practice used in the termin-
ology space for development of reusable terminologies.
The dissection approach was originally used by the
GALEN project [24]. In fact, the components in the
metadata structure are usually annotated with concept
codes from a standard terminology. In NCI caDSR, NCI
Thesaurus has been largely used for the annotation pur-
pose. Taking a look at Pattern 2 as shown in Table 2,
Pharmacologic Substance, an object class, has NCIt
code C1909 annotated; Begin Occurrence, a property,
has NCI codes C25431:C25275 annotated. In addition,
the post-coordination-based approach enabled us to glo-
bally retrieve all properties associated with a particular
object class. For example, there are globally 40 proper-
ties associated with the object class Pharmacologic
Substance in NCI caDSR, resulting in additional 37
more properties and 5 more associated value domains.
Figure 9 also shows such enrichment for the domain
Clinical Pharmaceutical. We believe that our approach
would produce a rich collection of archetype patterns
and constraints (e.g., datatypes, value sets, terminology
bindings, etc.) that could be used to facilitate detailed
clinical modeling in clinical cancer study domain for use
cases beyond TCGA.
The third implication is that we demonstrated the
value of using Semantic Web technologies and tools in
building such metadata repository. First, we created an
OWL rendering of CIMI Reference Model. This
allowed us to seamlessly integrate the CIMI Reference
Model with an existing OWL-based ISO 11179 model.
We envision that CIMI Reference Model and ISO
11179 are two complementary standards that could
greatly enhance the detailed clinical modeling and its
metadata management. Second, we used XML2RDF
Transformation technology to transform the XML-
based TCGA data dictionary and the XML-based
caDSR CDE dataset into a RDF-based format. This
allows us to use standard SPARQL query language to
define queries to retrieve metadata of a CDE across
datasets while this enables a high-throughput approach
for globally searching metadata of nearly 50,000 CDEs
recorded in the NCI caDSR. Third, we populated re-
usable archetype patterns against the OWL-based
schema using a RDF-based representation. This will
allow us to leverage the built-in OWL DL reasoning
capability and the RDF validation tools such as Shape
Expressions [25] to check the consistency and data
quality of CIMI-compliant detailed clinical models.
Conclusion
In summary, we developed a use case-driven approach
that enables a Semantic Web-based metadata repository
in support of authoring detailed clinical models in clin-
ical cancer study domains. Future work will include 1)
developing Semantic Web-based RESTful services for
the archetype patterns recorded in the metadata reposi-
tory; 2) building quality assurance mechanism for
CIMI-compliant detailed clinical models leveraging
OWL DL reasoning and RDF validation tools; 3) creat-
ing tools for authoring detailed clinical models using
the metadata repository as the backend; 4) developing
tools that enable the transformation of detailed clinical
models between RDF/OWL-based format and ADL-
based format.
Abbreviations
ADL: Archetype definition language; BCR: Biospecimen core resource;
caDSR: Cancer data standards repository; CDASH: Clinical data acquisition
standards harmonization; CDISC: Clinical data interchange standards
consortium; CEMS: Clinical element models; CIMI: Clinical information
modeling initiative; CDEs: Common data elements; DCMs: Detailed clinical
models; EHR: Electronic health record; EVS: Enterprise vocabulary services;
MDR: Metadata registry; NCI: National Cancer Institute; NHGRI: National
Human Genome Research Institute; ODM: Ontology definition meta-model;
OMG: Object management group; ONC: Office of the National Coordinator;
RDF: Resource description framework; RM: Reference model; SHARPn: Strategic
health it advanced research projects area 4; TCGA: The cancer genome atlas;
UML: Unified modeling language
Acknowledgments
The authors would like to thank Julie Evans and Dr. Rebecca Kush from
CDISC, for their kindly support and input.
Funding
The study is supported in part by a NCI U01 Project  caCDE-QA (U01
CA180940). The funding body did not participate in the design of the study
and collection, analysis, and interpretation of data and in writing the manuscript.
Availability of data and materials
All schemas and datasets produced in this study can be accessible publicly
at: https://github.com/gqjiang/cimi2rdf.
Sharma et al. Journal of Biomedical Semantics  (2017) 8:19 Page 10 of 11
Authors contributions
Conceived and designed the study: GJ, HRS. Developed the system: DS, HRS,
GJ. Designed and conducted the system evaluation: DS, HRS, GJ, CT, CW.
Wrote the paper: GJ, DS, HRS. Reviewed and edited the paper: CT, CW, CGC.
All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,
Rochester, MN 55905, USA. 2University of Texas Health Science Center at
Houston, Houston, TX, USA. 3Columbia University, New York, NY, USA. 4Johns
Hopkins University, Baltimore, MD, USA.
Received: 16 May 2016 Accepted: 30 May 2017
RESEARCH Open Access
The bacterial interlocked process ONtology
(BiPON): a systemic multi-scale unified
representation of biological processes in
prokaryotes
Vincent J. Henry 1,2, Anne Goelzer2* , Arnaud Ferré1, Stephan Fischer2, Marc Dinh2, Valentin Loux2,
Christine Froidevaux1 and Vincent Fromion2
Abstract
Background: High-throughput technologies produce huge amounts of heterogeneous biological data at all cellular
levels. Structuring these data together with biological knowledge is a critical issue in biology and requires
integrative tools and methods such as bio-ontologies to extract and share valuable information. In parallel, the
development of recent whole-cell models using a systemic cell description opened alternatives for data integration.
Integrating a systemic cell description within a bio-ontology would help to progress in whole-cell data integration
and modeling synergistically.
Results: We present BiPON, an ontology integrating a multi-scale systemic representation of bacterial cellular
processes. BiPON consists in of two sub-ontologies, bioBiPON and modelBiPON. bioBiPON organizes the systemic
description of biological information while modelBiPON describes the mathematical models (including parameters)
associated with biological processes. bioBiPON and modelBiPON are related using bridge rules on classes during
automatic reasoning. Biological processes are thus automatically related to mathematical models. 37% of BiPON
classes stem from different well-established bio-ontologies, while the others have been manually defined and
curated. Currently, BiPON integrates the main processes involved in bacterial gene expression processes.
Conclusions: BiPON is a proof of concept of the way to combine formally systems biology and bio-ontology. The
knowledge formalization is highly flexible and generic. Most of the known cellular processes, new participants or
new mathematical models could be inserted in BiPON. Altogether, BiPON opens up promising perspectives for
knowledge integration and sharing and can be used by biologists, systems and computational biologists, and the
emerging community of whole-cell modeling.
Keywords: Systems biology, Multi-scale systemic description, Prokaryotic biological processes, Mathematical
models, Biological ontology
* Correspondence: Anne.Goelzer@inra.fr
Equal contributors
2INRA, UR1404, MaIAGE, Université Paris-Saclay, Jouy-en-Josas, France
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Henry et al. Journal of Biomedical Semantics  (2017) 8:53 
DOI 10.1186/s13326-017-0165-6
Background
Systems biology emerged as a promising framework to
integrate the whole-cell for different model-organisms
[13]. However, current cell representations usually refer
to specific model organisms, which limits in practice the
transfer of whole-cell models to non-model organisms.
In contrast, bio-ontologies are a suitable framework for
systematically describing biological objects and thus fa-
cilitating knowledge transfer among organisms [4, 5]. In
this paper, we address the following question: how to
combine systems biology and bio-ontology?
Systems biology has its roots in engineering science and
conceptualizes the cell as a system composed of interacting
sub-systems [1, 611]. In this context, cellular processes are
typically described as biological subsystems whose inputs
(e.g. metabolites, proteins, or sequences, etc.) are converted
into outputs by dedicated molecular machines. The mo-
lecular machines are usually composed of proteins, con-
sume energy and chemical building blocks, and display a
characteristic of operation. This operation can be static or
dynamic, deterministic or/and stochastic and is generally
described by a formal mathematical model having inputs,
outputs and model parameters. For example, a mathemat-
ical model can be a nonlinear function or a set of ordinary
differential equations. The systemic representation of cells
is an efficient framework to interrelate all cellular entities
(metabolites, proteins, cellular processes, sequences, etc.),
together with their physical or biochemical properties (e.g.
kinetic parameters, etc.) [1, 2]. System biologists thus need
now an adequate format of systemic description of the
whole cell to transfer and share their models. Existing stan-
dardized formats for file exchange are adequate to exchange
mathematical models for specific cell processes [12, 13],
but remain limited to describe a whole-cell model, i.e. a sys-
temic multi-scale representation of interacting complex
subsystems.
Bio-ontologies have been developed to formalize and
integrate different pieces of biological knowledge [4].
The well-established Gene Ontology (GO) integrates the
molecular functions of gene products (GO-MF) with cel-
lular components (GO-CC) and biological processes
(GO-BP) [14]. The combined sub-ontologies are com-
monly used to annotate and characterize gene products
[5, 15], but there are also other useful bio-ontologies.
The Ontology of Microbial Phenotypes links the pheno-
types of bacteria to cellular processes [16]. The Ontology
of Genes and Genomes provides a list of genes from dif-
ferent organisms including prokaryotes [17], while the Se-
quence Ontology (SO) provides a detailed description of
polymers and polymer sequence patterns [18]. At another
level, the Pathway Ontology (PW) provides a classification
of metabolic, signaling and altered eukaryotic pathways
[19]. Independently, ChEBI (Chemical Entities of Bio-
logical Interest) acts as a reference for the classification of
general chemicals according to their chemical structures
and modifications [20]. The Systems Biology Ontology
(SBO) provides a controlled vocabulary for kinetic param-
eters and mathematical models of biological processes
[21]. Taken together, the existing bio-ontologies cover the
concepts necessary to the systemic representation of cells,
i.e., biological processes, molecules and mathematical
models of biological processes. However, the systemic rep-
resentation of the whole cell cannot be handled without
the addition of further logical relations between existing
ontologies.
In this paper, we demonstrates that a systemic multi-
scale representation of biological processes, the typical
perspective of systems biology, can be formally described
as an ontology, and how this ontology can be built based
on existing sparse bio-ontologies. As a proof of concept,
we developed the Bacterial interlocked Process ONtol-
ogy (BiPON) and showed that a) heterogeneous bio-
logical processes can be described with the systemic
representation and b) be linked automatically to math-
ematical models, and that c) information about these
processes can be enriched by automatic reasoning. As a
use case, we focus on bacterial gene expression pro-
cesses, which are well established and representative of
known biological processes. They cover, among many
other things, combination of polymers, sequence pat-
terns, single molecules or complexes within biological
processes, as well as cyclic or branched-point processes.
We demonstrated on the use case how a systemic repre-
sentation of living cells can be formally described and in-
tegrated into an ontological model, and what benefits
ensue from automatic reasoning on this ontology.
Methods
Description of biological processes, corpus building and
entity tagging
In the absence of an exhaustive controlled vocabulary in
systems biology, we use hereafter the notion of a bio-
logical process, which comprises the notions of (a) bio-
logical reaction and biochemical reaction as in KEGG
(Kyoto Encyclopedia of Genes and Genomes [22]) Reac-
tions database, (b) biological phenomenon, biological
pathway and biochemical pathway as in PW or KEGG
Pathway database, and finally (c) biological process as
in GO-BP. Moreover, we use the notion of a chemical
entity to denote any type of biological compound, in-
cluding metabolites, proteins, protein complexes, poly-
mers, to cite a few.
To develop a dedicated systemic representation for
each biological process involved in the bacterial gene ex-
pression, we applied the standard state-of-art approach
of system engineering. The approach involves two main
tasks. (A) We first gathered up-to-date available bio-
logical information about the biological process. (B) We
Henry et al. Journal of Biomedical Semantics  (2017) 8:53 Page 2 of 16
then converted the biological information into a sys-
temic representation using boxes, arrows, inputs and
outputs, and a mathematical model. We describe and
apply below the approach (A) and (B) on a specific ex-
ample (the formation of the 30S initiation complex) for
illustrative purposes. Note that the approach is generic
and can be applied on any biological process.
(A)We collected up-to-date knowledge about the bio-
logical processes from scientific literature (books,
peer-reviewed original articles, and reviews; see Add-
RESEARCH Open Access
Constructing an integrated gene similarity
network for the identification of disease
genes
Zhen Tian1, Maozu Guo1*, Chunyu Wang1, LinLin Xing1, Lei Wang2 and Yin Zhang2
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: Discovering novel genes that are involved human diseases is a challenging task in biomedical research.
In recent years, several computational approaches have been proposed to prioritize candidate disease genes. Most of
these methods are mainly based on protein-protein interaction (PPI) networks. However, since these PPI networks
contain false positives and only cover less half of known human genes, their reliability and coverage are very low.
Therefore, it is highly necessary to fuse multiple genomic data to construct a credible gene similarity network and
then infer disease genes on the whole genomic scale.
Results: We proposed a novel method, named RWRB, to infer causal genes of interested diseases. First, we construct
five individual gene (protein) similarity networks based on multiple genomic data of human genes. Then, an integrated
gene similarity network (IGSN) is reconstructed based on similarity network fusion (SNF) method. Finally, we employee
the random walk with restart algorithm on the phenotype-gene bilayer network, which combines phenotype similarity
network, IGSN as well as phenotype-gene association network, to prioritize candidate disease genes. We investigate the
effectiveness of RWRB through leave-one-out cross-validation methods in inferring phenotype-gene relationships. Results
show that RWRB is more accurate than state-of-the-art methods on most evaluation metrics. Further analysis shows that
the success of RWRB is benefited from IGSN which has a wider coverage and higher reliability comparing with current
PPI networks. Moreover, we conduct a comprehensive case study for Alzheimers disease and predict some novel
disease genes that supported by literature.
Conclusions: RWRB is an effective and reliable algorithm in prioritizing candidate disease genes on the genomic
scale. Software and supplementary information are available at http://nclab.hit.edu.cn/~tianzhen/RWRB/.
Keywords: Gene Ontology, Gene similarity networks, Similarity network fusion, Disease gene identification
Background
Prioritization of candidate disease genes is a fundamental
challenge in human health with applications to understand
disease mechanisms, diagnosis and therapy [15]. Many
human diseases are complex and polygenic, involving
linking genomic variation to clinical phenotype. Traditional
linkage analyses and association study have conducted sus-
ceptible genomic interval in the chromosomes [68].
However, since the susceptible locus may contain several
hundreds of genes, computational approaches are widely
accepted to further infer causal genes that are associated
with interested diseases [911].
Given a disease and its disease genes, the target of
prioritization is usually to measure the similarity bet-
ween candidate genes and the disease genes [1, 12, 13].
It is generally believed that it is the abnormal expression
of disease genes that lead to the diseases happen. The
disease genes are also called causal genes or disease
related genes for the diseases sometimes. Many methods
which take the guilt by association principle have been
* Correspondence: maozuguo@hit.edu.cn
1School of Computer Science and Engineering, Harbin Institute of
Technology, Harbin 150001, Peoples Republic of China
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32
DOI 10.1186/s13326-017-0141-1
proposed to prioritize candidate genes based on a com-
prehensive range of biological information [10, 1421].
They are devoted to fully characterize genes (or corre-
sponding gene products), to measure the similarity
between known disease genes and candidate genes more
precisely and reliably. These methods are usually called
feature-based methods [22]. The metric of similarity is
generally based on sequence-based features of genes
[2325], functional annotation of genes [13, 26, 27] and
protein-protein interaction data [28, 29]. The ultimate
goal is to discriminate disease genes and non-disease
genes based on certain characteristics of genes [30, 31].
More recently, many methods [3238] make use of
phenotype similarity between diseases to prioritize candi-
date disease genes [39, 40]. This is because phenotypic
similarity of diseases can help increase the total number of
known disease genes for less studied disease phenotypes
[41]. The underlying assumption for these methods is that
similar phenotypes are caused by functionally related
genes [12, 42]. These methods are usually called similarity-
based methods [22]. Lage [2] built a Bayesian model based
on PPI network and phenotype similarity network, and
then prioritized the candidate genes with the help of can-
didate protein complex. Kohler [32] first grouped diseases
into families and then employed a random walk from
known disease genes in its family to prioritize candidate
genes. Later, Wu [33] put forward a regression model,
named CIPHER, to exploit phenotype-gene associations.
More recently, Li [35] first constructed a heterogeneous
network by making the best use of the phenotype simila-
rity network and gene network as well as the phenotype-
gene relationship information. Then they employed the
random walk model, called RWRH, to infer disease genes.
Most methods for prioritizing candidate disease genes
above mainly rely on PPI networks. However, current
PPI networks mainly have two shortcomings. One is that
the coverage of the available PPI networks is typically
low [29, 43, 44]. Since the curated physical interactions
are generally preferred, they often lead to insufficient
coverage in human genome [45]. This may result in a
serious problem that some known disease genes cannot
be mapped into the PPI networks. To address this issue,
several researchers [6, 4648] have attempted to con-
struct gene semantic similarity network. For instance, Li
[6] employ a random walk with restart algorithm on the
multigraphs, which merges various genomic networks to
enlarge the range of candidate genes and increase the
noise tolerance of networks. However, these different
genomic networks do not integrate indeed. The weights
assigned to different networks are also difficult to
confirm.
The other is the low reliability of PPI networks [49].
Since a single data source is prone of bias and incom-
pleteness, integration of various genomic data sources is
highly demanded for the study of disease gene prio-
ritization [6, 10, 50, 51]. Although multiple data sources
are available, most methods only access one or two of these
databases, which all have their limitations. Chen [52]
proposed a method, called BRIDGE, which utilize a mul-
tiple regression model with lasso penalty to prioritize the
candidate genes by integrating disease phenotype similarity.
Zhang [53] adopted a Bayesian regression approach to
integrate multiple PPI networks. The approach takes the
strength of association between a query disease and a
candidate gene as a score to prioritize candidate genes.
However, to the best of our knowledge, constructing and
integrating multiple gene similarity networks for prioriti-
zing disease genes has not been investigated well. As a
result, there is still a need for the improvement in these
disease gene prioritization methods.
Motivated by the observations above, we proposed the
random walk with restart on phenotype-gene bilayer
network (RWRB) algorithm to prioritize candidate genes
of diseases. We firstly construct five individual gene
similarity networks based on genomic data of genes.
Then we obtain an integrated gene similarity network
(IGSN) via the similarity network fusion (SNF)
method. After that, combining the phenotype similarity
network, phenotype-gene association network and
IGSN, a phenotype-gene bilayer network is constructed.
In the end, we employ the RWRB algorithm on the
phenotype-gene bilayer network and prioritize candidate
disease genes on the whole genomic scale. On the
benchmark datasets, RWRB performs better than other
leading approaches. The framework of our proposed
method is shown in Fig. 1. It is noteworthy that, to take
advantage of more abundant genome data related to
genes, we treat sequence and domain similarity between
proteins as the similarity between their corresponding
protein-coding genes. Therefore, the similarity between
genes or proteins is collectively called gene similarity to
simplify in this article.
Methods
Datasets
Phenotype similarity network
In OMIM database, a phenotype is defined as a MIM
record. The similarity between phenotypes has been
calculated by text mining of MIM records [54]. We
downloaded the phenotype similarity network [39],
which contains pairwise similarity scores for 5080
phenotypes, covering the majority of recorded human
phenotypes in this database.
Phenotype-gene association network
The phenotype-gene relationship data is downloaded from
the OMIM database (http://omim.org/). After filter out
phenotypes which do not belong to the phenotype
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 28 of 79
similarity network above and have no known disease genes,
we collect 2133 phenotypes and 1893 disease genes involv-
ing 2386 phenotype-gene associations totally.
Gene data
Gene Ontology (GO) and Gene Ontology Annotation
(GOA) data of human is download from the GO website
(http://geneontology.org, dated November 2, 2015). The
numbers of annotated genes in cellular component
(CC), molecular function (MF) and biological process
(BP) ontologies are 16,938, 18,225, and 17,072, respec-
tively. Here, we consider all types of annotations which
contains Inferred from Electronic Annotations. Amino
acid sequences of proteins are obtained from the UniProt
database [55]. The number of protein sequence in human
database is 18,830. Domains of proteins are downloaded
from PFAM database (http://www.sanger.ac.uk/Software/
Pfam) [56]. Here, we only collected Pfam-A, a collection
of manually curated and functionally assigned domains,
instead of Pfam-B, which is computationally derived
collection of domains, to ensure accuracy in measuring
the similarity between proteins. The number of human
proteins annotated by Pfam-A is 18,523 involving 5333
kinds of domains in this database.
Construction of gene similarity networks based on genomic
data of genes
Constructing gene functional similarity networks based on
gene ontology
GO is a standardized and controlled vocabulary to de-
scribe genes and gene product attributes. It comprises
three orthogonal ontologies: CC, MF and BP, respec-
tively. In our research, CC, MF and BP ontology has
3817, 9943 and 27,864 terms, respectively.
Functional similarity between genes can be inferred
from the semantic relationships of their GO terms
[51, 57]. In this work, the functional similarity
between two genes is measured by Wang method
[58] taking BMA strategy because of its an outstand-
ing performance. For the sake of three ontologies are
independent, the functional similarity between genes
can be measured from three different ontologies.
Therefore, we obtain network on CC, MF and BP
ontology, respectively.
Fig. 1 The flow chart of the proposed method
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 29 of 79
Constructing protein similarity network based on protein
sequence
We used bitscores calculated by the Basic Local Alignment
Search Tool (BLAST) to create our sequence homology
dataset. First of all, we performed an all-versus-all compari-
son between proteins with an expectation-value threshold
of 10?6. Then, the similarity between proteins was norma-
lized according to their corresponding bitscores of
proteins. Then, applying this operation to all protein pairs,
we got the similarity network of protein sequences.
Constructing protein similarity network based on
protein domains
We calculated the Jaccard scores [59] between protein
domain set as domain similarity of proteins. The Jaccard
score between proteins p1 and p2 is defined as Dp1?Dp2=
Dp1?Dp2 , which is the ratio of the number of common
domains between p1 and p2 over the total number of
domains in p1 and p2. Dp denotes the domain set of
proteinp. There are totally 18,526 proteins involving
5333 kinds of domain used in our analysis. Applying this
operation to all protein pairs, thus we constructed a
domain similarity network.
The overlap among the five aspects of annotation
information about genes (proteins) above is unexpec-
tedly large, as shown in Fig. 2. Numbers in the figure
denote the number of genes that annotated by the
corresponding information in each part, where CC,
MF and BP denote corresponding annotations of genes.
Seq and Domain denote amino acid sequences and
domain of proteins.
Integrating gene similarity networks based on SNF
method
We have constructed five gene similarity networks based
on BP, CC, MF, sequence and domain information of
genes. In this subsection, we will employ SNF method
[60] to integrate these five networks.
Suppose W(m) (here m = 1,2,3,4,5) denotes one of the
adjacent matrices of gene similarity networks, we use
Fig. 2 A brief statistic about the number of genes (proteins) annotated by the corresponding information
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 30 of 79
Eq. (1) to compute the normalized weighted matrix of
W(m), which can be defined as:
P mð Þij ¼
W mð Þij
2
P
k?i
W mð Þik
if j?i
1
2
if j ¼ i
8>>><
>>>:
ð1Þ
The normalization used here is free of the scale of
self-similarity in the diagonal entries. It can avoid nu-
merical instabilities and ?jP(i, j) = 1 still holds.
At the same time, we define the local kernel matrix
S mð Þi;j , which is calculated by Eq. (2)
SðmÞij ¼
W ðmÞijP
k?Vi
ðmÞW
ðmÞ
ik
if j?V
i
ðmÞ;
0 otherwise
8><
>: ð2Þ
where V mð Þi denotes a set which contains K nearest neigh-
bors of gene i in the matrix W(m). Since local similarities
(high values) are more reliable than remote ones, we filter
out the low similarity neighbors and set these similarities
to zero. The K most similar genes for each gene in the
networks are preserved. The local neighborhoods are
further exploited to measure the local affinities among
genes [61]. Therefore, S(m) keeps the local structure ofW(m).
In summary, P(m) carries the full information about
the similarity of each gene to all others, whereas S(m)
only encodes the similarity to the K most similar genes.
Here, P(m) and S(m) are called status matrices and kernel
matrix [60], respectively.
To fuse the similarity networks, SNF takes the inter-
active process of the following update equation:
P mð Þtþ1 ¼ S mð Þ 
1
M?1
X
n?m
P nð Þt
 !
 S mð Þ
 T
ð3Þ
where m is the index of corresponding adjacent matrices
of similarity networks, and t is the iteration number. It
should be noted that we perform normalization on P mð Þtþ1
as in Eq. (1) after each iteration. Another way to think of
the updating rule (3) is
PðmÞtþ1ði; jÞ ¼
X
h?Vi
ðmÞ
X
l?Vj
ðmÞ
SðmÞi;h 
1
M?1
X
n?m
P
t
ðnÞ
 !
h;l
 SðmÞj;l
ð4Þ
Because the similarity information is only propagated
through the common neighborhood between genes, SNF
is robust to noise existing in genome data. Besides, if
two genes gi and gj have common neighbors in all of
similarity matrices, it should be well believed that they
have the high similarity. Whats more, SNF benefits the
fact that even if gi and gj are not very similar in one data
type, their similarity can be measured in another data
type and this similarity information can be propagated
through the fusion process [60, 62]. The illustrative
example for fusing two networks based on SNF is
shown in Fig. 3.
Fig. 3 Illustrative example of SNF steps. (a) Gene-gene similarity matrices based on CC and MF ontology, respectively. (b) Gene functional similarity
networks. Genes are represented by nodes and pairwise similarities between genes are represented by edges. (c) Network fusion by SNF updates
iteratively, making them more similar with each step. (d) The iterative network fusion results in convergence to the final integrated network. Edge color
indicates which data type has contributed to the given similarity
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 31 of 79
Finally, after t steps of iteration, these five matrices
will converge to a single integrated matrix, which can be
computed as:
P ¼ 1
M
XM
m¼1
Pt
ðmÞ ð5Þ
We obtain the primary integrated gene similar network
in this step.
Clustering coefficient-based threshold selection
The five gene similarity networks are fused as a primary
integrated gene similar network, whose nodes represent
the genes and edges represent the similarity between
genes. However, there is still a serious problem needing to
be addressed that how similar between two genes can be
connected in the network. Because most molecular
networks follow a power law or lognormal distribution
[12], we should set an appropriate threshold to ensure that
the primary integrated gene similarity network meets this
demand. The similarity between genes which is greater
than the proper threshold will be connected by edges.
Otherwise, the similarity will be set to zero [46]. In this
research, we adopt the clustering-coefficient-based
threshold selection method to select a proper threshold
for the primary integrated gene similarity network.
The clustering coefficient of a gene i in the network is
defined as:
Ci ¼ 2Eiki ki?1ð Þ ð6Þ
where Ei represents the number of edges between the ki
(>1) first neighbors of gene i. The clustering coefficient
of a network is defined as the average clustering
coefficient of its all nodes.
C ¼ 1
K
X
ki>1
Ci ð7Þ
where K denotes the total number of nodes in the
network.
The threshold selection for a network can be regarded
as a process, where edges are removed from the initially
complete graph by gradually increasing the similarity
threshold between genes. For each threshold r, we can
construct a network by the means of filtering out the
similarity lower than the threshold r. It is generally
believe that the clustering coefficient of molecular net-
works, denoted byC(r), should be significantly higher
than the that of the corresponding random network,
which is denoted by C0(r).
Therefore, we formulate a discrete optimization prob-
lem, in which the cutoff threshold should meet the
demand
C ¼ min
j
rj : C rj
 
?C0 rj
 
> C rjþ1
 
?C0 rjþ1
  
ð8Þ
over a set of thresholds 0 = r0 < r1 < ? < rj ? 1 < rJ = 1. In
Eq. (8), rj + 1 = rj + 0.001; C(r)and C0(r) denote the
clustering coefficients of the gene similarity network and
the corresponding random network at the threshold r,
respectively. The aim of this procedure is to find the first
local maximum, which means the first stop of monoton-
ically increasing of C(rj) ?C0(rj).
On the other hand, the clustering coefficient of a
corresponding random network is determined by
C0 ¼
k
2
?k
 2
k
3N
ð9Þ
where N is the total number of nodes in a network,
k
 ¼ 1=NPNi¼1ki k2 ¼ 1=NPNi¼1k2i .
Finally, after threshold selection for the primary
integrated gene similarity network, the IGSN that we
need is constructed. It is represented as G(V, E, t), where
V = { g1, g2, ? , gN} denotes the genes involving in IGSN,
and E = {eij = ? gi, gj ? |sim(gi, gj) > t} represents the edges
between genes with values greater than threshold t.
Construction of the phenotype-gene bilayer network
We have got three networks, which are phenotype
similarity network, IGSN and phenotype-gene association
network respectively. In this subsection, we make use of
the three networks above to construct a phenotype-gene
bilayer network. The construction process of phenotype-
gene bilayer network is illustrated in Fig. 4.
Suppose AP(m ×m), BGP(m × n) and WG(n × n) are
adjacency matrices for phenotype similarity network,
phenotype-gene association network and IGSN respect-
ively, where m and n represent the number of pheno-
types and genes in their respective networks. The
adjacency matrix of the phenotype-gene bilayer network
is denoted as
A ¼ AP BGP
BPG WG
 	
(10)
where BGP is the transpose of BPG.
Prioritizing candidate disease genes based on RWRB
The RWRB is a ranking algorithm, which simulates a
random walker moving from the seed nodes to their
immediate neighbors randomly and staying at the
current node(s) based on the probability transition
matrix [32, 63]. As for a random walk on the bilayer
network, we first construct the transition matrix M
based on matrix A, which is defined as
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 32 of 79
M ¼ ?MP 1??ð ÞMGP
1??ð ÞMPG ?MG
 	
ð11Þ
where MP, MGP and MG are the row-normalizing matri-
ces of AP, BPG and WG respectively; ? controls the
jumping probability between two similarity networks,
which are phenotype similarity network and IGSN. Then
the initial vector P(0) (at t = 0) can be defined as
follows:
P 0ð Þ ¼ 1??ð Þu 0ð Þ
?v 0ð Þ
 	
ð12Þ
where u(0) and v(0) denote the initial probability vector
for phenotype similarity network and IGSN. The param-
eter ? ? (0, 1) is used to weight the importance of pheno-
type similarity network and IGSN. The effect of the
parameters ? and ? on RWRB will be shown in the result
section. P(t) represents a vector in which the i-th element
holds the probability of finding the random walker on
node i at step t.
Based on the vector P(0), P(t) and the transition
matrix M, the probability vector at step t + 1 can be
given by
P t þ 1ð Þ ¼ 1??ð ÞMTP tð Þ þ rP 0ð Þ ð13Þ
where ? ? (0, 1) indicates the restart probability. At each
step, the random walker has a probability ? to return the
seed nodes.
After some steps, the walking process is converged if
the change between P(t) and P(t + 1) is lower than 10?6.
The steady probability P(?) is represented as P ?ð Þ
¼ 1??ð Þu ?ð Þ
?v ?ð Þ
 	
. As a result, genes which belong to the
control set are ranked according to their probability
scores in P(?). Gene which has the maximum in P(?)
among all the control gene set is considered as the most
probable gene that associates the phenotype.
Evaluation metrics of prediction performance
Phenotypes in OMIM database mainly have three types
[33, 35]: susceptible chromosomal locus and several
related disease genes are known; susceptible locus is
known, but no related genes are known; locus and re-
lated causal genes are unknown, but the phenotype is
known. Therefore, we use three leave-one-out cross-
validation experiments, i.e. linkage interval, genome-wide
scan and ab initio, which are detailedly introduced and
used in [35, 43], to validate our method.
Firstly, as for some phenotypes that susceptible
chromosomal locus and several related disease genes are
known, we take the cross validation against a linkage
interval experiment [43]. In each round of validation,
one phenotype-gene link is removed. We define the gene
associates with the removed link as the held out gene.
The phenotype and the rest disease genes related to this
Fig. 4 The construction process of phenotype-gene bilayer network
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 33 of 79
phenotype are used as the seed nodes. At the same time,
we define the control gene set that consists of the held
out disease gene and its 99 nearest genes according to
the NCBI refGene location. The performance of RWRB
is investigated by the capability to recover the held out
disease gene from the control gene set. We call this as
linkage interval experiment.
Secondly, since there are some phenotypes that
have no susceptible chromosomal locus but have
already experimental validated disease genes, we take
the validation against genes in the genome-wide scale.
In this experiment, we also remove a phenotype-gene
relationship and use the rest disease gene associated
with this phenotype as the seed nodes. Different to
linkage interval experiment, the control gene set
consists all the genes in the genome-wide scale except
the held out disease gene. The performance of RWRB
is investigated by the rank of held out gene in the
control gene set. We call this as the genome-wide
scan experiment.
Thirdly, as for some phenotypes without any known
disease genes and susceptible chromosomal locus, we
identify disease genes for these kinds of phenotypes from
the whole-genome scale. In this experiment, we first re-
move all the associations between this phenotype and its
disease genes, then run the RWRB algorithm which
treats this phenotype as seed node. In this situation, the
control gene set is defined as all the genes that in the
whole networks. Similar to genome-wide scan experi-
ment, the performance of RWRB is investigated by the
rank of held out gene in the control gene set. We call
this as ab initio experiment. The detail explanations for
the three approaches have been described by Li [35] and
Jiang [37].
At the same time, we also define three metrics to in-
vestigate the performance of RWRB. First is number of
successful predictions (NSP). For each experiment
above, in each round of validation, if the held out disease
gene is ranked as top 1 among the control gene set, we
consider it a successful prediction. Further, for a set of
validation runs in each experiment, we sum up the
number of successful predictions and treat it as a
metric that represents effectiveness of algorithms.
Second is the mean rank ratio (MRR), which is defined
as the average rank ratios of all held genes in control
gene sets in all validation runs. Third is the receiver op-
eration characteristic (ROC) curve. We plots the sensi-
tivity versus 1-specificity which subject to the threshold
separating the prediction classes [10]. Sensitivity refers
to the percentage of disease genes that are ranked above
a particular threshold, while specificity refers to the
fraction of control genes rank below the threshold. We
vary the threshold from 0.0 to 1.0 with the scale 0.01,
and draw the ROC curve. It is well accepted that smaller
MRR and larger AUC and NSP values indicate better
performance for a prioritization method [43].
Results
First of all, we will investigate the performance of RWRB
on three kinds of experiments. Then, we assess the effect
of parameters in RWRB algorithm. After that, the
proposed algorithm is compared with two similarity-
based methods, which are CIPHER [33] and RWRH [35]
and two feature-based methods which are PUDI [15]
and PriDiGe [14]. Finally, we predict novel causal genes
for Alzheimers disease and other common diseases
based on RWRB algorithm.
The performance of RWRB
In this subsection, we will investigate the performance of
RWRB on the three experiments using the three metrics.
The detail results are shown in Table 1. The ROC curves
on linkage interval and genome-wide scan experiments
are shown in Fig. 5.
As is shown in Table 1, the results of RWRB on NSP,
MRR and AUC metrics for linkage interval experiment
is 1384, 18.28, 0.8505, respectively. Then we further in-
vestigate the performance of RWRB on genome-wide
scan experiment and obtain a NSP of 311, a MRR of
22.17 and an AUC of 0.8417. In the end, we perform the
cross-validation approach against ab initio experiment.
The results on NSP, MRR and AUC are 223, 29.64 and
0.8144, respectively.
As is known to us, a random guess will yield a MRR of
50%, and an AUC of 50%, suggesting that the effective-
ness of RWRB in uncovering disease gene. Meanwhile,
the results also show the reliability of IGSN.
Then we further analyze the detail distribution of
disease genes ranked in the control gene set for linkage
interval and genome-wide scan experiment. The results
are presented in Fig. 6. As for linkage interval experi-
ment, we find that there are 1554 disease genes ranking
in top 10, where 1384 disease genes are rank one. 230
disease genes are ranked between 11 and 20, and 157
disease genes are ranked between 30 and 50.
As for genome-wide scan experiment, there are 498
disease genes ranking between 1 and 10. The number of
disease genes between 11 and 50 is 422. As we can see
from the results, most of held out genes can be rank in
top 100. The results on three experiments demonstrate
Table 1 The results of RWRB on the three experiments
Experiment NSP MRR AUC
Linkage interval 1384 18.28 0.8505
Genome-wide scan 311 22.17 0.8417
ab initio 223 29.64 0.8144
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 34 of 79
that RWRB has a high accuracy in inferring disease
genes on the genomic scale.
Effect of parameters on RWRB
There are totally three parameters in RWRB, which are
? ? and ?. The parameter ? denotes the restart probabil-
ity in Eq. (1). It has been well accepted that the param-
eter ? has a slight effect on the results and here we fix it
at 0.7 [35]. Next, we will investigate the influence of
parameter ? and ? for RWRB on the NSP metric.
The parameter ? represents the jumping probability
between phenotype similarity network and IGSN.
According to [35], larger ? will introduce more mutual
information between phenotype similarity network and
IGSN. To investigate the effect of this parameter on the
performance of RWRB, we tested our algorithm on
different values of ? ranging from 0.1 to 0.9 with an in-
crement of 0.1.
Results are shown in Table 2. The performance is im-
proved with the increase from 0.1 to 0.6 on the whole.
However, the performance is slightly decreased from 0.6
to 0.9. As for the linkage interval experiment, RWRB
gets the best performance at ? = 0.6, while RWRB gets
the largest NSP at ? = 0.7 on the genome-wide scan
experiment. The best results for ab initio experiment is
225 when ? = 0.6. Therefore, we suggest that the best ?
Fig. 5 ROC curves of RWRB on linkage interval and genome-wide scan experiments
Fig. 6 The distribution of disease genes ranked in top 100
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 35 of 79
value is 0.6 or 0.7 for RWRB on the experiments above.
Results demonstrate that the RWRB algorithm success-
fully makes the best use of the relationships between
phenotype similarity network and IGSN.
As is known to us, ? controls the impact of seed phe-
notypes and seed genes in the initial vector. To validate
the effect of parameter ? on RWRB, we tested our
algorithm on different values of ? ranging from 0.1 to
0.9 with the scale 0.1. We run RWRB on linkage interval
and genome-wide scan, ab initio experiments, and evalu-
ate its performance on the NSP metric. As is shown in
Table 3, the performance is improved with the increase
from 0.1 to 0.6 on both experiments. However, the per-
formance is slightly decreased from 0.6 to 0.9. As a re-
sult, the algorithm performs best when ? at 0.6. This
suggests that IGSN is more importance than phenotype
similarity network for RWRB.
Comparison with similarity-based methods
We compare RWRB with similarity-based methods
which are RWRH [35] and CIPHER [33], respectively.
The author [33] defines two topological distance on the
basis of two different neighborhood systems: shortest
path (SP) and direct neighbor (DN). Therefore, two ver-
sions of CIPHER are represented as CIPHER-SP and
CIPHER-DN, respectively. The results of each method
on NSP metric are presented in Table 4.
Because the number of phenotype-gene associations in
RWRB, CIPHER and RWRH models are different, we
compute the successful prediction percentages for each
method, which is defined as the ratio between NSP and
the total phenotype-gene associations in their corre-
sponding datasets. The experimental results are listed
in Table 5.
As for the linkage interval experiment, RWRB gets 1384
successful predictions, while RWRH, CIPHER-SP and
CIPHER-DN obtain 814, 709, 765 successful predictions,
respectively. The percentage of successful prediction for
RWRB is 0.58 which is the highest in all three methods.
As for the genome-wide scan experiment, the control
gene set is defined as the whole genes in IGSN. RWRB
get 311 successful predictions, while RWRH, CIPHER-
SP and CIPHER-DN obtain 245, 153, 165 suc cessful
predictions, respectively. Then number of successful pre-
dictions of RWRB is largest in the three methods. How-
ever, the percentage of successful predictions for RWRB
is 0.13 which is lower than that of RWRH (0.17).
On the ab initio experiment, there are 223 successful
predictions by RWRB, while RWRH, CIPHER-SP and
CIPHER-DN successfully predicted 201,140 and 157
cases, respectively. However, the percentage of successful
predictions of RWRH is the highest in the three
methods which is 0.14, whereas the other three methods
are almost neck and neck.
Comparison with feature-based methods
At the same time, we compare RWRB with two feature
based methods which are PUDI [15] and PriDiGe [14].
Here we only compare the precision (p), recall (r) and F-
measure (F) of these three methods, since they are from
different type of methods.
The metrics about precision, recall and F-measure for
PUDI and ProDiGe have been introduced by Yang [15].
Here, we will also use these metrics to evaluate the per-
formance of RWRB on linkage interval experiment. In
the experiment, we take the leave-one-out cross-validation
method. For the precision of RWRB, we define it as
Table 2 Performances of RWRB at different values of ? on
NSP metric
? Linkage interval Genome-wide scan ab initio
0.1 1306 295 165
0.2 1320 299 169
0.3 1337 304 181
0.4 1349 309 209
0.5 1384 311 223
0.6 1393 317 225
0.7 1386 319 211
0.8 1361 308 206
0.9 1357 304 174
To validate the effect of parameter ? on RWRB at different values, we fix ? at
0.5. Best results are in bold
Table 3 Performances of RWRB at different values of ? on NSP
metric
? Linkage interval Genome-wide scan ab initio
0.1 1286 299 175
0.2 1310 307 187
0.3 1344 310 193
0.4 1368 310 206
0.5 1384 311 223
0.6 1392 319 227
0.7 1391 317 217
0.8 1378 315 203
0.9 1354 306 172
To we validate the effect of parameter ? at different values, we fix ? at 0.5.
Best results are in bold
Table 4 The performance of each method on the NSP metric
Algorithms Linkage interval Genome-wide scan ab initio
RWRH 814 245 201
CIPHER-SP 709 153 140
CIPHER-DN 765 165 157
RWRB 1384 311 223
Note: Best results are in bold
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 36 of 79
the ratio between NSP and the number of all validation
runs. For the recall of RWRB, we define it as the ratio be-
tween the number of the held out genes whose rank pro-
portions are higher than 0.5 and the number of all the
held out genes. The F-measure is the harmonic mean of
precision and recall, which is defined as F = 2?p?r/(p + r).
The results for PUDI, ProDiGe and RWRB are shown
in Table 6. From the results, we can find that RWRB
achieves 82.3% recall and ranks first in the three
methods. Method PUDI wins 72.3% precision which is
13.7 and 0.9 better than RWRB and ProDiGe method,
respectively. At the same time, method PUDI achieves
76.5% F-measure which is 2.0% and 10.2% better than
RWRB and ProDiGe method, respectively. In this group
experiment, method PUDI performs best and RWRB
ranks second overall.
Prioritizing Alzheimers disease and other common
disease genes by RWRB: A case study
In this subsection, we will use RWRB to predict novel
causal genes of interested diseases. To validate the
effectiveness of our method, we will check whether our
predicted disease genes have been already found to
associate with the diseases in literature. Here, we select
16 multifactorial diseases which are used in [37] and list
the top 10 candidate genes for each disease. The results
are shown in Table 7. Here, we only select Alzheimers
disease (AD) as the case study to verify the performance
of RWRB.
AD is a progressive disease that usually starts slowly
and gets worse over time. In general, it causes 60% to
70% of cases of dementia. The cause of AD has not been
completely understood so far. The primary task is to
discover the disease genes to understand the nosogenesis
of genetic disease. There are many phenotypes for AD.
Here we select 104,300 as target phenotype to prioritize
disease gene. The corresponding susceptible region for
MIM:104,300 is 6p22.
As is shown in Table 7, the first prediction of RWRB
for MIM:104,300 is NOS2, which plays an important
role in neuroinflammation by generating nitric oxide
(NO), a critical signaling and redox factor in the brain
[64]. Further, the levels of NO fall in the brain to a
threshold may promote A? mediated damage. The
predicted gene NOS2 has a large impact on AD. The
second prediction gene for MIM:104,300 is NOS1. In
the brain and peripheral nervous system, nitric oxide
displays many properties for a neurotransmitter. The au-
thor [65] suggests that short alleles of the NOS1 exon
1fVNTR interacting with the epsilon 4 allele tend to
markedly increase the AD risk [65]. The fourth predicted
gene for AD is APBB1. A trinucleotide deletion of the
APBB1 gene was a factor protecting against late-onset
AD. Cousin [66] reported the results of a case/control
study and confirmed this relationship. The eighth predic-
tion is gene PGBD1. It locates at 6p22 which is the sus-
pectable region of MIM:104,300. Whats more, it currently
shows significant association in AlzGene according to
Genome-wide association study. Its gene product is spe-
cifically expressed in the brain and has been identified as
the key factors of AD. The results above show that the
combination of the similarity network integration and the
identification algorithm can successfully predict candidate
genes for interested disease.
Conclusions and discussion
In this paper, we propose a novel method, named RWRB,
to infer causal genes of interested diseases. We firstly con-
struct five gene similarity networks based on five different
types of genome data. Then we employ SNF method to in-
tegrate these gene similarity networks and get IGSN. After
that, we perform RWRB to prioritize disease genes. RWRB
is compared with the state-of-the-art models and achieves
a better performance on most evaluation metrics. Next,
we will discuss the highlights of this article.
The advantages of IGSN
The main object of our research is to overcome two draw-
backs of current PPI networks, i.e., their low reliability and
coverage. As a result, we construct the IGSN in this re-
search. Firstly, since IGSN is fused based on the five gene
(protein) similarity networks, its reliability should be higher
than existing that of PPI networks. The prioritization of dis-
ease genes can be benefited from IGSN. Secondly, IGSN
can significantly improve the coverage of human genes
comparing current PPI networks. It covers 19,065 genes,
which is twice the number of genes in HPRD network.
Therefore, the number of phenotype-gene associations in
RWRB algorithm is 2386, which is almost twice that in
RWRH and CIPHER methods whose number is 1444. As a
Table 5 The successful prediction percentages for each method
Algorithms Linkage interval Genome-wide scan ab initio
RWRH 0.56 0.17 0.14
CIPHER-SP 0.49 0.11 0.10
CIPHER-DN 0.52 0.12 0.11
RWRB 0.58 0.13 0.09
Note: Best results are in bold
Table 6 Overall comparison among different methods
Methods Precision Recall F-measure
PUDI 72.3 81.0 76.5
ProDiGe 72.4 75.9 74.1
RWRB 58.6 82.3 69.4
Note: Best results are in bold
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 37 of 79
result, the proposed method can make the best use of
phenotype-gene associations in OMIM database. Thirdly,
since IGSN is a single network which integrates multiple
gene similarity networks, there is no need for it to assign
weight values to different subnetworks.
Threshold selection for IGSN
The threshold selection is very important to the quality
of IGSN. This is because the threshold affects reliability
of IGSN, and may further determine the performance of
RWRB. As shown in Fig. 7, the first stop of
Table 7 Top-10 predicted causal genes of 16 multifactorial diseases
Phenotype name Phenotype ID Top ten predictions for each phenotype by RWRB
Alzheimers disease 104,300 NOS2 NOS1 APBB3 APBB1 EPX LPO APLP1 PGBD1 POR MTRR
Breast cancer 114,480 RB1 PTEN AR TP63 TP73 SDHD BUB1B GNAS PHB2 TSC1
Colon cancer 114,500 RB1 PTEN SDHD BRCA1 MLH1 MSH2 BRCA2 CREBBP TP63 TP73
Diabetes mellitus 125,853 INSR APOA5 VDR HMGA2 SLC2A2 LPL GHR INS USF1 LMNA
Gastric cancer 137,215 IL36A IL36G IL1A IL1F10 IL37 IL36B IL36RN APC IL18 MSH2
Atrial fibrillation 147,050 WAS SELL PAFAH2 SELE TIMD4 HAVCR2 IL13 IKBKG TNFRSF13B ICOS
Prostate cancer 176,807 HIP1R BRCA1 TP53 STK11 FGFR3 ZFHX4 SDHD RNASEL PRODH MSH2
Schizophrenia 181,500 SYN3 SYN1 MAPT DDO PRNP CHI3L2 APOL3 CHIA CHIT1 APOL1
Leukemia 190,685 FLNA FGFR2 RET GLI3 NF1 COL1A1 COL2A1 EVC TBX1 FLNB
Lung cancer 211,980 TP53 CDKN2A RB1 SDHD NRAS CYP2D6 BRCA1 CYLD DICER1 PTEN
Zellweger 214,100 FGFR2 FLNA COL2A1 MECP2 FGFR3 FLNB TP63 GLI3 GJA1 COL11A1
Leukemia 253,310 SMN1 GBA LMNA VAPB ATP7A ALS2 COL6A2 BSCL2 DCTN1 COL2A1
Asthma 600,807 IL2RG SCGB1D2 SCGB1D4 SCGB1D1 PAFAH2 SBDS WAS IGHM HPS1 ALOXE3
Leukemia 601,626 BCR PDGFRB PRF1 KMT2A BRCA2 MPL MLLT1 MCL1 MLLT6 RPS14
Obesity 601,665 FFAR4 GNAS SLC6A14 ASIP ENPP3 SDC1 ENPP2 SDC2 SDC4 MLN
Tuberculosis 607,948 CD2AP C5 SCNN1B CFTR TICAM2 FAM218A TLR1 TLR4 TLR6 SOCS2
Note: Predicted disease genes which are supported by literature are in bold for Alzheimers disease
Fig. 7 Cluster coefficient under each threshold for primary integrated gene similarity network. Black arrow points to the first peaks of the curve
and rectangular boxes show the corresponding threshold value. Red curve represents the cluster coefficient of the primary integrated gene similarity
network (CCP), and green curve denotes the cluster coefficient of the corresponding random network (CCR) at different thresholds. Blue curve depicts
the difference of cluster coefficient (DCC) between the two networks above. In this experiment, we select the best threshold at r = 0.005, and
construct IGSN based under this threshold
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 38 of 79
monotonically increasing of DCC (See legend of Fig. 7)
occurs at r = 0.005, which indicates that this threshold is
the most appropriate value to construct IGSN. Under
this threshold, the IGSN has 19,065 genes in our
experiment.
We further investigate the degree distributions of
IGSN under the selected threshold. Many previous
studies [67] have found that distribution of node con-
nectivity of molecular networks follows a power law.
However, some other research [68] argued that there are
some distributions, such as the lognormal distribution,
which can also depict the degree distribution better than
power law. In this research, we employ two models,
which are Gaussian distribution and Lognormal distribu-
tion, to investigate the distributions of IGSN. In order to
increase contrast, we import two other leading PPI
networks, which are BioGRID and HPRD networks.
The fitting performance on the distributions for each
network is represented by R-squares (R2). R2 provides a
measure of how well the data fits a certain model. As is
shown in Fig. 8, we find that the degree of IGSN fits the
lognormal distribution best, while BioGRID and HPRD
prefer to fit the power law distribution. As is shown in
Fig. 8 (c) and (d), the R2 results of IGSN for Gaussian
and Lognormal distribution are 0.87 and 0.94, respect-
ively. The R2 results of BioGRID and HPRD for fitting
Power law are 0.91 and 0.92, respectively, which are
shown in Fig. 8 (a) and (b). The degree distribution re-
sult shows that IGSN has the characteristics of molecu-
lar networks, rather than those of random networks.
Therefore, IGSN is a meaningful biological network.
In the future, our research should further be improved
from the following aspects. First, other genomic data of
genes needs to be integrated. Although we have
Fig. 8 The graphic view of degree distribution fitting results for BioGRID (a), HPRD (b) and IGSN (c, d). According to their performance on R2, the
results for IGSN fitting the Gaussian and Lognormal distribution are 0.87 and 0.92, shown with (c) and (d) respectively, while the results for
BioGRID (c) and HPRD (d) are 0.91 and 0.92 respectively
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):32 Page 39 of 79
measured the similarity between genes based on five
types of genome data, other information of genes is
needed to be integrated to the similarity networks.
Second, how to fuse the different similarity networks
properly is important to the ultimate integrated network.
Many previous studies have attempted to integrate
different semantic similarity network and gene expression
networks. However, some methods only assign equal
weighted to these networks and simply add them together,
while some others apply these networks separately. The
SNF method used in this article may overcome the draw-
backs above. However, the identification of the integrated
network is not a trivial assessment because there is no dir-
ect way to ascertain its rationality and correctness. In our
research, we resort to degree distribution of integrated net-
work and find it fit the lognormal distribution best. This
only shows the rationality from one property of the inte-
grated network. Therefore, we need to study more fused
methods of network further and make the integrated net-
work be in line with the characteristics of biological
networks.
Funding
M. Guo is supported by National Natural Science Foundation of China (61,271,346,
61,571,163, and 61,532,014) and the National Key Research and Development Plan
Task of China (Grant No. 2016YFC0901902). C. Wang is supported by Natural
Science Foundation of China (61402132), and X. Liu is supported by Natural
Science Foundation of China (91,335,112, 61,671,189). Publication costs for this
article was funded by National Natural Science Foundation of China (61571163).
Availability of data and materials
The dataset(s) supporting the conclusions of this article were downloaded
from the relevant public databases.
About this supplement
This article has been published as part of Journal of Biomedical Semantics Volume 8
Supplement 1, 2017: Selected articles from the Biological Ontologies and Knowledge
bases workshop. The full contents of the supplement are available online at https://
jbiomedsem.biomedcentral.com/articles/supplements/volume-8-supplement-1.
Authors contributions
ZT proposed the idea, implemented the experiments and drafted the manuscript.
MG initiated the idea, conceived the whole process and finalized the paper. CW,
LX, LW and YZ helped with data analysis and revised the manuscript. All authors
have read and approved the final manuscript.
Ethics approval and consent to participate
The human GO annotations are publicly available to all researchers and are free of
academic usage fees. There are no ethics issues. No human participants or
individual clinical data are involved with this study.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.
Author details
1School of Computer Science and Engineering, Harbin Institute of
Technology, Harbin 150001, Peoples Republic of China. 2Institute of Health
Service and Medical Information Academy of Military Medical Sciences
Beijing, Beijing 100850, China.
Published: 20 September 2017
Deléger et al. Journal of Biomedical Semantics  (2017) 8:37 
DOI 10.1186/s13326-017-0135-z
RESEARCH Open Access
Design of an extensive information
representation scheme for clinical narratives
Louise Deléger1,2, Leonardo Campillos2, Anne-Laure Ligozat2,3 and Aurélie Névéol2*
Abstract
Background: Knowledge representation frameworks are essential to the understanding of complex biomedical
processes, and to the analysis of biomedical texts that describe them. Combined with natural language processing
(NLP), they have the potential to contribute to retrospective studies by unlocking important phenotyping information
contained in the narrative content of electronic health records (EHRs). This work aims to develop an extensive
information representation scheme for clinical information contained in EHR narratives, and to support secondary use
of EHR narrative data to answer clinical questions.
Methods: We review recent work that proposed information representation schemes and applied them to the
analysis of clinical narratives. We then propose a unifying scheme that supports the extraction of information to
address a large variety of clinical questions.
Results: We devised a new information representation scheme for clinical narratives that comprises 13 entities, 11
attributes and 37 relations. The associated annotation guidelines can be used to consistently apply the scheme to
clinical narratives and are https://cabernet.limsi.fr/annotation_guide_for_the_merlot_french_clinical_corpus-
Sept2016.pdf.
Conclusion: The information scheme includes many elements of the major schemes described in the clinical natural
language processing literature, as well as a uniquely detailed set of relations.
Keywords: Knowledge representation, Clinical natural language processing
Introduction
The progressive adoption of electronic health records
(EHRs) is paving the way towards making available large
amounts of data for research. Raw EHR data may be trans-
formed into clinically relevant information and then be
used in traditional or translational research [1]. Natural
language processing is essential to phenotyping EHR data
because of the amount of clinical information buried in
the narrative content.
The path towards EHRs is nonetheless not free of chal-
lenges [1]. One of the hurdles is the coexistence of several
information models for representing clinical information
available in EHRs. The Clinical Document Architecture
(CDA) in the Health Level 7 (HL7) framework coexists
*Correspondence: neveol@limsi.fr
2LIMSI, CNRS, Université Paris - Saclay, Rue John von Neumann, 91405 Orsay,
France
Full list of author information is available at the end of the article
with the Clinical Element Model (CEM) [2] and other
standards such as the openEHR [3] and the ISO 13606 [4].
Developing equivalent clinical models is a key element
to achieve the semantic interoperability of EHR systems
[5]. The Clinical Information Modeling Initiative (CIMI)
[6] and the SemanticHealthNet (SHN) [7] initiative are
international efforts towards this goal. It can be argued
that informationmodels rely on terminologies that specify
the concepts used in the model [8]; for instance, medi-
cations in RxNorm10 or clinical terms in SNOMED CT.
However, information and terminology models tend to be
designed by different groups with dissimilar data struc-
tures. Some researchers have indeed attempted to validate
the use of terminologies in EHR-based standards (e.g.
SNOMED CT in the HL7 Clinical Document Architec-
ture) [9].
In this paper we will focus on a text-based represen-
tation of information that is text-anchored (i.e. mentions
of clinical entities) or that may be derived from text data
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Deléger et al. Journal of Biomedical Semantics  (2017) 8:37 Page 2 of 18
(e.g. relations between entities identified in clinical texts).
Addressing the unification of information models is out
of our scope here. Our goal is to put forth a represen-
tation scheme that will support secondary use of EHR
data for conducting a large variety of retrospective studies.
More specifically, we aim to support information extrac-
tion from clinical narratives in order to answer clinical
questions such as: What is the prevalence of incidental
findings in patients with suspected thromboembolic dis-
ease?, What is the contribution of CT venography in the
diagnosis of thromboembolic disease? or What are the
types and grades of toxicities experienced by colon cancer
patients receiving FOLFOX therapy?.
Simple and fast low-level annotations have already
yielded good results in mining large datasets, as exem-
plified in LePendu et al.s study on myocardial infarction
adverse drug effects in rheumatoid arthritis [10]. Another
study showed the benefit of exploiting medical concepts,
modality and relations between concepts extracted from
clinical narratives for accurate patient phenotyping [11].
Furthermore, recent research has shown that information
extraction from unstructured clinical narratives is essen-
tial to many clinical applications, including secondary use
of EHRs for clinical trial eligibility [12].
Overall, the information representation landscape
broadly includes two types of representations. First,
ontologies or encyclopedic representations that are very
detailed and removed from any direct application, with
the goal of providing a formal representation of domain
or subdomain knowledge. Second, a number of text-based
representations of information that are very-well suited to
an application they were designed for. Our need is for a
representation scheme with a broad scope that remains
close to applications grounded in clinical text. The goal is
to identify a representation that may connect easily with
major knowledge sources used in clinical Natural Lan-
guage Processing, while covering many aspects of clinical
knowledge covered in EHR narratives. We conducted a
review of annotation projects and associated annotation
schemes for clinical narratives. We found that while all
existing schemes had merit, no single scheme covered all
the aspects of knowledge representation that we sought, in
particular with respect to fine-grained relations between
clinical concepts. We then designed a new information
representation scheme that related to existing schemes
and attempted to integrate best representation practices.
This article describes a new information representation
scheme devised from on-going analysis of clinical narra-
tives. This scheme has been applied to annotate a large
corpus of French clinical reports described in [13], but is
intended to be generally applicable to clinical narratives in
several languages and medical specialties. The contribu-
tion of this paper is two-fold: first, we present an extensive
review of annotation projects and associated annotation
schemes for clinical narratives. Second, we provide mate-
rial for the annotation of clinical narratives, including a
new annotation scheme, companion annotation guide-
lines, and insight on how to devise an annotation method-
ology for a new project.
Background
Representation of information in clinical text corpora
Ethical issues need to be considered before carrying out
research on clinical narratives. Privacy issues require sup-
plementary measures to de-identify patient data before
releasing the corpus for research. De-identification is usu-
ally performed by removing or replacing Personal Health
Identifiers with surrogates [14]. This is one of the reasons
why clinical corpora are less available than corpora in the
biological domain [15, 16].
Improvements in clinical information processing have
been reported by adopting adequate annotation frame-
works [11, 15, 1719]. These have been developed in
two levels of representation. A low-level annotation is
concerned with linguistically and clinically grounded rep-
resentations to use within a document. This level is con-
cerned with defining (in annotation guidelines) mentions
of clinical and linguistic interest, and then marking these
instances in clinical text. Most annotation efforts in the
biomedical NLP community have followed this trend,
especially within the organisation of research challenges.
The second level of representation is a high-level
annotation that prioritizes formally integrating all the
annotated linguistic and clinical data. That is, this level
prioritizes processing the annotated information for rea-
soning over the whole EHR in a computationally action-
able way. Within the context of the Strategic Health
IT Advanced Research Project (SHARPn), [20] and [21]
developed a higher-level formal (OWL) clinical EHR rep-
resentation (implemented in cTAKES [22]). This repre-
sentation is based on the low-level annotation framework
explained in [23]. The SHARPn normalized data has been
thus converted automatically to the Resource Descrip-
tion Framework (RDF) format by using the CEM-OWL
specification. The Biological Expression Language (BEL)
[24] seems to be a mix between the low and high-level of
annotation for life science text (vs. clinical).
Our work has carried out a low-level annotation, but our
scheme can likely be compatible with a high-level repre-
sentation in the long-run. In the following section, we will
review other low-level annotation frameworks of clinical
corpora.
Related work
In this section, we focus on well-documented frameworks
issued from medium-scale projects, or schemes that have
been widely used in shared tasks or challenges. Additional
examples of annotation efforts of clinical data for specific
Deléger et al. Journal of Biomedical Semantics  (2017) 8:37 Page 3 of 18
applications or experiments, where the representation
scheme or annotation work is not the main focus, are
reported in [2536] (inter alia). These will not be reviewed
in detail herein, as we chose to provide an in-depth analy-
sis of efforts providing rich annotation guidelines that we
relied on to build our own scheme.We refer the readers to
a recent review of the litterature in clinical NLP for a more
complete overview of the field [37].
We review the annotation schemes outlined in Table 1,
in chronological order. Note that we classified the anno-
tations in the Informatics for Integrating Biology and the
Bedside (i2b2) challenges as entities or attributes, in order
to make clearer the comparison between schemes. How-
Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 
DOI 10.1186/s13326-017-0129-x
RESEARCH Open Access
Data- and expert-driven rule induction
and filtering framework for functional
interpretation and description of gene sets
Aleksandra Gruca* and Marek Sikora
Abstract
Background: High-throughput methods in molecular biology provided researchers with abundance of
experimental data that need to be interpreted in order to understand the experimental results. Manual methods of
functional gene/protein group interpretation are expensive and time-consuming; therefore, there is a need to
develop new efficient data mining methods and bioinformatics tools that could support the expert in the process of
functional analysis of experimental results.
Results: In this study, we propose a comprehensive framework for the induction of logical rules in the form of
combinations of Gene Ontology (GO) terms for functional interpretation of gene sets. Within the framework, we
present four approaches: the fully automated method of rule induction without filtering, rule induction method with
filtering, expert-driven rule filtering method based on additive utility functions, and expert-driven rule induction
method based on the so-called seed or expert terms  the GO terms of special interest which should be included into
the description. These GO terms usually describe some processes or pathways of particular interest, which are related
to the experiment that is being performed. During the rule induction and filtering processes such seed terms are used
as a base on which the description is build.
Conclusion: We compare the descriptions obtained with different algorithms of rule induction and filtering and
show that a filtering step is required to reduce the number of rules in the output set so that they could be analyzed by
a human expert. However, filtering may remove information from the output rule set which is potentially interesting
for the expert. Therefore, in the study, we present two methods that involve interaction with the expert during the
process of rule induction. Both of them are able to reduce the number of rules, but only in the case of the method
based on seed terms, each of the created rule includes expert terms in combination with the other terms. Further
analysis of such combinations may provide new knowledge about biological processes and their combination with
other pathways related to genes described by the rules. A suite of Matlab scripts that provide the functionality of a
comprehensive framework for the rule induction and filtering presented in this study is available free of charge at:
http://rulego.polsl.pl/framework.
Keywords: Functional description, Gene Ontology, Logical rules, Expert-driven rule induction
Background
Introdution
Over 20 years ago, high-throughput technologies for the
analysis of genomic data opened a new era in molecu-
lar biology and genetics. Since the beginning of the so-
called genomic era, advanced tools and techniques such
*Correspondence: aleksandra.gruca@polsl.pl
Institute of Informatics, Silesian University of Technology, Akademicka 16,
44-100 Gliwice, Poland
as DNA microarrays [1] and next-generation sequencing
(NGS) [2] systems allow for studying genomes, analyz-
ing cellular processes and interactions, which is the first
step of research leading to diagnosis of diseases and
invention of new drug, and treatment discovery [35].
However, to be effective, todays genomic technologies
require not only reagents and sophisticated laboratory
instruments but also application of new software, algo-
rithms, and knowledge discovery techniques in order
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 Page 2 of 14
to process and analyze huge amount of experimental
data [68].
Many of the experiments using genomic technologies
are focused on searching of co-regulated genes that play
an important role in some biological processes partic-
ularly interesting from the experimental point of view.
Typically, genes that work coordinately as genemodules or
gene networks are seen as groups characterized by similar
expression levels and can be found by applying clustering
methods to the expression data [913]. However, the func-
tional analysis and interpretation of gene clusters obtained
in such a way are difficult and time-consuming, especially
if each gene composing the group is manually analyzed by
an expert in the field, based on his or her experience and
literature searches.
To help the expert during such analysis, a lot of tools
have been invented and successfully applied during last
years. One of the most frequently used tools is the Gene
Ontology (GO) database, which is a collaborative effort to
address the need for consistent descriptions of gene prod-
ucts across databases [14]. The information in the GO
database is divided into three separate structures in the
form of directed acyclic graphs (DAGs): Biological Process
(BP), Molecular Function (MF) and Cellular Component
(CC). Each node of the graph has a label t called the
Gene Ontology term and has a unique seven-digit num-
ber, name, short description, and defined relationship to
one or more terms in the same domain.
The information included in the GO database is pro-
vided on different levels of specificity: the terms found
closer to the root of the graph (higher in the hierarchy) are
general descriptions, and as the graph is traversed down
to its leaves, the terms become more and more specific.
The important part of GO database are annotations that
associate gene products with particular terms in Gene
Ontology graph. Each gene product can be annotated to
zero or more terms of any ontology on any level of the
GO graph. Annotations are independent of each other, but
should be made on the most detailed level in the ontology
as annotating to a particular term implies annotation to all
its parent terms up to the root.
In this paper, we describe a comprehensive framework
for functional description of gene sets based on the so-
called logical rules that are combinations of GO terms.
The presented approach involves (i) method of rule induc-
tion which takes into account the structure of Gene
Ontology database, (ii) method of rule interestingness
assessment based on various subjective and objective cri-
teria, and (iii) the method of rule filtering that allows
removing the rules that are uninteresting from the expert
point of view from the output rule set. Finally, (iv) we
present a new, semi-interactive method of rule induction
which allows the expert to influence the process of rule
generation by providing a set of so-called seed or expert
terms, that is the GO terms of special interest, which
should be included into the description. These GO terms
usually describe some processes of particular interest, fre-
quently related to the experiment that is being performed.
During the rule induction and filtering process such
seed terms are used as a base on which the description
is built.
Using Gene Ontology database for functional analysis
The first approach to the automated functional interpre-
tation was the so-called single-term analysis in which,
based on the results of the statistical test, a list of
over-represented GO terms describing gene groups was
obtained. A number of tools were created based on the
idea of single-term analysis, which is still the most com-
mon approach used for functional interpretation of gene
sets [15].
Another approach to the methods of automated func-
tional interpretation was the introduction of more
advanced tools such as RuleGO [16] or GeneCodis
[17] that search for the so-called logical rules that
include combinations of GO terms. The rationale
standing behind such approach is that the combina-
tions of GO terms are more specific and therefore
can show significance, whereas single terms do not
show statistically significant enrichment or depletion. If
we analyze GO terms separately, some of them may
be too general to be included in the list of stati-
cally significant terms; however, their combination with
other terms may present some novel and interesting
information.
In our previous research [18], we showed that the num-
ber of possible statistically significant combinations of
co-existing pathways is huge and that a filtering step is
required in order to reduce the number of possible results.
However, frequently, an expert who designs an experi-
mentmight be interested in some specific process or event
related to the research. For example, in cancer research
searching for a gene signature, which could be potentially
useful for diagnosis or could suggest novel drug targets,
one may look for genes involved in particular biological
process or network related to transformation of normal
cells into cancer cells. Therefore, there is a concern that
automated filtering methods could remove some rules
that consists of GO terms potentially interesting to the
expert. To address this issue, we propose a new method-
ology of rule induction and filtering which allows for
including the expert domain knowledge into rule genera-
tion and filtering process. The new approach is based on
the RuleGO algorithm, and it allows the expert to influ-
ence the process of rule generation by defining the GO
terms of special interests, which are then included into
the rules and preserved in the output rule set after the
filtering step.
Gruca and Sikora Journal of Biomedical Semantics  (2017) 8:23 Page 3 of 14
Related work
So far, to find co-appearance of Gene Ontology terms,
association rule induction algorithms were applied.
Caramona-Saez et al. [19] proposed a method that com-
bines expression data and biological information. Later,
in another study, Caramona-Saez et al. [20] introduced
the Genecodis web-based tool for integrated analysis of
annotations from different sources. The method uses the
Apriori algorithm [21] to discover sets of annotations that
frequently cooccur in the analyzed group of genes. A
similar tool that allows finding combinations of anno-
tations from many different sources such as functional
categories, gene regulation, sequence properties, evolu-
tion, and conservation was presented by Hackenberg et
al. [22]. Also, Gruca [23] applied FP-growth algorithm to
find combinations of GO terms for functional description
of genes.
Research on the induction of rules that combine gene
expression data and biological information was also per-
formed [2426]. For example, in Lopez et al. [25], gene
groups described by similar values of the so-called struc-
tural features (e.g., gene length, the number of nucleotides
in the coding sequence, gene G+C content) with the cor-
responding GO terms are also joined by means of associa-
tion rules. Hvidsten et al. [27] proposed conditional rules
of the form "IF conjunction of conditions describing time
series of gene expression profile THENGO term". In a rule
conclusion, a set of Gene Ontology terms describing the
group were included.
Rule induction techniques mentioned earlier have two
basic drawbacks that can make obtained rules difficult or
even impossible to interpret. First, known rule induction
methods do not consider the fact that hierarchy of GO
terms could result in replacing a conjunction of attributes
with one, more specific GO term at the lowest level in the
GO graph hierarchy. Second, all the methods mentioned
earlier lead to generate a huge number of rules without
providing more advanced (apart from a p-value and a rule
coverage) methods of rule interestingness evaluation and
rule filtering.
In a previous study [18], we proposed the rule induction
algorithm which takes into account the structure of the
Gene Ontology graph and the method of selection of the
most important GO terms. The selection method is based
on the Rough Set Theory [28] and the asymmetrical indis-
cernibility relation. However, the number of induced rules
was still too large. Therefore, another method for rule fil-
tering based on subjective rule attractiveness measure was
proposed in Gruca and Sikora [29].
The problem of finding the minimal subset of the set
of rules, which has lower complexity and simultaneously
maximizes the value of the specified criterion (e.g., over-
all classification accuracy) is NP-complete and computa-
tionally expensive. For descriptive purpose or when the
classification ability is not the most important feature,
the rule elimination procedures (rule filtering) are based
on the minimum interestingness requirements (typically
some well-known rule interestingness measures are cho-
sen) [30, 31]. Some papers also refer to multicriteria rule
evaluation, and in such a case, machine learning [32] and
multicriteria decision-making [33] methods are applied.
These methods can be called supervised because they use
information obtained from an expert. For example, Lenca
[33] apply the PROMETHEE method [34] to select inter-
estingness measure which is able to order a rule set in a
manner most similar to the order provided by an expert.
In biological or medical applications, it is very impor-
tant to determine the rules containing information that
is interesting for a user. However, automatic selection of
elementary conditions included in the rule premises is
the main principle of rule induction algorithms, and rules
induced in this way may not always include knowledge
that is interesting and useful to the user.
To date, few studies have described how to design
the induction algorithm in such a way that it takes into
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 
DOI 10.1186/s13326-017-0124-2
RESEARCH Open Access
Modulated evaluation metrics for
drug-based ontologies
Muhammad Amith and Cui Tao*
Abstract
Background: Research for ontology evaluation is scarce. If biomedical ontological datasets and knowledgebases are
to be widely used, there needs to be quality control and evaluation for the content and structure of the ontology. This
paper introduces how to effectively utilize a semiotic-inspired approach to ontology evaluation, specifically towards
drug-related ontologies hosted on the National Center for Biomedical Ontology BioPortal.
Results: Using the semiotic-based evaluation framework for drug-based ontologies, we adjusted the quality metrics
based on the semiotic features of drug ontologies. Then, we compared the quality scores before and after tailoring.
The scores revealed a more precise measurement and a closer distribution compared to the before-tailoring.
Conclusion: The results of this study reveal that a tailored semiotic evaluation produced a more meaningful and
accurate assessment of drug-based ontologies, lending to the possible usefulness of semiotics in ontology evaluation.
Keywords: Ontology, Ontology evaluation, Quality assessment, Drug ontologies, Semiotics, Metrics, Knowledgebases
Background
Given a scenario where a researcher is to choose two
distinctly independent ontologies that cover a specific
domain, how would the researcher know which is suit-
able between the two? Or given another scenario where a
knowledge engineer is developing an ontological knowl-
edgebase, how would she evaluate the quality of the
ontology and know what to measure? This paper aims
to provide a direction in the area of ontology evaluation
using a system shaped by the theory of semiotics  the
study of meaning for signs and symbols, specifically for
biomedical ontologies.
Biomedical ontologies have influencedmedical research
with the impact and efforts of the Gene Ontology [1],
UMLS [2], SNOMED [3], etc. It is assumed that ontolog-
ical knowledgebases for biomedicine will grow to cover
many other sub-domains. Already, an NIH-funded ini-
tiative, the National Center for Biomedical Ontologies
(NCBO), exist to provide tools and hosting support
for ontologies, and an active community of biomedi-
cal researchers formed the Open Biomedical Ontologies
*Correspondence: cui.tao@uth.tmc.edu
School of Biomedical Informatics, University of Texas Health Science Center,
Fannin Street, Houston, Texas, USA
(OBO) Foundry [4] for rigorous standards for biomedical
ontologies.
Semiotics is formally defined as the the study of signs
and symbols and how they are used [5]. Abstractly, an
ontology, with its terms and labels, can be a symbolic rep-
resentation or signifier of a domain space that describe a
physical manifestation of the real world. However, framing
the ontology domain in semiotics is inherently common.
While touching upon the three branches of semiotics,
Sowa made a philosophical-oriented explanation of how
the study of signs relate to 1) the syntax of an ontology
(syntactic), 2) the meaning and logic derived from the syn-
tax (semantics ), and 3) the users or agents that interpret
or utilize the signs (pragmatics) [6]. Approaching ontol-
ogy evaluation from the semiotic frame is a natural choice
to assess the overall craftsmanship of the ontology.
Our research questions in this study focus on 1) whether
a semiotic-based approach for ontology evaluation can
provide meaningful assessments for biomedical ontolo-
gies, and 2) whether this approach can be tailored for
specific types of ontologies to providemore accurate qual-
ity assessments. The use-case focus will be drug-related
ontologies hosted on the National Center for Biomedical
Ontology BioPortal.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 2 of 8
National Center of Biomedical Ontologies
The National Center for Biomedical Ontologies (NCBO)
is a NIH-funded program to provide support tools, and
a repository to store a wide range of ontologies from
the biomedical field. Based on a random survey sam-
ple of selected ontologies conducted from August 2015
(n = 200), the authors of this paper searched for pub-
lished studies that coincided with the development and
the release of the ontology. The outcome of this brief sur-
vey revealed that most of the ontologies from this sample
did not have any documented evidence of any evaluation
(n1 = 183). A relatively small number had some evidence
of any evaluation (n2 = 17). We can surmise that there is a
need for evaluation, and that many biomedical ontologies
lack any formal evaluation.
Also from our review, we noted that if there was
any documented evidence of evaluations, the evaluation
focused on a specific type of assessment. Some report
statistical-related information denoting the number of
ontological elements (classes, properties, etc.) or struc-
tural elements (depth, breadth, etc.). Others reported
query-based or competency questions-driven approaches
to evaluate the degree to which the ontology fulfills a use-
case. A few utilized subject matter experts to review the
general content, and a few measured some specific appli-
cation tasks. Broadly, ontology evaluation appears to be
diversified and focused.
Semiotic Framework for Ontology Evaluation
While there are no agreed standard for ontology eval-
uation, researchers have proposed various evaluation
approaches, such as, metric-based evaluation [7, 8], cov-
erage of domain [9, 10], use-case and requirement assess-
ment [11], and comparison with other ontologies sharing
the same domain [7, 12]. In this study, we applied a
metrics-based method that is rooted in semiotic theory,
and also tailored this method to compare with ontologies
in a similar domain.
A semiotic framework approach for ontology evalua-
tion [13] was proposed by Burton-Jones, et al, nearly a
decade ago when DAML-based ontologies were in exis-
tence. Reorganizing the intrinsic and extrinsic views of
ontologies, it aims to be a holistic, domain-independent,
and customizable approach to evaluate a wide range of
ontologies by framing it in semiotic theory. Scores are
denoted by the pillars of semiotics  pragmatic, syntac-
tic, and semantic. An additional score, social, denotes
an ontologys ranking with other ontologies in a com-
munity. We intend to apply this metric suite for this
study. To derive some of those scores, external software,
like a triple store or WordNet-based APIs, are required.
Detailed discussion of the scoring metric is provided
here at [13], but we will summarize the aspects of the
metric in the following sub-sections. The Eq. (1) below
describe the overall quality evaluation score based on the
four scores.
Q = wq1 ? S + wq2 ? E + wq3 ? P + wq4 ? O (1)
The scores range from 0 to 1, where 1 is the highest and
0 is the lowest. Each of them weighted equally, yet there
are mechanisms to tailor the weights to provide more
influence of a certain aspect or diminish its influence. For
example, if one were to measure the quality of an ontol-
ogy that serves as a hierarchal terminology of terms, then
it would make sense to decrease the weight of the syn-
tactic score since it may under-utilize ontology features.
(2-5) describe the underlying derivatives of the individual
scores and their sub-scores.
Syntactic
Encoded ontologies enable machines to process and inter-
pret the knowledge embedded in the knowledgebase. The
syntactic score (2) describes the encoded readability of the
ontology. Lawfulness (SL) and richness (SR), sub-scores of
the syntactic score, represent conformity of the syntax,
and the utilization of the ontology syntactic features. SL is
calculated by the number of axiom-level violations based
on the OWL 2 standards over the total number of axioms.
The figures can be obtained using the OWL API. SR is
based on the number of ontological features utilized over
the total number of ontological features.
S = ws1 ? SL + ws2 ? SR (2)
Semantic
Terms or labels are one of the fundamental building
blocks of ontological knowledgebases. The semantic score
(3) rates the terms understandability from 3 sub-scores.
Interpretability (EI) rates the ontologys terms from cal-
culating the percentage of terms with at least one word
sense. Consistency (EC) denotes the percentage of terms
that are uniform among the ontology or lack of duplicate
terms (number of duplicates over total number of terms),
and clarity (EA) reveals how each term in the ontology are
ambiguous based on the average number of word senses
for each term (the average word sense per term over the
number of terms).
E = we1 ? EI + we2 ? EC + we3 ? EA (3)
Pragmatic
Pragmatic score (4) is composed of three sub-scores,
which includes comprehensiveness (PO), accuracy (PU),
and relevancy (PR). Comprehensiveness scores an ontol-
ogys domain coverage based on the percentage num-
ber of instances, classes, and properties of the ontology
to a group of ontologies. Accuracy and relevancy are
unique. The former requires domain experts to review
and assess the veracity of facts evoked from the ontology
 percentage of truthful statements. Relevancy varies and
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 3 of 8
depends on possible use-case of the ontology. For exam-
ple, if evaluators are concerned about the ontologys ability
to preform semantic-based searches, then a percentage
of how successful queries is recorded as the relevancy
score. (4) represents the composition of the pragmatic
score.
P = wp1 ? PO + wp2 ? PU + wp3 ? PR (4)
Social
While not particularly related to semiotics, the social
score (5) is an assessment of the ontologys standing
in comparison with other ontologies. The authority (OT)
sub-score is based on the percentage number of links that
the ontology extends with other ontologies and the history
(OH) sub-score is the percentage based on the number of
times the ontology was accessed.
O = wo1 ? OT + wo2 ? OH (5)
In the following sections, we will describe the method-
ology for utilization of the metric suite, and briefly discuss
drug-based ontological datasets. Afterward, the paper will
discuss the results and impact of our results for drug-
based ontologies.
Methods
We experimented with a set of biomedical ontologies
from NCBO Bioportal that have the most visits (based on
September 2015 data), according to the NCBO website. A
total of 66 ontologies were sampled, but 2 were removed
due to issues with the serialization of the files. With the 64
we calculated an aggregation of the scores and produced
the basic statistics (mean,median, etc.) from them. Table 1
shows the results of this effort.
Table 1 NCBO sample aggregate scores
Quality Mean Std. Deviation Min Max
Syntactic .64 .14 .18 .85
Lawfulness .92 .16 .27 1
Richness .36 .18 .07 .69
Semantic .88 .15 .09 .99
Interpretability .88 .14 .01 1
Consistency .84 .40 -.17 1
Clarity .96 .13 .14 1
Pragmatic .02 .07 0 .52
Comprehensiveness .02 .07 0 .52
Social .02 .02 0 .13
History .02 .02 0 .13
Overall Score .39 .05 .21 .48
We also gathered a set of drug-related ontologies (See
Drug Ontologies) and preformed the same aggregation
scoring (Table 2). In addition, we also examined each of
the scores to understand the quality of each drug ontol-
ogy and the whole set in general. Finally, we tailored the
metrics rooted on strengths and weakness of the drug
ontologies, and compared the non-tailored and tailored
aggregation.
Drug Ontologies
We reviewed the list of available biomedical ontologies
that were drug-related for selection in our study. The list
below are the drug ontologies used:
 RxNORM [14]
 VANDF (Veterans Health Administration National
Drug File) [15]
 DRON (Drug Ontology) [16]
 DINTO (Drug-Drug Interaction Ontology) [17]
 DIKB (Drug Interaction Knowledgebase) [18]
 VO (Vaccine Ontology) [19]
 PVOnto (Pharmacovigilance Ontology) [20]
The National Drug Data File, the National Drug File 
Reference Terminology, and Master Drug Data Base Clin-
ical Drugs were not included in our experiment due
unavailability of a downloadable file for testing.
The study utilized the latest version of OWL-API
v4.2.3 [21], MIT JWI v2.4 (for word senses) [22], apache-
commons-lang v3.4 [23], and minimal-json v0.9.4 [24] to
develop Java software code to calculate the scores. For
each of the downloaded ontologies, we collected scores
from the software and recorded the values. Scores that
relied on total times accessed and the number of classes,
instances, and properties were collected from NCBOs
RESTful API.
Table 2 Drug ontology scores (Equal Weighted)
Quality Mean Std. Deviation Min Max
Syntactic .67 .11 .56 .85
Lawfulness .97 .04 .91 1
Richness .36 .19 .15 .69
Semantic .83 .09 .69 .99
Interpretability .80 .31 .1 1
Consistency .73 .25 .37 1
Clarity 1 .01 .98 1
Pragmatic .14 .26 5.98E-04 .52
Comprehensiveness .14 .26 5.98E-04 .52
Social .14 .36 0 .01
History .14 .36 0 .01
Overall Score .45 .10 .31 .59
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 4 of 8
Results
The results are detailed in the subsequents sub-sections.
Certain scores were neglected due to lack of resources
to calculate them (authority, relevancy, and accuracy).
Equal weighted (EW) evaluation scoring was used (6).
Pragmatic score was simply the comprehensiveness due to
lack of resources to calculate accuracy and relevancy, and
the social score was only the history score for the same
reasons described.
QEW =(0.25 ? S)+(0.25?P)+(0.25?E)+(0.25 ? O) (6)
NCBO Bioportal Score (Sample size = 64)
Table 1 depicts the values resulting from the arithmetic
mean of the evaluation scores for the top 64 viewed
ontologies from September 2015. Themean for the overall
quality score for the sample amounted to 0.39 (? = 0.05).
To calculate the comprehensiveness score which required
knowing the number of classes, instances, and properties,
we tallied a total of 1,277,993, and a total accessed (for the
history score) at 152,424 based on the entire set, through
September 2015.
Semantic quality, from the sample set appeared to be
strongest with 0.88, and the weakest aspect appeared
to be social and pragmatic quality. At a more granu-
lar level, clarity which measured ambiguity of terms and
labels revealed a score of 0.96. Lawfulnesswhichmeasured
adherence to ontology standards was also high at 0.92.
Drug Ontology Scoring
Equal weighted scores
Table 2 provides data from equal weighted evaluation
scoring for the set of drug ontologies we assessed. 0.45
(? = 0.10) is the average mean for the 7 drug ontologies.
The total number of classes, instances, and properties
used to derive the comprehensiveness score was 169,862,
and the total number of times the ontology was accessed
was 351,616. This was used to formulate the history score
(social).
From the results and similar to the previous sample set,
semantic quality was the prominent with 0.83 (0.88 for
NCBO). For the sub-scores, clarity and lawfulness both
exhibited high ratings, 1 and 0.97 respectively.
Drug ontology-influencedmodulated scores
From the scores generated earlier, we devised a method
to customize the metrics to accommodate the set of drug
ontologies by modifying the weights. The semantic, prag-
matic, syntactic, and social were 0.83, 0.14, 0.67, and 0.14.
The values were converted proportionally to give weights
for semantic, pragmatic, syntactic, and social (0.46, 0.08,
0.38, and 0.08). With the new values, we replaced the
weights to attain (7), and recalculated our data. Table 3
shows the results from the modulated scoring with each
drug ontology with the unmodified scores,Qmod andQEW
Table 3 Examination of the weighted scores
QEW Qmod Diff S+E P+O
RxNORM 0.64 0.69 0.05 0.70 0.11
DIKB 0.44 0.75 0.31 0.88 0.00
DINTO 0.41 0.69 0.28 0.81 0.01
PVOnto 0.38 0.66 0.28 0.76 0.00
VANDF 0.35 0.57 0.22 0.67 0.02
VO 0.37 0.63 0.26 0.74 0.00
DRON 0.53 0.64 0.11 0.70 0.35
?(? ) 0.45 (0.10) 0.66 (0.05) 0.21 0.75 0.07
respectively. These values were the overall final scores for
Qmod and QEW .
Qmod=(0.38?S)+(0.08?P)+(0.46?E)+(0.08?O) (7)
From Table 3, RxNORM under the equal weighted eval-
uation metric amounted to 0.64 (6) and the modulated
score of 0.69 (7). Similar increases as a result of the modu-
lated scoring produced the same result for the other drug
ontologies. The means of the overall scores were 0.45 and
0.66 (before and after, respectively).
Discussion
In this section, the paper will discuss how the equal
weighted drug ontologies compared to the sample set of
NCBO ontologies (also equal weighted). The purpose is
to assess how an ontology or a group of specific type of
ontologies align with the quality of biomedical ontologies.
Also, this section will compare the equal weighted scor-
ing of drug ontologies and the modulated scoring of drug
ontologies. This will assess whether the modulated met-
rics represented the drug ontologies better than the equal
weighted version. Lastly, the paper will further examine
each individual scores of each drug ontology.
Comparative results with NCBO sample data
When calculating the comprehensiveness and history
score, we utilized the total number of ontological elements
and total times accessed relative to the set they belong
to. Therefore, we will neglected comparison between
pragmatic and social and focused on the other scores
between the NCBO sample and the drug ontology scores,
both of which were equal weighted. Without the afore-
mentioned scores, the overall average mean of the final
quality score were both 0.38, keeping the weights at
0.25 for syntactic and semantic. Closer inspection of the
values between the two tables (Tables 1 and 2) reveal
some close alignment with the greater body with NCBO
ontologies from the sample. Syntactic and its related sub-
scores resemble the same values, however, the semantic
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 5 of 8
quality scoresmight have some deviation. The consistency
sub-score, which scores an ontologys term uniformity
(minimal duplication of terms and labels), appear to be
distinguishable with NCBO sample aggregate (0.73 to
0.84). This could possible reveal that some drug ontolo-
gies may have some duplicated labels, and may have
to resolve those duplication if the ontology is to be
deemed consistent in its domain space within the semi-
otic framework. Since we are utilizing a sample set from
NCBO, any conclusion drawn should be cautiously con-
sidered. Nonetheless, one way of evaluating on ontology,
particularly one that is under-development is to com-
pare the scores with the greater body of biomedical
ontologies.
Comparative results with modulated drug ontology scores
We compared the overall quality scores (6) and the anal-
ogous modulated overall quality score (7) for each of
the drug ontologies (Table 3). With the equal weighted
approach, RxNORM and DRON produced higher qual-
ity scores (0.64 and 0.53). Examining their respec-
tive scores, specifically looking at S (syntactic) and E
(semantic) together (S+E), we noted that both RxNORM
and DRON were below average compared to other
drug ontologies (Table 3). However, looking at just P
(pragmatic) and O (social) together (P + O), RxNORM
and DRON score above average, while the rest of the
drug ontologies rates below average. So the relatively high
overall score of RxNORM and DRON was mainly due
to their advantage of being accessed more and being
more comprehensive than the other drug ontologies,
which alluded to some unfairness in the equal weighted
metrics.
Focusing the attention on the modulated weighted
scores for the drug ontologies, DIKB ended being the bet-
ter quality drug ontology over RxNORM with an overall
score of 0.75 than RxNORMs 0.69. DINTO also yielded
a score of 0.69. All of the drug ontologies exhibited an
increase (? = 0.21, ? = 0.1), but RxNORM and DRON
produced the smallest gains (0.05 and 0.11). Because the
modulated scoring increased the weights for syntactic and
semantic, where the quality scores of DIKB, DINTO, and
PVOnto exhibited relatively high values, DIKB, DINTO,
and PVOnto reported the largest gains. Also with the
lessen weights for pragmatic and social, RxNORM and
DRON did not have the high quality score that it had
previously.
The average for the entire drug ontology for the equal
weighted metrics was 0.45 (? = 0.10) and for modu-
lated weighted was 0.66 (? = 0.05). Figure 1 shows a
simple histogram of both the equal weighted and mod-
ulated weighted overall score. In general, the modulated
metric that we formulated, what could be, a more faith-
ful and authentic scoring for drug ontologies. The impact
Fig. 1 Density plot of overall quality scores
of this specific effort could provide direction for knowl-
edge engineers to utilize the semiotic framework to tailor
it for specific groups of ontologies. Also, it could be a start
towards a standard metric for any new drug ontologies
under-development or introduced.
Individual drug ontology scores
For each of the drug ontologies, Table 4 provides an
examination of individual scores and sub-scores. The fol-
lowing subsections will discuss some observations of these
values.
Syntactic level
DIKB, DINTO, and DRON exhibited strong seman-
tic quality (S) as evident by the high scores. Looking
at both DIKB and DINTOs richness (SR) and syntac-
tic (SL) sub-scores both rated very high, revealing low
ontological violations and utilized more ontological fea-
tures. DRONs richness score was below the average,
yet the average was particularly high. The strength of
DRON was due to the utilization of many ontologi-
cal features. Both RxNORM and VANDF rated below
average for syntactic quality, and both had the lowest
richness and syntactic, indicating relatively lower than
average use of ontological features and more standards
violations.
Because of the very high syntactic (SL) score, there was
a high standard of adherence to syntactical aspect with
drug ontologies. Richness (SR) varied among them as the
scores were differed greatly where half preformed better
than average. Observationally, the drug ontologies that
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 6 of 8
Table 4 Individual drug ontology quality scores
SL SR S EI EC EA E PO P OH O QEW Qmod
RxNORM 0.91 0.21 0.56 0.97 0.54 1.00 0.83 0.22 0.22 0.96 0.96 0.64 0.69
DIKB 1.00 0.67 0.84 0.96 0.87 0.98 0.93 0.00 0.00 0.00 0.00 0.44 0.75
DINTO 1.00 0.49 0.75 0.80 0.88 1.00 0.88 0.03 0.03 0.00 0.00 0.41 0.69
PVOnto 1.00 0.15 0.58 0.93 0.96 0.99 0.95 0.00 0.00 0.00 0.00 0.38 0.66
VANDF 0.91 0.21 0.56 0.96 0.37 1.00 0.77 0.05 0.05 0.03 0.03 0.35 0.57
VO 1.00 0.38 0.69 0.89 0.51 1.00 0.79 0.00 0.00 0.00 0.00 0.37 0.63
DRON 0.96 0.44 0.70 0.10 0.98 1.00 0.69 0.71 0.71 0.01 0.01 0.53 0.64
Mean 0.97 0.36 0.67 0.80 0.73 1.00 0.83 0.14 0.14 0.14 0.14 0.45 0.66
Median 1.00 0.38 0.69 0.93 0.87 1.00 0.83 0.03 0.03 0.00 0.00 0.41 0.66
St Dev 0.04 0.19 0.11 0.31 0.25 0.01 0.09 0.26 0.26 0.36 0.36 0.10 0.05
Min 0.91 0.15 0.56 0.10 0.37 0.98 0.69 0.00 0.00 0.00 0.00 0.35 0.57
Max 1.00 0.67 0.84 0.97 0.98 1.00 0.95 0.71 0.71 0.96 0.96 0.64 0.75
exhibited stronger syntactic richness tend to have higher
semantic (S) score.
Semantic level
Examining the semantic quality, DIKB, DINTO, and
PVOnto displayed the highest scores. All three denote
better than average sub-scores for interpretability (EI),
consistency (EC), and clarity (EA)  ontological terms
expressiveness, uniqueness, and ambiguity. DINTO
assessed less ambiguity, DIKBs unique trait appear to
be interpretability, and PVOnto strong point was the
consistent usage of terms and labels. VANDF rated lower
than average and lowest of the group for semantic quality.
This was due to consistency being drastically lower, even
though it exhibited expressive terms and less ambiguity
of the terms.
Overall, clarity is exemplary among the drug ontologies,
indicating less ambiguity among the terms, however they
vary with consistency and interpretability. Drug ontologies
could benefit from better selection of terms and finding
terms with better expressiveness (terms with at least one
word sense).
Pragmatic level
Noted earlier, pragmatic (P) score was limited by the use
of comprehensiveness (PO) sub-score. To reiterate, com-
prehensiveness was determined by the number classes,
instances, and properties over the total of those elements
in a set. Both DRON and RxNORM exhibited higher than
the median score for (P). DRON had substantially promi-
nent pragmatic score with 0.71 (? = 0.14, ? = 0.26).
Scores that denoted 0.00 had values very low to display to
two significant digits. Prolific drug ontologies tended to
be large in size and scope.
Social level
Similar to pragmatic (P), the social (O) score was deter-
mined by one sub-score  history (OH). Social mea-
sures the ranking of the ontology among the community.
RxNORM indicated a very prominent score of 0.96 (? =
0.14, ? = 0.36). With a median among them being 0,
most of the drug ontologies compared to RxNORM did
not have same level access or popularity. It is difficult to
determine ways to improve history (number of times of
accessed) of ontologies that are not as prolific. However,
if community ranking of an ontology is important to a
researcher or developer, this score would be an interesting
factor to consider in any decision making for biomedical
ontology selection or usage.
Limitations and Future Direction
This study utilized the Burton-Jones, et al. semiotic eval-
uation metric suite to assess NCBO ontologies, and drug-
related ontologies. Despite our efforts in revealing new
findings about drug ontologies and establishing a method
to tailor evaluation for a set of ontologies, some of what
was presented had some limitations.
One of them is the sample set of NCBO ontologies. In
the future, we would ideally like to have a larger body of
ontologies from NCBO to generate a more representa-
tive score for comparative purposes with other ontologies
or a group of ontologies, as we have shown in this study.
With a larger set, it is also possible to look at other factors
that can be considered for evaluation, like breadth, num-
ber of children nodes, etc. Also, a few of the scores we
could not produce values due to lack of time and human
resources to preform reviews for scores like accuracy or
relevancy. However, the benefit of the semiotic framework
Amith and Tao Journal of Biomedical Semantics  (2017) 8:17 Page 7 of 8
Fig. 2 Ontokeeper screenshot
for ontology evaluation is the openness to customize the
metric to suit certain situations, like the lack of subject
matter experts.
Initially, we investigated the option for an automated
approach to determine appropriate weights for the ontolo-
gies. However, we deduced that tailoring the weights is
subjective, and that an automated approach would likely
provide weights independently of a priori knowledge. Yet
one possibility that was considered, and perhaps a future
possibility, was investigating the use of genetic program-
ming algorithms [25] to approximate weights for the drug
ontologies, and then apply k-fold validation to establish if
the suggested weights are useful. Supervised learning or
other related approaches are potential options.
SEMS (Semiotic Evaluation Metric Suite) aka Ontokeeper
Another direction we are engaged is to develop a front-
end tool for users to evaluate ontologies very quickly, and
also to have some suggested ideas for users to improve
the ontology based on the scores [26]. The prototype web-
based tool was called SEMS (Semiotic Evaluation Metric
Suite), now called Ontokeeper, which supports most of
the automated score generation, and will facilitate the col-
lection of feedback from subject matter experts to assist
in the calculation of the accuracy score. Figure 2 shows a
sample screenshot of the updated version of Ontokeeper.
Conclusion
Using a semiotic framework for ontology evaluation,
this paper demonstrated a tailored metric that closely
approximated the quality of a set of NCBO drug ontolo-
gies. The scores and sub-scores from examination indi-
cated that NCBO drug ontologies could improve with
greater use of syntactic ontological features, better selec-
tion of terms and terms with expressive quality, and per-
haps improve consistency among the terms and labels.
Through the use of a multidimensional metric-based
approach, our efforts may be one of several promising
directions for biomedical ontology evaluation that needs
further investigation.
Acknowledgements
Research was partially supported by the National Library Of Medicine of the
National Institutes of Health under Award Number R01LM011829 and the
Cancer Prevention Research Institute of Texas (CPRIT) Training Grant
#RP160015.
Authors contributions
MA developed the draft and produced the data. TC revised the draft and
reviewed the results. Both authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 22 November 2016 Accepted: 17 March 2017
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 
DOI 10.1186/s13326-017-0136-y
SOFTWARE Open Access
RDFIO: extending Semantic MediaWiki for
interoperable biomedical data management
Samuel Lampa1* , Egon Willighagen2, Pekka Kohonen3,4, Ali King5, Denny Vrandec?ic´6,
Roland Grafström3,4 and Ola Spjuth1
Abstract
Background: Biological sciences are characterised not only by an increasing amount but also the extreme
complexity of its data. This stresses the need for efficient ways of integrating these data in a coherent description of
biological systems. In many cases, biological data needs organization before integration. This is not seldom a
collaborative effort, and it is thus important that tools for data integration support a collaborative way of working. Wiki
systems with support for structured semantic data authoring, such as Semantic MediaWiki, provide a powerful solution
for collaborative editing of data combined with machine-readability, so that data can be handled in an automated
fashion in any downstream analyses. Semantic MediaWiki lacks a built-in data import function though, which hinders
efficient round-tripping of data between interoperable Semantic Web formats such as RDF and the internal wiki format.
Results: To solve this deficiency, the RDFIO suite of tools is presented, which supports importing of RDF data into
Semantic MediaWiki, with metadata needed to export it again in the same RDF format, or ontology. Additionally, the
new functionality enables mash-ups of automated data imports combined with manually created data presentations.
The application of the suite of tools is demonstrated by importing drug discovery related data about rare diseases
from Orphanet and acid dissociation constants from Wikidata. The RDFIO suite of tools is freely available for download
via pharmb.io/project/rdfio.
Conclusions: Through a set of biomedical demonstrators, it is demonstrated how the new functionality enables a
number of usage scenarios where the interoperability of SMW and the wider Semantic Web is leveraged for
biomedical data sets, to create an easy to use and flexible platform for exploring and working with biomedical data.
Keywords: Semantic MediaWiki, MediaWiki, Wiki, Semantic Web, RDF, SPARQL, Wikidata
Background
While much attention has been paid to the ever growing
volumes of biological data from recently emerging high
throughput technologies [1, 2], the biological sciences are
importantly also characterised by the extreme complexity
of its data. This complexity stems both from the incredible
inherent complexity of biological systems, as well as from
the vast number of data formats and assisting technolo-
gies developed by the scientific community to describe
these systems. In order to provide a coherent description
of biological systems making use of the data sources avail-
able, data integration is of central importance [3]. Also,
*Correspondence: samuel.lampa@farmbio.uu.se
1Department of Pharmaceutical Biosciences, Uppsala University, SE-751 24,
Uppsala, Sweden
Full list of author information is available at the end of the article
while there are vast amounts of biological data publicly
available, for many problems the necessary data to be inte-
grated is still comparably small, however complex, and in
need of organization before integration.
Biological data integration is an active field of research
and a number of strategies have been presented for
addressing the data integration problem [4, 5]. Data inte-
gration involves a wide range of considerations, including
data governance, data licensing issues and technology. In
terms of technical solutions, the most central solution for
data integration proposed so far is a set of flexible and
interoperable data formats and technologies commonly
referred to as the Semantic Web [6, 7], with its main
underlying data format and technology, the Resource
Description Framework (RDF) [8, 9], accompanied by
technologies such as the SPARQL Protocol and RDF
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 2 of 13
Query Language (SPARQL) [10] and the Web Ontology
Language (OWL) [11].
The power of these data formats and technologies lie in
their ability to capture data, ontologies and linking infor-
mation between multiple ontologies in a single underlying
serialisation format. This enables disparate user commu-
nities to create data sets adhering to different ontologies
and adding linking information between datasets after-
wards. It furthermore enables generic tools to leverage
the ontology and linking information to present data
from multiple sources in a coherent, integrated fashion,
on-demand.
While most biological data today is not available in
RDF format, initiatives such as the Bio2RDF project [12]
are tackling this by providing a way to convert publicly
available datasets in non-RDF formats to RDF, by writ-
ing so called rdfizers for each dataset, and using a URI
normalisation scheme developed as part of the project
to ensure that URIs referring to the same object are
encoded in the same way [12]. More recent examples of
well supported RDF-ization efforts of biological data are
the Open PHACTS project and platform [13, 14], pro-
viding an integrated environment for working with data
and tools related to drug discovery, and the EBI RDF [15]
platform, which provides data from multiple of EBIs bio-
logical data sources in an integrated semantic data layer
where connections between multiple data sources can
easily be made, e.g. at the time of querying the data via the
SPARQL endpoint made available.
The heterogeneous nature of biological data also
means that the task of managing, annotating, curat-
ing and verifying it is prohibitively complex for a sin-
gle researcher to carry out because of the knowledge
needed to understand the many biological systems, data
formats and experimental methods involved. This high-
lights the importance of effective collaborative tools in
biology, to allow experts from multiple sub-fields within
biology to work together to build integrated biologi-
cal data sources. For example, in the chemicals and
nanomaterials safety science field, semantically annotated
databases with domain-specific ontologies are being used
to standardise collaborative community data entry and
curation [16, 17].
One successful approach to enable flexible collaboration
on biological data is wiki systems [18, 19]. Wikis facilitate
collaboration by removing technological complexity from
the editing process, allowing anyone with access to the
wiki to edit any part of it. Instead of complicated authen-
tication controls, it generally manages trust in the content
by saving every change in the system as a new revision,
not allowing deletion of content, and logging which user
did the change. This way, other users can review changes
made andmake any corrections needed or simply roll back
changes that do not fulfil the criteria set up for the data
source, resulting in a simple and friendly environment for
editing content for any user.
Plain-text wiki systems have a large drawback though:
They only allow plain text to be stored while lacking sup-
port for structured, machine-readable, data. To solve this
problem a solution proposed by a number of groups is
to combine a wiki system with support for storing struc-
tured data in the form of semantic facts, consisting of a
propertyvalue pair, closely mapping to the predicate and
object in RDF triples, and resulting in a combination of
the ease-of-use, and flexibility of wikis, with the ability
to create structured, machine-readable data. A review of
numerous Semantic Wiki implementations is available in
[20]. A recent wiki approach for databases was introduced
with the Wikibase software used by the Wikidata project
[21] and is already used in the life sciences [22, 23]
Semantic MediaWiki (SMW) [24] is currently one of the
most known and widely used semantic wikis. One of the
factors for its success is that it is based onMediaWiki [25],
the software powering Wikipedia and thousands of other
wikis. SMW allows to combine the unstructured content
of typical MediaWiki wikis, with structural semantic con-
tent, encoded using a dedicated syntax that extends the
MediaWiki syntax.
SMW has found a number of uses in biomedical con-
texts. Apart from often being used as an internal wiki
system at many labs, it has also been used in publicly
available resources, including MetaBase [26], a wiki-
database of biological databases, SNPedia [27], a wiki-
database focusing on medically and personally relevant
Short Nucleotide Polymorphisms (SNPs), the Gene Wiki
portal onWikipedia [28], and a catalog of a transcriptome
based cellular state information in mammalian genomes
in the FANTOM5 project [29].
SMW has many features to make it interoperable with
the rest of the Semantic Web, such as export of normal
wiki pages and the facts that relate them, as RDF/XML,
export of Categories as OWL classes and so called Con-
cepts [30] as OWL class descriptions [31]. Also, integra-
tion with third party semantic data stores is possible via
third party plugins. It also has a feature to enable so called
Vocabulary import, which is a way to link properties in
the wiki to predicates of external Semantic Web ontolo-
gies, by manually creating special articles that define these
links [32].
A notable limitation of SMW is the lack of a general RDF
data import function. That is, the ability to do automatic
batch import of RDF datasets into the wiki. Note that
such a functionality is distinct from the so called vocabu-
lary import feature described earlier, which only enables
manual linking of properties to ontology items, but no
automatic import of data, and no support for import-
ing plain RDF triples (OWL individuals), regardless of
whether an ontology is used or not.
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 3 of 13
This lack of a general RDF import function means
that usage scenarios such as bootstrapping new wikis
from existing data sources, or round-tripping between the
SMW data structure and the RDF data format used in the
wider Semantic Web, are not possible without external
tools. This has important consequences, since for exam-
ple round-tripping between SMW and RDF could provide
important benefits for data integration. As already men-
tioned, wiki systems have proven to be excellent platforms
for collaborative editing. Thus, by storing RDF data in a
text format closely resembling normal wiki syntax, it is
possible to leverage the benefits of a proven wiki platform
to lower the barrier to entry for new users to start edit-
ing semantic data. In other words, allowing full round-trip
between SMW and RDF data sets would allow to present
RDF data in a format more apt to collaborative editing and
curation, after which it can be exported again into the RDF
format for use in the wider Semantic Web.
Additionally, import of RDF data sets into SMW
would allow creating mash-ups, combining automati-
cally imported data sets of moderately large size with
manually created presentations of this data using the
querying and visualisation tools available in SMW or its
eco-system of third-party libraries. Based on these pos-
sibilities it can be concluded that RDF import in SMW
is an enabler of a number of usage scenarios useful in
data integration, including making working with seman-
tic data easier for users without deep knowledge of the
Semantic Web.
There exist a few solutions for semantic data import in
SMW, developed as third-party extensions. Among these,
Fresnel Forms [33] is focused on the import of an ontology
structure rather than plain RDF triples (OWL individ-
uals), and also requires running the Protégé software
outside of the wiki installation. Furthermore, the Linked
Wiki Extension [34] allows import of plain RDF triples but
does this by importing the triples into an external triple
store rather than inserting the data as SMW facts inside
the wiki source text, which is required for being able to
further modify the data in the wiki format.
To solve this lack of plain triples RDF data import into
SMW facts in the wiki text, a set of tools and SMW exten-
sions commonly named as the RDFIO suite was devel-
oped. These tools and extensions are presented below
together with biomedical demonstrators of the benefits of
the methodology.
Implementation
The RDFIO suite consists of the following parts:
1. A web form for importing RDF data via manual entry
or copy-and-paste.
2. A SPARQL endpoint allowing both querying and
creation of RDF triples via an INSERT INTO
statement, as well as RDF export by running
CONSTRUCT queries.
3. A SPARQL endpoint replicator, which can import
semantic data from an external SPARQL endpoint
(in essence creating a mirror of the data set).
4. A command-line import script for import of RDF
data stored in a file.
5. A command-line export script for export for RDF
data into a file.
6. A standalone command-line tool for converting RDF
triples into a MediaWiki XML file, for further import
using MediaWikis built-in XML import function,
named rdf2smw (referred to as rdf2smw below).
Tools 1-5 above were developed in the PHP program-
ming language, as modules of a common MediaWiki
extension called RDFIO. An overview picture of how these
parts are related to each other is available in Fig. 1. Tool 6
above, which is a standalone tool, was developed in the Go
programming language to provide shorter execution times
for the RDF-to-wiki page conversion of large data sets.
Tools 1-3 are implemented as MediaWiki Special-pages,
each providing a page with a web form related to their
task. Tools 1-5 all rely on the PHP based RDF library
ARC2 [35]. ARC2 provides its own MySQL-based data
store which is used for all its functions and which is
installed in the same database as the MediaWiki instal-
lation when installing RDFIO. To enable the ARC2 data
store to capture the data written as facts in the wiki a cus-
tom SMW data store was developed. It hooks into each
page write and converts the SMW facts of the page into
the RDF format used in the ARC2 store.
The most resource demanding part of the import pro-
cess is the creation of wiki pages in the MediaWiki soft-
ware. Thus, to enable previewing the structure of the wiki
pages, most importantly the wiki page titles chosen, before
running the actual import, the standalone tool in 6 above
was developed. By generating a MediaWiki XML file as
an intermediate step before the import, the user has the
option to view the wiki page content and titles in the
MediaWiki XML file in a text editor before running the
file through MediaWikis built-in import function. While
this is not a mandatory step, it can be useful for quickly
identifying whether any configuration settings should be
changed to get more useful wiki page titles, before the
more time-consuming MediaWiki import step is initiated.
The limitation of using the standalone tools is that any
manual changes would be overwritten by re-running the
import (although an old revision with the manual change
will be kept, like always in MediaWiki). We thus antici-
pate that the external tool will only be used for the ini-
tial bootstrapping of the wiki content, while any imports
done after manual changes have been made, will be done
using the PHP based import tool mentioned above, which
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 4 of 13
Fig. 1 Overview of the intended usage for the different parts of the RDFIO suite. The figure shows how RDF data can be retrieved from a set of
different sources, as well as being exported again. The parts belonging to the RDFIO SMW extension and the rdf2smw tool are marked with dashed
lines. The newly developed functionality in this paper is drawn in black while already existing functionality in MW and SMW is drawn in grey color.
Red arrows indicate data going into (being imported into) the wiki, while blue arrows indicate data going out of (being exported from) the wiki.
From top left, the figure shows: i) how RDF data files can be batch imported into SMW either by using the rdf2smw tool to convert them to
MediaWiki XML for further import using MediaWikis built-in XML import function, or via the importRdf.php commandline script in the RDFIO
SMW extension, ii) how plain triples (OWL individuals) can be imported from text files, or from web pages via copy and paste into a web form, iii)
how a remote triple store exposed via a SPARQL endpoint can be replicated by entering the SPARQL endpoint URL in a web form, iv) how new RDF
data can be created manually or dynamically in the SPARQL endpoint via SPARQL INSERT INTO statements supported by the SPARQL+
extension [44] in the ARC2 library, and finally, v) how data can also be exported via the SPARQL endpoint, using CONSTRUCT queries, or vi) by using
the dedicated exportRdf.php commandline script
supports updating facts in place without overwritingman-
ual changes.
Results and discussion
To solve the lack of RDF import in SMW, the RDFIO suite
was developed, including the RDFIO SMW extension
and the standalone rdf2smw tool. The SMW extension
consists of a set of functional modules, each consist-
ing of a MediaWiki Special page with a web form, or a
commandline script. A description of the features and
intended use of each of these parts follows. See also Fig. 1
for a graphical overview of how the different parts fit
together.
RDF import web form
The RDF import web form allows the user to import RDF
data in Turtle format either from a publicly accessible URL
on the internet, by manually entering or copy-and-pasting
the data into a web form. This allows users to import small
to moderate amounts of RDF data without the need for
command-line access to the computer where the wiki is
stored, as is often required for batch import operations.
The drawback of this method is that since the import
operation is run as part of the web server process, it is not
suited for large amounts of data. This is because it would
then risk using up too much computational resources
from the web server and making the website unresponsive
for other users for a single-server setting, which is often
used in the biomedical domain.
SPARQL import web form
The SPARQL import web form allows importing all
data from an external triple store exposed by a publicly
accessible SPARQL endpoint. Based on an URL pointing
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 5 of 13
to an endpoint it will in principle create a mirror of it,
since the data imported into the wiki will in turn be
exposed as a SPARQL endpoint (see the corresponding
section below). The import is done with a query that
matches all triples in the external triple store (In technical
terms, a SPARQL clause of the form: WHERE { ?s ?p
?o } ). In order not to put too much load on the web
server, the number of triples imported per execution is
by default limited by a pre-configured limit. This enables
performing the import in multiple batches. The user can
manually control the limit and offset values, but the off-
set value will also be automatically increased after each
import, so that the user can simply click the import but-
ton multiple times, to import a number of batches with
the selected limit of triples per batch.
SPARQL endpoint
The SPARQL endpoint (see Fig. 2) exposes all the seman-
tic data in the wiki as a web form where the data can
be queried using the SPARQL query language. The end-
point also allows external services to query it via the
GET or POST protocols. It can output either a formatted
HTML table for quick previews and debugging of queries,
a machine-readable XML result set, or full RDF triples
in RDF/XML format. The RDF/XML format requires the
use of the CONSTRUCT keyword in the SPARQL query
to define the RDF structure to use for the output. Using
CONSTRUCT to output RDF/XML basically amounts to a
web based RDF export feature, which is why a separate
RDF export web form was not deemed necessary.
The SPARQL endpoint also allows adding new data to
the wiki using the INSERT INTO statement available in
the SPARQL+ extension supported by ARC2.
RDF import batch script
The batch RDF import batch script (importRdf.php) is
executed on the command-line, and allows robust import
of large data sets. By being executed using the standalone
PHP or HHVM (PHP virtual machine) [36, 37] executable
and not the web server process, it will not interfere with
the web server process as much as the web form based
import. It will also not run into the various execution time
limits that are configured for the PHP process or the web
server. While a batch-import could also be implemented
using the web form by using a page reload feature, or an
AJAX-based JavaScript solution, this is a more complex
solution that has not yet been addressed due to time con-
straints. Executing the batch RDF import script in the
terminal can look like in Fig. 3.
Stand-alone RDF-to-MediaWiki-XML conversion tool
(rdf2smw)
The rdf2smw tool uses the same strategy for conversion
from RDF data to a wiki page structure as the RDFIO
Fig. 2 A screenshot of the SPARQL endpoint web form in RDFIO. A key feature of the SPARQL endpoint is the ability to output the original RDF
resource URIs of wiki pages, that were used in the original data imported. This can be seen by the checkbox option named Query by Equivalent
URIs and Output Equivalent URIs, named so because the original URIs are stored using the Equivalent URI special property, on each page created
in the import
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 6 of 13
Fig. 3 Usage of the command-line import tool in RDFIO. The figure shows examples of shell commands to use to import an RDF dataset, in this case
in N-triples format, saved in a file named dataset.nt. The steps are: i) Change directory into the RDFIO/maintenance folder, and then ii)
execute the importRdf.php script. One can set the variables --chunksize to determine how many triples will be imported at a time, and
--offset to determine how many triples to skip in the beginning of the file, which can be useful if restarting an interrupted import session. The
$WIKIDIR variable represents the MediaWiki base folder
extension but differs in the following way: Whereas the
RDFIO extension converts RDF to wiki pages and writes
these pages to the wiki database in one go, the standalone
tool first converts the full RDF dataset to a wiki page
structure and writes it to an XML file in MediaWikis
XML import format, as illustrated in Fig. 1. This for-
mat is very straightforward, storing the wiki page data as
plain text, which allows to manually inspect the file before
importing it.
Programs written in Go are generally orders of magni-
tude faster than similar programs written in PHP. This
performance difference together with the fact that the exe-
cution of the standalone rdf2smw tool is separate from
the web server running the wiki is crucial when import-
ing large data sets (consisting of more than a few hundred
triples) since the import requires demanding data opera-
tions in memory such as sorting and aggregation of triples
per subjects. This is themain reason why this external tool
was developed.
The usage of the tool together with MediaWikis built-in
XML import script is illustrated in Fig. 4.
RDF export batch script
The RDF export batch script (exportRdf.php) is a
complement to the RDF export functionality available in
the SPARQL endpoint, which analogously to the import
batch script allows robust export of large data sets with-
out the risk for time-outs and other interruptions that
might happen to the web server process or the users web
browser.
Executing the batch RDF export script in the terminal
can look like in Fig. 5.
An overview of the RDF import process
As can be seen in Fig. 1, all of the import functions run
through the same RDF-to-wiki conversion code except for
the rdf2smw tool which has a separate implementation of
roughly the same logic in the Go programming language.
The process is illustrated in some detail in Fig. 6 and
can be briefly be described with the following processing
steps:
 All triples in the imported chunk (number of triples
per chunk can be configured for the commandline
import script while the web form imports a single
chunk) are aggregated per subject resource. This is
done since each subject resource will be turned into a
wiki page where predicate-object pairs will be added
as SMW fact statements consisting of a
corresponding property-value pair.
 WikiPage objects are created for each subject
resource. The title for this page is determined from
the Uniform Resource Identifier (URI) of the subject,
or from some of the predicates linked to this subject,
according to a scheme described in more detail below.
 All triples with the same subject, which have now
been aggregated together, are turned into SMW facts
(property-value pairs), to be added to the wiki page.
Predicate and object URIs are converted into wiki
page titles in the process, so that the corresponding
Fig. 4 Command-line usage of the rdf2smw tool. The figure shows the intended usage of the rdf2smw command line tool. The steps are, one per
line in the code example: i) Execute the rdf2smw tool to convert the RDF data into a MediaWiki XML file. ii) Change directory into the MediaWiki
maintenance folder. iii) Execute the importDump.php script, with the newly created MediaWiki XML file as first argument. The $WIKIDIR
variable represents the MediaWiki base folder
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 7 of 13
Fig. 5 Usage of the command-line export tool in RDFIO. The figure shows examples of shell commands to use to export an RDF dataset, in this case
in N-triples format, into a file named dataset.nt. The steps are: i) Change directory into the RDFIO/maintenance folder, and then ii) execute
the exportRdf.php script, selecting the export format using the --format parameter. The --origuris flag tells RDFIO to convert SMWs
internal URI format back to the URIs used when originally importing the data, using the linking information added via SMWs Equivalent URI property
Fig. 6 A simplified overview of the RDF to wiki page conversion
process. The figure shows in a somewhat simplified manner, the
process used to convert from RDF data to a wiki page structure. Code
components are drawn as grey boxes with cog wheels in the right
top corner, while data are drawn as icons without a surrounding box.
From top to bottom, the figure shows how RDF triples are first
aggregated per subject, then converted into one wiki page per
subject, while converting all URIs to wiki titles, for new pages and links
to pages, where-after the pages are either written directly to the wiki
database (the RDFIO SMW extension), or converted to XML and
written to files (the standalone rdf2smw tool)
property and value will be pointing to valid wiki page
names. Naturally, if the object is a literal rather than
an URI, no transformation will be done to it. During
this process the pages corresponding to the created
property titles are also annotated with SMW data
type information, based on XML Schema type
information in the RDF source data.
 Optionally, the facts can be converted into a
MediaWiki template call, if there is a template
available that will write the corresponding fact, by the
use of its parameter values.
 In the rdf2smw tool only, the wiki page content is
then wrapped in MediaWiki XML containing meta
data about the page, such as title and creation date.
 In the RDFIO SMW extension only, the wiki page
objects are now written to the MediaWiki database.
Converting URIs to user friendly wiki page titles
The primary challenge in the described process is to figure
out user friendly wiki titles for the resources represented
by URIs in the RDF data. This is done by trying out a
defined set of strategies, stopping as soon as a title could
be determined. The strategies start with checking if there
is already a page available connected to the URI via an
Equivalent URI fact in the wiki text. If this is the case, this
existing title (and page) will be used for this triple. If that is
not the case, the following strategies are tried in the stated
order: 1) If there are any properties commonly used to
provide a title or label for a resource, such as dc:title
from the Dublin Core ontology [38], the value of that
property is used. 2) If a title is still not found, the base part,
or namespace of the URI is shortened according to an
abbreviation scheme provided in the RDF dataset in the
form of namespace abbreviations. 3) Finally, if none of the
above strategies could provide an accepted title, the local
part of the URI (The part after the last / or # character
in the URL) is used.
Performance
Table 1 provides information about the time needed
to import a given number of triples (100, 1000, 10000
or 100000) drawn as subsets from a test dataset (the
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 8 of 13
Table 1 Execution times for importing RDF data into SMW using
the importRdf.php script in the RDFIO extension (column 2) and
converting to MediaWiki XML files using the rdf2smw tool and
then importing the generated XML files with MediaWikis built-in
XML import tool respectively (column 3 and 4), for a few different
dataset sizes (column 1)
Number of Import RDF Convert to XML Import XML
Triples (RDFIO extension) (rdf2smw tool) (MediaWiki XML import)
100 24 s 0.00 s 17 s
1000 179 s (2m59s) 0.02 s 81 s (1m21s)
10000 1652 s (27m32s) 0.3 s 683 s (11m23s)
100000 16627 s (4h37m7s) 18 s 7063 s (1h57m43s)
Comparative Toxicogenomics Database [39], converted
to RDF by the Bio2RDF project), using the RDF SMW
extension directly via the importRdf.php command-
line script, as well as by alternatively converting the data
to MediaWiki XML files with the rdf2smw tool and then
importing them using MediaWikis importDump.php
script. Note that when importing using the rdf2smw tool
the import is thus performed in two phases.
The tests were performed in a VirtualBox virtual
machine running Ubuntu 15.10 64bit, on a laptop running
Ubuntu 16.04 64bit. The laptop used was a 2013 Lenovo
Thinkpad Yoga 12 with a 2-core Intel i5-4210U CPU, with
base and max clock frequencies of 1.7 GHz and 2.7 GHz
respectively, and with 8 GB of RAM. The PHP version
used was PHP 5.6.11. Time is given in seconds and where
applicable also in minutes and seconds, or hours, minutes
and seconds.
Manual testing by the authors show that the perfor-
mance of an SMWwiki is not noticeably affected bymulti-
ple users reading or browsing the wiki. An import process
of many triples can temporarily slow down the brows-
ing performance for other users because of table locking
in the database, though. This is a characteristic common
to MediaWiki wikis, when a large import operation is in
progress, or if multiple article updates are done at the
same time, unless special measures are taken, such as hav-
ing separate, replicated, database instances for reading, to
alleviate the load on the primary database instance.
Continuous integration and testing
The fact that RDFIO is an extension to a larger software
(SMW), which itself is an extension of MediaWiki and
that much of their functionality depends on state in a
relational database, has added complexity to the testing
process. Recently though, continuous integration systems
as well as improved test tooling for MediaWiki and SMW
has enabled better automated testing also for RDFIO.
We use CircleCI as continuous integration system and
results from this and other services are added as indicator
buttons on the README file on the respective GitHub
repositories.
As part of the build process, system tests are run for the
RDF import function and for the RDF export function,
verifying that the exported content matches the data that
was imported. In addition, work has been started to add
unit tests. User experience testing has been carried out in
real-world projects mentioned in the introduction, where
some of the authors were involved [16, 17].
Round-tripping
As mentioned above, a system test for the round-tripping
of data via the RDF and import and export functions is
run, to ensure that no data is corrupted in the process. It
is worth noting though that the RDF export will generally
output more information than what is imported. This is
because SMWdoes store certainmeta data about all pages
created, such as modification date etc. In the system test,
these data are filtered out so that the test checks only con-
sistency of the triples that were imported using RDFIO.
An example of the difference between the imported and
exported data can be seen in Fig. 7.
Known limitations
At the time of writing this, we are aware of the following
limitations in the RDFIO suite of tools:
 The rdf2smw tool supports only N-Triples format as
input.
 There is currently no support for importing triples
into separate named graphs, such that e.g. imported
and manually added facts could be separated and
exported separately.
 There is no functionality to detect triples for removal,
if updating the wiki with a new version of a previously
imported dataset, containing deprecated or having
some triples simply removed.
 Cases with thousands of triples for a single subject
leading to thousands of fact statements on a single
wiki page  while technically possible  could lead to
cumbersome manual editing.
These limitations are planned to be addressed in future
versions of the tool suite.
Demonstrators
Demonstrator I: Orphanet - rare diseases linked to genes
An important usage scenario for RDFIO is to visualise
and enable easy navigation of RDF data by bootstrap-
ping an SMW instance from an existing data source.
To demonstrate this, the open part of the Orphanet
dataset [40] was imported into SMW. Orphanet con-
sists of data on rare disorders, including associated genes.
The dataset was already available in RDF format through
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 9 of 13
Fig. 7 A comparison between data before and after an import/export round-trip. This figure shows to the left a dataset containing one single triple
in turtle format. To the right is shown the data resulting from performing an import/export round-trip  that is, importing the initial data into a
virtually blank wiki (The wiki front page Main Page being the only page in the wiki) and then running an export again. It can be seen in the
exported data how i) The Main Page adds a certain amount of extra data, and ii) how there is a substantial amount of extra metadata about each
resource added by SMW. The subject, predicate and value of the initial triple is color-coded with the same colours in both code examples (both
before and after) to make it easier to find
the Bio2RDF project [12], from where the dataset was
accessed and imported into SMW. This dataset consisted
of 29059 triples and was first converted to MediaWiki
XML using the standalone rdf2smw tool, which was then
imported using MediaWikis built-in XML import script.
This presented an easy to use platform for navigating the
Orphanet data, including creating listings of genes and
disorders. Some of these listings are created automatically
by SMW but additional listings can also be created on
any page in the wiki, including on the wiki pages repre-
senting RDF resources, by using the template feature in
MediaWiki in combination with the inline query language
in SMW [41].
An example of a useful user-created listing on an RDF
node, was to create a listing of all the disorder-gene
associations linking to a particular gene and the corre-
sponding disorder, on the templates for the corresponding
gene pages (For an example, see Fig. 8). In the same way,
a listing of the disorder-gene association linking to partic-
ular disorders and the corresponding genes, was created
on the templates for the corresponding disorder pages.
This example shows how it is possible, on a wiki page
representing an RDF resource, to list not only information
directly linked to this particular resource, but also infor-
mation connected via intermediate linking nodes. Con-
cretely, in the example shown in Fig. 8 we list a resource
type (diseases) on a page representing a gene even though
in the RDF data diseases are not directly linked to genes.
Instead they are linked via an intermediate gene-disorder
association node.
Demonstrator II: DrugMet - cheminformatics/metabolomics
The DrugMet dataset is an effort at collecting experi-
mental pKa values extracted from the literature, linked
to the publication from which it was extracted, and to
the chemical compounds for which it was measured. The
DrugMet dataset was initially created by manually adding
the details in a self-hosted Semantic MediaWiki. The data
was later transferred to the Wikidata platform [21] for
future-proofing and enabling access to the data for the
wider community.
This demonstrator highlights how this data could be fur-
ther curated by extracting the data again from Wikidata
into a locally hosted SMW for further local curation.
The data was exported fromWikidata using its publicly
available SPARQLREST interface [42]. The extraction was
done using a CONSTRUCT query in SPARQL allowing to
create a custom RDF format specifically designed for the
demonstrator. For example, in addition to the publication
and compound data, the query was modified to include
rdf:type information for all the compounds, which is
used by the RDFIO command line tool to generate aMedi-
aWiki template call and corresponding template, for all
items of this type.
After the data was imported into a local SMW wiki, it
allowed to create a page with an SMW inline query dis-
playing a dynamically sorted list of all the compounds,
their respective pKa values, and links to the publications
from where the pKa values were originally extracted. The
query for this extraction is shown in Fig. 9, and the list is
shown in Fig. 10.
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 10 of 13
Fig. 8 Screenshot of a wiki page for a gene in the Orphanet dataset. In the middle of the page, the listing of gene disorder associations and the
corresponding disorders is shown. Note that these details are not entered on this page itself, but are queried using SMWs inline query language
and dynamically displayed. To the right are details entered directly on the page
Implications of the developed functionality
The demonstrators above show that the RDFIO suite of
tools is successfully bridging the worlds of the easy-to-use
wiki systems and the somewhatmore technically demand-
ing wider Semantic Web. This bridging has opened up
a number of useful scenarios for working with seman-
tic data in a flexible way, where existing data in semantic
formats can easily and flexibly be combined by using the
templating and querying features in SMW. This leads to
a powerful experimentation platform for exploring and
summarising biomedical data, which earlier was not read-
ily accessible.
Availability
 Complete information about the RDFIO project can
be found at pharmb.io/project/rdfio
 A canonical location for information about the
RDFIO SMW extension is available at MediaWiki.org
at www.mediawiki.org/wiki/Extension:RDFIO
Fig. 9 The SPARQL query for extracting DrugMet data. This screenshot shows the SPARQL query for extracting DrugMet data in Wikidatas SPARQL
endpoint web form. This query can be accessed in the Wikidata SPARQL endpoint via the URL: goo.gl/C4k4gx
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 11 of 13
Fig. 10 A dynamic listing of DrugMet data. The listing shows a locally hosted SMWwiki with a list of compounds and related information. The list is a
custom, dynamically generated listing of Compound name, pKa value and a link to the publication from which each pKa value was extracted,
created using SMWs inline query language
 All the software in the RDFIO suite is available for
download on GitHub, under the RDFIO GitHub
organisation, at github.com/rdfio where the RDFIO
SMW extension is available at github.com/rdfio/rdfio,
the rdf2smw tool at github.com/rdfio/rdf2smw and
an automated setup of a virtual machine with a fully
configured SMW wiki with RDFIO installed is
available at github.com/rdfio/rdfio-vagrantbox.
Outlook
Planned future developments include enhancing the
rdf2smw tool with support formore RDF formats as input.
Further envisioned development areas are:
iv) Separating the ARC2 data store and SPARQL
endpoint into a separate extension, so that the core
RDFIO SMW extension does not depend on it. This
could potentially improve performance of data import
and querying, as well as make the core RDFIO exten-
sion easier to integrate with external triple stores via
SMWs triple store connector. v) Exposing the RDF import
functionality as a module via MediaWikis action API
[43]. This would allow external tools to talk to SMW
via an established web interface. vi) Allowing to store
domain specific queries tied to certain properties that
can, on demand, pull in related data for entities of a
certain ontology such as gene info from Wikidata, for
genes.
Availability and requirements
Project name: RDFIO
Project home page: https://pharmb.io/project/rdfio
Operating system(s): Platform-independent (Linux,
Windows, Mac)
Programming language: PHP (The RDFIO SMW exten-
tion), Go (The rdf2smw tool)
Other requirements: A webserver (Apache or Nginx), A
MySQL compatible database,MediaWiki, SemanticMedi-
aWiki, ARC2 (RDF library)
License: GPL2 (The RDFIO SMW extention), MIT (The
rdf2smw tool)
Conclusions
The RDFIO suite of tools for importing RDF data into
SMW and exporting it again in the same RDF format
(expressed in the same ontology) has been presented. It
has been shown how the developed functionality enables
a number of usage scenarios where the interoperabil-
ity of SMW and the wider Semantic Web is leveraged.
The enabled usage scenarios include; i) Bootstrapping
a non-trivial wiki structure from existing RDF data, ii)
Round-tripping of semantic data between SMW and the
RDF data format, for community collaboration of the
data while stored in SMW, and iii) Creating mash-ups of
existing, automatically imported data and manually cre-
ated presentations of this data. Being able to combine
the powerful querying and templating features of SMW
with the increasing amounts of biomedical datasets avail-
able as RDF has enabled a new, easy to use platform
for exploring and working with biomedical datasets. This
was demonstrated with two case studies utilising link-
ing data between genes and diseases as well as data from
cheminformatics/metabolomics.
Lampa et al. Journal of Biomedical Semantics  (2017) 8:35 Page 12 of 13
Abbreviations
AJAX: Asynchronous Javascript and XML. A technology to access a web service
from Javascript, for receiving content or performing actions; OWL: Web
ontology language; RAM: Random-access memory; RDF: Resource description
framework; SMW: Semantic MediaWiki; SPARQL: SPARQL protocol and RDF
query language; URI: Uniform resource identifier
Acknowledgements
The authors thank Joel Sachs for mentoring AK during the Gnome FOSS OPW
2014 project.
Funding
The work was supported by the Google Summer of Code program of 2010
granted to WikiMedia Foundation, the Gnome FOSS OPW program for 2014
granted to WikiMedia foundation, the Swedish strategic research programme
eSSENCE, the Swedish e-Science Research Centre (SeRC), and the
eNanoMapper project EU FP7, technological development and demonstration
(FP7-NMP-2013-SMALL-7) under grant agreement no. 604134.
Authors contributions
DV, SL: original concept; SL, OS, EW: planning and design; SL, AK:
implementation; SL, PK, EW, RG: applications. All authors read and approved
the manuscript.
Availability of data andmaterial
The source code of the published software is available at GitHub, http://
github.com/rdfio.
The data used in Demonstrator I in this study are available from the Bio2RDF
website, http://download.bio2rdf.org/release/3/orphanet/orphanet.html.
The data used in Demonstrator II in this study are available from a custom
query in the Wikidata SPARQL Endpoint, https://query.wikidata.org.
The query used in the Wikidata SPARQL endpoint is available on GitHub,
together with a direct link to the Wikidata SPARQL Endpoint with the query
prefilled, https://gist.github.com/samuell/45559ad961d367b5d6a2626
9260dc29a.
The authors declare that all other data supporting the findings of this study
are available within the article.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Pharmaceutical Biosciences, Uppsala University, SE-751 24,
Uppsala, Sweden. 2Department of Bioinformatics - BiGCaT, NUTRIM, Maastricht
University, P.O. Box 616, UNS50 Box 19, NL-6200 MD Maastricht, The
Netherlands. 3Institute of Environmental Medicine, Karolinska Institutet, SE-171
77 Stockholm, Sweden. 4Division of Toxicology, Misvik Biology Oy, Turku,
Finland. 5FanDuel Inc, Edinburgh, UK. 6Google Inc., 345 Spear Street, San
Francisco, USA.
Received: 2 May 2017 Accepted: 1 August 2017
RESEARCH Open Access
MeSH Now: automatic MeSH indexing at
PubMed scale via learning to rank
Yuqing Mao1,2 and Zhiyong Lu2*
Abstract
Background: MeSH indexing is the task of assigning relevant MeSH terms based on a manual reading of scholarly
publications by human indexers. The task is highly important for improving literature retrieval and many other
scientific investigations in biomedical research. Unfortunately, given its manual nature, the process of MeSH
indexing is both time-consuming (new articles are not immediately indexed until 2 or 3 months later) and costly
(approximately ten dollars per article). In response, automatic indexing by computers has been previously proposed
and attempted but remains challenging. In order to advance the state of the art in automatic MeSH indexing, a
community-wide shared task called BioASQ was recently organized.
Methods: We propose MeSH Now, an integrated approach that first uses multiple strategies to generate a
combined list of candidate MeSH terms for a target article. Through a novel learning-to-rank framework, MeSH Now
then ranks the list of candidate terms based on their relevance to the target article. Finally, MeSH Now selects the
highest-ranked MeSH terms via a post-processing module.
Results: We assessed MeSH Now on two separate benchmarking datasets using traditional precision, recall and F1-
score metrics. In both evaluations, MeSH Now consistently achieved over 0.60 in F-score, ranging from 0.610 to 0.
612. Furthermore, additional experiments show that MeSH Now can be optimized by parallel computing in order to
process MEDLINE documents on a large scale.
Conclusions: We conclude that MeSH Now is a robust approach with state-of-the-art performance for automatic
MeSH indexing and that MeSH Now is capable of processing PubMed scale documents within a reasonable time
frame. Availability: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.
Background
The rapid growth of scholar publications in biomedicine
makes the search of relevant information in literature in-
creasingly more difficult, even for specialists [1, 2]. To
date, PubMedthe U.S. National Library of Medicine
(NLM) premier bibliographic databasecontains over 24
million articles from over 5,600 biomedical journals with
more than a million records added each year. To facilitate
searching these articles in PubMed, a controlled vocabu-
lary called Medical Subject Headings (MeSH)1 was created
and updated annually by the NLM since 1960s. Currently,
MeSH 2015 consists of over 27,000 terms representing a
wide spectrum of key biomedical concepts (e.g. Humans,
Parkinson Disease) in a hierarchical structure. MeSH
terms are primarily used to index articles in PubMed for
improving literature retrieval: The practice of manually
assigning relevant MeSH terms to new publications in
PubMed by the NLM human indexers is known as MeSH
indexing [3]. Assigned MeSH terms can then be used im-
plicitly (e.g., automatic query expansion using MeSH) or
explicitly in PubMed searches [4]. Compared with the
commonly used keyword-based PubMed searches, MeSH
indexing allows for semantic searching (using the relation-
ship between the subject headings) and searching against
concepts not necessarily present in the PubMed abstract.
In addition to its use in PubMed, MeSH indexing re-
sults have also been used creatively in many other scien-
tific investigation areas, including information retrieval,
text mining, citation analysis, education, and traditional
bioinformatics research (see Fig. 1). When applied to in-
formation retrieval, MeSH and its indexing results have
been used to build tag clouds for improving the
* Correspondence: zhiyong.lu@nih.gov
2National Center for Biotechnology Information (NCBI), 8600 Rockville Pike,
Bethesda, MD 20894, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 
DOI 10.1186/s13326-017-0123-3
visualization of search results [5, 6] and to help distin-
guish between publication authors with identical names
[7, 8]. Another major use of MeSH indexing is in bio-
medical text mining, where it has been applied to prob-
lems such as document summarization [9], document
clustering [10], and word sense disambiguation [11].
MeSH indexing also serves several key roles in cit-
ation analysis, from identifying emerging research
trends [12, 13] to measuring similar journals [14] and
characterizing research profiles for an individual re-
searcher, institute or journal [15]. In the era of
evidence-based practice, MeSH becomes increasingly
important in assessing and training the literature
search skills of healthcare professionals [16, 17], as
well as in assisting undergraduate education in bio-
logical sciences [18]. Finally, much bioinformatics re-
search, such as gene expression data analysis [19, 20],
greatly benefits from MeSH indexing [2125].
Like many manual annotation projects [2630],
MeSH indexing is a labour-intensive process. As
shown in [3, 31], it can take an average of 2 to
3 months for an article to be manually indexed with
relevant MeSH terms after it first enters PubMed. In
response, many automated systems for assisting
MeSH indexing have been previously proposed. In
general, most existing methods are based on the following
techniques: i) pattern matching, ii) text classification, iii)
k-Nearest Neighbours, iv) learning-to-rank, or v) combin-
ation of multiple techniques. Pattern-matching methods
[32] search for exact or approximate matches of MeSH
terms in free text. Automatic MeSH indexing can also be
regarded as a multi-class text classification problem where
each MeSH term represents a distinct class label. Thus
many multi-label text classification methods have been
proposed, such as neural networks [33], Support Vector
Machines (SVM) [34, 35], Inductive Logic Programming
[36], naïve Bayes with optimal training set [37], Stochastic
Gradient Descent [38], and meta-learning [39]. While the
pattern matching and text classification methods use only
the information in the MeSH thesaurus and document it-
self, the k-Nearest Neighbours (k-NN) approach takes ad-
vantage of the manual annotations of documents similar
to the target document, e.g. [40, 41]. Additional informa-
tion, such as citations, can also be utilized for auto-
matic MeSH indexing. For example, Delbecque and
Zweigenbaum [42] investigated computing neighbour
documents based on the cited articles and cited au-
thors. More recently, Huang et al. [3] reported a
novel approach based on learning-to-rank algorithms
[43]. This approach has been shown to be highly suc-
cessful in the recent BioASQ2 challenge evaluations
[4446] and has also been adopted by many others
[47, 48]. Finally, many methods attempt to combine
results of different approaches [49, 50]. For instance,
the current production system in MeSH indexing at
the NLM is called Medical Text Indexer (MTI),
which is a hybrid system that combines both pattern
matching and k-NN results [51] via manually-
developed rules and continues to be improved over
the years [52, 53]. The proposed method in this work
is also a hybrid system but unlike MTI, which only
uses machine learning to predict a small set of MeSH
terms, it combines individual results and ranks the
entire set of recommendations through machine
learning instead of heuristic rules.
Despite these efforts, automatic MeSH indexing re-
mains a challenging task: the current state-of-the-art
performance remains at about 0.6 in F-measure [54].
Several factors contribute to this performance bottle-
neck: First, since each PubMed article can be assigned
with multiple MeSH terms, i.e. class labels, the task of
automatic MeSH indexing can be seen as a multi-class
Fig. 1 Applications of MeSH
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 2 of 9
classification problem. In this regard, the size of the
MeSH vocabulary makes automatic classification chal-
lenging: 2014 MeSH includes more than 27,000 main
subject headings and they are not equally used in index-
ing [31]. Second, MeSH indexing is a highly complex
cognitive task. It has been reported that the consistency
between human indexers is only 48.2% for main heading
assignment [55]. Lastly, both the MeSH vocabulary and
indexing principles keep evolving over time. For in-
stance, in response to emerging new concepts in the bio-
medical research, MeSH 2014 includes almost five times
more concepts than the edition of MeSH in 1963 that
only contains 5,700 descriptors. On the other hand, the
articles in PubMed are not re-indexed when MeSH gets
updated. Thus, it is not always obvious in selecting
benchmarking data sets for system development and
comparison.
In this paper, we propose a new method, MeSH Now,
to the automatic MeSH indexing task. MeSH Now is
built on our previous research [3] but has a number of
significant advancements: First, MeSH Now combines
different methods through machine learning. Second,
new post-processing and list-pruning steps are now
added in MeSH Now for improved performance. Third,
from a technical perspective, MeSH Now is optimized
using the latest MeSH lexicon and recent indexed arti-
cles for system training and development. Finally, MeSH
Now is implemented to operate in a parallel computing
environment, making it possible for large-scale process-
ing needs (e.g., providing computer results of new
PubMed articles for assisting human indexing). For
evaluation, we first test MeSH Now on a previous data-
set that was widely used in benchmarking. Furthermore,
we created a new benchmarking dataset based on the re-
cent BioASQ 2014 challenge task data. Our experimental
results show that MeSH Now achieves state-of-the-art
performance on both data sets.
Methods
Approach overview
Our approach reformulates the MeSH indexing task as a
ranking problem. Figure 2 shows the three main steps:
First, given a target article, we obtain an initial list of
candidate MeSH terms from three unique sources. Next,
we apply a learning-to-rank algorithm to sort the candi-
date MeSH terms based on the learned associations be-
tween the document text and each candidate MeSH
term. Finally, we prune the ranked list and return a
number of top candidates as the final system output.
Prior to these steps, some standard text processing was
performed such as removing stop words and applying a
word-stemming algorithm.
Input source I: K-nearest neighbours
We first adapt the PubMed Related Articles algorithm
[56] to retrieve k-nearest neighbours for each new
PubMed article. The assumption is that documents simi-
lar in content would share similar MeSH term annota-
tions. Previous work [3] has supported this assumption
by showing that over 85% of the gold-standard MeSH
annotations for a target document are present in its
nearest 20 neighbours.
Furthermore, we found that retrieving neighbours
from the whole MEDLINE database performed worse
than only retrieving neighbours from a subset of the
database (e.g., articles in the BioASQ Journal List, or
newly published articles). In particular, the results of
our approach are best when limiting the neighbour
documents to articles indexed in the last 5 years (i.e.
the articles were assigned with MeSH terms after
2009). As mentioned before, MeSH terms evolve
every year but the articles already indexed will never
be re-indexed. The same article would likely be
assigned with different MeSH terms in 2014 versus
20 years ago. Thus there are many outdated MeSH
Fig. 2 System overview
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 3 of 9
terms in those neighbour documents, which can be
harmful to the accuracy of our approach. Moreover,
the word frequencies are also different in the older and
more recent articles, which are closely related to the simi-
larity score for two articles. Therefore, we built our index
with only articles that were assigned with MeSH terms
after 2009, and retrieved the neighbour documents using
such a new index instead of retrieving similar documents
from the whole PubMed. When building our document
index for the PubMed Related Articles algorithm3, we also
make sure that all annotated MeSH terms are removed
such that they are not used in the computation of the
neighbour documents. In other words, the similarity be-
tween two documents is solely based on the words they
have in common.
The parameter k was fixed (k = 20) in [3], which
means the same number of neighbours will be in-
cluded for all target articles. However, we observed
that some articles may only have a few very similar
documents. We therefore adjust the parameter k dy-
namically between 10 to 40 in this work according to
the similarity scores of the neighbours: the smaller
the average similarity score of the neighbours, the
fewer neighbours will be used. Once those k-nearest
neighbour documents are retrieved, we collect all of
the unique MeSH terms associated with those neigh-
bour documents. Note that we only considered the
main headings and removed subheadings attached to
the main headings.
Input source #2: multi-label text classification
Motivated by [57], we implemented a multi-label text
classification approach where we treat each MeSH con-
cept as a label and build a binary classifier accordingly.
More specifically, we first train individual classification
models for each of the most frequently indexed 20,000
MeSH terms, as the remaining ones are rarely used in
indexing. Then we apply these models to the new article
and add those positively classified MeSH concepts as
candidates to the initial list. We also keep those associ-
ated numerical prediction scores and use them as fea-
tures in the next step.
Our implementation is based on the cost-sensitive
SVM classifiers [58] with Huber loss function [59]. Cost-
sensitive SVMs have been shown to be a good solution
for dealing with imbalanced and noisy data in biomed-
ical documents [60]. Let C+ denote the higher misclassi-
fication cost of the positive class and C? denote the
lower misclassification cost of the negative class, the cost
function is formulated as:
?
2
wk k2 þ Cþ
X
i:yi¼1
h yi ? þ w?xið Þð Þ þ C?
X
i:yi¼?1
h yi ? þ w?xið Þð Þ
where MeSH terms are treated as class labels C in the
classification, xi is a document of a given class (ie
assigned with a specific MeSH term), ? is a
regularization parameter, w is a vector of feature
weights, and ? is a threshold. The function h is the
modified Huber loss function and has the form:
h zð Þ ¼
?4?z;
1?zð Þ2;
0;
8
<
:
z??1
?1 < z < 1
1?z
We can choose C+ to be greater than C? to overcome
the dominance of negative points in the decision process
(here we set C+ = rC? and the ratio r to be 1.5). To train
these 20,000 classifiers, we used the MEDLINE articles
that were indexed with MeSH terms between January
2009 and March 2014.
Input source #3: MTI results
MTI is used as one of the baselines in the BioASQ Task,
which primarily uses MetaMap to map the phrases in
the text to UMLS (Unified Medical Language System)
concepts [61]. We thus add all MeSH terms predicted
by MTI as candidates, and obtained the feature vectors
for those MeSH terms. This is useful since the MTI re-
sults can return correct MeSH terms not found by the
other two methods.
Learning to rank
Once an initial list of candidate MeSH terms from
all three sources are obtained, we approached the
task of MeSH indexing as a ranking problem. In our
previous work, we trained the ranking function with
ListNet [62], which sorts the results based on a list
of scores. In this work we evaluated several other
learning-to-rank algorithms [43] on the BioASQ test
dataset, including MART [63], RankNet [64], Coord-
inate Ascent [65], AdaRank [66], and LambdaMART,
which are available in RankLib v2.24, and found that
LambdaMART achieved the best performance.
LambdaMART [67] is a combination of MART and
LambdaRank, where the MART algorithm can be
viewed as generalizations of logistic regression [63]
and LambdaRank is a method for learning arbitrary
information retrieval measures [68]. To train such a
model, LambdaMART uses gradient boosting to
optimize a ranking cost function where the base
learners are limited-depth regression trees. New trees
are added to an ensemble sequentially that best ac-
count for the remaining regression error of the train-
ing samples, i.e., each new tree greedily minimizes
the cost function. LambdaMART uses MART with
specified gradients and Newtons approximation.
LambdaMART is briefly presented as follows [67]:
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 4 of 9
First, we obtained a training set consisting of biomedical
articles with human assigned MeSH terms from MED-
LINE. For each article, we obtain an initial list of MeSH
terms from its neighbour documents. Each MeSH term is
then represented as a feature vector. For the list of MeSH
terms from its neighbour documents, denoted by {M1,
M2, , MN}, where N is the number of feature vectors and
Mi is the ith feature vector, we obtain a corresponding list
{y1, y2, , yN}, where yi?{0,1} is the ith class label. yi = 1 if
the MeSH term was manually assigned to the target article
by expert indexers of the NLM, otherwise yi =0.
BioASQ provided approximately 12.6 million PubMed
documents for system development. Since all PubMed
documents can be used as training data, we randomly
selected a set of 5,000 MEDLINE documents from the
list of the journals provided by BioASQ for training and
optimizing our learning-to-rank algorithm.
Features
We reused many features developed previously: neighbour-
hood features, word unigram/bigram overlap features,
translation probability features [69], query-likelihood fea-
tures [70, 71], and synonym features.
For neighbourhood features, we calculate both neigh-
bourhood frequency  the number of times the MeSH
term appears in the neighbours, and neighbourhood
similarity  the sum of similarity scores for these
neighbours.
For translation probability features, we use the IBM
translation model [69], which uses title and abstract as
source language, and MeSH terms as target language.
We then utilize an EM-based algorithm to train the
translation probabilities.
For query-likelihood features, we treat each MeSH
term as Query (Q), title and abstract as document, and
use two genres of query models: classic BM25 model
[70] and translation-based query model [71], to calculate
the probability of whether a MeSH term should be
assigned to the article.
In this work, we added a new domain-specific know-
ledge feature. We used a binary feature indicating
whether a candidate term is observed by MTI, which
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 5 of 9
relies heavily on the domain-specific UMLS Meta-
thesaurus [72], for generating its results.
To compute the average length of documents and the
document frequency for each word, a set of approxi-
mately 60,000 PubMed documents is assembled. These
documents are sampled from recent publications in the
BioASQ Select Journal List. The translation model and
the background language model were built through
training with this data set accordingly.
Post-processing and list pruning
We further improve our results with some post-
processing steps.
First, we observed that the Check tags (a special set of
MeSH Headings that are mentioned in almost every art-
icle such as human, animal, male, female, child, etc.5) es-
pecially the tags for the age factor are most difficult for
our approach. The reason is that the Check tags are fre-
quently present in the neighbour documents, e.g., an art-
icle describing a disease in children might have many
similar documents discussing about the same disease in
adults, which will result in assigning the undesirable
Check tag Adult to the new article. On the other hand,
it is improper to simply exclude the tag Adult if
Child already exists, because many articles in PubMed
indeed include both Adult and Child as MeSH terms.
More importantly, many Check tags related to age infor-
mation are added according to the full text article. In
BioASQ, we add the age check tags identified from the
abstract text. We first find the numbers near the explicit
age in the abstract, then predict the correct Age Check
Tag according to those numbers and the rules for age
check tags.
Second, to improve the precision, we remove the par-
ental MeSH terms when a more specific term is also
predicted. This heuristic is based on the principle that
indexers should prefer the most specific term applicable
instead of more general terms. Therefore in the candi-
date list, if a child term is ranked higher than its parent
term, we will remove the latter accordingly.
Finally, after each MeSH term in the initial list is
assigned a score by the ranking algorithm described
above, the top N ranked MeSH terms will be considered
relevant to the target article. N was set to be a fixed
number (N = 25) previously. We found, however, that
the average number of MeSH terms per article in the
BioASQ training data was only 12.7. Thus, we used an
automatic cut-off method to further prune the results
from the top ranked MeSH terms as follows:
Siþ1 < Si? log ið Þ??
where Si is the score of the predicted MeSH term at pos-
ition i in the top ranking list. The rationale for Formula
(1) is that if the (i + 1)th MeSH term was assigned with a
score much smaller than the ith MeSH term, the MeSH
terms ranked lower than i would not be considered rele-
vant to the target article. Formula (1) also accounts for
the fact that the difference between lower-ranked MeSH
terms is subtler than the difference between higher-
ranked MeSH terms. The parameter ? was empirically
set to be 0.3 in this research, and it can be tuned to gen-
erate predictions favouring either recall or precision.
Results
Benchmarking datasets
To demonstrate the progress of our development over
time and compare with other systems, we report our
system performance on two separate data sets. One of
them was widely used in previous studies: NLM2007 [3].
The NLM2007 dataset contains 200 PubMed documents
obtained from the NLM indexing initiative6. The other
is created from the BioASQ 2014 test datasets:
BioASQ5000.
In 2014, the BioASQ challenge task [45] ran for six
consecutive periods (batches) of 5 weeks each. For each
week, the BioASQ organizers distributed new unclassi-
fied PubMed documents, and participants have a limited
response time (less than 1 day) to return their predicted
MeSH terms. As new manual annotations become avail-
able, they were used to evaluate the classification per-
formance of participating systems. To be more general
(each BioASQ test set contains continuous PMIDs which
may belong to a limited set of journals), we randomly se-
lected 5,000 PubMed documents from the latest 9
BioASQ test sets (start from Batch 2 Week 2 in order to
avoid overlap with our system training data) to create
BioASQ5000, with their corresponding MeSH terms
already assigned by December 6, 2014. Compared to
NLM2007, BioASQ5000 is much larger in size and con-
tains more recent articles in 2014.
Comparison of different methods
Here we present our results when evaluated on the two
datasets. We first show results on the previously re-
ported benchmarking dataset, NLM2007 [3] in Table 1.
For comparison, we show the results of our previous
work as Huang et al., [3], and the results of the
Table 1 Evaluation results on NLM 2007 test set
Methods Precision Recall F1
MTI  2011 0.318 0.574 0.409
Huang et al. 2011 [3] 0.390 0.712 0.504
Text Classification 0.655 0.355 0.461
MTI  2014 0.568 0.525 0.545
MeSH Now 0.622 0.602 0.612
Bold data are the best value
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 6 of 9
previous and current versions of MTI (MTI 2011 and
MTI 2014). It should be noted that here we used
MeSH 2010 and retrieved neighbour documents pub-
lished before the articles in NLM2007, and our learning-
to-rank model was trained with documents published
before the articles in NLM2007, because the newly pub-
lished articles are assigned with new MeSH terms which
are not available in NLM2007. We can see that MeSH
Now makes significant improvement over our previous
method. We also notice that the results of MTI-2014 are
much better than those of its previous version. Both
MTI-2014 and text classification results (results of input
source #2) contribute to the MeSH Now performance
with better results generated by MTI than text
classification.
Table 2 shows the results on the BioASQ5000 dataset.
For comparison, we added the results of MTI First Line
(MTIFL_2014) and MTI Default (MTIDEF_2014), both
of which were used as baselines of the BioASQ chal-
lenge. This further verifies that our new approach out-
performs existing methods.
System throughput
The time complexity of large-scale automatic indexing is
crucial to real-world systems but rarely discussed in the
past. In Table 3, we present the average processing time
of each step of our method based on BioASQ5000 on a
single computer. We can see that text classification ap-
pears to be a bottleneck given the large size of the classi-
fiers (20,000). However, this step can be performed in
parallel so that the overall time can be greatly reduced.
For example, our current system takes approximately
9 h to process 700,000 articles via a computer cluster
where 500 jobs can run concurrently.
Discussion and conclusions
To better understand the differences between the
computer-predicted and human-indexed results, we
conducted an error analysis based on the results of
MeSH Now on BioASQ5000 dataset. First, we found
that the predicted MeSH terms with the lowest per-
formance belong to MeSH Category E: Analytical,
Diagnostic and Therapeutic Techniques and Equip-
ment, especially the Statistics as Topic subcategory,
such as Chi-Square Distribution, Survival Analysis,
etc. This is most likely due to the lack of sufficient
positive instances in the training set (i.e. the numbers
of these indexed terms in the gold standard are rela-
tively small). On the other hand, the most incorrectly
predicted MeSH terms are Check Tags (e.g. Male,
Female, Adult, Young Adult, etc.) despite that
the F1 scores of these individual Check Tags are rea-
sonably high (most are above the average). Because of
their prevalence in the indexing results, however, im-
proving their prediction is critical for increasing the
overall performance.
As mentioned before, MeSH Now was developed in
2014 based on the learning-to-rank framework we first
proposed in 2010 [3] for automatic MeSH indexing. At
the same time, our ranking framework was adopted by
several other state-of-the-art systems such as MeSHLa-
beler [73] and DeepMeSH [74]. MeSHLabeler is very
similar to MeSH Now with the major difference in using
a machine learning model to predict the number of
MeSH terms instead of heuristics. DeepMeSH further
incorporates deep semantic representation into MeSH-
Labeler for improved performance (0.63 in the latest
BioASQ challenge in 2016).
There are some limitations and remaining chal-
lenges in this work for the automatic MeSH indexing
task. First, our previous work revealed that 85% of
the gold-standard MeSH annotations should be
present in the candidate list based on the nearest 20
neighbours. However, our current best recall is below
65%, suggesting there is still room for improving the
learning-to-rank algorithm to promote the relevant
MeSH terms higher in the ranked list. Second, our
current binary text classification results are lower
than previously reported [35], partly because for all
classifiers we simply used the same training data,
which is quite imbalanced. We believe that the per-
formance of MeSH Now could be further improved
if better text classification results are available to be
integrated. Finally, we are interested in exploring the
opportunities of using MeSH Now in practical
applications.
Table 2 Evaluation results on BioASQ5000 test set
Methods Precision Recall F1
Huang et al. 2011 [3] 0.357 0.701 0.473
Text Classification 0.689 0.400 0.506
MTIFL  2014 0.621 0.517 0.564
MTI  2014 0.587 0.559 0.573
MeSH Now 0.612 0.608 0.610
Bold data are the best value
Table 3 Processing time analysis for different steps
Key steps in MeSH Now Average time per
document (ms)
Obtaining candidate terms via k-NN 1890.82
Obtaining candidate terms via MTI 570.33
Obtaining classification results from
each binary text classifier
25.63
Learning to Ranking 103.86
Post-Processing and List Pruning 1.85
Mao and Lu Journal of Biomedical Semantics  (2017) 8:15 Page 7 of 9
Endnotes
1http://www.ncbi.nlm.nih.gov/mesh/
2http://www.bioasq.org/
3http://www.ncbi.nlm.nih.gov/books/NBK3827/
4http://sourceforge.net/p/lemur/wiki/RankLib/
5http://www.nlm.nih.gov/bsd/indexing/training/
CHK_010.html
6http://ii.nlm.nih.gov/DataSets/
Acknowledgements
We would like to thank the MTI authors and the BioASQ organizers. We also
thank Dr. Robert Leaman for his proofreading of this manuscript. This
research is supported by the NIH Intramural Research Program, National
Library of Medicine, the National Natural Science Foundation of China
(81674099, 81603498), the Six Talent Peaks Project of Jiangsu Province, China
(XYDXXJS-047), the Qing Lan Project of Jiangsu Province, China (2016), and
the Priority Academic Program Development of Jiangsu Higher Education
Institutions (PAPD).
Availability of data and materials
The datasets supporting the conclusions of this article are available in http://
www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/MeSHNow/.
Authors contributions
ZL conceived the study. YM and ZL participated in its design, analyzed the
results and wrote the manuscript. YM collected the data, implemented the
methods and performed the experiments. Both authors read and approved
the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Nanjing University of Chinese Medicine, 138 Xianlin Avenue, Nanjing,
Jiangsu 210023, China. 2National Center for Biotechnology Information
(NCBI), 8600 Rockville Pike, Bethesda, MD 20894, USA.
Received: 30 June 2016 Accepted: 16 March 2017
SOFTWARE Open Access
Large-scale adverse effects related to
treatment evidence standardization
(LAERTES): an open scalable system for
linking pharmacovigilance evidence
sources with clinical data
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Abstract
Background: Integrating multiple sources of pharmacovigilance evidence has the potential to advance the science
of safety signal detection and evaluation. In this regard, there is a need for more research on how to integrate
multiple disparate evidence sources while making the evidence computable from a knowledge representation
perspective (i.e., semantic enrichment). Existing frameworks suggest well-promising outcomes for such integration
but employ a rather limited number of sources. In particular, none have been specifically designed to support both
regulatory and clinical use cases, nor have any been designed to add new resources and use cases through an
open architecture. This paper discusses the architecture and functionality of a system called Large-scale Adverse
Effects Related to Treatment Evidence Standardization (LAERTES) that aims to address these shortcomings.
Results: LAERTES provides a standardized, open, and scalable architecture for linking evidence sources relevant to
the association of drugs with health outcomes of interest (HOIs). Standard terminologies are used to represent
different entities. For example, drugs and HOIs are represented in RxNorm and Systematized Nomenclature of
Medicine  Clinical Terms respectively. At the time of this writing, six evidence sources have been loaded into the
LAERTES evidence base and are accessible through prototype evidence exploration user interface and a set of Web
application programming interface services. This system operates within a larger software stack provided by the
Observational Health Data Sciences and Informatics clinical research framework, including the relational Common
Data Model for observational patient data created by the Observational Medical Outcomes Partnership. Elements of
the Linked Data paradigm facilitate the systematic and scalable integration of relevant evidence sources.
Conclusions: The prototype LAERTES system provides useful functionality while creating opportunities for further
research. Future work will involve improving the method for normalizing drug and HOI concepts across the
integrated sources, aggregated evidence at different levels of a hierarchy of HOI concepts, and developing more
advanced user interface for drug-HOI investigations.
Keywords: Pharmacovigilance, Post-market drug safety, Clinical terminologies, Linked-data
* Correspondence:
Suite 419, 5607 Baum Blvd, Pittsburgh, PA 15202, USA
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Knowledge Base workgroup of the Observational Health Data Sciences and
Informatics (OHDSI) collaborative Journal of Biomedical Semantics  (2017) 8:11 
DOI 10.1186/s13326-017-0115-3
Background
A recent report from the United States Department of
Health and Human Services noted that, while medications
help millions of people live longer and healthier lives, they
are also the cause of approximately 280,000 hospital admis-
sions each year and an estimated one-third of all hospital
adverse events [1]. The field of post-market drug safety sur-
veillance focuses on applying the most current methodo-
logical advances to help identify undesired effects of drugs
and biologics. One of the major opportunities and chal-
lenges to safety investigators is that there are many disparate
evidence sources from which a safety concern might either
be identified or evaluated. These may include spontaneous
reporting systems, electronic health records, the literature,
Web search logs, and social media. [27]. Safety concerns
can also be predicted from the knowledge about the chem-
ical structure and pharmacological properties of drugs [8].
Combining multiple sources of biomedical evidence has
been shown to have value for improving the precision of au-
tomated signal identification [9], and for identifying both
established [10] and new safety concerns [11].
Consistent with these results, there has been a recent
call for more research on combinatorial signal detection
that is defined as integrating multiple disparate evidence
sources while making the evidence computable from a
knowledge representation perspective (i.e., semantic en-
richment) [12]. Examples of such features include:
 The use of formal (i.e., logically defined and
computable) definitions for the meaning of entities
represented in the database such as drugs and HOIs.
 Formally defined relationships between the entities
represented in each integrated evidence source.
 Computational methods for inferring new
knowledge from evidence, for example using rule-
based or machine learning methods.
Existing frameworks that have integrated various
sources in a way that provide some of these features in-
clude ADEPedia [13, 14], MetaADEDB [15], CATTLE
[16], and Bio2RDF [17]. Fig. 1 shows the evidence sources
integrated into these systems. As the figure indicates,
there are several alternate sources that could be integrated
including VigiBase®, pharmacovigilance signals from mul-
tiple sources (or using alternative methods) [18], elec-
tronic health records signals from multiple sources (or
using alternative methods) [19], alternate approaches to
extracting safety concerns from unstructured text using
natural language processing [20], and various sources of
drug-drug interaction evidence [21].
With the exception of Bio2RDF, none of the aforemen-
tioned systems have an open architecture that would en-
able the integration at large-scale, systematically, while
facilitating the integration of new sources. Bio2rdf does
have an open architecture. The code for loading a data
source into Bio2RDF is open source enabling motivated
scientists to create a local version of Bio2RDF and then in-
tegrate a new source by writing code that translates the
sources data into a Resource Description Framework
(RDF) graph according to Bio2RDF conventions [22].
They can also edit the translation code for existing sources
to alter decisions that are made during the integration
process. This ability is important, because there are a var-
iety of decisions made on how these sources are integrated
that can influence downstream analyses. Table 1 shows
some of the decisions to be made with respect the general
Fig. 1 The information sources of existing knowledge-based systems for pharmacovigilance. Citations to the sources mentioned can be found in
the Background section. EHR: electronic health record, AE: adverse event, EU: European Union, FAERS: Food and Drug Administration Adverse
Event Reporting System, CTD: Comparative Toxicogenomics Database
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 2 of 15
Extract, Translate, and Load (ETL) process for pharma-
covigilance (i.e., not specific only to Bio2RDF).
In a previous paper [23], we presented the vision of estab-
lishing an open-source community effort to develop a global
knowledge base of known associations between drugs and
HOIs: one that brings together and standardizes all available
information for all drugs and all HOIs from all electronic
sources pertinent to drug safety. To make this vision a reality,
a workgroup within the Observational Health Data Sciences
and Informatics (OHDSI) collaborative [24] was organized
for the purpose of developing a standardized knowledge base
for the effects of medical products, and an efficient proced-
ure for maintaining and expanding it. The main purpose of
the knowledge base is to make it simpler to access, retrieve,
and synthesize evidence so that users can develop an assess-
ment of causal relationships between a given drug and HOI
as accurate as current evidence provides.
This paper discusses the results of this workgroup
thus far. Specifically, the paper discusses the architec-
ture and functionality of a prototype system called
Large-scale Adverse Effects Related to Treatment Evidence
Standardization (LAERTES). LAERTES provides open and
scalable architecture for linking evidence sources relevant
to investigating the association of drugs with HOIs. The
remaining sections of this paper will discuss the motivating
user story, the systems architecture, implementation
details, and the current beta release.
Implementation
Motivating user story and goal
Safety physicians and risk management analysts investigate
new adverse drug event reports and emerging drug safety
signals. A typical way to express the requirements imposed
on a software system by a specific user group is via the so-
called user stories. The user story which drove the devel-
opment of the LAERTES platform is defined as follows:
As a safety physician or risk management analyst
monitoring the safety of a marketed drug, I want to do a
comprehensive search across known or emerging drug-
HOI evidence so I can thoroughly and expeditiously
triage emerging potential safety signals and assess their
potential impact.
In order to do that, a number of tasks have to be car-
ried out including:
1) quickly determining if a specific adverse event has
been previously reported for a given drug;
2) gauging if a potential safety concern is at the clinical
drug (e.g. simvastatin 20 mg oral tablet), ingredient
(e.g., simvastatin), or class (e.g., statins) level;
3) assessing the credibility of the sources reporting the
association, and
4) deciding what priority an adverse event signal might
warrant for further investigation.
LAERTES system and data architecture
The LAERTES system architecture (Fig. 2) includes three
main components that operate within a larger software eco-
system provided by the OHDSI clinical research framework.
The three components are 1) a Resource Description
Framework (RDF) data store that represents all included
evidence sources as Open Annotation Data (OA) model
[25], 2) a relational data store that enables both summary
queries providing an overview of evidence across all in-
cluded sources, and drill down queries that examine import-
ant information on specific evidence items; and 3) a web
services layer that hides the details of how to query the RDF
and relational component so that client programs can more
easily benefit from their combined functionality. The next
few sub-sections discuss these components in more detail.
RDF data and the drill down use case
RDF is a standard developed through the World Wide
Web Consortium (W3C) that uses Uniform Resource
Identifiers (URIs) and a graph-based data model to repre-
sent any kind of connected data [26]. Since the introduc-
tion of RDF as a key component of the Semantic Web, the
standard has become widely used, especially in the bio-
medical sciences [27]. In comparison with the relational
data model, the underlying graph model of RDF makes
querying across heterogeneous data sets simple. The data
represented in RDF data are computable and semantically
non-ambiguous through the use of URIs and ontologies.
RDF Linked Data provides a convention to ensure that
all data items across multiple connected graphs are easily
accessible using standard web technology.
In LAERTES, a specific piece of evidence in favor or
against an association between a drug and an HOI from
any integrated data source is represented using the Open
Annotation Data (OA) model. OA is a standard for
representing human and computer annotations that is
gaining broad adoption among many publishing com-
munities. In LAERTES, every item of evidence about a
drug-HOI pair is represented as an OA resource present
in the RDF data store (Fig. 2). The reasons for this ap-
proach are to 1) use a single standard approach to repre-
senting evidence items regardless of the source and 2)
harness the aforementioned benefits of Linked Data.
Each OA resource provides the data about the source
of a specific evidence item (the target) and the seman-
tic tags used to identify the record as a relevant evidence
item (the body or bodies). The body of each OA has
resources for drug, drug group, and HOI concepts that
are represented using standard vocabularies (Medical
Subject Headings (MeSH), Medical Dictionary for Regu-
latory Activities (MedDRA), Systematized Nomenclature
of Medicine - Clinical Terms (SNOMED-CT), and
RxNorm). To enable integration with other data sources
in the OHDSI clinical research framework and facilitate
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 3 of 15
Table 1 Decisions that are made during the process of integrating sources that can influence downstream pharmacovigilance
analyses
Data Type Feature Option for variability Performance questions
Product labels Product label outcome
mention
Named entity performance
(PPV and sensitivity)
Do improvements in entity recognition
performance improve system recall
and precision?
Section location (e.g., anywhere
vs specific sections)
Does identifying which sections are more
informative than others reduce noise?
Frequency information Threshold variation Does incorporation of ADE frequency improve
performance? What cut-off should be used?
Pharmacovigilance DBs (e.g.
FAERS, MedEffect, VigiBase)
Minimum detectable relative
risk
Threshold variation What is the appropriate cut-off for MDRR?
Is it HOI specific?
Database (s) chosen Does the database influence the value of
MDRR for this task?
Risk identification method Disproportionality metric What metric (e.g. PRR, EBGM, IC) leads to
the best performance? Is it HOI specific?
Number of cases in FAERS Threshold variation What is the appropriate cut-off for number
of case reports?
Drug Indication DB Indication listings in FDB Yes/no and when mentioned Does using on-label and off-label indication
knowledge improve performance?
Indexed literature Number of relevant
publications from the indexed
literature
Threshold variation Is there an appropriate cut-off for number
of publications? What is its variability relative
to specific HOIs and drugs?
Source of relevant
publications from the indexed
literature
Varying the combination of sources Should we be selective about the sources
used or chose all of them?
Drug and outcome mention
in relevant indexed literature
Named entity performance Do improvements in entity recognition
performance improve system recall
and precision?
Main MeSH terms vs supplemental What is the value of MeSH supplemental
terms relative to the primary index terms?
Scientific discourse tag of the location
of mention (e.g., intro, methods, results,
conclusions)
Does limiting identification of drug-HOI
co-mention to specifically tagged text
excerpts improve performance?
Publication type label (randomized trial,
case report, etc.)
Should the publication type of the
drug-HOI co-mention be tracked and
possibly weighted to improve performance?
Source of publication type label
(Embase, MeSH)
Is one publication type indexing system
better than the other for the question
answering task, or should they
be combined?
Topic of the source publication based
on latent semantic indexing
Does the use of tags assigned to text
sources by latent semantic indexing
improve system performance if used
as a feature?
Observational health data
(claims + EHR)
Minimum detectable relative
risk
Threshold variation What is the appropriate cut-off for MDRR?
Is it HOI specific?
Database (s) chosen Does the database influence the value of
MDRR for this task?
Risk identification method Analytic method What method (e.g. disproportionality
analysis, self-controlled case series, IC
temporal pattern discovery,
high-dimensional propensity score)
leads to the best performance?
Is it HOI specific?
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 4 of 15
the reuse of the LAERTES platform in the context of the
OHDSI analytical tools, the source codes from the rele-
vant terminology are replaced with the equivalent con-
cept_id from the concept table which is part of the
Observational Medical Outcomes Partnership (OMOP)
Standard Vocabulary [28]. Fig. 3 shows an entity rela-
tionship diagram for the OA resources created for ad-
verse drug reactions extracted from US drug product
labeling. Graphs with the same basic structure (an an-
notation resource linked to a target and a body) are
created for other evidence sources but given a
different type and selectors that are appropriate to
the source. For example, an OA resource that repre-
sents a drug-HOI evidence from MEDLINE MeSH
tag assignment would be given the type ohdsi:Pub-
MedDrugHOIAnnotation and a selector with the
exact text of the title and abstract.
Relational data and the summary use case
As the system diagram in Fig. 2 shows, aggregated evidence
exists for LAERTES in a relational data store. Within the
data store, there are linkouts to OA resources (described
below). A web application programming interface (API) is
able to interact with both the relational data store as well as
the RDF linkouts. This interoperable representational state
transfer (REST) API can be leveraged for user interaction ei-
ther directly or via third-party applications.
The schema for the primary tables used in the relational
data store is shown in Fig. 4. The evidence_sources table
holds metadata on each data source that has been loaded
into LAERTES. The table drug_hoi_relationship is used to
hold the concept identifiers for the drug and HOI pairs used
in the drug_hoi_evidence table. Drug and HOI concepts in
this table have been converted from the source terminologies
(e.g., MeSH, MedDRA) to RxNorm and SNOMED-CT
Table 1 Decisions that are made during the process of integrating sources that can influence downstream pharmacovigilance
analyses (Continued)
Cohort selection Patient ethnicity, age, sex, co-
morbidities, concurrent medications
Does cohort selection using these
features affect model performance?
What is the appropriate size and
diversity of the cohort to reduce
noise and bias?
Drug exposure conditions Length of exposure, dosage Does selecting minimum exposure
duration criteria and/ or drug dosage
information improve performance?
Study replicability Number of locations for confirming
results
How many replicates of the study
should be performed at different
institutions?
Observation period Observation duration threshold Does setting minimum observation period
durations improve performance?
PPV: positive predictive value, OMOP: Observational Medical Outcomes Partnership, ADE: adverse drug event, MDRR: minimal detectable reporting ratio, HOI:
health outcome of interest, DB: database, FAERS: Food and Drug Administration Adverse Event Reporting System, EBGM: empirical Bayes geometric mean. IC:
information component, FDB: First Data Bank (commercial drug knowledge base), EHR: electronic health record
Fig. 2 The overall architecture of LAERTES within the OHDSI clinical research software environment. REST: representational state transfer, OHDSI:
Observational Health Data Sciences and Informatics, API: application programming interface, DBMS: database management system, CDM:
common data model, OA: Open Annotation Data, RDF: Resource Description Framework
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 5 of 15
concepts using relationships present in the Standard Vocabu-
lary provided by the OHDSI clinical research framework
(OMOP Vocabulary in Fig. 4). Natural language processing
(NLP) is applied to sources that do not use a specific termin-
ology. For example, the validated NLP tool SPLICER [29] is
used to process United States product labeling from unstruc-
tured text to RxNorm drug and MedDRA HOI mentions. A
key point is that clinical datasets represented in the OHDSI
clinical research framework will use the standardized con-
cepts from RxNorm and SNOMED making it possible to
Fig. 3 An entity relationship diagram showing how data from US product labeling is represented as a semantically enriched Open Annotation
Data graph
Fig. 4 The data architecture of LAERTES. The system leverages the OMOP Vocabularies to describe drugs and health outcomes of interest via
standardized vocabulary concepts (concept table). LAERTES stores aggregated evidence in a summary table (drug_hoi_evidence) that provides a
linkout (evidence_linkout) to an Open Annotation Data representation of the source data. In the relational database, the linkout functions as a
foreign key to the adr_annotation table through a table (not shown) that maps the linkout to annotation identifiers (adr_annotation_uid). Client
programs can also use the linkout as a URL to retrieve JSON data from an RDF store that has a linked data version of the source open
annotation data
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 6 of 15
create queries that join the merged evidence in LAERTES
with clinical data.
The drug_hoi_evidence table provides aggregate sum-
mary statistics for every drug-HOI pair from a given
source, noting, wherever possible, if the evidence sup-
ports or refutes an association. Aggregation across the
sources is possible because all drug and HOI concepts
are translated to RxNorm and SNOMED-CT, respect-
ively. The aggregation is based on a number of differ-
ent scores and coefficients. For example, a specific
drug-HOI association might have evidence from spon-
taneous reporting in the form of adverse event counts
as well as the results of disproportionality analyses
over the reporting database (i.e., proportional report-
ing rates and other signal statistics [30]). If so, the
drug_hoi_evidence table would hold a distinct record
for both statistics while indicating each records type
in the statistic_value field.
Evidence linkouts  the bridge between the summary
and drill down use cases
An important data element in the drug_hoi_evidence
table is the evidence_linkout column. This holds a URL
that functions as a foreign key to the adr_annotation
table through a table (not shown) that maps the linkout
to annotation identifiers. This enables analysts using the
relational database to examine an OA represention of
the source records used to create the summary data lo-
cated in drug_hoi_evidence. Client programs can also
use the linkout as a URL to retrieve JSON data from
the RDF store that has a linked source open annota-
tion data for the purpose of displaying this evidence.
An example will help clarify the functionality. First,
the following SPARQL script (executable on the pub-
lic OHDSI RDF store [31]) shows how to query for a
source document in the RDF store shown in the sys-
tem diagram (Fig. 2):
# The URI to the source document from
which the anonymous PubMed drug-HOI OA
# resource represented by ?s is returned
in the ?sourceDocument variable
PREFIX oa: <http://www.w3.org/ns/oa#>
PREFIX ohdsi: <http://purl.org/net/
ohdsi#>
SELECT ?sourceDocument
WHERE {
?s a ohdsi:PubMedDrugHOIAnnotation;
oa:hasTarget ?target.
?target oa:hasSource ?sourceDocument.
} LIMIT 10
This query can be used to retrieve the evidence item for the
specific drug-HOI pair Simvastatin 20 MG Oral Tablet
(identifier:1539411) and HOI muscle weakness (identifier:
36516876) from a specific source (in this case the a MEDLINE
record). The important changes are shown in bold font:
# The URI to the source document that
provides evidence for an association
between simvastatin
# and Rhabdomyolysis is returned in the
?sourceDocument variable
PREFIX oa: <http://www.w3.org/ns/oa#>
PREFIX ohdsi: <http://purl.org/net/
ohdsi#>
SELECT ?sourceDocument
WHERE {
?s a ohdsi:PubMedDrugHOIAnnotation;
oa:hasTarget ?target;
oa:hasBody ?body.
# simvastatin
?body ohdsi:ImedsDrug ohdsi: 1539403.
# Rhabdomyolysis
?body ohdsi:ImedsHoi ohdsi: 45619309.
?target oa:hasSource ?sourceDocument.
}
SPARQL queries like this one can be sent to an RDF
endpoint, in order to facilitate the reuse of the annota-
tions through produced other Linked Data applications.
Returning our focus to the relational data model (Fig. 4),
each of the entries in the evidence_linkout column holds
a URL that encodes the specific SPARQL query needed
to retrieve OA resources for a given drug, HOI, and evi-
dence source. Testing revealed that the needs and pref-
erences of various users required the ability to access the
open annotation data as either RDF or relational data (for
example, a pharmaceutical companys IT infrastructure
might be more amenable to working with relational data
rather than RDF). At the same time, other users more
familiar with the interoperability and inference strengths of
RDF Linked Data will benefit from the RDF representation.
To accommodate this dual functionality, the exact same
encoded URLs are used as foreign keys to the adr_annota-
tion table through another table that maps the linkout to
annotation identifiers. The target and adr_body tables hold
a copy of the OA target and hasBody data (Fig. 3).
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 7 of 15
Evidence rollups
One of the key features of the proposed platform is that
safety evidence may be linked to the drug at different
levels of granularity: at the clinical drug product level,
comprising the active ingredients, strength, formulation,
and brand name for the product, but may also be more
coarsely defined simply as evidence for a particular ac-
tive ingredient. For example, an adverse event might be
mentioned in the drug product label for only one clinical
drug containing a specific active ingredient. However, a
published case report might discuss an adverse event
that appears to be associated with all the drugs contain-
ing the active ingredient. LAERTES supports querying
the evidence at four different rollup levels: (1) by
RxNorm drug ingredient, (2) by RxNorm drug ingredi-
ent and SNOMED-CT HOI, (3) by RxNorm drug ingre-
dient and RxNorm clinical drug, and (4) by full detail
which was across the RxNorm drug ingredient, RxNorm
clinical drug, and SNOMED_CT HOI. These rollup
queries are supported by a table called laertes_summary
(Fig. 5). Data are aggregated from the evidence items
and inserted into this table during the evidence load
process using queries against the tables shown in Fig. 4.
Results
Technical implementation
At the time of this writing, six evidence sources have
been loaded into LAERTES representing three literature
sources, two drug product label sources, and one
spontaneous reporting source. Table 2 provides a brief
summary of each source, the methods used to normalize
drug and HOIs to RxNorm and SNOMED-CT respectively,
and the number of drug-HOI pairs that were available be-
fore and after mapping. The specific code used to perform
normalization is available from the projects GitHub site
[32]. In general, custom Python scripts execute queries that
identify OHDSI concept identifiers for the source drug and
HOI concepts, and then use OHDSI Standard Vocabulary
mappings to translate from source concepts to RxNorm
and SNOMED-CT. Table 3 provides the overlap of distinct
drug-HOI pairs at the drug ingredient level across the three
broad categories of evidence (drug product labeling, pub-
lished literature, and spontaneous reporting).
Each evidence source was processed using a custom
Extract, Translate, and Load (ETL) module developed in
Python. All ETL modules follow a similar pattern involv-
ing 1) transforming the source data to an RDF OA graph
and 2) loading the graph into an RDF endpoint, and 3)
executing a query that generates statistics (e.g. count
data) and linkouts. Each linkout is URL-encoded and
then shortened using a custom implementation of the
HarryJerry Linx URL shortener [33]. Python scripts
merge the count and linkout data from each source into
data files that are loaded into the relational database. All
of the code used to create the current implementation is
available from the open source OHDSI/KnowlegeBase
project [32]. Evidence source updates currently occur
every 3 to 6 months and follow the same workflow.
Fig. 5 The drug roll-up table and example reports by order identifier
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 8 of 15
Accessing and using data in LAERTES
Interested persons can currently access the data in
LAERTES in a few different ways. The RDF database is
hosted on a public-facing server [31] and the authors
can provide direct access to the relational database upon
request (e.g., via direct email or a request posted to for-
ums.ohdsi.org). A proof-of-concept user interface has
been developed [34] and is also hosted on a publicly
accessible server (Fig. 6) [35]. This simple user inter-
face allows users to query LAERTES using OMOP
concept identifiers or concept names for drug ingre-
dients, drug products, or HOIs. The system presents
a summary of query results in a simple tabular for-
mat. Links are provided so that users explore drill
Table 2 Distinct drug-Health Outcome of Interest pairs by source
Source description Drug and HOI mapping method Distinct drug-HOI
pairs in source
Distinct drug-HOI
pairs in LAERTES (%)
Adverse drug reactions mined from US
drug product labels using a validated
natural language processing tool called
SPLICER [29]
Drugs were coded using RxNorm and
HOIs using MedDRA. The OMOP
Standard Vocabulary was used to map
MedDRA to SNOMED-CT.
272 436a 254 738 (93%)
Adverse drug events extracted from EU
Summary of Product Characteristics by
the PROTECT project
Drugs were mentioned by name and
HOIs using MedDRA codes. Drug
names were mapped to RxNorm using
a combination of simple string matching
and Bioportal ontology searches. Many
combination products and some
individual drugs were not mappable. All
mappings were manually reviewed for
accuracy.
26 989 24 537 (91%)
FDA Adverse Event Reporting System
counts and Proportional Reporting
Ratio from [45]
The OHDSI Usagi tool [46] was used to
map drug and HOI mentions to RxNorm
and MedDRA. The OMOP Standard
Vocabulary was used to map MedDRA
coded HOIs to SNOMED-CT. A paper
describing the database and mapping
method has been published [47].
3 766 382 2 753 078 (73%)
Abstracts from titles and abstracts
indexed in MEDLINE that describe
drug-HOI evidence according to
MeSH indexing [48]
Drug and HOI concepts were both coded
using MeSH. The OMOP Standard
Vocabulary was used to map from MeSH
drug concepts to RxNorm and MeSH HOI
concepts to SNOMED-CT.
79 119b 77 395 (97.8%)
Sentence spans from titles and
abstracts indexed in MEDLINE
that describe drug-HOI evidence
according to queries against the
Semantic Medline database
Drug and HOI concepts were both coded
using UMLS concept identifiers. The UMLS
Metathesaurus MRCONSO table was used
to map concepts to RxNorm, MeSH,
MedDRA, and SNOMED-CT. The OMOP
standard vocabulary was then used to map
drug concepts only available as MeSH to
RxNorm and HOI concepts only available as
MedDRA or MeSH concepts to SNOMED-CT.
5 023b 2 813 (56%)
Chemical disease associations
from the Comparative
Toxicogenomics Database
Drug and HOI concepts were both coded
using MeSH. The OMOP Standard Vocabulary
was used to map from MeSH drug concepts
to RxNorm and MeSH HOI concepts to
SNOMED-CT.
503 835 432 850 (86%)
aSPLICER drug-hoi pairs are at the clinical drug level. All other sources are at the ingredient level. bDoes not include drug-HOI evidence where the source refers to
the drug by its MeSH pharmacologic group name.
EU: European Union, FDA: Food and Drug Administration, HOI: Health outcome of Interest, OMOP: Observational Medical Outcomes Partnership, US: United States,
MedDRA: Medical Dictionary for Regulatory Activities, MeSH: Medical Subject Headings
Table 3 Overlap of distinct drug-HOI pairs at the drug ingredient level after mapping drugs to RxNorm and HOIs to SNOMED-CT
Literature (MEDLINE and CTD)
vs spontaneous reporting
(n = 3 049 743)
Product labeling (US and EU)
vs spontaneous reporting
(n = 2 702 577)
Literature (MEDLINE and CTD)
vs product labeling (US and EU)
(n = 566 379)
All three
(n = 3 057 406)
119 293 (3.9%) 87 279 (3.2%) 14 838 (2.6%) 14 295 (0.5%)
The counts and percentages shown contrast the sum of the union (shown in the heading) and intersection of the distinct drug-HOI pairs from both
sources mentioned.
CTD: Comparative Toxicogenomics Database, EU: European Union, US: United States
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 9 of 15
down information. This system uses some of the
several REST API calls that are documented on the
OHDSI Wiki [36].
Progress has been made on integrating the cross-
platform web services provided by the current version
of LAERTES with other applications in the OHDSI
clinical research framework. Specifically, the vocabu-
lary browsing component of the OHDSI ATLAS web
based tool [37] can use the LAERTES API to retrieve
the available evidence of drug-HOI associations that
it displays to users. Furthermore, a new extension to
ATLAS is under development that will enable search-
ing for drugs-HOI pairs with no evidence in any in-
cluded source. The outputs of this program are called
negative controls and can be used for investigating
drug-HOI associations using observational data to
calibrate the confidence intervals of statistical esti-
mates to address hidden biases within the observa-
tional dataset [38].
How LAERTES can support the user scenario
The current version of LAERTES is a prototype that can
support some of the requirements of the safety physician
and risk management analyst whose user story is men-
tioned at the beginning of this paper:
1) Quickly determining if a specific adverse event has
been previously been reported for a given drug:
LAERTES currently brings together three main
types of information where drug-HOI associations
are reported (spontaneous reports, labeling, and
published literature). The systems open architecture
makes it possible to add additional sources such as
data from clinical trials. Because drugs and HOIs are
normalized from the source terminologies to RxNorm
and SNOMED-CT, a single query accomplishes the
task of identifying existing evidence from any of the
included sources. This is possible via SPARQL and
SQL queries as well as through the Web API.
2) Identifying if a potential safety concern is at the
clinical drug, ingredient or class level: LAERTES
allows searches specifically at the clinical drug or
ingredient levels while also providing evidence
rollups which aggregate evidence at the ingredient
level (see Section Evidence Rollups). However, there
currently is only one source integrated into LAERTES
that provides evidence at the clinical drug level (US
drug product labeling). This is not likely to be a
limitation since, in the OHDSI clinical research
framework, all clinical drug data is loaded into the
CDM drug_exposure table (not shown) and then also
represented at the ingredient level in the CDM
drug_era table (also, not shown).
3) Identifying the credibility of the sources reporting
the association: Both the relational and RDF
components of LAERTES explicitly note the source
of an evidence item and, if relevant, the particular
type of evidence. For example, an evidence item
from a literature source would be explicitly tagged
with the method used to identify the evidence
Fig. 6 Experimental user interface to the LAERTES evidence base
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 10 of 15
(MeSH tags or natural language processing) and the
study type of the article from which the item was
found (clinical trial, case report, or other). Similarly,
drug-HOI evidence from product labeling is tagged
with the specific method used to identify it and the
section from which it was pulled. These tags are
intended to be useful for filtering or prioritizing evi-
dence based on a users perception of the relative
credibility of the sources or evidence type. However,
further research is necessary to test this assumption
and identify the requirements for other ways to help
users more rapidly assess evidence credibility.
4) Deciding what priority an adverse event signal might
warrant for further investigation: At present,
LAERTES provides only an experimental graphical
user interface (Fig. 6) but the workgroup is actively
working on a new user interface that will fully
support prioritizing an adverse event signal for
further investigation. The new user interface is being
designed to help users take full advantage of the new
possibilities created by bringing together the multiple
sources of drug-HOI evidence into the OHDSI clinical
research framework. As Listing 1 shows, LAERTES is
designed to work seamlessly with patient data that has
been loaded into the OMOP CDM. As a result, users
would be able to directly generate new drug-HOI
evidence from one or more clinical datasets (claims,
electronic health records, or registries) using OHDSI
population-level effect estimation methods [39]. These
methods, which are in development, promise rapid
large-scale exploration of a suspected drug-HOI
association using causal considerations which include
strength of association, consistency, temporality,
experiment, plausibility, coherence, biologic gradient,
specificity, and analogy [40].
Listing 1
An example of querying patient data on the OHDSI
CDM using drug HOI pairs present in the LAERTES
evidence base. The query counts the number of cases
present in the clinical dataset where a patient condi-
tion is recorded within 30 days of the start of a drug
(as indicated by data in the CDM drug_era table).
The results shown are a subset of the results that
were generated when the query was ran on a simu-
lated population available to the OHDSI research
community and the general public [41]. The results
provide summary information and Web links that
point to a summary of each source evidence item in
the LAERTES RDF store. These links could be used
by a third-party application to help the user further
drill down into the evidence that associated the
drug with the HOI.
 retrieve the count of distinct patients
exposed drug-HOI combination for which
there is
 evidence in LAERTES from MEDLINE or
European product labeling using RxNorm
drug identifier,
 SNOMED-CT drug identifier, evidence
type, evidence linkout, and
select rxnorm_drug, snomed_hoi,
evidence_type, evidence_linkout,
count(distinct person_id) pcount
from
(select sub1.person_id,
drug_hoi_relationship.rxnorm_drug,
sub1.drug_era_start_date,
sub1.drug_era_end_date,
drug_hoi_relationship.hoi,
drug_hoi_relationship.snomed_hoi,
sub1.condition_era_start_date,
drug_hoi_evidence.evidence_type,
drug_hoi_evidence.evidence_linkout
from
drug_hoi_evidence inner join
drug_hoi_relationship
on drug_hoi_evidence.drug_hoi_
relationship = drug_hoi_relationship.id
inner join
(select drug_era.person_id,
drug_era_start_date,
drug_era_end_date,
drug_concept_id,
condition_era_start_date,
condition_concept_id
from drug_era
inner join condition_era on drug_era.
person_id = condition_era.person_id
where condition_era.condition_era_start_
date > drug_era.drug_era_start_date
and condition_era.condition_era_
start_date - drug_era.drug_era_start_
date <= 30
) sub1
on sub1.drug_concept_id = drug_hoi_
relationship.drug and sub1.condition_
concept_id = drug_hoi_relationship.hoi
where evidence_type in
('MEDLINE_MeSH_CR','MEDLINE_MeSH_
ClinTrial','MEDLINE_SemMedDB_CR','
MEDLINE_SemMedDB_ClinTrial','
SPL_EU_SPC')
) sub2
group by rxnorm_drug, snomed_hoi,
evidence_type, evidence_linkout
order by pcount desc;
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 11 of 15
 RESULTS
rxnorm_drug | snomed_hoi | evidence_type
| evidence_linkout | pcount
Acetaminophen| Edema| MEDLINE_SemMedDB_
CR| https://goo.gl/ikKucQ | 14
Lisinopril| Abdominal pain| MEDLINE_
MeSH_CR| https://goo.gl/49EvSE | 14
Albuterol| Atrial fibrillation| MEDLINE_
MeSH_CR| https://goo.gl/IVPdzx | 13
Chlorthalidone| Hyperlipidemia|
MEDLINE_MeSH_ClinTrial| https://goo.gl/
gMTfzk | 13
Metformin| Edema| MEDLINE_MeSH_
ClinTrial| https://goo.gl/kL3GIz | 13
Enalapril| Anemia| MEDLINE_MeSH_CR|
https://goo.gl/AB1Lue |13
Captopril| Anemia| MEDLINE_MeSH_CR|
https://goo.gl/Cfhkzt | 13
The system satisfies non-functional requirements
The new system is entirely open source so that any
interested researcher can download, run, modify,
and extend the code to fit their purposes. For ex-
ample, the system currently does not have a data
source that provides adverse event data from social
media sources. Such new evidence sources could be
integrated in the platform by developing additional
Python ETL modules [42]. The system also provides
systematic evidence to facilitate OHDSIs methodo-
logical research efforts to enable the design, devel-
opment and evaluation of new analytical approaches
to observational research, and provides the basis for
estimating systematic error and performing empir-
ical calibration in all population-level estimation
routines [43].
Discussion
Drugs on the market need to be monitored for public
safety. A safety physician or risk management analyst
has to review all the available information for drug safety
issues, following what is currently a highly manual,
time-intensive, and error-prone process. Improvement of
the automation of bringing all the relevant evidence to-
gether in a consumable format will help such individuals
better achieve their goals. LAERTES provides an open
source framework that uses OHDSI technology to bring
together evidence from multiple sources in a way that
will enable the development of software to meet the user
goals mentioned at the beginning of this paper.
While the current version of LAERTES provides useful
functionality, it also leaves opportunities for further re-
search. In order to integrate the evidence sources, drug
and HOI concepts have to be converted into RxNorm
and SNOMED-CT concepts, respectively. Table 2 shows
that there are many cases where this conversion is in-
complete and some parts of the source data are not inte-
grated. Future work will examine ways to improve the
translation and mapping process.
Another opportunity for research is on how to appro-
priately aggregate evidence at different levels of a hier-
archy of HOI concepts. For example, we have observed
that some evidence sources map the concept myocardial
infarction (concept identifier 4329847) directly to the
SNOMED equivalent, whereas others map directly to a
more specific concept like acute myocardial infarction
(concept identifier 312327). The OMOP Vocabularies
can be used to address this issue using the hierarchy
provided. However, unlike drugs, it is not always clear
the appropriate level to rollup HOI concepts. Future
work will examine this issue in more detail and explore
the use of alternate definitions of concept similarity [44].
Limitations
Evidence sources that do not use standardized termin-
ologies have to be processed to map the source concept
names to SNOMED-CT and RxNorm concept codes.
Even for evidence sources that use controlled terminolo-
gies, there is often a conversion process required to inte-
grate them with all of the included LAERTES sources.
One limitation is that, because of the prototype nature
of the current version of LAERTES, we currently do not
have precise precision/recall metrics for each of the
methods we used (Table 2).
Another limitation is that some terminologies are
incompatible and result in imperfect mappings. In
cases where mapping is incomplete, some parts of the
source data will not be integrated. We mentioned
above the example of the MEDLINE source using
MeSH to code drugs at the ingredient level, while
drugs in US product labels are coded at the clinical
drug level. These cases can be addressed with the
drug rollup queries described above. Table 3 repre-
sents overlap between the general categories of
sources at the drug ingredient level using the drug
concept rollup strategy. However, unlike drugs, the
appropriate level to rollup HOI concepts is not al-
ways clear. Some examples seem straightforward such
as the myocardial infarction example mentioned
above which can be addressed using ancestor/descen-
dant relationships in the OMOP vocabulary. However,
it is less clear how to apply the strategy to the uni-
verse of HOI concepts because there exists many
different hierarchies depending on the disease and the
level of detail present in SNOMED. We did not
attempt to address this issue in the LAERTES
prototype.
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 12 of 15
Conclusion
Post-marketing drug safety surveillance is an import-
ant, continuous, and demanding process. Accurate
and timely identification and verification of safety sig-
nals remains a major challenge. DrugHOI evidence
exists in many sources, which are disjointed, and vari-
able in their representation of drugs and HOIs. The
current practice of reviewing drug-HOI evidence is a
highly manual time intensive process that is wrought
with opportunity for failure. Improvement of the
automation of bringing this information together in a
consumable format will greatly improve the pharma-
covigilance field. LAERTES provides a framework that
accepts data from multiple sources and leverages the
OMOP Vocabulary to translate those sources to one
terminology for drugs and one for conditions, while
also enabling integration with clinical data stored in
the OMOP CDM. In addition, LAERTES is open-
source facilitating domain experts participation in its
development. As the breadth of evidence available on
drug-HOI associationsis too wide for any individual
to be expert, an open-source model allows for niche
domain experts to contribute their knowledge improv-
ing the usefulness of LAERTES for the entire drug
safety community.
This paper started with a motivating safety physician
and risk management analyst user story to help guide
LAERTES use cases. LAERTES has already collated sev-
eral of the evidence sources individuals in this role
would traditionally use for investigating drug-HOI sig-
nals. The LAERTES workgroup believes that the frame-
work is in place to address the motivating example but
realizes there is more work to be done. The workgroup
fully expects and welcomes feedback from the commu-
nity; for example, on new data sources, improvements to
the Web API, and user interface design.
Abbreviations
ADE: Adverse drug event; AE: Adverse event; API: Application programming
interface; CDM: Common data model; CTD: Comparative Toxicogenomics
Database; DB: Database; DBMS: Database management system;
EBGM: Empirical Bayes Geometric Mean; EHR: Electronic health record;
ETL: Extract, translate, and load; EU: European Union; FAERS: Food and Drug
Administration Adverse Event Reporting System; FDB: First data bank (R);
HOIs: Health outcomes of interest; IC: Information component;
LAERTES: Large-scale adverse effects related to treatment evidence
standardization; MDRR: Minimal detectable reporting ratio; MedDRA: Medical
dictionary for regulatory activities; MeSH: Medical subject headings;
NLP: Natural language processing; OA: Open annotation data model;
OHDSI: Observational Health Data Sciences and Informatics;
OMOP: Observational Medical Outcomes Partnership; PPV: Positive predictive
value; RDF: Resource Description Framework; REST: Representational state
transfer; SNOMED-CT: Systematized Nomenclature of Medicine - Clinical
Terms; URIs: Uniform Resource Identifiers; US: United States; W3C: World
Wide Web Consortium
Acknowledgements
The following collaborating authors from the Knowledge Base workgroup of
the Observational Health Data Sciences and Informatics (OHDSI) collaborative
participated in creating LAERTES and authoring this manuscript:
Richard D. Boyce, University of Pittsburgh, Pittsburgh, PA, rdb20@pitt.edu
Erica A. Voss, Janssen Research & Development, LLC, Titusville, NJ,
EVoss3@its.jnj.com
Vojtech Huser, National Institutes of Health, Bethesda, MD,
vojtech.huser@nih.gov
Lee Evans, LTS Computing LLC, West Chester, PA,
levans@ltscomputingllc.com
Christian Reich, QuintilesIMS, Burlington MA, reich@ohdsi.org
Jon D. Duke, Georgia Tech Research Institute, Atlanta, GA,
Jon.Duke@gatech.edu
Nicholas P. Tatonetti, Herbert Irving Assistant Professor of Biomedical
Informatics, Columbia University, New York, NY, nick.tatonetti@columbia.edu
Tal Lorberbaum, Columbia University, New York, NY,
tal.lorberbaum@columbia.edu
Michel Dumontier, Stanford University, Stanford, CA,
michel.dumontier@gmail.com
Manfred Hauben, MD. MPH, Pfizer Inc, New York University Medical Center,
New York, NY, manfred.hauben@pfizer.com
Magnus Wallberg, Uppsala Monitoring Centre, Uppsala, Sweden,
Magnus.Wallberg@who-umc.org
Lili Peng, AstraZeneca R&D, Boston, MA, Lili.Peng@astrazeneca.com
Sara Dempster, AstraZeneca R&D, Boston, MA,
sara.dempster@astrazeneca.com
Yongqun He, University of Michigan Medical School, Ann Arbor, Michigan,
yongqunh@med.umich.edu
Anthony G. Sena, Janssen Research & Development, LLC, Titusville, NJ,
asena5@its.jnj.com
Vassilis Koutkias, Institute of Applied Biosciences, Center for Research &
Technology Hellas, Thermi, Thessaloniki, Greece, vkoutkias@certh.gr
Pantelis Natsiavas, Institute of Applied Biosciences, Center for Research &
Technology Hellas, Thermi, Thessaloniki, Greece, pnatsiavas@certh.gr
Patrick B. Ryan, Janssen Research & Development, LLC, Titusville, NJ,
ryan@ohdsi.org
Funding
This research was funded in part by the US National Institute on Aging
(K01AG044433), and the National Library of Medicine (R01LM011838).
Availability of data and materials
The datasets used and/or analyzed during the current study available from
the corresponding author on reasonable request. Links are provided within
this published article to web services and an experimental user interface
providing data from the LAERTES evidence base.
Authors contributions
This was a collaborative project. Please see the list of collaborating authors in
the Acknowledgements sections. RDB was the lead developer of LAERTES
and the main author on the manuscript. Other major contributors to writing
the manuscript and the design and implementation of LAERTES were EAV,
VH, LE, CR, JDD, NT, MD, MW, AGS, and PR. MH made significant
contributions to clarifying medication safety investigation use cases for
LAERTES. LP and SD contributed to data quality assurance and drug
development use cases. YH provided input on the representation of health
outcomes of interest. All authors read and approved the final manuscript.
Authors information
Not applicable.
Competing interests
The authors have no competing interests to report. The opinions expressed
are those of the authors and do not necessarily represent those of their
employers.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Received: 23 November 2016 Accepted: 13 January 2017
The Knowledge Base workgroup of the Observational Health Data Sciences and Informatics (OHDSI) collaborative
Journal of Biomedical Semantics  (2017) 8:11 
Page 13 of 15
Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 
DOI 10.1186/s13326-017-0155-8
RESEARCH Open Access
Querying clinical data in HL7 RIM based
relational model with morph-RDB
Freddy Priyatna1* , Raul Alonso-Calvo2, Sergio Paraiso-Medina2 and Oscar Corcho1
Abstract
Background: Semantic interoperability is essential when carrying out post-genomic clinical trials where several
institutions collaborate, since researchers and developers need to have an integrated view and access to
heterogeneous data sources. One possible approach to accommodate this need is to use RDB2RDF systems that
provide RDF datasets as the unified view. These RDF datasets may be materialized and stored in a triple store, or
transformed into RDF in real time, as virtual RDF data sources. Our previous efforts involved materialized RDF datasets,
hence losing data freshness.
Results: In this paper we present a solution that uses an ontology based on the HL7 v3 Reference Information Model
and a set of R2RML mappings that relate this ontology to an underlying relational database implementation, and
where morph-RDB is used to expose a virtual, non-materialized SPARQL endpoint over the data.
Conclusions: By applying a set of optimization techniques on the SPARQL-to-SQL query translation algorithm, we
can now issue SPARQL queries to the underlying relational data with generally acceptable performance.
Keywords: Clinical data, R2RML, SPARQL
Introduction
In the last years, clinical trials have started introducing
genomic variables [1]. This requires performing patient
stratification when selecting the patient population to
apply the clinical trials to. It involves the use of biomarkers
to create subsets within a patient population that pro-
vide more detailed information about how the patient will
respond to a given drug. Several datasets, commonly pro-
duced by different institutions and hence rather heteroge-
neous in general, need to be used for patient stratification
[2]. Interoperability among those datasets is made easier
by the use of biomedical standards and terminologies [3].
However, achieving such interoperability poses relevant
technological challenges [4]. In this work, we focus on a
semantic interoperability approach to homogenize differ-
ent data models into one Common Data Model (CDM).
For this task several projects such as HL7 Reference Infor-
mation Model (RIM) [5], i2b2 [6], OMOP [7] or CaGRID
*Correspondence: fpriyatna@fi.upm.es
1Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid,
Spain
Full list of author information is available at the end of the article
[8] have defined their own CDM capable of storing het-
erogeneous data coming from different sources. The basis
of the work presented in this paper is founded on the
semantic interoperability layer developed in the EURECA
project [9], which has been deployed and tested in several
healthcare institutions, such as the Institut Jules Bordet
[10], the MAASTRO Clinic [11], and the German Breast
Group [12].
In previous works [13] we already presented a HL7
RIM [5] relational database implementation used as a
CDM in the EURECA semantic interoperability layer. This
database aims to facilitate the interconnection with other
data sources wheremedical ontologies are also being used,
and has already been used for providing some form of
interoperability among real data sources [13] from the
aforementioned institutions. We are currently developing
ontology-based support to data access to facilitate such
integration and allow incorporating other datasets more
easily. This is the reason why we were looking into using a
Relational Database to RDF (RDB2RDF) solution. We also
provide a SPARQL endpoint to a virtual view so that users
are relieved from knowing the underlying schema of the
implemented database.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 2 of 12
RDB2RDF mappings are used to expose data from rela-
tional databases as RDF datasets. Two major types of data
access mechanisms are normally provided by RDB2RDF
tools: i) data translation (a specific case of ETL - Extract,
Transform, Load -), where data are materialized into RDF
datasets and stored in a triple store (e.g., Virtuoso), which
provides a SPARQL endpoint; and ii) query translation,
where SPARQL queries are directly translated into SQL
according to the specified RDB2RDF mappings, and eval-
uated against the relational database, and where results
are translated back using the mappings to conform with
the SPARQL query. In our case, we are interested in using
RDB2RDF mappings to make the data stored in our SQL
implementation available according to an ontology that
reflects the HL7 version 3 RIM. Furthermore, we have a
strong requirement to use a query translation approach,
given the importance of having fresh results, which cannot
always be ensured in the data translation approach.
Our first attempt [14] at applying RDB2RDF-based
query translation was with D2R server and mappings [15].
This approach was not applicable since the evaluation of
the SQL queries resulting from query translation was not
efficient enough. Moreover, in some cases, queries could
not be executed by the database management system (e.g.,
their length was excessive). This was already mentioned
in [16] which describes the experience of using RDB2RDF
tools in the domain of astronomy. The conclusion there
was that RDB2RDF tools were not feasible to be used in
such a context, and this conclusion was consistent with
our first attempt.
Later, we started using morph-RDB [17] with R2RML
mappings [18] for this purpose. We have obtained better
results that make this approach applicable in our context.
In this paper we describe our experience, which shows
that it is possible to use efficient RDB2RDF tools in the
medical domain.
This paper is organized as follows. In the
Background section we discuss our current model
for storing medical data, the HL7 RIM ontology, the
R2RML mapping language, and our query translation
engine morph-RDB. In the Methods section we dis-
cuss our methodology for mapping legacy data into
the HL7 RIM ontology, selection of SPARQL queries
for that ontology, and some optimization techniques
that have been implemented in morph-RDB. In the
Results and discussion section, we present our evalu-
ation. Finally in the Conclusions section, we provide
some conclusions and describe some of our future work
in this area, including our deployment plans in the
aforementioned healthcare institutions.
Background
In this section we will review the main foundations
of the work that we present in the paper, namely
HL7 and the HL7 RIM, the R2RML language, and
morph-RDB.
HL7 RIM
Recent years have witnessed a huge increase of biomedical
databases [19]. This increased availability opens up new
opportunities, while setting some new important chal-
lenges, especially with respects to their integration, which
is crucial to obtain a proportional increment of knowledge
in the biomedical area. In this context, it is common to
establish a CDM for the representation of biomedical data
which allow exploiting multiple established terminologies
to build a core concept dataset as the common medical
vocabulary of the platform
Among the many Detailed Clinical Models that have
been reviewed for the integration of biomedical datasets
[20], the HL7 v3 is one of the most relevant, since main
requirement for the CDM is that any data coming from
clinical institutions can be represented without loss of
information. The HL7 RIM offers a wide coverage for
representing clinical data and has proven useful for clin-
ical information exchange. The HL7 v3 standard defines
the RIM at its core. This definition consists of a UML
class diagram (it does not define a data structure or a
database model). Besides, issues such as the management
of data types are not trivially translatable into a database
model. As a consequence, we previously defined a rela-
tional model for it, which can be seen in Fig. 1 and
described in [13].
The HL7 RIM backbone contains three main classes:
Act, Role and Entity, which are linked together
by three association classes (Act-Relationship,
Participation and RoleLink). The core of the HL7
RIM is the Act class. An Act is defined as a record of
an event that has happened or may happen. Any health-
care situation and all information concerning it should
be describable using the RIM by including the type of
act (what happens), the actor who performs the deed
and the objects or subjects Entity that the act affects
to Role. Some additional information may be provided
to indicate location (where), time (when), manner (how),
together with reasons (why) or motives (what for). Act
and Entity classes have some specializations that add
some attributes, such as Observation (a subclass of
Act), or Person (a subclass of Entity).
This standard is able to represent almost any healthcare
situations and a wide variety of information associated
with it [21]. Based on this idea, we have defined a subset
of the HL7 RIM schema where we implement the classes
and attributes that are necessary to represent the scenario
for sharing clinical breast cancer clinical trials data:
 Act, with the subclasses Observation,
Procedure, SubstanceAdministration, and
Exposure.
Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 3 of 12
Fig. 1 Relational model of H7RIM. Our database schema implementing the HL7RIM model [13]
 Role.
 Entity, with the sub-classes LivingSubject,
Person, and Device.
 The classes;
i) ActProcedureApproachSiteCode, ii)
ActMethodCode,
iii) ActTargetSiteCode, iv)
ActObservationInterpretationCode, and
v) ActObservationValues related to Act.
Attribute data types are rather complex on the
RIM, so they are changed according to the men-
tioned scenario, following HL7 datatype specifications
[22]. Therefore some attributes were simplified in the
relational model compared to those defined by HL7
v3 standard. To improve performance and understand-
ing of the HL7 RIM schema, it is defined a set of
views. These views cover the access retrieval require-
ments for the clinical scenario. We defined a view
for each clinical contexts (Observation, Procedure,
SubstanceAdministration, and Exposure).
Therefore, the defined HL7 RIM-based CDM above ful-
fills the requirements needed for breast cancer clinical
trials scenario. Furthermore, we have created an ontology
that reflects the HL7RIM model [23], which is available
for others to reuse.
Figure 2 depicts a simplified schema of the implemented
database following the HL7 v3 RIM definition. However,
typically relationships among Entity and Role instances
are one-to-one. Moreover, the Act table is the backbone
but data is classified as one of its descendants (Obser-
vation, Procedure, Substance Administration, Exposure,
etc.). Thus the logical schema for querying an Act descen-
dant (i.e. Observation) from our database looks like the
schema represented in Fig. 3.
Therefore, every Act subclass in the HL7 v3 RIM data
schema can be represented as a star diagram  typi-
cally used in data warehouse definition. Our database can
be visualized as a snowflake diagram similar to the i2b2
star model [6]. Each event record will be a subclass of
Act (similarly to the i2b2 fact table). Entities and Roles
(patient, location, care provider, etc.) are lookup tables
called Dimensions.
Conversely to other works in literature that use
query translation [8], since Act tables contain the
biggest amount of data in the model, we have adopted
the approach of dividing complex queries into atomic
queries. Consequently, in order to efficiently execute
queries involving several instances of acts and rela-
tionships (e.g. temporal dependencies), these queries
are divided and results are later combined using set
operators [13].
Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 4 of 12
Fig. 2 Simplified HL7RIM model. Our simplified logical database schema implementing the HL7RIM model
R2RML
R2RML [18] is a W3C recommendation for the defini-
tion of a mapping language from relational databases to
RDF. An R2RML mapping document consists of a set
of Triples Maps rr:TriplesMap, used to specify the
rules to generate RDF triples from database rows/values.
A TriplesMap consists of:
 A logical table rr:LogicalTable that is either a
base table or SQL view, used to provide the rows to
be mapped as RDF triples.
 A subject map rr:SubjectMap that is used to
specify the rules to generate the subject component
of RDF triples.
 A set of predicate object maps
rr:PredicateObjectMap that is composed by a
set of predicate maps rr:PredicateMap and
object maps rr:ObjectMap (to generate the
predicate and object components of RDF triples,
respectively). If a join with another triples map is
needed, a reference object map rr:RefObjectMap
can be used. The other triples map to be joined is
specified in rr:parentTriplesMap and the join
condition is specified via rr:Join
Figure 4 illustrates an overview of an R2RML
TriplesMap class.
Subject maps, predicate maps, and object maps are term
maps, which are used to specify rules to generate the cor-
responding RDF triples element, and those rules can be
specified as a constant rr:constant, a database col-
umn rr:column, or a template rr:template. Figure 5
illustrates an overview of an R2RML TermMap class.
morph-RDB
morph-RDB is part of the morph suite [24]. It receives as
an input the connection details to a relational database,
an R2RML mapping document and a SPARQL query. It
translates the SPARQL query into the underlying rela-
tional database and translates the results back into a
format appropriate for the SPARQL query. The query
translator component in morph-RDB implements the
algorithm described in [17], which extends previous work
in [25] that defined a set of mappings and functions
in order to translate SPARQL queries posed against
RDB-backed triples stores into SQL queries, prove the
correctness of the query translation using the notion
semantic-preserving. In other words, the SPARQL query
realized as an SQL query returns the same answers
as the same SPARQL query executed over an R2RML
materialization. We extend their work by relating those
mappings and functions with the R2RML mapping
elements.
Fig. 3 Logical view of HL7RIM model. Logical view of observation data in the HL7RIM model
Priyatna et al. Journal of Biomedical Semantics  (2017) 8:49 Page 5 of 12
Fig. 4 R2RML TriplesMap overview. An overview of R2RML TriplesMap, taken from [18]
For an in-depth explanation of the query rewriting
REVIEW Open Access
Semantic annotation in biomedicine: the
current landscape
Jelena Jovanovi?1 and Ebrahim Bagheri2*
Abstract
The abundance and unstructured nature of biomedical texts, be it clinical or research content, impose significant
challenges for the effective and efficient use of information and knowledge stored in such texts. Annotation of
biomedical documents with machine intelligible semantics facilitates advanced, semantics-based text management,
curation, indexing, and search. This paper focuses on annotation of biomedical entity mentions with concepts from
relevant biomedical knowledge bases such as UMLS. As a result, the meaning of those mentions is unambiguously and
explicitly defined, and thus made readily available for automated processing. This process is widely known as semantic
annotation, and the tools that perform it are known as semantic annotators.
Over the last dozen years, the biomedical research community has invested significant efforts in the development of
biomedical semantic annotation technology. Aiming to establish grounds for further developments in this area,
we review a selected set of state of the art biomedical semantic annotators, focusing particularly on general purpose
annotators, that is, semantic annotation tools that can be customized to work with texts from any area of biomedicine.
We also examine potential directions for further improvements of todays annotators which could make them even
more capable of meeting the needs of real-world applications. To motivate and encourage further developments in
this area, along the suggested and/or related directions, we review existing and potential practical applications and
benefits of semantic annotators.
Keywords: Natural language processing (NLP), Biomedical ontologies, Semantic technologies, Biomedical text mining,
Semantic annotation
Background
Over the last few decades, huge volume of digital un-
structured textual content have been generated in bio-
medical research and practice, including a range of
content types such as scientific papers, medical reports,
and physician notes. This has resulted in massive and
continuously growing collections of textual content that
need to be organized, curated and managed in order to
be effectively used for both clinical and research pur-
poses. Clearly, manual curation and management of
such big corpora are infeasible, and hence, the biome-
dical community has long been examining and making
use of various kinds of Natural Language Processing
(NLP) methods and techniques to, at least partially, fa-
cilitate their use.
In this paper, we focus on a specific NLP task, namely
the extraction and disambiguation of entities mentioned
in biomedical textual content. Early efforts in biomedical
information extraction were devoted to Named Entity
Recognition (NER), the task of recognizing specific types
of biomedical entities mentioned in text [1]. For in-
stance, in the sentence The patient was diagnosed with
upper respiratory tract infection, a NER tool would
recognize that the phrase respiratory tract infection
denotes a disease, but would not be able to determine
what particular disease it is. Semantic annotation, the
NLP task of interest to this paper, makes a significant
advance, by not only recognizing the type of an entity,
but also uniquely linking it to its appropriate cor-
responding entry in a well-established knowledge base.
In the given example, a semantic annotator would not
only recognize that the phrase respiratory tract infec-
tion represents a disease, but would also identify what
disease it is by connecting the phrase with the concept
* Correspondence: bagheri@ryerson.ca
2Department of Electrical Engineering, Ryerson University, 245 Church Street,
Toronto, Canada
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jovanovi? and Bagheri Journal of Biomedical Semantics  (2017) 8:44 
DOI 10.1186/s13326-017-0153-x
C0035243 denoting Respiratory Tract Infections from
the UMLS Metathesaurus (see Table 1). This way, the se-
mantics of biomedical texts is made accessible to soft-
ware programs so that they can facilitate various
laborious and time consuming tasks such as search, clas-
sification, or organization of biomedical content.
While a suite of biomedical semantic annotation tools
is available for practical use, the biomedical community
is yet to heavily engage in and leverage the benefits of
such tools. The goal of this paper is to introduce (i)
some of the benefits and application use cases of bio-
medical semantic annotation technology, (ii) a selection
Table 1 An overview of ontologies, thesauri and knowledge bases used by biomedical semantic annotation tools discussed in the paper
BioPortal (http://bioportal.bioontology.org/) A major repository of biomedical ontologies, currently hosting over 500
ontologies, controlled vocabularies and terminologies. Its Resource Index
provides an ontology-based unified index of and access to multiple
heterogeneous biomedical resources (annotated with BioPortal ontologies).
DBpedia (http://wiki.dbpedia.org/) Wikipedia for machines, that is, a huge KB developed through a community
effort of extracting information from Wikipedia and representing it in a
structured format suitable for automated machine processing. It is the central
hub of the Linked Open Data Cloud.
LLD - Linked Life Data (https://datahub.io/dataset/linked-life-data/) LLD platform provides access to a huge KB that includes and semantically
interlinks knowledge about genes, proteins, molecular interactions, pathways,
drugs, diseases, clinical trials and other related types of biomedical entities. It
is part of the Linked Open Data Cloud (http://lod-cloud.net/)
NCBI Biosystems Database (https://www.ncbi.nlm.nih.gov/biosystems) Repository providing integrated access to structured data and knowledge
about biological systems and their components: genes, proteins, and small
molecules.
The NCBI Taxonomy contains the names and phylogenetic lineages of all the
organisms that have molecular data in the NCBI databases.
OBO - Open Biomedical Ontologies (http://www.obofoundry.org/) Community of ontology developers devoted to the development of a family
of interoperable and scientifically accurate biomedical ontologies. Well known
OBO ontologies include:
 Chemical Entities of Biological Interest (ChEBI) - focused on molecular entities,
molecular parts, atoms, subatomic particles, and biochemical roles and
applications
 Gene Ontology (GO) - aims to standardize the representation of gene and
gene product attributes; consists of 3 distinct sub-ontologies: Molecular
Function, Biological Process, and Cellular Component
 Protein Ontology (PRO) - provides a structural representation of protein-
related entities
SNOMED CT (http://www.ihtsdo.org/snomed-ct) SNOMED CT is considered the worlds most comprehensive and precise,
multilingual health terminology. It is used for the electronic exchange of
clinical health information. It consists of concepts, concept descriptions
(i.e., several terms that are used to refer to the concept), and concept
relationships.
UMLS (Unified Medical Language System) Metathesaurus (https://
www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/)
The most well-known and widely used knowledge source in the biomedical
domain. It assigns a unique identifier (CUI) to each medical concept and
connects concepts to each other thus forming a graph-like structure; each
concept (i.e. CUI) is associated with its semantic type, a broad category such
as Gene, Disease or Syndrome; each concept is also associated with several
terms used to refer to that concept in biomedical texts; these terms are pulled
from nearly 200 biomedical vocabularies. Some well-known vocabularies that
have been used by biomedical semantic annotators include:
 Human Phenotype Ontology (HPO) contains terms that describe phenotypic
abnormalities encountered in human disease, and is used for large-scale
computational analysis of the human phenome.
 Logical Observation Identifiers Names and Codes (LOINC) provides standardized
vocabulary for laboratory and other clinical observations, and is used for
exchange and/or integration of clinical results from several disparate sources.
 Medical Subject Headings (MeSH) is a controlled vocabulary thesaurus created
and maintained by U.S. National Library of Medicine (NLM), and has been
primarily used for indexing articles in PubMed
 RxNorm provides normalized names for clinical drugs and links between many
of the drug vocabularies commonly used in pharmacy management and drug
interaction software.
UniProtKb/Swiss-Prot (http://www.uniprot.org/uniprot/) Part of UniProtKB, a comprehensive protein sequence KB, which contains
manually annotated entries. The entries are curated by biologists, regularly
updated and cross-linked to numerous external databases, with the ultimate
objective of providing all known relevant information about a particular protein.
Jovanovi? and Bagheri Journal of Biomedical Semantics  (2017) 8:44 Page 2 of 18
of the publicly available general purpose semantic an-
notation tools for the biomedical domain, i.e., semantic
annotators that are not specialized for a particular bio-
medical entity type, but can detect and normalize en-
tities of multiple types in one pass, and (iii) potential
areas where the work in the biomedical semantic anno-
tation domain can be strengthened or expanded. While
the overview of application cases and state of the art
tools can be of relevance to practitioners in the biome-
dical domain, with the summary of potential areas for
further research, we are also targeting researchers who
are familiar with NLP, semantic technologies, and se-
mantic annotation in general, but have not been dealing
with the biomedical domain, as well as those who are
well aware of biomedical semantic technologies, but
have not been working on semantic annotation. By pro-
viding researchers with an insight into the current state
of the art in biomedical semantic annotation in terms of
the approaches and tools, as well as the research chal-
lenges, we aim to offer them a basis for engagement with
semantic annotation technology within the biomedical
domain and thus support even further developments in
the field.
The following section provides several examples of
practical benefits achievable through semantic anno-
tation of biomedical texts (see also Table 2). The paper
then examines the available tool support, focusing
primarily on general purpose biomedical annotators
(Tables 3 and 4). Still, considering the relevance and
large presence of entity-specific biomedical annotators,
i.e., tools developed specifically for semantic annotation
of a particular type of biomedical entities such as genes
or chemicals, we provide an overview of these tools, as
well. While examining the available tool support, we also
consider biomedical knowledge resources required for
semantic annotation (Table 1), as well as resources used
for evaluating the tools performance (Table 5). This is
followed by a discussion of the challenges that are pre-
venting current semantic annotators from achieving
their full potential.
Benefits and use cases
Better use of electronic medical record (EMR) in clinical
practice
Electronic medical records (EMRs) are considered valu-
able source of clinical information, ensuring effective
and reliable information exchange among physicians and
departments participating in patient care, and support-
ing clinical decision making. However, EMRs largely
consist of unstructured, free-form textual content that
require manual curation and analysis performed by do-
main experts. A recent study examining the allocation of
physician time in ambulatory practice [2] confirmed the
findings of previous similar studies (e.g. [3]), namely that
physicians spend almost twice as much time on the
management of EMRs and related desk work than on
direct clinical face time with patients. Considering the
inefficiency of manual curation of EMRs, automation of
the process is required if the potentials of EMRs are to
be exploited in clinical practice [4].
Semantic annotators provide the grounds for the re-
quired automation by extracting clinical terms from
free-form text of EMRs, and disambiguating the ex-
tracted terms with concepts of a structured vocabulary,
such as UMLS Metathesaurus. The identified concepts
can be subsequently used to search a repository of bio-
medical literature or evidence-based clinical resources,
Table 2 Example application cases of biomedical semantic annotation tools
Application Case (AC) The role of semantic annotation tool in the AC Biomedical resources relevant for the AC
(or representative examples, if multiple)
Semantic search of biomedical
tools and services [6]
Sematic search of biomedical tools and services enabled
by semantic annotation of users (free-form) queries with
concepts from UMLS Metathesaurus
Catalogs of and social spaces created around
biomedical tools and services, e.g.:
- myExperiment (http://www.myexperiment.org/)
- BioCatalogue (https://www.biocatalogue.org/)
Semantic search of domain
specific scientific literature [74]
Semantic annotation of PubMed entries with ontological
concepts related to genes and proteins
Ontologies used for the annotation of biomedical
Gonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 
DOI 10.1186/s13326-017-0133-1
SOFTWARE Open Access
An ontology-driven tool for structured
data acquisition using Web forms
Rafael S. Gonçalves* , Samson W. Tu, Csongor I. Nyulas, Michael J. Tierney and Mark A. Musen
Abstract
Background: Structured data acquisition is a common task that is widely performed in biomedicine. However,
current solutions for this task are far from providing a means to structure data in such a way that it can be
automatically employed in decision making (e.g., in our example application domain of clinical functional assessment,
for determining eligibility for disability benefits) based on conclusions derived from acquired data (e.g., assessment of
impaired motor function). To use data in these settings, we need it structured in a way that can be exploited by
automated reasoning systems, for instance, in the Web Ontology Language (OWL); the de facto ontology language for
the Web.
Results: We tackle the problem of generating Web-based assessment forms from OWL ontologies, and aggregating
input gathered through these forms as an ontology of semantically-enriched form data that can be queried using an
RDF query language, such as SPARQL. We developed an ontology-based structured data acquisition system, which we
present through its specific application to the clinical functional assessment domain. We found that data gathered
through our system is highly amenable to automatic analysis using queries.
Conclusions: We demonstrated how ontologies can be used to help structuring Web-based forms and to
semantically enrich the data elements of the acquired structured data. The ontologies associated with the enriched
data elements enable automated inferences and provide a rich vocabulary for performing queries.
Keywords: OWL, Ontology, Structured data, Data acquisition, Form generation
Background
Ontology-based form generation and structured data
acquisition was first pioneered almost 30 years ago. In
the early 1990s, Protégé-Frames used definitions of classes
in an ontology to generate knowledge-acquisition forms,
which could be used to acquire instances of ontology
classes [1, 2]. The rise of the Web Ontology Language
(OWL) [3, 4], standardized by the World Wide Web Con-
sortium (W3C) in 2004, caused a paradigm shift in knowl-
edge representation from frame-based to axiom-based.
Because of its axiom-based nature, it is more difficult to
acquire instance data based on OWL than it was based on
frames.WithOWL as the preferredmodeling language for
ontologies, class definitions are collections of description
logic (DL) axioms, and can no longer be seen as templates
*Correspondence: rafael.goncalves@stanford.edu
Stanford Center for Biomedical Informatics Research, Stanford University,
Stanford, CA, USA
for forms [5]. Unlike template-based knowledge represen-
tations, where what can be said about a class is defined
by the slots of the class template, axiom-based representa-
tions do not have this kind of locally scoped specification,
and allow any axiom describing the same class to be added
to the ontology, as long as the axiom does not lead to
inconsistencies. Template-based knowledge representa-
tion systems use closed-world reasoning and have local
constraints (e.g., cardinality of a slot for a particular class)
that can be validated easily, while in an axiom-based sys-
temwith the open-world assumption such local constraint
checking is much more problematic. Furthermore, in our
chosen application domain, assessment instruments have
specific formats that do not lend themselves to be seen as
representing instances of domain ontology classes. Items
in the instruments have potentially complex descriptions
of information to be collected, such as the severity of
pain with a particular quality, and at a specific anatomical
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Gonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 2 of 14
location. The challenge is to model the assessment instru-
ments and relate the assessed data to a domain ontology
with which one can formulate meaningful queries.
In this paper, we describe a system that we developed
for representing, acquiring, and querying assessment data
that uses: (1) an information model of assessment instru-
ments to drive the generation of data-acquisition Web
forms, (2) domain ontologies and standard terminolo-
gies to give formal descriptions of entities in our chosen
domain, and (3) a data model for the acquired informa-
tion that links the data to the domain ontologies and
standard terminologies. Such linkage makes it possible to
query and aggregate the data using the logical representa-
tion of the domain concepts in the ontologies. The choice
of Web forms as a method for acquiring data is due to
their widespread use and simplicity for data acquisition.
The form generation software we present here works with
forms modeled in OWL so long as these replicate our
design pattern for the form specification ontology. The
paper describes requirements on the underlying ontolo-
gies and informationmodels, and the steps for configuring
the software to generate forms and to acquire data using
clinical functional assessment as an exemplar.
Related work
In addition to the comparison with the Protégé-Frames
template-based instance acquisition method (in the
Background Section), we briefly contrast our work with
other systems that use ontologies in the construction of
forms for acquiring structured data.
Girardi et al. [6] describe an ontology-based data-
acquisition and data-analysis system where the structure
of the data depends on the ontology classes, in such a
way that the GUI structures (tables, headers, filter dialogs,
etc.) can be created at runtime based on the ontology
information. This system, like the earlier Protégé-Frames
system, assumes that classes in the ontology provide data-
acquisition templates that directly define user-interface
features. Our system is designed to work with OWL
ontologies where ontology axioms do not provide the
structural templates required in the system described by
Girardi et al. Instead, the structure of the data-acquisition
instrument has to be defined separately.
ObTiMA, described by Stenzhorn et al. [7], is another
ontology-based data-acquisition system. It is a clinical
trial-management application featuring a Trial Builder
module that a clinical researcher can use to build case
report forms (CRFs). Items in a CRF are constructed
by selecting concepts from a master ontology. A Patient
Data Management System provides a graphical user inter-
face that allows clinicians to fill in the CRFs relevant to
the patients current treatment situation. The design of
ObTiMA is very similar to the system we are proposing.
The main differences, aside from ObTiMAs specific focus
on clinical trial management, include (1) our use of OWL
tomodel not only the domain concepts, but also the struc-
tures of forms and data model, and (2) ObTiMAs use of
a tree view to represent concepts that can be selected to
define data items. It is understandable that, from the per-
spective of supporting a clinical researchers use of the
master ontology to construct CRFs, a tree view provides a
necessary simplification of the master ontology, although
it nevertheless constrains what can be expressed. It is
difficult to see how some of the complex concepts (e.g.,
constant pain caused by radiculopathy in the lower left
extremity) modeled in our work can be represented as
part of a tree structure.
The clinical documentation system developed by Hor-
ridge et al. [8] uses a template schema to allow a
technology-savvy clinician to create documentation tem-
plates that include the local structure of subforms, and
potentially complex clinical descriptions consisting of fea-
tures and their values. The features and values are mapped
to a medical ontology, and the system automatically gen-
erates ontological descriptions of the data elements based
on the mappings. Constrained by our goal to replicate
existing forms, we took the opposite approach where we
start with ontological descriptions of the data elements,
specify how they are used in assessment instruments as
part of the description of instruments, and generate forms
for the acquisition of data. Having the freedom to design
their documentation system, Horridge et al. avoided
the laborious work of manually modeling the domain
concepts.
Bona et al. developed a work that is similar to ours
[9]. They modeled the specifications of forms, question
groups, questions, and answers as extensions of the Infor-
mation Artifact Ontology (IAO),1 and the answers as the
result of the patient-history taking process. In their work,
the questions are just strings that have associated accept-
able answers, whereas we attempt to formalize much of
the information content in our assessment instruments in
terms of a domain ontology. Furthermore, it is not clear
that the system automatically generates data-acquisition
forms from the ontology-based form specifications.
Outside the domain of biomedicine, semantic wiki is
a generic Web-based technology from which one can
draw examples on how to arrive at a domain-independent
solution. Semantic wikis extend regular wikis with seman-
tic technologies, wherein each wiki article is an RDF
resource, and an instance of some resource such as a class
defined in the schema,2 which can be asserted to have
relations with other RDF resources. These relations are
defined by the authors of wiki articles, which could be a
challenging task to perform without previous knowledge
of the domain or the modeling. In a survey of semantic
wikis featuring OWL reasoning and SPARQL3 querying
facilities [10], a user evaluation of a chosen semantic
Gonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 3 of 14
wiki implementationIkeWiki [11]concluded that
authoring instance data in such a way is cumbersome,
even with users that are familiar with ontologies. A good
solution to this would be exploiting the relations defined
in the schema to provide wiki article templates whose
form input fields derive from those relations, thus making
it easier to create semantic wiki articles  essentially
the user would only have to fill in the values of those
relations, without having to understand the underlying
representation.
Another system that is very close to what we present
here is K-Forms [12]. This tool allows users to construct
forms using a graphical user interface, and then the result-
ing form structure is seamlessly encoded as an OWL
ontology. However, unlike our work, K-Forms does not
have a mechanism to associate form data (whether ques-
tions or answers) to user-specified domain ontologies,
meaning that the queryability will be constrained to the
semantics provided by the system, rather than the more
flexible approach that we aim for.
Implementation
In this sectionwe describe the software, informationmod-
els, and ontologies that we developed for OWL-based data
acquisition.
The architecture of the form generation and data acqui-
sition system we implemented is depicted in Fig. 1. The
tool takes as inputs an XML configuration file that speci-
fies the form layout, and a form-specification OWL ontol-
ogy that defines the content of the form (i.e., the actual
questions, answer options, etc.). The tool then generates
a form, and outputs answers to form questions in CSV,
RDF and OWL formats. We implemented our tool in Java,
using the OWL API v4.0.1 [13],4 and its source code is
publicly available on GitHub.5 A Web server is necessary
to deploy the application, so the project ships with an
embedded instance of Jetty.6 The requirements to run the
application are Java (v1.7 or above) and Apache Ant.7
To try out the software, we supply executable scripts
for Windows and UNIX-based operating systems, which
build and deploy the tool on the included JettyWeb server.
These scripts are hosted in our GitHub repository. First,
a user would clone the repository to their computer, and
then from a command line execute the appropriate script
for the operating system; use run-generator.sh on UNIX-
based systems and run-generator.bat onWindows. Alter-
natively, one can build the form-generator using Apache
Ant, and then deploy it onto the provided instance of Jetty.
The application will then be available to browse on the
designated localhost port. In addition to the tool itself, we
provide in the same GitHub repository 3 example form
configurations, the ontologies that we developed, some of
the data that we gathered via our tool, example SPARQL
queries over that data, and finally the results of executing
those queries on our data. Users can open the output data
and query it with the example SPARQL queries we supply,
using, for example, the Protégé ontology editor [2].
The two major stages in the application workflow are:
form generation and form input handling, as described
below.
(1) Form generation  Steps to produce a form:
(a) Process XML configuration file, gathering
form layout information, IRIs and bindings
to ontology entities
Fig. 1 Architecture of the system. The form-generation and data-acquisition software takes an XML configuration file and a form specification as
inputs. A form specification uses terms from the datamodel ontology to create question instances and to specify possible answers. It annotates
questions and answers with concepts from domain ontologies
Gonçalves et al. Journal of Biomedical Semantics  (2017) 8:26 Page 4 of 14
(b) Extract from the input form specification
ontology all relevant information
pertaining to each form element:
(b.1) Text to be displayed (e.g., section
header, question text)
(b.2) Options and their corresponding
text, where applicable
(b.3) The focus of each question
(c) Generate the appropriate HTML and
JavaScript code
(2) Form input handling  Once the form is filled in
and submitted:
(a) Process answer data and create appropriate
individuals
(b) Produce a partonomy of the individuals
created in (2.a) that mirrors the layout
structure given in the configuration
(c) Return the (structured) answers to the user
in a chosen format
An application can combine the data with the OWL
ontologies to make description logic queries that inter-
pret the data in terms of the semantics defined in the
ontologies.
In order to use our tool, a user will have to model ques-
tions and their descriptions in OWL, and then specify the
layout and content of the resulting form in an XML file.
In the following subsections, we will describe the OWL
modeling and configuration components in detail.
Modeling
Our goal was to develop a set of light-weight ontologies
and models with minimal ontological commitments, and
postponing alignment with possible upper-level ontolo-
gies to the future. Existing ontologies, such as the Infor-
mation Artifact Ontology, do not provide a modeling
of forms and questions that we could reuse. Further-
more, what we need is an information model that states,
for example, that the structure of a question on a form
includes a specific text string, not an ontology that char-
acterizes parts of information artifacts in terms of logical
descriptions (e.g., modeling the text of a question as an
instance of textual entity" class).
The modeling component of our software consists of
(1) a datamodel that specifies the structure of data-
acquisition forms and of the resultant data, (2) form
specifications that define specific data-acquisition forms
in terms of the datamodel structures and concepts and
relations in the domain ontologies, and (3) one or more
domain ontologies that define the concepts and relations
in an application domain. The domain ontologies that
we developed are hosted and maintained in our GitHub
repository.8
Datamodel
The datamodel, represented as an OWL ontology, is a
generic, context-free description of the information struc-
tures of a form. It models form elements such as sections
and questions, and the data elements generated from
a form (e.g., a string value from a text area, or val-
ues from an enumerated value set). Figure 2 summarizes
key aspects of our modeling: elements of a form are
asserted as subclasses of FormStructure, such as Form,
Section and Question. Each kind of FormStructure gen-
erates some kind of Data; every form submission gen-
RESEARCH Open Access
Optimization on machine learning based
approaches for sentiment analysis on HPV
vaccines related tweets
Jingcheng Du1, Jun Xu1, Hsingyi Song1, Xiangyu Liu2 and Cui Tao1*
Abstract
Background: Analysing public opinions on HPV vaccines on social media using machine learning based
approaches will help us understand the reasons behind the low vaccine coverage and come up with
corresponding strategies to improve vaccine uptake.
Objective: To propose a machine learning system that is able to extract comprehensive public sentiment on HPV
vaccines on Twitter with satisfying performance.
Method: We collected and manually annotated 6,000 HPV vaccines related tweets as a gold standard. SVM model
was chosen and a hierarchical classification method was proposed and evaluated. Additional feature sets evaluation
and model parameters optimization was done to maximize the machine learning model performance.
Results: A hierarchical classification scheme that contains 10 categories was built to access public opinions toward
HPV vaccines comprehensively. A 6,000 annotated tweets gold corpus with Kappa annotation agreement at 0.851
was created and made public available. The hierarchical classification model with optimized feature sets and model
parameters has increased the micro-averaging and macro-averaging F score from 0.6732 and 0.3967 to 0.7442 and
0.5883 respectively, compared with baseline model.
Conclusions: Our work provides a systematical way to improve the machine learning model performance on the
highly unbalanced HPV vaccines related tweets corpus. Our system can be further applied on a large tweets corpus
to extract large-scale public opinion towards HPV vaccines.
Keywords: Twitter, Social media, Sentiment analysis, Support vector machines, Hierarchical classification, Gold
standard
Background
Human papillomavirus (HPV) is thought to be respon-
sible for more than 90% of anal and cervical cancers,
70% of vaginal and vulvar cancers, and more than 60%
of penile cancers [1]. FDA approved HPV vaccines (Gar-
dasil, Cervarix and Gardasil 9) for the protection from
most of the cancers caused by HPV infections. However,
the HPV vaccines coverage in USA is still quite low es-
pecially for the adolescents. Only 39.7% of girls and
21.6% of boys have received all three required doses [2].
Analysis of public opinions over the HPV vaccines could
reveal the reasons behind the low coverage rate and can
help us provide new directions on improving future
HPV vaccines uptake and adherence.
As one of the most popular social media in the world,
Twitter attracts millions of users to share opinions on
various topics every day. On average, around 6,000
tweets are tweeted every second and 500 million tweets
are tweeted per day [3]. Besides, Twitter allows a limit of
140 characters on one post to its users. This restriction
pushes the users to be very concise to share their opin-
ions [4]. The huge number of concise tweets makes
Twitter a precious and rich data source to analyze public
opinions [5].
Due to the adaptability and accuracy, machine learning
based approach is one of the most prominent techniques
* Correspondence: cui.tao@uth.tmc.edu
1The University of Texas School of Biomedical Informatics, 7000 Fannin St
Suite 600, Houston, TX 77030, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Du et al. Journal of Biomedical Semantics  (2017) 8:9 
DOI 10.1186/s13326-017-0120-6
gaining interest in sentiment analysis (SA) on microblog-
ging posts [4]. However, few efforts have been done on
Twitter to explore public opinions towards vaccines
using machine learning based SA tools. Surian et al. ap-
plied unsupervised topic modeling to group semantically
similar topics and communities from HPV vaccines re-
lated tweets [6]. However, those topics are not closely re-
lated to sentiments towards vaccination. Salathé et al.
leveraged several supervised algorithms to mine public
sentiments toward the new vaccines [7]. Zhou and Dunn
et al. utilized connection information on social network
to improve opinion mining on identifying negative senti-
ment about HPV vaccines [8, 9]. However, those work
only covered limited coarse sentiment classifications
(positive, negative, neutral, etc.). In the HPV vaccination
domain, sentiment analysis at a more granular level is
necessary in addition to the current limited classifica-
tions. To serve as a feedback to public health profes-
sionals to examine and adjust their HPV vaccines
promotion strategies, the system not only needs to know
whether people have negative opinions towards HPV
vaccines but also should be able to extract the reasons
behind the negative opinions.
Thus, to access public opinions towards HPV vaccines
on Twitter in a more comprehensive way, a finer classifi-
cation scheme to HPV vaccination sentiment is needed.
In this paper, we introduced our efforts on using ma-
chine learning algorithms to access HPV vaccination
sentiment at a more granular level on Twitter. We built
a hierarchical classification scheme including 10 categor-
ies. To train the machine learning model, we manually
annotated 6,000 tweets as the gold standard according
to the classification scheme. We chose Support Vector
Machines (SVM) as the algorithm due to the perform-
ance in our pre-experiments. Due to the challenges of
machine learning approaches on the highly unbalanced
tweets corpus, we further did a series of optimization
steps to maximize the system performance. Standard
metrics including precision, recall, and F measure were
calculated to evaluate our results.
Methods
Data source and annotation
Data collection
English tweets containing HPV vaccines related key-
words were collected from July 15, 2015 to August 17,
2015. We used combinations of keywords (HPV, human
papillomavirus, Gardasil, and Cervarix) to collect public
tweets using the official Twitter application program-
ming interface (API) [10]. During the study period, we
have collected 33,228 tweets in total. After removing the
URLs and duplicate tweets, we randomly selected 6,000
tweets for annotation.
Annotation schema design
As were more interested in the concerns over HPV vac-
cination, we did a literature review to find out the com-
mon non-vaccination reasons of HPV vaccines [1114].
The most common barriers found for vaccination are
the worries about side effects, efficacy, cost, and culture-
related issues. We also went through a sample of tweets
and kept track of the major concerns on Twitter. Based
on our findings, a hierarchical classification scheme was
then built for the classifications of different HPV vaccin-
ation sentiments, see Fig. 1. Detailed definitions of each
category were provided in Table 1.
Gold standard annotation
We annotated each tweet based on its content. Three
annotators (part time) were employed in this annotation
process. Two of them have a public health background
and the other has health informatics background. The
annotators annotate the tweets according to the classifi-
cation scheme. The annotator first decides whether the
tweet is related to HPV vaccines or not. If it is related,
the annotator further decides if it is positive, negative, or
neutral. If it is negative, the annotator assigns one of the
categories under Negative to the tweet.
All tweets have been annotated by at least two annota-
tors in the first round. The third annotator was involved
when the two annotators have different annotations and
made the final decision in the second round. The first
round took up to one month. The second round took up
to two weeks. We applied the brat rapid annotation tool
for this process [15]. After the annotation, the Kappa
value was calculated from the annotators to evaluate the
quality [16].
The example tweets annotated in our gold standard
can be seen in the Additional file 1: Table S1A.
Fig. 1 Sentiment classification scheme for HPV vaccines related
tweets: The categories in colored rectangles (other than black) are
all possible sentiment labels that can be assigned to the tweets
Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 2 of 7
Machine learning system optimization
Our system is a modularized machine learning system
that consists different pre-processors and feature extrac-
tors. A detailed overview of the system can be seen in
Fig. 2a.
Tweets Pre-processing
 Text Normalizer. All upper-case letters were con-
verted to lower case ones. All hashtags and Twitter
user names (e.g. @twitter) were excluded. All URLs
were exchanged with string url (e.g. http://exam-
ple.com to url). We also replaced any letter occur-
ring more than two times in a row with two
occurrences (e.g. convert huungry, huuuungry to
huungry), proposed by Go A et al. [17].
 POS Tagger. We used TweeboParser [18, 19]
developed by Carnegie Mellon University to extract
POS tags for tweets. TweeboParser is trained on a
subset of new labeled corpus for 929 tweets (12,318
tokens) [19]. It provided a fast and robust Java-based
tokenizer and POS tagger for tweets.
Features extraction
Considering the characteristics of HPV vaccine related
tweets, we extracted the following features:
 Word n-grams. Contiguous 1 and 2 g of words are
extracted from a given tweet.
 Clusters. Previous work found that word cluster can
be used to improve the performance of supervised
NLP models [20]. We mapped tweets tokens to
TwitterWord Clusters developed by ARK group of
Carnegie Mellon University (the group is currently
in University of Washington). This largest clustering
mapped 847,372,038 tokens from approximately 56
million tweets into 1000 clusters. (e.g. tehy", thry,
theey, they et al. belong to a same cluster)
 POS tags. Part of speech tags were extracted by
TweeboParser as one of the features.
Machine learning algorithm
In our pre-experiment, we leveraged the basic n-grams
feature and applied Weka [21] to test and compare differ-
ent machine learning algorithms: Naïve Bayes, Random
Forest and Support Vector Machines (SVMs). As SVMs
outperformed the other two algorithms and it has known
performance on pervious sentiment analysis tasks [22], we
leveraged SVMs as the algorithms. SVMs are supervised
learning models with associated learning algorithms that
analyze data used for classification and regression analysis.
We implemented LibSVM package as the library for our
task. Default RBF kernel was used.
Table 1 Detailed definition of different sentiment categories for
HPV vaccines related tweets
Sentiment Description
Positive Show positive opinion or prompt the uptake of
HPV vaccine
Negative Safety Concerns or doubt on the safety issues of HPV
vaccine or present vaccine injuries
Efficacy Concerns or doubt on the effectiveness of HPV
vaccine
Cost Concerns on the cost of HPV vaccine (e.g.: money
or time)
Resistant Resistance to HPV vaccines due to cultural or
emotional issues
Others Other concerns
Neutral Related to HPV vaccine topic but contains no
sentiment or sentiment is unclear or contains both
negative and positive sentiment
Unrelated Not related to HPV vaccine topic
Fig. 2 Overview of the machine learning based system and optimization approach: (a) modularized machine learning system framework; (b)
machine learning optimization steps
Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 3 of 7
Machine learning system optimization
 Baseline model. To create a baseline sentiment analysis
model, we applied plain classification, used word-ngrams
as the feature and chose default SVMs parameters.
 Hierarchical classification VS plain classification.
Traditional multi-labels classification methods that
treat each category equally do not take into account
the hierarchical information. The highly imbalanced
structure of our gold standard could have a dramatic
effect on the system performance [18]. In order to
alleviate the effect of the imbalanced structure, we
tested the hierarchical classification and compared
the performance with the plain one. Three SVMs
models were trained independently. The first SVM
model categorized the tweets into Related and
Unrelated groups; the second one then categorized
the Related tweets into Positive, Negative and
Neutral groups; the third model further categorized
the Negative tweets into the five finest categories.
 Feature combinations. We tested the different
combinations of word n-grams, clusters and POS
tags features and evaluated their impact on the sys-
tem performance.
 Parameters optimization. For SVMs model with RBF
kernel, there are two major parameters needed to be
chosen beforehand for a given problem: C is the
cost of misclassification; ? is the parameter of the
kernel function [19]. The C parameter trades off
misclassification of training examples against
simplicity of the decision surface, while the ? defines
how far the influence of a single training example
reaches, with low values meaning far and high
values meaning close [23].
An overview of the optimization steps can be seen in
Fig. 2b.
Evaluation
To evaluate the performance of the machine learning algo-
rithms, we used 10-fold cross-validation. Standard metrics
were applied and the average score were calculated (includ-
ing precision, recall and F measure for each category and
Micro F measure and Macro F measure for overall per-
formance). For micro-averaged score, we summed up all
the individual true positives, false positives, and false nega-
tives of the system. For macro-averaged score, we took the
average of the F score of different classes.
Results
Annotation results
The Kappa value among the annotators was 0.851, which
indicated the high quality of this gold standard. Among
the human annotated corpus, 3,984 (66.4%) tweets were
related to HPV vaccine sentiments. Among the related
tweets, 1,445 (36.3%) of them showed negative opinions,
which is larger than both positive (1,153, 28.9%) and
neutral tweets (1,386, 34.8%). The major concern in gold
standard is safety issues (63.1% in Negative group). De-
tailed results can be seen in Fig. 3. The download link
for annotation results can be found in section Availabil-
ity of data and material.
Machine learning system optimization results
Baseline model performance
Choosing word-ngrams as the feature and default SVMs
parameters (C = 256 and ? = 2e-5), we applied the trad-
itional plain classification to create the baseline model.
Hierarchical VS Plain
The performance comparison between baseline model
(plain classification) and hierarchical classification can
be seen in Table 2. The hierarchical classification
method outperformed the plain method in each cat-
egory. For the micro-averaging and macro averaging F
score, hierarchical way significantly increased the per-
formance to 0.7208 and 0.4841 from 0.6732 and 0.3967
respectively. Specifically, for the category NegOthers
and NegEfficacy, the hierarchical method increased
0.3095 and 0.2593 on F score respectively.
Results for the evaluation on feature sets
Since the hierarchical method outperformed the plain
method significantly, we chose this way as default in our
following optimization steps. Default SVMs parameters
(C = 256 and ? = 2e-5) were used in this step. The 10-
Fig. 3 Sentiment distribution in 6,000 tweets gold standard.
(Neg: Negative)
Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 4 of 7
fold evaluation results for different feature sets combina-
tions can be seen in Table 3.
The highest micro-averaging and macro-averaging F
score were 0.73 and 0.4986, achieved by using the com-
bination of n-grams, POS, and word clusters features.
Adding POS and cluster feature set can both lead to
nearly 0.5% increase in micro-averaging F -score com-
pared with using word n-grams feature only (POS: from
0.7208 to 0.7263; Cluster: from 0.7208 to 0.7255). Adding
POS feature only achieved the highest performance for
Unrelated category, whereas adding cluster feature out-
performed on Neutral category. Except for Unrelated
and Neutral category, Adding POS and cluster feature
sets together achieved the highest performance.
Results for the Evaluation on Parameters Optimization
As adding POS and cluster feature sets together
achieved the best performance. The ideal way to find the
best parameters C and ? should be grid search method.
However, as we chose the hierarchical classification
methods, we need to train three SVMs models inde-
pendently. The grid search method will be much
computation-costly. To reduce the computation burden,
we decided to optimize the parameters in two steps: 1)
use the default C and grid search best ? combinations
for three SVMs models; 2) use the ? combinations that
achieved the best performance in step 1 and grid search
best C combinations for three SVMs models.
The default C and ? are 256 and 2e-5 respectively. For
the step one, we fix C to 256 for all the three models
and gave ? a range of {2e-7, 2e-6, 2e-5, 2e-4, 2e-3} for
the grid search. Since we have three models, we totally
tested 125 models in this step. The best ? combination
is: 2e-5 for the first SVMs model, 2e-4 for the second
one and 2e-4 for the third one. For the step two, we
chose the found ? combination in the step one and gave
C a range of {64, 128, 256, 512, 1024} for the grid search.
Due to the three models we have, 125 models were
tested in this step. The best C combination found is: 512
for the first SVMs model, 128 for the second one and
512 for the third one. The performance comparison be-
tween the best performing models after parameter
optimization and the model using default parameters
can be seen in Table 4. We can observe that by doing
Table 2 10-fold cross validation performance on the baseline
model and hierarchical classification model. (F: F-1 score; P: pre-
cision; R: recall; for the categories that do not indicate the
metric, F-1 score are used)
Classification
Model
Plain Classification (Baseline
model)
Hierarchical
Classification
Micro-
averaging
F 0.6732 0.7208
Macro-
averaging
P 0.4455 0.5402
R 0.3574 0.4386
F 0.3967 0.4841
Unrelated 0.8044 0.8599
Neutral 0.5792 0.6181
Positive 0.6528 0.7021
NegSafety 0.7006 0.7277
NegEfficacy 0 0.2593
NegCost 0 0
NegResistant 0 0
NegOthers 0.155 0.4645
Table 3 10-fold cross validation performance on different
feature sets combinations. (Feature sets: (a) Word n-grams; (b)
POS tags; (c) Clusters; F: F-1 score; P: precision; R: recall; for the
categories that do not indicate the metric, F-1 score are used)
Feature sets (a) (a) + (b) (a) + (c) (a) + (b) + (c)
Micro-averaging F 0.7208 0.7263 0.7255 0.73
Macro-averaging P 0.5402 0.5438 0.5396 0.5477
R 0.4386 0.4468 0.4442 0.4576
F 0.4841 0.4905 0.4872 0.4986
Unrelated 0.8599 0.864 0.859 0.8618
Neutral 0.6181 0.6226 0.625 0.6231
Positive 0.7021 0.7098 0.7123 0.7136
NegSafety 0.7277 0.734 0.7357 0.7542
NegEfficacy 0.2593 0.3214 0.2593 0.3793
NegCost 0 0 0 0
NegResistant 0 0 0 0
NegOthers 0.4645 0.4614 0.4724 0.4753
Table 4 10-fold cross validation performance among the best
performing model after C and ? optimization and the model
using default C and ?. (F: F-1 score; P: precision; R: recall; for the
categories that do not indicate the metric, F-1 score are used)
Model Model using
default C and ?
Best model using
optimized ? only
Best model using
optimized C and ?
Micro-
averaging
F 0.73 0.7352 0.7442
Macro-
averaging
P 0.5477 0.6889 0.6873
R 0.4576 0.5095 0.5142
F 0.4986 0.5858 0.5883
Unrelated 0.8044 0.8538 0.8633
Neutral 0.5792 0.6330 0.6470
Positive 0.6528 0.7239 0.7255
NegSafety 0.7006 0.7641 0.7617
NegEfficacy 0 0.4138 0.4068
NegCost 0 0.5 0.5
NegResistant 0 0 0
NegOthers 0.155 0.5144 0.5403
Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 5 of 7
parameters optimization, our machine learning model
has increased 1.442% and 8.97% on micro-averaging and
macro-averaging F score respectively. The optimized
model leads to significant increase on nearly all categor-
ies except for NegResistant category.
Discussions
Annotation results showed that there were still many
concerns over the HPV vaccine on Twitter during the
study period. The number of tweets holding negative
opinions on HPV vaccines exceeded the tweets holding
positive opinions. The major concern found was about
safety issues. As it is a relative small corpus, in the fu-
ture, we plan to apply this system on a large-scale tweets
corpus. We can leverage further analysis tool to track
the changes and to identify the patterns of different sen-
timents toward HPV vaccines over the time.
As the gold standard has a highly imbalanced structure
(highly uneven distribution of different categories), trad-
itional plain classification method cant take advantage
of the hierarchical classification information. The pro-
posed hierarchical classification method outperformed
the plain method significantly on overall performance
and on each category as well. Adding POS tags and word
clusters as a feature has already shown its effect on im-
proving performance on previous NLP tasks. Our ex-
periment further demonstrated its power in the multi-
classification tasks on tweets corpus for accessing vac-
cination purpose. Parameter optimization is very neces-
sary according to our results. It can greatly influence the
system performance, especially on some categories with
very limited number.
There are still several limitations of the work reported
here. A serious issue for our Twitter corpus is that it is
highly unbalanced, which means that the distribution of
different classes is highly diverse. It is very challenging
for machine learning system to handle classes with very
limited number. In the future, we plan to collect incorp-
orate more tweets of minority classes to the gold stand-
ard. In this work, we only used three feature sets. More
feature sets can be included to improve the performance,
including character n-grams, word dependency, struc-
ture feature, and sentiment lexicons feature. Rule-based
approaches might be more effective for classification on
minority classes. A hybrid system consisting of both ma-
chine learning and rule-based approach is supposed to
be very helpful.
Conclusions
We designed and conducted a study to classify HPV vac-
cine related tweets by the sentiment polarity using ma-
chine learning methods. A hierarchical scheme was
proposed for different sentiment classifications of HPV
vaccines. Ten different categories were included to cover
most types of public opinions for HPV vaccines. A gold
standard that is consisted of 6,000 randomly selected
tweets were manually annotated as the training dataset.
Different classification methods were evaluated. Differ-
ent combinations of feature sets and parameters were
tested to optimize the performance of the machine
learning model. Compared with the baseline model, the
hierarchical classification model with optimized feature
sets and model parameters has increased the micro-
averaging and macro-averaging F score from 0.6732 and
0.3967 to 0.7442 and 0.5883 respectively.
Our work provides a systematical way to improve the
machine learning model performance on the highly un-
balanced HPV vaccine related tweets corpus. Our system
can be further applied on a large tweets corpus to ex-
tract large-scale public opinion towards HPV vaccines.
Similar systems can be developed to explore other public
health related issues.
Additional file
Additional file 1: Table A. Sample tweets annotated in the gold
standard for each sentiment category (DOCX 43 kb)
Acknowledgements
N/A.
Funding
This research is partially supported by the National Library of Medicine of the
National Institutes of Health under Award Number R01LM011829. The
authors also gratefully acknowledge the support from the UTHealth
Innovation for Cancer Prevention Research Training Program Pre-doctoral
Fellowship (Cancer Prevention and Research Institute of Texas grant #
RP160015).
Availability of data and materials
The annotations of gold corpus can be found at: https://sbmi.uth.edu/
ontology/files/TweetsAnnotationResults.zip.
Authors contributions
JD collected the data, wrote the initial draft and revised subsequent draft.
JD, JX, XL and HS developed the method and performed the evaluation. HS
and XL provided expertise in classification scheme. CT provided institutional
support, and contributed to research design. All authors read and aproved
the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
N/A.
Ethics approval and consent to participate
This study received IRB approval from Committee for the Protection of
Human Subjects at The University of Texas Health Science Center at
Houston. The reference number is HSC-SBMI-16-0291.
Author details
1The University of Texas School of Biomedical Informatics, 7000 Fannin St
Suite 600, Houston, TX 77030, USA. 2The University of Texas School of Public
Health, 1200 Pressler Street, Houston, TX 77030, USA.
Du et al. Journal of Biomedical Semantics  (2017) 8:9 Page 6 of 7
Received: 20 November 2016 Accepted: 7 February 2017
Papanikolaou et al. Journal of Biomedical Semantics  (2017) 8:43 
DOI 10.1186/s13326-017-0150-0
RESEARCH Open Access
Large-scale online semantic indexing of
biomedical articles via an ensemble of
multi-label classification models
Yannis Papanikolaou1* , Grigorios Tsoumakas1, Manos Laliotis2, Nikos Markantonatos3
and Ioannis Vlahavas1
Abstract
Background: In this paper we present the approach that we employed to deal with large scale multi-label semantic
indexing of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge
(20132017), a challenge concerned with biomedical semantic indexing and question answering.
Methods: Our main contribution is a MUlti-Label Ensemble method (MULE) that incorporates a McNemar statistical
significance test in order to validate the combination of the constituent machine learning algorithms. Some
secondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to the
BioASQs super-set, the PubMed articles collection) and the proper parametrization of the algorithms used to deal
with this challenging classification task.
Results: The ensemble method that we developed is compared to other approaches in experimental scenarios with
subsets of the BioASQ corpus giving positive results. In our participation in the BioASQ challenge we obtained the first
place in 2013 and the second place in the four following years, steadily outperforming MTI, the indexing system of the
National Library of Medicine (NLM).
Conclusions: The results of our experimental comparisons, suggest that employing a statistical significance test to
validate the ensemble methods choices, is the optimal approach for ensembling multi-label classifiers, especially in
contexts with many rare labels.
Keywords: Semantic indexing, Multi-label ensemble, Machine learning, BioASQ, Supervised learning, Multi-label
learning
Background
Introduction
MEDLINE is the premier bibliographic database of the
National Library of Medicine (NLM) of the United States.
In June 2017 MEDLINE contained over 27 million ref-
erences to articles in life sciences with a focus on
biomedicine. Each of these articles is manually indexed
by human experts with concepts of the MeSH (Medical
Subject Headings) ontology (also curated by NLM), such
as Neoplasms, Female and Newborn. This manual index-
ing process entails significant costs in time and money.
*Correspondence: ypapanik@csd.auth.gr
1Department of Computer Science, Aristotle University, 54124 Thessaloniki,
Greece
Full list of author information is available at the end of the article
Human annotators need on average 90 days to complete
75% of the citation assignment for new articles [1]. For
a publication with novel and important scientific results,
the first period of its lifetime is quite important, yet it
is in this period that the publication remains semanti-
cally invisible. For instance, if a researcher is searching
for a particular MeSH term (e.g. Myopathy), he/she will
not be able to retrieve the latest non-indexed articles that
are related to this term, if they do not contain it liter-
ally. Moreover, the average indexing cost for an article
is $9.401.
MEDLINEs demand in manual indexing is steadily
increasing as evident from Fig. 1, which plots the number
of articles being added to MEDLINE each year from 1950
to 2017. At the same time, the available indexing budget at
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Papanikolaou et al. Journal of Biomedical Semantics  (2017) 8:43 Page 2 of 13
Fig. 1 Number of articles being added to MEDLINE each year from
1950 to 2017
NLM is flat or declining. This highlights the importance
of tools for automatic semantic indexing of biomedical
articles. Such tools can help increase the productivity of
human indexers by recommending them a ranked list of
MeSH descriptors relevant to the article they are currently
examining. In addition, such tools could replace junior
indexers (not senior revisers) for journals where these
tools achieve a high level of accuracy. Both usages of such
tools are currently adopted by NLM.
From a machine learning perspective, constructing an
automatic semantic indexing tool for MEDLINE poses a
number of important challenges. First of all, there is a
large number of training documents and associated con-
cepts. In 2017, MeSH contained 28,489 descriptors, while
PubMed contained over 27 million annotated abstracts.
Efficient yet accurate learning and inference with such
large ontologies and training sets is non-trivial. Addition-
ally, MEDLINE is growing at a non-trivial rate of more
than one million articles per year, i.e. more than 100 arti-
cles per hour. This calls for learning algorithms that can
work in an online fashion both in the sense of handling
additional training data as well as in the sense of being effi-
cient enough during prediction in order to cope with the
fast rate that new articles arrive. Furthermore, MEDLINE
contains abstracts from about 5000 journals covering very
different topics. This increases the complexity of the tar-
get function to be learned, as concepts may be associated
with different patterns of word distributions in different
biomedical areas.
MeSH concepts are hierarchically structured as a
directed acyclic graph indicating subsumption relations
among parent and child concepts. This structure is quite
complex, as it comprises 16 main hierarchies with depths
up to 12 levels and many children nodes belong to more
than one ancestors and to more than one of the main hier-
archies. While some progress has been recently achieved
on exploiting such relationships, it is not entirely clear
when and how these relationships help accuracy. As
MeSH evolves yearly on par with the medical knowledge
it describes, automatic indexing models must deal with
such changes, both explicit (i.e. addition, deletion, merg-
ing of concepts) and implicit (i.e. altered semantics of
concepts) ones. Also, each scientific document is typi-
cally annotated with several MeSH concepts. Such data
are known as multi-label [2] and present the additional
challenge of exploiting label dependencies to improve
accuracy. Figure 2 shows the distribution of the number
of labels per document which is Gaussian with a mean of
about 13 labels per document and a heavy tail on the right.
The distribution of positive and negative examples for
most of the MeSH concepts is very imbalanced [3].
Figure 3 plots the frequencies of labels (x-axis) versus
the number of labels having such frequency (y-axis) for a
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 
DOI 10.1186/s13326-017-0159-4
RESEARCH Open Access
A document-centric approach for
developing the tolAPC ontology
Aisha Blfgeh1,2*, Jennifer Warrender1, Catharien M. U. Hilkens3 and Phillip Lord1
Abstract
Background: There are many challenges associated with ontology building, as the process often touches on many
different subject areas; it needs knowledge of the problem domain, an understanding of the ontology formalism,
software in use and, sometimes, an understanding of the philosophical background. In practice, it is very rare that an
ontology can be completed by a single person, as they are unlikely to combine all of these skills. So people with these
skills must collaborate. One solution to this is to use face-to-face meetings, but these can be expensive and
time-consuming for teams that are not co-located. Remote collaboration is possible, of course, but one difficulty here
is that domain specialists use a wide-variety of different formalisms to represent and share their data  by the far
most common, however, is the office file either in the form of a word-processor document or a spreadsheet.
Here we describe the development of an ontology of immunological cell types; this was initially developed by domain
specialists using an Excel spreadsheet for collaboration. We have transformed this spreadsheet into an ontology using
highly-programmatic and pattern-driven ontology development. Critically, the spreadsheet remains part of the source
for the ontology; the domain specialists are free to update it, and changes will percolate to the end ontology.
Results: We have developed a new ontology describing immunological cell lines built by instantiating ontology
design patterns written programmatically, using values from a spreadsheet catalogue.
Conclusions: This method employs a spreadsheet that was developed by domain experts. The spreadsheet is
unconstrained in its usage and can be freely updated resulting in a new ontology. This provides a general
methodology for ontology development using data generated by domain specialists.
Keywords: Tawny-OWL, Document-centric, Ontology, Excel workflow
Introduction
Ontologies have been used extensively to describe many
parts of biology. They have two key features which make
their usage attractive: first, they can provide a mecha-
nism for standardising and sharing the terms used in
descriptions; and, second, they provide a computation-
ally amenable semantics to these descriptions, making
it possible to draw conclusions which are not explicitly
stated.
Ontologies are increasingly used to facilitate the man-
agement of knowledge and the integration of information
*Correspondence: a.blfgeh1@newcastle.ac.uk; abelfaqeeh@kau.edu.sa
1School of Computing Science, Newcastle University, NE1 7RU Newcastle
Upon Tyne, UK
2Faculty of Computing and Information Technology, King Abdulaziz
University, 21589 Jeddah, Saudi Arabia
Full list of author information is available at the end of the article
as in the SemanticWeb [1]. Biological data is not only het-
erogeneous but requires special knowledge to deal with
and can be large [2]. Ontologies are good for representing
complex and, potentially, changeable knowledge. There-
fore, they are widely used in biomedicine with examples
such as the Gene Ontology [3], ICD-10 (International
Classification of Diseases) [4] or SNOMED (Systematized
Nomenclature of Medicine) [5] being the best known.
However, building an ontology is a challenging task [6].
Ontologies often use languages with a complex underly-
ing formalism (such as OWL1 -Web Ontology Language-
for instance) especially when modelling complex domain
area such as biology or medicine. Moreover, ontology
building is normally a collaboration between domain spe-
cialists and ontology developers. However, any form of
multi-disciplinary collaboration is difficult. In the case,
for example, of the Gene Ontology, these challenges were
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 2 of 9
addressed through explicit community involvement using
meetings, focus groups and the like [7]. Other methodolo-
gies have adopted a more distributed approach [8].
It is, perhaps, because of these challenges that, despite
the computational advantages of ontologies, the oldest
and most common form of description in biology is free
text, or a semi-structured representation through the use
of a standardised fill-in form. These representations have
numerous advantages compared to ontologies: they are
richly expressive, widely supported by tooling and while
the form of language used in science (Bad English [9])
may not be easy to use, understand or learn, it is widely
taught and most scientists are familiar with it. Similarly,
most biologists are familiar with the tools used for pro-
ducing free-text and forms, either a word-processor doc-
ument or a spreadsheet. Tools for producing this form
of knowledge are wide-spread, richly functional both in
application and cloud-delivered form, and support highly
collaborative development.
The ontology community, conversely, has largely built
its own tool-chain for development. Tools such as Protégé
[10] are highly functional in their own right, but have a
user interface which is far removed from those that biol-
ogists are used to. There have been several responses to
this problem. First, it is possible to take existing ontol-
ogy tools and customise them for use within a specific
community, so that they have a familiar look and feel;
this is the approach taken by iCAT (Collaborative Author-
ing Tool)  a version of WebProtégé [11] built explicitly
for the ICD-11 community [12]. A second approach is to
enable existing ontology tools to ingest office documents;
for example, Cellfie [13] is a Protégé plugin which can
transform a spreadsheet into an OWL ontology, which
can then be developed further; however this is a one-
off process  once ingested, the data in the spreadsheet
is converted into OWL; further updates cannot be made
using the original spreadsheet formalism. Finally, tools
such as RightField [14] and Populous [15] add ontologi-
cal features to office documents, by allowing selection of
spreadsheet cells from a controlled vocabulary, followed
by export to OWL using OPPL (Ontology Pre-Processor
Language) [16] to express the patterns used in the trans-
formation [17].
These tools, however much they support the use of
office software, at some point require leaving this soft-
ware and moving into an ontology specific environment.
We have developed a new, highly-programmatic environ-
ment for ontology development called Tawny-OWL [6].
With this approach the ontology is developed as program-
matic source code, which is then evaluated to generate the
final ontology, either in memory or as an OWL file. This
offers a new methodology. In this research, we developed
a document-centric workflow centred on the use of office
tooling to construct the ontology; biologists generate and
maintain their dataset in an unconstrained Excel spread-
sheet; we then use this spreadsheet directly as part of our
source code2, driven by Tawny-OWL. In this model, we
can apply arbitrary validation and transformation of the
data held in the spreadsheet, into an ontological form. As
the spreadsheet is now part of the source code, rather than
being used as knowledge capture interface, it can be freely
updated and the final ontology regenerated.
In this paper, we describe the application of this
methodology to the generation of a catalogue of immuno-
logical cell types, called the tolAPC (tolerogenic antigen-
presenting cells) catalogue. We discuss the background
technology, the design decisions that we have faced and
the general implications that this approach has for ontol-
ogy development.
Background
The tolAPC catalogue is a list of immunological cell
types. It has been captured as part of the EU Cost
Action BM1305 A-FACTT (Action to Focus and Accel-
erate Cell-based Tolerance-inducing Therapies)3 which is
aimed at increasing data sharing and collaborative work-
ing across the community [18]. These cell types have been
tolerised  that is treated so that they suppress the
immune response  and have been created with the inten-
tion that they will be used therapeutically in a variety of
situations including: the treatment of auto-immune dis-
ease such as rheumatoid arthritis; or to reduce rejection
following transplantation [19]. Information about these
cells is, therefore, high value. The tolAPC catalogue con-
tains extensive details about these cell lines, including 9
sheets of data. The catalogue has been created as an
Excel spreadsheet, although it uses the spreadsheet only
to represent tabular information (i.e. there is no use of
equations or calculation in the spreadsheet). The spread-
sheet has been created by individual scientists freely; that
is, there is no formal constraint on the legal set of values
in each cell, just the social convention of copying previ-
ous cells. Figure 1 shows the structure of the spreadsheet
filled with false information due to the confidentiality of
the tolAPC catalogue.
Next, we describe Tawny-OWL; it is a fully program-
matic development environment for OWL. It has been
implemented in Clojure, which is a dialect of lisp, run-
ning on the Java Virtual Machine. It wraps the OWL-API
[20] which performs much of the actual work, includ-
ing interaction with reasoners, serialisation and so forth.
Tawny-OWL has a simple syntax which was originally
modelled on the Manchester OWL notation [21], modi-
fied to conform to standard Clojure syntax and to increase
regularity [22]. For example, we can create a new class
with an existential restriction as follows:
(defclass A :super (some r B))
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 3 of 9
Fig. 1 A mock sample of tolAPC catalogue to show the structure of the Excel spreadsheet
Or, we can define a new individual with a property
assertion:
{(defindividual i :fact (is r j))}
As a domain specific language embedded in a full pro-
gramming language, we also gain all the features of that
environment; for instance, we can create arbitrary pat-
terns simply by using a Clojure function. Consider for
example:
(defn some-only [property & classes]
(list (some property classes)
(only property
(or classes))))
Here defn introduces a new function, property
& classes are the arguments, and list packages the
return values as a list. some, only and or4 are defined
by Tawny-OWL as the appropriate OWL class construc-
tors. This allows a definition specifying an existential
relationship with a closure axiom as follows:
(defclass D :super (some-only r A B))
We also gain access to the full Clojure infrastructure: we
can edit and evaluate terms in a power editor or IDE (Inte-
grated Development Environment)5; write unit tests and
run them through a build tool [23], publish and version
using git, and continuously integrate our work with other
ontologies.
We have previously used this functionality to create the
karyotype ontology which is generated from a series of
interlocking sub-patterns [24], parameterised using literal
data structures in the source code. The karyotype ontol-
ogy is highly patternised, with almost all of the classes
coming from a single large pattern.
As a full programming environment, Clojure can also
read and parse arbitrary data formats, which can operate
as additional source during the generation of the ontology.
We have previously used this to scaffold a mitochondrial
ontology from a varied set of input files [25], or to add
multi-lingual annotation using key=value properties
files to the pizza ontology. We have also used this technol-
ogy with a spreadsheet to specify a set of ontological unit
tests for the karyotype ontology [23]. In this case, the val-
ues in the spreadsheet are used to generate a set of OWL
classes which are then checked for correct subsumption
using a reasoner. In this case, however, these ontological
statements are used only as part of a test suite, rather than
intended for downstream usage, and the spreadsheet was
created specifically for this purpose.
Methods
The data for the tolAPC catalogue was captured directly
in a spreadsheet largely co-ordinated through email. As
a pre-existing resource, it made little sense to rewrite
directly in OWL either using Protégé or Tawny-OWL 
to do so would have resulted in transcription errors,
and made updates more complex. However, as described
in the Background section, we have all the compo-
nents that we need to build an ontology directly from a
spreadsheet.
Therefore, we started the development process using
our new document-centric workflow that incorporates
Excel spreadsheet during development as described in
Fig. 2. We read the spreadsheet directly and extract all
values we need to instantiate the ontology patterns we
have already designed using the programming facilities
of Tawny-OWL. The final ontology can be saved as an
OWL file to be browsed using Protégé software or a web
browser.
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 4 of 9
Fig. 2Workflow using Excel spreadsheet and Tawny-OWL Patterns
Building the tolAPC ontology
In this section, we describe the issues that have arisen dur-
ing the process which can conceptually be split into three
phases6:
1. Extraction
2. Validation
3. Ontologisation
The extraction phase is straight-forward. Clojure offers
a number of libraries capable of reading a spreadsheet.
In the case of the tolAPC catalogue, we read the spread-
sheet using the Docjure library7, accessed directly from
the file system. It would also be simple and straight-
forward to read from a network which would support
building ontologies from cloud-hosted spreadsheets. Pre-
viously, for performance reasons, we have read and then
cached the results of tests generated from a spreadsheet
[23]; however, for the tolAPC catalogue performance is
such that the spreadsheet can be read in full every time
the environment is initialised, significantly simplifying the
development.
In the second phase, values extracted are validated
against a set of constraints specifying those which are
legal. For many of the fields, values are highly stereo-
typed having only a few different options; for example,
cells can either be Autologous or Allogeneic, while
expression levels can either be + or -. Currently, valida-
tion is performed through the use of ad hoc testing 
we expect to move to a more formal data constraint lan-
guage in future. The choice of validation depends on the
requirements and modelling choices made, which will be
discussed later.
In the third phase, values are ontologised. The top
level of the ontology which provides what we describe as
schema terms is written by hand using Tawny-OWL. In
the case of the tolAPC catalogue, this includes terms such
as CellType, Species and AntigenLoad. Next, a set
of patterns is defined using these schema terms. Finally,
these patterns are instantiated using the values from the
second phase, generating entities that we call patternised
terms.
During the development process, both reasoning and
manual inspection of the created ontology is used to
ensure that the process is happening as expected; for the
latter process, the ontology is saved to file and examined,
either in the Clojure development environment or within
Protégé, as shown in Fig. 3.
We next discuss the modelling issues that have arisen.
Modelling in the tolAPC ontology
All entities in the ontology need to be represented by
an IRI (Internationalized Resource Identifier). Two broad
schemes are used to generate IRIs: semantics free identi-
fiers which are generally numeric; and semantically mean-
ingful identifiers which are normally derived from the
common name for the entities. Generally, the latter are
easier to work with, while the former are easier to keep
stable over releases.
Currently, for the tolAPC ontology, schema terms have
IRIs which reflect their names (CellType uses an IRI with
a fragment of CellType), while patternised terms use an
ad hoc schema based on several of their properties (a sin-
gle property is not enough to ensure uniqueness). If we
wish to re-evaluate this situation at a later date, however,
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 5 of 9
Fig. 3 tolAPC ontology displayed from Protégé screen
Tawny-OWL simplifies the situation; we can easily allo-
cate IRIs to entities according to any scheme that we
choose, by changing a single function.
A recurrent issue in ontology modelling is whether to
use classes or individuals; within the tolAPC ontology,
we faced this question for cell types. There are a num-
ber of different criteria for making this decision [26]. We
considered briefly a realist perspective: modelled as a
single entity, cell types are probably best represented as
a metaclass, akin to a taxonomic species [27]; modelling
as multiple entities (differentiating between the protocol
and the cell type produced) would also be possible. How-
ever, there appears to be no clear principle to distinguish
between these options. Similar problems also arise for
proteins/cell-surface markers which are described in the
ontology. As an additional problem, these representations
introduce considerable unnecessary complexity [28].
We considered therefore the needs of our application:
it seems unlikely that we will ever need subclasses of a
cell type, but might reasonably wish for cell types to be
unique  to state that two cell types are necessarily the
same (or different) individual. For these reasons, wemodel
cell types as individuals. An example from the ontology
structure is shown in Fig. 4.
The tolAPC ontology largely models a set of cell types,
with the rest of the ontology designed to support these
Fig. 4 Class Structure in tolAPC ontology
descriptions. The ontology, as a result, contains very little
hierarchy, and is at the extreme end of a normalised ontol-
ogy [29]. Cell types are defined as individuals with a large
set of different property assertions, as can be seen from
the following definition:
(individual cell-name
:fact (is fromGroup group)
(is hasLocation loc)
(is fromClinicalDisease clinic-disease)
(is fromSpecies from-species)
(is hasStatus stat)
(is hasType c-type)
(is hasDescription desc)
(is hasActivation active)
(is hasAntigenLoad anti-load)
(is itsOrigin cell-org)
(is withStartMaterial start-material)
(is hasIsolation isol))
Here, cell-org, group, loc and others are variables,
therefore, this definition describes a pattern. fromGroup,
hasLocation and others are specific object properties
from the schema terms of the ontology. individual,
:fact and is are part of Tawny-OWL syntax. The whole
definition defines a new cell type, and its association with
a set of individuals.
The values of the property assertions fall into one of
three main categories.
Open but Limited:Many properties support a very lim-
ited, but nonetheless open, range of values. Examples of
these are withStartMaterial which describes the tis-
sue or part of the tissue from which the cells are derived.
These values are modelled as disjoint classes, explicitly
stated in the ontology. Although, we could have used an
external ontology at this point, as only a few options are
actually used, we have not imported one.
Constrained Partition: Many properties support an
exact number of options. These are modelled using a
Value Partition [30]. Fortunately, Tawny-OWL provides
explicit support for this design pattern, which allows
a relatively succinct definition. An example of this is
CellOrigin which is defined as follows:
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 6 of 9
(deftier CellOrigin
[[Allogeneic
:comment "Allogeneic stem cell transplant
uses a donor blood"]
[Autologous
:comment "Autologous stem cell transplant
uses a patient own blood"]])
Unconstrained Values: Some properties have uncon-
strained values such as Location, Group (i.e. the people
responsible for the cell type) or AntigenLoad. These are
currently modelled as individuals, created on-demand.
In some cases, these values also reuse terms from exter-
nal ontologies; currently, our Species term refers to the
NCBI (National Centre for Biotechnology Information)
taxonomy, although we do not import the full semantics
of this ontology as it would cause a considerable increase
in reasoning time, for relatively low reward.
In addition to these threemain categories, we are adding
phenotype descriptors to the cell types, in terms of raised
or lowered expression levels. For these, we are modelling
the expression levels as a value partition, while the over-
all phenotype is modelled using the N-ary relationship
pattern [31], as shown in Fig. 5.
Results
We have developed a new ontology describing immuno-
logical cell lines built by instantiating ontology design
patterns written programmatically, using values from a
spreadsheet catalogue. The development of the tolAPC
ontology is a work in progress. As can be seen from
Table 1, while parts of the tolAPC catalogue have been
recast, there are significantly more spreadsheet cells
which need to be converted.
Discussion
In this paper, we have described the development of the
tolAPC ontology, describing data about immunological
cell types. This ontology is unusual in that it is derived
directly from another data resource, the tolAPC cata-
logue which is maintained as an Excel spreadsheet. Essen-
tially, the ontology provides context and semantics to data
which is available in another form.
The value of recasting a spreadsheet into a form with
precise machine interpretable semantics is obvious, but
there are less apparent virtues arising from the process.
Table 1 Current statistics of excel sheet and tolAPC ontology
tolAPC catalogue Number of sheets 9
Number of cells 1181
Number of cell types 15
tolAPC ontology Number of classes 21
Number of individuals 101
Number of object properties 13
In the initial validation step, for example, we have had
to clarify parts of the tolAPC catalogue which are oth-
erwise unclear. For example, one cell-line is described
as Autologous/Allogeneic. The original author intent
here is unclear: this could be intended to mean either
autologous or allogeneic (possible), both (probably incon-
sistent) or just the absence of knowledge. Similarly the
process of ontologisation forces us to clarify some areas
of the biology; including questions about whether cell
types produced by the same protocol at different times
are the same or otherwise, which touches on issues of
reproducibility. Where these issues have arisen, either the
ontology schema, patterns or the spreadsheet can bemod-
ified accordingly. As shown in Fig. 2, information flows
in both directions between the spreadsheet and ontol-
ogy. Currently, validation is performed by hand specify-
ing constraints as enumerations of strings. In future, we
would like to move this to a more declarative approach;
fortunately, because Tawny-OWL is implemented over a
full programming language, there are a number of dif-
ferent data constraint languages, such as Prismatic [32],
or clojure.spec8. We expect richer validation will help to
enhance the ontology development process further.
The development of the tolAPC ontology is an ongoing
work where some parts of the tolAPC catalogue have been
adapted into the ontology, but there are other spread-
sheet cells which still need to be imported. Additionally,
while adding machine interpretable semantics is useful in
its own right, we have only started to address the issue
of interoperability with other ontologies. Currently, child
terms of Species re-use IRIs from the NCBI taxonomy;
the mapping between the free text used in the tolAPC
catalogue and the NCBI taxonomy is stored in a literal
Fig. 5 N-ary Relation in tolAPC ontology
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 7 of 9
data structure in source, but could also be stored in a flat-
file or subsidiary spreadsheet. We do not import the full
ontology for reasons of performance, a process known as a
soft import [33]. Developing a programmatically defined
ontology allows us to switch easily between soft, hard
and MIREOT-style semi imports [34]. Conversely, child
terms of ClinicalDisease do not currently relate to
other ontologies. At the current time, we have not pri-
oritised this process because confidentiality restrictions
on the tolAPC catalogue limit our ability to share the
results anyway. Adding this form of interoperability is not
complex though as we have already demonstrated with
Species and by using the scaffolding process described
previously [25].
This work is a further demonstration of the value of
programmatic and pattern-driven ontology development
using the Tawny-OWL library; it builds on earlier work
with: a karyotype ontology where patterns are instantiated
using in-code literal data structures; the mitochondrial
ontology which is scaffolded using a variety of different
input formats; or our reworking of SIO which patternises
a pre-existing ontology [35]. Patternisation allows the
development of an ontology to be performed rapidly and
repeatedly.
The fully programmatic environment also demonstrates
its value, as we have been able to add a new input format,
even a very complex format such as an Excel spreadsheet
with relative ease, building on tools provided by others.
This replicates our earlier experiences with Tawny-OWL;
we can reuse and repurpose existing tools not specifically
intended for use in ontology development, also adapt a
complete software development environment to the task.
The use of Excel spreadsheets to drive ontology pat-
terns is not new of course; it is directly supported with
Protégé plugins as well as with tools such as RightField
and Populous. The key addition of our methodology is
to incorporate the spreadsheet as a part of the ontology
source code. The spreadsheet can be updated, changed
and consulted by the domain specialists who created it,
and still remain part of the ontology development pro-
cess. The importance of the right format should not be
under-estimated; for example, early versions of the Gene
Ontology were developed in their own bespoke syntax
(later to evolve into OBO -Open Biomedical Ontologies-
Format), something which persisted for a considerable
time after the development and release of OWL. The
reasons for this were simple: OBO Format behaved well
in a version control system, and could be easily created,
edited and manipulated in a text editor, something not
true of RDF (Resource Description Framework)9 serial-
isation of OWL available at the time. We wish to build
on these lessons: ontologists should seek to interact and
build on the tools that domain specialists already use, if
they hope to describe the knowledge that these specialists
have. It is also for this reason, that we have not designed
an Excel template. Rather, we let the experts design and
create a suitable spreadsheet that matches their needs. So,
domain users will be happy and comfortable in using their
usual tool (Excel spreadsheet, designed according to their
needs) and ontology developers can conveniently program
the ontology using Tawny-OWL. Conversely, one disad-
vantage of this approach is that domain users normally
only interact with one part of the ontology source; the
spreadsheet may be correct with respect to the domain,
but the ontology wrong. We are, therefore, also investigat-
ing techniques for making the Tawny-OWL section of the
ontology more readable [36].
In future, we may consider designing a general template
for particular domain experts who do not have a clear
structure for their data, so that gives them the opportunity
to start organising their data in a semi-structured way;
there are a number of pre-existing schemas that we could
using, including MAGE-TAB [37] and later ISA-TAB [38].
The tolAPC ontology and the document-centric
approach it embodies is a first step toward establishing
a richer methodology, where we interact with domain
specialists using their own tool chain to capture knowl-
edge. In the future, we aim to combine other formats like
Word documents in the ontology development pipeline
and design a comprehensive template to communicate
effectively with domain specialists in order to build an
accurate and well designed ontology.
Conclusions
In this paper, we have successfully developed tolAPC
ontology based on the tolAPC catalogue using an Excel
spreadsheet as a source of information. Critically, the
spreadsheet is unconstrained by the ontology developers
having been freely developed by the domain users. More-
over, we have not converted the spreadsheet in a one-off
process; the spreadsheet is part of the source code for the
ontology and can be freely updated. Taken together this
demonstrates a newmethodology for building an ontology
which enable us to interact with domain specialists using
their preferred tools.
Endnotes
1 https://www.w3.org/TR/owl2-overview/
2 By source code, we mean the spreadsheet is not
imported but remains the preferred form for editing.
3 http://www.cost.eu/COST_Actions/bmbs/BM1305
4We have elided namespaces: or and some are also
core Clojure functions.
5We use Emacs but there is rich support in Vim, Eclipse,
IntelliJ, or LightTable
6 In practice, the tolAPC ontology is developed
iteratively.
Blfgeh et al. Journal of Biomedical Semantics  (2017) 8:54 Page 8 of 9
7 https://github.com/mjul/docjure
8 https://clojure.org/news/2016/05/23/introducing-
clojure-spec
9 https://www.w3.org/TR/1998/WD-rdf-schema-
19980409/
Abbreviations
A-FACTT: Action to focus and accelerate cell-based tolerance-inducing
therapies; iCAT: Collaborative authoring tool; ICD: International classification of
diseases; IDE: Integrated development environment; IRI: Internationalized
resource identifier; NCBI National centre for biotechnology information; OBO:
Open biomedical ontologies; OPPL: Ontology pre-processor language; OWL:
Web ontology language; RDF: Resource description framework; SNOMED:
Systematized nomenclature of medicine; tolAPC: Tolerogenic
antigen-presenting cells
Acknowledgements
We thank Dr Paloma Riquelme (University Hospital Regensburg) for help in
generating the tolAPC catalogue and participants of the A-FACTT network for
providing data for the tolAPC catalogue. This work has been presented in
ODLS 2016 in Halle (Saale), Germany. Therefore, we would like to thank ODLS
2016 reviewers and organisers for their effort and support.
Funding
COST is part of the EU Framework Programme Horizon 2020 (action to focus
and accelerate cell-based tolerance-inducing therapies, BM1305,
http://www.afactt.eu). Aisha Blfgeh is funded by a scholarship from King
Abdulaziz University, Jeddah, Saudia Arabia.
Availability of data andmaterials
The software tool, Tawny-OWL, is available from http://github.com/phillord/
tawny-owl. The tolAPC ontology is currently not available.
Authors contributions
AB implemented and designed the tolAPC ontology. JW implemented the
spreadsheet importer. CH conceived the tolAPC catalogue. PL conceived the
document-centric approach. All authors read and approved the final
manuscript.
Ethics approval and consent to participate
Newcastle University procedures determined that full ethical approval was not
required for this research.
Consent for publication
The Authors consent to publish this Work in the thematic series Biomedical
Ontologies (BIOONT series).
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1School of Computing Science, Newcastle University, NE1 7RU Newcastle
Upon Tyne, UK. 2Faculty of Computing and Information Technology, King
Abdulaziz University, 21589 Jeddah, Saudi Arabia. 3Institute of Cellular
Medicine, Newcastle University, NE1 7RU Newcastle Upon Tyne, UK.
Received: 13 February 2017 Accepted: 15 October 2017
Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 
DOI 10.1186/s13326-017-0167-4
RESEARCH Open Access
Integrating phenotype ontologies with
PhenomeNET
Miguel Ángel Rodríguez-García1,2, Georgios V. Gkoutos3,4,5, Paul N. Schofield6 and Robert Hoehndorf1,2*
Abstract
Background: Integration and analysis of phenotype data from humans and model organisms is a key challenge in
building our understanding of normal biology and pathophysiology. However, the range of phenotypes and
anatomical details being captured in clinical and model organism databases presents complex problems when
attempting to match classes across species and across phenotypes as diverse as behaviour and neoplasia. We have
previously developed PhenomeNET, a system for disease gene prioritization that includes as one of its components an
ontology designed to integrate phenotype ontologies. While not applicable to matching arbitrary ontologies,
PhenomeNET can be used to identify related phenotypes in different species, including human, mouse, zebrafish,
nematode worm, fruit fly, and yeast.
Results: Here, we apply the PhenomeNET to identify related classes from two phenotype and two disease ontologies
using automated reasoning. We demonstrate that we can identify a large number of mappings, some of which
require automated reasoning and cannot easily be identified through lexical approaches alone. Combining
automated reasoning with lexical matching further improves results in aligning ontologies.
Conclusions: PhenomeNET can be used to align and integrate phenotype ontologies. The results can be utilized for
biomedical analyses in which phenomena observed in model organisms are used to identify causative genes and
mutations underlying human disease.
Keywords: Phenotype, PhenomeNET, Disease gene prioritization, OWL, Automated reasoning
Background
Understanding the functions of genes and gene prod-
ucts is vital for our understanding of normal biology and
pathophysiology. In recent years the amount of geno-
type and phenotype data available for species as distinct
as man and model organisms such as nematode worms
has increased dramatically and continues to accelerate.
Insights from non-human species have an important role
to play in our understanding of human biology [1] and the
challenge is to mobilise this data in a way in which it can
be used to give meaningful insights into human physiol-
ogy and disease. While much data is now being captured
*Correspondence: robert.hoehndorf@kaust.edu.sa
1Computational Bioscience Research Center (CBRC), King Abdullah University
of Science and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia
2Computer, Electrical and Mathematical Sciences & Engineering Division
(CEMSE), King Abdullah University of Science and Technology, 4700 KAUST, PO
Box 2882, 23955-6900 Thuwal, Saudi Arabia
Full list of author information is available at the end of the article
formally using ontologies, data integration and compari-
son across species presents a major informatics challenge
[2]. This task requires that related phenotypes which span
levels of granularity as well as domains of knowledge, for
example behaviour or neoplasia, in organisms as anatom-
ically distinct as zebrafish and man, can be matched and
compared so as to allow findings in one species to be
related to others.
In response to this challenge we developed Phe-
nomeNET. PhenomeNET [3] was built in 2011 as a
system for disease gene discovery and prioritization.
PhenomeNET consists of an ontology integrating species-
specific phenotype ontologies based on the PATO ontol-
ogy [4] and relations between anatomical structures and
physiological processes, a database of gene-to-phenotype
associations, and a measure of similarity between sets
of phenotypes. Within PhenomeNET, species-specific
phenotype ontologies are combined so that phenotypes
observed in different species can be compared directly.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 2 of 11
The main application of PhenomeNET is the prioritiza-
tion of candidate genes for human diseases by comparing
human disease phenotypes to existing gene-phenotype
associations derived from model organisms. In particular,
human phenotypes associated with a disease can be com-
pared to phenotypes observed in mouse or other model
organisms using the integrated PhenomeNET ontology,
and similarity between phenotypes can then be used to
indicate the genetic basis of a disease. PhenomeNET has
been successfully used to find candidate genes for diseases
[3, 5], identify novel pathways [6], and repurpose drugs
using mouse model phenotypes [7, 8].
The PhenomeNET ontology was originally built by
formally integrating species-specific phenotype ontolo-
gies, permitting the relationship between classes of dif-
ferent phenotype ontologies to be determined through
deductive inference. For this purpose, PhenomeNET
relies on the UBERON [9] ontology that identifies
equivalences between anatomy ontologies of different
species, the Gene Ontology (GO) [10] as a means
to identify equivalent or related processes and func-
tions, and the PATO ontology [11] to identify the qual-
ities associated with anatomical entities or biological
processes.
Here, we use the PhenomeNET ontology to identify
alignments between phenotypes in different species. We
present our results based on three versions of the Phe-
nomeNET ontology: the first version consists of the plain
ontology using only the axioms provided in the Human
Phenotype Ontology (HPO) [12] and the Mammalian
Phenotype Ontology (MP) [13]; in the second version, we
extend our original ontology by adding additional lexical
and structural mappings generated with the Agreement-
MakerLight [14] system and represent them as equiv-
alent class axioms in our ontology; and in the third
version, we further generate mappings between classes
in the PhenomeNET ontology, the Disease Ontology
(DO) [15] and the Orphanet Rare Disease Ontology
(ORDO) [16].
We find that our axiomatic approach can identify a
large number of relations between classes that are not
currently identified by other systems that do not uti-
lize similar formal methods. However, our evaluation also
shows that a large number of mappings can still be iden-
tified through lexical and structural approaches, and that
a purely axiomatic approach will miss many mappings
that cannot currently be identified axiomatically due to
incomplete and underspecified formalization of pheno-
type classes. We illustrate how a combination of formal,
lexical and structural approaches generates the most com-
plete and comprehensive mappings between (phenotype)
ontologies, and these mappings improve the application
of phenotype ontologies in data analysis and translational
research.
Methods
Data sources and ontologies
In our experiments, we use the Human Phenotype
Ontology (HPO) [12], Mammalian Phenotype Ontology
(MP) [13], Human Disease Ontology (DO) [15], and
Orphanet Rare Disease Ontology (ORDO) [17] provided
as part of the Ontology Alignment Evaluation Initiative
2016 competition.
The HPO is an ontology of human phenotypes and con-
sists of 11,787 classes that provide a standarized vocabu-
lary for describing phenotypic abnormalities which have
been commonly encountered in human monogenic dis-
eases [18]. The MP is mainly used to characterize mouse
phenotypes, but can also be applied to other organisms. It
consists of 11,720 classes that have been organized into a
directed acyclic graph (DAG) and can be used to describe
abnormal phenotypes of physiological and anatomical sys-
tems, behavior, and survival [19].
DO provides a classification of human diseases accord-
ing to multiple axes related to genetic disorders, infectious
diseases, metabolic disorders. It consists 9247 classes that
aim at unifying the representation of human diseases
defined across a variety of developed biomedical vocabu-
laries [20].
ORDO is derived from the Orphanet database of rare
and orphan diseases and used to represent and catego-
rize the diseases within Orphanet. It consists of 12,960
classes which provides a structured vocabulary to repre-
sent relationships between phenomes, diseases, genes and
relevant features such genetic inheritance for analyzing
rare diseases [17].
Lexical mappings
We use the AgreementMakerLight (AML) [21], released
on 5 April 2016, to generate lexical mappings between
ontologies. We used the automatic match mode of the
AML with the default settings to generate the lexical map-
pings that were used to extend the PhenomeNet ontology.
The default settings of AML include use of the UBERON
ontology, DO, and Wordnet as background knowledge,
a lexical matcher, a word-based matcher that evaluates
occurrence of the same words in class labels and syno-
myms, and a string similarity measure (ISub).
In addition to mappings generated by the AML, we also
incorporate mappings between the ontologies obtained
from BioPortal [22]. For each mapping between classes
from two ontologies, we add an equivalent class axiom to
the PhenomeNET ontology.
Semantic similarity and evaluation data
For additional external evaluation of our generated map-
pings, we apply the PhenomeNET ontology to the priori-
tization of candidate genes of human disease [3]. We use
the phenotypes associated with knockout mice available
Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 3 of 11
from theMouse Genome Informatics (MGI) database [23]
and the phenotypes associated with human diseases from
the Human Phenotype Ontology database [12]. We apply
Resniks semantic similarity measure [24] together with
the Best Matching Average strategy [25] to combine class
similarities.
We evaluate the results using a list of genedisease
associations provided by the Human Phenotype Ontology
database [12] as well as a set of mouse models of human
disease provided by the MGI database [23].
Source code and experiments
Source code for the PhenomeNET matching system,
including parameter files, and the generated alignments,
are available at http://github.com/bio-ontology-research-
group/OAEI2016. Code to generate the PhenomeNET
ontology is available at https://github.com/bio-ontology-
research-group/phenomeblast/tree/master/fixphenotypes.
Results
Combining knowledge-based and lexical approaches for
ontology integration
We developed and extended the PhenomeNET ontology
to integrate several species-specific phenotype ontolo-
gies and identify mappings between phenotype classes.
Here, we consider a mapping between two classes (in
two ontologies) a formal relation between them, i.e., an
axiomatic relation such as equivalence, sub- or super-
class, or disjointness. An alignment between two ontolo-
gies is created by a set of mappings. Ontology matching
is the process of finding mappings between classes in two
ontologies. Ontology integration goes beyond identifica-
tion of an ontology alignment in that two or more ontolo-
gies are merged into a single ontology that encompasses
all classes in the original ontologies [14].
Phenotype classes in the HP and MP ontologies
are formally defined using the Entity-Quality (EQ)
pattern [4, 26]. Based on the EQ patterns, a phe-
notype is decomposed into an affected entity and
a quality that specifies how the entity is affected.
The Entity will usually be a class taken either from an
anatomy ontology or a physiology ontology. For exam-
ple, the phenotype class macroglossia (HP:0000158)
describes an anatomical abnormality and is defined as
equivalent to has part some (increased
size and (inheres in some tongue)and
(has modifier some abnormal)), relying on
the entity tongue (from the UBERON anatomy ontol-
ogy [9]) and the quality increased size (from PATO)
in its definition. The class abnormality of salivation
(HP:0100755) is a physiological abnormality and is
defined as equivalent to has part some (quality
and (inheres in some saliva secretion)
and (has modifier some abnormal)), where
saliva secretion is a class from the biological process
branch of the Gene Ontology (GO) [10].
The general pattern for defining a phenotype class in
both the HP and MP ontologies, given Entity E and Qual-
ity Q, is to declare them equivalent to has part
some (Q and inheres in some E). In some
cases, the Entity E is further constrained, e.g., by a loca-
tion in which a certain process may happen. The E
classes are generally taken either from the UBERON
cross-species anatomy ontology [9] or from the GO. As
the use of anatomy and physiology ontologies (UBERON
and GO) is shared between MP and HP, it is possible to
integrate both ontologies directly, based on the axiom pat-
terns used to constrain their classes. However, the type of
axiom pattern used in both ontologies results in a clas-
sification that is primarily based on the PATO ontology,
as the Quality Q is the main feature that distinguishes
different classes.
In the PhenomeNET ontology, we rewrite all axioms in
HP andMP using a pattern-based approach that allows us
to utilize axioms from anatomy and physiology ontologies
and enrich the classification of phenotype classes [11, 27].
In general, we declare phenotype classes defined using an
Entity E and Quality Q as equivalent to has part
some (E and has-quality some Q) and we fur-
ther add grouping classes that are defined as equivalent
to has part some ((part of some E)and
has-quality some Quality). For example, based
on the axiom that defines macroglossia (HP:0000158)
as equivalent to has part some (increased
size and (inheres in some tongue) and
(has modifier some abnormal)), we gener-
ate two new axioms: macroglossia Equivalent
To: has part some (tongue and has-quality
some increased size) as well as tongue
abnormality EquivalentTo: has partsome
((part of some tongue) and has-quality
some Quality). These two axioms, together with the
transitivity and reflexivity of the part of relation,
ensure that macroglossia becomes a subclass of tongue
abnormality, and that all phenotypes affecting the tongue
or a part of the tongue also become a subclass of tongue
abnormality. The aim of rewriting the axioms is to
base the classification of phenotype classes primarily on
anatomical or physiological entities instead of the quality,
and to utilize the axioms involving parthood in anatomy
and physiology ontologies [11, 28]. Crucially, all axioms
we generate fall in the OWL 2 EL profile [29, 30] and allow
efficient automated reasoning using optimized OWL 2 EL
reasoners such as ELK [31]. The first version of the Phe-
nomeNET ontology (PhenomeNET-Plain) consists only of
these axioms and no additional mappings.
In addition to this knowledge-based approach to linking
the HP and MP ontologies, we also add lexical mappings,
Rodríguez-García et al. Journal of Biomedical Semantics  (2017) 8:58 Page 4 of 11
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 
DOI 10.1186/s13326-017-0166-5
REVIEW Open Access
Experiences from the anatomy track in
the ontology alignment evaluation initiative
Zlatan Dragisic, Valentina Ivanova, Huanyu Li and Patrick Lambrix*
Abstract
Background: One of the longest running tracks in the Ontology Alignment Evaluation Initiative is the Anatomy track
which focuses on aligning two anatomy ontologies. The Anatomy track was started in 2005. In 2005 and 2006 the task
in this track was to align the Foundational Model of Anatomy and the OpenGalen Anatomy Model. Since 2007 the
ontologies used in the track are the Adult Mouse Anatomy and a part of the NCI Thesaurus. Since 2015 the data in the
Anatomy track is also used in the Interactive track of the Ontology Alignment Evaluation Initiative.
Results: In this paper we focus on the Anatomy track in the years 20072016 and the Anatomy part of the Interactive
track in 20152016. We describe the data set and the changes it went through during the years as well as the
challenges it poses for ontology alignment systems. Further, we give an overview of all systems that participated in
the track and the techniques they have used. We discuss the performance results of the systems and summarize the
general trends.
Conclusions: About 50 systems have participated in the Anatomy track. Many different techniques were used. The
most popular matching techniques are string-based strategies and structure-based techniques. Many systems also
use auxiliary information. The quality of the alignment has increased for the best performing systems since the
beginning of the track and more and more systems check the coherence of the proposed alignment and implement
a repair strategy. Further, interacting with an oracle is beneficial.
Keywords: Ontology alignment, Biomedical ontologies, Ontology alignment evaluation initiative
Background
In recent years many ontologies have been developed and
many of those contain overlapping information. Knowl-
edge of the inter-ontology relationships is important in
many cases. One example case is when we want to use
multiple ontologies, e.g., companies may want to use com-
munity standard ontologies and use them together with
company-specific ontologies. Other example cases are
integration, search and analysis of data in an environ-
ment where different data sources in the same domain
have been annotated with different but similar ontolo-
gies. It has been realized that this is a major issue and
much research has been performed on ontology align-
ment, i.e., finding mappings or correspondences between
concepts and relations in different ontologies [42]. The
research field of ontology alignment is very active with its
*Correspondence: patrick.lambrix@liu.se
Department of Computer and Information Science and Swedish e-Science
Research Centre, Linköping University, Linköping, Sweden
own yearly workshop as well as a yearly event, the Ontol-
ogy Alignment Evaluation Initiative (OAEI, http://oaei.
ontologymatching.org/, e.g., [41]), that focuses on evalu-
ating systems that automatically generate correspondence
suggestions. Many systems have been built and overviews
are found in [87, 99, 123, 144, 145] and at the ontol-
ogy matching web site http://www.ontologymatching.org.
The proceedings of the yearly Ontology Matching work-
shop contain descriptions of the systems participating
in the OAEI as well as summary papers discussing the
performance results for these systems in the OAEI.
One of the longest running tracks in the OAEI is the
Anatomy track which focuses on two ontologies from the
biomedical domain. This domain is one of the earliest
adopters of ontologies and a number of large ontologies
have been developed and are maintained. This domain
manages large volumes of high-complexity data with intri-
cate relationships. Focusing on a particular domain allows
the tools to exploit its inherent properties (for instance,
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 2 of 28
it limits the possible meanings of concept labels) and to
exploit existing resources as background knowledge. The
Anatomy track was started in 2005. In 2005 and 2006 the
task in this track was to align the Foundational Model
of Anatomy and the OpenGalen Anatomy Model. Since
2007 the ontologies used in the track are the Adult Mouse
Anatomy and a part of the NCI Thesaurus. Since 2015 the
data in the Anatomy track is also used in the Interactive
track of the OAEI.
In this paper we focus on the Anatomy track in the
years 20072016 and the Anatomy part of the Interactive
track in 20152016. We describe the data set (ontolo-
gies and reference alignment) and the changes it went
through during the years as well as the challenges it
poses in OAEI anatomy data and tasks Section. Fur-
ther, we give an overview of all systems that participated
during these years in the Anatomy track and the tech-
niques they have used (Participating systems Section).
We discuss the performance results of all systems that par-
ticipated during these years in the Anatomy track task 1
(Results in the OAEI anatomy track - task 1 Section),
tasks 2 and 3 (Results in the OAEI anatomy track - task
2 and 3 Section), task 4 (Results in the OAEI anatomy
track - task 4 Section) as well as in the Anatomy part
of the Interactive track (Results in the OAEI interac-
tive track - anatomy Section). We note that we do not
show all the performance results of the individual sys-
tems over the years, but instead summarize the general
trends. Our paper focuses on the whole period that the
track was organized and deals with trends and overviews
and multiple systems over the years rather than with
individual systems. For results of the individual systems
we refer to http://oaei.ontologymatching.org/ as well as
the OAEI summary papers1 in the proceedings of the
Ontology Matching workshops. Further, we summarize
our observations2 and discuss some possible improve-
ments and changes for the Anatomy track in Conclusion
Section. We start however with some general information
about ontology alignment and the evaluation of ontology
alignments.
Ontology alignment and ontology alignment
evaluation
In this section we give some background on ontology
alignment. We describe a framework for such systems as
well as the measures that are usually used for measuring
the performance of ontology alignment systems.
Ontology alignment
Many ontology alignment systems, although not all, are
based on the computation of similarity values between
entities in different ontologies and can be described as
instantiations of the general framework in Fig. 1. The
framework consists of two parts. The first part (I in Fig. 1)
computes correspondence suggestions (sometimes called
mapping suggestions or candidate mappings). The second
a
l
i
g
n
m
e
n
t
o
n
t
o
l
o
g
i
e
s
general
dictionary
instance
corpus
domain
thesaurus
matcher
matcher
matcher
Preprocessing
checker
conflict
user
II
I
accepted and
suggestions
rejected
filter
combination
suggestions
mapping
Fig. 1 Ontology alignment framework (e.g., [95])
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 3 of 28
part (II) interacts with the user to decide on the final
alignment (partly evaluated in the Interactive track). An
alignment algorithm receives as input two source ontolo-
gies. Part I typically contains different components. A
preprocessing component can be used to modify the orig-
inal ontologies, e.g., to partition the ontologies into map-
pable parts thereby reducing the search space for finding
correspondence suggestions. The algorithm can include
several matchers that calculate similarities between the
entities from the different source ontologies or mappable
parts of the ontologies. They often implement string-
based, structure-based, constraint-based and instance-
based strategies, as well as strategies that use auxiliary
information or a combination of these. Correspondence
suggestions are then determined by combining and fil-
tering the results generated by one or more matchers.
Common combination strategies are the weighted-sum
and the maximum-based strategies. The most common
filtering strategy is the (single) threshold filtering. By using
different preprocessing, matching, combining and filter-
ing techniques, we obtain different alignment strategies.
The result of part I is a set of correspondence suggestions.
In part II the suggestions are then presented to the user, a
domain expert, who accepts or rejects them. The accepted
suggestions are part of the final alignment. In an inter-
active system the acceptance and rejection of suggestions
may also influence further suggestions. Further, in parts I
(not in the figure) and II reasoning may be used to check
for conflicts and incoherence (see below) and the sug-
gested alignment (and ontologies) may be repaired. There
can be several iterations of parts I and II. The output of the
alignment algorithm is a set of correspondences between
entities from the source ontologies.
Performance measures
The performance of the systems in the OAEI has typically
been evaluated using measures related to the quality of
the alignment suggested by the systems (precision, recall
and F-measure with respect to a reference alignment) as
well as the run time of the systems. The precision of a
system is the ratio of the number of correctly suggested
correspondences by the system to the number of sug-
gested correspondences by the system. The recall of a
system is the ratio of the number of correctly suggested
correspondences by the system to the number of correct
correspondences according to the reference alignment. F-
measure is a harmonic mean between precision and recall
and is defined as:
F? = (1 + ?) precision · recall
? · precision + recall
In addition to these measures the Anatomy track has
also computed the recall+ of the systems. As anatomy
ontologies often contain similar names, even for different
species [64], it is expected that a matcher based on string
similarity should dowell. Therefore, such amatcher, called
StringEquiv, that combines a normalization step and exact
string matching, was implemented. The resulting correct
suggestions of this matcher were called trivial correspon-
dences and used as a baseline for recall+. In the most
recent reference alignment there are 946 such correspon-
dences out of a total of 1516 correspondences. The recall+
of a system is the recall of the system on the part of the ref-
erence alignment that was not found by StringEquiv and
measures thus how well the system finds non-trivial cor-
respondences. According to this definition the recall+ of
StringEquiv is equal to 03.
Anothermeasure is the coherence of the suggested align-
ment. An alignment is said to be coherent if the merged
ontology containing the original ontologies (in this case
AMA andNCI-A) and the alignment is coherent, i.e., does
not contain unsatisfiable4 concepts.
The data from the Anatomy track is also used in the
OAEI Interactive track where a user is simulated using an
oracle. In addition to the performance measures above,
also the number of requests to the oracle is used.
OAEI anatomy data and tasks
In this section we describe the data sets (ontologies and
reference alignment) and their histories as well as the tasks
in the Anatomy and Interactive tracks of the OAEI, the
particular challenges that this track poses to the alignment
systems and the evaluation procedure.
Ontologies and reference alignment
Ontologies
The Adult Mouse Anatomy ontology (AMA) is a part of
the Gene Expression Database5 and provides a spatial and
functional organization of adult mouse anatomical struc-
tures6. The National Cancer Institute (NCI) Thesaurus7
contains more than 100 000 concepts and covers a broad
range of topics in cancer research and clinical care. In the
OAEI we use a fragment of the NCI Thesaurus containing
information about the human anatomy (NCI-A).
In Table 1 we show the evolution of the ontologies
used in the Anatomy track. The 2007 version of AMA
contained 2744 concepts and 3 object properties. It con-
tained around 4500 subsumption axioms (is-a relations).
NCI-A contained 3304 concepts and 2 object proper-
ties. There were around 5500 subsumption axioms. The
knowledge representation language used for both ontolo-
gies was ALE . Both ontologies contained a large number
of annotation axioms (AMA - ca 3500, NCI-A - ca 15000).
Annotation axioms provide additional information such
as provenance information (e.g., creator and owner). In
the case of AMA and NCI-A these annotation axioms
included properties such as hasSynonym, hasRelatedID
and hasDefinition.
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 4 of 28
Table 1 Evolution of AMA and NCI-A and the reference alignment
AMA NCI-A Reference alignment
2007 2744 concepts, 3304 concepts, 1544 equivalence relations
3 object properties, 2 object properties,
ca 4500 subsumption axioms ca 5500 subsumption axioms
2008 Same as earlier Same as earlier Removed 20 correspondences
2010 Added 12 subsumption axioms Added 3 subsumption axioms Weakened 2 correspondences
Removed 6 subsumption axioms Removed 3 subsumption axioms Removed 1 correspondence
Added 17 disjointness axioms
2011 Same as earlier Same as earlier Added 28 correspondences
Removed 24 correspondences
The ontologies were changed in 2010. In AMA 12
new subsumption axioms were added and 6 subsump-
tion axioms were removed while in NCI-A 3 subsump-
tion axioms were added and 3 subsumption axioms were
deleted. In addition, 17 disjointness axioms were added
to the NCI ontology. This required the more expressive
knowledge representation languageALC for NCI-A.
Being developed by different teams and with different
purposes inmindAMA andNCI-A exhibit different prop-
erties with respect to their structure. Table 2 compares
the 2016 versions of the ontologies used in the Anatomy
track. The ontologies are comparable in number of con-
cepts but exhibit a large difference in terms of maximum
Table 2 Comparison between AMA and NCI-A
AMA NCI-A
# of concepts 2744 3304
# of direct subconcepts of
owl:Thing
1056 7
Maximum depth of the is-a
hierarchy
9 13
# equivalent concepts 0 0
# of inner concepts 483 674
# of leaf concepts 2261 2631
Maximum number of direct
subconcepts
129 125
# of concepts with one subconcept 74 125
# of concepts with multiple
superconcepts
110 277
Average leaves depth
(= (sum leaf concepts depth)/
(# leaf concepts)):
3 6
Average depth (= (sum concepts
depth)/(# concepts)):
3 6
Average number of subconcepts
(only concepts with subconcepts):
5 5
Average number of subconcepts
(all concepts):
1 1
and average depth of leaf concepts. The AMA structure
is flatter and approximately a third of the concepts are
directly under owl:Thing. NCI-A has a deeper organiza-
tion and the average depth of concepts for NCI-A is twice
as large as for AMA. These two ontologies share a large
number of lexically similar labels.
Reference alignment
The alignment between AMA andNCI-A was undertaken
as part of a project to enable linking data between them.
The alignment was developed by using automatic tools
as well as a manual approach. As a first step a simple
lexical comparison, a preliminary manual comparison by
domain experts as well as an approach combining lexi-
cal and structural similarity were used [64]. The lexical
component in the latter approach uses normalization of
terms, exact matching and synonyms from the Unified
Medical Language System (UMLS)8 Metathesaurus, while
the structural component is used as a verification step
where only correspondence suggestions whichmake sense
with respect to the structure of the ontologies are retained
[6]. The results of the first step were manually validated
by domain experts and resulted in 830 correspondences.
Further, a number of tools (DAG-OBO-edit [26], Protégé-
OWL [124] and COBrA [3]) were selected and used for
a further comparative analysis of AMA and NCI-A. It
was found that most differences between the ontologies
came from design decisions of the hierarchical organiza-
tion, the coverage of the ontologies and the granularity of
the ontologies. Based on this analysis a certain harmoniza-
tion and extending of the ontologies was performed. This
resulted in the versions of the ontologies that were used in
the OAEI, and the initial OAEI reference alignment9 that
contained 1544 equivalence relations (see Table 1).
The reference alignment was modified in 2008 to
remove 20 correspondences between concepts which
were not part of the ontologies. In 2010, the reference
alignment was slightly modified by weakening 2 corre-
spondences (transforming them into subsumption rela-
tions) and removing 1 correspondence. The changes were
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 5 of 28
done mostly to produce a coherent alignment as with the
pre-2010 versions of the ontologies and the pre-2010 ref-
erence alignment the merged ontology containing AMA,
NCI-A and the reference alignment was incoherent. The
subsumption correspondences were never used in the
evaluations. The latest changes in the reference alignment
were made in 2011 - 28 correspondences were removed
from the reference alignment and 24 new correspon-
dences were added.
In recent years, there have been a number of works,
e.g., [5, 64, 71, 97, 98, 102], as well as some personal cor-
respondence10 which suggested the existence of missing
and wrong is-a relations in the ontologies and missing
and wrong correspondences in the reference alignment.
However, the evaluation of suchmistakes requires domain
expertise and so far there has not been such an effort after
the latest changes in 2011.
Tasks
During the years different tasks were introduced in the
track:
 Task 1: Align AMA and NCI-A and optimize
F-measure.
 Task 2: Align AMA and NCI-A and optimize
F-measure with a focus on precision.
 Task 3: Align AMA and NCI-A and optimize
F-measure with a focus on recall.
 Task 4: Given a partial reference alignment consisting
of all trivial correspondences and 50 non-trivial
correspondences, align AMA and NCI-A and
optimize F-measure.
 Interactive track: Using an oracle (which may make
mistakes), align AMA and NCI-A and optimize
F-measure.
In the definition of F-measure, tasks 1, 4 and the inter-
active track use ? = 1, while task 2 uses ? = 5 and task 3
uses ? = 0.2.
Task 1 has been used in all editions of the OAEI
Anatomy track (20072016). Tasks 2 and 3 were part of
the track during 20072010, while task 4 was included in
2008201011. Since 2011 the coherence of the suggested
alignment is checked. Tasks 14 deal mainly with the non-
interactive part of an ontology alignment system (part I in
Fig. 1).
Since 2015 the data from the Anatomy track is used
in the OAEI Interactive track (run since 2013) which
aim is to evaluate the influence of user involvement for
interactive alignment tools. It is a first12 step towards an
evaluation of part II in Fig. 1. In the track users are repre-
sented by an oracle and tools can ask the oracle about the
correctness of correspondence suggestions and use this
information in the generation of other correspondence
suggestions.
Challenges
In the early years the Anatomy track contained the largest
ontologies and was therefore the track that evaluated scal-
ability of the systems. Nowadays, these ontologies are
considered to be medium-sized.
As the two ontologies share a large number of lexi-
cally similar labels, string matching-based algorithms do
quite well. Therefore, most systems use such algorithms.
The challenge is, however, to combine these kinds of
matchers with other types of matchers to improve the
results. Therefore, StringEquiv was used as a baseline
matcher to measure the influence of the other types of
matchers. Combining matchers in an effective way is
not easy and several systems did perform worse than
StringEquiv.
As shown in Table 2 the is-a structure of the two
ontologies is quite different. One challenge is, therefore,
to develop structure-based approaches that can deal with
different is-a structure and granularity.
The track allows the use of background information.
Systems need to find appropriate external sources and use
them effectively. These external sources may be domain
specific or contain general information. The sources may
also be incomplete and contain errors.
Task 4 was the only task in any of the OAEI tracks
that evaluated the use of a given partial reference align-
ment in the computation of new correspondence sug-
gestions. The partial reference alignment could be used
in the preprocessing, computation or filtering compo-
nents of the systems and new strategies needed to be
developed. Task 4 was, however, a difficult task. As the
trivial correspondences are given, string-based match-
ing does not give an improvement. Further, given the
fact that the partial reference alignment contains only
a few non-trivial correspondences, machine learning-
based matchers are likely to fail. As the is-a struc-
ture of AMA and NCI-A is not complete, structure-
based approaches can also not be used to their full
potential.
In the Interactive track there are several challenges.
The first challenge is to develop strategies for deciding
which correspondence suggestions to show to the oracle.
These questions should be important for the quality of the
final alignment. However, there should not be so many
questions as to overload the oracle. There should also be
not too much waiting time between the questions. Then
strategies for using the validation decisions of the oracle
should be developed. This is similar to task 4, but in this
case the system has decided which correspondences could
be part of the partial reference alignment and addition-
ally, there are also validation decisions about non-correct
correspondences. A further challenge in this track is that
the systems need to deal with an oracle that may make
mistakes.
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 6 of 28
Evaluation procedure
In the period 20072010 the full reference alignment was
not publicly available and all tests were done blind. The
authors of the tools were provided with the ontologies
and were asked to produce an alignment which was then
sent to the organizers of the track. The organizers would
then evaluate and compare the performance of the tools.
In 2010 the SEALS platform13 was introduced in the eval-
uation process for the Anatomy track. SEALS provides
an evaluation framework where participants register and
upload their tools to the portal. While the reference align-
ment was still not available, the tools could be run through
SEALS and the results for the tool would be directly avail-
able. The use of SEALS also meant that the organizers
could publish certain tests while keeping other tests blind.
In addition to receiving the results directly, the fact that
the tools were required to be uploaded made it possible to
run all tools on a single hardware which made the com-
parison of run times possible. Since 2011 the reference
alignment has been publicly available.
Initially, the authors of the tools could decide in which
track to participate, which made it possible to have spe-
cialized tools for certain type of task, e.g., matching
biomedical ontologies. However, from 2011 all tools are
evaluated in all tracks.
Participating systems
In this section we give an overview of the participation in
the Anatomy track and discuss the techniques used by the
different systems.
Participation
In total 50 different tools (not including different versions
of the tools) have been evaluated from 2007 to 2016 in the
Anatomy track. The numbers of participants for specific
years is given in Table 3. During 20072011 around 10
tools participated each year. During 20122016 the num-
ber of participants has varied from 20 tools in 2013 to 10
tools in 2015.
Tables 4 and 5 show the participants and the years in
which they participated. The table lists only the participa-
tions in the Anatomy track. During the years that systems
were allowed to choose tracks, some systems may have
chosen to participate in Anatomy during some years, and
not during other years. The latter are not taken up in the
table. Further, we only mark a participation in the case
of a successful evaluation, i.e, the system returned results
within the for that year predefined time frame.
Half of the systems has participated more than once.
The tools with the most participations (6) are Lily
and LogMap. Seven tools have participated 4 times,
6 tools 3 times and 10 tools twice. In the recent
instances of the track we can observe an increase in
tools which participate with different versions, such as
Table 3 Number of participating systems in the OAEI Anatomy
track during 20072016
Year Number of distinct tools Number of tools including
different versions
2007 11 11
2008 8 9
2009 10 10
2010 10 10
2011 10 11
2012 14 17
2013 16 20
2014 5 10
2015 11 15
2016 10 13
lightweight versions or versions which use background
knowledge.
Alignment techniques
For the overview of the systems in this section we used
the papers describing the systems in the OAEI parts of the
proceedings of the yearly Ontology Matching workshop.
In the case we needed some clarifications we have also
looked at the papers referenced in the OAEI papers. For
the overview of string-based matchers we also used [10].
We note that some of the participants in the earlier years,
may have newer versions of the systems that have features
that are not discussed in this paper.
In Table 6 we show the different components of the
participating systems. All systems implement part I while
some also implement part II and allow iterations. Many
systems do some kind of preprocessing. In most of the
cases the preprocessing step deals with preparing data
for the matchers. In other cases the systems partition
the ontologies to reduce the search space for the match-
ers. All systems have a matching component and these are
discussed shortly. The combination strategies are usu-
ally weighted sum (most common) or maximum-based
approaches. Some systems use a more advanced approach
where the weights for the weighted sum are selected using
a neural network (CIDER-CL, X-SOM, XMAP) or a genetic
algorithm (XMAPGen), using the overlap between the
results of the different matchers (CroMatcher), or using a
clustering algorithm (CSA). Most filtering is performed
using a single threshold. SAMBOdtf and X-SOM use a
double threshold filtering approach where the correspon-
dences with similarity values between the thresholds are
checked with respect to the structure of the ontologies,
or are requested to be validated by a user, respectively.
Lily uses a maximum entropy approach to calculate a
Dragisic et al. Journal of Biomedical Semantics  (2017) 8:56 Page 7 of 28
Table 4 Participating systems (with different versions) in the OAEI Anatomy track 20072016 (part 1)
System 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
AgreementMaker [14] [147] [15] [13] [16]
ALIN [56] [19]
AML, AML_bk2013 [50, 51] [49] [47] [46] [48]
Anchor-Flood [143] [141] [142]
AOAS [6, 169] [168]
AOT, [NA]
AOTL [91]
AROMA [22] [23] [24] [25] 
ASMOV [77] [74] [75] [76] [78]
BLOOMS [NA] [129]
CIDER-CL [155] [53]
CODI [121] [122] [69] 
COMMAND [113] 
CroMatcher [58] [57] [59]
CSA [NA] [154]
DKP-AOM, [45]
DKP-AOM-Lite [43] [44]
DSSim [114] [115] [117] [116]
Eff2Match [NA] [12]
Falcon-AO[67] [68]
FCA_Map[174] [173]
GeRoMeSuite+SMB [89] [130]
GMap [104] [105]
GOMMA, [92]
GOMMA-bk  [55] 
Hertuda [NA] [65] 
HotMatch [NA] [21] 
IAMA [NA] [172]
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 
DOI 10.1186/s13326-017-0152-y
RESEARCH Open Access
Towards refactoring the Molecular
Function Ontology with a UML profile for
function modeling
Patryk Burek1, Frank Loebe2* and Heinrich Herre1
Abstract
Background: Gene Ontology (GO) is the largest resource for cataloging gene products. This resource grows steadily
and, naturally, this growth raises issues regarding the structure of the ontology. Moreover, modeling and refactoring
large ontologies such as GO is generally far from being simple, as a whole as well as when focusing on certain aspects
or fragments. It seems that human-friendly graphical modeling languages such as the Unified Modeling Language
(UML) could be helpful in connection with these tasks.
Results: We investigate the use of UML for making the structural organization of the Molecular Function Ontology
(MFO), a sub-ontology of GO, more explicit. More precisely, we present a UML dialect, called the Function Modeling
Language (FueL), which is suited for capturing functions in an ontologically founded way. FueL is equipped, among
other features, with language elements that arise from studying patterns of subsumption between functions. We
show how to use this UML dialect for capturing the structure of molecular functions. Furthermore, we propose and
discuss some refactoring options concerning fragments of MFO.
Conclusions: FueL enables the systematic, graphical representation of functions and their interrelations, including
making information explicit that is currently either implicit in MFO or is mainly captured in textual descriptions.
Moreover, the considered subsumption patterns lend themselves to the methodical analysis of refactoring options
with respect to MFO. On this basis we argue that the approach can increase the comprehensibility of the structure of
MFO for humans and can support communication, for example, during revision and further development.
Keywords: Gene Ontology, Molecular Function Ontology, Unified Modeling Language, Ontology, Function
decomposition, Intensional subsumption
Background
Gene Ontology (GO) [1, 2] is an important, widely used,
very large and continuously growing resource for cata-
loging gene products. In 2000 GO contained less than
5000 terms, which increased to circa 13,000 in 2003
[1], exceeded 30,000 in 2010 [3] and is close to 45,000
terms in July 2017 [2]. The Molecular Function Ontol-
ogy (MFO) is a sub-ontology of GO of more than 11,000
terms in 2017. This growth of the ontology leads to a sub-
optimal structure [3]. Clearly, the GO Consortium itself
is constantly improving and evolving GO. In addition,
*Correspondence: frank.loebe@informatik.uni-leipzig.de
2Computer Science Institute, University of Leipzig, Augustusplatz 10, 04109
Leipzig, Germany
Full list of author information is available at the end of the article
size and importance of the ontology and the recogni-
tion of problems have motivated refactoring initiatives,
see [4, 5], for example. Overall, it turns out that modeling
and refactoring large ontologies such as GO are diffi-
cult tasks, which should be supported by human-friendly
representations. Serialization formats used for machine
processing of ontologies, such as the OBO flat file for-
mat [6] or the Web Ontology Language (OWL) [7], are
not the easiest to be used by humans. This motivates
proposing the adoption of human-friendly graphical nota-
tions for certain purposes, like languages used in software
engineering, already employed for the task of ontology
representation [8, 9].
The Unified Modeling Language (UML) [10, 11], devel-
oped and maintained by the Object Management Group
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 2 of 11
(OMG) [12], is the de facto standard for graphical con-
ceptual modeling of software systems. Moreover, UML
has a high potential for various applications that go
beyond software engineering, among them modeling
biological knowledge and biological ontologies [4, 13],
for several reasons. First, there is a rich infrastruc-
ture. Numerous tools for UML modeling are available
on the market and can be used out of the box for
visualizing biological ontologies as a whole or in part.
Another advantage is its adaptability. UML is equipped
with extension mechanisms such as stereotypes and pro-
files, which support the easy construction of domain-
or task-specific UML dialects. For example, a UML
profile for the OBO relations ontology is proposed
in [4].
In the present paper we investigate if UML, more
precisely, a dedicated dialect, can be utilized for mak-
ing the structure of the Molecular Function Ontol-
ogy more explicit and if it can support the refactor-
ing of MFO. The focus on MFO within GO results
from having dealt with the notion of function from a
general point of view in earlier work, e.g. [14]. The
Methods section establishes foundations by sketching
some features of functions in MFO, describing an inten-
sional understanding of subsumption, and introducing the
Function Modeling Language (FueL) as a UML dialect
that is suited for function modeling. Concerning results,
section Modeling molecular functions with FueL intro-
duces core elements of FueL that are required to analyze
the subsumption patterns that section Patterns of func-
tion subsumption defines based on FueL. Then we are
prepared to illustrate the application of FueL to MFO
in the Application section by modeling the structure
of molecular functions and proposing some refactoring
options. The Discussion section is mainly devoted to
related work and to the applicability of FueL at this stage.
It further indicates directions of future work, before the
paper ends with section Conclusions.
Methods
Molecular Function Ontology
Like all GO terms, functions in MFO are specified by
id, name, natural language definition and an optional
list of synonyms. For instance, the function of catalyz-
ing carbohydrate transmembrane transport is specified
by id: GO:0015144; name: carbohydrate transmembrane
transporter activity; definition: catalysis of the transfer of
carbohydrate from one side of the membrane to the other;
synonym: sugar transporter. Additionally, for each func-
tion its relations with other concepts can be captured. The
semantics of the relations that are used for this purpose
is provided by serialization languages such as the OBO
flat file format or OWL, and/or by the OBO relations
ontology (RO) [15]. In particular, functions in MFO are
organized into a hierarchy by means of the is_a link from
RO; furthermore, they are linked with processes by the
part_of relationship from RO; and in some cases they have
relations with concepts of other ontologies such as ChEBI
[16]. For instance, GO:0015144 is linked, by means of the
RO is_a relation, to its parent functions GO:1901476 car-
bohydrate transporter activity and GO:0022891 substrate-
specific transmembrane transporter activity, by means
of the RO part_of relation to the process GO:0034219:
carbohydrate transmembrane transport, and by means
of the RO transports_or_maintains_localization_of to
CHEBI:16646: carbohydrate.
From the above we see that the semantics of functions
in MFO is provided to a large extent by informal natural
language expressions and partially by relations with other
concepts.
Intensional subsumption
We propose defining the notion of function subsump-
tion, which is a backbone of MFO, upon an intensional
interpretation of the is_a relation. Typically, in the field
of ontology engineering the extensional aspect of the is_a
relation is stressed; in OWL, for instance, A is a sub-
class of B if every instance of A is an instance of B. The
same interpretation is used in RO, where is_a is defined
by the reference to the sets of all instances (extensions)
of the concepts. According to this understanding the is_a
relation is often called extensional subsumption, in con-
trast to its intensional counterpart(s), where we focus on
structural subsumption [17].
Instead of referring to instances, structural subsumption
is defined based on the structure of a concept. The latter
can be understood as a composition of conceptual parts
by means of various composing relations. For illustration
within GO itself, GO:0005215: transporter activity is jus-
tified to intensionally subsume GO:0022857: transmem-
brane transporter activity, because, following [17], both
are activities and they are (partially) defined by part_of
relations to GO:0006810: transport and to GO:0055085:
transmembrane transport, resp., and the latter is sub-
sumed by the former. Overall, the main assumption is that
concepts are complex structures which can be organized
into a subsumption hierarchy. The reading of intensional
subsumption is similar to inheritance in object-oriented
languages, where one class inherits its structure from
another. That enables the structuring of classes into hier-
archies. Note that extensional and intensional subsump-
tion need not be seen to be in conflict with each other,
but they can be understood as different facets of the
hierarchical organization of classes.
UML, UML profiles and FueL
The Unified Modeling Language (UML) [10, 11] is a
rich graphical modeling language developed originally
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 3 of 11
for the support of software engineering. Currently, its
applications go beyond software engineering, covering
a broad spectrum of domains, including systems and
enterprise modeling, as well as biological systems mod-
eling. The language is founded on an explicit distinction
between the static and the dynamic views of a system.
It introduces thirteen diagram types, grouped into two
sets: structural modeling diagrams and behavioral model-
ing diagrams. UML lacks constructs dedicated to function
modeling as such [18], but it provides several built-
in mechanisms that allow for an easy extension of the
language.
Among these extension mechanisms there are UML
profiles. A profile is a light-weight UML mechanism,
typically used for extending the language for par-
ticular platforms, domains or tasks [11, ch. 12]. It
specifies a set of extensions of the UML standard
metamodel which include, among others, stereotypes.
With stereotypes it is possible to extend the stan-
dard UML vocabulary with new, specialized model ele-
ments. A stereotype can be graphically represented
by a dedicated icon, though in the most straightfor-
ward form it is represented simply by a stereotype
name, surrounded by guillemets and placed above the
name of the stereotyped UML element, cf. «Function»
in Fig. 1.
We used the profile mechanism for developing a UML
extension, called Function Modeling Language (FueL)1,
aimed at supporting the modeling of functions, func-
tion ascription, and function decomposition. FueL defines
15 stereotypes for representing functions and function
structure, as well as 8 stereotypes for modeling function
decomposition, subsumption and function dependencies.
The full specification of FueL stereotypes is available in
[19]. Burek et al. [18] provides a detailed introduction
to FueL, based on requirements for function model-
ing derived from an elaborate review of corresponding
literature, in general, as well as of UML modeling con-
structs related to functions, in particular. In addition
to the profile, [18] comprises an axiomatic characteri-
zation of the core elements of FueL and discusses its
suitability for function modeling with respect to the
requirements identified.
In the remainder of the current paper we analyze to
which extent FueL can be used for modeling and refactor-
ingMFO. As a prerequisite for this analysis, we begin with
a condensed account of FueL.
Results
Modeling molecular functions with FueL
FueL enables the graphical modeling of functions both in
a compact and in an extended form. The compact form
is particularly suited for large models containing many
functions, whereas the extended form is designed for visu-
alizing the dependencies within the structure of a single
function or between several functions. Figures 1 and 2
present an exemplary FueL model, depicting the structure
of MFO function GO:0015144: carbohydrate transmem-
brane transporter activity. Figure 1 presents the compact
notation, whereas the extended notation is shown in Fig. 2.
The stereotypes utilized in the figures are discussed in the
remainder of this section.
Functions
A function in FueL is understood as a role that an entity
plays in the context of some goal achievement, e.g. in
a teleological process. Put differently, a role in virtue
of which the transition to a goal situation is achieved,
or which contributes to such achievement, constitutes a
function. An entity, like putative glucose uptake protein in
Fig. 2, that plays such a role has that role as its function.
This account of functions is similar to [20], where a bio-
logical function of a molecule is described as the role that
the molecule plays in a biological process. In this sense,
the function GO:0015144: carbohydrate transmembrane
transporter activity, defined in GO as catalysis of the
transfer of carbohydrate from one side of the membrane
to the other, depicts the catalyst role in the teleological
process of transferring carbohydrate from one side of the
membrane to the other.
In terms of the structure we can therefore say that a
function specification contains as its part a specification
of a goal achievement, understood as a teleological entity
which is specified in terms of a transformation from an
input situation to an output situation. As presented in
Fig. 1 A FueL model of a molecular function, displayed in the compact notation
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 4 of 11
Fig. 2 A FueL model of a molecular function, displayed in the extended notation
Figs. 1 and 2, a function is depicted by a UML classi-
fier with a stereotype «Function». It connects to its goal
achievement by an association with a stereotype «has-
goal-achievement» in the extended notation, whereas the
compact notation utilizes the attribute goal_achievement.
Goal achievements
In FueL, a goal achievement (GA) is defined as a teleo-
logical transition, i.e., as a transition to a certain output
situation (the goal). Note that transitions further exhibit
an input situation. The GA characterization applies at
both the individual and categorial level. With respect to
the latter, input and output are defined as follows:
 The input category x of goal achievement y is a
situation category such that every instance of y is a
transition starting from a situation instantiating x.
 The output x of goal achievement y is a situation
category specifying the situations in which instances
of y result by transition. Every instance of y is a
transition resulting in a situation instantiating x.
For example, the goal achievement (category) carbo-
hydrate transmembrane transport establishes the input
category, the instances of which are situations of carbo-
hydrate being on one of the two sides of the membrane,
and the output category, the instances of which are sit-
uations of carbohydrate being on the other side of the
membrane. This means that every instance of carbohy-
drate transmembrane transport exhibits a transition from
an instance of the input category to an instance of the
output category, i.e. from individual situations of carbohy-
drate located on one side of the membrane, to individual
situations of carbohydrate located on the other side of the
membrane.
In the compact notation, the input is captured by the
input attribute of a function, see Fig. 1. In contrast, Fig. 2
illustrates that an association with stereotype «has-input»
is used for connecting a function with its input in the
extended notation. The representation of outputs is anal-
ogous in both variants.
Typically, a transformation from an input to an output
situation is a process. At the categorial level, the GA can
then be understood as a process category. In the running
example, the GA is a teleological process category, namely
of carbohydrate transfer from one side of the membrane
to the other. This process exhibits the causal transition
from the situation of carbohydrate being on one side of
the membrane to the situation where carbohydrate is on
the other side of the membrane.
Mode of goal achievement
In some cases the specification of a function is not
reduced to a mere input-output pair, but it defines con-
straints on the method of function realization. For exam-
ple, the molecular functions GO:0015399: primary active
transmembrane transporter activity and GO:0015291: sec-
ondary active transmembrane transporter activity share
the same input: solute is on one side of the membrane,
and the same output: solute is on the other side of the
membrane. Therefore, the pure input-output views of
the functions are equal. However, they are distinct due to
the way in which they achieve the goal. The former func-
tion is realized by means of some primary energy source,
for instance, a chemical, electrical or solar source, whereas
the latter relies on a uniporter, symporter or antiporter
protein. Thus we see that the functions provide the same
answer to the question on what is to be achieved, how-
ever they provide different answers on how that is realized.
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 5 of 11
In order to represent this distinction, in FueL we intro-
duce another component of function structure, called
Mode of Goal Achievement (or Mode of Realization). The
mode x of the goal achievement y specifies the way in
which y transforms the input to the output situation. For
GO:0015399 the mode is: by some primary energy source,
for instance chemical, electrical or solar source, and for
GO:0015291 it is: by uniporter, symporter or antiporter
protein. The mode is a constraint on the function realiza-
tion, which does not affect the input or the output. For
example, if one adds to the function of transmembrane
transport the constraint that the transport should be real-
ized by the uniporter protein, then the input and the
output remain unchanged. However, the function as such
changes in that not every transportation process realizes
it, but only those that are driven by a uniporter protein.
Participants
Often goal achievements are expressed by action sen-
tences of natural language and thus the results of linguistic
analysis of action sentences can be applied to the analysis
of the structure of goal achievements. In linguistics, the
role that a noun phrase plays with respect to the action
or state described by the verb of a sentence is called a
thematic role [21]. The specifications of molecular func-
tions in MFO often contain two thematic roles  a patient
(called an operand in FueL) and an actor (called a doer
in FueL). An operand indicates the entity undergoing the
effect of the action. At the categorial level we say that
an operand y of the goal achievement x specifies a cate-
gory y such that instances of x operate on instances of y.
GO:0015144 operates on (transports) carbohydrate.
A doer is not as common in MFO as an operand. For
example, in the discussed carbohydrate transmembrane
transport function no doer is indicated. Typically, a doer
is a part of the GA in cases where the mode of realization
is provided. For instance, the functions GO:0015292: uni-
porter activity and GO:0015293: symporter activity both
specify the mode of realization and each indicates its doer,
namely the respective protein.
Patterns of function subsumption
Behind function subsumption various distinct relations
are actually implicitly hidden [14]. In this section we
introduce three patterns for function subsumption that
can be indicated by FueL stereotypes [19]. The subse-
quent Application section demonstrates the application
of those patterns to the modeling of MFO.
In FueL the notion of function subsumption is founded
on the subsumption of goal achievements. We say that
the function x is subsumed by the function y if the goal
achievement of x is subsumed by the goal achievement of
y. Since goal achievements are quite complex entities, it
is not trivial to answer the question of what it means that
one goal achievement subsumes another. Here, however,
the analysis of GA structure is helpful, which pertains to
the intensional aspects of the corresponding GA category,
as discussed in previous sections. Based on this approach
one can detect various patterns of function subsumption.
Operand specialization
Since function specifications often contain operands, it is
very common to construct a hierarchy of functions on the
basis of the taxonomic hierarchy of their operands. In fact,
this pattern is applied frequently in MFO. Consider, for
instance, the functions GO:0015075: ion transmembrane
transporter activity and GO:0008324: cation transmem-
brane transporter activity, linked by the is_a relation in
GO. As presented in Fig. 3 the relation between those two
functions is based on the relation of their operands, as
cation is subsumed by ion.
Function subsumption by operand specialization is
depicted in FueL with a specialization link with the stereo-
type «operand-spec». The supplier of the link is the
subsumed function, the client is the subsumer.
Mode addition
Another pattern of function subsumption, frequently met
in MFO, is based on modes of goal achievement. Consider
two functions presented in Fig. 4, GO:0022857: trans-
membrane transporter activity and GO:0022804: active
transmembrane transporter activity. Both share the same
operand, namely substance, as well as the same input-
output pair  operand is on one side of the membrane
and operand is on the other side of the membrane. In
this sense those functions are equal. However, they dif-
fer in that the former does not define any mode of
realization, whereas the latter has the following mode
defined: the transporter binding the solute undergoes a
Fig. 3 An example of operand specialization
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 6 of 11
Fig. 4 An example of specialization by mode addition
series of conformational changes. Therefore, one can say
that GO:0022804 specializes GO:0022857 by addition of a
mode. We say that function x is subsumed by the function
y by mode addition if x is subsumed by y and x has some
mode, whereas y has no mode assigned. Function sub-
sumption by mode addition is depicted in FueL by means
of a specialization link with stereotype «mode-added».
The subsumed function is the supplier of the link and the
subsuming function is a client.
Mode specialization
Subsumption of functions can be based on the mode
of realization also in cases where a parent function has
already a mode assigned. Consider, for instance, the
function GO:0022804: active transmembrane transporter
activity having the mode: transporter binds the solute
and undergoes a series of conformational changes and the
function GO:0015291: secondary active transmembrane
transporter activity with the mode: transporter binds the
solute and undergoes a series of conformational changes
driven by chemiosmotic energy sources, including uni-
port, symport or antiport. The latter clearly character-
izes particular modes of active transmembrane transport.
Consequently, it seems intuitive to say that GO:0015291
specializes GO:0022804 (as is the case in GO).We call this
type of function subsumption the subsumption by mode
specialization and define it as follows: The function x is
subsumed by the function y by mode specialization if x is
subsumed by y and mode r of x specializes mode s of y.
In FueL function subsumption by mode specialization is
depicted with a specialization link with stereotype «mode-
spec». The subsumed function is the supplier of the link
and the specialized function is a client.
Application
Objectives of applying FueL
In general, graphical modeling languages like UML are
broadly applied in connection with diverse tasks, such as
brainstorming, collaborative design, and the modeling of
key principles of systems and subject matters. Another
broad area of application concerns standardized visualiza-
tion, for example, for documentation purposes.
Regarding FueL more specifically, its application to
GO and MFO, in particular, pursues three objectives.
The first objective is the use of FueL for establish-
ing a semantic basis for molecular functions that sup-
ports the representation of functions in a systematic
way, beyond their textual description. Moreover, the dis-
cussed patterns represent basic knowledge of the inter-
relations between biological processes and molecular
functions. The part_of relation between biological pro-
cesses and molecular functions can be mapped to the has-
goal-achievement association between functions and goal
achievements. Figure 2 comprises a corresponding exam-
ple, where the process GO:0034219: carbohydrate trans-
membrane transport is modeled as a goal achievement of
the function GO:0015144: carbohydrate transmembrane
transporter activity.
The second and the main objective of applying FueL
to MFO is to explicitly document design choices and the
subsumption patterns utilized implicitly in MFO. Figure 5
presents such a documentation of a fragment of MFO in
terms of FueL. The patterns are indicated by the FueL
stereotypes, which enables an easy-to-grasp visualization
of the structure of MFO as well as of the underlying
design choices. Stereotypes further allow for displaying
multiple facets of function subsumption, as in the case of
GO:0022804, which can be understood to involve mode
addition as well as operand specialization. The explicit
specification of design choices makes the ontology much
more intelligible for human users, which is a major benefit
of this approach.
Thirdly, the application of FueL reveals potential for
the refactoring and revision of GO. Contributing to
the latter is another important objective of our work.
For instance, the application of FueL in modeling the
functions GO:0022857: transmembrane transporter activ-
ity and GO:0022891: substrate-specific transmembrane
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 7 of 11
Fig. 5 A segment of MFO modeled with FueL
transporter activity shows that both share similar goal
achievements: transfer of an operand from one side of a
membrane to the other, with input: operand is on one
side of the membrane, and output: operand is on the
other side of the membrane. Consequently and follow-
ing FueL, a potential difference between GO:0022857 and
GO:0022891 can be searched for in their operands. For
GO:0022857 that is a substance, whereas for GO:0022891
it is a specific substance or group of substances.
Analysis of refactoring options
Let us consider the previous case in greater detail, thereby
identifying three possibilities of analyzing and refactor-
ing MFO elements based on FueL. A first FueL view
on a selected set of functions that includes the two just
named is depicted in Fig. 5. It rests on the assumption
that a specific substance or group of substances can
be considered as a subclass of a substance. Accordingly,
Fig. 5 documents explicitly the pattern of subsumption
between GO:0022857 and GO:0022891, namely as a case
of operand specialization. The same aspect applies to
GO:0022804, the operand of which is also a specific
substance or group of substances.
This straightforward approach, however, may be recon-
sidered, especially the question of what the actual rela-
tion between a substance and a specific substance or
group of substances is. One indication may be derived
from GO:0022892: substrate-specific transporter activity
(not displayed in Fig. 5), which is another parent func-
tion of GO:0022891 in MFO. An operand of GO:0022892
is exemplified by macromolecules, small molecules or
ions. If we thus interpret a specific substance or group
of substances as macromolecules, small molecules or
ions, this seems to suggest that further functions such as
GO:0090482: vitamin transmembrane transporter activity
and GO:0015238: drug transmembrane transporter activ-
ity should also be considered as subclasses of substrate-
specific transmembrane transporter activity. The latter
is currently not the case in MFO, such that positioning
those functions under GO:0022891 is a refactoring option,
independently of adopting FueL as a representation lan-
guage. If FueL is employed, these considerations yield
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 8 of 11
an alternative to Fig. 5 (not shown in a separate figure),
where, for instance, GO:0090482 is an operand specializa-
tion of GO:0022891 instead of GO:0022857. GO:0022804,
based on its operand identical to that of GO:0022891,
would turn into a specialization of the latter by mode
addition.
Another possible refactoring originates from an analysis
of the subclasses of GO:0022891: substrate-specific trans-
membrane transporter activity. Examining those sub-
classes we find that they differ only in their operands.
Each of those functions specifies the transport of a spe-
cific kind of substance, for example, ion (GO:0015075)
or carbohydrate (GO:0015144). This suggests that the
distinction between the operands of GO:0022857 and
GO:0022891 is only superficial. According to this inter-
pretation, GO:0022891 is merely used for the organization
of the function taxonomy, i.e., for grouping all functions
that are distinguished by their operands. GO:0022891
would then be a duplication of GO:0022857, which is only
introduced into MFO for structuring purposes, but which
captures no distinct specification of a biological function.
The introduction of such grouping artifacts is a design
choice that is clearly not desirable, especially in com-
plex ontologies like MFO or GO overall. One reason for
avoiding them is that in many cases of using them sub-
classes occur after several steps of specialization that do
not or not exactly match the grouping specification. For
example, GO:0005402: cation:sugar symporter activity in
Fig. 5 may be questioned to be a (pure) substrate-specific
transmembrane transporter activity, given the subsump-
tion path via GO:0022804 involving mode addition and
mode specialization.
Concerning the purpose of better organization of the
taxonomy, we argue that FueL proves beneficial, not at
least due to its stereotyped links. As illustrated in Fig. 6,
the application of FueL allows for dropping GO:0022891
(if interpreted as a grouping artifact), on the one hand,
while on the other hand, FueL enables the explicit spec-
ification of design choices by stereotyped specialization
links. Note that this supports the local grouping of the
immediate, explicit subclasses of a given function based
on the link stereotypes.
The decision on such refactoring options, as in any
modeling enterprise, is the responsibility of the mod-
eler(s), i.e., GO developers in our case. Regarding refactor-
ing means and methods, however, we argue that the above
analysis demonstrates how graphical languages such as
FueL, similarly as in software and systems engineering,
can drive and support the revision of biological ontologies
like MFO. Although graphical modeling may not be effi-
cient for representing the complete content of large and
complex ontologies, we defend the position that graphi-
cal languages can still be extremely helpful, for example,
for depicting ontology fragments that exhibit problems.
Moreover, in view of ontology development as a col-
laborative enterprise, graphical modeling formalisms like
FueL help to conduct community based analysis in struc-
tured ways.
Fig. 6 A refactoring of the segment of MFO in Fig. 5
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 9 of 11
Discussion
The ideas underlying the structure of functions, intro-
duced in FueL, are the result of an analysis of the current
state of the art of function modeling in software, systems
and ontological engineering. For instance, the interpreta-
tion of a function in terms of a role is common not only in
biological systems [20], but also in function modeling in
mechanical engineering [2224].
The notion of goal achievement grasps the teleological
character of a function, its orientation towards some goal.
This aspect is stressed in many approaches to function
representation, e.g. [2527]. In particular, defining a func-
tion in terms of input-output pairs is present in modeling
technical artifacts [28, 29]. The mode of realization, also
called the way-of-function-achievement, which specifies
constraints on the method of how a function is realized,
can be found in [30], among others.
To the best of our knowledge, the presented patterns of
function decomposition are not collected and integrated
into any other single modeling framework, though the
techniques themselves are commonly used, especially in
software and systems engineering, e.g. see the function-
means-context link in [31] or the decomposition with
zig-zaging in [32].
Another aspect worth of discussion is the practical
applicability of the proposed approach, in particular, with
respect to GO and its Molecular Function Ontology. In
this connection it appears realistic to admit that the mere
existence of FueL as a UML profile does not render the
approach ready for an immediate, production-level adop-
tion in the day-to-day curation of function terms in MFO.
The tool set capable of handling MFO (and of GO over-
all), for example, in terms of its size and in accordance
with its recent turn to its representation in OWL exhibits
basically no connection to the world of UML and corre-
sponding modeling tools. Insofar the direct application of
FueL involves bridging this gapmanually, which is limiting
to small-scale, focused case analyses at the present stage.
Nevertheless, we think that the detailed discussion of
refactoring options in the previous section illustrates the
utility of such analyses. There is a significant potential in
view of the fact that, clearly, many more exemplary or
specific cases can and should be made based on MFO.
For instance, analyzing the terms GO:0016209: antioxi-
dant activity and GO:0003824: catalytic activity together
with their subclasses systematically, some of which they
share, one may raise the question of why GO:0004601:
peroxidase activity specializes antioxidant activity, but is
not subsumed by catalytic activity, despite the definition
of GO:0004601, which starts with Catalysis of reaction:
donor + [...]. Moreover, there are various groups of terms
of the form X regulator activity, X activator activity
and X inhibitor activity, at different levels of generality
(e.g., cf. receptor vs. acetylcholine receptor for X). Such
groups may justify a novel, common pattern of function
subsumption, namely based on the output of the corre-
sponding goal achievement. One further finds that goal
achievements are not yet present in GO in a number
of cases, i.e., there are no processes corresponding to
available functions.
Further analysis of MFO terms on the basis of FueL
constituents such as operands and modes leads to the
identification of functions that are specialized (1) almost
exclusively by mode additions or mode specializations,
whereas the subclasses of others (2) primarily rely
on operand specialization. GO:0009055: electron car-
rier activity may serve as an example of the former
case. At least seven out of its eight direct is_a children
clearly arise through mode addition or specialization,
e.g., GO:0045154: electron transporter, transferring elec-
trons within cytochrome c oxidase complex activity and
GO:0045156: electron transporter, transferring electrons
within the cyclic electron transport pathway of photosyn-
thesis activity. In contrast, GO:0004872: receptor activ-
ity has seven direct subclasses (apolipoprotein, cargo,
GO:0005055: laminin, pattern recognition, GO:0038023:
signaling, GO:0099600: transmembrane and virus receptor
activity), the subsumption links to which involve operand
specialization (and only cargo receptor activity a mode
addition, as well).
Besides such distinctions of the way in which a term
relates to its overall set of direct subclasses, we observe
that FueL-guided analysis can generally contribute to
comparing terms and their definitions more easily. This
applies in particular cases, e.g., when wondering about the
(in)difference between the operand signal of GO:0038023
and operand extracellular or intracellular signal of
GO:0099600. A decision on this question supports the
comparison of the overall definitions of both terms. Fur-
ther considerationsmay be concerned with amore general
perspective. Looking at the operands identified in our
analyses, we find that some operands are named by role
terms such as messenger (w.r.t. GO:0004872), others have
non-role names, e.g. laminin (w.r.t. GO:0005055), and yet
others mix both aspects, like hydrogen or electron acceptor
in GO:0016491: oxidoreductase activity. This yields a con-
necting factor to the field of roles and role analysis, cf. e.g.
[3335], which may lead to novel refactoring considera-
tions for MFO as well as to future refinements of function
subsumption patterns.
Overall, on the one hand we do see significant poten-
tial based on inspecting MFO manually in a systematic
and structured way, using FueL. On the other hand, the
purely manual approach is a limitation at the present stage
and hampers an extensive evaluation, which would ideally
involve direct participation by GO developers.
Despite the shortcoming regarding validation in prac-
tice, we argue that presenting and demonstrating our
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 10 of 11
approach in a biological context is already beneficial. The
aspect of applying it systematically to specific function
terms, which may also be conducted merely on the con-
ceptual basis of FueL, almost independently of the UML
language aspects, is elaborated above. But more can be
said. First, although we consider MFO as a major case
of interest, FueL is applicable to functions in arbitrary
domains and contexts, cf. [18]. The approach presented
may therefore be of interest concerning functions covered
in other biomedical ontologies. Secondly, we see many
routes of future work that can be pursued, possibly in
collaboration with other groups. In the context of MFO,
there are at least the ideas (1) to provide tools that sup-
port the use of FueL by ontology developers and augment
an established ontology lifecycle, as well as (2) to develop
(semi-)automated approaches and software that can be
applied to the existing MFO structure, for example, in
order to determine instances of subsumption patterns.
This leads to a final point here, though of no less impor-
tance, where subsumption patterns are a natural candidate
to deal with. Given OWL as the current basis of devel-
opment and reasoning of many biomedical ontologies, a
way to bridge between OWL and FueL is highly desir-
able, or  at the very least  the transfer of FueL-based
function analysis and representation to a corresponding
use of OWL. We expect either task to be ambitious.
FueL is equipped with a formalization in first-order logic
[18], which must be respected and related to clearly if
an OWL formalization or translation is derived from
FueL. Another issue along similar lines is the treatment
of UML stereotypes in OWL, as these are meta-classes in
UML. There are a number of conceivable options to tackle
their treatment in OWL, ranging from not making them
explicit over the use of punning or annotations [36] to
using multiple OWL ontologies for one FueLmodel. Iden-
tifying pros and cons of such options with respect to par-
ticular purposes in the context of biomedical ontologies
remains an interesting future effort.
Conclusions
In the current paper we present and discuss applica-
tions of UML and patterns of function subsumption to
the modeling and refactoring of biological ontologies. In
particular, we developed a UML profile for function mod-
eling, called the FunctionModeling Language (FueL) [19],
and apply it to the modeling and refactoring of segments
of the Molecular Function Ontology.
The application of FueL enables the systematic, graphi-
cal representation of functions and thereby of information
that is currently available in MFO mainly in the form of
textual descriptions. We elaborate that behind the exten-
sional is_a relation, which is used for the construction of
MFO, several different patterns of intensional subsump-
tion can be determined. Modeling MFO via FueL helps
in identifying pattern instances that occur implicitly in
MFO. Moreover, FueL provides the means of referring
to those patterns directly in the hierarchy of molecu-
lar functions. We argue that this can help in making
the ontology structure more comprehensible for human
users and that it supports communication. The claim is
demonstrated by an analysis and a model of an MFO frag-
ment with FueL, from which we derive several refactoring
options.
Besides proposing the adoption of FueL and the par-
ticular refactoring options in this paper, for future work
we consider first the continued analysis of MFO. Extend-
ing this to a larger scale may require establishing soft-
ware support, e.g., for identifying subsumption pattern
instances within MFO (semi-)automatically. Moreover,
FueL and its methods may also be transferred to or may
yield new methods for common languages of biomedical
ontologies, nowadays including OWL.
Endnote
1 In contrast to FuML in a preceding publication
[37] (cf. also the Acknowledgments section below), the
acronym FueL has been adopted for a better termino-
logical distinction from other efforts, like fUML [38] by
OMG.
Acknowledgements
This paper is an extended version of a submission [37] presented at the
International Conference on Biomedical Ontology (ICBO) 2015. We are grateful
to the involved ICBO reviewers and participants for valuable criticism. Likewise
the journal reviewers deserve our thanks for insightful and stimulating
comments.
Funding
We acknowledge support from the German Research Foundation (DFG) and
the University of Leipzig within the program of Open Access Publishing.
Availability of data andmaterials
Data sharing not applicable to this article as no datasets were generated or
analyzed during the current study.
Authors contributions
Primarily PB developed the UML profile that constitutes FueL [19], in
collaboration with HH. FueL is based on pursuing ontological analysis of the
notion of function, with contributions by PB, FL, and HH. PB conceived of the
idea of utilizing structural subsumption for the development of function
subsumption patterns. All three authors discussed the latter as well as the
application of FueL to MFO. Mainly PB and FL prepared the present paper,
supported by HH. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Burek et al. Journal of Biomedical Semantics  (2017) 8:48 Page 11 of 11
Author details
1Institute of Medical Informatics, Statistics and Epidemiology, University of
Leipzig, Haertelstrasse 16-18, 04107 Leipzig, Germany. 2Computer Science
Institute, University of Leipzig, Augustusplatz 10, 04109 Leipzig, Germany.
Received: 1 March 2016 Accepted: 15 September 2017
RESEARCH Open Access
Predicting activities of daily living for
cancer patients using an ontology-guided
machine learning methodology
Hua Min* , Hedyeh Mobahi, Katherine Irvin, Sanja Avramovic and Janusz Wojtusiak
Abstract
Background: Bio-ontologies are becoming increasingly important in knowledge representation and in the machine
learning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to the
SEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities of
daily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to understand
biomedical data.
Results: This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods were
applied to create predictive models for ADL disabilities for the first year after a patients cancer diagnosis. The first
method is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods for
reasoning with ontologies. The models showed that a patients race, ethnicity, smoking preference, treatment plan and
tumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performance
levels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADL
performance levels (P < 0.1) than methods without ontologies.
Conclusions: This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for ML
algorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rule
learning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. The
ontology-guided ML method achieved better performance than the method without ontologies. The presented method
can also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledge
and consistency with existing clinical expertise is critical.
Keywords: Machine learning, Bio-ontologies, Quality of life, Activities of daily living, SEER-MHOS
Background
Precision medicine is an emerging approach for dis-
ease prevention and treatment that takes into account
individualized patient information including genomics,
environment, and lifestyle [1]. This new era in medi-
cine and health requires advanced methodologies for
analyzing, synthesizing, and disseminating heteroge-
neous data, as well as the ability to harness existing
knowledge in order to discover relationships and cre-
ate computational models for improving care and
quality of life. The focus on big data analysis in the
biomedical field creates an even greater need for ad-
vanced computational methodologies that can translate data
into computer-interpretable knowledge and produce com-
prehensible models that can then be used to advance
patient-centric healthcare. Machine learning (ML) is already
widely used in creating predictive models within a variety of
arenas of big data analysis, and is gaining popularity in med-
ical and health applications [2].
One major challenge in ML is communicating the
meaning of data attributes and their significance to
the learning algorithm. Biomedical data are extremely
complex, heterogeneous, and characterized by intri-
cate semantics. Very few ML algorithms are capable
of interpreting data beyond the mechanical fitting of
input data/matrix of numbers into a given model.
* Correspondence: hmin3@gmu.edu
Department of Health Administration and Policy, College of Health and
Human Services, George Mason University, MS: 1J3, 4400 University Drive,
Fairfax, VA 22030-4444, USA
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Min et al. Journal of Biomedical Semantics  (2017) 8:39 
DOI 10.1186/s13326-017-0149-6
The majority of ML methods (including the most
popular Support Vector Machines, Random Forests,
Logistic Regression, etc.) work with and almost exclu-
sively focus on numeric data stored in flat tables
while ignoring the semantic relationships (meaning)
of data elements. Two existing ML disciplines that
address complex data are statistical relational learning
[3, 4] and inductive logic programming [5]. Both dis-
ciplines are concerned with the more general problem
of learning from datasets with complicated structures
(relational databases or predicates). However, while
healthcare data are particularly rich in knowledge, the
use of standard ML methods does not allow for the
encoding of attribute types, hierarchies, ontologies,
and other coding systems. Typically, in order to use
any background knowledge or ontological relation-
ships when applying ML methods, one needs to
encode these in problem representations, i.e., by cre-
ating additional dimensions that correspond to inter-
actions between existing ones. This is because the
ML method for input is a matrix of numerical data.
One example of using ontology in conjunction with
ML is the work by Kassahun et al. [6], in which the
researchers classified types of epilepsy patients and
their localization using an ontology-based classifica-
tion (OBC) methods that classified patients (slightly)
more accurately than clinicians.
An ontology formally represents domain knowledge as
a set of concepts and relationships between those con-
cepts. In artificial intelligence (AI), ontologies have been
applied as artifacts to represent human knowledge. They
are also critical components of knowledge management,
e.g. Semantic Web, business-to-business applications,
and natural language processing [710]. In biomedicine,
ontologies have been widely adopted and used in know-
ledge management, data integration, and decision sup-
port and reasoning [11, 12]. Bio-ontologies are slowly
emerging in data-driven science, including data mining
and ML, although mainly in the capacity and context of
natural language processing applications [13].
There are many existing bio-ontologies, each with a
scope, purpose, and role of its own with no industry
standard. Consequently, there are communication bar-
riers between the various information systems or appli-
cations when different vocabularies are used. In order to
address these barriers, the Unified Medical Language
System (UMLS) was developed by the National Library
of Medicine (NLM) in 1986 [14, 15]. The 2016 AB
version of the UMLS contains more than 3 million
concepts (CUIs) and 13 million unique concept names
(AUIs) from 199 source vocabularies [16]. The UMLS
establishes mappings between bio-ontologies by assign-
ing a concept unique identifier (CUI) to names from
various vocabularies that have the same meaning. The
vocabulary mappings allow computer systems to trans-
late data among the diverse information systems. Rich
relationships (22 million) between concepts in the
UMLS also provide a solid foundation for reasoning in
medical knowledge [11].
Thus, given the advantages of bio-ontologies know-
ledge, UMLS mappings, and the ability of ML to develop
and learn from predictive models, this paper aims to
describe and apply an ontology-guided ML method (em-
phasis on rule learning) by incorporating hierarchical
relationships from the UMLS. The UMLS is used to pro-
vide medical domain knowledge for the ML method to
understand the meaning and significance of the bio-
medical data, with regards to the existence of specific
hierarchical relationships between concepts. By applying
the ontology-guided ML method to SEER-MHOS data,
the technique is able to predict the cancer patients abil-
ity to perform activities of daily living (ADLs). The out-
come of which is generated rules that are highly
transparent and easy to understand. Thus, the rules can
be interpreted by the non-technical end users. This
proof of concept study suggests that the combination of
bio-ontologies and ML methods provides an advanced
computational and quantitative technique for analyzing
biomedical data.
Methods
AQ21 rule learning
AQ21 is a multi-task ML and data mining system for
attributional rule learning and rule testing that can be
applied to a wide range of classification problems [17]. It
was developed in the Machine Learning and Inference
Laboratory (MLI) at George Mason University. The sys-
tem has been recently extended to include features spe-
cific for processing biomedical data [18]. AQ21 is a type
of natural induction system that seeks to identify
patterns represented as attributional rules [19] that are
easily interpretable to end users. The basic form of an
attributional rule is: CONSEQUENT < = PREMISE
where both CONSEQUENT and PREMISE are conjunc-
tions of attributional conditions. Each attributional con-
dition involves attributes present in the data or
constructed by the program. Additionally, AQ21 can
learn rules with exceptions given by the formula CON-
SEQUENT < = PREMISE |_ EXCEPTION. The AQ21
system can also handle inconsistencies in data. The
system learns standard rules and generates exception
phrases that represent covered negative examples. EX-
CEPTION can be either an attributional conjunctive
description or a list of examples constituting exceptions
to the rule. In the medical datasets, the exceptions are
always negative examples such as cancer recurrence and
disease progression.
Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 2 of 8
Learning rules generated by AQ21 consist of several
steps, which can be classified as input preprocessing,
rule generation, and rule optimization. The steps are
generally executed in this order, although AQ21s learn-
ing process is iterative in several ways. Input preprocess-
ing includes rearranging data into classes, removing
ambiguous examples, and modifying representation
space through simple preprocessing methods (i.e.,
discretization, attribute selection) or more advanced
ones that employ constructive induction algorithms [20].
At its core, rule learning implements modification of a
simplified version of the algorithm quasi-optimal (Aq)
for constructing rules, which is a well-known sequential
covering algorithm [21]. The algorithm starts with a ran-
domly selected positive example, called the seed, and
generates all possible (high quality) rules that cover the
seed and do not cover (or approximately do not cover)
any of the negative examples. The best quality top rules
are then selected and stored. Among positive examples
not covered by these selected rules, another random
seed is selected and the operation is repeated. This
process results in a number of very general rules (typic-
ally more than needed) that need to be optimized and
prepared for output. Optimization of rules includes their
trimming, adjusting of generality through following hier-
archies, selection, and mapping of attributes. The overall
goal of AQ21 is to produce rules that maximize user-
defined quality criteria that typically provide tradeoff
between accuracy (precision/recall) and their simplicity
and transparency. Finally, the program employs a num-
ber of methods designed to provide output in human-
oriented forms, including the generation of the rules into
a natural language representation (layman terms) [22].
AQ21 is the latest development from a series of AQ
rule learners that dates back to the 1970s [23]. A num-
ber of well-known rule learners have been developed
over the last decades [2426], but many are not utilized
in mainstream research at the present time. In the past
few years the ML field has been dominated by statistical
methods that focused primarily on providing highly
accurate models. However, the community has begun to
slowly transition back to understandability and transpar-
ency of models produced, which is particularly import-
ant in biomedical applications.
Ontology-guided AQ21 (AQ21-OG)
AQ21-OG is an extension of the AQ21 rule learning
system. It applies hierarchical reasoning methods [27] to
include UMLS and other ontologies when analyzing data.
Currently, the program allows for mapping IS-A relation-
ships. The implementation of the AQ21-OG includes:
Step 1: Mapping data to the UMLS CUIs. This step is
used to identify the base CUIs. The candidate CUIs are
identified automatically (SQL) and then reviewed by
experts for the problematic mappings.
Step 2: Extracting complete sub-hierarchies by following
IS-A relationships using base CUIs. This is done by
following IS-A relationships in the UMLS for each
concept until the complete parent, child, and sibling
sub-hierarchy is extracted. The complete sub-hierarchy is
defined as the path from base CUI (furthest child(ren) in
the hierarchy) to the root (super parent, i.e. a parent
that is not also a child). This extraction is the basis for
the input file (in Step 4) that AQ21 will use to find the
farthest common ancestors for base CUIs (in Step 5).
Step 3: Resolving inconsistencies in the hierarchy. Due to
nature of the UMLS, a number of inconsistencies (e.g.,
cycles, duplicates) may happen when due to being
constructed from multiple source terminologies [2831].
Cycles are not permitted in AQ21, so they are resolved
by breaking links that connect back to concepts higher in
the hierarchy, as measured by distance from the root.
Other types of inconsistencies are removed from the
final hierarchy.
Step 4: Encoding extracted hierarchies into
ML-software readable format. AQ21 requires a list of
parent-child pairs for all relationships that form the
hierarchy. The data is read from text files that include
all semantic information required to correctly reason
with the data. Specifically, in the AQ21, hierarchical
relationships are part of the definition of attributes
domains (set of possible values) that describe data.
Step 5: Optimizing the rules by using the extracted
UMLS hierarchies from Step 2. AQ21-OG finds the
highest level of generalization in the hierarchy, which is
either consistent with data or maximizes the rule
quality measures. This is particularly valuable when
analyzing coded medical data with potentially hundreds
of thousands of binary attributes. For example, ICD-9-
CM diagnosis codes can result in the need to create
close to 10,000 binary attributes. Therefore, the need to
generalize those codes to reduce the number of features
is a necessity.
Study population
SEER-MHOS (Surveillance, Epidemiology, and End
Results - Medicare Health Outcomes Survey) data from
1998 to 2011 (1,849,311 records) were used to extract
comorbidities and activities of daily living (ADLs), as
well as cancer characteristics. This dataset links two
large population-based data that provide detailed infor-
mation about Medicare beneficiaries with cancer [32].
The SEER data extracted from the cancer registry
contains clinical, demographic and cause of death in-
formation for persons with cancer, while the MHOS
data is extracted from survey responses and provides
Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 3 of 8
information about the health-related quality of life
(HRQOL) of Medicare Advantage Organization (MAO)
enrollees.
A number of steps were followed to create the study
population dataset. First, the study population was lim-
ited to those who completed at least one survey before
their cancer diagnosis and one survey roughly one year
after the diagnosis. If a patient completed multiple sur-
veys, the surveys closest to before the cancer diagnosis
and the 1-year follow-up were used. These very strict
criteria significantly reduced the sample size and re-
sulted in a cohort of 723 cancer patients.
Dependent/Output Variables: the primary outcomes
were six ADLs (walking, dressing, bathing, moving in/
out of chair, toileting, and eating) reported in a patient
survey taken one year after the cancer diagnosis.
Independent/Input Variables: the potential predictors
were selected based on the prior research [3337] and
are as follows:
(1)Patient demographics: age, race and marital status
(2)Six ADLs reported in a patient survey taken before
the cancer diagnosis
(3)Thirteen self-reported comorbidities extracted from
a patient survey taken before the cancer diagnosis:
Angina Pectoris/Coronary Artery Disease, Arthritis
of Hand/Wrist, Arthritis of Hip/Knee, Back pain,
Congestive heart failure, Emphysema/Asthma/
Chronic obstructive pulmonary disease, Diabetes,
Crohns Disease/Ulcerative Colitis/Inflammatory
Bowel Disease, Hypertension, Myocardial Infarction,
Other Heart Conditions, Sciatica and Stroke
(4)Six cancer characteristics namely grade, staging,
tumor size, histology, tumor extension, and behavior
extracted from the SEER registry
(5)Cancer radiation and surgery treatment indicators
extracted from the SEER registry
Analysis of the SEER-MHOS data with AQ21 and AQ21-OG
The dataset was randomly divided into training (80%)
and testing (20%) sets. The training set was used to cre-
ate predictive models and the testing set was used to as-
sess the model discrimination. Models were first created
in order to find the predictor or set of predictors that
could be used to predict the outcome (the six ADLs post
cancer diagnosis). Two ML methods were used to create
models: AQ21 and AQ21OG as previously described
above. The quality of the two methods were assessed
using the number of positive (p), negative (n) cases cov-
ered by the generated rules and the quality of the rules
Q(w). The rule R quality, Q(R,w) with weight w, or just
Q(w) (denoted by q in the rule), is calculated using the
following formula described by Michalski and Kaufman
[38]. P and N indicate total numbers of positive and
negative examples in data (here, disabled vs. functionally
independent in terms of ADLs).
Q R;wð Þ ¼ compl Rð Þw consig Rð Þ1?w
where
compl Rð Þ ¼ p=P
consig Rð Þ ¼ p= pþ nð Þð Þ P= PþNð Þð Þð Þ PþNð Þ=N
The w is a weight (from 0 to 1) that represents the tra-
deoff between completeness and consistency gain. The
lower the w is, the more consistent the rules need to be
(fewer negative examples covered). The higher the w is,
the more complete the rules need to be (more positive
examples covered). Based on experimental evaluation of
the rules, we decided to select w = 0.3 which indicates
slightly higher weight for more consistent rules. This
value was used in both cases, with and without ontology.
Completeness is frequently referred to as recall in
machine learning. Consistency gain can be viewed as
normalized precision that measures how much precision
we gain over a random guess.
Table 1 Characteristics of Patients in the final dataset (n = 723)
Number %
Age
< 65 23 3%
6574 353 49%
7584 293 41%
> =85 54 7%
Top 5 Comorbidities
Hypertension 432 60%
Arthritis of Hip 274 38%
Arthritis of Hand 256 35%
Other Heart 181 25%
Sciatic 166 23%
Cancer Type
Bladder 57 8%
Breast 181 25%
Colorectal 105 15%
Head Neck 22 3%
Lung 87 12%
Melanoma 58 8%
Pancreas 11 2%
Prostate 166 23%
Stomach 11 2%
Uterus 25 3%
Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 4 of 8
Results
Patient cohort
This retrospective SEER-MHOS study included 723 can-
cer patients. The average age was 74.7 +/? 6.63 years. A
summary of the dataset is shown in Table 1. Table 2
shows the number of patients who reported ADL limita-
tions before and after cancer diagnosis. The increased
number of patients reporting disabilities after diagnosis
show that cancer has an impact on ADLs. Walking and
chairing-in/out were the most affected ADLs among
these Medicare recipients with cancer.
Rule induction from the SEER-MHOS
AQ21 methods generated a number of models (rulesets)
for describing and predicting patients deficiencies in
performing ADLs from the SEER-MHOS dataset. Below
is an excerpt of two sample rules, one from each AQ
method, from a model for predicting a decline in the
ability to perform bathing independently.
Sample 1: AQ21
[Bathing
impairment] < ==
[Race = Black, White, Chinese: 70, 245, 22%]
[Hispanic = No: 64, 241, 20%]
[Smoking = Some days, Not at all: 68, 238, 22%]
[Surgery = 51,40,27,0,45: 45, 113, 28%]
[Histology = Squamous cell neoplasm, Transitional
cell papillomas and carcinomas, Adenomas and
adenocarcinomas, Nevi and melanomas, Cystic,
mucinous and serous neoplasm, Ductal and
lobular neoplasm, Epithelial neoplasms, NOS: 74,
252, 22%]
[Stage = In situ, Localized only, Regional by direct
extension only: 69, 244, 22%]
[Primary site and
morphology = C0153458,C0153492,C0153532,
C0242787, C0949022,C0235653,
C0153483,C0153611,
C0153555,C0153435,C0346782,C0153491,C0153612:
30, 34, 46%]
: p = 22, n = 2, q = 0.642
Sample 2: AQ21-
OG
[Bathing
impairment] < ==
[Race = White, Chinese: 64, 219, 22%]
[Hispanic = No: 64, 241, 20%]
[Smoking = Some days, Not at all: 68, 238, 22%]
[Surgery = 32,51,40,0,45: 40, 95, 29%]
(Continued)
[Histology = Squamous cell neoplasm, Adenomas
and adenocarcinomas, Nevi and melanomas,
Cystic, mucinous and serous neoplasm, Ductal and
lobular neoplasm, Epithelial neoplasms, NOS: 68,
229, 22%]
[Cancer site = Lung and Bronchus, Melanoma,
Descending Colon, Rectum, Pancreas, Urinary
Bladder, Breast, Larynx
: 61, 169, 26%]
[Primary site and morphology = C0154077,
C0007102, C0153532, C0005684, C0153555,
C0024624, C0006142, C0235652, C0864875,
C0346647, C0345921, C0242379, C0346629,
C0345865, C0242788, C0034885, C0007107,
C0345713, C0587060, C1263771: 38, 49, 43%]
: p = 23, n = 2, q = 0.653
The predictors of bathing disability include patient
demographic (race and ethnicity), smoking history,
tumor characteristics (histology, stage, and cancer
sites) and treatment (surgery). The interpretation of
the first two lines of the first rule is: a patient is
likely to have bathing impairment if the patients race
is White, Black or Chinese and the ethnicity is non-
Hispanic. The surgery codes (treatments) in the
fourth line can be found from https://seer.cancer.gov/
manuals/2016/appendixc.html. The meaning of the
CUIs in the last line is presented in Appendix. The
first two numbers, following the colon, within each
condition (attribute) describe the number of patients
who have the bathing impairment and who do not
have the bathing impairment that satisfy the specific
condition. For example, among the White, Black or
Chinese patients, 70 of them have the bathing impair-
ment while the remaining245 patients do not have
the bathing problem. The last number is prevalence
of the positive class that indicates the ratio of the
number of positive (p) examples over the number of
positive and negative (n) examples, p/(p + n). The
rule outputs are similar using AQ21 and AQ21-OG.
However, the quality of the rule, as measured by
Q(w), generated by the second method (AQ21-OG) is
slightly more accurate. The last line in the rule set
describes the numbers of positive examples (p), nega-
tive example (n) covered by the rule, and the rule
quality. While the numbers dont appear to make a
large difference, the rules are simply an illustration of
the type of improvement made by the method. Table 3
Table 2 Number of patients reported ADL disabilities before
and after cancer diagnosis
ADLs No. of patients before
cancer diagnosis
% No. of patients after
cancer diagnosis
%
Bathing 39 5% 85 12%
Dressing 27 4% 61 8%
Eating 10 1% 32 4%
Chairing 65 9% 113 16%
Walking 98 14% 146 20%
Toileting 21 3% 50 7%
Table 3 Quality metrics for the AQ21 and AQ21-OG for the
sample rules
Sample 1 (AQ21) Sample 2 (AQ21-OG)
Precision 0.91 0.92
Recall 0.29 0.31
F1-score 0.44 0.46
Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 5 of 8
shows the precision, recall and F1-Score of both
AQ21 and AQ21-OG for the above two sample rules.
Although the recall in the Table 3 seems low, this is
the number for one example rule out of a set of
rules.
Note that the rules presented above correspond to
each other; however, AQ21 and AQ21-OG are not
guaranteed to generate similar rules. The ability to
generalize available data differentially within the hier-
archies derived from an ontology may steer the
process in a different direction causing the rules to
differ. Consequently, the quality of rules improves.
Table 4 shows the quality of the rules generated by the
two methods for each of the six ADLs. In all cases, the
Q(R, w) improved after including UMLS, except for
toileting which remained unchanged. A paired t-test
was performed to compare the sample means for the
quality of rules (Table 5). Although the sample size was
small, after adding ontology, the mean of Q(R, w) values
increased by 6% (P = 0.05). There was a statistically sig-
nificant difference (P < 0.1) between the effectiveness of
those two methods.
Discussion
AQ21 and its ontology-guided version, AQ21-OG, are
highly configurable and robust systems with features es-
pecially valuable for: learning from biomedical data such
as individual patient data, learning from aggregated data,
and using medical knowledge. One major advantage is
that AQ21-OG can optimize attributional rules with the
assistance of medical knowledge from the UMLS, for the
purposes of rule generalization based on the hierarchical
relationships. In this research, the rule generalization
procedure continued until negative data against medical
knowledge was found. This was done automatically by
the AQ21 to increase accuracy of the predictive models.
One big challenge for the ontology-guided ML
method is performance. Performance is impacted by:
(1) the extreme size and complexity of UMLS and
other medical ontologies which limit the application
of standard search methods and (2) the size of the
SEER-Medicare dataset. Although this study only
worked with a small subset of SEER-MHOS data, the
hierarchical structure from the UMLS was already
large and complicated. As previously discussed many
concepts in the UMLS contain more than one parent,
thus the generalized rules may contain more CUIs
due to the complexity of the UMLS.
One limitation of this study was that the method was
tested and validated based on a small sample of SEER-
MHOS patients (n = 723). Additionally, survey data are
typically not best suited for ML applications because of
their biases and subjectivity and limited potential use in
real decision support applications. Future work will in-
clude increasing the sample size by using the entire SEER-
MHOS data as an opposed to a subset of a 5% sample. On
the methodological side, AQ21 will be extended to handle
other types of semantic relationships in the UMLS. Fur-
ther, more experimental evaluation is needed to improve
accuracy of the generated rules in order to match the ac-
curacy of state-of-the-art statistical methods.
Conclusions
This paper presents how AQ21 and its ontology-guided
ML version AQ21-OG were successfully applied to the
SEER-MHOS data set and generated a set of models for
describing and predicting cancer patients deficiencies in
performing six ADLs. These models are highly transpar-
ent and relatively easy to understand. The results show
that the AQ21-OG outperforms the original AQ21 since
AQ21-OG can optimize attributional rules with the
assistance of medical knowledge from the UMLS. This
research further demonstrates that bio-ontologies can be
used to promote the effectiveness and efficiency of ML
in healthcare.
Table 4 Q(R, w*), precision, recall and F1-score calculated for a selected rule for each ADL
ADL AQ21 Q(R, w) AQ21-OG Q(R, w) AQ21 precision AQ21OG precision AQ21 recall AQ21-OG recall AQ21 F1-score AQ21-OG F1-score
Bathing 0.642 0.653 0.91 0.92 0.29 0.31 0.44 0.46
Chairing 0.489 0.546 0.92 1.0 0.12 0.13 0.21 0.23
Dressing 0.451 0.633 0.86 1.0 0.1 0.21 0.17 0.35
Eating 0.617 0.697 1.0 1.0 0.2 0.3 0.33 0.46
Toileting 0.584 0.584 1.0 1.0 0.16 0.16 0.28 0.28
Walking 0.457 0.472 1.0 1.0 0.07 0.08 0.13 0.15
*w = 0.3
Table 5 T-test results for comparison of two methods
Without ontology With ontology P
Mean 0.54 0.60 0.05
Variance 0.0071 0.0066
Min et al. Journal of Biomedical Semantics  (2017) 8:39 Page 6 of 8
Appendix
Acknowledgements
We thank our undergraduates Sava Vukomanovic and Ilirjeta Krasniqi for
preparing earlier version of this work as a conference paper.
Funding
The study was supported by the Thomas F. and Kate Miller Jeffress Memorial
Trust, Bank of America, Trustee. Publication of this article was funded in part
by the George Mason University Libraries Open Access Publishing Fund.
Availability of data and materials
The datasets generated and/or analyzed during the current study are not
publicly available due to the SEER-MEDICARE HEALTH OUTCOMES SURVEY
DATA USE AGREEMENT (DUA).
Authors contributions
HMin and JW conceived and designed the analysis. HM, KI, and SA ran the
analysis. All authors contributed to the interpretation of the data, revisions of
the manuscript and read and approved the final manuscript.
Ethics approval and consent to participate
It was a secondary data analysis of the SEER-MHOS.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 23 October 2016 Accepted: 6 September 2017
Giraldo et al. Journal of Biomedical Semantics  (2017) 8:52 
DOI 10.1186/s13326-017-0160-y
RESEARCH Open Access
Using semantics for representing
experimental protocols
Olga Giraldo1* , Alexander García1, Federico López2 and Oscar Corcho1
Abstract
Background: An experimental protocol is a sequence of tasks and operations executed to perform experimental
research in biological and biomedical areas, e.g. biology, genetics, immunology, neurosciences, virology. Protocols
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 
DOI 10.1186/s13326-017-0146-9
RESEARCH Open Access
Towards precision medicine: discovering
novel gynecological cancer biomarkers and
pathways using linked data
Alokkumar Jha, Yasar Khan, Muntazir Mehdi, Md Rezaul Karim, Qaiser Mehmood, Achille Zappa,
Dietrich Rebholz-Schuhmann and Ratnesh Sahay*
Abstract
Background: Next Generation Sequencing (NGS) is playing a key role in therapeutic decision making for the cancer
prognosis and treatment. The NGS technologies are producing a massive amount of sequencing datasets. Often,
these datasets are published from the isolated and different sequencing facilities. Consequently, the process of
sharing and aggregating multisite sequencing datasets are thwarted by issues such as the need to discover relevant
data from different sources, built scalable repositories, the automation of data linkage, the volume of the data,
efficient querying mechanism, and information rich intuitive visualisation.
Results: We present an approach to link and query different sequencing datasets (TCGA, COSMIC, REACTOME, KEGG
and GO) to indicate risks for four cancer types  Ovarian Serous Cystadenocarcinoma (OV), Uterine Corpus
Endometrial Carcinoma (UCEC), Uterine Carcinosarcoma (UCS), Cervical Squamous Cell Carcinoma and Endocervical
Adenocarcinoma (CESC)  covering the 16 healthy tissue-specific genes from Illumina Human Body Map 2.0. The
differentially expressed genes from Illumina Human Body Map 2.0 are analysed together with the gene expressions
reported in COSMIC and TCGA repositories leading to the discover of potential biomarkers for a tissue-specific cancer.
Conclusion: We analyse the tissue expression of genes, copy number variation (CNV), somatic mutation, and
promoter methylation to identify associated pathways and find novel biomarkers. We discovered twenty (20) mutated
genes and three (3) potential pathways causing promoter changes in different gynaecological cancer types. We
propose a data-interlinked platform called BIOOPENER that glues together heterogeneous cancer and biomedical
repositories. The key approach is to find correspondences (or data links) among genetic, cellular and molecular
features across isolated cancer datasets giving insight into cancer progression from normal to diseased tissues. The
proposed BIOOPENER platform enriches mutations by filling in missing links from TCGA, COSMIC, REACTOME, KEGG
and GO datasets and provides an interlinking mechanism to understand cancer progression from normal to diseased
tissues with pathway components, which in turn helped to map mutations, associated phenotypes, pathways, and
mechanism.
Keywords: Cancer genomics, Biomarkers, Multi-Omics, Pathways, Gynecological cancer, Linked data,
Semantic technologies
Background
Next Generation Sequencing (NGS) technologies open
new diagnostic and therapeutic ways for cancer research.
The resulting high-throughput sequencing data has to be
processed in complex data analytics pipelines including
annotation services. Unfortunately, there is not yet a
*Correspondence: ratnesh.sahay@insight-centre.org
Insight Centre for Data Analytics, NUIG, Galway, Ireland
well-integrated platform available for both clinical and
translational [15] research to fulfill these annotation
and analytical tasks. In addition, the large volumes and
growing variety of NGS data sources pose another chal-
lenge, since the computational infrastructure for the
biological interpretation will have to cope with very
large quantities and heterogeneities of data originating
from sequencing facilities [68]. More importantly, the
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 2 of 16
functional annotation of genomics data for cancer has
to take tissue-specificity into consideration and thus has
to avoid ambiguity while consolidating and aggregating
clinical outcomes from disparate resources. Similarly, a
computational platform that can consolidate variety of
data derived from electronic health records (EHRs), omics
technologies, imaging, and mobile health is a funda-
mental requirement to accelerate the recent precision
medicine initiative1 [9]. In our initial work [10] we pre-
sented an approach to link and query three large repos-
itories  TCGA2, COSMIC3, and Illumina Human Body
Map 2.04  to analyse the expression of specific genes in
different tissues and its variants by:
? Linking of gene expression, copy number variation
(CNV), somatic mutation data from two disjoint
resources (i.e., COSMIC and TCGA).
? Identifying sets of genes using the Illumina Human
Body Map 2.0 with relevance for ovarian cancer with
a comprehensive set of mutations.
In order to analyse the tumorigenesis of female gyneco-
logical cancer types, in this article we extend our previous
work [10] by including:
? Ovarian Serous Cystadenocarcinoma (OV), Uterine
Corpus Endometrial Carcinoma (UCEC), Uterine
Carcinosarcoma (UCS), Cervical Squamous Cell
Carcinoma and Endocervical Adenocarcinoma
(CESC) datasets.
? Methylation data to further understand potential
promoter genes based on methylation change and
biomarkers.
? REACTOME, KEGG and GO biological processes
datasets to understand cancer causing gene regulation
through associated pathways and biological processes.
To further understand the epigenetics, we retrieved the
genomic positions (loci), mutation frequency, change in
promotormethylation for each gene in the above four can-
cer types (OV, UCS, UCEC, & CESC). These are further
classified by biological processes involved in understand-
ing the mechanism and associated pathways. By doing this
we explore the variant and mutation prioritization using
16 different tissue types reported in the Illumina Body
Map 2.0. The differential expressed genes derived from
Illumina Human BodyMap 2.0  using the procedure sug-
gested by Trapnell, C. et al. [11]  are linked with different
tissue types and gene expressions in COSMIC and TCGA
datasets leading to a potential biomarker for a particular
tissue-specific cancer.
The proposed approach enriches mutations and methy-
lation by filling in missing links from COSMIC, TCGA,
REACTOME, KEGG and GO datasets providing a mech-
anism to analyse cancer progression from normal to
diseased tissues with key pathway components. Our key
objective is to understand the tumorigenesis of these four
gynecological cancer types (OV, UCS, UCEC, & CESC). In
order to retrieve the patterns of genes and tissue-specific
information from various cancer mutations reported in
multiple repositories; we encountered three computa-
tional challenges for linking and querying these multiple
distributed repositories: (i) transform heterogeneous data
repositories and their storage formats into standard RDF;
(ii) discovering links by finding specific patterns, i.e., cor-
relations for a gene with regards to CNV, mutation, gene
expression, and methylation datasets; and (iii) scalable
querying over the large volume datasets covering 16 dif-
ferent tissue types and the gene expression data from
different repositories. We propose a data-interlinked plat-
form called BIOOPENER5 that enables automated dis-
covery of data linkages and querying of information from
large-scale cancer and biomedical repositories.
The experiments conducted in this paper is aligned to
the transcriptome and epigenetics studies based on the
Human Body Map 2.0 (HBM) from Illumina which cov-
ers the following tissues: adrenal, adipose, brain, breast,
colon, heart, kidney, liver, lung, lymph, ovary, prostate,
skeletal muscle, testes, thyroid, and white blood cells. The
HBM provides gene-specific information across one or
more tissue types and intends to support the identifica-
tion of potential biomarkers for a targeted therapy. In this
study, our results not only discover novel biological out-
comes but also provides a linked datasets that assimilates
clinical outcomes from related data repositories.
The rest of the paper is structured as follows:
Motivation section motivates our working scenario
based on Illumina Human Body Map (HBM) 2.0,
cancer and biomedical databases (COSMIC, TCGA,
REACTOME, KEGG and GO); Methods section
presents the BIOOPENER methodology and architec-
ture; Results section discusses the results obtained
from the BIOOPENER platform; Related work section
presents the related work in linking and querying cancer
genomics repositories; and Conclusion section draws
the conclusion from our work.
Motivation
In order to understand the tumorigenesis, it is one
approach to compare normal and diseased tissue sam-
ples to interpret the changes in the expression patterns of
the genes with regards to the observed disease status. In
our case, Illumina Human Body Map (HBM) 2.0 serves
the purpose to identify similarities in gene expression
patterns using the studies across different tissue types,
where HBM discloses the similarities between human
tissues on the molecular and genetic level. Due to over-
laps between cancer behaviors, progression, and mutated
genes, we have selected top 1006 genes by a filtering
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 3 of 16
criteria based on the Reads Per Kilobase of transcript per
Million mapped reads (RPKM) values. Further, these top
100 genes identified are linked using the genetic features
such as genomic loci (start, end), beta value, cell cycle etc.
from previously observed studies in COSMIC and TCGA
repositories. The work presented in this article covers only
non-synonymous (NS) mutations. Since many somatic
mutations are passenger  synonymous mutations  and
do not impact tumorigenesis, we first select those genes
that are more likely to be drivers. The selection of driver
genes is based on the mutations frequency (RPKM value).
Illumina Human Body Map (HBM) 2.0: HBM covers
data from transcriptome studies for 16 tissue types. Sam-
ples for these 16 tissue types have been processed, aligned
and finally expression level have been determined [12].
Sequencing has been performed to provide both paired-
end and single-end libraries (read-length of 50bp and
75bp). A list of differentially expressed genes are extracted
using the step 2 (assemble expressed genes and tran-
scripts) of procedure suggested by Trapnell, C. et al. [11].
The gene expression data extracted from HBM samples
returns a very large list of more than 52000 genes. For
data processing reasons we chose to reduce the list and
therefore defined the cut-off for each RPKM value accord-
ing to the method suggested by Sandberg et.al [13]. As
a result, the data for each tissue type includes both the
coverages and the RPKM values as the corresponding
expression level. The RNA seq dataset provides addi-
tional relevant data such as CNV, fusion genes, structural
variation, differentially expressed genes, novel mutations,
splice junctions and transcriptome variations [14].
Annotation Databases (COSMIC & TCGA): The main
focus of this work is the identification of patterns
for cancer mutations and globally known mutations
and their types for selected differentially expressed
genes across different tissue types. Figure 1 shows the
correspondences, i.e., the associations or links that have
been established between the TCGA and COSMIC
databases for this purpose. For this task, our primary con-
cern has been the associations between the CNV, the
known mutations, and the gene expression data.
As part of our initial work [10], we have identified
instances to link in the COSMIC and TCGA datasets
(see Fig. 1). For example, GENE_NAME is used to
establish links between COMPLETE_MUTATION and
GENE_EXPRESSION datasets between both the repos-
itories. Similarly, GENE_NAME and HUGO_SYMBOL
has been used to link COMPLETE_MUTATION from
both the datasets. Further, CNV datasets from COS-
MIC and TCGA have been linked based on chr:start_end
position. From the computational perspective, the links
(owl:sameAs) between COMPLETE_MUTATION and
GENE_EXPRESSION datasets using the GENE_NAME
property allow to create a subset of driver genes from a
larger complete set of mutations.
Annotation Databases (REACTOME, KEGG, & GO
processes): Weobserve a set of prospective links through
the DNA methylation datasets  from COSMIC and
TCGA  to GO proliferation Ids. These links broaden our
understanding of the cell proliferation (with frequently
mutated genes) where changes in methylation level regu-
late the gene expression. In order to target certain genes,
it is important to find the affected cancer types and the
common pathways associated with the cell proliferation.
The KEGG and REACTOME datasets provide additional
links to identify genetic profiles from already identified
mutations in COSMIC and TCGA datasets. Clinical vari-
ations of any mutation from the REACTOME dataset will
help to explore clinical relevant targets, effects of down-
regulation of each pathway and alternate pathways for the
cell.
Figure 2 shows a set of prospective owl:sameAs
links between COSMIC, TCGA, REACTOME, KEGG,
Fig. 1 Links between COSMIC and TCGA datasets
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 4 of 16
Fig. 2 Links between COSMIC, TCGA, REACTOME, KEGG, and GO datasets
GO datasets. For example: (i) if Gene Symbol used
in the TCGA gene expression gets linked (through
owl:sameAs) with the Gene Symbol of COSMIC methy-
lation datasets, then a simple query can fetch result about
the changes in a promotor region associated with muta-
tions already identified in TCGA and COSMIC datasets;
(ii) similarly, ENSEMBL ID used in COMSIC, TCGA,
and Gene Ontology datasets can be linked to obtain the
transcript level changes with mutated gene in order to
understand the disease progression; (iii) finally, by linking
COSMIC and TCGA Methylation datasets provides us
the measure of beta value changes, the responders, and
non-responders based on hyper and hypomethylation. In
our initial work [10], we have identified MYH7 as one of
the potential biomarker based on copy number variation
(CNV) frequencies. In this article, we are aiming to link
the identified mutations (from COSMIC & TCGA) across
KEGG, REACTOME, and GO datasets to understand the
metabolic process of each reaction and the localization
of each component of a reaction further connecting the
metabolic process to pathways described in the KEGG
dataset.
Methods
The BIOOPENER approach is fundamentality similar to
the Bio2RDF7[15, 16] framework that created a mashup of
linked data connected through various linking properties
(e.g., xRef, owl:sameAs, x-relation) [17]. BIOOPENER
focus is specifically around discovering and exploiting
the owl:sameAs links for constructing complex feder-
ated queries  due to the precise owl:sameAs seman-
tics [18]  across multiple datasets. We now present
the BIOOPENERs architectural, linking, and querying
methodology.
BIOOPENER architecture
The BIOOPENER architecture is summarized in Fig. 3
showing all three major components. First, the RDFiza-
tion component that generates Linked Data from the
COSMIC, TCGA, REACTOME, KEGG, GO databases
results into several SPARQL endpoints. It is important
to note that, the two datasets (COSMIC and TCGA)
are converted from the raw format to RDF; further, we
linked COSMIC and TCGA to REACTOME8, KEGG9,
and GO10 datasets hosted at the Bio2RDF11. Second, the
linking component searches and discovers links between
selected datasets. The links discovered by this component
have an effect on the efficiency of the source selection, on
the query planning, and on the overall query execution
over distributed SPARQL endpoints. Third, the scalable
query federation component: it a single-point-of-access
through which distributed data sources can be queried in
the concerto.
The scalable query federation is based on the SPARQL
query federation engine called SAFE [19], which has
been developed for accessing distributed clinical trial
repositories. SAFE provides a single-point-of-access
through which distributed data sources can be queried
in unison. SAFE has been adapted to improve the effi-
cient integration of data from the different COSMIC,
TCGA, REACTOME, KEGG, GO SPARQL endpoints.
More specifically, SAFE makes use of a favorable distribu-
tion of data to reduce the number of sources required for
processing federated SPARQL queries (without compro-
mising recall). SAFE retrieves results from the large-scale
repositories by (i) efficient source selection as per the
capabilities of genomics repositories; (ii) query planning
mechanism to decompose a query and build resultant data
set from several sub-queries; (iii) query optimisation to
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 5 of 16
Fig. 3 BIOOPENER: Linking & Querying Cancer Genomic Resources
execute the sub-queries; and (iv) query execution mech-
anism retrieve and integrate results. This approach is
based on the principle that integrated data sources allow
querying of multiple data sources in a single search,
independently of their status being distributed or cen-
tralized, whereas traditional methods of data integra-
tion rather map the data models to a single unified
model.
RDFization
The raw data files  of COSMIC and TCGA repos-
itories  are available in the tab separated text (tsv)
format, which are transformed into the RDF format
using our in-house RDFizer tool that generates the N3
triples. The transformed RDF data from each cancer
type are hosted as different SPARQL endpoints. The four
types of data have been included from COSMIC, i.e.,
gene expression, gene mutation, CNV, and methylation.
From TCGA we have RDFized three types of data, i.e.,
CNV, gene expression and methylation for four cancer
types, namely Ovarian Serous Cystadenocarcinoma (OV),
Cervical Squamous Cell Carcinoma and Endocervical
Adenocarcinoma (CESC), Uterine Corpus Endometrioid
Carcinoma (UCEC) and Uterine Carcinosarcoma (UCS).
Table 1 shows the overall statistics of the RDF datasets:
row 1 represents for the COSMIC gene expression data
the corresponding triples generated (column 3), the num-
ber of subjects (column 4), the number of predicates
(column 5), the number of objects (column 6) and its RDF
data size (column 7). Rows 2-4 represent the same type
of data for the COSMIC gene mutation, CNV and methy-
lation data, respectively. A total of 154 million records
has been RDFized, producing approximately 1.2 billion
triples, for COSMIC datasets. Row 5-8 represents the
statistics for the RDF version of TCGA-OV, TCGA-CESC,
TCGA-UCEC, and TCGA-UCS, respectively. Rows 9-10
represent the RDF data statistics for KEGG, REACTOME
and GOA datasets, respectively. These three datasets are
external as we have not transformed them into the RDF
format but instead used the already available RDF versions
from Bio2RDF.
Linking
We propose a linked data based approach to create corre-
spondences (links) between dispersed cancer and biomed-
ical datasets. These datasets contain rich information and
helpful in answering the biological questions targeted in
this article. These links, once identified and established,
will sustain and support the query federation over dis-
tributed repositories (discussed in the Scalable query
federation section).
COSMIC and TCGA linking: we perform linking of the
COSMIC and TCGA datasets. We have employed the
owl:sameAs construct to establish links across entities
based on the semantic properties highlighted in Fig. 1.
For example, the entities that contain information about
Gene Symbol, TCGA_ID, ENSEMBL ID have been linked
using owl:sameAs. An example link between COSMIC
and TCGA is shown in the Listing 1, where two COSMIC
sample ids have been identified as being identical to two
TCGA patient bar code ids.
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 6 of 16
Table 1 RDF Data Statistics
No. Data Triples Subjects Predicates Objects Size (MB)
1 COSMIC GE 1184971624 148121454 18 148240680 10000
2 COSMIC GM 83275111 3620658 23 9004153 1400
3 COSMIC CNV 8633104 863332 10 921690 122
4 COSMIC Methylation 170300300 8292057 22 603135 2800
5 TCGA-OV 81188714 10974200 15 4774584 3774
6 TCGA-CESC 3763470 627652 43 481227 49557
7 TCGA-UCEC 553271744 19233824 91 68370614 84687
8 TCGA-UCS 1120873 183602 36 188970 10018
9 KEGG 50197150 6533307 141 6792319 4302
10 REACTOME 12471494 2465218 237 4218300 957
11 GOA 28058541 5950074 36 6575678 5858
<Link?1>
<Source>COSMIC</Source>
<Target>TCGA?OV</Target>
<link>
cosmic:TCGA?13?0920
<sameAs>
tcga:TCGA?13?0920
</link>
</Link?1>
<Link?2>
<Source>COSMIC</Source>
<Target>TCGA?OV</Target>
<link>
cosmic:TCGA?24?1850
<sameAs>
tcga:TCGA?24?1850
</link>
</Link?2>
Listing 1 COSMIC and TCGA Linking Example
The example links generated in our use-case are shown
in the Fig. 4. The COSMIC and TCGA datasets have
been integrated using the owl:sameAs construct. For
instance, MYH7 (which is an RDF resource of type Gene
Symbol) in both COSMIC and TCGA datasets is linked
using owl:sameAs. To understand the promotor genes
and their deviation, the methylation datasets of COS-
MIC and TCGA are linked to retrieve beta values for a
given set of CNVs. For instance, cg00000292 which is an
RDF resource of type Composite Element REF in both
COSMIC and TCGA datasets have been linked using
owl:sameAs. Similarly, Fig. 4 shows the owl:sameAs
links between COSMIC and TCGA datasets for TCGA-
13-0920 and TCGA-24-1850 (RDF resources of type
Sample_ID).
Linking COSMIC and TCGA with REACTOME,
KEGG, & GO: We link COSMIC and TCGA with Gene
Ontology (GO) datasets to understand the biological pro-
cessed involved with each mutation or CNVs and the
underlying impact of these mutations on cancer and
healthy cells. From the Fig. 4, it is evident that we
have linked ENSMUSP00000018795  which is an RDF
resource of type Ensemble ID  in COSMIC dataset
with the similar resource in GO dataset. This will help
in retrieving the gene behavior of healthy cells (from
Illumina Body Map) compared to the diseased TCGA
samples by tracking the GO process involved in the onco-
genesis. By enabling links between COSMIC and GO
datasets, we are now able to find links across Reactome
and KEGG datasets. This will allow tracking the changes
in healthy cells based on their pathway activities to iden-
tify the disease and biological process related pathways.
For instance, the Ensemble ID from COSMIC is linked
with the Ensemble ID in GO dataset providing us the
GO processes and the GO IDs associated with these
processes. These are further linked with their respec-
tive KEGG and Reactome IDs. The linking across these
datasets are shown in Fig. 4.
The number of links generated in case of COSMIC
and TCGA datasets, and the number of identified links
between KEGG, GO, and Reactome datasets are shown in
the Fig. 5. For instance, a total number of 121916 links are
generated in COSMIC to link them with TCGA. Similarly,
46112 links are generated to integrate TCGA with TCGA
Methylation datasets, 891612 links are generated to link
TCGA Methylation dataset with GOA (Gene Ontology
Annotation) dataset, and 41424 links are generated to
integrate TCGAMethylation dataset with Reactome.
On the other hand, we identified a total of 1049858
existing links  within Bio2RDF  between GOA and GO
datasets. A total of 1810 outgoing links to KEGG fromGO
and 7359 incoming links to GO from KEGG were identi-
fied. A total of 28808 links were discovered between GO
and Reactome datasets.
Scalable query federation
We have developed a query federation engine  called
SAFE  for accessing sensitive clinical data at different
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 7 of 16
Fig. 4 Example Links between COSMIC, TCGA, KEGG, REACTOME, and GO Datasets
locations [19]. Two main changes have been intro-
duced to SAFE for efficiently querying the COSMIC,
TCGA, KEGG, Reactome, and GO SPARQL endpoints.
First, standardise RDF query representation: in the ini-
tial version [19], SAFE issues queries for statistical clin-
ical information stored within distinct names graphs
for RDF data cubes [20]. Therefore, the internal query
processing (i.e., source selection, query planning, query
execution) had to be adapted to query the regular
RDFized versions of the COSMIC, TCGA, KEGG, Reac-
tome, and GO datasets. Second, access control had
to be disabled: SAFE imposes restrictions for data-
access as a feature (defined as Access Policy Model
[19]) while federating queries over multiple clinical sites,
i.e., imposing the data restrictions for different data
repositories. Since experiments conducted in this paper
mainly involve public repositories this feature has been
disabled.
Fig. 5 Link Statistics
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 8 of 16
Figure 3 shows SAFEs three main components within
the BIOOPENER platform: (i) Source Selection: performs
multilevel source selection based on the capabilities of
data sources; (ii) Query Planning: filters the selected
data sources based on access rights defined for each
user; and (ii) Query Execution: performs the execution of
sub-queries against the selected sources and merges the
results returned.
Source Selection: SAFE performs a tree-based two-level
source selection as shown in Fig. 6. At Level 1, like other
query federation engines [2123], we do triple-pattern-
wise endpoint selection, i.e., we identify the set of relevant
endpoints that will return non-empty results for the indi-
vidual triple pattern in a query. At Level 2 (unlike other
query federation engines), SAFE performs triple-pattern-
wise named graph selection, i.e., we identify a set of rel-
evant named graphs for all relevant endpoints already
identified at Level 1. SAFE relies on data summaries to
identify relevant named graphs.
Query Execution: The Listing 2 shows an SPARQL
query, which federates across COSMIC and TCGA data
asking for genomic loci of a mutated gene by chro-
mosome start points which then returns the disease
metastasis information along with the mutation type.
Answering such a query requires the integration of
COSMIC with TCGA and merging results from both
TCGA and COSMIC, and thus has to make use of
query federation. The results for the first four triple
patterns in the given query (i.e., cosmic:sample,
cosmic:gene, cosmic:start) are fetched from
COSMIC and the results for the next four triple patterns
(i.e., tcga:hybrid_ref, tcga:gene, tcga:start)
are fetched from TCGA. Further, both results are
merged on the basis of the last triple pattern (gene_c
owl:sameAs gene_t) which integrates COSMIC with
TCGA. Sample results for this query can be seen
in Fig. 9.
?cosmic_meth a cosmic:Methylation;
cosmic:sample ?sample;
cosmic:gene ?gene_c;
cosmic:start ?start_c.
?tcga_meth a tcga:Methylation;
tcga:hybrid_ref ?tcga_id;
tcga:gene ?gene_t;
tcga:start ?start_t.
?gene_c owl:sameAs ?gene_t.
}
Listing 2 SPARQL Query Federation: Genomic loci of a mutated
gene by chromosome start points
In our initial work [10] we queried mutations and CNV
data to identify the novel mutations and their somatic
behavior from healthy to cancer cells. The Listing 3 shows
a SPARQL query, which extracts promoter level changes
occurred due to mutations extracted from query shown
in the Listing 2. This requires linking across the COSMIC
andTCGAMethylation datasets. The first three triple pat-
terns fetch data from COSMIC and the next three triple
patterns fetch data from TCGA. The last triple pattern
provides a link  owl:sameAs between genes  for merging
data from both the data sources.
?cosmic_meth a cosmic:Methylation;
cosmic:gene ?gene_c;
cosmic:beta_value ?beta_value_c.
?tcga_meth a tcga:Methylation;
tcga:gene ?gene_t;
tcga:beta_value ?beta_value_t.
?gene_c owl:sameAs ?gene_t.
}
Listing 3 SPARQL Query Federation: Mutations causing
promoter level changes
The SPARQL query listed in Listing 4 have covered 3
distinct sources, i.e., methylation from TCGA and COS-
MIC datasets with associatedGeneOntology Annotations
Fig. 6 Tree-based two level source selection
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 9 of 16
(GOA). TCGA provides the changes in methylation per
composite element, whereas in COSMIC we have such
changes on the gene level. To retrieve both the gene and
promoter level information, we have queried genes from
both data sources and extracted all the promoter regions.
Once the promoter regions are identified, it is essential to
understand the processes involved in these regions. This
helped us to query GOA for extracting the processes on
the promoter and gene levels. If a gene level change do
not comply with promoter level changes, it is an indica-
tion of what processes of the gene have mutated them.
Such results can be obtained through a federated query
with three data sources, i.e. COSMIC, TCGA, and GOA.
The Listing 4 provides an example federated query where
the first three triple patterns are answered fromCOSMIC,
the next three triple patterns are answered from TCGA
and the seventh triple pattern merges result obtained
from COSMIC and TCGA through gene. The eighth
and ninth triple patterns fetch data from GOA which is
finally merged with COSMIC and TCGA datasets using
the gene information.
?cosmic_meth a cosmic:Methylation;
cosmic:gene ?gene_c;
cosmic:beta_value ?beta_value_c.
?tcga_meth a tcga:Methylation;
tcga:gene ?gene_t;
tcga:beta_value ?beta_value_t.
?gene_c owl:sameAs ?gene_t.
?go_gene go?vocab:process ?process.
?process dcterms:title ?cell_cycle.
?gene_t owl:sameAs ?go_gene.
}
Listing 4 SPARQL Query Federation: Methylation changes
The SPARQL query shown in Listing 5 finds associ-
ations between the genes, pathways and biological pro-
cesses. We queried the healthy genes from Illumina Body
Map against all mutations obtained from TCGA and
COSMIC to find their DNA and promoter level methyla-
tion changes. In order to explore the gain and loss on a
disease at the phenotype level, we have included KEGG
and REACTOME sources which map each discovered
gene with its biological process for phenotype and process
driven pathways. The Listing 5 shows a federated SPARQL
query, where the first three triple patterns are answered
from TCGA; and the next five triple patterns fetch and
merge data from REACTOME and GOA. The last five
triple patterns obtain results from KEGG and merge them
with the rest of results.
?tcga_meth a tcga:Methylation;
tcga:gene ?gene_t;
tcga:beta_value ?beta_value_t.
?go_gene go?vocab:process ?process.
?process dcterms:title ?cell_cycle.
?gene_t owl:sameAs ?go_gene.
?pathway a biopax:Pathway;
biopax:displayName ?display_name;
biopax:organism ?organism;
biopax:xref ?id. ?id biopax:id ?go_gene.
?kegg_res a kegg:Resource;
rdfs:label ?label;
dcterms:title ?title;
kegg?vocab:reference ?ref;
kegg?vocab:x?go ?process.
}
Listing 5 SPARQL Query Federation: Genes, pathways, and
biological processes
The Listing 6 retrieves the methylated promotor
regions. The query shown in Listing 6 extracts the location
of methylation based on the input genes, composite ele-
ment REF (promotor region) and chromosome number.
For instance, we have queried MYH7 (gene) for promo-
tor region cg05744229 at the chromosome 14 (region of
methylation) and extracted two promotor regions from
TCGA and COSMIC with the start value of DNA pro-
motor range such as 23904678 (TCGA) and 23435469
(COSMIC).
SELECT ?promoter_region ?start_c ?start_tWHERE {
?cosmic_meth a cosmic:Methylation .
?cosmic_meth cosmic:chromosome ?chr.
?cosmic_meth cosmic:gene ?promoter_region .
?cosmic_meth cosmic:start ?start_c . FILTER (?
promoter_region = <http://sels.insight.org/cancer?
genomics/gene/cg05744229>)
?tcga_meth a tcga:Methylation .
?tcga_meth tcga:gene <http://sels.insight.org/genomics/
gene/MYH7>.
?tcga_meth tcga:chr ?chr.
?tcga_meth tcga:start ?start_t. FILTER (?chr = <http://sels.
insight.org/genomics/chrom/14>)
}}
Listing 6 SPARQL Query Federation: Methylated promotor
regions
Listing 7 shows an example federated SPARQL query
derived from the Listing 2 for a specific gene, namely
MYH7. Similarly, we have executed the federated queries
shown in the Listings [2-6] for each of the hundred (100)
genes extracted from the Illumina Body Map, mentioned
above.
?cosmic_meth a cosmic:Methylation; cosmic:sample ?
sample; cosmic:gene ?gene_c; cosmic:start ?start_c.
?tcga_meth a tcga:Methylation; tcga:hybrid_ref ?tcga_id;
tcga:gene tcga:MYH7; tcga:start ?start_t.
?gene_c owl:sameAs tcga:MYH7.
}
Listing 7 SPARQL Query Federation: Genomic loci of MYH7 gene
by chromosome start points
The query execution time for these gene-specific
queries is shown in the Table 2. The Query column
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 10 of 16
Table 2 Query Execution Time (QE=Query Execution)
Query QE Time (msec) Results (No. of Triples) Datasets
Listing 2 2110 21390 (TCGA)(COSMIC)
Listing 3 5732 33264 (TCGA)(COSMIC)
Listing 4 43092 63765 (TCGA)(COSMIC)(GOA)
Listing 5 263463 232848 (TCGA)(GOA)(REACTOME)(KEGG)
Listing 6 3481 25669 (TCGA)(COSMIC)
lists individual queries (e.g., listings [2-6]), QE Time,
Results (No. of Triples) and Datasets columns show
the query execution time in millisecond (msec), number
of triples returned as a result and the datasets required for
executing individual queries.
Results
We analyse the genes having RPKM value > 0.3747 and
differentially expressed in all tissue types. Figure 7 shows a
list of 100 genes retrieved from the HBM datasets, which
are highly expressed in 16 different tissues. We have iden-
tified potential cancer types based on the gene patterns
for different tissues that helped further to understand the
behavior of most amplified cancer types. The overall goal
of this study is to understand the relevance and associa-
tion of mutation, genes expression, and promoter region
by:
? Analysing the normal tissues expression levels,
enriched and affected pathways along with their
associated expression levels and changes obtained
from the HBM 2.0 datasets.
? Analysing the normal tissues expression levels against
the somatic mutations linked and retrieved from the
COSMIC and TCGA datasets.
th
yr
oi
d
ov
ar
y
pr
os
ta
te
te
st
is
br
ai
n
br
ea
st
co
lo
n
ad
ip
os
e
ki
dn
ey
ad
re
na
l
le
uk
oc
yt
e
lu
ng
ly
m
ph
 n
od
e
sk
el
et
al
 m
us
cl
e
he
ar
t
liv
er
APCS
AMBP
ALB
FGA
APOC3
APOH
GC
FGL1
APOA2
CRP
ORM1
ORM2
HP
FGB
FGG
SAA2
SERPINA1
APOA1
AGT
SAA1
RBP4
C3
SCD
TG
PRM1
PRM2
TNP1
DES
PDK4
MT?TP
KLHL41
MYBPC1
CKM
GAPDH
ACTA1
MT?RNR1
MT?RNR2
MT?ND1
MT?ND2
MT?CO2
MT?ND4
MT?CO3
MT?ND3
MT?CYB
MT?ATP6
PLN
MYL3
ACTC1
MYH7
MYL2
FABP3
MB
MT?ND5
MT?CO1
MT?ND4L
MT?ATP8
MTATP6P1
MT?ND6
MALAT1
GPX3
SPP1
HBB
SCGB1A1
HLA?E
FTL
B2M
TMSB4X
SRGN
TMSB10
HLA?DRA
CD74
LYZ
S100A9
S100A8
RPS12
ACTB
EEF1A1
FABP4
TXNIP
SEMG1
MYL9
RPS27
RPS11
IGFBP4
IGLC3
IGLC2
IGLV3?19
IGKC
IGHV3?23
IGHG2
IGKV4?1
IGHG1
IGLV3?25
IGKV3?20
IGHV1?2
IGKV1?5
IGHM
IGHA1
JCHAIN
CCL21
Fig. 7 HBM: List of genes expressed in all tissues and highly expressed
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 11 of 16
? Classifying the mutations obtained from above two
steps in terms of biological processed and pathways
from GO, KEGG, and REACTOME
We now discuss and analyse the results obtained from
the BIOOPENER platform through linking and querying
the cancer and biomedical repositories.
Analysis: HBM, COSMIC, and TCGA
Initially, we have selected top 100 genes that are highly
expressed in all 16 tissues as shown in the Fig. 7 to
(i) retrieve their CNV, mutation, gene expression and
methylation annotations from cBioPortal12; (ii) retrieve
methylation from CNV annotator13 and UCSC Cancer
Genomics Browser14; and (iii) retrieve mutation datasets
from TCGA [24]. The results from TCGA (Fig. 8) clearly
indicate a mutation frequency elevated distribution of
these genes in UCS, CESC, UCEC and OV cancers. In
Fig. 8 TCGA query output from cBIO Portal (Blue:Deletion,
Red:Amplification, Green:Mutation, Brown:Multiple Alterations) [43]
Fig. 8 we observe average percentage case mutations in
the UCS, UCEC, CESC and OV cancers are 87.5% ,58.3%,
57.6%, 81.4% respectively. This outcome justifies the selec-
tion of UCS, UCEC, CESC and OV as good candidates for
further investigation due to its elevated amplification rate
and its multiple repetition in different experiments.
This study targets genes based on their contribution in
mutations15, the listing 8 shows the highly relevant driver
genes transforming healthy human tissues into diseased
ones for respective cancer types.
OV: TG, MRPS12, GAPDH, TXNIP, S100A9, S100A8, RPS27, ALB
, CRP, LYZ, and MYH7
CESC: ND5, TG, AGXT, MYH7, FGA, APOC3, APOA1, C3, APCS,
FBF1, SERPINA1, S100A9, and TXNIP
UCS:MRPS12, TG, SEMG1, ND5, DLC1, CKM, ND4, ND1, FGL1,
and RPS27
UCES: TG, MYH7, DLC1, C3, TXNIP, FGA, AGT, S100A8, CRP,
S100A9, APCS, and GC
Listing 8 Highly relevant driver genes for the OV, CESC, UCS, and
UCES cancer types
The overlap and frequency among these four cancer
types results into the discovery of top 20 biomarkers
shown in the listing 9). Table 3 shows the potential
chromosome locations chr14,chr5,chr6,chr19 and genes
TG,TXNIP,GC,MYH7 with high relevance in the progres-
sion of four gynecological cancer types.
TG, MRPS12, MYH7, DLC1, GAPDH, TXNIP, C3, ND5, S100A8,
RPS27, FGA, AGT, CRP, ALB, LYZ, APCS, GC, APOA2, MYBPC1,
ACTA1
Listing 9 Top 20 Biomarkers for the OV, CESC, UCS, and UCES
cancer types
Figure 9 shows the COSMIC and TCGA annotations.
The CNV datasets doesnt use Gene symbol property
(or predicate) and it is important to map (or link) genome
regions with gene symbols to retrieve CNV information
from different datasets. We implemented a linking rule
based on the chr_no,chr_start and char_end properties
(or predicates) to retrieve the CNV information across
datasets to identify genes within the extracted loci. Result
of this annotation are shown in the Table 3. It is evi-
dent that the MYH7 gene has many copies reported in
the COSMIC datasets as well as in the TCGA datasets
suggesting it a potential biomarker for four gynecolog-
ical cancer types. The TG and MYH7 genes are highly
mutated as they are repetitively appearing on multiple
chromosomes. For instance, MYH7 primarily carried the
LOSS type of a mutation for chr14 which is a dominant
mutation with all its regulation of over, under and nor-
mally expressed. Translational researchers may want to
repeat and re-validate the study for Pubmed ID:1398522
with the beta value  as a measure of methylation  of
0.041999536. The scaled estimation (Tumour purity) of
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 12 of 16
Table 3 loci information for highly expressed gene in ovarian cancer from HBM 2.0
Chr Star-End Mutation Type Genes PMID
19 90910 -715430 GAIN FGF22, RNF126, TG 2066845
120668450
9 4069657-4684967
591967-608659
11090336-11098891
8009428-8015596
8109010-8121257
1373387-1383725
11090336-11098891
10547511-10547923
3113846 -3134738
8115293 -8121487
9269903 -9294415
46587-510700
5106680-5106800
LOSS/GAIN LKB1,P16INK4A,TRAF2,XPA,
PTCH1,FANCC,DMRT3,WNK2,C9orf89,
SYK,CKS2,CTSL1,NTRK2,KIF27,PTPRD,
TLE4,CEP78,GNAQ,PRKACG
21062161
17311676
16585170
20668451
21781307
6 149661-384546 LOSS TAP1,NOL7,CD83,POUF3,MYH7,PLN,PKIB,PDSS2
OSTM1,NUS1,TG,NT5DC1,NR2E1,NKAIN2
21062161
20668451
21781307
20668451
21720365
5 15532-24132 GAIN TRIP13, TRIO,TARS,SUB1,SLC12A7,
SKP2,SDHA,RPL37,MYH7,RNASEN,RAI14,
RAD1,POLS,PDCD6,PAIP1,OSMR,NNT
18559093
21062161
14 23857092-23886486
23857082-23886607
LOSS MYH6, MYH7, TG, ACTA1 18559093
21062161
773.555 supports this gene (MYH7) from the methylation
aspect to detect promoter level changes in the four cancer
types. Further multiple genomic locations will help clin-
ical practitioners to find a potential CNV for a targeted
study ultimately helping towards a better prognosis.
Figure 10 shows the annotation of twenty (20)
discovered biomarkers (genes) where promoter level
changes are occurring on the extreme changes of -ve
or +ve beta-values in all the four cancer types stud-
ied in this article. The most affected genes due to
these promotor level changes are: MYH7, TG, DLC1,
S100A8. As reported in our initial work [10] major
changes are occurring nearby -0.773 beta-value and
their corresponding composite element reference ids
are cg01429391, cg05744229, cg26670875, cg18205205,
cg21242212, cg08240074, cg13785779, cg05744229. Most
of these changes are occurring around chromosome 1 and
14 and 5 UTR. Next section discusses the mechanism
behind these changes and their pathway analysis.
Analysis: GO, KEGG and REACTOME
We have identified twenty (20) genes in terms of mutation
frequencies and CNV together with the promoter level
changes in methylation data. However, we are unaware
of the mechanism involved in combined effects of these
twenty (20) genes. We have queried linked pathways
and coalitions over the GO, KEGG, and REACTOME
datasets. Figure 11  snippet generated from ClueGO
[25, 26]  shows the muscle filament sliding pathway as
a key in rare cancer types such as retinoblastoma where
effective actin filament formation with Myosin (MYH4)
is a prime regulator [27]. Our approach has identified
actin (ACTA1) and myosin (MYH7) combination with
MYBPC1 as the potential pathways causing promoter
Fig. 9 Linked annotations for MYH7 - COSMIC
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 13 of 16
Fig. 10 Promotor level methylation changes in biomaker genes
changes in gynecological cancers. Its evident that alter-
ations in the activity and/or expression patterns of actin-
bundling proteins could be linked to the cancer initiation
or progression [28]. Haitian Lu, et al. suggests that the
acute inflammatory response is associated with cancer
development because inflammatory micro-environment
inhabits various inflammatory cells [29]. A network of
signaling molecules are indispensable for the malignant
progression transformed cells attributed to the mutagenic
predisposition of persistent infection-fighting agents at
Fig. 11 Three pathways causing promoter changes in four gynecological cancer types
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 14 of 16
the sites of chronic inflammation causing cancer devel-
opment in various tissues [29]. In our case, the rea-
son behind significant methylation changes associates
with the pathway peptidyl-cysteine s-nitrosylation. The
dysregulation of s-nitrosylation in severe pathological
events including cancer onset, progression, and treatment
resistance leads to controlled epigenetic and treatment
response [30]. Figure 10 explains the gene associated with
each pathway and their contribution for OV, CESC, UCS,
and UCSC cancer types. In this article, we demonstrated
that well-connected datasets allow to construct complex
biomedical queries (e.g., listings 2-6) covering variety of
genetic and biological features (cnv, gene symbol, methy-
lation, cell cycle, protein, pathway, etc.) that can span
through broad range of multiple repositories.
Related work
Kandoth et al. [31] performed a cancer study with 12
cancer types to enable logical classifications for the large
amount of data generated by TCGA and ICGC. Saleem
et. al. [32] have covered TCGA database with few cancer
types and for a limited number of patient data. Simi-
larly, a reduced version of the COSMIC database has been
RDFized to explore on the mechanism of TP53 [33]. The
federation platform [34] called TopFed is being devel-
oped to measure the query execution time on TCGA
data set, which then has been further extended to cover
the biological outcomes identified fromMedline abstracts
[35]. A similar platform such as FIREBROWSE16, Web-
TCGA [36], and PCAWG17 have been built for TCGA
dataset covering a wide range of genomic signatures
and pan-cancer analysis. Gene and methylation annota-
tion platforms such as omics4tb18 and Genevisible [37]
help to decipher individual genes and their association
annotated from TCGA. From the computational perspec-
tive, our goal is not to create yet another repository (or
database), but to link the already existing ones for use in
various analytical methods. We demonstrated that well-
connected datasets allow to construct complex biomed-
ical queries (e.g., listings 2-6) covering variety of genetic
and biological features (cnv, gene symbol, methylation,
cell cycle, protein, pathway, etc.) that can span through
broad range of multiple repositories. The enrichmen-
t/linkage between COSMIC and TCGA datasets had been
crucial to identify novel mutations. The approaches taken
in DoCM [38], ICGC [39], and DIRECT [40] are comple-
mentary to our work in the sense that, discoveries sug-
gested by the BIOOPENER platform are the most likely
mutations/genes/pathways which can be further validated
through creating links with the well-curated reposito-
ries (DoCM, ICGS, and DIRECT ). Such validation is
outside the scope of this article; however, we do plan
to include well-curated databases in the next phase of
BIOOPENER project. Similarly, we plan to extend linking
with the ICGC [39] datasets that contains primary and
blood samples providing further insight into the metas-
tasis of primary tissues. Our current work covers copy
number variation (CNV), genes, somatic mutation, and
promotormethylation which targets highlymutated genes
(on different tissues) and associated pathways. As far as
we know, the work presented in this article is one of the
first initiatives in discovering biomarkers and pathways
for female gynecological cancer types covering five large-
scale cancer and biomedical repositories.
Discussion
As discussed above, the NGS technologies are produc-
ing a massive amount of sequencing datasets [5, 8]. A
top-up of approximately 40 petabytes of genomic infor-
mation every year is foreseen from a wide variety of
data sources published by human genome research cen-
ters worldwide [41]. Often, these datasets are published
from isolated and different sequencing facilities. In cancer
genomics, description of biological and genetic entities
are available in several overlapping and complementary
data sources containing complex genomic features, stud-
ies, and associations of such features [17, 42]. In order
to understand the tumorigenesis, it is often the case that
several genetic features, diseases, medical history, etc. are
studied together, therefore, one of the key challenge in
cancer genomics  a cornerstone of precision medicine 
is to discover gene-disease-drug data links and associ-
ations which may provide novel insight into new drug
development techniques tailored specific for an individ-
ual patient (or a group of patients) targeting prevention,
diagnosis and treatment of the diseases.
In cancer genomics field massive amount of data exist
with complex associations. To understand these complex
associations, it requires to fetch all possible gene-disease-
drug combinations, for instance:
? Multiple pathways are involved to translate a
particular gene
? A single disease can be treated by eliminating effect
of the combination of multiple drugs
? Selection of these drugs is majorally based on the
inhibitors (i.e., combination of gene-pathways)
? Effect of one pathway alteration can change the
modification of single gene and yields into multiple
genes
In this article, we aimed to understand the associations
between genetic, cellular and molecular features across
isolated cancer datasets giving insight into cancer pro-
gression from normal to diseased tissues. Correlation of
genes in OV, UCS, UCEC, & CESC clearly indicates that
gynecologically induced cancers do have common mech-
anism and overlapping pathways. Which means, a drug
Jha et al. Journal of Biomedical Semantics  (2017) 8:40 Page 15 of 16
created for one cancer type has a higher probability to be
effective for other associated cancer types.
Conclusion
In this paper, we have presented a data-interlinked plat-
form called BIOOPENER which enables querying dif-
ferent types of mutations and genomic alterations to
contribute to molecular and clinical insights of cancer
by defining most relevant variants and their prioritiza-
tion. This knowledge could be highly advantageous for a
targeted therapy and precision medicine based on gene
expression data. The presented experiments are based on
COSMIC, TCGA, REACTOME, KEGG, GO and HBM
2.0 datasets and have been used to identify sets of
genes with relevance for four female gynecological cancer
types - Ovarian (OV), Uterine Corpus Endometrial Carci-
noma (UCS), Uterine Carcinosarcoma (UCEC), Cervical
Squamous Cell Carcinoma and Endocervical Adenocar-
cinoma (UCES) - covering the 16 healthy tissue-specific
genes from Illumina Human Body Map 2.0. We discov-
ered 20 biomarkers (genes) in terms of mutation frequen-
cies and CNV along with the promoter level changes in
methylation data. We discovered three potential pathways
causing promoter changes in gynecological cancers. In
future, we plan to extend by covering the breast cancer
type including additional genomic signatures, e.g., fusion
gene, structural variations.
Endnotes
1 http://www.nature.com/nature/journal/v537/n7619_
supp/full/537S49a.html.
2 https://tcga-data.nci.nih.gov/tcga/.
3 http://cancer.sanger.ac.uk/cosmic.
4 https://www.ebi.ac.uk/gxa/experiments/E-MTAB-
513.
5 http://bioopenerproject.insight-centre.org.
6 https://github.com/yasarkhangithub/BioOpener/
blob/master/Top_100_Gene_List.txt.
7 http://bio2rdf.org/.
8 ftp://ftp.ebi.ac.uk/pub/databases/RDF/reactome.
9 http://download.bio2rdf.org/release/3/kegg/.
10 http://download.bio2rdf.org/release/3/goa/.
11 http://bio2rdf.org/.
12 http://www.cbioportal.org/.
13 https://omictools.com/cnv-annotation-category.
14 https://genome-cancer.ucsc.edu/.
15 https://github.com/yasarkhangithub/BioOpener/
blob/master/Mutation_Key_Genes_Cancerwise.xlsx.
16 http://firebrowse.org/.
17 http://pancancer.info/.
18 http://www.omics4tb.org/.
Acknowledgment
This article is based on a conference paper discussed at the SWAT4LS 2015,
Cambridge, UK [10].
Funding
This publication has emanated from research supported by the research grant
from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289.
Availability of data andmaterials
The BIOOPENER online demonstration website http://bioopenerproject.
insight-centre.org/ is available for the scientific uses and the relevant datasets
(in RDF) shown in the Table 1 are available at http://bioopenerfiles.insight-
centre.org/.
Authors contributions
AJ designed the study and helped in RDF data conversion, analysis and
concluding domain results. YK designed and implemented the query
federation and RDF conversion. MM and QM discovered the links across
cancer repositories. RK contributed to RDF data conversion and raw data
processing. AZ critically revised the manuscript. DR and RS have jointly
supervised the article. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 12 July 2016 Accepted: 30 August 2017
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 
DOI 10.1186/s13326-017-0154-9
RESEARCH Open Access
Analysis and visualization of disease
courses in a semantically-enabled cancer
registry
Angel Esteban-Gil1, Jesualdo Tomás Fernández-Breis2* and Martin Boeker3
Abstract
Background: Regional and epidemiological cancer registries are important for cancer research and the quality
management of cancer treatment. Many technological solutions are available to collect and analyse data for cancer
registries nowadays. However, the lack of a well-defined common semantic model is a problem when user-defined
analyses and data linking to external resources are required. The objectives of this study are: (1) design of a semantic
model for local cancer registries; (2) development of a semantically-enabled cancer registry based on this model; and
(3) semantic exploitation of the cancer registry for analysing and visualising disease courses.
Results: Our proposal is based on our previous results and experience working with semantic technologies. Data
stored in a cancer registry database were transformed into RDF employing a process driven by OWL ontologies. The
semantic representation of the data was then processed to extract semantic patient profiles, which were exploited by
means of SPARQL queries to identify groups of similar patients and to analyse the disease timelines of patients.
Based on the requirements analysis, we have produced a draft of an ontology that models the semantics of a local
cancer registry in a pragmatic extensible way. We have implemented a Semantic Web platform that allows
transforming and storing data from cancer registries in RDF. This platform also permits users to formulate incremental
user-defined queries through a graphical user interface. The query results can be displayed in several customisable
ways. The complex disease timelines of individual patients can be clearly represented. Different events, e.g. different
therapies and disease courses, are presented according to their temporal and causal relations.
Conclusion: The presented platform is an example of the parallel development of ontologies and applications that
take advantage of semantic web technologies in the medical field. The semantic structure of the representation
renders it easy to analyse key figures of the patients and their evolution at different granularity levels.
Keywords: Biomedical informatics, Semantic web, Cancer registry, Ontology
Introduction
Cancer registries are an important part of the health
information systems in local and regional health orga-
nizations. Regional and epidemiological cancer registries
are the foundation for cancer research and the qual-
ity management of cancer treatment. In most devel-
oped countries, the operation and the sampling of data
in cancer registries are statutory. Cancer registries are
*Correspondence: jfernand@um.es
2Dpto. Informática y Sistemas, Facultad de Informática, Universidad de Murcia,
IMIB-Arrixaca, Facultad de Informática, Campus de Espinardo, 30100 Murcia,
Spain
Full list of author information is available at the end of the article
complex structures for the documentation and analysis
of data from patients diagnosed with cancer [1, 2]. Dif-
ferent types of cancer registries collect patient data from
institutions (institutional), regions (regional) or complete
larger areas (epidemiological). Whereas epidemiological
registries provide mainly population-based information
onmorbidity and mortality, institutional and regional reg-
istries can provide fine-grained information on treatment
and conditional survival.
The information of regional cancer registries serves dif-
ferent requirements such as the quality control of patient
care, the comparison of patient-related outcome param-
eters and research support. Institutional and regional
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 2 of 16
registries are also the main data source for epidemiolog-
ical cancer registries. Regional cancer registries collect
information about diagnosis, therapies and course of the
disease [3], the most important being the histopathology
of the primary tumor, including tumor staging and grad-
ing. The long-term follow-up of the patients vital status
is one of the resource-intensive tasks of tumor registries
providing the basis for survival analysis.
Different software cancer registries solutions are cur-
rently available, such as METRIQ1, OncoLog Registry2 or
CNEXT3. The standardisation of the cancer registry soft-
ware is difficult because of a large set of rapidly changing
legal and scientific requirements. Most of these software
solutions suffer from two main limitations. The interop-
erability with other health applications such as Electronic
Medical Records (EMRs) is limited, which is a typical
problem of clinical information systems [4]. The hetero-
geneity of the underlying data models is a consequence of
the difference between data models in current cancer reg-
istry software [5, 6]. This imposes severe limitations on
research and on the progress of cancer studies when clini-
cal research activities need to integrate data from different
cancer registries of several regions.
There have been proposals to overcome the afore men-
tioned problems. In [7] the authors use the Unified Mod-
elling Language for modeling cancer registry processes
in a hospital. In [8] the authors propose a set of indica-
tors to evaluate specific quality measures in cancer care,
and [9] attempts to optimise cancer registries by means of
knowledge-based systems for monitoring patient records.
Unfortunately, these approaches do not guarantee the
generation of standard models and do not provide sat-
isfactory solutions to scenarios which require customis-
able, comparative analyses and data linking to external
resources [5].
On the technical side, the Semantic Web stack can
be employed to provide information with given well-
defined meaning, better enabling computers and people
to work in cooperation [10]. Ontologies [11] constitute the
standard knowledge representation mechanism for the
SemanticWeb, in which languages such as theWebOntol-
ogy Language (OWL) enable a formal representation of
the domain of interest. Important international initiatives
[12, 13] strive to ensure that the Semantic Web becomes
a fundamental system to achieve consistent and mean-
ingful representation, access, interpretation and exchange
of clinical data. These semantic web technologies have
already been used to represent cancer diseases, e.g. in [14],
an ontology models clinic-genomic cancer trials. Ontolo-
gies were also proposed to represent certain types of
cancer disease [15, 16].
The main objective of this study is the development of
a Semantic Web platform that facilitates the analysis and
visualisation of data from cancer registries including (1)
the representation of the disease course of a patient, (2)
the representation of the aggregated disease courses of a
group of patients, and (3) the definition of customisable
dashboards for patient selection and visualisation of the
data. The use of simulated data demonstrates the viability
of incorporating a local cancer registry into this model. A
comparative performance analysis of relational databases
and semantic repositories demonstrates excellent perfor-
mance measures for the semantic repository.
Background
Standards and classification systems in cancer registries
Most information contained in cancer registries is derived
from primary care interactions. For the purpose of struc-
tured secondary documentation, tumor documentaries
carefully reprocess primary documentation. In many
countries, a standardised common dataset has been devel-
oped to better support exhaustive data exchange with
the epidemiological cancer registries, proposing the clas-
sification of diagnostic and treatment information with
clinical coding systems.
The most important clinical classification system
applied in cancer registries is the International Classifica-
tion of Diseases version 10 (ICD-10) [17]. This classifica-
tion system is divided in chapters, with blocks of diseases.
For example, chapter II includes the classification for neo-
plasms between the blocks C00 and D48. These blocks are
subdivided in hierarchies that further specify the diagno-
sis. The ICD-O is a domain-specific extension of ICD for
cancer diseases. ICD-O is a dual classification allowing the
coding of topography (tumor site) and tumor morphol-
ogy. SNOMED CT [18] has adopted ICD-O codes for the
classification of tumor morphology.
Several staging systems for cancer have evolved over
time and continue to evolve with scientific progress. The
most important classification system is the Classification
of Malignant Tumours (TNM) [19], which is related to
the description of the anatomical extent of the disease.
This system is under constant development by the Union
for International Cancer Control and the American Joint
Committee on Cancer. The TNM staging is based on the
size or the extent of the primary tumor, the metastases in
regional lymph nodes, and the presence of metastasis or
secondary tumors formed by the spread of cancer cells to
other parts of the body.
Clinical procedures are also encoded with coding sys-
tems such as the ICD10-PCS (Procedure Coding System)
[20] denoting aspects such as the clinical classification of
the procedure, the surgical section or the body system.
Visualisation of clinical records
From the emergence of the electronic medical record
(EMR), the amount of data has increased exponentially
[21, 22]. The main objective of the EMR is representing
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 3 of 16
the clinical characteristics of a patient from several per-
spectives. For a variety of reasons [23] this objective has
not yet been achieved.
Visualisation methods are one way of facilitating the
representation and flexible exploitation of EMR data.
According to [24] there are two types of visualisation of
EMR data:
 Multimedia visualisation includes video, audio,
graphical plots, rich text, hyperlinks and other
multimedia contents [25, 26].
 Temporal visualisation depicts clinical timelines of
the health state of the patient [27, 28]. Some of these
representations are able to generate a prospective of
the future clinical characteristics of the patient using
data mining techniques over all the EMR [29, 30].
TheTimeLine project [24] combines the two approaches
with four key aspects of the user interface: demographics
and encounter information, medical problem list, graph-
ical timelines and the data viewer that allows the naviga-
tion over all data of the patient as bone scan, laboratory
data, etc. The main advantage of this project is that the
clinician can visualise all patient data without switching
between various information systems.
Semantic exploitation of data
Semantic representation
The methods for the transformation and semantic repre-
sentation of information follow similar approaches. They
can be classified in (1) those which generate a represen-
tation of the datasets in semantic formats being the result
of the application of mappings between the entry data
source and the ontology that provides the meaning for
the content; and (2) those which permit ontology-based
data access using data in traditional formats but querying
with semantic web query languages. Next, we describe the
most popular approaches and tools from both categories:
 D2RQ (Accessing Relational Databases as Virtual
RDF Graphs) allows to query data stored in
relational databases using SPARQL on virtual RDF
graphs [31]. This tool is totally automatic.
 Triplify allows to publish [32] the content of
relational databases as Linked Data [33] based on a
partially automatic transformation process.
 Linked Data Views (Virtuoso). OpenLink Virtuoso
[34] is a database management system that handles
several persistence models (relational, XML,
object-relational, virtual and RDF). Persistence
models stored in Virtuoso can be queried with
SPARQL based on the automatic representation as
Linked Data Views [35].
 XS2OWL (Representation of XML Schemas in
OWL syntax). XML schemas can be transformed
into OWL [36]. XML databases can be automatically
transformed and queried with SPARQL.
 RDB2OWL (A Database-to-Ontology Mapping
Language and Tool). Approach to transform the
data stored in relational databases into RDF or OWL
[37]. The user manually defines mappings between
the entries and the outputs. The transforming of
large ontologies can be tedious.
 Karma. It links a source model to ontologies to
generate a semantic representation of the data source
[38]. This process is partially automatic.
 Populous. Assistant for building ontologies [39], the
process being guided by patterns. Populous is able to
import CSV data.
 SWIT (Semantic Web Integration Tool). Semantic
transformation engine capable of generating RDF and
OWL repositories from both relational and XML
databases [40]. Besides transforming the data, SWIT
prevents the generation of logically inconsistent data
with the support of DL reasoners. The transformation
method has three main steps: (1) definition of the
mapping rules between the fields of the database and
the ontology; (2) generation of the OWL data; and (3)
importing the OWL data into the semantic data store.
Most approaches are based on the mappings between
the relational and semantic primitives of the correspond-
ingmodels languages. Performing only a syntactical trans-
formation, the meaning of the content is not really
exploited. In this work we use the SWIT transformation
approach, which preserves the meaning of the content
based on the specification of mappings between the enti-
ties of the source relational schema and the entities of the
target domain ontology.
Semantic querying
The amount of RDF data, and the development of applica-
tions that use semantic web technologies for storing, pub-
lishing and querying data has increased constantly in the
last decade [41]. Semantic endpoints in which the users
can exploit the data without any knowledge of SPARQL
have been developed. For example, Natural Language Pro-
cessing has been used to develop a question answering
system [42]. In other works, the authors use parametrised
queries to answer questions based on a template [43]. In
faceted search over RDF repositories, the user can refine
the filters over the results of each SPARQL query [41].
In the biomedical field, the use of semantic querying is
limited to the generation of semantic searchers or dash-
boards. BioDash is an example of semantic dashboard that
exploits heterogeneous data sources for drug discovery
[44]. Chem2Bio2RDF provides dashboards automatically
collecting associations within the systems chemical biol-
ogy space [45]. In this work, our goal is to go beyond the
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 4 of 16
state of the art by allowing users to dynamically define
their semantic dashboards.
Methods
Ontology construction
Best practices in ontology engineering recommend to
reuse existing content and to create modular ontologies
[46]. These recommendations are implemented reusing
concepts from different ontologies so that the resulting
ontology infrastructure is likely to be a networked ontol-
ogy. TheOBOFoundry has also developed a series of prin-
ciples for ontology construction which propose principles
for modularity, orthogonality and reusability [47].
The method for constructing the domain ontology used
in this work consisted in identifying the main entities that
should be represented, searching BioPortal for existing
ontologies containing classes representing these entities,
selecting the most appropriate ones (by our subjective
criteria), and extending them when necessary. The final
ontology has been implemented using Protégé4 in OWL-
DL, which is the OWL subset based on Description
Logics.
Data generation and representation
In this work, we have generated a simulated cancer reg-
istry dataset using the statistical distribution of a real
registry dataset, following the method proposed in [48].
Data provided by the National Cancer Registry of Ireland5
were used to obtain a patient distribution by age. The can-
cer registry was accessed on 10-05-2016 and we included
533409 cases diagnosed from 1994 to 2013. The patients
were generated in groups classified by gender and 5-years
age ranges (0-4, 5-9, 10-14, etc.). The last group of patients
contains people older than 85 years old.
For each group of patients we have calculated the proba-
bility distribution of diagnosing a concrete type of cancer,
and the probability distribution of receiving a particular
therapy (surgery, chemotherapy, radiotherapy, hormonal
therapy, ...) for a concrete diagnosis. These probabilities
were used to assign weights to every type of cancer with
its therapies for each group of patients. For example, for
patients between 60 and 64 years old, the probabilities for
different types of cancer are breast cancer (0.23), lung can-
cer (0.17), prostate cancer (0.17), and colorectal cancer
(0.08). For patients within this age range and diagnosed
with colorectal cancer the probabilities of the therapies
would then be: teletherapy (0.44), chemotherapy (0.44)
and surgical treatment (0.12). Figure 1 shows the stack of
distributions. When the random number is between 0.57
and 0.64 we assign colorectal cancer as the patients diag-
nosis. Then, we generate a new random number to assign
the first therapy and so on.
Furthermore, survival and mortality data were used for
extracting the evolution of the disease. Finally, we ensured
Fig. 1 Schema of probability distribution of diagnoses and therapies
that the amount of patients with more than one cancer
diagnosis meets the distribution of the real dataset.
Our simulated dataset consists in randomised cases. For
each case, we establish the gender and age of the patient.
Then, we apply a partially random distribution algorithm
for getting the patient characteristics. This algorithm uses
the weights assigned to each type of cancer, therapy
or course to generate distributions similar to the origi-
nal database. This algorithm is able to generate patients
with one or more diagnoses with various therapies and
courses following the probability distribution previously
calculated.
Such data have been represented in RDF by apply-
ing SWIT, whose transformation method has three main
steps: (1) definition of the mapping rules between the
database schema and the ontology; (2) generation of the
RDF data; and (3) importing the RDF data into the seman-
tic data store. We use a semantic repository to store the
data, which integrates two types of data sources: (1) an
OWL files server with the formal representation of the
domain, and (2) an RDF repository which stores the data.
Virtuoso6 is used as data store [49].
Exploitation model
Our approach includes a set of methods for exploiting the
information model in the semantic repository.
Ontology-driven search (ODS)
SPARQL is the language used for querying the data store.
We use our ontology-guided input text subsystem [50]
to make it easier for clinicians to exploit the data ware-
house. The main objective is to allow users to design and
execute SPARQL queries without knowing SPARQL. This
tool is an editor for SPARQL queries supported by an
OWL ontology. The OWL ontology provides the classes
and properties that can be used for creating the SPARQL
query that will be executed on the RDF repository. The
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 5 of 16
construction of the queries begins with the selection of
a main class of the ontology. For example, if we wish to
find patients, then the ODS begins with the selection of
the ontology class Patient. The user can define filters over
this class by using the data properties or object proper-
ties of the ontology. The use of owl:ObjectProperty permits
to include other concepts in the query. For example, if
we wish to find patients whose diagnosis is lung cancer,
the user can select the owl:ObjectProperty hasDiagnosis,
which is associated with the class Patient, which permits
to use the owl:ObjectProperty Pathological structure of
the class Diagnosis to select the class representing lung
cancer. The ODS is able to generate SPARQL queries in
which the subject is an ontology class, the predicate is a
property and the object can be either a value or other con-
cept. By selecting an owl:ObjectProperty, the user can add
other properties of this concept to the query. This service
follows the approach of template-based searches [43].
With this tool, the data store can be searched using the
properties defined in the ontology. Moreover, it allows
the generation of aggregated queries for the elaboration
of representative charts of the data store. The generated
queries can be stored for parameterisation and reuse.
Aggregate functions such as count, average, min or max
can be used.
The results of these queries can be linked with other
resources. The filters used can also be stored for later
reuse. The semantic search engine not only allows for data
retrieval but also for creating new classes in the semantic
model, which can be assimilated to OWL defined classes.
For example, the query for patients with colon cancer
could be defining the class Patient with colon cancer.
The members of this class are obtained by executing the
corresponding query.
Semantic profiles
Conceptually speaking, the semantic profile is defined as
the set of relations and properties of an individual. Seman-
tic profiles permit to identify groups of patients that share
the same properties and are therefore useful for compar-
ing and studying such groups. Ontologies are of special
interest for creating profiles because they allow to select
and aggregate individuals from a conceptual perspective.
Our approach can also generate the semantic profile of a
group of patients by applying one or more criteria.
Hence, we define a semantic profile as the subset of
semantic information of an individual that is interesting
for a particular analysis. The profile of the individual i is
calculated as shown in Eq. 1.
SP(i) = S(d) ? S(SP(o)) (1)
where S(d) represents a subset of the selected
owl:datatypeProperty and S(SP(o)) represents a function
that retrieves the individuals linked through owl:object-
Property axioms to i. The semantic profile is built by the
application of the ODS by using the entities defined in a
domain ontology. The ODS permits to select the proper-
ties of interest and to define the filtering and aggregation
conditions. The user can define the SPARQL queries
that will return the subset of properties and relationships
that provides the best description of the individual for
the specific case. This information is obtained for each
individual, and the results can be viewed as a cache of
the most important semantic information describing the
individuals.
Semantic profiles can be seen as a purpose-specific
application of the semantic search engine. Two types of
semantic profiles are of special relevance in the context of
this work, namely, the timeline representation of a patient
and the aggregated disease timeline representation of a
patient group with some common properties. Both are
described in the next sections.
Disease timeline of a cancer patient
The disease timeline of a patient contains information
about various health-related events (e.g. diagnosis, patient
conditions, therapies and the disease courses). Retrieving
these events for a patient requires data normalisation for
the representation of therapies by month. Figure 2 shows
that every diagnosis has an associated timeline which
includes therapies and the disease course, both ordered
by month. For example, we can show the timeline for a
breast cancer patient that includes the applied therapies
(surgical treatment, chemotherapy, etc.) for every period.
Furthermore, we can show the course of the disease and
its relation with changes in therapies. It also includes the
date of the diagnosis and the date of the last encounter.
Finally, the profile contains all the patients diagnoses and
a list of her conditions.
Aggregated disease timeline of a group of patients
The aggregated timeline of a patient group (see Fig. 3)
includes all the events of the selected patients who have
the same selection criteria for a given period and for a con-
crete diagnosis. The groups of patients are defined using
the ODS, which permits to define groups of patients with
the same diagnosis, staging, grading and age range. This
permits to obtain the semantic profile of each member of
the group. Then, the semantic profiles of the members of
the group are globally analysed, so obtaining a matrix that
contains the disease courses of the included patients for
every month of the disease. Using this method, the user is
able to generate, for example, a group of patients with lung
cancer with ages between 60 and 70 years old. In this case,
our service could represent which therapies are applied in
chronological order and which are the most likely courses.
At the same time, these graphical representations can be
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 6 of 16
Fig. 2 Schema of semantic profile of a cancer patient
used as new filters to recalculate the corresponding vari-
ables. For example, if the user selects to apply chemother-
apy as first therapy, the representation changes to reflect
the new scenario.
Enrichment analysis
Enrichment analysis is a type of statistical analysis that is
frequently used in biomedical domains [51]. Our enrich-
ment analysis method is based on the hypergeometric dis-
tribution method established for the GO:TermFinder to
determine the significance of a Gene Ontology annotation
to a list of genes [52], and the hypergeometric distribution
was developed using Apache Commons Math7.
This type of analysis is useful to compare several sub-
sets of patients with the same diagnosis. We perform a
statistical analysis of the ICD-10 codes to support the
users in the definition of diagnosis-based groups. We cal-
culate the P-value for each group as shown in Eq. 2.
P = 1 ?
k?1?
i=0
(M
i
)(N?M
n?i
)
(N
i
) (2)
where N is the total number of ICD10 codes used in the
cancer registry, M is the number of diagnoses annotated
with each ICD10 code, n is the number of ICD10 codes of
interest for a concrete patient group and k is the number
of ICD10 codes used for annotating each diagnosis.
Semantic dashboard
A semantic dashboard is a graphical representation of the
results of one or more queries. Semantic dashboards are
represented as ??L, V ?, isDashboard, U? where ?L, V ? are
Fig. 3 Overview of the generation of aggregated disease timeline of a patient group
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 7 of 16
the results of the SPARQL as key-value pairs ?L, V ?, and
U is who defined the dashboard. Each user can define and
customise her dashboards.
The semantic dashboard is implemented using the ODS
and permits to create aggregated data. The results can
be represented graphically and in tabular format. Based
on the persistence model of SPARQL queries, the repre-
sentations can be used for accessing the data instances
contained in each representation. Consequently, aggrega-
tion control boxes can be regarded as search filters of the
semantic search engine.
Figure 4 shows the query generated with the ODS for
searching patients over 70 years old and classified by
cancer type. In the left side we show the graphical repre-
sentation and in the right side the data in tabular format.
The semantic dashboards can also include multiple
aggregated queries and display comparative graphics.
Finally, dashboards can also be persisted, parameterised
by users and reused.
Recommendation
We have developed an algorithm based on Bayesian net-
works to suggest the most appropriate treatment for
a patient. This algorithm is based on the generation
of probabilistic models using semantic nodes profiles.
Bayes networks cannot have cycles [53], but our seman-
tic dataset might contain cycles. The semantic profiles
might have cycles due to, e.g., the repetitive application of
a given treatment to the patient. To solve this problem a
tree network is generated for each profile.
In case of being interested in knowing which treatment
is likely to be the most appropriate for a patient given a
number of features, the model would first retrieve all the
patients with such features, and then use their semantic
profile to generate the map of Bayesian networks with the
possible treatments by period (month, term, etc.). Once
a treatment is selected, the network is re-calculated to
improve the next recommendation. Given this dynamic
aspect of the network, the method requires that the user
indicates which characteristics might generate a cycle in
the network to prevent the algorithm from falling in an
infinite loop.
Results
The approach described in the previous section has been
applied in a scenario that simulates an institutional cancer
registry. An ontology modeling the semantics of an insti-
tutional cancer registry has been developed. This ontology
has driven the transformation of the simulated dataset
into RDF and its storage in the semantic data store. We
Fig. 4 Example of semantic dashboard
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 8 of 16
have implemented a Semantic Web platform that permits
users to exploit the cancer registry dataset by formulat-
ing incremental, customisable queries using a graphical
user interface based on the ODS and by generating dash-
boards on demand. The complex timelines of the disease
of individual and aggregated patients can also be explored
and analysed. Next, more details about these results are
provided.
The ontology
We have built a preliminary cancer registry ontology8
based on the existing ontologies and fulfilling the require-
ments of a local cancer registry. This first draft ontology
represents some aspects of cancer diseases and their treat-
ment pragmatically. The ontology reuses the Semantic-
science Integrated Ontology (SIO) [54] and the Ontology
for Biomedical Investigations (OBI) [55]. The ontology
incorporates concepts from clinical standards used in can-
cer such as ICD10, ICD-O-3, TNM staging, Karnofsky
index [56] and ASA index [57]. The ontology has been
defined in OWL-DL. The metrics of the ontology are as
follows (numbers in brackets represent the number of
entities added by our work). The ontology contains a total
of 20,551 classes (335), 28 properties (18) and 342 object
properties (29), with 152,529 logical axioms (2581). The
ontology defines the following classes:
 Patient represents a person with any type of cancer
disease. Properties: gender, birth date, diagnosis,
therapies and disease courses. This class is equivalent
to the class Patient in SIO.
 Patient condition represents the health condition of a
patient at a given time. Properties: reference date,
age, weight, height, Karnofsky index, ASA index and
the menopause status.
 Diagnosis represents the patient diagnosis at a given
time. Properties: ICD10 code, grading, staging,
therapies, date, pathological structure, anatomical
structure and tumor type. This class is equivalent to
the class Diagnosis in SIO.
 Therapy represents the patient therapies of a
diagnosis at a given time. Different kinds of therapy
such as Chemotherapy, Surgical Treatment, Nuclear
Medicine and others have been modeled in the
ontology as subclasses of Therapy. Properties:
medication, start date and end date.
 Disease course represents the development in time
(process) of a tumor disease of a certain type
(diagnosis) over a time interval at a given time point.
Different kinds of course such as Complete remission
(tumor is not detectable any longer), Progression
(tumor mass increases to a certain amount),
Recurrence (after complete or partial remission,
tumor mass increases again), and others have been
modeled in the ontology as subclasses of Disease
course. Properties of disease course are diagnosis,
patient conditions, stage, order and date. The
properties date and order are the key to sort the
courses of the patient for a concrete diagnosis. This
class is equivalent to the class Disease course in OBI.
 The ontology also includes some classes to represent
the TNM classification system of malignant tumors.
They include anatomical entities for cancer grading
and staging, e.g. Primary tumor, Regional Lymph
Nodes and Distant Metastasis hierarchies.
 Health Classification System is the superclass of all
classes representing coding artifacts of health related
classification systems. To build the taxonomies of
classifications for a cancer registry, we tried to reuse
other ontologies. For the ICD10 code we use the
ontology built in [58].
We have evaluated the quality of our ontology using
the Ontology Quality Evaluation Framework (OQuaRE)
[59]. OQuaRE is a framework for evaluating the qual-
ity of ontologies based on standards of software quality.
OQuaRE automatically calculates quality scores in the
range [1,5] for a series of characteristics and subchar-
acteristics. A score 1 indicates that it does not fulfill
the minimal requirements, 3 indicates that the ontology
meets the requirements, and 5 indicates that the ontology
exceeds the requirements. Table 1 shows the results for
our ontology. The scores for Functional Adequacy, Main-
tainability, Operability, Structural and Transferability are
over 4. The lowest results are achieved for Compatibility
and Reliability, although they are over 3. The results show
that our ontology has a high level of cohesion, consistency,
formalisation, modularity and reusability, which are the
most relevant aspects for the present work.
The semantic cancer registry system
We have implemented a prototype system9 based on the
methods described in previous sections. Figure 5 shows
the three main parts of this system. All the components
of our system have been developed from scratch except
SWIT, which is a previous result of our research group.
The upper part of the figure shows the data transforma-
tion module, which uses SWIT for transforming the orig-
inal data in semantic information stored into the semantic
data store.
The cancer registry ontology is the core of the system,
allowing for the computational management of the infor-
mation related to the cancer patients. All the services
offered by the prototype are implemented on top of this
core. The data transformation requires to map the source
data schema to the cancer registry ontology.
The lower part of the figure shows the other two mod-
ules of the system. The right one shows the module for
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 9 of 16
Table 1 OQuaRE Metrics for the Cancer Registry ontology
Subcharacteristic Value
Compatibility (3.25)
Replaceability 3.5
Functional Adequacy (4.69)
Clustering and Similarity 4.5
Consistent Search and query 4.8
Controlled Vocabulary 5.0
Guidance and Decision Trees 5.0
Indexing and Linking 4.67
Infering 5.0
Knowledge Acquisition 4.67
Knowledge Reuse 4.875
Reference Ontology 4.5
Results Representation 3.5
Schema and Value Reconciliation 4.75
Text Analysis 5.0
Maintainability (4.34)
Analysability 4.33
Changeability 3.86
Modification Stability 4.0
Modularity 5.0
Reusability 4.5
Testeability 4.33
Operability (4.83)
Learneability 4.83
Reliability (3.0)
Availability 4.0
Recoverability 2.0
Structural (4.67)
Cohesion 4.0
Consistency 5.0
Formal Relation Support 4.0
Formalisation 5.0
Redundancy 5.0
Tagledness 5.0
Transferability (4.25)
Adaptability 4.25
the analysis of individual patients, that is, extraction of
semantic profile and timeline analysis. The left one shows
the module for the analysis of groups of patients, which
also includes the graphical access to the disease courses
of those groups. The ODS permits to create groups of
patients that share some semantic properties. This per-
mits to generate charts and tables with accumulated data
of the semantic repository. In this case, the system pro-
vides an option for adding the grouping class or property,
so that it can be considered as a customisable dash-
board designer. The dashboard permits users to select and
aggregate the information on every class of the seman-
tic model. This module is the base for the construction
of other services such as the graphical representation of
the aggregated timelines of a group of patients or the
customisable dashboards.
The dashboard visualises the concepts of the model in
charted and grouped forms, and multiple, on-demand,
incremental dashboards can be built. For instance, a user
can generate a pie chart selecting patients by their first
therapy. The user can save any dashboard for querying the
results without needing to generate it again.
Application to the simulated dataset
We have performed an initial evaluation of the system.
We have generated a simulated database with 207.190
patients10. By the application of SWIT, the generated
dataset meets the constraints defined in our ontology,
whose entities are used for creating the RDF dataset.
The time for the transformation of the dataset from the
relational database to the semantic datastore has been
thirty-two minutes (Main features of the server: Intel
CoreTM i7-3770T Processor (8M Cache, up to 3.70 GHz),
8GB RAM, SATA2).We have carried out some tests based
on the execution of different types of queries to compare
the performance of the relational and semantic stores.
Table 211 shows that the time performance of the
semantic datastore is slower than the relational one for
basic queries that do not require joins. However, the
semantic datastore performs better than the relational
model, even with indexes, on this dataset for more com-
plex queries. The semantic datastore is also faster when
filtering by a single property of the class or the table
column.
Semantic dashboard
This tool permits users to formulate incremental, user-
defined queries with a graphical user interface based on
the ODS. Figure 6 shows a comparative graphic over the
therapies applied to patients diagnosed with colorectal
cancer in different age ranges. Table 3 shows the generated
query for this case. The query results can be displayed in
several customisable ways, allowing for the generation of
on-demand dashboards.
Graphical representation of the disease timeline of a patient
This service permits users to observe the main properties
of the timeline of a patient with a cancer disease. Figure 7
shows an excerpt of the therapy and course timeline of a
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 10 of 16
Fig. 5 Overview of the system
patient with pharynx cancer. In this view, users can see
the details of the diagnosis and of every therapy applied in
each period. Besides, users are provided with two evolu-
tion charts, which are based on the patient course and on
the Karnofsky index.
Graphical representation of the aggregated disease timeline
of a patient group
Figure 8 shows the selection and the aggregation of
patients using the following criteria: male patients aged
between 50 and 70, diagnosed with colorectal cancer,
Table 2 Results of the migration of the relational database to the
semantic data store
Query SQL SQL SPARQL SPARQL
count time count time
result result
Recovery all Patients 207.190 0,060s 207.190 0,189s
Recovery all Therapies 400.290 0,132s 400.290 0,317s
Recovery all Diagnosis 240.088 0,070s 240.088 0,220s
Recovery all Courses 108.297 0,030s 108.297 0,155s
Recovery patients
with diagnosis,
therapies and courses
207.190 1,048s 207.190 0,204s
Recovery all female
Patients
105.714 0,231s 105.714 0,189s
Recovery all female
Patients with more of
60 years old
62.603 0,245s 62.603 0,192s
and who have received Chemotherapy. Table 4 shows the
query generated for this case.
After the selection and the aggregation of patients, the
system generates charts that contain the therapies and
the disease courses of the patients. This service can be
employed as an exploratory therapy simulator. Optionally,
the entire time matrix can be recalculated by selecting a
certain therapy. This can help the user to estimate which
therapy is likely to be themost appropriate. Figure 9 shows
an excerpt of the panel for analysing the first two months
of the therapies of a group of 60 patients.
The enrichment analysis
Term enrichment was performed on several patient
groups using the hypergeometric distribution method for
the ICD10 code annotations on each diagnosis. First,
we used a sample of cancer cases related to over 300
patients. Our design requirement for this sample was to
include patients of both genders, so we discarded breast
and prostate cancer for this analysis. The sample con-
tained three main cohorts: diagnosis of lung cancer (469),
diagnosis of melanoma (338) and diagnosis of colorectal
cancer (311).
Table 5 shows the results associated with lung cancer for
males and females. The results show that the difference
between both groups is not significant for lung cancer but,
as shown in Table 6, it is significant for colorectal can-
cer. For example, Malignant neoplasm of rectum is clearly
over-represented in the gender male, which permits to
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 11 of 16
Fig. 6 Dashboard view
conclude that this diagnosis is much more common
in men.
Target users
The target users of our platform are described next:
 Physicians can use our platform to extract knowledge
from the cancer registry in aggregated form filtering
on the risk of patients by applying clinical criteria.
Furthermore, they can obtain a graphical
representation of the disease course of a concrete
patient or a group of patients.
 Health managers can use our platform to generate
customisable dashboards to prepare a follow-up of
the clinical services involved in the diagnosis or
therapies for cancer.
 Tumor documentaries can use the platform to detect
cases with incomplete or inconsistent documentation
for data curation.
Discussion
Cancer registries have become a basic tool for dis-
ease research and treatment. Nowadays, there are sev-
eral technological solutions able to manage and analyse
the information of patients with a determined diagnosis.
However, the lack of formal semantic models is a prob-
lem when personalised analyses or external data links
are required. In this paper we have presented a seman-
tic platform for the analysis and visualisation of records in
an institutional cancer registry. Based on the analysis of
requirements, we have developed an ontology that models
the semantics of a regional cancer registry. We have used
this model and SWIT for transforming and storing simu-
lated data from a cancer registry in a semantic data store.
Our approach permits users to formulate incremental,
user-defined queries with a graphical user interface based
on the ODS. The results of the queries can be displayed in
several customisable ways, allowing for the generation of
on-demand dashboards. The complex timelines of the dis-
ease of individuals and aggregated patients can be clearly
represented.
Rule-based systems and logic-based models have been
semantic approaches applied to cancer registries, such as
analysis of cancer registry processes [7], quality assur-
ance [8] and decision support [9]. Our approach innovates
by combining traditional technologies such as relational
databases and semantic web technologies. We have cre-
ated an OWL ontology for representing some aspects of
an institutional, local cancer registry. We have developed
an RDF repository whose structure is driven by the OWL
ontology and permits to work by exploiting the seman-
tics of the content. In this way retrieval is semantically
enabled, so that queries are independent of the relational
data structures of conventional databases. Our technolog-
ical infrastructure has permitted us to develop a semantic
searcher for navigating through the complete cancer reg-
istry, to extract semantic profiles of the patients, and to
analyse the structure of disease courses.
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 12 of 16
Table 3 SPARQL query generated by ODS for a dashboard
PREFIX ods:?http://www.imib.es/ontologies/disease-times?
SELECT count(DISTINCT ?s), ?t WHERE
{{?s rdf:type ?t FILTER (?t IN (ods:DrugTherapy, ods:Anti-hormoneTherapy,
ods:Anti-hormonal_anti-androgens, ods:Anti-hormonal_anti-estrogens,
ods:Anti-hormone_therapy_aromatase, ods:Other_Anti-hormoneTherapy,
ods:Chemotherapy, ods:Immunotherapy, ods:OtherdrugTherapy,
ods:Bisphosphonates, ods:Other_med_therapy,
ods:NuclearMedicineTherapy, ods:OpenRadionuclides,
ods:Other_nuclear_medicine_therapy, ods:RadioiodineTherapy,
ods:OtherTherapy, ods:Hyperthermia, ods:Locoregional_hyperthermia,
ods:Part-body_hyperthermia, ods:LightTherapy, ods:OtherLightTherapy,
ods:Selective_ultraviolet_phototherapy, ods:Wait_and_see,
ods:Radiotherapy, ods:Brachytherapy, ods:Interstitial_brachytherapy,
ods:Other_brachytherapy, ods:OtherHigh-voltageRadiotherapy,
ods:High-voltage_radiotherapy_n.n.bez.,
ods:Other_high-voltage_radiotherapy, ods:Whole-body_irradiation,
ods:Teletherapy, ods:OtherTeletherapy, ods:Teletherapy_n.n.bez.,
ods:Teletherapy_with_linear_accelerator, ods:StemCellTransplantation,
ods:AllogeneicSCT, ods:AutologousSCT, ods:SurgicalTreatment,
ods:Therapy ))} .
{{?s ods:hasDiagnosis ?a0.
{?a0 rdf:type ?ta0 FILTER (?ta0 IN (ods:Diagnosis))} }.
{?a0 ods:hasPathologicalStructure ?a01 .
{?a01 rdf:type ?ta01 FILTER (?ta01 IN (ods:Colorectal_cancer))} } .
{?s ods:hasPatient ?a1 . {?a1 rdf:type ?ta1 FILTER (?ta1 IN (ods:Patient))} } .
{?a1 ods:age ?a12 . FILTER (?a12 ?= 60)} }} group by ?t}
Our approach provides powerful and precise search
capabilities assisted by a customisable dashboard adapt-
able to the requirements of each user. This proposal is
very similar to the tools presented in [43], but we inno-
vate by permitting users to generate re-usable templates.
Furthermore, the templates do not only allow the gen-
eration of search forms but also of parameterised user-
customisable dashboards. The platform permits to use
the entities defined in the OWL ontology for creating
the queries in a more intuitive way than using a tradi-
tional relational model. Furthermore, the use of a NoSQL
database (e.g. RDF repository) allows to use a robust and
scalable architecture for large clinical data warehouses
[49]. Another important advantage of using semantic
knowledge modelling is the possibility of sharing informa-
tion and comparing clinical cases and processes.
The semantic profiles enable the generation of time-
lines for different patient records. Our approach combines
multimedia and temporal visualisations [24] which can
be customised by the users. The semantic profiles can
be aggregated, hence enabling the generation of time-
lines of a patient group with similar characteristics. This
visualisation can be used as a graphical representation
of a Bayesian network. Clinicians can interact with the
visualisation to discover likely courses of patients dis-
eases. The platform offers data analysis based on term
enrichment to support clinicians to generate groups of
patients.
Limitations
One limitation of this work is the application of a pre-
liminary version of an ontology of cancer registry data.
This ontology needs to be reviewed and extended. How-
ever, we believe that the OQuaRE quality scores of the
ontology permit to use it for proof-of-concept implemen-
tations and experiments such as the one presented in this
work.
Another limitation is the use of simulated data, because
real data would enable a more reliable (1) validation of the
correctness and completeness of the system, (2) testing of
the performance of the system, and (3) evaluation of the
impact of missing data in the performance [8].
In this work, we have been able to evaluate only
some components of the platform. A complete evaluation
Fig. 7 Excerpt of the timeline representation
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 13 of 16
Fig. 8 Ontology-driven searcher view
would mean to measure the following metrics: efficiency,
usability, usefulness of the graphical representations of
the analysis of the disease courses or patients or group
or patients and the capacity to develop new customizable
dashboards by the users.
Future work
The results of this work were shown in a clinical session
of the epidemiological service of our largest regional hos-
pital, the Virgen de la Arrixaca Hospital in Murcia, Spain.
The physicians showed their interest in applying the same
methodology to the Colorectal Cancer Prevention Pro-
gram of the Region of Murcia (Spain). This use case will
include real data from 322,869 patients recruited since
Table 4 SPARQL query generated by ODS for a filter
PREFIX ods:?http://www.imib.es/ontologies/disease-times?
SELECT DISTINCT ?s WHERE {
{?s rdf:type ?t FILTER (?t IN (ods:Patient))} .
{
{?s ods:hasDiagnosis ?a0 .
{?a0 rdf:type ?ta0 FILTER (?ta0 IN (ods:Diagnosis))}
} . {?a0 ods:hasPathologicalStructure ?a01 .
{?a01 rdf:type ?ta01 FILTER (?ta01 IN (ods:Colorectal_cancer))}
} . {?s ods:gender ?a1 . FILTER (str(?a1) = M)} .
{?s ods:age ?a2 . FILTER (?a2 ?= 50)} .
{?s ods:age ?a3 . FILTER (?a3 ?= 70)} .
{?s ods:hasTherapy ?a4 .
{?a4 rdf:type ?ta4 FILTER (?ta4 IN (ods:Chemotherapy))}
}
}
}
2006. Nowadays, the physicians can generate customis-
able dashboards12 and they are interested in a prediction
of their future level of risk of patients.
In addition, a study combining real data from the
Department of Epidemiology of Murcia Regional Health
Council (Spain) and the cancer registry of the Compre-
hensive Cancer Center Freiburg (Germany) is planned. On
the clinical side, this would permit to perform studies with
data originating in different registries as well as to perform
comparative studies on the characteristics and evolution
of cancer patients in different populations or on clinical
oncology practice in these regions. On the technical side,
this would permit to exploit the fact that ontology-based
approaches facilitate data integration. Although data inte-
gration has not been investigated in this work, we believe
that sharing the same ontology for different registries
would enable interoperability, and the data could be jointly
exploited by means of distributed SPARQL queries. By
the same means, they could also be used to create an
integrated data warehouse. The decision between both
implementation options depends on the requirements of
the use case, is due to the time cost of executing the dis-
tributed queries and the effort needed to maintain the
data warehouse. However, this effort does not imply major
changes in the RDF data representation. Such a study
could also test how the ontology copes with different reg-
istries, which we believe it is a relevant quality indicator
for our ontology.
We plan to extend the platform with studies of other
chronic pathologies, which might also include a clinical
validation. In this way, we plan to apply the platform for
monitoring clinical trials thanks to the flexibility of the
ODS and the customisable dashboards. Furthermore, we
plan to useD3SPARQL [60] to enrich the dashboard plots.
Finally, we would like to use this model to generate rules
that serve to automatically generate patient groups or for
quality assurance of the data.
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 14 of 16
Fig. 9 Excerpt of the aggregated disease timeline of a patient group
Conclusion
This work has demonstrated that ontologies and the RDF
repositories can be effectively combined for exploiting a
local cancer registry. On the one hand, we constructed an
ontology that models the knowledge of local cancer reg-
istry. On the other hand, we have used semantic web tech-
nologies for building a platform to analyse the complex
timelines of a patients with cancer. Besides, our seman-
tic structure has allowed for representing the aggregated
disease timelines of patient groups.
The semantic infrastructure has also permitted the
generation of graphical representations of the stored
knowledge in the cancer registry with the generation of
customisable dashboards.
The work is an example of how ontologies can guide
the entire life cycle of a analysis platform: data trans-
formation, exploitation and knowledge generation. These
technologies allow users to configure advanced searches,
Table 5 Term enrichment for ICD10 cores of Lung cancer
ICD 10 Code P-value P-value
Male Female
(308) (152)
C34.0 (Main bronchus) 0.77 0.35
C34.1 (Upper lobe, bronchus or lung) 0.42 0.77
C34.2 (Middle lobe, bronchus or lung) 0.48 0.58
C34.3 (Lower lobe, bronchus or lung) 0.71 0.45
C34.8 (Overlapping lesion of bronchus and lung) 0.51 0.63
C34.9 (Bronchus or lung, unspecified) 0.55 0.40
Table 6 Term enrichment for ICD10 cores of colorectal cancer
ICD 10 Code P-value P-value
Male Female
(175) (127)
C17.0 (Duodenum) 0.21 0.90
C17.1 (Jejunum) 0.31 0
C17.2 (Ileum) 0.94 0.47
C17.8 (Overlapping lesion of small intestine) 0 0.41
C17.9 (Small intestine, unspecified) 0.81 0.65
C18.0 (Caecum) 0.99 0.03
C18.1 (Appendix) 0.86 0.28
C18.2 (Ascending colon) 0.90 0.26
C18.3 (Hepatic flexure) 0.11 0.96
C18.4 (Transverse colon) 0.15 0.93
C18.5 (Splenic flexure) 0.59 0.79
C18.6 (Descending colon) 0.86 0.30
C18.7 (Sigmoid colon) 0.94 0.18
C18.9 (Colon, unspecified) 0.37 0.75
C19 (Malignant neoplasm of rectosigmoid
junction)
0.96 0.18
C20 (Malignant neoplasm of rectum) 6.39E-4 0.99
C21.0 (Anus, unspecified) 0.98 0.18
C21.1 (Anal canal) 0.87 0.30
C21.8 (Overlapping lesion of rectum, anus and
anal canal)
0 0.07
D01.0 (Colon) 0.81 0.65
D01.2 (Rectum) 0.56 0
Esteban-Gil et al. Journal of Biomedical Semantics  (2017) 8:46 Page 15 of 16
build custom dashboards and establish complex analysis
from semantic profiles. Furthermore, semantic technolo-
gies establishes the bases to link to external data sources
and comparative analysis with other organizations. We
believe that this work provides new insights about how
semantic technologies can be applied to the exploitation
of clinical data in general, and to clinical registries in
particular.
Endnotes
1 http://www.elekta.com/healthcare-professionals/
products/elekta-software/cancer-registry.html
2 http://www.oncolog.com/?cid=7
3 http://www.askcnet.org/
4 http://protege.stanford.edu/
5 http://www.ncri.ie/
6 http://virtuoso.openlinksw.com/dataspace/doc/dav/
wiki/Main/
7 http://commons.apache.org/proper/commons-math/
8 http://sele.inf.um.es/ontologies/cancer-registry2.owl
9 http://sele.inf.um.es/SECARE/
10 http://sele.inf.um.es/ontologies/individuals.zip
11The test has been carried out in a local machine with
MySQL 5 as relational database and Virtuoso 7 as RDF
repository.
12 http://sele.inf.um.es/SECOLON/
Abbreviations
ASA: American society of anesthesiologists; DL: Description logics; EMR:
Electronic medical record; ICD: International classification of diseases; OBO:
Open biomedical ontologies; ODS: Ontology-driven search; OQuaRE: Ontology
quality evaluation framework; OWL: Web ontology language; PCS: Procedure
coding system; RDF: Resource description framework; SNOMED CT: Systematic
nomenclature of medicine - clinical terms; SPARQL: SPARQL protocol and RDF
query language; SWIT: Semantic web integration tool; TNM: Classification of
malignant tumours; RDFS: Resource description framework schema
Acknowledgements
Not applicable
Funding
This project has been possible thanks to the Spanish Ministry of Economy,
Industry and Competitiveness and the FEDER Programme through grants
TIN2014-53749-C2-2-R, and by the Fundación Séneca (15295/PI/10,
19371/PI/14).
Availability of data andmaterials
The Cancer Registry ontology is freely available at http://sele.inf.um.es/
ontologies/cancer-registry.owl. The semantic Web Platform for the analysis
and visualisation of a cancer registry is available with the use case data at
http://sele.inf.um.es/SECARE/. The user and password to sign in is änonymous¨.
The RDF dataset is available at http://sele.inf.um.es/SECARE/individuals.zip.
Authors contributions
Conceived and designed the approach: AEG, JTFB, MB. Implemented the
approach and performed the experiments: AEG, JTFB, MB. Analysed the results:
AEG, JTFB, MB. Contributed to the writing of the manuscript: AEG, JTFB, MB. All
the authors have approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Fundación para la Formación e Investigación Sanitarias de la Región de
Murcia, Biomedical Informatics & Bioinformatics Platform, IMIB-Arrixaca, C/ Luis
Fontes Pagán, no 9, 30003 Murcia, Spain. 2Dpto. Informática y Sistemas,
Facultad de Informática, Universidad de Murcia, IMIB-Arrixaca, Facultad de
Informática, Campus de Espinardo, 30100 Murcia, Spain. 3Institute for Medical
Biometry and Statistics, Medical Center  University of Freiburg, Faculty of
Medicine, University of Freiburg, Stefan-Meier-Str. 26, 79104 Freiburg, Germany.
Received: 7 June 2016 Accepted: 19 September 2017
RESEARCH Open Access
Ontology-based literature mining of E. coli
vaccine-associated gene interaction
networks
Junguk Hur1* , Arzucan Özgür2 and Yongqun He3,4,5,6*
Abstract
Background: Pathogenic Escherichia coli infections cause various diseases in humans and many animal species.
However, with extensive E. coli vaccine research, we are still unable to fully protect ourselves against E. coli infections.
To more rational development of effective and safe E. coli vaccine, it is important to better understand E. coli
vaccine-associated gene interaction networks.
Methods: In this study, we first extended the Vaccine Ontology (VO) to semantically represent various E. coli
vaccines and genes used in the vaccine development. We also normalized E. coli gene names compiled from
the annotations of various E. coli strains using a pan-genome-based annotation strategy. The Interaction Network
Ontology (INO) includes a hierarchy of various interaction-related keywords useful for literature mining. Using VO,
INO, and normalized E. coli gene names, we applied an ontology-based SciMiner literature mining strategy to
mine all PubMed abstracts and retrieve E. coli vaccine-associated E. coli gene interactions. Four centrality metrics
(i.e., degree, eigenvector, closeness, and betweenness) were calculated for identifying highly ranked genes and
interaction types.
Results: Using vaccine-related PubMed abstracts, our study identified 11,350 sentences that contain 88 unique
INO interactions types and 1,781 unique E. coli genes. Each sentence contained at least one interaction type and
two unique E. coli genes. An E. coli gene interaction network of genes and INO interaction types was created.
From this big network, a sub-network consisting of 5 E. coli vaccine genes, including carA, carB, fimH, fepA, and
vat, and 62 other E. coli genes, and 25 INO interaction types was identified. While many interaction types represent
direct interactions between two indicated genes, our study has also shown that many of these retrieved interaction
types are indirect in that the two genes participated in the specified interaction process in a required but indirect
process. Our centrality analysis of these gene interaction networks identified top ranked E. coli genes and 6 INO
interaction types (e.g., regulation and gene expression).
Conclusions: Vaccine-related E. coli gene-gene interaction network was constructed using ontology-based literature
mining strategy, which identified important E. coli vaccine genes and their interactions with other genes through
specific interaction types.
* Correspondence: junguk.hur@med.und.edu; yongqunh@med.umich.edu
1Department of Biomedical Sciences, University of North Dakota School of
Medicine and Health Sciences, Grand Forks, ND 58202, USA
3Department of Microbiology and Immunology, Unit for Laboratory Animal
Medicine, University of Michigan Medical School, Ann Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 
DOI 10.1186/s13326-017-0122-4
Background
In addition to be harmless commensal strains, the versa-
tile E. coli bacterial species includes many pathogenic vari-
ants [1]. Depending on the site of infection, pathogenic E.
coli strains are divided into intestinal pathogenic E. coli
(IPEC) and extraintestinal pathogenic E. coli (ExPEC).
Example IPEC pathotypes include enteroaggregative E.
coli (EAEC), enterohaemorrhagic E. coli (EHEC), entero-
pathogenic E. coli (EPEC), and enterotoxigenic E. coli
(ETEC). The most common ExPEC pathotypes include
uropathogenic E. coli (UPEC), meningitis-associated E.
coli (MNEC), and avian pathogenic E. coli (APEC) [2].
These virulent E. coli strains cause various diseases (e.g.,
gastroenteritis and urinary tract infections) with big
damages worldwide. For example, ETEC is estimated to
cause 300,000 to 500,000 deaths per year, mostly in
young children [3].
To prevent diseases caused by pathogenic E. coli infec-
tions, extensive vaccine research has been conducted
[47]. The Vaccine Investigation and Online Information
Network (VIOLIN; http://www.violinet.org/) [8, 9], a
comprehensive web-based central resource for integrating
vaccine research data curation and literature mining ana-
lysis, currently includes over 40 manually annotated E. coli
vaccines. Among these vaccines, Dukoral, originally
intended for protection against Vibrio cholerae, provides a
moderate protection against ETEC infections in human
[10]. However, there is no other licensed human E. coli
vaccine available on the market, putting humans at risk of
E. coli infections. Therefore, more active research is
needed to develop new E. coli vaccines.
For rational pathogenic E. coli vaccine design, it is
critical to understand E. coli gene functions and E. coli-
host interaction mechanisms. With over 35,000 E. coli-
related articles published in PubMed, it is impossible to
read all these articles manually. Therefore, literature
mining becomes critical. In addition to pathogenic
strains, many E. coli strains are nonpathogenic. E. coli
is also widely used as a model organism in microbiology
studies and as a commonly used tool in recombinant bio-
logical engineering and industrial microbiology. Given so
many E. coli strains and different E. coli usages, it has been
a challenge in mining vaccine-related E. coli gene interac-
tions from the large pool of literature reports. In this
study, we use the commonly applied GENETAG-style
named entity annotation [11], where a gene interaction
can involve genes or gene products such as proteins.
While human gene names are well normalized based on
the HUGO Gene Nomenclature Committee (HGNC;
http://www.genenames.org/), a similar gene nomenclature
strategy for bacterial gene names has not been formed.
However, it is possible to normalize bacterial gene names
using the strategy of pan-genome. Specifically, a bacterial
species can be described by its pan-genome, which is
composed of core genes present in all strains, and dis-
pensable (or accessory) genes present in two or more
strains or unique to single strain [12, 13]. After a pan-
genome is generated, the gene/protein names of the pan-
genome of a bacterial species can be obtained by gene/
protein name merging and cleanup from the annotations
of all strains belonging to the bacteria species.
Integration of biomedical ontology with literature
mining can significantly improve its performance. An
ontology is a human- and computer-interpretable set of
terms and relations that represent entities in a specific
biomedical domain and how they relate to each other.
Previously, we applied the community-based Vaccine
Ontology (VO) [14] to enhance our literature mining of
interferon-gamma related [15], Brucella-related [16],
and fever-related [17] gene interaction networks within
the context of vaccines and vaccinations. Recently, we
have developed the Interaction Network Ontology
(INO) and successfully applied it to the studies of vac-
cine gene interactions [18] and host-Brucella gene
interactions [19]. In these studies, we used and ex-
panded SciMiner [20], a natural language processing
and literature mining program with a focus on scientific
article mining. SciMiner uses both dictionary- and rule-
based strategies for literature mining [20].
To better study gene interaction networks, we have
also developed a literature mining strategy CONDL,
standing for Centrality and Ontology-based Network
Discovery using Literature data [17]. The centrality ana-
lysis here refers to the application of different centrality
measures to calculate the most important genes (i.e.,
hub genes) of the resulting gene-gene interaction net-
work out of biomedical literature mining. Four types of
centrality measures have been studied: degree, eigen-
vector, closeness, and betweenness [17, 21]. The CONDL
strategy was applied to extract and analyze IFN-? and
vaccine-related gene interaction network [21] and vac-
cine and fever-related gene interaction network [17], and
our results showed that the centrality analyses could
identify important genes and raise novel hypotheses
based on literature mined gene interaction networks. In
this study, we applied this approach, together with the
pan-genome E. coli gene collection, to E. coli gene inter-
action networks using VO and INO to identify the cru-
cial E. coli genes and interaction types.
Methods
Pan-genome based E. coli gene name normalization
E. coli gene names from E. coli K12 genome have been
collected in EcoGene (http://www.ecogene.org/) [22],
which were used as the basis for our E. coli gene name
normalization. To integrate E. coli gene names from dif-
ferent E. coli genome annotations, we applied the pan-
genome strategy [12, 13]. Specifically, out of 75 E. coli
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 2 of 10
strains, we used the Vaxign program [23], which includes
the OrthoMCL ortholog searching program [24], to
generate an E. coli pan-genome that includes core E.
coli genes shared by all strains, and dispensable genes
present in two or more strains or unique to single
strain. After the E. coli pan-genome was generated, the
gene names of the pan-genome were reannotated by
merging together different gene names from these E.
coli strains when these gene names belong to the same
genes of the pan-genome. The reannotated gene names
were then used for next step literature mining.
VO modeling of E. coli vaccines and genes used in E. coli
vaccine development
E. coli VO ontology terms were obtained from the VIO-
LIN vaccines website (http://www.violinet.org/vaxquery/
vaccine_query_process.php?c_pathogen_id[]=25) that con-
tained 44 manually annotated E. coli vaccines. In addition
to specific E. coli vaccine representations (terms), we also
modeled and represented E. coli vaccine genes. Here, a
vaccine gene is defined as a microbial gene that has been
used as a gene targeted or genetically engineered in at
least one experimentally verified vaccine. For example, a
vaccine gene may encode for a protective protein antigen,
which can be expressed, purified, and used as the vaccine
antigen component in a subunit vaccine. Some vaccine
genes encode for virulence factors, and their mutations
result in the generation of live attenuated vaccines [25].
VO/INO-SciMiner tagging of genes/interaction terms and
vaccine terms
Our current study relies on the use of SciMiner (and its
variant VO-SciMiner). The original SciMiner achieved
87% recall, 71% precision and 76% F-measure on
BioCreAtIvE II Gene Normalization Task data [20]. In
terms of identifying vaccine ontology terms, VO-
SciMiner demonstrated 91% recall and 99% precision in
the domain of Brucella vaccines [16]. In the current study,
VO-SciMiner was further modified to be able to handle
the compiled pan-genome-based E. coli genes with a more
stringent name identification matching strategy.
The abstracts and titles of all PubMed records pub-
lished by the end of 2014 were used for the present lit-
erature mining study. Figure 1 illustrates our overall
workflow. SciMiner [20] and its variations, specialized
for specific ontologies (INO-SciMiner [18] and VO-
SciMiner [16]) were used to process sentences from
PubMed literature and to identify entities (E. coli VO
terms, and INO terms). VO-SciMiner was modified to
be able to handle the compiled pan-genome-based E.
coli gene. In order to focus on the genes related to E. coli
vaccine, the analysis was limited to the entities identified
from the articles in E. coli and vaccine context, defined
by a PubMed search of Escherichia coli [MeSH] and
vaccines [MeSH]. Figure 1 illustrates the overall work-
flow of our approach.
Co-occurrence analysis
The tagged genes were used to study the co-occurrence
of genes and vaccines in the same sentences. First, an E.
coli gene-gene interaction network was generated based
on the sentence-level co-occurrence of E. coli genes. The
E. coli gene-gene interactions were defined for any pos-
sible pairs of E. coli genes, two or more of which were
identified from same sentence. The VIOLIN vaccine data-
base [8, 9] includes 25 E. coli vaccine genes as shown on
the VIOLIN website: http://www.violinet.org/vaxquery/
query_detail.php?c_pathogen_id=25. These vaccine genes
have also been represented in the VO. These E. coli vac-
cine genes were used in our ontology-based literature
mining study, which aims to identify other E. coli genes
that co-occur with these vaccine genes in the same sen-
tences from peer-reviewed article abstracts.
This E. coli gene-gene interaction network was ex-
tended by INO to create a comprehensive vaccine-
centered E. coli gene-gene interaction network. In this
study, these additional entities were limited only to those
in the same sentences, where two or more E. coli genes
were mentioned.
Centrality analysis
The collected gene-interaction networks were subject to
centrality analysis. Four different centrality metrics were
Fig. 1 Project workflow. The presented study was limited to the
literature in the vaccine domain. Representative E. coli genes, obtained
through a pan-genome orthologue analysis, host genes as well as two
established biomedical ontologies of interactions (INO) and vaccines
(VO) were identified from the literature by SciMiner. Based on the
co-occurrence among these identified entities, vaccine-associated
E. coli gene-gene interaction network was generated and further
analyzed to identify the central genes and enriched biological functions
in this network
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 3 of 10
computed to identify the most important nodes (i.e.,
genes, vaccine genes, and INO terms) in the created
interaction networks using the Cytoscape plug-in
CentiScaPe [26]. The degree centrality of a node is the
number of nodes that are its first neighbors (i.e., directly
connected to the given node). The more connections a
node has, the more central it is based on degree centrality.
In degree centrality, all neighbors contribute equally to
the importance of a node. In eigenvector centrality, a node
contributes to the centrality of another node proportion-
ally to its own centrality. A node is more central, if it is
connected to many central nodes. The well-known PageR-
ank algorithm for ranking web pages is also based on
eigenvector centrality. Closeness and betweenness cen-
tralities depend on the position of a node in the net-
work. Closeness centrality is based on the distance of a
node to the other nodes in the network. The closer a
node is to the other nodes, the more important it is
considered to be. Betweenness centrality is based on
the number of shortest paths connecting two nodes
that pass over the given node. A node is more central,
if it acts like a bridge in the network, i.e., lies on many
shortest paths.
Ontology-based hierarchical classification of interaction
terms
All the interaction keywords identified in our literature
mining were mapped to INO terms. The OntoFox tool
[27] was used to extract these INO terms and additional
terms related to these INO terms. The Protégé OWL
editor [28] was used to visualize the hierarchical struc-
ture of these extracted terms.
Results
Pan-genome-based E. coli gene name normalization
Although EcoGene provides very good E. coli gene name
annotations, it mainly covers the E. coli strain K12.
However, many other E. coli strains are available and E.
coli gene names are very complicated with different
names across various strains. For example, the gene
names iroN and fepA are synonyms, and E. coli iroN
encodes for an outer membrane receptor FepA (http://
www.ncbi.nlm.nih.gov/gene/7324526). Similarly, E. coli
strain CFT073 gene C0393 (hemoglobin protease) has
100% sequence identity with the vacuolating autotran-
sporter toxin (vat) gene from many other E. coli strains
such as strain PAB48 (GenBank Accession ID:
KR094946.1). Another example is the E. coli gene rfaJ,
which has several synonyms such as waaJ (http://ecoli-
wiki.net/colipedia/index.php/rfaJ:Quickview). Such syno-
nym information is often not reported in EcoGene.
Therefore, we applied the pan-genome based strategy as
detailed in the Methods section in order to get a more
complete set of normalized E. coli gene names.
VO modeling of vaccines and related vaccine genes
The newest VIOLIN vaccine database includes 44 E. coli
vaccines. Only approximately half of these vaccines existed
in the initial release of VO back in 2012. In this study, we
updated VO by including all these vaccines in VO, and we
also added intermediate layer terms to better represent and
organize the relations among these terms. VO also repre-
sents 25 E. coli vaccine genes and how these vaccine genes
are used in E. coli vaccine formulations. Figure 2 provides
an example of E. coli subunit vaccine E. coli FimH with
CFA and then IFA. A subunit vaccine uses a subunit (typ-
ically a protein) of a pathogen organism as vaccine antigen.
This vaccine uses the E. coli protein FimH (an E. coli fim-
brial subunit and D-mannose specific adhesin) as the pro-
tective vaccine antigen, and it uses the complete Freunds
adjuvant (CFA) in the first vaccination and the incomplete
Freunds adjuvant (IFA) in the boost vaccination [29].
Some E. coli vaccines are live attenuated vaccines. One
method to make a live attenuated vaccine is to knock out a
virulence factor gene(s) in a wild-type virulent strain to
make it less virulent (i.e., attenuated) but keep the antige-
nicity. For example, the carA and carB genes, which form
a carAB operon, are virulent E. coli genes. Their mutations
Fig. 2 VO hierarchical structure and axioms of E. coli vaccines. a
Vaccine hierarchy that shows the E. coli vaccines. b Axioms of the E.
coli vaccine E. coli FimH with CFA and then IFA (VO_0001168). The
circled term FimH is the E. coli protein FimH. These are screenshots
with the Protégé OWL editor
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 4 of 10
in an E. coli strain led the development of the mutant
vaccine E. coli carAB mutant vaccine [30]. Such a viru-
lence factor gene whose mutation leads to the generation
of an experimental verified vaccine is named virmugen
[25]. In VO, an ontological axiom is used to represent the
relation between the vaccine and the mutated genes:
E. coli carAB mutant vaccine: not has_part some
(carA or carB)
In this ontological axiom, the relation not has part
means that the mutant vaccine strain does not have carA
and carB genes in the mutated bacterial genome.
The VO representation of the vaccine-gene relations
provides rationale for us to identify specific vaccine
genes and study how these vaccine genes are related to
other E. coli genes.
Literature mining statistics and interaction network
The complete abstracts and titles from PubMed,
published before December 31, 2014, were processed by
SciMiner to identify E. coli genes, INO and VO terms.
SciMiner identified 2,037 E. coli genes from 53,925 sen-
tences in articles indexed with Escherichia coli [MeSH].
The study was further limited to the articles in the vaccine
context (defined by vaccines [MeSH]), where SciMiner
identified a total of 1,781 unique E. coli genes that were
co-cited with at least one other E. coli genes at the
sentence level. A total of 16,887 INO terms (mapped to
88 unique INOs) were also identified in 11,350 sentences.
An interaction network of these E. coli genes and INO
terms within the vaccine context was visualized in Fig. 3a.
A subnetwork focused on known genes used in E. coli vac-
cines was generated as illustrated in Fig. 3b, which include
5 vaccine-genes (nodes in cyan), 62 E. coli non-vaccine
genes (nodes in red), and 25 INO terms (nodes in purple).
As seen in the carA and carB sub-network (Fig. 3c),
carA and carB were found in our literature mining to
interact with each other through different interaction
types including gene expression, gene fusion, dominant
regulation, and protein translation. For example, the re-
trieved sentence corresponding to the gene fusion inter-
action (INO_0000106) between these two genes is:
A construct was made in which the intergenic region
between the contiguous carA and carB genes was
deleted and the sequences encoding the carbamyl-
phosphate synthetase subunits were fused in frame [31].
In this case, after deletion of the intergenic region be-
tween these two genes, a fused carA-carB gene formed,
and the resulting fusion protein was activated 10-fold
relative to the native protein [31].
Meanwhile, our literature mining also found that carA
or carB interacts with other genes. For example, carB
interacts with pyrB through the induction interaction
type (INO_0000122) as shown in the following sentence:
In addition, however, exogenous uracil triggers cellu-
lose production, particularly in strains defective in either
carB or pyrB genes, which encode enzymes catalyzing
the first steps of de novo UMP biosynthesis. [32].
This sentence represents a complex interaction
process. Specifically, the direct induction interaction is
that exogenous uracil triggers cellulose production, and
such interaction occurs when the carB or pyrB gene was
defective. In this case, carB and pyrB genes are related,
since both encode enzymes that catalyze the frist steps
of de novo UMP biosynthesis [32]. In this case, the two
genes do not directly interact through the induction
type, i.e., it is not that carB (or pyrB) triggers pyrB (or
carB). Instead, the two genes are involved in providing a
condition to another induction interaction. Our study
found that such cases occur frequently.
Other sub-networks centered on the other vaccine
genes are available in Additional file 1. A Cytoscape file
containing the E. coli gene-vaccine interaction network
as well as the sub-networks centered on each vaccine-
gene is available in Additional file 2.
Centrality analysis
Our centrality analysis using the Fig 3b subnetwork
identified the centralities of three types of nodes (E. coli
vaccine genes, other E. coli genes, and INO terms) in the
literature mined network as shown in Fig. 3b. By identi-
fying top 10 nodes based on either of the four types of
centrality scores, 19 central nodes were identified
(Table 1). Out of the 19 central nodes, all the 5 E. coli
vaccine genes are in the list. The result is reasonable
since all the genes in Fig. 3b subnetwork are expected to
interact with at least one of these five E. coli genes. Eight
other E. coli genes are also found central in the list.
Besides identifying the central E. coli genes, we also
targeted the identification of central types of interactions
among these genes in the created vaccine associated E.
coli gene interaction network. Therefore, INO terms
(interaction types) were represented as nodes in the net-
work. Six INO terms were identified in the top node list
(Table 1). These terms (e.g., gene expression and regula-
tion) represent the most commonly identified interaction
types in vaccine-related E. coli gene interaction studies.
Different centrality measures provide different aspects
of the network (Table 1), since they define centrality in
different ways and capture central nodes based on differ-
ent aspects. While some node are central based on all
four centrality metrics, some are identified as central by
only one or two of the centrality metrics. Overall, degree
centrality and eigenvector centrality results are similar.
Interestingly, three out of the five vaccine genes were
ranked in the top 10 only by the betweenness centrality
metric, suggesting that these three vaccine genes are
critical to link together different sections in the network.
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 5 of 10
A node may be considered as important, even if it is
identified as central based on only one centrality metric.
Therefore, to summarize the importance of a node, the
minimum (i.e., top) rank of each node based on any of
the four centrality metrics is shown in Table 1.
INO ontology-based analysis of interaction types
Here is one example sentence identified from our study:
Complementation experiments indicated that both the
major fimbrial subunit gene, fimA, and the fimH gene in
combination with either the fimF or the fimG gene were
required for mannose-specific adhesion. [33].
This sentence represents the INO interaction type
regulation (INO_0000157). Specifically, the four genes
fimA, fimH, and fimF (or fimG) were found to regulate
(were required for) the mannose-specific adhesin [33].
Note that in our literature mining, the regulation rela-
tion does not have to be one gene regulating another
gene; it is also allowable for both genes regulating for a
specific phenotype.
For the INO interaction type detection, we used the
literature mining keywords collected in the INO. Spe-
cifically, in INO, we used the annotation property has
literature mining keywords (INO_0000006) to assign
many keywords used to represent the interaction type.
A
B
C
Fig. 3 The interaction network among E coli genes and INO terms. a Interaction network among all E. coli genes co-cited at a sentence-level with
INO terms in the vaccine context. b a sub-network focused on five E. coli genes (in cyan nodes) that are known to be used in E. coli vaccines. c a
sub-network of two vaccine genes, carA and carB, and their immediate neighbors in (b). Gene names with additional synonyms were represented
with the sign |. For example, iroN|fepA represents that this gene has two gene symbols iroN and fepA. Nodes in red represent E. coli genes,
except cyan nodes, and nodes in purple are INO terms identified in the same sentences of these E. coli genes. The pink dashed lines represent
interaction between E. coli gene and INO terms, while the black solid lines represent the interaction between E. coli genes
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 6 of 10
For example, required is a keyword assigned for the
INO interaction type regulation.
From our literature mining study, 25 specific INO
interaction types were identified. The hierarchical struc-
ture of these 25 INO interactions types is shown in Fig. 4.
As shown in this figure, the most common interaction
type is various types of regulation, including positive,
negative, and dominant regulation types. Other inter-
action types such as direct physical interactions and
gene expression types (including transcription and trans-
lations) are also included. Such an INO hierarchical ana-
lysis clearly illustrates how different genes interacted
with each other based on the reported literature papers.
Discussion
The contributions of this study are multiple. First, this
study for the first time applied ontology-based literature
mining method to analyze vaccine-related E. coli gene
interaction network using all PubMed abstracts. Con-
sidering the status of E. coli in microbiology, infectious
diseases, and the whole biology, such a study is important.
Second, our study employed pan-genome-based approach
to normalize E. coli gene names across various strains.
Third, this study represents the first-time application of
applying both VO and INO in supporting literature
mining of pathogen and vaccine-related gene-gene
interactions. Fourth, we further demonstrated that the
centrality-based analysis enhanced our ability in identi-
fying hub or critical genes or nodes in the E. coli gene-
vaccine intearction network.
The identification of those other E. coli genes that
interact with known E. coli vaccine genes from our study
provides scientific insights on E. coli vaccine research
and development. These genes as a whole provide an
explanation on the functions and biological processes of
these genes preferred for vaccine development. These
genes also provide new candidates for future vaccine de-
velopment. It should be noted that not all E. coli vaccine
genes were identified in our literature mining process,
since our analysis focuses on retrieving gene-gene inter-
actions instead of individual genes.
Compared to our previous vaccine-related Brucella
gene interaction literature mining study [16], the current
study includes the more challenging E. coli species and
Table 1 The most central nodes in the network. The top 10
nodes based on Degree (D), Eigenvector (E), Closeness (C), and
Betweenness (B) centrality metrics. The minimum (i.e., top) rank
of each node based on any of the four centrality metrics is shown
in the Min column
Type Name D E C B Min
Vaccine gene fimH 1 1 2 1 1
Vaccine gene fepA 2 2 1 2 1
E. coli fimA 3 7 9 6 3
E. coli ompT 4 4 3 4 3
E. coli hlyA 5 3 4 - 3
INO Inclusion 6 5 3 7 3
Vaccine gene vat - - - 3 3
Vaccine gene carA - - - 5 5
INO protein translation - - 5 - 5
E. coli yfcU 8 6 - - 6
INO gene expression 9 - 6 10 6
E. coli entF 7 - - - 7
E. coli chuA 9 8 7 - 7
E. coli tonB - - - 8 8
INO dominant regulation - - 8 - 8
INO association 9 9 9 - 9
INO regulation 9 - 9 - 9
Vaccine gene carB - - - 9 9
E. coli hlyD - 10 - - 10
The rankings of the terms are shown. Terms with the same centrality scores
have the same ranking. Abbreviations here: E. coli - E. coli gene, Vaccine
gene - E. coli vaccine gene; INO  INO term
Fig. 4 INO hierarchy of 25 interaction keywords identified in the
vaccine-related E. coli gene interaction network. OntoFox [27] was used
to extract the hierarchical structure among the 25 identified INO types.
The OntoFox option of includeAllIntermediates was used in the
process. The Protégé OWL editor was used for structure visualization
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 7 of 10
also for the first time employed a new INO-based inter-
action type analysis approach. In general, our study
found many commonly reported interaction types (e.g.,
expression and regulation) from the E. coli vaccine-gene
interaction network. We also found that different types
of regulation often are not about the direct regulatory
interactions between two genes (e.g. gene A regulates
gene B). Instead, they are often related to regulatory
interactions between the genes and another interaction
process or phenotype. For example, as shown in the
mannose-specific adhesion sentence described in the
Results section, the gene fimA and the gene fimH were
both required for a phenotype: mannose-specific adhe-
sion [33], rather than they had a direct interaction.
Another example is the carB vs pyrB interaction, which
was also shown in the Results section, where the two
genes participate in a pathway and a defective pathway
process results in the occurrence of an induction inter-
action [32]. These two examples represent quite com-
plex interactions that involve multiple components and
relationships that are represented by multiple literature
keywords as shown in our previous studies [18, 34].
Further research is required to automatically identify
such specific and complex patterns from the biomedical
literature.
It is possible that tagged E. coli genes from our litera-
ture mining and their associated ortholog genes in other
bacteria may likely co-occur with most vaccines for vari-
ous bacteria (instead of only E. coli). This aspect of study
is out of our scope for this study since we only focus on
E. coli in this study. However, our previous INO-based
study found that many genes co-occur in sentences with
vaccines, and we even developed an INO-based Fishers
exact test to perform enrichment analysis of tagged
genes in the scope of INO [18]. It is noted that the pre-
vious INO-based study focused on human genes [18]
while our current study focuses on bacterial genes. How-
ever, we envision that bacterial genes would perform
similarly. Our previous VO-based Brucella gene-vaccine
interaction study identified many interesting patterns
among the Brucella genes as well [16]. Furthermore,
many studies have found that the collection of bacterial
genes, proven to be useful in vaccine development, often
share common characteristics [25, 35, 36]. For example,
systematic analysis of a collection of experimentally
verified protective bacterial genes revealed multiple
conserved domains (or called motifs) and preferred sub-
cellular localizations among protective antigens [35, 36].
The collection and analysis of a set of virulence factors
(i.e., virmugens) whose mutations led to experimentally
verified live attenuated vaccines also discovered many
enriched virmugens patterns, for example, the frequent
usage of bacterial aroA genes as virmugens, and virmu-
gens often involving metabolism of nutrients (e.g., amino
acids, carbohydrates, and nucleotides) and cell mem-
brane formation [25]. These results out of systematical
analyses facilitate rational vaccine design. More re-
searches are warrantied to apply literature mining to
identify more specific vaccine-associated gene/protein
patterns and underlying biological and immunological
mechanisms.
Our literature mining method identifies gene-gene
interactions based on sentence-level co-citation analysis.
The directionality of the extracted gene-gene interac-
tions is not detected by the current SciMiner. Therefore,
the generated gene-gene interaction network is undir-
ected and the centrality scores are computed on this
undirected network. For example, if a sentence states
that Gene A activates Gene B, an undirected edge be-
tween Gene A and Gene B is included in the gene-gene
interaction network. The information that the direction-
ality of the interaction is from Gene A to Gene B is lost.
In our future work, we will develop new text mining and
statistical methods to identify the directionality informa-
tion regarding gene-gene interactions. With the direc-
tionality of extracted gene-gene interactions, it would be
easier to find provider or consumer roles for differ-
ent genes. We will investigate how centrality analysis is
affected when directionality information is incorporated.
A direction-based importance metric, such as SimRank
[37], can be measured to provide direction-based
weights to network nodes and generate more interesting
results.
Our future directions will be multiple. First, we plan to
improve our pan-genome-based gene name normalization
method to cover other pathogens and to include such a
strategy automatically in our SciMiner pipeline to study
other pathogens (including bacteria, viruses, and para-
sites). The performance of our SciMiner pipeline in host-
pathogen interaction literature mining will be thoroughly
evaluated using manually curated documents. Second, we
also plan to apply our methods to study host-pathogen/
vaccine interactions. In addition, we will extend the INO
modeling to better support ontology-based literature
mining. Furthermore, statistical and machine learning
methods [38, 39] will be explored to improve our litera-
ture mining and downstream analysis.
Conclusions
In this study, we first used a pan-genome-based
approach to collect and normalize E. coli genes and cor-
responding gene names, relied on the Vaccine Ontology
to obtain E. coli vaccines and vaccine genes, and applied
the Interaction Network Ontology to obtain possible
interaction keywords. These E. coli gene names, vaccine
names, vaccine genes, and interaction keywords were
then combinatorially used by SciMiner to process all
PubMed abstracts to construct a vaccine-related E. coli
Hur et al. Journal of Biomedical Semantics  (2017) 8:12 Page 8 of 10
gene-vaccine interaction network. From the contructed
interaction nework, our centrality analysis further identi-
fied hub or critical E. coli genes and the types of the
interactions involved in the network. New insights have
been identified using our systematic analysis. To our
knowledge, this is the first study of applying pan-
genome and ontology-based literature mining strategy to
construct E. coli gene interaction network and perform
systematic centrality analysis.
Additional files
Additional file 1: A PDF file containing three other gene-vaccine
sub-networks. (DOCX 932 kb)
Additional file 2: A Cytoscape session file containing the E. coli
gene-vaccine interaction network and its sub-networks. (CYS 76 kb)
Abbreviations
APEC: Avian pathogenic E. coli; CONDL: Centrality and ontology-based
network discovery using literature data; EAEC: Enteroaggregative E. coli;
EHEC: Enterohaemorrhagic E. coli; ExPEC: Extraintestinal pathogenic E. coli;
HGNC: HUGO gene nomenclature committee; INO: Interaction network
ontology; IPEC: Intestinal pathogen E. coli; MNEC: Meningitis-Associated E.
coli; UPEC: Uropathogenic E. coli; VIOLIN: Vaccine investigation and online
information network; VO: Vaccine ontology
Acknowledgements
The authors thank the participants of the 5th International Workshop on
Vaccine and Drug Ontology Studies (VDOS) 2016 for their valuable feedback.
Funding
This research was supported by grant R01AI081062 from the US NIH National
Institute of Allergy and Infectious Diseases (to YH) and the BAGEP Award of
the Science Academy (to AO).
Availability of data and materials
All data generated or analysed during this study are included in this
published article and Additional files.
Authors contributions
JH developed the ontology-based literature mining pipeline and generated
data with the vaccine domain use case. AO performed the centrality-based
analysis of the vaccine associated E. coli gene interaction network. YH devel-
oped the VO and INO and served as an E. coli vaccine domain expert. JH,
AO, and YH all participated in the project design, result interpretation, and
manuscript writing. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Department of Biomedical Sciences, University of North Dakota School of
Medicine and Health Sciences, Grand Forks, ND 58202, USA. 2Department of
Computer Engineering, Bogazici University, Istanbul 34342, Turkey.
3Department of Microbiology and Immunology, Unit for Laboratory Animal
Medicine, University of Michigan Medical School, Ann Arbor, MI 48109, USA.
4Department of Microbiology and Immunology, University of Michigan
Medical School, Ann Arbor, MI 48109, USA. 5Center for Computational
Medicine and Bioinformatics, University of Michigan Medical School, Ann
Arbor, MI 48109, USA. 6Comprehensive Cancer Center, University of Michigan
Medical School, Ann Arbor, MI 48109, USA.
Received: 24 December 2016 Accepted: 3 March 2017
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33
DOI 10.1186/s13326-017-0145-x
RESEARCH Open Access
An automatic approach for constructing a
knowledge base of symptoms in Chinese
Tong Ruan1*, Mengjie Wang1, Jian Sun1, Ting Wang1, Lu Zeng1, Yichao Yin2 and Ju Gao2
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: While a large number of well-known knowledge bases (KBs) in life science have been published as
Linked Open Data, there are few KBs in Chinese. However, KBs in Chinese are necessary when we want to
automatically process and analyze electronic medical records (EMRs) in Chinese. Of all, the symptom KB in Chinese is
the most seriously in need, since symptoms are the starting point of clinical diagnosis.
Results: We publish a public KB of symptoms in Chinese, including symptoms, departments, diseases, medicines,
and examinations as well as relations between symptoms and the above related entities. To the best of our
knowledge, there is no such KB focusing on symptoms in Chinese, and the KB is an important supplement to existing
medical resources. Our KB is constructed by fusing data automatically extracted from eight mainstream healthcare
websites, three Chinese encyclopedia sites, and symptoms extracted from a larger number of EMRs as supplements.
Methods: Firstly, we design data schema manually by reference to the Unified Medical Language System (UMLS).
Secondly, we extract entities from eight mainstream healthcare websites, which are fed as seeds to train a multi-class
classifier and classify entities from encyclopedia sites and train a Conditional Random Field (CRF) model to extract
symptoms from EMRs. Thirdly, we fuse data to solve the large-scale duplication between different data sources
according to entity type alignment, entity mapping, and attribute mapping. Finally, we link our KB to UMLS to
investigate similarities and differences between symptoms in Chinese and English.
Conclusions: As a result, the KB has more than 26,000 distinct symptoms in Chinese including 3968 symptoms in
traditional Chinese medicine and 1029 synonym pairs for symptoms. The KB also includes concepts such as diseases
and medicines as well as relations between symptoms and the above related entities. We also link our KB to the
Unified Medical Language System and analyze the differences between symptoms in the two KBs. We released the KB
as Linked Open Data and a demo at https://datahub.io/dataset/symptoms-in-chinese.
Keywords: Knowledge base, Symptoms in Chinese, Linked data, Information extraction
Background
Medical knowledge bases (KBs) play an important role in
healthcare research. Existing KBs vary from coding sys-
tems such as ICD10 [1], terminology systems such as
UMLS [2], clinical ontology systems such as SNOMED
CT [3] to medical databases such as DrugBank [4]. The
major objectives for these KBs are to provide knowl-
edge to medical workers and to promote standardization
*Correspondence: ruantong@ecust.edu.cn
1East China University of Science and Technology, Shanghai, China
Full list of author information is available at the end of the article
and interoperability for biomedical information systems
and services. Besides, there exist many different types of
biomedical KBs. For example, SIDER [5], and AMDD [6]
contain drug-related information. Diseasome [7], ParkDB
[8], and ChemProt [9] describe disease and disease-
related gene information. These KBs are necessary in
automatically processing and analyzing electronic medi-
cal records (EMRs) and then form the basis of the upper
information applications such as clinical decision support
systems.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 72 of 79
Currently there are many general-purpose KBs built
by automatic approaches. The DBpedia project [10]
extracted structured information from Wikipedia and
published them on the Web. YAGO [11] was derived
from Wikipedia, WordNet, and GeoNames. NELL [12],
SOFIE [13], and PROSPERA [14] extracted data from
the Web. The input data for NELL consisted of an ini-
tial ontology as well as a small number of instances.
SOFIE extracted ontological facts from natural language
texts and linked the facts into an ontology. PROSPERA
relied on the iterative harvesting of n-gram-itemset pat-
terns to generalize natural language patterns found in
texts.
There are also some studies in the medical field which
construct KBs automatically. Ayvaz et al. [15] built a
dataset of drug-drug interaction information from exist-
ing datasets including DrugBank, KEGG, NDF-RT, and
so on. Ernst et al. [16] constructed a knowledge graph
for biomedical science which extracted and fused data
from scientific publications, encyclopedic healthcare por-
tals and online communities. They used distant supervi-
sion in the extraction step, and used logical reasoning for
consistency checking.
However, most KBs are in English. The KBs in Chinese
are necessary in order to process the large amount of
EMRs in Chinese, which has been accumulated since the
wide adoption of hospital information systems a decade
ago. Of all KBs, the symptom KB in Chinese is mostly
required, since symptoms are the starting point of clin-
ical diagnosis and reflect the evolution of diseases. We
focus on symptoms and symptom-related entity extrac-
tion. Data sources and methods in the construction of
our KB are different from previous work, and the exper-
iments show that our KB gains roughly a higher preci-
sion than similar results in [16]. Our KB is constructed
by fusing data automatically extracted from eight main-
stream healthcare websites, three Chinese encyclopedia
sites, and symptoms extracted from a lager number of
EMRs as supplement. This automatic approach not only
avoids a large amount of manual work, but also keeps up
with changes when new entities and relations appear.
Methods
Data schema
The schema of our KB can be regarded as a simplified ver-
sion of UMLS. UMLS contains complex taxonomy includ-
ing physical objects, events, or even intellectual products.
Since our KB focuses on symptoms, we only choose parts
of UMLS related to our work, such as, Finding, Sign or
Symptom, and Disease or Syndrome.
We have summarized five concepts for our KB
driven by the requirements of processing and analyz-
ing EMRs. Besides Symptom, we add four concepts
directly related to symptom, namely, Disease, Medicine,
Department, and Examination. Traditional Chinese
medicine (TCM) describes symptoms that is different
from Western medicine. For example, Yin_deficiency
andQi_stagnation are TCM symptoms which have no
direct connection with Western medicine. In addition,
TCM diagnosis and Western medicine diagnosis are two
independent parts in EMR systems. TCM symptoms are
included in the part of TCM diagnosis. Taking the above
factors into consideration, Symptom is further divided
into TCM Symptom and Symptom of Western Medicine,
and Medicine is divided into TCM andWestern Medicine
similarly.
The schema graph is shown in Fig. 1. The center of
the graph is the concept Symptom. Symptom links to
other concepts with relations such as relevant_disease,
and has datatype properties such as location. The
Fig. 1 Schema Graph for Knowledge Base of Symptoms in Chinese. Each rectangle represents a concept and the bottom of each rectangle is an
instance
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 73 of 79
instances in Fig. 1 form a virtual scenario in clini-
cal practice. A person has Headache, Sneezing and
Aversion_to_cold which is a TCM symptom. Perhaps
he goes to the Internal_medicine_department, and
has the Blood_test and Temperature_measurement.
Finally, he is diagnosed with Summer_cold complicat-
ing with Tonsillitis and is suggested to take some Aspirin
and Bupleurum_tenue. Chinese usually take TCM and
Western medicine together.
Data extraction
Figure 2 presents the overall workflow of constructing our
KB. In the data extraction step, we first use specific HTML
wrappers [17] to extract entities and attributes from semi-
structured information in eight mainstream healthcare
websites. Then, we extract entities from three Chinese
encyclopedia sites. Entities obtained from healthcare web-
sites are fed as seeds to train a multi-class classifier and
classify entities from encyclopedia sites. Finally, we train a
Conditional Random Field (CRF) model to extract symp-
toms from EMRs. The details are described below.
Data extraction from healthcare websites
We collect eight mainstream healthcare websites (See in
Table 1). There are normally two kinds of web pages for
each website. One is the list page containing a list of enti-
ties. The other is the detail page containing the detail
description of a particular entity. All the above websites
contain list pages of symptoms, diseases and medicines.
However, list pages of department do not exist in JIANKE,
PCbaby, or fh21, and list pages of examinations only
appear in JIANKE, 120ask, and 39Health.
We take the symptom extraction as an example. All
the detail pages of symptoms in a website share similar
layouts. There is a portion called property box in the
detail page containing attribute-value pairs of an entity.
We map the property box to properties in schema graph
Table 1 Basic information of eight healthcare websites
Website name URL
Familydoctor http://www.familydoctor.com.cn/
JIANKE http://www.jianke.com/
120ask http://www.120ask.com/
QQYY http://www.qqyy.com/
39Health http://www.39.net/
99Health http://www.99.com.cn/
Fh21 http://www.fh21.com.cn/
Pcbaby http://www.pcbaby.com.cn/
manually and extract attribute-value pairs with HTML
wrappers. Since symptom may be a TCM Symptom or
Symptom of Western Medicine, we use the relevant
department attribute to classify. Specifically, if a symp-
tom is related with a department containing TCM (e.g,
Traditional Chinese Orthopedics, it will be labeled as
TCM Symptom. The symptom will be tagged as Symp-
tom ofWesternMedicine if it is related with a department
without TCM. An entity can be labeled as both TCM
Symptom and Symptom of Western Medicine. Finally,
synonymous relations are crawled from the abstracts of
entity pages with Hearst patterns described in [18]. For
example, when applying a pattern [entity1] is known as
[entity2] to sentence hyperpyrexia is known as fever, a
sameAs relation between hyperpyrexia and fever is
extracted.
We apply similar steps to other types of entities. We
use heuristic information in the detail page of a medicine
to distinguish TCM and Western medicine. If its descrip-
tion in detail page contains keywords such as Chinese
patent medicine and herbal medicine, and it is Western
medicine if its description contains pharmaceuticals,
chemicals, and so on.
Fig. 2Workflow of Constructing the Knowledge Base of Symptoms in Chinese. It contains three steps: (1) Extract data from healthcare websites,
encyclopedia sites and EMRs, respectively. (2) Align the extraction results. (3) Link our symptoms to concepts in UMLS
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 74 of 79
Data extraction from Chinese encyclopedia sites
We extract and classify entities from three largest Chinese
encyclopedia sites (i.e. Baidu Baike, Hudong Baike, and
Chinese Wikipedia).
Algorithm 1 shows the extraction process. First, we
take entities from healthcare websites as seeds (denote
as EntityList), and extract their categories in Chinese
encyclopedia sites (denote as SeedCateSet).
Algorithm 1 Extract Entities from Encyclopedia Sites
Input: EntityList, Baike EntitySet, k
Output: target EntitySet
1: for each eli ? EntityList
2: if eli exists in Baike EntitySet then
3: SeedCateSet ? eli.getcategory()
4: SeedCateList ? eli.getcategory()
5: for each catei ? SeedCateSet
6: frei ? the number of catei in SeedCateList
7: if frei < k then
8: low-confidence CS ? catei
9: for each ei ? Baike EntitySet
10: if ei.getcategory() ? SeedCateSet = ? &&
11: ei.getcategory() ? low-confidence CS =? then
12: target EntitySet ? ei
13: return target EntitySet
Then we crawl entities belonging to SeedCateSet. Sec-
ond, we classify SeedCateSet into low-confidence and
high-confidence by calculating the ratio of seeds in these
categories. Thirdly, for each entity in encyclopedia site,
if its categories contain low-confidence categories, it will
be regarded as noise and removed. We eventually col-
lect 62,013 entities from Baidu Baike, 59,704 entities
from Hudong Baike, and 2220 entities from Chinese
Wikipedia.
In the classification step, we take entities extracted from
healthcare websites as positive examples and entities from
list pages of health preservation, facial and psychol-
ogy in healthcare websites as negative examples. Then
train a Decision Tree [19] classifier with seven labels:
department, TCM, western medicine, symptom, disease,
examination and other.
The classifier uses two types of features, namely,
word formation and word distribution. The features are
obtained from five fields, namely, entity name, abstract,
content, full-text, and category of entity page, as shown in
Fig. 3 and Table 2. If an entity is classified as Symptom, we
will use heuristic rules to further determine whether it is
a TCM Symptom or Symptom of Western Medicine.
Data extraction from EMRs
Due to variations of symptoms in clinical practice, we
incorporate clinical vocabularies into our KB by extracting
Fig. 3 Five Fields of Entity Page in Encyclopedia Sites
symptoms from a large number of EMRs. In order to learn
symptoms of Western medicine, we extract texts from
the Physical Examination and Antidiastole_Western
medicine fields of EMRs. Texts within Disease Analy-
sis and Antidiastole TCM fields are selected for TCM
symptoms. Since data duplication takes place frequently
in the texts of EMRs, we remove the sentences appearing
more than once in the same record. We manually anno-
tate TCM symptoms and symptoms of Western medicine
in EMRs. Then we train a CRF [20] classifier to recog-
nize new symptoms. The features are classified as literal
features , position features, and part-of-speech (POS) fea-
tures as shown in Table 3. The literal features and position
features are adopted from [21].We set the context window
size to 3 since it achieves best results in [21].
Data fusion
Data fusion consists of three steps: entity type alignment,
entity mapping, and attribute mapping.
Table 2 Classification features for six entity types
Fields of page Classification features for six entity types
Entity name Ends with any words in (department, disease,
inflammation, tumour, syndrome, examination)
Abstract Contains any words in (symptom, syndrome,
symptoms of illness, disease name of TCM)
Content
Has more than 3 words in (function, specification,
adverse reaction, side effect, component, usage,
dosage),
Has more than 3 words in (cause, examination,
antidiastole, diagnosis, mitigation, pathogenesis,
clinical manifestation)
Full-text Contains any words in (Chinese patent medicine,
Chinese herbal medicine)
Category Contains any words in (medicine, disease, TCM,
drug, Chinese patent medicine, symptom)
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 75 of 79
Table 3 Features of CRF
Feature type Feature contents
Literal features Unigram Xi?3, Xi?2, Xi?1, Xi , Xi+1,
Xi+2, Xi+3
Bigram Xi?3Xi?2, Xi?2Xi?1, Xi?1Xi ,
XiXi+1, Xi+1Xi+2, Xi+2Xi+3
Trigram Xi?3Xi?2Xi?1, Xi?2Xi?1Xi ,
Xi?1XiXi+1, XiXi+1Xi+2,
Xi+1Xi+2Xi+3
Position features Indexi
POS features Unigram Pi?3, Pi?2, Pi?1, Pi , Pi+1,
Pi+2, Pi+3
Bigram Pi?3Pi?2, Pi?2Pi?1, Pi?1Pi ,
PiPi+1, Pi+1Pi+2, Pi+2Pi+3
Trigram Pi?3Pi?2Pi?1, Pi?2Pi?1Pi ,
Pi?1PiPi+1, PiPi+1Pi+2,
Pi+1Pi+2Pi+3
Firstly, We align entity types with a voting method,
i.e., the entity type receiving the most votes wins. When
two types have the same top votes,the entity type with
the higher priority wins. Priorities are determined by the
resources rankings in Alexa.
Secondly, we use the idea of Wang et al. [22] to map
entities. They used two variables, namely, commonness
and relatedness in combination to calculate similari-
ties between entities. In this paper, the commonness is
obtained by calculating string similarities between entity
names, while the relatedness is calculated with string sim-
ilarities of attribute values. For example, entity EA and EB
have the same type and share two attributes A1 and A2.
The commonness is defined as
StringSimilarity(EA,EB) = |LCS(EA,EB)|Max(|EA| , |EB|) (1)
where |LCS(EA,EB)| is the length of the longest common
subsequence between EA and EB. The relatedness is the
ratio of the number of similar facts. If the product of com-
monness and relatedness is higher than a threshold, we will
map EA to EB.
Finally, we map attribute from the extraction results
to our ontology. Since property boxes of each healthcare
website share the same attribute names, we manually map
attribute names in healthcare websites to our ontology.
However, in Chinese encyclopedia sites, the infoboxes of
entity pages exist lots of attribute names that are similar
in semantics but different in names. We map attributes
to our ontology according to type information of entities
and attribute values. For example, attribute symptom
of triple <vertigo of heat stroke, symptom, thirsty> is
mapped into symptom related symptom, because entity
vertigo of heat stroke and attribute value  thirsty are
both symptoms.
Link to UMLS
To investigate similarities and differences between symp-
toms in Chinese and English, we link our KB to UMLS, a
medical KB widely used in clinical practice and medical
informatics research. Xu et al. [23] used context simi-
larities to link phrases in English discharge summary
to Chinese discharge summary. However, there are no
such contexts in our situation. We first call the API of
Baidu Translate [24] to translate symptoms in our KB into
English phrases. Second, each phrase is transformed into
a bag of words (denote as BWCS), and the same opera-
tion is done to concepts in UMLS (denote as BWUMLS).
Third, the JaccardSimilarity [25] between elements in
BWCS and BWUMLS is calculated. Only when the value of
JaccardSimilarity is 1, do we make a linkage.
Results and discussion
Classification results
We apply Decision Tree algorithm to train a multi-
class classifier. Twenty-six features from five fields of
entity pages in encyclopedia sites are utilized in this
paper. We use ten fold cross-validation. Figure 4 shows
the results of our classifier in three encyclopedia sites,
and indicates that our classifier has high accuracy and
recall.
EMR data extraction results
We have collected 250,000 EMRs from Shanghai
Shuguang Hospital as our corpora. Two TCM experts
annotate symptoms mentioned in 1000 EMRs, which are
randomly divided into two parts. One containing 660
EMRs form the training set, and the rest form the test set.
Then we train a CRF model. The precision of TCM symp-
tom is 93.26%, and the precision of symptom of Western
medicine is 95.37%. Finally, we use the model to recognize
new symptoms from 249,000 EMRs. We have extracted
2376 symptoms, 387 of which are TCM symptoms and
1989 of which are symptom of Western medicine.
Statistic
Our KB has 135,485 distinct entities and 617,499 facts.
More precisely, there are 26,821 symptoms, 32,956 dis-
eases, 67,712 medicines, 292 departments, and 7704
examinations, shown in Fig. 5. Table 4 shows the dis-
tribution of entities from each source and the ulti-
mate results of our KB. We find: (1) The number of
symptoms from all sources is 41,020. It is far larger
than the number of distinct symptoms (i.e. 26,821)
in our KB, which shows the large-scale duplication
between different data sources. (2) The website that
contributes most to symptoms in our KB is fh21. It
contains 9780 symptoms and accounts for only 36.5%
of our KB, which shows the advantage of collecting
data from different sources. (3) The EMRs also add
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 76 of 79
Fig. 4 Classification Results for Encyclopedia Sites. The result for Examination is obviously lower than those for the other classifications, because
some entity pages in encyclopedia sites are irregular, leading to few features being used. Besides, Chinese Wikipedia has a few seed entities of
examination, so its result is worse than those for other encyclopedias
2376 new symptoms to our KB, showing the differ-
ences between symptoms used in websites and in clinical
practice.
For correctness evaluation, we use correctness ratio of
facts [26] to evaluate symptom-related facts. Each sam-
pled triple is evaluated by seven persons according to their
knowledge. We integrate the evaluating results by voting.
Since our KB is large, we use simple random sampling
method to draw samples.
Based on [26], we sampled 417 triples from 26,821
rdf:type triples whose objects are Symptom to calculate
the correctness of symptoms. Its correctness ratio is 98.1%.
Then, we sampled 423 triples from 295,946 symptom
related triples. The correctness ratio of symptom-related
facts is 95.9%.
We have collected 4298 links between symptoms in
our KB and concepts in UMLS (abbreviate as symp-
tom links). We sampled 385 triples to evaluate the
correctness of the links, and the correctness ratio is
92.0%. We calculate the semantic type distribution on
the symptom links in UMLS, shown in Fig. 6a. Nor-
mally, symptoms in our KB are expected to link to
concepts in Sign or Symptom or Finding. But 53.3% of
symptoms are linked to other semantic types in UMLS.
For example, Icterus (C0022346) and infectious jaun-
dice (C0241954) are common symptoms in Chinese,
while the semantic type of the former one is Patho-
logic Function and the latter one is Disease or Syn-
drome. This shows the range of symptom in life is much
broader than that in medical science. Attribute distri-
bution on symptom links in the two KBs are shown
in Fig. 6b and c. We define six attributes for symp-
toms in our KB, and UMLS defines 13 attributes for
the linked concepts. However, most properties in UMLS
do not have fixed domains and ranges. Thus, these
attributes can not be interpreted uniformly. For example,
RO in UMLS refers to an uncertain relation. In con-
trast, our KB provides relations with definite syntax and
semantics.
Conclusions and future work
The KB is constructed by fusing data automatically
extracted from eight mainstream healthcare websites,
three Chinese encyclopedia sites, and symptoms extracted
from a lager number of EMRs as supplement. Finally, we
obtain 135,484 entities to our KB, among them, the num-
ber of symptom entities is 26,821. The KB can be used
to annotate symptoms in EMRs. It can also be embed-
ded into EMR systems to help therapists with symptom
recommendations. In the future, we will try to construct
a whole KB of commonly used medical vocabularies in
Chinese, linking them to UMLS concepts.
a b
Fig. 5 Data Distribution on our KB. From (a), symptom-related facts account for 49.23% in all facts of our KB, which is the result of symptoms being
the focus of our KB. From (b), we find that medicine entities account for 50% of all entities, and 64.4% of them are TCM
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 77 of 79
Ta
b
le
4
En
tit
y
ev
al
ua
tio
n
of
di
ffe
re
nt
da
ta
so
ur
ce
s
En
tit
y
Ty
pe
Pr
ec
is
io
n
H
ar
ve
st
ed
en
tit
ie
s
Fa
m
ily
do
ct
or
JIA
N
KE
12
0a
sk
Q
Q
YY
39
H
ea
lth
99
H
ea
lth
fh
21
PC
ba
by
Ba
id
u
Ba
ik
e
H
ud
on
g
Ba
ik
e
C
hi
ne
se
W
ik
ip
ed
ia
EM
Rs
Re
su
lt
Sy
m
pt
om
0.
98
1
47
75
77
48
76
10
41
23
66
59
57
45
97
80
17
87
29
32
99
7
39
3
23
76
26
82
1
D
is
ea
se
0.
96
7
99
98
80
04
71
38
25
46
77
78
34
4
39
91
13
89
56
88
41
84
58
7

32
95
6
M
ed
ic
in
e
0.
98
3
89
3
12
81
32
71
21
75
63
25
14
23
51
21
87
9
22
15
2
27
46
9
36
5

67
71
2
D
ep
ar
tm
en
t
0.
97
6
31

55
12
37
31


19
2
12
5
53

29
2
Ex
am
in
at
io
n
0.
78
3

14
03
30
2

29
09



23
0
30
1
39

77
04
A
gg
re
ga
te
da
0.
93
8
15
69
7
18
43
6
18
37
6
88
56
23
70
8
75
43
18
89
2
40
55
31
19
4
33
07
6
14
37
23
76
13
54
85
a P
re
ci
si
on
va
lu
es
ar
e
av
er
ag
ed
an
d
nu
m
be
rs
of
ha
rv
es
te
d
en
tit
ie
s
ar
e
su
m
m
ed
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):33 Page 78 of 79
a b c
Fig. 6 Data Distribution on the Linked Results. a Semantic Type Distribution on Linked Concepts in UMLS, b Property Distribution on Linked
Symptom in our KB, c Property Distribution on Linked Concepts in UMLS
Abbreviations
CRF: Conditional random field; EMR: Electronic medical record; KB: Knowledge
base; POS: Part-of-speech; TCM: Traditional Chinese medicine; UMLS: Unified
medical language system
Acknowledgements
We would like to thank three medical experts from Shanghai Shuguang
Hospital for helping evaluate correctness of our data.
Funding
This work and the publication cost of this paper was supported by the 863
plan of China Ministry of Science and Technology (project No: 2015AA020107),
Action Plan for Innovation on Science and Technology Projects of
Shanghai(project No:16511101000), Research on the Construction Technology
of the Healthy and Aged Knowledge Base Based on the Combination of
Medical and Care (project No: 2015BAH12F01-05), and Research on Efficient
Query Algorithm for Large Scale Annotated Semantic Knowledge (project No:
61402173).
Availability of data andmaterials
We released the KB as Linked Open Data and a demo at https://datahub.io/
dataset/symptoms-in-chinese.
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Authors contributions
TR designed the schema of our KB and gave professional technical guidance
at each step, namely, data extraction, data fusion, and linkage. LZ used specific
HTML wrappers to extract entities and attributes from semi-structured
information in eight mainstream healthcare websites. JS extracted and
classified entities and attributes from three encyclopedia sites. TW trained a
CRF model to extract symptoms from ERMs. MW fused data from different
sources, including entity type alignment, entity mapping, and attribute
mapping. And she linked our KB to UMLS. As experts in healthcare, JG and YY
provided help and guidance in the process of data evaluation. All authors read
and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1East China University of Science and Technology, Shanghai, China. 2Shanghai
Shuguang Hospital, 200025 Shanghai, China.
Published: 20 September 2017
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 
DOI 10.1186/s13326-017-0117-1RESEARCH Open AccessDeveloping a knowledge base to support
the annotation of ultrasound images of
ectopic pregnancy
Ferdinand Dhombres1*, Paul Maurice1, Stéphanie Friszer1, Lucie Guilbaud1, Nathalie Lelong2, Babak Khoshnood2,
Jean Charlet3, Nicolas Perrot4, Eric Jauniaux5, Davor Jurkovic6 and Jean-Marie Jouannic1Abstract
Background: Ectopic pregnancy is a frequent early complication of pregnancy associated with significant rates of
morbidly and mortality. The positive diagnosis of this condition is established through transvaginal ultrasound
scanning. The timing of diagnosis depends on the operator expertise in identifying the signs of ectopic pregnancy,
which varies dramatically among medical staff with heterogeneous training. Developing decision support systems
in this context is expected to improve the identification of these signs and subsequently improve the quality of
care. In this article, we present a new knowledge base for ectopic pregnancy, and we demonstrate its use on the
annotation of clinical images.
Results: The knowledge base is supported by an application ontology, which provides the taxonomy, the vocabulary
and definitions for 24 types and 81 signs of ectopic pregnancy, 484 anatomical structures and 32 technical elements
for image acquisition. The knowledge base provides a sign-centric model of the domain, with the relations of signs to
ectopic pregnancy types, anatomical structures and the technical elements. The evaluation of the ontology and
knowledge base demonstrated a positive feedback from a panel of 17 medical users. Leveraging these semantic
resources, we developed an application for the annotation of ultrasound images. Using this application, 6 operators
achieved a precision of 0.83 for the identification of signs in 208 ultrasound images corresponding to 35 clinical cases
of ectopic pregnancy.
Conclusions: We developed a new ectopic pregnancy knowledge base for the annotation of ultrasound images. The
use of this knowledge base for the annotation of ultrasound images of ectopic pregnancy showed promising results
from the perspective of clinical decision support system development. Other gynecological disorders and fetal
anomalies may benefit from our approach.
Keywords: Application ontology, Knowledge base, Ectopic pregnancyBackground
Ectopic pregnancy is a common early pregnancy
complication
Ectopic pregnancy occurs in 1 to 2% of pregnancies in de-
veloped countries and is defined by the implantation of a
gestational sac outside the endometrial cavity of the uterus
[1, 2]. The direct mortality rate from ectopic pregnancy is* Correspondence: ferdinand.dhombres@aphp.fr;
ferdinand.dhombres@inserm.fr
1UPMC Medical Faculty (Paris 6), Department of Fetal Medicine in Armand
Trousseau Hospital (APHP), INSERM U1142 (LIMICS), 26 Avenue du Dr Arnold
Netter, 75012 Paris, UE, France
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This artic
International License (http://creativecommons
reproduction in any medium, provided you g
the Creative Commons license, and indicate if
(http://creativecommons.org/publicdomain/zeestimated to be 16.9 per 100,000 ectopic pregnancies [2],
and is responsible for 4 to 10% of pregnancy-related
deaths around the world [3]. Fallopian tubes are the most
common site for ectopics to implant (tubal ectopics) with
about 95% of ectopic pregnancies located there. For the
rest, the implantation occurs within the uterine wall, but
outside the endometrial cavity. Non-tubal ectopics are
more difficult to diagnose than tubal ectopics and are as-
sociated with a higher mortality and morbidity [4]. De-
layed diagnosis is the main factor for ectopic pregnancy
associated with maternal death [2] and also affects the
success rate of future pregnancies [5].le is distributed under the terms of the Creative Commons Attribution 4.0
.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
ive appropriate credit to the original author(s) and the source, provide a link to
changes were made. The Creative Commons Public Domain Dedication waiver
ro/1.0/) applies to the data made available in this article, unless otherwise stated.
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 2 of 13Ectopic pregnancy diagnosis relies on ultrasound
expertise
The positive diagnosis of ectopic pregnancy is established
through ultrasound scanning. More specifically, transvagi-
nal scanning has been demonstrated to be superior to
transabdominal ultrasound [4]. Consistent with continu-
ous improvement in imaging quality and expertise, it has
been recently suggested that a skilled operator could
achieve a definite diagnosis at the very first scan [6]. How-
ever, most hospitals still rely on a heterogeneous staff to
manage patients at risk for ectopic pregnancy, including
emergency physicians, sonographers, radiologists and/or
doctors in training [2, 7], with different levels of training
and expertise. Thus, three or more visits are needed for
50% of these patients [8].
A shared representation for ectopic pregnancy imaging
Existing repositories of medical terminologies and ontol-
ogies, namely the Open Biomedical Ontologies (OBO)
Foundry [9], the National Center for Biomedical Ontology
(NCBO) BioPortal [10] and the Unified Medical Language
System (UMLS) Metathesaurus [11] do not include a
comprehensive set of resources to represent ultrasound
signs. None of the resources reviewed in a recent survey
of biomedical imaging ontologies was suitable for ectopic
pregnancy [12]. This domain involves concepts from vari-
ous medical domains, namely medical imaging, human
anatomy and obstetrics/gynecology (OB/GYN). While
existing standard terminologies may support a formal and
shared representation for parts of our domain, as do the
Foundational Model of Anatomy (FMA) [13] and the
Radiology Lexicon (RadLex) [14], none of them provides
the appropriate granularity for ectopic pregnancy imaging.
More precisely, RadLex supports the representation of
signs from various imaging modalities (including 50 ultra-
sound imaging signs [15]), as well as their relations to
various medical conditions (including ectopic pregnancy),
which makes it the best resource for our domain. How-
ever, RadLex is insufficient, because there are no sub-
classes for ectopic pregnancy [RadLex:RID4942] andFig. 1 Graphical view of the "ectopic pregnancy" concept from the Radiology
neighborhood for "ectopic pregnancy"[RadLex:RID4942]RadLex only provides two related signs (ring of fire sign
[RadLex:RID35495] and interstitial line sign [RadLe-
x:RID35308]), as illustrated in Fig. 1.
Objectives
In this article, we present a new ectopic pregnancy know-
ledge base and its application to ultrasound image annota-
tion. In this knowledge base, the signs of ectopic pregnancy
are linked to specific types of ectopic pregnancy, the ana-
tomical structures involved and the technical elements of
imaging. We also developed an ontology to provide the vo-
cabulary used in the knowledge base, as well as an applica-
tion for annotating ultrasound images, which leverages the
knowledge base. We demonstrate the use of the knowledge
base on the annotation of clinical images.
Methods
In this section, we describe our approach to developing
a knowledge base for ectopic pregnancy imaging. We
start by describing the underlying ontology. We present
the knowledge base. Finally, we describe the application
developed to support the annotation of ectopic preg-
nancy ultrasound images. The overview of the ontology
and knowledge base development is presented in Fig. 2.
Ontology development
To build the ectopic pregnancy ontology (EPO), we ac-
quired concepts from a medical corpus. We also reused
concepts from existing terminologies. We organized
these concepts into hierarchies.
Acquiring concepts from text
We extracted terms from a medical corpus and orga-
nized them into concepts.
Extracting terms from a medical corpus
In order to cover the terms for the features to be anno-
tated on EP images (i.e., types of ectopic pregnancy image,
imaging signs, anatomical locations and technical ele-
ments for ultrasound image acquisition), we used NaturalLexicon (RadLex, version 3.13.1), with a full expansion of the concepts
Fig. 2 Overview of the design of the knowledge system for ectopic pregnancy: ontology design, reference image collection and application for
image annotation
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 3 of 13Language Processing (NLP) techniques [16] to extract and
select medical terms from a collection of medical texts
from two sources, namely the medical literature and de-
identified reports of ultrasound examinations. More spe-
cifically, we searched PubMed for all medical publications
indexed with the MeSH term "Pregnancy, Ectopic" from
January 2000 to December 2014 for which an abstract wasavailable, resulting in a collection of 2795 abstracts. Add-
itionally, we extracted 4260 de-identified ultrasound re-
ports form the Early Pregnancy Clinic database at the
University College London Hospital (UCLH), restricted to
ectopic pregnancy cases from October 2006 to April 2014.
The lexico-syntactic analysis of these texts was performed
using the part-of-speech tagger TreeTagger [17] and the
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 4 of 13term extractor YaTeA (http://search.cpan.org/dist/Lingua-
YaTeA/). A total of 40,237 single/multi-word candidate
terms were extracted.
Organizing extracted terms into concepts
The appropriate vocabulary for ectopic pregnancy was
developed from these candidate terms, using the plat-
form for ontology development from text Terminae/
DAFOE [16]. Two experts reviewed and selected candi-
date terms, and defined the relevant concepts for ectopic
pregnancy image description. The experts followed gen-
eral principles for ontology design (clarity, coherence,
extensibility, minimal encoding bias, minimal ontological
commitment) [1821].
Acquiring concepts from existing terminologies
Whenever possible, the experts reused elements from
existing terminologies, following previously described
methods [22, 23]. For example, fine-grained concepts for
the description of the pelvic anatomy in the FMA (e.g.,
the uterus [FMAID:17558] and all its parts) were added
to the ontology.
Organizing concepts into hierarchies
We organized the resulting concepts into a subsumption
hierarchy and we added annotations and logical defini-
tions to these concepts.
Organizing concepts into a subsumption hierarchy
We used a core ontology for the medical domain
developed in our academic center (ontoMénélas) to sup-
port the interoperability with other resources in our
organization [2429]. The subsumption hierarchy (i.e., is-
a or subClassOf relations) was developed in a top-down
approach [19, 30] leveraging expert knowledge in medical
imaging and OB/GYN, and by reusing existing resources.
In particular, we reused some of the subsumption relations
from the FMA (among the FMA concepts that were
added to the ontology) as previously described by the
RadLex group [31].
Annotations
All concepts for ultrasound signs of ectopic pregnancy
were manually annotated. The minimal set of annota-
tions included (i) one English label (ii) one textual defin-
ition in English (iii) one PubMed identifier (PMID) for
the concepts extracted from the PubMed corpus. Other
annotations were optional (e.g., synonyms and French
version of the annotations). The mappings of anatomical
concepts to FMA concepts were stored as annotations
in the ontology. The FMA labels and definitions for
these concepts were also added as annotations. All text-
ual annotations were based on SKOS predicates (prefLa-
bel, altLabel, definition) [32]. We used the biomedicalontology editor Protégé version 5 (http://protege.stanfor-
d.edu/) for editing the annotations.
Logical definitions
General concepts for categories of signs were defined in
intension as opposed to extension. These concepts corres-
pond to defined classes in the ontology. For example, the
concept color Doppler sign denotes an imaging sign, vis-
ible during an ultrasound examination, using the color
Doppler mode. Therefore, this concept is formalized with
a logical definition leveraging the property requiresMode
and the concept color Doppler mode. As a result, signs
whose definition contains requiresMode some color Dop-
pler mode would automatically be classified as subclasses
of color Doppler sign.
Implementation in OWL
The ontology was represented using the Web Ontology
Language, OWL [33]. The hierarchy was inferred with an
OWL-DL reasoner (Hermit 1.3.8), which also checked the
consistency of the ontology.
Knowledge base development
The ontology provides the vocabulary for describing ultra-
sound images of ectopic pregnancy, which we used for de-
veloping a sign-centric knowledge base to represent the
relations of each sign to ectopic pregnancy types, anatom-
ical structures and technical elements for the acquisition
of ultrasound images. Technical elements include the
examination route, the examination mode, and the
echographic view. For example, the ring of fire sign
concept is represented in Fig. 3 with its relations to a type
of ectopic pregnancy (tubal pregnancy through the rela-
tion epo:suggests), to anatomical structures (ampulla,
tubal isthmus, frimbrial portion through the relation
epo:hasLocation) and to technical elements (vaginal
route, color Doppler mode (2D), adnexal area view
through the relations epo:requiresRoute, epo:requires-
Mode, epo:requiresView, respectively).
This knowledge was asserted at the most general level
and propagated through the subsumption hierarchies of
the ontology. For example, although the concept ring of
fire sign is not explicitly linked to tubal pregnancy sign
in the knowledge base, this relation can be inferred from
ring of fire sign epo:suggests tubal pregnancy and tubal
pregnancy sign owl:equivalentClass (epo:suggests some
tubal pregnancy).
We assessed the domain and scope of the knowledge
base using competency questions, the answers to which
must be represented with relations (asserted or inferred)
from the knowledge base [34]. Such questions included
what are the different implantation sites of ectopic preg-
nancies?, what are the imaging signs of cesarean section
scar pregnancy?, which ultrasound mode is required to
Fig. 3 Simplified representation of the sign "ring of fire" in the knowledge base
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 5 of 13depict a ring-of-fire sign?, and what are the anatomical
structures visible in an adnexal area view?
Beside the development of the sign-centric knowledge
base, we selected from the medical literature ultrasound
images of ectopic pregnancies illustrating the signs rep-
resented in the knowledge base. We restricted the 2795
PubMed citations used for the text corpus to articles in
English, indexed with the MeSH term Ultrasonography
and for which the article was freely available. One of the
authors (PM) selected relevant images from the articles,
in which the ultrasound signs were precisely described
and illustrated. He annotated the signs in the knowledge
base with the PMID of the article. For example, the con-
cept ring of fire sign is annotated with PMID 18936028
in reference to an article describing this sign [35].
Application development
We developed an application for the annotation of ultra-
sound images of ectopic pregnancy. This application le-
verages both definitional knowledge from the ontology
and assertional knowledge from the knowledge base.
The main features of this application include:
i) searching for image annotations using terms from
the ontology,
ii) suggesting relevant signs based on the knowledge
base, and
iii) accessing reference images for a given sign.
The user interface was developed as a Java 7 web ap-
plication based on open-source elements. The ontologyand the knowledge base were stored in an RDF triple
store (Apache Jena 3.0). We used queries against a
SPARQL endpoint (Apache Fuseki) to access the know-
ledge base. Simple subsumption reasoning was sufficient
to access all the asserted and inferred knowledge from
the knowledge base. We established a set of SPARQL
rules to suggest the signs, anatomical structures and
technical elements associated with a given type of ec-
topic pregnancy selected by the user. For example, the
following SPARQL query retrieves all ectopic pregnancy
types having at least one sign from a given set of signs.
${selectedSigns} is a variable containing the URIs of
this set of signs, ${inferredGraph} is the inferred ontol-
ogy graph in the triplestore and "${language}" is the lan-
guage used for label display in the system:
PREFIX owl:<http://www.w3.org/2002/07/
owl#>
PREFIX rdf:<http://www.w3.org/1999/02/
22-rdf-syntax-ns#>
PREFIX epo:<http://www.semanticweb.org/
ontologies/epo.owl#>
PREFIX skos:<http://www.w3.org/2004/02/
skos/core#>
PREFIX rdfs:<http://www.w3.org/2000/01/
rdf-schema#>
SELECT DISTINCT ?disorder
?disorder_label ?disorder_definition
FROM ${inferredGraph}
WHERE {
VALUES ?sign {${selectedSigns}}
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 6 of 13?sign skos:hiddenLabel ?sign_id;
rdfs:subClassOf* epo:OPPIO_0000189 .
?disorder skos:prefLabel
?disorder_label;
rdfs:subClassOf epo:OPPIO_c000016 .
OPTIONAL {?disorder skos:definition
?disorder_definition .}
?sign rdfs:subClassOf* ?restr .
?restr owl:onProperty epo:suggests .
?restr owl:someValuesFrom/
rdfs:subClassOf* ?disorder .
FILTER(lang(?disorder_label)
= "${language}")
}
ORDER BY ?disorder_label
The result from this query is a list of ectopic preg-
nancy types (URI, label and definition) and can be used
in subsequent queries to suggest new signs associated
with these ectopic pregnancy types.Evaluation
We conducted an evaluation of the ontology, the know-
ledge base and the application. The ontology and the know-
ledge base were evaluated through a questionnaire and
users evaluated the application based on clinical cases.Evaluation of the ontology: Does the ontology contain
the appropriate vocabulary for ectopic pregnancy
ultrasound imaging?
The vocabulary provided by the ontology was presented
to a group of potential users with different levels of ex-
pertise. After a demonstration of the application followed
by a brief hands-on session to search for terms, we col-
lected feedback from each user by anonymous question-
naire. Questions assessed whether the terms for signs,
anatomical structures, types of ectopic pregnancy and
technical elements were consistent with their clinical prac-
tice and if they were able to find the signs they were look-
ing for in the application.Evaluation of the knowledge base: Are the suggested
signs and images useful?
The signs and images suggested by the knowledge base
were assessed by the same panel of users through an-
other questionnaire. We asked users if they learned new
signs for some types of ectopic pregnancy and whether
the reference images provided were helpful for analyzing
ultrasound images. Here we distinguished between jun-
ior and senior users, because our intuition was that the
juniors are more likely than seasoned physicians to learn
from our system.Evaluation of the application based on clinical cases
Using our application, users annotated ultrasound images
of ectopic pregnancy scans. This study was approved by
the ethic committee of the French National College of
Obstetrics and Gynecology (No CEROG 2015-GYN-
1002). The ultrasound scans (reports and images) were
randomly selected from ectopic pregnancy cases managed
at the Pyramids Medical Imaging Center in Paris and the
Early Pregnancy Unit at UCLH. All personally identifying
information was removed from the text of the reports,
from the content of the images and from the image meta-
data. Each observer was assigned a subset of 10 cases for
analysis, of which 5 were common to all observers and 5
were specific. For each case, the observers were asked to
annotate the images with the application. They were blind
to the content of the ultrasound report.
Our motivation for this preliminary evaluation was not
so much to assess whether all relevant signs had been an-
notated, but rather to ensure that the signs suggested by
our application were appropriate. In other words, we focus
on precision, not recall. Additionally, we evaluated the re-
producibility of the annotations among the observers.
Precision
The gold standard for the presence of signs on each
image was derived from the ultrasound reports provided
by the specialist centers. We measured the precision of
sign annotations provided by the observers (observed
signs) against the signs from the gold standard (relevant
signs). We used the usual definition for precision in in-
formation retrieval [36]:
precision ¼ relevant signsf g? observed signsf gj jj observed signsf gj
Reproducibility
The measure for assessing the reproducibility of the anno-
tations was the proportion of agreement for categorical as-
sessment across multiple observers [37]. The proportion
of agreement pa for a given sign in a given image was the
ratio of the number of agreements between the observers
(i.e., the number of pairs of observers who agree) for the
presence of the sign, over the number n of trials of agree-
ment (i.e., the total number of pairs of observers). For ex-
ample, considering a group of x = 6 observers, of whom 5
observers annotated one of the images with a given sign,
the number of trials of agreement is n = 1 + 2 +? + (x ?
1) = 15, and the number of agreements among the 5 ob-
servers is 10. Thus, pa is 10/15 = 66%. The 95% confidence
interval (CI) for pa was calculated from the Standard Error
of the proportion: SE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffipa 1?pað Þ=n
p
. Considering a
standard normal distribution for pa, the 95% CI is pa ±
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 7 of 131.96 × SE. Statistical computations were performed using
R version 3.2 and STATA version 14.Results
Ectopic pregnancy ontology
As of June 2016, the ectopic pregnancy ontology (version
1.1) contains 1388 concepts to describe ectopic pregnancy
ultrasound images, organized into several subsumption
hierarchies for types of ectopic pregnancies and the signs,
anatomical structures and technical elements of imaging
associated with ectopic pregnancy. The usual metrics for
ontology description are presented in Table 1. There are
24 classes for the types of ectopic pregnancy, as illustrated
in Fig. 4. The 90 concepts for ultrasound signs include
endometrial trilaminar pattern, tubal ring sign, and
ring of fire sign. While most sign concepts are repre-
sented as primitive classes, some of them are defined clas-
ses. For example, the concept color Doppler sign is a
defined class equivalent to [rdfs:subClassOf imaging sign
and epo:requiresMode some color Doppler mode]. In
general, we created defined classes for the categories of
signs by technical element (e.g., by the examination mode
(e.g., 2D ultrasound sign, color Doppler sign) and by
implantation site of ectopics (e.g., tubal pregnancy sign,
c-section scar pregnancy sign).
There are 484 concepts for anatomical structures of
the female pelvic anatomy (e.g., uterus, uterine tube,
zone of uterine tube and ampulla) and early gesta-
tional structures (e.g., gestational sac, trophoblast).
General anatomical concepts from the FMA were used
to seed the hierarchy (e.g., organ zone and non gesta-
tional anatomical structure). Specialized concepts (e.g.,
gestational sac) were added to extend the FMA hier-
archy as necessary for our application.
The technical element concepts were organized into
three hierarchies for examination route, examination
mode and echographic view. There are 3 examination
route subclasses (e.g., vaginal route), 9 examination modeTable 1 Ectopic Pregnancy Ontology (v1.1) metrics
Class count 1399
Object property count 44
Individual count 0
SubClassOf axioms count 2707
EquivalentClasses axioms count 50
DisjointClasses axioms count 39
AnnotationAssertion axioms count
- skos:prefLabel 1749
- skos:altLabel 298
- skos:definition 489
- epo:FMAID (FMA class UI) 295subclasses (e.g., color Doppler mode), and 17 echographic
view subclasses (e.g., longitudinal view of the uterus).
The asserted subsumption hierarchy of the ontology
involved 2707 relations. The domain and range of 44 re-
lations (e.g., hasLocation, suggests, requiresMode)
are defined in the ontology. Finally, this ontology in-
cludes no individuals, because instances of signs are the
actual signs observed on images from a clinical case.Knowledge base for image annotation
In the knowledge base, the 81 signs defined in the ontol-
ogy are related to ectopic pregnancy types, anatomical
structures and the three categories of technical elements
(the echographic view, the examination mode and the
examination route) as illustrated in Fig. 3. There are 169
asserted relations between these signs and the different
types of ectopic pregnancy, as some signs can be associ-
ated with several types of ectopic pregnancy. Similarly, the
signs can be related to multiple anatomical structures
(with 239 asserted relations), as well as multiple technical
elements (with 356 asserted relations). The asserted know-
ledge from the sign-centric knowledge base characteristics
is summarized in Table 2. After inference in the know-
ledge base, 618 inferred relations between signs and types
of ectopic pregnancy were produced, as well as 1503 in-
ferred relations between signs and technical elements.
The signs in the knowledge base are associated with
reference images and PubMed citations. One hundred
and six articles from 33 medical journals were
reviewed for establishing the collection of reference
images, resulting in the selection of 80 images depict-
ing relevant ultrasound signs. A total of 77 PMID an-
notations and 98 image annotations illustrate the signs
in the knowledge base.Application for ultrasound image annotations
An overview of the user interface of the application is
presented in Fig. 5. The image to annotate is displayed
in the top left corner of the screen. The annotation
search field at the bottom of the screen supports auto-
completion for terms related to ectopic pregnancy types,
anatomical locations, technical elements and ultrasound
signs. The results are displayed in a sliding panel and
the user can select the relevant terms, which are then
added as image annotations in the top right corner of
the screen as image annotation. As the user selects an-
notations, the system provides a selection of reference
images from the collection in the bottom left corner to
illustrate the selected annotations. Finally, in the bottom
right corner, the system suggests other signs of interest
based on the type of pregnancy, anatomical structure
and technical elements selected.
Fig. 4 Taxonomy of ectopic pregnancy by implantation sites (view of the Ectopic Pregnancy Ontology)
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 8 of 13Evaluation
Evaluation of the ontology: Does the ontology contain the
appropriate vocabulary for ectopic pregnancy ultrasound
imaging?
A total of 17 users (junior and senior OB/GYN practi-
tioners and radiologists, and sonographers from France
and the UK) were presented with the application. Their
feedback on the terms available in the ontology was gen-
erally favorable. More specifically, 100% of the users
found the vocabulary for the ectopic pregnancy signs to
be consistent with their clinical practice, 94,1% for the
anatomical structures and 82,4% for the terms describing
technical elements of imaging. Moreover, 82.4% were
able to find the signs they were looking for in the appli-
cation, without further assistance.
Evaluation of the knowledge base: are the suggested
signs and images useful?
Overall, half of the users (52.9%), including all five junior
users, learned about new signs associated with ectopic
pregnancy types. The reference images suggested by theTable 2 Characteristics of the ectopic pregnancy knowledge
base for imaging signs
Object property Axioms (n)
Relations
- Ultrasound Sign ® Ectopic Pregnancy <epo:suggests> 169
- Ultrasound Sign ® Anatomical Structure <epo:hasLocation> 239
- Ultrasound Sign ® Technical Element <epo:requires> 356
Annotations
- PubMed citations <epo:PMID> 77
- Image from reference collection <epo:ImagePath> 98application were always of often useful for 14 users
(82.4%). One user considered that the suggested images
were sometimes useful and two users considered the
suggested images rarely useful. As expected, the useful-
ness of the application depended on the expertise of the
user, with junior users benefitting most.Evaluation of the application based on clinical cases
Six independent observers, all OB/GYN practitioners
with different level of training in ultrasound imaging
(three seniors, two senior registrars and one registrar)
annotated 206 ultrasound images from 35 clinical cases
of ectopic pregnancy (five common cases and five add-
itional cases for each user). The cases are presented in
Table 3. The observers provided 1486 annotations with
an overall precision of 0.83. The precision for each sign
is presented in Fig. 6.
For the five common cases, the observers used 46 dis-
tinct signs to create 841 annotations. For 783 annota-
tions (covering 26 distinct signs), the annotation was
made by at least two observers. The 58 remaining anno-
tations (6.9%) were created by only one of the six ob-
servers and involved 20 distinct signs. The total
proportion of agreement for the presence of signs in im-
ages was 40.35% [38.64%-42.05%]95%CI. The reproduci-
bility for each sign annotation is presented in Table 4.Discussion
We have developed an application ontology, a know-
ledge base and an application for the annotation of
ultrasound images of ectopic pregnancy. This was the
first attempt to build semantic resources in this domain.
We discuss the significance of our findings, as well as
Fig. 5 Overview of the interface of the web application for ectopic pregnancy ultrasound image annotations. This graphical user interface was
developed using the AngularJS (https://github.com/angular/angular.js) and Bootstrap (https://github.com/twbs/bootstrap)
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 9 of 13the limitations and perspectives of this resource from
the perspective of clinical application development.
Significance
Using Sematic Web technologies [38] and ontologies
[18], we successfully developed a comprehensive, unam-
biguous, shared and computable representation of the
ectopic pregnancy ultrasound signs, for which existing
resources were insufficient.
The ontology and the knowledge base received positive
feedback from a panel of medical users (including mixed
medical staff and sonographers). This preliminary evalu-
ation demonstrates that they were able to identify mor-
phological ultrasound features for a particular diagnosis
and to associate them with pre-defined terms. The use
of a large and diverse corpus as our source of vocabulary
was critical for reaching a shared and fine-grained repre-
sentation of the domain [20]. As expected, the signs de-
scribed in the ontology are consistent with the mostTable 3 Types of ectopic pregnancy among the 35 ultrasound
cases used for the annotation evaluation
Type of ectopic pregnancy Cases in common Other cases
Tubal pregnancy 3 (20 images) 12 (67 images)
Cesarean section scar pregnancy 2 (13 images) 12 (74 images)
Cervical pregnancy - 4 (24 images)
Interstitial pregnancy - 2 (8 images)
Total 5 (33 images) 30 (173 images)important signs for tubal pregnancy diagnosis identified
in the recent meta-analysis by Richardson et al. [39].
The relevance of this application ontology is illustrated
by a high precision rate of 83%, which reflects the pro-
portion of correct sign annotations made by the ob-
servers. This result is especially encouraging at a time
when we are considering using this knowledge base in a
clinical decision support system.
The global proportion of agreement was 40.35%, which
is satisfactory considering the number of images (33) and
signs (26) involved. In comparison, a proportion of agree-
ment of 50% was reported for the binary assessment of
the abnormality of fetal heart rate in 20 cardiotocograms
by 5 observers [37]. Interestingly, some signs with moder-
ate proportions of agreement (e.g., tubal ring sign, pa =
27.3% [20.5-34.1]95%CI), had good precision rates (e.g., pre-
cision = .77 for tubal ring sign). Moreover, in these im-
ages, a more general sign with a higher agreement was
present (e.g., adnexal mass distinct from ovary, pa =
79.4% [73.5-85.3]95%CI). This was an expected effect of
sign suggestions in the application.
Limitations
Limitations of the ontology
Many biomedical ontologies developed recently have used
the basic formal ontology (BFO) [40] as their top-level
ontology for interoperability with other OBO ontologies.
Instead, we used our local core ontology for medicine
(Ménélas), because interoperability with other projects in
Fig. 6 Precision of ectopic pregnancy sign annotations in ultrasound images
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 10 of 13our institution was more important. Moreover, the use of a
top-level ontology was not a primary requirement in the
design of our application ontology. Similarly, we did not
use the popular MIREOT [41] strategy for referencing ex-
ternal resources in our ectopic pregnancy ontology. Be-
cause it was crucial for this application ontology to ensure
the stability of our application, we decided to restrict to a
minimum the ontological commitment that comes with the
reuse of external, evolving ontologies. However, we kept the
mapping to reference resources, such as the FMA.
Preliminary evaluation
In its current state, the application we developed only
supports the annotation of clinical images, not the diag-
nosis of the conditions represented on these images. For
our evaluation, most of the signs from the ontology used
in annotations were tubal pregnancy signs, cesarean-
section scar pregnancy signs and some signs that were
not specific of a location. While sufficient for evaluatingthe precision and reproducibility of the annotations, this
skewed dataset would be insufficient for the evaluation
of a diagnostic system.
Toward a clinical decision support system (CDSS) for
ectopic pregnancy diagnosis
There is a need for CDSS in the domain of ultrasound signs
for early pregnancy. Except in specialist centers, many
women with ectopic pregnancy will not be diagnosed by
transvaginal ultrasound at their first visit. However,
adequate management necessitates detailed ultrasound dif-
ferential diagnosis of the different early pregnancy compli-
cations [4, 42] which requires advanced training [43]. In
practice, only some of the initial transvaginal scans are per-
formed by experts, thus delaying the appropriate diagnosis
and treatment, increasing adverse outcomes and also gener-
ating a significant number of visits [8, 44, 45]. In this con-
text, a CDSS for early identification of relevant ectopic
pregnancy signs will likely benefit non-expert operators.
Table 4 Proportion of agreement on the presence of ultrasound signs in the 5 common cases of ectopic pregnancy
Sign annotations form the Ectopic Pregnancy Ontology Agreement (n) Lack of
agreement (n)
Images (n) Proportion of agreement
(% and [95% CI])
Endometrial trilaminar pattern 48 12 4 80.00 [69.88 - 90.12]
Adnexal mass distinct from ovary 143 37 12 79.44 [73.54 - 85.34]
Embryo visible outside the uterine cavity 45 15 4 75.00 [64.04 - 85.96]
Gestational sac outside the uterine cavity 226 104 22 68.48 [63.47 - 73.49]
Adnexal mass adjacent to ovary 122 58 12 67.78 [60.95 - 74.61]
Gestational sac or trophoblast in a myometrial defect in previous caesarean
section scar pregnancy site
119 76 13 61.03 [54.18 - 67.88]
Adnexal mass and corpus luteum at the same side 82 53 9 60.74 [52.50 - 68.98]
Adnexal rounded hyperechoic mass 63 57 8 52.50 [43.57 - 61.43]
Ring of fire sign 39 36 5 52.00 [40.69 - 63.31]
Yolk sac visible outside the uterine cavity 64 71 9 47.41 [38.99 - 55.83]
Adnexal mass as gestational sac with yolk sac 22 38 4 36.67 [24.48 - 48.86]
Caesarean section scar pregnancy peritrophoblastic blood flow 19 41 4 31.67 [19.90 - 43.44]
Intact endometrial midline echo 14 31 3 31.11 [17.58 - 44.64]
Tubal ring sign 45 120 11 27.27 [20.47 - 34.07]
Tubal ring without central identifying feature 24 66 6 26.67 [17.53 - 35.81]
Trophoblast visible outside the uterine cavity 110 325 29 25.29 [21.21 - 29.37]
Anterior distortion of uterus serosa 24 96 8 20.00 [12.84 - 27.16]
Smaller trophoblastic border distance to the anterior uterine serosa 33 162 13 16.92 [11.66 - 22.18]
Ectopic pregnancy wall more echogenic than corpus luteum wall 21 114 9 15.56 [9.45 - 21.67]
Fluid collection located centrally within the uterine cavity 4 41 2 13.33 [1.17 - 25.50]
Non intact endometrial midline echo 4 41 3 8.89 [0.57 - 17.21]
Gestational sac or trophoblast located at the level of internal cervical os 10 125 9 7.41 [2.99 - 11.83]
Gestational sac inside anterior myometrium and uterine cavity 1 44 3 2.22 [0.00 - 6.52]
Gestational sac located eccentricaly from uterine cavity 1 149 10 0.67 [0.00 - 1.98]
Total 40.35 [38.64 - 42.05]
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 11 of 13However, developing a CDSS for early pregnancy is challen-
ging for several reasons. Potential users have heterogeneous
expertise; there is no standard terminology describing the
relevant ultrasound signs; and the quality of ultrasound im-
ages varies significantly among operators.
We consider this ectopic pregnancy image annota-
tion application, with its underlying ontology and
knowledge base, a step toward a clinical decision sys-
tem for ectopic pregnancy diagnosis. Research in
CDSS based on ontologies has demonstrated differen-
tial diagnosis assistance in Human Genetics [46] or in
conventional Radiology [47].
The precision of the annotations derived from our
knowledge base is promising for developing a CDSS
for ectopic pregnancy ultrasound. The prospective
evaluation of a clinical decision support system
(CDSS) based on our knowledge base should demon-
strate improvement in clinical care. For example, the
expectation would be that, junior operators guided by
the signs suggested by the system achieve a betteranalysis of ultrasound images, and therefore reach the
correct diagnosis more often than without the system.
A specific challenge for such clinical evaluation is its
integration in the clinical workflow.
Finally, the knowledge base we developed could be
extended from ectopic pregnancy to early pregnancy
(i.e., including molar pregnancy, miscarriage and mul-
tiple pregnancy at early stages of development), and
more generally to the next stages of fetal develop-
ment (i.e., to represent ultrasound signs associated
with fetal disorders).Conclusions
We have developed a new ectopic pregnancy know-
ledge base for the annotation of ultrasound images.
The elements of this knowledge base (signs and types
of ectopic pregnancy, anatomical structures involved
and technical elements of imaging) are organized into
an ontology. We have demonstrated the use of this
Dhombres et al. Journal of Biomedical Semantics  (2017) 8:4 Page 12 of 13knowledge base for the annotation of ultrasound im-
ages of ectopic pregnancy, with promising results
from the perspective of clinical decision support sys-
tem development. Other gynecological disorders and
fetal abnormalities may benefit from our approach.
Abbreviations
CDSS: Clinical decision support system; FMA: Foundational model of
anatomy; NCBO: National center for biomedical ontology; NLP: Natural
language processing; OBO: Open biomedical ontologies; OWL: Ontology
web language; PMID: PubMed identification number; RadLex: Radiology
lexicon; SKOS: Simple knowledge organization system; UCLH: University
College London Hospital; UMLS: Unified Medical Language System
Acknowledgements
This work was conducted using the Protégé resource, which is supported by
grant GM10331601 from the National Institute of General Medical Sciences of the
United States National Institutes of Health. (website: http://protege.stanford.edu)
The lexicalization plugin Archonte 4.2 for Protégé 5, developed by Laurent
Mazuel and supported by INSERM U1142 LIMICS, was used to edit the SKOS
multilingual annotations in the ontology. (website: http://github.com/ics-
upmc/archonte)
The authors would like to thank Olivier Bodenreider for his expert advice and
support throughout the final editing of this manuscript.
Funding
The development of the web-application was founded by the SATT-Lutech
for the Pierre and Marie Curie University, Paris, France.
Availability of data and material
The ontology for ectopic pregnancy imaging is available on BioPortal:
http://bioportal.bioontology.org/ontologies/EPO
Authors contributions
All the authors were involved in the drafting or the revising at different stages
of the manuscript. FD and PM developed the ontology and knowledge base in
collaboration with JC, JMJ, EJ and DJ. FD lead the development of the web
application and the knowledge base integration. FD, PM, JMJ, and EJ
established the evaluation protocols (questionnaires and clinical cases). PM
presented the application and collected the questionnaires. DJ and NP, ectopic
pregnancy experts, provided the de-identified clinical material (text, images).
FD, NL, and BK did the statistical analysis. SF, LG tested the application at every
stage of development. All authors read and approved the final manuscript.
Competing interests
The authors declared that they have no competing interest.
Consent for publication
Not applicable.
Ethics approval and consent to participate
This study was approved by the institutional review board of the French
College of Obstetrics and Gynecology (No CEROG 2015-GYN-1002) for the
use of de-identified human data.
Author details
1UPMC Medical Faculty (Paris 6), Department of Fetal Medicine in Armand
Trousseau Hospital (APHP), INSERM U1142 (LIMICS), 26 Avenue du Dr Arnold
Netter, 75012 Paris, UE, France. 2INSERM U1153 (Obstetrical, Perinatal and
Pediatric Epidemiology Research Team, Center for Biostatistics and
Epidemiology), Maternité Port Royal, 53 Avenue de lObservatoire, 75014
Paris, UE, France. 3APHP DSI, INSERM U1142 (LIMICS), 15, rue de lÉcole de
Médecine, 75006 Paris, UE, France. 4Pyramides Medical Imaging Center, 13
av. de lOpéra, 75001 Paris, UE, France. 5University College Hospital (UCLH)
Department of Obstetrics and Gynaecology, Academic Department of
Obstetrics and Gynaecology, University College London (UCL) Institute for
Womens Health, 86-96 Chenies Mews, London WC1E 6HX, UE, UK.
6Department of Obstetrics and Gynaecology, Gynaecology Diagnostic and
Outpatient Treatment Unit, University College Hospital (UCLH), 235 Euston
RESEARCH Open Access
An ontological analysis of medical Bayesian
indicators of performance
Adrien Barton1,5*, Jean-François Ethier1,4,5, Régis Duvauferrier2,3 and Anita Burgun4
Abstract
Background: Biomedical ontologies aim at providing the most exhaustive and rigorous representation of reality as
described by biomedical sciences. A large part of medical reasoning deals with diagnosis and is essentially
probabilistic. It would be an asset for biomedical ontologies to be able to support such a probabilistic reasoning
and formalize Bayesian indicators of performance: sensitivity, specificity, positive predictive value and negative
predictive value. In doing so, one has to consider that not only the positive and negative predictive values, but also
sensitivity and specificity depend upon the group under consideration: this is the spectrum effect.
Methods: The sensitivity value of an index test IT for a disease M in a group g is identified with the proportion of
people in g who have M who would get a positive result to IT if the test IT was realized on them. This value can be
estimated by selecting a reference test RT for M and a sample s of g, and measuring the proportion, among
members of s having a positive result to RT, of those who got a positive result to IT. Similar approximation
strategies hold for prevalence, specificity, PPV and NPV. Indicators of diagnostic performances and their estimations
are formalized in the context of the OBO Foundry, built on the realist upper ontology Basic Formal Ontology (BFO).
Results: Entities and relations from the Ontology for Biomedical investigations (OBI) and the Information Artifact
Ontology (IAO) are used and complemented to represent reference tests and index tests, tests executions, tests
results and the relations involving those entities, as well as the values of indicators of performance and their
estimates. The computations taking as input several estimates of an indicator of performance to produce a finer
estimate are also represented. The value of e.g. sensitivity estimates should be dissociated from the real sensitivity
value  which involves possible, non-actual conditions, namely the result a person would get if a medical test
would be performed on her. Such conditions could not be directly represented in a realist ontology, but a
representation is proposed that introduces only actual entities by considering a disposition whose probability value
is the real sensitivity value. A sensitivity estimate is a data item which is about such a disposition.
Conclusions: This model provides theoretical basis for the representation of entities supporting Bayesian reasoning
in ontologies.
Keywords: Sensitivity, Specificity, Medical test, Spectrum effect, Disposition, Realist ontology, Informational entity
Background
Definition of indicators of performance
Biomedical ontologies aim at providing the most exhaust-
ive and rigorous representation of reality as described by
biomedical sciences. A large part of medical reasoning
deals with diagnosis and is essentially probabilistic. It
would be an asset for biomedical ontologies to be able to
support such a probabilistic reasoning.
Ledley and Lusteds seminal article [1] on Bayesian rea-
soning in medicine defines different kinds of probabilistic
entities. Consider for example the simple case of an in-
stance of test of type IT (for index test  a test whose ac-
curacy is being measured) aiming at detecting if a patient in
a group g has an instance of disease of type M.1 The per-
formance of test IT in diagnosing M can be quantified by
the positive predictive value of this test, hereafter abbrevi-
ated PPV, defined by the Oxford Handbook of Medical
* Correspondence: adrien.barton@gmail.com
Equal contributors
1Département de médecine, Université de Sherbrooke, Sherbrooke, Québec, Canada
5Centre de recherche du CHUS, CIUSSS de lEstrie-CHUS, Sherbrooke,
Québec, Canada
Full list of author information is available at the end of the article
© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 
DOI 10.1186/s13326-016-0099-4
Statistics [2] as the proportion of tested positives who are
true positives and by the negative predictive value, here-
after abbreviated NPV, defined as the proportion of tested
negatives who are true negatives. These values provide the
probability that a patient has or not the disease, depending
upon the result (positive or negative) to the test.
However, such values depend on some characteristics
of the patient. If a patient received a positive test, the
probability that he has the disease can for example depend
upon his sex, his status of smoker or non-smoker, and
other biological or environmental parameters. In particular,
it depends on the prevalence of the disease among the
group of persons with those characteristics.
Therefore, the statistical data communicated in the
medical literature for a test are generally not the positive
and negative predictive values, but the so-called sen-
sitivity and specificity. The Oxford Handbook of
Medical Statistics defines sensitivity as the propor-
tion of those who have the disease who are correctly
identified by the test as positive ([2], p. 340) and spe-
cificity as the proportion of those who do not have
the disease who are correctly identified by the test as
negative. The PPV and NPV can be computed on the
basis of the prevalence Prev, sensitivity Se and specifi-
city Sp thanks to the following Bayesian equations:
PPV ¼ Prev:Se
Prev:Seþ 1?Prevð Þ 1?Spð Þ
NPV ¼ 1?Prevð Þ:Sp
Prev: 1?Seð Þ þ 1?Prevð Þ:Sp
In the remainder of the article, sensitivity, specificity,
PPV and NPV will be called (Bayesian) indicators of
performance and abbreviated IPs.
In the wake of Ledley and Lusted [1] the sensitivity
and specificity values have often been considered as de-
pending only on the pathophysiological characteristics of
the disease and of the test, and thus as being independent
of the group of people under consideration. However, sen-
sitivity and specificity values do in fact depend upon the
group under consideration: this is the spectrum effect
[3].
The spectrum effect
If IT is an index test and M is a disease, lets introduce
f1(IT,M) as the proportion of individuals who get a posi-
tive result to IT, among individuals who have M, which
fits with the usual definition of sensitivity (as provided
by [2]). The main problem with this definition is that it
does not specify the reference population. "The individ-
uals who have M are part of which population: the
population in a given sample? The population of a spe-
cific country? The whole human population? Ledley and
Lusted [1] considered that sensitivity and specificity
depend upon pathophysiological characteristics of the
disease, but not upon the population in consideration. If
this was the case, the proportion of people tested posi-
tive among the diseased would be the same in any group
under consideration  abstracting from statistical fluctua-
tions due to randomness. However, as has been recog-
nized by the medical literature, but regularly omitted, this
hypothesis is false for at least two reasons. First, most tests
are not inherently dichotomous but rely on a
categorization of individuals based on continuous traits
[3]. Second, various populations can express various dis-
ease characteristics (such as various degrees of severity
[4]) that will influence the chance to get a positive result
to a test.
The latter can be illustrated with the following ex-
ample. Suppose that around 80 % of people having
rheumatoid arthritis have a rheumatoid factor (RF), and
would with certainty receive a positive result to a test
that would perfectly2 detect this factor; and that the
remaining 20 % do not have a rheumatoid factor, and
would receive a negative result (yet do have the disease).
The diseased population is then composed of two sub-
groups: a subgroup sg1 whose members would all get
for sure a positive result to IT, and a subgroup sg2
whose members would all get for sure a negative result
(see Fig. 1). The sensitivity calculated in this example
would be 80 %.
Nevertheless, in reality, those proportions vary
based upon various characteristics of the patients. For
example, RF presence increases with age at onset of
disease in juvenile arthritis [5]. As a result, the sensi-
tivity of a test for RF will increase according to the
age of the individuals of the population being tested.
Its sensitivity will be lower in younger patients and
higher in older patients.
Fig. 1 Variation of sensitivity depending on the group
under consideration
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 2 of 13
Therefore, f1 is not a well-defined function: the value
of the proportion does not depend only upon IT and M,
but also upon the population g under consideration
(which could be, for example, the whole human popula-
tion, the Canadian smoker population, the female popula-
tion, etc.). This is the spectrum effect, which can also be
manifested, for example, as a dependence of sensitivity
and specificity on the degree of severity of the disease in
the group under consideration [4].
The sensitivity can therefore depend on the group g
under consideration. A better candidate than f1(IT,M) to
the definition of the sensitivity value would be the func-
tion f2(g,IT,M) defined as the proportion
3 among
people in g who have M of those who would get a posi-
tive result to IT if the test IT was realized on them 
the mention in italic is necessary, as a test IT will not be
realized on all individuals who have M, but on a sample
only. The next part will distinguish three related entities:
the real sensitivity4 value, its estimates, and the measure-
ments of proportion in samples. It will also explain how
such entities should be distinguished in an ontology of IPs.
Methods
Proportion measurement in a sample
It is impossible to know f2(g,IT,M) with certainty in
practice, for two reasons. The first reason is that it is
often not possible to determine with certainty, through
reasonable means, whether a given person has the dis-
ease M or not; in some cases, the only way to be certain
would be to perform an autopsy on the deceased patient.
Therefore, one needs to use a reference test, which is
the best diagnostic test that is reasonable to perform in
the present context (for more on the distinction between
a reference test and the associated disease, see section
The challenge of representing indicators of performance
in an ontology below).
If the patient receives a positive result to this reference
test, it will be concluded that he has the disease; if he re-
ceives a negative result, it will be concluded that he does
not have it. But those inferences can be wrong: the refer-
ence test might lead to a positive result for a non-
diseased person, or a negative result for a diseased per-
son. If RT is a reference test for M and IT is an index
test (of unknown accuracy) for M, then one can define
the function f3(g,IT,RT) as the proportion, among indi-
viduals of g who would get a positive result to RT if the
test RT had been performed on them, of people who
would get a positive result to IT if the test IT was real-
ized on them. Since RT is a reference test for M,
f3(g,IT,RT) approximates f2(g,IT,M). Both values can dif-
fer though: this is a first epistemic limit to the know-
ledge of f2(g,IT,M).
On top of this, f3(g,IT,RT) is not directly measurable.
As a matter of fact, a test IT is never realized on a
population as large as e.g., the whole population of
smokers, or the whole male population. It is however
possible to approximate f3(g,IT,RT) by performing both
tests IT and RT on individuals in a sample s judged as
being representative of the population g. Lets define
f4(s,IT,RT) as the proportion, among members of s who
got a positive result to RT, of those who got a positive re-
sult to IT. If s is a representative sample of g, then
f4(s,IT,RT) does approximate f3(g,IT,RT)  and thus, by
transitivity, does approximate f2(g,IT,M). Note that as
long as the sample s is not perfectly representative of g,
f4(s,IT,RT) will differ at least slightly from f3(g,IT,RT)
(which also differs from f2(g,IT,M)): this is a second
limit to the knowledge of f2(g,IT,M).
Lets illustrate those two limits of estimations with a
study [4] which analyzes the quality of the Neer test
(here written IT) for diagnosing the shoulder impinge-
ment syndrome (written M), a syndrome that is charac-
terized by rotator cuff muscles inflammation near the
sub-acromial space. In this study, the Neer test IT is re-
alized on a sample (written s) of 552 patients, judged as
representative of the target population (g). Park et al.
[4] take as reference test (RT) the surgical observation.
Here, f4(s,IT,RT) is the proportion of people in the
sample who have received a positive result to the Neer
test, among those diagnosed as positive by surgical op-
eration. f4(s,IT,RT) approximates f3(g,IT,RT), namely
the proportion of individuals in the target population g
who would get a positive result to the Neer test among
those who would get a positive result by surgical observa-
tion, if those tests were performed on them. Finally,
f3(g,IT,RT) itself approximates f2(g,IT,M), which is the
proportion of individuals in g who would receive a posi-
tive Neer test result among those who have an im-
pingement syndrome. Thus, f4(s,IT,RT) approximates
f2(g,IT,M).
Note that similar approximation strategies hold for
prevalence, specificity, PPV and NPV. Concerning e.g.
specificity, one could thus define f 2(g,IT,M) as the pro-
portion5 among people in g who dont have M of those
who would get a negative result to IT if the test IT was
performed on them; and f 4(s,IT,RT) as the proportion,
among members of s who got a negative result to RT,
of those who got a negative result to IT. Thus,
f 4(s,IT,RT) approximates f 2(g,IT,M).
Sensitivity value and sensitivity estimates
Now that those definitions have been given, we can de-
termine which entity the word sensitivity refers to in
the medical literature. At first sight, this term might ap-
pear polysemic. To illustrate this, lets consider a study
which evaluates the quality of an exercise test in the
diagnosis of coronary artery disease, and claims: The
sensitivity varied substantially according to sex (women
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 3 of 13
30 % and men 64 %) [6]. On one hand, the statement
sensitivity varies substantially according to the sex sug-
gests that sensitivity depends on the target population g
in consideration, and that there is a sensitivity value for
the female population, and another one for the male
population. This formulation thus suggests that sensitivity
value is given by the function f2(g,IT,M). However, the
value 30 % assigned to the sensitivity of the test for women
refers to a proportion which has been measured by the au-
thors in a sample of 37 women, using coronary angiog-
raphy as a reference test. This might thus suggest that the
sensitivity value is in fact given by the function f4(s,IT,RT)
However, two arguments suggest that the sensitivity
value should be interpreted as f2(g,IT,M) rather than
f4(s,IT,RT). First, the value which is ultimately relevant
for medical practice is f2(g,IT,M): if s is a sample of g
and RT is a reference test for M, f4(s,IT,RT) is of interest
for the medical practitioner only insofar as it provides
an information on the diseaseM and the target population
g from which the sample is taken  that is, insofar as it
provides an estimate of f2(g,IT,M). Indeed, the fact that a
few people who got a positive result to RT in a given sam-
ple have got a positive or negative result to a test IT has
medical relevance only insofar as it teaches us something
about how diseased people in the target population (not
only in the sample) will react to this test IT.
Second, the sensitivity value is usually given with a
95 % confidence interval (see e.g., [7] or [8]), which esti-
mates the likely range of error in determining the sensitivity
value. But f4(s,IT,RT) can be measured with certainty,
6 and
thus the confidence interval cannot characterize the uncer-
tainty on our knowledge of f4. On the other hand, there is
some uncertainty on the knowledge of f2(g,IT,M) and
f3(g,IT,RT), as they are estimated on the basis of f4(s,IT,RT).
Therefore, the 95 % confidence interval would characterize
the uncertainty on the knowledge of f3(g,IT,RT), which is
taken as a proxy for f2(g,IT,M).
7
Thus, those two arguments suggest that the term sensi-
tivity should refer to f2(g,IT,M)  which is relative to a
disease and a target population  rather than to f4(s,IT,RT)
 which is relative to a reference test and a sample.8 As
for f4(s,IT,RT), it can be interpreted as the value of a meas-
urement of proportion in a sample, which provides an es-
timate of the sensitivity value.
Therefore, a sentence such as The sensitivity varied
substantially according to sex (women 30 % and men
64 %) should, more rigorously, be formulated as: The
sensitivity varies substantially depending on the sex:
through measurement of proportions in samples, its
value was estimated to be 30 % for the women, and 64 %
for the men. We could prefer the first formulation, more
compact, for practical reasons; but it is important to
remember that it is only a shortcut for the second
formulation.
Accordingly, we will need to dissociate three different
kinds of entities. First, tests execution on a sample s, re-
ferring more precisely to the process of performing tests
IT and RT and measuring the numbers of true positive,
false positive, true negative and false negative as opera-
tionalized by IT and RT - for example, the false positive
are people who are tested positive by the index test IT
but negative by the reference test RT in the sample s.
Second, the proportion of true positives among positives
(as given by the reference test) is relative to the index
test, the reference test and the sample, and its value is
given by the function f4(s,IT,RT); as such, it provides an
estimate of the sensitivity value. Third, the real sensitiv-
ity, which is relative to an index test, a disease and a
population g, and whose value f2(g,IT,M) is given by the
proportion of people in the group who would have a
positive result to the test IT among those who are dis-
eased. The real sensitivity would provide a better infor-
mation than a sensitivity estimate on the probability that
a random member of the group g would get a positive
test result, in case he has the disease. However, its value
f2(g,IT,M) cannot be known with certainty, contrarily to
the value of the sensitivity estimate f4(s,IT,RT).
More generally, those considerations can be adapted
to other indicators of performance (specificity, PPV and
NPV), as well as the prevalence. In particular, f 2(g,IT,M)
should refer to the real specificity value, whereas
f 4(s,IT,RT) can be interpreted as the value of a measured
proportion in a sample that provides an estimate of the
real specificity value. In particular, real sensitivity, speci-
ficity, PPV and NPV, as we have defined them above, de-
pend neither on the sample nor on the reference test.
However, they are estimated on the basis of proportion
measurements which depend both on the sample and
the reference test. Accordingly, when a study [9] men-
tions cadaveric prevalence of the rotator cuff tears, this
expression should be understood as a linguistical shortcut
denoting a proportion measurement in a sample when the
cadaverical analysis is adopted as reference test; and the
radiological prevalence should be understood as a pro-
portion measurement when the radiological analysis is
adopted as reference test. The real prevalence, how-
ever, does not depend on the reference test.
Aggregation of sensitivity estimates
Finally, we need to add a last layer to this model. Ap-
proximations of sensitivity taken in different samples,
with different index tests, can be combined in order to
build a finer estimate of sensitivity for a more encom-
passing category of index tests. Consider for example the
meta-analysis [7] which assess the quality of peripheral
thermometers in detecting fever. They use as reference
test a pulmonary artery catheter, and consider 29 studies
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 4 of 13
assessing the sensitivity and specificity of those devices.
Combining those values, they come up with an estimate
of 0.64 for the sensitivity and of 0.96 for the specificity.
The challenge of representing indicators of performance
in an ontology
To the extent that they aim at representing biomedical
knowledge and enabling medical reasoning, biomedical
ontologies should provide a formalization of IPs as well
as the prevalence, by dissociating e.g. the real sensitivity
from the sensitivity estimates, and the process leading to
those estimates. This article will introduce such a
formalization in the context of the OBO Foundry [10],
one of the most massive set of interoperable ontologies in
the biomedical domain, built on the upper ontology Basic
Formal Ontology (BFO) 1.1 [11].
BFO endorses a realist methodology, which carefully
dissociates material entities (such as disorders) from
informational entities (such as diagnosis). In common
medical practice, a disease may be diagnosed in ideal
circumstances by a given gold standard test, which can
be defined as the most accurate reference test; but the dis-
ease, the diagnosis, and the result to a gold standard test
are three different entities that should be distinguished. As
a matter of fact, many human diseases already existed a
few thousands of years ago, much before they could be di-
agnosed. Moreover, a diagnosis can be wrong or imprecise.
Finally, a given gold standard can be later replaced by a
better one: this shows that the disease cannot be defined
by a positive result to a gold standard - otherwise, there
could not be, by definition, a better gold standard. Thus,
while a diagnosis of a disease represents the best know-
ledge by some health or research professional of the pres-
ence of the disease in a particular patient, a diagnosis is
not equivalent to a disease: it is rather about a disease.
This formalization is compatible with IAO (Information
Artifact Ontology [16]) and OGMS (Ontology for General
Medical Sciences).
The question of how probabilistic notions can be rep-
resented in ontologies has been tackled from different
perspectives in the past. For example, [12] has proposed
the alternative PR-OWL format that extends the clas-
sical OWL format; we take here a different approach,
which does not aim at changing the OWL format. Solda-
tova and colleagues [13] have described a model in
which probabilities can be assigned to research state-
ments. We build here upon an alternative approach [14],
in which probabilities can be assigned to dispositions.
Sensitivity and specificity have been recently introduced
in the Ontology of Biological and Clinical Statistics (OBCS
[15]) as subclasses of Data item. We will partly endorse
and refine this classification, by considering estimates of
sensitivity and specificity as subclasses of Data Item, and
extend this classification to PPV and NPV. A data item, as
defined by the Information Artifact Ontology (IAO) [16],
is intended to be a truthful statement about something. In
order to formalize IPs, one should thus clarify which en-
tities in the real world they are about.
Proportion measurements are data items that are ob-
tained from some processes named "proportion mea-
sures", which involve performing two kinds of tests (the
index test and the reference test) in a sample. On the
other hand, we have defined a real sensitivity value
f2(g,IT,M) as the proportion of people who would get a
positive result by IT among those who have the disease
M. But note here the conditional structure: what is re-
ferred to is the proportion of true positives among dis-
eased if IT was performed on them. In realistic
situations, however, as explained above, the sensitivity
value will be estimated by performing the test on a sam-
ple of the population only  not the entire population g;
thus, f2(g,IT,M) is the value of a non-actual proportion.
9
However, possible-but-non-actual situations cannot be
straightforwardly represented in a realist ontology like
BFO. To solve this problem, we will formalize the real IP
value as the probability assigned to a disposition borne by
an instance of group of individuals; and estimates of IPs as
data items which are about such a disposition. This will
provide a formal characterization of IPs and their esti-
mates based on proportion measurements.
Results
The formalization that will be presented here can be visu-
alized on Fig. 2 and Fig. 3, in which classes are in rectan-
gles, instances in boxes with rounded edges, and the
numerical value assigned by datatype properties in ellipses.
Unless specified otherwise, all the relations used here be-
long to BFO 1.1 [11].
Test results and sensitivity estimate
Let us first start with the formalization of test results and
the IP estimates they lead to (see Fig. 1).10 A Medical_test
will be here considered as a subclass of Planned_process
(as defined by OBI, the Ontology for Biomedical Investiga-
tions [17]) which consists in the observation of a given
feature to infer the presence of another feature  in the
case of interest, a pathological entity such as a disease.
Consider a medical test11 IT1 and a disease M:
Medical_test is_a Planned_process
IT1 is_a Medical_test
M is_a Disease
Suppose that we are interested in the sensitivity and
specificity of test IT1 for diagnosing M in a group g1.
This group g1 will be formalized as a collection of
humans (for more on collections, see [18]). To estimate this
sensitivity and specificity, one can select a sample s1
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 5 of 13
considered to be representative of g1 (which will be called
the reference class). Thus:
g1 instance_of Collection_of_humans
s1 instance_of Sample_of_humans
Sample_of_humans is_a Collection_of_humans
s1 part_of g1
Lets now introduce the class of tests RT1 which are
reference tests for M:
RT1 is_a Medical_test
s1 is composed of n humans, named p1, p2,,pn.
Two12 tests will be performed on each pi: an instance of
RT1, named thereafter rt1,i, and an instance of IT1,
named it1,i; thus, for every i between 1 and n:
pi instance_of Human
pi part_of s1
pi participates_in rt1,i
pi participates_in it1,i
We introduce tests_executions1,IT1,RT1 which has as
part all the tests rt1,i and it1,i for i between 1 and n and
the recording of which members of the sample are true
positives (those who have been tested positive both by
IT1 and RT1), true negatives (those who have been tested
negative both by IT1 and RT1), false positives (those who
have been tested positive by IT1 but negative by RT1)
Fig. 2 Real sensitivity and specificity values and their estimates
Fig. 3 Aggregation of several sensitivity estimates
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 6 of 13
and false negatives (those who have been tested negative
by IT1 but positive by RT1). This recording leads (OBI:-
has_specified_output) to the creation of the instance of
Data_set named tests_resultss1,IT1,RT1:
tests_executions1,IT1,RT1 instance_of Planned_process
rt1,i part_of tests_executions1,IT1,RT1
it1,i part_of tests_executions1,IT1,RT1
tests_resultss1,IT1,RT1 instance_of Data_set
tests_executions1,IT1,RT1 has_specified_output
tests_resultss1,IT1,RT1
The tests_resultss1,IT1,RT1 will then serve as input
(OBI:has_specified_input) to a planned process noted
computationSe1 which computes a sensitivity estimates
noted estimateSe1 , by calculating the proportion of true
positives among positives:13
computationSe1 is_a Planned_process
estimateSe1 is_a Data_item
computationSe1 has_specified_input
tests_resultss1,IT1,RT1
computationSe1 has_specified_output estimate
Se
1
Finally, we can use the datatype property OBI:has_-
specified_value to relate estimateSe1 with its numerical
value f4(s1,IT1,RT1):
estimateSe1 has_specified_value f4(s1,IT1,RT1)
Similar strategies can hold for representing Specificity,
PPV and NPV and their estimates.14
Aggregation of sensitivity estimates
We will now show how various sensitivity estimates can
be aggregated for a finer sensitivity estimate (cf. Fig. 3).
Suppose that we have another sample s2 (also a part_of g),
composed of n humans named q1, q2, ..., qn'. We can per-
form another measure of sensitivity for a related (possibly
identical to IT1) index test IT2 for M in g on this sample,
using a related (possibly identical to RT1) reference test
RT2, by performing instances of RT2 named rt2,j (for j
between 1 and n) and instances of IT2 named it2,j on each
member qj of s2. One can then define the entity tests_exe-
cutions2,IT2,RT2 as a planned process which has as part
those tests rt2,j and it2,j, and which has as output tests_re-
sultss2,IT2,RT2; the latter serves as input to another compu-
tation of sensitivity computationSe2 , which has as output
another estimate of sensitivity estimateSe2 , to which the
value f4(s2,IT2,RT2) can be assigned (the latter being the
proportion, among people who have been tested positive by
RT2 in s2, of people who had a positive result to IT2).
As explained earlier, various sensitivity estimates can
be combined to estimate the value of the sensitivity of a
test for M in g. If IT1 and IT2 on one hand, and RT1 and
RT2 on the other hand, are similar enough (in particular,
if they are identical), those results might be gathered to
come up with a finer estimate of the sensitivity value.
More specifically, if IT1 and IT2 can be subsumed under
a common index test class IT0, and RT1 and RT2 can also
be subsumed under a common reference test class RT0,
then their values can be compiled mathematically (for ex-
ample by meta-analysis methods) to come up with the
value of a (hopefully finer) estimate named estimateSe1,2,
whose value is given by a function h(s1,IT1,RT1,s2,IT2,RT2).
This can be generalized to the aggregation of more than
two former estimates.
We can here introduce a planned process of computa-
tion of sensitivity named computationSe1,2, which takes as
input both estimateSe1 and estimate
Se
2 , and the output of
such a process, a data item named estimateSe1,2:
computationSe1,2 instance_of Planned_process
estimateSe1,2 instance_of Data_item
computationSe1,2 has_specified_input estimate
Se
1
computationSe1,2 has_specified_input estimate
Se
2
computationSe1,2 has_specified_output estimate
Se
1,2
estimateSe1,2 has_specified_value h(s1,IT1,RT1,s2,IT2,RT2)
We will not aim at giving the details of this function h,
which is the responsibility of the statistician, not the on-
tologist  who focuses on how to represent such values.
Finally, since estimateSe1 or estimate
Se
1,2 are informational
entities, they must be about some entities. To determine
what those entities are about, we will need to formalize the
entity to which is assigned the real sensitivity value.
Real sensitivity value
As said earlier, estimates of sensitivity of IT for M in g
aim at estimating the real sensitivity value, which is given
by the proportion of members of g who would get a posi-
tive result to IT among those who have M. However, the
condition of performing the test IT on the members of g
is never realized, because the test is performed (at best) on
one or several samples of the population, not on the whole
population g: the performance of test IT on the members
of g is a possible (leaving aside practical difficulties), non-
actual condition. Interpreting specificity, PPV, and NPV
along the former lines would also imply such possible,
non-actual conditions.
BFOs realist methodology [19] implies that all instances
should be actual entities. Thus, one cannot represent
directly such a possible-but-not-actual condition in an
ontology based on BFO. In order to solve this difficulty,
we will introduce a strategy named randomization,
which will clarify the nature of the real sensitivity value
as a probability assigned to an actual entity, namely a
disposition. This will also clarify what an estimate of
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 7 of 13
sensitivity is about, namely about this disposition. Thus, it
will enable to represent IPs in a realist fashion, compliant
with BFOs methodology.
From proportions to objective probabilities: the
randomization strategy
We will explain first how the proportion of a subgroup
in a group can be formalized as a probability value
assigned to a disposition; this will help explaining later
how the proportion of a subgroup in a group undergoing
a possible, non-actual condition can be formalized along
similar lines.
Dispositions are entities that can exist without being
manifested; an example of disposition is the fragility of a
glass, which can exist even when the glass does not
break. We will use Röhl & Jansen's model of disposition
[20] in BFO, which associates to every instance of dis-
position one or several instances of realizations, and one
or several instances of triggers (a trigger is the specific
process that can lead to a realization occurring). In this
model, the fragility of a glass is a disposition of the glass
to break (the breaking process is the realization) when it
undergoes some kind of stress (the process of undergo-
ing such a stress is the trigger); this disposition inheres
in the glass. Starting with the definition of these entities
and their relations at the instance level, Röhl & Jansen
proceed to formalize them at the universal level. Previ-
ous work [14] has shown how to adapt this model to
probabilistic dispositions. Thus, an instance of balanced
coin is the bearer of an instance of disposition to fall on
heads (the realization process) when it is tossed (the
trigger process), to which an objective probability 1/2
can be assigned.
We will now extend the scope of this model to the
situation at hand. Consider the prevalence Prev(g,M),
which was defined above as the proportion of persons
having M in the actual population g. We can define the
disposition dPrevg,M , borne by the group g, that a person
randomly drawn in g has M. More specifically, lets write
Tg the process randomly drawing a person in g, and
Rg,M the process drawing by Tg someone who has M:
the triggers of dPrevg,M are instances of Tg and its realizations
are instances of Rg,M. Following the lines of previous work
[14], one can thus define the probability assigned to the
disposition15 dPrevg,M , which is the probability of drawing
randomly someone who has M in g. This probability is
equal to the proportion of individuals who have M in g,
that is, to Prev(g,M): if there are e.g., 10 % diseased
people in g, then the probability of drawing randomly a
diseased person in g is 10 %. Thus, the prevalence value
can be identified to the objective probability assigned
to the disposition dPrevg,M . We name this strategy the
randomization of the proportion of persons having
M in g.
The randomization strategy may not be necessary to
formalize a proportion in an actual group, such as the
prevalence. But this strategy can also be applied to pro-
portions of people in groups which are subject to a pos-
sible, non-actual condition  and thus, be relevant to
formalize sensitivity and other IPs, and their estimates.
As a matter of fact, the real sensitivity value f2(g,IT,M)
was defined as the proportion of people who would get
a positive result to IT among Ms bearers in g. This value
can be randomized as follows. We can define dSeg,IT,M as
the disposition16 to draw randomly, among the individ-
uals of g who have M, someone who is tested positive by
IT. More specifically, lets define the process TSeg,IT,M as
the performance of test IT on the individuals in g, and
random draw of an individual among those who have
the disease M;17 and the process RSeg,IT,M as the drawing
by TSeg,IT,M of someone who got a positive result to IT.
The triggers of dSeg,IT,M are instances of T
Se
g,IT,M, and its re-
alizations are instances of RSeg,IT,M . As it happens, the real
sensitivity value f2(g,IT,M) is the objective probability
assigned to this disposition dSeg,IT,M,: indeed, if there are
e.g., 15 % of the diseased people in g who would get a
positive result by IT, then the probability of randomly
drawing someone who got a positive test result by IT
among diseased people in g if test IT would be per-
formed on them is equal to 15 %.
Specificity value can be defined along similar lines, as
probabilities assigned to actual dispositions borne by the
group g noted dSpg,IT,M (and similarly for the PPV and
NPV). Although both dSeg,IT,M and d
Sp
g,IT,M are dispositions
inhering in g, they have different triggers and different
realizations; the process TSpg,IT,M is the performance of
test IT on the individuals in g, and random draw of an
individual among those who do not have the disease M
and the process RSpg,IT,M is the drawing by T
Sp
g,IT,M of
someone who got a negative result to IT.
Assignment of real sensitivity values to dispositions
Let us now consider how to formalize these probability
values in ontologies. dSeg,IT,M is a disposition individual in-
hering in the group g; and a probability value can be
assigned to this disposition using a datatype property
has_probability_value [15]. This probability value is
what we called the real sensitivity value:18
dSeg,IT,M has_probability_value f2(g,IT,M)
Thanks to our analysis above, we can now answer our
original question, and state what sensitivity estimates
such as estimateSe1 or estimate
Se
2 are about
19 - namely,
about this disposition:
estimateSe1 is_about d
Se
g1,IT1,M
estimateSe2 is_about d
Se
g2,IT2,M
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 8 of 13
Also, if the samples s1 and s2 are considered by the
statistician as representative enough of a general popula-
tion g0 encompassing g1 and g2, if RT1 and RT2 are consid-
ered as similar enough to be representative in the same
way of the disease M, and if IT1 and IT2 are considered as
similar enough to be representative of a more general
index test IT0, then:
estimateSe1,2 is_about d
Se
g0,IT0,M
As dSeg,IT,M is an individual, it cannot be related directly
to the classes IT and M, but only indirectly, through the
following formalization. First, dSeg,IT,M can be seen as an
instance of a disposition class written DSeIT,M, which has
as trigger the process class TSeIT,M: performance of test
IT on the members of a group, and random draw of a
person among those who have the disease M; and as
realization the process class RSeIT,M defined as drawing by
TSeIT,M of someone who got a positive result to IT. We
can then introduce two new relations sensitivity_dispositio-
n_of_test and sensitivity_disposition_for (abreviated as
se_of_test and se_for_disease) relating DSeIT,M with IT and M:
dSeg,IT,M instance_of D
Se
IT,M
DSeIT,M is_a Disposition
DSeIT,M se_of_test IT
DSeIT,M se_for_disease M
These two relations se_of_test and se_for_disease are
introduced for pragmatic reasons of facility of use: on a
foundational level, DSeIT,M and M (resp. IT) could be re-
lated through a complex array of relations and entities
that involve the relation has_trigger between DSeIT,M and
TSeIT,M, as well as a sequence of relations between T
Se
IT,M
and M (resp. IT). Such an analysis would raise interest-
ing theoretical questions, as instances of DSeIT,M can exist
even if no instance of M or IT do exist - we therefore
face here issues similar to the ones addressed by [20]
and [21].
Figure 2 represents classes and particulars involved in
formalizing tests execution and results, sensitivity estimates,
the disposition this estimate is about, and the real sensitivity
value. Figure 3 represents the classes and particulars in-
volved in formalizing aggregation of sensitivity estimates
into a finer estimate. Specificity, PPV and NPV can be for-
malized along similar lines, as data items about dispositions
related to tests and diseases through relations that could be
labeled sp_of_test, sp_for_disease, ppv_of_test, ppv_for_-
disease, npv_of_test, and npv_for_disease.
Example of application
An example will now illustrate this formalization.
McTaggart and colleagues [8] have performed a meta-
analysis to determine the accuracy of point-of-care tests
for detecting albuminuria (lets call IT0 the class of such
index tests), using as reference test a laboratory test
albumin-creatinine ratio-ACR (lets call RT0 the class of
such reference tests).
They take into account ten studies in their article.
Consider for example Lloyd et al. [22], which measures
the accuracy of semiquantitative Clinitek® microalbumin
urine dipstick with a cutoff value indicating albumineria
at 3.4 mg/mmol (lets call IT1 the class of such index
tests), with a laboratory ACR test with the same cutoff
value as a reference (lets call RT1 the class of such refer-
ence tests). A sample s1 of 204 diabetic patients (labelled
here p1,1, p1,2,, p1,204) was considered. On each of
those patients, one measurement of IT1 called a1,i,1 and
one of RT1 called rt1,i,1 is performed. The 2x204 = 408
processual entities are all part of a general tests execution
process labelled tests_executions1,IT1,RT1, which leads after
computation to the informational entity estimateSe1 , giving
the proportion of measure pairs in which IT1 led to a posi-
tive result among those in which RT1 led to a positive
result. This proportion is 83.8 %, and therefore, the
value f4(s1,IT1,RT1) of the informational entity estima-
teSe1 is 0.838.
Writing g the human population, we have s1 part_of g;
also, RT1 is_a RT0 and IT1 is_a IT0. Therefore,
f4(s1,IT1,RT1) provides an estimate of f2(g,IT0,RT0), which
is the sensitivity value of a point-of-care test in detecting al-
buminuria in the general population. However, other stud-
ies are pooled with this one by McTaggart and colleagues
[8] to provide a better estimate of f2(g,IT0,RT0). All together,
they lead to the value h(s1,IT1,RT1,,s10,IT10,RT10) which
provides an estimate of the value of f2(g,IT0,RT0).
Note that the ten studies taken into account in this
meta-analysis include different kinds of patients. Seven
studies involve each a different sample of patients (lets
call them s1, s2, ., s7) with diabetes mellitus, one of
them (s7) involving young patients with type 1 diabetes.
Two studies consider samples of patients (s8 and s9)
with kidney disease, diabetes mellitus, or both. Finally,
one study includes a sample (s10) of patients treated for
advanced chronic kidney disease in a renal outpatient
clinic. Lets call g the human population, g1 the mem-
bers of g who have diabetes mellitus, g2 the members of
g who have a kidney disease and g0 the members of g
who have either diabetes mellitus or a kidney disease
(that is, g0 is the mereological sum of g1 and g2). All si
are part of g, the human population. Thus, the meta-
analysis made by McTaggart and colleagues [8] provides
an estimation of f2(g,IT0,RT0) or f2(g0,IT0,RT0). If the
meta-analysis had been performed on s1-s7 only, then it
would have provided an estimation of f2(g1,IT0,RT0); and
if it had been performed on samples of patients with
kidney disease only, then it would have provided an es-
timation of f2(g2,IT0,RT0).
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 9 of 13
Note also that various cutoff values can be used to de-
fine the presence of albuminuria, varying between
2.65 mg/mmol to 3.4 mg/mmol, and those values are
chosen by the medical sub-community who is con-
ducting the study (the same cutoff value is taken for
both IT0 and RT0 in each study). Therefore, the clas-
ses IT0 and RT0, which mention detecting albumin-
uria without specifying a cutoff value, are not
scientifically defined: those classes are not universals,
but rather collection of particulars [19] whose nature
is partly social ([8] acknowledge this limitation in
their meta-analysis).
Alternative meta-analysis could use a subset of those
studies to estimate various sensitivities, for example the
sensitivity f2(g1,IT1,RT1) of point-of-care test with a
reference of laboratory ACR test, with albuminuria de-
fined as ACR greater than 3.4 mg/mmol, in the refer-
ence class of patients with diabetes mellitus; or the
sensitivity f2(g2,IT2,RT2) of point-of-care test, with a
reference of laboratory ACR test, with albuminuria de-
fined as ACR greater than 2.65 mg/mmol, in the refer-
ence class of patients with kidney disease; etc. A well-
founded semantic representation of sensitivity should
thus make clear what is the reference class, as well as
the class of index test and reference test.
Discussion and conclusions
We have thus provided a practically tractable formalization
of IPs in a realist ontology, which clearly dissociates IPs real
values, their estimates and the related proportion measure-
ments. It has defined the central entities that are concerned
by an IP estimation in a way that is compliant with
OBO Foundry. In particular, it addresses the difficulty
of considering possible, non-actual conditions in a realist
ontology based on BFO by introducing dispositions.
This model could then be extended in three directions.
A first step would be to clarify the ontological status of
the two following entities: sample sizes on one hand;
and 95 % confidence interval for sensitivity and specifi-
city values on the other hand. A second step would be
to clarify the relations se_of_test and se_for_disease,
which could be reduced to basic relations and entities
already accepted in the OBO Foundry. A third step
would be to use this model in an ontology-based diag-
nostic system that would compute positive predictive
values or negative predictive values from the prevalence,
sensitivity and specificity values. More generally, it could
be articulated with medical Bayesian networks. As a
matter of fact, the notion of medical test used here could
be generalized to a very general notion of test consisting
in inferring the presence of an entity on the basis of the
knowledge of the presence of another entity; as such, it
could serve as a foundation for the integration of Bayes-
ian reasoning into ontologies.
This model could be used in two kinds of computer
applications targeted at two different kinds of audiences.
First, clinicians could determine more easily which kind
of sensitivity and specificity (or PPV and NPV) estimates
they could use when diagnosing a disease for a given pa-
tient, by having a clearer view of the subjects characteris-
tics in each samples on which those IP estimates are
based. As a matter of fact, section 3.4 illustrates how an
ontological analysis can make explicit what are the index
test, the reference test and the sample associated with a
sensitivity estimation. Universal qualities that are instanti-
ated by all members of the sample - such as having dia-
betes mellitus, being a man, being more than 65 years old,
etc. - would enable to determine what could be the refer-
ence class g associated with a sensitivity estimate. This en-
ables to determine, when applying some given IP values to
a specific patient with given characteristics, whether this
application is warranted or not.
Second, statisticians could determine more easily
which kind of sensitivity estimates they could aggregate
together. If several estimations of IPs are represented
ontologically according to the structure shown above,
one could use this ontological structure to determine
which estimations of IPs could be combined to obtain a
finer estimate. First, one would have to find a group g0
that would encompass the reference classes (such as g1
and g2) associated with those studies. Second, one would
have to analyze whether there exists some general index
test class such as IT0 (resp. some general reference test
class such as RT0) which would subsume the various
index tests classes such as IT1 and IT2 (resp. reference
tests such as RT1 and RT2) that are used in those studies.
Once those are found, one could use meta-analytic
methods to derive a value for f2(g0,IT0,RT0) from the
other studies. Future work will aim at building an ontol-
ogy of medical tests to facilitate finding such encompass-
ing index and reference test classes.
As it takes into account the dependence of IPs upon
the group of people considered, it has the potential to
contribute to the development of precision medicine
[23] in context of learning health systems [24, 25], an
emerging approach that takes into consideration patients
characteristics and dispositions, including individual
variability in genes, to offer more personalized prevent-
ive, diagnostic and therapeutic strategies.
Endnotes
1These will be abbreviated in the following as a test
IT and the patient has M. Note that a test may aim at
diagnosing a disease, in which case it can be called indi-
cator of diagnostic performance. However, it may also
aim at evaluating the presence of a disorder, a patho-
logical process [26], a predisposition to a disease, a sign,
a symptom, or other various medically relevant entities
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 10 of 13
(such as a glycemia higher than 1.26 g/l). Several tests
results can then be considered to draw a diagnostic
conclusion for a disease. Therefore, in the general case,
indicators of performance are indicators of assay perform-
ance rather than indicators of diagnostic performance (we
thank an anonymous reviewer for this suggestion of ter-
minology). Also, a test does not need to be performed on
a human  it can be performed on a non-human animal.
In the following, we will consider tests aiming at diagnos-
ing a disease on a human, but our considerations can
be straightforwardly adapted to tests aiming at evaluat-
ing another medically relevant entity on a human or
non-human animal.
2In practice, such a test is not perfect; thus, it could be
analyzed as a chain of two tests: one that detects the
rheumatoid factor on the basis of e.g., some chemical re-
action, and another one that detects rheumatoid arthritis
on the basis of the presence of the rheumatoid factor.
3More specifically, it should be interpreted as the ex-
pected value of such a proportion  but we will ignore
here this additional subtlety.
4The article will concentrate on the case of sensitivity,
but it can be similarly adapted to other IPs.
5Here again (see footnote 3), this should be interpreted
as the expected value of such a proportion.
6At least for all practical purposes: from a theoretical
point of view, every measurement can be wrong, even
pure observations.
7If one assumes that the sample is representative of
the target population, there should be no selection
bias (which occurs when proper randomization is not
achieved). However, the sensitivity values that would
be obtained using two different samples could be
slightly different since randomness at the selection
process will yield slightly different samples. That is
why statisticians use confidence interval for character-
izing sensitivity and specificity.
8We might also speak of a sensitivity in a sample for
the function f2(s,IT,M), that is, the proportion of people
who are tested positive by IT among the diseased person
in the sample s. But it might be confusing to speak of
both the sensitivity in a target population and the sen-
sitivity in a sample; and the first and the second argu-
ments above may justify keeping the label sensitivity
for this proportion in a target population g  that is, for
f2(g,IT,M).
9Let us summarize. On one hand, f2(g,IT,M) is the
value of a non-actual proportion (because the test IT is
not performed on all members of g), which cannot be
known with certainty, but only estimated. On the other
hand, both f4(s,IT,RT) and f2(s,IT,M) (see footnote 8) are
values of actual proportions (because the tests IT and RT
are performed on all members of s); and although
f2(s,IT,M) cannot be known with certainty (because we
cannot know with certainty who has the disease: we can
only use a reference test  at best the gold standard  to
determine who are those individuals), f4(s,IT,RT) can be
known with certainty for all practical purposes (because
we can know with certainty who got a positive result
to RT).
10We have created an ontology according the lines of
what is described below, built on OBI, called BIPO
(Bayesian Indicator of Performance Ontology). It can be
found at https://github.com/OpenLHS/BIPO. It contains
24 classes, 12 object properties, 2 data properties and 42
logical axioms.
11We will not take a stance on whether Medical_test
should be interpreted as identical to OBI:Assay, as pro-
posed by [27].
12Note that in some cases, several pairs of tests will be
performed on a person. See e.g., Kimberger et al. (2007),
which measures the accuracy of a temporal artery
thermometer in detecting fever (defined as a temperature
greater than 37.8 °C), with respect to a reference standard
given by a bladder thermometer: four measurement pairs
of temporal artery temperature and bladder temperature
are performed on each of the seventy patients of the sam-
ple considered by the authors. To represent such a case,
one can introduce for every human pi a sequence of four
reference tests rt1,i,1, rt1,i,2, rt1,i,3 and rt1,i,4 .and four index
tests it1,i,1, it1,i,2, it1,i,3 and it1,i,4; but the formalization that
is described below remains similar.
13See e.g., http://vassarstats.net/clin1.html for an ex-
ample of webpage supporting this kind of computation.
14As a reminder, not only the values of PPV and NPV
but also the values of sensitivity and specificity depend
on the group under consideration (this is the spectrum
effect), and it is not the task of the ontologist to deter-
mine which ones should be idealized as constant (for all
practical matters) across groups and which ones should
be considered as variable: the task of the ontologist is to
represent those values and the entities those values de-
pend upon.
15[15] assigned a probability to a triplet (d,T,R) rather
than to a disposition d, because it had to take into ac-
count dispositions that may have several classes of trig-
gers or realizations (that is, multi-trigger and multi-track
dispositions [20]). However, in the present situation,
dSeg,M is simple-trigger and simple-track: all its triggers
are instances of TSeg , and all its realizations are instances
of RSeg,M. Therefore, the probability value assigned to
(dSeg,M,T
Se
g ,R
Se
g,M) can be, for practical matters, assigned dir-
ectly to dSeg,M.
16Such dispositions should not be confused with other
dispositions in the medical domain. First, diseases have
been formalized as dispositions by the Ontology for
General Medical Sciences (OGMS) [26]. Second, there
can be predispositions to diseases that could be
Barton et al. Journal of Biomedical Semantics  (2017) 8:1 Page 11 of 13
formalized as disposition. However, the disposition to
draw randomly, among the individuals of g who have M,
someone who is tested positive by IT, exists independently
of whether the disease (or a predisposition to this disease)
is formalized or not as a disposition. Note also that this
disposition inheres in a group of people, whereas a disease
as a disposition (as formalized by OGMS), or a predispos-
ition to a disease, inheres in a single person.
17In general, we cannot determine in practice with cer-
tainty which individuals of g have M, and which do not
(see the discussion about gold standard tests above); but
the practical impossibility to realize this trigger does not
preclude to define this entity.
18We could also introduce the entity real_sensitivity-
g,IT,M instance of Data_item, as a sibling of estimate
Se
1 such
that real_sensitivityg,IT,M has_specified_value f2(g,IT,M)
(cf. [14], in which real_sensitivityg,IT,M was denoted
seg,IT,M). However, the value f2(g,IT,M) assigned to such an
entity will never be known with certainty. We could substi-
tute to this value the best estimate of the sensitivity value,
as was proposed in [14]; however, such a model could not
represent in a single ontology various estimates of the
same sensitivity  whereas it is possible in the present
framework, which also makes unnecessary the introduc-
tion of the informational entity real_sensitivityg,IT,M.
19It is important to differentiate what a sensitivity esti-
mate is about (namely a disposition) from how it has
been mathematically obtained (for example, by weight-
ing different proportion measurements)  as explained
earlier, the latter will not be represented in the ontology,
as various mathematical methods can be used.
Abbreviations
General abbreviations for indicators of performance
IP: (Bayesian) Indicators of performance; NPV: Negative predictive value;
PPV: Positive predictive value; Prev: Prevalence; Se: Sensitivity; Sp: Specificity
Other general abbreviations
ACR: Albumin-creatinine ratio; RF: Rheumatoid factor
Classes and instances abbreviations for disposition-related entities
dPrevg,M : Disposition (borne by the group g) that a person randomly drawn
among the individuals in g would have M; dSeg,IT,M: Disposition (borne by the
group g) that a person randomly drawn among the individuals of g who
have M would have a positive result to IT; this is an instance of DSeIT,M; D
Se
IT,M: A
subclass of Sensitivity disposition such that DSeIT,M se_for_disease M and D
Se
IT,M
se_of_test IT; DSpIT,M: A subclass of Specificity disposition such that D
Sp
IT,M
sp_for_disease M and DSpIT,M sp_of_test IT; Tg: The process of drawing randomly
a person in g; the triggers of dPrevg,M are instances of Tg; T
Se
g,IT,M: The process of
performing test IT on the individuals in g, and then drawing randomly an
individual among those who have the disease M; the triggers of dSeg,IT,M are
instances of TSeg,IT,M; Rg,M: The process of drawing by Tg someone who has M;
the realizations of dPrevg,M are instances of Rg,M; R
Se
g,IT,M: The process of drawing
by TSeg,IT,M someone who got a positive result to IT; the realizations of d
Se
g,IT,M
are instances of RSeg,IT,M;
Other classes abbreviations
IT / IT0 / IT1 / IT2: A subclass of Medical test which is an index test (test whose
indicator of performance is being estimated); M: A subclass of Disease; RT /
RT0 / RT1 / RT2: A subclass of Medical test which is a reference test
Other instances abbreviations
g / g0 / g1 / g2: An instance of Collection of humans which is a general
human population; pi / qj: An instance of Human; it1,i (resp. it2,j): An instance
of (index) Medical test performed on person pi (resp. qj); rt1,i (resp. rt2,j): An
instance of (reference) Medical test performed on person pi (resp. qj); s / s1 /
s2: An instance of Sample of humans;
Functions abbreviations
f1(IT,M): Proportion of individuals who get a positive result to IT, among
individuals who have M; f2(g,IT,M): Proportion, among members of g who
have M, of those who would get a positive result to IT if the test IT was
realized on them; this is the real sensitivity value of IT for M;
f3(g,IT,RT): Proportion, among members of g who would get a positive result
to RT if the test RT was realized on them, of those who would get a positive
result to IT if the test IT was realized on them; f4(s,IT,RT): Proportion, among
members of sample s who had a positive result to RT, of those who got a
positive result to IT; this is an estimate of the real sensitivity value of IT for M,
performed on a sample s, with RT as a reference test; f2(g,IT,M): Proportion,
among members of g who dont have M, of those who would get a
negative result to IT if the test IT was realized on them; this is the real
specificity value of IT for M; f4(s,IT,RT): Proportion, among members of sample
s who had a negative result to RT, of those who got a negative result to IT;
this is an estimate of the real specificity value of IT for M, performed on a
sample s, with RT as a reference test; h(s1,IT1,RT1,s2,IT2,RT2): Estimate of the
sensitivity value obtained by aggregating the estimate on sample s1 and the
estimate on sample s2
Acknowledgements
We would like to thank two anonymous reviewers for their comments that
led to significant improvements in our model and in the manuscript, as well as
assistance during various presentations of this work for their suggestions. ABa
would like to thank the bourse de fellowship of the department of
medicine of Sherbrooke University for financial support. This manuscript is
an extended version of work presented at ICBO (International Conference
on Biomedical Ontology) 2015.
Authors contributions
ABa conceived the formalization of the real indicator of performance values,
JFE and ABa conceived the formalization of the estimation of indicators of
performances, and ABa and JFE developped the BIPO ontology, in light of inputs
from ABu and RD. JFE and RD provided the medical examples supporting the
formalization.
ABa drafted the manuscript with important feedbacks from JFE, RD and ABu. All
authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Author details
1Département de médecine, Université de Sherbrooke, Sherbrooke, Québec,
Canada. 2INSERM UMR 1099, LSTI, Rennes, France. 3CHU de Martinique, Université
Antilles-Guyane, Fort-de-France, France. 4INSERM UMR_S 1138 Eq 22, Université Paris
Descartes, Hôpital européen Georges Pompidou, AP-HP, Paris, France. 5Centre de
recherche du CHUS, CIUSSS de lEstrie-CHUS, Sherbrooke, Québec, Canada.
Received: 2 February 2016 Accepted: 6 September 2016
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 
DOI 10.1186/s13326-017-0126-0
SOFTWARE Open Access
Dead simple OWL design patterns
David Osumi-Sutherland1* , Melanie Courtot1, James P. Balhoff2 and Christopher Mungall3
Abstract
Background: Bio-ontologies typically require multiple axes of classification to support the needs of their users.
Development of such ontologies can only be made scalable and sustainable by the use of inference to automate
classification via consistent patterns of axiomatization. Many bio-ontologies originating in OBO or OWL follow this
approach. These patterns need to be documented in a form that requires minimal expertise to understand and edit
and that can be validated and applied using any of the various programmatic approaches to working with OWL
ontologies.
Results: Here we describe a system, Dead Simple OWL Design Patterns (DOS-DPs), which fulfills these requirements,
illustrating the system with examples from the Gene Ontology.
Conclusions: The rapid adoption of DOS-DPs by multiple ontology development projects illustrates both the ease-of
use and the pressing need for the simple design pattern system we have developed.
Keywords: OWL, OBO, Design pattern
Background
Biologists classify biological entities in many different
ways. A single neuron may be classified by structure
(pseudo-bipolar), electrophysiology (spiking), neurotrans-
mitter (glutamatergic), sensorymodality (secondary olfac-
tory neuron), location(s) within the brain (antennal lobe
projection neuron,mushroom body extrinsic neuron), etc.
A transport process occurring in a cell may be classi-
fied by the type of chemical transported, where transport
starts and ends, and by what membranes are crossed.
Bio-ontologies provide a widely used method for doc-
umenting such classifications and the relationships that
apply between members of classes, such as partonomy.
These classifications and relationships are central to the
successful use of bio-ontologies in helping biologists make
sense of the ever increasing volumes of data they work
with. They are critical to the use of the Gene Ontology
(GO) [1] and its associated annotations in interpreting
genomic data via its application in enrichment analysis [2].
They are critical to the functioning of Virtual Fly Brain in
grouping and querying neuroanatomical data [3].
*Correspondence: davidos@ebi.ac.uk
1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome
Campus, CB10 1SD Cambridge, UK
Full list of author information is available at the end of the article
To be successful in this role, bio-ontologies need to
capture all of the many forms of classification that are
important to biologists; but maintaining this manually
becomes impractical as ontologies grow. Without formal-
ization, the reasons for existing classifications are often
opaque. The larger an ontology, the harder it is for human
editors to find all valid classifications when adding a term,
or to work out how to re-arrange the hierarchy when new
intermediate classes are added.
The alternative to manually asserting classification is
to use OWL inference to automate it. OWL equivalence
axioms can be used to specify necessary and sufficient
conditions for class membership. Standard reasoning soft-
ware can then build a class hierarchy by finding classes
that fulfill these conditions.
Many bio-ontologies now follow this approach,
including the Uber Anatomy Ontology (Uberon) [4], the
GO [5], the Ontology of Biomedical Investigations (OBI)
[6], the Drosophila Anatomy Ontology (DAO) [7], the
Cell Ontology (CL) [8] and the Ontology of Biological
Attributes (Ontology of Biological Attributes (OBA) [9].
In the GO, over 52% of the classification is automated.
Much of this classification leverages the structure of
imported ontologies; for example, classification of trans-
port processes in the GO relies on a classification of
chemicals provided by the chemical ontology ChEBI [10]
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 2 of 7
and on object property axioms specified in the OBO
relations ontology.
A critical requirement for ongoing development of these
ontologies is the specification of design patterns to guide
the consistent OWL axiomatization required for auto-
mated classification. In many of these ontologies, classes
are annotated with textual descriptions that follow stan-
dard patterns which also need to be documented. Where
formal, machine-readable design patterns are sufficiently
detailed, they can be used to quickly generate new classes,
update old ones when a pattern changes, and automati-
cally generate user-facing documentation.
OWL design pattern systems
There is an extensive literature on ontology design pat-
terns in OWL [11, 12]. Much of this is based on an
approach known as Content Ontology Design Patterns
(CODPs; see [12]) for an overview). CODPs are small,
autonomous ontologies that specify multiple classes and
properties. CODPs are typically re-used by one of two
methods. Either the pattern is imported and new sub-
classes and sub-properties of pattern entities are instanti-
ated in the target ontology, or it is used as a template, with
entities in the pattern being given new identifiers in the
namespace of the target ontology.
The GO and several other ontologies including CL and
OBA already use standard patterns to generate new class
terms via the TermGenie tool [13]. In GO, around 80% of
new class terms are added via this route. This tool allows
new terms to be added by specifying a desgin pattern
and a set of fillers for variable slots. Unlike CODPs, these
design patterns are not autonomous: they import classes
and object properties from various ontologies. Thismeans
that their semantics are dependent on those of the ontolo-
gies they import from. This is by design: the patterns
are intended to leverage classification and axiomatiza-
tion from external ontologies to drive classification in the
target ontology.
Design patterns in TermGenie are specified directly in
Javascript. This specification is opaque to most human
editors and is not easily reusable outside the context of
TermGenie. The other major mechanisms for specifying
design patterns for programmatic use are the languages
Tawny OWL [14] and Ontology PreProcessing Language
(OPPL) [15]. These are very powerful tools for gener-
ating and manipulating ontologies, but are not easy for
ontology editors without strong technical backgrounds
to write. They are also tied to specific languages and
implementations, limiting their use.
Many editors of bio-ontologies are biologists with
limited computational expertise beyond a basic under-
standing of some subset of OWL (typically limited to the
subset of OWL that can be encoded in OBO 1.4 [16]),
which they interact with via Manchester Syntax rendering
and graphs in graphical editing tools such such as Pro-
tégé [17]. A simple, lightweight standard for specifying
design patterns is needed in order to make their devel-
opment and use accessible to these editors. This standard
should be readable and editable by anyone with a basic
knowledge of OWL. It must also be easy to use pro-
grammatically without the need for custom parsers  i.e.
it should follow some existing data exchange standard
that can be consumed by any modern programming lan-
guage. Based on these requirements, we have defined
a lightweight, YAML Aint Markup Language (YAML)-
based syntax for specifying design patterns, called Dead
Simple OWL Design Patterns, or DOS-DPs (inversion of
two letters is an homage to the Web Ontology Language,
OWL, on which it is based).
Implementation
We have developed a formal specification of DOS-DPs
using JSON-schema [18] draft 4 for use in validation and
documentation. This is available from the DOS-DP repos-
itory [19], which also lists recommendations for additional
validation steps. Description fields in the schema doc-
ument intended usage. Where appropriate, the schema
document also includes fields that document mappings
to relevant OWL entities. We use the Python jsonschema
package to validate the schema and test it against exam-
ple patterns. Table 1 contains a summary of schema field
types and how they are used.
Approach
DOS-DPs are designed to be easy to read, edit and parse.
We chose YAML because it is relatively easy to read and
write compared to other common data exchange formats
such as JSON and XML, and can be consumed by a wide
range of programming languages. In order to take advan-
tage of JSON-Schema for specification and validation,
DOS-DPs are restricted to the JSON compatible subset of
YAML [20].
Each design pattern can have an arbitrary number of
variables. For ease of reading, writing and parsing, vari-
able interpolation uses printf, a standard part of most
modern programming languages.
OWL is expressed using Manchester Syntax [21], the
most human-readable and editable of the OWL syntaxes,
and the one most editors with a basic knowledge of
OWL are likely to have encountered. For ease of read-
ing and editing, quoted, human-readable identifiers are
used for OWL entities throughout the pattern. These are
assumed to be sufficient to uniquely identify any OWL
entity within a pattern. Dictionaries are used to map read-
able identifiers to compact URIs (CURIEs)  prefixed
short form identifiers. A JSON-LD context is used to map
these to full IRIs. The entity IRIs recorded in this way
can be used to check reference ontologies to find the
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 3 of 7
Table 1 DOSDP JSON schema fields
Field type Used to Mandatory subfields Optional subfields Used in
Printf_owl Specify a logical OWL axiom
using printf to substitute
variable values
axiom_type, text, vars annotations logical axioms
Printf_annotation Specify an annotation using
printf to substitute variable
values
annotationProperty, text,
vars
annotations annotations
List annotation Specify a list of annotation
property axioms of a single
type using a list of values
specified by a data list
variable
annotationProperty, value - annotations
Printf_owl_convenience Specify a logical OWL axiom
of a prespecified type, using
printf to substitute variable
values.
text, vars annotations equivalentTo, subClassOf,
disjointWith, GCI
Printf annotation obo Specify an annotation
axiom of a prespecified
type using a list of values
specified by a data list
variable
text, vars annotations, xrefs def, name, comment,
List_annotation_obo Specify a list of annotation
property axioms of a
single type, pre-specified
type. using a list of values
specified by a data list
variable
value - xrefs, exact_synonyms, . . .
Field type: Name of schema field type (JSON schema definition). Used to: Description of field usage. Used in: Schema Fields in which this field type is used
current validity and status of all entities referenced in
a pattern.
While the full specification of DOS-DPs is intended to
be generic and expressive, a major aim is to hide com-
plexity from editors wherever possible. To this end, we
define convenience fields that are suitable for use in com-
mon, simple design patterns. We also allow extensions
that import and extend the core JSON schema and that
specify default values for high level fields. For example, we
define an extension to support the OBO standard. This
defines convenience fields for expressing OBO standard
annotations and specifies a default annotation property
for readable identifiers and an OBO standard base URI
pattern.
Figure 1 shows an example design pattern for gener-
ating classes of transport across a membrane defined by
cargo type and membrane type. Figure 1a shows a pattern
following the OBO extension. Figure 1b shows the same
pattern expressed using the more verbose DOSDP core-
specification. Figure 2 shows an example class generated
using this pattern.
Details
Patternmetadata
Each pattern is identified by an IRI. The short form
of this IRI is recorded in a pattern_name field, and,
by convention, is used for the file name. Each pattern
optionally includes an extension specification, indicat-
ing the extension to be used in interpreting the pattern
document. In 1a this is set to OBO.
Dictionaries
In both versions of the pattern, the fields classes and
relations serve as dictionaries for the OWL classes and
object properties respectively used in the pattern, map-
ping human readable identifiers (keys) to short_form
identifiers (values). The core pattern specifies an anno-
tation property to use as a source of readable identifiers
via the readable_identifier field. This is not required in
the OBO extension version, as the extension specifies a
default value of rdfs:label for this. The full pattern
also contains an additional dictionary of OWL annotation
properties. These are not required in the OBO extension,
which specifies dedicated fields for annotation properties
used in the OBO standard. The core DOSDP specification
also defines a dictionary field for OWL data properties.
Input fields
All patterns contain one or more variable specification
fields. These are simple objects in which the keys are vari-
able names and the values specify variable range. The
vars field specifies variables that range over OWL classes,
specified as Manchester syntax expressions. For example,
the value of the cargo variable in Fig. 1 is specified by
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 4 of 7
Fig. 1 DOS-DP for defining classes of transmembrane import (based
on an example from the GO.) Panel A shows the DOS-DP using the
OBO extension. Panel B shows the same pattern expressed using the
core specification (classes, relations and vars fields omitted from panel
B for brevity). In Panel A, annotations are specified using dedicated
fields (def, name, xrefs). The mapping from these to OWL
annotation properties is specified in the OBO extension schema. This
mapping is made explicit in Panel B, using an annotation_property
dictionary and the annotationProperty field in axiom specifications
under annotations. Throughout both versions of the pattern, paired
fields text and vars specify printf text and fillers respectively. The
value field is used with the data_list_var def_xrefs to specify a list
database_cross_reference annotations on the definition
the class expression: chemical entity or transcript. The
quoted OWL entity names in this expression are spec-
ified in the dictionaries. Both patterns also include an
example of a variable that takes a data type as an input.
The data_list_vars field specifies variables whose values
are lists in which all elements share an OWL data type,
specified in the value of the variable field. For example
def_dbxref in Fig. 1 is specified to be a list of (XSD) strings.
Output fields
The core schema has just two output fields: annotations
for annotation property axioms and logical_axioms for
logical owl axioms. The value of both of these fields is a list
of axiom specifications. Each axiom specification includes
a specification of axiom type (logical type or annotation
property). Content is either specified using printf sub-
stitution of variable values into a text string (field type
printf_annotation or printf_owl in Table 1 or by specify-
ing a list of values to be used to generate multiple axioms
Fig. 2 Example pattern implementation. An example of a term,
leucine transport across the plasma membrane, generated using the
pattern in Fig. 1. Note the automated classification under amino acid
transport across the plasma membrane, specified using the same
pattern
of the same type (e.g. field type list_annotation in Table 1.
Where OWL entities (specified as vars) are used to specify
Printf substitution, the readable label of the entity is
used. Axiom specifications can also be used to specify
annotations of the specified axiom.
In our example, the annotations field is used to specify
an rdfs:label axiom and a definition axiom. In both
cases a text output is specified using a text field to specify
a printf statement and a vars field to specify an ordered
list of fillers. The definition axiom specification specifies a
set of axiom annotations using a database_cross_reference
annotation property. These axioms will be generated
using a list of strings provided in the data_list_var
def_dbxref. The results can be seen in Fig. 2.
The OBO version (1) encodes the same information
using named fields: name, def, and xrefs. These fields fol-
low the tag names used in OBO format [16]. The field
specifications (in the OBO JSON schema doc) map these
fields to the relevant OWL annotation properties, remov-
ing the need for ontology pattern developers to specify
these mappings in an annotation property dictionary.
The logical_axioms field in Fig. 1b specifies just one
equivalence axiom. This is a very common pattern for
defining classes. To make specifying this type of pat-
tern easier, we define convenience fields that can be used
whenever there is only one axiom of a given type per
pattern. The pattern in 1a uses the convenience field for
equivalentTo to concisely capture the single logical axiom
in this pattern.
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 5 of 7
Discussion
Limitations
DOS-DPs are designed to be simple and clear. There are
a number of obvious ways that they could be made more
powerful but which we have avoided in order to retain
simplicity and clarity.
By design, DOS-DPs lack a mechanism for relating pat-
terns to each other via inheritance or composition. Such
mechanisms would add a technical burden to their, use
requiring additional tooling, and so be a barrier to their
adoption. Manual maintenance of design pattern hierar-
chies also risks re-creating the maintenance problem that
these patterns are meant to solve.
For the sake of simplicity, DOS-DPs also lack a sys-
tem for specifying optional clauses. This places some
burden on the development of patterns that naturally
form a subsumption hierarchy. However, the relationships
between patterns can easily be derived by generating a
set of OWL classes using default fillers (variable ranges)
and classifying the results using a reasoner. This clas-
sification can then be used as a way of testing sets of
DOS-DPs and to generate a browsable hierarchy of related
patterns.
Adoption
DOS-DPs are used both as formal documentation, and
as part of the ontology-engineering pipelines in the GO,
OBA, the Environmental Ontology (ENVO) [22], the
Plant Trait Ontology [23], the Plant Stress and Disease
Ontology [24], the Agriculture Ontology, and the Envi-
ronmental Conditions and Exposures Ontology [25]; the
central DOS-DP GitHub repo has a list of all adopters. See
Figs. 1 and 2 for an example of a pattern used extensively
in the GO.
One heavy user of (OPPL) patterns is Webulous, an
application that allows specification of OWL classes using
templates loaded into Google spreadsheets. It should be
straightforward to develop a version of Webulous that
supports design patterns specified as DOS-DPs, removing
the need for expertise in OPPL to specify new patterns.
Similarly, it should be possible to extend Tawny-OWL to
support DOS-DPs. This could prove to be a very effec-
tive combination of accessible design pattern specification
with a computationally powerful language for writing and
manipulating OWL ontologies.
Patterns inevitably evolve as use-cases evolve. Changing
all uses of an existing pattern by hand is impractical
unless the number of uses is relatively low. For branches
of ontologies where all terms follow a completely stereo-
typed pattern, we can specify whole branches simply by
specifying a DOS-DP together with a URI and set of
variable fillers for each term. We plan to use this to pro-
grammatically generate suitable branches of the GO at
each release.
Where more flexibility is required, DOS-DPs could be
used to update existing terms that are part of a human-
edited ontology file. A system of tagging terms by the
pattern they implement would allow all relevant terms to
be identified. DOSDP-scala [26] can be used to identify
existing classes within an ontology that follow a speci-
fied pattern, returning the fillers populating each vari-
able in the pattern. If an ontology pattern changes then
DOSDP-scala can also be used to test whether tagged
terms conform to the old pattern, flagging those that do
for automated update and those that do not for manual
inspection.
Conclusions
As can be seen from Fig. 1, which shows a pattern for
defining terms in the GO, DOS-DPs are easy to read and
write. The choice of YAML limits the need for balancing
brackets and commas. The use of printf, Manchester
syntax, and labels for OWL entities makes the pattern easy
to read. Figure 2, which shows an application of the pat-
tern specified in Fig. 1, illustrates how similar the pattern
is to the way human editors interact with ontology classes
in a GUI editor like Protégé [17]. As well as ease of reading
and writing, our other aim is language independence. Cur-
rently there are partial (OBO-specific) implementations
in Python [27] and Jython [28, 29], along with the Scala-
based pattern matcher [26]. TermGenie is being extended
to consume DOS-DPs. These implementations cover pat-
tern validation and the addition of new classes. They also
allow for generation of markdown format documentation
from design patterns.
Availability and requirements
Project name:Dead Simple OWLDesign Patterns (DOS-
DP). The specification and recommendations for valida-
tion are available from [29] under the GNUGeneral Public
License v3.0.
Programming language and requirements: The schema
is specified using JSON-schema [18]. This specification
can be consumed by any language for which a schema
checker exists (see [18]).
Abbreviations
ChEBI: Chemical entities of biological interest; CL: Cell ontology; CODP:
content ontology design pattern; CURIE: Compact URI; DOS-DP: Dead simple
OWL design pattern; GO: Gene ontology; GUI: Graphical user interface; IRI:
Internationalized resource identifier; JSON: JavaScript object notation; OBA:
Ontology of biological attributes; OBO: Open biomedical ontologies; OPPL:
Ontology preprocessing language; OWL: Web ontology language; XML:
Extensible markup language; XSD: XML schema description; YAML: YAML aint
markup language
Acknowledgements
We sincerely thank Helen Parkinson, the Gene Ontology Consortium and the
Sample, Phenotypes and Ontologies Group at the EBI for providing a fertile
intellectual environment for the development of DOS-DPs.
Osumi-Sutherland et al. Journal of Biomedical Semantics  (2017) 8:18 Page 6 of 7
Funding
This work was funded by the Gene Ontology Consortium P41 grant from the
National HumanGenome Research Institute (NHGRI) [grant 5U41HG002273- 14]
and by BD2K [grant U01 HG009453]. CJM acknowledges the support of the
Director, Office of Science, Office of Basic Energy Sciences, of the US
Department of Energy (DE-AC02- 05CH11231). MC was funded by the Gene
Ontology Consortium P41 grant from the National Human Genome Research
Institute (NHGRI) [grant 5U41HG002273- 14] and EMBL-EBI core funds.
Availability of data andmaterials
The DOS-DP specification and recommendations for validation are available
from [29].
Authors contributions
DOS-DPs were developed by DOS in with suggestions from CJM and JPB.
Many of the uses of DOS-DPs described here were implemented by CJM. MC
supervised some of this work and contributed to drafting this paper. JPB
developed DOSDP-scala. All authors read and approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome
Campus, CB10 1SD Cambridge, UK. 2RTI International, 27709 Research Triangle
Park, NC, USA. 3Genomics Division, Lawrence Berkeley National Laboratory,
94720 Berkeley, CA, USA.
Received: 23 November 2016 Accepted: 29 March 2017
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 
DOI 10.1186/s13326-017-0157-6
RESEARCH Open Access
Entity recognition in the biomedical
domain using a hybrid approach
Marco Basaldella1, Lenz Furrer2, Carlo Tasso1 and Fabio Rinaldi2*
Abstract
Background: This article describes a high-recall, high-precision approach for the extraction of biomedical entities
from scientific articles.
Method: The approach uses a two-stage pipeline, combining a dictionary-based entity recognizer with a
machine-learning classifier. First, the OGER entity recognizer, which has a bias towards high recall, annotates the terms
that appear in selected domain ontologies. Subsequently, the Distiller framework uses this information as a feature for
a machine learning algorithm to select the relevant entities only. For this step, we compare two different supervised
machine-learning algorithms: Conditional Random Fields and Neural Networks.
Results: In an in-domain evaluation using the CRAFT corpus, we test the performance of the combined systems
when recognizing chemicals, cell types, cellular components, biological processes, molecular functions, organisms,
proteins, and biological sequences. Our best system combines dictionary-based candidate generation with
Neural-Network-based filtering. It achieves an overall precision of 86% at a recall of 60% on the named entity
recognition task, and a precision of 51% at a recall of 49% on the concept recognition task.
Conclusion: These results are to our knowledge the best reported so far in this particular task.
Keywords: Named entity recognition, Text mining, Machine learning, Natural language processing
Background
The scientific community in the biomedical domain is a
vibrant community, producing a large amount of scientific
findings in the form of data, publications, reports, and so
on, each year, making it difficult for scholars to find the
right information in this large sea of knowledge.
To tackle this problem, researchers have developed dif-
ferent text mining techniques with the goal of detect-
ing the relevant information for the intended purpose.
This papers focus is the technique called Named Entity
Recognition (herein NER), which solves the problem of
detecting terms belonging to a limited set of predefined
entity types.
NER can be performed on both generic documents,
to recognize concepts like person, date or location, or
on technical documents, to recognize concepts like cells,
*Correspondence: rinaldi@cl.uzh.ch
Equal contributors
2University of Zurich, Institute of Computational Linguistics and Swiss Institute
of Bioinformatics, Andreasstrasse 15, CH-8050 Zürich, Switzerland
Full list of author information is available at the end of the article
diseases or proteins. NER can be used by itself, with the
goal of recognizing themere presence of a term in a certain
portion of the text, or as a preliminary stage for Concept
Recognition (CR), also known as Entity Linking or Nor-
malization, where the term is not only recognized but also
linked to a terminological resource, such as an ontology,
through the use of a unique identifier [1].
NER can be solved using several techniques:
 Using manual, hand-written rules. A group of experts
develops these rules using domain knowledge. The
rules typically rely on orthographic patterns, such as
particular use of capitalization or punctuation. Even
though rule-based systems can perform well if
sufficient expert time is available for creating the
rules, their maintenance requires repeated manual
efforts, since the rules need to be revised or even
entirely replaced whenever the system is adapted to
new data (different entity types, another text
collection). In the biomedical domain, plain rule-
based approaches like [2] have become rare; however,
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 2 of 14
they continue to be used in combination with other
techniques, such as dictionary look-up [3, 4].
 Using dictionaries to recognize known entities. An
automatic process looks for all the possible entities
(possibly, all the words in a document) in one or
more dictionaries (or ontologies, or databases, or
gazetteers) of known entities. This method has the
obvious drawback that new entities cannot be
recognized, because they are not yet present in the
dictionary. Pafilis et al. [5] and [6] use a
dictionary-based approach for NER of species and
chemicals, respectively.
 Using machine learning techniques. A machine
learning method, like Support Vector Machines or
Conditional Random Fields (herein CRF), can be
trained to recognize entities in a fragment of text
using a large set of pre-annotated documents as
examples. If trained properly, a machine learning
model can potentially recognize entities that are not
yet inserted in dictionaries or ontologies. The
drawback of this approach is the fact that training
material is not always available for a certain domain,
or if present, it may be unsatisfactory in terms of
quality or size. Examples for a CRF-based approach
are presented in [7, 8].
 Using a hybrid approach. Two or more of the
previously mentioned approaches are used together
to combine their strengths and, hopefully, overcome
their weaknesses. For example, [9] and [10]
successfully use a hybrid dictionary-machine learning
approach.
This paper presents an extension of the hybrid solu-
tion introduced in [11]. In that paper, we present a hybrid
dictionary-machine learning approach, where the dictio-
nary stage, performed by OntoGenes Entity Recognizer
(OGER) [12, 13], generates a high recall, low precision set
of all the possible entities that can be found in a docu-
ment and then the machine learning stage, performed by
Distiller [14], filters these entities trying to select only the
relevant ones.
The aim of this work is to improve the system
presented in [11] by exploring new techniques both
for the dictionary and the machine-learning stage, in
particular by replacing the original machine learn-
ing approach with one based on CRFs. We present
these techniques, analyzing the new methods we intro-
duced, and we evaluate them on the CRAFT corpus
[15], a set of documents from the biomedical domain
where the relevant concepts have been linked to sev-
eral ontologies. Then, we compare the results obtained
with the ones found in the literature, exploring the
potential of using the system as a concept recognition
pipeline.
Methods
CRAFT corpus
The Colorado Richly Annotated Full Text (CRAFT) cor-
pus is a set of articles from the PubMed Central Open
Access Subset [16], a part of the PubMed Central archive
licensed under Creative Commons licenses, annotated
with concepts pointing to several ontologies.
The corpus is composed of 67 annotated articles avail-
able in the public domain, plus 30 articles that have been
annotated but are reserved for future competitions and
have to date not been released.
The ontologies used in the corpus are:
 ChEBI: Chemical entities of Biological Interest [17],
containing chemical names
 CL: Cell Ontology [18], containing cell type names
 Entrez Gene [19], containing gene names
 GO: Gene Ontology [20]. CRAFT provides two
sub-ontologies, one for physical entities (cellular
components, CC) and one for non-physical entities
(biological processes andmolecular functions,
BPMF).
 NCBI Taxonomy: the US National Center for
Biotechnology Information Taxonomy [21],
containing names of species and other taxonomic
ranks
 PR: Protein Ontology [22], containing protein names
 SO: Sequence Ontology [23], containing names of
biological sequence features and attributes
In total, the available articles are annotated with over
100,000 concepts. Moreover, each of the 67 articles con-
tains linguistic information, such as tokenized sentences,
part-of-speech information, and dependency parse trees.
For our experiments, we used all terminology resources
except for NCBI Entrez Gene. We decided to omit Entrez
Gene from the evaluation against CRAFT for a number
of reasons. For one, the distribution of the CRAFT cor-
pus does not include a reference version (unlike all other
terminologies); this means that we would have to use
an up-to-date version of Entrez Gene, which potentially
differs significantly from the version used in the anno-
tation process. Secondly, Entrez Gene contains a large
number of terms that overlap with frequent words of
the general vocabulary (such as was, and, this), tak-
ing care of which requires considerable additional effort,
such as manually creating blacklists. Furthermore, omit-
ting Entrez Gene has been suggested earlier by other
scholars (e.g. ([24], p. 8)).
We associated each (sub-)ontology with a single entity
type. For NCBI Taxonomy, we regarded species and
higher taxonomic ranks (genus, order, phylum etc.) from
both cellular organisms and viruses to a common entity
type organism. For the Gene Ontology, we followed
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 3 of 14
CRAFTs division into physical and non-physical entities,
i.e. we distinguished cellular components from biologi-
cal processes/molecular functions.
OGER
The OntoGene group has developed an approach for
biomedical entity recognition based on dictionary lookup
and flexible matching. Their approach has been used in
several competitive evaluations of biomedical text mining
technologies, often obtaining top-ranked results [2528].
Recently, the core parts of the pipeline have been imple-
mented in a more efficient framework using Python [29]
and are now developed under the name OGER (Onto-
Genes Entity Recognizer). These improvements showed
to be effective in the BioCreative V.5 shared task [30]: in
the technical interoperability and performance of annota-
tion servers (TIPS) task, our system achieved best results
in four out of six evaluation metrics [31]. In the TIPS task,
participants were asked to provide an on-line service for
on-the-fly annotation of biomedical entities in given doc-
uments. The tasks goal was to investigate the feasibility
of installing an inter-institutional annotation cluster (con-
trolled by a biomedical annotation metaserver), therefore
the evaluation was based on processing time and avail-
ability of the participating systems [32]. OGER achieved
single first place in the speed measures (average response
time, mean time per document volume) and shared first
place in the stability measures (mean time between fail-
ures, mean time to repair).
OGER offers a flexible interface for performing
dictionary-based NER. It accepts a range of input formats,
e.g. PubMed Central full-text XML, gzip-compressed
chunks of Medline abstracts as made available for down-
load by PubMed, BioC XML [33], or simply plain text.
It provides the annotated terms along with the corre-
sponding identifiers either in a simple tab-separated text
file, in brats standoff format [34], in BioC XML, or in
a number of other, less common formats. It allows for
easily plugging in additional components, such as alterna-
tive NLP preprocessing methods or postfiltering routines.
We run an instance of OGER as a permanent web ser-
vice which is accessible through an API and a web user
interface [35].
For term matching, we used the terminology resources
included in the CRAFT corpus. We extracted the relevant
information from the various sources and converted it
into a unified, non-hierarchical format, in order for it to be
accepted by the annotation pipeline. For the format con-
version, we used the back-end software of the Bio Term
Hub [36], which is a meta-resource for biomedical ter-
minological resources. Through a web interface [37], any
user can obtain a customized dictionary, which is com-
piled on the fly from a number of curated, openly available
terminology databases.
By concatenating the selected seven terminologies, we
obtained a dictionary with 1.26 million terms pointing to
864,000 concept identifiers. Based on preliminary tests,
we removed all entries with terms shorter than three char-
acters or terms consisting of digits only; this reduced the
number of entries by less than 0.1%. In OGER, the entries
of the term dictionary were then preprocessed in the same
way as the documents with respect to tokenization, stem-
ming, and case sensitivity, as described below. Finally, the
input documents were compared to the dictionary with an
exact-match strategy.
OGER was configured to have a moderate bias towards
recall, at the cost of precision. We chose this strategy, tai-
lored to a greater number of false positives (i.e. lower pre-
cision) but less false negatives (i.e. greater recall), because
in the overall architecture OGERs output is filtered or
used as a feature among many in the subsequent step,
so we let the subsequent ML step decide which of the
annotations produced by the dictionary step are actually
useful.
After sentence splitting, the input documents were tok-
enized with a simple method based on character class:
any contiguous sequence of either alphabetical or numer-
ical characters was considered a token, whereas any other
characters (punctuation and whitespace) were considered
token boundaries and were ignored during the dictionary
look-up. This lossy tokenization already has a normal-
izing effect, in that it collapses spelling variants which
arise from inconsistent use of punctuation symbols. For
example, the variants SRC 1, SRC-1, and SRC1 were
all conflated to the two-token sequence SRC, 1. In
a small evaluation on the training set, we verified that
this results in a moderate improvement of overall recall
(+ 2.7 percentage points) and worked particularly well for
sequences (+ 3.5) and proteins (+ 8.4), while the effect
on precision was negative, but smaller (? 2.1). A simi-
lar approach is described in [38], where the authors refer
to it as regularization. All tokens were then converted
to lowercase, except for acronyms that collide with a
word from general language (e.g. WAS). We enforced
a case-sensitive match in these cases by using a list of
the most frequent English words. As a further normal-
ization step, Greek letters were expanded to their letter
name in Latin spelling, e.g. ? ? alpha. Since both
spellings are common in the biomedical literature, con-
verting all occurrences to a canonical form allowed us
to increase the number of matches. Finally, we applied
stemming to all tokens except for the acronyms, using
NLTKs [39] implementation of the Lancaster stemmer
[40]. We favored this algorithm over the more widely used
Porter stemmer because of its greater strength, i.e. its
higher amount of conflations produced, which increases
the overlap between the dictionary and the documents, in
line with our aim for higher recall.
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 4 of 14
As a tweaking step, we fine-tuned the above default con-
figuration for the individual entity types. Based on their
respective coverage in the training set, we adjusted the
parameters as follows:
proteins no stemming
sequences less strict acronym filter (more cases of case-
insensitive matching)
cells always case-insensitive matching (even for
acronyms)
cellular components always case-insensitive matching
Distiller
The Distiller framework [41] is an open source project
which aims to build a flexible, extensible system for a
variety of natural language processing tasks [14].
The main focus of the Distiller framework is the task
of Automatic Keyphrase Extraction (herein AKE), which
is the process of extracting relevant phrases from a docu-
ment [42]. AKE is quite different from NER, as while the
former is interested in finding the small set of the most rel-
evant terms in a document, the latter is focused on finding
all the terms of some selected types.
AKE can be performed as both an unsupervised and
supervised task, and Distiller actually has its roots in an
unsupervised approach [43]. However, current state-of-
the-art systems use mostly a supervised approach [44],
so the framework offers the possibility to use such tech-
niques as well.
Supervised AKE is performed using a standard super-
vised machine-learning pipeline. The first step is generat-
ing the candidate keyphrases, using their part-of-speech
tags to select certain phrasal patterns. Then, the candidate
keyphrases are assigned some features, using statistical
[42], linguistic [45], or semantic [46] knowledge. Finally, a
machine learning algorithm is trained and then evaluated
over a set of documents associated with human-assigned
keyphrases.
Ensemble system
In order to integrate Distiller with OGER together and
build an effective NER system, the candidate generation
phase of the former system has been replaced by OGERs
output. In fact, the original candidate generation phase
of the Distiller has to be completely discarded, because it
is tailored to recognizing generic noun phrases, which
might not even be technical terms.
For this reason, in this work we follow and extend the
same process we presented in [11], so the entity extraction
pipeline is structured as follows:
1. Given an input document, OGER matches all the
biomedical terms that appear in at least one of the
selected ontologies;
2. Distiller receives the terms selected by OGER and
assigns them some features, preparing them to be
processed by a machine learning system;
3. A machine learning system, trained on the CRAFT
corpus, selects the relevant entities in the document
using the information generated in the previous steps.
The machine learning algorithms used are neural net-
works (NN), as they were the best performing algorithm
in [11], and Conditional Random Fields (CRF), as they
are currently considered the state-of-the-art algorithm,
as pointed out in [24]. The architecture is slightly dif-
ferent for the two algorithms: In the NN case, Distiller
acts as a filter on OGERs output, i.e. it performs a
binary accept/reject classification for each entity candi-
date. In contrast, the CRF-based version considers any
token sequence in the text as an entity candidate, using
OGERs annotations only as a feature amongmany. Hence,
the output of the NN pipeline is always a subset of OGERs
output, whereas this restriction does not hold for the CRF
pipeline.
For both algorithms, training is performed using 10-fold
cross validation. As in [11], in the present paper we split
the corpus for training/testing purposes, so the evalua-
tion is performed on 20 documents only. However, there
are two crucial differences from [11]. Fist, in our previ-
ous work we trained a binary classifier, i.e. a single model
to detect all entity types, while here we train a separate
model for each entity type present in the CRAFT cor-
pus. Second, here we evaluate our system not only on the
named entity recognition task, i.e. considering the spans
and entity types produced by our ensemble system, but
we also evaluate our system on concept recognition, i.e.
taking into account the concept identifiers produced by
OGER.
Features
Due to the differences of the algorithms, we used slightly
different features to train NNs and CRFs. In fact, the
main difference between neural networks and condi-
tional random fields features is that the former works
well with both n-ary features and continuous-valued fea-
tures, while the latter works better when using n-ary
features (labels) only. For this reason, some features
are implemented as continuous valued in NN and as
binary labels in CRF, to adapt them to the algorithm
used. For example, while with NN we used a counter
to determine how many uppercase characters are con-
tained in a term, the corresponding CRF feature would
be a binary label indicating the presence of uppercase
characters in the token. In any case, since the library
used to train CRF supports the use of numerical valued
features as well, we also tried to use the exact same fea-
tures used in NNs training for the CRFs training, but the
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 5 of 14
resulting performance was lower than using just binary
labels.
The features and the configuration used to train our
algorithms are listed in Table 1. The features we selected
are derived from [11] where, after a process of feature
selection, the best performing feature set used informa-
tion about the shape of the token, i.e. the number of capital
letters, the number of uppercase characters, and so on,
plus some domain knowledge, i.e. features about the affixes
of the words and the presence of Greek letters inside
them.
In detail, affixes (i.e. prefixes and suffixes) are partic-
ularly useful in the biomedical domain because they are
often associated with a particular meaning. For example,
chemical compounds often end with -ide, like sodium
chloride (the common table salt), diseases often end with
-itis or -pathy (like arthritis or cardiopathy), and
so on.
In order to implement this feature, we used the Bio Term
Hub resource [36], and we generated four affixes lists, one
each for two- and three-character prefixes and suffixes
appearing in the following ontologies:
 Cellosaurus [47], developed by the Swiss Institute of
Bioinformatics;
 Chemical compounds and diseases found in the
Comparative Toxicogenomics Database (CTD) [48],
developed by the North Carolina State University;
 Entrez Gene [19], developed by the US National
Center for Biotechnology Information;
 Medical Subject Headings (MeSH) [49], developed
also by the US National Center for Biotechnology
Information (restricted to the subtrees organisms,
diseases, and chemicals and drugs);
 reviewed records from the Universal Protein
Resource (Swiss-Prot) [50], developed by the joint
USA-EU-Switzerland consortium UniProt.
Table 1 Feature sets: features used by the NN and CRF (see the Features section for details)
Neural network Conditional random fields
Implementation
Software R [67], nnet library CRFSuite [68]
Model parameters 1 hidden layer of size
2 × (number of input features), softmax out-
put layer
Training algorithm: averaged
perceptron, default epsilon, 2 words window
Input n-grams selected by OGER Single tokens
Features
Candidate character count Count 
Candidate is all uppercase Label yes/no Label yes/no
Candidate is all lowercase Label yes/no Label yes/no
Candidate contains Greek (i.e. alpha, ? ) Label yes/no Label yes/no
Candidate contains dashes (-) Count Label yes/no
Candidate contains numbers Count Label yes/no
Candidate ends with a number Label yes/no Label yes/no
Candidate contains capital letter not in first position Label yes/no Label yes/no
Candidate contains lowercase characters Count Label yes/no
Candidate contains uppercase characters Count Label yes/no
Candidate contains spaces Count Label yes/no
Candidate contains symbols Count Label yes/no
2-3 character affixes appearing in an ontology in [36] Normalized frequency Label yes/no
Candidate is symbol  Label yes/no
Candidates part-of-speech  Yes, using [69]
Candidates stem  Yes, using [70]
Candidate pre-selected by OGER  Yes (see the Features section)
Total features 36 About 2.8 million
Tagging speed (on an Intel 4720HQ CPU) 1286 tokens/sec 632 tokens/sec
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 6 of 14
To weigh affixes based on their frequency, each affix a
from a terminological resourceD is assigned a normalized
score s ?[0, 1] computed in this way:
s(a,D) = freq(a,D)max({freq(a1,D) . . . freq(a|D|,D)})
where freq(a,D) is the frequency of an affix a in D. Two
weights (one prefix and one suffix) for each ontology con-
tained in BioTermHub are used as input for the neural net-
works, while for CRF we use only a binary flag indicating
whether the affix can be found in the selected ontology. As
mentioned before, we chose to use affix information dif-
ferently for CRF and NN, since after testing both binary
features and weighted features on both algorithms, we
determined that the former approach performed better on
CRF and the latter on NN.
For CRF, we also tried to add prefixes and suffixes of
each token as features, similarly to what we found in
[7]. We trained several models by adding affixes of two,
three, four, and five characters to each feature set, with
the model increasing from ?2.8 million features to ?11.6
million features, but we found no significant improve-
ment in performance with respect to using our dictionary
approach only.
Unfortunately, this approach would not have been fea-
sible when using neural networks, because it would have
generated a very large set of additional features, making
the network practically impossible to train.
Finally, it is also worth noting another fundamental dif-
ference between the NN and the CRF approach.While the
NN receives as input only the tokens selected by OGER,
the input of the CRF is composed by the whole tokenized
document, and the selection of a token as a potential entity
by OGER is used as a feature, as pointed out in Table 1.
For this reason, CRFs are able to recognize entities that are
not recognized by OGER, while NNs cannot do this, since
they know only the portions of the document selected by
the dictionary-based step, and therefore act simply as a
filter on OGERs output.
Test hardware
We ran the OGER and Distiller systems on a computer
equipped with an Intel i7 4720HQ quad core processor
running at 2,6 GHz, 16 GB RAM and a Crucial M.2 M550
SSD. The operating system was Ubuntu 16.04 LTS.
OGER obtained a performance of 5994 tokens/second
when running in single thread mode, while the Distiller
system processed 1286 tokens/second when using NN
and 632 tokens/second when using CRF. If necessary, the
OGER system can be parallelized in a straightforward
manner. Its efficiency is demonstrated also by the excel-
lent result obtained in the recent TIPS challenge [31] (see
the OGER section).
Results
We examined the performance of our systems in two sep-
arate evaluations. First we evaluated the performance of
NER proper, i.e. we regarded only offset spans and the
(coarse) entity type of each annotation produced by each
system, ignoring concept identifiers. This is a direct con-
tinuation of the work presented in [11]. Subsequently,
we describe the results of a preliminary concept recog-
nition (CR) evaluation. To this end, we augmented the
ML-based output with concept identifiers taken from the
dictionary-based pre-annotations, which enabled us to
draw a fair comparison to previous work in CR on the
CRAFT corpus.
Named entity recognition
We present the results obtained in Table 2. They are
compared with the previous version of our system, as
described in [11].
The best recall is obtained by the new version of the
OGER system, which obtains an overall performance
higher than the previous OGER/Distiller pipeline. The
66% recall score obtained by the system offers an 11%
improvement over the previous version but, perhaps more
importantly, the precision of the annotations is much
higher, almost doubling the precision from 34 to 59%.
The higher quality of the annotations is reflected by
the fact that the neural network pipeline, which received
only minor improvements from the previous version, now
displays a less significant drop in recall, with a score of
60%, just six point less than OGER. In the version pre-
sented in [11], the recall drop when adding the machine
learning filtering stage was much higher (18%). The neu-
ral network version improves the precision score as well,
with a 1% increase. This result brings the overall F1-Score
to 0.70, which makes this version of the system the best
performing one.
On the other hand, the combined OGER-CRF pipeline
obtains a somewhat underwhelming performance, with
lower precision, recall and thus F1-score when compared
Table 2 Comparison of the NER performance obtained in this
paper with the previous version of the system [11]
System Precision Recall F1
OGER 2016 0.34 0.55 0.42
OGER+Distiller 2016 0.85 0.37 0.51
OGER 0.59 0.66 0.62
OGER+Distiller NN 0.86 0.60 0.70
OGER+Distiller CRF 0.69 0.49 0.58
OGER+Distiller Mixed 0.87 0.63 0.73
Distiller CRF 0.71 0.47 0.58
The best values are highlighted in boldface
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 7 of 14
to the NN. Still, bothmodels perform better than the older
version, corroborating the validity of our approach.
Running the Distiller with a CRF-trained model without
OGERs preprocessing step, i.e. without instructing the
CRF which tokens have been marked as entities accord-
ing to OGER, leads to an acceptable result, with higher
precision but lower recall (see Table 2, this pipeline is
called Distiller CRF). This is unsurprising, since CRFs
are known to be good systems for named entity recogni-
tion; still, using the CRF without using OGERs dictionary
would imply the important caveat that the system would
not be able to associate terms with concept identifiers, but
only to recognize their presence and type.
In Tables 3, 4, and 5 we analyze the performance of
our system for the individual entity types. Here we con-
sider both a strict evaluation, which considers correct only
annotations where reference and system spans match per-
fectly, and a more lenient evaluation scheme, where we
consider a system annotation partially correct if it over-
laps just partially with a CRAFT annotation. To be pre-
cise, we used the average measure as defined by GATEs
Annotation Diff Tool [51], which is the mean of preci-
sion/recall/F1 in the strict and lenient measures. When
a predicted annotation overlaps partially with a reference
annotation, it counts both as a false positive (FP) and a
false negative (FN) in the strict measure. In the lenient
measure, this corresponds to a true positive (TP). Conse-
quently, in the average measure, a partial match is counted
as 12 TP,
1
2 FP, and
1
2 FN, such that the denominators
in precision and recall (TP+FP and TP+FN, respectively)
remain constant across all three measures. If more than
one predicted annotation overlaps with the same refer-
ence annotation, it is counted as a partial match only once;
additional predictions are counted as false positives. The
same holds for a predicted annotation overlapping with
multiple reference annotations, which contribute to the
false-negative count.
It is interesting to see that the NN model obtains very
good F1-Scores (> 70%) on almost all entity types, but
has some problems to identify biological processes and
molecular functions. Nevertheless, the problems on this
category do not hinder the performance of the general
model, which is able to obtain the best precision on all cat-
egories except for cells. Unfortunately, an almost optimal
precision is not always followed by a good recall, like in
the previously mentioned case of biological processes and
molecular functions, where we have 78% precision and
only 22% recall.
The CRF model achieves good or acceptable F1-Scores
for the majority of the entity types, but appears to
have trouble with correctly identifying chemicals and 
severely  sequences. In fact, the scores for these two
entity types are so low that the overall score for the CRF
pipeline is lower than OGER, even though CRF clearly
beats OGER in five out of seven entity types. This par-
tially explains the counterintuitive finding that a plain
dictionary-based system achieves better results than the
CRF-based system.
Using these results, we built a mixed system com-
posed of the best performing models, i.e. using CRFs for
cells, biological processes and molecular functions, and
cellular components, and NNs for the other entity types.
This model, labeled OGER+Distiller Mixed in Table 2, is
obviously the best possible system, with a 3% increase in
F1-score when compared to the NN approach; however,
this is a purely academic exercise, since in practice it is
very difficult to combine the NN and the CRF models due
to their very different nature.
Finally, the assumption that CRFs are able to recognize
entities which are not detected by OGER is evident by
looking at the recall figures in Table 4. In fact, consid-
ering cells, biological processes and molecular functions,
and cellular components, we see that the CRF pipeline
improved OGERs recall figures by 11%, 20% and 7%,
respectively. This happens because for CRF we adopt a
token-by-token process, where a correct item can be a
token annotated by the CRAFT annotators but not by
OGER, while in the NN pipeline the correct samples are
Table 3 Per-entity-type breakdown of the precision scores obtained by the different pipelines
Evaluation method: strict Evaluation method: average
Entity type OG OG+NN OG+CRF OG OG+NN OG+CRF
All 0.59 0.86 0.69 0.61 0.89 0.80
Chemicals 0.44 0.89 0.48 0.45 0.89 0.50
Cells 0.88 0.88 0.95 0.93 0.94 0.96
Biological processes/molecular functions 0.39 0.78 0.68 0.45 0.88 0.73
Cellular components 0.51 0.91 0.87 0.52 0.92 0.90
Organisms 0.29 0.98 0.82 0.29 0.98 0.83
Proteins 0.49 0.86 0.74 0.50 0.87 0.80
Sequences 0.46 0.89 0.23 0.48 0.91 0.27
The best values are highlighted in boldface
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 8 of 14
Table 4 Per-entity-type breakdown of the recall scores obtained by the different pipelines
Evaluation method: strict Evaluation method: average
Entity type OG OG+NN OG+CRF OG OG+NN OG+CRF
All 0.66 0.60 0.50 0.69 0.61 0.58
Chemicals 0.73 0.68 0.26 0.75 0.68 0.27
Cells 0.77 0.67 0.88 0.77 0.71 0.89
Biological processes/molecular functions 0.25 0.22 0.45 0.29 0.25 0.49
Cellular components 0.60 0.56 0.67 0.61 0.58 0.69
Organisms 0.92 0.91 0.91 0.92 0.91 0.92
Proteins 0.84 0.75 0.66 0.85 0.75 0.72
Sequences 0.67 0.64 0.08 0.69 0.65 0.09
The best values are highlighted in boldface
n-grams annotated both by OGER and the CRAFT cor-
pus annotators. Unsurprisingly, these three entity types
are the ones where the CRF obtains an overall F1-score
higher than NN.
Concept recognition
The NER pipelines presented in this work are designed
to perform entity annotation in terms of identifying rel-
evant text regions without assigning identifiers to the
recognized terms. Nonetheless, OGER performs concept
recognition by default, and in the experiments reported
above, the identifiers produced were simply ignored in
the downstream processing and evaluation steps. For that
reason, we decided to verify the potential of using the
pipelines in a CR setting by carrying out an additional
experiment.
We chose a simple strategy to reintroduce the concept
identifiers provided by OGER into the output of the ML
systems. This step was as straightforward as joining the
corresponding annotations in OGERs and Distillers out-
put. For the combined OGER-CRF pipeline, this meant
that CRF annotations were removed if no matching entry
was found in OGERs output.
We did not resolve ambiguous annotations; instead,
multiple identifiers could be returned for the same span.
While having no disambiguation at all is arguably a defi-
ciency for a CR system, it is not imperative that each
and every ambiguity is reduced to a single choice. This is
particularly true when evaluating against CRAFT, which
contains a number of reference annotations with multi-
ple concept identifiers. For example, in PMID: 16504143,
PMCID: 1420314, the term fish (occurring in the last
paragraph of the Discussion section) is assigned six
different taxonomic ranks.
Table 6 shows the performance of the described sys-
tems in a CR evaluation against CRAFT, as well as the
results for a number of other systems as reported by
Tseytlin et al. [24], who carried out a series of experiments
using the same dataset. All figures reflect the average
evaluation method as described in the previous section;
however, at least for our systems, the difference between
strict and average evaluation is so small that the numbers
are the same at the given level of precision. Please note
that the results reported by [24] are not perfectly com-
parable to the ones we obtained, since the former were
tested on the whole CRAFT corpus, while our approach
was evaluated on 20 documents only (since we used the
remaining documents to train our system), as described in
the Ensemble system section. Still, the comparison
shows that even a relatively simple approach is sufficient
Table 5 Per-entity-type breakdown of the F1 scores obtained by the different pipelines
Evaluation method: strict Evaluation method: average
Entity type OG OG+NN OG+CRF OG OG+NN OG+CRF
All 0.62 0.70 0.58 0.65 0.72 0.67
Chemicals 0.55 0.77 0.34 0.56 0.77 0.35
Cells 0.80 0.76 0.91 0.84 0.81 0.92
Biological processes/molecular functions 0.30 0.35 0.54 0.35 0.39 0.58
Cellular components 0.55 0.70 0.75 0.56 0.71 0.78
Organisms 0.44 0.94 0.87 0.45 0.94 0.88
Proteins 0.62 0.80 0.70 0.63 0.80 0.76
Sequences 0.54 0.75 0.12 0.57 0.76 0.13
The best values are highlighted in boldface
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 9 of 14
Table 6 Performance of the presented systems in a CR evaluation, compared to results reported in [24]
System Precision Recall F1
OGER 0.32 0.52 0.40
OGER+Distiller NN 0.51 0.49 0.50
OGER+Distiller CRF 0.49 0.29 0.37
MMTx 0.43 0.40 0.42
MGrep 0.48 0.12 0.19
Concept Mapper 0.48 0.34 0.40
cTakes Dictionary Lookup 0.51 0.43 0.47
cTakes Fast Lookup 0.41 0.40 0.41
NOBLE Coder 0.44 0.43 0.43
Please note that, as stated in the Concept recognition section, the systems described in [24] are evaluated on the whole corpus, while we use 20 documents for testing and
the remainder for training. The best values are highlighted in boldface
to transform our NER pipeline into a CR system with
reasonable quality. This is particularly true for the OGER-
NN configuration, where both precision and recall are as
good as or better than the figures for all the reported
systems.
Table 7 shows the CR performance on all the considered
entity types and for all the configurations of the system.
Here we see that applying NN filtering after the dictio-
nary matching results in an increment of precision in all
cases except for cells, where the drop is negligible. As for
the choice of ML algorithm, the NN pipeline is almost
always the best performing model, winning in all entity
types except for cells in terms of F1-score. However, the
main difference between NNs and CRFs is that the for-
mer retains a good recall, with the worst drop of just 9%
for proteins, while the latter shows a considerable drop
in recall in many categories. In particular, while the CRF
precision scores are generally good, if not almost opti-
mal for some categories, the results for chemicals and
sequences are very bad in terms of recall, hindering the
general performance of the system.
Discussion
In this section, we analyze the results of the experiments
presented in the Results section and we contextualize
them with related work.
Error analysis
NN pipeline
Since the NN output depends heavily on OGERs input,
many of its mistakes are caused by the quality of the dic-
tionary matching. While the precision reached by this
model in filtering out non-interesting terms is quite high
at 85%, the majority of the errors of the NN pipelines con-
sist in generic terms, like verbs, adverbs, and so on, that
the neural network is not able to filter out. For example,
in three documents, the word error itself is erroneously
marked as an entity. Moreover, just using regular expres-
sions to detect common suffixes and manually inspecting
the results, we see that about 5% of the errors are adjec-
tives, about 5% are adverbs, and about 9% are verbs in
-ing form. Another typical error is constituted by common
substantives marked wrongly as entities. For example, the
Table 7 Per-entity-type breakdown of Precision, Recall, and F1 obtained by the different pipelines in the CR evaluation
Precision Recall F1
Entity type OG OG+ OG+ OG OG+ OG+ OG OG+ OG+
NN CRF NN CRF NN CRF
All 0.32 0.51 0.49 0.52 0.49 0.29 0.40 0.50 0.37
Chemicals 0.28 0.59 0.93 0.61 0.57 0.19 0.39 0.58 0.32
Cells 0.88 0.87 0.98 0.72 0.66 0.68 0.79 0.75 0.81
Biological processes/molecular functions 0.35 0.72 0.73 0.19 0.17 0.05 0.25 0.27 0.10
Cellular comp. 0.49 0.87 0.89 0.59 0.56 0.52 0.54 0.68 0.65
Organisms 0.16 0.49 0.47 0.71 0.70 0.67 0.26 0.58 0.55
Proteins 0.45 0.84 0.91 0.83 0.74 0.64 0.59 0.79 0.75
Sequences 0.27 0.59 0.37 0.53 0.51 0.06 0.36 0.54 0.10
The best values are highlighted in boldface
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 10 of 14
word region by itself makes 4% of the errors. These
errors can probably be easily eliminated by using dedi-
cated part-of-speech features, and will eventually be tack-
led in future versions of the system.
The other most common category of errors are anno-
tations that are found by OGER but not present in
the CRAFT corpus. For example, in document PMID:
15917436, PMCID: 1140370, the genes M13mp18 and
M13mp19 are detected by OGER but not in the CRAFT
corpus. These errors are hard to catch with dedicated
features and, on the other hand, can potentially deliver
useful information, since the annotation is conceptually
correct, so we do not consider them as a highly critical
problem of our system. Nevertheless, these errors have
a high impact when evaluating the model. For example,
in document PMID: 16870721, PMCID: 1540739, the NN
pipeline selects the protein p53, which is not present
in the ontologies of the CRAFT corpus, but since this
particular protein is central in the paper, it is repeated
many times throughout the document, and its selection by
the NN pipelines accounts for about 4% of the total false
positives of the pipeline.
The false negatives of the NN pipeline depend largely
on the false negatives of OGER, since it cannot select
anything that has not been selected by OGER. Still, the
NN can theoretically remove correct OGER selections,
and analyzing the results of our system we see that this
happens in practice, too. In fact, the number of the false
negatives of the NN increases of about 20%, as expected
by the lower recall of the NN pipeline when compared
to OGERs output. These false negatives are mostly short
strings, like BLM, CD21, p34, with no apparent con-
nection with the category of the word: 43% of the false
negatives introduced by the NN are in fact words of 3 or
4 characters (shorter terms had been removed from the
dictionary initially, as described in the OGER section).
CRF pipeline
One of the strengths of the model, i.e. detecting enti-
ties that are not annotated as such, is also a potential
weakness while evaluating its precision. For example, in
document PMID: 17696610, PMCID: 1941754, the model
produces the annotation Mei1, which is the name of a
gene/protein. This word is not annotated in the CRAFT
corpus with respect to the Protein Ontology, because at
the time of CRAFTs annotation, the Protein Ontology
did not include Mei1 (recent versions of the ontology do
include it). CRAFT has annotations for Mei1 with respect
to Entrez Gene, but this resource was ignored in our eval-
uation, as is described in the CRAFT corpus section.
Differently from the false positives produced by the neu-
ral network pipeline described in the previous section, this
time the concept is not annotated by OGER either. How-
ever, the CRF identifies that Mei1 is a protein: this is again
conceptually correct, but still a mistake for the sake of
evaluating the performance of the system on the CRAFT
corpus.
We argue that, while these annotations hinder the
evaluation performance of the models, they are actually
desirable. This way, in fact, we are able to annotate what
OGER is not able to recognize due to shortcomings in
the dictionary used, thus providing information which we
think is valuable for the user. Moreover, the ability of the
model to select entities that are not yet present in a knowl-
edge base is one of the desirable aspects of using a ML
approach, as pointed out in the Background section.
In our example, Mei1 could have been a recently dis-
covered protein which then is not yet present in the
ontologies used; we argue that the ability of recognizing it
as an entity and to classify its type, even without the abil-
ity of linking it to an actual knowledge base, would be a
positive feature of our system.
On the other hand, the CRF model makes many errors
where it includes punctuation in the annotation, like com-
mas, parentheses, and so on. The relatively high frequency
of this error is evident when we consider that the CRF
pipeline benefits from the average evaluation (which con-
siders partial matches as partially correct) by improving
the precision score by 20%, while the NN pipeline shows a
mere 3% improvement.
This model also fails to recognize certain words or their
derivatives. For example, about 5% of the false negatives
of the CRF pipeline consist in the failure of recognizing
the word gene or words with the same root (like genes,
genetic, genome, etc.); another 2% is due to the failure
of recognizing the word expression or similar. More-
over, many acronyms are not recognized: in the document
PMID: 15061865, PMCID: 400732, the model fails to rec-
ognize every reference to the acronym D2R (Dopamine
D2 receptor), weighing about 2% of the total false nega-
tives. The same holds for the acronyms or short names
(MLH1 or MLH3, PPAR?, PPAR?, etc.), with about
15% of the false negatives being acronyms.
A particular case are annotations where the annota-
tion span is wrong. For example, in document PMID:
17425782, PMCID: 1858683, the pygopus genes Pygo1
and Pygo2 often appear as Pygo1/Pygo2; in the
CRAFT corpus, the single gene names are annotated sepa-
rately, while the CRF annotated the whole string. The gene
is also often mentioned as Pygo1 gene, and here again
in the CRAFT corpus it is annotated as Pygo1, while the
CRF selects the whole phrase.
OGER
False negatives in the OGER output have an impact on
the entire system which is much higher than the one of
false positives. This is particularly true in combination
with the NN postfilter, since annotations missing from
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 11 of 14
the beginning remain unreachable in any subsequent step.
In combination with the CRF system, where the OGER
annotations are seen as one feature among many, the
effect is less pronounced, since initial false positives might
be corrected eventually; however, OGERs false negatives
still have a strong influence on the CRFs decision. There-
fore, this error analysis focuses on annotations that were
missed by the OGER system.
A frequent cause for missing annotations are synonyms
that are not covered by the dictionaries. For example,
antibody is used in document PMID: 12925238, PMCID:
194730 for the more specific term immunoglobulin com-
plex, while the Gene Ontology does not list antibody
as a name for this (or any other) concept. Missing syn-
onyms occurred with all entity types, and they were the
predominant source of error for proteins (estimated to be
more than one third of all misses). Sometimes, terms are
abbreviated in text, such as olfactory receptor in docu-
ment PMID: 14611657, PMCID: 329117, which should be
olfactory receptor activity according to the dictionary.
This was seen frequently with cellular components (more
than 25%), organisms, biological processes and molecu-
lar functions (less than 10%), and proteins (around 5%).
Another cause of misses are splitting and reordering of
multi-word terms, as is both illustrated with gene expres-
sion, which is rephrased as expression of [. . . ] gene in
the same document. Along with hyperonymy (see the
antibody example above), splitting and reordering con-
tributed considerably to the low recall we obtained for
biological processes and molecular functions. Linguistic
variation at the level of morphology was another source
of mismatch between the terminologies and the texts.
Derivation (changes in part-of-speech, such as mam-
mal/mammalian, nucleus/nuclear, gene/genetic)
was the most common problem for organisms (more than
25%), and also occurred among cellular components and
sequences (less than 10%). Likewise, plural forms were not
always mapped correctly to their singular forms listed in
the dictionaries, especially in the case of acronyms (such
as cDNAs), where stemming was disabled.
Some instances are very close misses. For example,
given the dictionary spelling PPAR-delta, OGER was
unable to capture PPAR? in document PMID: 15328533,
PMCID: 509410. Even though the matching strategy was
designed to be robust against this kind of variation in
terms of punctuation and transliteration, this particular
case fell through the net. In order for two variants to be
considered equivalent, a hyphen may only be dropped if
it connects two strings with a different character class
(such as alphabetic and numeric, e.g. CRB-1 matches
CRB1). However, a transition from one script to another
(Latin?Greek) does not qualify as a token boundary on
a par with a hyphen  a design decision which should be
reconsidered.
Related work
The field of named entity recognition has decades of his-
tory, with early work focusing on extracting a single entity
type, such as protein names, from scientific papers [52].
Later on, some scholars started to introduce the use of ter-
minological resources as a starting point for solving this
problem [53].
The most recent state-of-the-art performance is
obtained by using supervised machine-learning based
systems. For extracting chemical names, [7] describes
how two CRF classifiers are trained on a corpus of journal
abstracts, using different features and model parameters.
The output of the two classifiers is merged in differ-
ent ways, attempting to combine the strengths of each
method, using a-posteriori knowledge (performance on a
test set) or the models own confidence. The approach in
[8] also tackles chemical name extraction with CRF, partly
using the same software basis as the previous one. The
system is trained in a semi-supervised setting by adding a
large collection of unlabeled abstracts and full-text doc-
uments. For tagging gene names, [54] describes another
supervised sequence-labeling approach. The output of a
CRF classifier is post-processed through graph propaga-
tion in order to account for unseen data occurring in the
test set.
There is growing interest in hybrid machine learning
and dictionary systems such as the one described in [10],
which obtains interesting performance on chemical entity
recognition in patent texts. The authors of [55] use differ-
ent approaches for different entity types (machine learn-
ing for chemical names, dictionary-based for organism
and assay entities); given the complementary application,
this is not a hybrid approach in the strict sense. A con-
trastive overview that also covers rule-based approaches
is given in [56]. While focusing on chemical entity recog-
nition, their findings are equally applicable to other entity
types.
In the field of entity linking, dictionary-based methods
are predominant, since the prediction of arbitrary identi-
fiers cannot be modeled in a generalized way. In [57], the
authors explore ways to improve established information
retrieval techniques formatching protein names and other
biochemical entities against ontological resources. Using
the CRAFT corpus, they measure the impact of case sen-
sitivity and the information gain of individual tokens in
multi-word terms. Another strategy borrowed from infor-
mation retrieval for increasing the coverage of recognized
entities is term expansion, i.e. indexing additional term
synonyms drawn from another source of knowledge. For
example, known orthologous relations can be exploited by
substituting a mentioned protein with an evolutionarily
and functionally equivalent protein from another species.
This is applied to the detection of protein interactions in
full text in [58]. The TaggerOne system [59] uses a joint
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 12 of 14
model for tackling NER and linking at the same time 
yet another example of a hybrid system that combines
machine learning and dictionaries. Using an annotated
corpus for training, the NER task is learned through a
semi-Markov model, which is an adaptation of Markov
models well-suited for detecting multi-word terms. For
linking, the extracted terms and the dictionary entries are
projected into the same vector space. Machine learning
also often plays an important role when it comes to entity
disambiguation. As an example, in [60] disambiguation
is addressed with word embeddings, which are used for
comparing the context of annotated terms with dictionary
definitions of the candidate concepts.
The Colorado Richly Annotated Full Text (CRAFT) cor-
pus [15, 61] has been built specifically for evaluating
these kinds of systems. In [62] (and, with some more
detail, in [63]), the authors used the corpus to evaluate
several concept recognition tools, showing how they per-
form on the individual entity types in the corpus. Later,
Tseytlin et al. [24] compared their ownNOBLE coder soft-
ware against other concept recognition algorithms, show-
ing a top F1-score of 0.44. Another system that makes
use of CRAFT for evaluation purposes is described in
[64]. In a series of experiments including all entity types
except for sequences, the authors were able to outperform
existing systems in terms of F1-score, achieving approx-
imately 93%, 75%, 78%, 60%, 54%, 44%, and 50% in an
exact-match evaluation (NER, no identifiers) for species,
cells, cellular components, chemicals, genes and proteins,
genes, and biological processes and molecular functions,
respectively.
Conclusions
In this paper, we have presented an efficient, high-quality
system for biomedical NER. We have shown that it can
be easily extended to produce concept identifiers, achiev-
ing state-of-the-art results in a Concept Recognition (CR)
evaluation. The presented system is a two-stage pipeline
with a dictionary-based pre-annotator (OGER) and a
machine-learning classifier (Distiller). In a contrastive
evaluation, we examined the respective quality of the pre-
annotations and two different classification approaches.
We evaluated both processing speed and annotation
quality in a series of in-domain experiments using the
CRAFT corpus. OGERs scalability and efficiency was
also demonstrated in the recently held TIPS task of the
BioCreative V.5 challenge. For the NER performance, we
compared a NN classifier, which acted as a postfilter of
the dictionary annotations, to a CRF classifier that used
OGERs output as a feature among many. While the CRF
pipeline showed interesting behavior by predicting terms
that were missing from OGERs dictionary, it was beaten
by the NN system for the majority of the entity types
and in the global evaluation, where the latter achieved a
precision of 86% at a recall of 60% (F1: 70%). By augment-
ing the classifier output with concept identifiers from the
pre-annotations, we were able to perform a CR evaluation.
Again, the NN system outperformed the CRF approach
with a precision of 51% at a recall of 49%, which is well in
line with scores reported in related literature.
As future work, we will consider to tackle specific ter-
minological categories where the classifiers fail to obtain
good performance, like biological processes and molecu-
lar functions. Moreover, OGER performance can still be
improved in terms of recall by allowing multi-word terms
to be reordered or shortened, or even split apart. Also,
recall can be increased bymeans of term expansion, e.g. by
collecting additional synonyms from other sources (termi-
nologies or corpora). In particular for biological processes
and molecular functions, these two strategies can be com-
bined to generate new term variants, as is shown in [65].
Furthermore, we consider improving the classification
performance of our system on the entity types where we
fail to obtain satisfactory results and, more importantly,
to develop a concept disambiguation stage that is able
to choose between the many concept IDs suggested by
OGER. Finally, in order to better compare our work to
other state-of-the-art systems, we will consider to extend
the evaluation to the full CRAFT corpus by using other
resources to train our system, or to other corpora, like the
ShARe corpus [66] used by [24].
Abbreviations
AKE: Automatic keyphrase extraction; API: Application programming interface;
CR: Concept recognition; CRAFT: Colorado richly annotated full text corpus;
CRF: Conditional random fields; ML: Machine learning; NCBI: the US National
Center for Biotechnology Information; NER: Named entity recognition; NLP:
Natural language processing; NN: Neural network; PMCID: PubMed Central ID;
PMID: PubMed ID; TIPS: Technical interoperability and performance of
annotation servers (track at the BioCreative V.5 challenge)
Acknowledgements
The authors wish to express their gratitude, for their helpful suggestions, to
the anonymous reviewers. We are also grateful to the organizers of the SMBM
2016 conference and the journal editors for giving us the opportunity to
expand that work in the present paper.
Funding
The research activity of the OntoGene/BioMeXT group at the University of
Zurich are supported by the Swiss National Science Foundation (grant
CR30I1_162758).
Availability of data andmaterials
The CRAFT corpus, used for testing purposes in this paper, is available from
http://bionlp-corpora.sourceforge.net/CRAFT/. The OGER system is available
upon request. A web service (web interface and REST API) providing
annotations of arbitrary text in multiple formats is available from https://pub.
cl.uzh.ch/projects/ontogene/oger/. The Distiller framework is an open source
software developed by the Artificial Intelligence Laboratory at the University of
Udine and it is available at https://github.com/ailab-uniud/distiller-CORE/.
Authors contributions
The work described in this paper was initiated during an academic visit by MB
at the OntoGene/BioMeXT group. LF adapted the OGER system to the needs
of the experiments described in this paper, and provided all the data required.
MB adapted the Distiller tool, integrating and testing machine learning
Basaldella et al. Journal of Biomedical Semantics  (2017) 8:51 Page 13 of 14
algorithms which had not yet been considered in that tool. CT and FR provided
advice, guidance, and support. All authors read and approved the paper.
Authors information
Marco Basaldella is a PhD student at the University of Udine. He is a member of
the Artificial Intelligence Laboratory and his research area is information
extraction, focusing in particular on keyphrase extraction and entity
recognition and on the applications of machine learning in the
aforementioned areas.
Lenz Furrer is a PhD student at the University of Zurich. Member of the
OntoGene/BioMeXT group, his research area is biomedical text mining, with a
focus on entity and relation extraction at a large scale.
Carlo Tasso is a full professor in Computer Science at the University of Udine.
He founded the Artificial Intelligence Laboratory in 1984 and has ever since
worked in the fields of artificial intelligence, information retrieval, content
personalization and intelligent knowledge-based systems.
Fabio Rinaldi is a senior researcher and principal investigator at the University
of Zurich and at the Swiss Institute of Bioinformatics. He is the leader of the
OntoGene/BioMeXT group (http://www.biomext.org/) which specializes in
biomedical text mining.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Università degli Studi di Udine, Via delle Scienze 208, 33100 Udine, Italy.
2University of Zurich, Institute of Computational Linguistics and Swiss Institute
of Bioinformatics, Andreasstrasse 15, CH-8050 Zürich, Switzerland.
Received: 17 February 2017 Accepted: 19 September 2017
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 
DOI 10.1186/s13326-017-0112-6
RESEARCH Open Access
SAFE: SPARQL Federation over RDF Data
Cubes with Access Control
Yasar Khan1, Muhammad Saleem2, Muntazir Mehdi1, Aidan Hogan3, Qaiser Mehmood1,
Dietrich Rebholz-Schuhmann1 and Ratnesh Sahay1*
Abstract
Background: Several query federation engines have been proposed for accessing public Linked Open Data sources.
However, in many domains, resources are sensitive and access to these resources is tightly controlled by stakeholders;
consequently, privacy is a major concern when federating queries over such datasets. In the Healthcare and Life
Sciences (HCLS) domain real-world datasets contain sensitive statistical information: strict ownership is granted to
individuals working in hospitals, research labs, clinical trial organisers, etc. Therefore, the legal and ethical concerns on
(i) preserving the anonymity of patients (or clinical subjects); and (ii) respecting data ownership through access
control; are key challenges faced by the data analytics community working within the HCLS domain. Likewise
statistical data play a key role in the domain, where the RDF Data Cube Vocabulary has been proposed as a standard
format to enable the exchange of such data. However, to the best of our knowledge, no existing approach has looked
to optimise federated queries over such statistical data.
Results: We present SAFE: a query federation engine that enables policy-aware access to sensitive statistical datasets
represented as RDF data cubes. SAFE is designed specifically to query statistical RDF data cubes in a distributed
setting, where access control is coupled with source selection, user profiles and their access rights. SAFE proposes a
join-aware source selection method that avoids wasteful requests to irrelevant and unauthorised data sources. In
order to preserve anonymity and enforce stricter access control, SAFEs indexing system does not hold any data
instancesit stores only predicates and endpoints. The resulting data summary has a significantly lower index
generation time and size compared to existing engines, which allows for faster updates when sources change.
Conclusions: We validate the performance of the system with experiments over real-world datasets provided by
three clinical organisations as well as legacy linked datasets. We show that SAFE enables granular graph-level access
control over distributed clinical RDF data cubes and efficiently reduces the source selection and overall query
execution time when compared with general-purpose SPARQL query federation engines in the targeted setting.
Keywords: SPARQL query federation, Data access policy, Linked Data, Healthcare and life sciences
Background
Inspired by the publication of hundreds of Linked
Datasets on the Web, researchers have been investigat-
ing federated querying techniques to enable access to
this decentralised content. Query federation aims to offer
clients a single-point-of-access through which distributed
data sources can be queried in unison. In the context of
Linked Data, various optimised query federation engines
*Correspondence: ratnesh.sahay@insight-centre.org
1Insight Centre for Data Analytics, NUIG, Galway, Ireland
Full list of author information is available at the end of the article
have been proposed that can federate multiple SPARQL
interfaces [17].
However, in the context of the Healthcare and Life
Sciences (HCLS) domain  where data-integration is
often likewise vital  the requirements for a federated
query engine can be rather more specialised. First, real-
world HCLS datasets contain sensitive information: strict
ownership is granted to individuals working in hospi-
tals, research labs, clinical trial organisers, etc. There-
fore, the legal and ethical concerns on (i) preserving
the anonymity of patients (or clinical subjects); and (ii)
respecting data ownership through access control; are
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 2 of 22
key challenges faced by the data analytics community
working within the HCLS domain [811]. Second, many
clinical datasets within the HCLS domain are composed
of numerical observations that form multi-dimensional
corpora of statistical information; for example, clinical
trial data are often composed of various dimensions of
patient attributes observed numerically along a temporal
dimension. Thus to draw conclusions about biomarkers,
side-effects of drugs, correlations within patient groups,
etc., requires applying statistical analyses to custom slices
of multi-dimensional data.
The current research is then motivated by the needs of
three clinical organisations: University Hospital Lausanne
(CHUV)1, Cyprus Institute of Neurology and Genetics
(CING)2, and ZEINCRO3. These organisations wish to
develop a platform for analysing clinical data across mul-
tiple clinical sites, which would allow for increasing the
total number of patients that are included in each analysis,
thus increasing the statistical power of conclusions related
to biomarkers, effectiveness and/or side-effects of drugs
or combinations of drugs, correlations between patient
groups, etc. The ultimate goal is to enable the collaborative
identification of new drugs and treatments while reducing
the high costs associated with clinical trials. With respect
to this motivating scenario, the aforementioned general
requirements apply: strict access control and adequate
methods to represent and query statistical data across
different sites are crucial aspects of this use-case.
Thus while query federation approaches enable the inte-
gration of data from multiple independent sources  as
required by our motivating use-case  traditional federa-
tion approaches have not considered methods to enforce
policy-based access control nor to deal specifically with
statistical data  as also required by our use-case and in
many other HCLS use-cases. Hence the central question
tackled by this paper is how existing federated approaches
can be modified to take both access control and statisti-
cal data into account while maintaining good performance
characteristics.
The key challenges for federated querying (in gener-
alised settings) are efficient source selection [7, 12] (i.e.,
determining which sources are (ir)relevant) and query
planning (i.e., determining an efficient query execution
strategy). These challenges change significantly when
access control and statistical data are taken into account.
More specifically, our hypothesis in this paper is that
access control can facilitate more selective source selec-
tion by reducing the amount of data to be processed at an
early stage of federated query processing, while statistical
data represented as data cubes follow certain principles
of locality that can be exploited to restrict the sources
selected, reducing overall query time.
With respect to integrating policy-based access control,
query federation engines often apply source selection at
the level of endpoints, whereas in a controlled environ-
ment, a user may only have access to certain informa-
tion within an endpoint. Adding an access control layer
to existing SPARQL query federation engines thus adds
unique challenges: (i) source selection should be granular
enough to enable effective access control, and (ii) it should
be policy-aware to avoid wasteful requests to unautho-
rised resources. However these challenges also present
an opportunity to optimise the source selection process
by increasing the granularity (in line with the access-
control policies) and the selectivity (by quickly filtering
resources to which users do not have access) of source
selection.
On the other hand, with respect to statistical data, a
common approach to representing such data is to use
data cubes. In fact, the RDF Data Cube Vocabulary [13]
has been standardised by the W3C precisely to enable
the integration of multi-dimensional (e.g., statistical) data
from multiple sites, as required by our HCLS use-case.
Thus the statistical data from our use-case can be rep-
resented using this vocabulary, where existing federation
engines could then be applied over the resulting sources
(like any RDF dataset). However, our hypothesis is that the
fixed structure of RDF data cubes implies certain local-
ity conditions that are exploitable by the federated engine
to optimise execution; in particular, we propose that spe-
cialised query planning for RDF Data Cubes can achieve
better performance than a general federated query engine
by taking such locality restrictions into account when
selecting the sources to which individual triple patterns
should be sent.
In this paper, we present SAFE: a SPARQL Query
Federation Engine that supports policy-based access
to sensitive statistical data in a federated setting. As
previously discussed, SAFE is motivated by the needs of
three clinical organisations in the context of an EU project
who wish to enable controlled federation over statistical
clinical data  such as data from clinical trials  owned
and hosted in situ by multiple clinical sites, represented
in the form of data cubes. However, the methods pro-
posed by SAFE can be used in other settings involving
data cubes outside of the HCLS domain (even for open
data).
Given that a wide variety of work has already been
conducted on SPARQL query federation engines [17],
it is important to highlight that our focus is on a
higher level than such works: our core hypothesis does
not relate to low-level join algorithms nor to commu-
nication protocols, for example, but rather focuses on
the level of specialised source-selection and query plan-
ning algorithms for access-controlled federated over data
cubes. Hence rather than propose and implement a fed-
erated query approach from scratch, we adapt a general-
purpose query federation engine (FedX [1]), which already
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 3 of 22
implements the low-level federation primitives that SAFE
requires.
More specifically, SAFE extends upon the FedX engine
[1] with two high-level novel contributions: (i) graph-
level source selection, which is required to implement
more granular access-control, and which is enabled by a
novel data-summary generation technique and associated
algorithm, (ii) optimisations for federated query process-
ing over statistical data that are represented using the
RDF Data Cube Vocabulary. With these modifications,
we show that when compared with FedX, HiBISCuS [14]
and SPLENDID [2], in such settings, SAFE can (i) support
more granular graph-level access control than possible
by simply layering access control on top of an existing
engine that uses endpoint-level source selection, and can
(ii) efficiently reduce the query execution time, the data
summary generation time, and the overall data summary
size, when federating specifically over RDF data cubes.
We perform experiments with datasets and queries taken
from our motivating use-case; to justify the claim that
SAFE can be applied in other statistical scenarios, we addi-
tionally perform experiments over RDF data cubes taken
from other domains.
In our initial work [15] SAFE has been evaluated against
the FedX engine [1] which it extends. In this article, we
extend our previous work by (i) improving the source
selection algorithm and providing extended analysis
thereof; (ii) developing an automated technique to gener-
ate data summaries (i.e., indexes) with lower relative sizes
compared to the raw data and faster generation time; (iii)
evaluating against two additional query federation engines
(HiBISCuS and SPLENDID); (iv) increasing the number of
queries and datasets for evaluation experiments. We high-
light that contribution (ii) is particularly important when
one considers updates to a source: while caching data
summaries in the federated query engine has the advan-
tage of enabling much more targeted source selection
while minimising runtime queries to (potentially) remote
sources, a disadvantage of using such summaries is the
additional overhead of having to keep them up to date
as the underlying sources change. This latter disadvan-
tage can be partially mitigated by using lightweight sum-
maries that are efficient to recompute over the updated
sources.
The rest of the paper is structured as follows:
Motivating scenario section discusses our motivational
scenario where data from different clinical locations need
to be queried and aggregated. Related work section
discusses background and related work. Methods
section presents the three main components of SAFE
query planning. Results section presents evaluation
of SAFE with respect to queries over various sta-
tistical datasets and Conclusions section concludes
our work.
Motivating scenario
As previously discussed, our work stems from the ambi-
tions of three clinical organisations  University Hospital
Lausanne (CHUV)4, Cyprus Institute of Neurology and
Genetics (CING)5, and ZEINCRO6  who are in the pro-
cess of developing a platform for analysing clinical data
across multiple clinical sites, allowing the reuse of remote
data in a controlled manner. We now discuss important
aspects of this use-case in the context of our research and
in the context of the requirements it places on the SAFE
federated engine.
Use of Linked Data
With their stated goal of integrating clinical data in a con-
trolled manner in mind, the three clinical organisations
mentioned are partners in the Linked2Safety EU project7.
The two main goals of the Linked2Safety project are (i)
the discovery of data about eligible patients  also known
as subject selection criteria  that can be recruited for
clinical trials from multiple clinical sites; and (ii) enabling
multi-centre epidemiological studies to facilitate better
understanding of relationships between pathological pro-
cesses, risk factors, adverse events, and between genotype
and phenotype. Although Linked Data technologies can
help enable multi-site interoperability and integration, the
community largely focuses on datasets that can be made
open to the public. In contrast, clinical data is often of
an extremely sensitive nature and there is often strict
legislation in place protecting the privacy of patients.
Legal and ethical implications of patient privacy
According to EU Data Protection Directive 95/46/EC8,
clinical studies that involve patient-specific information
must adhere to data-access restrictions that preserve
patient anonymity. More specifically, a data access mech-
anism must ensure that patient identity cannot be discov-
ered by direct or indirect means using the dataset [16].
Similar legislation exists in other jurisdictions. To avoid
sharing of individual patient records, the Linked2Safety
consortium has developed a data mining approach for
transforming raw clinical data into statistical summaries
that may aggregate (or indeed redact) multiple dimensions
of raw data as required for a particular application.
The result is a set of anonymised data cubes whose
dimensions correspond to insensitive clinical parameters
without personal information [17]. The resulting mul-
tidimensional output contains sufficient granularity to
quickly decide if the dataset is relevant for a given analy-
sis  e.g., to understand the scale and dimensions of the
data  and to perform high-level meta-analysis of aggre-
gated data. These data cubes are represented in a standard
format  namely RDF Data Cube vocabulary per the
recent W3C recommendation [13]  to enable interoper-
ability (e.g., use of controlled vocabularies for dimensions)
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 4 of 22
and to allow the later use of Linked Data publishing/access
methods.
Although the data considered are aggregated and do not
contain personal information about patients, deanonymi-
sationmay still be possible [18]: one cannot open a dataset
and fully guarantee that it will not (indirectly) compro-
mise patient anonymity [19]. Likewise, if a (bio)medical
dataset necessarily involves genetic data, there exist
identifying markers by which patients can be directly
deanonymised; thus genetic data can only be pseu-
doanonymised [16]. Given such issues, in practice, sharing
clinical datasets  even aggregated statistics  is often
conducted under a strict legal framework between parties.
Running example
In order to employ stricter data access restrictions
on the anonymised multi-dimensional RDF data cubes,
we require an access-controlbased query-federation
approach that enforces and optimises for restricted user
access over these RDF data cubes. Likewise we wish to be
able to optimise for certain locality conditions present in
RDF representations of statistical data. To further illus-
trate and motivate, we now provide a walkthrough of
an example that is representative of the main use-case
scenario.
Figure 1 shows four sample data cubes published by
three different clinical sites. Each observation repre-
sents the total number of patients exhibiting a particu-
lar adverse event. For example, the CHUV-S1 observa-
tions describe the total number of patients (in the Cases
column) that exhibit a particular combination of three
adverse events: Diabetes, (Abnormal) BMI_Abnormal
(Body Mass Index) and/or Hypertension. The value 0 or
1 indicates if the condition is present or not. For example,
the second row in CHUV-S1 indicates that there are 26
cases presenting with both Diabetes and Hypertension
but without BMI_Abnormal.
These data cubes can be represented in the RDF
Data Cube vocabulary [13], whose goal is to enable
the interlinking and integration of statistical data cubes
over the Web. The RDF resulting from representing the
aforementioned data cube in this vocabulary is shown
in Fig. 2.9 Individual data cubes are assigned to separate
named graphs [20], where, e.g., :CHUV-S1, :CHUV-S4
are names for two RDF graphs from the same :CHUV
source, whereas :CING-S2 is a another named graph
published in a different source (:CING). Though not
shown for brevity, each such graph is associated with its
own provenance information. Each such named graph
represents an independent data cube with disjoint sets of
observations, encoded as RDF resources (e.g., :obs_7);
this locality of data on information about observations
suggests the possibility of optimising the source selection
process to not only consider individual triple patterns, but
rather observations as a whole when answering queries.
However, it is important to note that the values of the
dimensions (e.g., 1) and the properties used to capture the
dimensions (e.g., sehr:Cases) are shared across data
cubes, and thus across graphs and sources: locality does
not apply in such cases.
As suggested by these example data, while the RDF
Data Cube vocabulary provides terms to capture the
structure of a data cube (which we discuss in more detail
in RDF data cubes section), additional vocabulary is
required to capture the clinical terminology needed by the
clinical partners. Hence the Linked2Safety consortium
has developed the Semantic EHR Model [21] (using the
prefix sehr in Fig. 2) to capture a unified clinical termi-
nology that covers the needs of the three clinical partners
[21]; terms such as sehr:Cases, sehr:Diabetes,
sehr:HIV, etc., constitute this vocabulary, cap-
turing statistical dimensions of the clinical data
cubes.
Once the data cubes are published by clinical sites,
query functionality must be made accessible to clinical
researchers in a manner that can abstract the details of
the underlying sources. Given that the underlying data
are in RDF, a natural candidate is to use SPARQL [22]:
the standard query language for RDF supported by a
wide variety of tools. Figure 3 shows a sample SPARQL
query specifying subject-selection criteria, asking for the
counts of cases that involve some combination of diabetes,
Fig. 1 Example (2D) of data cubes published by CHUV, CING and ZEINCRO
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 5 of 22
Fig. 2 Datacubes represented using the RDF data cube vocabulary
abnormal BMI, and hypertension. An answer returned by
the query, i.e., number of cases, will play a major role
in deciding the resources (i.e., number of subjects, loca-
tion, etc.) required for conducting a clinical trial. How-
ever, answering such a query requires integrating RDF
data cubes with three dimensions  Diabetes, Hyper-
tension, BMI_Abnormal  and the respective counts
originating from multiple clinical sites. For this, query
federation techniques  that allow for answering queries
over multiple independent data sources  will be required.
Referring back to Fig. 1, only three of the data cubes
(:CHUV-S1, :CING-S2 and :ZEINCRO-S3) contain all
required dimensions. An answer returned by the query
(Fig. 3) should list counts (i.e., cases) from these three RDF
data cubes.
Fig. 3 Example of subject selection criteria for clinical trials
However, as mentioned previously, these data cubes
cannot be published openly on the Web, but are rather
subject to strict access control policies. An important
question then relates to how such a policy mechanism
can be formulated over the RDF data cubes given in
Fig. 2. Towards answering this question, the consortium
has proposed the Access Policy Model (prefix lmds),
which describes the users profiles (their activity, location,
organisation, position and role) and their respective access
rights (e.g., read, write) [23]. An example is provided in
Fig. 4, where we see this model used to state that the
user :James has read-level access to two named graph
representing two independent data cubes residing in two
locations: :CHUV-S1 and :CHUV-S2; by default, users
do not have any privileges. In our scenario, the most com-
mon form of access-control policies are applied at the level
of named graphs (i.e., data cubes).
Now when answering the SPARQL query in Fig. 3,
we must take into account these data-access policies.
For example, assuming the query is executed by the
user :James, we know that he has access to
the :CHUV-S1 and :CING-S2 RDF data cubes only.
Therefore, the query federation engine should retrieve
results only from :CHUV-S1 and :CING-S2 and should
not consider :ZEINCRO-S3 for querying.
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 6 of 22
Fig. 4 Snippets of user profile, access policy and data cube source information. a User profile, bData cubes stored within named grap, c Access policy
Problem statement
In summary, from prior works we have a vocabulary
to represent data cubes as RDF [13], a vocabulary to
describe the clinical terminology of the partners in the
project [21], a wide variety of proposals on how to execute
SPARQL queries in federated settings [17], and a vocab-
ulary for describing data-access policies over these data
cubes [23]. However, we have no work looking at putting
all these aspects together. Thus the motivation for our
research is to investigate how to enable efficient access-
controlled query federation over statistical data cubes. As
argued in the introduction, we cannot use existing fed-
eration engines off-the-shelf since they do not provide
source-selection with the granularity needed to imple-
ment graph-level access control. Likewise the regular
structure of data cubes suggests optimisations that would
not be possible in a more general RDF/SPARQL sce-
nario. Hence the core research questions we tackle in this
paper are:
 How can we efficiently implement source-selection in
a federated scenario on the level of graphs (as needed
to efficiently support graph-level access control)?
 Can we optimise the query federation process
specifically for querying federated collections of RDF
data cubes in a manner that allows us to outperform
off-the-shelf engines?
Towards tackling these questions  questions that are
key to realising the ambitions of the Linked2Safety project 
we will later propose the SAFE query federation engine.
Related work
The scenario described in the previous section touches
upon three main areas: query federation, access control
and data cubes. We now discuss related literature in these
three areas, focusing on those works that deal in particular
with the Semantic Web standards (e.g., with RDF and
SPARQL) as relevant in our scenario.
SPARQL query federation
Many query federation engines have been proposed for
SPARQL (e.g., [17, 14, 2427]). Such engines accept an
input query, decompose it into sub-queries, decide rel-
evance of individual data sources (typically considering
sources at the level of endpoints) for sub-queries, forward
the sub-queries to the individual endpoints accordingly
andmerge the final results for the query. Such engines aim
to find and execute optimised query plans that minimise
initial latency and total runtimes. This can be achieved
by (i) using accurate source selection to minimise irrele-
vant messages, (ii) implementing efficient join algorithms,
(iii) and using caching techniques to avoid repeated sub-
queries.
Source selection is typically enabled using a local
index/catalogue and/or probing sources with queries at
runtime. The former approach assumes some knowl-
edge of the content of the underlying endpoints and
requires update/synchronisation strategies. However, the
latter approach incurs a higher runtime cost, having to
send endpoints queries to determine their relevance for
various sub-queries. Thus, many engines support a hybrid
of index and query-based source selection.
Table 1 gives an overview of existing SPARQL query
federation engines with respect to source selection type,
physical join operators, use of caching and explicit sup-
port for updates. We also remark on whether code is
available for the system. In this setting, our work builds
upon an existing federated engine  FedX  with support
for an access-control layer over statistical data represented
as RDF data cubes.
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 7 of 22
Table 1 Overview of existing SPARQL query federation engines
Systems Source selection Join type Code Policy Cache Update
ADERIS [27] Index Nested loop ? ? ? ?
ANAPSID [26] Query & index Adaptive ? ? ? ?
Avalanche [24] Query & index Distributed, merge ? ? ? ?
DARQ [4] Index Nested loop, bind ? ? ? ?
DAW [25] Query & index Based on underlying system ? ? ? ?
FedSearch [3] Query & index Bind, pull-based rank ? ? ? ?
FedX [1] Query Nested loop, bind ? ? ? ?
LHD [5] Query & index Hash, bind ? ? ? ?
SPLENDID [2] Query & index Hash, bind ? ? ? ?
HiBISCuS [14] Query & index Nested loop, bind ? ? ? ?
FEDRA [6] Query & index Nested loop, bind ? ? ? ?
SAFE [15] Query & index Nested loop, bind ? ? ? ?
Access control for SPARQL
Various authors have explored access control models for
SPARQL query engines. Gabillon and Letouzey [28] pro-
pose applying access control over named graphs and
views, which are defined as graphs dynamically gener-
ated using SPARQL CONSTRUCT or DESCRIBE queries.
Costabello et al. [29] propose SHI3LD: an access control
framework for SPARQL 1.1 query engines that operates
on the level of named graphs where permissions are based
on the context of the user in the setting of a mobile device;
permissions are checked using SPARQL ASK queries. Kir-
rane et al. [30] propose using stratified Datalog rules
to enforce an access control model that operates over
quad patterns, thus offering higher granularity of con-
trol. Bonatti et al. [31], propose reactive policies that
can model, for example, access-control settings through
an EventConditionAction paradigm. Daga et al. [32], on
the other hand, propose to use the Datanode ontology  for
describing data flows  to model policy propagation rules.
SAFE is designed specifically to query statistical RDF
data cubes in a distributed setting, where access control
is coupled with source selection and both operate on the
same level of granularity: named graphs. Access control 
deny or allow access  is based on user profiles and their
access rights, which are described in the Access Policy
Model created for the purposes of the Linked2Safety
project [23].
RDF data cubes
The RDF Data Cube Vocabulary (QB) [13] is a stan-
dard for describing data cubes as RDF, providing terms
to represent the structure of such cubes in an agreed-
upon manner, facilitating interoperability in the exchange
and interlinkage of data cubes on the Web. We use qb:
as a prefix to refer to this vocabulary. The core classes
in the vocabulary are: qb:DataSet, whose instances
represent individual data cubes; and qb:Observation,
whose instances represent a coherent tuple of measure-
ment values (as per the example in Fig. 2 where each
observation refers to a tuple of values in the original
data cubes of Fig. 1). In terms of describing individual
observations  such as the type of measure (area, dura-
tion, volume, location, etc.), units of measure (m2, s, m3,
lat/long, etc.), and so forth  QB recommends use of
the Statistical Data and Metadata eXchange (SDMX)10
standard, which supports expressing such features in an
interoperable manner. QB also allows for expressing fur-
ther features of data cubes, with a prominent example
being slices, where each slice is a group of observations
with certain values on given dimensions that can, for
example, be annotated with further meta-data or linked
to/from other data; as a brief example, in Fig. 2, we could
use QB to define a slice of of the cube :CING-S2 to rep-
resent statistics on patients with diabetes (with the value 1
for sehr:Diabetes), which would include :obs_4 but
not :obs_3.
Although QB has been standardised relatively recently,
there have been a few research works looking at exploit-
ing data in this format. For example, a number of tools
have been proposed to help users to create, publish and
subsequently analyse RDF data cubes [33, 34]. On the
other hand, Kämpgen et al. [35] look at methods to derive
mappings to integrate disparate data cubes together into
a global view. Relating more specifically to querying data
cubes, Kämpgen and Harth [36] look at the performance
of using traditional relational tools (MySQL/Mondrian)
for OLAP-style queries with a platform using QB and
an off-the-shelf SPARQL store (Virtuoso), including the
effects of materialising views on query runtimes. In gen-
eral however, while there have been some initial works
looking to exploit RDF data cubes, to the best of our
knowledge, no work has tackled the scenario of processing
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 8 of 22
queries over a federation of data cubes that are access-
restricted in various remote locations.
Methods
SAFEs architecture is summarised in Fig. 5, which shows
its three main components: (i) Source Selection: per-
forms multilevel source selection based on the capabil-
ities of data sources; (ii) Policy Aware Query Planning:
filters the selected data sources based on access rights
defined for each user; and (iii) Query Execution: performs
the execution of sub-queries against the selected sources
and merges the results returned. In the following, we
describe these components in detail. The first two compo-
nents (Source Selection and Policy Aware Query Planning)
are described in Algorithm 2 whereas the third compo-
nent (Query Execution) is delegated to the FedX query
engine [1].
Source selection
SAFE performs a tree-based two-level source selection as
shown in Fig. 6. At Level 1, like other query federation
engines [1, 2, 5, 14, 26], we perform triple-pattern-wise
endpoint selection, i.e., we identify the set of relevant
endpoints that will return non-empty results for the indi-
vidual triple patterns in a query. At Level 2 (unlike other
query federation engines), SAFE performs triple-pattern-
wise named graph selection, i.e., we identify a set of relevant
named graphs containing RDF data cubes for all relevant
endpoints already identified at Level 1. SAFE relies on data
summaries to identify relevant named graphs.
Data summaries
SAFEs data-summary generation algorithm is shown in
Algorithm 1. The algorithm takes the set of all datasets
(for example, CHUV, CING and ZEINCRO) as input and
generates a concise data summary that enables graph-
level source selection (as needed for the coupling with
graph-level access control). By proposing a specialised
algorithm for this setting, we claim that SAFE has signif-
icantly improved data-summary generation times when
compared to other index-based approaches for general
settings; this allows for faster recomputation of sum-
maries when the underlying sources change. The data
summaries themselves and the algorithm used to generate
them are explained in the following.
We assume a set of datasetsDwhere each datasetD ? D
is a RDF dataset: D := {(u1,G1), . . . (un,Gn)}, where each
(ui,Gi) is a named graph with (unique) URI ui. In our case,
named graphs refer to individual RDF data cubes as we do
not consider a default graph. We denote all graph names
by names(D), the set of all graphs by graphs(D), and a
particular graph in the dataset by D(u) := G. We denote
by preds(G) := {p | ?s, o : (s, p, o) ? G} the set of all
predicates in G and, overloading notation, by preds(D) :=?
(u,G)?D preds(G), we denote the set of all predicates in
D. Finally, we assume that each dataset is published as an
endpoint at a specific location, where loc(D) denotes the
location (endpoint URL) of the dataset D.
For each dataset D ? D, where each graph in D
contains an RDF data cube, SAFE stores the follow-
ing as a data summary: (i) the endpoint URL loc(D)
(lmds:endpointUrl) (line 4 of Algorithm 1); (ii) the
set of all graph names (lmds:cube/lmds:graph) (line
6 of Algorithm 1); and (iii) a map for each pred-
icate appearing in the dataset to the set of names
corresponding to the graphs in which it appears
(lmds:cubeProperties) (lines 78 of Algorithm 1).
Fig. 5 SAFE architecture
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 9 of 22
Fig. 6 Tree-based two level source selection
Algorithm 1: SAFE data summaries generation
algorithm
Data:D = {D1, . . . ,Dn}, loc(·)
/* Set of all data sources and their
mapping to endpoint URLs */
1 S ? {} ; /* SAFE summaries to be retrieved
as output */
2 for each D ? D do /* for each data source in
D */
3 initialise SD ; /* initialise summary for
dataset D */
4 SD.setURL(loc(D)) ;
5 for each (u,G) ? D do /* for each graph in
the dataset */
6 SD.addGraphName(u) ;
7 for each p ? preds(G) do /* for each
predicate in the graph */
8 SD.mapPut(p, SD.mapGet(p) ? {u}) ; /* map
pred. to gr. name */
9 S ? S ? {SD} ;
10 return S ; /* Data summaries of all sources
*/
We thus denote the set of all data summaries for D as S
and the data summary for a particular source as S(D). A
snippet of a data summary generated for the sample RDF
data cubes published by three clinical sites (CHUV, CING,
ZEINCRO) of Fig. 1 is shown in Fig. 7, where CHUV contains
two RDF data cubes (:CHUV-S1, :CHUV-S4), CING con-
tains one RDF data cube (:CING-S2), and ZEINCRO also
contains only one RDF data cube (:ZEINCRO-S3).
From the collection of raw data summaries S , we then
compute some indexes that will be used to accelerate the
source-selection process:
1. The set of all predicates in a given dataset D:
preds(D). For instance, for the dataset CHUV from
the running example (Fig. 1, Fig. 7), we have that
preds(CHUV) = {sehr:Diabetes,
sehr:BMI_Abnormal, sehr:Hypertension,
sehr:Smoking, sehr:Gender, sehr:Cases }.
2. The set of properties unique to a graph with name u
in the dataset D, where, overloading notation:
upreds(u,D) := {p ? preds(D(u)) | u? : u? =
u ? p ? preds(D(u?))}. For instance, from the
running example, we have that upreds(:CHUV-S1,
CHUV) =
{sehr:BMI_Abnormal,sehr:Hypertension}
and upreds(:CHUV-s4,CHUV) =
{sehr:Smoking,sehr:Gender}.
3. The set of graph names in D that have at least one
unique property:
unames(D) := {u ? names(D) | upreds(u,D) = ?}.
For instance, from the running example,
unames(CHUV) = {:CHUV-S1,:CHUV-S4}.
These indexes will be used in the following source selec-
tion algorithm.
Source selection algorithm
SAFEs source selection maps individual triple patterns to
graphs within sources that contain data relevant for the
query. However, in doing so, SAFE exploits certain locality
properties exhibited by RDF data cubes: more specifi-
cally, we assume that subjectsubject joins (henceforth:
ss joins) can only be satisfied by data local to a given RDF
data cube. This locality assumption is based on the idea
that data cubes stand as self-contained data structures
within their respective named graph.More specifically, we
assume that datasets, observations, slices, measures, etc.,
are not split over multiple data cubes/named graphs. For
example, in Fig. 2, this assumption restricts the possibility
of an observation instance, such as :obs_1, appearing as
a subject in two distinct named graphs: in other words, the
observation is assumed to be a local resource unique to
a given data cube. This locality restriction applies equally
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 10 of 22
Fig. 7 SAFE data summaries
within a datasetD and across all datasetsD.11 Hence when
deciding the named graphs that may be relevant for a
given triple pattern, we also consider other triple patterns
that share the same subject; for example, in Fig. 3, triple
patterns 27 (lines 58) form an ss join and will be con-
sidered in unison, where although all sources will match
the second and third triple pattern, these sources will not
be considered relevant unless they are relevant for other
triple patterns with the same subject.
The source selection process is detailed in Algorithm 1.
The algorithm takes the set of all available datasets D,
their data summaries S , and a a set of Basic Graph Pat-
terns12 BGP as input. These BGPs correspond to all BGPs
appearing in the input query, where each such BGP will
be processed separately since it may correspond to, for
example, optional or union patterns in the input query,
rather than a standard join. The algorithm also accepts an
access policy P and a user profile U ; for the moment, we
focus on the source selection, where we will provide more
details of the user-level policies and access control in the
section that follows. As output, the algorithm returns the
set of relevant sources and corresponding named graphs
identified for individual triple patterns.
We now discuss in detail the operation of the algorithm:
Line 1: The source selection algorithm will return a set
of candidate graphs for each triple pattern; these will
be stored in R, which is initialised on Line 1. The
sources relevant for different BGPs will be kept sep-
arate in the results since different sources may be
selected for the same triple pattern in two different
BGPs.
Line 2: The source selection algorithm will begin pro-
cessing all BGPs in the query one-by-one. Each such
BGP may refer to different parts of the query that
may require a certain operation, such as a UNION or
OPTIONAL clause, etc.; these will later be processed
and combined by the FedX engine.
Lines 35: Within each BGP, the algorithm proceeds by
grouping triple patterns according to their subject
and processing each subject-group separately. The
algorithm first takes all triple patterns for a given
subject and then extracts all (bound) predicates for
that subject.
Lines 67: For each dataset, if it contains all the predi-
cate IRIs in the subject group, then it may contain
relevant graphs (otherwise the algorithm continues
to the next dataset).
Lines 818: The algorithm uses information about
graphs that contain unique predicates in a given
dataset to potentially filter sources. If the subject-
group contains such a predicate, then only that
graph can be relevant. However, if the subject-group
contains two (or more) predicates that are unique for
different graphs, then no graph can contain relevant
data and the algorithm proceeds to the next dataset.
If no such predicates are found in the subject-group,
potentially all graphs in the dataset are considered
relevant.
Lines 1924: After all subject groups for the current BGP
have been processed, for triple patterns with vari-
ables as predicates, some may be restricted to the
sources of their subject group, while others may be
matched to all possible graphs. In the latter case, to
increase the selectivity of source selection, for such
a triple pattern where at least the subject or object
is bound, we will send an ASK query to each dataset
to see if it may be relevant or not, i.e., to see if the
dataset contains data for that subject and/or object.
If so, all graphs from that dataset are added.
Line 25: The sources selected for the current BGP are
added to the results.
Lines 2628: Wewill then check each graph selected (for
some triple pattern in some BGP) against the poli-
cies and user profile, removing any graphs for which
the user does not have access. We describe this pro-
cess in more detail in Security and authentication
section.
Source selection example
From our running example, let us consider the query
shown in Fig. 3, which contains one BGP with seven
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 11 of 22
Algorithm 2: Access-policybased triple-patternwise source selection method
Data:D,BGP,S ,P,U
/* Datasets, BGPs of a SPARQL query, Data Summaries, Access Policies, User Profile */
1 R ? {} ; /* initialise relevance set */
/* process each BGP from the query independently ... */
2 for each bgp ? BGP do /* for each BGP */
/* process source selection for local subject groups separately ... */
3 for each s such that ?p, o : (s, p, o) ? bgp do /* for each unique subject in bgp */
4 bgps ? {t ? bgp | subj(t) = s} ; /* get all patterns with that subject */
5 Preds ? {p | ?o : (s, p, o) ? bgps ? p is bound } ; /* get all bound predicates */
6 for each D ? D do /* for each dataset */
7 if Preds ? S .preds(D) then /* if dataset covers predicates */
8 us ? ?; /* name of a unique graph for bgps (initially null) */
9 for each u ? S .unames(D) do /* for each gr. name with unique pred. */
10 if Preds ? S .upreds(u,D) = {} then /* if (u,G) unique for bgps */
11 if us = ? then /* if (u,G) first such unique graph */
12 us ? u ; /* store the name */
13 else /* multiple unique graphs imply no results for bgps in D */
14 goto 6 ; /* continue to next dataset */
15 if us = ? then /* precisely one unique graph found */
16 R ? R ? (bgps × {us} × loc(D)) ; /* add for all patterns in bgps */
17 else /* no unique graph found */
18 R ? R ? (bgps × S .names(D) × loc(D)) ; /* add all graphs in D */
/* ask queries for patterns with unbound predicates matching all graphs thus far
... */
19 for each (s, p, o) ? bgp ? BGP such that p is unbound ? (s is bound ? o is bound) do
20 if R contains all possible graphs for (s, p, o) then /* no locality restriction found */
21 R ? R ? {(t,u, d) ? R | t = (s, p, o)} ;
22 for each D ? D do /* for each dataset */
23 if ASK((s, p, o),D) = true then /* use ASK query to check relevance */
24 R ? R ? ({(s, p, o)} × S .names(D) × loc(D)) ; /* add all graphs */
25 R ? R ? {R} ; /* store sources selected for the current BGP */
/* do policy-based graph filtering to prune selected graphs ... */
26 for each (u, d) such that ?t : (t,u, d) ? R ? R do /* for each graph selected as relevant */
27 if ¬authorise(d,u,P,U) then /* if user not permitted to access that graph */
28 remove(R,u, d) ; /* remove unauthorised graph for all patterns and BGPs */
29 returnR ; /* return relevant and permitted sources for all triple patterns and BGPs */
triple patterns. The first step is to group the BGP into
subject groups, which will result in two sub-BGPs, as
follows:
?dataset a qb:DataSet
?observation qb:dataSet ?dataset
?observation a qb:Observation
?observation sehr:Diabetes ?diabetes
?observation sehr:BMI_Abnormal ?bmi
?observation sehr:Hypertension ?hypertension
?observation sehr:Cases ?cases
The first subject group will be matched to all graphs
containing such a triple. For the second subject group, the
set of all predicates is:
Preds = {qb:dataSet,rdf:type,sehr:Diabetes,
sehr:BMI_Abnormal,
sehr:Hypertension,sehr:Cases}
For each dataset, the algorithm checks to ensure that all
predicates in the subject group are covered by the predi-
cates in that dataset; this is the case for all three datasets
in Fig. 2 (:CHUV, :CING, and :ZEINCRO). Again, given
the locality restrictions, ss joins are answerable only over
a given dataset/graph, and hence we can safely filter other
datasets from consideration for all triple patterns in that
subject group.
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 12 of 22
Next, for each such dataset, the algorithm analy-
ses graphs that uniquely contain predicates within
that dataset. For example, for the :CHUV dataset, the
first graph :CHUV-S1 contains the unique predicates
sehr:BMI_Abnormal and sehr:Hypertension,
while :CHUV-S4 contains the unique predicates
sehr:Smoking and sehr:Gender. Since both
sehr:BMI_Abnormal and sehr:Hypertension
appear in the current subject group, only :CHUV-S1 will
be selected as relevant for all triple patterns from the
dataset :CHUV. This means, for example, that :CHUV-S4
will not be selected for the triple patterns referring to
sehr:Diabetes, sehr:Cases, etc., even though
data exists to match those patterns; due to the locality
restrictions, such triple patterns cannot form a join with
the sehr:BMI_Abnormal and sehr:Hypertension
triple patterns.
Since no triple patterns contain unbound predicates,
no ASK queries are sent. Then the selected sources are
added for the current BGP. For the first triple pattern,
all graphs in all datasets will be matched. For all triple
patterns in the second subject-group, the following three
datasetgraph pairs will be selected: (:CHUV,:CHUV-S1),
(:CING,:CING-S2), (:ZEINCRO,:ZEINCRO-S3). Finally,
user authorisation is checked for all graphs, where if
authorisation is not available, the graph is filtered; we
will discuss this access-control process in more detail in
Security and authentication section.
Endpoints with selected named graphs will then be
queried using standard federation techniques. For this,
we use the FedX query engine [1], amending the query
rewriter to append the relevant graph information for
each endpoint.
Source selection correctness
The source selection algorithm assumes certain locality
restrictions that must hold in the data for the algorithm
to be correct. In particular, for a given set of datasets D,
we assume that if there exists a dataset D, a named graph
name (u,G) ? D, and a triple (s, p, o) ? G, then there
does not exist a dataset D?, a named graph (u?,G?) and
a triple (s, p?, o?) ? G? such that D = D? or u? = u. In
other words, we assume that subjects are unique to a given
named graph (considering all named graphs and datasets
inD).
With this locality restriction, we can then begin to con-
sider the correctness of Algorithm 2. The goal of the algo-
rithm is to ensure that all possible answers for each BGP
in the input (i.e., the answers possible for each BGP over
a local merge of all data in D) can be generated by joining
the union of the results of individual triple patterns eval-
uated over the sources selected for those patterns. Once
this process is assured, we delegate the processing of the
query to FedX, which can use standard query execution
methods to execute, for example, joins, left joins, unions,
filters, etc., over the results of each BGP, producing the
final query results for the user. First wemust highlight that
a known and non-trivial obstacle to completeness in fed-
erated scenarios is presented by blank nodes [37]; hence,
per the running examples, we assume that no blank nodes
are used in the data.
More formally, let D denote the result of merging all
datasets in D into a single global dataset13, let bgp =
{t1, . . . , tn} denote a BGP, and let [[D]]bgp denote the result
of evaluating bgp over D [38]. Next let R denote the
sources selected for bgp by Algorithm 2, let R(t) denote
a dataset composed of the named graphs selected for the
triple pattern t in R, and let [[R(t)]]t denote the evalua-
tion of triple pattern t over that dataset. The correctness
condition we wish to satisfy is then as follows: [[D]]bgp =
[[R(t1)]]t1  . . . [[R(tn)]]tn .
Let us start with a base case where the BGP has only sin-
gleton subject groups, meaning that no two triple patterns
share a subject, and where all predicates are bound. In this
case, the algorithm will select all datasets and (at least) all
graphs with matching predicates. In this case, it is not dif-
ficult to show that [[R(t)]]t =[[D]]t for all t ? bgp, and thus
we have that [[D]]bgp =[[D]]t1  . . . [[D]]tn , which is the
definition of the evaluation of BGPs [38]. Likewise, if we
consider only singleton subject groups, but where some
predicates are not bound, again the ASK queries will fil-
ter only irrelevant graphs, where it is again not difficult
to show that [[R(t)]]t =[[D]]t for all t ? bgp in this gener-
alised case. In fact, these two base cases refer to standard
techniques in federated SPARQL query processing.
What is left then is to verify the correctness of selecting
sources by subject group. For the moment, we can assume
that bgp forms one subject group and that all predicates
are bound. Assume for the purposes of contradiction that
as a result of Algorithm 2, [[D]]bgp =[[R(t1)]]t1  . . . 
[[R(tn)]]tn . First off, given that [[R(ti)]]ti ?[[D]]ti for 1 ? i ? n
(since R(ti) is a slice of data from D and BGPs are mono-
tonic), we have that [[D]] bgp ?[[D]]t1  . . . [[D]]tn ,
i.e., that we return only correct answers. Thus we are left
with the case that [[D]]bgp[[R(t1)]]t1  . . . [[R(tn)]]tn .
When could such a case occur? First of all, joins within
a named graph between the triple patterns in bgp are
not restricted by the algorithm, which only applies a triv-
ial condition that all predicates in the subject-group be
matched in the relevant datasets and that unique predi-
cates in the graph are also satisfied. Hence for such a case
to occur  for the algorithm to miss results  we would
need a join to occur across graphs on (at least) the sub-
ject position. However, such a join would clearly break our
locality restriction, and hence we have a contradiction.We
highlight that it does not matter if the subject term is a
variable or a constant here, nor does it matter if the con-
stituent triple patterns in the subject group additionally
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 13 of 22
have joins in other positions: such subject-groups can still
only generate results with terms sourced from a single
graph.
Finally, combining everything, we know that each sub-
BGP pertaining to a subject group must return complete
results, and hence the join of these sub-BGPs must also be
complete. Thus the correctness condition (soundness and
completeness) holds.
Source selection complexity analysis
With respect to the complexity of Algorithm 2, we analyse
worst-case asymptotic runtime complexity considering
RDF terms as symbols of constant length: i.e., we do not
consider the length of IRIs or literals since they are typi-
cally bounded by some small constant factor. To keep the
analysis concise, we will consider q to be the size of the
query encoding the number of triple patterns in the union
of BGP; note that with this factor, we can abstract away the
presence of multiple BGPs, the number of predicate in the
query, etc., since these are bounded by q. Likewise we will
consider g to be the total number of graphs in all datasets,
and d to be the number of datasets available (note that d
is bounded by g). Finally we denote by p the number of
unique predicates in all graphs (more specifically, this is
the cardinality of the set of all terms appearing in the pred-
icate position of any graph) and by t the total number of
triples in all graphs.
Creating the subject groups for a BGP and extracting
the predicates for those groups can be done by sorting the
triple patterns in the query, which is feasible in O(q log(q))
in the worst-case with, e.g., a Merge sort implementation.
For each subject group, we must check for each dataset
that all predicates in that subject group are contained in
the predicates for each dataset. This is feasible by a Merge
sort over all predicates in the subject group (whose cardi-
nality we denote by ps) with all predicates in the dataset,
resulting in O((ps + p) log(ps + p)) complexity for a given
dataset and subject-group, or O(d((ps+p) log(ps+p))) for
all datasets and a given subject group. When considering
all subject groups, we can more coarsely (but concisely)
represent the complexity as O(qd((q + p) log(q + p))),
replacing both ps and the number of subject groups with
q by which both are bounded.
Next, for each graph with unique predicates, we need to
check if any such predicate appears in the subject group.
This is bounded byO(qg(q+p) log(q+p)) since we need to
check each graph once (again, the first q bounds the num-
ber of subject groups, and the second and third q bound
the number of predicates in each subject group).
Since the complexity O(qg(q+ p) log(q+ p)) asymptot-
ically bounds the other factors, it represents the overall
complexity up until Line 18 and thus the complexity
of the analysis assuming all triple patterns have bound
predicates and no access control is in place.
On Lines 1924, the algorithm performs ASK queries
to each dataset for each triple pattern matching the given
criteria (bounded by q). In the general case, resolving ASK
queries is NP-complete in combined complexity (consid-
ering both the size of the query and the data), even in the
case that the query only contains a BGP and no features
like optionals and filters; this is because each such query
requires finding a homomorphism from the query graph
to the data graph, where the graph homomorphism prob-
lem is NP-complete. However, since we only issue ASK
queries with one triple pattern, and since the arity of the
triple pattern is bounded, this step is feasible in (at least)
time linear in the size of the data (for example, running a
simple scan over all data), and so for all patterns, we have
a resulting worst-case complexity of O(qt) from this part
of the algorithm.
Hence before considering access control on Lines
2628, we canmerge these two factors to give a worst-case
complexity of O(qg(q+ t) log(q+ p)). We emphasise that
this is a coarse upper-bound in that it encapsulates dis-
joint cases whereby, e.g., a querys subject groups repeat
all predicates in the query and a query has no bound
predicates, which is an impossible case to occur in one
query; however, the complexity needs to bound all such
cases since they affect different complexity parameters
in different ways. In summary, as we will show, for real-
world cases  where queries are small, predicates are few,
and ASK queries are accelerated with pre-computed index
schemes  the algorithm is much more efficient than this
worse-case complexity bound may suggest.
Finally it is worth mentioning that the combined
complexity for evaluating SPARQL queries is PSpace-
complete [38], meaning that in a worst-case analysis, the
actual evaluation of the query will dominate the source
selection process. However again in practice, evaluating
SPARQL queries can often be done efficiently in spite of
such worst-case analyses: in particular, queries are often
quite simple, having, for example, low treewidth (an indi-
cator of how cyclical the interconnection of patterns in
the query are), where queries with bounded treewidth are
(oftendepending on the exact query features permitted)
fixed-parameter tractable [39]. The core conclusion is that
worst-case analyses do not paint a complete picture: it is
important to consider empirical performance results as
well.
Security and authentication
The proposed policy aware SPARQL federation functions
on top of a security and authentication mechanism
employed within the Linked2Safety software platform
[40]. The security of user profiles, access policy, index
(data summaries), and data cubes residing behind
different SPARQL endpoints is based on a Public Key
Infrastructure (PKI), which binds public keys with
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 14 of 22
respective user identities by means of a Certificate
Authority (CA). The user related information (profile and
policy), data cubes, and data summaries are stored and
hosted by the data owners, i.e., by the healthcare organ-
isation that gather the data. Each user identity must be
unique within each CA domain maintained across organi-
sations. For each user (profile), the user identity, the public
key, their binding, validity conditions and other attributes
are made un-forgeable through public key certificates
issued by the CA. A log auditing mechanism keeps track
of the query, the user and the data-cubes returned.
The process of allowing a user to access RDF data-cubes
(and data summaries) is based on two axes: the first one
is to authenticate the user, which allows the system to
verify that the user is who he/she claims to be. This is
also a prerequisite in order to know the role that an expert
user has in the Linked2Safety system since after verifying
the user, we can extract his/her role and corresponding
data-access privileges. The second axis is to authorise the
expert user to access the requested RDF data cubes if cer-
tain criteria based on his/her profile information are met,
including role, working area, origin, etc. Each encrypted
data-cube is sent (along with the signed hash-digest)
with the public key of the client who requested the data
(from his/her certificate). Upon successful verification,
the expert user (profile) is authorised to perform a par-
ticular query over the SPARQL endpoint. The results are
signed and encrypted by the clinical partners before being
returned to the expert user, using the certificates. The
Linked2Safety platform automatically verifies the origin
of the data (non-repudiation), the sources they were sent
from (authentication) and decrypts them (data integrity),
before providing the query results.
Again, the security and authentication platform for the
Linked2Safety project is the subject of existing work [40],
which SAFE considers as a black box. Integration with this
platform is represented in Lines 2628 of Algorithm 2,
whereby applying source selection on the level of graphs
allows the query federation process to be directly inte-
grated with the graph-level access-control policies in place
for the given stakeholders. An example of such access poli-
cies is given in Fig. 4 where we see that the user :James
is permitted access only to two graphs: :CHUV-S1 and
:CING-S2. In Fig. 8, we provide a SPARQL query
that asks if the user :James has access to the graph
:CHUV-S1; in the running example, this will return true.
Let us consider again the example discussed for source
selection where SAFE is executing the query in Fig. 3 over
the federation of RDF data cubes outlined in Fig. 2. With-
out access control  prior to Line 26 in Algorithm 2  the
source selection algorithm will select the three dataset
graph pairs: (:CHUV,:CHUV-S1), (:CING,:CING-S2),
(:ZEINCRO,:ZEINCRO-S3). However, the authenti-
cated user does not have access to the latter source, and
hence this graph will be filtered as a source, thus ensur-
ing the user does not (attempt to) break the access policies
of the stakeholders; if the source were not filtered, the
request to access :ZEINCRO-S3would rather be rejected
at remote query-execution time.
Note that in the following evaluation section, we focus
on the performance of the SAFE engine for executing
federated queries and do not directly measure the per-
formance of access control for the following two main
reasons: (i) SAFE makes a sequence of calls to an external
framework described in previous work [40] where such
an evaluation would relate more to the external frame-
work than to SAFEs design; (ii) it is unclear to what we
could compare the results since other federated query
engines do not implement such access control. Instead,
we highlight that the main contribution of SAFE for
access control is implementing graph-level source selec-
tion, which enables tight integration with such an access
control system; we will compare the performance of this
more granular source selection in the following section
with existing engines that offer dataset-level source
selection.
Results
In Motivating scenario section, based on our motivating
scenario, we introduced two core research questions:
 How can we efficiently implement source-selection in
a federated scenario on the level of graphs (as needed
to efficiently support graph-level access control)?
Fig. 8 SPARQL query authenticating a user against a data cube/named graph
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 15 of 22
 Can we optimise the query federation process
specifically for querying federated collections of RDF
data cubes in a manner that allows us to outperform
off-the-shelf engines?
In terms of the first question, we have proposed the
SAFE engine, which performs graph-level source selec-
tion that allows it to be integrated with graph-level access
control; however, we have yet to see how efficient this
alternative form of source selection when executing fed-
erated queries. In terms of the second question, we have
proposed certain locality restrictions applicable in the
context of RDF data cubes to refine the source selection
process; however, we have yet to see how this optimisation
compares to existing engines.
Along these lines, in this section we present the results
of our evaluation comparing SAFE with three existing
SPARQL query federation engines  FedX, HiBISCuS and
SPLENDID  for a variety of queries and datasets along a
series of metrics and aspects.
Experimental setup
The experimental setup (i.e., datasets, setting, queries and
metrics) for evaluation are described in this section. Note
that the experimental material discussed in the follow-
ing section and an implementation of SAFE are avail-
able online at https://github.com/yasarkhangithub/SAFE,
which we will refer to in the following as the homepage.
Datasets We use two groups of datasets exploring two
different use cases.
The first group of datasets (internal) are collected
from the three clinical partners involved in our primary
use case as described in Motivating scenario section.
These datasets contain aggregated clinical data repre-
sented as RDF data cubes and are privately owned/
restricted.
The second group of datasets (external) are col-
lected from legacy Linked Data containing sociopoliti-
cal and economical statistics (in the form of RDF data
cubes) from theWorld Bank, IMF (International Monetary
Fund), Eurostat, FAO (Food and Agriculture Organization
of the United Nations) and Transparency International.
The World Bank data contains a comprehensive set of
information about countries around the globe, such as
observations on development indicators, financial state-
ments, climate change, research projects, etc. The IMF
data provides a range of time series data on lending,
exchange rates and other economic and financial indica-
tors. The Eurostat data provides statistical indicators that
enable comparison between countries and regions across
Europe. The Transparency International data includes a
Corruption Perceptions Index (CPI), which ranks coun-
tries and territories based on how corrupt their public
sector is perceived to be. The FAO data covers the areas of
agriculture, forestry and fisheries. The Linked Data cubes
space (Fig. 9) shows how these legacy datasets are inter-
linked with each other. These datasets provide links to
each other using skos:exactMatch and owl:sameAs
properties. The circles represent datasets while edges rep-
resent unidirectional or bidirectional links between any
two datasets. These external datasets are available on the
homepage.
Table 2 gives an overview of the experimental datasets,
where we see that the largest dataset is IMF with 44 mil-
lion triples describing 4 million observations with a total
raw data size of 3.5 GB. On the other hand, the dataset
with the highest dimensionality is FAO, with 247 unique
properties. The largest INTERNAL dataset is CHUV with
0.8 million triples and 96 thousand observations; it also
has the highest dimensionality, evidenced by 36 unique
predicate terms.
Setting Each dataset was loaded into a different SPARQL
endpoint on separate physical machines. All experiments
are carried out on a local network, so that network
cost remains negligible. The machines used for exper-
iments have a 2.60 GHz Core i7 processor, 8 GB of
RAM and 500 GB hard disk running a 64-bit Win-
dows 7 OS. Each dataset is hosted as a Virtuoso (Open
Source v.7.2.4.2) SPARQL endpoint hosted physically on
separate machines. Each instance of Virtuoso is config-
ured with NumberOfBuffers = 680000, MaxDirty
Buffers = 500000 and MaxQueryMem = 8G. Fur-
ther parameters used to configure Virtuoso are available
on the homepage.
Queries A total of 15 queries are designed to evaluate
and compare the query federation performance of SAFE
against FedX, HiBISCuS and SPLENDID based on the
metrics defined. We define five queries for the federa-
tion of internal datasets (QL-*) and ten for the federation
of external datasets (QE-*). The list of external queries is
made available on the homepage. The queries are of vary-
ing complexity and have varying type of characteristics as
noted in Table 3 where we summarise the characteristics
of these queries following similar dimensions to that used
in the Berlin SPARQL benchmark [41]. The row for the
number of sources in this table indicates those matched
by at least one triple pattern in the query.
Metrics For each query type we measured (i) the number
of sources selected; (ii) the average source selection time;
(iii) the average query execution time; and (iv) the num-
ber of ASK requests issued to sources. The performance
of SAFE, FedX, HiBISCuS and SPLENDID was compared
based on these metrics. The query results produced by
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 16 of 22
Fig. 9 Linked Data cubes space
SAFE, FedX, HiBISCuS and SPLENDID are the same for
all queries.
Experimental results
In this section, we present the experimental results gath-
ered for the given datasets, setting, queries and metrics
discussed previously.
Index Generation Time and Compression Ratio:
SAFEs index/data summaries generation approach is
compared with various state-of-the-art approaches and
the comparison results are shown in Table 4. The compar-
ison metrics used are index generation time (time), index
size (size) and the index reduction (ratio: computed as
1 ? index sizetotal dump size ).
As can be seen from the results, the index sizes for
all approaches are much smaller than the relative size
of the raw data dump. In the case of FedX, no indexes
are created since relevant sources are determined on-
the-fly at runtime. Aside from FedX, SAFE produces the
smallest indexes by focusing only on meta-data about
predicates and named graphs: for external datasets hav-
ing a raw dump size of 8 GB, SAFE generates an index of
size 23 KB, achieving 99.99% reduction, while for internal
datasets, with a raw size of 51MB, SAFE achieves a 99.98%
(8 KB) index reduction. It should be noted however that
although in relative terms HiBISCuS and SPLENDID pro-
duce indexes that are 211 times larger, the absolute sizes
of the indexes are relatively small across all engines, where
Table 2 Overview of experimental datasets
Dataset Type ? trip ? obsv ? sub ? pred ? obj Data
CHUV INT 0.8 M 96 K 96 K 36 88 31 MB
CING INT 0.1 M 17 K 17 K 21 51 5 MB
ZEINCRO INT 0.4 M 49 K 49 K 24 59 15 MB
Total INT 1.3 M 162 K 162 K 81 198 51 MB
World Bank EXT 15 M 1.7 M 1.7 M 240 2.1 M 1.9 GB
IMF EXT 44 M 4 M 4 M 181 1.4 M 3.5 GB
Eurostat EXT 0.3 M 38 K 38 K 64 31 K 125 MB
Trans. Int. EXT 43 K 3928 4198 49 11 K 121 MB
FAO EXT 11 M 1.4 M 1.4 M 247 0.2 M 1.93 GB
Total EXT 72 M 7 M 7 M 923 4 M 8 GB
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 17 of 22
Table 3 Summary of query characteristics
Characteristics QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5
? of Patterns 9 8 10 16 11 10 12 9 7 11 6 6 3 7 5
? of Sources 3 4 4 3 4 3 3 4 3 3 3 3 3 3 3
? of Results 41 1 10 41 64 22208 5 10 108779 4380 1701 17199 760 39312 10
Filters ? ? ?
>9 pattens ? ? ? ? ? ? ? ?
Negation ?
LIMITmodifier ? ? ? ?
ORDER BYmodifier ? ?
DISTINCTmodifier ? ? ? ? ? ? ? ? ? ? ? ?
REGEX operator ? ?
UNION operator ? ?
reduction rates remain above 99.9% for all three engines
over all datasets.
As far as index generation time is concerned, aside from
FedX which incurs no index generation costs, SAFE has
a significant gain over all the approaches. In particular,
with respect to the EXTERNAL datasets, SAFEs index gen-
eration time is 102 s as compared to 1,772 s and 369 s
for HiBISCuS and SPLENDID, respectively. SAFE has the
lowest time for INTERNAL dataset as well though in over-
all terms, these times are quite small. Hence we see that,
for example, upon updates to the INTERNAL federation
of datasets, SAFE could recompute indexes from scratch
in around 10 seconds. Of course for larger datasets, this
(re-)indexing grows to the order of minutes.
While FedX incurs no index generation nor mainte-
nance costs, we propose that SAFEs indexes will reduce
the load on remote endpoints and ultimately the overall
query-execution time, and thus presents a good trade-
off in a federated setting. These claims will be explored
Table 4 Index generation time and compression ratio
System Time Size Ratio
External datasets
SAFE 102 s 23 KB 99.998%
HiBISCuS 1772 s 112 KB 99.994%
SPLENDID 369 s 252 KB 99.988%
FedX   
Internal datasets
SAFE 10 s 8 KB 99.984%
HiBISCuS 26 s 20 KB 99.961%
SPLENDID 74 s 21 KB 99.959%
FedX   
in the context of subsequent metrics (namely number of
ASK queries, source selection time and query execution
time).
Triple pattern-wise sources selected: Table 5 shows the
total number of triple pattern-wise (TP) sources selected
by SAFE, FedX, HiBISCuS and SPLENDID for all the
queries. For the purposes of comparability, we count the
number of datasets selected as relevant sources since only
SAFE additionally selects relevant graphs. The last col-
umn in Table 5 shows the average number of TP sources
selected by each approach across all queries.
FedX performs source selection at the triple-pattern-
level using ASK queries for each triple pattern to find
out precisely which sources can answer an individual
triple pattern. Thus, on the level of individual triple
patterns, FedX selects all and only the actual contribut-
ing sources for those patterns. However, these sources
might not be relevant after performing a join between
two triple patterns, i.e., results from some sources might
be excluded after join. HiBISCuS uses a hybrid source
selection approach by using both ASK queries and data
summaries to prune the number of relevant sources.
SPLENDID uses VOID descriptions of sources to identify
the relevant sources for each triple pattern of the query.
SPLENDID also make use of ASK queries for source selec-
tion in cases where the query has bound variables that are
not covered in the VOID descriptions.
The results show that on average HiBISCuS and SAFE
have better source selection algorithms in terms of the
average number of sources selected (10 and 10, respec-
tively). The results in Table 5 show that FedX and
SPLENDID overestimate the set of sources that contribute
to the final query results. In the query execution times
section, we will see that source overestimation leads to
higher query execution times. Thus both HiBISCuS and
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 18 of 22
Table 5 Sum of triple-pattern-wise sources selected for each query
System QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5 Avg
SAFE 7 11 13 16 14 12 14 12 10 11 6 6 3 9 5 10
FedX 8 18 20 24 24 15 19 19 11 22 14 16 7 15 13 16
HiBISCuS 9 6 10 16 12 5 12 9 3 11 14 16 7 15 13 10
SPLENDID 8 13 20 24 18 16 20 19 12 22 14 16 7 15 13 16
SAFE outperform FedX and SPLENDID (in the average
case) by not only considering sources relevant to a given
triple-pattern, but also the other triple patterns in the
query. For example, by using join-aware source selection
designed for RDF data cubes, SAFE manages to filter fur-
ther potential sources that do not contribute to the end
results.
Although SAFE does not have a clear advantage over
HiBISCuS in terms of the number of datasets selected
as sources, SAFE does have an extra advantage over
the other engines not illustrated by Table 5: the SAFE
source selection algorithm prunes sources at the granular-
ity of graphs, further restricting the data to be considered
beyond datasets.
Number of SPARQL ASK requests: Table 6 shows the
total number of SPARQL ASK requests used to perform
source selection for each query. FedX is an index-free
approach and performs runtime SPARQL ASK requests
during source selection for each triple pattern in query:
hence without any indexes, FedX must run many more
such queries than the other engines that do support index
information. HiBISCuS uses a hybrid approach that uses
both runtime SPARQL ASK requests and pre-computed
data summaries during source selection for each triple
pattern in a query. SPLENDID uses VOID descriptions
as well as ASK requests in case of bound variables in the
query for which VOID does not offer relevant informa-
tion. Hence, by considering indexes, both HiBISCuS and
SPLENDID greatly reduce the number of ASK queries
used during source selection. On the other hand, SAFE
uses data summaries for source selection, reverting to
SPARQL ASK requests only when there is an unbound
predicate in a triple pattern and no locality restrictions
are found to apply on the subject group to which that pat-
tern belongs. None of our evaluation queries have such a
triple pattern; hence there are no SPARQL ASK requests
for SAFE.
Though flexible in the generic case  particularly in the
case of frequent updates to underlying sources  index-
free approaches can incur a large cost in terms of SPARQL
ASK requests used for source selection, which can in turn
increase overall query execution time.
Source selection time: Figure 10 compares the source
selection time for SAFE, FedX, HiBISCuS and SPLENDID
for each query, where the y-axis is presented in log-scale.
The rightmost set of bars compares the average source
selection time over all queries. Given that the indexes of
HiBISCuS, SPLENDID and SAFE remain quite small rel-
ative to total data sizes, they can easily be loaded into
memory, where lookups can be performed in millisec-
onds. On the other hand, executing remote ASK queries
are orders of magnitude more costly. Hence we see that
the source selection time for SAFE is lower than the other
approaches since SAFE uses ASK queries more sparingly,
as previously discussed.
Query execution time: For each query, a mean query
execution time was calculated for SAFE, FedX, HiBISCuS
and SPLENDID by running each query ten times.
Figure 11 then compares the mean query execution times
of SAFE, FedX, HiBISCuS and SPLENDID for all queries.
Again, the y-axis is log-scale. We set a timeout of 25
minutes on query execution; with these settings, FedX
times-out in the case of four queries, HiBISCuS in three
queries and SPLENDID in twelve queries (note that we do
not show average execution times across all queries since
it would be unclear what value to assign to queries that
time-out). On the other hand, SAFE does not time out
in any such case. Looking at query response times, SAFE
outperforms the other engines in all queries. The fastest
Table 6 Number of SPARQL ASK requests used for source selection
System QE-1 QE-2 QE-3 QE-4 QE-5 QE-6 QE-7 QE-8 QE-9 QE-10 QL-1 QL-2 QL-3 QL-4 QL-5 Avg
SAFE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
FedX 54 48 60 96 72 60 72 54 48 66 18 18 9 21 15 47
HiBISCuS 12 12 6 24 12 18 12 12 12 12 0 0 0 0 0 9
SPLENDID 17 16 25 12 24 25 23 14 27 32 14 16 7 15 13 19
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 19 of 22
Fig. 10 Comparison of source selection time
query (QE-1) is executed by SAFE within 100 ms, while
the slowest query (QL-4) takes approximately 2 minutes.
In total, SAFE executes 7 of the 15 queries in less than a
second.
Discussion
There are a number of factors that can influence the over-
all query execution time of a query federation engine,
such as join type, join order selection, block and buffer
size, etc. However, given that SAFE is based on the
FedX architecture, we can attribute the observed runtime
improvements to three main factors: (i) source selec-
tion time is reduced (as we have seen in the previous
sets of results); (ii) fewer sources are queried meaning
less time spent waiting for responses; (iii) source prun-
ing at graph level within a source leads to querying
fewer triples and (iv) triple patterns are more selective in
SAFE, where, for example, our join-awareness makes it
unlikely that all rdf:type triple patterns will need to
be retrieved/queried for all sources but rather only from
sources where such a triple pattern joins with a more
selective one. Taken together, these four main observa-
tions explain the time saving observed for our presented
use-cases, where the third and fourth observation in par-
ticular  the locality conditions on ss joins designed for
RDF data cubes combined with a more granular graph-
level selection  play a significant role for restricting the
amount of data generated for low-selectivity triple pat-
terns. By making specific locality-based optimisations for
the case of RDF data cubes, and combining this with
finer-grained source selection on the level of graphs,
SAFE can perform beyond what would is possible in off-
the-shelf SPARQL federation engines designed for the
general case.
Fig. 11 Comparison of query execution time
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 20 of 22
Aside from query execution times, there are also a num-
ber of other important factors to consider, such as the
load on the remove servers, and also the ability to cope
with updates to the individual sources. In terms of load
on the remove servers, we argue that by generating fewer
ASK queries during the source-selection process, and in
general by applying a more granular source-selection that
requires processing fewer data, SAFE generates less load
on remove servers, reducing the costs associated with
hosting such services. With respect to updates, SAFE is
less flexible than the index-free FedX approach, which
requires no special action upon source updates; however,
SAFEs index is rather lightweight and can be recomputed
from scratch (over an entire federation of sources) in the
order of seconds for smaller datasets, and minutes for
larger datasets, which should be acceptable except in the
case that updates are very frequent and strong notions of
distributed consistency are required. On the other hand,
the benefit of SAFEs indexing approach has been demon-
strated in terms of source selection times, load on servers,
query execution times, and so forth. Hence there is a clear
trade-off, where we argue that in the case of SAFE, for
most settings, the inflexibility with respect to updates is
paid off in terms of overall efficiency.
Finally, the SAFE source selection algorithm has the
additional advantage of allowing tight integration with a
graph-level access-control framework. While this access-
control requirement was the original motivation behind
the design of SAFE, and in particular its graph-level
granularity, as these experiments have shown, select-
ing sources on the level of graphs, in combination with
join-aware optimisations, also gives general performance
benefits even in the case that access-control is not needed.
Conclusions
In this paper, we have presented SAFE: a query federation
engine that enables policy-based access to sensitive sta-
tistical datasets represented as RDF data cubes. The work
is motivated in particular by the needs of three clinical
organisations who wish to develop a platform for collabo-
ratively analysing clinical data that spans multiple clinical
sites, thus improving the statistical power of conclusions
that can be drawn (versus one source alone). Clinical
data  even in aggregated form  is of a highly sensi-
tive nature, and thus query federation engines must take
access policies into account.
In our initial work [15] SAFE has been evaluated against
the FedX engine, in this article, we extend our previous
work by (i) evaluating against two additional query feder-
ation engines (HiBISCuS and SPLENDID); (ii) increasing
the number of queries and datasets for evaluation exper-
iments; (iii) presenting a variety of improvements and
extended analyses for the data summary computation and
source selection procedures.
SAFE is developed as an extension on top of the
FedX federation engine to support two main features: (i)
optimisations tailored for federating queries over RDF
data cubes; and (ii) source selection using highly com-
pressed data summaries on the level of named graphs
that allows for integration with an existing access con-
trol layer. We evaluated these extensions based on our
internal data sets (private data owned by clinical organi-
sations) as well as external data sets (public data available
from the LOD cloud) in order to measure the effi-
ciency of SAFE against FedX, HiBISCuS and SPLENDID.
Our evaluation results show that, for our use-case(s),
SAFE outperforms FedX, HiBISCuS and SPLENDID
in terms of source selection and query execution
times.
In terms of future work, there are still a number of
possible routes to explore with respect to improving the
performance of SAFE. For example, in considering RDF
data cubes described in individual named graphs, we cur-
rently only include locality restrictions on subject groups
(ss joins); however, there is the possibility to enforce
such restrictions on other types of joins when they involve
terms from the QB vocabulary, such as, for example,
so joins on predicates like qb:dataSet. Furthermore,
the structure of such data also suggests a closer look
at the underlying join operator implementations: rather
than relying on the general FedX query processor, SAFE
could instead benefit from, for example, the ability to
push joins to remove sources following available locality
guarantees.
Endnotes
1 http://www.chuv.ch/
2 http://www.cing.ac.cy/
3 http://www.zeincro.com/
4 http://www.chuv.ch/
5 http://www.cing.ac.cy/
6 http://www.zeincro.com/
7 http://www.linked2safety-project.eu/
8 http://www.dataprotection.ie/docs/EU-Directive-95-
46-EC/89.htm
9We omit definitions of prefixes for brevity since they
are inessential to the discussion.
10 http://www.iso.org/iso/catalogue_detail.htm?
csnumber=52500
11Currently we assume this locality of ss joins occurs
in all cases since we deal purely with RDF data cubes;
however, our algorithm could be trivially extended to drop
or relax this locality principle in the presence of certain
predicates or on instances of certain classes that may be
described in multiple named graphs.
Khan et al. Journal of Biomedical Semantics  (2017) 8:5 Page 21 of 22
12 http://www.w3.org/TR/sparql11-query/#
BasicGraphPatterns
13A possible corner-case occurs here if graphs with
the same name appear in multiple datasets, but we will
assume that such a case does not occur.
Acknowledgements
This article is based on a conference paper discussed at the SWAT4LS 2014,
Berlin, Germany [15]. We thank the anonymous reviewers for their useful
comments.
Funding
This publication has emanated from research supported in part by the
research grant from Science Foundation Ireland (SFI) under Grant Number
SFI/12/RC/2289, EU FP7 project Linked2Safety (contract number 288328), EU
FP7 project GeoKnow (contract number 318159), the Millennium Nucleus
Center for Semantic Web Research under Grant NC120004, and Fondecyt
Grant No. 11140900.
Availability of data andmaterials
The authors have made all the data and materials used in the manuscript
available. The source-code (AGPL License) of SAFE and queries and datasets used
in the experiments can be found at https://github.com/yasarkhangithub/SAFE.
Authors contributions
YK and MS have lead the entire design and execution of the study and lead
drafting the manuscript. MM has helped in formalising algorithms in the
manuscript, and assisted in designing the experiments. AH has provided the
background information and related work of the study. QM helped in
preparing the datasets for performing experiments. DRS and RS have equally
contributed in supervising the entire study. All authors have equally reviewed
and agreed upon the submission of the manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Author details
1Insight Centre for Data Analytics, NUIG, Galway, Ireland. 2AKSW, University of
Leipzig, Leipzig, Germany. 3Centre for Semantic Web Research, DCC,
University of Chile, Santiago, Chile.
Received: 13 July 2016 Accepted: 10 January 2017
Segura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 
DOI 10.1186/s13326-017-0156-7
RESEARCH Open Access
Simplifying drug package leaflets written
in Spanish by using word embedding
Isabel Segura-Bedmar* and Paloma Martínez
Abstract
Background: Drug Package Leaflets (DPLs) provide information for patients on how to safely use medicines.
Pharmaceutical companies are responsible for producing these documents. However, several studies have shown
that patients usually have problems in understanding sections describing posology (dosage quantity and
prescription), contraindications and adverse drug reactions. An ultimate goal of this work is to provide an automatic
approach that helps these companies to write drug package leaflets in an easy-to-understand language. Natural
language processing has become a powerful tool for improving patient care and advancing medicine because it
leads to automatically process the large amount of unstructured information needed for patient care. However, to the
best of our knowledge, no research has been done on the automatic simplification of drug package leaflets. In a
previous work, we proposed to use domain terminological resources for gathering a set of synonyms for a given
target term. A potential drawback of this approach is that it depends heavily on the existence of dictionaries, however
these are not always available for any domain and language or if they exist, their coverage is very scarce. To overcome
this limitation, we propose the use of word embeddings to identify the simplest synonym for a given term. Word
embedding models represent each word in a corpus with a vector in a semantic space. Our approach is based on
assumption that synonyms should have close vectors because they occur in similar contexts.
Results: In our evaluation, we used the corpus EasyDPL (Easy Drug Package Leaflets), a collection of 306 leaflets
written in Spanish and manually annotated with 1400 adverse drug effects and their simplest synonyms. We focus on
leaflets written in Spanish because it is the second most widely spoken language on the world, but as for the
existence of terminological resources, the Spanish language is usually less prolific than the English language. Our
experiments show an accuracy of 38.5% using word embeddings.
Conclusions: This work provides a promising approach to simplify DPLs without using terminological resources or
parallel corpora. Moreover, it could be easily adapted to different domains and languages. However, more research
efforts are needed to improve our approach based on word embedding because it does not overcome our previous
work using dictionaries yet.
Keywords: Text simplification, Lexical simplification, Word embeddings, Drug package leaflets
Background
Since 2001, according to a directive of the European Par-
liament (Directive 2001/83/EC) [1], every drug product
must be accompanied by a package leaflet before being
placed on the market. This document provides informa-
tive details about a medicine, including its appearance,
actions, side effects and drug interactions, contraindica-
tions, special warnings, among others. This directive also
*Correspondence: isegura@inf.uc3m.es
Computer Science Departament, Universidad Carlos III de Madrid, Avenida de
la Universidad, 30, Madrid, Spain
required that drug package leaflets (DPLs) must be writ-
ten in order to provide clear and comprehensible infor-
mation for patients because their misunderstanding could
be a potential source of drug related problems, such as
medication errors and adverse drug reactions.
In 2009, the European Commission published a guide-
line [2] with recommendations and advices in order to
issue package leaflets with accessible and understandable
information for patients. However, recent studies [3, 4]
show that the readability and understandability of these
documents have not been improved during the last years.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Segura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 Page 2 of 9
In particular, a recent work [5] about readability of DPLs
corresponding to 36 drugs downloaded from European
Medicines Agency website in 2007, 2010 and 2013 years,
concluded that there was no improvement in the read-
ability of the package leaflets studied between 2007 and
2013, despite the European Commissions 2009 guideline
on the readability of these leaflets that established differ-
ent rules to guarantee that patients can easily understand
them. Therefore, further efforts must be made to improve
the readability and understandability of DPLs in order
to ensure the proper use of medicines and to increase
patient safety.
One of the main reasons why the understandability has
not been improved is that these documents still con-
tain a considerable number of technical terms describing
adverse drug reactions, diseases and other medical con-
cepts. Posology (dosage quantity and prescription), con-
traindications and adverse drug reactions seem to be the
sections most difficult to understand [6]. To help solving
this problem, we propose an automatic system to sim-
plify those terms describing adverse drug effects in DPLs.
Text simplification is a natural language processing (NLP)
task that aims to rewrite text into an equivalent one with
less complexity for readers. Text simplification techniques
have been applied to simplify texts from different domains
such as crisis management [7], health information [810],
aphasic readers [11], language learners [12].
To the best of our knowledge, our previous work [13]
is the only research about the automatic simplification
of DPLs. We focus on lexical simplification, that is, the
substitution of complex concepts with simpler synonyms.
Moreover, we focus on leaflets written in Spanish because
it is the second most widely spoken language on the
world1. Our first approach consisted of choosing the
most frequent synonym as the simplest one. To do this,
we used specialized dictionaries for medicine for obtain-
ing the set of synonym candidates for a given term,
and then, we calculated their frequencies in a large col-
lection of documents. In this new work, we focus our
efforts on exploring a domain-independent and language-
independent approach, such as the use of word embed-
ding. A word embedding is a function that transforms
words into real-value vectors. This representation ensures
that similar words have similar vectors, that is, their vec-
tors are close together. At this time there is a explosion
of research based on word embeddings applied to a wide
variety of NLP tasks with very successful results. Although
several works have already exploited the use of word
embeddings for detecting complex words [14], building
parallel corpus for text simplification [15] or substitution
of complex words [16], the lexical simplification of DPLs
is still an unexplored field. In addition, our work is one of
the few studies that addresses the simplification of texts
written in Spanish.
Related work
There are two main subtasks of text simplification: lexical
and syntactic simplification. Lexical simplification basi-
cally consists of replacing complex concepts with simpler
synonyms, while syntactic simplification aims to reduce
the grammatical complexity of a text while preserving its
meaning. Comprehensive surveys of the text simplifica-
tion field can be found in [17, 18]. We have to distinguish
between readability and understandability because these
concepts capture different aspects of the complexity of
the text. Readability concerns the length and structure
of sentences (syntax) and consequently requires syntac-
tic simplification approaches to split sentences in shorter
units with simpler structure. On the other side, under-
standability is about the difficulty to interpret a word
[19] and it requires lexical simplification approaches. Our
work here focuses on improving the understandability
of DPLs by replacing terms describing drug effects by
simpler synonyms.
The main challenges of lexical simplification are (a)
the difficulty of recognizing if a word is a complex term
and (b) identifying the correct synonym for a particu-
lar context in which the word appears (this is crucial,
especially when the word is polysemous). For the first
issue (a), a common heuristic used is to select as com-
plex words those that have a low frequency in a corpus
(complex words tend to be rarer), but also to combine fre-
quency with word length (words withmore than a number
of syllables/characters could be considered complicated
words). In Semeval 2012 English Lexical Simplification
challenge2 with ten participant systems, the evaluation
results showed that proposals based on frequency gave
good results comparing to other sophisticated systems.
Similarly, the complex word identification task of SemEval
2016 [20] showed that though decision trees and ensemble
methods achieved satisfactory performance, word fre-
quency is still the most efficient predictor of word com-
plexity. Decision trees and ensemble methods perform
better than neural networks because the small size of the
training dataset. The only system exploiting word embed-
dings was developed by Sanjay et al. [14], who trained a
support vector machine (SVM) algorithm to identify com-
plex words. In addition to word embeddings, the feature
set also included orthographic word features, similarity
features and Part-of-Speech (POS) tag features.
Parallel corpora of original and simplified texts can be
used for automatic text simplification. Biran et al. [21]
used English Wikipedia and Simple English Wikipedia
[22] (which was developed applying Easy-to-Read (E2R)
guidelines3) to calculate the complexity of a word as the
ratio of its frequencies in each corpus.
For the issue (b), there are two main approaches: using
lexical resources or using context words and n-grams
models. Lexical resources (such asWordNet [23]) are used
Segura-Bedmar and Martínez Journal of Biomedical Semantics  (2017) 8:45 Page 3 of 9
to propose synonyms as candidates in order to replace
complex wordS. Lexical resources are also combined with
probabilistic models, as has been tried in [24]. In the sec-
ond approach, word contexts are used in [25] and [21],
where a vector space model is used to capture lexical
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 
DOI 10.1186/s13326-017-0114-4
RESEARCH Open Access
Building a model for disease classification
integration in oncology, an approach based
on the national cancer institute thesaurus
Vianney Jouhet1,2*, Fleur Mougin2, Bérénice Bréchat1,2 and Frantz Thiessard1,2
Abstract
Background: Identifying incident cancer cases within a population remains essential for scientific research in
oncology. Data produced within electronic health records can be useful for this purpose. Due to the multiplicity of
providers, heterogeneous terminologies such as ICD-10 and ICD-O-3 are used for oncology diagnosis recording
purpose. To enable disease identification based on these diagnoses, there is a need for integrating disease
classifications in oncology. Our aim was to build a model integrating concepts involved in two disease classifications,
namely ICD-10 (diagnosis) and ICD-O-3 (topography and morphology), despite their structural heterogeneity. Based
on the NCIt, a derivative model for linking diagnosis and topography-morphology combinations was defined and
built. ICD-O-3 and ICD-10 codes were then used to instantiate classes of the derivative model. Links between
terminologies obtained through the model were then compared to mappings provided by the Surveillance,
Epidemiology, and End Results (SEER) program.
Results: The model integrated 42% of neoplasm ICD-10 codes (excluding metastasis), 98% of ICD-O-3 morphology
codes (excluding metastasis) and 68% of ICD-O-3 topography codes. For every codes instantiating at least a class in
the derivative model, comparison with SEER mappings reveals that all mappings were actually available in the
model as a link between the corresponding codes.
Conclusions: We have proposed a method to automatically build a model for integrating ICD-10 and ICD-O-3 based
on the NCIt. The resulting derivative model is a machine understandable resource that enables an integrated view of
these heterogeneous terminologies. The NCIt structure and the available relationships can help to bridge disease
classifications taking into account their structural and granular heterogeneities. However, (i) inconsistencies exist
within the NCIt leading to misclassifications in the derivative model, (ii) the derivative model only integrates a part
of ICD-10 and ICD-O-3. The NCIt is not sufficient for integration purpose and further work based on other
termino-ontological resources is needed in order to enrich the model and avoid identified inconsistencies.
Keywords: NCI thesaurus, Oncology, Terminology, Semantic integration, ICD-10, ICD-O-3
Background
With the increasing adoption of electronic health records
(EHRs), the amount of data produced at the patient
bedside is rapidly increasing. These data provide new
perspectives to: create and disseminate new knowledge;
consider the implementation of personalized medicine
*Correspondence: vianney.jouhet@isped.u-bordeaux.fr
1CHU de Bordeaux, Pole de sante publique, Service dinformation medicale,
unit IAM, F-33000 Bordeaux, France
2Univ. Bordeaux, Inserm, Bordeaux Population Health Research Center, team
ERIAS, UMR 1219, F-33000 Bordeaux, France
and offer to patients the opportunity to be involved in
the management of their own medical data [1]. Secondary
use of biomedical data produced throughout patient care
is an essential issue [2] and is the subject of numerous
studies over several years [16]. Since 2007, the American
Medical Informatics Association emphasized the value of
secondary use of medical data: Secondary use of health
data can enhance healthcare experiences for individuals,
expand knowledge about disease and appropriate treat-
ments, strengthen understanding about the effectiveness
and efficiency of our healthcare systems, support public
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 2 of 12
health and security goals, and aid businesses inmeeting the
needs of their customers [4].
In the oncology field, it is necessary to identify and
describe incident cancer cases within a population in
order to facilitate research and public health monitor-
ing. For instance, cancer registries exhaustively record
incident cases of cancer in a given territory (which cor-
respond to all new cancer cases occurring over a geo-
graphical territory). This task remains time consuming
if it is performed manually. As early as 1998, a techni-
cal report was drawn up by the International Agency for
Research on Cancer describing the methods used by dif-
ferent registries for establishing automated procedures to
identify new cases using available data [7]. Methods have
been proposed for automatically identifying and register-
ing cancers using structured data indexed with standard
terminologies [812].
However, many different medical specialties are con-
tributing to record information in EHRs. As a result,
within EHRs, data describing diseases are recorded
according to multiple heterogeneous terminologies even
for a single disease occurring in a single patient. For
instance, in France, reimbursement data use the 10th
revision of the International Statistical Classification of
Diseases and Related Health Problems (ICD-10) [13] to
describe diseases, whereas pathology data use either ADI-
CAP (a French pathology terminology) or the 3rd edi-
tion of the International Classification of Diseases for
Oncology (ICD-O-3) [14] and data frommultidisciplinary
meetings in oncology use ICD-O-3. Providing an inte-
grated access to these disease classifications may improve
automated cancer identification.
Although ICD-10 and ICD-O-3 both describe cancer
diseases, they exhibit differences in terms of structure
and granularity. Thus, it is necessary to identify or to
build a resource that allows the integration of cancer
disease classifications, taking into account these hetero-
geneities. To achieve this goal, relations must be defined
between the involved concepts, such as a neoplasm is a
disease and has a specified morphology as well as a speci-
fied topography. The National Cancer Institute thesaurus
(NCIt)provides reference terminology covering vocabulary
for clinical care, translational and basic research, and pub-
lic information activities(cited from http://ncit.nci.nih.
gov/, visited 2015-01-22). It is described as a controlled
terminology which exhibits ontology-like properties in its
construction and use [15]. These characteristics open
up the possibility [...] in linking together heterogeneous
resources created by institutions external to the NCI [16].
Thus, the NCIt could be used as a resource to bridge the
gap between disease classifications, which are structurally
heterogeneous.
However, since 2005, it has been shown on many occa-
sions that the NCIt remains flawed [1618] and especially
that logic-based reasoning over the NCIt should be used
cautiously. On the other hand, re-building a model from
scratch would be time consuming and comes with no
guarantee of avoiding inconsistencies. Despite the lim-
itations described above, the NCIt contains knowledge
that could be useful for our integration purpose. In this
manuscript, we propose an approach to build a resource
based on a subset of the NCIt, linking the three axes that
refer to diseases as described in ICD-10 and ICD-O-3, i.e.,
the diagnosis as well as its morphology and its topography.
ICD-10
Within ICD-10, chapter 2 corresponds to neoplasms.
It is divided into four axes depending on the behav-
ior of the tumor (namely Malignant neoplasms, In situ
neoplasms, Benign neoplasms and Neoplasms of uncer-
tain or unknown behavior). Within the Malignant neo-
plasms block, ICD-10 categories differentiate primary
tumors from metastatic secondary tumors. In the same
way as for ICD-O-3, a neoplasm cannot have multi-
ple behaviors. ICD-10 describes each neoplastic disease
as a whole concept represented by a unique code. For
instance, C50.2: Malignant neoplasm upper-inner quad-
rant of breast describes two characteristics of the cancer
disease:
 The behavior (Malignant) which is part of the
morphology description.
 The site of origin (upper-inner quadrant of breast)
which corresponds to the topography.
ICD-O-3
ICD-O-3 is a multi-axial classification used in cancer reg-
istries in order to record the anatomic site (topography)
and the morphology of a neoplasm. The morphology is
coded with five digits. The first four digits represent the
histological description and the fifth digit indicates the
behavior (i.e. whether benign ormalignant) of a neoplasm.
As a result, it is not possible for a morphology to have
multiple behaviors. The topography code indicates the site
of origin of a neoplasm; in other words, where the tumor
arose [14]. From the ICD-O-3 point of view, any mor-
phology code can be associated with any topography code.
Some tumor morphologies have a usual primary site
but it is expressly stated that these associations are pro-
vided only to help coders and should not be considered
as systematic (and unique) topography-morphology com-
binations. An example is given in [14]: An unusual, but
possible, example would be the diagnosis osteo-sarcoma
of kidney, for which the kidney topography code (C64.9)
would be used instead of bone, NOS (C41.9) [. . . ]. Thus,
ICD-O-3 describes a disease by combining the morphol-
ogy of the tumor and the topography from where the
tumor arises. As a result, each neoplastic disease is not
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 3 of 12
described as a whole concept entailed by a unique code
within ICD-O-3.
Concepts involved in ICD-10 and/or ICD-O-3
Even if they are called disease classifications, ICD-10 and
ICD-O-3 are in fact used within EHR for recording diag-
noses. The diagnosis is a way for the physician to describe
the disease, which corresponds to an evolving process but,
in fact, it is not the disease itself. A single disease may
have multiple diagnoses all along its clinical course (for
instance, an in situ neoplasm may evolve and become a
malignant invasive neoplasm) but the disease (process)
remains the same. Thus, when used in this context, disease
classifications are, in fact, kinds of diagnoses which can
be viewed as opinions about the undergoing disease. This
assertion is in accordance with the definition proposed by
Scheuermann et al. in [19] who claim that a diagnosis is
a conclusion of an interpretive process that has as input a
clinical picture of a given patient and as output an asser-
tion to the effect that the patient has a disease of such and
such a type. A diagnosis is a continuant entity that, once
made, will survive through time, and is often supplanted by
further diagnoses. The diagnostic process is thus iterative:
the clinician is forming hypotheses during history taking,
testing these during physical exam, forming new hypotheses
as a result, and so on.
In the oncology field, a diagnosis describes two major
facts about the disease: (i) the type of tumoral cells (Mor-
phology) and (ii) its site of origin (Topography). Thus,
ICD-10 and ICD-O-3 both allow to record diagnoses but
their structure differs slightly. As a result, three different
kinds of concepts are involved when considering these two
terminologies:
 The morphology of the tumor, which is a
representation of the pathological description of the
tumor reported at a given time. Morphology is
represented within the ICD-O-3 morphology axis.
 The topography of the tumor, which is a
representation of the anatomical site of origin of the
tumor reported at a given time. Topography is
represented within the ICD-O-3 topography axis.
 The diagnosis, which is a representation of the
reported description of the tumor and encompasses
information about both the topography and the
morphology of the tumor. Diagnosis is represented as
such within ICD-10 and can be built by combining an
ICD-O-3 topography and an ICD-O-3 morphology.
Because it is not possible to state that a diagnosis is
equivalent to either a topography or a morphology, it
is obviously not possible to find equivalences between
concepts represented within these two terminologies.
The unique correspondences that can be found between
ICD-10 and ICD-O-3 concepts are thus a diagnosis (i.e.,
an ICD-10 code) mapped to a topography-morphology
combination (i.e., a pair of an ICD-O-3 topography code
and an ICD-O-3 morphology code).
The national cancer institute thesaurus (NCIt)
NCI Thesaurus (NCIt) is NCIs reference terminology.
NCIt provides the concepts used in caCORE and caBIG to
establish data semantics. It covers terminology for clinical
care, translational and basic research, and public infor-
mation and administrative activities. NCIt is also a widely
recognized standard for biomedical coding and reference,
used by a broad variety of public and private partners both
nationally and internationally [20].
In the NCIt, topographies are described in theAnatomic
structure, system, or substance axis. Morphologies and
diagnoses are represented within the same hierarchy, sub-
sumed by Neoplasm. Thus, no specific axis for tumor
morphologies is defined and diagnoses are modeled as
anatomic specializations of morphologies. For example,
Breast adenocarcinoma is_a Adenocarcinoma is stated in:
Breast adenocarcinoma ? Adenocarcinoma
? Breast carcinoma
Some NCIt concepts are annotated as being mapped
to some ICD-O-3 morphologies. For example, Invasive
ductal carcinoma, not otherwise specified is annotated as
beingmapped to two ICD-O-3morphology codes (8500/3
Infiltrating duct carcinoma, NOS and 8521/3 Infiltrat-
ing ductular carcinoma). The semantics of this mapping
annotation are not defined (i.e., exact match or another
type of relationship). In the NCIt, even if the term disease
is employed, it is not clear whether Neoplasm repre-
sents the disease or the diagnosis. For instance, in the
NCI term Browser (https://ncit.nci.nih.gov/ncitbrowser/
pages/home.jsf?version=16.10e), Neoplasm is defined as
A benign or malignant tissue growth. . .  and An abnor-
mal mass of tissue. . . . Disease classifications are mainly
used in EHR for diagnoses recording. In the remain-
ing part of this manuscript, we use the NCIt concept
Neoplasm as a kind of diagnosis describing the disease.
An OWL-DL representation of the NCIt is freely avail-
able in the Web ontology Language (OWL) format on
the NCI website (https://cbiit.nci.nih.gov/evs-download/
thesaurus-downloads). Although logic-based reasoning
can be made with this OWL-DL representation, some
inconsistencies have been identified and it has been
shown that the NCIt should be used cautiously for this
purpose [1618].
NCI Metathesaurus [21]
The NCI Metathesaurus (NCIm) is a biomedical termi-
nology database that covers most terminologies used by
NCI for clinical care, translational and basic research,
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 4 of 12
and public information and administrative activities [21],
including ICD-10 and ICD-O-3. It has been built and is
maintained by the NCI. Its structure and a significant part
of its concepts is based on the UMLS Metathesaurus [22].
Inside the NCIm, elements coming from different termi-
nologies but representing the same biomedical notion are
grouped into the same Concept Unique Identifier (CUI).
Methods
We focused our study on primary tumor descriptions,
ignoring metastases and uncertain behaviors. ICD-10 and
ICD-O-3 do not have a formal representation. In [23],
authors recommend to use SKOS to describe the knowl-
edge of such resources. In order to bridge these two termi-
nologies, it is necessary to identify how concepts that are
represented within them (diagnosis, morphology, topog-
raphy) are related. These relationships should therefore
be represented at the conceptual level so that they could
be machine readable. Moreover, concepts represented by
terminologies should be conceptually defined and related
to corresponding codes. As a result, the targeted model
remains independent from terminologies to be integrated,
thus enabling the integration of other disease classifica-
tions. Our approach was to follow the W3C recommen-
dations to define formal and semi-formal hybrid models
[24] in order to build a model combining SKOS for the
description of terminologies and OWL for representing
involved concepts and for defining relationships between
these concepts, as proposed in [23]. Figure 1 presents the
organization of the proposed model using Graffoo [25].
The methods are composed of three steps:
 Defining a formal pattern for linking diagnosis,
topography and morphology
 Building a model based on the NCIt corresponding to
the formal pattern
 Instantiating the model with terminologies
Defining a formal pattern for linking diagnosis,
topography andmorphology
In order to link ICD-10 and ICD-O-3 concepts, it is
necessary to determine which relationships are involved
and how these relationships associate concepts with each
other. A topography-morphology combination in ICD-
O-3 leads to a diagnosis description. ICD-O-3 axes can
be viewed as descriptors that, when combined, provide
necessary and sufficient information to represent a diag-
nosis. For instance, the diagnosis Malignant neoplasm of
lower-outer quadrant of breast in ICD-10 can be defined
as a malignant neoplasm arising from the lower-outer
quadrant of breast (because it is defined as a presumed
or stated primary malignant tumor within ICD-10). As
stated above, the topography of a tumor (and more pre-
cisely, its primary site) is the anatomical site from which
Fig. 1 Graffoo [25] representation of the proposed model. The model is formal and semi-formal hybrid. Terminologies (ICD-10 and ICD-O-3) are
represented in SKOS. Above them, a formal model is represented in OWL. Every OWL class of the formal model are subclasses of skos:Concept so
that they can be instanciated by terminological artifacts
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 5 of 12
a tumor arises. As a result, a diagnosis has a specific
relationship with the topography. The mention of aris-
ing from is ambiguous because this relationship implies
that the tumor arises from the topography as a whole
(lower-outer quadrant of breast) or from a part of this
topography (a part of lower-outer quadrant of breast).
Indeed, if a digestive systems tumor is reported, it may
refer to a tumor that originates from a part of the diges-
tive system and not from the whole digestive system. In
order to capture the fact that a primary tumor refers to
a primary site as a whole and all its parts, we need spe-
cific topography classes. In [26], the W3C describes a way
to represent those reflexive parts (e.g., Class(CarPart_-
reflexive complete unionOf(Car CarPart))). This pattern
(called S-node) has been proposed for the biomedical
domain in [27, 28]. Formally, we can define the Malig-
nant neoplasm of the lower-outer quadrant of breast as a
diagnosis whosemorphology is a malignant neoplasm and
whose primary site is the reflexive part of the lower-outer
quadrant of breast. For describing the link between a diag-
nosis and its morphology as well as its anatomical site, we
need to introduce the two following relationships (object
properties in OWL parliance):
 has_morphology: for modeling the relation between a
diagnosis and the type of cells (morphology) that are
stated to be involved in the tumor described by the
diagnosis.
 has_primary_site: for modeling the relation between
a diagnosis and an anatomical site (topography) that
is stated to be the origin of the tumor described by
the diagnosis.
In description logics, the definition of the Malignant
neoplasm of the lower-outer quadrant of breast diagnosis
can be stated as follows:
Malignant neoplasm of lower outer quadrant of
breast ?
Diagnosis
?? has_morphology.Malignant neoplasm
?? has_primary_site.Lower outer quadrant of
breast Reflexive part
In addition, because of its expressivity, ICD-O-3 pro-
vides finer-grained information about the morphology of
diagnoses than ICD-10 does. For instance, an adenocar-
cinoma arising from the lower-outer quadrant of breast
can be reported using ICD-O-3. In ICD-10, there is no
code corresponding to this diagnosis. However, an ade-
nocarcinoma being a type of malignant neoplasm, an
adenocarcinoma arising from the lower-outer quadrant of
breast can be defined as a type of malignant neoplasm
arising from the lower outer quadrant of breast (which is a
coarser grained concept that exists in ICD-10). Formally,
this can be expressed in description logics as follows:
(Diagnosis)
?? has_morphology.Adenocarcinoma
?? has_primary_site.Lower outer quadrant of breast
Reflexive part) ?
Malignant neoplasm of lower outer quadrant
of breast
Building a model based on the NCIt corresponding to the
formal pattern
Building a part-whole lattice
In order to address the integration of diagnoses (ICD-10)
with topographies andmorphologies (ICD-O-3), the NCIt
relationship disease_has_primary_anatomic_site is of par-
ticular interest. The NCIts definition of this relationship
is: A role used to relate a disease to the anatomical site
where the originating pathological process is located. The
domain and the range for this role are Disease, Disorder or
Finding and Anatomic Structure, System, or Substance.
This relationship is equivalent to the has_primary_site
relationship defined in the previous subsection. As dis-
cussed on page 4, we consider that the primary anatomic
site of a tumor encompasses the site itself and all its parts
(this definition is in accordance with is located, which
is mentioned in the NCIt definition of the disease_has_-
primary_anatomic_site relationship). In order to make
this description possible, we have built a subsumption
lattice composed of classes defined as the reflexive part
of each Anatomic Structure, System, or Substance. For
instance, the Lower outer quadrant of breast Reflexive part
was defined as follows:
Lower outer quadrant of breast Reflexive part ?
Lower outer quadrant of breast
?? part_of.Lower outer quadrant of breast
The lattice was built with DL-reasoning over classes
defined using two part-whole relationships available in
the NCIt (namely anatomic_structure_is_physical_part_-
of and anatomic_structure_has_location).
Isolatingmorphologies
In contrast, no morphology axis is distinguished as such
within the NCIt, and it is not possible to find a relation-
ship equivalent to the aforementioned has_morphology.
However, theNCIt provides amapping between diagnoses
and ICD-O-3 morphology codes. We have added classes
corresponding to ICD-O-3 morphologies as types of the
NCIt concept Findings and, based on the NCIt mappings,
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 6 of 12
we have built new refined diagnosis concepts defined
according to the following model:
[NCIt diagnosis concept (refined)] ?
[NCIt concept]
?? disease_has_finding.[Morphology mapped to
the NCIt concept]
For instance, in the NCIt, Adenocarcinoma is mapped
to the 8140/3 ICD-O-3 morphology (Adenocarcinoma,
NOS) and we defined the corresponding refined NCIt
concept as follows:
Adenocarcinoma (refined) ?
Adenocarcinoma
?? disease_has_finding.Adenocarinoma, NOS
(ICD-O-3 morhology)
Each of the morphologies were classified depending
on their tumoral behavior as described in ICD-O-3
(i.e., benign, malignant primary, in situ, malignant
metastatic, unknown whether benign or malignant,
unknown whether primary or metastatic).
Building themodel
Using reflexive part anatomic concepts and morpholo-
gies and adapting the description logics expressions pro-
posed in page 4, we have defined a formal pattern to
describe relationships between diagnoses, morphologies
and topographies within the derivative NCIt. In descrip-
tion logics, the pattern is the following:
Diagnosis ?
?disease_has_finding.Morphology
?? disease_has_primary_anatomic_site.
Topography_Reflexive_Part
Based on the defined pattern, we implemented and
executed the following algorithm:
+ For each (Morphology identified ?
[Morphology])
+ For each (Topography_Reflexive_Part identified
? [Topography])
+ Build [expression] of the form:
? disease_has_finding.[Morphology]
?? disease_has_primary_anatomic_site.
[Topography]
+ If at least one subclass of [expression] exists
in the NCIt then
+ Build the Diagnosis class defined as
equivalent to [expression]
Finally, we have implemented the model presented in
Fig. 1 containing:
1. Morphologies,
2. Reflexive part topographies,
3. Diagnoses identified by the aforementioned
algorithm.
Instantiating themodel with disease classifications
ICD-O3 ICD-O3 morphologies were represented as
instances of the built-in morphology classes. ICD-O-3
topographies were represented as instances of the built-in
reflexive part topographies. Reflexive part topographies to
be instantiated by ICD-O-3 topographies were identified
as follows:
1. Identify mappings between ICD-O-3 topographies
and NCIt concepts having the same CUI within the
NCIm,
2. Define these codes as instances of the corresponding
NCIt concept,
3. Retrieve the corresponding reflexive part topography
after DL-reasoning.
ICD-10 ICD-10 codes were represented as instances of
built-in diagnoses classes. Diagnoses to be instantiated by
ICD-10 codes were identified as follows:
1. Identify mappings between ICD-10 codes and NCIt
concepts having the same CUI within the NCIm,
2. Define concepts corresponding to ICD-10 codes
based on the NCIt definition (by adding a restriction
for the primary site based on the NCIt concept
formal definition) and ICD-10 (by adding a
restriction for the behavior) semantics. For instance :
 Breast, Unspecified (C50.9) is a malignant
primary neoplasm within the ICD-10
classification.
 Breast, Unspecified (C50.9) has the same CUI as
Malignant Breast Neoplasm (C9335) within the
NCIm.
 Malignant Breast Neoplasm (C9335) has as
associated primary site Breast (C12971) within
the NCIt.
 The built expression describing Breast,
Unspecified (C50.9) was then:
Malignant Breast Neoplasm
?? disease_has_finding.Malignant primary
neoplasm
?? disease_has_primary_anatomic_site.Breast
3. Retrieve the corresponding diagnosis after
DL-reasoning.
Evaluation of the model
The National Cancer Institute provides, within the
Surveillance, Epidemiology, and End Results (SEER) Pro-
gram, a set of tools for ICD conversions [29]. We used
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 7 of 12
the 2014-05-08 conversion file of ICD-O-3 to ICD-9-
CM, to ICD-10 (Causes of Death) and to ICD-10-CM
(available at http://seer.cancer.gov/tools/conversion/) as a
gold standard for evaluating how ICD-O-3 and ICD-10
could be related. Based on this file, we have rebuilt ICD-
O-3 topography-morphology combinations mapped to
ICD-10 codes. A 2-step evaluation was then performed:
 For each combination, we queried the proposed
model in order to evaluate how many branches of the
diagnosis lattice were instantiated by both the ICD-10
code and the topography-morphology combination.
 We tried to build mappings based on the proposed
model with a simple algorithm (ICD-10 codes and
topography-morphology combinations with the
minimum hierarchical edge-based distance were
considered as mapped) and compared it with the gold
standard.
Results
All the analyses were processed over the OWL-DL ver-
sion of the NCIt (14.11d) available at http://evs.nci.nih.
gov/ftp1/NCI_Thesaurus/.
Built model based on the NCIt
A total of 6720 topographies involved in at least one
diagnosis definition was identified and the corresponding
topographies reflexive parts were introduced in the NCIt
in order to build the topography reflexive parts lattice
(Section: Building a part-whole lattice). A total of 1120
NCIt codes was identified as being related to 1094 ICD-O-
3 morphology codes. The 1094 corresponding morphol-
ogy classes were added to the model and automatically
classified under six general morphology classes depending
on their behavior leading to a set of 1100 possible mor-
phologies. Combining the 1100 morphology classes with
the 6720 reflexive part topographies, 7392000 expressions
were built. A total of 20133 (0.27%) expressions sub-
suming at least one NCIt code were identified and the
corresponding classes were introduced in the model as
diagnoses.
Instantiating the model with disease classifications
Table 1 presents the part of each terminology that was
covered by the final model. The numbers of codes to be
integrated were:
 409 ICD-O-3 topographies
 873 ICD-O-3 morphologies (excluding /6 Malignant
neoplasms, stated or presumed to be secondary and
/1 Neoplasms of uncertain and unknown behavior)
 727 ICD-10 neoplasms (excluding C81-C96
Malignant neoplasms, stated or presumed to be
Table 1 Part of ICD-O-3 and ICD-10 terminologies integrated
within the final model
Total number Instantiating the final model
ICD-O-3 Topographies 409 278 (68.0%)
ICD-O-3 Morphologies 873 860 (98.5%)
ICD-10 727 302 (41.5%)
ICD-10 Benign 180 73 (40.5%)
ICD-10 In situ 66 22 (33.3%)
ICD-10 Malignant 481 207 (43.0%)
secondary and D37-D48 Neoplasms of uncertain or
unknown behavior)
Using the NCIm, 298 ICD-O-3 topography codes were
linked to 540 NCIt codes. Within these NCIt codes, 29
were not subclasses of Anatomic Structure, System, or
Substance. Among the 298 ICD-O-3 topography codes,
20 were related only to these 29 codes and were then
excluded (e.g., C05.1 Soft palate, NOS was erroneously
mapped to Malignant Soft Palate Neoplasm). Thus, 278
topography codes were finally included within the model
as instances of the corresponding NCIt codes and clas-
sified as instances of topography reflexive parts after
DL-reasoning. Using the NCIm, 302 ICD-10 codes were
linked to NCIt codes. Building the corresponding expres-
sions and after DL-reasoning, we were able to add 302
ICD-10 codes as instances of 380 diagnoses.
Characteristics of the final model
The resulting model is constituted of 113643 axioms,
including 27953 classes (6720 topographies, 1100 mor-
phologies and 20133 diagnoses). A total of 1440 codes
were instantiated (278 ICD-O-3 topographies, 860 ICD-
O-3 morphologies and 302 ICD-10 codes).
Within the model, a significant part of ICD-10 (51%)
and ICD-O-3 topography codes (28%) are instances of
multiple classes (Table 2). This situation arises when the
hierarchy of diagnoses within the NCIt is not in accor-
dance with the topography or the morphology that we
used to describe them. For example Colon Cavernous
Hemangioma is a direct subclass of the following expres-
sions:
 ? disease_has_finding.Cavernous hemangioma
? ? disease_has_primary_anatomic_site.Colorectal
Region Reflexive part
 ? disease_has_finding.Cavernous hemangioma
? ? disease_has_primary_anatomic_site.Colon
Reflexive part
 ? disease_has_finding.Hemangioma,NOS
? ? disease_has_primary_anatomic_site.Colorectal
Region Reflexive part
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 8 of 12
Table 2 Number of codes instantiating multiple classes in the
model
N Instances of multiple classes
ICD-O-3 Topographies 278 79 (28.4%)
ICD-O-3 Morphologies 860 0 ( - %)
ICD-10 302 153 (50.7%)
ICD-10 Benign 73 26 (35.6%)
ICD-10 In situ 22 22 ( 100%)
ICD-10 Malignant 207 105 (50.7%)
 ? disease_has_finding.Hemangioma,NOS
? ? disease_has_primary_anatomic_site.Colon
Reflexive part
The explanation for this situation is twofold: (1) there is
neither an anatomic_structure_has_location relationship,
nor an anatomic_structure_is_physical_part_of relation-
ship between Colon and Colorectal Region within the
NCIt; (2) Colon Cavernous Hemangioma is described
as having these two anatomic structures as a primary
site. On the other hand, through the NCIt diagnosis lat-
tice, Colon Cavernous Hemangioma is described as being
a subclass of the concepts Cavernous hemangioma and
Hemangioma, NOS.
Comparison with the SEER conversion file
Based on the SEER conversion file, excluding metastatic
and uncertain behaviors from ICD-10 and ICD-O-3
morphologies, we were able to build 103950 mappings
between an ICD-10 code and an ICD-O-3 topography-
morphology combination. Due to the absence of some
correspondences within the NCIm between the NCIt and
ICD10 and between the NCIt and ICD-O-3, some ICD-
10 and ICD-O-3 codes do not instantiate any class in
the derivative model. As a result, 59% of these map-
pings could not be evaluated (because the ICD-10 code,
the ICD-O-3 topography or the ICD-O-3 morphology
was missing). Table 3 presents the results of the evalu-
ation over the 42260 mappings combining codes which
instantiate classes within the resulting model. The model
relates 100% of the mappings through at least a diagnosis.
A significant part of these mappings (36%) are related to
more than one branch of the diagnosis lattice, especially
for hematopoietic tumors (70%). Using the simple algo-
rithm described above, the model was able to identify 42%
of the mappings of the SEER file (61% for solid tumors
and 5% for hematopoietic tumors). A quarter of these
topography-morphology combinations were also mapped
to another ICD-10 code, which is not consistent with the
SEER file.
Discussion
Implementedmethods to build the model
We achieved to automatically build a model based on
the NCIt, describing topographies, morphologies and
diagnoses that can be instantiated by both ICD-O-3 and
ICD-10 codes. As no morphological axis is available
within the NCIt, we have adapted the NCIt by adding
concepts corresponding to ICD-O-3morphologies, which
we related to the corresponding diagnoses (based on the
ICD-O-3 annotation of the NCIt).
For the description of topographies, we have built an
organ reflexive part lattice that enables the description of
a primary site as encompassing the site itself and all its
parts. These reflexive parts have been proposed for the
biomedical domain in [27].
The diagnoses lattice was then automatically generated
by DL-reasonners based on the topography and morphol-
ogy lattices avoiding is_a overloading [17]. As a result, the
obtained diagnoses subsumption lattice is a valid formal
representation of diagnoses hierarchy in respect to the
definitions of classes contributing to their description
In order to instantiate the model, we have used the
NCIm to identify links between the NCIt and the termi-
nologies (namely, ICD-10 and the ICD-O-3 topography
axis). For ICD-10, we had to add a restriction based on
the semantics available within the ICD-10 classification
in order to ensure that primary tumors were described
according to a primary site. The resulting model could not
Table 3 Comparison with the SEER conversion program according to the tumor type (hematopoietic and solid tumors) and the
number of branches of the diagnosis lattice that are identified for an ICD-10 code / ICD-O-3 combination
All Hematopoietic tumors Solid tumors
N = 42260 (%) N = 14213 (%) N = 28047 (%)
Related in the model* 42260 (100.0) 14213 (100.0) 28047 (100.0)
More than 1 brancha 15234 (36.1) 9910 (69.7) 5324 (18.9)
Mappings rebuilt from the model** 17766 (42.0) 739 (5.2) 17027 (60.7)
Non unique mappingsb 4886 (27.5) 333 (45.1) 4553 (26.7)
*Related in the model means that there is at least a common diagnosis inside the model that is instantiated by both the ICD-10 code and the ICD-O-3 combination
**Mappings rebuilt from the model corresponds to the mappings that we were able to rebuild automatically from the model
aMore than 1 branch means that there is more than one branch of the diagnosis lattice that was instantiated by both the ICD-10 code and the ICD-O-3 combination
bNon unique mappings means that the topography-morphology combination was also mapped to another ICD-10 code (inconsistent with the SEER file)
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 9 of 12
be instantiated completely by ICD-10 and ICD-O-3 codes
for different reasons:
 The NCIt completeness for describing diagnoses. As
our method relies on NCIt diagnoses, the resulting
classes which were built depend on their existence
within the NCIt (e.g., C00.1 External Lower Lip
malignant neoplasm is not available within the NCIt).
 The NCIm provides a way to identify common
concepts using the CUI but it can be incomplete or
wrong (e.g., 20 ICD-O-3 topographies were mapped
erroneously to non-anatomic concepts).
However, when the codes were found, we were able
to identify a common diagnosis for all cases described
in the SEER conversion program and, using a simple
algorithm, 42% of the SEER mappings (corresponding to
codes instantiating the model) could be rebuilt from the
model. Our aim is not to enable conversion between codes
but to provide a machine usable and semantically inte-
grated view over them. From this perspective, the model
is consistent because it provides links between ICD-10
diagnoses and ICD-O-3 topography-morphology com-
binations when they exist within the SEER conversion
file. Moreover, the model describes many more possible
relationships between diagnoses than the SEER conver-
sion program does. For instance, in the latter, there is
no relationship between Adenocarcinoma, NOS  Colon,
NOS (C18.9  M8140/3) and Malignant neoplasm of rec-
tosigmoid junction (C19.9) whereas our model identifies
successfully that they are both instances of Malignant,
primary site - Large Intestine Reflexive part.
Choice of the NCIt
In the biomedical field, other description logics-based ter-
minologies exist. Specifically, SNOMED-CT® provides not
only topography, morphology and diagnosis dimensions
but also implements relationships between these con-
cepts. However, the NCIt is specific to the oncology field
and provides useful knowledge related to neoplasm diag-
noses. The NCIt is freely and easily accessible.In contrast,
SNOMED-CT has amuchmore restrictive affiliate license
agreement and it is not easily accessible for countries
which are notmembers of the International Health Termi-
nology Standards Development Organisation (IHTSDO).
In addition, it has been shown that SNOMED-CTs for-
mal representation suffers from the same flaws [30, 31]
as the NCIt and has to be used cautiously while needing
logic-based reasoning. Thus, a similar evaluation could be
carried out on SNOMED-CT in order to estimate whether
it could be useful for integrating disease classifications in
oncology and to compare the results with what was found
when using the NCIt.
Limitations of the NCIt for integration purposes
In [18], Schultz et al. discussed that the OWL-DL ver-
sion of the NCIt may lead to unexpected results which
were not visible due to the lack of use cases needing logic-
based reasoning over the OWL-DL version of the NCIt.
The integration of heterogeneous disease classifications
corresponds to such a use case. We have identified some
limitations due to inconsistencies.
On the one hand, the NCIt provides concepts describ-
ing cancer diagnoses and, on the other hand, concepts
describing the tumor topography. It also provides rela-
tionships which are involved in topography-morphology
combinations, themselves expected to be equivalences of
diagnoses. Its formal representation and the availability
of an OWL version enable reasoning and the imple-
mentation of DL-queries. However, some intrinsic char-
acteristics prevent its direct use for the integration of
cancer disease classifications: (i) the absence of distinction
between morphologies and diagnoses; (ii) diagnosis con-
cepts described as having a specific primary site but not
its parts. We have proposed a method to address these
issues and to automatically build a consistent model based
on the NCIt and the intrinsic semantics available within
ICD-O-3 and ICD-10.
The obtained model representing diagnosis was clas-
sified using DL-reasoning ensuring concistency of the
subsumption lattice. Linking these derivative consistent
classes with diagnosis as represented within the NCIt can
be used as an auditing tool. During the building pro-
cess, a significant part of NCIt concepts was retrieved as
subclasses of multiple diagnosis classes. As a result, the
corresponding ICD-10 codes were defined as instances
of multiple diagnosis classes and 36% of SEER mappings
evaluated were retrieved as being related to more than
one branch in the diagnosis lattice. For instance, the ICD-
10 code C18.0Malignant neoplasm: Caecum was mapped
to the NCIt concept C9329 Malignant Cecum Neoplasm,
which is related to multiple anatomic sites: Gastrointesti-
nal System,Cecum,Colon, Intestine andColorectal Region.
As there is no relationship between Cecum, Colorectal
Region and Colon within the NCIt (except that they are
part of the large intestine), C18.0 instantiates the following
classes:
 Malignant, primary site  Cecum Reflexive part
 Malignant, primary site  Colon Reflexive part
 Malignant, primary site  Colorectal Region Reflexive
part
Two issues can be identified: (i) Malignant Cecum Neo-
plasm should not have Colon as an associated anatomic
site within the NCIt because Cecum is neither a part,
nor a subclass of Colon, (ii) Cecum and Colon should be
related to Colorectal Region. The former is due to is_a
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 10 of 12
overloading and has been discussed in [17]. The latter
issue is due to the lack of part_of relationships within
the NCIt. Another important issue can be identified for
in situ neoplasms. 100% of the in situ ICD-10 codes are
instances of more than one diagnosis within the model.
The NCIt asserts that a Carcinoma In situ is a Carcinoma,
which seems to be true. However, in the NCIt, Carcinoma
is related to the Carcinoma, NOS ICD-O-3 morphology
(having an invasive behavior) and Carcinoma In situ is
related to the Intraepithelial carcinoma, NOS ICD-O-3
morphology (having an in situ behavior). Consequently,
the subsumption relationship between Carcinoma In situ
and Carcinoma is not consistent because a tumor cannot
be both invasive and in situ at the same time. For instance,
D05 Carcinoma in situ of breast is mapped to the NCIt
concept C3641 Stage 0 Breast Cancer, which is related to
the Intraepithelial carcinoma, NOS and Epithelioma, NOS
ICD-O-3 concepts through the NCIt lattice. As a result,
D05 instantiates the following classes:
 Intraepithelial carcinoma, NOS - Breast Reflexive part
 Epithelioma, NOS - Breast Reflexive part
Because Intraepithelial carcinoma, NOS has an in situ
behavior and Epithelioma, NOS has a malignant, invasive
behavior, it is not consistent to be an instance of these
two diagnoses. This issue emphasizes erroneous map-
pings that may exist between ICD-O-3 and the NCIt due
to ambiguous labels. A simple solution to this problem
would be to add a concept representing the Carcinoma
category of which both Carcinoma a Carcinoma In situ
should be subclasses.
It is noteworthy that these patterns, which are mainly
due to is_a overloading, can easily be retrieved by search-
ing for codes which are instances of multiple diagnoses.
By linking ICD-O-3 and ICD-10 terminologies to the
NCIt and adding some restrictions based on their own
semantics, ourmethodmay provide a useful auditing solu-
tion. Identifying those codes within the resulting model
may enable discovery within the NCIt of: (i) structural
inconsistencies (e.g., Malignant cecum neoplasm related
to Colon), (ii) missing concepts (e.g., Carcinoma invasive
that can be related to the ICD-O-3 concept Carcinoma,
NOS) and (iii) missing relationships between concepts
(e.g., Cecum which should be defined as a part of Colorec-
tal Region).
In order to build the organ reflexive part lattice, parts
of anatomical concepts were identified using transitive
part-whole properties available within the NCIt (namely
anatomic_structure_is_physical_part_of and anatomic_-
structure_has_location). This results in including cell
parts as (indirect) subclasses of topographies (e.g. Birbeck
Granule part_of Langerhans Cell part_of Epidermis part_-
of Skin). This would suggest that we allow a neoplasm
to have Birkbeck Granule as primary site. Since the
range of the disease_has_primary_anatomic_site property
includes cells parts, such an assertion is allowed in the
NCIt. Thereby, the built hierarchy is in accordance with
the NICt representation of primary sites. As discussed in
[28], the transitivity of the part_of property remains con-
troversial. For instance, in [32], Rescher stated that A
part (i.e., a biological sub-unit) of a cell is not said to be
a part of the organ of which that cell is a part, which is
in contradiction with that stated within the NCIt. How-
ever, diagnoses retained in the derivative model where
those subsuming at least an NCIt concept so that diag-
nosis definitions remain realistic (because NCIt describes
only existing, even if sometimes rare, tumors). Neverthe-
less, further work should be done in order to address this
issue. In this context, patterns proposed by Schulz and
Hahn in [28] are to be investigated.
Perspectives
The NCIt is known to contain some inconsistencies.
Thus, the OWL-DL version of the NCIt should be used
cautiously. However, this resource is helpful in order to
build a formal model for integrating heterogeneous can-
cer disease classifications. Indeed, the NCIt remains a rich
knowledge resource and, as shown in this work, it is pos-
sible to extract parts of this resource and reorganize them
so as to correct some of these inconsistencies (such as is_a
overloading). Even if classes introduced in the derivative
model are consistent (they have been classified based on
their formal definition), instantiating them with terminol-
ogy codes can lead to misclassifications. Our approach
for classes instantiation is based on the classification of
NCIt concepts within the derivative model, which in
turn depends on the formal definition of NCIt concepts.
We have proposed an approach, which identified possible
misclassifications thanks to multiple classes instantiation
by a single code. While this approach enables to find
inconsistencies, there is a need for methods capable of
selecting the class that should be instantiated ultimately in
this situation.
Using the NCIm CUI to map the NCIt to ICD-O-3 and
ICD-10 can be useful but is not enough because mappings
are missing and some are inconsistent. We are currently
working on a method based on the NCIm to identify
additional mappings.
SNOMED-CT® exposes comparable structural charac-
teristics with diagnosis, anatomic and even morpho-
logical concepts as well as relationships between them.
Future work will explore SNOMED-CT as a resource
for integration purpose. As SNOMED-CT is known to
have the same inconsistencies as NCIt, we will study
the feasibility of using both SNOMED-CT and the NCIt
to build a consistent model addressing semantic and
Jouhet et al. Journal of Biomedical Semantics  (2017) 8:6 Page 11 of 12
structural heterogeneities between disease classifications
in oncology.
Topographies representation needs to be refined in
order to avoid inconsistencies and define consistent lev-
els of granularity for the propagation of the disease_-
has_primary_anatomic_site property. The Foundational
Model of Anatomy (FMA) Ontology [33] is a domain
ontology that represents a coherent body of explicit declar-
ative knowledge about human anatomy [34]. Further
work will explore the ability to define these topographies
based on the FMA.
The main goal of this work is to provide a consistent
resource for the integration of heterogeneous disease clas-
sifications in oncology. While our approach based on the
NCIt seems promising, two main limitations have been
discussed (misclassification of codes within the deriva-
tive model and incomplete coverage due to NCIm map-
ping methods). The proposed pattern for the integration
of disease classifications in oncology enables to extract
knowledge from available resources. We will apply the
same approach to other resources (e.g., SNOMED-CT
and FMA) so as to enrich the derivative model. This
future work will allow a better coverage and we will take
advantage of existing links between concepts within these
resources (i.e., anatomical descriptions from the FMA)
and between resources (i.e., mappings available within the
NCIm).
In addition, based on this derivative model, algorithms
for disease identification can be built. This resource
can manage heterogeneity by providing an integrated
view of diagnoses recorded in EHRs. As a result, algo-
rithms based on classes of the derivative model can use
transparently available data coded with disease classifica-
tions and focus on building consistent rules for disease
identification.
Conclusion
We have proposed a method to automatically build a
model for integrating ICD-10 and ICD-O-3 based on
the NCIt. The resulting derivative model is a con-
sistent machine understandable resource that enables
an integrated view of these heterogeneous terminolo-
gies. The NCIt structure and the available relationships
can help to bridge disease classifications taking into
account their structural and granular heterogeneity. How-
ever, (i) inconsistencies exist within the NCIt leading
to misclassifications when instantiating the derivative
model with terminologies, (ii) the derivative model
only integrates a part of ICD-10 and ICD-O-3. The
NCit is not sufficient for integration purpose and fur-
ther work based on other termino-ontological resources is
needed in order to enrich the model and avoid identified
inconsistencies.
Additional file
Additional file 1: The derivative model is available at: https://github.
com/vianneyJouhet/ModelTerminologyIntegrationOncology/tree/master/
data. (OWL 31932 kb)
Funding
Not applicable.
Availability of data andmaterials
Data sharing not applicable to this article as no datasets were generated or
analysed during the current study.
Authors contributions
VJ developed and implemented methods, conducted the analysis, interpreted
results and wrote the manuscript. FM developed methods, interpreted results
and contributed to the manuscript. BB contributed to methods, interpreted
results and contributed to the manuscript. FT contributed to methods,
interpreted results and contributed to the manuscript. All authors read and
approved the final manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Received: 12 August 2016 Accepted: 11 January 2017
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38
DOI 10.1186/s13326-017-0138-9
RESEARCH Open Access
Multiple kernels learning-based biological
entity relationship extraction method
Xu Dongliang1, Pan Jingchang1* and Wang Bailing2
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: Automatic extracting protein entity interaction information from biomedical literature can help to
build protein relation network and design new drugs. There are more than 20 million literature abstracts included in
MEDLINE, which is the most authoritative textual database in the field of biomedicine, and follow an exponential
growth over time. This frantic expansion of the biomedical literature can often be difficult to absorb or manually
analyze. Thus efficient and automated search engines are necessary to efficiently explore the biomedical literature
using text mining techniques.
Results: The P, R, and F value of tag graph method in Aimed corpus are 50.82, 69.76, and 58.61%, respectively. The P,
R, and F value of tag graph kernel method in other four evaluation corpuses are 25% higher than that of all-paths
graph kernel. And The P, R and F value of feature kernel and tag graph kernel fuse methods is 53.43, 71.62 and 61.30%,
respectively. The P, R and F value of feature kernel and tag graph kernel fuse methods is 55.47, 70.29 and 60.37%,
respectively. It indicated that the performance of the two kinds of kernel fusion methods is better than that of simple
kernel.
Conclusion: In comparison with the all-paths graph kernel method, the tag graph kernel method is superior in terms
of overall performance. Experiments show that the performance of the multi-kernels method is better than that of the
three separate single-kernel method and the dual-mutually fused kernel method used hereof in five corpus sets.
Keywords: Tag-graph kernel, Entity relationship extraction, Multi-kernels learing
Background
There are more than 20 million literature abstracts
included in MEDLINE, which is the most authoritative
textual database in the field of biomedicine.The biomed-
ical literature is difficult to detect manually because of
growing number of papers. Thus biomedical entity rela-
tionship extraction is necessary to analysis biomedical
literature.Biomedical entity relationship extraction is the
extraction of inter-entity specific semantic relationships in
text [1, 2]. Besides, it is benefit for semantic similarity [3],
biological network construction [4, 5] and ontology term
prediction [6, 7].
*Correspondence: pjc@sdu.edu.cn
1School of Mechanical, Electrical and Information Engineering, ShanDong
University, WenHua West Road, 264209 WeiHai, China
Full list of author information is available at the end of the article
In the biomedical texts, the entity relationships contain
gene-disease association [810], drug-drug interaction
[1113], protein-protein interaction. Biomedical relation
extraction aiming to automatically discover relations from
these biomedical articles with high efficiency and accu-
racy, is becoming an increasingly well understood alterna-
tive to manual knowledge discovery. In this article, entity
relationship extraction refers to the extraction of entity
relationship that appears in the same sentence. Consider-
ing the extraction of protein interaction relationships as
an example, as shown in Fig. 1. Sentence is a sentence
comprising a natural language in the biological literature,
i.e., an object to be extracted; Protein means a biologi-
cal entity named protein, which is present in the sentence
to be extracted, and three proteins coexist in the sen-
tence in the figure, namely,IL-8,CXCR1 and CXCR2,
respectively. Candidate Named Entity Pair refers to the
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 2 of 79
Fig. 1 Sampling example of protein interaction (The PMID of the literature where the sentence is found is 23041326, and PMID refers to the retrieval
number biological literature coded by PubMed)
candidate relationship pairs comprising two proteins and
three candidate entity relationship pairs contained in the
sentence, as shown in the figure, two of which are cor-
rect protein relationship pairs. These relationship pairs
are marked by two actual performance arrows in the
figures. The entity relationship extraction is the accurate
extraction of the two correct entity relationship pairs.
A knowledge network of biological entity can be pre-
dicted and established by extracting biological entity rela-
tionship [14]. A heavily studied area in biological text
mining concerns the relationships known as protein-
protein interactions (PPI). Massive PPI have accumulated
continuously with the exponential growth of biomedical
literature.
The remainder of the paper is organized as fol-
lows: Section II reviews the related work. Section III is
overview of our approach, which contains introduction
of our approach (A type of tag graph kernel method),
Characteristics-based kernels, extension dependency path
tree kernel and fused kernel method. In section IV, we
construct an experiment to evaluate our approach and
fused kernel method. Section V is our conclusion.
Biological entity relationship extraction methods can
be categorized into three categories statistical machine
learning method [15, 16], co-occurrence-based [17, 18]
and pattern-based method [19, 20].
The co-occurrence-based method is a graphical repre-
sentation of relationships between terms [21, 22]. Antono
et al. [23] proposed new method known as WeMine-
P2P based on WeMine Aligned Pattern Clustering algo-
rithm which discovers and identifies the localized and
co-occurring conserved patterns and regions allowing
variable length and pattern variations.
Although the co-occurrence-based method is simple
and easy to use, the hypothesis depended on by this
method fails to completely reflect the actual situation
of massive and complicated biological texts, therefore
leading to a relatively poor accuracy. Therefore, the co-
occurrence-based method is usually applied to the crude
extraction stage, indicating that all candidate relationship
pairs are extracted. The more accurate extraction of entity
relationships requires fusing other information to filter
the extracted candidate relationship pairs.
The patterns defined are used to match the labeled
sequence in the pattern-based methods.The pattern-
based method contains two methods: the method based
on extraction-pattern [24] and the method based on tem-
plate [25]. The extraction-pattern-based method summa-
rizes entity relationship to obtain several extraction rules
in the texts by using the natural language processing tool.
The template-base method explores the entity relation-
ships from the aspect of syntax or part of speech to
summarize a series of templates by utilizing the natural
language processing. Peng et al. [26] proposed a pattern-
based biomedical relation extraction system with a new
framework. There are three characteristics: 1) generating
patterns by adjusting syntactic variations, 2) improving
the coverage of patterns by using sentence simplification,
3) the referential relations can be identified. Some sys-
tems which are implemented by the pattern-base method
depend on pre-defined patterns at the surface textual
level [2729].Other parsers are used with hand-crafted
patterns [3032].
Compared with the above two methods, machine
learning-based approaches which are driven by data and
set of annotated corpora are effective [3336]. But the
quality and the number of annotated corpora are signifi-
cant effort to the performance of systems.
Machine learning-based approaches include the fol-
lowing two ways: supervised-machine-learning-based
method [37] and semi-supervised-machine-learning-
based method [38, 39]. Supervised machine learning
methods have been employed with great success in PPI
extraction. However, they usually require a large amount
of annotated data for training which are expensive
to obtain in practical applications. Kamada et al. [37]
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 3 of 79
proposed a method to predict strengths of PPIs by
employing protein domain information. Jiang et al. [38]
proposed a multi-label correlated semi-supervised
machine learning method. It can effectively solve the
problem of labeled data by exploring the intrinsic
relationship between related classes.
The semi-supervised-machine-learning-based method
includes the method based on characteristic [40, 41] and
the method based on kernel [42, 43].
In this paper, a type of tag graph kernel method for
extracting protein relationship was proposed and com-
bined with feature-based kernel and extension path graph
kernel into a fused kernel learning method.
Methods
In this article, the kernel method is used as a function
to calculate the similarity between two objects. We used
three kernels to calculate the inter-entity relationships
from three aspects, which can avoid losing important
features and strengthen similarity measurement.
Characteristics-based kernels
Characteristic selection is the main work of using
characteristic-based kernel function for extracting the
protein interaction relationships, where lexical item fea-
ture, entity distance and keyword are regarded to features.
1) Item feature
In this work, we used the following three types of key-
word item features: the keyword items included in the two
protein entity names, the keyword items between the two
protein entity names, and the keyword items around the
two protein entity names.
One protein name may contain multiple words, such as
the sentence in Fig. 1, where the bold part indicates a pro-
tein entity name, and its characteristic value in the char-
acteristic vector can be denoted as a1_(IL)-8, a2_CXCR1,
and a3_CXCR2.
In case that lexical item between two protein entity
names is absent, then the characteristics are considered
dull. Such as, in the sentence in Fig. 1, the word and"
between protein CXCR1 and protein CXCR2 is expressed
as b1_and in the characteristic value in the characteristic
vector.
Given the two proteins, CXCR1 and CXCR2, in the sen-
tence in Fig. 1, the three words at the left side of CXCR1
are through, their and receptors and their character-
istic values in the characteristic vector can be expressed as
l1_through, l2_their, l3_receptors. Lexical item is absent at
the right side of CXCR2, and this feature item is set to dull.
2) Keyword feture
Many words (keywords) around or between two protein
entities can designate the protein relationship, includ-
ing has and receptors. In this paper, when a keyword
emerges around or between two proteins, the keyword
is inserted to the keyword form (there are about 600
keywords in the keyword form). As for the sentence in
Fig. 1, the corresponding key word, receptors are found
in the key word form, and its characteristic value in the
characteristic vector is expressed as k_receptors.
3) Entity distance entity
The number of interval words between two proteins is
called distance. The shorter the distance, the closer the
relationship. Therefore, a shorter distance between two
proteins demonstrates a higher possibility of their inter-
action. If the inter-entity distance is equal to or less than
three words, then the corresponding characteristic value
is expressed as d_3; if the inter-entity distance is greater
than three words but equal to or less than eight words,
then the corresponding characteristic value is expressed
as d_8; if the inter-entity distance is greater than eight
words but equal to or less than 15 words, then the corre-
sponding characteristic value is expressed as d_15; if the
inter-entity distance is greater than 15 words, then the
corresponding characteristic value is expressed as d_16.
The characteristics of two protein entities (IL)-8 and
CXCR1 extraction characteristics in the sentence in Fig. 1
are expressed in Table 1.
In this work, we employed the radial-based function
as the kernel function for calculating the feature vector
(Formula (4)), in which s indicates the covariance matrix.
K(x, y) = exp
[
?||x ? y||
2
2s2
]
(1)
Extension dependency path tree kernel
Formula (5) is the definition of extension path dependency
path tree kernel which is one of convolution tree kernel
(c which is in the lower right corner is convolution). For-
mula (5) shows that the tree structure is the representation
of the protein entity. And the similarity of semanteme
between syntax analysis tree T1 and T2 is calculated by the
same number of structural subtree. Calculation process is
as follows: first, the big tree is broken down into many
different sub-trees; second, calculating the similarities of
these sub-trees; third, the similarity of the big tree is got by
Table 1 (IL)-8 and CXCR1 characteristics
Characteristic name Characteristic value
Lexical item in the two a1_(IL)-8, a2_CXCR1
Protein names
Lexical item between the b1_has, b2_an, b3_important,. . .
Two protein names b17_their, b18_receptors
Lexical item around the l1_Interleukin, r1_and
Two protein names
Key word feature k_receptors
Entity distance entity d_16
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 4 of 79
summing the similarity of the sub-trees. The dependence
path tree kernel [44] and the shortest path tree kernel [45]
is two of classical convolution tree.
Kc(T1,T2) =
?
n1?N1
?
n2?N2
(n1, n2) (2)
In this article, original dependency path tree ker-
nels are selected for the extension to form the tension
dependency path tree kernels. A dependence relationship
analysis is conducted (the analysis process is shown in
Fig. 2) using The expression of rsfA is under the con-
trol of both ENTITY1 and ENTITY2. as example. The
path tree between ENTITY1 and ENTITY2 is (DEPEN-
DENCY(CONJ(ENTITY1,ENTITY2))). Apparently, the
information of this tree is insufficient for the judgment of
the inter-entity relationship. The solution provided hereby
is used to extend the length of the dependency path when
the path length is less than three. The path between
ENTITY1 and ENTITY2 in the above example can
be extended into (DEPENDENCY(PREP(control, of ))
POBJ((of, ENTITY1)) (CONJ(ENTITY1, ENTITY2))).
The algorithm is shown in Algorithm 1.
Algorithm 1 Ext_Dep_Path_Sim(n1,n2)
Input: n1,n2
Output: Similarity of T1 and T2
1: if (the generator between n1 and n2 is defferent)
2: (n1, n2) = 0
3: else if(n1 and n2 is marked as pre-terminal)
4: (n1,n2) = 1 × ?
5: else recursively calculate the following formula
6: (n1,n2) = ?
Nl(n1)?
k=1
(1+(cl(n1,k),cl(n2,k)))
7: End if
Where, n1 and n2 is root node of T1 and T2;?(0< ? < 1)
is the attenuation factor;Nl(n1) at line 06 is the number
of child nodes of n1; n1 and n2 have the same genera-
tive, so Nl(n1) = Nl(n2); In which cl(n,k) is the kth child
node of node n; (cl(n1,k),cl(n2,k)) represents calculat-
ing the number of same subtrees between tree T1 and T2
by a recursive algorithm. Hence, the time complexity of
algorithm is O(n1log(min(n1,n2))).
The function value between the same trees is much
larger than that of different trees when the scale of the
tree is very large. We adopted two ways to stop the func-
tion value become too much large: a) The function value
is normalized by formula(6); b) In order to reducing the
impact of subtree scale, we imported the attenuation fac-
tor ? to multiple the similarity contribution of the subtree
on its father node.
K ?(T1,T2) = K(T1,T2)?K(T1,T1)K(T2,T2) (3)
Tag Graph kernel
Definition 1 Graph kernel: set G as a finite or infinite
graph set, and function ? : G×G ?R is called one graph
kernel. In the presence of one Hilbert space (which is prob-
ably infinitely dimensional) F and one mapping ? : G?F
thus, all the points g, g? ?G, ?(g,g?)=< ?(g), ?(g?)> and
< ·, · > represents the dot product of Hilbert space F.
The current graph kernel methods are mainly divided
into three categories: diffuse graph kernel, volume graph
kernel, and path graph kernel. The authors of this article
propose the tag graph kernel method. The core is used to
compare the quantity of public channels of the two graphs
through hashtag to measure their similarity.
Definition 2 Directed tag graph: given v is one node set,
? is one directed edge set and ? ? ?×?, ? is a tag set, and m
? ? × ? is a mapping from ? to ? , then graph G = (?, ?,m)
is a directed tag graph.
Definition 3 Adjacency matrix: given [E]ij = 1 ? (?i, ?j)
? ?, and [E]ij = 1 ? (?i, ?j) /? ?, then matrix E is an
adjacency matrix of directed tag graph G.
Definition 4 Tag matrix: given tag set ? = {?1, ?2, · · ·}, if
[L]ri = 1 ? ?r = label(?i), and [L]ri = 0 ? ?r = label(?i),
then matrix L is the tag matrix of directed tag graph G.
Definition 5 Matrix inner product: matrix A and
matrix B are the matrices of two m×n, and the inner
product of matrix A and matrix B is defined as ?A,B? =
m?
i=0
n?
j=0
AijBij.
Fig. 2 Demonstration of extension dependency path tree kernel
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 5 of 79
Given G and G? as two directed tag graphs, on the basis
of hashtag, the all-paths hashtag graph kernel function is
shown as Formula (7):
K(G,G?)
=
r??
r=0
?r
?
Lr
( ??
i=0
? iEi
)
LTr , L?r
( ??
i=0
? iEi
)
L?Tr
?
=
r??
r=0
|k|?
m=0
|k|?
n=0
?r
[
Lr
( ??
i=0
? iEi
)
LTr
]
mn
[
L?r
( ??
i=0
? iEi
)
L?Tr
]
mn
(4)
where, E and E? are the adjacency matrices of G and G?,
respectively, and L0,L1, · · ·,Lr , and L?0, L?1, · · ·, L?r are the
hashtags of G and G?, respectively. Matrix [ En]i j repre-
sents the number of all paths in directed tag graph G with
a length of n from node ?i to node ?j.
??
i=0
?iEi can fuse all
paths with different lengths between different nodes into
graph G. K is the set consisting of all hashtags, r? is the
upper limit of hashtag top class, and ? (0 < ? < 1) is the
path weight parameter of adjacency matrix. ?r(?r > 0) is
the top class of hashtags, and the setting of ?0,?1, · · ·,?r
can effectively distinguish the effects of the hashtag at
different top classes on the different categories of tasks.
Kernel fusion
The three kernel methods used in this article have their
own advantages and disadvantages. The feature-based
kernel is simple and effective but cannot obtain the sen-
tence structural information. Extension dependency path
can obtain the sentence structural information but ignores
the deep grammar information. Tag graph kernels can
utilize both the results of the grammar analysis and the
characteristics of words but ignores the words with a rel-
atively long distance and the path similarity of over three
words. To sum up, the authors of this article propose a
method based on the multi-kernel fusion to extract bio-
logical entity relationships. For each kernel, the similarity
is measured according to its field, as shown in Formula (8).
K(x, y) =
m?
i=1
Ki(x, y) (5)
where i represents the quantity of kernels, m=3. To
achieve the kernel fusion of different analysis structures,
the feature weight ? is imported, and ?i > 0,
?
i
?i = 1.
However, the kernel weighted sum is used to replace the
simple multi-kernel summing, as shown in Formula (9):
K(x, y) =
m?
i=1
?iKi(x, y) (6)
At this point, the single-kernel target function is turned
into as follows:
Ld =
?
t
?t ? 12
?
t
?
s
?t?srtrs
?
i
?iKi
(
xt , xs
)
(7)
The multi-kernel combination also appears in Discrim-
inant (11):
g(x) =
?
t
?trt
?
i
?iKi
(
xt , xs
)
(8)
The value of ?i is used through training, and the value
determines the role of the corresponding kernels in the
discriminant.
Results and discussion
To evaluate the multiple-kernel-learning-based method
proposed herein, we conducted computational experi-
ments and compared with the existing method.
Experimental evaluation index
In the biomedical entity relationship extraction research,
there are three evaluation indices which are the following:
(Precision, P), (Recall, R) and (F-score, F).
P = TPTP + FP (9)
R = TPTP + EN (10)
F = 2 ? P ? RP + R (11)
Where TP represents the number of correctly catego-
rized positive examples, TN represents the number of
correctly categorized negative examples, FP represents
the number of wrongly categorized positive examples, and
FN represents the number of wrongly categorized nega-
tive examples. P refers to the precision of the algorithm,
and R refers to the integrity of reaction algorithm. F value
Table 2 Statistical form of corpus information
Corpus set Number of texts Number of sentences Number of positive examples Number of negative examples Total number of examples
Aimed 225 1955 1000 4834 5834
IEPA 50 145 335 482 817
BioInfer 863 1100 2534 7132 9666
HPRD50 200 486 163 270 433
LLL 45 77 164 166 330
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 6 of 79
Table 3 Comparison between tag graph kernel and all-paths graph kernel in terms of their performance
Tag graph kernel method All-paths graph kernel
Corpus set P R F P R F
BioInfer 51.64 68.92 59.73 46.89 62.13 57.25
Aimed 50.82 69.76 58.61 44.97 65.82 55.46
HPRD50 55.64 67.81 70.01 49.76 64.38 68.21
IEPA 61.58 76.91 74.23 56.48 72.36 70.65
LLL 71.92 70.84 77.43 67.19 66.95 72.68
is the harmonic mean of the two evaluation indices of P
and R and is currently the main evaluation index for the
current biomedical entity relationship extraction study.
Experimental corpus
In this section, we used five evaluation corpuses [46]
which are authoritative evaluation corpuses in the
biomedical entity relationship extraction research. Sta-
tistical information on the five experimental corpuses,
Aimed, IEPA, BioInfer, HPRD50, and LLL, are shown in
Table 2.
Experimental results
All-paths graph kernel method [43] is one of the most typ-
ical methods in the protein relationship extraction study.
Table 3 shows the comparison of tag graph kernel method
and all-paths graph kernel method in terms of their per-
formance in the five corpus sets. Evidently, the perfor-
mance of the tag graph kernel method in five corpus sets
is superior to that of the all-paths graph kernel method.
The P, R, and F value of tag graph method in Aimed
corpus are 50.82, 69.76, and 58.61%, respectively. The cor-
responding values of all-paths graph kernel method are
44.97, 65.82, and 55.46%, respectively. The P, R, and F
value of tag graph kernel method in other four evaluation
corpuses are 2-5% higher than that of all-paths graph ker-
nel. The results indicate that the overallperformance of tag
graph kernel method is superior to that of all-paths graph
kernel.
In order to compare two kinds of kernel fusion meth-
ods with the three simple kernel methods, we conducted
experiments on the BioInfer corpus which is moderate
scale. The results are shown in Table 4. In the three
separate kernel methods, the tag graph kernel method
proposed herein has the best performance followed by the
extension dependency path tree kernel. The three kernel
methods have a better performance than the single kernel
methods. Furthermore, two kernels fuse methods which
one is tag graph kernel method obtained the better per-
formance. The P, R and F value of feature kernel and tag
graph kernel fuse methods is 53.43, 71.62 and 61.30%,
respectively. The P, R and F value of feature kernel and
tag graph kernel fuse methods is 55.47, 70.29 and 60.37%,
respectively. Experiment results have indicated that the
performance of the two kinds of kernel fusion methods is
better than that of simple kernel. Hence, the fussed ker-
nel methods indeed improve the performance of protein
relationship extraction method.
As shown in Table 5, the three-kernel-fused methods
and fused kernel methods remain relatively stable in the
five kinds of corpus sets. The fused kernel method has
the best performance in all aspects, and the proposed tag
graph kernel method has the second best performance.
The parameters in the tag graph are the parameters with
the best results after r? and Br have gone through a large
amount of training. Compared with P and R, the F value
in the five corpuses sets changes greatly. For example,
the F value of the four methods in the BioInfer corpus
ranges from 52 to 62%, whereas the F-value in the LLL
corpus ranges from 68 to 91%. Such result is mainly due
to the changes in the distribution of positive and nega-
tive changes of corpus, which greatly affect the F value,
whereas other evaluation parameters are insensitive to
the changes in the positive and negative example ratio in
corpus. The negative examples in Aimed and Bioinfer cor-
puses far outnumber the positive examples. Thus, the F
value of the two corpuses is significantly lower than that
of other corpuses, such as LLL.
Conclusion
In this paper, a tag graph kernel method used hashtag
was proposed, which is combined with extension-
path-tree-kernel-basedmethod and characteristic-kernel-
basedmethod, a fused kernel learningmethod was further
Table 4 Performance of different kernel methods in BioInfer
corpus
Method P R F
Characteristics-based kernels 45.61 63.57 56.24
Extension dependency path tree kernel 41.32 69.76 52.58
Tag graph kernel 51.64 68.92 59.73
Feature kernel + path tree kernel 49.86 70.12 60.25
Feature kernel + tag graph kernel 55.43 71.62 61.30
Path tree kernel + tag graph kernel 55.47 70.29 60.37
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):38 Page 7 of 79
Table 5 Performance of different kernel methods in five types of corpuses
Corpus set Evaluation parameters Characteristics- based kernels Extension path dependency kernel Tag graph kernel Kernels from
three-kernel fusion
Aimed P 45.34 42.31 50.82 57.45
R 61.25 68.54 69.76 72.31
F 55.36 52.63 58.61 60.98
IEPA P 56.84 52.48 61.58 73.82
R 72.92 69.35 76.91 81.06
F 87.15 63.79 74.23 79.57
BioInfer P 45.61 41.32 51.64 91.69
R 63.57 69.76 68.92 71.62
F 56.24 52.58 59.73 62.35
HPRD P 50.26 49.96 55.64 61.87
R 67.59 66.31 67.81 72.35
F 75.38 69.78 70.01 85.48
LLL P 53.59 83.34 71.92 75.69
R 70.12 69.78 70.84 78.37
F 68.43 88.03 77.43 90.12
proposed. Experimental results indicate that the P, R and
F value of the tag graph kernel method is higher on five
evaluation corpuses in comparison with the all-paths-
graph kernel method. And the performance of multi-
kernel fusion methods proposed herein is the best of all
of methods used in this article. Obviously, multi-kernel
fusion methods can make up for the defect in simple ker-
nel and improve the performance of protein relationship
extraction method.
Acknowledgements
This research was partially supported by the National Natural Science of China
under grant No. 61371177,No. U1431102, Science and Technology Major
Project in ShanDong under grant 2015ZDXX0201B04, Science and technology
development Program in Shandong Province under grant 2014GGX101053,
Science and technology development Program in Weihai Province under
grant 2014GGX101053.
Funding
The publication costs for this article were funded by Shandong University.
Availability of data andmaterials
If you need data and material in the paper,please contact with xudongliang.
Email: xudongliang@sdu.edu.cn
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Authors contributions
XD and PJ designed and implemented the tag graph kernel method. WB and
XZ combine the tag graph kernel methods with Characteristics-based kernel
and extension path graph kernel into a fused kernel learning method. All
authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1School of Mechanical, Electrical and Information Engineering, ShanDong
University, WenHua West Road, 264209 WeiHai, China. 2School of Computer
Science and Technology, Harbin Institute of Technology, WenHua West Road,
264209 WeiHai, China.
Published: 20 September 2017
Sharp Journal of Biomedical Semantics  (2017) 8:2 
DOI 10.1186/s13326-016-0110-0RESEARCH Open AccessToward a comprehensive drug ontology:
extraction of drug-indication relations from
diverse information sources
Mark E SharpAbstract
Background: Drug ontologies could help pharmaceutical researchers overcome information overload and speed
the pace of drug discovery, thus benefiting the industry and patients alike. Drug-disease relations, specifically
drug-indication relations, are a prime candidate for representation in ontologies. There is a wealth of available
drug-indication information, but structuring and integrating it is challenging.
Results: We created a drug-indication database (DID) of data from 12 openly available, commercially available, and
proprietary information sources, integrated by terminological normalization to UMLS and other authorities. Across
sources, there are 29,964 unique raw drug/chemical names, 10,938 unique raw indication target terms, and
192,008 unique raw drug-indication pairs. Drug/chemical name normalization to CAS numbers or UMLS concepts
reduced the unique name count to 91 or 85% of the raw count, respectively, 84% if combined. Indication target
normalization to UMLS phenotypic-type concepts reduced the unique term count to 57% of the raw count. The 12
sources of raw data varied widely in coverage (numbers of unique drug/chemical and indication concepts and
relations) generally consistent with the idiosyncrasies of each source, but had strikingly little overlap, suggesting that
we successfully achieved source/raw data diversity.
Conclusions: The DID is a database of structured drug-indication relations intended to facilitate building practical,
comprehensive, integrated drug ontologies. The DID itself is not an ontology, but could be converted to one more
easily than the contributing raw data. Our methodology could be adapted to the creation of other structured
drug-disease databases such as for contraindications, precautions, warnings, and side effects.
Keywords: Drug indications, Drug-disease relations, Drug ontologies, Drug information integration, UMLS, WHO-ATCBackground
Biomedical information overload and the potential of
formal ontologies to help overcome it are well recog-
nized [13]. Information overload is but one threat to
the viability of the traditional pharmaceutical industry.
Others include the rising costs of laboratory research,
clinical trials, litigation over anomalous harmful side
effects, and increasing times to market [4]. The success
of the Gene Ontology (GO) as an in silico molecular
biology research tool [5] suggests that drug ontologies
could have a similar impact on drug research. The
advance of practical ontologies into the pharmaceuticalCorrespondence: sharp@merck.com
Scientific Information Management, Merck Research Laboratories, 770
Sumneytown Pike, West Point, Philadelphia, PA 19486, USA
© The Author(s). 2017 Open Access This artic
International License (http://creativecommons
reproduction in any medium, provided you g
the Creative Commons license, and indicate if
(http://creativecommons.org/publicdomain/zedomain has been much anticipated [68], and is becom-
ing evident [9, 10].
Pioneering reports on ontology-based, in silico drug
discovery have emerged [1113]. The basic goal is
ontology-assisted inference of surprising and/or more-
likely-to-succeed new drug candidate compounds for
known uses, thus cutting costs and time to market. Drug
ontology-assisted inference could also be applied to find-
ing new uses for known compounds (drug repurposing)
[14], or personalized genome-dependent safety/efficacy
profiling (pharmacogenomics) [1518]. These ontologies
include drug relations to chemically similar compounds,
diseases (therapeutic classifications, indications, side
effects), and biological pathways (mechanisms of action,
molecular target proteins or their genes, secondary disease-
gene and protein-protein interactions). In principle, suchle is distributed under the terms of the Creative Commons Attribution 4.0
.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
ive appropriate credit to the original author(s) and the source, provide a link to
changes were made. The Creative Commons Public Domain Dedication waiver
ro/1.0/) applies to the data made available in this article, unless otherwise stated.
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 2 of 10ontologies could be expanded to encompass many more di-
mensions of drug information [19, 20]; that is, they can be
made more comprehensive.
For further progress in building comprehensive drug
ontologies, rich and well-structured knowledge (content)
about biological pathways and chemically similar
compounds is readily available from resources such as
GO, GenBank [21], DrugBank [22], PubChem [23], and
ChemIDplus [24]. Rich drug-disease knowledge also is
readily available, but usually as unstructured (free) text;
e.g., DailyMed [25]. Thus the well-structured but
relatively shallow WHO-ATC drug classification [26]
has been utilized as a source for drug-disease
knowledge [12, 13].
It is important to distinguish between diseases, indica-
tions, contraindications, side effects, and other such
dimensions of drug information. A drug indication can
be a disease1 that the drug is used for (i.e., to treat,
prevent, manage, diagnose, etc.). An important subset
are approved indications which have been through a
formal, country-specific regulatory vetting process. But
drugs can also be indicated for medical conditions which
may not be considered diseases, such as pregnancy.
Drugs can also be indicated for procedures, such as
contrast media for radiology. In ontological terms,
medical conditions (of which diseases are a subclass)
and medical procedures constitute the range of drug
indications. They also constitute the range of very differ-
ent, even orthogonal, drug relations such as contraindi-
cations, precautions, and warnings. The range for side
effects, on the other hand, is arguably limited to
diseases. Thus it is important to specify which of
these relations is being addressed. This paper ad-
dresses indications, but much of it is extensible to
other drug-disease relations.Methods
We created a drug-indication database (DID) using con-
tent from openly available, commercially available, and
Merck proprietary information resources. To integrate
the data, we attempted to identify distinct triples of a
drug, indication, and indication subtype (treat, prevent,
manage, diagnose, etc.), and then normalize each
component to a standard terminology or code. The raw
data varied widely in format, from well-structured,
vocabulary-controlled triples to hierarchical classifica-
tions to free text. While the DID itself is not an
ontology, it could be converted to one more easily than
the contributing raw data.Sources
Raw data on drug/chemical-indication relations were
collected from the following resources.DailyMed
DailyMed [25] is a free drug information resource pro-
vided by the U.S. National Library of Medicine (NLM)
that consists of digitized versions of drug labels (also
called package inserts) as submitted to the U.S. Food
and Drug Administration (FDA). The information format
of the labels is mostly free text but with standard section
headings, including Indications & Usage. DailyMed was
of special interest because of its comprehensive coverage,
open availability, and the package inserts combination of
format consistency, rich detail, and provenance (manufac-
turer-written, scientifically vetted, and FDA-approved).
DrugBank
DrugBank [22] is a unique bioinformatics and chemin-
formatics resource that combines detailed drug (i.e.
chemical, pharmacological and pharmaceutical) data
with comprehensive drug target (i.e. sequence, structure,
and pathway) information provided by the University of
Alberta. Many records include an explicit Indication
field populated with free text values, and leveraging
these was of special interest due to DrugBanks rich
coverage of molecular target information.
MeSH PA
MeSH (Medical Subject Headings) [27] is NLMs con-
trolled vocabulary used to index Medline/Pubmed [28]
articles by scientific topics including drugs, chemicals,
diseases, and other biomedical conditions, processes,
and procedures. MeSH has ontology-like hierarchical
and other relationships between concepts, but it does
not consistently link drugs to diseases/conditions/pro-
cesses explicitly (e.g., Aspirin to Fever). It does,
however, have a special Pharmacological Action (PA)
relationship which links drugs and other chemicals to
therapeutic classes (e.g., Aspirin to Antipyretics)
which could be mapped to diseases/conditions/processes
(e.g., Antipyretics to Fever).
NDFRT
NDFRT (National Drug File Reference Terminology) [29]
is produced by the U.S. Veterans Health Administration
and is openly available from several resources including
NLMs UMLS (Unified Medical Language System) [30].
Like MeSH PA, NDFRT consists of controlled vocabulary
terms connected by specific relationship names, five of
which could be considered pointers to indications or PAs:
may_treat, may_prevent, may_diagnose, has_mecha-
nism_of_action, has_physiological_effect.
PDR
PDR (Physicians Desk Reference) is a commercially
published compilation of manufacturers prescribing in-
formation (package insert) on prescription drugs,
Fig. 1 Example USAN raw data
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 3 of 10updated annually. [31] Its long history (65 editions) and
ubiquitous hardcopy availability give PDR a certain
provenance. Section 3 - Product Category Index classi-
fies drugs (trade names) by disease (e.g., ALCOHOL
DEPENDENCE) and/or PA (e.g., ANALGESICS).
ChEBI
ChEBI (Chemical Entities of Biological Interest) [32]
consists of a database and ontology supplied by the
European Bioinformatics Institute. The has_role rela-
tionship of the ontology connects drugs and chemicals
to functions, including PAs (e.g., antibacterial drug;
anti-ulcer drug; proton pump inhibitor).
CTD
CTD (Comparative Toxicogenomics Database) [33, 34]
is supplied by North Carolina State University and
Mount Desert Island Biological Laboratory, Salisbury
Cove, Maine. The Chemical-Disease Associations file
consists of pairs of MeSH terms connected by the rela-
tionships therapeutic and/or marker/mechanism and
annotated by evidence type; we used the direct evi-
dence subset.
USAN TC
USANs (United States Adopted Names) are the official
U.S. generic names chosen for drugs by the USAN
Council in consultation with the drugs sponsoring com-
pany [35]. Each name has a variety of structured (but
not necessarily vocabulary controlled) relations signify-
ing proprietary, chemical, and therapeutic information.
This information is published annually in the USP
Dictionary of United States Adopted Names (USAN) and
International Drug Names [36] and monthly by the
American Medical Association (AMA) [37] (Fig. 1), and
Merck encodes it in our internal vocabulary system
(eVOC). The Therapeutic Claim (TC) values include
disease names, PAs, and indication subtypes such as
treatment of and prevention of.
WHO-ATC resources
WHO-ATC (World Health Organization Anatomic-
Therapeutic-Chemical) is a five-level drug classifica-
tion hierarchy specifying (typically, from top to
bottom) the anatomical system acted upon, thera-
peutic action, and chemical nature of the drug. The
hierarchy can convey multiple indications/PAs for a
given drug. WHO-ATC is widely accepted as a stand-
ard for drug classification, including in the Merck
eVOC system. We obtained WHO-ATC data from
two WHO datasets purchased by Merck and
additional mappings in eVOC; these are referred to as
WHO_ATC [38], WHO_DD [39], evoc_ATC, and evoc_-
eProj in the rest of this document. (All evoc_eProj andsome evoc_ATC data represent Merck proprietary infor-
mation and therefore have been removed from the at-
tached DID subset, Additional file 1.)
Parsing and filtering
These resources and their contributions to our database
are summarized in Table 1. Parsed refers to converting
the raw data to triples of a drug, indication, and indica-
tion subtype. In the process of parsing, some raw data
was found to be irrelevant, redundant, and/or intract-
able, and therefore was removed from further processing
(filtered). Differences in contribution counts from fil-
tered to parsed correlate inversely with how well-
structured and vocabulary-controlled were the raw
source data, from low (ChEBI, CTD, MeSH PA, NDFRT)
to high (DailyMed, DrugBank).
Filtering is not qualitatively different from initial
subsetting (Table 1, column 3). For example, ChEBIs,
CTDs, and MeSH PAs relatively large initial
contributions can be attributed to their higher cover-
age of non-drug chemicals and non-therapeutic quasi-
indications (e.g., Carcinogens; Mutagens). These could
be considered irrelevant to pharmacy/prescription appli-
cations of the DID, but were left in for drug discovery
applications. ChEBIs contribution was reduced 48% by
filtering out irrelevant (non-indication) has_role objects
(e.g., metabolite; prodrug; epitope), but CTDs
marker/mechanism subset (63%) was not removed due to
its potential use in future analysis. DailyMeds filtering re-
duction was even larger but aimed at very different targets:
Table 1 Source contributions of drug-indication data
source abbrev source name or description subset if any version/date number of drug-indication pairs
initial filtered parsed
ChEBI Chemicals of Biological Interest
Ontology
has_role relations 104/June 1, 2013 16,415 8,598 8,598
CTD Comparative Toxicogenomics
Database
Chemicals-Diseases Associations,
direct evidence subset
May 2, 2014 82,000 81,214 81,214
DailyMed NLMs database of FDA package
inserts
single component title (product
name) & Indications sections with
tractable text length (<540)
March 20, 2011 15,834 1,612 3,840
DrugBank U. Alberta open access DB of drug
target and other info
title (drug name) & Indications
sections
3.0/2011 1,599 1,595 6,004
MeSH PA Medical Subject Headings Pharmacologic
Action relations
2013/Dec. 3, 2012 26,293 25,847 25,908
NDFRT National Drug Formulary Reference
Terminology
may_treat & may_prevent relations 2009AA (UMLS) 50,775 5,294 5,294
PDR Physicians Desk Reference Section 3 - Product Category Index 2006 3,150 1,204 2,169
USAN_TC United States Adopted Names
Therapeutic Claims
March 31, 2014
(eVOC)
6,569 5,954 7,234
WHO_ATC World Health Organization Anatomic-
Therapeutic-Chemical, Defined Daily
Dose index
2005 16,276 7,807 9,004
WHO_DD World Health Organization Drug
Dictionary
single generic compounds with ATC
codes (minus 2005 WHO-ATC overlap
and herbals BNA = 9)
Sept. 2013 40,736 21,764 25,674
evoc_ATC WHO-ATC codes in Mercks eVOC
generic names dictionary
single generic compounds with ATC
codes (minus WHO-ATC & WHO-DD
overlap)
May 6, 2014 65,552 16,269 19,093
The numbers refer to candidate drug-indication pairs in the initial raw data extract (initial), after filtering for internal redundancy, relevance, and/or tractability
(filtered), and after parsing of free text into single concepts (parsed) as described in the main text. The filtered count is the number of unique pairs of raw drug
name (DID column D) and indication entire value/string (column AQ), while the parsed count is the number of unique pairs of raw drug name and indication target/
substring (column AR). evoc_eProj data are not shown
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 4 of 10combination products (20%), intractably long (>539 charac-
ters) Indication & Usage texts (37%), and redundant
Indication & Usage values paired with the same drug gen-
eric name differing only by dosage, formulation, trade
name, or supplier (33%). NDFRTs (90%) and PDRs (62%)
filtering reductions were also due primarily to conflating
various forms (trade names, in PDRs case) of the same gen-
eric name.2
Initial counts from the WHO-ATC resources are based
on viewing each level of the WHO-ATC hierarchy as a
separate indication, rather than combining them into a
single raw term. Filtering resulted in reductions of 52%
(WHO_ATC), 47% (WHO_DD), and 75% (evoc_ATC)
reflecting removal of combination and ill-formed drug
names, and non-indication and redundant classification
terms (e.g., Antithrombotic Agents at nested hierarchical
levels [B01 and B01A]).
It must be emphasized that the parsing, filtering,
and normalizing (see below) done in this work
employed a wide variety of ad hoc methods and
manual curation commensurate with the raw data/
source diversity.Normalizing drug names
Various types of drug identifiers are exemplified in Fig. 1,
including a generic name (in this case a USAN, aftobetin
hydrochloride), chemical names, a structural formula,
sponsor code designations, and a CAS (Chemical
Abstracts Service) Registry Number. Other types not
shown in Fig. 1 include trade names (e.g., Tylenol corre-
sponding to the generic name acetaminophen), FDAs
UNII (Unique Ingredient Identifier; e.g., A1FCZ940WA
for aftobetin hydrochloride), and InChI (International
Chemical Identifier) Key (e.g., GMWHTUNMFTUKHH-
NDUABGMUSA-N for aftobetin hydrochloride) [40].
The equivalence of such terms for the exact same chem-
ical entity can sometimes be debated due to details such
as isomerism, salt forms, hydration, formulation, and dos-
age, but they are commonly considered synonyms, with
the generic name as the preferred term (PT).
Thus, to parse out the drug identifier in each raw
drug-indication record, we looked for source database
fields or elements containing these types of terms, and
attempted to normalize them to generic names using the
sources own and/or other synonym dictionaries. These
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 5 of 10dictionaries included those available from ChemIDplus,
ChEBI, CTD, DrugBank, UMLS, and Mercks eVOC. In
addition, to resolve conflicts among these dictionaries,
we attempted to derive a preferred PT via CAS
number mapping and ranking the dictionaries in the
order ChemIDplus > ChEBI > DrugBank > eVOC > CTD.
For example, in ChemIDplus the PT for CAS number
103-90-2 is acetaminophen but in ChEBI it is para-
cetamol. Thus the DID enables ChEBI drug-indication
data for paracetamol to be grouped with other sources
drug-indication data for acetaminophen. UMLS is not
a rich source of CAS numbers, but supplies an equally
language-neutral CUI (Concept Unique Identifier).
Normalizing indications
In the DID and its non-proprietary subset (Additional
file 1), indications associated with each drug name are
encoded at four basic levels of granularity.
 Raw entire value/string (column AQ): the raw
sources term/text, including entire DailyMed
Indications & Usage sections converted to single-
line sentences.
 Raw target/substring (column AR): a term/phrase
within or based on the entire value/string, denoting
a distinct indication concept. If the target/substring
is the same as the entire value/string, it is flagged
with Y in column AS.
 UMLS entry term (column AU) that best matches
the target/substring and conforms to our semantic
type preference for phenotypes (diseases and other
biological conditions, processes, and functions; see
below). UMLS mapping was done using ad hoc perl
scripts designed to work with UMLS flat files
(2013AA version), MetaMap [41], and/or NLMs
online UMLS browser [42]. Each UMLS entry
term is tagged for whether it is preferred (P)
or a non-preferred synonym (S) (column AX).
For readability in the DID, all P terms were
converted to proper case and all S terms were
converted to lower case using Excel string
functions.
 UMLS preferred term (column AV) and
corresponding CUI (column AW) were computed
from UMLS 2013AA flat files to unify all encoding
at this level, even if raw values consisted of UMLS
terms or CUIs (MeSH PA and NDFRT), except for
mappings only available in more recent UMLS
versions via NLMs online browser.
Indication semantic types
For UMLS encoding of indication concepts, we had a
preference for UMLS concept terms classified under
UMLS semantic types signifying phenotypes (diseasesand other biological conditions, processes, and func-
tions). The goal of this was to reduce encoding scatter.
For example, the raw term antibacterial agent exactly
matches a UMLS synonym under Anti-Bacterial
Agents (CUI C0279516) classified under semantic type
Antibiotic (A1.4.1.1.1.1). But calling a drug an anti-
bacterial agent is equivalent to saying that its indication
is Bacterial Infections (C0004623, classified under
Disease or Syndrome B2.2.1.2.1). By mapping Anti-
Bacterial Agents/C0279516 to Bacterial Infections/
C0004623, raw data that encode to either are unified.
This is tantamount to trading lexical match precision for
increased terminological reduction (explained below).
In the DID and Additional file 1, initial indication
mappings to non-phenotypic semantic type UMLS terms
are encoded in columns BD-BL with their remapping to
phenotypic type CUIs in AT-BC. If the initial non-
phenotypic type mapping could not be mapped to a pheno-
typic type CUI, it is encoded in AT-BC. For example,
Cephalosporins (a WHO-ATC category, among other in-
stances) maps to C2266959/Antibiotic/A1.4.1.1.1.1, but is
stuck there because UMLS had no phenotypic type term
such as cephalosporin activity; cephalosporin effect; or
cephalosporin-sensitive infection.
Indication subtypes
In prior work [19, 20] we observed that drug indications
are often classified or annotated by subtypes such as
approved vs. non-approved, or treatment vs. prevention.
The current works expanded raw data scope brought to
light additional types with lexical cues such as thera-
peutic/pharmacologic class prefixes (Antidiabetic),
suffixes (Anxiolytic), and head nouns (beta-adrenergic
agonist; Lipoprotein Lipase Activators; smoking ces-
sation adjunct). Some of these distinctions are likely to
be even more substantial than treatment vs. prevention;
e.g., Antineoplastics and Carcinogens both map to
cancer but in opposite ways, one inhibitory or negative,
the other causative or positive. This suggests an indication
subtype hierarchy representing a gradient of granularity
with raw terms like treatment and prevention at the
bottom/leaf level and negative and positive at the top.
In between would be lexical root forms such as treat
representing treats; treating; treatment; etc. If so
encoded in the DID, users could select the most
appropriate indication subtypes and level of granularity
for their use case. We identified indication subtypes based
on Excel string searches (treat; anti; inhibit; etc.) in
the raw entire value/string (column AQ).
Terminological reduction
The inherent value of terminological normalization is
the core principle of controlled vocabularies that have
been used to organize, search, and represent information
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 6 of 10for over a century [43]. To measure the success of our
terminological normalization efforts, we defined
terminological reduction (TR) as TR = (N + X)/U, where
N = number of unique normalized names, X = number of
unique raw names which remain unnormalized, and
U = number of unique original raw names.
Results
Database overview
The Merck in-house version of the DID (January 2015
release) contains 198,415 rows of data representing
unique quadruplets of source, raw drug/chemical name,
raw indication target term, and indication UMLS CUI.
Across sources, there are 29,964 unique raw drug/chem-
ical names, 10,938 raw indication target terms, and
192,008 unique raw drug/indication pairs. Additional file
1 is a copy of this spreadsheet minus 5,557 rows (3%)
containing Merck proprietary information. Therefore re-
producing these counts and the following analyses on
Additional file 1 would yield slightly different quantita-
tive results, but not substantially alter our qualitative
conclusions. Additional file 1s schema worksheet
shows the DID schema and two example records.
Drug name normalization
Drug name mapping to CAS numbers is encoded in
DID columns E-H. CAS numbers were assigned to 87%
of the DID rows and 71% of the unique raw drug names,
providing TR of the unique names to 91%. The preferred
authority ChemIDplus alone covered 84% of the rows
and 68% of the unique raw drug names. Almost all
(98%) of these CAS number mappings are based on
exact (case-insensitive) matches to the ChemIDplus or
other standards PT for that CAS number, or to a
source-specified synonym (<syn per source>). The
synonym matches were manually curated and obvious
broader term (BT) and narrower term (NT) matches
were reclassified as such. For BT and NT matches the
directionality is raw-to-standard; e.g., raw arformoterol
fumarate is a NT (a salt, derivative, analog, or formula-
tion of) the closest ChemIDplus term which has a CAS
number, Arformoterol. Also distinguished are quasi-
synonym matches such as cidofovir anhydrous: Cido-
fovir. The intent is to offer users multiple match quality
levels as options for filtering. The individual drug name
mappings to ChEBI, ChemIDplus, and CTD are encoded
in DID columns I-AC.
Drug name mapping to UMLS is encoded in DID
columns AD-AM. UMLS CUI mapping, compared to
CAS number mapping, produced superior coverage of
DID rows (96% vs. 87%) and unique raw DB drug names
(89% vs. 71%), and superior TR (85% vs 91%). The differ-
ence is at least partly due to the higher numbers of
synonym and narrower UMLS matches, which may bean artefact of unequal curation effort or UMLS coverage
of broad classes (e.g.,antiseptics) which by nature do
not have CAS numbers.
Indication normalization
Ninety-nine percent of DID rows represent unique
triplets of raw data source (column B), drug name
(column D), and indication target/substring (column
AR), the other 1% representing compound matches
where more than one UMLS term was needed to cover
the indication concept completely. There are 10,938
unique values of the target/substring, of which 28 (0.3%)
could not be mapped to UMLS. The rest mapped to
7,522 UMLS entry terms and thence to 6,227 UMLS
PT/CUIs of the preferred semantic type (columns
AT-BC), yielding a TR of 57%.
Indication semantic type normalization
Unlike the drug name normalization mappings, the
indication UMLS mappings have a sizable prevalence
of quasi-synonym match types (column AT; 46% of
rows, 30% of unique target/substrings). This is attrib-
utable to our preference for indication normalization
to phenotypic-type UMLS terms, operationalized in the
semantic type normalization step. Non-phenotypic-type
terms were thus reduced from 29% of DID rows among
initial UMLS mappings (columns BD-BL) to 3% among
final (AT-BC), primarily terms of type Pharmacologic
Substance/A1.4.1.1.1 (25% initial, 1% final). The preva-
lence rank of Pharmacologic Substance/A1.4.1.1.1 chan-
ged from first to 13th, reflecting the large contributions
from ChEBI, CTD, MeSH, PDR, USAN, and WHO-ATC
consisting or raw therapeutic/pharmacologic class terms
(e.g., Analgesics; Antineoplastics; Carcinogens).
Indication subtypes
Indication subtype data are contained in DID columns
AN-AP. These data are very preliminary and incomplete.
Supplementing and refining it is one of our ongoing
extensions of this work.
Comparison of sources
Coverage
Table 2 summarizes how much of the data was covered
by each of the 12 sources after normalization. CTD cov-
ered by far the largest number of unique drug-indication
relations (49%), followed by MeSH_PA, WHO_DD, and
eVOC_ATC (1014%), followed by the others (15%).
With the exception of USAN_TC, this rank-order
pattern also held for drug/chemical names alone. For
indications alone, CTD also covered 49%, followed by
DrugBank (34%), DailyMed (23%), USAN_TC (18%),
NDFRT (16%), and the others (58%).
Table 2 Comparison of sources coverage of unique drug
names, indication terms, and drug-indication relations after
normalization
Source %normalized
drug indication drug-indication pairs
CTD 33 49 49
MeSH_PA 27 6 14
WHO_DD 28 5 14
evoc_ATC 26 5 11
ChEBI 17 8 5
WHO_ATC 11 5 5
DrugBank 6 34 4
USAN_TC 23 18 4
NDFRT 6 16 3
DailyMed 4 23 2
evoc_eProj 3 6 1
PDR 3 5 1
Percentages are relative to total counts of 25,278 unique normalized drug names,
6,228 unique normalized indication terms, and 167,087 unique normalized
drug-indication relations
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 7 of 10Overlap
Table 3 summarizes overlap, a measure of the unique-
ness of each sources contribution to the DID, defined
as the number of sources that contributed each
unique drug and indication (target) term and drug-
indication pair, before (raw) and after normalization,Table 3 Comparison of sources overlapping coverage of unique dr
and after normalization
source raw normaliz
drug indic (target) drug-indic pairs drug
All 1.64 1.30 1.02 1.87
evoc_ATC 1.62 3.40 1.03 2.09
WHO_ATC 3.81 3.37 1.07 4.66
WHO_DD 1.91 3.29 1.03 2.59
MeSH_PA 2.81 1.33 1.03 3.08
PDR 5.56 1.60 1.17 6.14
evoc_eProj 1.00 1.80 1.00 2.21
ChEBI 2.51 1.14 1.01 3.11
USAN_TC 3.03 1.47 1.02 3.34
DailyMed 4.68 1.60 1.15 5.18
NDFRT 5.46 2.45 1.41 6.24
DrugBank 5.63 1.49 1.20 6.24
CTD 2.41 1.53 1.03 2.62
Numbers represent the average number of sources sharing each term or term pair,
drug name score of 1.00 for evoc_eProj means that system only shares its raw drug
terms in its internal data systems. When these are normalized, as much as possible,
names representing evoc_eProj content are shared with enough other DID normali
even though some company codes do not yet have public domain generic names. Data
indic) for the individual sourcesand the difference. Consistent with overall TR, the
biggest effect of normalization was seen in the
increase in shared indication terms with the descend-
ing rank-order following the tendency of each source
to express indications in other-than-phenotypic-type
terms (Table 3, column 9).
The pooled (all sources) shared term data can also be
viewed as a Zipf distribution [44] (Fig. 2) showing, again,
the larger effect of normalization on indication than drug
terms or drug-indication pairs. Strikingly, no raw drug
names were shared by more than 10 of our 12 resources,
and only four normalized drug names were shared by all
12 (Dexamethasone; Hydrocortisone; Methyldopa;
Nitroglycerin). The most-shared (by 11 sources) norma-
lized drug-indication pairs were Aspirin:Pain and
Methyldopa:Hypertensive Disease (the UMLS PT for
hypertension).
Richness
Each sources average numbers of indications per drug
name and drug names per indication, before and after
normalization, measure what might be called the rich-
ness of their drug-indication information. CTD had by
far the highest (10) average raw indication targets per
drug/chemical name, consistent with its low overlap and
high coverage. Following CTD was a cluster in the range
of 3.54 indication targets/drug that included DailyMed,
MeSH_PA, DrugBank, and NDFRT, then a cluster in the
2.73.3 range that included WHO_DD, WHO_ATC,ug names, indication terms, and drug-indication relations before
ed change
indic drug-indic pairs drug indic drug-indic pairs
1.80 1.14 0.23 0.50 0.12
6.77 1.38 0.47 3.37 0.35
6.67 1.96 0.85 3.30 0.89
6.54 1.40 0.68 3.25 0.37
4.54 1.43 0.27 3.21 0.40
4.68 2.32 0.58 3.08 1.15
4.17 1.44 1.21 2.37 0.44
3.40 1.72 0.60 2.26 0.71
3.30 1.81 0.31 1.83 0.79
2.79 1.48 0.50 1.19 0.33
3.61 1.76 0.78 1.16 0.35
2.62 1.70 0.61 1.13 0.50
2.06 1.08 0.21 0.53 0.05
computed within each sources coverage. For example, the low outlier raw
names with itself, reflecting the use of Merck company codes as preferred
to public domain generic names, the score rises to 2.21; that is, these generic
zed content to push the non-self average from zero up to 1.21 (=2.211.00)
are sorted in descending order of the change indication scores (column 9; change/
Fig. 2 Zipf distributions of sources overlapping coverage of unique
drug names, indication terms, and drug-indication relations before
and after normalization
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 8 of 10evoc_eProj, PDR, and evoc_ATC, and finally ChEBI (1.8)
and USAN_TC (1.2). These numbers were little changed
by normalization. The biggest changes were actually
negative (0.4 more raw than normalized indications/drug
for MeSH_PA and evoc_eProj).
The highest average numbers of drug names per
raw indication target were provided by WHO_DD (69),
evoc_ATC (56), and MeSH_PA (55). This same cluster
also showed the biggest effect of normalization. At the
low end, DailyMed and DrugBank data showed the most
dramatic effect of processing, their average indications/
drug increasing from approximately 1 (raw entire values)
to 2 (raw targets) to 3 (normalized indications).
Discussion
Our DID is intended to facilitate building practical,
comprehensive, integrated drug ontologies. As for com-
prehensiveness, we achieved high source/data diversity
as evidenced by a low overall degree of coverage overlap
consistent with the idiosyncrasies of each source (non-
drug chemicals, free text, hierarchical terms, etc.).
Diversity is not equivalent to comprehensiveness, but is
indicative of it. As for integration, indication normalization
to phenotypic-type UMLS concepts provided substantial
TR (57%). However, drug/chemical name normalization
(TR 84%) was poor by comparison; therefore there was
almost no effect of overall normalization on the average
number of indications per drug.
WHO_DDs, WHO_ATCs, evoc_eProjs, PDRs, and
evoc_ATCs richness may be somewhat artificial in that
it may be mainly due to WHO-ATCs and PDRs very
general higher hierarchical categories. However, this fea-
ture may facilitate clustering of drug-indication relationsand so explain WHO-ATCs wide acceptance as a stand-
ard for drug classification and discovery research.
Because its true richness was not captured, DailyMed
raises major issues for further development of the DID.
These include the cost of dealing with the current (differ-
ent) downloading, subsetting, and sectional parsing
options, and developing better, less manual, free text-to-
UMLS mapping methods. On the benefit side, methods
applicable to DailyMeds Indications & Usage sections
are expected to be adaptable/re-usable for contraindica-
tions, side effects, and other dimensions of drug informa-
tion. Relevance to clinical use cases is recognized [45] but
DailyMeds fit to early-stage drug discovery has been ques-
tioned [46]. NDFRT presents the opposite conundrum. In
a spot check of two drugs, we [20] found major discrepan-
cies between NDFRTs may_prevent and may_treat rela-
tions and the approved clinical indications. Therefore
these relations may be a poor fit to clinical drug ontology
use cases. However, as a representation of possible drug
indications conveyed by co-occurrence of MeSH terms in
Medline, they may be ideal for early-stage drug discovery.
Also, NDFRTs may_diagnose, has_mechanism_of_action,
and has_physiological_effect relations will be examined for
future inclusion in the DID.
Finally, CTDs high-coverage, low-overlap outlier sta-
tus raises suspicion that its marker/mechanism subset
(63%) may not be relevant to drug indications and there-
fore should be examined and possibly excluded from
future DID releases.
Conclusions
The DID is a database of structured drug-indication re-
lations created using openly available, commercially
available, and Merck proprietary information resources
and terminological normalization tools. It is intended to
facilitate building practical, comprehensive, integrated
drug ontologies. The DID has good source/raw data
diversity as measured by low coverage overlap, and
significant integration/normalization as measured by
terminological reduction. Numerous opportunities exist
for data cleaning, addition, and other improvements.
Our methodology could be adapted to the creation of
other structured drug-disease databases such as for
contraindications, precautions, warnings, and side effects.
Endnotes
1Following UMLS, we take diseases to be synonym-
ous with disorders. We also mean diseases to convey
the larger sense of pathological or aversive states that
might otherwise be distinguished as signs, symptoms,
abnormalities, deficiencies, injuries, etc.
2Although different forms of the same generic name can
in principle be specific to different indications, our confla-
tion of NDFRT is not lossy because NDFRT appears to
Sharp Journal of Biomedical Semantics  (2017) 8:2 Page 9 of 10cross-generalize them regardless. For example, finasteride
is marketed as a 1 mg tablet indicated to treat male-
pattern baldness and a 5 mg tablet indicated to treat
benign prostatic hyperplasia. But NDFRT has may_treat
relations to both Alopecia and Prostatic Hyperplasia
(the corresponding MeSH PTs) for all three: Finasteride
1 mg Tab; Finasteride 5 mg Tab; and Finasteride. In
another example, Bismuth and all of its salt variants have
relations to Escherichia Coli Infections; Virus Diseases;
Helicobacter Infections; and Dysentery, Bacillary pre-
sumably related to bismuth subsalicylates gastrointestinal
effects but definitely inappropriate for Bismuth Hydro-
xide which is a hazardous industrial chemical. In another
example, radioactive and hazardous Iodine, I-125
inappropriately shares the Iodine relations to Burns;
Leg Ulcer; Radiation Injuries; Staphylococcal Infec-
tions; and Surgical Wound Infection.
Additional file
Additional file 1: Drug-Indication Database non-proprietary subset.
(XLSX 61806 kb)
Abbreviations
AMA: American Medical Association; BAN: British Approved Name;
BT: broader term; CAS number or CAS#: Chemical Abstracts Service Registry
Number; ChEBI: Chemical Entities of Biological Interest; CTD: Comparative
Toxicogenomics Database; CUI: Concept Unique Identifier [UMLS];
DB: database; DID: Drug-Indication Database; eVOC: electronic VOCabularies
[Merck internal system]; FDA: U.S. Food and Drug Administration; GN: generic
[drug] name; GO: Gene Ontology; InChI: International Chemical Identifier;
MedDRA: Medical Dictionary for Reporting Activities; MeSH PA: Medical
Subject Headings Pharmacological Action [relations]; MeSH: Medical Subject
Headings; NDFRT: U.S. National Drug Formulary Reference Terminology;
NLM: U.S. National Library of Medicine; NLP: natural language processing;
NT: narrower term; OBO: Open Biological & Biomedical Ontologies;
OTC: over-the-counter [drugs]; PDR: Physicians Desk Reference; PT: preferred
term; SNOMEDCT: Systematized NOmenclature of MEDicine Clinical
Terminology; TR: terminological reduction; UMLS: Unified Medical Language
System; UNII: UNique Ingredient Identifier; USAN TC: United States Adopted
Names Therapeutic Claim; USAN: United States Adopted Names; USP: United
States Pharmacopeia; UTS: UMLS Terminology Services; WHO-ATC: World
Health Organization Anatomic-Therapeutic-Chemical [classification];
WHO-DD: World Health Organization Drug Dictionary
Acknowledgements
I would like to thank my Merck colleagues, especially Jyoti Shah, Karen
Marakoff, and Carol Rohl, for their contributions to this work. Also I would
like to thank Olivier Bodenreider at NLM, Nicholas Belkin at Rutgers
University, and the Merck Educational Assistance Program for their
contributions to my Ph.D. thesis [20], of which this work is an extension.
Availability of data and materials
The non-proprietary subset of the DID is included with this paper
(Additional file 1).
Authors information
The author holds a M.A. in Biochemistry and a Ph.D. in Information Science.
He has been involved in biomedical vocabularies, ontologies, and
information systems at NIH, NLM, and Merck since 1988.
Competing interests
The author has been employed by Merck & Co., Inc., since 1994.Received: 14 July 2015 Accepted: 16 December 2016
RESEARCH Open Access
Therapeutic indications and other use-
case-driven updates in the drug ontology:
anti-malarials, anti-hypertensives, opioid
analgesics, and a large term request
William R. Hogan*, Josh Hanna, Amanda Hicks, Samira Amirova, Baxter Bramblett, Matthew Diller, Rodel Enderez,
Timothy Modzelewski, Mirela Vasconcelos and Chris Delcher
Abstract
Background: The Drug Ontology (DrOn) is an OWL2-based representation of drug products and their ingredients,
mechanisms of action, strengths, and dose forms. We originally created DrOn for use cases in comparative effectiveness
research, primarily to identify historically complete sets of United States National Drug Codes (NDCs) that represent
packaged drug products, by the ingredient(s), mechanism(s) of action, and so on contained in those products. Although
we had designed DrOn from the outset to carefully distinguish those entities that have a therapeutic indication from
those entities that have a molecular mechanism of action, we had not previously represented in DrOn any particular
therapeutic indication.
Results: In this work, we add therapeutic indications for three research use cases: resistant hypertension, malaria, and
opioid abuse research. We also added mechanisms of action for opioid analgesics and added 108 classes representing
drug products in response to a large term request from the Program for Resistance, Immunology, Surveillance and
Modeling of Malaria in Uganda (PRISM) project. The net result is a new version of DrOn, current to May 2016, that
represents three major therapeutic classes of drugs and six new mechanisms of action.
Conclusions: A therapeutic indication of a drug product is represented as a therapeutic function in DrOn. Adverse
effects of drug products, as well as other therapeutic uses for which the drug product was not designed are dispositions.
Our work provides a framework for representing additional therapeutic indications, adverse effects, and uses of drug
products beyond their design. Our work also validated our past modeling decisions for specific types of mechanisms of
action, namely effects mediated via receptor and/or enzyme binding. DrOn is available at: http://purl.obolibrary.org/obo/
dron.owl. A smaller version without NDCs is available at: http://purl.obolibrary.org/obo/dron/dron-lite.owl
Keywords: Biomedical ontology, Drug product, Therapeutic indication, Mechanism of action, Patient centered outcomes
research, Anti-hypertensive, Anti-malarial, Opioid analgesic, Function, Disposition
* Correspondence: hoganwr@ufl.edu
Department of Health Outcomes and Policy, University of Florida, Clinical
and Translational Research Building, 2004 Mowry Road, P.O. Box 100219,
Gainesville, FL 32610, USA
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 
DOI 10.1186/s13326-017-0121-5
Background
The Drug Ontology (DrOn) is a Web Ontology Language
version 2 (OWL2) based representation of drug products
and their ingredients, mechanisms of action, strengths,
and dose forms, as well as packaged drug products as rep-
resented by United States National Drug Codes (NDCs)
[13]. The primary goal of DrOn is to support analyses of
large, drug-related datasets such as pharmacy claims and
electronic health record (EHR) data. Pharmacy claims
datasets have traditionally been available to researchers
from public and private payers (or third-parties that serve
as the gateway to those payers data). In addition, state-
wide prescription drug monitoring programs to combat
opioid abuse are increasingly objects of study and capture
NDCs of packaged drug products. These datasets use
NDCs to identify the specific drug product that was dis-
pensed to the patient as well as the drug products pack-
aging and manufacturer. With the advent of research
consortia and networks such as the Observational Health
Data Sciences (OHDSI) collaborative [4] and the National
Patient Centered Clinical Research Network (PCORnet)
[5], massive EHR data sets with prescribing records nor-
malized to the RxNorm terminology are increasingly avail-
able to researchers. RxNorm is a standard medication
terminology built by the National Library of Medicine to
standardize prescribing data [6].
Research using these increasingly available and growing
claims and EHR datasets is facilitated by the ability to query
their drug data based on various characteristics of the drug
product prescribed or dispensed, such as therapeutic indi-
cation (e.g. hypertension) or mechanism of action of an ac-
tive ingredient (e.g. beta blocker), rather than on the drug
product itself. But the prototypical prescribing record in an
EHR dataset identifies the drug product at the level of in-
gredient, dosage, and dose formfor example, furosemide
20 mg oral tabletusing a concept unique identifier from
RxNorm (i.e., RxCui). The prototypical dispensing record
in a claims database and prescription drug monitoring data-
base uses an NDC to identify the drug product and its
manufacturer and packaging: for example, a bottle of 100
furosemide 20 mg oral tablets from Sanofi-Aventis branded
as Lasix. In the former case, the prescribing record will typ-
ically have an RxCui of 310429 for the furosemide 20 mg
tablet. In the latter case, the dispensing record will typically
have an NDC of 00039006710 for a bottle of 100 tablets.
To query prescribing and/or dispensing records for all anti-
hypertensivesor for all products with a beta-blocker as an
ingredientthe researcher needs an easy way to generate
lists of all RxCuis and/or NDCs, respectively, that represent
drug products with these characteristics. Our goal in creat-
ing DrOn was to create this ability.
Artifacts that pre-existed DrOn were not sufficient
for our purposes for various reasons. For example,
any given version of RxNorm only contains the NDCs
that were active at the time the version was released,
but DrOn contains a historical record of NDCs.
RxNorm, therefore cannot support querying historical
EHR and claims datasets spanning multiple years and
sometimes even decades without significant process-
ing of past versions. This is not to criticize RxNorm:
its primary use case was to support prescribing and
ordering of medications in a clinical setting (and
hence its central focus is the so-called Semantic Clin-
ical Drug). In addition, RxNorm is not available as an
OWL2 artifact to support integration with other on-
tologies such as those available from the Open Bio-
medical and Biological Ontology Foundry [7]. Unlike
RxNorm and all other OWL2 artifacts of which we
are aware, DrOn also represents the binding of ingre-
dient compounds to cytochrome P450 (CYP) isoen-
zymes as either substrate, inhibitor, or both to
support research on potential drug-drug interactions
[1]. Lastly, although the National Drug File Reference
Terminology (NDF-RT) is another OWL2 artifact
with drug information such as therapeutic indications
and mechanisms of action, it (a) makes basic scien-
tific mistakes such as saying that ophthalmic timolol
may treat systemic hypertension and that oral vanco-
mycin may treat bacterial endocarditis and (b) has
not been updated in its OWL2 form since 2013.
The increasing applicability and relevance of DrOn
was the motivation for the work described here. Specific-
ally, there were three use-cases from three major
research-focused projects that included representing
anti-hypertensive, anti-malarial, and analgesic thera-
peutic indications of certain drug products.
Although we had designed DrOn from the outset to
avoid the scientific mistakes about therapeutic uses of
drugs made by NDF-RT and other artifacts, prior to this
work DrOn did not represent any particular therapeutic
indication(s) of drug products. Furthermore, the use case
motivating analgesic indications also required represent-
ing the mechanisms of action of opioid analgesics and
antagonists, which we describe here. Also, the project
motivating inclusion of information about which drug
products are anti-malarials submitted a request for a
large number of terms that resulted in additional signifi-
cant development of DrOn that we report here. Lastly,
we illustrate for the first time a method for querying
DrOn to generate sets of RxCuis and/or NDCs to meet
the use cases driving DrOn development.
Methods
We first present the three research projects and the use
cases that they presented. Then we describe our meth-
odology for addressing the use cases. Lastly, we discuss a
software tool that we developed and used to query sets
of RxCuis or NDCs from DrOn.
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 2 of 11
Use cases
OneFlorida clinical data research network
The OneFlorida Clinical Research Consortium is a state-
wide infrastructure in Florida for implementation science,
comparative effectiveness research, and pragmatic clinical
trials [8]. It applied for and was awarded Phase II funding
as a Clinical Data Research Network (CDRN) in the Na-
tional Patient Centered Clinical Research Network (PCOR-
net). As part of PCORnet, OneFlorida is required to
develop a patient cohort around a common condition, for
which it chose hypertension [8].
Computable phenotypes are commonly used to identify a
subpopulation of interest [9]. In particular, researchers in
OneFlorida are studying resistant hypertension. Develop-
ment of the resistant hypertension cohort requires ex-
tremely accurate counting of how many anti-hypertensives
a patient is taking. To support the development of accurate
computable phenotypes for hypertension that, for example,
do not categorize patients as having hypertension who are
receiving ophthalmic timolol but no anti-hypertensive drug,
it was necessary to represent hypertension in DrOn as a
therapeutic indication of drug products rather than of mo-
lecular compounds. Because the OneFlorida data warehou-
secalled the OneFlorida Data Trustincorporates both
claims and EHR data, it is necessary to query for lists of
both NDCs and RxCuis which are used to standardize EHR
data to identify the drug product prescribed (at the pre-
scription stage, which manufacturer and packaging are not
known or even typically specified). In DrOn, a drug product
is a tablet, capsule, portion of solution, portion of cream,
etc. (prescription) whereas a packaged drug product is a
bottle of tablets or capsules, a tube of cream, a vial of solu-
tion, etc. for sale, distribution, delivery to the hospital ward
from the pharmacy, etc. (dispensing). Figure 1 shows the re-
lationships among packaged drug products, drug products,
and ingredients as captured in DrOn.
Program for Resistance, Immunology, Surveillance and
Modeling of Malaria (PRISM)
The Program for Resistance, Immunology, Surveillance and
Modeling of Malaria in Uganda (PRISM) is an International
Center of Excellence for Malaria Research (ICEMR) that
serves East Africa. It is a collaboration between Makerere
University in Uganda and the University of California San
Francisco. The National Institute for Allergy and Infectious
Disease (NIAID) created the ICEMR program in 2010 to
establish a worldwide network of research centers in
malaria-endemic settings to develop infrastructure for
researchers and practitioners working in various settings,
especially governments and healthcare institutions, to
combat malaria.
PRISM required representing anti-malarial indications of
drug products. It also submitted a request for a large num-
ber of terms to support matching drug codes in various
data sets to DrOn. Upon cursory review, it appeared that
many of these requests matched classes already in DrOn.
However, after discussion it became apparent that PRISM
required semantics for their requests that differed from the
classes that we created in DrOn to match RxNorm. Specif-
ically, RxNorm semantic clinical drugs (e.g. amoxicillin
50 mg/mL oral suspension) and semantic clinical drug
forms (e.g. amoxicillin oral suspension) list all active ingre-
dients exhaustively in a drug productthat is, they pre-
clude the possibility of having additional active ingredients.
For example, the class amoxicillin oral suspension does not
subsume the class amoxicillin/clavulanate oral suspension,
because it cannot have (by definition) additional active in-
gredients besides amoxicillin. The consequence is that in
RxNorm (and therefore in DrOn) amoxicillin oral suspen-
sion is a siblingnot a parentof amoxicillin/clavulanate
oral suspension. Other combination drug suspensions with
amoxicillin are also siblings. In addition, there is no com-
mon parent in RxNorm or DrOn for these drug products
(i.e., there is no class amoxicillin-containing suspension as a
common parent to the numerous siblings). However,
PRISM required classes that had the semantics of amoxicil-
lin-containing suspension that subsumes all kinds of sus-
pensions containing amoxicillin (both with and without
other active ingredients).
Prescription drug monitoring program research
Prescription Drug Monitoring Programs (PDMPs) such as
Floridas Electronic-Florida Online Reporting of Controlled
Substance Evaluation Program (E-FORCSE®) [10] utilize
Fig. 1 Representation of packaged drug products, drug products, ingredients, and the relationships among them in DrOn
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 3 of 11
databases of prescribed, controlled medications that include
limited patient information, prescriber information, and
NDCs for the dispensed medication. These databases may
be accessed by prescribers (to view their prescribing histor-
ies), pharmacists, and law enforcement. Health researchers
may also utilize this information to monitor (1) the intro-
duction of newly controlled substances on the market, (2)
medical morphine exposure in high-risk patient popula-
tions, (3) multi-drug prescribing associated with overdose
(e.g. overlapping prescriptions of opioids and benzodiaze-
pines), and (4) the impact of opioid prescribing policies
(e.g. on reducing prescribing of short-acting formulations
as the first option) on real-world prescribing behavior.
Opioid research with the PDMPs required the ability to
query opioid analgesics based on (1) a combination of
therapeutic indication (pain, medication assisted therapy
for opioid dependence) and mechanism of action (binding
to opioid receptors), (2) different mechanisms of action,
specifically binding to mu, delta, and/or kappa opioid re-
ceptors in either antagonistic or synergistic ways, and (3)
whether a drug is short- or long-acting. These queries are
important to pharmacoepidemiologists for understanding
the abuse potential and addictive properties of drug
products.
Methodology of DrOn development
Hogan et al. [3] and Hanna et al. [1, 2] describe our
methods of developing DrOn. DrOn development occurs
in two major parallel processes. The first process is trad-
itional, manual editing using the Protégé ontology editor.
We add to DrOn all information about the therapeutic
indications and mechanisms of action of drug products
and their ingredients through this manual process: we
do not automatically import it from any other source.
As described below, we search numerous sources of in-
formation to develop a comprehensive list of drug prod-
ucts with a particular indication or ingredients with a
particular mechanism of action, and we then curate the
information manually using Protégé. The second process
involved in building DrOn is automated construction of
classes from RxNorm.
DrOn is an OBO ontology and follows the OBO Foun-
drys realist methods and principles of ontology develop-
ment [7]. The ability to avoid confusing ophthalmic timolol
as an anti-hypertensive and oral vancomycin as having any
efficacy whatsoever against bacterial endocarditis was a key
validation of the realist approach we took in [3]. This ability
was the result of a key insight from our realist analysis that
a therapeutic indication is a property of a drug product
(tablet, ointment, cream, solution, etc.) with one or more
active ingredients; whereas the mechanism of action is typ-
ically the property of a chemical compound (Fig. 2). For ex-
ample, a molecule of timolol has the capability to
competitively bind a beta-adrenergic receptor, but one
molecule by itself has no ability to treat hypertension; it re-
quires instead a number of timolol molecules on the order
of Avogadros number to lower blood pressure. The drug
product, therefore, has a sufficient number of molecules, as
well as a formulation, that can be targeted towards a spe-
cific indication or indications. Thus, a timolol oral tablet
drug product has the indication of hypertension (but not
glaucoma); a portion of timolol ophthalmic solution has the
indication of glaucoma (but not hypertension); and one
timolol molecule by itself has no indication.
Besides dose form and intended route(s) of administra-
tion, the strength (quantity of active ingredient(s)) of a
drug product also affects its therapeutic indication. For
example, finasteride is prescribed in 5 mg dosages to
treat benign prostatic hyperplasia but in 1 mg dosages to
treat androgenetic alopecia. This example further sup-
ports our claim that the drug product, not the molecule,
is the bearer of a therapeutic indication.
DrOn is available as both the full ontology, including
NDCs, and a version without NDCs that includes all the
manually edited content as well as content derived from
RxNorm and imported from the Chemical Entities of Bio-
logical Interest ontology (ChEBI), the Protein Ontology
(PRO), and other ontologies. This lite version of DrOn is
available at [11]. The reader who is interested in reprodu-
cing our results on reasoning (methods and results dis-
cussed below) will find this version most amenable to the
task. We recommend using the Fact++ reasoner in Pro-
tégé version 5.1, and adjusting the Java heap size to 2GB
or more (4GB is recommended).
Methods for representing therapeutic indications
We analyzed therapeutic indications according to the realist
perspective that we have maintained throughout the devel-
opment of DrOn. Based on Hogan et al. we had already de-
termined that a therapeutic indication is a property of a
drug product and thus is some subtype of specifically
dependent continuant per Basic Formal Ontology (BFO)
[3]. Our analysis in this work focused further on whether it
is more specifically a quality, role, or disposition, and if the
latter, whether it is more specifically a function.
Methods for representing opioid analgesic mechanisms of
action
Opioid-acting compounds primarily act by binding ? (mu),
? (kappa), and ? (delta) opioid receptors (there are as many
as 17 kinds of opioid receptors in total). These receptors
are located in the cellular membranes of peripheral and
central nervous system neurons. The major effect of this
binding is to prevent the release of neurotransmitters at the
presynaptic nerve terminal, although opioid receptors also
exist at the postsynaptic neuron with inhibitory effects. The
consequent reduction in neurotransmission is what causes
analgesic effects as well as side effects (or sometimes even
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 4 of 11
desired effects) such as decreased bowel motility leading to
constipation.
Authors SA, BB, RE, TM, and MV looked for lists of
compounds with opioid activity in various resources, each
one focusing on a particular resource. The resources we
searched included DrugBank, NDF-RT, BioPortal, Onto-
bee, ChEBI, Wikipedia, and Goodman and Gilmans Man-
ual of Pharmacology and Therapeutics [12], as well as
general Internet searches using Google. Each person also
determined whether each compound on the list is used in
drug products for analgesic activity. Once each person
had generated as complete a list as she or he could from
her or his assigned resource, we deduplicated the lists to
produce a master list [13].
Once we had obtained a master list, the same authors
subsequently searched the same kinds of resources for
whether each compound was a mu agonist/antagonist,
kappa agonist/antagonist, and/or a delta agonist/antagon-
ist. Note that some compounds have agonist activity at
one receptor but antagonist activity at another receptor.
For example, fentanyl is a mu agonist and delta antagon-
ist, although in bulk its mu agonist effects dominate such
that it produces analgesia.
Methods for creating new classes
Authors SA, BB, MD, RE, TM, and MV each created
one subset of the requested OWL classes for the PRISM
project such that the union of these subsets covered all
the requests for which we could identify the active ingre-
dients. These six class creators created about 20 classes
each. To avoid conflicts in the DrOn git repository on
BitBucket, author JH created a fork of the repository
called dron-workspace. Authors WRH and AH then set
up a separate OWL file for each class creator. This
allowed each one to check in his or her OWL file with-
out any need for a merger that could break the RDF/
XML in any OWL file. If another class creator had
checked in his or her file before, synchronization was
easily achieved by doing a git pull command before is-
suing a git push to the centralized repository. Finally,
author JH merged the results of all six OWL files into a
new dron-hand.owl module of DrOn.
For one group of term requests, route of administra-
tion was applicable. This group of terms involved drug
solutions (i.e., one or more active ingredients dissolved
in a liquid medium). Some drug solutions are designed
for intravenous administration; some are designed for
both ophthalmic and otic adminstration (i.e., one formu-
lation can be administered either way); and some are
formulated only for ophthalmic administration. We have
not yet represented routes of administration of drug
products in DrOn; this task is out of the scope of this
paper and remains for future work. For drug solution
terms, we created a generic solution class with an
equivalent class definition. As an example, we define
gentamicin solution as:
drug solution and (has_proper_part
some (scattered molecular aggregate
and (is bearer of some active
ingredient role)
and (has granular part some
gentamicin)))
Then we created classes gentamicin intravenous solu-
tion and gentamicin ophthalmic solution as primitive
children of the gentamicin solution class.
Software tool for querying DrOn
To query sets of RxCuis or NDCs from DrOn for use in
research with EHR and claims datasets, we developed
dron-query, an open-source, Java-based, command-line
software application that uses the pre-existing OWL-API
(Web Ontology Language Application Programming
Interface) library. The dron-query application is available
on GitHub at [14], and there is a getting started Wiki
page at [15]. Essentially, dron-query (1) takes as input a
description logic (DL) query formatted in Manchester
Syntax (a standard syntax for writing description logic
axioms in Protégé among others), (2) executes it using a
description-logic reasoner, and (3) outputs for every
class that meets the criteria specified in the query its (a)
IRI, (b) rdfs:label, and (c) RxCui annotation value. If the
Fig. 2 Representation of molecular dispositions to capture mechanisms of action in DrOn
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 5 of 11
query is for packaged drug products, the NDC is the
rdfs:label output for the class, and the RxCui field in the
output is null. But if the query is for drug products, the
RxCui field is populated in addition to the IRI and
rdfs:label.
We developed and executed DL queries using dron-
query for the three use cases, and report the results.
Results
We represented (1) anti-malarial function and asserted it
as a function of 57 drug products (the reasoner infers it
to be a function of an additional 203 drug products), (2)
anti-hypertensive function and asserted it as a function
of 326 drug products (inferred for 2419 additional prod-
ucts), (3) analgesic function and asserted it as a function
of 413 drug products (inferred for 2779 additional prod-
ucts), and (4) six opioid mechanisms of action and
asserted them as dispositions of 59 chemical com-
pounds. We created 108 new classes in response to the
PRISM term request.
Therapeutic indications
A drug product has the potential to treat a disease or
symptom or other condition, and this potential is only
realized upon administration of the product toand
subsequent action onan organism. Note that here we
are using the word potential in the sense of an ability
or capability and not in the sense of probability of the
ability or capability of being realized.
Administering a drug product does not guarantee
realization of this potential; for example, one dose might
be insufficient, or edema of the bowel wall might inhibit
absorption of the drug, or the patient might have a
genotype that results in excessive metabolism of the ac-
tive ingredient into an inactive metabolite.
According to Basic Formal Ontology, this situation
means that a therapeutic indication of a drug product is
a realizable entity or one of its subtypes. BFO defines
realizable entity as a specifically dependent continuant
that has at least one independent continuant entity as its
bearer, and whose instances can be realized (manifested,
actualized, executed) in associated processes of specific
correlated types in which the bearer participates [16].
Furthermore, because the physical makeup of the
drug product confers this potential, and because
physical changes to the drug product can cause it to
lose its ability to treat instances of a particular type
of symptom or disease, a therapeutic indication is a
disposition (a subtype of realizable entity). BFO de-
fines disposition as a realizable entity that is such
that, if it ceases to exist, then its bearer is physically
changed [16]. Drug products can only lose their
therapeutic potential through physical change. For ex-
ample, a portion of epinephrine solution contained in
a self-injection device degrades upon exposure to air
and light, diminishing and ultimately removing with
time its disposition to treat hypersensitivity reactions.
Lastly, for a therapeutic indication of a drug product for
which the product was intentionally manufactured, a
therapeutic indication is a function, which per BFO is a
kind of disposition. BFO defines function as disposition
that exists in virtue of the bearers physical make-up, and
this physical make-up is something the bearer possesses be-
cause of how it came into beingeither through natural se-
lection (in the case of biological entities) or through
intentional design (in the case of artifacts) [16]. Because
drug products are extensively designed and planned to
have certain therapeutic indications, these indications are
functions. We also note that this usage is consistent with
the most recent exposition of functions as they are repre-
sented by BFO [17]. Figure 3 illustrates the relationship of
drug products to therapeutic functions.
Note that this means that given the definitions of func-
tion and disposition in BFO, the therapeutic potential(s)
of a drug product for which the product was manufac-
tured are functions, whereas therapeutic uses that are dis-
covered later (for example, off label uses of a drug per
the Food and Drug Administration) are initially disposi-
tions that are not functions. Should the manufacturer sub-
sequently manufacture the product with the intention of
including these additional therapeutic indications, then
these potentials are functions.
Note, however, that no particular instance of disposition
becomes an instance of function. For example, consider a
drug product that is a tablet with an indication (function)
to treat X. Suppose that later researchers discover that
the tablet also has the disposition to treat Y. Then all the
tablets manufactured prior to the addition of this indication
bear a function to treat X and a disposition to treat Y,
whereas all the tablets manufactured after the change bear
two functions: to treat X and to treat Y. It is possible
that the exact same physical basis for the disposition exists
for the function. This is analogous to the chopsticks ex-
ample of Spear et al. where there are two identical sticks of
wood (in terms of structure and form) one with the func-
tion to handle food because it was designed and manufac-
tured to handle food, and one with a disposition but not a
function to handle food because it came to have its struc-
ture incidentally [17].
Therefore, a therapeutic indication (function) is a sub-
type of therapeutic potential (disposition), where the
Fig. 3 A drug product is the bearer of a therapeutic function
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 6 of 11
former is the result of an intensively planned and exe-
cuted manufacturing process and the latter is broader
and includes effects of drug products that are discovered
after they are manufactured. For example, bupropion
tablets were originally designed and manufactured for
depression. Later, researchers discovered that they also
helped with smoking cessation, so companies started
manufacturing these tablets for smoking cessation in
addition to depression, ergo a new therapeutic function.
Bupropion tablets manufactured prior to the change
have one function; bupropion tablets manufactured after
the change have two functions.
This distinction also helps to differentiate thera-
peutic indications from adverse effects of drug prod-
ucts. The realization of a disposition of an oxycodone
tablet to cause constipation is an undesired effect. It
is not a therapeutic indication (function). The dispos-
ition of oxycodone tablets to create a dependence (or
addiction) is also an adverse event. We would repre-
sent in DrOn the dispositions to adverse events. Per
the Ontology of Adverse Events (OAE), an adverse
event is a process [18]. This process is typically the
realization of certain dispositions of the drug product.
Thus, we have also identified a key way to link DrOn
to OAE: a disposition of a drug product (DrOn) is re-
alized by an adverse event process (OAE).
In some cases, a type of disposition can have some
instances whose realizations are therapeutic and other
instances whose realizations are adverse events. For
example, erythromycin when used to treat infection
can have the adverse event of diarrhea caused by its
disposition to increase gastric motility. However, the
same disposition to increase gastric motility is some-
times used therapeutically to treat gastroparesis
caused by diabetes mellitus. In other cases, the type
of disposition can have some instances whose realiza-
tions are both therapeutic effects and adverse events,
such as when the therapeutic effect is taken to an ex-
treme (e.g., bleeding from anti-coagulant therapy and
hypotension from anti-hypertensive therapy).
Anti-hypertensive therapeutic function
Def: a therapeutic function of a drug product that is re-
alized by administration of the drug product resulting in
a decrease of systemic arterial pressure.
Hypertension is a sustained elevation in the pressure
exerted by blood on the systemic arteries (as opposed to
pulmonary arteries) of an organism. This condition is
well known to be a risk for multiple morbidities includ-
ing coronary artery disease, stroke, and kidney disease.
Drug products manufactured to treat hypertension all
have the disposition of lowering this pressure when ad-
ministered in the proper form and according to the
proper route of administration.
Anti-malarial therapeutic function
Def: a therapeutic function of a drug product that is re-
alized by administration of the drug product resulting in
creation of a material basis of a resistance to malaria in-
fection disposition.
In other words, a drug product like a choloroquine
tablet confers upon administration a protective resist-
ance to an infection of a certain kind, namely protective
resistance to individuals of one of four Plasmodium spe-
cies. The Infectious Disease Ontology defines protective
resistance as: A disposition that inheres in a material
entity in virtue of the fact that the entity has a part (e.g.
a gene product), which itself has a disposition to mitigate
damage to the entity [19]. Resistance to an infectious
agent then is a type of protective resistance. This kind of
resistance can be either acquired or innate (e.g., an ex-
treme but common case of innate resistance is the resist-
ance of one species to the infectious agents that
commonly infect another species). Acquired resistance
can occur through acquired immunity, through adminis-
tration of anti-infective drug products, and through
other mechanisms.
An anti-malarial is thus a drug product that, when ad-
ministered, confers a protective resistance to humans
against the four species of Plasmodium that cause mal-
aria in humans. DrOn imports resistance to malaria in-
fection, which is itself defined as A resistance to infection
by P. vivax, P. ovale, P. malariae, and/or P. falciparum.
Analgesic therapeutic function
Def: a therapeutic function of a drug product that is re-
alized by administration of the drug product resulting in
blocked realization of a disposition to pain.
We follow the definition of pain by Smith and Ceus-
ters as a type of process: an unpleasant experience on
the part of a human subject that is both sensory and
emotional and that is of a type that is either canonical
pain  or phenomenologically indistinguishable from ca-
nonical pain [20]. The physical basis of the pain can
range from activation of the nociceptive system (canon-
ical pain) to damage to the nociceptive system (neuro-
pathic pain) to changes in the cognitive system (e.g.,
pain behavior without nociception).
The net result is that certain physical changes result in
a disposition to experience pain, and the ultimate effect
of analgesics is to block the realization of this dispos-
ition. Restated, they confer a blocking disposition to pain
dispositions.
Adding mechanisms of action for opioid analgesics
We defined six dispositions, all of which inhere in mole-
cules. There are two each (agonistic and antagonistic)
for each of the three major opioid receptors that were
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 7 of 11
relevant to the E-FORCSE use case. These definitions all
follow the template:
A disposition of a molecule to bind an instance of <
kind of receptor > in a manner that < activates,
inhibits > the realization of the biological function of
the receptor.
We then link the function to the molecule using
OWL2 axioms of the form:
<molecule> subClassOf (is bearer of
some <disposition>)
(where is bearer of  is an object property, <molecule >
is a class, and < disposition > is a class).
Note that for compounds in DrOn that are repre-
sented by ChEBI class internationalized resource iden-
tifiers (IRIs), we are asserting this axiom directly on
ChEBI classes (which is true of nearly all the opioid
compounds). Furthermore, we note that we do not
yet represent in DrOn the processes that realize these
dispositions, nor the other participants in these pro-
cesses (e.g., the mu receptor itself ). The reason is that
our use cases have not yet required it. However, we
acknowledge that it might be useful to query for all
drug products whose ingredients act on mu receptors,
regardless of whether the action inhibits or activates
the receptor. It therefore remains future work to
enhance the representation in DrOn to include the
processes that realize molecular dispositions and their
participants. We note that the Gene Ontology [21]
has terms for the processes (e.g., GO:0031698 beta-2
adrenergic receptor binding), and the Protein
Ontology [22] has terms for the receptors (e.g.,
PR:000001193 beta-2 adrenergic receptor) that should
be reused for this purpose. For more on DrOns rep-
resentation of drug ingredients (molecules and aggre-
gates of them) and their dispositions and roles, see
Hanna et al. [1].
All told, we added at least one of the six dispositions
to 59 molecule types (Table 1).
We note that not all drug products with (an ingredient
that has) an opioid agonist mechanism have an analgesic
indication. For example, loperamide tablets are used to
treat diarrhea and are not indicated for pain relief (be-
cause they bind opioid receptors in the nerve cells of the
gut almost exclusively).
PRISM term request
We added 108 classes in response to the PRISM term
request: 89 classes have an equivalent class axiom, and
19 drug solution classes have necessary axioms only.
Figure 4 shows the class amoxicillin suspension and that
the Fact++ reasoner has inferred that the 10 amoxicillin
suspension classes (that derive from RxNorm) are sub-
sumed under it.
Using dron-query tool for the use cases
Anti-hypertensive query
We used dron-query to query several lists of RxCuis for
the anti-hypertensive use case. Specifically, we queried
all the RxCuis for anti-hypertensive products whose in-
gredients had one of the following mechanisms of ac-
tion, with one set of RxCuis per mechanism of action:
beta-adrenergic blockade, calcium channel blockade,
NKCC2 inhibition (loop diuretics), sodium-chloride co-
transporter inhibition (thiazide and thiazide-like di-
uretics), angiotensin converting enzyme inhibition, and
angiotensin receptor blockade.
The template for each query in Manchester syntax was
the following:
'drug product' and ('is bearer of' some
'anti-hypertensive function') and
(has_proper_part some ('has granular
part' some ('is bearer of' some
<disposition to bind some enzyme/
receptor>)))
Table 1 Counts of molecules with various opioid mechanisms
of action
Action Type of receptor
mu kappa delta
Agonist 28 11 9
Antagonist 4 3 3
Fig. 4 The new class amoxicillin suspension and its inferred children
in DrOn
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 8 of 11
For beta blockers, the exact query is:
'drug product' and ('is bearer of' some
'anti-hypertensive function') and
(has_proper_part some ('has granular
part' some ('is bearer of' some 'non-
activating competitive beta-adrenergic
receptor binding disposition')))
This query returns 612 classes representing drug prod-
ucts at multiple levels of granularity (i.e., metoprolol oral
tablet, metoprolol 50 mg oral tablet, Lopressor 50 mg
oral tablet).
To get a set of NDCs for anti-hypertensive drug prod-
ucts with beta blocker ingredients, the query is:
packaged drug product and
(has_proper_part some ('is bearer of'
some 'anti-hypertensive function') and
(has_proper_part some ('has granular
part' some ('is bearer of' some 'non-
activating competitive beta-adrenergic
receptor binding disposition'))))
This query returns 4,831 classes and their NDCs.
Anti-malarial query
For all drug products with an anti-malarial function, the
query is:
'drug product' and ('is bearer of' some
anti-malarial function)
This query returns 260 drug products and their
RxCuis.
Opioid analgesic query
For all analgesics that have a mu agonist as an ingredi-
ent, the query is:
'drug product' and ('is bearer of' some
analgesic) and (has_proper_part some
('has granular part' some ('is bearer of'
some 'mu agonist')))
This query returns 3034 drug products and their
RxCuis. The analogous query for packaged drug prod-
ucts (not shown) returns 8142 packaged drug products
and their NDCs.
Discussion
Based on use cases from three research projects, we ex-
tended DrOn with three therapeutic functions and six
mechanisms of action of drug products. We also added
108 classes in response to term requests, including amoxi-
cillin suspension and chloroquine tablet. These classes
have different semantics than RxNorm classes because
they do not exhaustively list all active ingredients. For ex-
ample, the new class amoxicillin suspension subsumes all
suspensions that have amoxicillin as one ingredient, in-
cluding compound drug products such as amoxicillin/cla-
vulanate oral suspension. Lastly, we illustrated the use of
the dron-query software tool for the three use cases.
The three therapeutic functionsanti-hypertensive,
anti-malarial, and analgesicare fairly diverse, providing
preliminary evidence that our approach is general. There
are no common biological pathways, or even organ sys-
tems, to hypertension, malaria, and pain. Although the
blood is involved with both hypertension and malaria,
the anti-hypertensive drugs all have a mechanism of ac-
tion that take place elsewhere (especially the kidneys,
where they bind cellular receptors).
We have laid an ontological basis for representing add-
itional therapeutic indications, off-label usages, and adverse
effects of drug products. And per the original design of
DrOn, we capture the therapeutic indication on the appro-
priate entity (drug product and not molecule). In so doing,
we do not mistake ophthalmic timolol for an anti-
hypertensive or oral timolol for a glaucoma drug. Repre-
senting actual dispositions of drug products towards ad-
verse events, and thereby linking DrOn and OAE, remains
future work.
DrOn is extensible in the manner described for repre-
sentation of additional therapeutic functions. Ongoing
work at present includes representing the therapeutic
function to treat asthma.
In addition, our past approach to representing mecha-
nisms of action was in this work easily adapted to opiates
and opioids. However, we note that all the mechanisms of
action represented in DrOn to date involve drug molecules
binding to molecular entities in the cellular membrane (e.g.,
receptors, enzymes, and transporters). Representing mecha-
nisms of action that involve other kinds of processes might
not follow this pattern. For example, because anti-infectives
act on a symbiont of the organism to which they are ad-
ministered, as opposed to the organism itself, representing
them could be more complex. Nevertheless, the space of
compounds that exert a biological effect through receptor
binding and enzyme inhibition is large and diverse.
We do not at present relate the molecular mechanism(s)
of action of a molecule to the relevant therapeutic func-
tions of a drug product that incorporates the molecule, or
vice versa. Thus, for atenolol oral tablet there is no con-
nection between its function to treat hypertension and the
dispositions of its atenolol molecules to bind beta-
adrenergic receptors. This task is future work. Because
our representation does not require capturing the rela-
tionship between mechanisms of action and therapeutic
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 9 of 11
functions however, we can represent in DrOn the thera-
peutic functions of drug products for which no mechan-
ism of action at the molecular level is yet known.
For the PDMP research, we have begun but not com-
pleted work on whether particular opiate and opioid-
containing drug products are short vs. long acting
formulations. We merely note here that duration of action
is a property of not just the half-life of the active ingredi-
ent(s) but also the particular way a dose is manufactured
(e.g., normal vs. delayed vs. extended release tablets and
capsules) and the genotype of the individual (e.g., single
nucleotide polymorphisms that result in poor vs. ultra-
rapid metabolism of drugs and prodrugs).
This work was limited to the extensions necessary to
meet specific research use cases. We have thus not stud-
ied clinical uses. However, we note that were a system
such as an EHR to recommend oral vancomycin to treat
endocarditis, or ophthalmic timolol to treat hyperten-
sion, a clinician might begin to doubt the system. In
addition, for our research use cases, we have not yet
compared counting anti-hypertensives, hypertensive pa-
tients, or patients with resistant hypertension using
DrOn to counting them using NDF-RT or other arti-
facts. This task remains as future work.
We also did not represent the intended routes of ad-
ministration of drug products, which prevented inclu-
sion of an equivalent class axiom on all classes added in
response to PRISM term requests. This task also re-
mains future work. We note that the Vaccine Ontology
has a set of classes representing routes of administration
that will likely be applicable [23].
We also note that none of the use cases discussed here
relate yet to pharmacogenomics or analyzing pharmaco-
dynamics or pharmacokinetics of drugs. However DrOn is
being used in other projects in this manner: we refer the in-
terested reader to Brochhausen et al. [24], which discusses
ontological representations of potential drug-drug interac-
tions and their pharmacodynamics and pharmacokinetics,
and how these representations reuse DrOn classes.
Conclusions
We successfully captured therapeutic indications of drug
products in the Drug Ontology as functions and other
therapeutic uses of drugs as dispositions, in keeping with
the definitions of disposition and function in Basic For-
mal Ontology. We represented the anti-hypertensive, anti-
malarial, and analgesic indications of numerous drug
products in DrOn in this manner. We also represented
the mechanisms of action of opioid analgesics (and other
opioid drug products), and we included over 100 new
classes in response to a term request from the PRISM pro-
ject. We also demonstrated how to use the dron-query
tool to extract from DrOn subsets of drug-product and
packaged-drug-product classes and their annotations for
various use cases. To date, our results show promise that
our methods are applicable to other therapeutic indica-
tions and mechanisms of action of drug products and
their ingredients, respectively.
Abbreviations
BFO: Basic formal ontology; CDRN: Clinical data research network;
CYP: Cytochrome P450; DrOn: Drug ontology; E-FORCSE: Floridas electronic-
Florida online reporting of controlled substance evaluation program;
EHR: Electronic health record; ICEMR: International center of excellence for
malaria research; NDC: National drug code; NDF-RT: National drug file reference
terminology; NIAID: National institute for allergy and infectious disease;
OAE: Ontology of adverse events; OBO: Open biological and biomedical
ontologies; OWL2: Web ontology language version 2; PCORnet: National patient-
centered clinical research network; PDMP: Prescription drug monitoring program;
PRISM: Program for resistance, immunology, surveillance and modeling of
malaria in uganda; RDF: Resource description framework; XML: eXtensible
markup language
Acknowledgements
We would like to thank Werner Ceusters for discussions on BFO definitions
during which he also pointed out the existence of the recent publication
clarifying functions and dispositions as defined by Basic Formal Ontology.
This acknowledgement does not imply that he agrees with all the
statements in this manuscript.
Funding
This work was supported in part by the NIH/NCATS Clinical and Translational
Science Awards to the University of Florida UL1 TR000064/UL1 TR001427 and
by award CDRN-1501-26692 from the Patient Centered Outcomes Research
Institute (PCORI). The content is solely the responsibility of the authors and
does not necessarily represent the official views of NIH/NCATS or PCORI.
Availability of data and materials
The Drug Ontology itself is available at the following permanent URL: http://
purl.obolibrary.org/obo/dron.owl. Also, the BitBucket repository where we
manage DrOn development is located at: https://bitbucket.org/uamsdbmi/
dron. The various spreadsheets we created in the process of this work are
publicly available on Google Docs at the URLs cited in the manuscript.
Authors contributions
Author WRH conceived the work and drafted the initial version of the
manuscript, a revised manuscript in response to comments of reviewers for
the International Conference on Biomedical Ontology, and a manuscript in
response to the invitation to submit to the Journal. Author AH assisted with
refining the ontological analysis of therapeutic indications and creating
definitions of anti-hypertensive, anti-malarial, and analgesic. Authors AS,
BB, MD, RE, TM, and MV performed work as described in the Methods
section, and also helped with the definitions just mentioned. Author CD
helped define the opioid analgesic use cases and identified additional
source materials for creating comprehensive lists of opioid compounds and
their receptor binding. All authors reviewed, provided feedback on, and
approved the manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
This research does not describe any individuals personal information and
thus this consent is also not applicable.
Ethics approval and consent to participate
This research is not human subjects research and thus approval and consent
are not applicable.
Hogan et al. Journal of Biomedical Semantics  (2017) 8:10 Page 10 of 11
Received: 11 November 2016 Accepted: 24 February 2017
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 
DOI 10.1186/s13326-017-0162-9
RESEARCH Open Access
Matching disease and phenotype
ontologies in the ontology alignment
evaluation initiative
Ian Harrow1*, Ernesto Jiménez-Ruiz2, Andrea Splendiani3, Martin Romacker4, Peter Woollard5,
Scott Markel6, Yasmin Alam-Faruque7, Martin Koch8, James Malone9 and Arild Waaler2
Abstract
Background: The disease and phenotype track was designed to evaluate the relative performance of ontology
matching systems that generate mappings between source ontologies. Disease and phenotype ontologies are
important for applications such as data mining, data integration and knowledge management to support
translational science in drug discovery and understanding the genetics of disease.
Results: Eleven systems (out of 21 OAEI participating systems) were able to cope with at least one of the tasks in the
Disease and Phenotype track. AML, FCA-Map, LogMap(Bio) and PhenoMF systems produced the top results for
ontology matching in comparison to consensus alignments. The results against manually curated mappings proved
to be more difficult most likely because these mapping sets comprised mostly subsumption relationships rather than
equivalence. Manual assessment of unique equivalence mappings showed that AML, LogMap(Bio) and PhenoMF
systems have the highest precision results.
Conclusions: Four systems gave the highest performance for matching disease and phenotype ontologies. These
systems coped well with the detection of equivalence matches, but struggled to detect semantic similarity. This
deserves more attention in the future development of ontology matching systems. The findings of this evaluation
show that such systems could help to automate equivalence matching in the workflow of curators, who maintain
ontology mapping services in numerous domains such as disease and phenotype.
Keywords: Biomedical ontology, Ontology alignment, OAE, Evaluation, Phenotype, Disease
Background
The Pistoia Alliance Ontologies Mapping project1 was set
up to find or create better tools and services for mapping
between ontologies (including controlled vocabularies) in
the same domain and to establish best practices for ontol-
ogy management in the Life Sciences. The project has
developed a formal process to define and submit a request
for information (RFI) from ontology matching system
providers to enable their evaluation.2 A critical compo-
nent of any ontology alignment system is the embedded
matching algorithm, therefore the Ontologies Mapping
*Correspondence: ian.harrow@pistoiaalliance.org
Equal contributors
1Pistoia Alliance Ontologies Mapping Project, Pistoia Alliance Inc, USA
Full list of author information is available at the end of the article
project is supporting their development and evaluation
through sponsorship and organisation of the Disease and
Phenotype track (added in 2016) for the OAEI campaign
[1]. In this paper we describe the experiences and results
in the OAEI 2016 Disease and Phenotype track.3
The Disease and Phenotype track is based on a real
use case where it is required to find two pairwise align-
ments between disease and phenotype ontologies: (i)
Human Phenotype Ontology [2] (HP) to Mammalian
Phenotype Ontology [3] (MP), and (ii) Human Disease
Ontology [4] (DOID) to Orphanet Rare Disease Ontol-
ogy4 (ORDO). The first task maps between human and
the more general mammalian phenotype ontologies. This
is important for translational science in drug discovery,
since mammalian models such as mice are widely used
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 2 of 13
to study human diseases and their underlying genetics.
Mapping human phenotypes to other mammalian phe-
notypes greatly facilitates the extrapolation from model
animals to humans. The second task maps between two
disease ontologies: the more generic DOID and the more
specific ORDO, in the context of rare human diseases.
These ontologies can support investigative studies to
understand how genetic variation can cause or contribute
to disease.
Currently, mappings between the aforementioned
ontologies within the disease and phenotype domain are
mostly generated manually by bioinformaticians and dis-
ease experts. Inclusion of automated ontology matching
systems into such curation workflows is likely to improve
the efficiency and scalability of this process to expand the
coverage across many source ontologies. Automation of
mappings is also important because the source ontologies
are dynamic, often having more than ten versions per year
which means the mappings must be maintained to remain
useful and valid.
Preliminaries
In this paper we assume that the ontologies are repre-
sented using the OWL 2 Web Ontology Language [5],
which is a World Wide Web Consortium (W3C) rec-
ommendation.5 Description Logics (DL) are the formal
underpinning of OWL 2 [6].
An ontology mapping (also called match or correspon-
dence) between entities of two ontologies O1,O2 is typ-
ically represented as a 4-tuple ?e, e?, r, c? where e and e?
are entities of O1 and O2, respectively; r ? {,,?} is
a semantic relation; and c is a confidence value, usually,
a real number within the interval (0 . . . 1]. Mapping con-
fidence intuitively reflects how reliable a mapping is (i.e.,
1 = very reliable, 0 = not reliable).
An ontology alignment M between two ontologies,
namely O1,O2, is a set of mappings between O1 and
O2. In the ontology matching community, mappings are
typically expressed using the RDF Alignment format [7].
In addition, mappings can also be represented through
standard OWL 2 axioms (e.g., [8]). This representation
enables the use of the OWL 2 reasoning infrastructure
that is currently available.
When mappings are translated into OWL 2 axioms, an
aligned ontology OM = O1 ? O2 ? M is the result of
merging the input ontologies and an alignment between
them. The aligned ontology is also an OWL 2 ontology.
An ontology matching system is a program6 that, given
two input ontologies O1 and O2, generates an ontology
alignmentMS.
An ontology matching task is typically composed by
one or more pairs of ontologies with their correspondent
reference alignments MRA. Reference alignments can be
of different nature: gold standards, silver standards and
baselines. Gold standards are typically (almost) complete
mapping sets that have been manually curated by domain
experts, while silver standard mapping sets are not nec-
essarily complete nor correct. Finally, baseline mappings
typically represent a highly incomplete set of the total
mappings. In this paper we use a type of silver standard
that has been created by voting the mappings produced by
several matching systems. In the remainder of the paper,
we refer to this (silver standard) mapping set as consensus
alignments.
The standard evaluation measures, for a system gen-
erated alignment MS, are precision (P), recall (R) and
f-measure (F) computed against a reference alignment
MRA as follows:
P = |M
S ? MRA|
|MS| , R =
|MS ? MRA|
|MRA| , F = 2 ·
P · R
P + R
(1)
Standard precision and recall have, however, limitations
when considering the (OWL 2) semantics of the input
ontologies and the mappings. Hence a mapping m such
that m ? MS and m ? MRA will penalise the stan-
dard precision value even though OMRA |= m, that is,
m is inferred or entailed (using OWL 2 reasoning) by the
union of the input ontologies O1 and O2 and the refer-
ence mappingsMRA. Analogously, a mappingm such that
m ? MS andm ? MRA will penalise standard recall, even
though the aligned ontology OMS can entail m. In this
paper we adopt the notion of semantic precision and recall
as defined in Eqs. 2 and 3 to mitigate the limitations of the
standard measures (the interested reader please refer to
[9, 10] for alternative definitions).
Semantic precision and recall, as presented in this paper,
may still suffer from some limitations [11]. In order to
reduce the impact of these limitations, when computing
semantic precision and recall, equivalence mappings (?)
are split into two subsumption mappings ( and ).
Note that when evaluating the mappings produced by a
matching system against (incomplete) baseline mappings,
only semantic recall should be taken into account.
P(sem) = |{m ? M
S | OMRA |= m}|
|MS| (2)
R(sem) = |{m ? M
RA | OMS |= m}|
|MRA| (3)
An ontology is incoherent [12] if it contains logical
errors in the form of unsatisfiable concepts. If the union
of the input ontologies O1 and O2 and the reference
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 3 of 13
mappings MRA is incoherent, semantic precision and
recall, as defined in Eqs. 2 and 3, may lead to unex-
pected results. In this case, mapping repair (e.g., [1214])
techniques should be applied before computing semantic
precision and recall.
Methods
The Ontology Alignment Evaluation Initiative7 (OAEI)
is an annual campaign for the systematic evaluation of
ontologymatching systems [1, 1517]. Themain objective
is the comparison of ontology matching systems on the
same basis and to enable the reproducibility of the results.
The OAEI included 9 different tracks organised by dif-
ferent research groups and involving different matching
tasks.
The novel Disease and Phenotype8 track was one of the
new additions in the OAEI 2016 campaign. The track aims
at evaluating the performance of systems in a real-world
use case where pairwise alignments between disease and
phenotype ontologies are required.
The Disease and Phenotype track closely followed the
OAEI phases as summarised in Fig. 1.
Dataset
The Disease and Phenotype track comprises two match-
ing tasks that involve the alignment of the Human
Phenotype Ontology (HP), the Mammalian Phenotype
Ontology (MP), the Human Disease Ontology (DOID),
and the Orphanet Rare Disease Ontology (ORDO).
Table 1 shows the metrics provided by BioPortal of these
ontologies.
Task 1: pairwise alignment of the HP and the MP ontolo-
gies (HP-MP matching task).
Task 2: pairwise alignment of the DOID and the
ORDO ontologies (DOID-ORDOmatching task).
Preparation phase
As specified by the OAEI the ontologies and (public)
reference alignments were made available in advance
during the first week of June 2016. The ontologies
and mappings were downloaded from BioPortal [18] on
June 2nd.
The mappings were obtained using a script that, given a
pair of ontologies, uses BioPortals REST API9 to retrieve
all mappings between those ontologies. We focused only
on skos:closeMatch (BioPortal) mappings10 as suggested in
[19], and we represented them as equivalencemappings.11
The BioPortal-based alignment between HP and MP con-
sisted in 639 equivalence mappings, while the alignment
between DOID and ORDO included 1,018 mappings.
Mappings were made available in both RDF Alignment
and OWL 2 formats.
The preparatory phase gives the opportunity to both
OAEI track organisers and participants to find and correct
problems in the datasets. During this phase we noticed
that the BioPortal mappings were highly incomplete.12
Hence, the participants were notified that the BioPortal-
based mappings were to be used as a baseline and not as a
gold standard reference alignment. Given the limitations
of the BioPortal mappings we were in need of creating
a (blind) consensus reference alignment to perform the
(automatic) evaluation (see details in the Evaluation phase
section).
All (open) OAEI datasets were released on July 15th,
2016 and did not evolve after that.
Execution phase
System developers had to implement a simple inter-
face and to wrap their tools including all required
libraries and resources in order to use the SEALS
infrastructure.13 The use of the SEALS infrastructure
ensures that developers can perform a full evaluation
locally and eases the reproducibility and comparability of
the results.
This phase was conducted between July 15th and
August 31st, 2016. During this time OAEI organisers
attended technical issues reported by the developers. We
also requested system developers to register their systems
and their intention to participate in the different OAEI
tracks by July 31st. Thirty systems were registered, from
which 14 seemed potential participants of theDisease and
Phenotype track.
Evaluation phase
Participants were required to submit their wrapped tools
by August 31st, 2016. From the 30 registered systems only
21 were finally submitted, and 13 were annotated (by the
system developers) as participants of theDisease and Phe-
notype track. The final results were published on theOAEI
website by October 15th.
Fig. 1 Phases of the OAEI 2016 Disease and Phenotype track. Important Dates: D1 (publication of preliminary datasets), D2 (final datasets released), D3
(system registration), D4 (system submission), D5 (publication of evaluation results and presentation in the Ontology Matching workshop [1])
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 4 of 13
Table 1 Metrics of the track ontologies. Source: NCBI BioPortal on 2nd June 2016
Ontology Number of axioms Number of classes Maximum depth Avg. number of children
HP 137,289 11,786 15 3
MP 129,036 11,721 15 3
DOID 124,362 9248 12 3
ORDO 188,991 12,936 11 16
Note that the metric average number of children excludes the leaf nodes
Algorithm 1 Steps followed in the evaluation
Input:O1,O2: ontologies in matching task;MRAm :
manually generated alignment; Systems: ontology
matching systems participating in the task.
 Generation of system alignments with SEALS infras-
tructure:
1: for each Systemi in Systems do
2: MSi ? Systemi(O1,O2)  Computes system
alignment
3: end for
 Generation of consensus alignments:
4: MRAc2 ? ConsensusAlignment
(MS1 . . .MSn, 2
) 
With vote 2
5: MRAc3 ? ConsensusAlignment
(MS1 . . .MSn, 3
) 
With vote 3
 Aligned ontologies for consensus reference align-
ments:
6: OMRAc2 ? O1 ? O2 ? MRAc2  RepairMRAc2 if required
7: OMRAc3 ? O1 ? O2 ? MRAc3  RepairMRAc3 if required Evaluation for each system generated alignments:
8: for eachMSi inMS1 . . .MSn do Aligned ontology forMSi :
9: OMSi ? O1 ? O2 ? MSi  RepairMSi if required Results against consensus alignment with vote 2:
10: P2 ? SemanticPrecision
(
MSi ,OM
RA
c2
)
11: R2 ? SemanticRecall
(
MRAc2 ,OM
S
i
)
 Results against consensus alignment with vote 3:
12: P3 ? SemanticPrecision
(
MSi ,OM
RA
c3
)
13: R3 ? SemanticRecall
(
MRAc3 ,OM
S
i
)
 Results against manually generated alignment:
14: Rm ? SemanticRecall
(
MRAm ,OM
S
i
)
 Manual assessment of unique system mappings:
15: USi ? UniqueMappings
(
MSi ,OM
RA
c2
)
16: {Pm,PC,NC} ? ManualAssessment
(USi
)
17: end for
The evaluation for the Disease and Phenotype track was
semi-automatic with support of the SEALS infrastruc-
ture. Systems were evaluated according to the following
criteria for each of the matching tasks of the Disease and
Phenotype track:
 Semantic precision and recall with respect to the
consensus alignments.
 Semantic recall with respect to manually generated
mappings.
 Manual assessment of unique mappings produced by
a participant system.
Algorithm 1 formalizes the steps followed in the evalua-
tion for each of theDisease and Phenotypematching tasks.
The following subsections below comment on the main
points of the evaluation process.
Consensus alignments. The consensus alignments are
automatically generated based on the alignments pro-
duced by the participating systems in each of thematching
tasks of the track. For the evaluation we have selected the
consensus alignments of vote=2 (i.e., mappings suggested
by two or more systems) and vote=3 (i.e., mappings sug-
gested by three or more systems). In the case where both
an equivalence and a subsumption mapping contribute to
the consensus, the equivalence relationship prevails over
the subsumption. The use of vote=2 and vote=3 was
motivated by our experience in the creation of consen-
sus alignments [20]. Consensus alignments with vote?4
are typically highly precise but also very incomplete unless
the number of contributing systems is significant.14 Note
that, when there are several systems of the same fam-
ily (i.e., systems participating with several variants), their
(voted) mappings are only counted once in order to
reduce bias.15
Note that consensus alignments have numerous lim-
itations. It allows us to compare how the participating
systems perform only in relation to each other. Some of
the mappings in the consensus alignments may be erro-
neous (false positives), as it only requires 2 or 3 systems
to agree on the erroneous mappings they find. Further-
more, the consensus alignments may not be complete,
as there will likely be correct mappings that no or only
one system is able to find. Nevertheless, consensus align-
ments help to provide some insights into the performance
of a matching system.
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 5 of 13
Semantic precision and recall. As introduced in the
Preliminaries section, the semantic precision and recall
take into account the implicit knowledge derived from
the ontologies and the mappings via OWL 2 rea-
soning.16 Hence, the methods SemanticPrecision and
SemanticRecall in Algorithm 1 receive as input a set of
mappings M and a coherent ontology O. Both methods
return as output the value of |M?||M| whereM? is a subset of
M such that the mappingsm ? M? are entailed byO (i.e.,
O |= m).
Manually generated mappings. These reference map-
pings were created through manual curation by eight
disease informatics experts, who are authors of this paper,
all working within or for the pharmaceutical industry for
three areas of phenotype and disease; namely carbohy-
drate and glucose metabolism, obesity and breast cancer.
These sets of reference mappings comprised of 29 pair-
wise mappings between HP and MP and 60 pairwise
mappings between DOID and ORDO across the three
areas. They included some relationships of equivalence,
but most of them represented subsumption relationships.
The three areas were selected as representative samples
which were known already to be present across the four
source ontologies. Inclusion of these manually defined
mappings enabled a real-world evaluation of recall for the
two matching tasks. The future editions of the track will
increase the number of manual mappings through inclu-
sion of additional areas relevant to the phenotype and
disease domain.
Unique mappings and manual assessment. Unique
mappings are mappings generated by an ontology match-
ing system that have not been (explicitly) suggested
by any of the other participating systems, nor entailed
by the aligned ontology using the consensus alignment
with vote=2
(
OMRAc2
)
. The method UniqueMappings in
Algorithm 1 receives as input a set of mappings M and
the (coherent) ontology OMRAc2 and returns as output M?
where M? ? M such that the mappings m ? M? are not
entailed byOMRAc2
(
i.e.,OMRAc2 |= m
)
.
Manual assessment over unique mappings has been
performed by an expert in disease informatics from the
pharmaceutical industry. This assessment aims at comple-
menting the evaluation against the consensus alignments
of those mappings that, although being suggested or voted
by only onematching system,may still be correct.We have
focused the assessment on unique equivalencemappings
and we have manually evaluated up to 30 mappings for
each system in order to (roughly) estimate the percentage
of correct mappings (i.e., precision, Pm in Algorithm 1)
and the positive/negative contribution to the total num-
ber of unique mappings (PC and NC in Algorithm 1),
that is, the weight of the correct (i.e., true positives) and
incorrect (i.e., false positives) mappings. Intuitively, the
positive contribution (see Eq. 4) of a system producing a
small set of unique mappings will most likely be smaller
than a system producing a larger set of unique (andmostly
correct) mappings. The negative contribution (see Eq. 5)
will weight the number of incorrect uniquemappings with
respect to the total. Negative and positive contributions,
for a set of unique mappings USi computed by a system i,
are defined as follows:
PositiveContribution
(
USi
)
=
?
?USi
?
? · Precision (USi
)
?n
j=1
?
?USj
?
? (4)
NegativeContribution
(
USi
)
=
??USi
?? · (1 ? Precision (USi
))
?n
j=1
?
?USj
?
?
(5)
Results
We have run the evaluation of the Disease and Phenotype
track in a Ubuntu Laptop with an Intel Core i7-4600U
CPU @ 2.10 GHz x 4 and allocating 15 Gb of RAM. From
the 13 systems registered to the track (out of 21 OAEI par-
ticipants), 11 systems have been able to cope with at least
one of the Disease and Phenotype matching tasks within
a 24 h time frame. Results for all OAEI tracks have been
reported in [1].
Participating systems
AML [21, 22] is an ontology matching system originally
developed to tackle the challenges of matching biomedical
ontologies. While its scope has since expanded, biomed-
ical ontologies have remained one of the main drives
behind its continued development. AML relies on the use
of background knowledge and it also includes mapping
repair capabilities.
DiSMatch [23] estimates the similarity among concepts
through textual semantic relatedness. DiSMatch relies on
a biomedical domain-adapted variant of a state-of-the-
art semantic relatedness measure [24], which is based on
Explicit Semantic Analysis.
FCA-Map [25] is an ontology matching system based on
Formal Concept Analysis (FCA). FCA-Map attempts to
push the envelope of the FCA to cluster the commonalities
among classes at various levels.
LogMap [26, 27] relies on lexical and structural indexes
to enhance scalability. It also incorporates approximate
reasoning and repair techniques to minimise the number
of logical errors in the aligned ontology.
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 6 of 13
LogMapBio [28] extends LogMap to use BioPortal [18]
as a (dynamic) provider of mediating ontologies, instead
of relying on a few preselected ontologies. LogMap-
Bio retrieves the most suitable top-10 ontologies for the
matching task.
LogMapLt is a lightweight variant of LogMap, which
essentially only applies (efficient) string matching tech-
niques.
LYAM++ [29] is a fully automatic ontology matching sys-
tem based on the use of external sources. LYAM++ applies
a novel orchestration of the components of the matching
workflow [30].
PhenomeNET [31] alignment system comes in three
flavours, which rely on three different versions of the
PhenomeNET ontology [32]. PhenomeNET-Plain (Phe-
noMP) relies on a plain ontology which only uses the
axioms provided by the HP ontology and the MP ontol-
ogy. PhenomeNET-Map (PhenoMM) utilizes additional
lexical equivalence axioms between HP and MP pro-
vided by BioPortal. Finally, PhenomeNET-Full (PhenoMF)
relies on an extended version of the PhenomeNET ontol-
ogy with equivalence mappings to the DOID and ORDO
ontologies obtained via BioPortal and the AML matching
system [21].
XMap [33] is a scalable matcher that implements parallel
processing techniques to enable the composition of basic
ontology matchers. It also relies on the use of external
resources such as the UMLS Metathesarus [34].
Use of specialised background knowledge
The use of (specialised) background knowledge is allowed
in the OAEI, but participants are required to spec-
ify which sources their systems rely on to enhance the
matching process. AML has three sources of background
knowledge which can be used as mediators between
the input ontologies: the Uber Anatomy Ontology [35]
(Uberon), the Human Disease Ontology [4] (DOID)
and the Medical Subject Headings17 (MeSH). LYAM++
also makes use of the Uberon ontology [35]. LogMap-
Bio uses BioPortal [18] as dynamic mediating ontology
provider, while LogMap uses normalisations and spelling
variants from the general (biomedical) purpose UMLS
Lexicon.18 XMAP uses synonyms provided by the
UMLS Metathesaurus [34]. Finally, PhenoMM, PhenoMF
and PhenoMP rely on different versions of the Phe-
nomeNET19 ontology [32] with variable complexity as
described above.
Evaluation against BioPortal (baseline) mappings
Table 2 shows the results in terms of semantic recall
against the baseline mappings extracted from BioPortal as
described in the Methods Section (Preparation phase).
In the DOID-ORDO task, LYAM++ failed to complete
the task while PhenoMM and PhenoMP produced empty
mapping sets.
BioPortal mappings mostly represent correspondences
with a high degree of lexical similarity and, as expected,
most of the systems managed to produce alignments with
a very high recall. DiSMatch, LYAM++, PhenoMM (in
the DOID-ORDO task) and PhenoMP were the exception
and produced very low results with respect to the base-
line mappings. As mentioned in the Methods Section,
since the BioPortal mappings were highly incomplete, the
results in terms of (semantic) precision were not signif-
icant. For this reason, we needed to create consensus
alignments for each task.
Creation of consensus alignments
In the MP-HP matching task 11 systems were able to
produce mappings. Mappings voted by LogMap and Phe-
nomeNET families were only counted once, and hence
there were 7 independent system groups contributing to
the consensus alignment. In the DOID-ORDO match-
ing task 8 systems generated mappings and there were 6
independent system groups contributing to the consensus
alignment.
Table 3 (resp. Table 4) shows the size of the differ-
ent consensus alignments from vote=1, i.e., mappings
suggested by one or more system groups, to vote=7
(resp. vote=6), i.e., mappings suggested by all system
groups, in the HP-MP matching task (resp. DOID-ORDO
task). It is noticeable that in the HP-MP task there
were 0 mappings where all systems agreed, while in
the DOID-ORDO task there were only 36. The num-
ber of mappings suggested by one system or more is
specially large because PhenomeNET systems produce a
large number of subsumption mappings. If only equiv-
alence mappings of PhenomeNET systems are taken
into account, the number of mappings with vote=1
would be 3433 in the HP-MP task and 2708 in the
DOID-ORDO task.
Table 2 Recall against BioPortal (baseline) mappings
System AML DiSMatch FCA-Map LYAM++ LogMap LogMapBio LogMapLt PhenoMF PhenoMM PhenoMP XMap
HP-MP 1.0 0.25 0.998 0.014 0.997 1.0 0.994 1.0 1.0 0.412 0.995
DOID-ORDO 0.993 0.048 0.984 - 0.942 0.950 0.943 0.994 0.0 0.0 0.967
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 7 of 13
Table 3 Consensus alignments for the HP-MP matching task
Min. Votes 1 2 3 4 5 6 7
Mappings 217039 2308 1588 1287 677 152 0
Seven (family) system groups contributing
As described in the Methods Section we have selected
the consensus alignments of vote=2 and vote=3. These
consensus alignments for HP-MP contain 2308 and
1588 mappings, respectively; while for DOID-ORDO
they include 1883 and 1617 mappings, respectively.
Table 5 shows some examples of mappings included with
the consensus alignments of vote=2 and vote=3. Also
shown are some examples of manually created mappings
and (correct/incorrect) unique mappings from ontology
matching systems.
Results against consensus alignments
The union of the input ontologies together with the con-
sensus alignments or the mappings computed by each of
the systems was coherent and thus, we did not require to
repair any of the mapping sets to calculate the semantic
precision and recall. Note that the downloaded ontol-
ogy versions from BioPortal did not contain any explicit
or implicit disjointness. Tables 6 and 7 show the results
achieved by each of the participating systems against the
consensus alignments with vote=2 and vote=3. In the
DOID-ORDO task, LYAM++, PhenoMM and PhenoMP
failed to produce mappings and they were not included in
Table 7.
We deliberately did not rank the systems since, as men-
tioned in the Methods section, the consensus align-
ments may be incorrect or incomplete. We have simply
highlighted the systems producing results relatively close
to the consensus alignments. For example, in the HP-MP
task, LogMap is the system producing an alignment that
is closer to the mappings voted by at least 2 systems, while
FCA-MAP produces results very close to the consensus
alignments with vote=3.
The use of semantic precision and recall allowed us to
provide a fair comparison for the systems PhenoMF, Phe-
noMM and PhenoMP. These systems discover a large set
of subsumption mappings that are not explicit in the ref-
erence alignments, but they are still valid (i.e., they are
entailed by the aligned ontology using the reference align-
ment). For example, the standard precision of PhenoMF
Table 4 Consensus alignments for the DOID-ORDO matching
task
Min. Votes 1 2 3 4 5 6
Mappings 50,998 1883 1617 1447 991 36
Six (family) system groups contributing
in the HP-MP task is 0.01 while the semantic precision
reaches the value of 0.76.
Tables 6 and 7 also include the results of BioPortal map-
pings against the consensus alignments. Precision values
are perfect, but recall is very low, which confirms our
intuitions (recall Preparation phase section) about the
incompleteness of BioPortal mappings.
It is striking howXMap and LogMapLt produced results
very similar to the ones obtained by the BioPortal map-
pings. Closer scrutiny of these results showed us that
the computed mappings were indeed very similar to the
BioPortal mappings (i.e., the F-measure of XMap and
LogMapLt against the baseline mappings provided by
BioPortal is ? 0.95 in both tasks).
This could be expected for LogMapLt, since it only
relies on simple string matching techniques as the match-
ing system underlying BioPortal [36]. However, the results
for XMap are unexpected since it produced top-results
in the other biomedical-themed tracks of the OAEI
2016 [1].
Results against manually created mappings
Table 8 shows the results in terms of semantic recall
against the manually created alignments. The results
obtained in the HP-MP are relatively large positive values
in general, especially for PhenoMF and PhenoMM that
achieve a semantic recall of 0.90. The numbers for the
DOID-ORDO, however, are much smaller values and only
LogMap, LogMapBio and DisMatch are able to discover
a few of the manually curated mappings. LogMapBio
obtained the best semantic recall value with 0.17, which
is far from the top results in the HP-MP task. The afore-
mentioned results are also reflected when considering the
consensus alignments. In the HP-MP task, both the con-
sensus alignments with vote 2 and 3 obtained reasonably
good results. However the picture changes dramatically
in the DOID-ORDO task where none of the manually
curated mappings are covered by the mappings agreed by
2 or more systems. The most likely explanation for this
result is that the manual mappings for DOID-ORDO rep-
resent more complex subsumption mappings which were
not possible to (semantically) derive for the other map-
pings. Table 8 also shows the results for the BioPortal
mappings, which, as expected, have a coverage of curated
mappings very similar to the obtained by LogMapLt and
XMap systems.
The use of semantic recall together with the standard
measure, as in previous section, allowed us to provide
more realistic results and a fair comparison with the Phe-
nomeNET family systems. As it can be observed in the
HP-MP task (Table 8), the standard recall, unlike the
semantic recall, obtained by the other participants was
very low and not comparable to the PhenomeNET family
systems.
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 8 of 13
Table 5 Example mappings in the Disease and Phenotype track
Entity 1 Entity 2 Rel. Source
x-linked chondrodysplasia punctata (DOID_0060292) Chondrodysplasia punctata (Orphanet_93442) ? (only) consensus alignment vote=2
Meningeal melanomatosis (DOID_8243) Diffuse leptomeningeal melanocytosis ? Consensus alignment vote=3
(Orphanet_252031)
Reactive arthritis (DOID_6196) Reactive arthritis (Orphanet_29207) ? Consensus alignment vote=3
Hypoplastic scapulae (HP_0000882) Short scapula (MP_0004340) ? (only) consensus alignment vote=2
Macrocytic anemia (HP_0001972) Macrocytic anemia (MP_0002811) ? Consensus alignment vote=3
Unerupted tooth (HP_0000706) Failure of tooth eruption (MP_0000121) ? Consensus alignment vote=3
Breast leiomyosarcoma (DOID_5285) Rare malignant breast tumor  Manually created
(Orphanet_180257)
Abnormality of body weight (HP_0004323) Abnormal body weight (MP_0001259) ? Manually created
Microcephaly (HP_0000252) Decreased brain size (MP_0000774) ? AML unique mapping (correct)
Skeletal dysplasia (HP_0002652) Abnormal skeletal muscle morphology ? AML unique mapping (incorrect)
(MP_0000759)
Carbohydrate metabolism disease (DOID_0050013) Disorder of carbohydrate metabolism ? LogMapBio unique mapping (correct)
(Orphanet_79161)
Spinocerebellar ataxia type 35 (DOID_0050982) Transglutaminase 6 (Orphanet_279644) ? LogMapBio unique mapping (incorrect)
Female hypogonadism (HP_0000134) Small ovary (MP_0001127) ? PhenoMF unique mapping (correct)
While the top performing algorithms were able to detect
equivalence matches across whole source ontologies for
the two mapping tasks giving high F-measures (Tables 6
and 7), it is clear from detection of the curated align-
ments that these proved much more difficult with a trend
for lower semantic recall across both tasks (Table 8).
This result was not surprising because the curated align-
ments mostly comprised of subsumption relationships
rather than equivalence. Table 5 shows two examples
of curated mappings; the equivalence mapping between
abnormality of body weight and abnormal body weight
was suggested by at least one the systems, while the sub-
sumption mapping between breast leiomyosarcoma and
rare malignant breast tumor was not discovered by any of
the systems.
Results for manual assessment of unique mappings
Tables 9 and 10 show the results of the manual
assessment of the unique mappings generated by the
participating systems. As mentioned in the Methods
section we manually analysed up to 30 unique equiva-
lence mappings for each system to estimate the precision
of the generated mappings not agreed with other sys-
tems. Table 5 shows examples of unique mappings com-
puted by AML, LogMapBio and PhenoMF. Note that,
we focus on equivalence mappings since PhenomeNET
Table 6 Results against consensus alignments with vote=2 and vote=3 in the HP-MP task
System-mappings Mappings Precision-2 F-Measure-2 Recall-2 Precision-3 F-Measure-3 Recall-3
BioPortal (baseline) 639 1.00 0.50 0.33 1.00 0.60 0.43
AML 1755 0.93 0.86 0.80 0.85 0.90 0.94
DiSMatch 644 0.55 0.30 0.21 0.45 0.28 0.20
FCA ? Map 1590 0.98 0.85 0.75 0.94 0.93 0.92
LYAM + + 381 0.41 0.12 0.07 0.17 0.06 0.04
LogMap 2011 0.94 0.92 0.91 0.77 0.86 0.97
LogMapBio 2151 0.92 0.92 0.93 0.75 0.85 0.98
LogMapLt 667 1.00 0.51 0.34 1.00 0.62 0.45
PhenoMF 204,089 0.76 0.83 0.92 0.63 0.76 0.95
PhenoMM 198,149 0.77 0.83 0.91 0.64 0.76 0.94
PhenoMP 169,660 0.78 0.67 0.58 0.64 0.57 0.51
XMap 650 1.00 0.50 0.33 1.00 0.61 0.44
Precision and Recall represent their semantic variants
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 9 of 13
Table 7 Results against consensus alignments with vote=2 and vote=3 in the DOID-ORDO task
System-mappings Mappings Precision-2 F-Measure-2 Recall-2 Precision-3 F-Measure-3 Recall-3
BioPortal (baseline) 1018 0.99 0.71 0.55 0.99 0.76 0.62
AML 2098 0.85 0.91 0.97 0.78 0.87 1.00
DiSMatch 335 0.23 0.08 0.05 0.19 0.07 0.04
FCA ? Map 1803 0.97 0.96 0.96 0.89 0.94 0.99
LogMap 1667 0.95 0.91 0.88 0.91 0.92 0.94
LogMapBio 1804 0.92 0.91 0.90 0.86 0.90 0.95
LogMapLt 1000 0.99 0.72 0.56 0.99 0.76 0.62
PhenoMF 40,612 0.95 0.89 0.83 0.95 0.94 0.92
XMap 1030 0.98 0.72 0.57 0.98 0.77 0.63
Precision and Recall represent their semantic variants
systems produce a large amount of (unique) subsumption
mappings.
BioPortal mappings, as expected, contains a very low
number of uniquemappings in the DOID-ORDO task and
no unique mappings in the HP-MP task.
It is noticeable in the HP-MP task that, although DiS-
Match and LYAM++ produced very low results with
respect to the consensus alignments (see Table 3), the pos-
itive contribution of their unique mappings is one of the
highest. Nevertheless, their negative contribution has also
an important weight. PhenomeNET systems produced
the most precise set of unique mappings although their
positive contribution was lower than other systems.
In the DOID-ORDO matching task, AMLs unique
mappings contains the higher number of true positives
with a reasonable number of false positives. LogMapBio
provided the best trade-off between positive and negative
contribution.
The last row in Tables 9 and 10 shows (excluding Bio-
Portal mappings) the total number of unique mappings,
its (average) precision, and the total (aggregated) positive
and negative contribution.
Results in the OAEI interactive matching track
The OAEI interactive track20 aims at offering a systematic
and automated evaluation of matching systems with user
interaction to compare the quality of interactive match-
ing approaches in terms of F-measure and number of
required interactions. The interactive track relies on the
datasets of the OAEI tracks: Conference, Anatomy, Large-
bio, and Disease and Phenotype; and it uses the reference
alignments of each track as oracle in order to simulate
Table 8 Results against curated alignments
System-mappings
HP-MP task DOID-ORDO task
Standard recall Semantic recall Standard recall Semantic recall
BioPortal (baseline) 0.17 0.52 0.00 0.00
AML 0.28 0.76 0.00 0.00
DiSMatch 0.07 0.14 0.02 0.03
FCA ? Map 0.21 0.62 0.00 0.00
LYAM + + 0.00 0.00 - -
LogMap 0.24 0.66 0.02 0.12
LogMapBio 0.28 0.69 0.03 0.17
LogMapLt 0.17 0.52 0.00 0.00
PhenoMF 0.90 0.90 0.00 0.00
PhenoMM 0.90 0.90 - -
PhenoMP 0.83 0.83 - -
XMap 0.17 0.52 0.00 0.00
Consensus vote = 1 0.90 0.90 0.05 0.20
Consensus vote = 2 0.31 0.79 0.00 0.00
Consensusvote = 3 0.24 0.66 0.00 0.00
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 10 of 13
Table 9 Manual assessment of unique mappings and estimated positive and negative contribution in the HP-MP task
System-mappings Unique mappings Precision Positive contrib. Negative contrib.
BioPortal (baseline) 0 -
AML 122 0.87 8.63% 1.33%
DiSMatch 291 0.83 19.80% 3.96%
FCA ? Map 26 0.96 2.04% 0.08%
LYAM + + 226 0.70 12.91% 5.53%
LogMap 130 0.93 9.90% 0.71%
LogMapBio 176 0.93 13.40% 0.96%
LogMapLt 0 - - -
PhenoMF 89 1.00 7.27% 0.00%
PhenoMM 85 1.00 6.94% 0.00%
PhenoMP 80 1.00 6.53% 0.00%
XMap 0 - - -
Total 1225 0.91 87.42% 12.58%
the interaction with a domain expert with variable error
rate [1].
In this section we briefly present the results with
the Disease and Phenotype datasets in the OAEI 2016
interactive track, which represents a side contribu-
tion of the work presented in this paper. For more
details and results, the interested reader please refer to
state-of-the-art papers on interactive ontology alignment
[1, 3739].
The consensus alignment with vote=3 was used as
oracle in the Disease and Phenotype interactive track.
Table 11 shows the obtained F-measure by AML and
LogMap when simulating an interaction with a perfect
user (i.e., always gives the correct answer when asked
about the validity of a mapping).21 Both systems increase
the F-measure with respect to the non-interactive results
(see Tables 6 and 7) with a gain between 0.03 and 0.11.
It is noticeable that the number of required requests by
LogMap is around 4-5 times larger than AML.
Discussion
The OAEI has been proven to be an effective campaign
to improve ontology matching systems. As a result, avail-
able techniques are more mature and robust. Neverthe-
less, despite the impressive state-of-the-art technology in
ontology alignment, new matching tasks like those pre-
sented in this paper are very important for the OAEI
campaign since they introduce new challenges to ontol-
ogy alignment systems. For example, our preliminary tests
with the Disease and Phenotype dataset revealed that only
the 2015 versions of AML and LogMap, among the sys-
tems participating in the OAEI 2015, were able to cope
with the track ontologies.
In the OAEI 2016 campaign there were 11 systems that
were able to produce results in at least one of the Disease
and Phenotype matching tasks. The four systems: AML,
FCA-Map, LogMap (and its Bio variant) and PhenoMF
produced alignments relatively close to the consensus
alignments for theDisease and Phenotype evaluation tasks
Table 10 Manual assessment of unique mappings and estimated positive and negative contribution in the DOID-ORDO task
System-mappings Unique mappings Precision Positive contrib. Negative contrib.
BioPortal (baseline) 5 0.40
AML 308 0.87 30.40% 4.68%
DiSMatch 259 0.40 11.80% 17.70%
FCA ? Map 61 0.83 5.79% 1.16%
LogMap 80 0.90 8.20% 0.91%
LogMapBio 144 0.97 15.85% 0.55%
LogMapLt 7 0.50 0.40% 0.40%
PhenoMF 3 1.00 0.34% 0.00%
XMap 16 0.56 1.03% 0.80%
Total 878 0.75 73.81% 26.19%
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 11 of 13
Table 11 Results in the OAEI interactive track
Task System F-measure Gain Requests
HP-MP AML 0.93 0.03 388
LogMap 0.97 0.11 1928
DOID-ORDO AML 0.96 0.09 413
LogMap 0.99 0.07 1602
as described in this paper. The results against curated
alignments proved to be more challenging since they go
beyond equivalent matches to include matches of seman-
tic similarity, especially subsumption relationships. This
finding suggests that while the systems performed well
enough for detection of equivalent mappings, in future it
would be good to improve their performance for detec-
tion of semantic similarity matches. For example, Phe-
nomeNET systems showed potential advantage though
exploiting a specialised background knowledge embed-
ded within the system. LYAM++ is also specialised in the
use of background knowledge, but it did not perform well
in the Disease and Phenotype track, unlike in the OAEI
Anatomy track, probably due to the lack of a suitable
source of background knowledge for this track.
The OAEI also includes two biomedical-themed tracks,
namely Anatomy and Largebio [1]. The complexity of the
matching tasks is similar to the Anatomy track in terms of
ontology size and expressiveness, while the Largebio tasks
represent a significant leap in complexity with respect
to the other OAEI test cases. The main differences with
respect to the evaluation in the Disease and Phenotype
track are the following: (i) we constructed two consen-
sus reference alignments, unlike the Anatomy track where
there exist a curated reference alignment [40] and the
Largebio track where the reference alignment has been
extracted from the UMLS Metathesaurus [8]; (ii) we per-
formed an evaluation with respect to manually created
mappings and a manual assessment of unique mappings
produced by participating systems; and (iii) we used
semantic precision and recall together with the standard
measures.
The findings of the Disease and Phenotype evaluation
show the potential of the top performing ontology match-
ing systems that could help to automate the workflow
of curators, who maintain ontology mapping services in
numerous domains such as the disease and phenotype
domain. Furthermore, the constructed consensus align-
ments substantially improve available mapping sets pro-
vided by BioPortal.
Conclusions
We have presented the methodology followed in the novel
Disease and Phenotype track and the results in the OAEI
2016. The top systems in the track coped well with the
detection of equivalence matches, but struggled to detect
subsumption matches. This deserves more attention in
the future development of ontology matching systems.
The Pistoia Alliance Ontologies Mapping project has
gained much value from participation in the 2016 OAEI
campaign through sponsorship and design of this new
track on Disease and Phenotype. We believe that there is
a real need for ontology matching algorithm developers
to collaborate with ontology curators to improve the scale
and quality of workflows necessary to build and maintain
ontology mapping resources.
We are in an exploding information age with increasing
amounts of human biology and genetics data in particular
from sequencing technology improvements, biobanks and
smart portable devices. This drives the need for stronger
ontological standards, tools and services for ontology
mapping to enable more efficient application of all this
information. We expect that the Disease and Phenotype
track will evolve in future campaigns as a strong use case
which is widely applicable in the life sciences and beyond.
Evolution of the track
The OAEI 2017 will include a new edition of the track,
which will be composed by the same tasks as in 2016
(with updated ontology versions) and two additional tasks
requiring the pairwise alignment of:
 HP and MESH (Medical Subject Headings)
ontologies; and
 HP and OMIM (Online Mendelian Inheritance in
Man) ontologies.
The alignment between HP andMESH is a new require-
ment of the Pistoia Alliance Ontologies Mapping project,
while the mapping between HP and OMIM is placed
within the scope of the Research Council of Norway
project BigMed to improve the suggested genes associ-
ated to a given phenotype in state of the art tools like
PhenoTips [41].
In the future editions of the Disease and Phenotype
track, apart from including new datasets and updated ver-
sions, we aim to enhance the evaluation in a number
of ways. We will consider new metrics like the map-
ping incoherence [12], the functional coherence [42] or
the redundancy (minimality) [43] to evaluate the com-
puted alignments. We also intend to redefine the notion of
semantic precision and recall, using the using the seman-
tic closure of the (aligned) ontologies, in order to include
the cases where the aligned ontology is incoherence (i.e.,
contains unsatisfiable classes).
We plan to increase the number of manually gener-
ated mappings considering additional areas relevant to
the phenotype and disease domain. In addition, we will
also work towards the semi-automatic creation of gold
Harrow et al. Journal of Biomedical Semantics  (2017) 8:55 Page 12 of 13
standard reference alignments for the tasks by combin-
ing the consensus alignments and the manually generated
mappings.
Endnotes
1 http://www.pistoiaalliance.org/projects/ontologies-
mapping
2 https://pistoiaalliance.atlassian.net/wiki/display/PUB/
Ontologies+Mapping+Resources
3The contents of this paper have been partially reported
in the OAEI 2016 annual report [1], published within
the informal proceedings of the Ontology Matching
workshop [44].
4 http://www.orphadata.org/cgi-bin/inc/ordo_
orphanet.inc.php
5 https://www.w3.org/TR/owl2-overview/
6Typically automatic, although there are systems that
also allow human interaction
7 http://oaei.ontologymatching.org/
8 http://oaei.ontologymatching.org/2016/phenotype/
9 http://data.bioontology.org/documentation#Mapping
10 https://www.bioontology.org/wiki/index.php/
BioPortal_Mappings
11We did not consider mappings labelled as
skos:exactMatch since they represent correspondences
between entities with the same URI, and thus these
mappings are redundant if translated into OWL 2 axioms.
12Our tests with last year participants revealed a large
amount of missing valid mappings. The Results section
quantifies this degree of incompleteness.
13 http://oaei.ontologymatching.org/2016/seals-eval.
html
14We may consider vote ? 4 in future editions of the
Disease and Phenotype track as the contributing partici-
pants increase.
15 There could still be some bias through systems
exploiting the same resource, e.g., UMLS.
16We rely on the OWL 2 reasoner HermiT [45].
17 http://bioportal.bioontology.org/ontologies/MESH
18 https://www.nlm.nih.gov/pubs/factsheets/umlslex.
html
19 http://aber-owl.net/ontology/PhenomeNET
20 http://oaei.ontologymatching.org/2016/interactive/
21 From the Disease and Phenotype track participating
systems only AML, LogMap and XMap implement an
interactive algorithm. We have discarded XMap from the
results since its number of oracle/user requests was very
low in the Disease and Phenotype track.
Abbreviations
DL: Description logics; DOID: Disease ontology; F: F-measure; HP: Human
phenotype ontology; M: Mappings; MP: Mammalian phenotype ontology; O:
Ontology; OA: Ontology alignment; OAEI: Ontology alignment evaluation
initiative; ORDO: Orphanet rare disease ontology; OWL: Web ontology
language; P: Precision; R: Recall; RA: Reference alignment; RDF: Resource
description framework
Acknowledgements
We would like to thank the organisers and participants of the OAEI campaign.
We also thank the anonymous reviewers for their comments and suggestions
to improve the paper.
Funding
This work was partially funded by the Pistoia Alliance Ontology Mappings
project, the BIGMED project (IKT 259055), the SIRIUS Centre for Scalable Data
Access (Research Council of Norway, project no.: 237889), the EU project
Optique (FP7-ICT-318338), and the EPSRC projects ED3 and DBOnto.
Availability of data andmaterials
OAEI 2016 datasets available from: http://oaei.ontologymatching.org/2016/
phenotype/.
OAEI 2017 datasets available from: http://oaei.ontologymatching.org/2017/
phenotype/.
Main entry point for the Disease and Phenotype track: http://sws.ifi.uio.no/oaei/
phenotype/.
Authors contributions
IH, EJR and AS organised and designed the experiments of the track. EJR
conducted the automatic evaluation. IH prepared the manually curated
mappings and performed the manual assessment which was checked by AS,
MR, PW, SM, YAF and JM. All authors contributed to the writing of the
manuscript. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Pistoia Alliance Ontologies Mapping Project, Pistoia Alliance Inc, USA.
2Department of Informatics, University of Oslo, Oslo, Norway. 3Novartis, Basel,
Switzerland. 4Roche Pharma Research and Early Development, pRED
Informatics, Roche Innovation Center, Basel, Switzerland. 5GlaxoSmithKline
R&D, Stevenage, UK. 6BIOVIA 3DS, San Diego, USA. 7Eagle Genomics,
Cambridge, UK. 8OSTHUS, Aachen, Germany. 9FactBio, Cambridge, UK.
Received: 13 April 2017 Accepted: 27 October 2017
RESEARCH Open Access
NCBO Ontology Recommender 2.0: an
enhanced approach for biomedical
ontology recommendation
Marcos Martínez-Romero1* , Clement Jonquet1,3, Martin J. OConnor1, John Graybeal1, Alejandro Pazos2
and Mark A. Musen1
Abstract
Background: Ontologies and controlled terminologies have become increasingly important in biomedical research.
Researchers use ontologies to annotate their data with ontology terms, enabling better data integration and
interoperability across disparate datasets. However, the number, variety and complexity of current biomedical
ontologies make it cumbersome for researchers to determine which ones to reuse for their specific needs. To
overcome this problem, in 2010 the National Center for Biomedical Ontology (NCBO) released the Ontology
Recommender, which is a service that receives a biomedical text corpus or a list of keywords and suggests
ontologies appropriate for referencing the indicated terms.
Methods: We developed a new version of the NCBO Ontology Recommender. Called Ontology Recommender 2.0,
it uses a novel recommendation approach that evaluates the relevance of an ontology to biomedical text data
according to four different criteria: (1) the extent to which the ontology covers the input data; (2) the acceptance
of the ontology in the biomedical community; (3) the level of detail of the ontology classes that cover the input
data; and (4) the specialization of the ontology to the domain of the input data.
Results: Our evaluation shows that the enhanced recommender provides higher quality suggestions than the
original approach, providing better coverage of the input data, more detailed information about their concepts,
increased specialization for the domain of the input data, and greater acceptance and use in the community.
In addition, it provides users with more explanatory information, along with suggestions of not only individual
ontologies but also groups of ontologies to use together. It also can be customized to fit the needs of different
ontology recommendation scenarios.
Conclusions: Ontology Recommender 2.0 suggests relevant ontologies for annotating biomedical text data. It
combines the strengths of its predecessor with a range of adjustments and new features that improve its reliability
and usefulness. Ontology Recommender 2.0 recommends over 500 biomedical ontologies from the NCBO BioPortal
platform, where it is openly available (both via the user interface at http://bioportal.bioontology.org/recommender,
and via a Web service API).
Keywords: Ontology selection, Ontology recommendation, Ontology evaluation, Semantic Web, Biomedical
ontologies, NCBO BioPortal
* Correspondence: marcosmr@stanford.edu
1Stanford Center for Biomedical Informatics Research, 1265 Welch Road,
Stanford University School of Medicine, Stanford, CA 94305-5479, USA
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 
DOI 10.1186/s13326-017-0128-y
Background
During the last two decades, the biomedical community
has grown progressively more interested in ontologies.
Ontologies provide the common terminology necessary
for biomedical researchers to describe their datasets, en-
abling better data integration and interoperability, and
therefore facilitating translational discoveries [1, 2].
BioPortal [3, 4], developed by the National Center for
Biomedical Ontology (NCBO) [5], is a highly used plat-
form1 for hosting and sharing biomedical ontologies. Bio-
Portal users can publish their ontologies as well as submit
new versions. They can browse, search, review, and com-
ment on ontologies, both interactively through a Web
interface, and programmatically via Web services. In 2008,
BioPortal2 contained 72 ontologies and 300,000 ontology
classes. As of 2017, the number of ontologies exceeds 500,
with more than 7.8 million classes, making it one of the
largest public repositories of biomedical ontologies.
The great number, complexity, and variety of ontologies
in the biomedical field present a challenge for researchers:
how to identify those ontologies that are most relevant for
annotating, mining or indexing particular datasets. To
address this problem, in 2010 the NCBO released the
first version of its Ontology Recommender (henceforth
Ontology Recommender 1.0 or original Ontology Rec-
ommender) [6], which informed the user of the most ap-
propriate ontologies in BioPortal to annotate textual data.
It was, to the best of our knowledge, the first biomedical
ontology recommendation service, and it became widely
known and used by the community.3 However, the service
has some limitations, and a significant amount of work has
been done in the field of ontology recommendation since
its release. This motivated us to analyze its weaknesses and
to design a new recommendation approach.
The main contributions of this paper are the following:
1. A state-of-the-art approach for recommending
biomedical ontologies. Our approach is based on
evaluating the relevance of an ontology to biomedical
text data according to four different criteria, namely:
ontology coverage, ontology acceptance, ontology
detail, and ontology specialization.
2. A new ontology recommendation system, the NCBO
Ontology Recommender 2.0 (henceforth Ontology
Recommender 2.0 or new Ontology
Recommender). This system has been implemented
based on our approach, and it is openly available at
BioPortal.
Our research is particularly relevant both for re-
searchers and developers who need to identify the most
appropriate ontologies for annotating textual data of
biomedical nature (e.g., journal articles, clinical trial de-
scriptions, metadata about microarray experiments,
information on small molecules, electronic health re-
cords, etc.). Our ontology recommendation approach
can be easily adapted to other domains, as it will be il-
lustrated in the Discussion section. Overall, this work
advances prior research in the fields of ontology evalu-
ation and recommendation, and provides the commu-
nity with a useful service which is, to the best of our
knowledge, the only ontology recommendation system
currently available to the public.
Related work
Much theoretical work has been done over the past
two decades in the fields of ontology evaluation, selec-
tion, search, and recommendation. Ontology evaluation
has been defined as the problem of assessing a given
ontology from the point of view of a particular criter-
ion, typically in order to determine which of several on-
tologies would best suit a particular purpose [7]. As a
consequence, ontology recommendation is fundamen-
tally an ontology evaluation task because it addresses
the problem of evaluating and consequently selecting
the most appropriate ontologies for a specific context
or goal [8, 9].
Early contributions in the field of ontology evaluation
date back to the early 1990s and were motivated by the
necessity of having evaluation strategies to guide and
improve the ontology engineering process [1012].
Some years later, with the birth of the Semantic Web
[13], the need for reusing ontologies across the Web
motivated the development of the first ontology search
engines [1416], which made it possible to retrieve all
ontologies satisfying some basic requirements. These
engines usually returned only the ontologies that had
the query term itself in their class or property names
[17]. However, the process of recommending ontologies
involves more than that. It is a complex process that
comprises evaluating all candidate ontologies according
to a variety of criteria, such as coverage, richness of the
ontology structure [1820], correctness, frequency of
use [21], connectivity [18], formality, user ratings [22],
and their suitability for the task at hand.
In biomedicine, the great number, size, and complexity
of ontologies have motivated strategies to help re-
searchers find the best ontologies to describe their
datasets. Tan and Lambrix [23] proposed a theoretical
framework for selecting the best ontology for a particu-
lar text-mining application and manually applied it to a
gene-normalization task. Alani et al. [17] developed an
ontology-search strategy that uses query-expansion
techniques to find ontologies related to a particular do-
main (e.g., Anatomy). Maiga and Williams [24] con-
ceived a semi-automatic tool that makes it possible to
find the ontologies that best match a list of user-
defined task requirements.
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 2 of 22
The most relevant alternative to the NCBO Ontology
Recommender is BiOSS [21, 25], which was released in
2011 by some of the authors of this paper. BiOSS evalu-
ates each candidate ontology according to three criteria:
(1) the input coverage; (2) the semantic richness of the
ontology for the input; and (3) the acceptance of the
ontology. However, this system has some weaknesses that
make it insufficient to satisfy many ontology reuse needs
in biomedicine. BiOSS ontology repository is not updated
regularly, so it does not take into account the most recent
revisions to biomedical ontologies. Also, BiOSS evaluates
ontology acceptance by counting the number of mentions
of the ontology name in Web 2.0 resources, such as Twit-
ter and Wikipedia. However, this method is not always ap-
propriate because a large number of mentions do not
always correspond to a high level of acceptance by the
community (e.g., an ontology may be popular on Twitter
because of a high number of negative comments about it).
Another drawback is that the input to BiOSS is limited to
comma-delimited keywords; it is not possible to suggest
ontologies to annotate raw text, which is a very common
use case in biomedical informatics.
In this work, we have applied our previous experience
in the development of the original Ontology Recom-
mender and the BiOSS system to conceive a new ap-
proach for biomedical ontology recommendation. The
new approach has been used to design and implement the
Ontology Recommender 2.0. The new system combines
the strengths of previous methods with a range of en-
hancements, including new recommendation strategies
and the ability to handle new use cases. Because it is inte-
grated within the NCBO BioPortal, this system works with
a large corpus of current biomedical ontologies and can
therefore be considered the most comprehensive biomed-
ical ontology recommendation system developed to date.
Our recommendations for the choice of appropriate
ontologies center around the use of ontologies to per-
form annotation of textual data. We define annotation
as a correspondence or relationship between a term and
an ontology class that specifies the semantics of that
term. For instance, an annotation might relate leucocyte
in some text to a particular ontology class leucocyte in
the Cell Ontology. The annotation process will also re-
late textual data such as white blood cell and lymphocyte
to the class leucocyte in the Cell Ontology, via synonym
and subsumption relationships, respectively.
Description of the original approach
The original NCBO Ontology Recommender supported
two primary use cases: (1) corpus-based recommenda-
tion, and (2) keyword-based recommendation. In these
scenarios, the system recommended appropriate ontol-
ogies from the BioPortal ontology repository to annotate
a text corpus or a list of keywords, respectively.
The NCBO Ontology Recommender invoked the
NCBO Annotator [26] to identify all annotations for the
input data. The NCBO Annotator is a BioPortal service
that annotates textual data with ontology classes. Then,
the Ontology Recommender scored all BioPortal ontol-
ogies as a function of the number and relevance of the
annotations found, and ranked the ontologies according
to those scores. The first ontology in the ranking would
be suggested as the most appropriate for the input data.
The score for each ontology was calculated according to
the following formula4:
scoreðo; tÞ ¼
X
ðannotationScoreðaÞ þ 2  hierarchyLevelðaÞÞ
log10ðjojÞ
?a ? annotationsðo; tÞ
such that:
score o; tð Þ?? : score o; tð Þ?0
annotationScoreðaÞ ¼ 10 if annotationType ¼ PREF
8 if annotationType ¼ SYN
(
hierarchyLevelðaÞ ?Z : hierarchyLevelðaÞ ? 0
Here o is the ontology that is being evaluated; t is the
input text; score(o, t) represents the relevance of the
ontology o for t; annotationScore(a) is the score for the
annotation a; hierarchyLevel(a) is the position of the
matched class in the ontology tree, such that 0 represents
the root level; |o| is the number of classes in o; and anno-
tations(o,t) is the list of annotations (a) performed with o
for t, returned by the NCBO Annotator.
The annotationScore(a) would depend on whether the
annotation was achieved with a class preferred name
(PREF) or with a class synonym (SYN). A preferred
name is the human readable label that the authors of the
ontology suggested to be used when referring to the
class (e.g., vertebral column), whereas synonyms are alter-
nate names for the class (e.g., spinal column, backbone,
spine). Each class in BioPortal has a single preferred name
and it may have any number of synonyms. Because syno-
nyms can be imprecise, this approach favored matches on
preferred names.
The normalization by ontology size was intended to
discriminate between large ontologies that offer good
coverage of the input data, and small ontologies with
both correct coverage and better specialization for the
input datas domain. The granularities of the matched
classes (i.e., hierarchyLevel(a)) were also considered, so
that annotations performed with granular classes (e.g.,
epithelial cell proliferation) would receive higher scores
than those performed with more abstract classes (e.g.,
biological process).
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 3 of 22
For example, Table 1 shows the top five suggestions of
the original Ontology Recommender for the text Melan-
oma is a malignant tumor of melanocytes which are
found predominantly in skin but also in the bowel and
the eye. In this example, the system considered that the
best ontology for the input data is the National Cancer
Institute Thesaurus (NCIT).
In the following sections, we summarize the most rele-
vant shortcomings of the original approach, addressing
input coverage, coverage of multi-word terms, input
types and output information.
Input coverage
Input coverage refers to the fraction of input data that is
annotated with ontology classes. Given that the goal is
to find the best ontologies to annotate the users data,
high input coverage is the main requirement for ontology-
recommendation systems. One of the shortcomings of the
original approach is that it did not ensure that ontologies
that provide high input coverage were ranked higher
than ontologies with lower coverage. The approach was
strongly based on the total number of annotations
returned by the NCBO Annotator. However, a large num-
ber of annotations does not always imply high coverage.
Ontologies with low input coverage can contain a great
many classes that match only a few input terms, or match
many repeated terms in a large text corpus.
In the previous example (see Table 1), EHDA (Human
Developmental Anatomy Ontology) was ranked at the
second position. However, it covers only two input
terms: skin and eye. Clearly, it is not an appropriate
ontology to annotate the input when compared with
LOINC or EFO, which have almost three times more
terms covered. The reason that EHDA was assigned a
high score is that it contains 11 different eye classes (e.g.,
EHDA:4732, EHDA:3808, EHDA:5701) and 4 different
skin classes (e.g., EHDA:6531, EHDA:6530, EHDA:7501),
which provide a total of 15 annotations. Since the recom-
mendation score computed using the original approach is
directly influenced by the number of annotations, EHDA
obtains a high relevance score and thus the second pos-
ition in the ranking. This issue was also identified by
López-García et al. in their study of the efficiency of
automatic summarization techniques [27]. These au-
thors noticed that EHDA was the most recommended
ontology for a broad range of topics that the ontology
actually did not cover well.
Multi-word terms
Biomedical texts frequently contain terms composed of
several words, such as distinctive arrangement of micro-
tubules, or dental disclosing preparation. Annotating a
multi-word phrase or multi-word keyword with an onto-
logical class that completely represents its semantics is a
much better choice than annotating each word separ-
ately. The original recommendation approach was not
designed to select the longest matches and consequently
the results were affected.
As an example, Table 2 shows the top 5 ontologies
suggested by the original Ontology Recommender for
the phrase embryonic cardiac structure. Ideally, the first
ontology in the ranking (SWEET) would contain the
class embryonic cardiac structure. However, the SWEET
ontology covers only the term structure. This ontology
was ranked at the first position because it contains 3
classes matching the term structure and also because it
is a small ontology (4549 classes).
Furthermore, SNOMEDCT, which does contain a class
that provides a precise representation of the input, was
ranked in the 5th position. There are 3 other ontologies in
BioPortal that contain the class embryonic cardiac struc-
ture: EP, BIOMODELS and FMA. However, they were
ranked 8, 11 and 32, respectively. The recommendation
algorithm should assign a higher score to an annotation
that covers all words in a multi-word term than it does to
different annotations that cover all words separately.
Input types
Related work in ontology recommendation highlights the
importance of addressing two different input types: text
corpora and lists of keywords [28]. The original Ontology
Recommender, while offering users the possibility of
selecting among these two recommendation scenarios,
would treat the input data in the same manner. To satisfy
Table 1 Ontologies suggested by the original Ontology Recommender for the sample input text Melanoma is a malignant tumor of
melanocytes which are found predominantly in skin but also in the bowel and the eye
Rank Ontology No. annotations Terms annotated Score
1 NCIT 21 melanoma, malignant tumor, melanocytes, found, skin, bowel, eye 55.2
2 EHDA 15 skin, eye 38.3
3 EFO 10 melanoma, malignant tumor, skin, bowel, eye 35.9
4 LOINC 18 melanoma, malignant, tumor, skin, bowel, eye 35.9
5 MP 9 melanoma, skin, bowel, eye 34.8
*See "List of abbreviations"
For each ontology, the table shows its position in the ranking, the acronym of the ontology in BioPortal*, the number of annotations returned by the NCBO
Annotator for the sample input, the terms annotated (or covered) by those annotations and the ontology score
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 4 of 22
users expectations, the system should process these two
input types differently, to better reflect the information
coded in the input about multi-word boundaries.
Output information
The output provided by the original Ontology Recom-
mender consisted of a list of ontologies ranked by rele-
vance score. For each ontology, the Web-based user
interface displayed the number of classes matched and
the size of each recommended ontology. In contrast, the
Web service could additionally return the particular
classes matched in each ontology. This information
proved insufficient to assure users that a recommended
ontology was appropriate and better than the alternatives.
For example, it was not possible to know what specific in-
put terms were covered by each class. The system should
provide enough detail both to reassure users, and to give
them information about alternative ontologies.
In this section we have described the fundamental
limitations of the original Ontology Recommender and
suggested methods to address them. The strategy for
evaluating input coverage must be improved. Addition-
ally, there is a diversity of other recently-proposed evalu-
ation techniques [8, 19, 25] that could enhance the
original approach. Particularly, there are two evaluation
criteria that could substantially improve the output pro-
vided by the system: (1) ontology acceptance, which rep-
resents the degree of acceptance of the ontology by the
community; and (2) ontology detail, which refers to the
level of detail of the classes that cover the input data.
Description of the new approach
In this section, we present our new approach to biomed-
ical ontology recommendation. First, we describe our
ontology evaluation criteria and explain how the recom-
mendation process works. We then provide some imple-
mentation details and discuss improvements to the user
interface.
The execution starts from the input data and a set of
configuration settings. The NCBO Annotator [26] is
then used to obtain all annotations for the input using
BioPortal ontologies. Those ontologies that do not pro-
vide annotations for the input data are considered
irrelevant and are ignored in further processing. The
ontologies that provide annotations are evaluated one
by one according to four evaluation criteria that ad-
dress the following questions:
1. Coverage: To what extent does the ontology
represent the input data?
2. Acceptance: How well-known and trusted is the
ontology by the biomedical community?
3. Detail: How rich is the ontology representation for
the input data?
4. Specialization: How specialized is the ontology to
the domain of the input data?
According to our analysis of related work, these are
the most relevant criteria for ontology recommenda-
tion. Note that other authors have referred to the
coverage criterion as term matching [6], class match
measure [19] and topic coverage [28]. Acceptance is re-
lated to popularity [21, 25, 28], because it measures
the level of support provided to the ontology by the
people in the community. Other criteria to measure
ontology acceptance are connectivity [6], and connect-
edness [18], which assess the relevance of an ontology
based on the number and quality of connections to an
ontology by other ontologies. Detail is similar to struc-
ture measure [6], semantic richness [21, 25], structure
[18], and granularity [24].
For each of these evaluation criteria, a score in the
interval [0,1] is typically obtained. Then, all the scores
for a given ontology are aggregated into a composite
relevance score, also in the interval [0,1]. This score
represents the appropriateness of that ontology to de-
scribe the input data. The individual scores are com-
bined in accordance with the following expression:
scoreðo; tÞ ¼ wc  coverageðo; tÞ þ wa  acceptanceðoÞ
þwd  detailðo; tÞ þ ws  specializationðo; tÞ
where o is the ontology that is being evaluated, t rep-
resents the input data, and {wc, wa, wd, ws} are a set
of predefined weights that are used to give more or
less importance to each evaluation criterion, such that
wc + wa + wd + ws = 1. Note that acceptance is the only
criterion independent from the input data. Ultimately,
the system returns a list of ontologies ranked accord-
ing to their relevance scores.
Ontology evaluation criteria
The relevance score of each candidate ontology is
calculated based on coverage, acceptance, detail, and
Table 2 Top 5 ontologies suggested by Ontology Recommender
1.0 for the sample input text embryonic cardiac structure
Rank Ontology No. annotations Terms covered Score
1 SWEET 3 structure 13.7
2 NCIT 4 embryonic, cardiac, structure 10.5
3 HUPSON 2 cardiac, structure 10.1
4 VSO 1 structure 9.8
5 SNOMEDCT 2 embryonic cardiac structure 8.9
For each ontology, the table shows its position in the ranking, the acronym of
the ontology in BioPortal, the number of annotations returned by the NCBO
Annotator for the sample input, the terms annotated (or covered) by
those annotations and the ontology score
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 5 of 22
specialization. We now describe these criteria in
more detail.
Ontology coverage
It is crucial that ontology recommendation systems sug-
gest ontologies that provide high coverage of the input
data. As with the original approach, the new recommen-
dation process is driven by the annotations provided by
the NCBO Annotator, but the method used to evaluate
the candidate ontologies is different. In the new algo-
rithm, each annotation is assigned a score computed in
accordance with the following expression5:
annotationScore2 að Þ ¼ annotationTypeScore að Þð
þmultiWordScore að ÞÞ  annotatedWords að Þ
with:
annotationTypeScore að Þ ¼ 10 if annotationType ¼ PREF
5 if annotationType ¼ SYN

multiWordScore að Þ ¼ 3 if annotatedWords að Þ > 1
0otherwise

In this expression, annotationTypeScore(a) is a score
based on the annotation type which, as with the original
approach, can be either PREF, if the annotation has
been performed with a class preferred name, or SYN, if
it has been performed with a class synonym. Our
method assigns higher relevance to scores done with
class preferred names than to those made with class
synonyms because we have seen that many BioPortal
ontologies contain synonyms that are not reliable (e.g.,
Other variants as a synonym of Other Variants of Basa-
loid Follicular Neoplasm of the Mouse Skin in the NCI
Thesaurus).
The multiWordScore(a) score rewards multi-word an-
notations. It gives more importance to classes that anno-
tate multi-word terms than to classes that annotate
individual words separately (e.g., blood cell versus blood
and cell). Such classes better reflect the input data than
do classes that represent isolated words.
The annotatedWords(a) function represents the num-
ber of words matched by the annotation (e.g., 2 for the
term blood cell).
Sometimes, an ontology provides overlapping annota-
tions for the same input data. For instance, the text
white blood cell may be covered by two different classes,
white blood cell and blood cell. In the original approach,
ontologies with low input coverage were sometimes
ranked among the top positions because they had mul-
tiple classes matching a few input terms, and all those
annotations contributed to the final score. Our new
approach addresses this issue. If an ontology provides
several annotations for the same text fragment, only the
annotation with the highest score is selected to contrib-
ute to the coverage score.
The coverage score for each ontology is computed as
the sum of all the annotation scores, as follows:
coverage o; tð Þ ¼ norm
X
annotationScore2 að Þ
 
?a?selectedAnnotations Að Þ
where A is the set of annotations performed with the
ontology o for the input t, selectedAnnotations(A) is the
set of annotations that are left after discarding overlap-
ping annotations, and norm is a function that normalizes
the coverage score to the interval [0,1].
As an example, Table 3 shows the annotations per-
formed with SNOMEDCT for the input A thrombocyte
is a kind of blood cell. This example shows how our ap-
proach prioritizes (i.e., assigns a higher score to) annota-
tions performed with preferred names over synonyms
(e.g., cell over entire cell), and annotations performed
with multi-word terms over single-word terms (e.g.,
blood cell over blood plus cell). The coverage score for
SNOMEDCT would be calculated as 5 + 26 = 31, which
would be normalized to the interval [0,1] by dividing it
by the maximum coverage score. The maximum cover-
age score is obtained by adding the scores of all the an-
notations performed with all BioPortal ontologies, after
discarding overlapping annotations.
It is important to note that this evaluation of ontology
coverage takes into account term frequency. That is,
matched terms with several occurrences are considered
more relevant to the input data than terms that occur
less frequently. If an ontology covers a term that appears
several times in the input, its corresponding annotation
score will be counted each time and the coverage score
for the ontology accordingly will be higher. In addition,
because we select only the matches with the highest
score, the frequencies are not distorted by terms embed-
ded in one another (e.g., white blood cell and blood cell).
Our approach accepts two input types: free text and
comma-delimited keywords. For the keyword input type,
only those annotations that cover all the words in a
Table 3 SNOMEDCT annotations for the input A thrombocyte is
a kind of blood cell
Text Matched class (type) Annotation score Selected
thrombocyte platelet (SYN) 5 Yes
blood cell blood cell (PREF) (10 + 3)*2 = 26 Yes
blood blood (PREF) 10 No
cell cell structure (SYN) 5 No
cell cell (PREF) 10 No
cell entire cell (SYN) 5 No
The table shows the text fragment covered by each annotation, the name and
type of the matched class, the annotation score, and the annotations selected
to compute the relevance score for SNOMEDCT
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 6 of 22
multi-word term are considered. Partial annotations are
immediately discarded.
Ontology acceptance
In biomedicine, some ontologies have been developed
and maintained by widely known institutions or research
projects. The content of these ontologies is periodically
curated, extensively used, and accepted by the commu-
nity. Examples of broadly accepted ontologies are SNO-
MEDCT [29] and Gene Ontology [30]. Some ontologies
uploaded to BioPortal may be relatively less reliable,
however. They may contain incorrect or poor quality
content or simply be insufficiently up to date. It is import-
ant that an ontology recommender be able to distinguish
between ontologies that are accepted as trustworthy and
those that are less so.
Our approach proposes to estimate the degree of
acceptance of each ontology based of information
extracted from ontology repositories or terminology
systems. Widely used examples of these systems in bio-
medicine include BioPortal, the Unified Medical Language
System (UMLS) [31], the OBO Foundry [32], Ontobee
[33], the Ontology Lookup Service (OLS) [34], and Aber-
OWL [35]. The calculation of ontology acceptance is
based on two factors: (1) The presence or absence of the
ontology in ontology repositories; and (2) the number of
visits (pageviews) to the ontology in ontology repositories
in a recent period of time (e.g., the last 6 months). This
method takes into account changes in ontology accept-
ance over time. The acceptance score for each ontology is
calculated as follows:
acceptance oð Þ ¼ wpresence  presenceScore oð Þ
þwvisits  visitsScore oð Þ
where:
 presenceScore(o) is a value in the interval [0,1] that
represents the presence of the ontology in a
predefined list of ontology repositories. It is
calculated as follows:
presenceScoreðoÞ ¼
Xn
i¼1
wpi  presenceiðoÞ
where wpi represents the weight assigned to the presence
of the ontology in the repository i, with
Xn
i?1
wpi ¼ 1, and:
presencei oð Þ ¼ 1 if o is present in repository i0 otherwise

 visitsScore(o) represents the number of visits to the
ontology on a given list of ontology repositories in
a recent period of time. Note that this score can
typically be calculated only for those repositories
that are available on the Web and that have an
independent page for each provided ontology. This
score is calculated as follows:
visitsScoreðoÞ ¼
Xn
i¼1
wvi  visitsiðoÞ
where wvi is the weight assigned to the ontology visits on
the repository i, with
Xn
i?1
wvi ¼ 1 ; visitsi(o) represents
the number of visits to the ontology in the repository i,
normalized to the interval [0,1].
 wpresence and wvisits are weights that are used to
give more or less importance each factor, with
wpresence + wvisits = 1.
Figure 1 shows the top 20 accepted BioPortal ontol-
ogies according to our approach at the time of writing
this paper. Estimating the acceptance of an ontology by
the community is inherently subjective, but the above
ranking shows that our approach provides reasonable
results. All ontologies in the ranking are widely known
and accepted biomedical ontologies that are used in a
variety of projects and applications.
Ontology detail
Ontologies containing a richer representation for a specific
input are potentially more useful to describe the input than
less detailed ontologies. As an example, the class melanoma
in the Human Disease Ontology contains a definition, two
synonyms, and twelve properties. However, the class mel-
anoma from the GALEN ontology does not contain any
definition, synonyms, or properties. If a user needs an
ontology to represent that concept, the Human Disease
Ontology would probably be more useful than the GALEN
ontology because of this additional information. An ontol-
ogy recommender should be able to analyze the level of de-
tail of the classes that cover the input data and to give
more or less weight to the ontology according to the degree
to which its classes have been specified.
We evaluate the richness of the ontology representa-
tion for the input data based on a simplification of the
semantic richness metric used by BiOSS [25]. For each
annotation selected during the coverage evaluation step,
we calculate the detail score as follows:
detailScore að Þ ¼ definitionScore að Þ þ synonymsScore að Þ þ propertiesScore að Þ
3
where detailScore(a) is a value in the interval [0,1] that
represents the level of detail provided by the annotation a.
This score is based on three functions that evaluate the
detail of the knowledge representation according to the
number of definitions, synonyms, and other properties of
the matched class:
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 7 of 22
definitionScoreðaÞ ¼ 1 if jDj?kdjDj=kd otherwise
(
synonymsScoreðaÞ ¼ 1 if jSj?ksjSj=ks otherwise
(
propertiesScoreðaÞ ¼ 1 if jPj?kpjPj=kp otherwise
(
where |D|, |S| and |P| are the number of definitions, syno-
nyms, and other properties of the matched class, and kd,
ks and kp are predefined constants that represent the
number of definitions, synonyms, and other properties, re-
spectively, necessary to get the maximum detail score. For
example, using ks = 4 means that, if the class has 4 or
more synonyms, then it will be assigned the maximum
synonyms score, which would be 1. If it has fewer than 4
synonyms, for example 3, the synonyms score will be
computed proportionally according to the expression
above (i.e., 3/4). Finally, the detail for the ontology would
be calculated as the sum of the detail scores of the annota-
tions done with the ontology, normalized to [0,1]:
detail o; tð Þ ¼
X
detailScore að Þ
Aj j ?a?selectedAnnotations Að Þ
Example: Suppose that, for the input t = Penicillin is an
antibiotic used to treat tonsillitis, there are two ontologies
O1 and O2 with the classes shown in Table 4.
Assuming that kd = 1, ks = 4 and kp = 10, the detail
score for O1 and O2 would be calculated as follows:
detail O1;tð Þ ¼
2

1þ 2 4= þ 7 10=

3
þ
3

1þ 1þ 1

¼ 0:87
detail O2;tð Þ ¼
2

0þ 1 4= þ 3 10=


3
þ
3

0þ 0þ 2 10=

¼ 0:13
Given that O1 annotates the input with two classes
that provide more detailed information than the classes
from O2, the detail score for O1 is higher.
Ontology specialization
Some biomedical ontologies aim to represent detailed
information about specific subdomains or particular
tasks. Examples include the Ontology for Biomedical
Fig. 1 Top 20 BioPortal ontologies according to their acceptance scores. The x-axis shows the acceptance score in the interval [0, 100]. The y-axis
shows the ontology acronyms. These acceptance scores were obtained by using UMLS to calculate the presenceScore(o), BioPortal to compute
the visitsScore(o), and assigning the same weight to pageviewsScore(o) and reposScore(o) (wpv = 0.5, wrepos = 0.5)
Table 4 Example of ontology classes for the input Penicillin
is an antibiotic used to treat tonsillitis
Ontology Class No. definitions No. synonyms No. properties
O1 penicillin 1 2 7
antibiotic 1 7 16
O2 penicillin 0 1 3
tonsillitis 0 0 2
The table shows the ontology name, the class name, and the number of class
definitions, synonyms and properties
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 8 of 22
Investigations [36], the Human Disease Ontology [37]
and the Biomedical Resource Ontology [38]. These on-
tologies are usually much smaller than more general
ones, with only several hundred or a few thousand
classes, but they provide comprehensive knowledge for
their fields.
To evaluate ontology specialization, an ontology rec-
ommender needs to quantify the extent to which a can-
didate ontology fits the specialized nature of the input
data. To do that, we reused the evaluation approach ap-
plied by the original Ontology Recommender, and
adapted it to the new annotation scoring strategy. The
specialization score for each candidate ontology is calcu-
lated according to the following expression:
specialization o; tð Þ ¼ normX
annotationScore2 að Þ þ 2  hierarchyLevel að Þð Þ
log10 oj jð Þ
 !
?a?A
where o is the ontology being evaluated, t is the input
text, annotationScore2(a) is the function that calculates
the relevance score of an annotation (see Section Ontology
coverage), hierarchyLevel(a) returns the level of the
matched class in the ontology hierarchy, and A is the set
of all the annotations done with the ontology o for the
input t. Unlike the coverage and detail criteria, which con-
sider only selectedAnnotations(A), the specialization criter-
ion takes into account all the annotations returned by the
Annotator (i.e., A). This is generally appropriate because
an ontology that provides multiple annotations for a spe-
cific text fragment is likely to be more specialized for that
text than an ontology that provides only one annotation
for it. The normalization by ontology size aims to assign a
higher score to smaller, more specialized ontologies. Ap-
plying a logarithmic function decreases the impact of on-
tologies with a very large size. Finally, the norm function
normalizes the score to the interval [0,1].
Using the same hypothetical ontologies, input, and
annotations from the previous example, and taking into
account the size and annotation details shown in Table 5,
the specialization score for O1 and O2 would be
calculated as follows:
specializationðO1; tÞ ¼ norm ð10þ 2  5Þ þ ð5þ 2  3Þ
log10ð120000Þ
 
¼ norm 31
5:08
 
¼ normð6:10Þ
specializationðO2; tÞ ¼ norm ð5þ 2  6Þ þ ð10þ 2  12Þ
log10ð800Þ
 
¼ norm 51
2:90
 
¼ normð17:59Þ
It is possible to see that the classes from O2 are lo-
cated deeper in the hierarchy than are those from O1.
Also, O2 is a much smaller ontology than O1. As a
consequence, according to our ontology-specialization
method, O2 would be considered more specialized for
the input than O1, and would be assigned a higher
specialization score.
Evaluation of ontology sets
When annotating a biomedical text corpus or a list of
biomedical keywords, it is often difficult to identify a
single ontology that covers all terms. In practice, it is
more likely that several ontologies will jointly cover the
input [8]. Suppose that a researcher needs to find the
best ontologies for a list of biomedical terms. If there is
not a single ontology that provides an acceptable cover-
age it should then evaluate different combinations of on-
tologies and return a ranked list of ontology sets that,
together, provide higher coverage. For instance, in our
previous example (Penicillin is an antibiotic used to treat
tonsillitis), O1 covers the terms penicillin and antibiotic
and O2 covers penicillin and tonsillitis. None of those
ontologies provides full coverage of all the relevant input
terms. However, by using O1 and O2 together, it is pos-
sible to cover penicillin, antibiotic, and tonsillitis.
Our method to evaluate ontology sets is based on the
ontology combinations approach used by the BiOSS
system [21]. The system generates all possible sets of 2
and 3 candidate ontologies (3 being the default maximum,
though users may modify this limit according to their spe-
cific needs) and it evaluates them using the criteria pre-
sented previously. To improve performance, we use some
heuristic optimizations to discard certain ontology sets
without performing the full evaluation process for them.
Table 5 Ontology size and annotation details for the ontologies in Table 4
Ontology Size Class Annotation type Annotation score Hierarchy level
O1 120,000 penicillin PREF 10 5
antibiotic SYN 5 3
O2 800 penicillin SYN 5 6
tonsillitis PREF 10 12
This table shows the number of classes (size) of each ontology, the class names, the annotation types, the annotation scores, and the level of each class in the
ontology hierarchy, such that 1 corresponds to the root (or top) level, 2 correspond to the level below the root classes, 3 to the next level, and so on
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 9 of 22
For example, a set containing two ontologies that cover
exactly the same terms will be immediately discarded
because that sets coverage will not be higher than that
provided by each ontology individually.
The relevance score for each set of ontologies is calcu-
lated using the same approach as for single ontologies,
in accordance with the following expression:
scoreSetðO; tÞ ¼ wc  coverageSetðO; tÞ þ wa
 acceptanceSetðOÞ þ wd
 detailSetðO; tÞ þ ws
 specializationSetðO; tÞ
where O = {o | o is an ontology} and |O| > 1. The
scores for the different evaluation criteria are calcu-
lated as follows:
 coverageSet: It is computed the same way as for a
single ontology, but takes into account all the
annotations performed with all the ontologies in the
ontology set. The system selects the best
annotations, and the sets input coverage is
computed based on them.
 acceptanceSet, detailSet, and specializationSet:
For each ontology, the system calculates its coverage
contribution (as a percentage) to the sets coverage
score. The recommender then uses this contribution
to calculate all the other scores proportionally. By
using this method, the impact (in terms of
acceptance, detail and specialization) of a particular
ontology on the set score will vary according to the
coverage provided by such ontology.
Implementation details
Ontology Recommender 2.0 implements the ontology
recommendation approach previously described in this
paper. Figure 2 shows the architecture of Ontology
Recommender 2.0. Like its predecessor, it has two in-
terfaces: a Web service API,6 which makes it possible
to invoke the recommender programmatically, and a
Web-based user interface, which is included in the
NCBO BioPortal.7
The Web-based user interface was developed using
the Ruby-on-Rails Web framework and the Javascript
language. Server side components were implemented
using the Ruby language. These components interact with
other BioPortal services to retrieve all the information
needed to achieve the recommendation process.
The typical workflow is as follows. First, the Ontology
Recommender calls the Annotator service to obtain all
the annotations performed for the input data using all
BioPortal ontologies. Second, for each ontology, it in-
vokes other BioPortal services to obtain the number of
classes in the ontology, the number of visits to each
ontology in a recent period of time, and to check the
presence of the ontology in UMLS. Third, for each an-
notation performed with the ontology, it makes several
calls to retrieve the number of definitions, synonyms
and properties of the ontology class involved in the an-
notation. The system has four independent evaluation
modules that use all this information to assess each
candidate ontology according to the four evaluation cri-
teria proposed in our approach: coverage, acceptance,
detail, and specialization. Because of the system's
modular design, new ontology evaluation modules can
be easily plugged in.
NCBO provides a Virtual Appliance for communities
that want to use the Ontology Recommender locally.
This appliance is a pre-installed copy of the NCBO soft-
ware that users can run and maintain. More information
about obtaining and installing the NCBO Virtual Appli-
ance is available at the NCBO Wiki.8
The system uses a set of predefined parameters to
control how the different evaluation scores are calcu-
lated, weighted and aggregated. Given that high input
coverage is the main requirement for ontology recom-
mendation systems, the weight assigned by default to
ontology coverage (0.55) is considerably higher than the
weight assigned to ontology acceptance, detail and
specialization (0.15). Our system uses the same cover-
age weight than the BiOSS system [21]. The default
configuration provides appropriate results for general
ontology recommendation scenarios. However, both the
web interface and the REST service allow users to adapt
the system to their specific needs by modifying the
weights given to coverage, acceptance, knowledge de-
tail, and specialization. The predefined values for all de-
fault parameters used by Ontology Recommender 2.0
are provided as an additional file [see Additional file 2].
Some Ontology Recommender users may need to ob-
tain repeatable results over time. Currently, however,
any changes in the BioPortal ontology repository, such
as submitting a new ontology or removing an existing
one, may change the suggestions returned by the
Ontology Recommender for the same inputs. BioPortal
services do not provide version-based ontology access,
so services such as the Ontology Recommender and the
Annotator always run against the latest versions of the
ontologies. A possible way of dealing with this short-
coming would be to install the NCBO Virtual Appli-
ance with a particular set of ontologies and keep them
locally unaltered.
The Ontology Recommender 2.0 was released in
August 2015, as part of BioPortal 4.20.9 The traffic data
for 2016 reflects the great interest of the community on
the new system, with an average of 45.2 K calls per
month to the Ontology Recommender API, and 1.2 K
views per month on the Ontology Recommender
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 10 of 22
webpage. These numbers represent an increase of more
than 600% in the number of calls to the API over 2015,
and more than 30% in the number of pageviews over
2015. Other widely used BioPortal services are Search,
with an average of 873.9 K calls per month to the
API, and 72.9 K pageviews per month in 2016; and
the Annotator, with an average of 484.8 K calls per
month to the API, and 3 K pageviews per month in
2016. Detailed traffic data for the Ontology Recom-
mender and other top used BioPortal services for the
period 20142016 is provided as an additional file
[see Additional file 1]. The source code is available in
GitHub10 under a BSD License.
User interface
Figure 3 shows the Ontology Recommender 2.0 user
interface. The system supports two input types: plain
text and comma-separated keywords. It also provides
two kinds of output: ranked ontologies and ranked
ontology sets. The advanced options section, which is
initially hidden, allows the user to customize (1) the
weights applied to the evaluation criteria, (2) the max-
imum number of ontologies in each set (when using
the ontology sets output), and (3) the list of candidate
ontologies to be evaluated.
Figure 4 shows an example of the system's output
when selecting keywords as input and ontologies as
output. For each ontology in the output, the user inter-
face shows its final score, the scores for the four evalu-
ation criteria used, and the number of annotations
performed with the ontology on the input. For instance,
the most highly recommended ontology in Fig. 4 is the
Symptom Ontology (SYMP), which covers 17 of the 21
input keywords. By clicking on the different rows of the
column highlight annotations, the user can select any
of the suggested ontologies and see which specific input
terms are covered. Also, clicking on a particular term in
the input reveals the details of the matched class in Bio-
Portal. All scores are translated from the interval [0, 1]
to [0, 100] for better readability. A score of '0' for a given
Fig. 2 An overview of the architecture and workflow of Ontology Recommender 2.0. (1) The input data and parameter settings are received
through any of the system interfaces (i.e., Web service or Web UI), and are sent to the system's backend. (2) The evaluation process starts. The
NCBO Annotator is invoked to retrieve all annotations for the input data. The system uses these annotations to evaluate BioPortal ontologies, one
by one, according to four criteria: coverage, acceptance, detail and specialization. Because of the system's modular design, additional evaluation
criteria can be easily added. The system uses BioPortal services to retrieve any additional information required by the evaluation process. For
example, evaluation of ontology acceptance requires the number of visits to the ontology in BioPortal (pageviews), and checking whether the
ontology is present in the Unified Medical Language System (UMLS) or not. Four independent evaluation scores are returned for each ontology
(one per evaluation criterion). (3) The scores obtained are combined into a relevance score for the ontology. (4) The relevance scores are used to
generate a ranked list of ontologies or ontology sets, which (5) is returned via the corresponding system's interface
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 11 of 22
ontology and evaluation criterion means that the ontol-
ogy has obtained the lowest score compared to the rest
of candidate ontologies. A score of '1' means that the
ontology has obtained the highest score, in relation to all
the other candidate ontologies.
Figure 5 shows the Ontology sets output for the same
keywords displayed in Fig. 4. The output shows that using
three ontologies (SYMP, SNOMEDCT and MEDDRA) it
is possible to cover all the input keywords. Different colors
for the input terms and for the recommended ontologies
in Fig. 5 distinguish the specific terms covered by each
ontology in the selected set.
Limitations
One of the shortcomings of the current implementation
is that the acceptance score is calculated using data from
Fig. 3 Ontology Recommender 2.0 user interface. The user interface has buttons to select the input type (i.e., text or keywords) and output
type (i.e., ontologies and ontology sets). A text area enables the user to enter the input data. The Get Recommendations button triggers the
execution. The advanced options button shows additional settings to customize the recommendation process
Fig. 4 Example of the Ontologies output. The user interface shows the top recommended ontologies. For each ontology, it shows the position
of the ontology in the ranking, the ontology acronym, the final recommendation score, the scores for each evaluation criteria (i.e., coverage,
acceptance, detail, and specialization), and the number of annotations performed with the ontology. The highlight annotations button
highlights the input terms covered by the ontology
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 12 of 22
only two platforms. BioPortal is used to calculate the
visits score, and UMLS is used to calculate the presence
score. There are other widely known ontology repositor-
ies that should be considered too. We believe that the
reliability of the current implementation would be in-
creased by taking into account visits and presence infor-
mation from additional platforms, such as the OBO
Foundry and the Ontology Lookup Service (OLS). Ex-
tending our implementation to make use of additional
platforms would require us to have a consistent mechan-
ism to check the presence of each candidate ontology
into other platforms, as well as a way to access updated
traffic data from them.
Another limitation is related to the ability to identify
different variations of a particular term. The coverage
evaluation metric is dependent on the annotations iden-
tified by the Annotator for the input data. The Annota-
tor deals with synonyms and term inflections (e.g.,
leukocyte, leukocytes, white blood cell) by using the syno-
nyms contained in the ontology for a particular term.
For example, Medical Subject Headings (MeSH) pro-
vides 11 synonyms for the term leukocytes, including
leukocyte and white blood cells. As a consequence, the
Annotator would be able to perform an annotation be-
tween the input term white blood cells and the MESH
term leukocytes. However, not all ontologies provide
such level of detail for their classes, and therefore the
Annotator may not be able to appropriately perform an-
notations with them. The NCBO, in collaboration with
University of Montpellier, is currently investigating
several NLP approaches to improve the Annotator ser-
vice. Applying lemmatization to both the input terms
and the dictionary used by the Annotator is one of the
methods currently being tested. As soon as these new
features will be made available in the Annotator, they
will automatically be used by Ontology Recommender.
Evaluation
To evaluate our approach, we compared the performance
of Ontology Recommender 2.0 to Ontology Recom-
mender 1.0 using data from a variety of well-known public
biomedical databases. Examples of these databases are
PubMed, which contains bibliographic information for the
fields of biomedicine and health; the Gene Expression
Omnibus (GEO), which is a repository of gene expression
data; and ClinicalTrials.gov, which is a registry of clinical
trials. We used the API provided by the NCBO Resource
Index11 [39] to programmatically extract data from
those databases.
Experiment 1: input coverage
We selected 12 widely known biomedical databases and
extracted 600 biomedical texts from them, with 127
words on average, and 600 lists of biomedical keywords,
with 17 keywords on average, producing a total of 1200
inputs (100 inputs per database). The databases used are
listed in Table 6.
Given the importance of input coverage, we first exe-
cuted both systems for all inputs and compared the
coverage provided by the top-ranked ontology. We
Fig. 5 Example of the Ontology sets output. The user interface shows the top recommended ontology sets. For each set, it shows its position
in the ranking, the acronyms of the ontologies that belong to it, the final recommendation score, the scores for each evaluation criteria
(i.e., coverage, acceptance, detail, and specialization), and the number of annotations performed with all the ontologies in the ontology
set. The highlight annotations button highlights the input terms covered by the ontology set
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 13 of 22
focused on the top-ranked ontology because the major-
ity of users always select the first result obtained [40].
The strategy we used to calculate the ontology coverage
differed depending on the input type:
 For texts, the coverage was computed as the
percentage of input words covered by the ontology
with respect to the total number of words that
could be covered using all BioPortal ontologies
together.
 For keywords, the coverage was computed as the
percentage of keywords covered by the ontology
divided by the total number of keywords.
Figures 6 and 7 show a representation of the coverage
provided by both systems for each database and input
type. Tables 7 and 8 provide a summary of the evalu-
ation results.
For some inputs, the first ontology suggested by
Ontology Recommender 1.0 provides very low coverage
(under 20%). This results from one of the shortcomings
previously described: Ontology Recommender 1.0 occa-
sionally assigns a high score to ontologies that provide
low coverage because they contain several classes
matching the input. The new recommendation ap-
proach used by Ontology Recommender 2.0 addresses
this problem: Virtually none of its executions provide
such low coverage.
For example, Table 9 shows the ontologies recom-
mended if we input the following description of a dis-
ease, extracted from the Integrated Disease View (IDV)
database: Chronic fatigue syndrome refers to severe, con-
tinued tiredness that is not relieved by rest and is not
directly caused by other medical conditions. See also:
Fatigue. The exact cause of chronic fatigue syndrome
(CFS) is unknown. The following may also play a role in
the development of CFS: CFS most commonly occurs in
women ages 30 to 50.
Ontology Recommender 1.0 suggests the Bone Dyspla-
sia Ontology (BDO), whereas Ontology Recommender
2.0 suggests the NCI Thesaurus (NCIT). Because BDO
covers only 4 of the input terms, while NCIT covers 17,
the recommendation provided by Ontology Recom-
mender 2.0 is more appropriate than that of its
predecessor.
Ontology Recommender 2.0 also provides better
mean coverage for both input types (i.e., text and key-
words) across all the biomedical databases included in
the evaluation. Compared to Ontology Recommender
1.0, the mean coverage reached using Ontology
Recommender 2.0 was 14.9% higher for texts and
19.3% higher for keywords. That increase was even
greater using the ontology sets output type provided
by Ontology Recommender 2.0, which reached a mean
coverage of 92.1% for texts (31.3% higher than the
Ontology Recommender 1.0 ratings) and 89.8% for
keywords (26.9% higher).
For the selected texts, the average execution time of
Ontology Recommender 2.0 for the "ontologies" output
is 15.4 s, 43.9% higher than the Ontology Recommender
1.0 execution time (10.7 s). The ontology recommenda-
tion process performed by Ontology Recommender 2.0
is much more complex than the one performed by the
original version, and this is reflected by the execution
times. The average execution time for keywords is simi-
lar in both systems (9.5 s for Ontology Recommender
1.0 and 9.4 s for Ontology Recommender 2.0). When
dealing with keywords, the complex process performed
by Ontology Recommender 2.0 is compensated by its
ability to discard unnecessary annotations before staring
Table 6 Databases used for experiment 1
Database name Acronym Topic Source field Type
ARRS GoldMiner GM Biomedical images Image caption Text
Autism Database (AutDB) AUTDB Autism spectrum disorders Phenotype profile Text
Gene Expression Omnibus GEO Gene expression Summary Text
Integrated Disease View IDV Disease and treatment Description Text
PubMed PM Biomedicine Abstract Text
PubMed Health Drugs PMH Drugs Why is this medication prescribed? Text
Adverse Event Reporting System AERS Adverse events Adverse reactions Keywords
AgingGenesDB AGDB Aging related genes Keywords Keywords
ClinicalTrials.gov CT Clinical trials Condition Keywords
DrugBank DBK Drugs Drug category Keywords
PharmGKB-Gene PGGE Relationships about drugs, diseases and genes Gene related diseases Keywords
UniProt KB UPKB Proteins Biological processes Keywords
The table shows the database name, its acronym, the main topic of the database, the specific field from which the information was extracted, and the type of
textual data extracted (i.e., text or keywords)
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 14 of 22
the ontology evaluation process. These execution times
are substantially better than those reported for similar
systems. For example, the BiOSS system [21] needed an
average of 207 s to process 30 keywords with a repository
of 200 candidate ontologies. Performance of Ontology
Recommender 2.0 is reasonable for general scenarios,
where the quality of the suggestions is typically more im-
portant than the execution time.
Experiment 2: refining recommendations
Our second experiment set out to examine whether
Ontology Recommender 2.0 is effective at discerning
how to make meaningful recommendations when ontol-
ogies exhibit similar coverage of the input text. Specific-
ally, we were interested in analyzing how the new version
uses ontology acceptance, detail and specialization to
prioritize the most appropriate ontologies.
We started with the 1200 inputs (600 texts and 600
lists of keywords) from the previous experiment, and
selected those inputs for which the two versions of
Ontology Recommender suggested different ontologies
with similar coverage. We considered two coverage
values similar if the difference between them was less
than 10%. This yielded a total of 284 inputs (32 input
texts and 252 lists of keywords). We executed both sys-
tems for those 284 inputs and analyzed the ontologies
obtained in terms of their acceptance, detail and
specialization scores.
Figure 8 and Table 10 show the results obtained. The
ontologies suggested by Ontology Recommender 2.0
have higher acceptance (87.1) and detail scores (72.1)
Fig. 6 Coverage distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0 (solid blue line), using the
individual ontologies output, for 600 texts extracted from 6 widely known databases (100 texts each). Vertical lines represent the mean coverage
provided by the first ontology returned by Ontology Recommender 1.0 (dotted red line) and 2.0 (dashed-dotted blue line). The X-axis indicates the
percentage of words covered by the ontology. The Y-axis displays the number of inputs for which a particular coverage percentage was
obtained. AUTDB: Autism Database; GEO: Gene Expression Omnibus; GM: ARRS GoldMiner; IDV: Integrated Disease View; PM: PubMed; PMH:
PubMed Health Drugs
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 15 of 22
Fig. 7 Coverage distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0 (solid blue line), using
the individual ontologies output, for 600 lists of keywords extracted from 6 widely known databases (100 lists of keywords each). Vertical lines
represent the mean coverage provided by the first ontology returned by Ontology Recommender 1.0 (dotted red line) and 2.0 (dashed-dotted blue
line). The X-axis indicates the percentage of input keywords covered by the ontology. The Y-axis displays the number of inputs for which a
particular coverage percentage was obtained. AERS: Adverse Event Reporting System; AGDB: AgingGenesDB; CT: ClinicalTrials.gov; DBK: DrugBank;
PGGE: PharmGKB-Gene; UPKB: UniProt KB
Table 7 Summary of evaluation results for text inputs
Database Mean
lengtha
Executions with coverage < 20%b Mean coverage (top ranked ontology)c Execution time (seconds)
1.0* 2.0** 1.0* 2.0** 2.0 (sets)*** 1.0* 2.0** 2.0 (sets)***
AUTDB 128.8 12.0% 0.0% 66.8% 76.0% 90.3% 12.2 18.3 26.9
GEO 146.4 8.0% 0.0% 70.7% 76.9% 92.9% 11.2 17.2 26.1
GM 55.7 48.0% 0.0% 46.6% 82.6% 94.8% 9.6 12.3 15.2
IDV 150.2 28.0% 0.0% 50.6% 71.8% 89.3% 9.5 13.1 21.1
PM 208.9 7.0% 0.0% 69.1% 73.8% 93.1% 13.8 21.2 36.9
PMH 77.4 13.0% 0.0% 61.1% 73.5% 91.9% 8.0 10.5 13.3
Mean 127.9 19.3% 0.0% 60.8% 75.7% 92.1% 10.7 15.4 23.2
aMean of the number of words for the inputs extracted from the database
bPercentage of executions where the coverage of the top recommended ontology was lower than 20%
cMean coverage provided by the top ranked ontology
*Ontology Recommender 1.0; **Ontology Recommender 2.0; ***Ontology Recommender 2.0 (ontology sets output)
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 16 of 22
than those suggested by Ontology Recommender 1.0.
Importantly, the graphs show peaks of low acceptance
(<30%) and detail (<20%) for Ontology Recommender
1.0 that are addressed by Ontology Recommender 2.0.
The ontologies suggested by Ontology Recom-
mender 2.0 have, on average, lower specialization
scores (65.1) than those suggested by Ontology
Recommender 1.0 (95.1). This is an expected result,
given that the recommendation approach used by
Ontology Recommender 1.0 is based on the relation
between the number of annotations provided by each
ontology and its size, which is our measure for
ontology specialization.
Ontology Recommender 1.0 is better than Ontology
Recommender 2.0 at finding small ontologies that
provide multiple annotations for the users input.
However, those ontologies are not necessarily the
most appropriate to describe the input data. As we
have seen (see Section 1.2.1), a large number of anno-
tations does not always indicate a high input cover-
age. Ontology Recommender 1.0 sometimes suggests
ontologies with high specialization scores but with
very low input coverage, which makes the ontologies
inappropriate for the users input. The multi-criteria
evaluation approach used by Ontology Recommender
2.0 has been designed to address this issue by evaluating
ontology specialization in combination with other cri-
teria, including ontology coverage.
Experiment 3: high coverage and specialized ontologies
We set out to evaluate how well Ontology Recommender
2.0 prioritizes recommending small ontologies that pro-
vide appropriate coverage for the input data. We created
15 inputs, each of which contained keywords from a very
specific domain (e.g., adverse reactions, dermatology, units
of measurement), and executed both versions of the
Ontology Recommender for those inputs.
Table 11 shows the particular domain for each of the
15 inputs used, and the first ontology suggested by each
version of Ontology Recommender, as well as the size of
each ontology and the coverage provided.
Analysis of the results reveals that Ontology Recom-
mender 2.0 is more effective than Ontology Recom-
mender 1.0 for suggesting specialized ontologies that
provide high input coverage. In 9 out of 15 inputs (60%),
the first ontology suggested by Ontology Recommender
2.0 is more appropriate, in terms of its size and coverage
provided, than the ontology recommended by Ontology
Recommender 1.0. Ontology Recommender 2.0 considers
input coverage in addition to ontology specialization,
which Ontology Recommender 1.0 does not. In addition,
Ontology Recommender 2.0 uses a different annotation
scoring method (the function annotationScore2(a); see
Section 2.1.1) that gives more weight to annotations
that cover multi-word terms. There is one input (no.
13), for which the ontology suggested by Ontology Rec-
ommender 2.0 provides higher coverage (88% versus
Table 8 Summary of evaluation results for keyword inputs
Database Mean lengtha Executions with coverage < 20%b Mean coverage (top ranked ontology)c Execution time (seconds)
1.0* 2.0** 1.0* 2.0** 2.0 (sets)*** 1.0* 2.0** 2.0 (sets)***
AERS 29.5 6.0% 0.0% 54.6% 97.8% 99.6% 10.2 9.1 10.5
AGDB 8.2 5.0% 0.0% 53.4% 67.5% 82.9% 6.5 9.9 10.9
CT 16.6 12.0% 2.0% 61.4% 76.5% 84.8% 9.9 8.4 10.2
DBK 5.9 13.0% 1.0% 60.5% 74.7% 89.6% 4.3 6.8 7.3
PGGE 15.4 2.0% 0.0% 73.1% 80.5% 83.0% 9.9 9.1 10.3
UPKB 29.9 18.0% 0.0% 74.6% 96.3% 99.1% 16.5 13.1 16.9
Mean 17.6 9.3% 0.5% 62.9% 82.2% 89.8% 9.5 9.4 11.0
aMean of the number of words for the inputs extracted from the database
bPercentage of executions where the coverage of the top recommended ontology was lower than 20%
cMean coverage provided by the top ranked ontology
*Ontology Recommender 1.0; **Ontology Recommender 2.0; ***Ontology Recommender 2.0 (ontology sets output)
Table 9 Comparison of the terms covered by Ontology Recommender 1.0 and Ontology Recommender 2.0 for the input text
previously shown
Ontology Position Terms covered
Ontology Recommender 1.0 Ontology Recommender 2.0
BDO 1 23 Chronic, severe, chronic, unknown
NCIT 2 1 Chronic fatigue syndrome, severe, continued, rest, directly, medical, Fatigue, exact, cause,
chronic, fatigue syndrome, unknown, suggest, due to, following, role, development, ages
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 17 of 22
80%), but it is bigger than the ontology recommended
by Ontology Recommender 1.0 (324 K classes versus
119 K). In 5 out of 15 inputs (33%), both systems rec-
ommended the same ontology.
Discussion
Recommending biomedical ontologies is a challenging
task. The great number, size, and complexity of biomed-
ical ontologies, as well as the diversity of user require-
ments and expectations, make it difficult to identify the
most appropriate ontologies to annotate biomedical data.
The analysis of the results demonstrates that ontologies
suggested using our new recommendation approach are
more appropriate than those recommended using the
original method. Our acceptance evaluation method has
proved to be successful to rank ontologies, and it is cur-
rently used not only by the Ontology Recommender, but
also by the BioPortal search engine. The classes returned
when searching in BioPortal are ordered according to the
general acceptance of the ontologies to which they belong.
We note that, because the system is designed in a
modular way, it will be easy to add new evaluation cri-
teria to extend its functionality. As a first priority, we
intend to improve and extend the evaluation criteria
currently used. In addition, we will investigate the effect
of extending the Ontology Recommender to include
relevant features not yet considered, such as the fre-
quency of an ontologys updates, its levels of abstrac-
tion, formality, granularity, and the language in which
the ontology is expressed.
Indeed, using metadata information is a simple but
often ignored approach to select ontologies. Coverage-
based approaches often miss relevant results because
they focus on the content of ontologies and ignore more
Fig. 8 Acceptance, detail and specialization distribution for the first ontology suggested by Ontology Recommender 1.0 (dashed red line) and 2.0
(solid blue line), for the 284 inputs selected. Vertical lines represent the mean acceptance, detail and specialization scores provided by Ontology
Recommender 1.0 (dotted red line) and 2.0 (dashed-dotted blue line). The X-axis indicates the acceptance, detail and specialization score provided
by the top ranked ontology. The Y-axis displays the number of inputs for which a particular score was obtained
Table 10 Mean acceptance, detail and specialization scores provided by the two versions of Ontology Recommender for
experiment 2
Text (32 inputs) Keywords (252 inputs) All (284 inputs)
1.0* 2.0** 1.0* 2.0** 1.0* 2.0**
Mean acceptance 91.3 99.2 39.8 85.2 45.7 87.1
Mean detail 5.8 56.1 15.7 73.9 14.6 72.1
Mean specialization 94.7 90.3 94.8 61.6 95.1 65.1
*Ontology Recommender 1.0; **Ontology Recommender 2.0
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 18 of 22
general information about the ontology. For example,
applying the new Ontology Recommender to the Wiki-
pedia definition of anatomy12 will return some widely-
known ontologies that contain the terms anatomy, struc-
ture, organism and biology, but the Foundational Model
of Anatomy (FMA), which is the reference ontology
about human anatomy will not show up in the top 25 re-
sults. Our specialization criterion uses the content of the
ontology and the ontology size to discriminate between
large ontologies and small ontologies that have better
specialization. However, ontologies that provide multiple
annotations for the input data are not always specialized
to deal with the input domain. Sometimes very special-
ized ontologies for a domain may provide low coverage
for a particular text from the domain. In this scenario,
metadata about the domain of the ontology (e.g., 'anat-
omy' in the case of FMA) could be used to enhance our
ontology specialization criterion by limiting the sugges-
tions to those ontologies whose domain matches the in-
put data domain. We are currently refining, in
collaboration with the Center for Expanded Data Anno-
tation and Retrieval (CEDAR) [41] and the AgroPortal
ontology repository [42], the way BioPortal handles
metadata for ontologies in order to support even more
ontology recommendation scenarios.
Our coverage evaluation approach may be further en-
hanced by complementing our annotation scoring
method (i.e., annotationScore2) with term extraction
techniques. We plan to analyze the application of a
term extraction measure, called C-value [43], which is
specialized for multi-word term extraction, and that
has already been applied to the results of the NCBO
Annotator, leading to significant improvements [44].
There are some possible avenues for enhancing our
assessment of ontology acceptance. These include con-
sidering the number of projects that use a specific
ontology, the number of mappings created manually
that point to a particular ontology, the number of user
contributions (e.g., mappings, notes, comments), the
metadata available per ontology, and the number, publi-
cation date and publication frequency of ontology ver-
sions. There are other indicators external to BioPortal
that could be useful for performing a more comprehen-
sive evaluation of ontology acceptance, such as the
number of Google results when searching for the ontol-
ogy name or the number of PubMed publications that
contain the ontology name [21].
Reusing existing ontologies instead of building new
ones from scratch has many benefits, including lower-
ing the time and cost of development, and avoiding
duplicate efforts [45]. As shown by a recent study [46],
reuse is fairly low in BioPortal, but there are some
ontologies that are approaching complete reuse (e.g.,
Mental Functioning Ontology). Our approach should
be able to identify these ontologies and assign them a
lower score than those ontologies where the knowledge
was first defined. We will study the inclusion of add-
itional evaluation criteria to weigh the amount of
Table 11 Experiment 3 results
Input Size Input domain Ontology Recommender 1.0 Ontology Recommender 2.0
Result Size Coverage (%) Result Size Coverage (%)
1 23 Adverse reactions SNOMEDCT 324,129 52.1 MEDDRA 68,261 91.3
2 8 Autism spectrum disorder ASDPTO 284 100.0 ASDPTO 284 100.0
3 14 Bioinformatics operations SWO 4,068 100.0 EDAM 3,240 100.0
4 30 Biomedical investigations NCIT 118,941 63.3 OBI 3,055 100.0
5 26 Cell types SYN 14,462 76.9 CL 6,532 88.4
6 14 Clinical research NCIT 118,941 78.6 OCRE 389 100.0
7 13 Dermatology DERMLEX 6,106 92.3 DERMLEX 6,106 92.3
8 14 Environmental features ENVO 2,307 100.0 ENVO 2,307 100.0
9 18 Enzyme sources NCIT 118,941 55.6 BTO 5,902 72.2
10 19 Fish anatomy NIFSTD 124,337 68.4 TAOcpr 3,428 79.0
11 44 Human diseases RH-MESH 305,349 52.3 DOID 11,280 95.5
12 13 Mathematical models in life sciences MAMO 100 100.0 MAMO 100 100.0
13 25 Primary care NCIT 118,941 80.0 SNOMEDCT 324,129 88.0
14 23 Signs and symptoms NCIT 118,941 65.2 SYMP 936 91.3
15 25 Units of Measurement TEO 687 92.0 TEO 687 92.0
The size of each ontology (number of classes) and the coverage provided are also shown. The best results for each input (lowest ontology size and highest coverage),
are highlighted in bold
The table shows the input size (number of keywords) and domain, as well as the first ontology suggested by Ontology Recommender 1.0 and Ontology
Recommender 2.0
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 19 of 22
original knowledge provided by a particular ontology
for the input data.
The current version of Ontology Recommender uses a
set of default parameters to control how the different
evaluation scores are calculated, weighted and aggregated.
These parameters provide acceptable results for general
ontology recommendation scenarios, but some users may
need to modify the default settings to match their needs.
In the future, we would like the system to use an auto-
matic weight adjustment approach. We will investigate
whether it is possible to develop methods of adjusting the
weights dynamically for specific scenarios.
Ontology Recommender helps to identify all the ontol-
ogies that would be suitable for semantic annotation.
However, given the number of ontologies in BioPortal, it
would be difficult, computationally expensive, and often
useless to annotate user inputs with all the ontologies in
the repository. Ontology Recommender could function
within BioPortal as a means to screen ontologies for
use with the NCBO Annotator. Note that the output of
the Annotator is a ranked list of annotations performed
with multiple ontologies, while the output of the Ontol-
ogy Recommender is a ranked list of ontologies. A user
might be offered the possibility to Run the Ontology
Recommender first before actually calling the Annota-
tor. Then only the top-ranked ontologies would be used
for annotations.
A user-based evaluation would help us understand the
systems utility in real-world settings. Our experience
evaluating the original Ontology Recommender and
BiOSS showed us that obtaining a user-based evaluation
of an ontology recommender system is a challenging
task. For example, the evaluators of BiOSS reported that
they would need at least 50 min to perform a high-
quality evaluation of the system for each test case. We
plan to investigate whether crowd-sourcing methods, as
an alternative, can be useful to evaluate ontology recom-
mendation systems from a user-centered perspective.
Our approach for ontology recommendation was de-
signed for the biomedical field, but it can be adapted
to work with ontologies from other domains so long as
they have a resource equivalent to the NCBO Annota-
tor, an API to obtain basic information about all the
candidate ontologies, and their classes, and alternative
resources for extracting information about the accept-
ance of each ontology. For example, AgroPortal [42] is
an ontology repository based on NCBO BioPortal tech-
nology. AgroPortal uses Ontology Recommender 2.0 in
the context of plant, agronomic and environmental
sciences.13
Conclusions
Biomedical ontologies are crucial for representing
knowledge and annotating data. However, the large
number, complexity, and variety of biomedical ontol-
ogies make it difficult for researchers to select the
most appropriate ontologies for annotating their data.
In this paper, we presented a novel approach for
recommending biomedical ontologies. This approach
has been implemented as release 2.0 of the NCBO
Ontology Recommender, a system that is able to find
the best ontologies for a biomedical text or set of
keywords. Ontology Recommender 2.0 combines the
strengths of its predecessor with a range of adjust-
ments and new features that improve its reliability
and usefulness.
Our evaluation shows that, on average, the new system
is able to suggest ontologies that provide better input
coverage, contain more detailed information, are more
specialized, and are more widely accepted than those
suggested by the original Ontology Recommender. In
addition, the new version is able to evaluate not only in-
dividual ontologies, but also different ontology sets, in
order to maximize input coverage. The new system can
be customized to specific user needs and it provides
more explanatory output information than its predeces-
sor, helping users to understand the results returned.
The new service, embedded into the NCBO BioPortal,
will be a more valuable resource to the community of
researchers, scientists, and developers working with
ontologies.
Endnotes
1The BioPortal API received 18.8 M calls/month on
average in 2016. The BioPortal website received 306.9 K
pageviews/month on average in 2016 (see Additional
file 1 for more detailed traffic data). The two main Bio-
Portal papers [3, 4] accumulate 923 citations at the time
of writing this paper, with 145 citations received in
2016.
2http://bioportal.bioontology.org/
3At the time of writing this paper, there are 63 cita-
tions to the NCBO Ontology Recommender 1.0 paper
[6]. The Ontology Recommender 1.0 API received 7.1 K
calls/month on average in 2014. The Ontology Recom-
mender webpage received 1.4 K pageviews/month on
average in 2014. Detailed traffic data is provided in
Additional file 1.
4This formula is slightly different from the scoring
method presented in the paper describing the original
Ontology Recommender Web service [6]. It corresponds
to an upgrade done in the recommendation algorithm in
December 2011, when BioPortal 3.5 was released, for
which description and methodology was never pub-
lished. The normalization strategy was improved by
applying a logarithmic transformation to the ontology
size to avoid a negative effect on very large ontologies.
Mappings between ontologies, used to favor reference
Martínez-Romero et al. Journal of Biomedical Semantics  (2017) 8:21 Page 20 of 22
ontologies, were discarded due to the small number of
manually created and curated mappings that could be
used for such a purpose. The hierarchy-based semantic
expansion was replaced by the position of the matched
class in the ontology hierarchy.
5The function is called annotationScore2 to differentiate
it from the original annotationScore function.
6The API documentation is available at http://data.
bioontology.org/documentation#nav_recommender
7The Web-based user interface is available at http://
bioportal.bioontology.org/recommender
8https://www.bioontology.org/wiki/index.php/
Category:NCBO_Virtual_Appliance
9BioPortal release notes: https://www.bioontology.org/
wiki/index.php/BioPortal_Release_Notes
10https://github.com/ncbo/ncbo_ontology_recommender
11The NCBO Resource Index is an ontology-based
index that provides access to over 30 million biomedical
records from 48 widely-known databases. It is available
at: http://bioportal.bioontology.org/.
12https://en.wikipedia.org/wiki/Anatomy
13http://agroportal.lirmm.fr/recommender
Additional files
Additional file 1: Ontology Recommender traffic summary. Summary of
traffic received by the Ontology Recommender for the period 20142016,
compared to the other most used BioPortal services. (PDF 27 kb)
Additional file 2: Default configuration settings. Default values used by
the NCBO Ontology Recommender 2.0 for the parameters that control
how the different scores are calculated, weighted and aggregated.
(PDF 9 kb)
Abbreviations
BIOMODELS: BioModels ontology (BIOMODELS); COSTART: Coding Symbols
for Thesaurus of Adverse Reaction Terms; CPT: Current Procedural
Terminology; CRISP: Computer Retrieval of Information on Scientific Projects
thesaurus; EFO: Experimental Factor Ontology; EHDA: Human Developmental
Anatomy Ontology, timed version; EP: Cardiac Electrophysiology Ontology;
FMA: Foundational Model of Anatomy; GO: Gene Ontology;
HUPSON: Human Physiology Simulation Ontology; ICD9CM: International
Classification of Diseases, version 9 - Clinical Modification; ICPC: International
Classification of Primary Care; LOINC: Logical Observation Identifier Names
and Codes; MEDDRA: Medical Dictionary for Regulatory Activities;
MEDLINEPLUS: MedlinePlus Health Topics; MESH: Medical Subject Headings;
MP: Mammalian Phenotype Ontology; NCIT: National Cancer Institute
Thesaurus; NDDF: National Drug Data File; NDFRT: National Drug File -
Reference Terminology; OMIM: Online Mendelian Inheritance in Man;
PDQ: Physician Data Query; RCD: Read Codes, Clinical Terms version 3;
RXNORM: RxNORM; SNOMEDCT: Systematized Nomenclature of Medicine -
Clinical Terms; SWEET: Semantic Web for Earth and Environment Technology
Ontology; SYMP: Symptom Ontology; VANDF: Veterans Health Administration
National Drug File; VSO: Vital Sign Ontology
Acknowledgments
The authors acknowledge the suggestions about the problem of recommending
ontologies provided by the NCBO team, as well as their assistance and advice on
integrating Ontology Recommender 2.0 into BioPortal. The authors also thank
Simon Walk for his report on the BioPortal traffic data. Natasha Noy and Vanessa
Aguiar offered valuable feedback.
Funding
This work was supported in part by the National Center for Biomedical
Ontology as one of the National Centers for Biomedical Computing,
supported by the NHGRI, the NHLBI, and the NIH Common Fund under
grant U54 HG004028 from the U.S. National Institutes of Health. Additional
support was provided by CEDAR, the Center for Expanded Data Annotation
and Retrieval (U54 AI117925) awarded by the National Institute of Allergy
and Infectious Diseases through funds provided by the trans-NIH Big Data to
Knowledge (BD2K) initiative. This project has also received support from the
European Unions Horizon 2020 research and innovation programme under
the Marie Sklodowska-Curie grant agreement No 701771 and the French
National Research Agency (grant ANR-12-JS02-01001).
Availability of data and materials
 Project name: The Biomedical Ontology Recommender.
 Project home page: http://bioportal.bioontology.org/recommender.
 Project GitHub repository: https://github.com/ncbo/
ncbo_ontology_recommender.
 REST service parameters: http://data.bioontology.org/
documentation#nav_recommender.
 Operating system(s): Platform independent.
 Programming language: Ruby, Javascript, HTML.
 Other requirements: none.
 License: BSD (http://www.bioontology.org/BSD-license).
 Datasets used in our evaluation: https://git.io/vDIXV.
Authors contributions
MMR conceived the approach, designed and implemented the system, and
drafted the initial manuscript. CJ participated in technical discussions and
provided ideas to refine the approach. MAM supervised the work and gave
advice and feedback at all stages. CJ, MJO, JG, and AP provided critical
revision and edited the manuscript. All authors gave the final approval of the
manuscript.
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Stanford Center for Biomedical Informatics Research, 1265 Welch Road,
Stanford University School of Medicine, Stanford, CA 94305-5479, USA.
2Department of Information and Communication Technologies, Computer
Science Building, Elviña Campus, University of A Coruña, 15071 A Coruña,
Spain. 3Laboratory of Informatics, Robotics and Microelectronics of
Montpellier (LIRMM), University of Montpellier, 161 rue Ada, 34095
Montpellier, Cdx 5, France.
Received: 28 October 2016 Accepted: 13 April 2017
RESEARCH Open Access
DisSetSim: an online system for calculating
similarity between disease sets
Yang Hu1, Lingling Zhao2, Zhiyan Liu2, Hong Ju3, Hongbo Shi4, Peigang Xu2, Yadong Wang2* and Liang Cheng4*
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China. 16 December 2016
Abstract
Background: Functional similarity between molecules results in similar phenotypes, such as diseases. Therefore, it is
an effective way to reveal the function of molecules based on their induced diseases. However, the lack of a tool
for obtaining the similarity score of pair-wise disease sets (SSDS) limits this type of application.
Results: Here, we introduce DisSetSim, an online system to solve this problem in this article. Five state-of-the-art methods
involving Resniks, Lins, Wangs, PSB, and SemFunSim methods were implemented to measure the similarity score of pair-
wise diseases (SSD) first. And then pair-wise-best pairs-average (PWBPA) method was implemented to calculated the
SSDS by the SSD. The system was applied for calculating the functional similarity of miRNAs based on their induced
disease sets. The results were further used to predict potential disease-miRNA relationships.
Conclusions: The high area under the receiver operating characteristic curve AUC (0.9296) based on leave-one-out cross
validation shows that the PWBPA method achieves a high true positive rate and a low false positive rate. The system can
be accessed from http://www.bio-annotation.cn:8080/DisSetSim/.
Keywords: Functional similarity, Similarity score, Disease sets, Disease-miRNA relationships
Background
The similarity of pair-wise disease sets (SDS) has drawn
more and more attention in identifying functional similar-
ity of the disease-caused molecules [1], predicting poten-
tial relationships between diseases and molecules [28],
and so on. In previous studies, Wang et al. utilized the
SDS to construct a human miRNA functional similarity
network (MFSN) [1]. And Sun et al. used the SDS to
predict novel disease lncRNA relationships [9].
The performance of calculating the SDS is mainly
based on the method for computing the similarity of
pair-wise diseases (SD). Currently, seven state-of-art
methods involving Resniks [10], Lins [11], Wangs [12],
process-similarity based (PSB) [13], SemFunSim [14],
ILNCSIM [15], and FMLNCSIM [16] methods were
frequently used for computing the SD. Among these
methods, Resniks [10], Lins [11], and Wangs methods
[12] are designed earlier for Gene Ontology (GO) [8,
17]. And these methods were introduced for calculating
the SD by DOSim [18] and DisSim [19]. Resniks and
Lins methods [10, 11] are based on information content
(IC) for computing similarity between terms of ontology.
And Wangs method [12] is based on the hierarchical
structure of the ontology. PSB and SemFunSim methods
are newly developed for Disease Ontology (DO) [20].
PSB method [13] utilized the association of biological
process between genes to calculate disease similarity. In
comparison, SemFunSim method [14] considered more
types of the functional associations including protein-
protein interaction [21], human mRNA co-expression
[22], and so on.
Resources for calculating the similarity score of pair-
wise diseases (SSD) mainly includes the vocabularies of
diseases and disease-related genes. The frequently used
disease vocabularies contain Online Mendelian Inherit-
ance in Man (OMIM) [23], Medical Subject Headings
* Correspondence: ydwang@hit.edu.cn; liangcheng@hrbmu.edu.cn
2Department of Computer Science and Technology, Harbin Institute of
Technology, Harbin 150001, Peoples Republic of China
4College of Bioinformatics Science and Technology, Harbin Medical
University, Harbin 150001, Peoples Republic of China
Full list of author information is available at the end of the article
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28
DOI 10.1186/s13326-017-0140-2
(MeSH) [24], and DO [20]. OMIM records the names of
genetic disorders without providing semantic associations
between them. MeSH provides a hierarchy of terms in
biomedical domain. It contains 16 categories, of which
only C and F03 involve disease names. In comparison with
OMIM and MeSH, DO has been established around the
concept of disease, and it aims to provide a clear definition
for each disease. The disease-related genes are scattered in
the databases, such as Gene Reference into Function
(GeneRIF) [25], OMIM [23], Genetic Association
Database (GAD) [26] and Comparative Toxicogenomics
Database (CTD) [27]. It is better to use relationships of all
of these databases.
pair-wise-all pairs-maximum (PWAPM) method and
pair-wise-best pairs-average (PWBPA) method are
optional for calculating similarity of pair-wise term sets
[28]. For comparing multiple aspects, the best measure
is the PWBPA method, which is widely utilized in calcu-
lating similarity of DO and GO term sets [1, 7, 9, 12].
Although DOSim [18] and DisSim [19] implemented
the disease similarity methods in R package and web
interface, no tools provided the function to calculate the
similarity score of pair-wise disease sets (SSDS)
currently. In this article, we designed and implemented
an online tool DisSetSim to calculate the SSDS. Five
state-of-art disease similarity methods (Resniks, Lins,
Wangs, PSB, and SemFunSim) and the PWBPA method
was implemented in the tool. The system is freely avail-
able at http://www.bio-annotation.cn:8080/DisSetSim/.
Methods
Date sources
Data sets of DisSetSim are from open source databases, and
they are listed in Table 1. DO [20] records disease names. It
provides terms for calculating disease similarity. GeneRIF
[25], OMIM [23], GAD [26] and CTD [27] are manually
curated databases of disease-related genes. All of diseases in
these databases are mapped to terms in DO based on SIDD
[29]. GO annotation (GOA) [30] includes functional anno-
tation of genes. HumanNet is the gene functional network
of human. In addition, HMDD v2.0 [31] contains disease-
related miRNAs, diseases of which were manually mapped
to terms in DO by OAHG [32].
Methods for calculating similarity score of pair-wise
diseases
Five state-of-art methods involving Resniks [10], Lins
[11], Wangs [12], PSB [13], and SemFunSim methods
[14] have been implemented for calculating the SSD.
Resniks and Lins methods are based on IC. The IC of
a disease t is described as Eq. 1:
IC tð Þ ¼ ?log2
nt
N
; ð1Þ
where N is the total number of genes annotated by
diseases, and nt is the number of genes annotated by t.
Assuming t1 and t2 are two diseases, the similarity of
them is defined by Resnik as following [10]:
SimResnik t1; ; t2ð Þ ¼ IC tMICAð Þ; ð2Þ
where tMICA is the most informative common ancestor
(MICA) of t1 and t2. Lin defines the similarity of t1 and
t2 as Eq. 3 [11]:
SimLin t1; ; t2ð Þ ¼ 2?IC tMICAð ÞIC t1ð Þ þ IC t2ð Þ : ð3Þ
Assuming T1 is the set involving t1 and all of its ances-
tor terms of ontology. Semantic contribution of term t
to t1 is represented as following:
St1 tð Þ ¼
1 t ¼ t1
St1 tð Þ ¼ max w?St1 t
0  j t 0?T 1
 
t?t1
(
;
ð4Þ
where w is the contribution factor of each semantic
relationship. According to Wang et al. [1], w is defined
as 0.5 for IS_A relationship of DO [20]. Then, all the
semantic contributions of T1 to t1 is SV(t1), which is
defined as following:
SV t1ð Þ ¼
X
t?T1
St1 tð Þ: ð5Þ
Assuming T2 is the set involving t2 and all of its ances-
tor terms, the similarity between t1 and t2 is defined as
following by Wangs method [12]:
SimWang t1; t2ð Þ ¼
X
t?T 1?T 2
St1 tð Þ þ St2 tð Þð Þ
SV t1ð Þ þ SV t2ð Þ : ð6Þ
Assuming t1 and t2 can be related with m and n bio-
logical processes of GO based on hypergeometric test,
respectively, the similarity of t1 and t2 is defined by the
PSB method as following:
Table 1 Data sources
Data source Web site
DO http://disease-ontology.org/
CTD http://ctdbase.org/
GeneRIF http://www.ncbi.nlm.nih.gov/gene/about-generif
GAD https://geneticassociationdb.nih.gov/
OMIM http://www.omim.org/
GO & GOA http://www.geneontology.org
HumanNet http://www.functionalnet.org/humannet/
OAHG bio-annotation.cn/OAHG/
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 20 of 79
SimPSB t1; t2ð Þ ¼ 12
Xm
i¼1
max
1?j?n
Sim p1i; p2j
  
m
0
BBB@
þ
Xn
j¼1
max
1?i?m
Sim p2j; p1i
  
n
1
CCCCA
ð7Þ
where p1i and p2j is the ith and jth significant related
biological process terms of t1 and t2, respectively.
Sim(p1i, p2j) represents similarity between two processes
p1i and p2j, which is defined as Eq. 8:
Sim p1; p2ð Þ ¼
1
2
? ICGO p1ð Þ þ ICGO p2ð Þð Þ?
n p1?p2ð Þ
n p1?p2ð Þ
?
ICGO p1ð Þ
Max ICGOð Þ
?
ICDO p1ð Þ
Max ICDOð Þ ?
ICGO p2ð Þ
Max ICGOð Þ ?
ICDO p2ð Þ
Max ICDOð Þ ;
ð8Þ
where ICGO and ICDO represent IC based on GO and
DO, respectively. n(p1 ?p2) and n(p1 ?p2) denote the
number of common genes of p1 and p2, and the number
of total genes of p1 and p2, respectively.
Assuming G1 and G2 represent related gene sets of t1
and t2, respectively. Then, the similarity of t1 and t2 by
the SemFunSim method can be described as following:
SimSemFunSim t1;t2ð Þ¼
Xm
i¼1
max
1?j?n
Sim g1i; g2j
  
þ
Xn
j¼1
max
1?i?m
Sim g2j; g1i
  
mþn
?
m
?GMICA?
?
n
?GMICA?
ð9Þ
where |GMICA| represents the number of genes in
GMICA. m and n denote the number of genes in G1 and
G2, respectively. Sim(g1i, g2j) is the functional similarity
score between genes g1i and g2j, which could be obtained
from HumanNet [33].
Method for calculating similarity score of pair-wise dis-
ease sets
The PWBPA method was utilized for calculating the
SSDS. The similarity of two disease sets T1 and T2 is de-
fined as following:
PWBPA T1;T2ð Þ ¼
PN
i¼0
max
0<j?M
Sim ti; tj
 þP
M
j¼0
max
0<i?N
Sim tj; ti
 
N þM ;
ð10Þ
where T1 and T2 contains N and M diseases,
respectively. ti and tj represents ith and jth terms of T1
and T2, respectively.
Predicting potential association between diseases and
miRNAs
Functional similarity between miRNAs could be calcu-
lated based on their related disease sets. Similarities of
each pair-wise miRNAs are utilized to establish a MFSN.
Node of the network represents miRNA. Weight of edge
is the functional similarity score. Then, disease-related
miRNAs were prioritized using the network ranking
algorithm named random walk with restart (RWR) [7].
The random walker starts on one or several seed
nodes and then randomly transits to neighboring nodes
considering the probabilities of the edges between the
two nodes. And the probability to return to the seed
nodes is supposed as ?. Then, RWR algorithm can be
defined as following:
Ptþ1 ¼ ?P0 þ 1??ð ÞAPt ð11Þ
where P0 denotes the initial probability vector, Pt is a
vector in which the ith element represents the probabil-
ity of finding the walker at node i and step t, A is the
column-normalized adjacency matrix of the network.
The algorithm was performed until the difference
between Pt and Pt+1 falling below 10
?10, which means all
the nodes become stable.
In this study, the known miRNAs of a disease were
considered as seed nodes. The unknown miRNAs of it
could be scored based on RWR on the MFSN. After
ranking the miRNAs based on the scores, disease-related
miRNAs could be prioritized.
Implementation
DisSetSim has been implemented on a JavaEE framework
and run on the web server (2-core (2.26 GHz) proces-
sors) of Ucloud [34]. The four-layer architecture involv-
ing DATABASE, ALGORITHM, TOOLS, and VIEW
layer is shown in Fig. 1 The detailed description of the
architecture is fixed as following.
(1) DATABASE layer. This layer stores DO, disease-
related genes, and functional associations between genes.
These are exploited by ALGORITHM layer for calculat-
ing the similarity between disease sets.
(2) ALGORITHM layer. Five algorithms of measuring
the similarity between DO terms have been imple-
mented, which include Resniks, Lins, Wangs, PSB, and
SemFunSim methods. And the method named PWBPA
for calculating the SSDS were also implemented.
(3) TOOL layer. Two tools including PairSim and
BatchSim have been provided for exploring the SSDS.
PairSim calculates the similarity for a given pair of
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 21 of 79
disease sets, and BatchSim computes similarity between
each pair of multiple disease sets.
(4) VIEW layer. Web pages are provided for viewing the
results. It shows the similarity of pair-wise disease sets.
Results
Web interface
DisSetSim provides two tools PairSim and BatchSim for
querying the SSDS. The details about the usage of these
two tools are described as follows.
The usage of PairSim
Figure 2a shows a case for searching the similarity score
of a given pair of disease sets. The web page for input-
ting disease sets is http://www.bio-annotation.cn:8080/
DisSetSim/ basic-init. Each of these disease sets could be
inputted in a textbox. A disease set is comprised by
several diseases. And each disease is represented by the
identifier of term in DO. All the term identifiers could
be downloaded from the hyperlink disease terms in the
inputting page. Here, we click the example button to
use our example. Then, we choose one of the five
methods (Resniks, Lins, Wangs, PSB, and SemFunSim)
for calculating the SSD. After submitting this pair of
disease sets, the system could return the similarity score
based on the PWBPA method.
The usage of BatchSim
Figure 2b shows a case for searching the similarity score
of all the pairs based on the selected files. The web page
for inputting disease sets is http://www.bio-annota-
tion.cn:8080/DisSetSim/batch-init. Two files including
disease sets should be selected before submitting. The
file should be a plain text which contains several disease
sets. Each disease set must be in a newline, and each dis-
ease set contains several disease IDs which are separated
by commas. The size of uploaded file must be <2 Mb.
Here, we selected our example file in this page. Then,
we choose one of the five methods for calculating the
SSD. After clicking the submit button, the system could
return the similarity score of all the pairs of the selected
files based on the PWBPA method.
miRNA functional similarity network
By applying DisSetSim to the inputted disease sets of
miRNAs, the similarity score of each miRNA pair could
be obtained. Using miRNA as node and similar miRNAs
as edge, the MFSN was constructed based on various
similarity cutoffs. As shown in Fig. 3a, the number of links
dramatically decreases when the cutoff increases
from low value to high value. When the cutoff is
equal to or bigger than 0.7, the link numbers remain
relatively stable. Therefore, we use 0.7 as cutoff for
the MFSN. In total, 1042 miRNA-miRNA functional
associations between 346 miRNAs were obtained as
MFSN (Fig. 3c). Similar to the most of the reported
biological networks, the degree of this MFSN also shows a
scale-free distribution [5, 9, 3537]. It means that most of
the miRNAs only have a few functionally similar miRNAs,
and a few of miRNAs have a numerous functional similar
miRNA (Fig. 3b).
Here, the PWBPA method was utilized for calculat-
ing similarity between disease sets, and SemFunSim
was used as computing the similarity of pair-wise dis-
eases. This is because that the SemFunSim method
was proven to obtain the best performance [14]. Alterna-
tively, other state-of-art methods could also be chosen to
construct MFSN.
Resnik
ALGORITHM
DATABASE
TOOL
Disease Ontology Disease-related genes
VIEW
Lin Wang PSB SemFunSim
PWBPA
Functional interaction 
between genes
PairSim BatchSim
Fig. 1 System overview of DisSetSim
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 22 of 79
Disease-related miRNAs
By applying the above similarity scores of miRNAs,
novel disease-related miRNAs were predicted based on
RWR algorithm (See Methods section). To evaluate
the performance of the similarity scores of miRNAs,
leave-one-out cross validation of 5710 known experi-
mentally confirmed miRNA-disease associations, in-
cluding 265 diseases with at least two miRNAs, were
used for this assessment. For a disease of interest,
each known miRNA of this disease was left out as
the testing case, and the remaining miRNAs of this
disease were used as seed nodes. All the miRNAs
except the miRNAs of this disease were considered as
candidate miRNAs. We then examined how well the
testing miRNA ranked relative to the candidate miR-
NAs. If the ranking of this testing miRNA exceeded a
given cutoff, we regarded this miRNA-disease associ-
ation as successfully predicted. As a result, an area
under the ROC curve (AUC) of 0.9296 was achieved
(Fig. 4), which demonstrated that our miRNA func-
tional similarity was effective in recovering known
experimentally confirmed disease-related miRNAs.
Discussion
As the best of our knowledge, non-coding RNAs
(ncRNAs) attract more and more attentions because
of their important regulation roles in molecular level.
However, the lack of protein limits the identification
of their function. Here the application of our tool in
constructing MFSN and predicting miRNA-disease
associations provides a novel way to help for explor-
ing the function of miRNAs especially for prioritizing
A
B
Fig. 2 Schematic workflow of DisSetSim. a Schematic workflow of PairSim. b Schematic workflow of BatchSim
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 23 of 79
miRNA-disease associations. This application can be
extended to other ncRNAs, such as lncRNAs and
circRNAs. Although methods for calculating the SDS
have been implemented by previous methods, it is
not easy to calculate the SSDS. Therefore, DisSetSim
benefits researchers for exploring the function of
disease-related molecular.
Conclusions
In this article, we designed and developed a web system
DisSetSim to calculate the SSDS. Five state-of-art
methods were implemented (see METHODS section)
for calculating disease similarity. And the PWBPA
method was implemented for calculating the SSDS. Two
tools involving PairSim and BatchSim provide the
function to obtain the SSDS by inputting a pair-wise
disease sets and multiple disease sets, respectively.
The functional similarity of miRNAs could be calcu-
lated based on our system. Here, the similarity of each
pair-wise miRNAs was calculated. And then a MFSN
was constructed based on miRNA similarity. The
network was further utilized to predicate disease-related
miRNAs based on RWR. The high AUC (0.9296) shows
the MFSN is very suitable for predicting potential rela-
tionships between diseases and miRNAs.
Abbreviations
CTD: Comparative Toxicogenomics Database; DO: Disease Ontology; GAD: Genetic
Association Database; GeneRIF: Gene Reference into Function; GO: Gene Ontology;
GOA: GO annotation; IC: information content; MFSN: miRNA functional similarity
network; MICA: the most informative common ancestor; ncRNA: non-coding RNAs;
OMIM: Online Mendelian Inheritance in Man; PSB: process-similarity based;
PWAPM: pair-wise-all pairs-maximum; PWBPA: pair-wise-best pairs-average;
PWR: random walk with restart; SD: pair-wise diseases; SSD: the similarity score of
pair-wise diseases; SSDS: the similarity score of pair-wise disease sets.
Acknowledgments
Yadong Wang and Liang Cheng are the corresponding authors. Lingling
Zhao and Zhiyan Liu are the co-first authors.
Funding
This work was supported by the Major State Research Development Program
of China [No. 2016YFC1202302], the National Natural Science Foundation of
China (Grant No. 61502125, $2000), Heilongjiang Postdoctoral Fund (Grant
No. LBH-Z15179, $800), and China Postdoctoral Science Foundation (Grant
No. 2016 M590291, $1000).
Availability of data and materials
The system can be accessed from http://www.bio-annotation.cn:8080/DisSetSim/.
Fig. 3 Construction and characteristics of the miRNA functional similarity network. a Cumulative distribution of the edges between miRNAs when
using various similarity cutoffs. b Degree distribution for miRNA in the miRNA functional similarity network. c The miRNA functional similarity network
Fig. 4 ROC curve of the PWBPA method based on leave-one-out cross
validation on known experimentally verified miRNA-disease associations
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):28 Page 24 of 79
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Authors contributions
YH, LZ and ZL implemented the first version of the online system. HJ, HS, PX
updated the system. YW and LC wrote the manuscript. All authors read and
approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Author details
1Harbin Institute of Technology, School of Life Science and Technology,
Harbin 150001, Peoples Republic of China. 2Department of Computer
Science and Technology, Harbin Institute of Technology, Harbin 150001,
Peoples Republic of China. 3Department of information engineering,
Heilongjiang Biological Science and Technology Career Academy, Harbin
150001, Peoples Republic of China. 4College of Bioinformatics Science and
Technology, Harbin Medical University, Harbin 150001, Peoples Republic of
China.
Published: 20 September 2017
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 
DOI 10.1186/s13326-017-0163-8
RESEARCH Open Access
Identifying genotype-phenotype
relationships in biomedical text
Maryam Khordad* and Robert E. Mercer
Abstract
Background: One important type of information contained in biomedical research literature is the newly discovered
relationships between phenotypes and genotypes. Because of the large quantity of literature, a reliable automatic
system to identify this information for future curation is essential. Such a system provides important and up to date
data for database construction and updating, and even text summarization. In this paper we present a machine
learning method to identify these genotype-phenotype relationships. No large human-annotated corpus of
genotype-phenotype relationships currently exists. So, a semi-automatic approach has been used to annotate a small
labelled training set and a self-training method is proposed to annotate more sentences and enlarge the training set.
Results: The resulting machine-learned model was evaluated using a separate test set annotated by an expert. The
results show that using only the small training set in a supervised learning method achieves good results (precision:
76.47, recall: 77.61, F-measure: 77.03) which are improved by applying a self-training method (precision: 77.70, recall:
77.84, F-measure: 77.77).
Conclusions: Relationships between genotypes and phenotypes is biomedical information pivotal to the
understanding of a patients situation. Our proposed method is the first attempt to make a specialized system to
identify genotype-phenotype relationships in biomedical literature. We achieve good results using a small training set.
To improve the results other linguistic contexts need to be explored and an appropriately enlarged training set is
required.
Keywords: Genotypes, Phenotypes, Genotype-phenotype relationship, Semi-automatic corpus annotation,
Self-training, Computational linguistics
Background
Many research experiments are being performed to dis-
cover the role of DNA sequence variants in human health
and disease and the results of these experiments are pub-
lished in the biomedical literature. An important category
of information contained in this literature is the newly
discovered relationships between phenotypes and geno-
types. Experts want to know whether a disease is caused
by a genotype or whether a certain genotype determines
particular human characteristics. This information is very
valuable for researchers, clinicians, and patients. There
exist some manually curated resources such as OMIM [1]
which are repositories for this information, but they do
not provide complete coverage of all genotype-phenotype
*Correspondence: mkhordad@alumni.uwo.ca
Department of Computer Science, University of Western Ontario, 1151
Richmond Street, N6A 5B7 London, Canada
relationships. Because of the large quantity of literature
possessing this information, a reliable automatic system
to identify these relationships for future curation is desir-
able. Such a system provides important and up to date data
for database and ontology construction and updating, and
even for text summarization.
Related work
Identifying relationships between biomedical entities by
analyzing only biomedical text
Finding the relationships between entities from infor-
mation contained in the biomedical literature has been
studied extensively and many different methods to
accomplish these tasks have been proposed. Generally,
current approaches can be divided into three types:
Computational linguistics-based (e.g., [24]), rule-based
(e.g., [5, 6]), and machine learning and statistical methods
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 2 of 16
(e.g., [7, 8]). Furthermore some systems (e.g., [911]) have
combined these approaches and have proposed hybrid
methods.
RelEx [10] makes dependency parse trees from the text
and applies a small number of simple rules to these trees
to extract protein-protein interactions. Leroy et al. [12]
develop a shallow parser to extract relations between enti-
ties from abstracts. The type of these entities has not been
restricted. They start from a syntactic perspective and
extract relations between all noun phrases regardless of
their type. SemGen [9] identifies and extracts causal inter-
action of genes and diseases from MEDLINE citations.
Texts are parsed using MetaMap. The semantic type of
each noun phrase tagged by MetaMap is the basis of this
method. Twenty verbs (and their nominalizations) plus
two prepositions, in and for, are recognized as indicators
of a relation between a genetic phenomenon and a disor-
der. Sekimizu et al. [2] use a shallow parser to find noun
phrases in the text. The most frequently seen verbs in the
collection of abstracts are believed to express the relations
between genes and gene products. Based on these noun
phrases and frequently seen verbs, the subject and object
of the interaction are recognized.
Coulet et al. [4] propose a method to capture phar-
macogenomics (PGx) relationships and build a semantic
network based on relations. They use lexicons of PGx
key entities (drugs, genes, and phenotypes) from Phar-
mGKB [13] to find sentences mentioning pairs of key
entities. Using the Stanford parser [14] these sentences
are parsed and their dependency graphs1 are produced.
According to the dependency graphs and two patterns,
the subject, object, and the relationship between them
are extracted. This research is probably the closest to
the work presented here, the differences being that the
method to find relationships is rule-based and the enti-
ties of interest include drugs. Direct comparison with our
results is difficult because the genotype-phenotype rela-
tionships with their associated precision and recall values
are not presented separately. Temkin and Gilder [3] use
a lexical analyzer and a context free grammar to make
an efficient parser to capture interactions between pro-
teins, genes, and small molecules. Yakushiji et al. [15]
propose a method based on full parsing with a large-scale,
general-purpose grammar.
The BioNLP module [5] is a rule-based module which
finds protein names in text and extracts protein-protein
interactions using pattern matching. Huang et al. [6] pro-
pose a method based on dynamic programming [16] to
discover patterns to extract protein interactions. Katrenko
and Adriaans [8] propose a representation based on
dependency trees which takes into account the syntac-
tic information and allows for using different machine
learning methods. Craven [7] describes two learning
methods (Naïve Bayes and relational learning) to find
the relations between proteins and sub-cellular struc-
tures in which they are found. The Naïve Bayes method
is based on statistics of the co-occurrence of words.
To apply the relational learning algorithm, text is first
parsed using a shallow parser. Marcotte et al. [17]
describe a Bayesian approach to classify articles based
on 80 discriminating words, and to sort them accord-
ing to their relevance to protein-protein interactions.
Bui et al. [11] propose a hybrid method for extracting
protein-protein interactions. This method uses a set of
rules to filter out some PPI pairs. Then the remaining
pairs go through a SVM classifier. Stephens et al. [18],
Stapley and Benoit [19], and Jenssen et al. [20] discuss
extracting the relation between pairs of proteins using
probability scores.
Supervised learning approaches have been used to rec-
ognize concepts of prevention, disease, and cure and
relations among these concepts. Work using a standard-
ized annotated corpus beginning with Rosario and Hearst
[21] and continuing with the work of Frunza and Inkpen
[22, 23] and Abacha and Zweigenbaum [24, 25] has seen
good performance progress.
An approach to extract binary relationships between
food, disease, and gene named entities by Yang et al. [26]
has similarities to the work presented here because it is
verb-centric.
Most of the biomedical relation extraction systems
focus on finding relations between specific types of named
entities. Open Information Extraction (OIE) systems aim
to extract all the relationships between different types of
named entities. TextRunner [27], ReVerb [28], and OLLIE
[29] are examples of OIE systems. They first identify
phrases containing relations using part-of-speech patterns
and syntactic and lexical constraints, and then with some
heuristics detect related named entities and relation verbs.
PASMED [30] extracts diverse types of binary relations
from biomedical literature using deep syntactic patterns.
Advanced OIE systems [31, 32] have been proposed to
extract nominal and n-ary relations.
Increasing interest in neural network models, such as
deep [33], recurrent [34], and convolutional [35] net-
works, and their applications to Natural Language Pro-
cessing, such as word embeddings [36] have provided
a new set of techniques for relationship identification,
some which deal with relationships of a general nature,
such as Miwa and Bansal [37], and some which deal
with biomedical relationships, such as Jiang et al. [38].
Our method is a more traditional pipeline method
identifying genotypes and phenotypes, and then using
surface, syntactic, and dependency features to identify
the relationships. So, rather than developing an exten-
sive overview of these neural network models, we instead
point the reader to Liu et al.s excellent summary of these
methods [39].
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 3 of 16
Identifying genotype-phenotype relationships using
biomedical text and/or other curated resources
The research works mentioned in the previous section
have been highlighted because they are concerned with
identifying various relations among biomedical entities by
analyzing only the natural language context in whichmen-
tions of these relations and entities are immersed. There is
a vast literature presenting research focussed specifically
on the genotype-phenotype relation. Most of this research
presents the discovery of novel genotype-phenotype rela-
tions based on biomedical evidence and is beyond the
intent of this paper and would be out of place to be sur-
veyed here. Incidentally, it is this type of literature that we
are interested in mining to extract genotype-phenotype
relationships.
While not finding genotype-phenotype relationships,
many research works are concerned with a related
question: disease-gene relationships. One of the earli-
est works in this area is that of Doughty et al. [40]
which provides an automated method to find cancer-
and other disease-related point mutations. The method of
Singhal et al. [41] to find disease-gene-variant triplets
in the biomedical literature makes strong use of a num-
ber of modern natural language tools to analyze the text
in which these triplets reside, but this method also uses
information mined from all of the PubMed abstracts, the
Web, and sequence analysis which requires the use of a
manually curated database. Another research work that
investigates gene variants and disease relationships is that
of Verspoor et al. [42]. Another work that investigates
mutation-disease associations is Mahmood et al. [43].
A recent review of algorithms identifying gene-disease
associations using techniques based on genome variation,
networks, text mining, and crowdsourcing is provided by
Opap and Mulder [44].
Other literature reports on techniques to extract
genotype-phenotype relationships combining biomedical
text mining with a variety of other resources. An exam-
ple of this type of technique is the pioneering work of
Korbel et al. [45]. Being the first to use evidence from
biomedical literature, it uses the correlation of gene and
phenotype mentions in the text together with compar-
ative genome analysis that depends on a database of
orthologous groups of genes to provide gene-phenotype
relationship candidates. Novel relationships that were not
mined directly from the text are reported. Another type
of technique, exemplified by the work of Goh et al. [46]
is the integration of curated databases to find genotype-
phenotype relationship candidates.
A work by Bokharaeian et al. [47] which is very close
to the research presented here uses two types of Sup-
port Vector Machines for their learning method and the
type of relationship being identified is between single-
nucleotide polymorphisms (SNPs) and phenotypes. This
work presents three types of association (positive, nega-
tive, and neutral) and three levels of confidence (weak,
moderate, and strong).
In each of the referred to works, either the presentation
of the genotype-phenotype relationship is complicated by
being part of a larger relationship, such as in the work
of Coulet et al. [4], or the method to suggest the rela-
tionship requires information found in manually curated
databases, such as the works of Korbel et al. [45], Goh
et al. [46], and Singhal et al. [41]. Our work then stands out
by being different on each of these fronts: we identify only
the genotype-phenotype relationships and we use only the
text in the PubMed abstract being analyzed. Also, we are
not attempting to find new relationships, rather we are
only mining those relationships that occur in the abstract.
In addition, we are using a machine learning method that
requires human annotated data. We view the method pro-
vided in this paper as complementing these othermethods
in the ways just described.
Briefly then, in this paper we discuss a semi-supervised
learningmethod for identifying genotype-phenotype rela-
tionships from biomedical literature.We start with a semi-
automatic method for creating a small seed set of labelled
data by applying two named entity relationship tools [48]
to an unlabelled genotype-phenotype relationship dataset.
This initially labelled genotype-phenotype relationship
dataset is thenmanually cleaned. Then using this as a seed
in a self-training framework, a machine learned model is
trained. It is worth noting that throughout this paper we
do not take into account the phenotypes at the subcellular
level. The evaluation results are reported using precision,
recall and F-measure derived from a human-annotated
test set. Precision (or positive predictive value) is the ratio
of correct relationships in all relationships found and can
be seen as a measure of soundness. Recall (or sensitiv-
ity) is the ratio of correct relationships found compared
to all correct relationships in the corpus and can be used
as a measure of completeness. F-measure combines pre-
cision and recall as the harmonic mean of these two
numbers.
Semi-supervised learning
To train machine learning systems, it is easier and cheaper
to obtain unlabelled data than labelled data. Semi-
supervised learning is a bootstrapping method which
incorporates a large amount of unlabelled data to improve
the performance of supervised learning methods which
lack sufficient labelled data.
Much of the semi-supervised learning in Computational
Linguistics uses the iterative bootstrapping approach, ini-
tially proposed by Riloff and Shepherd [49] for building
semantic lexicons, which later evolved into the learning
of multiple categories [50]. These methods have further
transformed to the semi-supervised learning of multiple
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 4 of 16
related categories and relations as a method to enhance
the learning process [51].
Instead of using this category of semi-supervised learn-
ing, we use a methodology called self-training. Ng and
Cardie [52] proposed this type of semi-supervised learn-
ing to combat semantic drift [53, 54], a problem with
the bootstrapped learning of multiple categories. They
used bagging and majority voting in their implementa-
tion. A set of classifiers get trained on the labelled data,
then they classify the unlabelled data independently. Only
those predictions which have the same label by all clas-
sifiers are added to the training set and the classifiers
are trained again. This process continues until a stop
condition is met. For Clark et al. [55] a model is sim-
ply retrained at each iteration on its labelled data which
is augmented with unlabelled data that is classified with
the previous iterations model. According to this sec-
ond method, there is only one classifier which is trained
on labelled data. Then the resulting model is used to
classify the unlabelled data. The most confident predic-
tions are added to the training set and the classifier is
retrained on this new training set. This procedure repeats
for several rounds. We adopt this latter methodology in
our work.
Rule-based andmachine learning-based named entity
relationship identification tools
Ibn Faiz [48] proposed a general-purpose software
tool for mining relationships between named entities
designed to be used in both a rule-based and a machine
learning-based configuration. This tool was originally
tailored to recognize pairs of interacting proteins and
has been reconfigured here for the purpose of iden-
tifying genotype-phenotype relationships. Ibn Faiz [48]
extended the rule-based method of RelEx [10] for iden-
tifying protein-protein interactions. In this method the
dependency tree of each sentence is traversed according
to some rules and various candidate dependency paths are
extracted.
This extended method is able to detect the more general
types of relationships found between named entities in
biomedical text. For example the rule-based system is able
to find relationships with the following linguistic patterns,
where PREP is any preposition, REL is any relationship
term, and N is any noun:
 ENTITY1 REL ENTITY2; e.g., GENOTYPE causes
PHENOTYPE
 Relations in which the entities are connected by one
or more prepositions:
 ENTITY1 REL (of | by | to | on | for | in |
through | with) ENTITY2; e.g., PHENOTYPE is
associated with GENOTYPE
 (PREP | REL | N)+ (PREP)(REL | PREP | N)*
ENTITY1 (REL | N | PREP)+ ENTITY2; e.g.,
expression of PHENOTYPE by GENOTYPE
 REL (of | by | to | on | for | in | through | with |
between) ENTITY1 and ENTITY2, e.g.,
correlation between GENOTYPE and
PHENOTYPE.
 ENTITY1 (/ | \ | ?) ENTITY2; e.g.,
GENOTYPE/PHENOTYPE correlation.
In addition to the linguistic patterns this method requires
a good set of relationship terms. To find protein-protein
interaction relationships, a list of interaction terms (a
combination of lists from RelEx [10] and Bui et al. [11])
was used by Ibn Faiz to elicit protein-protein interactions.
In the work reported below an appropriate set of relation-
ship terms for genotype-phenotype relationships has been
developed and used in the rule-based system to recognize
this type of relationship.
Ibn Faiz [48] also used his general-purpose tool in a
machine learning approach using a maximum entropy
classifier and a set of relationship terms appropriate for
identifying protein-protein interactions. This approach
considers the relationship identification problem as a
binary classification task. The Stanford dependency
parser produces a dependency tree for each sentence. For
each pair of named entities in a sentence, proteins in
this case, the dependency path between them, the parse
tree of the sentence, and other features are extracted.
These features include: dependency features coming from
the dependency representation of each sentence, syntactic
features, and surface features derived directly from the raw
text (the relationship terms and their relative position).
The extracted features along with the existence of a rela-
tionship between named entity pairs in a sentence make a
feature vector. A machine learning model is trained based
on the positive (a relationship exists) and negative (a rela-
tionship does not exist) examples. To avoid sparsity and
overfitting problems, feature selection is used. Because
the maximum entropy classifier and the linguistic depen-
dency and syntactic features are the common foundation
for this technique, only an appropriate set of relationship
terms need to be provided for genotype-phenotype rela-
tionship identification. In the work reported below, the
same set of relationship terms as used in the rule-based
approach are used in the machine-learning approach.
Methods
A block diagram showing the complete workflow is pro-
vided in Fig. 1. Details of this workflow are presented in
the following.
Curating the data
As mentioned before we did not have access to any
data prepared specifically for the genotype-phenotype
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 5 of 16
Fig. 1Workflow
relationship identification task, so our first task was to
collect a sufficient number of sentences containing phe-
notype and genotype names that include both genotype-
phenotype relationships and non-relationships. Three
sources of data have been used in this project:
 Khordad et al. [56] generated a corpus for the
phenotype name recognition task. This corpus is
comprised of 2971 sentences from 113 full papers. It
is designated as the MKH corpus henceforth.
 PubMed was queried for genotype and phenotype
and correlation and 5160 abstracts were collected.
 Collier et al. [57] generated and made available to us
the Phenominer corpus which contains 112 PubMed
abstracts. Both phenotypes and genotypes are
annotated in this corpus, but not their relationships.
The annotation was carried out with the same
experienced biomedical annotator who accomplished
the GENIA corpus [58] tagging. Phenominer
contains 1976 sentences with 1611 genotypes and 472
phenotype candidates. However, there are two issues
with this corpus:
 The phenotypes at the cellular level are
labelled in the Phenominer corpus. Our work
on genotype-phenotype relationships does not
consider this type of phenotype because the
linguistic context is different from
relationships involving the non-cellular level
phenotypes.
In all of the steps explained below, this type of
phenotype is included. We report precision,
recall, and F-measure with and without this
type of phenotype involved in
genotype-phenotype relationships labelled in
the test set.
 Generic expressions (e.g., gene, protein,
expression) referring to a genotype or a
phenotype earlier in the text are tagged in this
corpus as genotypes and phenotypes. For
example locus is tagged as a genotype in the
following sentence: Our original association
study focused on the role of IBD5 in CD; we
next explored the potential contribution of
this locus to UC susceptibility in 187
German trios.
The work reported here only considers
explicitly named genotypes and phenotypes.
Thus, including these examples will have a
slightly negative effect on the trained model
and any relationships that include entities that
are named implicitly will not be identified in
the test set, reducing the precision and recall
slightly.
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 6 of 16
Genotype and phenotype names were already anno-
tated in the third resource and phenotypes were already
annotated in the first resource. So, we had to annotate
genotypes in the first resource and genotypes and pheno-
types in the second resource. BANNER [59], a biomedical
NER system, has been used to annotate the genotype
names and an NER system specialized in phenotype name
recognition [56] has been used to annotate the phenotype
names. Only sentences with both phenotype and genotype
names have been selected from the above resources to
comprise our data and the remaining sentences have been
ignored. In this way, we have collected 460 sentences from
the MKH corpus, 3590 sentences from the PubMed col-
lection and 207 sentences from Phenominer. These 4257
sentences comprise our initial set of sentences. All the
sentences are represented by the IOB label model (Inside,
Outside, Beginning). The phenotype names and genotype
names are tagged by their token offset from the beginning
of each sentence because they can occur multiple times in
a sentence.
Training set
At the beginning of the project we did not have any
labelled data. Instead of using annotators knowledge-
able in biomedicine to label a sufficiently large corpus of
biomedical literature, we decided instead to use the previ-
ously described relationship identification tools modified
to work with our data and use their agreed upon out-
puts, cleaned by a non-expert, as our labelled training set.
This methodology has allowed us to partially evaluate this
method of semi-automatic annotation.
As mentioned previously, the rule-based and machine
learning-based systems for identifying biomedical rela-
tionships have been appropriately tailored to this task by
supplying a set of genotype-phenotype relationship words
that are appropriate for identifying this type of biomed-
ical relationship. This set of relationship words includes
a list of 20 verbs and two prepositions (in and for) from
Rindflesch et al. [9] which encode a relationship between
a genetic phenomenon and a disorder and the PPI rela-
tionship terms from Ibn Faizs work [48] which we found
to apply also to genotype-phenotype relationships.2
Our initial corpus is separately processed by the
rule-based and the machine learning-based relationship
identification tools. Each of these tools find some rela-
tionships in the input sentences. After the results are
compared, those sentences that contain at least one agreed
upon relationship3 are initially considered as the train-
ing set. From the original corpus, 519 sentences com-
prised the initial training set as the result of this process.
However, as these tools have been developed as general
named entity relationship identifiers, we could not be
certain that even their similar results produce correctly
labelled examples. Therefore, the initial training set was
further processed manually. Some interesting issues were
observed.
1. Some sentences do not state any relationship
between the annotated phenotypes and genotypes.
Instead, these sentences only explain the aim of a
research project. However, these sentences are
labelled as containing a relationship by both tools;
e.g., The present study was undertaken to
investigate whether rare variants of TNFAIP3 and
TREX1 are also associated with systemic sclerosis.
2. The negative relationships stated with the word no
are considered positive by both tools; e.g., With the
genotype/phenotype analysis , no correlation in
patients with ulcerative colitis with the MDR1 gene
was found.
3. Some sentences from the Phenominer corpus are
substantially different compared to other sentences,
because of the two issues we discussed earlier about
this corpus. The phenotypes below the cellular level
have different relationships with genotypes. For
example, they can change genotypes while the
supercellular-level phenotypes are affected by
genotypes and are not capable of causing any change
to them.
4. Some cases have both tools making the same
mistakes: suggesting incorrect relationships (i.e.,
negative instances are suggested as positive
instances) or missing relationships (i.e., positive
instances are given as negative instances).
After making corrections (see issues 2 and 4) and delet-
ing sentences exhibiting issues 1 and 3, 430 sentences
remained in the training set. These corrections and dele-
tions were made by the first author. To increase the train-
ing set size, 39 additional sentences have been labelled
manually and have been added to the training set. The
data set is skewed: there are few negative instances. To
address this imbalance, 40 sentences without any relation-
ships have been selected manually and have been added to
the training set. As shown in Table 3, the final training set
has 509 sentences. There are 576 positive instances and
269 negative instances.
Test set
To ensure that the training set and the test set are inde-
pendent, the test set is chosen from the initial set with
the training set sentences removed. To select the sen-
tences to be included in the test set, the results from
processing our initial set with the two general purpose
relationship identification tools have been used. In some
cases both tools identify relationships from the same sen-
tence but the relationships differ. For example in sentence
Common esr1 gene alleles-4 are unlikely to contribute
to obesity-10 in women, whereas a minor importance of
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 7 of 16
esr2-19 on obesity-21 cannot be excluded. the machine
learning-based tool finds a relationship between esr2-19
and obesity-21 but the rule-based tool claims that there is
also a relationship between esr1 gene alleles-4 and obesity-
10. Since we were confident that this type of sentence
would provide a rich set of positive and negative instances,
this type of sentence is extracted to make our initial test
set of 298 sentences.
In order for the test set to provide a reasonable evalua-
tion of the trained model, the sentences must be correctly
labelled. A biochemistry graduate student was hired to
annotate the initial test set. Pairs of genotypes and pheno-
types are extracted from each sentence and her task was to
indicate whether there is any relationship between them.
Issues 1 and 3 discussed in the previous section have
been observed by the annotator in some of the sentences.
Also, there are some cases where she is not sure if there
is a relationship or not. Furthermore, she disagreed with
the phenotypes and genotypes annotated in 54 sentences.
After deleting these 54 problematic sentences the final test
set comprises 244 sentences (which contain 536 positive
instances and 287 negative instances). See Table 3.
Unlabelled data
After choosing the training and testing sentences from the
initial set of sentences, the remaining sentences have been
used as unlabelled data. The unlabelled set contains 3440
sentences. A subset of these (408 sentences containing 823
instances which approximates the number found in the
original training set) are used in the self-training step4.
Training a model with the machine learning method
Now that we have a labelled training set, it is pos-
sible to train a model using a supervised machine
learning method to be evaluated on the test set. We
have applied the maximum entropy classifier devel-
oped for relationship identification (described above) [48]
for our genotype-phenotype relationship identification
application. A genotype-phenotype pair is represented by
a set of features derived from a sentence. Tables 1 and 2
provide the list of features.
Dependency parse trees can contain important infor-
mation in the dependency path between two named enti-
ties. Figure 2 shows the dependency tree produced by
the Stanford dependency parser5 for the sentence The
association of Genotype1 with Phenotype2 is confirmed..
The dependency path between the phenotype and the
genotype is Genotype1-prep_of -association-prep_with-
Phenotype2. Association is the relationship term in this
path and prep_of and prep_with are the dependency rela-
tionships related to it. The presence of a relationship term
can be a signal for the existence of a relationship and
its grammatical role along with its relative position gives
valuable information about the entities involved in the
relationship. Sometimes two entities are surrounded by
more than one relationship term. Key term is introduced
to find the relationship term which best describes the
interaction. Ibn Faiz [48] used the following steps to find
the key term: when one step fails the process continues to
the next step, but if the key term is found in one step the
following steps are ignored.
Table 1 List of dependency features
Features Description
Relationship term Root of the portion of the dependency tree connecting pheno-
type and genotype
Stemmed relationship term Stemmed by MALLET
Relative position of relationship term Whether it is before the first entity, after the second entity or
between them
The relationship term combinedwith the dependency relation-
ship
To consider the grammatical role of the relationship term in the
dependency path.
The relationship term and its relative position
Key term Described in Ibn Faizs four step method [48]
Key term and its relative position
Collapsed version of the dependency path All occurrences of nsubj/nsubjpass are replaced with subj,
rcmod/partmod with mod, prep x with x and everything else
with O, a placeholder to indicate that a dependency has been
ignored.
Second version of the collapsed dependency path Only the prep_* of dependency relationships are kept.
Negative dependency relationship A binary feature that shows whether there is any node in
the path between the entities which dominates a neg depen-
dency relationship. This feature is used to catch the negative
relationships.
prep_between A binary feature that checks for the existence of two consecu-
tive prep_between links in a dependency path.
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 8 of 16
Table 2 List of syntactic and surface features
Features Description
Syntactic features
Stemmed version of relationship term in the Least Common Ancestor
(LCA) node of the two entities
If the head6 of the LCA node of the two entities in the syntax tree is a
relationship term then this feature takes a stemmed version of the head
word as its value, otherwise it takes a NULL value.
The label of each of the constituents in the path between the LCA and
each entity combined with its distance from the LCA node
Surface features
Relationship terms and their relative positions The relationship terms between two entities or within a short distance (4
tokens) from them.
1. Any relationship term that occurs between the
entities and dominates them both in the dependency
representation is considered to be the key term.
2. A word is found that appears between the entities,
dominates the two entities, and has a child which is a
relationship term. That child is considered to be the
key term.
3. Any relationship term that occurs on the left of the
first entity or on the right of the second entity and
dominates them both in the dependency
representation is considered to be the key term.
4. A word appears on the left of the first entity or on the
right of the second entity, dominates the two entities,
and has a child which is a relationship term. That
child is considered to be the key term.
Self-training algorithm
The first model is trained using the training set and the
machine learning method described earlier. To improve
the performance of our model, a self-training process
has been applied. Figure 3 outlines this process. This
process starts with the provided labelled data and unla-
belled data. The labelled data is used to train a model
which is used to tag the unlabelled data. In most self-
training algorithms the instances with the highest confi-
dence level are selected to be added to the labelled data.
However, as has been observed in some self-training algo-
rithms, choosing the most confident unlabelled instances
and adding them to the labelled data can cause overfitting
[60]. We encountered a similar overfitting when we added
themost confident unlabelled instances. So we considered
the following two measures to select the best unlabelled
instances.
 The confidence level must be in an interval. It must
be more than a threshold ? and less than a specified
value ? .
 The predicted value of the selected instances must be
the same as their predicted value by the rule-based
system.
In each iteration an at most upper-bounded number of
instances are selected and added to the labelled data to
prevent adding lots of incorrectly labelled data to the
training set in the first iterations when the model is not
powerful enough to make good predictions.
We used relationship identification output from the
PPI-tailored rule-based tool as an added level of conser-
vatism in the decision to add an unlabelled instance to
the training set. It has only moderate performance on
genotype-phenotype relationship identification. So, using
this tools advice along with the confidence level means
that the relationship must be of a more general nature
than just genotype-phenotype relationships. However, at
some point this conservatism holds the system back from
learning broader types of relationships in the genotype-
phenotype category. Therefore this selection factor is used
only for the first i iterations, and after i iterations the best
Fig. 2 Dependency tree related to the sentence The association of Genotype1 with Phenotype2 is confirmed
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 9 of 16
Fig. 3 The self training process
unlabelled data is chosen based only on the confidence
level. Again, here, the confidence level must be in an
interval.
This proposed self-training algorithm has been tried
with various configurations and each variable in this pro-
cess has been given several values. Each resulting model
has been tried separately with our test set and the best sys-
tem is selected based on its performance on the test set. In
our best configuration 15 unlabelled instances are added
to the labelled data in each iteration, in the first 5 itera-
tions predictions made by the rule-based system are taken
into account, the least confidence level is 85%, the high-
est confidence level is 92% and the process stops after 6
iterations.
Results and discussion
The proposed machine-learned model has been evalu-
ated using the separate test set manually annotated by
a biochemistry graduate student. The distribution of our
data (number of sentences and number of genotype-
phenotype pairs in each set) is illustrated in Table 3. The
numbers of positive instances and negative instances in
the unlabelled data are not available.
Table 4 shows the results obtained by the super-
vised learning algorithm and the proposed self-training
algorithm. The results of testing Ibn Faizs rule-based and
Table 3 Distribution of data in our different sets
Data set Sentences Instances Positive
instances
Negative
instances
Training set 509 845 576 269
Test set 244 823 536 287
Unlabelled data 408 823 N/A N/A
machine learning-based relationship identification tools
[48] originally configured to find protein-protein inter-
actions have been included in the table for comparison
purposes. Although these tools were not configured to
be used for our application, as can be seen in the table,
the PPI-configured tools, especially the rule-based system,
have good precisions. This performance by the rule-based
system led us to consider the rule-based predictions as
one factor in choosing which unlabelled data to add to the
labelled data. The recalls of the PPI-configured tools are
quite low as one would expect. The precision results mean
that there are some linguistic structures that are common
between protein-protein and genotype-phenotype rela-
tionships and these structures are useful for distinguishing
correct from incorrect relationship candidates.The low
recall values indicate there are some genotype-phenotype
relationship contexts which are specific to this type of
relationship and the relation terms used to configure the
general purpose relationship tools are key to finding these
relationships.
As illustrated in Table 4, we get good performance by
using a small initial training set and then we are able
to gain a modest improvement by using our proposed
self-training algorithm. The initial results with the
small training set were: precision: 76.47, recall: 77.61,
F-measure: 77.03. The self-training algorithm gave the
Table 4 Evaluation results
Method Precision Recall F-measure
Supervised learning method 76.47 77.61 77.03
Self-training method 77.70 77.84 77.77
PPI-configured ML-based tool 75.19 53.17 62.29
PPI-configured rule-based tool 77.77 38.04 51.09
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 10 of 16
following results: precision: 77.70, recall: 77.84, F-
measure: 77.77. The self-training step provided only
slightly more than 10% extra training examples (90 rela-
tionship instances added to the original 845 instances), so
the modest performance improvement is not unexpected.
The following details will help to better appreciate these
results. First, we have not attempted to find the best
parameter settings by using the test set to determine these
settings (this would lead to over-fitting to the test set).
Rather, we have experimented with various parameter set-
tings to understand how the semi-supervisedmethodmay
work.We are using themodified learnedmodel on the test
set only to give precision and recall values to gauge the
appropriateness of this technique. Second, instead of hav-
ing a separate validation set and choosing the best model
based on its performance with this set, every learned
model (682 models were developed using 22 parameter
settings and 1 to 31 iterations of the semi-supervised
training step) has been tested with the test set. So, the
results can be interpreted as: if a particular parameter
setting and number of iterations of the semi-supervised
algorithm would have produced the best model based
on its performance on the validation set, this parameter
setting and number of iterations of the semi-supervised
algorithm would give the results based on its performance
on the test set. Rather than reporting the best F-measure
over all parameter settings, the data was studied to see
certain trends. In particular, the reported values are for
the best performing model in the semi-supervised iter-
ation that happens before a decline in precision that is
witnessed in almost all of the parameter settings. This we
determined to be the sixth iteration. We chose this trend
because the semi-supervised method at this point had
provided the best ratio of true to false positives which we
considered a worthwhile goal. Although some parameter
settings performed better in terms of precision than these
reported results, it was felt that using this (almost) global
trend in precision as a cutoff point would be a better mark
of the performance rather than looking solely at a single
parameter setting that might be seen to be over-fitted to
the test set.
Graphs of the precision, recall, and F-measure val-
ues for each parameter setting for the 31 iterations of
the semi-supervised learning algorithm are presented in
Figs. 4, 5, and 6, respectively. Table 5 highlights the max-
imum values for each of these measures. The values for
each of these measures for all 682 parameter settings can
be found in https://github.com/mkhordad/Pheno-Geno-
Fig. 4 Precision values on the test set for all 22 parameter settings for 31 semi-supervised learning iterations
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 11 of 16
Fig. 5 Recall values on the test set for all 22 parameter settings for 31 semi-supervised learning iterations
Fig. 6 F-measure values on the test set for all 22 parameter settings for 31 semi-supervised learning iterations
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 12 of 16
Table 5 Maximum values for precision, recall, and F-measure
Precision Recall F-Measure
Parameter setting Maximum value Iteration Maximum value Iteration Maximum value Iteration
0.82 0.92 0.7699 4 0.8138 17 0.7880 19
0.83 0.92 0.7714 5 0.8287 31 0.7911 31
0.84 0.92 0.7709 7 0.8250 30 0.7889 26
0.85 0.92 0.7780 5 0.8268 31 0.7935 17
0.86 0.92 0.7709 5 0.8156 13 0.7870 12
0.87 0.92 0.7743 5 0.8063 20 0.7788 20
0.88 0.92 0.7698 5 0.8231 23 0.7907 23
0.85 0.93 0.7770 6 0.8268 24 0.7870 12
0.86 0.93 0.7757 5 0.8324 25 0.7856 25
0.87 0.93 0.7689 4 0.8287 15 0.7857 19
0.88 0.93 0.7704 7 0.8343 20 0.7946 17
0.89 0.93 0.7665 1 0.8399 27 0.7923 30
0.85 0.94 0.7755 9 0.8250 31 0.7836 14
0.86 0.94 0.7712 2 0.8250 31 0.7849 19
0.87 0.94 0.7741 5 0.8436 26 0.7961 26
0.88 0.94 0.7689 5 0.8194 25 0.7849 13
0.89 0.94 0.7715 6 0.8156 13 0.7892 13
0.85 0.95 0.7694 2 0.8156 20 0.7866 11
0.86 0.95 0.7688 2 0.8287 19 0.7896 15
0.87 0.95 0.7694 2 0.8268 31 0.7848 11
0.88 0.95 0.7705 7 0.8231 28 0.7875 14
0.89 0.95 0.7681 10 0.8212 21 0.7848 13
Extraction. There are two general trends in all of the
parameter settings that we tried. First, there is a short
increase in precision followed by a slow decline in this
measure. Second, a short decline in recall is followed
by a general increase in this measure until the point
(approximately iteration 15 to 17) when few new instances
are being added to the training set. See Fig. 7 for a
presentation of the addition of instances to the train-
ing set for each parameter setting. It should be noted
that shortly after iteration 15, few instances are avail-
able to be added to the training set. The minimum and
maximum value range proves to be too narrow in some
instances, but eventually all experimental settings lack
instances to add. The precision and recall curves tend
to flatten out at about this point. It would be inter-
esting to see how an increase in unlabelled instances
would affect the outcome of the semi-supervised
learning.
Recalling the work of Singhal et al. [41], they investi-
gated disease-gene-variant triplets, which is close to the
focus of this paper, and they provided precision, recall,
and F-measure values based on the performance of their
system on two datasets curated from human-annotated
PubMed articles concerning prostate and breast cancer.
The precision, recall, and F-measure results were 0.82,
0.77, and 0.794, and 0.742, 0.73, and 0.74, respectively for
the two datasets. Also recalling the work of Bokharaeian
et al. [47], they investigated relationships between SNPs
and phenotypes. Looking at their reported results that are
closest to what is reported here, they achieve precision up
to 69.2, recall up to 68.7, and F-measure up to 71.3. With
the understanding that the datasets are different and the
relationships being identified are closely related but not
exactly the same, we can say that the method presented
here, which is based only on the natural language text
surrounding the genotype-phenotype relationship, com-
pares favourably with the results obtained by these other
methods.
Looking forward, some improvements to the current
model can be suggested. Some of these improvements
are typical of the machine-learning paradigm. First is the
balance of positive and negative examples in the train-
ing set. While we tried to add some negative sentences
to our data to make it more balanced, Table 3 shows that
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 13 of 16
Fig. 7 Instances added for all 22 parameter settings for 31 semi-supervised learning iterations on the test set
our data is still biased: the number of negative instances
is less than the number of positive instances. A more
balanced training set is likely to improve the performance
of the trained model. Second, the quality of the original
set of examples which forms the seed for the self-training
algorithm affects the ability of that algorithm to increase
the size of our training set. Because the best results were
reached only after 6 iterations, the last training set has
only 935 instances. Our suggestion is to add more man-
ually annotated sentences to the original seed training
set, so that the first model made by this set makes better
predictions with a stronger level of confidence.
In addition to these methodological improvements, the
similarity of false positives and false negatives can indi-
cate some aspects of the problem to focus on. For instance,
our system incorrectly finds relationships in sentences
which address the main objective of the research being
discussed, i.e., those sentences suggesting the possibility
of a relationship rather than stating a relationship. Finding
and ignoring such sentences would improve the results.
As mentioned before, certain relationships contained
in the Phenominer corpus are undetectable in the test
set data because the relationship identification system
does not have the appropriate biological and linguistic
knowledge to recognize them. Table 6 shows the results
after deleting the Phenominer sentences from our test
set. The improved results (precision: 80.05, recall: 81.07,
F-measure: 80.55) demonstrate the true performance of
the relationship tool to identify relationships for which it
was constructed to find. Detecting these problematic rela-
tionships would require some significant changes to the
system.
First, the current system does not recognize relation-
ships that deal with sub-cellular phenotypes. To include
this type of phenotype, biomedical knowledge will need
to be enhanced to identify these phenotypes in the text.
Our system was built to consider only clinically observ-
able phenotypes. Additionally, the linguistic knowledge
will need to be supplemented because the direction of
this relationship is different. Second, the current sys-
tem is not able to extract complicated relations where
a pronoun refers to a phenotype or a genotype in the
same sentence or the previous sentences (anaphora),
or where a non-explicit noun phrase is used to refer
Table 6 Results after deleting Phenominer sentences from the
test set
Method Precision Recall F-measure
Supervised learning method 80.20 79.79 80.00
Self-training method 80.05 81.07 80.55
Khordad and Mercer Journal of Biomedical Semantics  (2017) 8:57 Page 14 of 16
(e.g., the gene), or where a part of or the whole geno-
type or phenotype is omitted (ellipsis) in a sentence.
For example in the following sentence Serum levels of
anti-gp70 Abs-7 were closely correlated with the pres-
ence of renal disease-16, more so than anti-dsDNA Abs-
24. only the relationship between anti-gp70 Abs-7 and
renal disease-16 is identified by our system but the
more complicated relationship between renal disease-
16 and anti-dsDNA Abs-24 is missed. Resolving these
problems will require a more sophisticated linguistic
model, the focus of computational linguistics research
generally.
Conclusions
To summarize, our contributions in this paper are the
following:
 Reconfiguring a generic relationship identification
method to perform genotype-phenotype relationship
identification.
 Proposing a semi-automatic method for making a
small training set using two relationship
identification tools.
 Developing a self-training algorithm to enlarge the
training set and improve the genotype-phenotype
relationship identification results.
 Analysing the results and specifying the types of
sentences and relationships that our system has poor
performance finding and giving some suggestions on
how to improve the results.
In conclusion, we have generated a machine-learned
model dedicated solely to the identification of genotype-
phenotype relationships mentioned in biomedical text
using only the surrounding text. With a test corpus, we
have provided a baseline measure of precision, recall, and
F-measure for future comparison. An analysis of the false
negatives and false positives from this corpus have sug-
gested some natural language processing enhancements
that would decrease the false negative and false positive
rates. From a biological perspective, determining the type
of relationship, e.g., does the relationship describe a direct
expression of a gene or is the relationship indicative of a
pathway effect, would be an important aspect of the rela-
tionship to mine from the text and is an interesting next
research direction to consider.
Endnotes
1A directed graph representing dependencies of words
in a sentence.
2 Seven verbs from [9] are not found in [48]. The approx-
imately 270 relationship words (808 surface forms) can
be found in https://github.com/mkhordad/Pheno-Geno-
Extraction. These words have a good overlap with the
current relations in the UMLS Semantic Network that
were used in Sharma et al.s verb-centric approach [61].
3Genotype-phenotype pairs that have a relationship are
the positive instances. Genotype-phenotype pairs that do
not have a relationship are the negative instances. The
sentences mentioned have both positive and negative
instances.
4 Each self-training iteration requires each sentence to
be evaluated using the current model. Using the full unla-
belled set proved to be too computationally expensive for
the experimental setting, so a subset was used instead.
5 http://nlp.stanford.edu/software/stanford-
dependencies.shtml
6Collins head finding rule [62] has been used.
Acknowledgements
Support for this work was provided through a Natural Sciences and
Engineering Research Council of Canada (NSERC) Discovery Grant to Robert E.
Mercer. Interactions with Nigel Collier were greatly appreciated.
Funding
Support for this work was provided through a Natural Sciences and
Engineering Research Council of Canada (NSERC) Discovery Grant to Robert E.
Mercer. The funding body played no role in the design of the study, nor in the
collection, analysis, and interpretation of data, nor in the writing of the
manuscript.
Availability of data andmaterials
The software and the data are available at: https://github.com/mkhordad/
Pheno-Geno-Extraction.
Authors contributions
MKH carried out the literature survey, developed the approach described in
the paper, conceived the design of the study, performed the statistical
analysis, and drafted the manuscript. RM participated in the design of the
study, performed some of the analysis, and helped to draft the manuscript. All
authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 12 July 2016 Accepted: 28 October 2017
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 
DOI 10.1186/s13326-017-0148-7
RESEARCH Open Access
Towards achieving semantic
interoperability of clinical study data with FHIR
Hugo Leroux* , Alejandro Metke-Jimenez and Michael J. Lawley
Abstract
Background: Observational clinical studies play a pivotal role in advancing medical knowledge and patient
healthcare. To lessen the prohibitive costs of conducting these studies and support evidence-based medicine, results
emanating from these studies need to be shared and compared to one another. Current approaches for clinical study
management have limitations that prohibit the effective sharing of clinical research data.
Methods: The objective of this paper is to present a proposal for a clinical study architecture to not only facilitate the
communication of clinical study data but also its context so that the data that is being communicated can be
unambiguously understood at the receiving end. Our approach is two-fold. First we outline our methodology to map
clinical data from Clinical Data Interchange Standards Consortium Operational Data Model (ODM) to the Fast
Healthcare Interoperable Resource (FHIR) and outline the strengths and weaknesses of this approach. Next, we
propose two FHIR-based models, to capture the metadata and data from the clinical study, that not only facilitate the
syntactic but also semantic interoperability of clinical study data.
Conclusions: This work shows that our proposed FHIR resources provide a good fit to semantically enrich the ODM
data. By exploiting the rich information model in FHIR, we can organise clinical data in a manner that preserves its
organisation but captures its context. Our implementations demonstrate that FHIR can natively manage clinical data.
Furthermore, by providing links at several levels, it improves the traversal and querying of the data. The intended
benefits of this approach is more efficient and effective data exchange that ultimately will allow clinicians to switch
their focus back to decision-making and evidence-based medicines.
Keywords: FHIR, CDISC ODM, Interoperability, Clinical research data, Longitudinal clinical study
Background
Clinical research plays a vital role in advancing medical
knowledge and improving clinical outcome. It is becoming
increasingly clear that results from clinical studies need
to be shared and compared to one another in order to
support efficient evidence-based medicine [1] and reduce
the costs of conducting these studies. By the same token,
Hsu et al. [2] argue that to fulfil the goals of precision
medicine requires the mining and aggregation of clinical
data from multiple sources and entails novel approaches
to obtaining contextual observations. Hume et al. [3] state
that: clinical research can no longer be considered an iso-
lated venture and is increasingly conducted in network
*Correspondence: hugo.leroux@csiro.au
The Australian E-Health Research Centre, CSIRO Health and Biosecurity, Level
5, Health Sciences Building 901/16, Royal Brisbane and Womens Hospital,
Herston 4029, Queensland, Australia
structures where seamless data exchange is critical to oper-
ational efficiency and effectiveness. The challenge when
comparing results from different data sets is to ensure that
we are comparing corresponding data sets.
The Operational Data Model (ODM) [4] is an XML1-
based standard from the Clinical Data Interchange Stan-
dards Consortium (CDISC) that was originally developed
to facilitate the exchange, archival and audit trail require-
ments of clinical information but whose use has been
extended to cover cases not initially anticipated [3], such
as integrating health records within clinical research sys-
tems. The Federal Drug Administration has mandated the
use of the CDISC standards for the electronic capture and
reporting of clinical study data [5]. ODM is particularly
well-suited for a data capture context [3, 611]. It is a
mature data interchange standard that has proven useful
for exchanging both document and message formats [3].
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 2 of 14
Its strength is in its relative simplicity, adaptability
through the use of extensions [3] and in its ability to sup-
port the creation of a broad range of customisable Clinical
Report Forms (CRFs) [3, 12].
ODM, however, lacks a rich-enough information model
to capture the innate contextual information of the clinical
study data [7, 13]. Its relative simplicity, has impacted on
its ability to advance all aspects of interoperability, limit-
ing its support for data mapping, data types, terminology
and semantic representation [3]. In spite of its efficacy
as a data interchange, ODM has some shortcomings in
the mapping of semantically identical data elements due
to lack of support for semantics associated with the data
elements [3, 11].
ODM can be considered to represent syntactic interop-
erability (as defined by [14]) of clinical data as it provides
a vehicle for clinical data to be shared using an XML-
based model. However, our aim is to achieve semantic
interoperability. Semantic interoperability is the ability, for
health information systems, to exchange information and
automatically interpret the information exchanged mean-
ingfully and accurately in order to produce useful results
as defined by the end users of both systems [14, 15]. Exten-
sions to ODM, such as the Clinical Data Acquisition Stan-
dards Harmonization (CDASH) [16] and the Biomedical
Research Integrated Domain Group (BRIDG) [17] provide
a reference model, although as stated by [18]: studies that
use CDASH CRFs achieve semantic alignment through a
shared data standard, rather than through specific seman-
tics. Furthermore, there is no requirement for the CDASH
model to be used within ODM [7]. Moreover, uptake of
CDASH and BRIDG to provide data semantics has been
limited [3]. As a result, ODM is ill-suited for advancing
the semantic interoperability solution that is required to
achieve cross-study exploration of the clinical studies as
there is the potential for the data to be interpreted in a way
that was not originally intended by the study initiators.
The ability to achieve cross-study analysis also neces-
sitates clinical studies to adopt a more streamlined
data structure [7]. However, the monolithic nature
of the ODM data model favours a one-dimensional
traversal of the clinical data along its hierarchy of
Study-Subject-StudyEvent-Form-ItemGroup-Item.
More effective exploration and querying of the clinical
data, especially when dealing with longitudinal studies,
requires more direct access to the data, particularly at the
Study Event, Subject and Item levels [6, 7, 19].
The Fast Healthcare Interoperable Resources [20]
(FHIR) framework, a HL72 standard that has been swiftly
adopted by the health-care community [2123], looks the
likely candidate for overcoming this challenge. It is geared
towards communication of clinical data using HL7 mes-
saging protocols but is also supported by a rich informa-
tion model to achieve semantic interoperability of clinical
data. This makes FHIR the natural match to complement
the ODM standard [8] as ODM shares several design prin-
ciples, such as making use of extensions for edge cases
and human readability, with FHIR [3]. Furthermore, FHIR
has the potential to incorporate existing electronic health
record (EHR) data to augment the findings of retrospective
observational studies. As intimated by Kubick [24], FHIR
canmake it possible to reach inside of EHRs not just to cap-
ture data, but to monitor protocol progress, provide safety
alerts, and allow much greater visibility into trial conduct
and can lead to dramatic improvements in study efficiency
and drug safety.
This research builds upon the approach [8] to integrate
clinical data extracted in CDISC ODM format into several
FHIR resources with a view to achieving semantic inter-
operability of clinical study data. In the next section, we
outline the approach taken to map the ODM-based data
and metadata onto eight FHIR resources. In particular, we
outline the suitability of the FHIR resources in support-
ing the ODM model and on all the assumptions made to
reintroduce the contextual information to the data. We
then critique this approach. Consequently, we propose
the FHIR ClinicalStudyPlan resource to capture the
clinical study metadata, including the potential to encode
the study protocol as part of the model. This is followed
by a description of the FHIR ClinicalStudyData
resource that describes the clinical study data. Finally, it
leads into a discussion on the design principles of the
two proposed FHIR resources and on their suitability for
representing clinical study data.
Integrating ODMwith FHIR
This section outlines the approach described in [8] to
integrate the ODM data model to a selection of eight
FHIR3 resources to capture both the data and meta-
data properties of the ODM data model. The CDISC
ODM data model [4] consists of two main hierarchies:
a Clinical Data and a Metadata hierarchy, as
depicted in Fig. 1, that are referenced using the same
object identifier (OID). These two parallel hier-
archies ensure that the clinical study follows a prede-
termined structure of subject, event, form, item
group and item. Figure 2 outlines the FHIR resources
chosen to model the ODM data. The entities in red (Care-
Plan and Questionnaire) denote metadata concepts. The
remaining entities, in blue, model the clinical data at var-
ious levels of the ODM hierarchy. Solid lines are used to
denote the links between the entities.
The approach taken to map the ODM data into FHIR
resources is a semi-automatic process. As the ODM data
model does not natively provide any mechanism to cap-
ture the contextual information relating to the study, the
data semantics needs to be re-introduced during this pro-
cess. This can only be achieved if the person doing the
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 3 of 14
Fig. 1 The ODM data model. Illustrates the logical organisation of the ODMmodel into the data andmetadata hierarchies
mapping has access to all the conceptual information
defining the study. Figure 3 illustrates how the hierarchical
ODMmodel has been mapped to the FHIR resources.
Study
A study defines static information about the structure
of an individual study. We choose to model the Study
component from ODM using the CarePlan resource
because we want to model the activities planned for the
patient during the study in the context of the study pro-
tocol. CarePlan provides a link to the study coordinator
through the participant attribute and study proto-
col through the support attribute. Furthermore, the
CarePlan resource offers a number of attributes, such
as context, category and description that can
provide additional context to the care plan.
Subject
While the Subject represents a critical element of the
study, its role is quite subdued in ODM. In particular,
the specification provides no functionality to record the
subjects attributes such as gender or date of birth,
recommending that these be modelled as clinical data
within the forms. The logical mapping for the Sub-
ject in FHIR is the Patient resource. Relevant con-
textual information, such as the patients gender, date
of birth and care provider, can be encapsulated within
the resource. The clinical data for each subject is con-
tained within a ClinicalImpression resource that
is linked to the Patient resource. The care plan is
linked to this resource using the plan attribute. The
ClinicalImpression permits very pertinent infor-
mation to be associated to the patients data through the
Fig. 2 The FHIR data model. Depicts themetadata (red) and data (blue) FHIR resources and their links that comprise the data model to transform the
clinical data from ODM to FHIR. The CarePlan and Questionnaire resources are used to capture themetadata for the study. A Patient
resource is used to represent the study participant while the clinical data for this participant is contained within a ClinicalImpression
resource. The study events are captured within the EpisodeOfCare resource and the Encounter resource represents one atomic event. The
QuestionnaireResponse resource captures the form responses and the Observation resource illustrates those responses that are
analogous to a patients observations. The QuestionnaireResponse resource is linked back to the Questionnaire resource
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 4 of 14
Fig. 3Mapping the ODM data model to the FHIR resources. Illustrates how the CDISC ODMmodel (depicted by unshaded rectangles) is overlaid
with the FHIR resources. The Metadata section, depicted on the right of the model with red rectangles to represent the FHIR resources, is mapped
to the CarePlan resource at the Study and Study Event level, and to the Questionnaire resource to represent the form and its
composition. The Data section is depicted on the left of the model with the FHIR resources depicted as blue rectangles. The Patient resource
represents the study participant. The ClinicalImpression resource captures the clinical data for this participant and they are both linked to
the ODMmodel at the Subject Data level. As both the EpisodeOfCare and Encounter resource correspond to study events, they are
mapped at the StudyEventData level. The QuestionnaireResponse resource captures the form responses and is linked to the form data
and its composition. Finally, the Observation resource is used to capture those responses that are more analogous to a patients observations
use of the trigger, investigations and summary
attributes.
Study event
A study event comprises a StudyEventDef and a
StudyEventData component that are referenced using
a common OID. The StudyEventDef manages the set
of forms to be completed at this phase of the study
and represents an activity within the CarePlan
resource. StudyEventDef entities define scheduled and
unscheduled events and these are defined within the
detail.scheduled attribute of the activity. The
StudyEventData entity contains clinical data collected
during a subjects visit. We chose the EpisodeOfCare
resource for this entity because it provides details about
the group of activities and their purpose pertaining
directly to a patient. A study event may result in many
visits from a patient. Each individual visit is modelled
as an Encounter and is linked to the episode of care
through the episodeOfCare attribute. The patient
attribute links the resource to the study subject while
the assessor attribute provides a link to the clinician
conducting the clinical assessment.
Form
A form defines a collection of data items collected
during the study and termed a case report form. A
form comprises a FormDef and a FormData compo-
nent that are referenced using a common OID. The
form is linked to CarePlan through the activity.
actionResulting attribute. The FormDef defines the
form structure and its questions. The logical mapping of
forms in FHIR is the Questionnaire resource. This
resource contains the typical attributes for questionnaires,
such as an identifier, version, publisher and status, but
can also be customised using the extension mechanism
in FHIR. The FormData entity contains the clinical data
associated with the form. The logical mapping for the
FormData in FHIR is the QuestionnaireResponse
resource. The benefits of using the QuestionnaireRe-
sponse resource are that the order of the responses is
maintained and these can be linked and validated against
the questions asked. Conversely, however, few mecha-
nisms exist to standardise the generation of CRFs for clini-
cal studies [8, 11]. This limits the reuse of CRFs unchanged
across protocols [11]. Furthermore, the tendency is to
organise data items, relevant to a research protocol, into
individual CRFs based on considerations other than log-
ical grouping [8, 11] but one that befits the data capture
process [8]. Owing to the strong coupling between the
form design and the ODM model, until such a time that
implementations of ODM allow for a clear demarcation
between the form design and its display, we advise against
modelling the CRF per se as a FHIR resource [7].
Item group
The ItemGroupDef and ItemGroupData entities
constitute an item group referenced using a commonOID.
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 5 of 14
The ItemGroupDef entity defines the optional group-
ing of questions on a form. Groups are defined using
the Questionnaire.group attribute. The FHIR spec-
ification stipulates that a group attribute define either a
question or a group but not both. The ItemGroupData
contains the clinical data detailing the responses for the
item group. FHIR organises these grouped responses
within the QuestionnaireResponse.group attribute.
Similar to forms, items are often grouped to match the
data collection process and not necessarily because of
their semantic similarity [8, 11].
Item
At the item level, the ItemDef and ItemData entities
define each question and its subsequent response. The
ItemDef entity defines the question asked during the
study along with defining attributes such as the datatype,
data size, measurement unit, permissible range and
code list. The Questionnaire.group.question
attribute is the most appropriate to define the ItemDef
entity. The logical mapping for the ItemData entity
is the QuestionnaireResponse.group.question
attribute. The response to the question is then con-
tained within the question.answer sub-attribute.
This model works best in a lifestyle study scenario using
questionnaires in the traditional question-answer mode.
In the case of longitudinal clinical studies where the
responses are analogous to a patients observations during
an episode of care, we believe the ItemData entity to be
more appropriately represented using the Observation
resource. Furthermore, as outlined in the FHIR speci-
fications, data captured in questionnaires can be diffi-
cult to query after the fact. Individual items within a
QuestionnaireResponse or an Observation are
subsequently linked back to the Encounter in which
they occur.
Discussion
An implementation of the mapping between ODM
and FHIR is available at http://healthinet.it.csiro.au/
net/jbs/odmFhir. We have semantically enriched the
original ODM data with relevant domain information
from SNOMED CT4 and LOINC5. The implementation
demonstrates that the FHIR resources provide a good
fit to semantically enrich the extracted data from the
CDISC ODM. In spite of its shortcomings in providing
context to the clinical data, the CDISC ODM provides
a sound hierarchical framework for capturing the clini-
cal data. However, as outlined in [25], a mapping process
invariably leads to the loss of pertinent information. On
the metadata side, for example, a study is modelled as
a CarePlan. The CarePlan resource, however, is not
used in its intended manner in that it does not relate
to a particular individual. Similarly, despite being chosen
to capture the clinical data, the ClinicalImpression
resource has no capability to model the study hierarchy.
As a result, it relies on several other FHIR resources, such
as EpisodeOfCare and Encounter, which are also
not used as intended, to describe the hierarchy. As stated
by Kubick [24], it is preferable to avoid data transforma-
tions, if possible, especially when this involves massaging
the data to fit into different formats, as this opens up
the possibility of introducing errors and reducing the data
reliability. Another issue relates to discrepancies between
the data types defined within the ODM and FHIR mod-
els. In addition to the type, ODM allows the permissible
range of the resulting data and, in the case of decimal
values, the length of the permissible value to be defined.
The answer attribute within the QuestionnaireResponse
resource has no such capability. The Observation
resource is the only one to allow such a definition.
The main challenge of the mapping process, however,
relates to the FHIR specifications. Being an emerging
and evolving standard, FHIR is in a great state of flux.
As such, FHIR resources are constantly being updated
between releases. The implications are that relationships
described using one version of the FHIR specifications
may no longer be available in a subsequent version.
The Questionnaire and ClinicalImpression
resources are two resources that have undergone several
changes.
Clinical study design using FHIR
Kubick [24] advocates (i) for the adoption of FHIR for clin-
ical research; (ii) for clinical data to be captured directly at
the source; and, (iii) for data transformation to be avoided
whenever possible. Similarly, Huser et al. [10] argue that
the adoption of a single format for study protocols and
study results decreases the development time required to
import studies into the repository or to exchange data
between systems. Besides, the FHIR model has the poten-
tial to manage clinical data in its own right [8]. Conse-
quently, we propose the introduction of two new FHIR
resources to capture the data and metadata from the
clinical study. These resources have been integrated in
a data model, as illustrated in Fig. 4, which corresponds
to the mapping, in FHIR, for a typical research study.
The ClinicalStudyPlan resource, outlined in Fig. 5,
defines the study and provides an overview of the planned
activities. The ClinicalStudyData resource, outlined
in Fig. 6, describes the data captured as part of the study
organised around the events and visits of the patient.
ClinicalStudyPlan
The ClinicalStudyPlan resource comprises several
attributes to capture the fundamental concepts within the
study. Thus the identifier attribute provides a unique
identifer for the resource. A title attribute captures
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 6 of 14
Fig. 4 The clinical research data model in FHIR. Illustrates themetadata (red) and data (blue) resources comprising the clinical data model for
describing and capturing the research study natively in FHIR. The study plan can be described using either the ClinicalStudyPlan or
PlanDefinition resource. The latter can be further defined using the ActivityDefinition resource. The Questionnaire resource
provides the definition for forms within the study plan. A link to the study plan is contained within the ClinicalStudyData resource. The
ClinicalStudyData resource encapsulates the clinical data comprising the research study. It facilitates links to the Patient resource, to
describe the study participant. It further describes investigations that can be a QuestionnaireResponse or a series of Observation or
ImagingManifest resources. The ImagingManifest resource further defines an ImagingStudy resource to describe the imaging study
being conducted
the title under which the study is publicly known. An
officialTitle attribute holds the scientific title of the
study. The date of registration of the study is contained
in the registrationDate field and the regulatory
agency effecting the registration is depicted within the
authoringBody field. A mandatory status attribute
specifies the current state of the resource. The study spon-
sors can be described in the sponsor field, which allows
a Group resource to be defined. The publicContact
attribute specifies the contact details of the person
responsible for general enquiries about the study. An
investigator attribute discloses the principal investi-
gator for the study; a person tasked at initiating the study,
developing the study protocol and responding to scien-
tific enquiries about the study. A textual description of
the aims of the study is provided by the description
attribute. The actual or forecasted date of first participant
enrolment is recorded in the dateFirstEnrolment
and the expected total number of participants enrolled is
captured in the sampleSize attribute.
The desired outcome of the study is captured within
the goal attribute. Each goal is further divided into
three sub-attributes. The name of the outcome is con-
tained within the outcome attribute. A metric attribute
describes the metric or method of measurement used to
evaluate the outcome and finally a timepoint attribute
records the timepoints of interest in which to achieve the
goal.
We then define the activities that constitute the study.
In [8], we outlined how the Questionnaire resource
is insufficient to capture all activities from clinical stud-
ies, especially longitudinal ones. By defining all aspects of
actions resulting from clinical studies within activity
attributes, we facilitate the definition of both tradi-
tional questions and more observational measurements.
A scheduled attribute allows the timing of an activ-
ity to be defined. We chose an actionResulting
attribute to describe the questionnaire developed as part
of the activity. We then define a detail attribute to pro-
vide a detailed description of sub-activities that will ulti-
mately lead to Observation and ImagingManifest
resources in FHIR. This attribute thus provides three
sub-attributes to document the category, type and ratio-
nale for each sub-activity. We also chose to record the
Practitioner or Organisation involved in the
activity through the performer attribute and provide
a reference to the activitys location using a location
attribute. Finally, a note attribute allows any comments
relating to the clinical study plan to be recorded.
We have started engaging with the HL7 FHIR-I6 [26]
and RCRIM7 [27] working groups. The FHIR commu-
nity, however, intends to release, as part of STU38, a
PlanDefinition resource that captures many of the
functionalities of the ClinicalStudyPlan resource.
PlanDefinition
PlanDefinition9 is a resource proposed by the HL7
community that is at the ballot phase and that they intend
to release as part of STU3 in late 2016. We will only com-
ment on the main concepts as this is a draft proposal
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 7 of 14
Fig. 5 The ClinicalStudyPlan resource. Describes the elements comprising the ClinicalStudyPlan resource. This resource has been
generated using the FHIR Build Process [45] based on the FHIR Guide to Designing Resources [46]. The build process builds
the resource and generates the webpage that describes the resource, as depicted in this Figure. The table structure is defined in the Resource
Definition page [47], which also provides a definition of the flags; ?! indicates that the element is a modifying element, while  indicates
that this element is part of the summary set. The activity element allows either the definition of detailed items or a Questionnaire resource
to be specified
that is still subject to change at short notice. Unlike the
ClinicalStudyPlan, this resource has not been designed to
address the planning of clinical research specifically but
it is flexible enough to undertake this role. However, sim-
ilar to the ClinicalStudyPlan resource, it contains
attributes to represent the plans unique identifier, name,
status, purpose and contributor. In addition, it defines the
version as well as attributes that capture the type of plan
defined, the clinical usage foreseen for the plan, a natural
language description of the plan, dates of publication
and last review, the context of use (coverage) of the plan
as well as the topics described.
Central to this resource is the definition of actions
(actionDefinition) to occur as part of the plan.
Each action has an identifier, a label, a title, a descrip-
tion of the action both in natural language and as
Codeable entities and a link to supporting documenta-
tion for the action. Each action further defines a condition
for whether as well as some triggers to specify when
the action should occur. A description of the activity
comprising this action can be further defined within an
ActivityDefinition resource.
The proposed ActivityDefinition10 resource
provides a conceptual description of an action that
should be undertaken. Similar to the PlanDefinition
resource, it is at the ballot phase and is intended to be
released as part of STU3 in late 2016. It contains similar
organisational attributes as the PlanDefinition resource. A
detailed definition of the activity can be achieved using
a CodeableConcept element. A category attribute
defines the type of activity undertaken and a timing
attribute specifies when the activity should occur.
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 8 of 14
Fig. 6 The ClinicalStudyData resource. Describes the elements comprising the ClinicalStudyData resource. This resource has also
been generated using the FHIR Build Process [45] based on the FHIR Guide to Designing Resources [46]. The table structure
is defined in the Resource Definition page [47], which also provides a definition of the flags; ?! indicates that the element is a modifying
element, while  indicates that this element is part of the summary set. The event element describes the events occurring throughout the study.
An event can be further divided into visits. Each visit defines an investigation, which can be only one of the following: a
QuestionnaireResponse resource or a series of Observation or ImagingManifest resources
ClinicalStudyData
The ClinicalStudyData resource describes the
data captured during the study. An identifier is
defined to provide a link to the primary identifiers
for the study. For external identifiers, such as a hos-
pital patient id, an externalIdentifier attribute
is provided. A mandatory patient attribute pro-
vides a reference to the patient being assessed. The
ClinicalStudyData resource provides a link to either
the ClinicalStudyPlan or PlanDefinition
resource, through the plan attribute, to uniquely iden-
tify the study that this clinical data instance represents.
A status attribute defines the current state of the
resource. We also chose to keep a record of past statuses
in a statusHistory attribute that captures the past
statuses as well as the time that the event was in the
specified state. The time period during which the patient
underwent the clinical assessment is depicted in the
period attribute.
We then define the event occurring during the course
of the study. An event represents the execution of one
or more activities during the course of the study to
assess the patient. Each event transitions through a
number of states and the state is contained within the
status attribute. A type attribute describes the type
of the event. The clinicians involved in this event are
described within the participant attribute, which fur-
ther defines their role and a reference to the involved
member, be it a Practitioner or an Organization.
A summary of the event is provided within the summary
attribute.
In clinical study parlance, an event can last any-
thing from a few seconds to several months or even
years. Consequently, we define a visit attribute to
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 9 of 14
describe one or more encounters between the clinician
and the patient. Visits can be planned or unplanned
and this is defined within the scheduled attribute.
The timing of the visit is described within the timing
attribute. Finally, we define an investigations
attribute to capture one or more clinical investigations
during the course of the visit. These take the form
of a reference to either a QuestionnaireResponse
or a series of Observation or ImagingManifest
resources.
Demonstrating the clinical study design with FHIR
We illustrate the fit of the FHIR data model by discussing
a clinical study focussed on cardiovascular episodes. Our
focus is to highlight the impact that the addition of con-
textual information, and their relationships with the data
elements, have on the semantic relevance and interpre-
tation of the clinical data. Typically, the output from
a clinical study, in ODM XML format, is as depicted
below:
<ItemGroupData ItemGroupOID
="IG_VITAL_READINGS_8016"
ItemGroupRepeatKey="1"
TransactionType="Insert">
<ItemData ItemOID="I_VITAL_SYSTOLICBP_
1220" Value="119">
<MeasurementUnitRef
MeasurementUnitOID="MU_MMHG"/>
</ItemData>
<ItemData ItemOID="I_VITAL_DIASTOLICBP_
4036" Value="79">
<MeasurementUnitRef
MeasurementUnitOID="MU_MMHG"/>
</ItemData>
</ItemGroupData>
This states that the study participant has a blood
pressure of 119/79 mmHg but provides no informa-
tion on how the measurements were obtained. Handler
[28] outlines nine factors that may affect the accuracy
of blood pressure measurements. To assist the user in
making informed decisions about the clinical data, rel-
evant contextual information, such as illustrated below,
should be provided with the data. This additional meta-
data tells us that the readings were taken at 10:00 am
by a nurse from the left upper arm and in a sitting
position. While it is important to standardise the data
and metadata, what is missing is the relationship to the
initial blood pressure measurements. When the mea-
surements are presented as a series of unrelated data
elements, they cannot reliably be interpreted (Appendix
in [5]).
<ItemGroupData ItemGroupOID
="IG_VITAL_READINGS_8016"
ItemGroupRepeatKey="1"
TransactionType="Insert">
<ItemData ItemOID
="I_VITAL_SYSTOLICBP_1220" Value="119">
<MeasurementUnitRef
MeasurementUnitOID="MU_MMHG"/>
</ItemData>
<ItemData ItemOID
="I_VITAL_DIASTOLICBP_4036" Value="79">
<MeasurementUnitRef
MeasurementUnitOID="MU_MMHG"/>
</ItemData>
</ItemGroupData>
<ItemGroupData ItemGroupOID
="IG_VITAL_CONTEXT_4378"
ItemGroupRepeatKey="1"
TransactionType="Insert">
<ItemData ItemOID
="I_VITAL_READINGSTIME_1720"
Value="10:00">
<MeasurementUnitRef
MeasurementUnitOID="MU_HHMM"/>
</ItemData>
<ItemData ItemOID
="I_VITAL_READINGSTAKENBY_4404"
Value="Nurse"/>
<ItemData ItemOID
="I_VITAL_READINGSBODYPART_8890"
Value="Left upper arm"/>
<ItemData ItemOID
="I_VITAL_READINGSBODYPOSITION_1915"
Value="Sitting"/>
</ItemGroupData>
The FHIR framework, in particular resources such as
Observation, provides the means to accurately rep-
resent the relationships between the data elements so
that they can be understood and interpreted more effec-
tively. A representation of the blood pressure above,
implemented using the Observation resource, is illus-
trated in the Additional file 1. In addition to capturing
the blood pressure measurements described previously,
the Observation resource provides a reference to the
Patient resource to identify the study participant and
to the Practitioner resource to identify the clini-
cian performing the blood pressure measurement. More
importantly, it natively encapsulates the contextual infor-
mation, such as the body part and body position, as well as
the ability to interpret the measurements.
By taking advantage of resources that encapsulates
rich interrelated clinical data, as demonstrated by the
Observation resource, the ClinicalStudyData
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 10 of 14
resource facilitates the definition of the entire research
study, in terms of the subjects enrolled; the clinical data
associated with these subjects and the experiments under-
taken. In addition, it provides a framework, through the
plan element, for the study plan to be associated with the
clinical data. An implementation of the ClinicalStudyData
and ClinicalStudyPlan resources is provided at http://
healthinet.it.csiro.au/net/jbs/.
Discussion
The implementations of the ClinicalStudyPlan and
ClinicalStudyData resources demonstrate the fit of
the FHIR standard in capturing andmanaging clinical data
from research studies. The pertinence of this finding is
that the clinical data no longer need to be transformed
from an arbitrary standard into FHIR resources, thus
reducing the risk of introducing errors and losing fidelity.
The proposed models achieve semantic interoperability
by defining a set of common elements for describing the
actions performed on the data as well as defining common
elements for describing the data and its context through
the use of controlled terminologies and ontologies. This,
then allows the resources to be shared and processed
across systems. The FHIR resources provide the means
to navigate and access the clinical data at numerous lev-
els with the addition of several dimensions at the patient,
event, activity and data item level, thereby negating the
limitations of the monolithic and rigid hierarchy of the
ODM data model.
The World Health Organisation (WHO) has released
a list of twenty mandatory items for the definition of a
study protocol [29] so that the given trial can be con-
sidered fully registered. We present, in Table 1, a list-
ing of the twenty items alongside the attributes from
the ClinicalStudyPlan and PlanDefinition
resources. We have not provided a study type as
the ClinicalStudyPlan inherently suggests a clinical study.
We have also chosen not to explicitly define the
source of monetary funds and countries of
recruitment as these are primarily associated with
clinical trials. However, it is our support for eligibility
criteria that is particularly inadequate. In our defence,
our focus here has been the definition of an alterna-
tive structural representation to CDISC ODM for clinical
study design. Furthermore, we regard the formulation of
an effectual eligibility criteria as non-trivial and one that
we deemed out of scope for this paper. We intend to
engage with the HL7 community to embed computable
study protocol criteria within our resource as adequate
representation of the study protocol is very useful and
important [10]. Previous attempts, such as the CDISC
Protocol Representational Model (PRM), have had lim-
ited adoption by the clinical study community [10]. (PRM
[30] is a UML11-based standard that developed a set of
Table 1 Listing of the 20 WHO items for clinical study protocol
WHO trial registration
data set
PlanDefinition ClinicalStudyPlan
1. Primary registry and
trial identifying number
Identifier Identifier
2. Date of registration in
primary registry
RegistrationDate
3. Secondary identifying
numbers
Identifier ExternalIdentifier
4. Source(s) of monetary or
material support
5. Primary sponsor Sponsor
6. Secondary sponsor(s) Sponsor
7. Contact for public
queries
Publisher PublicContact
8. Contact for scientific
queries
Investigator
9. Public title Title Title
10. Scientific title OfficialTitle
11. Countries of
recruitment
12. Health condition(s) or
problem(s) studied
Purpose* Description
13. Intervention(s) ActionDefinition Activity
14. Key inclusion and
exclusion criteria
15. Study type Type* *
16. Date of first enrolment DateFirstEnrolment
17. Target sample size SampleSize
18. Recruitment status Status Status
19. Primary outcome(s) Coverage* Goal
20. Key secondary
outcomes
Coverage* Goal
An asterisk (*) indicates that this data item is partially or indirectly addressed in the
model
standardised protocol concepts that was intended to be
used alongside the other CDISC and HL7 standards.)
The appeal in definining a visit as part of an
event in the ClinicalStudyData resource is to
more accurately describe protracted events within mul-
timodal longitudinal clinical studies. It is often useful,
in the case of lengthy events, to be able to define
a sub-event and subsequently record the study par-
ticipants attendance to the sub-event. Consequently,
the outcome of those visits can be represented as a
QuestionnaireResponse, ImagingManifest or an
Observation through the investigations attribute.
The QuestionnaireResponse, ImagingManifest
and Observation resources suit different types of clin-
ical studies [8]. The pertinence of the Observation
resource is the ability to store important contextual infor-
mation alongside the clinical data, the ability to interpret
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 11 of 14
the observation in the context of a controlled vocabulary
or ontology and the ability to provide some justification as
to the absence of a measurement [8].
While the PlanDefinition resource can be used
to describe the study plan, it still has some inadequacies
to overcome. As the PlanDefinition resource has
not been specifically designed to address the planning
of clinical research, it logically has to be more generic.
Consequently, it is unclear how the PlanDefinition
resource relates to the FHIR resources designed to cap-
ture the clinical data that it defines. Furthermore, it is
also unclear what mechanism is envisaged to ensure that
the data capture resources conform to the plan defini-
tion. Moreover, as the resource has not been designed
for a clinical research domain, the PlanDefinition
resource also lacks the necessary mechanisms to fully
define the study protocol. In particular, it does not offer
the option of recording the date of registration,
the sponsor(s), the date of enrolment,
expected sample size, study type and study
outcome(s). More importantly, in our view, is the lack
of support for machine-processable inclusion and exclu-
sion criteria to be embedded within the PlanDefinition
resource. While the PlanDefinition defines a trigger
and a condition element, these relate to the execution
of the PlanDefinition resource and do not constitute the
definition of the conditions addressing the eligibility of
the participants to participate in the study. We advocate
for the eligibility criteria to be designed in a manner to
influence and advance the study design and form gen-
eration as outlined in [3] in their five phases of clinical
research data lifecycle.
While the ClinicalStudyPlan and PlanDefinition
resources are structurally similar, there are sub-
tle differences between them. It is unclear how the
Questionnaire resource (indicated by dotted lines in
Fig. 4) fits within the PlanDefinition resource. This may, in
our view, restrict its ability to be used for anthropological
studies or surveys.
Related work
Prior to FHIR, several information models have been
proposed to standardise the representation of clinical
information. The Clinical Element Model (CEM) is an
information model designed to provide a consistent archi-
tecture for representing clinical information in EHR sys-
tems [31]. The ISO 13606 standard is an international
standard published by ISO that specifies the information
models and vocabularies needed for the interoperability of
EHR systems [32]. Both models aim to address the issue
of semantic interoperability by standardising the data,
metadata and their relationships similar to our approach.
Numerous research have centred around the CDISC
models recently. Dugas [25] describes two tools to convert
forms between the CDISC ODM and HL7 CDA12 for-
mats to facilitate the sharing of electronic health records
(EHRs) and clinical data to address the problem of redun-
dant documentation in both systems. His findings reflect
our position that the conversion process is lossy because
the CDISC and HL7 models serve different purposes
and hence have different properties. Similarly, the SALUS
project [33, 34] is a former attempt to adapt CDISC
standards to build a semantic framework to improve the
interoperability between clinical research and clinical care
domains. More specifically, it looks at combining the
strengths of CRFs with those of EHRs to address adverse
drug reactions. We envisage our proposed FHIR clini-
cal study model to facilitate the incorporation of existing
EHR data to augment the capabilities of retrospective
observational studies similar to their approach. Jiang et al.
[35] have developed and evaluated a Semantic Web-based
approach for the generation of domain-specific templates
from the integration of the BRIDG model and the ISO
21090 data types, to support clinical study metadata stan-
dards development. Vadakin and Hinkson [36] discuss the
CDISC PRM and outline its importance in supporting
research study design, registration, tracking and in pro-
viding a single-source of protocol content electronically.
They stress that typical protocol document is not useful
for information management and re-use. PRM standard-
ises the protocol content into a structured document that
is easier to understand and to exchange, in machine-
readable format, across systems [36]. We are mindful of
their findings in order to address the issue of the protocol
definition within our research data model.
A topical area of research has been the standardisation
and structuring of clinical forms. Abler et al. [18] discuss
the need for a language for forms that can effectively
record the logical relationships between questions or sets
of questions asked in the forms. Richesson and Nadkarni
[11] provide a review of the electronic data capture
standards landscape and discuss their current limitations.
Bruland et al. [9] discuss the standardisation of CRFs
to achieve interoperability in clinical research. They
outline the difficulties of promoting the standardising
and structured representation of forms in the context of
data exchange and propose a mapping model between
the National Cancer Institute forms and CDISC ODM
files semantically annotated using the Alias element. As
stated in [8], the tendency would be to organise the forms
within a Questionnaire resource in FHIR. However,
this understates the nature of the information captured
and the choice of a QuestionnaireResource,
Observation and ImagingManifest resource
ensures the optimal capture of the information.
The Linked Clinical Data Cube (LCDC) [6, 7, 19]
describes a semantic web approach to investigate the
association of the semantic statistics vocabularies with
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 12 of 14
clinical data exchange standards and demonstrate their
fit in achieving the semantic enrichment of clinical study
data with a view to fulfilling semantic interoperabil-
ity. The LCDC defines a set of modularised data cubes
that helps manage the multi-dimensional and multi-
disciplinary nature of clinical data. It requires mapping
to the RDF Data Cube [37] and DDI13-RDF Discovery
[38] vocabularies to organise the data and links to domain
ontologies to semantically enrich it. The LCDC repre-
sents the precursor to our data model in FHIR. The HL7
working group on Semantic Interoperability [39] has ini-
tiated work on translating the XML and JSON version of
FHIR into FHIR RDF. Once completed, this should allow
the integration of the FHIR data model with the semantic
statistics vocabularies.
Future work
We intend to engage with the FHIR community to address
the full support for the definition of eligibility criteria
within the FHIR resources. There is a need for the cur-
rent text-based criteria to be formalised and provided in
machine-readable format to facilitate computerised deter-
mination of eligibility [10]. Machine-processable defini-
tion of eligibility criteria will not only mould the study
design but can influence the patients recruitment process
as outlined in [40].
The FHIR specification provides the functionality,
through the FHIR mapping language [41], to transform
clinical data from one model to another. We intend to
take advantage of this functionality to map the FHIR
clinical data model back to the CDISC standards. As out-
lined earlier, the regulatory bodies favour the use of the
CDISC standards for the reporting of clinical studies [5].
By using FHIR to model the clinical study data, we capture
the contextual information, and fulfil the requirements of
the FDA by retrofitting the clinical data to the CDISC
models.
We also aim to support the formulation of tempo-
ral constraints to assist in the scheduling of activities
as outlined in [42], which describes a knowledge-based
approach to specifying and monitoring temporal con-
straints in relational databases.
Conclusion
This paper has presented a proposal for a clinical study
architecture to support the semantic interoperability of
clinical data using the FHIR resources. We have shown
how the clinical research community is likely to benefit
from the adoption of FHIR resources to capture and man-
age clinical study data. In this regard, we have outlined a
method to link clinical data from the XML-based CDISC
ODMmodel to a selective group of FHIR resources.While
we have revealed a fit between the ODM model and the
FHIR resources, we do not regard this as a long term
solution. First, owing to the evolving nature of the FHIR
specifications, this mapping is likely to change at a whim.
Second, it is preferable to avoid data transformations but
for data to be captured directly at the source.We have thus
proposed two FHIR models, a ClinicalStudyPlan
and a ClinicalStudyData resource, and shown that
they can natively manage clinical data. We have compared
our work to the proposed HL7 PlanDefinition FHIR
resource and discussed their suitability in adequately rep-
resenting the research study protocol definition. We have
demonstrated, with the help of a working example, the fit
of our clinical data model in interpreting clinical research
data. Our work has built the foundations to not only facil-
itating the syntactic but also semantic interoperability of
clinical research data.
Endnotes
1 Extensible Markup Language [43]
2Health Level Seven
3Based on the Standard for Trial Use 3 September 2016
version
4 Systematized Nomenclature of Medicine Clinical Ter-
minology
5 Logical Observation Identifiers Names and Codes
6 FHIR-Infrastructure
7 Regulated Clinical Research InformationManagement
8 Standard for Trial Use 3
9This resource can be found at http://hl7.org/fhir/
2016Sep/plandefinition.html
10This resource can be found at http://hl7.org/fhir/
2016Sep/activitydefinition.html
11Unified Modeling Language
12Clinical Document Architecture
13Data Documentation Initiative
Additional file
Additional file 1: The file observation_example.json lists the Observation
resource as described in the Demonstrating the clinical study design with
FHIR section in json format. (JSON 2 kb)
Abbreviations
CDA: Clinical document architecture; CDASH: Clinical data acquisition
standards harmonization; CDISC: Clinical data interchange standards
consortium; CEM: Clinical element model; DSTU2: Draft standard for trial use 2;
FHIR: Fast Healthcare interoperable resources; FHIR-I: FHIR-Infrastructure; HL7:
Health level seven; ISO: International organization for standardization; LCDC:
Linked clinical data cube; LOINC: Logical observation identifiers names and
codes; ODM: Operational data model; RCRIM: Regulated clinical research
information management; SNOMED CT: Systematized nomenclature of
medicine clinical terminology; STU3: Standard for trial use 3; XML: Extensible
markup language
Leroux et al. Journal of Biomedical Semantics  (2017) 8:41 Page 13 of 14
Acknowledgements
The authors would like to thank Drs Marlien Varnfield and Parnesh Raniga for
reviewing the paper.
An initial version of this paper has been published on Researchgate at [44].
Funding
This research has not been funded from an external grant.
Availability of data andmaterials
The mapping of the clinical data from CDISC ODM to FHIR can be accessed at:
http://healthinet.it.csiro.au/net/jbs/odmFhir.
The web resources and implementation of the new FHIR resources have been
included at: http://healthinet.it.csiro.au/net/fhir/.
A working example showcasing the two FHIR resources can be found at:
http://healthinet.it.csiro.au/net/jbs/implementation.
Authors contributions
HL designed the FHIR resources that AM and ML reviewed. HL implemented
the FHIR resources and part of the clinical study examples and drafted the
manuscript. AM implemented part of the clinical study examples. AM and ML
reviewed the manuscript. All authors read and approved the final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Publishers Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Received: 2 June 2016 Accepted: 3 September 2017
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31
DOI 10.1186/s13326-017-0142-0
RESEARCH Open Access
Dynamically analyzing cell interactions in
biological environments using multiagent
social learning framework
Chengwei Zhang1, Xiaohong Li1*, Shuxin Li1 and Zhiyong Feng2
From Biological Ontologies and Knowledge bases workshop on IEEE BIBM 2016
Shenzhen, China.16 December 2016
Abstract
Background: Biological environment is uncertain and its dynamic is similar to the multiagent environment, thus the
research results of the multiagent system area can provide valuable insights to the understanding of biology and are
of great significance for the study of biology. Learning in a multiagent environment is highly dynamic since the
environment is not stationary anymore and each agents behavior changes adaptively in response to other coexisting
learners, and vice versa. The dynamics becomes more unpredictable when we move from fixed-agent interaction
environments to multiagent social learning framework. Analytical understanding of the underlying dynamics is
important and challenging.
Results: In this work, we present a social learning framework with homogeneous learners (e.g., Policy Hill Climbing
(PHC) learners), and model the behavior of players in the social learning framework as a hybrid dynamical system. By
analyzing the dynamical system, we obtain some conditions about convergence or non-convergence. We
experimentally verify the predictive power of our model using a number of representative games. Experimental
results confirm the theoretical analysis.
Conclusion: Under multiagent social learning framework, we modeled the behavior of agent in biologic
environment, and theoretically analyzed the dynamics of the model. We present some sufficient conditions about
convergence or non-convergence and prove them theoretically. It can be used to predict the convergence of the
system.
Keywords: Multiagent learning, Cell interaction, Nonlinear dynamic
Background
All living systems live in environments that are uncertain
and dynamically-changing. However, it is remarkable that
these systems survive and achieve their goals by exhibiting
intelligent features such as adaption and robustness. Bio-
logical system behaviors [1] and human diseases [2] are
often the outcome of complex interactions among a very
large number of cells and their environments [3, 4].
*Correspondence: xiaohongli@tju.edu.cn
1School of Computer Science and Technology, Tianjin University, Peiyang Park
Campus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, China
Full list of author information is available at the end of the article
Similarly, in the multiagent system [59], an important
ability of an agent is to adjust its behavior adaptively to
facilitate efficient coordination among agents in unknown
and dynamic environments. If we regard the cells in the
biological system as the agents in the multiagent system,
we can analyse the cells behavior using the theory of
multiagent system. So understanding collective decision
made by such intelligent multiagent system is an inter-
esting research topic not only for artificial intelligent but
also for biology. The conclusion of the theoretical analy-
sis can be applied to the research of biology, for example,
the results of convergence can be used for explaining the
phenomenon of cells group behaviour.
© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 44 of 79
Now, computational methods have been widely used to
solve biological problems [10, 11]. Many researchers have
investigated biological systems which are composed of
cells and their environments via modeling and simulation
[1, 12]. There are two principal approaches: population
based modeling and discrete agent based modeling. Pop-
ulation based modeling approximates the cells within any
grid box by a set of variables associated with the grid box
[13, 14]. Discrete agent based modeling maps each cell to
a discrete simulation entity [13, 15, 16].
We use multiagent learning techniques to model the
behaviors of each cell agent, which is an important tech-
nique to achieve efficient coordination in multiagent sys-
tem area [9, 1719]. Until now, significant amount of
efforts have been devoted to develop effective learning
techniques for different multiagent interaction environ-
ments [2023]. In the multiagent environments, each
agent interacts with the agent selected from its neigh-
borhood randomly each round, and updates its strategy
based on the feedback in the current round. To describe
the behavior of an agent, one common line of researches
is to extend existing reinforcement learning techniques
in single-agent environment to multiple-agent interaction
environment. However, due to the violation of Markov
property, the existing theoretical guarantees do not hold
any more in multiagent environment. It is important and
challenging for us to model the multi-agent environ-
ment and analyse the learning dynamics of multiagent
environments.
This paper presents a social learning framework to sim-
ulate the dynamics of multiagent system in biological
environment and a theoretical analysis of the learning
dynamics of this model is also given. The analysis results
shed lights on how and when the consistent knowledge in
terms of equilibrium can be or not be evolved among the
population of agents. In the social learning framework, all
agents play PHC strategy [24] for decisionmaking, and use
a weighted graphmodel for neighbor selection. In the part
of theoretical analysis, we present a theoretical model to
analyze the learning dynamics of the learning framework.
The purpose of analysing the learning dynamics is to judge
whether the learning algorithm that the agent adopt can
converge or not. The intention behind is that convergence
to an equilibrium has been the most commonly accepted
goal to pursue in multiagent learning literature. Firstly, we
model the overall dynamics among agents as a system of
differential equations. Then, some conditions are proved
to be the sufficient condition of convergence or non-
convergence. It can be used to predict the convergence
of the system. Finally, we estimate the prediction through
simulation experiment. The experimental results confirm
the predictive outcomes of our theoretical analysis.
The remainder of the paper is organized as follows.
Method section first reviews normal-form game and the
basic gradient ascent approach with a GA-based algo-
rithm named PHC, and then introduces the multiagent
learning framework where all the agents are PHC learn-
ers. In the Result and discussion section, we present
the theoretical model of the learning dynamics of agents,
and prove convergence and non-convergence conditions
by analyze geometrical behaviors of the hybrid dynamic
system in the help of nonlinear dynamic theory. In the
Experimental simulation section, we evaluate the pre-
dictive ability of our theoretical model by comparing it
with the simulation results. Lastly we conclude the paper
and point out future directions in Conclusion section.
Method
Notation and definition
Normal-form games
In a two-player, two-action, general-sum normal-form
game, the payoff for each player i ? {k, l} can be specified
by a matrix as follows,
Ri =
[
r11i r12i
r21i r22i
]
(1)
Each player i selects an action simultaneously from its
action set Ai = {1, 2}, and the payoff of each player is
determined by their joint actions. For example, if player k
selects the pure strategy of action 1 while player l selects
the pure strategy of action 2, then player k receives a
payoff of r12k and player l receives the payoff of r21l .
Apart from pure strategy, each player can also employ a
mixed strategy to make decisions. Amixed strategy can be
represented as a probability distribution over the action
set and a pure strategy is a special case of mixed strate-
gies. Let pk ? [0, 1] and pl ? [0, 1] denote the probability
of choosing action 1 by player k and player l, respec-
tively. Given a joint mixed strategy profile (pk , pl), the
expected payoffs of player l and player k can be computed
as follows,
Vk (pk , pl) =r11k pkpl + r12k pk (1 ? pl) + r21k (1 ? pk) pl
+ r22k (1 ? pk) (1 ? pl) (2)
Vl (pk , pl) =r11l pkpl + r21l pk (1 ? pl) + r12l (1 ? pk) pl
+ r22l (1 ? pk) (1 ? pl) (3)
A strategy profile is a Nash Equilibrium (NE) if no player
can get a better expected payoff by changing its current
strategy unilaterally. Formally,
(
p?k , p?l
) ? [0, 1]2 is a NE, iff
Vk
(
p?k , p?l
) ? Vk (pk , p?l ) and Vl (p?k , p?l ) ? Vl (p?k , pl) for
any (pk , pl) ? [0, 1]2.
Gradient ascent (GA) and PHC algorithm
When a game is repeatedly played, an individually ratio-
nal agent updates its strategy with the propose of max-
imizing its expected payoff. We know that the gradient
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 45 of 79
direction is the fastest increasing direction, thus it is a
well-deserved way to model the behavior of agent using
gradient ascent algorithm. Agent i that employs GA-based
algorithm updates its policy towards the direction of its
expected reward gradient, which is shown in the following
equations.
p(t+1)i ? ?
?Vi
(
p(t)
)
?pi
(4)
p(t+1)i ? [0,1]
(
p(t)i + p(t+1)i
)
(5)
The parameter ? is the size of gradient step. [0,1] is
the projection function mapping the input value to the
valid probability range of [ 0, 1], which is used for prevent-
ing the gradient from moving the strategy out of the valid
probability space. Formally, we have
[0,1](x) = argminz?[0,1] |x ? z| . (6)
To simplify the notation, let us define ui = r11i + r22i ?
r12i ? r21i and ci = r12i ? r22i . For the two-player case, the
Eqs. 4 and 5 can be represented as follows,
p(t+1)k ? [0,1]
(
p(t)k + ?
(
ukp(t)l + ck
))
(7)
p(t+1)l ? [0,1]
(
p(t)l + ?
(
ulp(t)k + cl
))
. (8)
In the case of infinitesimal size of gradient step (? ? 0),
the learning dynamics of the agent can be modeled as a
system of differential equations. Further, it can be ana-
lyzed using dynamic system theory [25]. It is proved that
the strategies of all agents will converge to a Nash equilib-
rium, or if the strategies do not converge, agents average
payoff will converge to the average payoff of Nash equilib-
rium [26]. The policy hill-climbing algorithm (PHC) is a
combination of gradient ascent algorithm and Q-learning
where each agent i adjusts its policy p to follow the gra-
dient of expected payoff (or the value function Q). It is
shown in the Algorithm 1.
Here, ? ? (0, 1] and ? ? (0, 1] are learning rate, and
Q values are maintained just as in normal Q-learning.
The policy is improved by increasing the probability of
selecting the highest valued action based on the learning
rate ?.
Modeling multiagent learning
Under the multiagent social learning framework with N
agents, each agent interacts with one of its neighbors
selected randomly from its neighborhood each round.
The neighborhood of each agent is determined by its
underlying network topology. The interaction between
each pair of agents is modeled as a two-player normal-
Algorithm 1 The policy hill-climbing algorithm (PHC)
for agent i ? {r, c}
1: Let ? ? (0, 1] and ? ? (0, 1] be learning rates.
Initialize Qi (a) ? 0, pi (a) ? 1|Ai| .
2: repeat
3: Select action a ? Ai according to mixed strategy pi
with suitable exploration.
4: Observe reward r and update Q value
Qi (a) ? (1 ? ?)Qi(a) + ?r
5: Step p closer to the optimal policy w.r.t. Q,
pi(a) ? pi(a) + a
while constrained to a legal probability distribution,
a =
{ ??a a = argmaxa?Qi(a?)?
a? =a ?a? otherwise
?a = min
(
pi(a), ?|Ai|?1
)
6: until the repeated game ends
form game. During each interaction, each agent selects
its action following a specified learning strategy, which
is updated repeatedly based on the feedback from the
environment at the end of interaction. The framework is
presented in Algorithm 2.
Algorithm 2 Overall interaction protocol of the social
learning framework
1: repeat
2: for each agent in the population do
3: Chose one of its neighbors with a certain proba-
bility.
4: Play a two-player normal-form game with this
neighbor.
5: Select a action according to its mixed strategy
with suitable exploration.
6: end for
7: Environmental feedback.
8: for each agent in the population do
9: Observing reward r and update its policy based
on its past experience according to specific poli-
cies.
10: end for
11: until the repeated game ends
We use graph G = (V ,E) to model the underlying
neighborhood network, which is composed by N = |V |
agents. The edges E = {eij}, i, j ? V represent social
contacts among agents, where eij denotes the probabil-
ity that agent i chooses agent j to interact with. We have?
j?V eij = 1?eii = 0. Here, we propose an adaptive strat-
egy for agents to make their decisions in social learning
framework with PHC learning strategy, which is shown in
Algorithm 3.
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 46 of 79
Algorithm 3 Learning process in the multiagent
framework for agent i ? V
1: Let ? ? (0, 1] and ? ? (0, 1] be learning rates.
Initialize Qi (a) ? 0, pi (a) ? 1|Ai| .
2: repeat
3: Select agent j ? V according to E with probability
eij.
4: Select action a ? Ai according to mixed strategy pi
with suitable exploration.
5: Observe reward r according to interaction between
i and j.
6: Update Q value
Qi (a) ? (1 ? ?)Qi(a) + ?r
7: Step p closer to the optimal policy w.r.t. Q,
pi(a) ? pi(a) + a
while constrained to a legal probability distribution,
a =
{ ??a a = argmaxa?Qi(a?)?
a? =a ?a? otherwise
?a = min
(
pi(a), ?|Ai|?1
)
8: until the repeated game ends
Result and discussion
Analysis of the multiagent Learning Dynamics
In this section, we present a theoretical model to estimate
and analyze the learning dynamics of the abovemultiagent
learning framework in Algorithm 3. We extend notations
in section to the multiagent environment. Without loss of
generality, we consider the case with two-action only.
Assume that the payoff that an agent receives only
depends on the joint action, then the payoff for agent
i ? V can be defined as a fixed matrix Ri,
Ri =
[
r11i r12i
r21i r22i
]
(9)
where rmni denotes the payoff received by agent i when
i selects action m and its neighbor selects n. Here, we
use the pi to denote the probability that the player i
selects action 1. Then the mixed strategy (p1, p2, . . . , pN )
in multiagent framework can be considered as a point in
R
N constrained to the unit square. The expected payoff
Vi (p1, p2, . . . , pN ) of player i can be computed as follows,
Vi(p1, p2, . . . , pn)
=
?
j?V eijVi,j
(
pi, pj
)
=uipi
?
j?V eijpj + cipi +
(
r21i ? r22i
)
pj + r22i
(10)
where ui = r11i + r22i ? r12i ? r21i , ci = r12i ? r22i ,
Vi,j
(
pi, pj
) = r11i pipj + r12i pi (1 ? pj) + r21i (1 ? pi) pj +
r22i (1 ? pi)
(
1 ? pj
)
, and eij is the probability that the
agent i selects agent j to interact with.
Each agent i updates its strategy in order to maximize
the value of Vi. Recall the Eqs. 4 and 5, we can obtain
p(k+1)i =
?

[
p(k)i + ??piVi(p1, p2, . . . , pN )
]
=
?

[
p(k)i + ?
(
ui
?
j?V eijpj + ci
)] (11)
where parameter ? is the size of gradient step.
As ?p ? 0, it is straightforward that the Eq. 11 becomes
differential equation. Considering the step size to be
infinitesimal, the unconstrained dynamics of the all play-
ers strategies can be modeled by the following differential
equations.
p?i = ui
?
j?V eijpj + ci, i ? {1, 2, . . . ,N} (12)
Equation 12 can be simplified as follows using some
notation,
P? = UEP + C (13)
where P = (p1, p2, . . . , pN )T , P? = (p?1, p?2, . . . , p?N )T
and C = (c1, c2, . . . , cN )T . The matrix U =
diag(u1,u2, . . . ,uN ) is the diagonal matrix generated by
(u1,u2, . . . ,uN ).
For the constrained dynamics of the strategies, we can
model it as the following equations,??
?
p?i = 0 pi = 0 ? Gi ? 0
p?i = 0 pi = 1 ? Gi ? 0
p?i = Gi otherwise
(14)
where Gi = ui ?j?V eijpj + ci.
Notice that Eq. 14 is a hybrid system composed of two
parts: a series of continuous linear differential dynamic
systems in the respective domain space and a switch
mechanism between differential dynamic systems when
dynamic touch the boundary. Generally, it is hard to
obtain a complete conclusion by analyzing dynamics of a
general hybrid system, even though the differential sys-
tem is linear. But we can still find some convergence and
non-convergence conditions under certain instances(i.e.,
Eq. 14).
Non-convergence condition of the multiagent learning
framework
According to the above definition, we have the fol-
lowing general result under which non-convergence is
guaranteed.
Theorem 1 In an N agent, two-action, integrated gen-
eral sum game, every player follows the constrained
dynamics of the strategy we defined in Eq. 14. If the follow-
ing two conditions are met,
1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,
that UEP? + C = 0,
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 47 of 79
2. There exists a pair of pure imaginary eigenvalues of
matrix UE,
then there exists a set P ? [ 0, 1]N , that the solution of
the initial value problem of Eq. 14 with P(0) ? P can not
converge.
Proof Considering the complexity of the hybrid system
represented by Eq. 14, we begin with the unconstrained
ones. Based on the theorems of differential equations
dynamical systems [25], we calculate the analytic solution
of Eq. 13. Homogenizing the in-homogeneous equation by
substituting P with P = X + P?, where UEP? + C = 0, we
get
X? = UEX.
Here, UE is an N × N matrix, then there is a invertible
matrix T = (v1, . . . , vN ) that can transform UE into J,
T?1UET = J =
?
??
J1 · · ·
... . . .
...
· · · Jm
?
??
The Ji is a square matrix and its form is one of the
following two,
(1)
?
????
? 1 · · ·
? 1
... . . .
...
· · · ?
?
???? (2)
?
????
D2 I2 · · ·
D2 I2
... . . .
...
· · · D2
?
????
where D2 =
[
? ?
?? ?
]
, I2 =
[
1 0
0 1
]
, ?,? , ? ? R and
? = 0. Here, J is the Jordan normal form of matrixUE. Ji is
the Jordan block corresponding to ?i, which is a repeated
eigenvalue of UE with multiplicity ni. If eigenvalue ?i is a
real number, then Ji is in the form (1), else Ji is in the form
(2). Suppose that ?1, . . . , ?k are matrix UEs real eigenval-
ues, and ?k+1, . . . , ?m is matrix UEs complex eigenvalues,
then we have n1 + . . . + nk + 2
(
nk+1 + . . . nm
) = N .
Then the analytic solution of function X? = UEX with
initial value X(0) will be
X(t) = exp (tUE)X(0) = T
?
??
etJ1
. . .
etJm
?
??T?1X(0).
Using the notation Y (t) = T?1X(t), we have
Y (t) = exp (tJ)Y (0) =
?
??
etJ1
. . .
etJm
?
??Y (0).
Suppose that ?k = ?i is a pure imaginary eigenvalue
of UE with multiplicity nk , so ?¯k = ??i is an eigenvalue
of UE with multiplicity nk . Then J has a block Jk , Jk =?
????
D2 I2 · · ·
D2 I2
... . . .
...
· · · D2
?
????, where D2 =
[
0 ?
?? 0
]
.
Due to etD2 = exp
(
t
[
0 ?
?? 0
])
=
[
cos?t sin?t
? sin?t cos?t
]
,
there must exist a pair of items about vector Y (t) as
follows.
{
yi(t) = yi(0) cos?t + yi+1(0) sin?t
yi+1(t) = ?yi(0) cos?t + yi+1(0) sin?t
If yi(0) = 0 ? yi+1(0) = 0, then Eq. 14 has a peri-
odic solution. Let vi and vi+1 to denote eigenvector of
T = (v1, . . . , vN ) corresponding to ?k and ?¯k , respectively.
Note that X(t) = TY (t), then the solution of Eq. 13 with
the initial value P(0) ? S is cyclical, where
S = {P ? [ 0, 1]N |P = k1v1 + k2v2 + P?, k1, k2 ? R} .
Because of P? ? (0, 1)N , there must exists a ? > 0 for
the deleted neighborhood B(P?; ?) ? (0, 1)N of P?,
B(P?; ?) = {x ? RN |0 < ||x ? P?||2 < ?} ? (0, 1)N
Let P denote S
?
B(P?; ?), the solution of the Eq. 14 with
any initial value belongs to P is cyclical, which means the
algorithm corresponding to the Eq. 14 can not converge.
Theorem 1 shows that there exist some situations in
which the agents fail to converge under the multiagent
social learning framework. Before giving the details of
those situations, we need to introduce the following nota-
tions first.
According to the theorem 1, T is the transformation
matrix for T?1UET = J , T = (v1, v2, . . . , vN ). Let
vj1, vj2, . . . , vjnj denote eigenvectors associated to eigen-
value ?j, j = 1, 2, . . . ,m. According to properties of the
matrix transformations [27], vj1, vj2, . . . , vjnj are linearly
independent. Classify column vectors of the transfor-
mation matrix T into three parts corresponding to ?,
V1 = {vi|Re(?i) < 0}, V2 = {vi|Re(?i) = 0} and
V3 = {vi|Re(?i) > 0}. Now we are ready to give the pre-
cise description of the subspace where the agents fail to
converge, which is summarized in the following theorem.
Theorem2 If Eq. 14meets both conditions of Theorem 1,
and ?k = ?i, ?k = ??i are a pair of pure imaginary eigen-
values of UE, then there exists a pair of vectors vk , v?k ? V2,
 > 0, and a set P = S ? B(P?; ?), where
S=
{
P ? [ 0, 1]N |P=X + P?,X ? span(V1 ? {vk , v?k})
}
,
B(P?; ?) = {x ? RN |0 < ||x ? P?||2 < ?} ? [ 0, 1]N ,
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 48 of 79
that the solution of the initial value problem of the Eq. 14
with P(0) ? P cant convergence.
Proof According to Theorem 1, we have the solution of
the initial value problem that the Eq. 14 with P(0) ? S ?
B(P?; ?) can not convergence. Here
S = {P ? [ 0, 1]N |P = X + P?,X ? span({vk , v?k})}
For the eigenvalue ?i associated to vector vi ? V1, there
are Re(?i) < 0. According to conclusions of bifurcation
theory [25], the subspace span(V1) is a stable submanifold
of the unconstrained dynamics (13), which means every
trajectory start from S? will eventually convergence to P?,
where
S? = {P ? [ 0, 1]N |P = X + P?,X ? span(V1)} .
Then trajectories start from S will eventually conver-
gence to S, thus we got the final conclusion that the
solution of the initial value problem of the Eq. 14 with
P(0) ? P cant convergence.
Note that Theorem 1 and 2 are just sufficient conditions
of non-convergence.
Convergence condition of the multiagent learning
framework
In most cases, the conditions that guarantee the conver-
gence of a algorithm are more valuable.
Theorem 3 In an N agent, two-action, integrated gen-
eral sum game, every player follows the constrained
dynamics of the strategy we defined in Eq. 14. If the follow-
ing two conditions are met,
1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,
that UEP? + C = 0,
2. All of the eigenvalues of matrix UE has negative real
part,
then all the solutions of the initial value problem of Eq. 14
with P(0) ? [0, 1]N will converge eventually.
Proof The conclusion is obvious. It is known that the
construction of the linear dynamic system is stable. If all
eigenvalues of matrix UE have negative real part, then
point P is a stable equilibrium point. It means that all the
solutions of the initial value problem of the Eq. 14 with
P(0) ? [0, 1]N will converge to P.
Theorem 3 proposes a sufficient condition to identify
the convergence of dynamic in Eq. 14. We know that
it is hard to calculate eigenvalues of a matrix with high
dimensional. Here, we propose a more realistic conver-
gence condition which is suitable for multiagent learning
framework shown in Algorithm 3.
Theorem 4 In an N agent, two-action, integrated gen-
eral sum game, every player follows the constrained
dynamics of the strategy we defined in Eq. 14. If matrix
UE is symmetrical, then all the solution of the initial
value problem of Eq. 14 with P(0) ? [0, 1]N will converge
eventually.
Proof It is known that the eigenvalues of real symmetric
matrix are real numbers [27]. We analyze all the cases of
Eq. 14 when all of the eigenvalues of matrix UE are real:
1. There exists a point P? = (p?1, p?2, . . . , p?N) ? (0, 1)N ,
that UEP? + C = 0.
2. There are no such a point, that UEP? + C = 0.
For case 1), if all eigenvalues of matrix UE are negative
number, then point P is a stable equilibrium points; oth-
erwise, all the solutions of the initial value problem of the
hybrid system with P(0) ? [0, 1]N will move away from P
toward boundary of the hybrid system [25]. Because the
domain of hybrid system represented by 14 has bound-
ary(i.e., P(t) ? [0, 1]N ), then there must exists a point
P? = (p?1, . . . , p?N)T in the boundary of the domain, where
(p?i = 0 ? Gi ? 0) ? (p?i = 1 ? Gi ? 0) for all i ? V . The
dynamic P(t) will converge to P? eventually.
Similarly, we can find a point P? = (p?1, . . . , p?N)T in the
boundary of the hybrid system domain in case 2) and the
dynamic P(t) will converge to P? eventually. The theorem
must hold.
Based on conclusions of Subsections Non-convergence
condition of the multiagent learning framework and
Convergence condition of the multiagent learning frame-
work , we can determine the learning dynamics of any
cases we defined in Eqs. 14 and 13. However, the com-
putational complexity may be prohibitive when the model
size becomes too large. In the next section, we consider
a special case under an interesting network structure
which can be analyzed with relatively light computational
complexity for any network size.
The simplest case whose underlying topology is a ring
We consider the case when the underlying topology is a
ring, and each agent only interacts with the neighbor on
its right-hand side in each interaction. As defined in the
previous section, the adjacency matrix E is
E = {eij}N×N , i, j ? {1, 2, . . . ,N},
where eij =
{
1 j = (i + 1)modN
0 else .
According to Eq. 14, the constrained dynamics of this
special case can be modeled as follows:??
?
p?i = 0 pi = 0 ? Gi ? 0
p?i = 0 pi = 1 ? Gi ? 0
p?i = Gi otherwise
(15)
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 49 of 79
where Gi = uipi+1 + ci, i = {1, 2, . . . ,N ? 1}, and GN =
uNp1+cN . Through analyzing the dynamics of this model,
we have the following conclusion.
Theorem 5 In an N-player, two-action, integrated
general-sum game, every agent follows the constrained
dynamics of the model in Eq. 15. If one of the agents
converges to a strategy, then every agent will converges
eventually.
Proof Suppose agent k converges at some time, accord-
ing to the definition, its strategy pk will be a constant. In
Eq. 15, we have Gk?1 = uk?1pk + ck?1 be a constant,
which means convergence of player k implies convergence
of player k ? 1. By induction, every agent will converge
eventually.
According to the above theorem, we can easily obtain
the following proposition.
Proposition 1 In Eq. 15, if there exists a dominant
strategy for some players, then their strategies will asymp-
totically converge to a Nash equilibrium.
According to the above conclusion, finally we present
the following unconvergence result.
Theorem 6 In an N agent, two-action, integrated gen-
eral sum game, every player follows the constrained
dynamics of the strategy we defined in Eq. 15. If every
player has no dominant strategy, and met one of the
following conditions,
1. N = 4k, k ? N and ?Ni=1 ui > 0.
2. N = 4k + 2, k ? N and ?Ni=1 ui < 0.
then there exists a set P ? [0, 1]N that the solution of the
initial value problem of the Eq. 15 with P(0) ? P cant
converge.
Proof According to the definitions above, the payoff
matrix of player i is
Ri =
[
r11i r12i
r21i r22i
]
, i ? {1, 2, . . . ,N},
and ui = r11i + r22i ? r12i ? r21i , ci = r12i ? r22i . Then we have
uici
= (r11i + r22i ? r12i ? r21i ) (r12i ? r22i )
= (r11i ? r21i ) (r12i ? r22i ) ? (r12i ? r22i )2
(16)
Since every agent has no dominant strategy, we have(
r11i ? r21i
) (
r12i ? r22i
)
< 0.
Thus we have uici < 0, and
ci
ui
= ?
(
r12i ? r22i
)
(
r11i ? r21i
) ? (r12i ? r22i ) =
1
1 +
(
r21i ?r11i
)
(
r12i ?r22i
)
.
Set p?i = ? ciui and P? =
(
p?1, p?2, . . . , p?N
)T , then we have
P? ? (0, 1)N andUEP+C = 0. Considering the Eq. 15, by
calculating the eigenvalue of matrix UE, we have
?N = u1u2 . . .uN =
N?
i=1
ui.
If N = 4k, k ? N and ?Ni=1 ui > 0, then matrix UE
has a pair of pure imaginary eigenvalue. Otherwise, ifN =
4k+2, k ? N and?Ni=1 ui < 0, thenmatrixUE has a pair of
pure imaginary eigenvalue. According to Theorem 1, there
exists a set P ? [0, 1]N that the solution of the initial value
problem of Eq. 15 with P(0) ? P can not convergence.
Experimental simulation
In this section, we compare the empirical dynamics of
the multiagent social learning framework composed by
PHC learners with theoretical prediction of our hybrid
dynamic model. We perform two experiments that satisfy
the Theorem 1 and 4, respectively.
A non-convergence multiagent Game
In this subsection, we consider a 4-player, two-action
game. The game is defined as follows,
R1 =
[
1 0
0 1
]
,R2 =
[
1 0
0 1
]
,R3 =
[
1 0
0 1
]
,R4 =
[
1 0
0 1
]
E =
?
???
0 1/2 0 1/2
1/2 0 1/2 0
0 1/2 0 1/2
1/2 0 1/2 0
?
???
Metrix Ri, i ? {1, 2, 3, 4} is the payoff matrix of agent i,
and element eij of matrix E is the probability that player i
selects player j in each interaction. In this game, we have
u1 = u3 = 2, u2 = u4 = ?2, c1 = c3 = ?1, and c2 = c4 =
1. Then the unconstrained dynamic model of this game is
P? = UEP + C, where
UE =
?
???
0 1 0 1
?1 0 ?1 0
0 1 0 1
?1 0 ?1 0
?
??? ,C = (?1, 1,?1, 1)T .
This game has a P? = (1/2, 1/2, 1/2, 1/2)T ? (0, 1)4,
which satisfies UEP? + C = 0. Matrix UE has a pair of
pure imaginary eigenvalues which is ?1 = 2i and ?1 =
2i. The eigenvectors are v1 = (0, 1/2, 0, 1/2)T and v2 =
(1/2, 0, 1/2, 0)T corresponding to ?1 and ?2. Let P(0) =
P?+k1v1+k2v2. As long as k1 and k2 are sufficiently small,
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 50 of 79
0 500 1000 1500 2000 2500 3000
0
0.2
0.4
0.6
0.8
1
Round
P
p1
p2
p3
p4
Fig. 1 Agent dynamics of game satisfying the conditions of Theorem 1
according to Theorem 1, the solution of the initial value
problem of game 1 with P(0) cant converge.
In Fig. 1, the dynamic solution of the game with initial
value P(0) is plotted, where k1 = k2 = 0.1. Each of the
four lines in Fig. 1 shows the strategys dynamic changing
of each agent, respectively. We can see that the strategies
of those agents do not converge. Obviously, the simulation
results are consistent with the theoretical prediction.
A convergence multi-agent Game
In this subsection, we consider a 4-player, two-action
game. The game is defined as follows,
Ri =
[
1 0
0 1
]
, i ? {1, 2, 3, 4}
E =
?
???
0 1/2 0 1/2
1/2 0 1/2 0
0 1/2 0 1/2
1/2 0 1/2 0
?
???
Metrix Ri, i ? {1, 2, 3, 4} is the payoff matrix of agent i,
and element eij of matrix E is the probability that player i
selects player j in each interaction. In this game, we have
ui = 2 and ci = ?1, i ? {1, 2, 3, 4}. Then the uncon-
strained dynamic model of this game is P? = UEP + C,
where
UE =
?
???
0 1 0 1
1 0 1 0
0 1 0 1
1 0 1 0
?
??? ,C = (?1,?1,?1,?1)T .
Because matrix UE is symmetrical, according to
Theorem 4, the solution of the initial value problem of this
game with any P(0) ? [0, 1]4 will converge eventually.
Figure 2 illustrates dynamics of the PHC learners strat-
egy for the game with initial value initial value P(0) =
(1/2, 1/2, 1/2, 1/2)T . Each of the four lines in Fig. 2 shows
Fig. 2 Agent dynamics of game satisfying the conditions of Theorem 4
The Author(s) Journal of Biomedical Semantics 2017, 8(Suppl 1):31 Page 51 of 79
the strategys dynamic changing of each agent, respec-
tively. We can see that the strategies of those agents con-
verge eventually, which are consistent with the theoretical
prediction.
Conclusion
In this work, we proposed a multiagent social learning
framework to model the behavior of agent in biologic
environment, and theoretically analyzed the dynamics of
multiagent social learning framework using non-linear
dynamic theories. We present some sufficient conditions
about convergence or non-convergence and prove them
by the theoretically analysis. It can be used to predict the
convergence of the system. Experimental results show that
the predictions of our dynamic model are consistent with
the simulation results.
As future work, more extensive study of the dynam-
ics of multiagent social learning framework with PHC
learners is needed. Other worthwhile directions include
to improve the PHC algorithm, to develop more realistic
multiagent social learning framework to model the realis-
tic interactions among cells in biologic environments, and
to achieve better convergence performance based on our
theoretical findings.
Acknowledgements
We thank the reviewers valuable comments for improving the quality of this
work.
Funding
This work has partially been sponsored by the National Science Foundation of
China (No. 61572349,61572355).
Availability of data andmaterials
All data generated or analysed during this study are included in this published
article.
About this supplement
This article has been published as part of Journal of Biomedical Semantics
Volume 8 Supplement 1, 2017: Selected articles from the Biological Ontologies
and Knowledge bases workshop. The full contents of the supplement are
available online at https://jbiomedsem.biomedcentral.com/articles/
supplements/volume-8-supplement-1.
Authors contributions
CZ contributed to the algorithm design and theoretical analysis. SL had a main
role in the editing of the manuscript. XL and ZF contributed equally to the the
quality control and document reviewing. All authors read and approved the
final manuscript.
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Author details
1School of Computer Science and Technology, Tianjin University, Peiyang Park
Campus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, China.
2School of Computer Computer Software, Tianjin University, Peiyang Park
Campus: No.135 Yaguan Road, Haihe Education Park, 300350 Tianjin, China.
Published: 20 September 2017
