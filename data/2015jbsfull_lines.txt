JOURNAL OF
BIOMEDICAL SEMANTICS
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 
DOI 10.1186/s13326-015-0013-5
REVIEW Open Access
Linking gene expression to phenotypes via
pathway information
Irene Papatheodorou*, Anika Oellrich and Damian Smedley
Abstract
Establishing robust links among gene expression, pathways and phenotypes is critical for understanding diseases and
developing treatments. In recent years there have been many efforts to develop the computational means to traverse
from genes to gene expression, model pathways and classify phenotypes. Numerous ontologies and other controlled
vocabularies have been developed, as well as computational methods to combine and mine these data sets and
establish connections. Here we discuss these efforts and identify areas of future work that could lead to a better
integration of genes, pathways and phenotypes to provide insights into the mechanisms under which gene
mutations affect expression and pathways and how these effects are manifested onto the phenotype.
Keywords: Phenotypes, Pathways, Gene expression, Ontologies
Introduction
A fundamental aspect of disease research involves the
understanding of biological processes that underpin
observed phenotypes. In order to achieve this level of
understanding, diseases need to be described as collec-
tions of measured phenotypes and these phenotypes need
to be analysed in relation to their genetic causes and
genomic effects and linked with information on molecu-
lar interactions. One consequence of these efforts could
be the ability to produce predictive models of phenotypes
from genomic profiles with the aim of describing diseases
more accurately. Such models will be helpful in under-
standing the genetic basis and molecular mechanisms
leading to complex or rare developmental diseases and the
process of ageing, as well as the characterisation and pro-
gression of cancer types. In particular, models built from
model organism datasets can be translated into insights
on humans in areas such as disease gene identification and
drug target testing.
Methods for assigning genotypes to phenotypes have
been developed and used intensively [1-3]. Thesemethods
include genome wide association studies (GWAS) that are
applied to identify causative genotypes for various con-
ditions and phenotypes. For example, a case-controlled
*Correspondence: ip8@sanger.ac.uk
Equal contributors
Mouse Developmental Genetics, Wellcome Trust Sanger Institute, Wellcome
Trust Genome Campus, CB1 10SA, Hinxton, UK
genome wide association study identified five loci to be
associated with the susceptibility of osteoarthritis [4].
However, the identification of loci (and with that the geno-
type) still leaves a gap as to what molecular mechanisms
are at play to yield the observed phenotype. As a conse-
quence, GWAS studies are usually followed by functional
experiments, trying to unravel the biological mechanisms
that could influence the phenotype given the identified
genotype.
A functional follow-up experiment to fill the gap
between genotype and phenotype is the assessment of
expression levels of genes in the vicinity of the identified
GWAS loci in one or more tissues. In the study concern-
ing osteoarthritis [4], the authors investigated further the
gene expression and the protein expression of associated
genes using RT-PCR (genes) and immunohistochemical
staining. Through these functional studies, they identified
high levels of nucleostemin (encoded by the GNL3 gene)
in osteoarthitis patients.
A potential next step in connecting the identified geno-
type with the phenotype is to establish a link between the
expression of genes and the observed phenotypes, which
has been attempted by numerous studies [5-7]. A recent
example involves the characterisation of phenotypes in
yeast using high-throughput transcriptomic analyses [5].
Data classification methods have been used extensively
to characterise healthy or diseased tissue [6] from the
context of gene expression.
© 2015 Papatheodorou et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 2 of 7
In the examples of the GWAS and high-throughput gene
expression studies, although the genetic and genomic out-
comes of the disease can be associated with phenotypes,
the biological events leading to the phenotype at the sys-
tems level are not discovered. Signalling and metabolic
pathway analyses can inform on the specific mechanisms
of the genetic causes of the phenotypes. Recent work by
Harper et al. [8] presents a method for augmenting path-
way data with phenotypes from high-throughput genetic
screens in bacteria in order to discover causative genes.
To date, many databases (e.g. [9-11]) and ontologies (e.g.
[12-14]) have been developed to describe genes, path-
ways and phenotypes across different species (see Figure 1
and Additional file 1). However, the semantic integra-
tion of these resources needed to computationally anal-
yse the experimental set-up described above, is still at
its infancy. This hinders the development of generalised
data analysis methods that combine genes, gene expres-
sion, pathways and phenotypes. Here, we identified three
areas of research that need to be further developed in
order to facilitate computational prediction of the biolog-
ical mechanisms that link genotypes to phenotypes: (i)
the ontological characterisation of phenotypes, (ii) linking
gene expression and phenotypes and (iii) linking pathways
and phenotypes. We describe the current state-of-the-art
for each of the three areas in the following sections indi-
vidually and highlight potential future challenges where
identified.
Ontological characterisation of phenotypes
Due to the availability of phenotype data from several
model organisms (see Figure 1, e.g., phenotypes from
the International Mouse Phenotyping Consortium [15]),
options are not limited to human systems but may include
data from several different species. Furthermore, data
obtained through different experiments and stored in
different data resources can vary in the detail the infor-
mation is represented with [16]. As a consequence, three
major aspects of data integration need to be addressed:
(i) the integration across the different levels of complex-
ity within an organism, (ii) the integration across species,
and (iii) the frequencies of occurrences of phenotypes
(quantification). These three aspects are further illus-
trated in Figure 2. To facilitate data integration, numerous
Figure 1 Databases and ontologies for information on genes, pathways and phenotypes. The diagram shows the information flow from
genes to phenotypes via pathways. There are a large number of databases storing gene expression and other genomic data, with most of them
species specific that include links to a phenotype ontology term. In addition, there is a large number of phenotype ontologies that are not organism
specific, such as a mammalian phenotype ontology and the cellular phenotype ontology (CMPO). There exist a few databases providing genotype
to phenotype links, although most of this information is covered by species specific genomic databases. There are many small-scale species specific
or pathway-type specific databases and a few large general pathway databases (KEGG, Pathway Commons, REACTOME). Pathway ontologies exist
but are not widely used yet. Although in general there are good links among genes and pathways and genes and phenotypes, associations
between pathways and phenotypes are lacking.
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 3 of 7
Figure 2 Three challenges when representing and comparing phenotypes. The diagram illustrates the three challenges that need to be
overcome in order to link gene expression and phenotypes using pathways: (A) the integration across the different levels of complexity within an
organism, (B) the integration across species, and (C) the frequencies of occurrences of phenotypes (quantification)  purple colour represents
individuals possessing phenotype of interest (examples in parentheses taken from Angelman syndrome in OrphaNet).
ontologies have been developed that define the meaning
of biological concepts, such as the Gene Ontology (GO)
[13].
Integration across different levels of organismal
complexity
Phenotypes span different levels of complexity and range
from a molecular level to the entire organism, such as
the cellular level, the tissue level or the organ level (see
Figure 2(A)) [17]. Existing biomedical ontologies cover
several levels of complexity, e.g., ontologies that repre-
sent gene function (GO) as well as tissue information
(e.g. BRaunschweig ENzyme DAtabase (BRENDA) tissue
ontology (BTO) [14]) or organism level (e.g. the Mam-
malian Phenotype Ontology (MP) [12] or the Human
Phenotype Ontology (HPO) [18]). In order to facilitate
reasoning over the different levels of complexity needed
to describe an individual with ontologies, the ontolo-
gies have to be aligned and mapped to one another.
While mapping efforts are ongoing to align ontolo-
gies across species covering the same level of com-
plexity, e.g., the alignment of anatomy as provided by
UBERON [19], the seamless integration of ontologies
across the different levels of complexity is still ongoing
work.
Integration across species
In order to computationally compare phenotypes across
different species, the existing phenotype data needs to
be semantically annotated in a way that would facilitate
the comparison. Traditionally, model organisms as well
as human data were semantically represented using pre-
composed phenotype ontologies. In a pre-composed phe-
notype ontology such as MP or HPO, one concept cor-
responds to one phenotype and can directly be used
for annotation. However, a comparison is only possible
as long as the same pre-composed ontology is used for
annotation.
To overcome this limitation of pre-composed pheno-
type ontologies, post-composed phenotype representa-
tions have been suggested. One approach that is broadly
used, for example to post-compose MP and HPO and
represent zebrafish mutants in the Zebrafish Model
Organism database [20], is the description of phenotypes
using Entity-Quality (EQ) statements. Entity-Quality (EQ)
statements enable the composition of phenotypes using
species-independent ontologies [21], e.g. GO (for the rep-
resentation of processes) or UBERON (a cross-species
anatomy ontology). While some of the statements have
been generated and verified automatically [22], manual
verification is still needed to ensure the correct repre-
sentation. How species can be compared based on pre-
and post-composed phenotype annotations is illustrated
in Figure 2(B).
The applicability of the generated EQ statements is
demonstrated by their usage in a variety of projects,
which e.g., predict the involvement of genes in diseases
and pathological processes [2,3] and gene function [23].
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 4 of 7
Despite the successful applications of pre- and post-
composed phenotype annotations, the harmonised appli-
cation of phenotypes in conjunction with gene, expression
and pathway data is still very limited.
Frequencies of occurring phenotypes
The quantification of phenotype data is beginning to
become available: databases such as OrphaNet [24]
describe disease phenotypes with additional quantifiers,
e.g., the phenotype dwarfism (OrphaNet clinical sign
id: 53350) is very frequent in patients with a 12q14
micro deletion syndrome (OrphaNet disorder id: 12544)
or the phenotype strabismus (OrphaNet clinical sign id:
5870) is occurring occasionally in patients with Angel-
man syndrome (OrphaNet disorder id: 90). OrphaNet
assigns phenotype annotations and frequency informa-
tion represented with an OrphaNet-specific vocabulary
(see Figure 2(C)).
A similar strategy has been applied to annotate human
genetic disorders described in the Online Mendelian
Inheritance inMan (OMIM) database [9]. Each disorder is
described using concepts of the HPO and, optionally, fre-
quency information can be added to each of the assigned
phenotype annotation [25]. Despite great efforts, fre-
quency information is not available for all the annotations
assigned and only available via the download file.
While clinical databases already work on the inclu-
sion of quantified phenotype data, model organism
databases lag behind by not providing this information.
Thus, quantified phenotype information cannot yet be
used for cross-species data analysis and computational
modelling.
Linking gene expression to phenotypes
The ease of obtaining whole genome expression datasets
has enabled more thorough classification of phenotypes
associated with the expression of sets of genes [6]. A large
number of studies attempt to identify groups of genes
whose expression is responsible for a particular pheno-
type, such as disease or tissue morphology [26]. More
complex experimental designs attempt to associate the
phenotype with dynamic or systems views of gene expres-
sion [27]. The techniques used to link the phenotype
to causative patterns of gene expression largely depend
on the experimental design and the technology used to
profile gene expression.
Gene expression signatures
In order to characterise a phenotype in terms of gene
expression, most studies attempt to identify the minimum
number of genes whose expression patterns determine the
phenotype in question. This group of genes is referred to
as a gene signature in the literature and once defined and
validated has important practical applications to disease
diagnosis and prognosis, as well as the discovery of new
therapies. In cancer genomics, for example, classifications
of tumour types from high-throughput gene expression
and/or copy number profiles have helped unravel the
complexity of different cancer types and have led to a bet-
ter understanding of cancer progression and the identifi-
cation of new diagnostic biomarkers. For example, Marisa
et al. [6] produced a transcriptome-based classification of
566 colon cancer samples to discover six different molec-
ular subtypes of the disease, that associated with distinct
clinicopathological characteristics and corresponded to
different relapse times. Aravinthan et al. [28] defined a
signature of 40 genes that appear upregulated in hepato-
cyte senescence as opposed to controls and then validated
this by finding enrichment of these genes in public data
sets representing liver conditions such as steatohepatitis,
alcohol-related hepatitis and HCV-related cirrhosis [28].
Given enough data sets, existing data mining methods
can assign patterns of gene expression to the phenotypes
under study. Although the linkage of gene expression sig-
natures to phenotype associations is an important step
in determining the causal link between genotypes and
phenotypes, it is still difficult to establish the underly-
ing biological mechanism from gene expression data sets
alone.
Complex experimental designs
More complex experimental designs are used in order to
refine the mechanisms under which gene expression can
lead to a certain phenotype. Here, the experimental design
attempts to address issues such as the influence of envi-
ronmental factors, time and interplay between tissues. An
individual study example comes from the work by Äijö
et al. [27] where statistical modelling based on Gaussian
processes is used to analyse the differentiation of human
Th17 cells. The authors expose CD4+ T cells to two dif-
ferent types of ligands and record the gene expression
using RNA-seq over five different time-points. The anal-
ysis can describe the dynamics and provide insight into
the kinetics of gene expression that lead to the different
outcomes of T cell activation depending on the ligands
used.
Tissue-specific and temporal based gene expression
with matching phenotype measurements could be identi-
fied by appropriate experimental controls. However, these
are often absent or impractical to implement in large-
scale phenotyping assays or in cases ofmeta-analyses from
already available data sets [7]. Generally, formore complex
experimental designs to be more case-specific, custom-
made computational solutions are usually required in
order to analyse the gene expression data according to
the different variables. Efforts are being made to gener-
alise these tools and resources so that analysis of complex
designs can be made easier. Examples include software
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 5 of 7
for the analysis of time-series data sets. The DyNB tools
suite in [27] and NextmaSigPro [29] are examples of
software tools that enable analysis of time-series data
sets.
Efforts have also been made to tackle the complexities
of tissue specific gene expression in whole organism gene
expression data sets by developing resources such as tis-
sue specific gene expression atlases [30-32]. These data
sets can be used as benchmarks to explore experiments
on whole organisms. Small organisms such as Drosophila
Melanogaster are difficult to dissect on a large scale and
sometimes tissue specific expression must be inferred
from whole body profiles, rather than directly measured.
Innocenti et al. [33] extracted tissue specific genes from
whole fly gene expression by use of FlyAtlas [30,34]. FlyAt-
las is a database that holds information on genes expressed
in 25 adult fly tissues originally obtained by tissue specific
microarray profiling in wild type flies.
Gene expression analyses are very useful in identify-
ing groups of genes that could characterise a pheno-
type. Although they do not provide much detail on the
specific mechanisms under which the original stress or
mutation leads to the observed phenotype, they can be
used as a starting point for subsequent analyses that
can narrow down candidate pathways and processes and
generate hypotheses for more detailed experiments that
can eventually shed light on the exact causes of the
phenotype.
Linking pathways to phenotypes
Deriving the underlying mechanism of the phenotype,
given the initial mutations and/or resulting gene expres-
sion, involves the integration of knowledge on pro-
tein interactions and pathways [35]. There are different
types of pathway analyses frequently used depending
on the nature of the pathways: protein-protein inter-
actions; gene-regulatory pathways; quantitative reaction
modelling that includesmetabolic, pharmacokinetic mod-
elling. Methods for analysing these types of pathways have
been previously reviewed in [35,36]. Linking these types
of analyses with gene expression depends on whether
there are already candidate pathways of interest and what
is their degree of annotation. It also depends on the
hypotheses of the studies investigated and whether they
involve a small and specific pathway where knowledge of
quantitative reactions matter and are available or whether
they involve a large integrational study where broadness
of pathway connections are important, usually at a cost of
using detailed quantitative information on the kinetics of
the interactions involved.
Pathwaymodels
Data for quantitative pathway analyses usually come from
direct protein level measurements, therefore enabling the
use of computational simulations for the formulation of
predictive hypotheses that can subsequently be tested
experimentally. Such approaches have the potential to
produce predictive mathematical models describing the
underlying mechanisms at high-levels of detail [37].
Panetta et al. [38] study the variations of methotrexate
accumulation in cells of acute lymphoblastic leukemia
patients using pharmacokinetic and pharmacodynamic
models. By employing these methods they characterise
how perturbations in the folate pathway, target of
methotrexate, vary across the tumour subtypes (pheno-
types) and how they relate to genetic variation and gene
expression.
Quantitative pathway modelling methods are not easy
to implement on a large scale and are mostly useful
when there is already substantial knowledge of the bio-
logical process involved. In cases where the underlying
biological process is unknown or poorly defined, high-
throughput protein interaction data or high-level pathway
information from pathway databases can help disentan-
gle the mechanisms that are responsible for or induced
by the observed gene expression. Boolean logic and other
logic-based approaches, such as [39,40], have been used
successfully for qualitative pathway analyses, to gener-
ate hypotheses that link gene expression, pathways and
phenotypes.
Knowledge integration
Pathway databases such as REACTOME [10] or KEGG
[41] contain a wide range of developmental, signalling,
metabolic, as well as disease pathways. These are well-
linked to other resources, such as Ensembl [11] and
Uniprot [42], for better integration with gene and protein
information. Currently they support pathway enrichment
analyses for a set of interesting genes or proteins and
provide tools for visualising the pathways in the context
of these interesting molecules. In addition, REACTOME
provides ontological links between pathways, therefore
allowing the exploration of interactions and relationships
across different pathways. Often these pathway resources
do not contain exactly the same pathways and in order
to enable more comprehensive analyses, their data sets
need to be merged. Resources such as BioSystems [43]
attempt to collect and disseminate all available pathways
from the available databases. However, due to lack of a
widely used controlled vocabulary describing the avail-
able pathways, such attempts fail to fully semantically
integrate data from different pathway databases. There
has been significant progress in developing ontologies
and standard formats for descriptions of pathway compo-
nents and reactions (SBO, SBML, [44]). However, these
have mainly been focused on describing the mathematical
interactions within pathways in order to enable simula-
tions. Therefore, they have not been widely adopted by
Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 6 of 7
all pathway databases in order to enable more effective
integration.
Further work also needs to focus on linking the differ-
ent levels of information, protein levels, gene expression
andmetabolic and signalling pathways into computational
models that can handle qualitative and quantitative path-
way parameters. Integrating different kinds of data sets
from different species to solve a single, common biologi-
cal process is an invaluable step in pathway analyses, but
remains a difficult task. Advances in text-mining meth-
ods, as well as more accurate orthologous relationships
between the genes of different species will help overcome
these problems.
A major remaining problem in the linkage of genes and
their expression signatures to pathways to phenotypes is
the limited knowledge of the mapping between pathways
and phenotypes. This is a difficult task mainly due to the
lack of appropriate data sets that would enable the infer-
ence of such connections on the large scale. However,
high-throughput phenotyping projects such as the Inter-
national Mouse Phenotyping Consortium [15] have the
potential to provide sufficient data sets for the inference
of such links.
Finally, recent efforts on multi-scale models of organs
attempt to bridge the gap between molecular pathways
and physiology through projects such as the Virtual Phys-
iological Human [45] and the Virtual Liver, a collaborative
effort to produce a physiological model of the liver that
interacts with pathways and other molecular component
in order to support simulations and the understanding of
the liver function in health and disease [46]. Such efforts
are still in their initial steps but have the potential to facil-
itate a better understanding of the relationship between
genes and phenotypes.
Conclusions
High-resolution gene expression data sets are provid-
ing more insight into the functional consequences of the
genotype as well as clues into the mechanisms that might
control the phenotype. At the same time, research utilising
pathway analysis and data integration has been increas-
ingly important in explaining the biological mechanisms
under which genotypes (and gene expression) influence
phenotypes. Some form of pathway analysis is routinely
part of gene expression studies. However, this is hindered
by the lack of detailed pathway maps and quantitative
information on the reactions. From the perspective of
phenotype characterisation, the development of different
types of ontologies and links between them is increas-
ingly improving the integration of gene, tissue, anatomical
and disease data sets within and between species. These
improvements are creating the basis for more detailed
associations between genes, pathways and phenotypes in
the future.
Additional file
Additional file 1: Table detailing all databases and ontologies that
appear in the text.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
All authors read and approved the final manuscript.
Acknowledgements
This work was supported by the Wellcome Trust grant [098051] and the
National Institutes of Health (NIH) grant [1 U54 HG006370-01].
Received: 29 October 2014 Accepted: 19 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 
DOI 10.1186/s13326-015-0020-6
RESEARCH ARTICLE Open Access
Region Evolution eXplorer  A tool for
discovering evolution trends in ontology
regions
Victor Christen1*, Michael Hartung1,2 and Anika Groß1,2
Abstract
Background: A large number of life science ontologies has been developed to support different application
scenarios such as gene annotation or functional analysis. The continuous accumulation of new insights and
knowledge affects specific portions in ontologies and thus leads to their adaptation. Therefore, it is valuable to study
which ontology parts have been extensively modified or remained unchanged. Users can monitor the evolution of an
ontology to improve its further development or apply the knowledge in their applications.
Results: Here we present REX (Region Evolution eXplorer) a web-based system for exploring the evolution of
ontology parts (regions). REX provides an analysis platform for currently about 1,000 versions of 16 well-known life
science ontologies. Interactive workflows allow an explorative analysis of changing ontology regions and can be used
to study evolution trends for long-term periods.
Conclusion: REX is a web application providing an interactive and user-friendly interface to identify (un)stable
regions in large life science ontologies. It is available at http://www.izbi.de/rex.
Keywords: Ontology evolution, Ontology visualization, Ontologies
Background
In recent years ontologies have become increasingly
important for annotating, sharing and analyzing data
in the life sciences [1,2]. For instance, functional term
enrichment analysis [3] use ontologies to propagate infor-
mation along their structure to find over-represented
terms w.r.t. a list of interesting genes. The heavy usage
of ontologies leads to a steady modification of their con-
tent [4,5]. In particular, ontologies are adapted to incor-
porate new knowledge, eliminate initial design errors or
achieve changed requirements. Tools like Protégé [6] sup-
port the development and change of ontologies. This
process is usually distributed since especially large ontolo-
gies can not be maintained by single developers, such that
collaborative work is performed [6,7]. Typically, the over-
all development of an ontology is coordinated by a project
leader or consortium, and multiple developers contribute
*Correspondence: christen@informatik.uni-leipzig.de
1Department of Computer Science, Universität Leipzig, Augustusplatz 10,
Leipzig, Germany
Full list of author information is available at the end of the article
knowledge in their field of expertise. Ontology providers
release new versions on a regular basis or whenever a sig-
nificant amount of changes were performed. Users should
thus always consider the newest ontology version in their
applications to avoid errors from previous versions and to
be up-to-date w.r.t. the modeled knowledge.
Due to the ontologys size and complexity, the prob-
lem arises that coordinators, developers and users want to
know whether specific parts (regions) of a large ontology
have changed or not. We see different use cases where a
tool support is required:
 Region Evolution Analysis: Users may question
which regions have evolved in what way in a specific
period of time. For instance, there can be regions
exhibiting a high degree of instability. These regions
may have been in the focus of development and
underlay many modifications. This might be caused
by the topics modeled within these regions, e.g.,
current topics require permanent modifications to be
up-to-date. By contrast, a stable region might be
already completed or was of low interest during
© 2015 Christen et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 2 of 12
recent ontology development. Furthermore,
interesting insights come up when studying the
evolution of a region over time, e.g., by considering
the change intensity in the past five years. Another
use case would be the comparison of the evolution in
different regions, e.g., a head-to-head comparison of
two regions can provide information whether these
regions have evolved in a similar way or show a
different evolution behavior.
 Ontology Development and Project
Coordination: In ontology development projects
coordinators usually face the problem how to track
and measure the ongoing development in an
ontology. This especially holds for large and
distributed projects when the ontology to be
developed covers a number of different topics. In
such cases project coordinators are interested in the
evolution of different ontology parts. In particular,
they like to see (1) how work has progressed and (2)
like to detect potential for future development.
Having a tool that can flexibly compute where, when
and how many changes occurred, an improved
project controlling and decision management can be
achieved. For instance, if work in an area did not
progress as planned, resources can be re-scheduled
accordingly in order to complete the work.
The controlling is not limited to project coordinators.
Also, developers can inform themselves about the
evolution in different regions and may find
interesting starting points to participate, e.g., regions
with topics they are aware of.
 Dependent Data and Algorithms: Biomedical
datasets like genes, images or electronic health
records are typically annotated with concepts of
ontologies. Thus, they depend on the ontology
content and exhibit another use case for REX. For
instance, if a user considers the anatomy part of the
NCI Thesaurus (NCIT) [8] for annotating local data
such as radiology pictures, she would like to know
how this part has evolved recently, i.e., is the part
unstable or stable. Thus, one can estimate whether or
not an adaptation of the annotations would be
feasible. Moreover, ontology-based algorithms or
applications might be affected by ontology changes.
For instance, if results of a gene set enrichment
analysis [3] are located in a strongly evolving
ontology part, it should be re-done based on the
newest ontology version to see how results change.
By contrast, results located within stable ontology
parts are likely to remain unchanged. In own previous
work [9] we already used such techniques to figure
out how the results of real gene set enrichment
analyses changed over time and how these changes
are related to ontology modifications.
A number of existing web applications provide query
functionalities for specific ontologies like the popular
Gene Ontology (GO) (e.g., [10,11]). Furthermore, life sci-
ence ontologies can be accessed through platforms like
BioPortal [12] or OBO Foundry [13]. Although it is pos-
sible to retrieve different versions of an ontology, such
platforms rarely provide information about evolution, i.e.,
users have the problem to figure out how an ontology has
evolved compared to their version in use. Recently, some
web tools offer access to information about the evolution
of the GeneOntology (GO). GOChase [14] allows to study
the history of individual GO concepts and Park et al. [15]
propose graph-based visualization methods to view mod-
ified GO terms. In own previous work we designed the
OnEX web application [16] for versioning as well as quan-
titative and concept-based evolution analysis of life sci-
ence ontologies. Our tool CODEX [17] can be used to
determine a diff between two ontology versions covering
complex changes (e.g., concept merge or split). For a gen-
eral overview on ontology and schema evolution including
diff computation we refer to [4]. In summary, currently
available tools lack the functionality to analyze and com-
pare evolution in different ontology parts especially for
large ontologies with several version releases.
We therefore present the novel web application REX
(Region Evolution eXplorer). REX can be used (1) to
determine differently changing regions for periodically
updated ontologies, and (2) to interactively explore the
change intensity of those regions. REX provides a com-
parative trend analysis such that users and developers
can monitor the long-term evolution for their regions of
interest, e.g., to track the work or coordinate future devel-
opment. To show the applicability of REX, we evaluate the
tool by analyzing evolution trends in four representative
life science ontologies. REX is online available at http://
www.izbi.de/rex and provides a web service interface for
programmatic access at http://dbs.uni-leipzig.de/wsrex.
This paper is an extended version of [18] presented at
DILS 2014. For this version REX has been improved and
provides additional features such as the specification of
individual cost models and a web service interface for
programmatic access. We further describe possible use
cases for REX and outline opportunities for future work
in more detail. New region evolution analyses have been
performed on four representative life science ontologies.
The base region discovery algorithm used by REX has
been published in [19]. This algorithm allows to detect
(un)stable ontology regions for an arbitrary number of
ontology versions. However, in this form the algorithm is
only applicable offline, i.e., the research community can
not make use of it. With the help of REX the algorithm
is applicable in two ways: (1) by interactively analyz-
ing region evolution via the web application and (2) by
remotely accessing the web service interface. REX fits
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 3 of 12
into our tool suite for ontology evolution management
as follows. REX is build upon the OnEX repository [16]
offering versioning capabilities for life science ontologies,
i.e., ontologies and their versions available in OnEX can
be analyzed with REX as well. If someone is interested
in detailed changes between two particular ontology ver-
sions we refer to the CODEX tool [17] which provides
ontology version comparison (diff ) facilities.
Methods
The region discovery method proposed in [19] enables the
detection of changing and stable ontology regions. The
basic idea is to compute change intensities for regions
based on changes between several succeeding versions of
an ontology within a specific time interval. First, we briefly
describe the applied cost model and region measures. We
then describe the region discovery method as well as an
algorithm to identify trends in the evolution of ontolo-
gies. We present the infrastructure of REX and describe
its different workflows and features.
Region discovery methods
Change costs
An ontology consists of a set of concepts which are inter-
related by different relationships like is-a and part-of.
Each ontology concept has an unambiguous identifier and
is further defined by a set of attributes like its name,
synonyms or definition. Discovering changing or stable
ontology parts requires the definition of a cost model to
measure the influence of changes on ontology concepts.
In general, ontology content can be added (addition),
removed (deletion) or modified (update). Here we distin-
guish between seven basic change operations for ontol-
ogy concepts, their attributes and relationships between
concepts listed in Table 1. These basic change opera-
tions cover all modifications that typically occur in an
ontology and are suitable to detect changing ontology
regions. More complex change operations (e.g., concept
moves) are composed of these basic operations and can be
derived by aggregating basic changes to a more compact
representation [20]. For instance, a move of a concept
within the ontology hierarchy is composed of an addition
(addR) and a remove (delR) of a relationship. Further-
more, typical changes like name and property changes are
covered by the change operation chgAttValue. Relation-
ship changes with is-a or other semantics (e.g., part-of ) are
represented by addR/delR. Our cost model now assigns
change costs to each basic change operation, i.e., we can
represent the impact of change operations by different
costs (see change costs used in REX in Table 1). For
instance, we can assign higher costs to deletions since
they might have a higher impact on dependent applica-
tions than additions. Note, that users can adapt the cost
model according to their application scenario. If a user
is especially interested in regions that have been heavily
extended, she should rank additions higher than dele-
tions. To reflect the impact of changes on concepts, we
introduce two types of concept costs: (1) local costs lc(c)
cover the impact of change operations that directly influ-
ence a concept c, e.g., the change of an attribute value
or the addition/deletion of a child concept have a direct
impact, and (2) aggregated costs ac(c) are used to reflect
all changes occurring in the is-a descendants of a concept
c, e.g., leaf additions/deletions indirectly influence ances-
tor concepts. We will later describe how we assign local
and aggregated costs to concepts.
Regions andmeasures
An ontology region OR consists of an ontology concept
(region root rc) and its is-a subgraph, i.e., it covers all leaf
and inner concept changes within this region. The defi-
nition of our regions covers the experience that changes
often occur in the boundary of an ontology, e.g., addition
of leaves or subgraphs to extend the knowledge of a spe-
cific topic. Of course our regions also cover changes on
inner concepts since all intermediate concepts between
the root and the leaves are part of the region. As an exam-
ple Figure 1 (left) illustrates part of an anatomy ontology.
We can consider the regions lung and tonsil each con-
sisting of three concepts. Note that the complete ontology
can also be regarded as a region defined by the ontology
root organ.
Table 1 Change operations and change cost model
Change operation Description Change costs
Attributes
addC Addition of a new concept 1
delC Deletion of a concept 2
Relationships
addR Addition of a new relationship 0.5/0.5
delR Deletion of a relationship 1.0/1.0
Concepts
addA Addition of a new attribute 0.5
delA Deletion of an attribute 0.5
chgAttValue Modification/change of an attribute value 0.5
The table shows which change operations and corresponding change costs we utilize in REX. For relationships we split the costs and assign them to the source and
target concept, respectively.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 4 of 12
Figure 1 Example part of an anatomy ontology. The figure shows a small yet comprehensive example anatomy ontology to illustrate regions as
well as local (lc(c)) and aggregated (ac(c)) costs (left). For instance, the region lung consists of three concepts and has aggregated costs of four.
The table on the right shows the corresponding results when applying the region measures (abs_size, abs_costs, avg_costs) in this example.
So far, REX provides a set of measures to describe the
change intensity of ontology regions. For each OR one
can determine its absolute size (abs_size(OR)) w.r.t. the
number of concepts. Absolute change costs of an OR
(abs_costs(OR)) are represented by the aggregated costs of
its root ac(rc). The average change costs per concept in
OR can be computed as the fraction of absolute change
costs and the region size: avg_costs(OR) = abs_costs(OR)abs_size(OR) .
Applying these measures to our example results in the val-
ues displayed in Figure 1 (right). The lung region changed
more intensively (avg_costs(?lung?) ? 1.33) compared to
tonsil (avg_costs(?tonsil?) ? 0.67). The overall change
intensity of the ontology is 67 ? 0.86.
Our general aim is to determine (un)stable ontology
regions w.r.t. a specific time interval (tstart , tend), i.e.,
changes between ontology versions released in this inter-
val need to be considered. For this purpose we show first
how we can determine local (lc) and aggregated costs (ac)
for two versionsOold andOnew. Later we will describe how
we can generalize the two-version approach for an arbi-
trary number of versions. For further details about both
algorithms we refer to [19]. We will highlight the main
steps since the REX application is the main contribution
of this article.
Region discovery for two versions
The general procedure for two versions is depicted in the
following algorithm (computeAggregatedCosts):
Algorithm 1: computeAggregatedCosts
Input: ontology versions Oold and Onew, change costs ?
Output: ontology version Onew with assigned aggregated
costs
1 Oold ? Onew ? diff(Oold ,Onew);
2 assignLocalCosts(Oold ? Onew, ? ,Oold ,Onew);
3 Oold ? aggregateCosts(Oold);
4 Onew ? aggregateCosts(Onew);
5 transferCosts(Oold ,Onew);
6 return Onew;
The algorithm accepts two versions Oold, Onew and a
cost model ? . Its four main steps are: (1) diff computation,
(2) local cost assignment, (3) cost propagation and (4)
cost transfer. We first need to determine the difference
between both input versions (line 1). For this purpose we
can use existing Diff algorithms such as PromptDiff [21]
or COntoDiff [20]. The result is the diffOold?Onew con-
sisting of a set of change operations that occurred between
Oold and Onew.
Using the diff and the change costs ? we next assign
local costs to concepts which are involved in changes (line
2). Depending on the type of change we assign local costs
to concepts in the old or new version. Additions are reg-
istered in the new version while deletions are covered in
the old version. The assignment further depends on the
kind of ontology element that has been changed. Costs
from changes on a concept or its attributes are assigned
to the concept itself while costs for relationships are split
and assigned to the source and target concept of the
relationship, respectively.
We now use the two ontology versions annotated with
local costs to derive the aggregated costs per concept (line
3-4). In particular, we propagate local costs along is_a
paths upwards to the root(s). Due to multi-inheritance we
may need to split costs during propagation. The aggre-
gated costs ac(c) of a concept c can be determined as
follows:
ac(c) =
?
c??children(c)
ac(c?)
|parents(c?)| + lc(c)
The aggregated costs ac(c?) of each child c? are divided
by the number of parents the child has (|parents(c?)|).
These costs are summed up for each child of the consid-
ered concept c and added to its local costs lc(c) to finally
get its aggregated costs ac(c). We thus distribute costs in
the case of multiple inheritance and finally ensure that
the root concept(s) of the ontology contain the overall
sum of all assigned local costs. In our example in Figure 1
(left) the aggregated costs of organ (ac(?organ?) = 6) are
computed based on the aggregated costs of its children
ac(?lung?) = 4 and ac(?tonsil?) = 2 as well as its own local
costs lc(?organ?) = 0.
In order to determine (un)stable regions in the new ver-
sion, we need to transfer costs from Oold into Onew (line
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 5 of 12
5). We therefore sum up aggregated costs which belong to
the same concept in the old/new version. After this step
we can apply our region measures as defined earlier or use
the new ontology version with aggregated costs for further
processing (see Multiple Version algorithm).
Region discovery formultiple versions
We generalize our basic algorithm for multiple released
versions O1, . . . , On by executing it n ? 1 times so that
we successively determine aggregated costs (for each ver-
sion change Oi?1 ? Oi) and transfer them to the newest
version On. In On we can apply the previously described
region measures. The overall algorithm findRegions
looks as follows:
Algorithm 2: findRegions
Input: ontology versions O1, . . . ,On, change costs ?
Output: newest version On with determined change
intensities (e.g., abs_costs, avg_costs)
1 forall the succeeding versions Oi ? Oi+1 do
2 Oi+1 ? computeAggregatedCosts(Oi,Oi+1, ?);
3 computeRegionMeasures(On);
4 return On;
Trend discovery for regions
Using the region discovery method (findRegions) one
can determine the most (un)stable regions for a spe-
cific time interval. To better monitor region changes over
long periods of time and to figure out trends in their
evolution, we propose a further method for trend dis-
covery based on sliding windows. The overall procedure
trendDiscovery looks as follows: Using the region
discovery method (findRegions) one can determine
the most (un)stable regions for a specific time interval. To
better monitor region changes over long periods of time
and to figure out trends in their evolution, we propose a
further method for trend discovery based on sliding win-
dows. The overall procedure trendDiscovery looks as
follows:
Algorithm 3: trendDiscovery
Input: time interval (tstart , tend), ontology O, ontology region
of interest OR ? O, change costs ? , window size ?, step
width 
Output: time-based stability valuesmeasuredCosts
1 t ? tstart ; measuredCosts ? ?;
2 while t + ? < tend do
3 versions ? getReleasedVersions(O, (t ? ?, t));
4 latestVersion ? discoverRegions(versions, ?);
5 regionCosts ? getStabilityValuesForRegion
6 (OR, latestVersion);
7 measuredCosts.put((t, regionCosts));
8 t ? t + ;
9 returnmeasuredCosts;
The algorithm works on an ontology O, a time interval
(tstart , tend) and an ontology region of interest OR to be
monitored. We further use a sliding window of size ?, a
step width  and change costs ? . In particular, we suc-
cessively shift the window beginning at tstart ? ? over the
time interval until we reach its end tend . In each step we
first determine the released ontology versions within the
window (line 3). We then calculate and save the costs (e.g.,
avg_costs) for OR by calling the region discovery algo-
rithm (discoverRegions) for the versions within ?.
We thus generate a time-based map (line 6) containing
information about the change intensity of OR at specific
points in time in the defined window. The results are
visualized for users in the Trend Analysis component of
REX.
Web application
Architectural overview
REX is based on a three-layered architecture displayed
in Figure 2. The back-end consists of the OnEX repos-
itory [16] which currently provides access to more than
1,000 versions of 16 popular life science ontologies. Note
that it supports the import of ontologies in different for-
mats such asOWL andOBO. Users can analyze integrated
versions with the offered facilities of REX. The server
layer is implemented in Java and realizes different ser-
vices to access ontology versions in OnEX. Moreover, it
provides services to calculate the region measures and to
perform trend and quantitative analyses. Every service is
encapsulated in its own module, such that it is possible to
change the region discovery algorithm independently of
the other modules. Results are transformed such that the
application can visualize ontologies and changing regions
in graphs. Moreover, we provide a web service for pro-
grammatic access. So far, it computes the average costs
per concept for a particular ontology and time interval.
Ontology developers are thus able to integrate REX func-
tionalities into their own applications. For instance, a set
of annotations could be automatically rejected, if the aver-
age costs of involved concepts exceed a given threshold.
The front-end is a platform-independent web application
based on the Google Web Toolkit (GWT)[22] and the
graph library InfoVis[23]. In the following we discuss the
analysis facilities of REX, namely the Structural Analysis,
Quantitative Change Analysis and Trend Analysis, as well
as the web service interface, in more detail.
Structural analysis
The structural analysis component represents the evolu-
tion of regions in an ontology for a specified time interval
as a graph (Figure 3). The component is mainly divided
into a Browser View as well as a table to search and fil-
ter results (Table View). First the user needs to specify the
ontology name and the time period to review in the Input
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 6 of 12
Figure 2 Three-layered architecture of REX. The figure shows the architecture of REX consisting of three layers: (1) knowledge base layer, (2) server
layer, (3) presentation layer.
form. Moreover, users can adapt the applied change cost
model according to their analysis szenario (Change Cost
Model). The system then performs the region discovery
algorithms and generates a graph to visualize the results
(Browser View). Each node in the graph represents an
ontology concept, is-a relationships are displayed as edges
between the nodes. The layout is circular and displays a
concept and its near neighborhood, i.e., its descendants
and parent nodes (either with or without labels). Users can
easily identify interesting sub regions by selecting a con-
cept in the graph (Browser View) or in the Table View.
This concept is then shown as the central node in the
Browser View. It is possible to navigate in both directions
through the ontology. For instance, if one is interested in
a specific sub region and its content, one clicks on the
node and the graph will display the sub region in more
detail. In contrast, one can also navigate to a more gen-
eral concept (surrounded by blue circles) to see sibling
regions of the current one. The colors signal the measured
change intensity (avg_costs) of a region. Red stays for high
change intensity whereby green is used to mark stable
regions. Thus, users can easily figure out where (un)stable
regions are located. We provide two coloring schemes: (1)
interval-based grouping or (2) equal distribution between
Figure 3 Structural Analysis component. The figure shows the structural analysis component of REX.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 7 of 12
min/max avg_costs. For each concept in the graph, small
info boxes (mouse over) provide further information like
the accession number, concept name/label or the mea-
sured avg_costs.
In general the number of concepts and relationships
in an ontology is very high. Thus, it is difficult to rec-
ognize interesting regions only by browsing through the
graph especially for large ontologies. Moreover, users may
be interested in the change intensity of specific regions.
The Table View therefore allows users to filter and sort
ontology regions by their accession number, name and
avg_costs. In particular, search criteria can be specified
in the head of the table to find regions of interest. For
instance, one can filter out all regions in the Adult Mouse
Anatomy Ontology containing the name heart. Users can
simply select their region of interest in the table and move
to the Browser View for its visualization. To get a more
detailed view of occurred changes, users can request the
local Change History of a selected concept at the bottom
of the table.
Quantitative change analysis
To get information about how many changes occurred
in an ontology for a specific time interval REX offers
the quantitative change analysis component (Figure 4
left). Users can generate diagrams to see the differences
between released ontology versions in statistical (quan-
titative) form, i.e., we count and visualize how many
changes (addC, delC, addR, delR) occurred. In particular,
users can display the number of changes in one ontology
for a specific time interval, e.g., GOBiological Processes in
2013.Moreover, one can compare the evolution of two dif-
ferent ontologies for a specified time interval or compare
two different time intervals for the same ontology. Users
can thus identify interesting ontologies and time periods
for later region analyses.
Trend analysis
The trend analysis component can be used to study
and compare the long-term evolution of selected regions
(Figure 4 right). Users first need to specify the ontology,
the time interval (first and last version) and the window
size and step width (number of versions). Next they are
able to select regions of their interest either by search-
ing the respective accession number/concept name or by
choosing from top-level concepts of the ontology. REX
executes the proposed trendDiscovery algorithm to
measure the avg_costs for the selected regions at differ-
ent points in time. The results are converted into a line
chart which displays the trend of the measured avg_costs
for each region over time. Users are thus able to compare
the change intensity for different regions of interest within
one diagram.
Web service
In addition to the web application, we provide a JAX web
service for programmatic access to REX. The web service
interface is available at http://dbs.uni-leipzig.de/wsrex?
wsdl. Programmers can apply the region discovery meth-
ods for a specified ontology and a defined time interval.
Using the provided WSDL description they can gener-
ate the corresponding client classes to enable web service
interaction. We provide three methods building on each
other:
 getAvailableOntologies returns all existing ontologies
in our OnEX repository.
 getVersions returns a list of available versions for a
specified ontology.
 calculateRegions calculates the average costs for each
concept in the specified ontology and time interval. It
returns a list of concepts including accession
numbers, concept names and the computed average
costs for each concept.
Results and discussion
In the following we will describe and discuss some
selected results generated with REX. In particular, we
will present results for the following well-known life
science ontologies: Gene Ontology (GO) with its sub
Figure 4 Quantitative Change and Trend Analysis components. The figure shows the quantitative change and trend analysis components of REX.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 8 of 12
ontologiesMolecular Functions (GO-MF), Biological Pro-
cesses (GO-BP) and Cellular Components (GO-CC), the
Thesaurus of the National Cancer Institute (NCIT), Adult
Mouse Anatomy ontology (MA) and Chemical Entities of
Biomedical Interest (ChEBI). We will focus on results for
the recent past (mainly 2012-2013). Note that users can
flexibly use REX to explore evolution trends for regions in
other available ontologies for arbitrary time intervals. We
first discuss the evolution in general (quantitative statis-
tics) and show the change intensities for whole ontologies.
We then describe the usage of the structural analysis and
trend analysis components of REX by different examples.
Evolution in general
Usually, the evolution of an ontology can be described by
the number of basic changes (e.g, addC, delC, addR,
delR) occurred. For a start, the quantity of change opera-
tions provides an indication of how an ontology evolved,
e.g., an ontology exhibiting a small number of changes
over the time can be classified as stable. However, the
location, i.e., information about the region where changes
occurred is missing. Table 2 shows the quantity of addi-
tions and deletions of concepts and relationships for the
considered ontologies in 2012 and 2013 generated with
the quantitative change analysis component of REX. Over-
all, every ontology has been modified in the considered
time intervals. An exception forms MA, where no (only
one) version was released in 2012 (2013). In general the
ontologies grow, i.e., the quantity of insertions (add) is
higher than the quantity of deletions (del). Most changes
occurred in NCIT and ChEBI, e.g., more than 12,000 con-
cepts have been added in both ontologies. However, there
has also been an increased number of deletions, i.e., the
ontologies were optimized by rearranging concepts in the
hierarchy or by merging multiple redundant concepts into
a single one.
We apply our region algorithm to measure the change
intensity of whole ontologies. In particular, we use the root
concept(s) of an ontology as regions, i.e., we aggregate
all costs in the root(s) and can thus estimate the over-
all ontology change intensity for a specific time interval.
Additional file 1: Table S1 displays the change intensi-
ties (abs_size, abs_costs, avg_costs) for all ontologies under
investigation in 2012 and 2013. The ontologies show dif-
ferent behaviors in their change intensities. In both peri-
ods ChEBI exhibits the highest absolute costs. Its change
intensity even increased from 2012 compared to 2013
(avg_costs: 0.88 ?0.95). Similarly, other ontologies like
GO-CC or NCIT have been modified more extensively in
2013. In contrast, the GO sub ontologies GO-BP and GO-
MF show decreased change intensities in 2013 compared
to 2012, i.e., modification actions on these ontologies have
been reduced. Regarding GO, GO-BP is the sub ontol-
ogy with the most frequent changes in both years. MA
is relatively stable since only slight changes occurred in
2013.
Structural analysis
After focusing on the overall ontology change inten-
sity, we will now show how one can use the structural
analysis component to explore details about the evolu-
tion in different regions of an ontology. We describe the
usage of the structural analysis component for GO-MF in
2013. GO-MF has two parts namely molecular_function
(GO:0003674) which contains all active molecular func-
tions and obsolete_molecular_function (GO:0008369)
used to collect all obsolete (inactive) concepts. All main
regions are direct children of GO:0003674. The browser
view shows, that the majority of these regions are unsta-
ble (see red nodes next to the central node in Figure 5
left). For instance, transporter activity (GO:0005215)
has avg_costs of 0.4 which are greater than those of
molecular_function (0.12). Furthermore, many children
(sub regions) of transporter activity show high avg_costs
(Figure 5 middle). This indicates that the whole region of
transporter activity has significantly changed compared
to other regions in 2013 that show low avg_costs since less
or even zero changes occurred. For instance, the channel
Table 2 Quantitative analysis results
2012 2013
addC delC addR delR addC delC addR delR
GO-BP 2,914 51 11,940 2,844 1,159 91 5,742 2,812
GO-MF 461 62 1,159 379 126 6 431 179
GO-CC 185 3 581 124 219 4 597 341
ChEBI 7,961 60 15,803 1,713 4,323 70 17,010 2,830
NCIT 4,878 109 6,064 1,115 8,327 174 9,183 958
MA - - - - - - - -
The table shows the quantity of changes occurred in the ontologies under investigation. We distinguish between addC, delC, addR and delR changes for two periods
namely 2012 and 2013. We considered available versions (at least two) within a period. MA has released no (only one) version in 2012 (2013). Thus, no statistics are
provided for MA.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 9 of 12
Figure 5 Structural analysis for GO-MF in 2013. The figure shows the sub graphs of the root concept GO:0003674 molecular_function (left),
GO:0005215 transporter activity (middle) and GO:0016247 channel regulator activity (right). Measured change intensities (avg_costs) are displayed
using a red-green scale (green: stable, i.e., less avg_costs; red: unstable, i.e., increased avg_costs).
regulator activity(GO:0016247) region has avg_costs of
zero, i.e., no concept in this region has been modified in
2013 (Figure 5 right).
Instead of browsing, one can use the table view to locate
interesting regions by specifying different filter criteria.
For instance, to select all regions in GO-MF related to
the term protein, one can specify a filter condition on
the name column (Figure 6). REX selects and displays all
regions that satisfy this criteria, e.g., for GO-MF in 2013
we find 557 regions related to protein. Users can further
specify conditions on avg_costs to find strongly chang-
ing or stable regions. In our case we may look for regions
related to protein having avg_costs > 1, i.e., we search
for unstable regions related to protein (Figure 6). We can
thus reduce the selection from 557 to 14 regions satisfy-
ing both criteria. Based on this selection (and a possible
sorting) users can now select a region of interest to cre-
ate a corresponding graph in the browser view for a more
detailed inspection.
We further allow to modify the applied cost model.
Dependent on the application scenario users might
be mainly interested in one/some of the used change
operations (e.g., addC, addR, . . . ) , i.e., they should rank
the respective costs higher. One user might like to know
which ontology parts were of high research interest and
have been strongly extended in the near past (many
additions). Another user might be looking for regions
where many deletions took place since she needs to
know whether her application is affected by many infor-
mation reducing changes (many deletions). To visual-
ize the impact of different cost models, we exemplary
assign high costs to deletions (delC, delR, delA) and addi-
tions (addC, addR, addA), respectively. Figure 7 shows
results for the concept heart development in GO-BP
between September 2012 and 2014. Red nodes on left
(right) denote regions where predominantly deletions
Figure 6 Specification of a filter on the name column and avg_costs
in the table view. The figure shows the specification of a filter on the
name column for GO-MF in 2013. In particular, we search for all
regions related to protein having avg_costs > 1. For GO-MF in 2013
14 regions satisfy this criteria.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 10 of 12
Figure 7 Application of different cost models. The figure shows results for the application of different cost model specifications for the concept
heart development in GO-BP between 09-2012 and 09-2014. To visualize the impact of different cost models, we assign high costs to deletions
(left) and additions (right), respectively. Red nodes on the left (right) denote regions where predominantly deletions (additions) took place.
(additions) took place. The results show that a variation
of the cost model impacts the computation of stable or
unstable regions. Many subregions of heart development
have been mainly extended (red nodes on the right side)
whereas only two subregions where affected by a high
number of deletions (red nodes on the left side).
Trend analysis
As an example, we will show results for a two-year trend
analysis in NCIT between 2012 and 2013. In particu-
lar, we select the three regions Chemotherapy_Regimen
(C12218), Molecular_Abnormality (C3910) and Activity
(C43431) and measure their change intensity (avg_costs).
We choose a sliding window of six versions (window size
?) and shift the window by one version in each step
(step width ). Figure 8 displays the generated result
chart. The three regions show a different behavior in their
change intensity. The work on Molecular_Abnormality
was mainly performed in the beginning of 2012 (avg_costs
up to 0.9) before its change intensity decreased to nearly
zero, i.e., one might consider this region as one that
became stable over time. The Chemotherapy_Regimen
(C12218) region was stable in the complete period
(avg_costs <0.05), i.e., the development in this region
was probably performed before 2013 and it seems that
the region will be stable in the near future as well. On
the other hand, such a long-term stable region might
have just been of low interest in the past and needs
future development. In contrast, the region on Activity
(C43431) has been continuously adapted during the whole
analysis period. It seems to be of high research interest
and is still under development such that it is likely to
be further changed in the next months or years. Users
that are especially interested in content of this region for
Figure 8 Trend analysis for selected regions of NCIT between 2012-2013. We perform a trend analysis for three regions of NCIT between 2012-2013:
Chemotherapy_Regimen (C12218), Molecular_Abnormality (C3910) and Activity (C43431). The figure shows how their change intensity
(avg_costs) evolved over time when using a sliding window of length six months and a step width of one month.
Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 11 of 12
their analyses/workflows need to take care of the ongoing
evolution. In contrast those working within the Molec-
ular_Abnormality and Chemotherapy_Regimen regions
can assume that their regions of interest will be relatively
stable in the near future. The trend analysis of REX is
valuable to support ontology development since the evo-
lution of ontologies can be monitored over longer periods
in time. Of course, the interpretation of trend results is
up to the user and depends on their specific application
scenario.
Conclusions and future work
REX provides interactive access to information about the
evolution of life science ontologies. Users can explore
(un)stable ontology regions by different workflows. The
knowledge about changing ontology regions can be used
to support ontology-based algorithms and analysis. Fur-
thermore, the development of large life science ontologies
can be monitored with REX, i.e., developers and project
coordinators can inform themselves about ongoing work
in different ontology parts.
For future work, we plan to extend REX such that
users are able to perform region analysis on their indi-
vidual ontologies. We will further extend the change cost
computation of REX by involving alternative metrics for
changing concepts. For instance, we can involve seman-
tic similarities or distances between ontology concepts
(see [24] for an overview) to include the near context
of a changed concept, i.e. changes on ancestor as well
as descendant concepts. Effects of dense local changes
might have more impact, and could by ranked higher
during change intensity computation. Moreover, we like
to perform a more detailed evaluation with ontology
developers to analyze how REX can be used in ontology
development and application scenarios. In [9] we already
used the Region Discovery Algorithm to analyze Gene
Ontology changes in the context of the widely used term
enrichment analyses. It would be further interesting to see
if specific evolution trends are in accordance with editorial
policies or specific activities in sub-domains. It might be
helpful to provide a suitable presentation of REX results,
e.g., by integrating its functionalities into tools used by
the ontology developers or annotation curators. Currently,
the GOA consortium uses the tool Protein2GO for anno-
tation and emphasizes curation and quality control of
GO annotations [25]. So far, it does not involve informa-
tion on ontology evolution. Curators could be supported
by presenting REX change intensities for newly created
and existing annotations to indicate whether further qual-
ity control might be necessary, e.g., due to significant
changes in the considered ontology part. To better sup-
port the ontology development process with information
about the evolution in different ontology regions, we like
to provide REX plugins for common tools like Protégé [26]
or OBO-Edit [27]. The plugins should be able to flex-
ibly present ontologies and their changing regions. For
instance, developers might prefer a reduced presentation
of the hierarchies, e.g., by focusing on highly changing
regions that cover frequently used concepts or by divid-
ing concepts of an ontology into smaller, moremanageable
units [28].
Additional file
Additional file 1: Table S1. Change intensity of complete ontologies in
2012 and 2013. The table shows the change intensity for each ontology
under investigation in 2012 and 2013. The three columns per year display
the ontology size (abs_size) and the measured absolute costs (abs_costs) as
well as average costs (avg_costs). The red-green scale for avg_costs
highlights ontologies with high (red) and low (green) change costs. We
performed the region discovery algorithm for released versions in 2012
and 2013, and considered the root concept(s) as region(s). For ontologies
with multiple root concepts we summed up the absolute costs per root
concept and calculated the average costs w.r.t. the overall ontology size.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
VC has implemented the web application. MH and AG designed the region
analysis algorithms, integrated the ontology versions and participated in the
GUI component design. All authors participated in the evaluation and
contributed to write, read and approve the final manuscript.
Acknowledgements
We acknowledge support from the German Research Foundation (DFG) and
Universität Leipzig within the program of Open Access Publishing. A short
version of this publication has been published as application paper at the
conference on Data Integration in the Life Sciences (DILS) 2014.
Author details
1Department of Computer Science, Universität Leipzig, Augustusplatz 10,
Leipzig, Germany. 2Interdisciplinary Center for Bioinformatics, Universität
Leipzig, Härtelstr. 16 - 18, Leipzig, Germany.
Received: 30 September 2014 Accepted: 17 April 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 
DOI 10.1186/s13326-015-0018-0RESEARCH ARTICLE Open AccessEvaluating drug-drug interaction information in
NDF-RT and DrugBank
Lee B Peters1, Nathan Bahr2 and Olivier Bodenreider1*Abstract
Background: There is limited consensus among drug information sources on what constitutes drug-drug interactions
(DDIs). We investigate DDI information in two publicly available sources, NDF-RT and DrugBank.
Methods: We acquire drug-drug interactions from NDF-RT and DrugBank, and normalize the drugs to RxNorm. We
compare interactions between NDF-RT and DrugBank and evaluate both sources against a reference list of 360 critical
interactions. We compare the interactions detected with NDF-RT and DrugBank on a large prescription dataset. Finally,
we contrast NDF-RT and DrugBank against a commercial source.
Results: DrugBank drug-drug interaction information has limited overlap with NDF-RT (24-30%). The coverage of
the reference set by both sources is about 60%. Applied to a prescription dataset of 35.5M pairs of co-prescribed
systemic clinical drugs, NDF-RT would have identified 808,285 interactions, while DrugBank would have identified
1,170,693. Of these, 382,833 are common. The commercial source Multum provides a more systematic coverage
(91%) of the reference list.
Conclusions: This investigation confirms the limited overlap of DDI information between NDF-RT and DrugBank.
Additional research is required to determine which source is better, if any. Usage of any of these sources in
clinical decision systems should disclose these limitations.
Keywords: Drug-drug interactions, NDF-RT, DrugBankBackground
Motivation
An important component of electronic health record
systems is the use of clinical decision support (CDS) to
improve medication safety [1,2]. Preventable adverse
drug reactions include those resulting from drug-drug
interactions (DDIs) [3,4]. Among other things, CDS sys-
tems leverage drug-drug interaction information to re-
duce the possibility of adverse drug events [5,6]. While
there are many sources of DDI information, and many
commercially available systems which contain this infor-
mation, there is limited consensus on what constitutes
critical and non-critical DDI [7].
One source of publicly available DDI information is
contained in the National Drug File Reference Termin-
ology (NDF-RT) and available through the NDF-RT appli-
cation programming interface (API). Several applications* Correspondence: obodenreider@mail.nih.gov
1Lister Hill National Center for Biomedical Communications, National Library
of Medicine, National Institutes of Health, Bethesda, Maryland, USA
Full list of author information is available at the end of the article
© 2016 Peters et al.; licensee BioMed Central.
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.developed for medication management take advantage of
the API for checking interactions in medication lists (e.g.,
the iOS app Dosage for medication information and re-
minders). However, it was announced before the writing
of this article that the DDI information would be soon re-
moved from NDF-RT [8]. Our search for another publicly
available source of DDI information led us to evaluate the
DrugBank data source as a possible replacement for the
NDF-RT DDIs.
In this paper, our objective is to evaluate the DDI in-
formation in NDF-RT and DrugBank. More specifically,
we contrast NDF-RT and DrugBank interactions against
each other, and contrast both against a previously pub-
lished reference set of critical DDIs [9]. We also contrast
NDF-RT and DrugBank in their ability to detect DDIs in
a large prescription dataset. Finally, we compare the
coverage of the reference set by DrugBank and NDF-RT
to that of a commercial source.This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 2 of 10Drug information sources
The following sections detail the characteristics of the
drug information sources used in this research. We use
RxNorm to harmonize drugs between the two sources of
DDIs under investigation, NDF-RT and DrugBank.
NDF-RT
The National Drug File Reference Terminology (NDF-RT)
is a resource developed by the Department of Veterans
Affairs (VA) Veterans Health Administration, as an exten-
sion of the VA National Drug File [10]. It is updated
monthly. NDF-RT covers 7319 active moieties (level = in-
gredient). In addition to providing information about indi-
vidual drugs (e.g., mechanism of action, physiologic effect,
therapeutic intent), NDF-RT also provides a set of 10,831
drug-drug interactions (DDIs). DDIs are asserted at the
ingredient level and accompanied by a mention of severity
(significant or critical). For example, NDF-RT asserts a
critical interaction between omeprazole and clopidogrel
and asserts a significant interaction between diltiazem and
lovastatin. The version used in this study is dated July 7,
2014 and was accessed through the NDF-RT API [11].
(Provision of DDI information in NDF-RT was discontin-
ued in November 2014).
DrugBank
Developed with funding from Genome Canada, DrugBank
is a knowledge base containing extensive biochemical
and pharmacological information about drugs, their
mechanisms and their targets [12]. DrugBank covers
7683 active moieties. Although not primarily developed
for clinical use, DrugBank provides a set of 12,128 drug-
drug interactions (DDIs), asserted at the ingredient
level, along with a brief textual description of the inter-
action, and information about the possible molecular
basis of the interaction (target-based, enzyme-based,
transporter-based). For example, DrugBank asserts an
interaction between omeprazole and clopidogrel bisul-
fate, described as Omeprazole may decrease serum
concentrations of the active metabolite(s) of clopido-
grel. Clopidogrel prescribing information recommends
avoiding concurrent use with omeprazole, due to the
possibility that combined use may result in decreased
clopidogrel effectiveness. The possible molecular basis
of the interaction is reported to be enzyme-based or
transporter-based. The version used in this study (4.0) was
downloaded from the DrugBank website on July 1, 2014.
RxNorm
RxNorm is a standardized nomenclature for medications
produced and maintained by the U.S. National Library
of Medicine (NLM) [13]. While NDF-RT is one of the
drug information sources integrated in RxNorm, Drug-
Bank is not. However, most ingredients from DrugBankare covered by RxNorm and RxNorm can be used to
map drugs between DrugBank and NDF-RT. Moreover,
RxNorm provides a rich network of relations among
various types of drug entities. For example, RxNorm ex-
plicitly asserts that clopidogrel bisulfate is the precise
ingredient of the ingredient clopidogrel, making it pos-
sible to normalize the various salts and esters of a drug
to their base form. RxNorm also integrates the Anatom-
ical Therapeutic Chemical (ATC) classification, which
makes it possible to extract the ATC class for most
drugs. For example ATC classifies clopidogrel as a Plate-
let aggregation inhibitors excl. heparin. The July 2014
version of RxNorm is used in this study and was
accessed through the RxNorm API [14].
Related work
Extracting drug-drug interaction (DDI) information from
textual resources, such as the biomedical literature or
structured product labels is an active field of research
[15-18]. Other researchers predict DDIs from a variety of
resources [19].
As mentioned earlier, researchers have shown the
benefit of integrating DDI information in CPOEs (e.g.,
[1,2]). However, not all studies demonstrate improve-
ment on medication safety, especially due to the large
number of alerts produced by some systems [20], which
raises questions about the quality of the underlying DDI
information.
Given the limited consensus across sources of DDI in-
formation [7], researchers have proposed criteria for
assessing high-priority DDIs [21] and for calibrating
CDS systems [22]. An expert panel was convened to
identify high-severity, clinically significant DDIs for the
Office of the National Coordinator for Health Informa-
tion Technology (ONC) as part of the Meaningful Use
incentive program [9]. Candidate DDIs were assessed by
the panel based on a number of factors, including sever-
ity levels across medication knowledge bases, conse-
quences of the interaction, predisposing factors, and
availability of therapeutic alternatives. The resulting list
contains 360 interacting pairs of individual drugs con-
taining 86 unique drugs. This list will be referred to as
the reference set of DDIs in the following sections.
Specific contribution
The specific contribution of our work is to contrast two
publicly available sources of DDI information, NDF-RT
and DrugBank, through an assessment of the overlap of
their content, and of their coverage of a reference set of
DDIs. Moreover, we compare the ability of these two
sources to identify DDIs in a large prescription dataset,
and contrast them against a commercial source. To the
best of our knowledge, this is the first such comparative
investigation of NDF-RT and DrugBank DDI information.
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 3 of 10Methods
Our approach to evaluating drug-drug interaction (DDI)
information in NDF-RT and DrugBank can be summa-
rized as follows. We acquire the list of drug-drug inter-
actions in NDF-RT and DrugBank, as well as a reference
set of DDIs. We map all drugs from the three sets to
RxNorm and further normalize them to ingredient en-
tities. We then compare the lists of pairs of interacting
drugs across sources in order to determine the shared
coverage between NDF-RT and DrugBank, as well as the
coverage of the reference set by both sources. We
characterize the differences among DDI sets in terms of
drug classes. We also compare the interactions detected
with NDF-RT and DrugBank in a large prescription
dataset. Finally, we compare the coverage of the refer-
ence set by DrugBank and NDF-RT to that of a com-
mercial source.
Acquiring DDI information
NDF-RT
We used the NDF-RT API [11] to first extract the full
set of DDIs (DRUG_INTERACTION_KIND concepts),
then to extract each associated drug concept (level = in-
gredient) in the pair.
DrugBank
The DrugBank XML and schema definition files were
downloaded from the DrugBank web site. We extracted
the interaction data from the XML file and created a
table of drug name pairs for the interacting drugs.
Reference set
The reference set of DDIs was created from the drug
names listed in Table two of [9] by associating each ob-
ject drug with all corresponding precipitant drug(s)
within a given interaction class. One pair involving a
multi-ingredient drug (azathioprine and mercaptopu-
rine) was eliminated, because multi-ingredient drugs are
generally not consistently represented across sources.
Normalizing drugs in reference to RxNorm
After obtaining the drug name pairs for interacting
drugs, we mapped the drugs to RxNorm by retrieving
the RxNorm identifiers (RxCUIs). For NDF-RT, the
RxCUI is part of the drugs concept properties, so we
used the NDF-RT API to extract the RxCUI from the
drug properties. For DrugBank and the reference set, we
used the RxNorm API [7] to find the RxCUI from the
drug name. More specifically, we used exact and nor-
malized string matches to map drug names to RxNorm.
We then normalized to RxNorm ingredients those
drugs which mapped to RxNorm entities. Some of the
drugs corresponded to base ingredients (e.g., doxacur-
ium), while others corresponded to salt forms thatRxNorm classifies as precise ingredients (e.g., doxacur-
ium chloride). In order to establish a consistent drug
representation across all three data sets for comparison,
we converted precise ingredients to RxNorm ingredients
(e.g., doxacurium chloride to doxacurium) using the
RxNorm API.
We eliminated from the comparison those pairs for
which at least one of the drugs could not be found in
RxNorm. For example, the DrugBank drug cerivastatin,
which was withdrawn from the U.S. market in 2001, is
not present in RxNorm.
Comparing interactions across sources
Having normalized all drugs to RxNorm ingredients, we
compared the lists from DDIs of NDF-RT and DrugBank
in order to determine the similarities and unique fea-
tures of each source. In addition, we compared each
with the reference set of DDIs to determine how many
of the reference DDIs each source covered.
Characterizing differences
To determine if there was a pattern of missing interac-
tions, we abstracted the pairs of interacting drugs into
pairs of classes from the Anatomical Therapeutic Chem-
ical (ATC) Classification System. We mapped the drugs
directly to their 4th-level ATC classes using the RxNorm
API and identified those pairs of ATC classes for which
the proportion of DDIs in common between NDF-RT
and DrugBank was low, using the Jaccard score. Of note,
some NDF-RT and DrugBank drugs are not represented
in ATC. So only DDIs for which both the object and
precipitant drugs are present in ATC were analysed.
Comparing coverage of interactions from actual
prescription data
In order to assess the difference between NDF-RT and
DrugBank interactions based on usage data, we collected
drug pairs generated from actual patient prescription lists.
Checking medication lists for drug-drug interactions repre-
sents a practical use case and is offered as a service by
many medication list applications. It also provides some
frequency of usage of the co-prescribed drug pairs extracted
from the lists. The data was acquired from Symphony
Health Solutions (http://symphonyhealth.com/). It included
one year of prescription data from the Washington, D.C.
area from July 1, 2011 through June 30, 2012. Prescrip-
tion information included prescriber, de-identified pa-
tient information, and specific medication information,
including the drug name and strength. The drug infor-
mation was mapped into RxNorm clinical drugs. From a
list of co-prescribed drugs, we generated all possible
pairwise combinations within the drug list (ignoring the
order of drugs in the pair, since the distinction between
object and precipitant drugs is not required for DDI
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 4 of 10testing). We ignored topical drugs from the prescription
list, because DDI information represented at the ingre-
dient level in NDF-RT and DrugBank usually refers to
interactions between systemic drugs. For example, we
generated the following pair of RxNorm clinical drug,
24 HR Diltiazem Hydrocloride 360 MG Extended Re-
lease Oral Capsule (830795) and Lovastatin 20 MG Oral
Tablet (197904). We mapped the clinical drugs to their
active moieties in RxNorm (Diltiazem Hydrocloride and
Lovastatin, respectively) and further normalized those
to RxNorm ingredients, as we did with all object and
precipitant drugs from the various DDI lists. Here, we
normalized Diltiazem Hydrocloride to Diltiazem. In
summary, we transformed the pair of RxNorm clinical
drugs extracted from the prescription list into the pair
of RxNorm ingredients (Diltiazem, Lovastatin) for com-
parison to the DDIs in NDF-RT and DrugBank.
Publicly available vs. commercial DDI sources
To compare the coverage of the reference set by Drug-
Bank and NDF-RT to that of a commercial source, we
investigated the Multum drug knowledge base through
the interaction checker of the website Drugs.com
(http://www.drugs.com/), against which we tested the
360 DDIs of the reference set.
Results
DDI information in NDF-RT, DrugBank and the reference set
Table 1 summarizes the number of DDIs in the three
data sets.
Reference set
The reference set contained 360 DDIs; all the drugs
mapped to RxNorm and were all classified as ingredi-
ents. The 360 DDIs covered 86 RxNorm ingredients.
DrugBank
DrugBank contained 12,128 DDIs defined in the XML
file. DDIs involving drugs with no mapping to RxNorm
were discarded (418 DDIs involving 46 drugs). Analysis
of all the discarded DDIs revealed several reasons why
the DDIs were eliminated. Some DDIs involved drugs
that were either withdrawn from public use (e.g., cerivas-
tatin, ephedra, heptabarbital) or not approved by the
U.S. Food and Drug Adminstration (e.g., cinolazepam,
carbetocin) and those drugs could not be mapped toTable 1 DDI counts in the three datasets
Data set DDI counts
Total from source Mapped to RxNorm Normalized
Reference 360 360 360
DrugBank 12128 11762 11552
NDF-RT 10831 9452 9392RxNorm. Additionally, 518 DrugBank DDIs were elimi-
nated through the ingredient normalization process. For
example, the DDIs containing zuclopenthixol, zuclo-
penthixol acetate and zuclopenthixol deconoate were
normalized to produce a single set with zuclopenthixol
as the ingredient, eliminating the redundant pairs con-
taining the salt forms. The resulting 11,552 normalized
DDIs covered 1153 RxNorm ingredients.
NDF-RT
NDF-RT contained 10,831 DDIs extracted from the data
set. DDIs involving drugs with no mapping to RxNorm
were discarded (1379 DDIs involving 38 drugs). Analysis
of all the discarded DDIs revealed that some DDIs were
associated with drugs which referenced obsolete RxNorm
concepts, many of these vaccine drugs that were recently
removed from RxNorm. Additionally, 60 NDF-RT DDIs
were eliminated through the ingredient normalization
process. The resulting 9,392 normalized DDIs covered
1079 RxNorm ingredients.
In the remainder of this paper, DDIs refer to pairs of
object and precipitant drugs normalized to RxNorm in-
gredients. However, even after normalization to RxNorm
ingredients, the coverage of drugs is not expected to be
the same in NDF-RT and DrugBank. For example, vac-
cines and other biologicals are present in NDF-RT, but
out of scope for DrugBank. When analysing DDIs across
the two sources, breakdown by pharmacological classes
will reflect such differences in drug coverage.
Comparing interactions across sources
The matching DDIs between the three data sets are
shown in Table 2.
Overlap between DrugBank and NDF-RT
Overall, the 2801 DDIs common to NDF-RT and Drug-
Bank represent a 30% coverage rate of NDF-RT by
DrugBank and a 24% coverage rate of DrugBank by
NDF-RT. Example of common DDIs include diltiazem/
lovastatin and itraconazole/sirolimus. Examples of DDIs
in DrugBank only include acebutolol/Insulin Lispro and
metronidazole/terfenadine. Examples of DDIs in NDF-
RT only include amiodarone/sotalol and meperidine/
linezolid.
When we only consider DDIs from the reference set,
the overlap between DrugBank and NDF-RT is significantlyTable 2 Matching DDIs across data sets
Data set Number of matching DDIs
Reference DrugBank NDF-RT
Reference 360 211 207
DrugBank 211 11552 2801
NDF-RT 207 2801 9392
Table 3 Reference Set DDI and coverage in NDF-RT and DrugBank
Grp # DDI group
description
# pairs NDF-RT
matches
DrugBank
matches
Object members Precipitant members
3 Amphetamine and
derivatives  MAO
inhibitors
60 30 (50%) 30 (50%) Dexmethylphenidate, Dextroamphetamine, Methylphenidate,
Lisdexamfetamine, Phendimetrazine, Pseudoephedrine,
Amphetamine, Benzphetamine, Diethylproprion, Phentermine,
Atomoxetine, Methamphetamine
Tranylcypromine, Phenelzine, Isocarboxazid, Procarbazine,
Selegiline
4 Atazanavir  Proton
pump inhibitors (PPIs)
5 5 (100%) 5 (100%) Atazanavir Omeprazole, Lansoprazole, Pantoprazole, Rabeprazole, Esmoprazole
8 Fluoxetine - MOA
inhibitors
55 39 (71%) 43 (78%) Fluoxetine, Paroxetine, Citalopram, Escitalopram,
Sertraline, Fluvoxamine, Duloxetine, Nefazodone,
Desvenlafaxine, Milnacipran, Venlafaxine
Tranylcypromine, Phenelzine, Isocarboxazid, Procarbazine, Selegiline
11 Irinotecan 
Ketoconazole
23 1 (4%) 5 (22%) Irinotecan Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,
Darunavir, Lopinavir, Tipranavir, Fosamprenavir, Clarithromycin,
Erythromycin, Telithromycin, Amiodarone, Verapamil, Diltiazem,
Ketoconazole, Itraconazole, Fluconazole, Voriconazole, Nefazodone,
Aprepitant, Cimetidine
16 Narcotic analgesics 
MAO inhibitors
30 25 (83%) 15 (50%) Meperidine, Methadone, Tapentadol, Fentanyl,
Tramadol, Dextromethorphan
Tranylcypromine, Phenelzine, Isocarboxazid, Procarbazine, Selegiline
22 Ramelteon-fluvoxamine 4 0 (0%) 2 (50%) Ramelteon Fluvoxamine, Amiodarone, Ticlopidine, Ciprofloxacin
23 Rifampin  ritonavir 60 41 (68%) 25 (42%) Bosentan, Rifapentine, Carbamazepine, Rifabutin,
Rifampin, St. Johns wort
Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,
Darunavir, Lopinavir, Tipranavir, Fosamprenavir
25 HMG Co-A reductase
inhibitors  protease
inhibitors
40 38 (95%) 33 (83%) Simvastatin, Lovastatin Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,
Darunavir, Lopinavir, Tipranavir, Clarithromycin, Erythromycin,
Telithromycin, Amiodarone, Verapamil, Diltiazem, Tranylcypromine,
Phenelzine, Isocarboxazid, Procarbazine, Selegiline
27 Telithromycin 
ergot alkaloids and
derivatives
60 13 (22%) 35 (58%) Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir,
Amprenavir, Darunavir, Lopinavir, Tipranavir,
Clarithromycin, Erythromycin, Telithromycin,
Ketoconazole, Itraconazole, Voriconazole
Ergotamine, Methylergonovine, Dihydroergotamine, Ergonovine
28 Tizanidine 
ciprofloxacin
7 6 (86%) 6 (86%) Tizanidine Fluvoxamine, Amiodarone, Ticlopidine, Ciprofloxacin, Mexiletine,
Propafenone, Zileuton
30 Tranylcypromine 
procarbazine
1 1 (100%) 1 (100%) Tranylcypromine Procarbazine
31 Triptans  MAO
inhibitors
15 8 (53%) 11 (73%) Sumatriptan, Zolmitriptan, Rizatriptan Tranylcypromine, Phenelzine, Isocarboxazid, Moclobamide,
Methylene blue
TOTAL 207 (58%) 211 (59%)
Peters
et
al.Journalof
Biom
edicalSem
antics
 (2015) 6:19 
Page
5
of
10
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 6 of 10higher. The 146 reference DDIs common to NDF-RT and
DrugBank represent 71% of the 207 reference DDIs cov-
ered by NDF-RT and 69% of the 211 reference DDIs cov-
ered by DrugBank.
Reference set coverage
Table 3 shows the breakdown by the reference set
groups of the DDI mapping for DrugBank and NDF-RT.
DrugBank contained 211 DDIs (59%) from the reference
set, compared with 207 DDIs (58%) for NDF-RT. There
were 146 DDIs (42%) from the reference set which were
both in the NDF-RT and DrugBank, including diltia-
zem/lovastatin, simvastatin/amiodarone and atazana-
vir/omeprazole. Conversely, there were 88 DDIs (24%) in
the reference set which were not contained in either
DrugBank or NDF-RT, including irinotecan/indinavir,
ketoconazole/ergonovine and milnacipran/selegiline.
DrugBank contained 65 DDIs from the reference set
which were not in NDF-RT (e.g., lovastatin/tipranavir
and ramelteon/fluvoxamine), and NDF-RT contained 61
DDIs from the reference set not in DrugBank (e.g., flu-
oxetine/procarbazine and simvastatin/saquinavir). Drug-
Bank had coverage in all groups from the reference set,
though only 100% coverage in two groups. NDF-RT had
coverage of all but one group (#22 ramelteon-fluvox-
amine). The coverage of the reference set of drug-drug
interactions by DrugBank and NDF-RT is available as
Additional file 1.
Of the 86 drugs contained in the reference set, one
(moclobamide, involved in 3 DDIs) did not exist in NDF-
RT and two (dexmethylphenidate, involved in 5 DDIs and
methylene blue, involved in 3 DDIs) did not exist in Drug-
Bank. In addition, there were four other drugs (bosentan,
lopinavir, methadone and zileuton) which were present inTable 4 Top ATC Class Counts in NDF-RT and DrugBank
NDF-RT
ATC Class Name DDI
Antibiotics 709
Anticholinesterases 25
Fluoroquinolones 401
Hydantoin derivatives 266
Macrolides 443
Non-selective monoamine reuptake inhibitors 311
Other antidepressants 309
Protease inhibitors 1130
Protein kinase inhibitors 759
Selective immunosuppressants 307
Selective serotonin reuptake inhibitors 341
Triazole derivatives 427
Vitamin K antagonists 292
*Boldface values indicate the ten top categories in each source.DrugBank, but not involved in any of the DDIs from the
reference set. DrugBank did have DDIs for these four
drugs outside of the reference set.
Pharmacologic classes
We mapped the DDIs from NDF-RT and DrugBank to
4th-level ATC drug classes to see if there were distinctive
class differences between the two sets of DDIs. A small
proportion of the drugs, such as avanafil, lopinavir and
zileuton (14% in NDF-RT, 11% in DrugBank) were not
represented in ATC and the corresponding DDIs were
excluded from the analysis.
Table 4 shows the top frequency count of ATC classes
for NDF-RT and DrugBank DDIs, along with the num-
ber of drugs from each class represented in the source
of DDI. For example, NDF-RT has 401 DDIs involving
11 drugs for the fluoroquinolone class, while DrugBank
has 335 interactions involving 14 drugs for this class.
Fluoroquinolones is in the top-10 classes for the fre-
quency of DDIs in NDF-RT, but not in DrugBank. Two
classes, protein kinase inhibitors and selective immuno-
suppressants, seem underrepresented in DrugBank,
while three classes, anticholinesterases, hydantoin deriv-
atives and non-selective monoamine reuptake inhibitors,
seem underrepresented in NDF-RT.
Table 5 shows the ATC pairs containing the most
DDIs from NDF-RT and DrugBank, but a low propor-
tion of shared DDIs between the two sources (evidenced
by the low Jaccard scores). The Adrenergic and dopa-
minergic agents - Non-selective monoamine reuptake in-
hibitors class pair, for example, has no NDF-RT DDIs
and 78 DrugBank DDIs. Examples in this class pair in-
clude dopamine/amitriptyline and norepinephrine/amox-
ipine. Conversely, in the Protease inhibitors - ProteinDrugBank
drugs DDI drugs
22 744 25
4 411 7
11 335 14
4 406 4
6 502 9
10 736 10
11 457 10
11 751 11
19 389 20
13 150 14
6 380 6
4 502 4
3 427 3
Table 5 Top ATC Class Pairs
ATC Class Pair total DDIs NDF-RT only DrugBank only both Jaccard
Protease inhibitors - Protein kinase inhibitors 103 72 10 21 0.20
Benzodiazepine derivatives - Protease inhibitors 88 5 52 28 0.32
Adrenergic and dopaminergic agents - Non-selective monoamine reuptake inhibitors 78 0 78 0 0
Antibiotics - Other quaternary ammonium compounds 71 35 8 28 0.39
Penicillins with extended spectrum - Tetracyclines 63 0 58 5 0.08
Protein kinase inhibitors - Triazole derivatives 60 32 10 18 0.30
Beta blocking agents, non-selective - Sulfonamides, urea derivatives 56 10 14 32 0.57
Macrolides - Protein kinase inhibitors 56 27 14 15 0.27
Figure 1 Frequency of DDIs in prescription pairs.
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 7 of 10kinase inhibitors class pair, many more specific DDIs
exist in NDF-RT, such as boceprevir/bosutinib, than in
DrugBank. Finally, the Beta blocking agents, non-
selective - Sulfonamides, urea derivatives class pair illus-
trates a situation where both sources share a majority of
DDIs, but also have a significant number of specific
DDIs. For example, sotalol/glyburide is common, but
sotalol/tolazamide is specific to NDF-RT and sotalol/gli-
clazide is specific to DrugBank.
Comparing coverage of interactions from actual
prescription data
From the prescription dataset, almost 35.9 million pairs
were extracted, representing 816,258 unique pairs of
RxNorm clinical drugs. Restricted to systemic drugs, the
dataset included 35.5 million pairs of clinical drugs
(808,285 unique). Each clinical drug maps to at least one
ingredient, and multi-ingredient drugs map to several in-
gredients, resulting in multiple ingredient pairs for a
given pair of clinical drugs. For example, starting from
the pair of clinical drugs Primidone 250 MG Oral Tablet
and Carbidopa 25 MG/Levodopa 100 MG Oral Tablet,
we generate the following two pairs of ingredients, (Pri-
midone, Carbidopa) and (Primidone, Levodopa), because
the clinical drug Carbidopa 25 MG/Levodopa 100 MG
Oral Tablet maps to two ingredients, Carbidopa and
Levodopa. After mapping of the clinical drugs to ingredi-
ents, there were 45.2 million pairs of co-prescribed
drugs, ranging in frequency between 1 and 158,515 (me-
dian = 18). Of these, 808,285 pairs matched with NDF-
RT DDIs (2153 unique), while 1,170,693 pairs matched
with DrugBank DDIs (2823 unique). There were 382,833
pairs that matched with both NDF-RT and DrugBank
(988 unique). There were 26,672 matches with the refer-
ence set (88 unique). Figure 1 shows the frequency of
the DDIs (in parenthesis) with the unique number of
DDIs found in the prescription dataset.
Examples of frequently co-prescribed drugs identified
as a DDI by both NDF-RT and DrugBank include Diltia-
zem/Lovastatin and Clarithromycin/Simvastatin. Co-
prescribed drugs identified as a DDI by NDF-RT, butnot DrugBank include Amlodipine/Simvastatin and Dil-
tiazem/Oxycodone. Conversely, DDIs identified by Drug-
Bank, but not NDF-RT, include Amoxicillin/Ethinyl
Estradiol and Hydrochlorothiazide/Insulin Lispro. Table 6
shows the most frequently co-prescribed pairs recog-
nized as a DDI by one or both of the sources.
Publicly available vs. commercial DDI sources
We were able to match 328 (91%) DDIs of the reference
set to Multum DDIs, a significantly higher proportion
compared to NDF-RT and DrugBank. The level of sever-
ity provided by Multum for these DDIs (on a 3-level
scale mild, moderate or major) was consistent with the
notion of high-severity claimed by the authors of the
reference set, since 302 DDIs were rated major and the
other 26 were rated moderate.
Discussion
In this section, we discuss some of the reasons for the
limited overlap of DDI information among sources. We
also analyse its implications for our interactions API.
Limited overlap between NDF-RT and DrugBank DDIs
Findings
One important finding of this investigation is the limited
overlap (<30%) between DrugBank and NDF-RT DDIs,
Table 6 Top DDI Pairs found in Prescription Data
Drug pair Freq DrugBank DDI NDF-RT DDI
Amlodipine-Simvastatin 76980 X
Glipizide-Metoprolol 15469 X X
atorvastatin-Fenofibrate 14897 X X
Fenofibrate-Simvastatin 14060 X X
Albuterol-Metoprolol 14030 X
Insulin Glargine-Metoprolol 13492 X
Metoprolol-Sertraline 13311 X
atorvastatin-Diltiazem 13218 X X
Digoxin-Furosemide 12958 X
Lisinopril-Triamterene 12399 X X
cyclobenzaprine-Tramadol 12211 X
Diltiazem-Metoprolol 11738 X
Oxycodone-Sertraline 11710 X
Diltiazem-Simvastatin 11484 X X
Alprazolam-Fluoxetine 11194 X
Sertraline-Trazodone 11066 X
Amoxicillin-Ethinyl Estradiol 11044 X
Lisinopril-Spironolactone 10248 X X
Clonazepam-Fluoxetine 10149 X
Fenofibrate-rosuvastatin 9954 X X
Clonidine-Metoprolol 9827 X X
Hydralazine-Metoprolol 9556 X
duloxetine-Trazodone 9236 X
clopidogrel-pantoprazole 9177 X
Escitalopram-Metoprolol 8396 X
Levofloxacin-Prednisone 8335 X
Escitalopram-Oxycodone 8228 X
Citalopram-Trazodone 8081 X
Citalopram-Oxycodone 8016 X
Metoprolol-salmeterol 7703 X
Atenolol-Glipizide 7597 X X
Fluoxetine-Trazodone 7579 X
carvedilol-Digoxin 7517 X
Fluoxetine-Oxycodone 7508 X
carvedilol-Insulin Glargine 7496 X
Citalopram-Metoprolol 7474 X
Escitalopram-Trazodone 7421 X
Alprazolam-Omeprazole 7339 X
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 8 of 10although both sources cover roughly the same number
of DDIs. We found 6591 DDIs specific to NDF-RT and
8751 specific to DrugBank, while only 2801 DDIs were
common to both sources. Differences in scope (e.g., the
lack of vaccine DDIs in DrugBank) account for only a
small portion of the differences. Differential coveragepersisted after abstracting DDIs to the corresponding
drug classes from ATC. For example, NDF-RT had many
more DDIs than DrugBank for Protease inhibitor drugs,
and had much fewer DDIs for Anticholinesterases drugs.
Likewise, many ATC class pairs had a small proportion
of DDIs in common, evidenced by low Jaccard scores.
For example, among the 56 DDIs for the Macrolides -
Protein kinase inhibitors class pair, only 15 are common
to NDF-RT and DrugBank (Jaccard = 0.27). The low
overlap rate also applied to our more frequently pre-
scribed drug pairs in the prescription data set. When we
examined the co-prescribed drugs from the prescription
data set, we found significant differences in identification
of DDIs between the two sources for the most frequently
co-prescribed pairs. Of the top 10 most frequently pre-
scribed drug pairs identified as potential DDIs by Drug-
Bank, only 5 were recognized as NDF-RT DDIs (see
Table 6 for details).
DDI severity
We wondered if the differential coverage observed be-
tween NDF-RT and DrugBank could be due in part to
different editorial guidelines for curating less severe
DDIs, assuming the most severe DDIs would be covered
more consistently by both sources. This did not seem to
be the case since the 360 high-severity DDIs from the
reference set are only partially covered by NDF-RT and
DrugBank. While NDF-RT provides an indication of se-
verity (critical or significant) for its DDIs, DrugBank
does not. Nevertheless, we tested if DrugBank would
cover a larger proportion of NDF-RT critical DDIs. In
fact, DrugBank DDIs accounted for 31% of the total crit-
ical DDIs in NDF-RT and for 29% of the total significant
DDIs. In other words, there is no difference in the cover-
age of NDF-RT by DrugBank in terms of DDI severity.
(Even when correcting for the absence of vaccine DDIs
in DrugBank, the overlapping DDIs accounted for just
40% of the critical DDIs in NDF-RT.)
Differential coverage of DDIs among sources
Our finding of limited overlap between NDF-RT and
DrugBank should not be surprising given similar find-
ings in studies comparing drug knowledge bases for crit-
ically important DDIs [21]. There is no standardization
for what constitutes a drug-drug interaction, and DDI
curators have to consider a variety of drug and patient
specific factors in their decision to include pairs of drugs
in their DDI lists. These factors include the severity of
the interaction, the probability of the interaction, patient
characteristics (e.g., specific patient groups, such as eld-
erly patients), and evidence supporting the interaction
(quantity of evidence, as well as the quality of the evi-
dence). The number of factors to consider makes this a
complex and daunting task. Moreover, the maintenance
Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 9 of 10of DDI lists itself is an issue, as additional evidence for
or against DDIs becomes available over time. Finally, dif-
ferent strategies for prioritizing the various factors also
accounts for some of the differences observed across
sources.
Implications for the interactions API
Overall, our investigation of NDF-RT and DrugBank as
sources of DDIs for our API provides a mixed picture.
Not only do they both provide incomplete coverage of
the reference set (about 60% each), but their overlap is
also limited (42%). A similar difference could be ob-
served when we simulated interaction detection based
on actual usage data. We also confirmed that a commer-
cial source, Multum, provided a more systematic cover-
age of the reference set.
With the provision of DDI information being discon-
tinued in NDF-RT in November 2014, we decided to use
DrugBank as a replacement for our interactions API.
This solution is far from ideal, because this investigation
did not establish the accuracy of either source, but sim-
ply assessed differences among them. On the other hand,
DDI determination is not an exact science and both
NDF-RT and DrugBank provide valuable information to
support medical decision. NDF-RT DDIs are associated
with levels of severity, while DrugBanks come with a
short description. In practice, the absence of severity
levels in DrugBank is a disadvantage, as severity is an
important consideration for determining when to alert
physicians to a potential DDI. Severity is also a require-
ment when checking DDI in the context of the Mean-
ingful Use incentive program.
It is important to keep in mind that no clinical deci-
sion system, as good as it is, can be a substitute for med-
ical advice. Our API not only clearly discloses the origin
of the information it provides, but also reminds our
users to seek advice from health professionals before
making decisions about their medications. In the future,
we plan to perform a more systematic and qualitative in-
vestigation of publicly available and commercial DDI
sources in order to better assess the differences among
these sources.
Conclusions
This study is the first comparative investigation of DDI
information in two publicly available sources, NDF-RT
and DrugBank. We compared the two sources not only
to themselves, but also to a reference set of DDIs, and
assessed their ability to identify DDIs in a large prescrip-
tion dataset. We also contrasted NDF-RT and DrugBank
against the commercial source Multum.
This investigation confirms the limited overlap be-
tween DDI information between NDF-RT and Drug-
Bank, even for the reference dataset. Additional researchis required to determine which source is better, if any.
Usage of any of these sources in clinical decision systems
should clearly disclose these limitations.
Additional file
Additional file 1: Coverage of the reference set of drug-drug inter-
actions by DrugBank and NDF-RT.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
LP downloaded, normalized and compared the main datasets. OB created
and normalized the reference dataset. NB characterized the differences
against ATC drug classes and the commercial source of DDI information. All
the authors contributed to writing the manuscript. All the authors have read
and approved the final manuscript.
Acknowledgements
This work was supported by the Intramural Research Program of the NIH,
National Library of Medicine.
Author details
1Lister Hill National Center for Biomedical Communications, National Library
of Medicine, National Institutes of Health, Bethesda, Maryland, USA.
2Department of Medical Informatics and Clinical Epidemiology, Oregon
Health & Science University, Portland, Oregon, USA.
Received: 22 November 2014 Accepted: 30 March 2015
Published: 11 May 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 
DOI 10.1186/s13326-015-0017-1RESEARCH ARTICLE Open AccessExploring adverse drug events at the class level
Rainer Winnenburg1, Alfred Sorbello2 and Olivier Bodenreider3*Abstract
Background: While the association between a drug and an adverse event (ADE) is generally detected at the level
of individual drugs, ADEs are often discussed at the class level, i.e., at the level of pharmacologic classes (e.g., in
drug labels). We propose two approaches, one visual and one computational, to exploring the contribution of
individual drugs to the class signal.
Methods: Having established a dataset of ADEs from MEDLINE, we aggregate drugs into ATC classes and ADEs into
high-level MeSH terms. We compute statistical associations between drugs and ADEs at the drug level and at the
class level. Finally, we visualize the signals at increasing levels of resolution using heat maps. We also automate the
exploration of drug-ADE associations at the class level using clustering techniques.
Results: Using our visual approach, we were able to uncover known associations, e.g., between fluoroquinolones
and tendon injuries, and between statins and rhabdomyolysis. Using our computational approach, we systematically
analyzed 488 associations between a drug class and an ADE.
Conclusions: The findings gained from our exploratory techniques should be of interest to the curators of ADE
repositories and drug safety professionals. Our approach can be applied to different drug-ADE datasets, using different
drug classification systems and different signal detection algorithms.
Keywords: Adverse drug events, Drug classes, Anatomical Therapeutic Chemical (ATC) drug classification system,
Class effect, Heat maps, PharmacovigilanceBackground
Motivation
According to the Agency for Healthcare Research and
Quality (AHRQ), adverse drug events (ADEs) result in
more than 770,000 injuries and deaths each year and
cost up to $5.6 million per hospital [1]. Drug safety is
addressed through the drug development process, not
only during clinical trials [2], but also through postmarket-
ing surveillance, by analyzing spontaneous reports [3],
observational data [4] and the biomedical literature [5].
While the association between a drug and an adverse
event is generally detected at the level of individual drugs
(e.g., between aspirin and Reye syndrome [6]), ADEs are
often discussed at the level of pharmacologic classes.
Examples include the ototoxicity of aminoglycosides [7],
the association between statins and rhabdomyolysis [8],
and between vaccines and Guillain-Barré syndrome [9].* Correspondence: obodenreider@mail.nih.gov
3Lister Hill National Center for Biomedical Communications, National Library
of Medicine, National Institutes of Health, Bethesda, MD, USA
Full list of author information is available at the end of the article
© 2016 Winnenburg et al.; licensee BioMed Ce
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.These examples illustrate the need for investigating ADEs
at the class level, i.e., after aggregating individual drugs
into pharmacologic classes.
Some ADEs can be observed with every individual
drug in a class. This is often the case when the ADE is
related to the physiologic effect of the drug. For ex-
ample, bleeding is a common effect of anticoagulants,
such as vitamin K antagonists [10]. Conversely, some
ADEs are associated with some class members, but not
with all of them. For example, a recent review reports a
differential risk of tendon injuries with various fluoro-
quinolones, the highest risk being with ofloxacin [11].
From an ontological perspective, it is interesting to
explore whether the ADE is an inherent property of the
class (inherited by every member of the class) or rather
a property of some members only. In practice, when
there is a high risk of an ADE for a class (i.e., a strong
class-level signal), one may want to drill down and
investigate the drug-level signal for each individual drug
in the class to discover if the class-level signal results
from uniformly high drug-level signals, or is rather
driven by an intense signal for a small number of drugs,ntral. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 2 of 10while the other drugs in the class would not exhibit a high
risk for this ADE. The former reflects a class property
inherited by each drug, whereas the latter reflects a drug
property, i.e., a property for some of the drugs only.
The objective of this work is to explore the contribution
of individual drugs to the class signal. More specifically, we
propose two approaches, one visual and one computa-
tional, to identifying class effects, i.e., cases when all
drugs in a class have the same ADE (as opposed to cases
where the class signal is driven by only a few drugs from
the class).
Drug and ADE terminologies
The following sections detail the characteristics of the
resources used in this research. We use MeSH for aggre-
gating ADEs and ATC for drug classification purposes.
We also use RxNorm to harmonize drugs between
MeSH and ATC.
MeSH
The MeSH thesaurus is the controlled vocabulary used
to index documents included in the MEDLINE database
[12]. It contains over 27,000 descriptors (main headings)
organized in sixteen hierarchical tree structures. Each
tree contains up to eleven levels denoting aboutness
relationships between the terms. For example, the term
Rhabdomyolysis is classified under Muscular Diseases
in the Diseases tree. Version 2014 of MeSH is used in
this study.
ATC
The Anatomical Therapeutic Chemical (ATC) classifica-
tion [13], a system developed by the World Health
Organization (WHO) Collaborating Centre for Drug
Statistics Methodology, is recommended for worldwide
use to compile drug utilization statistics. The system
includes drug classifications at 5 levels; anatomical, thera-
peutic, pharmacological, chemical and drugs or ingredi-
ents. For example, the 4th-level ATC class Vitamin K
antagonists (B01AA) has the following 5th-level drugs as
members: acenocoumarol, dicumarol, fluindione, phe-
nindione, phenprocoumon, tioclomarol and warfarin.
The 2014 edition of ATC used in this study contains
4,580 5th-level ATC drugs and 1,256 drug classes.
RxNorm
RxNorm is a standardized nomenclature for medications
produced and maintained by the U.S. National Library
of Medicine (NLM) [14]. Both ATC and MeSH are inte-
grated in RxNorm, making it possible for us to use
RxNorm to link MeSH drugs to their classes in ATC.
Moreover, RxNorm provides a rich network of relations
among various types of drug entities, making it possible
to normalize the various salts and esters of a drug(precise ingredients) to their base form (ingredient).
The April 2014 version of RxNorm is used in this study
and was accessed through the RxNorm API [15].
Related work
ADE extraction and prediction
There is a large body of research on the extraction of
drug ADE associations from various sources (e.g.,
[3-5,16]), in which terminologies are usually leveraged
for the normalization of drugs (e.g., to RxNorm and
ATC) and adverse reactions, for example to the Com-
mon Terminology Criteria for Adverse Events (CTCAE)
and the Medical Dictionary for Regulatory Activities
(MedDRA). Researchers have also created repositories of
ADEs, such as ADEpedia [17] and used network analysis
to analyze and predict drug-ADE associations [18]. In
our effort to explore the ADEs at the class level, we use
an existing dataset of drug-ADE pairs obtained from
prior work on extracting drug-ADE pairs from MED-
LINE indexing.
Research on class effect
Many researchers have investigated whether a given
ADE was specific to a drug or common to all drugs in
the corresponding class. Examples of such investigations
include the exploration of antiepileptic-induced suici-
dality [19], association between anti-VEGF agents and
dysthyroidism [20] or avascular necrosis of the femoral
head [21], association between dipeptidyl-peptidase-4
inhibitors and heart failure [22] or angioedema [23], and
atypical antipsychotic-induced somnambulism [24]. A
search for class effect in the titles of PubMed articles
retrieves over one hundred citations. Such efforts, how-
ever, generally investigate one specific drug class and
one specific ADE. In contrast, we propose a method
for assessing the class effect over a wide range of drug
classes and ADEs.
Specific contribution
The specific contribution of our work is to combine
existing drug safety signal detection and visualization
techniques, and to leverage drug terminologies for ex-
ploring adverse drug events at the class level. We extend
the visual exploration with an automated computational
approach to identifying class effects, allowing their system-
atic detection from any dataset of drug-ADE associations.
Methods
Our approach to exploring ADEs at the class level can
be summarized as follows. We first establish a dataset of
ADEs by extracting drug-ADE pairs from MEDLINE.
Then we aggregate drugs into ATC classes and ADEs
into high-level MeSH terms. We compute the associ-
ation between drugs and ADEs at the drug level and at
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 3 of 10the class level. In our visual approach, we use heat maps
to visualize the signal at increasing levels of resolution
to distinguish between drug-level and class-level ADEs.
In our computational approach, we achieve the same
result by leveraging clustering techniques. While the
visual approach requires manual selection of the classes
and ADEs of interest, the computational approach is
completely automated and can be applied over a wide
range of drug classes and ADEs.Extracting drug  adverse event pairs from the literature
Our dataset consists of pairs of drugs and ADEs ex-
tracted from the MEDLINE database, using an approach
similar to [5]. We use combinations of MeSH descrip-
tors (and supplementary concepts) and qualifiers to
identify, on the one hand, drugs involved in ADEs (e.g.,
ofloxacin/adverse effects) and, on the other, manifesta-
tions reflecting an ADE (e.g., tendinopathy/chemically
induced). We improved upon [5] by also taking into
account those MeSH descriptors inherently indicative of
adverse events (e.g., Drug-induced liver injury). We
collected the resulting list of drug-manifestation pairs
for each ADE (e.g., ofloxacin-tendinopathy).Linking MEDLINE drugs to ATC classes
We map all MeSH drugs extracted from MEDLINE to our
target terminology, ATC, for aggregation purposes, using
RxNorm.Table 1 Example of contingency table representing
drug-ADE associations in MEDLINE
With this ADE Without this ADE
Articles mentioning this drug a b
Articles not mentioning this drug c dMapping MeSH drugs to ATC drugs through RxNorm
ingredients
Both ATC and MeSH are integrated in RxNorm. For ex-
ample, the RxNorm drug rosuvastatin (301542) is linked
to both the MeSH drug rosuvastatin (C422923) and the
5th-level ATC drug rosuvastatin (C10AA07). Individual
drugs in MeSH correspond to ingredients (IN) and pre-
cise ingredients (PIN) in RxNorm. We normalize the
drugs by mapping PINs to their corresponding INs. For
example, RxNorm explicitly asserts that valproic acid is
the precise ingredient of the ingredient valproate.
Of note, a given drug can be represented multiple
times in ATC. Typically, topical drugs and systemic
drugs have different ATC codes for the same active
moiety. For example, the anti-infective ofloxacin has two
codes in ATC, depending on whether it is classified as
an antibacterial drug for systemic use (J01MA01) or as
an ophthalmological drug (S01AE01). However, we con-
sider unique drugs, not multiple codes, when we associ-
ate drugs with their ADEs. We only use the codes to
link drugs to their classes. The individual MeSH drugs
extracted from MEDLINE and which map to ATC
constitute the set of eligible drugs for this study.Establishing drug class membership
In ATC, the 5th-level drugs are linked to one or more
4th-level classes. For example, ofloxacin is a member of
the two Fluoroquinolones drug classes (J01MA and
S01AE). For the purpose of comparing class-level ADEs
to drug-level ADE, we require that the classes contain a
sufficient number of members. In practice, we exclude
all drug classes with fewer than 4 drug members in our
set of drugs. In this proof-of-concept investigation, this
threshold was selected as a trade-off between retaining a
sufficient number of classes and getting a meaningful
interpretation of the characteristics of the drugs in these
classes.
Aggregating adverse event terms in MeSH
ADEs can be expressed at different levels of granularity.
The MeSH hierarchy has multiple levels, enabling MED-
LINE indexers to capture information at the appropriate
level of granularity. However, for analytical purposes, it
is useful to aggregate detailed ADEs into coarser ADE
classes, similarly to what we do for the drugs. We use
descriptors at the second level of the MeSH hierarchy for
aggregation purposes. For example, we would aggregate
Tendinopathy (tree number C05.651.869) and Rhabdo-
myolysis (C05.651.807) to the second-level descriptor
Muscular Diseases (C05.651).
Computing adverse event signals at the drug level
In pharmacovigilance, safety signal detection consists in
the identification of an association between a drug and an
adverse event (AE). In this study, we use the traditional
proportional reporting ratio (PRR) [25] in computing stat-
istical associations for unique drug- and drug class-AE
pairs. PRR is a simple disproportionality method for signal
detection that is easy to compute and sufficient in the con-
text of this study. Based on the frequencies shown in
Table 1, the PRR is defined as follows:
PRR ¼ a= aþ bð Þð Þ= c= cþ dð Þð Þ ð1Þ
We calculate signals for all possible combinations of
drugs and ADEs that co-occur in at least one MEDLINE
article. We apply the usual zero-cell correction to tables
where b or c is equal to 0 (by adding 0.5 to each count
in the 2 × 2 table). For all pairs that do not co-occur in
the literature, we set the PRR to a neutral value of 1.
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 4 of 10Computing adverse event signals at the class level
At the class level, we compute the signal using a similar
approach. For drug classes, we count articles mentioning
any drug from this drug class (a and b) and articles men-
tioning any other drug (c and d). For ADE classes, we
count articles with any ADE from this ADE class (a and
c) and articles with any other ADE (b and d).
Exploring ADE signals at different levels
We want to determine whether the class signal is
driven by the strong signal of only a few drugs or is
distributed among all drugs from that class. To this
end, we visually explore the signal at different levels of
granularity, from drug class-ADE class, to individual
drug-ADE class, to individual drug-individual ADE.
Visual patterns reflect the contribution of the drug
signal to the class signal. We draw on the techniques
popularized by gene expression data studies, combining
clustering and heat map visualization [26], for explor-
ing the relations between drugs and ADEs. We rely on
the R statistical software package (version 3.1.2) for
implementation. More specifically, we use hclust for
clustering (using complete linkage and Euclidean dis-
tance) and heatmap for visualization.
Drug class-ADE class signal
We start by plotting all ATC4 drug classes against all
ADE classes, using the drug class signal. To reduce the
amplitude of the PRR signal, we plot the logn transform
of the PRR for all eligible class pairs. We perform hier-
archical clustering on both drug classes and ADE classes
to group pairs of drug classes and ADE classes with
similar signals. On the resulting heat map, strong signals
will appear in white and yellow, while weak signals will
be displayed in red.
Drug-ADE class signal
While a low-resolution map is sufficient to identify
strong class signals and the corresponding broad ADE
classes, a higher resolution is required to investigate the
distribution of the class signal among the individual
drugs members. Starting from the strongest signalsFigure 1 Patterns of associations between members of drug class CD(d1,..,dobserved in the previous step for a given drug class
(e.g., PRR above 10), we plot the signal for each drug in
the class. Here again, we perform hierarchical clustering
of both individual drugs and ADE classes (based on the
drug-level PRR, as opposed to the class-level PRR used
in the previous step). This heat map exhibits the distri-
bution of the class signal among the individual drug
members. In some cases, we see the emergence of
characteristic patterns illustrated in Figure 1:
1. A solid column (vertical bar) with medium intensity
(bright orange/ yellow) reflects an ADE (class) that
is equally distributed among all members of the
class, corresponding to a class property.
2. Several incomplete, non-overlapping vertical bars in
different columns, with medium intensity, reflect
ADEs (ADE classes) associated with subsets of the
class members, but not all members. This pattern
corresponds to the properties of sets of individual
drugs, rather than the property of the class itself.
3. Isolated spots or small islands of high intensity
reflect associations between one drug (or few drugs)
from the class and an ADE (class), corresponding to
individual drug properties.Drug-ADE signal
Finally, to assess individual ADEs, we plot the drug-
level signal for each ADE in the ADE classes present at
the previous step. As before, we perform hierarchical
clustering on both drugs and ADEs (based on the drug-
level PRR). This heat map exhibits the distribution
of the ADE class signal among the individual ADEs.
Patterns similar to those described above can also be
observed.Automating the detection of class effects
While the visual approach provides an intuitive explor-
ation of the ADEs within a drug class, its manual nature
restricts its large applicability. Here we propose an auto-
mated approach to identifying class effects in the same
dataset.n) and the manifestation of an adverse event class CE(e1,..,em).
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 5 of 10Intuition
In case of a class effect, the PRRs are expected to be
homogeneous among all drug members in a class for a
given ADE, and we should not be able to identify dis-
tinct subgroups among them. Conversely, if we can iden-
tify subgroups among the drugs, it means that the class
signal is driven by some drugs more than others, which
is not characteristic of a class effect.
Implementation
For a given drug class and ADE pair, we have computed
the class-level signal (as described in section 2.5) and
the drug-level signal for each drug in the class (as
described in section 2.4). Only classes with at least four
drug members are considered. Because PRRs are propor-
tions, we use their logn-transformed value to approach a
normal distribution.
To examine the distribution of the PRRs for individual
drugs in the class, we use k-means clustering with
Euclidean distance to identify two clusters (k = 2) among
the (logn-transformed) PRRs. We then compare the
means between the two clusters using Welch's t-test,
which accommodates unequal variances in samples. Of
note, in some cases, when the PRRs for all drugs in a
class are very similar, k-means clustering only produces
a single cluster. In this case, we assume that this cluster
is homogeneous by design. When we obtain only one
cluster or when the hypothesis of a difference between
the means of the two clusters is rejected (p-value > 0.05),
we conclude to a class effect.
For example, the 4th-level ATC class selective sero-
tonin reuptake inhibitors (N06AB) has a (logn-trans-
formed) PRR of 4.30 for the ADE sexual dysfunctions.
We partition the PRRs for the individual drugs into two
clusters: {fluoxetine (4.25), fluvoxamine (3.85)} and {ser-
traline (3.68), citalopram (3.57), paroxetine (3.77), escita-
lopram (3.57)}. There is no significant difference between
the means of the two clusters (p-value 0.28). Thus we
conclude that all the individual drugs contribute to the
signal for the drug class, which is the characteristic of a
class effect.
Results
Drug-ADE dataset
We collected 189,800 MEDLINE articles, from which we
extracted 371,417 drug-ADE pairs. The 244,692 MeSH
drug instances mapped to 1,966 distinct 5th-level ATC
drugs, and were aggregated into 598 4th-level ATC clas-
ses, of which 261 had at least four drugs. The 282,691
adverse event instances (3,043 distinct MeSH terms)
were aggregated into 314 2nd-level descriptors in MeSH.
The coarse matrix (Figure 2) reflects the association
between each of the 261 drug classes of interest and the
314 ADE classes. The dataset used for our computationalapproach includes all the 3,043 individual ADEs for each
of the 261 drug classes under investigation (794,223 pairs).
Visual approach
Drug class-ADE class signal
Figure 2 represents the heat map of 261 drug classes
and 314 ADE classes, with drug classes in rows and
ADE classes in columns. Because of the large number of
classes, the labels are not legible at this resolution. (A
high-resolution version of the heat maps is available
as Additional file 1). However, bright yellow spots or
islands are clearly visible. For example, the yellow rect-
angle right at the center corresponds to the association
between fluoroquinolones and various kinds of tendon
injuries. Isolated bright spots are equally interesting. For
example, the strong signal between statins and muscular
diseases is represented by a single bright spot.
Drug-ADE class signal
The left part of Figure 3 shows examples of interesting
patterns. There is a solid bar for all members of the
statins class and the ADE class muscular diseases. And
there is an incomplete column involving 8 of the 14
members of the fluoroquinolones class for the ADE class
tendon injuries. Isolated spots are also visible, for
example, between rosuvastatin and chronic fatigue
syndrome, and between fleroxacin and radiation injuries
and radiation-induced neoplasms.
Drug-ADE signal
The right part of Figure 3 also shows examples of inter-
esting patterns, with higher resolution than before. For
example, the solid bar between the statins class and the
ADE class muscular diseases, visible on the left, is
conserved, but we can now see that its signal is driven
by the specific ADE rhabdomyolysis.
Computational approach
Of the 794,223 pairs of (drug class, ADE), the large
majority correspond to cases where at least one of the
drugs in the class has no reported association with the
ADE in the pair. In the visual approach, we assigned
such combinations a neutral PRR of 1 for display pur-
poses, resulting in many red areas on the heat map. In
the computational approach, however, we ignored such
cases, because we cannot distinguish between absence of
evidence and evidence of absence for the drug-ADE
association. As a consequence, only 488 drug class-ADE
pairs could be explored for class effect. The class PRRs
for these pairs ranged from 0.11 to 373.97 (before logn
transformation), with 134 pairs having a PRR above 10
and 214 pairs having a PRR above 5.
The clustering process yielded two clusters in 457
cases (93%) and a single cluster in 31 cases (7%). When
Figure 2 Heat map of drug classes and ADE classes (based on the class signals).
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 6 of 10two clusters were identified, the difference between their
means was not significant in 337 (74%) and significant
in 120 (26%) of the 457 cases. Of note, a significant
difference between the clusters does not necessarily rule
out the possibility of a class effect, because the average
PRRs may be high in both clusters.
Examples of pairs with a single cluster include (cortico-
steroids, femur head necrosis) and (fibrates, muscular
diseases). Examples of pairs with two clusters between
which no difference could be found include (tetracycline
and derivatives, tooth discoloration), (statins, rhabdo-
myolysis) and (selective serotonin reuptake inhibitors,sexual dysfunctions, psychological). In many of the pairs
with two significantly different clusters, the PRRs were
high in both clusters, suggesting a class effect despite
the presence of two distinct clusters. For example, in the
pair (other aminoglycosides, labyrinth diseases) the aver-
age PRR is 57 in the first cluster (7 drugs) and over 350
in the second cluster (2 drugs). While drugs from the
second cluster (arbekacin and dibekacin) show a higher
risk of ototoxicity, the risk for the drugs from the first
cluster seems high enough (PRR = 57) for labeling oto-
toxicity a class effect. In contrast, there are pairs with
two significantly different clusters where the PRRs are
Figure 3 Detailed heap maps for individual drug classes (based on the individual drug signals); a) Fluoroquinolones, ADE classes and drugs;
b) Fluoroquinolones, ADEs and drugs; c) Statins, ADE classes and drugs; d) Statins, ADEs and drugs.
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 7 of 10high in one cluster and low in the other. For example, in
the pair (selective serotonin reuptake inhibitors, long QT
syndrome), only the drugs citalopram and escitalopram
exhibit a high PRR (about 20), while other drugs from
this class have low PRRs (e.g., sertraline and paroxetine
have PRRs between 1 and 2).
Discussion
Findings
Visual approach
Using our visual approach to exploring ADEs at the
class level, we were able to uncover known associations,
e.g., between fluoroquinolones and tendon injuries, and
between statins and rhabdomyolysis. More specifically,
exploring the signal at increasingly higher levels of reso-
lution revealed a difference between fluoroquinolones
and statins. Although both drug classes exhibit a strongclass-level signal for their respective ADEs, only 8 of the
14 individual fluoroquinolones showed an association
with tendon injuries, while all statins were associated
with rhabdomyolysis. This difference illustrates the dis-
tinction between a class effect (statins), i.e., inherited by
all members, and the property of a subset of the class
members.
Computational approach
The computational approached proposed here auto-
mates the interactive strategy for exploring the class
signal introduced with the visual approach. The patterns
detected on the heat map (Figure 1) correspond to cases
where all drugs from the class have roughly similar PRRs
(solid bar), or where groups of drug with different PRR
levels can be found (incomplete bar or isolated spot).
Translated into clusters for automated processing, the
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 8 of 10solid bar corresponds to a single cluster or two clusters
with similar PRR levels (no significant difference be-
tween the clusters), while the incomplete bar corre-
sponds to two distinct clusters with significant difference
between their average PRR levels. For example, for the pair
(statins, rhabdomyolysis), we found two clusters with no
significant difference. In contrast, the pair (fluoroquino-
lones, tendon injuries) was excluded from automatic pro-
cessing, because association with tendon injuries had been
reported for only four drugs (ciprofloxacin, fleroxacin,
pefloxacin and ofloxacin), while no information was
available for the other ten fluoroquinolones in this class
(e.g., trovafloxacin). In this case, expertise is required to
distinguish between less toxic drugs and drugs recently
marketed for which no ADEs have been reported as of
yet. For this reason, a proper determination of class
effect could be suggested for only 488 pairs based on
the dataset we exploited.
Applications
The findings gained from our exploratory techniques
should be of interest to the curators of ADE repositories
and drug safety professionals. One drug safety issue has
to do with the information found in drug labels, where
ADEs can be labeled in reference to a specific drug or to
an entire class of drugs. For example, the drug label for
citalopram includes a warning for QT prolongation (not
found in other SSRIs, such as sertraline). In contrast, the
label for minocycline refers to an ADE for its class:
THE USE OF DRUGS OF THE TETRACYCLINE
CLASS DURING TOOTH DEVELOPMENT [] MAY
CAUSE PERMANENT DISCOLORATION OF THE
TEETH. To make this determination, drug safety offi-
cers must be able to access not only safety information
for a given drug, but also safety information for the
other members of its class. The approaches we propose
here support effective review of safety information in the
context of drug classes.
To assess the relevance of our determination of a
potential class effect with respect to information found
in the FDA-approved structured package labels available
as part of DailyMed [27], one of the authors (AS) with
a drug safety background reviewed the top-20 pairs
selected by our computational approach. These pairs are
20 of the 488 pairs with the highest class-level PRR
(>40), for which 2 clusters had been identified, but no
significant difference between the clusters had been
found. These pairs included well-known class effects
mentioned in drug labels, including (tetracycline and de-
rivatives, tooth discoloration), (statins, rhabdomyolysis)
and (selective serotonin reuptake inhibitors, sexual dys-
functions, psychological) and (selective serotonin reuptake
inhibitors, serotonin syndrome). In five cases, the ADE is
mentioned for all the drugs in the class, but the druglabel does not make explicit reference to the class in the
warning. In six other cases, it was not possible to verify
the information because there was no label available for
some of the drugs in the class (e.g., drugs not marketed
in the U.S.). Finally, the remaining cases included false
positives, where an ADE known to be associated with a
given systemic drug was wrongly associated with topical
forms of the drug (because our underlying dataset does
not contain information about routes of administration).
Overall, these results suggest that, while potentially
helpful to drug safety officers for exploring ADEs for
drugs in the context of their classes, our approaches to
identifying class effect should only be used to support
determinations made by domain experts.Limitations and future work
A vast majority of the drug class-ADE pairs explored by
our computational approach ended up not being amen-
able to class effect determination, because no ADE infor-
mation was retrieved for at least one of the drugs in the
class. Our class definitions are based on ATC and in-
cluded drugs not marketed in the U.S., which made it
difficult to compare this information with warnings con-
tained in the drug labels from DailyMed. Restricting the
definition of drug classes to U.S. marketed drugs would
have led to a more meaningful comparison with Dai-
lyMed information. Moreover, having additional infor-
mation about the drugs would allow us to distinguish
between older drugs for which no ADEs have been men-
tioned (i.e., evidence of absence for the ADE) and drugs
more recently marketed for which there has not been
enough time for collecting safety information through
case reports (i.e., absence of evidence for the ADE).
Also missing from our current approach is an assess-
ment of the strength of evidence for the drug-ADE sig-
nal based on study design. For example, randomized
clinical trials could be given preference over non-
comparative observational studies and case reports [28] .
However, because our dataset is extracted from the bio-
medical literature, we could easily provide supporting in-
formation, such the number of articles in which the
ADE is reported for the drugs, as well as the publication
type (e.g., case report vs. clinical trial).
We are aware that our dataset of drug-ADE pairs
extracted from the biomedical literature is biased (e.g.,
towards case reports). However, our approach is agnostic
to the source used to derive the signal. In future work, we
are planning to apply it to the data from the FDA Adverse
Event Reporting System (FAERS). We could also leverage
natural language processing (NLP) techniques to extract
ADE pairs from text. Advanced NLP techniques would be
able to extract the polarity of ADEs (i.e., negated ADEs),
helping to assess evidence of absence of ADEs.
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 9 of 10The signal detection method used in this investigation
is extremely simple and may not be as robust as dispro-
portionality score algorithms developed more recently.
For example, limitations inherent in the use of PRR
include inability to account for temporal trends and
confounding by age, sex, or concomitant drugs [29].
Here again, our approach is agnostic to the methods
used for signal detection and could easily be adapted to
more sophisticated scores.
Finally, while aggregation plays a central role in our ap-
proach, ATC and MeSH are not the only terminologies
that can support aggregation. For example, the Established
Pharmacologic Classes distributed by FDA together with
the Structured Product Labels may offer an alternative
drug classification system. Our method for aggregating
ADEs in MeSH was limited to one level across all subdo-
mains and would benefit from refinement. Also, termin-
ologies such as MedDRA offer not only an alternative, but
groupings of ADEs across hierarchical structures.
Conclusions
We presented two complementary approaches to exploring
the contribution of individual drugs to the class signal for
ADEs. The visual approach supports the interactive
exploration of the class signal at increasing levels of
resolution. We showed that specific visual patterns in
heat maps are associated with class effects. Additionally,
we presented a computational approach, complemen-
tary to the visual approach, meant to assess the class
effect over a wide range of drug classes and ADEs sys-
tematically and automatically. In both cases, we were
able to find support for multiple known class effects.
Some of our findings were difficult to corroborate
against drug labels of DailyMed for a variety of reasons.
Our approach can be applied to other drug-ADE data-
sets, using various drug classification systems and signal
detection algorithms. The findings gained from our ex-
ploratory techniques should be of interest to the cura-
tors of ADE repositories and drug safety professionals.
Additional file
Additional file 1: High-resolution heat maps of drugs and ADEs at
different levels of granularity.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RW and OB designed the visual and computational frameworks. RW
implemented them. AS provided domain expertise and evaluated the results.
RW and OB wrote the manuscript. All the authors have read and approved
the final manuscript.
Acknowledgements
A preliminary version of this work was presented to the Vaccine and Drug
Ontology Studies (VDOS-2014) workshop. This work was supported by theIntramural Research Program of the NIH, National Library of Medicine (NLM).
This work also received support from the US Food and Drug Administration
(FDA) through the Center for Drug Evaluation and Research (CDER) Critical
Path Program [interagency agreement with NLM (XLM12011 001)] and from
the Office of Translational Sciences at CDER. While conducting this research,
RW was supported by an appointment to the NLM Research Participation
Program administered by the Oak Ridge Institute for Science and Education
through an interagency agreement between the U.S. Department of Energy
and the National Library of Medicine. The authors want to thank Ana
Szarfman, Rave Harpaz and Anna Ripple for useful discussions.Disclaimer
The findings and conclusions expressed in this report are those of the
authors and do not necessarily represent the views of the US Food and
Drug Administration or the US Government.
Author details
1Center for Biomedical Informatics Research, Stanford University, Stanford,
CA, USA. 2Center for Drug Evaluation and Research, US Food and Drug
Administration, Silver Spring, MD, USA. 3Lister Hill National Center for
Biomedical Communications, National Library of Medicine, National Institutes
of Health, Bethesda, MD, USA.
Received: 6 December 2014 Accepted: 30 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 
DOI 10.1186/s13326-015-0029-x
RESEARCH Open Access
A framework for ontology-based question
answering with application to parasite
immunology
Amir H. Asiaee1, Todd Minning2, Prashant Doshi1* and Rick L. Tarleton2
Abstract
Background: Large quantities of biomedical data are being produced at a rapid pace for a variety of organisms. With
ontologies proliferating, data is increasingly being stored using the RDF data model and queried using RDF based
querying languages. While existing systems facilitate the querying in various ways, the scientist must map the
question in his or her mind to the interface used by the systems. The field of natural language processing has long
investigated the challenges of designing natural language based retrieval systems. Recent efforts seek to bring the
ability to pose natural language questions to RDF data querying systems while leveraging the associated ontologies.
These analyze the input question and extract triples (subject, relationship, object), if possible, mapping them to RDF
triples in the data. However, in the biomedical context, relationships between entities are not always explicit in the
question and these are often complex involving many intermediate concepts.
Results: We present a new framework, OntoNLQA, for querying RDF data annotated using ontologies which allows
posing questions in natural language. OntoNLQA offers five steps in order to answer natural language questions. In
comparison to previous systems, OntoNLQA differs in how some of themethods are realized. In particular, it introduces
a novel approach for discovering the sophisticated semantic associations that may exist between the key terms of a
natural language question, in order to build an intuitive query and retrieve precise answers. We apply this framework
to the context of parasite immunology data, leading to a system called AskCuebee that allows parasitologists to pose
genomic, proteomic and pathway questions in natural language related to the parasite, Trypanosoma cruzi. We
separately evaluate the accuracy of each component of OntoNLQA as implemented in AskCuebee and the accuracy of
the whole system. AskCuebee answers 68% of the questions in a corpus of 125 questions, and 60% of the questions
in a new previously unseen corpus. If we allow simple corrections by the scientists, this proportion increases to 92%.
Conclusions: We introduce a novel framework for question answering and apply it to parasite immunology data.
Evaluations of translating the questions to RDF triple queries by combining machine learning, lexical similarity
matching with ontology classes, properties and instances for specificity, and discovering associations between them
demonstrate that the approach performs well and improves on previous systems. Subsequently, OntoNLQA offers a
viable framework for building question answering systems in other biomedical domains.
Keywords: Chagas, Natural language, Ontology, Parasite data, Question answering
*Correspondence: pdoshi@cs.uga.edu
1THINC Lab, Department of Computer Science, University of Georgia, Athens,
GA, USA
Full list of author information is available at the end of the article
© 2015 Asiaee et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License
(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,
provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 2 of 25
Background
New biomedical data is increasingly housed in resource
description framework (RDF) triple stores such as
Virtuoso [1] and AllegroGraph [2], annotated using rich
ontology schemas and queried using an RDF query lan-
guage called SPARQL [3]. The RDF data model has the
advantage of making the relationships between the data
items explicit, and provides a straightforward way for
annotating data using ontologies. An example of this
is the semantic problem solving environment for the
immunology of the parasite,Trypanasoma cruzi (T. cruzi),
which utilizes an RDF triple store for hosting the par-
asites genomic (microarray), proteomic (transcriptome)
and pathway data [4]. The data is annotated using the
parasite experiment ontology (PEO) and queried using
the open-source Cuebee [5] that provides an interface
for facilitating the parasitologists formulation of SPARQL
queries. Another example is the translational medicine
ontology and knowledge base [6], which utilizes the uni-
fying ontology to annotate integrated genomic, proteomic
and disease data, along with patient electronic records.
The data may be browsed in a RDF triple store.
Simple Web-based forms that allow choosing attributes
have been the user interface of choice for traditional
biomedical relational databases [7]. To promote ease of
querying, systems that utilize ontology-based RDF data
have experimented with various interfaces: iSparql [8],
NITELIGHT [9] and BioSPARQL [10] facilitate for-
mulating SPARQL queries by allowing the biomedical
scientists to browse ontology concepts and pinpoint a sub-
graph that pertains to the question in his or her mind.
GINSENG [11], a guided-input natural language search
engine, and Cuebee [5] progressively guide the scientists
by suggesting concepts and relationships that decompose
the question into a RDF triple based query, which is then
internally translated into SPARQL. The triples are in the
form of subject ? relationship ? object where sub-
ject and object represent ontology concepts. As Asiaee
et al. [12] notes, such guidance is tightly coupled to
the particular ontology structure, and query formulation
requires comfort with the structure otherwise the final
query is unintuitive to the user.
In this article, we introduce a novel framework, OntoN-
LQA, for querying RDF data annotated using ontologies.
The specific contributions of this framework are:
1. It allows posing queries as natural language questions
thereby requiring minimal translation between the
question in users mind and the computer query.
2. We present a new approach for answering natural
language questions on structured data that combines
machine learning with semantic computing: use of
existing ontologies, their structure and annotated
data, and triple-based queries.
3. OntoNLQA is applied in the context of parasite
immunology. The resulting system called AskCuebee
allows parasitologists to pose genomic, proteomic
and pathway questions in natural language related to
the parasite, T. cruzi, for the first time.
4. AskCuebee automatically answers 68% of a corpus of
125 questions in 5-fold cross-validation, and 60% of
the questions in a previously unseen corpus. This
latter proportion increases to 82.5 % if we allow
simple corrections by the user to the output of some
of the components.
OntoNLQA is significant due to two reasons: First, it
improves on the disadvantages of existing biomedical
data retrieval systems. In a systematic evaluation, Asiaee
et al. [12] demonstrate the benefits and limitations of
existing ontology-driven query formulation systems. A
major limitation is that scientists using these systems
require an understanding of the ontology structure in
order to quickly formulate queries. For example, queries
may require using intermediate concepts in the ontology
when there is no direct relationship between the concepts
that scientists have in mind.
To illustrate, consider this question in the context of T.
cruzi immunology using the parasite experiment ontology
(PEO) [13]:Which researchers study Neomycin drug resis-
tance? PEO formalizes the experimentation processes
in parasite research. Figure 1 illustrates the connection
between the two concepts researcher and Neomycin drug
resistance in PEO. Notice that study corresponds to a
path that includes two ontology properties, has agent and
has output value, and an intermediate ontology class,
sequence extraction, which is not stated in the question.
Because questions may not explicitly state how the tar-
get concepts are related, the scientists RDF query is tied
down to the structure of the ontology and this prob-
lem exaggerates when multiple intermediate concepts are
needed.
Second, and the more important motivation derives
from the fact that a capability to pose questions in plain
language is a natural way of obtaining answers. It involves
minimal effort expended toward translating the question
in the scientists mind to a query acceptable to the system,
which includes the effort involved in acquainting with the
query interface. In our informal discussions with biomed-
ical scientists, this capability was consistently identified as
the one that is most preferred.
OntoNLQA seeks to automatically translate a question
into RDF triples, and build a SPARQL query to retrieve the
answers from data stored in the RDF data model. Toward
this, the framework utilizes a design comprised of five
components working in a sequence: The first two iden-
tify important entities in the scientists question. These
are constituent words that are nouns and verbs, and
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 3 of 25
Fig. 1 In order to formulate a query for the question above, the scientist needs to relate the two concepts, researcher and Neomycin drug resistance,
using the intermediate concept in the ontology, sequence extraction, that connects the two. Realizing how these are related requires an
understanding of the ontology design and its structure
relate to the concepts and relationships in the domain.
Accuracy is important here because words erroneously
deemed important get carried forward through all the
components. The third component matches the entities
identified previously to classes and properties of the ontol-
ogy. The last two components receive a set of ontology
classes and properties, and find semantic associations
between the entities. These associations could be multi-
ple paths comprised of classes and properties represented
as sequences of RDF triples, which are translated into
SPARQL to query the RDF data.
OntoNLQA is not specific to a domain with multiple
strategies and methods possible to realize each compo-
nent. We instantiate this framework in the context of par-
asite immunology, and develop a system called AskCuebee
that allows parasitologists to pose genomic, proteomic
and pathway questions in natural language related to the
parasite, T. cruzi. A significant amount of data including
internal laboratory data sets, KEGG pathway data, and
genomic data on orthologs such as Leishmania major and
Trypanosoma brucei from TriTrypDB [7] is available in a
RDF store for querying. The data is annotated using PEO.
We evaluate the accuracy of each component of
OntoNLQA as implemented in AskCuebee, and the accu-
racy of the whole system.
AskCuebee has been deployed in the Tarleton lab for
use in their day-to-day research and replaces a previous
traditional relational database system1.
While the field of natural language processing has
long investigated the challenges of designing systems that
respond to questions in natural language [1418], these
do not make use of ontologies or the RDF data model.
Few recent ontology-based retrieval systems [19, 20] allow
queries as natural-language questions and seek to extract
subject ? predicate ? object triples directly from the
input question using pattern matching. However, a sig-
nificant limitation is that the extracted triples may not
be present as is in the ontology because the scientists
question may not be cognizant of the ontologys struc-
ture. Furthermore, as we illustrated previously, entities
in the question may not be directly related motivating
sophisticated ways of connecting them to form an intu-
itive query. Consequently, a large subset of the questions
are challenging to answer automatically, thereby necessi-
tating user involvement to refine the triples. For example,
Aqualog [19] could not answer 42% of the questions in
its corpus automatically resorting to manual intervention
for these questions. A small subset of the systems [11, 21]
refine the question in real-time  as it is being typed  by
suggesting concepts and relationships from the ontology
to the scientist. These occupy a middle ground between
those that truly allow questions in natural language and
those in which queries are RDF triples.
Our driving biomedical domain pertains to the
immunology and pathogenesis of the parasite T. cruzi
infection, which causes the Chagas disease. This dis-
ease was recently labeled the new HIV/AIDS of the
Americas [22]. About 7 million people, mostly in Latin
America, are infected with this disease. Data available
for querying by AskCuebee was collected in order to
study how immune control and maintenance occurred
during a T. cruzi infection and how it managed to avoid
immune clearance. Data on DNA cloning steps for gene
knockout studies, on generation of gene knockout strains,
whole-genome transcript abundances for the four life-
cycle stages of T. cruzi, orthologous genes in related
organisms and protein identifications based on peptide
spectra are all included as RDF data.
Article outline
Next, in the Methods section, we describe the design of
OntoNLQA, discuss the details of each component and
how each component is utilized in AskCuebee in the con-
text of the parasite, T. cruzi, immunology research. We
report on the results of evaluating the methods employed
in AskCuebee as well as demonstrate the performance of
each component and the performance of the whole sys-
tem in the Results and discussion section. We also discuss
the contributions and limitations of our framework based
on our evaluation results in this section. We present a
comprehensive review of related work with a focus on
ontology-based retrieval systems in Related Works. We
conclude the article by summarizing our approach, main
findings and providing thoughts on future directions in
the Conclusion section.
Methods
OntoNLQA presents a new approach for answering ques-
tions posed in natural language to RDF data annotated
using an ontology. We begin by providing an overview
of the framework followed by details on each compo-
nent and how it is applied in the context of a driving
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 4 of 25
biological domain, as well as its evaluation. As we discuss
below, multiple alternatives present themselves for real-
izing each component of the framework, and we discuss
their benefits and limitations.
Overview of OntoNLQA
Briefly, our approach in OntoNLQA is to identify the
important entities present in the question, which are then
found in the ontology and semantic associations between
the entities in the ontology are discovered. This approach
encounters three main challenges:
1. OntoNLQA needs to parse the question and identify
the important entities;
2. It must find the ontology classes, properties and
instances (data) in the ontology(ies) that correspond
to the identified entities; and
3. Find semantic associations involving the ontology
classes and properties, which need not be on a single
directed path. Express these in the form of RDF
triples that are translated into a computational query
for the RDF data.
While these challenges are common to some of
the previous ontology-based natural language systems,
OntoNLQA differs in its approach toward addressing them.
These challenges suggest a pipeline of operations on the
data beginning with the question in natural language, as
illustrated in Fig. 2. On receiving a question in natural
language, OntoNLQA performs linguistic pre-processing of
the question during which punctuation symbols, quota-
tion marks, parenthesis and any other character in the
question generally deemed to be irrelevant to extracting
the important information, are filtered out. This results
in a processed question. Words and phrases relevant to
the domain and of import to understanding the question
are deemed as important entities and extracted from the
processed question by utilizing entity recognition tech-
niques. These entities are then found in the ontology using
lexical matching. Ontology classes matched to the enti-
ties form the end points of any semantic associations that
are additionally constrained to include matched ontology
properties, if any. These associations are represented as a
sequence of RDF triples, which are then transformed into
SPARQL queries that retrieve the answer.
Operations on the data in Fig. 2 are performed by
the components of the system. Subsequently, OntoNLQA
is composed of five components as we show in Fig. 3.
The first two components, which include linguistic pre-
processing (box annotated 1 in Fig. 3) and entity recogni-
tion (box annotated 2) address the first challenge, which
is similar to the well-known problem of named entity
recognition [23]. Our primary goal in extracting entities
is to match them with their corresponding ontology ele-
ments. Therefore, the labels in our context is a set of
ontology classes and properties. A third component (box
annotated 3) matches each extracted entity from the pre-
vious component to a specific ontology class, property or
instance.
This component addresses the second challenge of find-
ing corresponding ontology elements for the identified
entities.
The final two components handle the challenge of find-
ing relationships between the ontology elements, repre-
senting them as RDF triples, and translating these into
a computational query. Semantic association discovery is
nontrivial when more than two ontology elements need to
Fig. 2 An illustration of the flow of data in OntoNLQA emphasizing the operation performed on the data at each step. Dotted lines show the
operation on the data. For example, lexical matching gives the ontology classes, properties and instances similar to the extracted entities. The direction
of the arrows denotes the direction of flow of the data
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 5 of 25
Fig. 3 The design of OntoNLQA involving five general components that operate on the scientists question to eventually obtain the answer
be related (box annotated 4). Discovered semantic associ-
ations may be represented as RDF triples. These are used
in generating a computational query for the RDF data by
the query formulation and answer retrieval component
(box annotated 5).
Components of OntoNLQA and their Implementation in
AskCuebee
In this subsection, we describe the components of the
framework in detail. For each, we discuss various meth-
ods for realizing the components functionality, whichmay
be beneficial in different contexts, and its utilization in
AskCuebee.
We apply OntoNLQA to the context of T. cruzi par-
asite immunology data as illustrated in Fig. 4. We call
this application, AskCuebee (boxes annotated 2 and 3),
which is assisting parasitologists at the Center for Trop-
ical and Emerging Diseases at the University of Georgia,
and their collaborators worldwide. The parasite, T. cruzi,
is the agent of Chagas disease in humans. This disease is
prevalent throughout Latin America and is often fatal.
Linguistic pre-processing
All questions undergo linguistic pre-processing to filter
constituents that are not key toward a computational
understanding of the question. This pre-processing 
commonly utilized inmany question-answering systems 
is generally known to improve the accuracy of detecting
important entities. The pre-processing starts with tok-
enization: breaking down the string of characters into
words, phrases, symbols, or other meaningful elements.
This is followed by removing stop words such as the
Fig. 4 OntoNLQA is a general framework and its realization in the context of our driving biological problem is called AskCuebee
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 6 of 25
definite articles, to, was, and many others. Standard
lists of stop words are available [24]. In addition, punctu-
ation symbols are removed, abbreviations are expanded,
and comparative relationships in words are identfied using
grammar dependencies [25] and parts of speech tag-
ging [26], and replaced by their mathematical symbols; for
example, greater than 1 is replaced by > 1.
The accuracy of linguistic pre-processing may be
enhanced by using domain-specific lexicons or dictio-
naries such as UMLS or MeSH, if these are relevant,
though its use should be considered carefully due to
the concomitant increase in run time [27]. Much of the
previously mentioned functionality for pre-processing is
available in free computer applications such as Stanford
CoreNLP [28], LingPipe [29] and OpenNLP [30].
StanfordCoreNLP in AskCuebee Each question posed
by the user is viewed by the system as a string of charac-
ters. Therefore, common operations such as tokenization,
extracting the roots of words (stemming), and removing
the punctuation symbols are essential. AskCuebee per-
forms these using the standard operations found in the
Stanford CoreNLP library [28]. Furthermore, consider the
following two example questions:
1. Show me the 3 prime forward sequences for all genes
in metacyclic stage with log2 ratio higher than 1 and
standard deviation below 0.5.
2. Which protein group numbers have spectral values
between 40 and 50?
In question (1), notice that while there are three num-
bers mentioned, two of these are involved in comparative
relationships, 1 and 0.5. Thus, the comparative relation-
ships we seek to identify are log2 ratio > 1 and standard
deviation < 0.5. In question (2), the comparative relation-
ships are more complex as two relationships are combined
into one using a conjunction. Therefore, we seek to extract
two relationships, spectral values > 40 and spectral values
< 50. These questions illustrate that we additionally need
conversions between numbers and text, and extraction
of comparative relationships. Both these require com-
plex operations that include part-of-speech tagging such
as detecting the nouns, verbs and identifying grammar
dependencies, which are provided by Stanford CoreNLP.
In addition, we detect abbreviations from a list that we
maintain.
We introduce a simple method that uses dependency
grammar to detect the majority of the comparative rela-
tionships. The first step is to detect the comparative
phrases in the question and transform them into dis-
tinct patterns. For example, standard deviation below 0.5
from question (1) is transformed to standard deviation
less than 0.5 and spectral values between 40 and 50 from
question (2) is converted to spectral values greater than
40 and spectral values less than 50. Next step converts the
distinct patterns into a computational form by identifying
the operands (standard deviation and 0.5) and operators
(less than). Again, a dependency grammar is combined
with part-of-speech tagging to create rules for detecting
operands and operators.
Entity recognition
Given the processed question, this component in the
framework is tasked with identifying and labeling enti-
ties that are relevant to obtaining the answer. Several
approaches may be used toward entity recognition.
These include supervised learning  a branch of
machine learning  that utilizes statistical models for the
task. A classifier is trained using a large corpus of data
records, each of which is labeled with the target entity
names. Entities in new data records are then identified
and labeled by the classifiers. Potential classifiers include
the hidden Markov model [3134], maximum-entropy
Markov model [35, 36], support vector machines [37], and
conditional random fields [38], all of which have been
utilized for entity recognition. Among these, conditional
random fields have distinguished themselves with their
comparatively more accurate performance [3941].
Supervised learning usually requires a large training
corpus to learn a classifier that performs well. In the
absence of large data sets, the alternative method of semi-
supervised learning uses a small collection of data to train
an initial classifier, which is then used to label new and
previously unseen samples of data. These labeled data are
subsequently utilized to retrain the classifier. A common
technique for semi-supervised learning is bootstrapping,
which requires a small set of seed supervised data for the
initial learning [42].
Other approaches not based on machine learning rely
on dictionaries and rules. A simple approach is to locate
lexically similar dictionary terms for each potential entity
in the question [4346]. The approaches differ in how they
search the dictionary with some using BLAST [47], and
the data sets that constitute the dictionary. For example,
Krauthammer et al. [43] utilizes GeneBank as the dic-
tionary. Alternately, general rules in the form of string
patterns may be available. If a rule is satisfied by a term
and its context in the question, the corresponding label is
used to annotate the term [4851].
Between the different approaches for entity recogni-
tion, machine learning methods are currently receiving
increased attention in general [52, 53]. Regardless of semi-
or fully-supervised methods, we need a set of labels for
entity recognition. Presence of a domain ontology pro-
vides a natural source for these labels. In this regard, an
important consideration is the number of labels that are
needed, which is often proportional to the size of the data
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 7 of 25
set. A large data set may permit better discrimination and
therefore more labels. On the other hand, a smaller data
set necessitates fewer labels. In this case, we may select
ontology classes and properties that appear at a higher
hierarchical level in the ontology graph. Let CO denote
this set from ontology, O. Such labels tend to be general,
and each is useful toward annotating several terms in the
question.
Conditional random fields for entity recognition in
AskCuebee Dictionary-based methods require domain-
specific dictionaries. While substantial overarching dic-
tionaries for biomedicine such as UMLS and MeSH are
indeed available for use, these are not designed to be spe-
cific to any particular organism. Biomedical ontologies,
if available, serve to provide another source of dictionary
terms usually specific to a domain. In addition to finding
a dictionary relevant to the domain of interest, a limita-
tion of this approach is that dictionary look up could get
expensive if the dictionary is very large and unindexed. On
the other hand, machine-learning based supervised clas-
sification may need large training data in order to achieve
reasonable performance.
Among supervised learning methods, conditional ran-
dom fields (CRF) [38] demonstrate superior performance
for biomedical entity recognition. For example, CRFs were
utilized by the best performing system on the i2b2 medi-
cal concept extraction task [41], by highly ranked systems
on BioCreAtIve gene mention recognition tasks [39, 40]
(9 of 19 highest ranked systems use CRFs) and on JNLPBA
bioentity recognition task [54]. This motivates consider-
ing CRFs in AskCuebee as well. We briefly review CRFs in
Appendix A.
AskCuebee employs a linear-chain CRF and a pop-
ular quasi-Newton method called limited memory
Broyden-Fletcher-Goldfarb-Shanno [55] for optimizing
parameters. The parameters are the feature weights, ?j, in
a CRF.
Critical to the performance of CRFs is finding a set
of feature functions. The simplest features of a natural
language question are the word tokens themselves. In
addition, AskCuebee uses four different types of features
for training CRFs: orthographic, word shape, dictionary
and contextual features:
 Orthographic features: Biomedical entities often
share orthographic characteristics. These consist of
capitalized letters, include digits and possibly some
special characters as well. Thus, such features are not
only useful in detecting various types of biomedical
entities but are easily implemented using patterns or
regular expressions. Appendix A includes a list of the
orthographic features utilized in AskCuebee.
 Word shape: Words annotated with the same entity
label may have the same shape. For example, a type of
abbreviation may not have numerical digits and gene
IDs are a combination of digits and letters.
 Contextual features: These features take into account
the properties of preceding and following tokens for a
current token in order to determine the target label.
 Dictionary features: For each noun or verb phrase in
the input question we calculate their similarity scores
with all ontology elements. If the highest similarity
score is higher than a threshold (for instance, 0.6), we
find the upper-level class or property of that specific
ontology element that is a training label. Then, we
activate a dictionary feature for the identified training
label. This feature is useful when the target entities
belong to more than one label.
Ontology elementmatching
Entity labels are ontology classes and properties, which
could be general and appear at the higher levels of the
ontology hierarchy. However, the RDF data annotated by
the ontology is often linked to more specific classes and
properties. Consequently, we may search the ontology for
more specific matches with the recognized entities in the
questions. If an entity, e, is associated with a label, c ? CO,
where CO is the set of all classes and properties in ontol-
ogy, O, then, let Sc be the set of subclasses and properties
in the ontology hierarchy rooted at c. Labeling the entity
with c allows us to limit our search for a more specific
match to the elements of Sc. Importantly, this reduces the
computational expense when the whole ontology may be
very large as is often the case with biomedical ontologies.
A suitable approach for the matching is to use text
similarity measures to calculate the degree of similarity
between an entity and a specific ontology class or prop-
erty. A similarity measure scores the degree of similarity
between two text phrases by viewing them as sequences of
characters. Common measures that are utilized include:
 ISUB similarity [56] designed for aligning
ontologies [57]. This method identifies the longest
common substring and records its length. It removes
this substring and searches for the next longest
substring until no common substring is identified.
The sum of the common substrings scaled with the
length of the original strings is the commonality in
the two strings. ISUB subtracts this commonality
from the difference of the two strings. The difference
is based on the length of the unmatched substrings.
 Levenshtein-based similarity (also known as
Needleman &Wunsch) [58] uses the Levenshtein
distance [59] to determine the similarity of two
sequences of characters. It calculates the best
alignment between two sequences of characters as
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 8 of 25
the fewest number of mutations necessary to build
one sequence from the other.
 Smith and Waterman based similarity [60] looks for
longest common substrings between two phrases,
and based on that produces the degree of similarity.
This measure is similar to Needleman-Wunsch, and
is commonly used in BLAST for aligning genome and
protein sequences.
 Cosine-based similarity [61] is a widely reported
measure for similarity between two vectors. This
measure models phrases as vectors of characters and
calculates the cosine between the two vectors. This
provides a score that is interpreted as the degree of
similarity between two chunks of texts.
 Jaccard-based similarity [62] calculates the degree of
similarity of two phrases by calculating the size of the
set of intersection of the terms in the two phrases
compared to the size of the set of union of the terms.
No particular measure in the above list dominates any
other measure in performance. Subsequently, we may
evaluate all of them for use in domain-specific systems
such as AskCuebee. Classes and properties in Sc that
match sufficiently well with the entity, e, become a part of
the candidate list. Based on the cardinality of the candi-
date list, three situations arise as discussed below:
Case (1): In the straightforward case where the candi-
date list has only one member, the matched subclass or
property is retained.
Case (2): If the candidate list has multiple members, we
need to retain one among them. Here, we may consider
the context: the other entities identified in the question
and how each candidate relates with the ontology classes
and properties that label the other entities. For example,
we may rank order the candidates based on how many
direct paths each has with the other labels found in the
ontology. We may retain the candidate with the most
paths, which is indicative of contextual consistency.
Case (3): Finally, the candidate list could be empty.
Because none of the ontology subclasses or properties
were a close lexical match, our next step is to identify a
match in the RDF data. We may lookup the rdfs:type of
the matched instances in the data set to obtain the corre-
sponding ontology classes or properties. If multiple such
classes obtain, the candidate list has multiple members
requiring the approach in case (2) above to retain one.
Semantic association discovery
Specific ontology classes and properties that label the
identified entities in the question now need to be related
to each other. Two ontology elements have a seman-
tic binary relationship if a directed or undirected path
connects them in the ontology graph. However, scien-
tists questions often include multiple entities. OntoNLQA
differs from previous systems in how it handles this sit-
uation. We must find an n-ary semantic relationship
between all of them. While pairwise binary relationships
may be found between each pair of labels, these paths
must be linked with each other.
An approach to relating them is to find the lowest com-
mon ancestor (LCA). This is the ontology class that is
the ancestor of each entity label. An ancestor is any class
that lies on the path from the root of the ontology to
the label class. If there are multiple such common ances-
tors, we pick the one that is most specific and is therefore
lowest in the hierarchy. This ancestor would coincide
with Resniks most informative common ancestor [63] if
attention is limited to just the subclass taxonomy of the
ontology. However, the latter requires finding the proba-
bilities of ontology classes typically using term frequencies
in domain texts. Furthermore, the LCA may be different
when named properties in an ontology are considered.
An an illustration, consider Fig. 5 which shows the
semantic relationship between labels Cell Cloning and
Gene that appear in PEO. The binary relationship between
these two labels is a direct path in PEO. In this path, there
are several intermediate ontology concepts such as drug
selection and transfection (marked differently), which are
a part of the relationship. Of course, the length of such
paths depends on the design and structure of the partic-
ular ontology. As there is one pair only in this example, a
single path is sufficient to obtain the semantic association
between the two labels.
The graphs in Figs. 6 and 7 consider examples from
questions containing more than two identified entities,
resulting in more complex semantic associations between
the ontology elements. In Fig. 6, we are looking for seman-
tic relationships between five prime forward region, spec-
tral count and proteome analysis concepts, which were
chosen as entity labels. This n-ary relationship may not
be a single path between the elements. However, there
are pairwise paths between each pair of the ontology ele-
ments. Notice that proteome analysis is present on all
these paths and is the LCA.
As a third example, consider the ontology classes, gene,
spectral count, pathway, and experiment justification in
Fig. 7. It is straightforward to find the pairwise paths
between the classes in the ontology subgraph. However,
unlike the previous example, none of these include a com-
mon ancestor. As we show in Fig. 7, the LCA, process, is
an intermediate concept and does not belong to the set of
labels for the entities in the question.
In order to find semantic associations between multiple
ontology classes or properties, we discuss two methods
below:
Method 1: Semantic association discovery based on
the LCA Notice that the presence of an LCA for the
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 9 of 25
Fig. 5 The semantic path between the ontology concepts Cell
Cloning and Gene ID Tc00.1047053509463.30. The lowest common
ancestor is Cell Cloning
matched classes or properties in an ontology provides a
way to obtain the semantic association between them.
From the LCA, we may obtain the shortest path that
connects the LCA to each ontology class while including
any identified property. Consequently, we obtain multiple
paths each of which has the LCA at one end.
This motivates finding an efficient way to compute the
LCA. In Appendix B, we discuss an offline approach that
precomputes the LCA for each pair of classes in the
ontology at hand and is currently the fastest. Wemay then
simply look up the LCA table to find all LCAs for every
ontology entity pair. This process continues recursively
until we identify a single LCA for all of the entity labels.
Figure 8 illustrates this recursive algorithm. Note that this
recursive procedure iterates over all LCAs for every pair
until one of them leads to the final solution. If the algo-
rithm fails to find any LCA for the entities, it concludes
that there is no semantic association between the ontology
classes. In order to find the shortest path from the LCA to
each ontology class in the set of entity labels, we may use
bidirectional search [64] to speed up the path finding.
Method 2: Semantic association discovery based on
path finding An alternative approach for finding seman-
tic associations is based on path queries. For example,
SPARQL 1.1 provides facilities to find a path between two
elements in RDF data. We may use these path-finding
queries to find the semantic paths betweenmultiple ontol-
ogy classes and properties. We present a simple method
that includes finding all the paths between the ontol-
ogy elements and selecting a common node among these
paths. Specifically,
 We begin by finding pairwise paths: these are paths
between every pair of ontology elements in the set of
labels. We sort them based on their length in
ascending order.
 Note that multiple paths may exist between a pair of
ontology elements. We create a set,
{allPairwisePaths}, that contains sets of all the
pairwise paths between every pair of the elements.
 In the next step, a Cartesian product of the sets in
allPairwisePaths is obtained. Each member of the
product set is itself a set of pairwise paths between all
the ontology elements.
 For each member of the product set, we identify an
ontology class that is common to all the paths, if
available, and store these common classes in a set,
CommonNodes.
 Finally, this approach selects a class in the set,
CommonNodes, that has the shortest paths to the
ontology elements.
Both the above methods result in semantic paths, which
are then converted into sequences of RDF triples in a
straightforward way.
Association discovery in AskCuebee In order to dis-
cover the associations between the matched PEO classes
and properties, OntoNLQA suggests either precomputing
the LCA or running path queries between each pair of
matched ontology elements and finding their intersection.
While the former has an offline step of precomputing the
Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 10 of 25
Fig. 6 This graph shows the semantic paths between the ontology concepts, five prime forward regions, proteome analysis, spectral value> 40 and
spectral value< 50. The common node between all is proteome analysis, which forms the LCA
LCA between all pairs of classes in the ontology, the latter
is fully online. We evaluate the two approaches and select
one for inclusion in AskCuebee.
Query formulation and answer retrieval
The final component of OntoNLQA translates RDF triples
into a computational query in the language of SPARQL.
This translation is straightforward because the RDF
triples directly represent SPARQL graph patterns.
If the RDF triple sequences constituting the semantic
paths need to be displayed, we may utilize any modal-
ity including simply showing the sequences or marking
them on the ontology graph and displaying the subgraph.
As an example, we may utilize the display of RDF triple
sequences by a system such as Cuebee [5].
The SPARQL query is then sent to any query endpoint
such as OpenLink Virtuoso [1], OpenRDF Sesame [65] or
AllegroGraph [2], all which allow storing large amounts
of annotated RDF data and query it using SPARQL. The
answers may be displayed to end users in a tabular or any
other visual format depending on the context and scientist
JOURNAL OF
BIOMEDICAL SEMANTICS
Groza et al. Journal of Biomedical Semantics  (2015) 6:21 
DOI 10.1186/s13326-015-0008-2
RESEARCH ARTICLE Open Access
Capturing domain knowledge frommultiple
sources: the rare bone disorders use case
Tudor Groza1*, Tania Tudorache2, Peter N Robinson3 and Andreas Zankl4
Abstract
Background: Lately, ontologies have become a fundamental building block in the process of formalising and storing
complex biomedical information. The community-driven ontology curation process, however, ignores the possibility
of multiple communities building, in parallel, conceptualisations of the same domain, and thus providing slightly
different perspectives on the same knowledge. The individual nature of this effort leads to the need of a mechanism
to enable us to create an overarching and comprehensive overview of the different perspectives on the domain
knowledge.
Results: We introduce an approach that enables the loose integration of knowledge emerging from diverse sources
under a single coherent interoperable resource. To accurately track the original knowledge statements, we record the
provenance at very granular levels. We exemplify the approach in the rare bone disorders domain by proposing the
Rare Bone Disorders Ontology (RBDO). Using RBDO, researchers are able to answer queries, such as: What phenotypes
describe a particular disorder and are common to all sources? or to understand similarities between disorders based on
divergent groupings (classifications) provided by the underlying sources.
Availability: RBDO is available at http://purl.org/skeletome/rbdo. In order to support lightweight query and
integration, the knowledge captured by RBDO has also been made available as a SPARQL Endpoint at http://bio-lark.
org/se_skeldys.html.
Multiple perspectives over the same domain
Ontologies represent a formalised description of the con-
cepts and relationships in a domain. For example, they can
be used to model and capture knowledge around a par-
ticular set of hereditary disorders (e.g., bone dysplasias),
in addition to their underlying genetic mechanisms (relat-
ing disorders to genes) and their observable traits (relating
disorders to phenotypes). Over the course of the past
decade, they have become one of the main mechanisms
used in building intelligent systems and algorithms to sup-
port, among others, the study of cross-species phenotype
networks [1-3], gene screening, prediction and prioritiza-
tion [1,4] or disorder prediction [5,6]. This rising adop-
tion, in particular by the biomedical community, has led
to the proliferation of the number of ontologies published
openly via repositories such as the NCBO BioPortal [7]
or the Open Biological and Biomedical Ontology (OBO)
*Correspondence: t.groza@garvan.org.au
1School of ITEE, The University of Queensland, St Lucia, Australia
Full list of author information is available at the end of the article
Foundry [8]. An effect of this uptake has been the need for
a community-driven process [9], which requires a shared
understanding of the rules and primitives that govern the
domain under scrutiny. More concretely, experts required
appropriate mechanisms to update and evolve ontologies
and the domain knowledge, in order to ensure an accurate
knowledge transfer. A relevant example of such collab-
orative knowledge curation is the development of the
International Classification of Disorders (ICD-11) [10], or
efforts like the Gene Ontology [11], the Human Pheno-
type Ontology [12] or the International Classification for
Nursing Practice (ICNP) [13].
Recognising the need for community-oriented knowl-
edge curation does not, however, take into account that
multiple communities may target, in parallel, the concep-
tualisation of the same domain. This, in turn, leads to
ontologies that provide slightly different perspectives on
the same knowledge. These perspectives differ in:
1. focus  the particular aspects of the domain  e.g.,
given a particular set of disorders, a community may
© 2015 Groza et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Groza et al. Journal of Biomedical Semantics  (2015) 6:21 Page 2 of 15
focus more on the genetic mechanisms, while
another one on the clinical presentation,
2. granularity  the level of detail considered in the
knowledge modelling process  e.g., given the same
context, the knowledge captured by a community
may include the prevalence of the disorders or the
age of onset, while in other cases this knowledge may
be omitted, or
3. in the underlying interests or priorities of those
creating them  for example, a community may just
be interested in the clinical presentation of the
disorders because of its focus on clinical diagnosis.
We are currently missing a key mechanism to allow
us to create a comprehensive overview of the different
perspectives on the domain knowledge.
Ontology integration is a field that has been extensively
studied in the past. Although several definitions of ontol-
ogy integration exist in the literature [14], it is usually
referred (as is in our case) to the process of combining two
or more ontologies about the same subject into a single
unified ontology. One step in the integration process is to
find the correspondences (a.k.a., mappings) between the
semantically-related entities, which can be done manu-
ally or (semi-automatically). The ontology-matching topic
that has seen an enormous amount of research, as shown
by several comprehensive and thorough surveys that are
available in the literature [15-18]. All these approaches
are focused on trying to identify in a (semi-)automatic
manner the correspondences between entities in differ-
ent ontologies. This is not the focus of our work, as the
mappings between the two resources were identified in a
manual fashion by a domain expert. Even more, existing
work on ontology integration does not keep track of the
provenance of the integrated entities. This novel aspect of
our work brings several benefits, which are discussed later
in the paper.
In the biomedical domain, many ontologies create
JOURNAL OF
BIOMEDICAL SEMANTICS
Suzuki et al. Journal of Biomedical Semantics  (2015) 6:30 
DOI 10.1186/s13326-015-0028-yRESEARCH Open AccessDevelopment of an Ontology for
Periodontitis
Asami Suzuki1,2, Takako Takai-Igarashi3,1*, Jun Nakaya3,4 and Hiroshi Tanaka3,1Abstract
Background: In the clinical dentists and periodontal researchers community, there is an obvious demand for a
systems model capable of linking the clinical presentation of periodontitis to underlying molecular knowledge. A
computer-readable representation of processes on disease development will give periodontal researchers opportunities
to elucidate pathways and mechanisms of periodontitis. An ontology for periodontitis can be a model for integration
of large variety of factors relating to a complex disease such as chronic inflammation in different organs accompanied
by bone remodeling and immune system disorders, which has recently been referred to as osteoimmunology.
Methods: Terms characteristic of descriptions related to the onset and progression of periodontitis were manually
extracted from 194 review articles and PubMed abstracts by experts in periodontology. We specified all the relations
between the extracted terms and constructed them into an ontology for periodontitis. We also investigated matching
between classes of our ontology and that of Gene Ontology Biological Process.
Results: We developed an ontology for periodontitis called Periodontitis-Ontology (PeriO). The pathological
progression of periodontitis is caused by complex, multi-factor interrelationships. PeriO consists of all the required
concepts to represent the pathological progression and clinical treatment of periodontitis. The pathological processes
were formalized with reference to Basic Formal Ontology and Relation Ontology, which accounts for participants in the
processes realized by biological objects such as molecules and cells. We investigated the peculiarity of biological
processes observed in pathological progression and medical treatments for the disease in comparison with Gene
Ontology Biological Process (GO-BP) annotations. The results indicated that peculiarities of Perio existed in 1)
granularity and context dependency of both the conceptualizations, and 2) causality intrinsic to the pathological
processes. PeriO defines more specific concepts than GO-BP, and thus can be added as descendants of GO-BP
leaf nodes. PeriO defines causal relationships between the process concepts, which are not shown in GO-BP. The
difference can be explained by the goal of conceptualization: PeriO focuses on mechanisms of the pathogenic
progress, while GO-BP focuses on cataloguing all of the biological processes observed in experiments. The goal
of conceptualization in PeriO may reflect the domain knowledge where a consequence in the causal relationships
is a primary interest. We believe the peculiarities can be shared among other diseases when comparing processes
in disease against GO-BP.
Conclusions: This is the first open biomedical ontology of periodontitis capable of providing a foundation for an
ontology-based model of aspects of molecular biology and pathological processes related to periodontitis, as
well as its relations with systemic diseases. PeriO is available at http://bio-omix.tmd.ac.jp/periodontitis/.
Keywords: ontology, periodontitis, biomedical process, bone remodeling, Gene Ontology, osteoimmunology* Correspondence: takai@megabank.tohoku.ac.jp
3Tohoku Medical Megabank Organization, Tohoku University, Sendai, Japan
1Department of Computational Biology, Graduate School of Biomedical
Science, Tokyo Medical and Dental University, Bunky?, Japan
Full list of author information is available at the end of the article
© 2015 Suzuki et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License
(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,
provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Suzuki et al. Journal of Biomedical Semantics  (2015) 6:30 Page 2 of 13Background
Biological high throughput analysis generates huge
amounts of biomedical data that can be used for investi-
gating disease mechanisms, and semantic technologies
are therefore expected to contribute to the effective use
of these data. Many biomedical ontologies such as the
Gene Ontology (GO), the Disease Ontology (DO), the
Ontology for General Medical Science (OGMS), the
Human Disease Ontology (DOID) and the Infectious
Disease Ontology (IDO) have been developed to provide
support for sophisticated biomedical information sys-
tems [1, 2]. These ontologies are providing a means for
the consistent representation of scientific data and the
domain entities made possible by these data [3].
Disease has been one of the major targets for ontol-
ogy development. The Unified Medical Language Sys-
tem (UMLS) [4] and the Medical Subject Headings
(MeSH) , a source vocabulary included in UMLS, [5]
are long established thesauri that explicate numerous
medical terms. For many decades, the World Health
Organization has provided International Classification
of Diseases (ICD) [6]. Terminologies and ontologies in
biology and medicine have also been reviewed by
Freitas et al. [7]. Meanwhile, large-scale genomic projects,
including the SNP consortium [8], the ENCODE pro-
ject [9], the NIH Knockout Mouse Project [10], the
Welcome Trust Case Control Consortium [11], and the
1000 Genome Project [12], have tried to catalogue
comprehensive relationships between genes and dis-
eases. These data collections require the development
of ontologies that integrate genes with clinical out-
comes [1320]. Masci et al. created comprehensive
definitions of dendritic cells in order to distinguish
many derivatives of dendritic cells according to the pro-
gression of immune responses [20]. Mungall et al. investi-
gated ontological mapping of mutation phenotypes/
diseases across species [14]. Rubin et al. integrated
existing ontologies for neuronal connectivity in order to
explicate abnormalities of neuronal diseases systematically
[16]. Feltrin et al. and Lindeberg et al. expanded GO in
order to explicate muscle biology and plant pathology by
adding specifications of pathological disorders and muta-
tional phenotypes, respectively [1719].
GO is an established ontology that consists of the fol-
lowing three sub-ontologies: Cellular Component (CC),
the parts of a cell or its extracellular environment; Mo-
lecular Function (MF), the elemental activities of a gene
product at the molecular level; and Biological Process
(BP), operations or sets of molecular events with a
defined beginning and end, pertinent to the functioning
of integrated living units such as cells, tissues, organs,
and organisms. While GO classifies internal processes
in any biological phenomenon with external links to
entries in the databases of genes by relationships ofassociated-to, a specific relationship to GO [21,22],
disease-centered ontologies (DO, OGMS, DOID, and
IDO) only describe relationships between the processes
and external perturbations, including pathogens, drugs,
environmental factors, and medical devices, on the
diseases. As many bio-medical researchers strive to
understand diseases in the context of networks and
pathways in order to realize better and personalized
diagnoses and treatments in clinical medicine, molecu-
lar interpretations of both the external causes and the
internal processes of disease have demanded biological
high throughput analyses in order to elucidate molecu-
lar mechanisms of pathogenesis and progression. GO is
currently used in biological high throughput analyses
by detecting overrepresented GO terms in case groups
relative to a control group [2325]; however, we do not
consider that GO covers all the required internal pro-
cesses of disease, because internal disease processes
may be different from the processes observed under
healthy conditions. The difference can be observed not
only in the classification of the internal processes, but
also in the relationships between the internal processes
and their participants and activators and/or suppressors to
these processes [26]. Some ontologies have attempted to
include annotations of genes associated with internal
processes specific to the diseases such as an ontology for
diabetes [2, 27]; however, those ontologies cover only a
small number of existent diseases.
Periodontitis is a multifactorial disease causing in-
flammation in periodontal tissue. The pathogenesis of
periodontitis includes numerous biological entities such
as oral microorganisms and immune and genetic fac-
tors, physical effects such as dental occlusion, drugs,
and chemicals, environmental factors, and interactions
with systemic diseases such as diabetes and cardiovas-
cular diseases.
As systematic mechanisms underlying periodontitis
are complex, it remains difficult to elucidate relation-
ships and interactions between the multiple risk factors
through studies on individual molecules only. Analyses
based on pathways and networks are required in order
to elucidate relationships and interactions from omics
data, such as genomics, transcriptomics, proteomics,
and metabolomics observed in molecular pathways that
are involved in the pathogenesis and progression of
periodontitis [28]. Actually, gene expression data from
periodontal tissue has allowed the partial elucidation of
such molecular pathways [29]. However, previous ana-
lyses of detecting disease-specific processes have not
been so successful. This may be due to the complexity
of periodontitis, as well as to the way results from
omics analyses have been semantically interpreted. In
omics analysis, GO is generally used in annotations of the
data; however, few processes specific to periodontitis are
Table 1 The full texts and abstracts used in this study
Review articles No. of articles
Full texts 101
Abstracts only 79
Unavailable 14
Total 194
Suzuki et al. Journal of Biomedical Semantics  (2015) 6:30 Page 3 of 13included in GO. This was our motivation to enhance the
GO by using a periodontitis-specific extension.
Kornman proposed a systems model to link the clin-
ical presentation of periodontitis to underlying molecu-
lar knowledge and thus better clarify the pathogenesis
of periodontal diseases [30].
In the past decade, molecular details have been elucidated
in periodontitis comparison with other osteoimmunology-
based diseases such as rheumatoid arthritis [31].
Osteoimmunology is an interdisciplinary science inves-
tigating the interplay between the skeletal and the
immune systems. The main contributors to osteoim-
munology are bone effector cells such as osteoclasts or
osteoblasts, and immune cells, particularly lymphocytes
and monocytes [32]. Osteoimmunology has now become
one of the most prominent research areas in clinical biol-
ogy, and periodontitis is considered to be a good model
for the study of the common mechanisms in osteoimmu-
nology and the progression of target diseases. Several
studies relating periodontitis to osteoimmunology have re-
cently been reported [33, 34].
In this paper, we report on Periodontitis-Ontology
(PeriO), an ontology we developed for periodontitis.
This ontology covers and formally describes a variety of
entities that stand in relation to periodontitis. PeriO
describes relationships between molecular mechanisms
of inner processes in periodontitis and pathogenesis
and progression in a clinical view of the disease, as well
as relationships between molecular influences of drugs
and environmental molecules and clinical medications
and treatments.
Content integrated into our PeriO includes the follow-
ing: 1) functional classification of bacterial molecules in
periodontal lesions; 2) interactions between periodontitis
and other systemic diseases; 3) environmental chemicals
affecting periodontitis; and 4) processes of medical treat-
ments for and the molecular pathogenesis of periodontitis.
Methods
PeriO is based on our previous development of ontology,
which specified the bone resorption response induced by
periodontitis [35]. Bone resorption is one part of a larger
process in the onset and progression of periodontitis; the
entire process is composed of many biological processes
and clinical actions. We systematized the entire process in
this study as PeriO.
Extraction of terms relating to periodontitis
We retrieved review articles for periodontitis by from
the PubMed using keywords of periodontitis, biology,
human and review on April 30, 2014. In order to col-
lect mentions of molecules and cells participating in
processes we formalized in PeriO, we investigated
JOURNAL OF
BIOMEDICAL SEMANTICS
Aoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3
http://www.jbiomedsem.com/content/6/1/3REVIEW Open AccessImplementation of linked data in the life sciences
at BioHackathon 2011
Kiyoko F Aoki-Kinoshita1, Akira R Kinjo2, Mizuki Morita3, Yoshinobu Igarashi4, Yi-an Chen4, Yasumasa Shigemoto5,
Takatomo Fujisawa5, Yukie Akune1, Takeo Katoda1, Anna Kokubu1, Takaaki Mori1, Mitsuteru Nakao6,
Shuichi Kawashima7, Shinobu Okamoto7, Toshiaki Katayama7 and Soichi Ogishima8*Abstract
Background: Linked Data has gained some attention recently in the life sciences as an effective way to provide
and share data. As a part of the Semantic Web, data are linked so that a person or machine can explore the web of
data. Resource Description Framework (RDF) is the standard means of implementing Linked Data. In the process of
generating RDF data, not only are data simply linked to one another, the links themselves are characterized by
ontologies, thereby allowing the types of links to be distinguished. Although there is a high labor cost to define an
ontology for data providers, the merit lies in the higher level of interoperability with data analysis and visualization
software. This increase in interoperability facilitates the multi-faceted retrieval of data, and the appropriate data can
be quickly extracted and visualized. Such retrieval is usually performed using the SPARQL (SPARQL Protocol and
RDF Query Language) query language, which is used to query RDF data stores. For the database provider, such
interoperability will surely lead to an increase in the number of users.
Results: This manuscript describes the experiences and discussions shared among participants of the week-long
BioHackathon 2011 who went through the development of RDF representations of their own data and developed
specific RDF and SPARQL use cases. Advice regarding considerations to take when developing RDF representations
of their data are provided for bioinformaticians considering making data available and interoperable.
Conclusions: Participants of the BioHackathon 2011 were able to produce RDF representations of their data and
gain a better understanding of the requirements for producing such data in a period of just five days. We
summarize the work accomplished with the hope that it will be useful for researchers involved in developing
laboratory databases or data analysis, and those who are considering such technologies as RDF and Linked Data.
Keywords: Semantic Web, Data integration, PDBj, DDBJ, Glycobiology, Alzheimers disease, Faceted search interfaceIntroduction
As technologies in the life sciences advance among vari-
ous -omics fields in the post-genomic age, increasing
amounts of a wide variety of data are being generated,
making it difficult to query and find relationships between
the data. Currently, many of the databases that allow data
to be downloaded often provide them in their own propri-
etary format or as tab- or comma-delimited text files. In-
corporating such data requires much data manipulation
and integration, which is usually difficult for most* Correspondence: ogishima@sysmedbio.org
8Department of Bioclinical informatics, Tohoku Medical Megabank
Organization, Tohoku University, Seiryo-cho 4-1, Aoba-ku, Sendai-shi Miyagi
980-8575, Japan
Full list of author information is available at the end of the article
© 2014 Aoki-Kinoshita et al.; licensee BioMed
Creative Commons Attribution License (http:/
distribution, and reproduction in any medium
Domain Dedication waiver (http://creativecom
article, unless otherwise stated.researchers. In order to process such data, most biologists
would use Excel and would probably need to write scripts
to find matching data across different data files, if not
done manually. Moreover, data matching may be difficult
because of different levels of detail of the data provided,
requiring disambiguation/clarifiation of data types, which
is a difficult process. Even for bioinformaticians, there is a
great amount of ad hoc data processing which becomes
quite a burden. Moreover, high-activity databases often
update their data on a regular basis, often increasing the
burden to continuously import the necessary information.
Another burden lies in the need to develop individual
query tools for each database, which may be limited in
functionality and focus solely on the database at hand, stillCentral. This is an Open Access article distributed under the terms of the
/creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly credited. The Creative Commons Public
mons.org/publicdomain/zero/1.0/) applies to the data made available in this
Aoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3 Page 2 of 13
http://www.jbiomedsem.com/content/6/1/3requiring researchers to integrate data from multiple
sources manually.
In the midst of such activity, Linked Data has gained
some attention recently in the life sciences as an effective
way to provide and share data. As a part of the Semantic
Web, data are linked so that a person or machine can ex-
plore the web of data. With Linked Data, when a user has
some data, he/she can find other, related, data [1] rather
easily. Resource Description Framework (RDF) is the
standard means of implementing Linked Data. By using
RDF, database providers can publish data contents that are
accessible via URIs. In addition, each data contains links
to other related data that are (preferably) provided in RDF.
Thus, by crawling through the URIs that are linked to one
another, a wide range of inter-related data can be retrieved
using Semantic Web technologies. As an example, the Sin-
dice portal provides a search engine to query RDF data
across all domains. Using existing web standards, Sindice
collects Semantic Web data, updated every five minutes,
and allows users to search and query across this data [2].
In the process of generating RDF data, not only are
data simply linked to one another, the links themselves
are characterized by ontologies, thereby allowing the
types of links to be distinguished. Although it may re-
quire a lot of effort for data providers to define an ontol-
ogy, the merit lies in the higher level of interoperability
with data analysis and visualization software. That is, re-
lated data are linked to one another via ontologies con-
taining URIs, thus facilitating the multi-faceted retrieval
of data, where the appropriate data can be quickly ex-
tracted and visualized. Such retrieval is usually per-
formed using the SPARQL (SPARQL Protocol and RDF
Query Language) query language, which is used against
RDF data stores, or triplestores [3]. For the database
provider, such interoperability will surely lead to an in-
crease in the number of users.
This manuscript will describe the experiences and dis-
cussions shared among participants of BioHackathon
2011 who went through the development of RDF repre-
sentations of their own data and developed specific RDF
and SPARQL use cases within a period of five days. For
bioinformaticians considering making data available and
interoperable, this manuscript will provide advice re-
garding considerations to take when developing RDF
representations of their data.
Review
Current landscape of Semantic Web in the life sciences
Linking Open Data (LOD) is a recent movement encour-
aging data providers to develop and to publish their data
in a semantically connected manner. It is recommended
that datasets are exposed and shared as Linked Data in
RDF format, where URIs interlink resources on the Se-
mantic Web. As shown in the LOD cloud diagram [4],life science data occupies one of the major domains of
LOD. This situation is primarily brought by the Bio2RDF
project [5] which translated major public bioinformatics
databases into RDF and provided them as SPARQL end-
points. This pioneering work showed that distributed
datasets in the life sciences can be effectively integrated
through Semantic Web technology.
The semantics of RDF data is described by an ontology,
which describes basic concepts in a domain and defines
relations among them. It provides the basic building
blocks comprising its structure: classes or concepts, prop-
erties, and restrictions on properties. As a result, an ontol-
ogy provides a common vocabulary for researchers who
need data integration, data sharing, semantic annotation,
and extraction of information in the specific domain. To
take advantage of Linked Data, one will eventually need to
make use of ontologies. Several ontologies have already
been carefully designed by experts in particular fields.
BioPortal is a useful web resource for developers to find
a particular ontology in the life sciences. It provides an
open repository and search engines for biological ontol-
ogies [6]. Moreover, the BioPortal Ontology Recom-
mender system uses a set of keywords describing a
domain of interest and suggests appropriate ontologies for
representing the query [7]. The Open Biological and Bio-
medical Ontologies (OBO) Foundry provides biomedical
ontologies, such as the well-known Gene Ontology (GO),
with the goal of creating a suite of orthogonal interoper-
able reference ontologies in the biomedical domain [8].
The BioGateway project [9] attempts to query complex
biological questions for obtaining scientific knowledge
from RDF datasets in the semantic systems biology do-
main. They integrated SwissProt [10] protein annotations
and taxonomic information with gene ontology annota-
tions (GOA), ontologies provided by the open biological
and biomedical ontologies (OBO) foundry and in-house
developed ontologies such as cell cycle ontology (CCO).
This system presented an example of how SPARQL quer-
ies can retrieve meaningful biological knowledge when the
Semantic Web database contains rich information sup-
ported by fine-grained ontological annotations.
As the use of Semantic Web technologies increases, de-
mand for SPARQL endpoints for major databases is
raised. In response to these demands, UniProt has re-
leased their data in RDF and provides a publicly avail-
able SPARQL endpoint (http://beta.sparql.uniprot.org/).
European Bioinforamtics Institute in the European Mo-
lecular Biology Laboratory (EMBL-EBI) recently started
to provide RDF and SPARQL endpoints for several da-
tabases hosted at EBI (http://www.ebi.ac.uk/rdf/). Bio-
Mart [11] is one of the de facto standard databases
integrating various resources in biology, and the system
is widely used in many organizations [12]. A SPARQL
query interface has been implemented since the version
Aoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3 Page 3 of 13
http://www.jbiomedsem.com/content/6/1/30.8 release, enabling users to query the metadata of any
BioMart system from Semantic web applications [13].
* LinkDB
LinkDB is a database that compiles relationships be-
tween database entries that have been serviced as the
backbone of GenomeNet for nearly 20 years. As of
August, 2011, a total of over 780,000,000 relationships
between entries from over 160 life science databases
have been registered. The data structure of LinkDB is
triples, consisting of pairs of database entries and their
relationships. Thus, it is very suitable for converting to
RDF. The following three entry relationships are defined
in LinkDB: equivalent (the same molecule but from dif-
ferent databases), original (hyperlinks to target database
entry provided in the subject database entry), and re-
verse (opposite of original; subject database entry is ref-
erenced by target). These relationships could be used as
predicates when generating RDF. During this BioHacka-
thon, all of the LinkDB entries were converted to RDF.
A manual describing how to use this data is available at
http://www.genome.jp/linkdb/linkdb_rdf.html.
* PDBj
The Protein Data Bank Japan (PDBj), a member of the
worldwide Protein Data Bank (wwPDB), is a database of
atomic structures of proteins and other biological mac-
romolecules. PDBj has recently started providing the
contents of its entries in terms of RDF (http://rdf.
wwpdb.org/). The RDF-formatted PDB entries are re-
ferred to as PDB/RDF in the following. The original
PDB entries are provided in a format called macromol-
ecular crystallographic information format (mmCIF),
which is in turn defined by the PDB exchange (PDBx)
dictionary [14,15]. The PDBx dictionary defines categor-
ies and items for describing various aspects of macro-
molecular structures. The OWL ontology of the PDB/
RDF is essentially a direct translation of the PDBx dic-
tionary augumented with additional classes and proper-
ties to handle links between different data sources.
In the PDB/RDF service, each PDB entry can be
accessed via a specific URL such as http://rdf.wwpdb.
org/pdb/1GOF for the PDB entry 1GOF. This page con-
tains mostly a list of links to the categories contained in
the entry. By following these links, for example, http://
rdf.wwpdb.org/pdb/1GOF/entityCategory, one finds a
list of links to entity category elements. Each category
element can be also accessed by a URL such as http://
rdf.wwpdb.org/pdb/1GOF/entity/1 which contains the
data describing the molecular entity whose the primary
key is 1 (in this particular example, the entity is galact-
ose oxidase. The PDBx dictionary also defines relations
between related categories, and this is reflected in PDB/
JOURNAL OF
BIOMEDICAL SEMANTICS
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 
DOI 10.1186/s13326-015-0019-z
RESEARCH ARTICLE Open Access
Concept selection for phenotypes and
diseases using learn to rank
Nigel Collier1,2*, Anika Oellrich3 and Tudor Groza4
Abstract
Background: Phenotypes form the basis for determining the existence of a disease against the given evidence.
Much of this evidence though remains locked away in text  scientific articles, clinical trial reports and electronic
patient records (EPR)  where authors use the full expressivity of human language to report their observations.
Results: In this paper we exploit a combination of off-the-shelf tools for extracting a machine understandable
representation of phenotypes and other related concepts that concern the diagnosis and treatment of diseases.
These are tested against a gold standard EPR collection that has been annotated with Unified Medical Language
System (UMLS) concept identifiers: the ShARE/CLEF 2013 corpus for disorder detection. We evaluate four pipelines as
stand-alone systems and then attempt to optimise semantic-type based performance using several learn-to-rank
(LTR) approaches  three pairwise and one listwise. We observed that whilst overall Apache cTAKES tended to
outperform other stand-alone systems on a strong recall (R = 0.57), precision was low (P = 0.09) leading to
low-to-moderate F1 measure (F1 = 0.16). Moreover, there is substantial variation in system performance across
semantic types for disorders. For example, the concept Findings (T033) seemed to be very challenging for all systems.
Combining systems within LTR improved F1 substantially (F1 = 0.24) particularly for Disease or syndrome (T047) and
Anatomical abnormality (T190). Whilst recall is improved markedly, precision remains a challenge (P = 0.15, R = 0.59).
Introduction
Phenotypes are generally regarded as the set of observ-
able characteristics in an individual. Examples include
body weight loss and abnormal sinus rhythm. Pheno-
types are important because they help to form the basis for
determining the classification and treatment of a disease.
Although coding systems such as the Human Pheno-
type Ontology (HPO) [1] and the Mammalian Phenotype
Ontology (MPO) [2] have made substantial progress in
organising the nomenclature of phenotypes, authors typ-
ically report their observations using the full expressivity
of human language. In order to fully exploit a machine
understandable representation of phenotypic findings,
it is necessary to develop techniques based on natural
language processing that can harmonise linguistic varia-
tion [3-5]. Furthermore, such techniques need to operate
on a range of text types such as scientific articles, clinical
*Correspondence: nhc30@cam.ac.uk
Equal contributors
1University of Cambridge, Cambridge, UK
2European Bioinformatics Institute (EMBL-EBI), Cambridge, UK
Full list of author information is available at the end of the article
trials and patient records [6] in order to enable appli-
cations that require inter-operable semantics. Use cases
might include automated cohort extraction to support
research into a particular rare genetic disorder or support
for curating databases of human genetic diseases such
as the Online Mendelian Inheritance in Man database
(OMIM) [7]. We envision the final result to be a represen-
tation that decomposes the phenotype terms according
to their elementary conceptual units (building block con-
cepts) and harmonises them to ontologies such as the
FoundationalModel of Anatomy (FMA) [8] for anatomical
structures, the Phenotype Attribute and Trait Ontology
(PATO) [9] for qualities and Gene Ontology (GO) [10]
for biological processes. Our view is that the techniques
must be able to support the capture of phenotypes from
both physical objects and processes as well as cutting
across levels of granularity from the molecular level to the
organism level.
Finding the names of technical terms in life science
texts  known as named entity recognition  has been
the topic of intensive study over the last decade. Ground-
ing or normalising these terms to a logically structured
© 2015 Collier et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 2 of 12
domain vocabulary  an ontology  has proven to be
a substantial challenge, e.g. [11,12], because of idiosyn-
crasies in naming, the need to exploit syntactic structure
in the case of disjoint terms, the paucity of annotated
corpora for training and evaluation and the incomplete-
ness of the target ontologies themselves. To accomplish
this task, concept identification systems have emerged
with different analytical goals. In this paper, we investi-
gate the utility of four existing conceptual coding pipelines
(i.e. MetaMap [13], Apache cTAKES [14], NCBO anno-
tator [15] and BeCAS [16]) in order to identify and har-
monise the phenotypes and other concepts related to the
diagnosis and treatment of diseases. These tools do not
explicitly consider phenotypes as a conceptual category
but rather provide groundings from text to a range of
building block concepts which we hope to exploit. In order
to provide a basis for comparing these tools quantita-
tively and qualitatively, we have chosen to harmonise their
outputs to Unified Medical Language System (UMLS)
concept unique identifiers (CUIs) and semantic types as
the common coding standard. Concept unique identifiers
provide a way to encode senses of words and phrases,
e.g. culture as either anthropological culture or labora-
tory culture [17]. UMLS semantic types provide a broad
classification of all the UMLS concepts contained in the
MetaThesaurus as well as a structuring of those seman-
tic types. There are approximately 133 semantic types and
54 relationships between them. UMLS annotations were
assigned at the sentence level. Textual annotations used
the ShARE/CLEF 2013 corpus [18] which we describe
later. We have identified the concept classes which are the
most promising building blocks  such as T184 Sign and
Symptom - and evaluated based on these. Our approach
aims to work towards the composition of phenotypes in
future work based on the building block outputs of the
systems reported here. We chose to focus on the uncus-
tomised use-case of the four base systems as a way of
exploring their immediate utility to users who did not have
access or resources to build annotated training data or the
ability to build their own post-processing rules.
In addition to evaluating the suitability of each individ-
ual system on the ShARE/CLEF 2013 corpus, we inves-
tigate possibilities to optimise the outputs of systems
using an ensemble approach. In order to take advantage
of the complementarity in concept recognition and go
beyond a simple voting mechanism, we have employed
several learn-to-rank (LTR) methods  more specifically
three pairwise ranking approaches: SVMRank [19], Rank-
Boost [20] and RankNet [21]; and one listwise ranking
approach: ListNet [22]. Such methods learn to optimise
contraints pairwise or list wise based on a set of features
and a predefined ranking of the input. In our setting,
each sentence, treated as an instance, is described via five
feature blocks by the individual CR systems. Using the
ShARE/CLEF 2013 training data, the ranking of the sys-
tems is assigned based on the ground truth and a model
is learned such that it maximises the ranking correla-
tion. The final optimised ensemble and model is tested
on the ShARE/CLEF 2013 test data set. The learn to
rank ensemble enables us to accept the choices of more
than one system in the event of a closely tied rank-
ing. We found that combining systems within learn to
rank improved F1 substantially compared to stand-alone
systems.
Methods
Data
For evaluation and training the re-ranker we chose to use
the ShARE/CLEF e-health 2013 Task 1 evaluation data set
of 300 de-identified clinical records from theMultiparam-
eter Intelligent Monitoring in Intensive Care (MIMIC)
II data-base (http://mimic.physionet.org/database.html)
with stand-off annotations for disorders. This is a mixed
corpus that includes discharge summaries, echo reports
and radiology reports used in an intensive care unit set-
ting. 200 notes were designated for training and 100 for
testing. Annotation was done by two annotators plus
open adjudication. Access to the corpus required appro-
priate registration with MIMIC II and the completion of
a US human subjects training certificate. The distribution
of UMLS semantic types for disorder-related text spans
can be seen in Tables 1 and 2. Note that we removed
minor classes with frequencies of 1 (i.e. T002, T031, T049,
T058, T059, T121, T197). As can be seen in Tables 1
and 2, the majority of semantic types relate to diseases,
symptoms and pathological functions together with a
substantial minority of annotations for injuries, congen-
ital and anatomical abnormalities and mental/behavioral
dysfunctions.
Table 1 ShARE/CLEF e-health training corpus semantic
types
ID UMLS Semantic type Freq. Unique Av. term length
T047 Disease or syndrome 1803 410 1.97
T184 Sign or symptom 842 163 1.56
T046 Pathologic function 518 133 1.65
T037 Injury or poisoning 213 96 2.00
T019 Congenital abnormality 184 25 3.61
T190 Anatomical abnormality 103 36 1.77
T191 Neoplastic process 92 49 1.87
T048 Mental or behavioral
dysfunction
84 32 1.76
T033 Finding 45 15 2.90
T020 Acquired abnormality 40 17 1.93
Distribution of UMLS semantic types for annotations by frequency and
frequency without duplication as well as the average term length in tokens.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 3 of 12
Table 2 ShARE/CLEF e-health test corpus semantic types
ID UMLS Semantic type Freq. Unique Av. term length
T047 Disease or syndrome 1723 371 1.88
T184 Sign or symptom 816 149 1.51
T046 Pathologic function 520 113 1.59
T037 Injury or poisoning 106 33 1.75
T019 Congenital abnormality 96 18 1.88
T190 Anatomical abnormality 125 26 1.74
T191 Neoplastic process 73 34 2.02
T048 Mental or behavioral
dysfunction
137 32 1.67
T033 Finding 13 6 1.11
T020 Acquired abnormality 41 21 1.62
Distribution of UMLS semantic types for annotations by frequency and
frequency without duplication as well as the average term length in tokens.
An example source sentence from the corpus is shown
in Figure 1 along with actual gold standard concept
annotations, harmonized semantic types and a potential
decompositional mapping to PATO and FMA for one clin-
ical phenotype (neck stiffness). Here C* annotations
correspond to concept annotations and T* annotations
correspond to harmonised semantic types.
The distributions for train and test possess good agree-
ment but, at the same time, also interesting differences:
the average length of mentions of T019 congenital abnor-
mality appears remarkably longer in the training corpus,
and there are relatively fewer T037 injury or poisoning and
T019 congenital abnormality instances in the testing set.
Moreover, we observe a greater variety of T037 instances
in the testing corpus.
Examples of what we might consider interesting phe-
notypes occur across all anntoated UMLS semantic
types as well as for unannotated strings. For example,
Right ventricular [is mildly] dilated (C0344893 | T019),
wall motion abnormality (no CUI) and hypotension
(C0520541 | T047). In other cases, the class shows a
disease and not a phenotype, e.g. complex autonomous
disease (C0264956 | T046). We note that unannotated
strings were not explicitly quantified in the present study
reported here and and are left for future study.
Experimental setup
We follow standard metrics of evaluation for the task
using F1, i.e. the harmonicmean of recall (R) and precision
(P). This is the same metric used by participants of the
ShARE/CLEF 2013 Task 1. F1 is calculated as F1 = 2PR/
(P + R), with P = TP/(TP + FP) and R = TP/(TP + FN)
where TP is the number of system suggestions where the
semantic type and the CUI is the same as the gold stan-
dard; FP is the number of system suggestions where the
semantic type and/or the CUI do not match the gold
standard; and FN is the number of spans in the gold
standard which the system failed to suggest. The major
difference between our evaluation and the ShARE/CLEF
shared task is that we evaluate at the sentence level and
not the mention level, i.e. the focus is on predicting con-
cept labels for the sentence as a whole and not the starting
and ending positions of those annotations in the sentence.
Consequently, our experimental results are not directly
comparable with those achieved by systems participat-
ing in the ShARE/CLEF Tasks. Evaluation is conducted
using blind data not used in system development data or
training.
Different applications require a different approach to
defining a true positive, false negative etc. In this case we
have considered a correct match to be recorded when a
complete match occurs between system output and gold
standard for both the identifier and the semantic type of
that concept in UMLS. In line annotation is not consid-
ered explicitly within this evaluation. Clearly any further
application requiring the explicit annotation of relation-
ships between concepts within the sentence would require
this. The evaluation protocol reported here supports
use cases such as statistical association analysis between
the co-occurring concepts and document indexing/
retrieval.
Individual system descriptions
The problemwe consider is how to select a set of disorder-
related SNOMED CT concepts for any given sentence.
Disorder-related concepts are chosen because of their
relevance to phenotype recognition. SNOMED-CT was
chosen as the ontology for harmonisation because it offers
a joint coding ontology for all the base systems. A number
of factors complicate the task including: (a) in line with
our desire to test off-the-shelf performance, the system
pipelines were not tuned in any way for predicting the spe-
cific set of disorder-related semantic types appearing in
the corpus, (b) the annotation scheme allows for disjoint
Figure 1 Example of sentence annotations from the ShARE/CLEF corpus. The example shows concept annotations for headache; (C0018681 |
T184), neck stiffness (CO151315 | T184) and unable to walk (C0560048 | T033). An example decomposition for neck stiffness is shown with an
illustrative mapping to PATO:0001545 (inflexible) and FMA:Neck.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 4 of 12
(e.g. Right ventricular . . . dilated) and overlapping anno-
tation spans; and (c) clinical texts contain a high number
of abbreviations causing additional complications for term
identification and harmonisation.
We consider four uncustomized base concept annota-
tion systems based on clinical natural language process-
ing: NCBO Annotator, BeCAS, cTAKES and MetaMap.
With the exception of MetaMap, all the other systems
were used with their default parameters. Other systems
that could have been applied here include ConceptMap-
per [23], Whatizit [24] and Bio/MedLee [25]. These sys-
tems were either difficult to access or did not provide
a route to UMLS concept harmonizations. The systems
we applied adopt a range of techniques but tend to avoid
deep parsing. Instead, they make use of a range of shal-
low parsing, sequence-based machine learning (e.g. for
named entity recognition and part of speech tagging) and
pattern-based techniques, supplemented with restrictions
and inferences on source ontologies such as SNOMED
CT [26]. In all cases it should be noted that we dealt with
black box systems.
NCBO Annotator (M1) The NCBO Annotator is an
online system that identifies and indexes biomedical con-
cepts in unstructured text by exploiting a range of over
300 ontologies in BioPortal. These ontologies include
many that have particular relevance to disorders and
phenotypes such as SNOMED CT, LOINC (Logic Obser-
vation Identifiers, Names and Codes) [27], the FMA
and the International Classification of Diseases (ICD-
10) [28]. NCBOAnnotator operates in two stages: concept
recognition and semantic expansion. Concept recogni-
tion performs lexical matching by pooling terms and their
synonyms from across the ontologies and then apply-
ing a multiline version of grep to match lexical variants
in free text. During semantic expansion, various rules
such as transitive closure and semantic mapping using the
UMLS Metathesaurus are used to suggest related con-
cepts from within and across ontologies based on extant
relationships.
BeCAS (M2) BeCAS (the BioMedical Concept Annota-
tion System) is the newest integrated system of the four
that we tried. The pipeline of processes involves the fol-
lowing stages: sentence boundary detection, tokenization,
lemmatization, part of speech (POS) tagging and chunk-
ing, abbreviation disambiguation, and concept unique
identifier (CUI) tagging. The first four stages are per-
formed by a dependency parser that incorporates domain
adaptation using unlabelled data from the target domain.
CUI tagging is conducted using regular expressions for
specific types such as anatomical entities and diseases.
Dictionaries used as sources for the regular expressions
include the UMLS, LexEBI [29] and the Jochem joint
chemical dictionary [30]. During development the con-
cept recognition system was tested on abstracts and full
length scientific articles using an overlapping matching
strategy.
Apache cTAKES (M3) cTAKES consists of a staged
pipeline of modules that are both statistical and rule-
based. The order of processing is somewhat similar to
MetaMap and consists of the following stages: sentence
boundary detection with OpenNLP, tokenization, lexical
normalisation (SPECIALIST lexical tools), part of speech
tagging and shallow parsing using OpenNLP trained in-
domain on Mayo Clinic EPR concept recognition, nega-
tion detection using NegEx [31] and temporal status
detection. Concept recognition is conducted within the
boundaries of noun phrases using dictionary matching
on a synonym-extended version of SNOMED CT and
RxNORM [32] subset of UMLS. Evaluation was con-
ducted with a focus on EPRs but also using corpora from
the scientific literature.
MetaMap (M4-M9) MetaMap is a widely used and
technically mature system from the National Library of
Medicine (NLM) for finding mentions of clinical terms
based on CUI mappings to the UMLS Metathesaurus.
The UMLS Metathesaurus forms the core of the UMLS
and incorporates over 100 source vocabularies including
the NCBI taxonomy, SNOMED CT and OMIM. Output
is to the 135 UMLS semantic types. The system exploits
a fusion of linguistic and statistical methods in a staged
analysis pipeline. The first stages of processing perform
mundane but important tasks such as sentence bound-
ary detection, tokenization, acronym/abbreviation iden-
tification and POS tagging. In the next stages, candidate
phrases are identified by dictionary lookup in the SPE-
CIALIST lexicon and shallow parsing using the SPECIAL-
IST parser. String matching then takes place on the UMLS
Metathesaurus before candidates are mapped to the
UMLS and compared for the amount of variation. A final
stage of word sense disambiguation uses local, contextual
and domain-sensitive clues to arrive at the correct CUI.
MetaMap is unique in providing a rich set of
options [33] to allow the user to customise the approach
the system takes to concept mapping.We chose to explore
a range of options including what we considered a high
precision strict approach to matching as well as nega-
tion detection with NegEx. The variations of MetaMap we
explored were:
 M4: MetaMap -A -negex  using strict matching
and negation detection
 M5: MetaMap -A -y  using strict matching and
forcing MetaMap to perform word sense
disambiguation on equally scoring concepts
 M6: MetaMap -g  allowing concept gaps
 M7: MetaMap -i  ignoring word order
 M8: MetaMap  using the base version
 M9: MetaMap -A  using strict matching only
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 5 of 12
Ensemble approach
In addition to the nine basic systems M1 to M9, we eval-
uated several ranking approaches that rank the quality
of basic system outputs based on a sentence-level and
concept-level features. These features include individual
source sentence vocabulary, the semantic types suggested
by the system and the vocabulary for the suggested con-
cept labels. More sophisticated features will be tested in
the future. We believe that the chosen features serve as a
useful first step for evaluating the ranking approach. The
approaches we tested make use of a scoring function to
rank each systems output set of concept labels against the
training data. These rankings are used together with the
features to train a learn-to-rank (LTR) model. We evalu-
ated four different ranking algorithms based on pairwise
and list wise comparisons to maximise the ranking corre-
lation for all categories, where the categories represent the
nine basic systems.We explore the underlying assumption
that a set of features exists that can predict when one sys-
tem will perform better on a given sentence than another.
The ranking function we applied was the F1 metric that
we used to evaluate each system described in detail in the
section below.
Ranking essentially aims to establish which hypoth-
esis about sentence-level concept annotations is most
likely given the available evidence. Labelled instances are
provided during training as feature vectors. Each label
denotes a single rank that is determined by comparing the
F1 scores for each system based on the concepts they out-
put on that sentence against the set of gold standard con-
cepts. The goal of training each of the ranking approaches
is to find a model that correctly determines the ordering
of systems on a given sentence. Afterwards we can either
choose the predictions from the single highest ranking
system or combine a group of highly ranking systems.
The feature blocks used by the ensemble model are
listed in Table 3. During testing, a feature vector is pro-
vided for each system (methods M1 . . . M9) and the
LTR model determines a score which is then converted
to an ordered ranked list by the ensemble. In practice the
semantic types suggested by the top system are selected.
If the first rank is shared between multiple systems, the
top outputs from the top ranking systems are combined
by taking the union.
The LTR systems that we investigated include three
pairwise LTR  SVMRank [19], RankNet [21], and Rank-
Boost [20]  and one listwise LTR  ListNet [22]. Table 4
provides a succinct comparative overview of the two types
of LTR, as initially described in [34].
Results
Comparison of stand-alone systems
Table 5 presents results for each of the stand-alone sys-
tems at a macro level, while Table 6 lists results structured
Table 3 Feature blocks used to build the ensemblemodel
Feature block Description
FB1 A Boolean set of features for the system identifiers
(i.e. M1 . . . M9);
FB2 A Boolean set of features for the semantic types
that are predicted by the system to appear and not
appear in the sentence (i.e. T047, T184, . . . etc.);
FB3 A set of integer valued features for the counts of
vocabulary terms appearing in UMLS concepts that
are predicted by the systems to appear in the
sentence; In total the set consisted of 1,008 UMLS
CUIs;
FB4 A set of integer valued features for the counts of
vocabulary terms appearing in the sentence; The
vocabulary consisted of 13,565 terms;
FB5 A set of integer valued features for the 45
cluster distributed semantic classes which match
to FB3. The 45 cluster classes derived by Richard
Socher and Christoph Manning from PubMed
are available at http://nlp.stanford.edu/software/
bionlp2011-distsim-clusters-v1.tar.gz
according to semantic type. Note that we did not perform
any learning procedure at this stage on the gold stan-
dard corpus. We can see several noteable results including
the relatively better performance of systemM3 (cTAKES),
both at the macro level (0.16 F1, compared to 0.08 F1
achieve by the next system in line  M5), as well as
across most semantic types  with the exception of T190
(Anatomical abnormality) where system M4 does best.
No single system though achieves both winning recall
and precision in the type-based setting. System M5 for
example (MetaMap -A -y) generally achieves the high-
est precision. We can also note a wide disparity in F1 by
systems across semantic types.
In general the stand-alone systems performed better on
T047, T184 and T048. In contrast, performance on T037,
Table 4 Brief comparative overview on the learn to rank
approaches, adapted from [34]
Pairwise learn to rank Listwise learn to rank
Goal Ranking by learning on
object pairs
Ranking by learning on
object lists
Loss function pairwise loss, e.g., hinge
loss, exponential loss,
logistic loss
listwise loss, e.g., cross
entropy loss, cosine loss
Advantages Theoretical aspects are
well studied
Considers the relationship
among objects to their full
extent
Disadvantages Considers only pairwise
orders; May be biased
towards lists with more
objects
Theoretical aspects are
less well studied
Algorithms SVMRank [19];
RankNet [21];
RankBoost [20]
ListNet [22]
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 6 of 12
Table 5 Comparison of stand-alone systems on training
data
System P R F1
M1: NCBO Annotator 0.0393 0.5044 0.0729
M2: BeCAS 0.0146 0.0134 0.0140
M3: Apache cTAKES 0.0933 0.5675 0.1602
M4: MetaMap -A -negex 0.0389 0.2992 0.0689
M5: MetaMap -A -y 0.0498 0.2505 0.0831
M6: MetaMap -g 0.0387 0.2905 0.0683
M7: MetaMap -i 0.0392 0.2994 0.0693
M8: MetaMap 0.0389 0.2992 0.0689
M9: MetaMap -A 0.0389 0.2992 0.0689
Macro precision, recall and F1 of the individual systems on the training data. The
highest scoring system F1 is shown in bold.
T190, T033 and T019 tended to be weak. Stronger per-
formance might be partly correlated with shorter average
term length (see Table 2) but this is not an entirely satis-
fying explanation. Another possible explanation is hinted
at by the fact that the more challenging classes are at the
lower end of frequencies in the EPR data. This might indi-
cate that the semantic resources which the systems draw
on have been less intensively developed and might not
provide such extensive lexical support as more frequent
classes.
Learn-to-rank results
Using documents as the sampling unit, we performed
randomised 10-fold cross validation on the ShARE/CLEF
training data. 9 parts of the data were selected without
replacement to train the four LTR models from scratch
and 1 part was used to test. The 10 test parts were then
joined together and recall, precision and F-score were
calculated as in the stand-alone evaluation.
In the testing stage, we experimented with all combi-
nations of feature blocks and also with different settings
for LTR parameters. Best results were achieved using fea-
ture blocks FB1, FB2 and FB4, in addition to the following
model parameters:
 SVMRank: a value of 30 for the trade-off between
training error and margin;
 RankNet: 100 epochs, 1 hidden layer with 10 nodes
and a learning rate of 0.00005;
 RankBoost: 300 rounds to train and 10 threshold
candidates to search;
 ListNet: 1500 epochs and a learning rate of 0.00001;
Feature blocks FB3 and FB5 were not found to improve
performance in these experiments.
Finally, in order to gain a deeper understanding in
the ensembles behaviour, we have experimented with
different tie-breaking strategies, at different top-K ranking
Table 6 Comparison of stand-alone systems on training
data
ID Sys P R F1 ID Sys P R F1
T047 M1 0.39 0.55 0.45 T191 M1 0.24 0.30 0.26
M2 0.03 0.01 0.02 M2 0.05 0.03 0.04
M3 0.44 0.63 0.52 M3 0.29 0.64 0.40
M4 0.58 0.28 0.38 M4 0.21 0.25 0.23
M5 0.72 0.22 0.34 M5 0.38 0.23 0.28
M6 0.58 0.27 0.37 M6 0.21 0.25 0.23
M7 0.58 0.28 0.38 M7 0.22 0.25 0.23
M8 0.58 0.28 0.38 M8 0.21 0.25 0.23
M9 0.58 0.28 0.38 M9 0.21 0.25 0.23
T184 M1 0.35 0.61 0.45 T048 M1 0.28 0.49 0.35
M2 0.02 0.01 0.01 M2 0.04 0.03 0.03
M3 0.47 0.58 0.52 M3 0.45 0.55 0.50
M4 0.62 0.41 0.49 M4 0.53 0.34 0.42
M5 0.68 0.36 0.47 M5 0.67 0.27 0.38
M6 0.61 0.40 0.49 M6 0.54 0.35 0.43
M7 0.61 0.41 0.49 M7 0.54 0.34 0.42
M8 0.62 0.41 0.49 M8 0.53 0.34 0.42
M9 0.62 0.41 0.49 M9 0.53 0.34 0.42
T046 M1 0.28 0.62 0.39 T033 M1 0.01 0.36 0.01
M2 0.03 0.04 0.03 M2 0.00 0.00 0.00
M3 0.30 0.69 0.42 M3 0.00 0.11 0.00
M4 0.50 0.34 0.40 M4 0.00 0.13 0.01
M5 0.50 0.26 0.34 M5 0.00 0.13 0.01
M6 0.49 0.33 0.39 M6 0.00 0.13 0.01
M7 0.50 0.34 0.41 M7 0.00 0.13 0.01
M8 0.50 0.34 0.40 M8 0.00 0.13 0.01
M9 0.50 0.34 0.40 M9 0.00 0.13 0.01
T037 M1 0.19 0.24 0.21 T020 M1 0.36 0.50 0.42
M2 0.00 0.00 0.00 M2 0.00 0.00 0.00
M3 0.26 0.34 0.30 M3 0.33 0.57 0.42
M4 0.38 0.22 0.28 M4 0.36 0.36 0.36
M5 0.42 0.21 0.28 M5 0.36 0.21 0.27
M6 0.36 0.20 0.25 M6 0.36 0.33 0.35
M7 0.37 0.21 0.27 M7 0.36 0.36 0.36
M8 0.38 0.22 0.28 M8 0.36 0.36 0.36
M9 0.38 0.22 0.28 M9 0.36 0.36 0.36
T190 M1 0.12 0.44 0.19 T019 M1 0.40 0.11 0.18
M2 0.01 0.01 0.01 M2 0.00 0.00 0.00
M3 0.12 0.55 0.19 M3 0.58 0.14 0.23
M4 0.28 0.22 0.25 M4 0.27 0.07 0.11
M5 0.32 0.19 0.24 M5 0.34 0.06 0.11
M6 0.28 0.22 0.25 M6 0.25 0.07 0.11
M7 0.28 0.22 0.25 M7 0.27 0.07 0.11
M8 0.28 0.22 0.25 M8 0.27 0.07 0.11
M9 0.28 0.22 0.25 M9 0.27 0.07 0.11
Type-based micro precision, recall and F1 of the individual systems on the
training data. The highest scoring system F1 for each semantic type is shown in
bold. Note that italics scores indicate the highest level achieved for recall and
precision for each semantic type by any system.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 7 of 12
levels. In our context, the outcome of applying LTR on
an instance is a ranked list of the individual systems for
that instance, together with the associated weight. Hence,
to compute the standard performance metrics such that
the results are comparable to those of the stand-alone
systems, the LTR ranking has to be transformed into a
hard classification outcome. This is realised by introduc-
ing a cut-off at a desired top-K level, which entails that
the systems ranked above K will participate in the classifi-
cation outcome. In a setting where multiple systems may
be ranked above the threshold (possible even for K = 1), a
tie-breaking strategy is required. We have considered two
strategies: (i) a union strategy, where the individual anno-
tations of all top-K ranked systems are merged via a set
union, and the union is considered the final classification
result on that particular instance; and (ii) an oracle strat-
egy, where using the ground truth, we aim to choose the
single system among the top-K ranked that maximises the
performance metric.
While the first strategy does not require any a priori
knowledge and is usable in a proper application sce-
nario, the second is only usable when the ground truth is
known. Thus, it is not applicable for appropriate testing.
However, we included this strategy in order to under-
stand the actual contribution of the individual systems to
the ensemble result. Consequently, the tables listing the
macro-performance metrics of the ensemble on both the
ten-fold cross validation (Table 7 and 8), as well as on the
blind test data (Table 9) are accompanied by a measure of
individual system contribution to the final outcome. Note
that, under normal circumstances, the sum of all indi-
vidual contributions should be 1.0. However, this is true
only for the oracle strategy, where a single system is cho-
sen to represent the ensemble. The union strategy may
involve several systems, each of which will score points for
contributing to the ensemble result.
Returning to the results, the overall macro performance
of the LTR approaches on ten-fold cross validation using
the ShARE/CLEF training set is listed in Table 7. At top-1
rank level, the best union strategy was achieved by SVM-
Rank with an F1 of 0.24, while the Oracle strategy shows
RankBoost to outperform the other models with an F1 of
0.28. These compare to the best single system, as shown
in Table 5, which was cTAKES (M3) with F1 = 0.16 
representing a contribution of +8 and +11 points of F1,
respectively. The decrease in ranking threshold leads to a
natural decrease in F-Score for the union strategy (since it
become more and more inclusive), and at the same time
with an increase in F-Score for the Oracle strategy (since
it enlarges the pool from which it can choose the opti-
mal solution)  from 0.24 (top-1) to 0.19 (top-2) and 0.18
(top-3) for union and from 0.28 (top-1) to 0.36 (top-2) and
0.38 (top-3) for Oracle. Independently of the threshold or
model, however, the results of the stand-alone systems are
reflected in the individual contributions of the systems in
the ensemble (as shown in Table 7).With a few exceptions,
most of which are in the union strategy, M3 (cTakes) is
themost prominent contributor to the ensemble outcome,
paired, subject to the LTR model, either with M1 (NCBO
Annotator) or M9 (MetaMap strict).
It is interesting to note that, while using the Union strat-
egy the LTR outcome is consistent across different top-K
levels  SVMRank achieving the best results  the same
does not hold for the Oracle strategy, which shows three
models achieving the best results at three top-K levels.
There are, however, some patterns that emerge from the
individual system contributions. For example, RankNet
shows a clear preference towards M1 and M3 only. SVM-
Rank, ListNet and RankBoost use predominantly M1 and
M3, augmented with M9, M4 and M7 respectively. Sur-
prisingly M4 (MetaMap with Negex) appeared to have
minimal impact in the ensemble although it features more
prominently in several Oracle experiments.
The ensemble approach improved performance for all
semantic types with the exception of two cases, where
the performance was slightly reduced: T046 (F1: 0.42 to
0.40), T020 (F1: 0.42 to 0.41). More importantly, in some
cases, the improvement was substantial, e.g., 6% on T047
and T048 or 5% on T190. In terms of LTR model, differ-
ent models preferred different types  the results being
split between SVMRank and RankBoost. T047, T184 and
T190 were dominated by SVMRank and T191 and T084
by RankBoost.
In order to show the generalizability of the ensembles,
we ran them on the ShARE/CLEF held out set. The over-
all results listed in Table 9 show an average improvement
in performance of 2% across different tie-breaking strate-
gies and top-K levels. Furthermore, the individual system
contributions follow the same patterns as discussed on
the cross-validation results. Finally, as shown in Table 10,
most semantic types achieved stronger performance on
the testing data with T019, T037, T046, T184 and T190
showing strong gains. This indicates the potential variance
in the data sample.
Discussion
Examples of complications
Short forms Whilst we still need to conduct a detailed
drill down analysis we can see from a preliminary sur-
vey that one of the most significant sources of error
is the strong prevalence of undefined abbreviations in
the clinical texts, e.g. cp for C0008031: [chest pain], la
enlargement for C0344720: [left atrium enlargement], n
for C0027497: [nausea]. Without pre-processing to nor-
malise to full forms, the degree of ambiguity in the short
forms causes difficulties for the four systems which cannot
be solved in the ensemble. In contrast, full forms of short
forms were often found by the approaches employed.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 8 of 12
Table 7 Learn to rank on training data
Learn to rank performance Individual system contribution
Top-K Strategy Model P R F1 M1 M2 M3 M4 M5 M6 M7 M8 M9
Top-1 Union SVMRank 0.1513 0.5960 0.2413 0.25 0.04 0.45 0.01 0.00 0.01 0.00 - 0.23
ListNet 0.1153 0.5880 0.1928 0.73 0.03 0.21 0.02 - 0.00 - - -
RankNet 0.0924 0.5206 0.1570 1.00 - - - - - - - -
RankBoost 0.1296 0.6125 0.2139 0.46 0.05 0.50 - - 0.01 0.28 0.28 0.28
Oracle SVMRank 0.1513 0.5960 0.2413 0.25 0.04 0.45 0.01 0.00 0.01 0.00 - 0.23
ListNet 0.1153 0.5880 0.1928 0.73 0.03 0.21 0.02 0.00 - - -
RankNet 0.0924 0.5206 0.1570 1.00 - - - - - - - -
RankBoost 0.1791 0.6113 0.2770 0.27 0.04 0.49 - - 0.01 0.18 0.00 -
Top-2 Union SVMRank 0.1122 0.6426 0.1911 0.58 0.07 0.66 0.03 0.00 0.02 0.00 0.23 0.40
ListNet 0.0996 0.6566 0.1730 0.94 0.06 0.88 0.10 0.01 0.01 0.00 - -
RankNet 0.0989 0.6477 0.1716 1.00 - 1.00 - - - - -
RankBoost 0.1084 0.6469 0.1857 0.55 0.09 0.61 0.01 0.00 0.28 0.76 0.76 -
Oracle SVMRank 0.2340 0.6316 0.3415 0.09 0.00 0.67 0.01 0.00 0.01 - - 0.23
ListNet 0.2390 0.6439 0.3486 0.19 0.00 0.75 0.05 0.01 0.00 - - -
RankNet 0.2533 0.6363 0.3624 0.17 - 0.83 - - - - - -
RankBoost 0.2385 0.6359 0.3469 0.11 0.00 0.57 - - 0.02 0.29 0.00 -
Top-3 Union SVMRank 0.1051 0.6545 0.1811 0.61 0.18 0.68 0.07 0.02 0.03 0.24 0.40 0.77
ListNet 0.0921 0.6761 0.1621 0.97 0.60 0.96 0.40 0.03 0.03 0.01 - -
RankNet 0.0943 0.6486 0.1647 1.00 1.00 1.00 - - - - -
RankBoost 0.1048 0.6532 0.1806 0.56 0.11 0.63 0.29 0.23 0.75 0.97 0.97 -
Oracle SVMRank 0.2469 0.6409 0.3565 0.07 0.00 0.62 0.02 0.01 0.01 0.00 - 0.28
ListNet 0.2716 0.6596 0.3848 0.13 0.00 0.72 0.14 0.01 0.00 - - -
RankNet 0.2536 0.6367 0.3627 0.17 0.00 0.83 - - - - - -
RankBoost 0.2553 0.6397 0.3650 0.10 0.00 0.56 - 0.09 0.02 0.23 0.00 -
Macro precision, recall and F1 at different top K levels. The highest scoring system F1 for each level (both union and oracle strategies) is shown in bold. The table also
shows the individual contribution of the systems to the final score where italics scores indicate the highest contributing individual system(s) to each ensemble.
Lack of context A common problem in clinical texts
is known to be a lack of grammatical context. For exam-
ple, a line in a record might consist only of a single noun
phrase without end of line punctuation such as Left bun-
dle branch block C0023211: [left bundle branch block].
Whilst this should in theory be less of a problem for
algorithms that employ only local contextual patterns it,
nevertheless, presents issues for sentence boundary detec-
tion, which might introduce unexpected errors. In short-
ened sentences, omission of the subject is often a problem,
e.g. relative afferent defect can only be fully understood
in the context of the preceding sentence referring to
ocular discs and therefore achieving a normalisation on
C0339663: [afferent pupillary defect].
Complex grammatical structures and inferences
Disjoint concept mentions and inferences add an extra
layer of difficulty to the task. An example including
a long distance relationship as well as an inference is
shown in the following sentence: On motor exam, there
is generally decreased bulk and tone, decreased sym-
metrically, there is generalised wasting . . . . Firstly, an
inference is required to find the anatomical entity in
question, which in this example is the muscle indicated
by motor exam and the context provided in the sen-
tence decreased bulk and tone and wasting. Secondly,
the inferred entity then needs to be connected with
other distant text spans in the sentence such as gener-
ally decreased bulk and tone and generalised wasting to
yield the intended annotations C0026846: [muscle wast-
ing] and C0026827: [decreased muscle tone]. However, we
note here that inference is not consistently handled in the
gold standard. For example,. . . the gastrointestinal ser-
vice felt that an upper gastrointestinal bleed secondary to
non-steroidal anti-inflammatory drugs was . . .  is anno-
tated with C0413722: [non-steroidal anti-inflammatory
drugs] in the gold standard, suppressing the information
that there is an adverse reaction (upper gastrointestinal
bleed secondary to). If a system were to use matching
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 9 of 12
Table 8 Type-based learn to rank on training data
ID Sys P R F1 ID Sys P R F1
T019 SVMRank 0.46 0.15 0.23 T047 SVMRank 0.53 0.64 0.58
ListNet 0.47 0.13 0.20 ListNet 0.42 0.65 0.51
RankNet 0.47 0.11 0.18 RankNet 0.43 0.57 0.49
RankBoost 0.43 0.16 0.23 RankBoost 0.46 0.68 0.55
T020 SVMRank 0.32 0.57 0.41 T048 SVMRank 0.44 0.62 0.52
ListNet 0.29 0.50 0.36 ListNet 0.42 0.54 0.47
RankNet 0.33 0.42 0.37 RankNet 0.39 0.51 0.44
RankBoost 0.21 0.50 0.30 RankBoost 0.48 0.67 0.56
T033 SVMRank 0.01 0.32 0.02 T184 SVMRank 0.46 0.63 0.53
ListNet 0.01 0.48 0.02 ListNet 0.40 0.66 0.50
RankNet 0.01 0.48 0.03 RankNet 0.39 0.62 0.48
RankBoost 0.01 0.45 0.02 RankBoost 0.45 0.64 0.53
T037 SVMRank 0.34 0.33 0.33 T190 SVMRank 0.20 0.61 0.30
ListNet 0.26 0.24 0.25 ListNet 0.17 0.58 0.27
RankNet 0.22 0.21 0.22 RankNet 0.15 0.44 0.22
RankBoost 0.30 0.29 0.29 RankBoost 0.16 0.60 0.25
T046 SVMRank 0.29 0.66 0.40 T191 SVMRank 0.31 0.56 0.40
ListNet 0.25 0.67 0.36 ListNet 0.37 0.44 0.40
RankNet 0.27 0.61 0.37 RankNet 0.35 0.36 0.36
RankBoost 0.24 0.67 0.35 RankBoost 0.35 0.56 0.43
Note that the highest scoring system F1 for each semantic type is shown in bold.
and local context rules, it may miss this annotation as its
inference system would expect to annotate secondary to
non-steroidal anti-inflammatory drugs, which, to the best
of our knowledge, does not exist as an ontology concept.
Coordination Coordinating terms occur in a variety of
forms, e.g. in comma lists or with and and or leading
to head sharing. For example, abdomen soft, non-tender,
non-distended should give C0426663: [abdomen soft]
and C0424826: [abdomen non-distended]. Whilst short
forms and coordination are known issues that are handled
by state-of-the-art biomedical named entity recognition
pipelines, the lack of context in clinical reports and in par-
ticular the disjointed nature of some complex phenotypes
has not yet been adequately considered
Comparison with other ensemble approaches
Although there has been quite a lot published on the
subject of concept normalisation and a large body of lit-
erature on named entity recognition, there is relatively
little work on comparing and combining existing systems
in ensemble approaches. In particular, learn-to-rank is
a fairly recent technique for concept normalisation. To
the best of our knowledge, it has only been applied once
before by Leaman et al. [35] for diseases, a subset of the
semantic types that we test here. Leaman et al. report
promising results on a subset of the NCBI disease corpus
and, in fact, their system came first in the ShARE/CLEF
Task 1b.
Ensembles have though been used before for the recog-
nition of clinical concepts. Kang et al. [36] for example
employed dictionary and statistical pattern based tech-
niques on the 2010 I2B2 corpus of EPRs, for term recog-
nition (but not concept normalisation) achieving the third
level of performance in the shared task. Xia et al. [37]
show the effects of combining MetaMap and cTAKES for
the same ShARE/CLEF data we have shown here. Their
combination strategy is a simple rule-based approach that
accepts all outputs from the higher precision system and
then checks for conflicts in the output of the high recall
system before accepting new CUIs.
One line of investigation we want to pursue in future
work is to decouple the ranking of concepts from system
baskets, i.e. instead of treating the rank of a whole basket
of concepts as the target we provide individual concepts
for each system and then learn to rank these. This would
potentially allow us to better control for systems that are
strong on some concepts and weaker on others.
Limitations
All of the individual systems applied in our base study
were used without customization, e.g. training or special
post-processing rules. This is in contrast to the systems
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 10 of 12
Table 9 Learn to rank on test data
Learn to rank performance Individual system contribution
Top-K Strategy Model P R F1 M1 M2 M3 M4 M5 M6 M7 M8 M9
Top-1 Union SVMRank 0.1712 0.6426 0.2703 0.17 0.04 0.50 - - 0.01 0.00 - 0.28
ListNet 0.1271 0.6170 0.2108 0.65 0.04 0.26 0.05 - 0.00 - - -
RankNet 0.0923 0.5096 0.1562 1.00 - - - - - - - -
RankBoost 0.1408 0.6524 0.2316 0.43 0.05 0.50 - - 0.01 0.28 0.28 0.28
Oracle SVMRank 0.1712 0.6426 0.2703 0.17 0.04 0.50 - - 0.01 0.00 - 0.28
ListNet 0.1271 0.6170 0.2108 0.65 0.04 0.26 0.05 - 0.00 - - -
RankNet 0.0923 0.5096 0.1562 1.00 - - - - - - - -
RankBoost 0.1872 0.6504 0.2907 0.23 0.05 0.51 - - 0.01 0.20 0.00 -
Top-2 Union SVMRank 0.1244 0.6986 0.2112 0.51 0.07 0.62 - - 0.01 0.00 0.28 0.50
ListNet 0.1107 0.7109 0.1915 0.87 0.07 0.88 0.13 0.03 0.02 0.00 - -
RankNet 0.1070 0.7028 0.1857 1.00 - 1.00 - - - - - -
RankBoost 0.1188 0.7034 0.2032 0.53 0.09 0.62 0.01 0.01 0.28 0.76 0.76 0.76
Oracle SVMRank 0.2350 0.6869 0.3501 0.07 0.00 0.63 - - 0.00 0.00 - 0.29
ListNet 0.2534 0.6981 0.3718 0.14 0.00 0.77 0.06 0.02 0.00 0.00 - -
RankNet 0.2629 0.6905 0.3808 0.15 - 0.85 - - - - - -
RankBoost 0.2420 0.6908 0.3584 0.10 0.00 0.58 - 0.01 0.02 0.29 0.01 -
Top-3 Union SVMRank 0.1157 0.7081 0.1990 0.53 0.10 0.63 - 0.01 0.28 0.50 0.93
ListNet 0.1019 0.7287 0.1788 0.91 0.57 0.92 0.44 0.09 0.06 0.02 0.00 -
RankNet 0.1029 0.7045 0.1796 1.00 1.00 1.00 - - - - - -
RankBoost 0.1128 0.7109 0.1947 0.54 0.11 0.64 0.29 0.22 0.75 0.98 0.98 0.98
Oracle SVMRank 0.2444 0.6933 0.3615 0.06 0.00 0.59 - 0.01 0.00 - 0.33
ListNet 0.2773 0.7126 0.3993 0.11 0.00 0.72 0.12 0.04 0.01 0.00 - -
RankNet 0.2643 0.6914 0.3824 0.15 0.01 0.85 - - - - - -
RankBoost 0.2593 0.6956 0.3777 0.09 0.00 0.57 - 0.10 0.02 0.22 0.00 -
Macro precision, recall and F1 at different top K levels. The highest scoring system F1 for each level (both union and oracle strategies) is shown in bold. The table also
shows the individual contribution of the systems to the final score where italics scores indicate the highest contributing individual system(s) to each ensemble.
in the ShARE/CLEF 2013 shared task which usually
employed machine learning on the labelled target domain
data to detect relevant spans of text for named entities
and to filter the suggested concept identifiers so that they
were optimized for the detected spans. Both of these steps
led to substantial improvements on the results of the
uncustomized individual systems that we report here. We
believe that in particular the lack of a post-processing step
to filter concepts which did not directly appear in the text
or were overlapping with other concepts led to substan-
tially degraded precision than shared task participants.
For example we found that our individual systems sug-
gested many unannotated concepts related to the patient
such as date of birth, gender, age and history of illness
as well as generic concepts that were part of more spe-
cific ones. The best tuned system in the ShARE/CLEF
2013 Task 1 (named entity recognition and normaliza-
tion to SNOMED-CT at mention level) achieved an F1
of 0.75 for named entity recognition and an accuracy
score of 0.59 for harmonization using strict matching
criteria. Taken together with the F1 improvement we
observed in the ensemble approach, this finding rein-
forces the generally held view that domain tuning is a
necessary step to achieving high F1, even with relatively
mature concept recognition tools such as the ones we have
employed.
Our choice of sentence-level concept harmonisation
was motivated by a use-case where the user requires
extraction of concepts from the document, e.g. for docu-
ment or section classification, but does not require intra-
sentential relationships between concepts, e.g. for text
mining. The later would require mention-level harmon-
isation by the four individual systems but our previous
experiments [38] have again indicated the challenge of
attempting this without some form of tuning. In future
work we would like to look at expanding our approach to
exploit domain-adaptation methods, e.g. Latent Dirichlet
Allocation (LDA), on mention-level annotation to allow
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 11 of 12
Table 10 Type-based learn to rank on test data
ID Sys P R F1 ID Sys P R F1
T019 SVMRank 0.51 0.21 0.29 T047 SVMRank 0.52 0.66 0.58
ListNet 0.59 0.18 0.28 ListNet 0.41 0.65 0.50
RankNet 0.54 0.16 0.24 RankNet 0.38 0.51 0.44
RankBoost 0.47 0.21 0.29 RankBoost 0.45 0.68 0.54
T020 SVMRank 0.29 0.53 0.37 T048 SVMRank 0.48 0.68 0.56
ListNet 0.34 0.50 0.41 ListNet 0.40 0.49 0.44
RankNet 0.34 0.48 0.40 RankNet 0.38 0.48 0.43
RankBoost 0.29 0.55 0.38 RankBoost 0.44 0.68 0.54
T033 SVMRank 0.00 0.07 0.00 T184 SVMRank 0.52 0.62 0.57
ListNet 0.00 0.27 0.00 ListNet 0.44 0.61 0.51
RankNet 0.00 0.27 0.01 RankNet 0.40 0.58 0.47
RankBoost 0.00 0.20 0.00 RankBoost 0.48 0.62 0.54
T037 SVMRank 0.37 0.50 0.43 T190 SVMRank 0.28 0.69 0.40
ListNet 0.35 0.46 0.40 ListNet 0.28 0.65 0.39
RankNet 0.31 0.44 0.37 RankNet 0.25 0.55 0.34
RankBoost 0.35 0.49 0.41 RankBoost 0.28 0.68 0.39
T046 SVMRank 0.37 0.70 0.48 T191 SVMRank 0.27 0.54 0.36
ListNet 0.37 0.70 0.48 ListNet 0.23 0.34 0.27
RankNet 0.36 0.56 0.44 RankNet 0.17 0.24 0.20
RankBoost 0.35 0.70 0.47 RankBoost 0.25 0.53 0.34
Note that the highest scoring system F1 for each semantic type is shown in bold.
direct comparison with the techniques employed in
ShARE/CLEF 2013.
Conclusions
Clinical phenotype recognition is essential for interpret-
ing the evidence about human diseases in clinical records
and the scientific literature. In this paper, we have eval-
uated the F1 of four off-the-shelf concept recognition
systems for identifying some of the building blocks in
clinical phenotypes as well as disease-related concepts.
Future work will have to develop additional filters for
this purpose. Our investigation of LTR techniques has
clearly shown that the methods we adopted are supe-
rior to the off-the-shelf systems used separately but still
fall short of Oracle-based settings indicating that further
enhancements are required in either feature selection or
sampling.
The tests have been run on the open gold-standard
ShARE/CLEF corpus harmonised to UMLS semantic
types. Findings indicate that cTAKES performs well com-
pared to its peers but that annotation performance varies
widely across semantic types, and that MetaMap with
strict matching and word sense disambiguation can have
superior precision. We presented an approach using sev-
eral learn-to-rank methods that gave greatly improved
performance across semantic types. The best ensemble
- SVMRank - using the union tie-breaking strategy and
the oracle tie-breaking strategies achieved the Top-1 rank-
ing level on training data. The results on the test data
were similar with both tie-breaking strategies at Top-1
ranking.
The results indicate the continued challenge of con-
cept annotation and, in particular, the need to consider
the grammatical relations within phenotypementions.We
have not yet tested the effectiveness of these approaches
in an operational setting, e.g. for speed of processing
or stability. We would like to extend our approach on
further clinical benchmark data sets as they become avail-
able in order to understand better the relative merits
of external feature sets such as FB3 and FB4. In the
immediate future, we plan on continuing to improve our
approach by extending the distributed feature representa-
tion employed in the meta-classifier, e.g. with LDA, and
by exploring additional ways of sampling and combining
system outputs.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
NC, AO and TG formulated the experimental setup. AO and TG performed the
experiments. NC, AO and TG interpreted the results. NC, AO and TG wrote the
manuscript. All authors read and approved the final manuscript.
Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 12 of 12
Acknowledgements
We gratefully acknowledge the kind permission of the ShARE/CLEF eHealth
evaluation organisers for facilitating access to the ShARE/CLEF eHealth corpus
used in our evaluation. Also we thank the anonymous reviewers for their kind
contribution to improving the final version of this paper. Nigel Colliers
research is supported by the European Commission through the Marie Curie
International Incoming Fellowship (IIF) programme (Project: Phenominer, Ref:
301806). Tudor Grozas research is funded by the Australian Research Council
(ARC) Discovery Early Career Researcher Award (DECRA)  DE120100508.
Author details
1University of Cambridge, Cambridge, UK. 2European Bioinformatics Institute
(EMBL-EBI), Cambridge, UK. 3Wellcome Trust Sanger Institute, Cambridge, UK.
4School of ITEE, the University of Queensland, St. Lucia, Australia. 5Garvan
Institute of Medical Research, Darlinghurst, Sydney, Australia.
Received: 4 November 2014 Accepted: 1 April 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 
DOI 10.1186/s13326-015-0002-8
RESEARCH ARTICLE Open Access
Completing the is-a structure in light-weight
ontologies
Patrick Lambrix1,2*, Fang Wei-Kleiner1 and Zlatan Dragisic1,2
Abstract
Background: With the increasing presence of biomedical data sources on the Internet more and more research
effort is put into finding possible ways for integrating and searching such often heterogeneous sources. Ontologies
are a key technology in this effort. However, developing ontologies is not an easy task and often the resulting
ontologies are not complete. In addition to being problematic for the correct modelling of a domain, such
incomplete ontologies, when used in semantically-enabled applications, can lead to valid conclusions being missed.
Results: We consider the problem of repairing missing is-a relations in ontologies. We formalize the problem as a
generalized TBox abduction problem. Based on this abduction framework, we present complexity results for the
existence, relevance and necessity decision problems for the generalized TBox abduction problem with and without
some specific preference relations for ontologies that can be represented using a member of the EL family of
description logics. Further, we present algorithms for finding solutions, a system as well as experiments.
Conclusions: Semantically-enabled applications need high quality ontologies and one key aspect is their
completeness. We have introduced a framework and system that provides an environment for supporting domain
experts to complete the is-a structure of ontologies. We have shown the usefulness of the approach in different
experiments. For the two Anatomy ontologies from the Ontology Alignment Evaluation Initiative, we repaired 94 and
58 initial given missing is-a relations, respectively, and detected and repaired additionally, 47 and 10 missing is-a
relations. In an experiment with BioTop without given missing is-a relations, we detected and repaired 40 newmissing
is-a relations.
Keywords: Ontologies, Ontology engineering, Ontology debugging
Background
With the increasing presence of biomedical data sources
on the Internet more and more research effort is put into
finding possible ways for integrating and searching such
often heterogeneous sources. Semantic Web technologies
such as ontologies, are becoming a key technology in
this effort. Ontologies provide a means for modelling the
domain of interest and they allow for information reuse,
portability and sharing across multiple platforms. Efforts
such as the Open Biological and Biomedical Ontologies
(OBO) Foundry [1], BioPortal [2] and Unified Medical
Language System (UMLS) [3] aim at providing reposi-
tories for biomedical ontologies and relations between
*Correspondence: patrick.lambrix@liu.se
1Department of Computer and Information Science, Linköping University,
Linköping, Sweden
2Swedish e-Science Research Centre, Linköping University, Linköping, Sweden
these ontologies thus providing means for annotating and
sharing biomedical data sources. Many of the ontolo-
gies in the biomedical domain, e.g., SNOMED [4] and
Gene Ontology [5], are, regarding knowledge represen-
tation, light-weight ontologies. They are taxonomies or
can be represented using the EL description logic or
small extensions thereof (e.g. [6] and the TONES Ontol-
ogy Repository [7])a. Therefore, in this paper, we consider
ontologies that are represented by TBoxes in the EL fam-
ily, which consist of axioms such as Carditis  Fracture,
with the intended meaning that Carditis is a Fracture,
where Carditis and Fracture are concepts and the relation-
ship is an is-a relation. (For detailed syntax see Section
Preliminaries). A set of such terminological axioms is a
TBox.
Developing ontologies is not an easy task and often the
resulting ontologies (including their is-a structures) are
© 2015 Lambrix et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 2 of 26
not complete. In addition to being problematic for the cor-
rect modelling of a domain, such incomplete ontologies
also influence the quality of semantically-enabled applica-
tions. Incomplete ontologies when used in semantically-
enabled applications can lead to valid conclusions being
missed. For instance, in ontology-based search, queries
are refined and expanded by moving up and down the
hierarchy of concepts. Incomplete structure in ontologies
influences the quality of the search results. As an exam-
ple, suppose we want to find articles in PubMed [8] using
the MeSH [9] term Scleral Disease. By default the query
will follow the hierarchy of MeSH and include more spe-
cific terms for searching, such as Scleritis. If the relation
between Scleral Disease and Scleritis is missing in MeSH,
we will miss 922 articles in the search result, which is
about 57% of the original resultb. The structural informa-
tion is also important information in ontology engineering
research. For instance, most current ontology alignment
systems use structure-based strategies to find mappings
between the terms in different ontologies (e.g. overview
in [10]) and the modeling defects in the structure of the
ontologies have an important influence on the quality of
the ontology alignment results.
In this paper we tackle the problem of completing the
is-a structure of ontologies. Completing the is-a structure
requires adding new correct is-a relations to the ontology.
We identify two cases for finding relations which need
to be added to an ontology. In case 1 missing is-a rela-
tions have been detected and the task is to find ways of
making these detected is-a relations derivable in the ontol-
ogy. There are many approaches to detect missing is-a
relations, e.g., in ontology learning [11] or evolution [12],
using linguistic [13] and logical [14,15] patterns, by using
knowledge intrinsic to an ontology network [16-21], or by
using machine learning and statistical methods [22-26].
However, in general, these approaches do not detect all
missing is-a relations and in several cases even only few.
Therefore, we assume that we have obtained a set of miss-
ing is-a relations for a given ontology (but not necessarily
all). In the case where our set of missing is-a relations con-
tains all missing is-a relations, completing the ontology is
easy. We just add all missing is-a relations to the ontol-
ogy and a reasoner can compute all logical consequences.
However, when the set of missing is-a relations does not
contain all missing is-a relations - and this is the common
case - there are different ways to complete the ontology.
The easiest way is still to just add the missing is-a relations
to the ontology. For instance, T in Figure 1 (and Figure 2)
represents a small ontology inspired by Galen ontology
(http://www.openclinical.org/prj_galen.html), that is rele-
vant for our discussions. Assume that we have detected
that Endocarditis  PathologicalPhenomenon and Gran-
ulomaProcessNonNormalProcess are missing is-a rela-
tions (M in Figure 1). Obviously, adding these relations to
the ontology will repair the missing is-a structure. How-
ever, there are other more interesting possibilities. For
instance, adding Carditis  CardioVascularDisease and
GranulomaProcess  PathologicalProcess also repairs the
missing is-a structure. Further, these is-a relations are
correct according to the domain and constitute new is-
a relations (e.g. Carditis  CardioVascularDisease) that
were not derivable from the ontology and not originally
detected by the detection algorithmc. We also note that
from a logical point of view, adding Carditis  Fracture
andGranulomaProcessNonNormalProcess also repairs
the missing is-a structure. However, from the point of
view of the domain, this solution is not correct. Therefore,
as it is the case for all approaches for dealing with model-
ing defects, a domain expert needs to validate the logical
solutions.
In case 2 no missing is-a relations are given. In this
case we investigate existing is-a relations in the ontology
and try to find new ways of deriving these existing is-a
relations. This might pinpoint to the necessity of adding
new missing is-a relations to the ontology. As an exam-
ple, let us assume that our ontology contains relations
T ? M in Figure 1. If we assume now that we want
to investigate new ways of deriving relations in M then
obviously adding Carditis  CardioVascularDisease and
GranulomaProcess  PathologicalProcess would be one
possibility given that both are correct according to the
domain.
The basic problem underlying the two cases can be
formalized in the same way as a new kind of abduc-
tion problem (formal definitions in Section Abduction
framework). Abduction is a reasoning method to gen-
erate explanations for observed symptoms and mani-
festations. When the application domain is described
by a logical theory, it is called logic-based abduction
[27]. Logic-based abduction is widely applied in diagno-
sis, planning, and database updates [28], among others.
Further, as we have seen above, there may be differ-
ent ways to complete the is-a structure of ontologies.
Therefore, we propose two preference criteria on the solu-
tions for this new abduction problem as well as different
ways to combine them and conduct complexity analy-
sis on important decision problems regarding the various
preference criteria for ontologies represented using EL
or EL++.
The contributions of this paper are the following.
 We formalize the repairing of the missing is-a
structure in an ontology as a generalized version of
the TBox abduction problem (GTAP).
 We present complexity results for the existence,
relevance and necessity decision problems for GTAP
in ontologies represented in EL and EL++ with and
without the preference relations subset minimality
Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 3 of 26
Figure 1 Small EL example. (C is the set of atomic concepts in the ontology. T is a TBox representing the ontology. M is a set of missing is-a
relations. Or is the oracle representing the domain expert).
and semantic maximality as well as three ways of
combining these (maxmin, minmax, skyline). Subset
minimality is a preference criterion that is often used
in abductive reasoning problems. Semantic
maximality is a new criterion that is important for
GTAP.
 We provide algorithms for finding a skyline optimal
solution to GTAP in ontologies represented in EL
and EL++. Although in theory, maxmin optimal
solutions are normally preferred, in practice, they
cannot be guaranteed and skyline optimal solutions
are the best we can do.
 We provide a system and show its usefulness through
experiments.
Methods
Preliminaries - description logics EL and EL++
Description logics are knowledge representation lan-
guages. In description logics concept descriptions are
constructed inductively from a set NC of atomic concepts
and a set NR of atomic roles and (possibly) a set NI of
individual names. The concept constructors for EL++ are
the top concept , the bottom concept ?, nominals, con-
junction, existential restriction and a restricted form of
concrete domains. In this paper, we consider the version
of EL++ without concrete domains. Note that this simpli-
fication does not affect the complexity results presented
later on. For the syntax of the different constructors see
Table 1.
Figure 2 Graphical representation of the EL example in Figure 1. (Ovals represent concepts. Full arrows represent is-a relations between concepts
in the ontology. Dashed arrows represent missing is-a relations).
Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 4 of 26
Table 1 EL++ syntax and semantics
Name Syntax Semantics
Top  I
Bottom ? ?
Nominal {a} {aI }
Conjunction C  D CI ? DI
Existential ?r.C {x ? I |?y ? I :
restriction (x, y) ? rI ? y ? CI}
GCI CD CI ? DI
RI r1 ? . . . ? rkr rI1 ? . . . ? rIk ? rI
An interpretation I consists of a non-empty set I and
an interpretation function ·I which assigns to each atomic
concept A ? NC a subset AI ? I , to each atomic role
r ? NR a relation rI ? I × I , and to each individ-
ual name a ? NI an element aI ? I . The interpretation
function is straightforwardly extended to complex con-
cepts. An EL++ TBox (named CBox in [6]) is a finite set of
general concept inclusions (GCIs) and role inclusions (RIs)
whose syntax can be found in the lower part of Table 1.
Note that a finite set of GCIs is called a general TBox. An
interpretation I is a model of a TBox T if for each GCI
and RI in T , the conditions given in the third column of
Table 1 are satisfied.
EL has the restricted form of EL++ which allows
for concept constructors of top concept , conjunction
and existential restriction. An EL TBox contains only
GCIs.
The main reasoning task for description logics is sub-
sumption in which the problem is to decide for a TBox
T and concepts C and D whether T |= CD. Sub-
sumption in EL++ is polynomial even w.r.t. general
TBoxes [6].
Abduction framework
In the following we explain how the problem of find-
ing possible ways to repair the missing is-a structure in
a ontology is formalized as a generalized version of the
TBox abduction problem as defined in [29]. We assume
that our ontology is represented using a TBox T in a lan-
guage L which in this paper is EL or EL++. Further, we
have a set of missing is-a relations which are represented
by a set M of atomic concept subsumptions. In case 1
in Section Background, these missing is-a relations were
detected. In case 2 the elements in M are existing is-a
relations in the ontology that are temporarily removed,
and T represents the ontology that is obtained by remov-
ing the elements in M from the original ontology. (They
can later be added again after completing the ontology.)
To complete the is-a structure of an ontology, the ontol-
ogy should be extended with a set S of atomic concept
subsumptions (repair) such that the extended ontology is
consistent and entails the missing is-a relations. However,
the added atomic concept subsumptions should be correct
according to the domain. In general, the set of all atomic
concept subsumptions that are correct according to the
domain are not known beforehand. Indeed, if this set were
given then we would only have to add this to the ontol-
ogy. The common case, however, is that we do not have
this set, but instead can rely on a domain expert that can
decide whether an atomic concept subsumption is correct
according to the domain. In our formalization the domain
expert is represented by an oracle Or that when given an
atomic concept subsumption, returns true or false. It is
then required that for every atomic concept subsumption
s ? S, we have that Or(s) = true. The following definition
formalizes this.
Definition 1. (GENERALIZED TBOXABDUCTION) Let
T be a TBox in language L and C be the set of all atomic
concepts in T. Let M = {Ai  Bi}ni=1 with Ai,Bi ? C be a
finite set of TBox assertions. Let Or : {Ci Di | Ci,Di?C} ?
{true, false}. A solution to the generalized TBox abduction
problem (GTAP) (T ,C,Or,M) is any finite set of TBox
assertions S = {Ei  Fi}ki=1 such that ?Ei, Fi : Ei, Fi ? C,?Ei, Fi : Or(Ei  Fi) = true, T ? S is consistent and
T ? S |= M. The set of all such solutions is denoted as
S(T ,C,Or,M).
As an example, consider GTAP P as defined in Figure 1.
Then {Carditis  CardioVascularDisease, Inflammation-
Process  PathologicalProcess, GranulomaProcess 
InflammationProcess} is a solution for P . Another
solution is {Carditis  CardioVascularDisease, Granu-
lomaProcess  PathologicalProcess} as shown in Section
Background.
There can be many solutions for a GTAP and, as
explained in Section Background, not all solutions are
equally interesting. Therefore, we propose two preference
criteria on the solutions as well as different ways to com-
bine them. The first criterion is a criterion that is not
used in other abduction problems, but that is particularly
important for GTAP. In GTAP it is important to find solu-
tions that add to the ontology as much information as
possible that is correct according to the domain. There-
fore, the first criterion prefers solutions that imply more
information.
Definition 2. (MORE INFORMATIVE) Let S and S? be
two solutions to the GTAP (T ,C,Or,M). S is said to be
more informative than S? iff T ? S |= S? and T ? S? |= S.
Further, we say that S is equally informative as S? iff T ?
S |= S? and T ? S? |= S.
Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 5 of 26
Consider two solutions to P , S1 = {Inflammation-
Process  PathologicalProcess, GranulomaProcess 
InflammationProcess}d and S2 = {InflammationProcess
 PathologicalProcess, GranulomaProcess  Pathologi-
calProcess}. In this case solution S1 is more informative
than S2.
Definition 3. (SEMANTIC MAXIMALITY) A solution S
to the GTAP (T ,C,Or,M) is said to be semantically max-
imal iff there is no solution S? which is more informative
than S. The set of all semantically maximal solutions is
denoted as Smax(T ,C,Or,M).
An example of a semantically maximal solution to P
is {InflammationProcess  PathologicalProcess, Granu-
lomaProcess  InflammationProcess, Carditis  Cardio-
VascularDisease}.
The second criterion is a classical criterion in abduc-
tion problems. It requires that no element in a solution is
redundant.
Definition 4. (SUBSET MINIMALITY) A solution S to
the GTAP (T ,C,Or,M) is said to be subset minimal iff
there is no proper subset S?  S such that S? is a solu-
tion. The set of all subset minimal solutions is denoted as
Smin(T ,C,Or,M).
An example of a subset minimal solution for P
is {InflammationProcess  PathologicalProcess, Granu-
lomaProcess InflammationProcess}. On the other hand,
solution {Carditis  CardioVascularDisease, Inflamma-
tionProcess  PathologicalProcess, GranulomaProcess 
InflammationProcess} is not subset minimal as it contains
Carditis  CardioVascularDisease which is redundant for
repairing the missing is-a relations.
In practice, both of the above two criteria are desir-
able. We therefore define ways to combine these criteria
depending on what kind of priority we assign for the single
JOURNAL OF
BIOMEDICAL SEMANTICS
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 
DOI 10.1186/s13326-015-0004-6RESEARCH ARTICLE Open AccessSupporting the annotation of chronic obstructive
pulmonary disease (COPD) phenotypes with text
mining workflows
Xiao Fu1*, Riza Batista-Navarro1,2, Rafal Rak1 and Sophia Ananiadou1Abstract
Background: Chronic obstructive pulmonary disease (COPD) is a life-threatening lung disorder whose recent
prevalence has led to an increasing burden on public healthcare. Phenotypic information in electronic clinical
records is essential in providing suitable personalised treatment to patients with COPD. However, as phenotypes are
often hidden within free text in clinical records, clinicians could benefit from text mining systems that facilitate
their prompt recognition. This paper reports on a semi-automatic methodology for producing a corpus that can
ultimately support the development of text mining tools that, in turn, will expedite the process of identifying
groups of COPD patients.
Methods: A corpus of 30 full-text papers was formed based on selection criteria informed by the expertise of COPD
specialists. We developed an annotation scheme that is aimed at producing fine-grained, expressive and
computable COPD annotations without burdening our curators with a highly complicated task. This was
implemented in the Argo platform by means of a semi-automatic annotation workflow that integrates several text
mining tools, including a graphical user interface for marking up documents.
Results: When evaluated using gold standard (i.e., manually validated) annotations, the semi-automatic workflow
was shown to obtain a micro-averaged F-score of 45.70% (with relaxed matching). Utilising the gold standard data
to train new concept recognisers, we demonstrated that our corpus, although still a work in progress, can foster
the development of significantly better performing COPD phenotype extractors.
Conclusions: We describe in this work the means by which we aim to eventually support the process of COPD
phenotype curation, i.e., by the application of various text mining tools integrated into an annotation workflow.
Although the corpus being described is still under development, our results thus far are encouraging and show
great potential in stimulating the development of further automatic COPD phenotype extractors.
Keywords: Corpus annotation, Phenotype curation, Automatic annotation workflows, Ontology linking, Corpora for
clinical text mining, Chronic obstructive pulmonary diseaseBackground
An umbrella term for a range of lung abnormalities,
chronic obstructive pulmonary disease (COPD) pertains to
medical conditions in which airflow from the lungs is re-
peatedly impeded. This life-threatening disease, known to
be primarily caused by tobacco smoke, is not completely* Correspondence: xiao.fu-2@manchester.ac.uk
Equal contributors
1National Centre for Text Mining, School of Computer Science, University of
Manchester, Manchester Institute of Biotechnology, 131 Princess Street,
Manchester, UK
Full list of author information is available at the end of the article
© 2015 Fu et al.; licensee BioMed Central. This
Attribution License (http://creativecommons.o
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.reversible and is incurable. COPD was ranked by the
World Health Organization as the fifth leading cause of
death worldwide in 2002, and is predicted to become the
third by year 2030. Estimates have also shown that the
mortality rate for COPD could escalate by at least 30%
within the next decade if preventive measures are not im-
plemented [1].
The disease and clinical manifestations of COPD are
heterogeneous and widely vary from one patient to
another. As such, its treatment needs to be highly perso-
nalised in order to ensure that the most suitable therapyis an Open Access article distributed under the terms of the Creative Commons
rg/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 2 of 11is provided to a patient. COPD phenotyping allows for
well-defined grouping of patients according to their
prognostic and therapeutic characteristics, and thus in-
forms the development and provision of personalised
therapy [2].
The primary approach to recording phenotypic infor-
mation is by means of electronic clinical records [3].
However, as clinicians at the point of care use free text
in describing phenotypes, such information can easily
become obscured and inaccessible [4]. In order to ex-
pedite the process of identifying a given patients COPD
group, the phenotypic information locked away within
these records needs to be automatically extracted and
distilled for the clinicians perusal.
Capable of automatically distilling information expressed
in natural language within documents, text mining can be
applied on clinical records in order to efficiently extract
COPD phenotypes of interest. However, the development
of sophisticated text mining tools is reliant on the availabil-
ity of gold standard annotated corpora, which serve as
evaluation data as well as provide samples for training ma-
chine learning-based approaches.
This paper presents our ongoing efforts on the annota-
tion of COPD phenotypes in a collection of scientific pa-
pers. In our previous publication [5] on which this work
is built upon, we proposed to form a corpus of clinical
records from the Multiparameter Intelligent Monitoring
in Intensive Care II (MIMIC II) Clinical Database [6,7].
However, our UK-based expert collaborators (i.e., stake-
holders who will incorporate our text mining technology
into their systems in the near future) recently pointed
out that there are substantial discrepancies between the
hospital system in the US (on which MIMIC II is fo-
cussed) and that in the UK. After considering their ad-
vice, we decided to utilise scientific articles from various
COPD-relevant journals, rather than build a corpus of
clinical records which are highly US-specific. As previ-
ous work demonstrated techniques which successfully
extracted information from unseen data even if the
training/development data used was of a different docu-
ment type [8], we believe that a gold standard corpus of
full scientific articles should still allow for the develop-
ment of phenotype extraction tools for clinical records.
Nevertheless, our collaborators are still currently work-
ing on obtaining a subset of clinical records from their
own hospital, which will also be annotated to become
part of an augmented version of our corpus.
In embarking on this effort, we are building a resource
that will support the development of text mining methods
for the automatic extraction of COPD phenotypes from
free text. We envisage that such methods will ultimately
foster the development of applications which will enable
point-of-care clinicians to more easily and confidently
identify a given COPD patients group, potentially leadingto the provision of the most appropriate personalised
treatment. Furthermore, text mining methods can be
employed in order to facilitate the linking of COPD phe-
notypes with genotypic information contained in pub-
lished scientific literature.
In the remainder of this paper, we firstly provide a re-
view of the state of the art (Related Work). We proceed
to describing our methods for corpus development
(Methods), including our strategy for document selec-
tion followed by our proposed annotation scheme. A
discussion of our text mining-assisted annotation work-
flow is also provided. We then share the results and ana-
lysis of our evaluation (Results and Discussion). Lastly,
we conclude the paper with a summary of our contribu-
tions and an overview of ongoing and future work.
Related work
Various corpora have been constructed to support the
development of clinical natural language processing
(NLP) methods. Some contain annotations formed on
the basis of document-level tags indicating the specific
diseases that clinical reports pertain to. In the 2007
Computational Medicine Challenge data set [9], radi-
ology reports were assigned codes from the ninth
revision of the International Classification of Diseases-
Clinical Modification (ICD-9-CM) terminology [10]. In
similar corpora, chest X-ray reports were manually la-
belled with any of four pneumonia-related concepts [11]
whilst any of 80 possible disease names were assigned to
documents in another collection of clinical records [12]
with the assistance of automatic tools MetaMap Transfer
(MMTx) [13] for concept recognition and NegEx [14]
for negation detection. Whilst suitable for evaluating in-
formation retrieval methods, such document-level anno-
tations cannot sufficiently support the extraction of
phenotypic concepts which are described in clinical re-
cords in largely variable ways, making it necessary for
automated methods to perform analysis by looking at
their actual mentions within text.
Several other clinical corpora were thus enriched with
text-bound annotations, which serve as indicators of
specific locations of phenotypic concept mentions within
text. For instance, all mentions of signs or symptoms,
medications and procedures relevant to inflammatory
bowel disease were marked up in the corpus developed
by South et al [15]. Specific mentions of diseases and
signs or symptoms were similarly annotated under the
ShARe scheme [16,17] and additionally linked to terms
in the SNOMED Clinical Terms vocabulary [18]. Whilst
the scheme developed by [19] had similar specifications,
it is unique in terms of its employment of an automatic
tool to accelerate the annotation process. One difficulty
encountered by annotators following such scheme, how-
ever, is with manually mapping mentions of phenotypic
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 3 of 11concepts to vocabulary terms, owing to the high degree
of variability in which these concepts are expressed in
text. For instance, many signs or symptoms (e.g., gradual
progressive breathlessness), cannot be fully mapped to
any of the existing terms in vocabularies.
Alleviating this issue are schemes which were designed
to enrich corpora with finer-grained text-bound annota-
tions. The Clinical e-Science Framework (CLEF) annota-
tion scheme [20] which defined several clinical concept
types and relationships, required the decomposition of
phrases into their constituent concepts which were then
individually assigned concept type labels and linked
using any of their defined relationships. Also based on a
fine-grained annotation approach is the work by Mun-
gall et al. [21] on the ontology-driven annotation of
inter-species phenotypic information based on the EQ
model [22]. Although their work was carried out with
the help of the Phenote software [23] for storing, man-
aging and visualising annotations, the entire curation
process was done manually, i.e., without the support of
any NLP tools. The effort we have undertaken, in con-
trast, can be considered as a step towards automating
such EQ model-based fine-grained annotation of pheno-
typic information.
In this regard, our work is unique amongst annotation
efforts within the clinical NLP community, but shares
similarities with some phenotype curation pipelines
employed in the domain of biological systematics. Cura-
tors of the Phenoscape project [24] manually link
EQ-encoded phenotypes of fishes to the Zebrafish Model
Organism Database using Phenex [25] which is a tool for
managing character-by-taxon matrices, a formal approach
used by evolutionary biologists. To accelerate this process,
Phenex has been recently enhanced with NLP capabilities
[26] upon the integration of a text analytic known as
CharaParser [27]. Based on a combination of bootstrap-
ping and syntactic parsing approaches [28], CharaParser
can automatically annotate structured characteristics of
organisms (i.e., phenotypes) in text, but currently does not
have full support for linking concepts to ontologies [29].
Also facilitating the semi-automatic curation of systemat-
ics literature is GoldenGATE [30], a stand-alone applica-
tion modelled after the GATE framework [31], which
allows for the combination of various NLP tools into text
processing pipelines. It is functionally similar to our Web-
based annotation platform Argo [32] in terms of its
support for NLP workflow management and manual valid-
ation of automatically generated annotations. However,
the latter fosters interoperability to a higher degree by
conforming to the industry-supported Unstructured Man-
agement Information Architecture [33] and allowing
workflows to be invoked as Web services [34].
By producing our proposed fine-grained phenotype
annotations which are linked to ontological concepts, weare representing them in a computable form thus mak-
ing them suitable for computational applications such as
inferencing and semantic search. The Phenomizer tool
[35], for instance, has demonstrated the benefits of en-
coding phenotypic information in a computable format.
Leveraging the Human Phenotype Ontology (HPO) [36]
whose terms are linked to diseases in the Online Men-
delian Inheritance in Man (OMIM) vocabulary [37], it
supports clinicians in making diagnoses by semantically
searching for the medical condition that best matches
the HPO signs or symptoms given in a query. We envis-
age that such an application, when integrated with a re-
pository of phenotypes and corresponding clinical
recommendations, e.g., Phenotype Portal [38] and the
Phenotype KnowledgeBase [39], can ultimately assist
point-of-care clinicians in more confidently providing
personalised treatment to patients. Our work on the an-
notation of COPD phenotypes aims to support the de-
velopment of similar applications in the future.Methods
We describe in this section our strategies for collecting
documents for the corpus and our proposed annotation
scheme. We also elaborate on the technology behind our
text mining-assisted annotation methodology.Document selection
In forming our corpus, we collected pertinent journal ar-
ticles from the PubMed Central Open Access subset
(PMC OA). As a preliminary step, we retrieved a list of
journals which are most relevant to COPD by querying
PMC OA using the keywords chronic, obstructive,
pulmonary, disease, respiratory and lung. This re-
sulted in ten journal titles whose archives were then
searched for the keywords chronic obstructive pulmon-
ary disease and COPD. A total of 974 full-text articles
were retrieved in this manner. The journal titles and art-
icle distribution over them are shown in Figure 1.
Upon consideration of our constraints in terms of re-
sources such as time and personnel, we decided to trim
down the document set to 30 full articles. This was carried
out by compiling a list of COPD phenotypes based on the
combination of terms given by our domain experts and
those automatically extracted by Termine [40] from the
COPD guidelines published jointly by the American Thor-
acic Society and the European Respiratory Society in 2004
[41]. The resulting term list (provided as Additional file 1)
contains 1,925 COPD phenotypes which were matched
against the content of the initial set of 974 articles. In order
to ensure that the documents in our corpus is representa-
tive of the widest possible range of COPD phenotypes, we
ranked the documents according to decreasing number of
their contained unique matches. We then selected the 30
246
13
573
39
62
2
39
BMC Pulmonary Medicine
Clinical Medicine Insights.
Circulatory, Respiratory and
Pulmonary Medicine
International Journal of
Chronic Obstructive
Pulmonary Disease
Pulmonary Circulation
Pulmonary Medicine
Heart, Lung and Vessels
Preventing Chronic Disease
Figure 1 Distribution of COPD-relevant articles over COPD-focussed journals. A total of 974 full-text articles were retrieved from 10 journals
in the PubMed OpenAccess subset.
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 4 of 11top-ranked articles as the final document set for our
corpus.
A simple yet expressive annotation scheme
To capture and represent phenotypic information, we
developed a typology of clinical concepts (Table 1) tak-
ing inspiration from the definition of COPD phenotypes
previously proposed [2], i.e., a single or combination of
disease attributes that describe differences between indi-
viduals with COPD as they relate to clinically meaning-
ful outcomes (symptoms, exacerbations, response to
therapy, rate of disease progression, or death). After
reviewing the semantic representations used in previous
clinical annotation efforts, we decided to adapt andTable 1 The proposed typology for capturing COPD phenotyp
Type Description
1) Problem an overall category for any COPD indications of
a) MedicalCondition* any disease or medical condition; includes COP
comorbidities
b) RiskFactor* a phenotype signifying a patients increased ch
having COPD
i) SignOrSymptom* an observable irregularity manifested by a COP
ii) IndividualBehaviour* a patients habits leading to susceptibility of ha
iii) TestOrMeasureResult* findings based on COPD-relevant examinations
2) Treatment any medication, therapy or program for treating
3) TestOrMeasure an overall category for any COPD-relevant exam
or measures/parameters
a) RadiologicalTest any of the radiological tests for detecting COPD
b) MicrobiologicalTest an examination of a COPD- relevant specimen
c) PhysiologicalTest a measurement of a COPD patients capacity to
Types marked with an asterisk (*) were adapted from the PhenoCHF scheme.harmonise concept types from the annotation schemes
applied to the 2010 i2b2/VA Shared Task data set [42]
and the PhenoCHF corpus [43]. In the former, concepts
of interest were categorised into broad types of problem,
treatment and test/measure. However, it was determined
upon consultation with clinical experts that a finer-
grained typology is necessary to better capture COPD
phenotypes. For this, we looked into the semantic types
used in the annotation of phenotypes for congestive
heart failure in the PhenoCHF corpus, which are fine-
grained yet generic enough to be applied to other med-
ical conditions. We adapted some of those types and
organised them under the upper-level types of the i2b2/
VA scheme.es
Example(s)
concern frequent exacerbator
D emphysema, pulmonary vascular disease, asthma,
congestive heart failure
ances of increased levels of the c-reactive protein, alpha1 antitrypsin
deficiency
D patient chronic cough, shortness of breath, purulent sputum
production
ving COPD smoking for 25 years
increased white blood cell counts, FEV1 45% predicted
COPD oxygen therapy, pulmonary rehabilitation, pursed lips
breathing
inations increased compliance of the lung, FEV1, FEV1/FVC ratio
computed tomography scanning, high resolution computed
tomography
complete blood count
exercise 6-min walking distance
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 5 of 11Most phenotypes exemplified in Table 2 span full
phrases, especially in the case of risk factors such as in-
creased compliance of the lung, chronic airways obstruc-
tion and increased levels of the c-reactive protein. Some
of the previously published schemes for annotating clin-
ical text have proposed the encoding of phenotypes
using highly structured, expressive representations. For
the symptom expressed as chronic airways obstruction,
for example, the CLEF annotation scheme [20] recom-
mends its annotation to consist of a has_location rela-
tionship between chronic obstruction (a condition) and
airways (locus). The EQ model for representing pheno-
types [21], similarly, would decompose this phenotype
into the following elements: airways as entity (E) and
chronic obstruction as quality (Q). Whilst we recognise
that such granular representations are ideal for the pur-
poses of knowledge representation and automated
knowledge inference, we feel that requiring them as part
of the manual annotation of free-text documents signifi-
cantly complicates the task for domain experts who may
lack the necessary background in linguistics.
We therefore propose an annotation methodology that
strikes a balance between simplicity and granularity of
annotations. On the one hand, our scheme renders the
annotation task highly intuitive by asking for only simple
text span selections, and not requiring the creation of
relations nor the filling in of template slots. On the other
hand, we also introduce granularity into the annotations
by exploiting various semantic analytic tools, described
in the next section, which automatically identify con-
stituent ontological concepts. The contribution of apply-
ing automated concept identifiers is two-fold. Firstly,
automatic concept identification as a pre-annotation
step helps accelerate the manual annotation process by
supplying visual cues to the annotators. For instance, the
symptom expressed within text as increased resistance of
the small airways becomes easier for an annotator toTable 2 Examples of phenotypic information represented usin
COPD Phenotypes Automatically recognized
underlying concepts
chronic airways obstruction chronic airways obstruction
parenchymal destruction parenchymal destruction
decrease in rate of lung function decrease in rate lung function
chronic bronchitis N/A
myocardial infarction N/A
enhanced response to inhaled
corticosteroids
enhanced response to corticostero
FEV1 45% predicted FEV1
alpha1 antitrypsin deficiency alpha1 antitrypsin deficiencyrecognise, seeing that the elementary concepts resistance
and airways have been pre-annotated. Secondly, as the
constituent concepts will be linked to pertinent ontol-
ogies, the semantics of the expression signifying the
symptom, which will be manually annotated as a simple
text span, is nevertheless encoded in a fine-grained and
computable manner. Shown in Table 2 are some exam-
ples of annotated phenotypes resulting from the applica-
tion of our scheme.
Text mining-assisted annotation with Argo
Our proposed methodology employs a number of text
analytics to realise its aims of reducing the manual effort
required from annotators and providing granular com-
putable annotations of COPD phenotypes. After analys-
ing several documents, we established that treatments
are often composed of drug names (e.g., Coumadin in
Coumadin dosing) whilst problems typically contain
mentions of diseases/medical conditions (e.g., myocar-
dial infarction), anatomical concepts (e.g., airways in
chronic airways obstruction), proteins (e.g., alpha1 anti-
trypsin in alpha1 antitrypsin deficiency), qualities (e.g.,
destruction in parenchymal destruction) and tests (e.g.,
FEV1 in FEV1 45% predicted). These observations, con-
firmed by COPD experts, guided us in selecting the
automatic tools for recognising the above-mentioned
types and for linking them to relevant ontologies.
We used Argo [32], an interoperable Web-based text
mining platform, to both integrate our elementary analytics
into a processing workflow and to manage its execution.
Argos rich library of processing components gives its users
access to various text analytics ranging from data readers
and writers to syntactic tools and concept recognisers.
From these, we selected the components which are most
suitable for our tasks requirements, and arranged them in
a multi-branch automatic annotation workflow, depicted
in Figure 2. The workflow begins with a Document Readerg our proposed annotation scheme
Automatically linked ontological concepts
chronic (PATO:0001863) respiratory airway (UBERON:0001005)
obstructed (PATO:0000648)
parenchyma (UBERON:0000353) damaged (PATO:0001167)
decreased rate (PATO:0000911) lung (UBERON:0002048)
function (PATO:0000173)
chronic bronchitis (DOID:6132)
myocardial infarction (DOID:5844)
ids enhanced (PATO:0001589) response to (PATO:0000077)
corticosteroid (ChEBI:50858)
Forced Expiratory Volume 1 Test (NCIT:C38084)
alpha-1-antitrypsin (PR:000014678) decreased amount
(PATO:0001997)
Figure 2 Our semi-automatic annotation workflow in Argo.
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 6 of 11that reads the records from our corpus, followed by the
Cafetiere Sentence Splitter which detects sentence bound-
aries. Resulting sentences are then segmented into tokens
by the GENIA Tagger which also provides part-of-speech
(POS) and chunk tags, and additionally recognises protein
mentions [44].
After running the syntactic tools, the workflow splits
into four branches. The first branch performs joint anno-
tation of concepts pertaining to Problem, Treatment and
TestOrMeasure by means of the NERsuite [45] compo-
nent, a named entity recogniser (NER) based on an imple-
mentation of conditional random fields [46]. Supplied
with a model trained on the 2010 i2b2/VA challenge train-
ing set [47], this NER is employed to provide domain ex-
perts with automatically generated cues which could aid
them in marking up full phrases describing COPD pheno-
types. Meanwhile, the NERsuite component in the second
branch is configured to recognise disease mentions using
a model trained on the NCBI Disease corpus [48]. The
third branch performs drug name recognition using the
Chemical Entity Recogniser, an adaptation of NERsuite
employing chemistry-specific features and heuristics [49]
which was parameterised with a model trained on the
Drug-Drug Interaction (DDI) corpus [50]. Finally, by
means of the Truecase Asciifier, Brown, OBO Anatomy
and UMLS Dictionary Feature Extractors, the last branch
extracts various features required by the Anatomical En-
tity Tagger which is capable of recognising anatomical
concepts [51]. The Annotation Merger component col-
lects annotations produced by the various concept recog-
nisers whilst the Manual Annotation Editor allows human
annotators to manually correct, add or remove automatic-
ally generated annotations via its rich graphical user inter-
face (Figure 3).
Finally, the workflows last component, the XMI Writer,
stores the annotated documents in the XML Metadata
Interchange standard format, which allows us to reuse theoutput in other workflows if necessary. Eventually, the an-
notations can be made available in several other standard
formats, such as RDF and BioC [52], which will be accom-
plished directly in Argo through its various serialisation
components. We note that the automatic tool for recog-
nising qualities is still under development, as are the com-
ponents for linking mentions to concepts in ontologies.
Nevertheless, we describe below our proposed strategy for
ontological concept identification.
Linking phenotypic mentions to ontologies
In order to identify the ontological concepts underlying
COPD phenotypic information, the mentions automatic-
ally annotated by our concept recognisers will be nor-
malised to entries in various ontologies, namely, the
Phenotype and Trait Ontology (PATO) [53] for qualities,
Human Disease Ontology (DO) [54] for medical condi-
tions, Uber Anatomy Ontology (UBERON) [55] for ana-
tomical entities, Chemical Entities of Biological Interest
(ChEBI) [56] for drugs, Protein Ontology (PRO) [57] for
proteins and the National Cancer Institute Thesaurus
(NCIT) [58] for tests/measures.
The NCBO Annotator [59], formerly Open Biomedical
Annotator, offers a solution to this problem by employ-
ing a Web service that automatically matches text
against specific ontologies. It is, however, not sufficient
for the requirements of our task as it is very limited in
terms of variant-matching [60], obtaining only exact
string matches against terms and synonyms contained in
ontologies. As observed from the examples in Table 2,
there is a large variation in the expressions comprising
COPD phenotypes. Consequently, many of these expres-
sions do not exist in ontologies in the same form. More
suitable, therefore, is a sophisticated normalisation
method that takes into consideration morphological var-
iations (e.g., alpha1 antitrypsin vs. alpha-1-antitrypsin),
inflections (e.g., obstruction vs. obstructed), syntactic
Figure 3 The user interface for linking mentions to ontologies.
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 7 of 11variations (e.g., decrease in rate vs. decreased rate) and
synonym sets (e.g., deficiency vs. decreased amount and
destruction vs. damage).
Argos library includes several automatic ontology-
linking components employing approximate string match-
ing algorithms [61]. Furthermore, the Manual Annotation
Editor provides a user-friendly interface for manually
supplying or correcting links to ontological concepts
(Figure 4). Ongoing development work on improving this
ontology-linking tool includes: (a) enhancement of the
normalisation method by the incorporation of algorithms
for measuring syntactic and semantic similarity, and (b)
shifting from Argos currently existing ontology-specific
linker components to a generic one that allows for linking
mentions against any ontology (from a specified set). Once
ready, the new component will be added to Argos library.
Instances of the component will then be integrated into
our semi-automatic workflow to facilitate the linking of
annotated mentions to the respective ontologies.
Results and discussion
After applying the Argo workflow described above on the
30 articles in our corpus, we asked one of our collaboratingFigure 4 The Manual Annotation Editors graphical user interface. The
finer-grained COPD phenotype annotations.domain experts to manually validate the automatically gen-
erated annotations. In this section, we present the results
of two types of evaluation. Firstly, the quality of the Argo-
generated concept annotations was measured by compar-
ing them against gold standard data, i.e., the annotations
manually validated by the domain expert. Secondly, we car-
ried out a preliminary evaluation of the gold standard an-
notations that we have obtained thus far by utilising them
in the development of machine learning-based concept
recognisers. It is worth noting that our gold standard data
is currently limited to our experts annotations on only
nine out of the 30 papers that she has examined thus far
(equivalent to 1,701 sentences). Table 3 presents the num-
ber of unique concepts for each type, as manually anno-
tated by our domain expert. One can see that the most
prevalent types are Treatment, RiskFactor, MedicalCondi-
tion, TestOrMeasure, Drug and AnatomicalConcept (in
order of decreasing frequency).
Table 4 depicts the evaluation of Argos automatically
generated annotations against the gold standard, pre-
sented by concept type. We note that only the five most
frequently occurring concept types (which are common
between the manually validated annotations we have atarticle excerpt shown is annotated using our proposed scheme for
Table 3 Number of unique concepts for each type, based
on the nine manually annotated articles
Concept type Number of unique
concepts
Treatment 430
RiskFactor 415
MedicalCondition 371
TestOrMeasure 282
Drug 192
AnatomicalConcept 96
Quality 59
Protein 40
Total 1,885
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 8 of 11hand and the automatically generated annotations) were
included in the evaluation. Two different modes of
matching were applied: exact matching, which considers
a system annotation as correct only if it has the same
concept type label and exactly the same boundaries as a
gold standard annotation; and relaxed matching, which
counts even a partially overlapping system annotation as
correct as long the non-overlapping tokens consist of
only articles and modifiers (i.e., they have only DT, JJ
or RB as POS tags). We note that for a given pheno-
typic expression, not only the full string is being evalu-
ated, but also each of its subsumed concepts. It can be
observed from Table 4 that in general, the semi-
automatic workflow obtains unsatisfactory performance
using exact matching. After performing some error ana-
lysis, we observed that majority of discrepancies were
brought about by the incorrect inclusion or exclusion of
articles or modifiers in noun phrases, e.g., phospho-
diesterase inhibitor (for a nonselective phosphodiesterase
inhibitor), an acute exacerbation (for acute exacerba-
tion). Thus we next employed relaxed matching, which
revealed that the semi-automatic workflow obtains mod-
erate performance over all evaluated concept types (ex-
cept for TestOrMeasure).Table 4 Evaluation of annotations automatically generated b
data
Exact matching
Precision Recall
AnatomicalConcept 0.1923 0.7527
Drug 0.5861 0.2744
MedicalCondition 0.0290 0.2842
TestOrMeasure 0.1425 0.0680
Treatment 0.3080 0.1494
Micro-average 0.2670 0.2283
Macro-average 0.3037 0.3057
Results are reported for only nine full-text papers.It is obviously more desirable for a semi-automatic
workflow to approximate the gold standard annotations
(i.e., to produce exact matches rather than partial ones).
Nevertheless, Argos automatically generated annotations
proved to be helpful in a number of cases. For example,
the automatic workflow was able to correctly annotate
partially correct annotations such as sputum (for sputum
smear), pulmonary (for pulmonary TB) and COPD-sta-
ging (for COPD) served as visual cues to the annotator.
Based on her experience in annotating our corpus, she
feels that having pre-supplied annotations, albeit incom-
plete or incorrect, is preferable over not having any an-
notations at all. We are, however, aware of the potential
bias that having pre-supplied annotations may bring
about, i.e., failure to annotate concepts completely
missed by automatic annotation due to reliance on visual
cues. To avoid this scenario, the annotator has been
asked to read all of the sentences thoroughly and to keep
in mind that the cues are not to be relied on. She has
adhered to this guideline throughout her annotations.
Applying the gold standard annotations to an informa-
tion extraction task, we employed NERsuite, an imple-
mentation of the conditional random fields (CRFs)
algorithm, to develop a new set of concept recognisers.
Samples were represented using features which are by de-
fault extracted by NERsuite, including character, token,
lemma and part-of-speech tag n-grams (within a distance
of 2 from the token under consideration), chunk tags, as
well as a comprehensive set of orthographic features (e.g.,
presence of uppercase or lowercase letters, digits, special
characters). The resulting models were then evaluated in
two ways. Firstly, for each concept type, models were
trained and subsequently evaluated in a 10-fold cross-
validation manner, whose results are presented in Table 5
alongside those obtained by the Argo components. In gen-
erating the folds, the articles were split at the paragraph
level, giving a total of 381 shorter documents. Secondly, to
facilitate evaluation on unseen data, each of the automat-
ically and manually annotated subset of nine papers was
subdivided into training (75% or 286 paragraphs) andy the text mining-assisted workflow against gold standard
Relaxed matching
F-score Precision Recall F-score
0.3063 0.2814 0.9038 0.4292
0.3738 0.7921 0.6463 0.7118
0.2868 0.3697 0.6313 0.4663
0.0920 0.1914 0.1039 0.1347
0.2012 0.4688 0.4015 0.4325
0.2462 0.4050 0.5243 0.4570
0.3047 0.4207 0.5374 0.4719
Table 5 Results of 10-fold cross validation of concept recognisers, using exact matching
Concept recognisers currently in Argo Concept recognisers trained on our corpus
Precision Recall F-score Precision Recall F-score
AnatomicalConcept 0.2361 0.6617 0.3428 0.7602 0.4990 0.5912
Drug 0.7318 0.2161 0.3283 0.8576 0.4499 0.5873
MedicalCondition 0.3986 0.2436 0.3010 0.8510 0.4590 0.5932
TestOrMeasure 0.0766 0.0182 0.0289 0.6850 0.3190 0.4332
Treatment 0.4330 0.1021 0.1635 0.8276 0.3458 0.4829
Micro-average 0.3305 0.1776 0.2310 0.7929 0.3970 0.5291
Macro-average 0.3752 0.2483 0.2988 0.7963 0.4145 0.5452
Performance is compared with that of the components utilised in the text mining-assisted workflow.
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 9 of 11held-out data (25% or 95 paragraphs). Models trained on
the former were then evaluated using annotations con-
tained in the latter. Table 6 presents the evaluation results
under this setting.
We show that by using our gold standard annotations as
training data, we were able to develop concept recognisers
whose performance is drastically better than those we
employed in our semi-automatic workflow. This signifi-
cant improvement ranged from 24.84 (for Anatomical-
Concept) to 40.43 (for TestOrMeasure) percentage points
according to 10-fold cross validation, and from 19.49 (for
AnatomicalConcept) to 40.45 (for TestOrMeasure) ac-
cording to the fixed split evaluation. This implies that our
corpus can stimulate the development of more suitable
automatic COPD phenotype extractors. We expect that as
more gold standard annotations become available to us (i.
e., as our domain expert completes the validation of more
documents in our corpus), the better equipped we will be
in boosting the performance of our automatic COPD con-
cept recognisers.
Conclusions
In this paper, we elucidate our proposed text mining-
assisted methodology for the gold-standard annotation of
COPD phenotypes in a corpus of full-text scientific arti-
cles. We demonstrate with the proposed scheme that the
annotation task can be kept simple for curators whilstTable 6 Results of evaluation using a fixed split over 381 par
set: 25% or 95 paragraphs), using exact matching
Concept recognisers currently in Argo
Precision Recall F-scor
AnatomicalConcept 0.2602 0.6145 0.3656
Drug 0.6885 0.1900 0.2979
MedicalCondition 0.4494 0.2492 0.3206
TestOrMeasure 0.0250 0.0041 0.0070
Treatment 0.4111 0.0847 0.1404
Micro-average 0.3735 0.1614 0.2254
Macro-average 0.3669 0.2285 0.2816producing expressive and computable annotations. By
constructing a semi-automatic annotation workflow in
Argo, we seamlessly integrate and take advantage of sev-
eral automatic NLP tools for the task. Furthermore, we
are providing the domain experts with a highly intuitive
interface for creating and manipulating annotations. The
comparison of annotations automatically generated by the
workflow against manually validated ones (i.e., gold stand-
ard) reveals an F-score of 45.70% using relaxed matching.
New concept recognisers trained on these gold standard
annotations demonstrate dramatically better performance
(i.e., with a 20- to 30-percentage point margin in terms of
F-scores) over the off-the-shelf components used in the
Argo workflow.
Manual expert validation of the text mining-generated
annotations on the remaining 21 papers in the corpus is
still ongoing. In the meantime, we are enhancing our
ontology concept linkers, which, once ready, will be applied
on the gold standard concepts to enrich our corpus with
computable annotations. Our expert collaborators are also
working hard on obtaining a subset of clinical records from
their hospital, which will then be used to augment our cor-
pus. With the resulting resource, which will be made pub-
licly available upon completion, we aim to support the
development and evaluation of text mining systems that
can ultimately be applied to evidence-based healthcare and
clinical decision support systems.agraphs (training set: 75% or 286 paragraphs; held-out
Concept recognisers trained on our corpus
e Precision Recall F-score
0.8000 0.4314 0.5605
0.7966 0.4196 0.5497
0.8673 0.3899 0.5380
0.6719 0.2966 0.4115
0.8400 0.2903 0.4315
0.8034 0.3552 0.4926
0.7952 0.3656 0.5009
Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 10 of 11Additional file
Additional file 1: List of COPD phenotypes used to retrieve articles
from the PubMed OpenAccess subset.Competing interests
The authors declare that they have no competing interests.
Authors contributions
XF carried out the collection of documents for the corpus, the design of the
annotation scheme and evaluation of the automatic tools. RB participated in
the design of the annotation scheme, supervised the annotation and
formulated the evaluation strategies. RR contributed towards the
development of the text mining-assisted workflow and in enriching the Argo
library with necessary components. XF, RB and RR drafted the manuscript. SA
provided research direction and supervised all steps of the work. All authors
read and approved the final manuscript.
Acknowledgements
The authors would like to thank Drs. Nawar Bakerly and Andrea Short of the
Salford Royal NHS Foundation Trust and University of Manchester, who have
provided their expertise on COPD to guide the clinical aspects of this work.
Special mention goes to Andrea who has graciously contributed a significant
amount of her time to provide us with annotations.
The first author is financially supported by the University of Manchesters
2013 President's Doctoral Scholar Award. This work is also partially supported
by the Medical Research Council (Supporting Evidence-based Public Health
Interventions using Text Mining [Grant MR/L01078X/1]) and by the Defense
Advanced Research Projects Agency (Big Mechanism [Grant DARPA-BAA-14-
14]).
Author details
1National Centre for Text Mining, School of Computer Science, University of
Manchester, Manchester Institute of Biotechnology, 131 Princess Street,
Manchester, UK. 2Department of Computer Science, University of the
Philippines Diliman, Quezon City 1101, Philippines.
Received: 11 November 2014 Accepted: 22 February 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 
DOI 10.1186/s13326-015-0021-5
RESEARCH ARTICLE Open Access
Discovering relations between indirectly
connected biomedical concepts
Dirk Weissenborn1,2*, Michael Schroeder2 and George Tsatsaronis2
Abstract
Background: The complexity and scale of the knowledge in the biomedical domain has motivated research work
towards mining heterogeneous data from both structured and unstructured knowledge bases. Towards this direction,
it is necessary to combine facts in order to formulate hypotheses or draw conclusions about the domain concepts.
This work addresses this problem by using indirect knowledge connecting two concepts in a knowledge graph to
discover hidden relations between them. The graph represents concepts as vertices and relations as edges, stemming
from structured (ontologies) and unstructured (textual) data. In this graph, path patterns, i.e. sequences of relations,
are mined using distant supervision that potentially characterize a biomedical relation.
Results: It is possible to identify characteristic path patterns of biomedical relations from this representation using
machine learning. For experimental evaluation two frequent biomedical relations, namely has target, and may treat,
are chosen. Results suggest that relation discovery using indirect knowledge is possible, with an AUC that can reach
up to 0.8, a result which is a great improvement compared to the random classification, and which shows that good
predictions can be prioritized by following the suggested approach.
Conclusions: Analysis of the results indicates that the models can successfully learn expressive path patterns for the
examined relations. Furthermore, this work demonstrates that the constructed graph allows for the easy integration of
heterogeneous information and discovery of indirect connections between biomedical concepts.
Keywords: Relation discovery, Biomedical concepts, Text mining
Background
Motivation and objectives
Knowledge discovery is an important field of research,
especially in the biomedical domain, in which the scale
and growth of accumulated knowledge of all kinds is
already beyond the capabilities of a single human to keep
up with. This has motivated research towards mining
knowledge from heterogeneous data of both structured
and unstructured knowledge bases (KBs). The parallel use
of structured and unstructured data is important because
they are complementary. Structured KBs contain explicit
but inadequately covered knowledge. In contrast, unstruc-
tured KBs contain nearly all of the domain specific knowl-
edge but lack in simplicity with regards to automated
analysis.
*Correspondence: dirk.weissenborn@dfki.de
1DFKI Projektbüro Berlin, Alt-Moabit 91c, 10559 Berlin, Germany
2Biotechnology Center, Technische Universität Dresden, Tatzberg 47/49,
01307 Dresden, Germany
An example of how fast the reporting of scientific
findings grows in this domain is illustrated in Figure 1,
where the number of scientific publications indexed by
PubMed is shown to be increasing in an exponential
fashion over the past decades. Similar findings can be
observed for structured data by examining the growth of a
representative database in the biomedical domain, namely
the Unified Medical Language System (UMLS), shown in
Figure 2.
Besides the obstacles that the large scale of the data
brings into the task of extracting information there is also
the issue of combining pieces of knowledge together to
cover as many aspects as possible which can potentially
lead to new knowledge. For example, typical information
extraction techniques focusing on drugs aim at extract-
ing targets, adverse effects and indications, which cannot
succeed by limiting the applied methods to a small frag-
ment of drug related information. Hence, it is necessary
to combine facts in order to formulate hypotheses or
draw conclusions about the domain concepts. This work
© 2015 Weissenborn et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 2 of 19
Figure 1 Growth of PubMed indexed scientific literature since 1965.
The figure plots the number of PubMed indexed articles per year, for
the period 1965-2010. The plot shows that the indexed literature
grows exponentially (blue line). In parallel, the annotation of the
PubMed articles with MeSH terms has so far managed to follow this
growth (red line)a.
attempts to address this problem by using indirect knowl-
edge connecting two concepts to discover hidden relations
between them.
In contrast to relation extraction, which aims at rec-
ognizing direct mentions of relations within a sentence
or document between two concepts, relation discovery
between indirectly connected concepts attempts to find
Figure 2 Growth of UMLS Metathesaurus in the past decade. In this
plot, the growth of the UMLS metathesaurus in terms of number of
included concepts is presented. The plot refers to the period from
2002 until the present. For this past period, the growth curve is steep,
approximating an exponential tendencyb.
hidden, yet unknown relations that can be derived from
sequences of already known and established facts. The
first reported and most famous discovery of this kind was
the finding of Swanson in 1986 that fish oil may treat
Raynauds syndrome [1]. He came to this conclusion by
combining the two simple facts from different scientific
studies that on the one hand fish oil has beneficial effects
on blood viscosity and on the other hand patients suf-
fering from Raynauds syndrom demonstrate increased
blood viscosity. Until that time there was no direct con-
nection between the concepts of fish oil and Raynauds
syndrome, but only an indirect connection through the
concept of blood viscosity, which indicated that fish oil
may treat Raynauds syndrome which was indeed verified
in 1989.
To be able to extract indirect connections between con-
cepts, knowledge from all sources is represented by a
graph comprising concepts as vertices and labelled edges
connecting the concepts. Edges are created by either
extracting explicit knowledge from structured databases
in form of triples or by analysing unstructured textual
data. The idea of using graphs to represent knowledge
to find connections between concepts is not novel. It
has been exploited in both ontology based [2] and liter-
ature based approaches [3]. Representing knowledge in
such a way provides a simple framework that is poten-
tially easy to interpret, makes the integration of hetero-
geneous data straightforward and is useful for finding
indirect connections, i.e. paths, in the graph between
concepts.
The task of finding connections between two concepts
and identifying their meaning is called relation discov-
ery. Besides being able to recognize that some connec-
tion exists it is also important to understand what kind
of relation is expressed to discover the hidden relations
represented by the given connections.
Relation discovery is performed on top of the aforemen-
tioned graph representation by using supervised machine
learning to learn path patterns that frequently occur
between concept pairs of a specific relation and can
therefore be considered characteristic for that relation.
A trained model can in turn be used to discover a
specific relation between indirectly connected concept
pairs.
This work extends the study for DILS 2014 by the
introduction of a new approach and the validation on
a manually created drug repositioning dataset. Further-
more, the approach is explained and discussed in great
detail with additional inspections into the clustering of
relations by LDA. Our main contributions lie in the
joint exploitation of linguistic information and struc-
tured knowledge in a simple, extensible graph repre-
sentation for fully automatized, indirect discovery of
relations.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 3 of 19
Related work
Most work on knowledge discovery from unstructured,
textual data focuses on extracting relations between
two concepts mentioned in one sentence. This is very
important for many applications such as the curation
of databases. However, in his famous work Swanson
has shown the potential of combining facts from dif-
ferent sources to discover new, yet unknown knowledge
[1].
Recently, many studies have been conducted on find-
ing hidden relations between concepts indirectly. Most of
these works are purely based on statistical analysis of con-
cept co-occurrence profiles fromMEDLINE, which differs
from our approach in that they do not take any linguistic
information into account, e.g., Frijters et al. [4] and Cohen
et al. [5].
Srinivasan P. et al. [6] developed a system that discov-
ers relations by searching for interesting paths between
two concepts from a start concept of interest through
a set of co-occurring concepts of predefined types from
MEDLINE that in turn co-occur with a set of potential
target concepts of predefined types without exploiting
existing linguistic information. The main difference is
that the whole process is manually guided and intended
to aid scientists in the search of new relations whereas
our approach is completely automated. Furthermore, no
machine learning is applied to find interesting connec-
tions, but only a hand-made weighting scheme based on
the ideas of TF-IDF. More sophisticated studies building
upon this idea include the work of Hristovski et al. [7] and
Vidal et al. [8].
BioLiterate, a system developed by Goertzel et al. [9],
is designed to discover relations which are not contained
in any individual abstract using probabilistic inference.
In contrast to this work their approach is based on a
collection of hand-built rules, that map linguistic con-
structs onto a probabilistic reasoning system. Further-
more, it does not make use of any structured knowledge
base.
Arguably the most similar work to ours is the work of
Lao et al. [10]. As in the current work, the authors use
a combination of structured and unstructured knowledge
to infer relations between concepts using a sequence of
related concepts. They use an open domain, web-scale
corpus to train a classifier based on logistic regression
with a huge amount of training examples represented by
vectors of a very large feature space. However, the require-
ments of this work, namely a limited amount of training
data and amuch smaller textual corpus, require a different
way of modeling and training.
Table 1 provides an overview of the aforementioned
works in comparison to the current approach with respect
to different aspects concerning the requirements and used
methodologies.
Table 1 Comparison of related work with respect to: use of
liguistic information, use of manually designed rules,
application in restricted domain, possibility of using
sparse training data
Work Linguistic Manual Restricted Sparse
Goertzel et. al (2006) [9] x x x x
Frijters et al. (2010) [4] x (x)
Cohen et al. (2010) [5]
Lao et al. (2012) [10] x
Srinivasan et al. (2004) [6] x x x
Current work x x x
Methods
Terminology
In this work an atomic piece of knowledge is consid-
ered as a triple (ci, l, cj), consisting of a pair of concepts
(ci, cj), e.g., (aspirin, inflammation), and a label l, e.g.,may
treat, representing a relation Rl to which the pair (ci, cj)
belongs. Furthermore, indirect knowledge connecting two
concepts cs and ct is defined as a sequence of triples start-
ing with concept cs and ending in concept ct , where the
second concept of each triple must be equal to the first
concept of its following triple. Table 2 summarizes the
notation used in this article.
Utilized biomedical knowledge sources
Nowadays, plenty of data is freely available and easy to
access, but each data source has a different knowledge
representation, called a schema. The schema defines how
concepts can be described and how they can relate to each
other. For structured knowledge sources such as databases
Table 2 Summary of the terminology and notation used
throughout themanuscript
Symbol Explanation
ci a concept
C a set of concepts
l a label representing a relation
Rl binary relation with label l
(ci , l, cj)/triple a pair of concepts (ci , cj) connected by relation with label l
R a set of triples
G the knowledge graph
P a path in G
f a feature vector
E+/E? a set of positive/negative examples
? set of model parameters
X observable variables or observations defined by a model
H hidden or latent variables defined by a model
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 4 of 19
the set of relations and concepts as well as their rep-
resentations are well defined, whereas for unstructured
knowledge sources like text this is not the case. Natural
language is far more expressive than the schemas of any
structured knowledge source because it is not restricted
to a fixed set of concepts and relations, but at the same
time it is much harder to interpret because natural lan-
guage can express different pieces of knowledge with the
same representation (polysemy), and one piece of knowl-
edge in many different ways (synonymy). In the following,
we describe the different biomedical knowledge sources
that are used in this work, and how they have been
utilized.
UnifiedMedical Language System
The most popular structured knowledge base for biomed-
ical text mining is the Unified Medical Language Sys-
tem (UMLS), which consists of 3 different resources,
namely the Metathesaurus, the Semantic Network and
the Specialist Lexicon. The Metathesaurus is a multi-
lingual vocabulary database which combines knowledge
from many different structured knowledge sources. It
contains a large amount of biomedical concepts, infor-
mation about them (e.g., their semantic type, a descrip-
tion, etc.) and how they are related to each other. The
Semantic Network comprises a set of semantic types
and relations connecting the semantic types to each
other. It provides a consistent, semantic categorization
of Metathesaurus concepts. The Specialist Lexicon is a
general English lexicon consisting of biomedical terms
which is not used in this work. The roughly 3 million
concepts contained in the Metathesaurus of the 2013AB
Release form the basis of the knowledge representation in
this work which means that concepts from other knowl-
edge sources have to be mapped to concepts of the
Metathesaurus.
DrugBank
DrugBank [11] is an open drug and drug-target database.
A target of a drug refers to a protein that a drug is
able to bind to. DrugBank is not yet part of the UMLS.
Therefore, a mapping fromDrugBank to UMLS ids is nec-
essary, which can partially be achieved by mapping their
respective concept names to each other. By following this
approach it is possible to map 1125 targets (proteins or
genes) and 2663 drugs from DrugBank to UMLS, which
results in a total of 1228 distinct drug-target pairs mapped
from all FDA-approved drug-target pairs documented in
DrugBank.
MEDLINE
As unstructured knowledge source MEDLINE is used.
It is the collection of all publication abstracts from
all life-science journals indexed by PubMed. MEDLINE
is the most widely used, freely available textual cor-
pus for biomedical text mining. Furthermore, an already
annotated version of MEDLINE exists. The annota-
tion is performed frequently by the National Library
of Medicine (NLM) using the MetaMap program [12],
which annotates natural language text with concepts
of the UMLS Metathesaurus. The 2012 MetaMapped
MEDLINE corpus is used as unstructured textual knowl-
edge source containing all publications until November
18, 2011.
When using the MetaMapped MEDLINE corpus care
should be taken. E.g., MetaMap has problems annotating
genes with aliases which are common english words such
as impact or rare. In this work we exclude gene/protein
annotations for common english words. Furthermore,
we only consider annotations of UMLS concepts of the
following semantic types or their respective subtypes:
Organisms, Clinical Drug, Substances, Sign or Symptom,
Anatomical Structure, Molecular Sequence, Body Space
or Junction, Body Location or Region, Pathologic Func-
tion, Injury or Poisening.
Dependency trees
Dependency trees [13] are syntactic constructs of sen-
tences in which each node of the tree represents a token
(word or symbol) of the underlying sentence and each
arch represents a dependency between two tokens of
that sentence. In dependency grammars (DG) the verb
always takes the central role of the sentence and is
therefore always the root of the tree independent from
the rest. Furthermore DGs do not require any order-
ing of the sentence words and are thus also applicable
to languages in which the order of words in a sen-
tence is all the same (e.g., in Czech or Turkish). Unlike
phrase structure grammars (constituency grammars) DGs
do not explicitly structure sentences into phrases but
rely only on dependencies between words in a sentence
[14]. An example of a dependency tree is shown in
Figure 3.
Knowledge representation
Integrating knowledge from heterogeneous data sources
into one coherent representation (schema) is a complex
process. One of the main difficulties is the mapping of
the concepts in each data source to each other. This has
been done for all used knowledge sources in this work as
explained in their respective description. Mapping rela-
tions from different knowledge sources to each other is
even more complicated and can potentially result in a loss
of information. To circumvent this problem relations are
kept explicitly in the form they occurred in the sources
which results in a huge relation space with a lot of redun-
dancy. The relation space, however, can be reduced by
using semantic vector representations for the relations
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 5 of 19
Figure 3 An example of a dependency tree of a sentence. The dependency tree of the following sentence is illustrated: Aspirin is used in the
treatment of inflammation and not nasal polyps .
obtained by applying co-occurrence based dimensional-
ity reduction algorithms as will be described later in the
description of the encoding.
Knowledge graph
In order to find indirect connections between concepts
quickly, both structured and unstructured knowledge is
represented by a graph. Similar to the work of [10] a
directed, edge-labelled graph G = (C,R) is used, com-
prising a set of concepts C as vertices and a set of labelled
edges (triples) R = C × L × C between them, where
L denotes the set of all possible relation labels. If there
is a pair (ci, cj) ? Rl in one of the knowledge sources,
an edge (ci, l, cj) is added to R. In other words, only if a
pair of concepts is known to be in a relation with label
l, then there is an edge labelled with l in G connect-
ing this pair of concepts. Note that a triple can occur
more than once in R, which means that R is actually a
multiset. A path of concepts P in G of length n is an n-
tuple of vertices P = (c1, ..., cn), where ?i, 1 ? i < n :
?l ? L : (ci, l, ci+1) ? R, meaning that there must be
at least one edge between the concepts ci and ci+1 for
every i.
Knowledge extraction
Structured knowledge sources, such as UMLS and Drug-
Bank, already contain labelled relations Rl ? C × C. The
information of all relations Rl, i.e. its concept pairs (ci, cj)
together with its label l, can directly be inserted into the
graph by adding all concepts ci and cj of all pairs to C
as vertices and all corresponding triples (ci, l, cj) to R as
edges.
Extracting triples from unstructured, textual data
requires a more elaborate strategy. Since MEDLINE, the
used textual data, is already annotated with biomedical
concepts of the UMLS Metathesaurus, this task reduces
to extracting only the relations between concepts found
in one sentence. Previous work on relation extraction has
shown that the dependency path between two concepts
in a sentence typically contains all necessary informa-
tion to recognize a specific underlying relation between
them (e.g., [15-18]). A dependency path is a path in a
dependency tree, which is a syntactic construct of a sen-
tence as explained in the previous section. It is important
not to confuse the notion of dependency path, which are
edges in G, with the notion of a path in the knowledge
graph G.
Triples are only extracted from sentences when a pair
of concepts, or more precisely their headwords in the
dependency tree, connected by a dependency path are
found that contains at least one verb form. If the depen-
dency path does not contain any verb form, it is assumed
that there is no relation present in this sentence. On
the other hand, if two or more verb forms are found on
the dependency path which are part of two distinct sub-
sentences connected by some conjunction, it is assumed
that there is no direct relation in the sentence between
such concept pairs present and these triples are discarded
as well. Furthermore, conjunction and apposition edges
are removed from the dependency paths together with
their head words because in most cases they represent
simple enumerations which do not effect the semantics
of the relation being expressed between the two con-
cepts in question. If there is a negated noun or verb
form present on the dependency path, the whole path
will be treated as negated as well. Furthermore, there
is also the issue of extracting triples connected by very
long dependency paths. Long dependency paths can be
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 6 of 19
very unspecific and confusing, and they are more likely to
contain parsing errors. Moreover, such paths occur usu-
ally very rarely in the corpus which makes them hard to
interpret when using statistical methods. After manual
inspection a maximum length of 6 was chosen to pre-
vent that. Once a pair of concepts (ci, cj) is extracted from
a sentence together with its post-processed dependency
path p, a triple (ci, p, cj) can be inserted into the knowledge
graph G the same way as for structured knowledge. Note
that there is no mapping from the extracted dependency
paths to any specific predefined relation label. Therefore,
every possible dependency path can be viewed as a single
relation.
As an example of this procedure it is possible to extract
the following triples from the sentence shown in Figure 3:
 (aspirin, nsubjpass? use prep? in pobj? treatment prep?
of pobj? , inflammation)
 (aspirin, neg nsubjpass? use prep? in pobj? treatment prep?
of pobj? ,nasal polyps)
In the second triple the conj-sequence is removed from
the path and the overall path is negated because it contains
a negated noun phrase.
For both unstructured and structured triples (ci, l, cj)
in the knowledge graph, there is always an inverse triple
(cj, l?1, ci). During extraction these inverses are excluded
and only one triple (ci, l, cj) is added to the graph to
avoid including redundant information. However, during
path search we also consider triple (cj, l?1, ci) to be
existent.
An example sub-graph of the resulting knowledge graph
can be found Figure 4.
Modelling
In order to discover that a pair of concepts cs (source)
and ct (target) is in a relation q in question using indi-
rect connections between them, i.e., paths in G of length
greater than 2, a model must be trained to recognize
typical graph path patterns for q from positive and neg-
ative training pairs. Direct connections are excluded to
avoid explicit inference of the relation in question. Dur-
ing the application of the model, a set of graph paths
extracted between cs and ct are presented to the model
which in turn calculates a confidence score between 0 and
1 of assigning label q to the concept pair (cs, ct) in ques-
tion. In the following we describe these steps in more
detail.
Two types of models are considered for modelling the
problem of discovering a relation between a pair of con-
cepts given a set of paths connecting them. The first type
of modelling directly extracts features from the set of
paths between a pair and uses the resulting feature vec-
tor as input for any kind of vector classifier. This method
will be referred to as the pair-based approach. A second
approach is based on a model that assigns confidence
scores to graph paths rather than to the pairs themselves.
Given all graph paths and their respective scores between
two concepts, the final confidence score for the con-
cept pair is calculated by averaging its best k graph path
Figure 4 A sub-graph of the knowledge graph. This sub-graph consists of example paths connecting the two concepts C0000545
(Eicosapentaenoic Acid) and C1825292 (FFAR1 gene) which are part of the has target relation.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 7 of 19
scores. The problem of considering all paths and averag-
ing them to form the final score is that nearly all graph
paths are uninformative with respect to the relation that
exists between the source and the target concept. This
would introduce a lot of noise and disturb the resulting
scores.
Relation label encoding
Encoding a graph path P = (c1, ..., cn) requires an encod-
ing of each connection (ci, ci+1) in P as a feature vector
f(ci,ci+1), resulting in a sequence of feature vectors of length
(n ? 1) which is used by the model for training of infer-
ence, respectively. f(ci,ci+1) is defined as the sum of all
feature vectors of each relation label l occurring between
ci and ci+1 (see Equation 1). Feature vectors for all possi-
ble relation labels l can be created in different ways. The
following two sections explain how this is achieved in this
work.
f(ci,ci+1) =
?
(ci,l,ci+1)?R
fl (1)
One-of-N encoding. The simplest way of encoding a
relation label l is the one-of-N encoding, where only the
l-dimension of the feature vector has value 1 and all oth-
ers are 0, as the name of suggests. This encoding, however,
is very poor because it does not take any semantic simi-
larities or even synonymy among the relation labels into
account, which leads to an explosion of the feature space
growing as large as there are different relations. Especially
for unstructured relation labels (i.e., dependency paths)
there are many ways of expressing the same underlying
relation, resulting in a lot of redundancy. With a large
number of training examples this can be handled by the
model, but if training examples are sparse, there is a need
of encoding relations in a much smaller semantic feature
space or otherwise the model will overfit to the training
data.
Semantic encoding. Mapping relation labels into a
semantic space has already been done in other studies
such as the work of Yao et al. [19]. Extracting seman-
tic vectors for words co-occurring in documents is a well
studied problem and thus, there are numerous algorithms
that solve this task. Examples are latent semantic analysis
(LSA, [20]), reflective random indexing (RRI, [5]), the gen-
eralization of principle component analysis (gPCA, [21])
or latent Dirichlet allocation (LDA, [22]). The basic idea
for constructing a semantic space of relations is to con-
sider a pair of connected vertices, i.e. concepts, (ci, cj) in
G as a document di,j and the label of each edge between
them as a word occurring in di,j. Using this transformation
for all connected concept pairs of G, the above mentioned
algorithms can be used natively to construct semantic fea-
ture vectors of a specified size for each relation label or in
case of LDA even for each concept pair at the same time.
In the experiments LDA is used because its underlying
model fits well to this problem.
By transforming pairs of concepts to documents and
relation labels to words, LDAs latent topics can be con-
sidered as the true but hidden relations between a
pair of concepts. Each true relation has many differ-
ent forms of representations in natural language text
or databases. At the same time, the number of possi-
ble true relations between a pair of concepts is usually
very low and in many cases even one, thus they are also
very sparse. These two aspects can be reflected in LDA
by setting the hyper-parameters of the model to some-
thing well below 1. The idea of modeling relations with
LDA was already investigated in a similar form by Yao
et al. [23].
In case of using LDA features, we define fl as the con-
ditional probability distribution over all possible latent
topics t given relation label l:
f tl = p(t|l)
p(t|l) ? p(t) · p(l|t), (2)
where f tl is the value of the t-th dimension of fl.
p(t) and p(l|t) are directly extracted from the trained
LDA model. Furthermore, in case of LDA, all pair fea-
ture vectors f(ci,cj) are also normalized after summing
over all feature vectors of labels occurring on edges
between (ci, cj).
In order to validate that LDA is able to learn semantic
vector representations of relations, the 15 most occurring
dependency paths of the 100 semantically most similar
dependency paths to themay treat and has target relation
were extracted. The resulting sets of relations are shown in
Table 3. From the examples, it can be seen that most of the
extracted dependency paths for both of the relations are
actually textual representations of them, which supports
the claim that semantic vectors of relations can indeed be
learned using LDA.
Pair-based approach
For pair-based classification a vector classifier must be
trained which takes as input a feature vector. Similar to
the previous work [10] a logistic regression model is used
in this approach. Given a set of graph paths Pcs,ct , the
feature vector fcs,ct for (cs, ct) is defined as the normal-
ized sum of the feature vectors representing the paths
P ? Pcs,ct . The feature vector of a path P = (c1, ..., cn)
is calculated from its corresponding sequence f(ci,ci+1) ?
R
N of feature vectors by transforming their outer prod-
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 8 of 19
Table 3 The 15most popular relations taken from the 100
semantically closest relations to the has target andmay
treat relation
Relation Most similar relations
has target
dobj??? form prep??? with pobj???
nsubjpass?????? degrade agent???? by pobj???
nn?? activity nsubjpass?????? inhibit agent???? by pobj???
nsubj??? inhibit dobj??? phosphorylation prep??? of pobj???
nsubj??? show dobj??? affinity prep??? for pobj???
nsubjpass?????? show xcomp???? interact prep??? with pobj???
dep??? form dobj???
nsubj??? inhibit prep??? in pobj??? presence prep??? of pobj???
nsubjpass?????? cross ? linked prep??? to pobj???
dep??? form nsubjpass??????
dobj??? inhibit prep??? with pobj???
nsubj??? potentiate dobj??? activity prep??? of pobj???
nsubjpass?????? prepare agent???? by pobj??? reaction prep??? of pobj???
nn?? substrate prep??? include pobj???
nsubj??? act prep??? by pobj???
may treat
pobj??? with prep??? patient nsubjpass?????? treat prep??? with pobj???
nsubj??? be prep??? in pobj??? treatment prep??? of pobj???
nsubj??? be attr?? treatment prep??? for pobj???
nn?? patient partmod????? treat prep??? with pobj???
nsubjpass?????? use prep??? in pobj??? treatment prep??? of pobj???
nsubjpass?????? use prep??? for pobj??? treatment prep??? of pobj???
pobj??? with prep??? treat prep??? for pobj???
dobj??? receive prep??? for pobj???
attr?? be prep??? in pobj??? treatment prep??? of pobj???
pobj??? with prep??? patient rcmod???? treat prep??? with pobj???
nsubjpass?????? administer prep??? to pobj??? patient prep??? with pobj???
nsubjpass?????? use prep??? in pobj??? patient prep??? with pobj???
dobj??? use prep??? in pobj??? patient prep??? with pobj???
nsubj??? improve prep??? in pobj??? patient prep??? with pobj???
nsubj??? have prep??? in pobj??? patient prep??? with pobj???
uct which is an (n ? 1)-dimensional tensor into a vector
representation.
fcs,ct =
?
P?Pcs ,ct fP????P?Pcs ,ct fP
??? (3)
fP = ?
(
f(c1,c2) ? · · · ? f(cn?1,cn)
)
(4)
The resulting vector consists of Nn?1 dimensions,
where each dimension corresponds to a tuple in
{1, 2, · · · ,N}(n?1) which represents a position in the for-
mer tensor. The following equation is an example of trans-
forming the outer product of two 4-dimensional vectors u
and v into a vector representation.
? (u ? v) = ?
(
uvT
)
= ?
?
???
?
???
u1v1 u1v2 u1v3 u1v4
u2v1 u2v2 u2v3 u2v4
u3v1 u3v2 u3v3 u3v4
u4v1 u4v2 u4v3 u4v4
?
???
?
???
=
?
???????????
u1v1
u1v2
u1v3
u1v4
u2v1
· · ·
u4v3
u4v4
?
???????????
One problem of using this type of encoding is the
exponentially growing feature space depending on the
maximumpath lengthm. The pair-based approach in con-
junction with the plain one-of-N encoding has a feature
space in which each dimension corresponds to a sequence
of relation labels. The model learns which of those rela-
tion label sequences are characteristic for the relation the
model is being trained on which is reflected by a high pos-
itive weight for the corresponding dimension. This is very
useful for the interpretation of what themodel has learned
because the extraction of highly weighted relation label
sequences from the trained model is very easy.
Path-based approach
For path-based classification a binary sequence classifier
must be trained, which takes as input a sequence of fea-
ture vectors f(ci,ci+1) ? RN (see Equation 1) constructed
from the graph path P = (c1, ..., cn) in question and out-
puts a confidence score. This can be modelled by using
any kind of vector-classifier which takes as input a feature
vector of length (m · N), if the maximum possible length
m of a sequence is known, or it is possible to use a proper
sequence classifier. For the former logistic regression and
for the latter a combination of two hidden markov mod-
els, one trained on positive example paths for q (pHMM)
and the other only trained on negative example paths
(nHMM), were chosen. In case of the HMMs a path P is
applied to both HMMs during inference and the probabil-
ity of sequence P given the respective HMM is computed.
The confidence score of assigning label q to path P is
finally calculated by combining the two probabilities in the
following way:
pHMM(q|P) = p(P, q)p(P, q) + p(P,¬q)
= p(P|q)p(P|q) + p(P|¬q) (5)
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 9 of 19
where p(P|q) = ppHMM(P) denotes the probability of
P calculated by the positive HMM and p(P|¬q) =
pnHMM(P) the probability calculated by the negative
HMM, assuming p(q) = p(¬q), which is a strong assump-
tion. In reality p(¬q) >> p(q), however, this would put
too much weight on the outcome of the negative HMM.
In practice, we are typically more interested in finding a
good ordering of tuple candidates for a relation instead of
real probabilities for single candidate tuples based on the
output of both models.
The advantage of this path-based approach over the
pair-based model is that the feature space grows only lin-
early with the maximum path length m in the case of
logistic regression and it is even constant with increasing
m for the HMM approach.
Graph path discovery
To extract paths for a concept pair (cs, ct) to a maximum
path length m, a bidirectional search [24] is performed,
i.e., searching is done by starting in both vertices cs and
ct until a maximum path length of
?m+1
2
?
from each side
is reached. Although search is still exponential in time
and space complexity, it requires only the square root of
resources compared to naive search from the source to
the target vertex. Finally, similar to the work of [10], the
probability of expanding the search to a neighbor ver-
tex cj from the current vertex ci is given by the following
formula:
pexplore(cj|ci) = min
(
1,
?
h + |N(ci)|
|N(ci)|
)
(6)
where N(c) denotes the set of neighbors of vertex c and
h is a big number (e.g., 100,000 in this work). Usually the
number of neighbors is not very high, which means that
in most cases every neighbor will be explored.
Example paths of different sizes (2-4) can be seen in
Figure 4, which illustrates a sub-graph of the knowl-
edge graph containing paths between a drug and its
target.
Training
Models are trained using distant supervision, which
assumes that paths between a pair of concepts of relation
q are representing q and can therefore be considered posi-
tive training examples for q. Even though this assumption
is strong it has been shown to be very effective in previous
studies [25,26].
Training examples for a specific relation q can directly
be extracted from its relation Rq ? C × C contained in at
least one of the structured knowledge sources (e.g., Drug-
Bank and/or UMLS). Amodel for relation label q is trained
with a set of positive training examples E+q ? Rq and a
set negative training examples E?q ? C × C, which is con-
structed from E+q by pairing all source concepts of E+q with
a random target concept of E+q , ensuring that Rq?E?q = Ø.
By using the same concepts in both the positive and the
negative training set, it is ensured that the model does
learn only about the paths between the pairs rather than
also learning characteristics about the different concepts
of the two training sets.
Given a set of positive (E+q ) and negative training exam-
ples (E?q ) for relation label q, a graph path classifier is
trained on all extracted graph paths for each concept pair
of E+q and E?q . For HMMs, standard EM training (Baum-
Welch algorithm) is applied, and for logistic regression,
training is performed using gradient ascent on the likeli-
hood function using LBFGS with L2-regularization.
As described previously, only very few of the graph
paths extracted between concepts of a positive pair are
real indicators for the relation label q. This is a problem
when training the path-based classifier model, because it
means that most of the extracted positive example graph
paths, which are the paths between concepts of a positive
concept pair are actually negative or noisy examples. To
deal with this problem most of the noise from the pos-
itive path training examples can be removed as follows.
First, a model is trained on the initial, noisy examples.
Subsequently, the trained model is used to score all pos-
itive graph paths in order to eliminate noisy paths by
only keeping those positive graph paths that were scored
higher than a specific threshold (e.g., 0.5 in our case). In
turn, a completely new classifier model can be trained
on the pruned set of positive graph paths and the orig-
inal set of negative graph paths. This procedure can be
repeated several times, though once was already enough
in our experiments. Training the path-based HMM clas-
sifier in such a way has shown to be more effective
in the conducted experiments and a clearer separation
between the distribution of the confidence scores of the
positive compared to the negative training examples was
observed.
Finally, for learning semantic relation vectors the LDA
model was trained using the efficient sparse stochastic
inference algorithm developed by [27], which is particu-
larly useful when dealing with huge amounts of training
data.
Results and discussion
Graph generation
For the already annotated MEDLINE corpus of 2012,
triples were extracted by extracting dependency paths
between two annotated concepts in each sentence.
ClearNLP [28] was used for dependency parsing, because
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 10 of 19
it is very fast and provides existing models trained on
medical text. The resulting set of triples was stored in a
titan [29] graph database. During extraction only depen-
dency paths of length up to 6 were considered. The result-
ing graph contains 278,061 vertices (i.e., concepts) with
an average degree of 600 in- and outgoing edges, result-
ing in 83 million edges (i.e., extracted triples) of around
16 million different labels (i.e., dependency paths), where
each label thus occurs on average 5.2 times. In total, 29.7
million pairs of vertices are connected to each other. Both
vertex degrees and edge label occurrences follow a very
heavy tailed distribution (see Figure 5), i.e., most of the
vertices and edge labels only occur very scarcely. Because
there is so little data for those concepts and dependency
paths, there is no value in keeping those for statistical
learning methods. Therefore, the graph was pruned at a
total concept occurrence of at least 40 for vertices and a
total label occurrence of at least 50 for edges, after man-
ual inspection of the occurrence statistics (see Figure 5).
The pruned, unstructured part of the knowledge graph
contains 84,635 vertices and around 39 million edges with
104,953 different labels between around 9 million con-
nected concept pairs. Another 2.8 million pairs for rela-
tions stemming from UMLS and DrugBank were added
to the graph as edges, but no new concepts were intro-
duced, because the graph would have grown too large if
Figure 5 Distribution of vertex degree and edge labels in unpruned, unstructured part of the knowledge graph, in log-scale. Figure (a) shows the
distribution of vertex degrees. Similarly, Figure (b) shows the distribution of edge labels.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 11 of 19
all concepts of the UMLS would have been included as
vertices.
Path search
Finding paths between two concepts in such a highly
connected graph is computationally challenging, because
search time increases exponentially with the specified
maximum path length. Thus, given a pair of concepts
(cs, ct), only paths up to a certain maximum length m =
4 were extracted by performing a bidirectional search.
During search, synonymsc of concepts on the currently
explored path were not allowed to be explored in the next
step. One problem that arises is the fact that some ver-
tices, called hubs, are connected to many concepts (e.g.,
the concept of cell), which lets the search space explode if
hubs are explored. However, paths running through such
hubs can be considered less informative than paths run-
ning through scarcely connected vertices, because hubs
are very general concepts. Therefore, to avoid this prob-
lem and to make the search algorithm faster, highly con-
nected vertices (degree greater than 100,000) are excluded
from search. Furthermore, in some cases the number of all
possible paths gets very large even with a maximum path
length m = 4. Therefore, search time was limited to 40
seconds per pair.
Datasets and training
Experiments were conducted on two different datasets,
pertaining to two different relations, though the approach
is applicable for learning any new relation, provided that it
comprises concepts from the UMLS metathesaurus. The
first dataset contains 438 concept pairs of the may treat
relation taken from the UMLS. It was constructed with
two restrictions in mind. First, it was ensured that no
drug or disease concept occurred more than once in the
whole dataset and second, every concept in that dataset
had to be part of the pruned graph. The former restriction
assured that the diseases are not dominated by one disease
type (e.g., neoplasms, cardiovascular diseases etc.), but
that many types of diseases are represented proportionally
in each category. The latter restriction was made because
for the extraction of paths the pair of concepts in ques-
tion has to be part of the graph. Figures 6 and 7 show the
distribution of drug and disease types, respectively, con-
tained in that dataset. The second dataset consists of 744
pairs of the has target relation extracted from DrugBank
Figure 6 Distribution of drug types in themay treat dataset. The distribution of the drug types occurrences in themay treat dataset is shown.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 12 of 19
Figure 7 Distribution of disease types in themay treat dataset. The
distribution of the disease types occurrences in themay treat dataset
is shown.
and mapped to UMLS. As for themay treat dataset it was
ensured that all concepts are part of the pruned knowl-
edge graph but multiple occurrences of one concept were
allowed. Figures 8 and 9 show the distribution of drug
and disease types, respectively, contained in that dataset.
Both datasets were constructed by extracting all concept
pairs that are contained in the respective relation from
the UMLS and afterwards the pairs were filtered with the
aforementioned restrictions in mind. Negative examples
were constructed as described in the previous section.
Note that ensuring the exclusiveness of positive and nega-
tive examples can lead to a slightly smaller set of negative
examples. The used datasets are publicly available and can
be found as Additional file 1.
During path extraction, edges labelled with may treat
or has target, respectively, and the sibling (SIB) label were
ignored. The sibling relation expresses that two concepts
have the same parent concept. We found that sibling con-
cepts usually have very similar relations. For example,
drugs of the same family often treat the same diseases.
Thus paths like cs
SIB??? cx may treat?????? ct and cs SIB???
cx
has target?????? ct occurred frequently as positive training
examples for themay treat and has target relation, respec-
tively. Those obvious connections could potentially distort
the results. If not stated otherwise, concept pairs, for
which no paths of the specified lengths could be found,
were excluded in the experimental evaluation. The num-
ber of exclusions depends on the maximum allowed path
length. E.g., only around 36% of all has target pairs have
paths of length 2 (i.e., direct connections). Finally, all mod-
els and training algorithms mentioned in the previous
section were implemented using the FACTORIE toolkit
[30], version 1.0.0-RC1.
Results
All results were obtained by evaluating the proposedmod-
els on the datasets using 10-fold cross validation, if not
stated otherwise. Classification performance was evalu-
ated by the area under the curve (AUC) value of the
ROC-curve, a common classification evaluation method
for information retrieval systems. Other evaluation met-
rics based on the precision of the system are not use-
ful in this context because the datasets consist of an
equal number of positive and negative examples, which
is not the case in reality, where there are much more
negative example pairs (e.g., consider all possible drug-
disease combinations from which only small fraction is
in a may treat relation). Sensitivity (true positive rate)
and specificity (false positive rate), which make up the
ROC curve, are independent of the prior distribution of
positive and negative examples. Special focus should be
given to the steepness of the ROC curves at their begin-
ning, because it can indicate that the models learned some
very characteristic path-patterns for a relation (e.g., see
Tables 4 and 5). Note that example pairs for which no
paths were found were excluded in the evaluation of the
experiment.
Comparison ofmodels and feature types
Table 6 shows the performance of the different mod-
els on the two datasets encoded with both plain one-
of-N and LDA features using only paths of length 3.
The first finding is that the pair-based approach consis-
tently outperforms the path-based approach, for which
logistic regression seems to be the better model. This
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 13 of 19
Figure 8 Distribution of drug types in the has target dataset. The distribution of the drug types occurrences in the has target dataset is shown.
outcome can be explained by considering the fact that
the pair-based approach relies on a much larger fea-
ture space (exponential in the maximum path length
m) compared to the two path-based approaches (lin-
ear and constant in m), providing more information to
the model that seems to be necessary for sophisticated
classification.
Results on the has target dataset show that the AUC can
reach up to 0.8 compared against a random baseline with
0.5 AUC, which picks a class label at random. Thus, our
approach demonstrates its ability to recognize the signal
of the relation.
Another interesting finding is that the ROC-curves of
the pair-based approach are very steep at the beginning up
to a recall level of around 0.6 (see Figure 10). In particular,
this can be observed, when using plain features on the has
target dataset. This indicates that there are some common
path patterns which can be learned by the model and be
used to infer the has target relation. Table 4 shows some
highly weighted example patterns learned by the model.
Figure 9 Distribution of target types in the has target dataset. The distribution of the target types occurrences in the has target dataset is shown.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 14 of 19
Table 4 Example plain path patterns of length 3 for the has
target relation with high feature weights learned by
pair-based logistic regression
Highly weighted feature Explanation(
dep??? induce prep??? in pobj???
)
,(
pobj??? in prep??? express nsubjpass??????
) The substance is inducedinto something, in which
the target (gene/protein)
is expressed.
(
nn?? level nsubjpass?????? measure prep??? in pobj???
)
,(
pobj??? with prep??? associate nsubjpass??????
) Some levels of thesubstance were measured
in something that
is associated with the
target.
(
nn?? level nsubjpass?????? measure prep??? in pobj???
)
,(
pobj??? to prep??? susceptible acomp???? be nsubj???
) Some levels of thesubstanceweremeasured
in something, to which
the target is susceptible.
The impact of the different feature types cannot directly
be inferred from Table 6. In the may treat dataset the
LDA encoding seems to help a lot, but on the has tar-
get dataset, which contains about double the amount of
training examples, it does not. To evaluate the impact
of the different feature types, experiments with different
amounts of training examples of the has target-dataset
were conducted. The results were obtained using cross-
validation and are presented in Figure 11.
Models trained with one-of-N features depend highly
on the amount of supplied training data, whereas models
trained on examples with LDA features do not. This shows
Table 5 Example plain path patterns of length 3 for the
may treat relation with high feature weights learned by
pair-based logistic regression
Highly weighted feature Explanation(
pobj??? with prep??? treat dobj???
)
,(
nsubjpass?????? diagnose prep??? in pobj???
) The drug treatssomething (e.g., a
symptom) that is
diagnosed
together with
the disease.
(
pobj??? by agent???? suppress nsubjpass??????
)
,(
nsubj??? increase prep??? at pobj???
) The drugsuppresses
something that
is increased by
the disease.
(
nsubj??? mimic dobj??? effect prep??? of pobj???
)
,(
nsubj??? appear xcomp???? have dobj??? effects prep??? on pobj???
) The drugsbehavior mimics
the effect of
somethingwhich
seems to have an
effect on the
disease.
Table 6 Results using different models and encoding (path
length 3)
Dataset Model AUC
Plain LDA
may treat LRpair 0.61 0.73
LRpath 0.62 0.71
HMMpath 0.48 0.68
has target LRpair 0.78 0.72
LRpath 0.64 0.67
HMMpath 0.59 0.60
LR logistic regression, HMM Hidden Markov Model, path - path- based feature
encoding; pair - pair-based feature encoding.
With bold, the best AUC values for Plain and LDA are highlighted.
the potential of encoding relations with LDA, as it trans-
fers them into a much lower-dimensional, semantic space,
which reduces the amount of required training data. How-
ever, it can also be seen that information is lost in that
process which explains the lower performance achieved
with a larger training set.
Impact of path length
In order to evaluate the impact of the maximum path
length on the overall performance on the two datasets,
experiments were conducted with the pair-based logistic
regression model on all paths up to length 3 and 4, respec-
tively. Table 7 shows that using paths of length 4 does
not improve the overall performance on the classification
task. This could be due to the fact, that with increasing
maximum length the number of additional informative
paths gets lower, while the total number of extracted paths
gets exponentially bigger and so does the noise and fea-
ture space. This can lead to overfitting of the model to
the training data, because training data is too sparse com-
pared to the large feature space. Figure 10 summarizes the
results of the previous two sections by showing the ROC-
curves for the two datasets with different feature types and
maximal lengths.
Temporal impact of established knowledge
From the previously presented results it is not clear how
much the classifiers depend on the maturity of the respec-
tive knowledge that relates two concepts. It might be the
case that the trained models are only able to discover
relations between pairs that are known to be in that rela-
tion for a long time, which should be reflected in the
amount of literature that implicitly relates these concepts
to each other. A validation dataset consisting of 42 drug
repositioning cases, that were collected manually from lit-
erature, has been used to validate the performance of the
classifier trained on the entire may treat dataset in that
respect. Drug repositioning refers to the application of
known drugs to new diseases. It is an interesting use case
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 15 of 19
Figure 10 ROC curves for the has target andmay treat datasets. Figure (a) shows the ROC curves produced based on the validation conducted on
the has target dataset. Similarly, Figure (b) shows the ROC curves produced based on the validation conducted on themay treat dataset.
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 16 of 19
Figure 11 Change of classification performance using different amounts of training data. The difference in classification performance is plotted
when a varying number of training examples is used for the LDA and the plain feature extraction method respectively.
scenario because these drugs and diseases are usually well
known and described in the literature individually, even
though their connection might have only been established
recently.
The resulting scores are ordered by year of FDA
approval and are presented in Figure 12. The first find-
ing is that the scores seem to be independent from the
year of approval. The classifier is able to classify evenmost
of the very recent repositioning cases with a high score.
These results show that recently established knowledge
can be discovered by this approach and suggest that even
the discovery of new knowledge might be possible. It is
noticeable that the confidence scores of the classifier are
in general very high on the repositioning dataset, consid-
ering that the average classification score of negative pairs
for this classifier is 0.57 with only little variation among
the scores of those negative examples.
Note, that 4 of the 42 examples in the drug repositioning
dataset are also contained in the training set for the may
treat relation, namely 1967-0, 1999-0, 2001-3, 2002-0.
However, they account for less than 10% and therefore do
not affect the qualitative observations of this experiment.
Using indirect connections for relation discovery
In many approaches to knowledge discovery (e.g., for
database curation), only direct mentions of two concepts
Table 7 Impact of maximum path lengths using pair-based
logistic regression
Dataset Length AUC
Plain LDA
may treat 3-3 0.61 0.73
3-4 0.62 0.75
has target 3-3 0.78 0.72
3-4 0.80 0.70
The notation n-mmeans that only paths of minimum length n and maximum
lengthm are allowed.
in one sentence are being considered to assert a spe-
cific relation between two concepts. This approach can be
reflected in our setting by only considering paths of length
2 (i.e. only direct connections), which were excluded for
all previous experiments. The exclusion from the previ-
ous experiments follows the rationale that this approach
aims to find new, unknown facts, based on indirect con-
nections between concepts. Furthermore, the problem of
only using direct connections is that only around 36% of
the has target pairs and 46% of the may treat pairs have
direct connections in the graph, which means that it is
not possible to classify more than those correctly. The
improvements of adding indirect connections as features
can be seen in Figure 13. By using indirect connections
almost twice the number of positive examples can be
ranked highly compared to the case of only using direct
connections. Note that pairs of the has target dataset
which do not have any connections of length 2 or 3,
respectively, were also included in this experiment to illus-
trate the recall improvements when indirect connections
are included as features.
Discussion
The results of the experiments show the potential of
the suggested approach. By considering indirect knowl-
edge, models can be trained to discover hidden rela-
tions between concepts that cannot be extracted directly.
This has several potential applications. One application is
the curation of databases, where new knowledge can be
inferred by combining already established facts. Another
example is the inference of completely new knowledge,
like the task of drug repositioning. A model can learn
from examples typical patterns of indirect connections
between a drug that has been repositioned to a disease.
This requires a simple adoption of the current approach to
only consider knowledge that has been established prior
to the first mention of a drug being a potential reposition-
ing candidate for a disease. Moreover, a trained model can
Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 17 of 19
Figure 12 Confidence scores of trainedmay treat classifier using LDA features on a drug repositioning dataset. The figure shows the results of the
application of the trainedmay treat classifier, to a drug repositioning dataset, with real case studies of repositioning collected from the period 1955
to 2013. The average classification score of negative training pairs is included as baseline at 0.57.
be used to find interesting indirect connections between
two concepts with respect to a specific relation provided
that a curated gold standard of this information can be
generated. Predefined relations are not necessarily a pre-
requisite of the approach, but only a set of concept pairs is
needed to learn characteristic path patterns. In general, it
is very simple to integrate new knowledge sources or learn
path patterns of any relation in the knowledge graph. The
Figure 13 ROC curves using a varying number of path lengths. The
figure shows the ROC curves for using paths of only length 2 and
paths of both length 2 and 3 on the has target dataset.
simplicity in the design of the approach is a great advan-
tage that offers a lot of flexibility regarding the knowledge
sources that can be included in the knowledge graph, and
which has many potential applications.
However, besides all the positive aspects of the sug-
gested approach, there are also problems some of which
are not easily solvable whereas others could be resolved
in future work. The construction of the unstructured
knowledge graph consists of several stages in which errors
occur that accumulate in the resulting graph. For exam-
ple, the concept annotation using MetaMap is in some
cases very poor, especially for genes. The simple word
an gets very frequently annotated with the DIAPH3
gene which has the alias AN. Other examples include the
word impact annotated with the IMPACT gene, and
rare with the Retinoic Acid Response Element (short
alias RARE). Moreover, in scientific articles sentences are
more complicated than in other texts because they tend
to consist of many nested sub-sentences which makes the
linguistic analysis, especially for the dependency parser,
more difficult.
Another issue is incomplete knowledge. For example,
the extraction of information from text does not take co-
JOURNAL OF
BIOMEDICAL SEMANTICS
Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 
DOI 10.1186/s13326-015-0001-9
SOFTWARE Open Access
Similarity-based search of model organism,
disease and drug effect phenotypes
Robert Hoehndorf1,2*, Michael Gruenberger3, Georgios V Gkoutos4 and Paul N Schofield3
Abstract
Background: Semantic similarity measures over phenotype ontologies have been demonstrated to provide a
powerful approach for the analysis of model organism phenotypes, the discovery of animal models of human disease,
novel pathways, gene functions, druggable therapeutic targets, and determination of pathogenicity.
Results: We have developed PhenomeNET 2, a system that enables similarity-based searches over a large repository
of phenotypes in real-time. It can be used to identify strains of model organisms that are phenotypically similar to
human patients, diseases that are phenotypically similar to model organism phenotypes, or drug effect profiles that
are similar to the phenotypes observed in a patient or model organism. PhenomeNET 2 is available at http://aber-owl.
net/phenomenet.
Conclusions: Phenotype-similarity searches can provide a powerful tool for the discovery and investigation of
molecular mechanisms underlying an observed phenotypic manifestation. PhenomeNET 2 facilitates user-defined
similarity searches and allows researchers to analyze their data within a large repository of human, mouse and rat
phenotypes.
Keywords: Phenotype, Semantic similarity, Ontology
Background
Our increasing ability to phenotypically characterize
genetic variants of model organisms, coupled with sys-
tematic and hypothesis-driven mutagenesis efforts, is
resulting in a wealth of information about phenotypes.
Increasingly, phenotype associated information is repre-
sented using ontologies [1], and methods for systematic
analysis of phenotypes need to utilize the knowledge con-
tained in these ontologies [2]. One successful analysis
approach, leveraging ontologies, is the use of semantic
similarity, which applies a similarity measure between
terms in phenotype ontologies so as to compute the phe-
notypic similarity between entities that are represented
by them [3]. Phenotypic similarity between different bio-
logical entities can be indicative of a large number of
*Correspondence: robert.hoehndorf@kaust.edu.sa
1Computational Bioscience Research Center, King Abdullah University of
Science and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia
2Computer, Electrical and Mathematical Sciences & Engineering Division, King
Abdullah University of Science and Technology, 4700 KAUST, 23955-6900
Thuwal, Saudi Arabia
Full list of author information is available at the end of the article
biological relations that span multiple scales, and can be
effectively utilised so as to reveal gene function [4], muta-
tions underlying genetically-based diseases [5-8] as well as
drug-target relationships [9].
One challenge in making these analysis methods and
results available to a wide range of researchers is the com-
plexity involved in preparing the underlying data and the
time required to perform the analysis. We have devel-
oped PhenomeNET 2, a system that provides a web-based
interface to perform similarity-based searches over a large
repository of phenotypes. PhenomeNET 2 is based on
the PhenomeNET platform which pre-computes similar-
ity between a wide range of model organisms, diseases
and drug effect profiles, but does not allow searches based
on user-specified phenotype profiles. PhenomeNET 2 can
now be used tomeasure semantic similarity between user-
specified phenotypic profiles and phenotypes observed
in rat, mouse, nematode worm, slime mold and fruitfly
strains and variants, human diseases and drug-associated
biological effects. The PhenomeNET 2 public webserver
is available at http://aber-owl.net/phenomenet.
© 2015 Hoehndorf et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 2 of 6
Implementation
Overview
Figure 1 provides a high-level overview of the compo-
nents of PhenomeNET 2. These consist of a frontend,
implemented in PHP, and a backend consisting of two
parts: an ontology-based phenotype integration service
that integrates and translates phenotype ontologies of
multiple species, and a similarity service that computes
the semantic (phenotypic) similarity between phenotype
descriptions.
It was previously only possible to explore the Phe-
nomeNet using genes or their identifiers, or labels or
identifiers of diseases that were already included in the
network. A key use case for PhenomeNET 2 is the dis-
covery of phenotypically related mutants and diseases
using investigators own phenotype profiles for search-
ing the network. In order to achieve this, PhenomeNET 2
implements several updates in comparison to the original
PhenomeNET system [5]:
 PhenomeNET 2 has a completely novel and updated
user interface, which facilitates search of animal
model phenotypes, disease phenotypes or drug effect
profiles based on combinations of user-specified
terms from the MP or HPO;
 PhenomeNET 2 contains a revised phenotype
knowledge base over which similarity is computed:
additions include phenotypes from the rat model
organism database [10] and the slime mold model
organism database [11], drug effect profiles [9], and
disease phenotypes from Orphanet [6]; yeast and
zebrafish phenotypes, which were included in the
original PhenomeNET knowledge base, were
Figure 1 PhenomeNET 2 analysis and architecture overview.
removed in PhenomeNET 2 as they do not use a
pre-composed phenotype ontology for characterizing
abnormalities in mutants;
 similarity computation has been reimplemented in
C++ to improve query performance and reduce the
memory footprint.
Cross-species integration
PhenomeNET 2 accepts phenotype descriptions that cor-
respond to terms that are available from either the Human
Phenotype Ontology (HPO) [12] or the Mammalian Phe-
notype Ontology (MP) [13]. Using the definitions cre-
ated for phenotype ontologies [14], we have previously
developed a method to integrate phenotype ontologies
of multiple species into a single framework that can be
used to translate phenotypes between different species
[5]. For this purpose, we integrate species-specific phe-
notype ontologies based on the formal definitions that
have been created for these ontologies [14]. Cross-species
integration is achieved by using the species-independent
anatomy ontology Uberon [15] and the Gene Ontol-
ogy [16] to integrate anatomical entities and biological
processes and functions across species, and the species-
independent ontology of qualities PATO [17] to charac-
terize the type of abnormal phenotypes observed. These
ontologies are combined with anatomy ontologies such
as the Mouse Anatomy ontology [18] and the Founda-
tional Model of Anatomy [19] using a knowledge-based
approach for combining anatomy and phenotype ontolo-
gies [20]. A description logic reasoner can then be used
to infer sub- and super-class relations across mouse and
human phenotype ontologies.
As a new addition, we have added the Dictyostelium
Phenotype Ontology [11] to the set of ontologies in Phe-
nomeNET 2. To integrate this ontology, we have added
formal PATO-based entity-quality definitions [17] to 505
classes. The definitions we created are available at http://
aber-owl.net/aber-owl/dicty/dicty-xp.obo.
In PhenomeNET 2, the integration and inference
method is implemented in Java and relies on the OWL
API [21] and the ELK OWL reasoner [22]. The integrated
phenotype ontology used by PhenomeNET 2, and the
source code for performing the ontology integration and
reasoning, is freely available from the projects website.
Phenotype knowledge base
PhenomeNET 2 utilizes a knowledge base that consists of
animal model phenotypes (slime mold, nematode worm,
fruitfly, rat, mouse), disease phenotypes (Orphanet and
OMIM), and drug effects (SIDER). In comparison to
PhenomeNET, we have added drug effect phenotypes
(described previously [9]), slime mold and rat pheno-
types. To add rat phenotypes, we downloaded the pheno-
Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 3 of 6
type annotations of rat genes with the MP from the Rat
Genome Database ftp://rgd.mcw.edu/pub/data_release/
annotated_rgd_objects_by_ontology/rattus_genes_mp and
incorporated them in PhenomeNET 2 similarly to mouse
phenotypes. In particular, we conjunctively combine the
individual phenotype classes and treat this conjunction
as a phenotypic representation of the gene within Phe-
nomeNET 2. Using this method, we incorporated 6,464
MP phenotypes annotations to 1,057 rat strains, 1,545
genes and 1,860 rat QTLs.
Similarly, we obtain slime mold phenotypes annotated
with the Dictyostelium Phenotype Ontology from Dicty-
Base (http://dictybase.org/db/cgi-bin/dictyBase/download/
download.pl?area=mutant_phenotypes&ID=all-mutants.txt)
and represent the slime mold mutants as a conjunction of
phenotypes.
Genedisease association datasets
We use several curated datasets to evaluate the per-
formance of PhenomeNET 2 for prioritizing candidate
genes of disease. We use the curated set of genedisease
associations from the Rat Genome Database available
at ftp://rgd.mcw.edu/pub/data_release/annotated_rgd_
objects_by_ontology/rattus_genes_rdo, where we filter
the genedisease associations and use only those that
have a direct annotation with an OMIM identifier. We
further use OMIMs genedisease associations, and iden-
tify the rat ortholog using the orthologs provided by
the Rat Genome Database (ftp://rgd.mcw.edu/pub/data_
release/RGD_ORTHOLOGS.txt). Finally, we also use the
curated mouse disease models from the Mouse Genome
Informatics (MGI) database (ftp://ftp.informatics.jax.org/
pub/reports/MGI_Geno_Disease.rpt), excluding condi-
tional mutations and assigning a genedisease association
between gene G and disease D if the genotype annotated
with D involves a mutation in G.
Similarity-based search
The similarity computation in PhenomeNET 2 is imple-
mented in C++ to improve performance over Java-based
implementations. For similarity computation, we use the
groupwise similarity measure SimGIC [23], i.e., the Jac-
card index weighted with information content of each
class. Specifically, information content I(C) of an ontology
class C is based on the probability P(X = C) that a geno-
type or disease annotation X in the phenotype knowledge
base is C:
I(C) = ? log(P(X = C)) (1)
Given two complex phenotypes P and R, where P is
characterized by the ontology classes Cl(P) = P1, . . . ,Pn
and R is characterized by the classes Cl(R) = R1, . . . ,Rm,
we define the similarity between P and R as:
sim(P,R) =
?
x?Cl(R)?Cl(P)
I(x)
?
y?Cl(R)?Cl(P)
I(y) (2)
where Cl(X) is the smallest set containing X that is closed
against the super-class relation in MP, i.e., Cl(X) = {x|x ?
Xor ?y : y ? X ? y MP x} (where y MP xmeans that y is
a subclass of x in MP).
Phenotype similarity is computed using only MP terms
due to the higher performance in prioritizing candidate
genes for diseases using MP [24]. The repository of phe-
notype descriptions over which similarity is computed
consists of the phenotype descriptions available from the
Mouse Genome Informatics (MGI) [25], Rat Genome
Database [10], WormBase [26], DictyBase [11], Saccha-
romyces Genome Database [27], OnlineMendelian Inher-
itance in Man (OMIM) [28], Orphanet [29] and SIDER
databases [30].
The PhenomeNET 2 interface is implemented in PHP
using the Bootstrap CSS stylesheets, and the Phe-
nomeNET 2 interface employs webservices from the
Ontology Lookup Service [31,32] at the European Bioin-
formatics Institute to display ontology structures of the
MP and HPO. Information is processed on the webserver
in PHP which forwards the user-based query to the Java
backend through a Unix socket connection, and receives
the response from the Java backend also through a Unix
socket connection.
Results and discussion
We have developed PhenomeNET 2 which extends
the PhenomeNET platform and enables similarity-
based searches for user-specified phenotype profiles
over a repository of animal model phenotypes, human
Mendelian diseases and drug effect profiles. Our imple-
mentation of PhenomeNET 2 is available at http://aber-
owl.net/phenomenet.
We evaluated the performance of PhenomeNET 2 for
prioritizing candidate genes of disease using rat pheno-
types. As rat models are ranked based on their phenotypic
similarity to the disease, we use a receiver operating char-
acteristic (ROC) curve [33] to evaluate the results. A ROC
curve is a plot of the true positive rate as a function of
the false positive rate, and is derived by comparing pre-
dicted associations against those asserted in the cognate
model organism database. The ROC curve for prioritiz-
ing rat disease models as well as mouse disease models
is shown in Figure 2. The area under the ROC curve is
0.65 when using genedisease associations from the Rat
Genome Database as evaluation set and 0.68 when using
OMIMs genedisease associations as evaluation set.
Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 4 of 6
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Tr
ue
 P
os
iti
ve
 R
at
e
False Positive Rate
Candidate gene prioritization
RGD disease annotations
OMIM disease annotations
MGI disease annotations
x
Figure 2 Performance of candidate gene prediction in
PhenomeNET 2. RGD disease annotations prioritize rat models and
use RGDs disease model annotations as true positives. OMIM disease
annotations prioritize rat models and use OMIMs diseasegene
associations as true positives; OMIM genes are mapped to rat genes
through orthology. MGI disease annotations prioritize mouse models
and use MGIs disease models as true positives. The ROCAUCs are
0.65, 0.68 and 0.86, respectively.
The low recovery of disease annotations from rat mod-
els is likely a consequence of the method of annotation
used by the Rat Genome Database and the inclusion
of very large numbers of olfactory receptor genes in
the annotated gene corpus. Of the total 1,545 rat genes
annotated to MP, 1,265 are olfactory receptors which
each bear a single annotation to taste/olfaction pheno-
type (MP:0005394). Furthermore, the extensive use of
electronic inference through orthology, and the sepa-
rate criteria used for disease and phenotype annotation
means that the disease phenotypes and the annotated
phenotypes of individual rat models often do not match,
i.e., it would be impossible to infer even the domain of
the asserted human or mouse diseases from the pheno-
type annotations for many genes. For example, Col2a1
(RGD:2375) is annotated only to the Chondrodystrophy
(MP:0002657) phenotype but to 30 disease classes as
varied as Stickler syndrome, Femur head necrosis, hypothy-
roidism and myopia using a disparate range of human
disease associations and types of evidence.
To further evaluate query performance and its suitabil-
ity for real-time user queries, we constructed 1,000 ran-
dom queries, each consisting of 10 randomly selected MP
classes, and performed a similarity-based search across
our phenotype repository using the PhenomeNET 2 sys-
tem. An average query using PhenomeNET 2 system
with 10 phenotype terms in the query takes 5.1 seconds
to complete. Compared to the Groovy-based implemen-
tation of PhenomeNET, this is a 12-time improvement
in performance, and this improved performance enables
real-time user-specified queries.
There are several further related tools that use similar
algorithms and perform similar analyses. In particular,
the Phenomizer [34] is a tool for diagnosing patients
based on semantic similarity searchers over OMIM dis-
eases using the Human Phenotype Ontology. Phenomizer
is implemented in Java and can also perform real-time
and user-specified searches. However, it currently uses
the Human Phenotype Ontology and is limited to search-
ing diseases available in the OMIM repository, while
PhenomeNET 2 uses a larger repository and can search
phenotypes across multiple model organism species,
diseases and drug effect profiles.
Another related software is PhenoDigm [35], a system
similar to PhenomeNET in that it precomputes similarity
between model organisms and diseases. PhenoDigm does
not currently support user-defined queries over its repos-
itory of phenotypes. Finally, functionally the most similar
tool to PhenomeNET 2 is the search interface provided
by the Monarch Initiative (http://monarchinitiative.org/
analyze/phenotypes/). The Monarch Initiative provides
the possibility to search mouse and zebrafish models as
well as human diseases based on a set of user-specified
phenotypes. The main differences to PhenomeNET 2 are
the choice of similarity measure and the underlying phe-
notype knowledge base: the Monarch search tool utilizes
the OWLSim tools [7] to compute semantic similarity
instead of simGIC used by PhenomeNET 2, uses a sin-
gle integrated phenotype ontology (the Monarch ontol-
ogy) instead of a combination of multiple species-specific
phenotype ontologies used by PhenomeNET 2, and incor-
porates zebrafish phenotypes but no fly, worm, slimemold
or drug effect phenotypes.
In the future, we plan to incorporate different similarity
measures. For example, we intend to experiment with
using the Semantic Measures Library (SML) [36] and
allow users to select multiple different similarity measures
for their search. However, the use of a generic library
written in Java will require careful evaluation of query
performance.
Conclusions
Whilst PhenomeNET provides a powerful means to
explore the phenomic space occupied by model organ-
isms, human genetic diseases, and pharmacological agents
captured in major data resources, PhenomeNET 2 pro-
vides the ability to take a newly-derived phenotypic pro-
file from the experimental or genetic manipulation of an
organism, or an un-diagnosed patient, and conduct the
phenotypic equivalent of a user-defined BLAST-type
Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 5 of 6
search across a repository of phenotypes. Such a tool is
of interest to many communities concerned with phe-
nomics and the analysis of phenotypes. For example, the
results of a PhenomeNET 2 search will allow investigators
to construct hypotheses about the pathways in which the
gene under investigation is involved by looking for closely
related phenotypes [37], or, in phenotype-driven stud-
ies, prioritize candidate genes in either human or mouse.
The ability to search through drug-related phenotypes will
also help in the formulation of hypotheses about potential
genetic underpinnings of otherwise uncharacterized phe-
notypes through knowledge of drug targets, or in estab-
lishing potential therapeutic strategies where loss of gene
function and drug induced phenotypes are concordant.
Availability and requirements
 Project name: PhenomeNET 2
 Project home page: http://aber-owl.net/
phenomenet and https://code.google.com/p/
phenomeblast
 Operating system(s): Platform independent
 Programming language: Groovy, Java, C++, PHP
 Other requirements: Boost library, OWLAPI, ELK
reasoner
 License: New BSD license
 Any restrictions to use by non-academics: none
Competing interests
The authors declare that they have no competing interests.
Authors contributions
RH, GVG and PNS conceived of the study, evaluated the results and wrote the
paper. MG implemented the interface, RH implemented the backend and
evaluation software. All authors read and approved the final version of the
manuscript.
Acknowledgments
No special funding was received for this study.
Author details
1Computational Bioscience Research Center, King Abdullah University of
Science and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia.
2Computer, Electrical and Mathematical Sciences & Engineering Division, King
Abdullah University of Science and Technology, 4700 KAUST, 23955-6900
Thuwal, Saudi Arabia. 3Department of Computer Science, Aberystwyth
University, Llandinam Building, SY23 3DB Aberystwyth, UK. 4Department of
Physiology, Development & Neuroscience, University of Cambridge, Downing
Street, CB2 3EG Cambridge, UK.
Received: 12 September 2014 Accepted: 24 January 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Alm et al. Journal of Biomedical Semantics  (2015) 6:20 
DOI 10.1186/s13326-015-0014-4
RESEARCH ARTICLE Open Access
Annotation-based feature extraction from sets
of SBMLmodels
Rebekka Alm1,2*, Dagmar Waltemath3, Markus Wolfien3, Olaf Wolkenhauer3,5 and Ron Henkel4
Abstract
Background: Model repositories such as BioModels Database provide computational models of biological systems
for the scientific community. These models contain rich semantic annotations that link model entities to concepts in
well-established bio-ontologies such as Gene Ontology. Consequently, thematically similar models are likely to share
similar annotations. Based on this assumption, we argue that semantic annotations are a suitable tool to characterize
sets of models. These characteristics improve model classification, allow to identify additional features for model
retrieval tasks, and enable the comparison of sets of models.
Results: In this paper we discuss four methods for annotation-based feature extraction from model sets. We tested
all methods on sets of models in SBML format which were composed from BioModels Database. To characterize each
of these sets, we analyzed and extracted concepts from three frequently used ontologies, namely Gene Ontology,
ChEBI and SBO. We find that three out of the methods are suitable to determine characteristic features for arbitrary
sets of models: The selected features vary depending on the underlying model set, and they are also specific to the
chosen model set. We show that the identified features map on concepts that are higher up in the hierarchy of the
ontologies than the concepts used for model annotations. Our analysis also reveals that the information content of
concepts in ontologies and their usage for model annotation do not correlate.
Conclusions: Annotation-based feature extraction enables the comparison of model sets, as opposed to existing
methods for model-to-keyword comparison, or model-to-model comparison.
Keywords: Feature extraction, Model similarity, Bio-ontologies, SBML
Introduction
Thanks to standardization efforts in Systems Biology [1],
modelers today have access to high-quality, curated mod-
els in standard formats. The Systems Biology Markup
Language (SBML) [2] is an XML-based standard format
to encode models as interactions between biological enti-
ties. The emerging networks are furthermore enriched
with semantic annotations [3] which link model parts to
external knowledge in domain-specific ontologies (bio-
ontologies) [4]. Many SBML models live in open model
repositories such as BioModels Database [5], the Phys-
iome Model Repository [6], or JWS Online [7]. These
*Correspondence: rebekka.alm@igd-r.fraunhofer.de
1Department of Multimedia Communication, University of Rostock,
Joachim-Jungius-Str. 11, 18051, Rostock, Germany
2Fraunhofer Institute for Computer Graphics Research IGD,
Joachim-Jungius-Str. 11, 18059, Rostock, Germany
Full list of author information is available at the end of the article
repositories distribute computational models and asso-
ciated data in standard formats. They support neces-
sary management tasks, including curation, annotation,
search, version control, data visualization etc. to different
extents.
BioModels Database implements a native, SQL-based
search [5]. An alternative search is the ranked model
retrieval [8]. Here, models and their annotations are
mapped on pre-defined model features (e. g., model
organism, author, biological entity), leading to a charac-
teristic term vector for each model. The properties of
this vector are numeric values mostly describing term fre-
quency and inverse document frequency (TF-IDF) [9].
The ranking is determined by the comparison of search
terms (i. e. provided keywords) with the extracted char-
acteristic term vector per model. Current approaches are
solely capable of comparing a set of keywords against an
indexed corpus of models and retrieve matching models.
In addition, it is possible to create a characteristic term
© 2015 Alm et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Alm et al. Journal of Biomedical Semantics  (2015) 6:20 Page 2 of 13
vector directly from a model and, subsequently, query a
corpus by example.
For example, a standard search for the keywords cell
cycle in BioModels Database retrieves all models in the
corpus that are relevant to the term cell cycle. Together,
all models returned by this search can be seen as a new,
cell cycle focused, model set (or corpus). The same is
possible for keywords such as apoptosis, calcium oscil-
lation or NF-?B. At this point, we end up with different
sets of thematically related models. To characterize such
a set and, later on, compare them, features describing this
specific model set will be helpful. However, it is prob-
lematic to identify suitable characteristics for arbitrary or
thematically focused sets of models.
In this paper we present four methods for annotation-
based feature extraction from arbitrary sets of SBML
models. Our methods build on combinations of existing
approaches for feature extraction [10-13]. We exemplify
our methods by comparing the characteristic features of
thematic sets to the features of arbitrary sets of SBML
models. The thematic sets were extracted fromBioModels
Database and represent the cell cycle, apoptosis, calcium
oscillation, and NF-?B. Concepts, i. e. terms in the ontol-
ogy, were extracted from three major bio-ontologies used
to semantically enrich models (GO, ChEBI, SBO). We
argue that our methods contribute to the determination
of similarity between sets of SBML models. They also
provide statistics on the use of ontology terms in SBML
models, and on the relation between ontology terms and
models.
Background
Bio-ontologies
SBML is an XML format. It uses an RDF scheme to add
semantic annotations to model parts [14]. Among the
ontologies that are used to enrich SBMLmodels, we chose
here the following three ontologies, which we believe are
the most relevant in model annotation: An ontology of
gene and gene product attributes, theGene Ontology (GO)
[15]; an ontology of chemical entities, the Chemical Enti-
ties in BIology (ChEBI) [16]; and an ontology for modeling
in biology, the Systems Biology Ontology (SBO) [3].
The GO is proposed andmaintained by theGene Ontol-
ogy Consortium. It aims at standardizing the representa-
tion of gene and gene product attributes across species
and databases by a structured, precisely defined, com-
mon, controlled vocabulary. GO covers three domains.
The most important relationships within each domain are
is-a and part-of. Additionally, each concept is linked
to other kinds of information, including many gene and
protein keyword databases.
ChEBI is an ontology of chemical entities of biological
interest. All database entries are is_a linked within the
ontology. Chemical classifications of ChEBI are aligned
with the classification of chemical processes in the GO,
and the majority of chemical processes in GO are defined
in terms of the ChEBI entities that participate in them.
The SBO provides a set of controlled vocabularies of
terms commonly used in Systems Biology. It consists of
seven orthogonal branches. Terms within each branch are
linked by standard is_a relationships. Formal ties to SBO
have been developed for several representation formats in
Systems Biology. SBML elementsa, for example, carry an
optional sboTerm attribute, which allows for a precise
definition of the meaning of encoded model entities and
their relationships.
Feature extraction from ontologies
For feature extraction it is important to group similar
items and to find categories that represent the content of
the objects.
Several techniques to determine similarity use distance
measures as a basis. Common techniques are euclidian or
cosinus distance in vector space [17] or the editing dis-
tance for text [9,17-19]. In the context of this work the
techniques to distances in ontologies and tree structures
are of significance.
The hierarchical structure of the ontology can be used to
determine the (semantic) similarity between objects [17].
A distinction is made between two approaches; the graph-
theoretic and information-theoretic approach.
Examples for the graph-theoretic approach are the
works of Bernstein et al. [17] and Wang et al. [20]. They
describe the traditional approach for distance determi-
nation in ontologies using the number of edges between
the nodes. The inheritance structure is represented in
a directed acyclic graph in which the specialization of
objects increases with each level. In such a graph the
ontology distance can be described as the shortest path
between two nodes. The shorter the distance between
two nodes, the more similar they are. The problem with
this approach is the assumption that the edges represent
uniform distances within a taxonomy; i.e. the semantic
connections are of equal weight. Li et al. therefore inves-
tigate in [21] how path length, depth and local semantic
density influence the quality of the similarity function.
They come to the conclusion, that for a semantic knowl-
edge base especially path length and depth are important
to get similarity results that compare to the human per-
ception of similarity. The similarity values are used in
cluster analysis approaches for hierarchical clustering [22].
Applied to the feature extraction task, we group concepts
based on their distance in the ontology graph for one bio-
ontology at a time. The top-down approach starts with a
cluster containing all concepts and then splits this cluster
into smaller groups. The bottom-up approach starts with
clusters only containing one concept. Those clusters are
merged into larger clusters.
Alm et al. Journal of Biomedical Semantics  (2015) 6:20 Page 3 of 13
The most prominent representative of the information-
theoretic approach is Resnik [12,13]. This approach
exploits the information content of objects to compare.
The more information two objects have in common, the
more similar they are. The information content of a con-
cept c is dependent on the concepts probability. The prob-
ability p(c) is calculated by the frequency freq(c) of the
concept and the count N of all concepts of the ontology. It
is formally defined by Resnik [12]:
p(c) = freq(c)N (1)
If all concepts in an ontology are subordinate to one
item, then this item has the greatest probability of 1,
because its classification always applies. However, the
smaller the probability of a concept is, the higher is its
information content. The information content IC can be
calculated by the negative logarithm of the likelihood:
IC(c) = ? log2 p(c) (2)
For example, the root term of the Gene Ontology sum-
marizes all concepts of the ontology and consequently
has an information content of zero. A child concept such
as establishment of localization (GO_0051234) that sum-
marizes 1408 concepts has a higher information content
of 3.34 and a leaf concept such as natural killer cell
mediated cytotoxicity directed against tumor cell target
(GO_0002420) has the highest information content of
10.59.
In order to determine the common information con-
tent of two objects, one considers the deepest element
that classifies both objects together. The information con-
tent of this element is the degree of mutual information
content.
The Information Content can be used to address the
problem of overgeneralization when using parent con-
cepts as representatives for child concepts [23]. The chal-
lenge of feature extraction in ontologies is to find sum-
marizing features that do not generalize too strongly.
Concepts further up in the ontology are less specific than
concepts further down in the ontology and, thus, have
less information content. Counting the number of refer-
ences of a concept and its successor concepts would rank
the general concept always highest, as it has more refer-
ences. The counting approach does not consider the loss
of specificity when moving up the ontology. Trißl et al.
propose a similarity-based scoring function where a gen-
JOURNAL OF
BIOMEDICAL SEMANTICS
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 
DOI 10.1186/s13326-015-0006-4
RESEARCH Open Access
Evaluating a variety of text-mined features for
automatic protein function prediction with
GOstruct
Christopher S Funk1*, Indika Kahanda2, Asa Ben-Hur2 and Karin M Verspoor3,4
Abstract
Most computational methods that predict protein function do not take advantage of the large amount of information
contained in the biomedical literature. In this work we evaluate both ontology term co-mention and bag-of-words
features mined from the biomedical literature and analyze their impact in the context of a structured output support
vector machine model, GOstruct. We find that even simple literature based features are useful for predicting human
protein function (F-max: Molecular Function = 0.408, Biological Process = 0.461, Cellular Component = 0.608). One
advantage of using literature features is their ability to offer easy verification of automated predictions. We find
through manual inspection of misclassifications that some false positive predictions could be biologically valid
predictions based upon support extracted from the literature. Additionally, we present a medium-throughput
pipeline that was used to annotate a large subset of co-mentions; we suggest that this strategy could help to speed
up the rate at which proteins are curated.
Keywords: Text mining, Protein function prediction, Biomedical concept recognition
Introduction
Characterizing the functions of proteins is an important
task in bioinformatics today. In recent years, many com-
putational methods to predict protein function have been
developed to help understand functions without perform-
ing costly experiments. Most computational methods use
features derived from sequence, structure or protein inter-
action databases [1]; very few take advantage of the wealth
of unstructured information contained in the biomedical
literature. Because little work has been conducted using
the literature for function prediction, it is not clear what
type of text-derived information will be useful for this task
or the best way to incorporate it. In this work, we evaluate
two different types of literature features, co-occurrences
of specific concepts of interest as well as a bag-of-words
model, and assess the most effective way to combine
them for automated function prediction. We also provide
many examples of the usefulness of literature features for
verification or validation of automated predictions.
*Correspondence: christopher.funk@ucdenver.edu
1Computational Bioscience Program, University of Colorado School of
Medicine, 80045 Aurora, CO, USA
Full list of author information is available at the end of the article
Background
Literature mining has been shown to have substantial
promise in the context of automated function prediction,
although there has been limited exploration to date [2].
The literature is a potentially important resource for this
task, as it is well known that the published literature is
the most current repository of biological knowledge and
curation of information into structured resources has not
kept up with the explosion in publication [3]. A few teams
from the first Critical Assessment of Functional Anno-
tation (CAFA) experiments [1] used text-based features
to support prediction of Gene Ontology (GO) functional
annotations [4].
Wong and Shatkay [5] was the only team in CAFA that
used exclusively literature-derived features for function
prediction. They utilized a k-nearest neighbor classifier
with each protein related to a set of predetermined char-
acteristic terms. In order to have enough training data for
each functional class, they condensed information from
all terms to those GO terms in the second level of the
hierarchy, which results in only predicting 34 terms out
of the thousands in the Molecular Function and Biologi-
cal Process sub-ontologies. Recently, there has been more
© 2015 Funk et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 2 of 14
in-depth analysis into how to use text-based features to
represent proteins from the literature without relying on
manually annotated data or information extraction algo-
rithms [6]. This work explored using abstracts along with
unigram/bigram feature representation of proteins.
Another team, Björne and Salakoski [7], utilized
events, specifically molecular interactions, extracted from
biomedical literature along with other types of biological
information from databases; they focused on predicting
the 385 most common GO terms.
The work we presented in the first CAFA [8] is on
a different scale from these previous efforts, and inte-
grates information relevant for predicting protein func-
tion from a range of sources. We utilize as much of the
biomedical literature as possible and are able to make pre-
dictions for the entire Gene Ontology, thanks to a struc-
tured output support vector machine (SVM) approach
called GOstruct [9]. We found in that previous work
that features extracted from the literature alone approach
performance of many commonly used features from non-
literature sources, such as protein-protein interactions
derived from a curated resource. However, we used only
concept co-occurrence features  focusing on simple,
scalable features  leaving open many questions about the
best strategy for representing the literature for the task of
automated protein function prediction.
In this work, we therefore explore a variety of text-
mined features, and different ways of combining these
features, in order to understand better the most effective
way to use literature features for protein function pre-
diction. We have extended our workshop paper [10] by
refining the enhanced GO extraction rules, performing
more extensive analysis of the data at the functional class
level, and extending validation through manual curation
using a medium-throughput curation pipeline.We again
explore these questions in the context of the structured
output SVMmodel, GOstruct.
Methods
An overview of our experimental setup can be seen in
Figure 1 with more specific details about each process
following.
Data
We extracted text features from two different literature
sources: (1) 13,530,032 abstracts available from Medline
on October 23, 2013 with both a title and abstract text
and (2) 595,010 full-text articles from the PubMed Open
Access Collection (PMCOA) downloaded on November
6, 2013. These literature collections were processed
identically and features obtained from both were com-
bined. Gold standard Gene Ontology annotations for
both human and yeast genes were obtained from the
Gene Ontology Annotation (GOA) data sets [11]. Only
annotations derived experimentally were considered (evi-
dence codes EXP, IDA, IPI, IMP, IGI, IEP, TAS). Fur-
thermore, the term Protein Binding (GO:0005515) was
removed due to its broadness and overabundance of
annotations. The human gold standard set consists of
over 13,400 proteins annotated with over 11,000 func-
tional classes while the yeast gold standard set consists
of over 4,500 proteins annotated with over 6,500 func-
tional classes. Even though the gold standard sets are
large, only proteins where there is enough training data
will produce predictions. Additionally, to produce mean-
ingful area under the curve (AUC) scores only GO terms
Figure 1 Overview of the experimental setup used for function prediction.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 3 of 14
with at least 10 annotations in the gold standard are con-
sidered as possible prediction targets; this corresponds to
509 Molecular Function classes, 2,088 Biological Process
classes, and 345 Cellular Component classes.
Literature features
Two different types of literature features were extracted
and evaluated, co-mentions and bag of words. Co-
mentions are mentions of both a specific protein and
concept from the Gene Ontology that co-occur with a
specified span of text; they represent a simple knowledge-
directed approach to represent the information contained
within the biomedical literature. Another representa-
tion of biomedical information is to relate proteins to
words mentioned in the surrounding context; this is a
knowledge-free approach because we are not grounding
what we relate to proteins into some ontology, but only
strings.
Text-mining pipeline
A pipeline was created to automatically extract the two
different types of literature features using Apache UIMA
version 2.4 [12]. Whole abstracts were provided as input
and full-text documents were provided one paragraph at
a time. The pipeline consists of splitting the input doc-
uments into sentences, tokenization, and protein entity
detection through LingPipe trained on CRAFT [13], fol-
lowed by mapping of protein mentions to UniProt identi-
fiers through a protein dictionary. Then, Gene Ontology
(GO) terms are recognized through dictionaries provided
to ConceptMapper [14]. Finally, counts of GO terms
associated with proteins, and sentences containing pro-
teins, are output. A modified pipeline to extract pro-
teins, GO terms, or any entity from an ontology file
from text is available at http://bionlp.sourceforge.net/nlp-
pipelines/. Details of the individual steps are provided
below.
Proteinmention extraction
The protein dictionary consists of over 100,000 protein
targets from 27 different species, all protein targets from
the CAFA2 competition (http://biofunctionprediction.
org). To increase the ability to identify proteins in text,
synonyms for proteins were added from UniProt (UniProt
Consortium 2008) and BioThesaurus version 0.7 [15].
Gene ontology term extraction
The best performing dictionary-based system and param-
eter combination for GO term recognition identified
in previous work was used [16]. ConceptMapper (CM)
is highly configurable dictionary lookup system that is
a native UIMA component. CM is highly configurable
through the use of many parameters. The list of parame-
ters used to extract GO terms is in Additional file 1.
Two different dictionaries were provided to CM to
extract Gene Ontology mentions from text: original and
enhanced. Both dictionaries are based on GO from 2013-
11-13. The original directly utilizes GO terms and syn-
onyms, with the exception that the word activity was
removed from the end of ontology terms. The enhanced
dictionary augments the original dictionary with addi-
tional synonyms for many GO concepts. Rules were man-
ually created by examining variation between ontology
terms and the annotated examples in a natural language
corpus. This enhanced dictionary improved GO recogni-
tion F-measure performance on CRAFT corpus [13,17]
by 0.1 (from 0.49 to 0.59), through application of term
transformation rules to generate synonyms.
A simple rule deals with the many GO terms of the form
X metabolic process, which we have observed often do
not occur literally in published texts. For example, for
term GO:0043705, cyanophycin metabolic process syn-
onyms of cyanophycin metabolism and metabolism of
cyanophycin are generated. It is also noted that most of
the terms in GO are nominals, so it is important to gener-
ate part of speech variants. There are also many positive
regulation of X terms; not only will we generate syn-
onyms of positive regulation of such as stimulation
and pro, but if there exist inflectional and derivational
variants of X we can also substitute that in. For example,
apoptotic stimulation and pro-apoptotic are added for
positive regulation of apoptosis (GO:0043065). The ver-
sion of the enhanced dictionary differs from the dictionary
originally used for CAFA2, as described in [10].
Co-mentions
Co-mentions are based on co-occurrences of entity
and ontology concepts identified in the literature text.
This approach represents a targeted knowledge-based
approach to feature extraction. The co-mentions we use
here consist of a protein and Gene Ontology term that
co-occur anywhere together in a specified span. While
this approach does not capture relations as specific as an
event extraction strategy [7], it is more targeted to the pro-
tein function prediction context as it directly looks for the
GO concepts of the target prediction space. It also has
higher recall since it doesnt require an explicit connec-
tion to be detected between the protein and the function
term.
For these experiments, we considered two spans: sen-
tence and non-sentence. Sentence co-mentions are two
entities of interest seen within a single sentence while
non-sentence co-mentions are those that are mentioned
within the same paragraph/abstract, but not within the
same sentence. The number of co-mentions extracted for
human and yeast proteins using both dictionaries can be
seen in Table 1. For human proteins, the enhanced dic-
tionary identifies 1,500 more GO terms than the original
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 4 of 14
Table 1 Statistics of co-mentions extracted from bothMedline and PMCOA using the different dictionaries for identifying
GO terms
Human
Dictionary Span Unique proteins Unique GO terms Unique co-mentions Total co-mentions
Original
sentence 12,826 14,102 1,473,579 25,765,168
non-sentence 13,459 17,231 3,070,466 147,524,964
combined 13,492 17,424 3,222,619 173,289,862
Enhanced
sentence 12,998 15,415 1,839,360 33,199,284
non-sentence 13,513 18,713 3,725,450 196,761,554
combined 13,536 18,920 3,897,951 229,960,838
Yeast
Dictionary Span Unique proteins Unique GO terms Unique co-mentions Total co-mentions
Original
sentence 5,016 9,471 317,715 2,945,833
non-sentence 5,148 12,582 715,363 18,142,448
combined 5,160 12,819 748,427 21,088,281
Enhanced
sentence 5,063 12,877 414,322 3,853,994
non-sentence 5,160 13,769 901,123 23,986,761
combined 5,167 14,018 939,743 27,840,755
dictionary, which, leads to a 35% increase in the number
of co-mentions identified (?56 million more).
Bag-of-words
Bag-of-words (BoW) features are commonly used in many
text classification tasks. They represent a knowledge-free
approach to feature extraction. For these experiments,
proteins are associated to words from sentences in which
they were mentioned. All words were lowercased and
stop words were removed, but no type of stemming or
lemmatization was applied.
Feature representation
The extracted literature information is provided to the
machine learning framework as sets of features. Each pro-
tein is represented as a list of terms, either Gene Ontology
or words, along with the number of times the term co-
occurs with that protein in all of the biomedical literature.
An example entry from the co-mention features is as fol-
lows: Q9ZPY7, co_GO:0003675=6, co_GO:0005623=2,
co_GO:0009986=2, co_GO:0016020=2. . . . We utilize a
sparse feature representation and only explicitly state the
non-zero features for both co-mentions and BoW.
Experimental setup
We evaluate the performance of literature features using
the structured output SVM approach GOstruct [9].
GOstruct models the problem of predicting GO terms as
a hierarchical multi-label classification task using a sin-
gle classifier. As input, we provide GOstruct with different
sets of literature features for each protein, as described
above, along with the gold standard GO term associa-
tions of that protein, used for training. From these feature
sets, GOstruct learns patterns associating the literature
features to the known functional labels for all proteins
in the training set. Given a set of co-occurring terms
for a single protein, a full set of relevant Gene Ontology
terms can be predicted. In these experiments, we use
no additional resource beyond the literature to represent
proteins.
GOstruct provides confidence scores for each predic-
tion; therefore, all results presented in this paper are
based upon the highest F-measure over all sets of con-
fidence scores, F-max [1]. Precision, recall, and F-max
are reported based on evaluation using 5-fold cross val-
idation. To take into account the structure of the Gene
Ontology, all gold standard annotations and predictions
are expanded via the true path rule to the root node of
GO. The true path rule states that the pathway from
a child term all the way up to its top-level parent(s)
must always be true. We then compare the expanded
set of terms. (This choice of comparison impacts the
interpretations of our results, which is discussed further
below). All experiments were conducted on both yeast and
human.
Note that the true path rule is only utilized during the
evaluation of features through machine learning system
(as discussed in Impact of evaluation metric on perfor-
mance). All numbers reported about the performance and
predictionsmade by themachine learning system have the
rule applied, while numbers strictly referring to counts of
co-mentions mined from the literature do not.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 5 of 14
Human evaluation of co-mentions
To support evaluation of the accuracy of the co-mention
features, we sampled a number of them and asked a
human assessor to rate each one as good (True Positive)
or bad (False Positive), i.e., whether or not it cap-
tures a valid relationship. To assess accuracy of co-
mentions as a whole, 1,500 sentence co-mentions were
randomly sampled from the 33.2 million co-mentions
for annotation. Additionally, three smaller subsets of co-
mentions of specific functional classes, totaling about
500 co-mentions, were selected for annotation to assess
accuracy of sentence co-mentions for specific functional
classes. In total, there were around 3,000 full sentences
annotated.
To enable fast annotation of this rating, we developed
an approach that allows for medium-throughputmanual
annotation of co-mentions, about 60-100 per hour. The
sentence co-mentions are transformed to brat rapid anno-
tation tool (http://brat.nlplab.org/) format. The annotator
views both the identified protein and functional concept
in differing colors within the context of the entire sen-
tence. The annotator is only required to connect them
with a single relationship, either Good-Comention or
Bad-Comention. The annotator was instructed to view
the labeled protein and GO concept as correct and to
only annotate Good-Comention when there exists a
relationship between the specified entities. While a rela-
tionship may exist between the annotated GO category
and another exact mention of the labeled protein, that
would be considered incorrect for the purposes of this
annotation, i.e., it is a decision relative to individual men-
tions of the protein in a specific textual context. We
utilized these annotations to assess quality of a random set
of co-mentions and also to label subsets of co-mentions
containing particular functional concepts.
Results and discussion
Exploring the use of co-mention features
We mined co-mentions from two different text spans and
explore four different ways to use them.
1. only using sentence co-mentions
2. only using non-sentence co-mentions
3. combining counts from sentence and non-sentence
co-mentions into one feature set in the input
representation
4. using two separate feature sets for sentence and
non-sentence co-mentions
The spans were explained in more detail above, under the
Co-mentions section.
The performance of these four different strategies for
combining the co-mention features for the enhanced
dictionary can be seen in Figure 2. Each branch of GO
is predicted and evaluated separately, but the way to
combine features is the same for all branches. Similar
trends are seen with the original dictionary (data not
shown).
Using the two types of co-mentions as two separate fea-
ture sets provide the best performance on all branches of
GO (see green shapes in Figure 2). These two types of
co-mentions encode different but complementary infor-
mation and the classifier is able to build a better model by
considering them separately.
We utilized our medium-throughput human annota-
tion pipeline and curated 1,500 randomly sampled sen-
tence co-mentions; we found that?30% (441 out of 1,500)
appeared to correctly relate the labeled protein with the
labeled function. From these results it seems that sentence
co-mentions contain a high false positive rate, most likely
due tomanymentions of proteins or GO concepts within a
single sentence. Methods for filtering sentences that con-
tain ambiguous mentions, due to both ambiguous protein
names and many annotations within sentences containing
complex syntactic structure, are still to be explored. Addi-
tionally, more complicated relationship or event detection
would reduce the number of false positives seen and
provide the classifier with higher quality sentence co-
mentions, but significantly reduce the total number of
identified co-mentions. It is unclear which method would
be preferred for function prediction features.
Interestingly, non-sentence co-mentions perform better
than sentence co-mentions. This goes against intuition, as
co-mentions within a sentence boundary act as a proxy to
a relationship between the protein and its function. How-
ever, it was seen in Bada et al. [18] that often function
annotations do not occur within a sentence boundary with
the corresponding protein. While coreference resolution
may be required to correctly resolve such relationships,
capturing function concepts in close proximity to a pro-
tein appears to be a useful approximation. This could be
the reason why non-sentence co-mentions perform bet-
ter. Based upon these results, from now on, when we say
co-mention features we are referring to using both sen-
tence and non-sentence as separate feature sets but within
the same classifier.
To establish a baseline we utilized the co-mentions
themselves as a classifier; the co-mentions are used as
the final predictions of the system. We performed eval-
uations using both original and enhanced co-mentions.
Results from combining counts between sentence and
non-sentence co-mentions are presented in Table 2. The
baseline leads to very low precision for all branches but
we do see impressive levels of recall. This signifies that
information from the literature is able to capture relevant
biological information, but because we are able to identify
many different co-mentions the false positive rate is fairly
high.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 6 of 14
Figure 2 Precision, recall, and F-max performance of four different co-mention feature sets on function prediction. Better performance is to
the upper-right and the grey iso bars represent balance between precision and recall. Diamonds  Cellular Component, Circle  Biological Process,
Square  Molecular Function.
Performance on human proteins
We report performance of all four feature sets on human
proteins in Table 2. Comparing the performance of the
co-mention features, we find that the original co-mention
features produce the better performance on Molecular
Function (MF), while the enhanced co-mentions perform
slightly better on both Biological Process (BP) and Cellu-
lar Component (CC). The most surprising result is that
bag of words performed as well as it did, considering the
complexity of the Gene Ontology with its many thousands
of terms. Many text classification tasks utilize BoW and
achieve very good performance while some have tried to
recognize functional classes from text with BoW models
with poorer results [19,20]. Their applicability to function
prediction has only begun to be studied in this work and
Wong et al. [6]. One explanation for their performance
could be due to their higher utilization of the biomedi-
cal literature; co-mentions only capture information when
both a protein and GO term are recognized together while
BoW only relies on a protein to be recognized. In other
words, the knowledge-based co-mentions are limited by
the performance of automatic GO concept recognition, a
challenging task in itself [16], while the BoW features have
no such limitation. In support of that, we note that on
average, there are 2,375 non-zero BoW features per pro-
tein, whereas there are an average of 135 sentence and
250 non-sentence non-zero co-mention features per pro-
tein. The results reported here are for human proteins; in
Additional file 2 we provide results in yeast exhibiting the
same trends observed in human.
Overall, best performance for all branches of the Gene
Ontology is seen when using both co-mentions and the
bag-of-words features. This suggests that all types of fea-
tures provide complementary information. In view of this
observation, we explored an alternative to using the fea-
tures in combination to train a single classifier, which is
to train separate classifiers and combine their scores. This
approach gave similar results to those reported here (data
not shown). It can be difficult to understand the impact
of each type of feature solely by looking at the overall
performance, since it is obtained by averaging across all
proteins; we dive deeper in the following sections and
provide examples that indicate that using co-mentions
produces higher recall than precision.
Another observation to make is that performance for all
three branches of GO as measured using the macro-AUC
is very similar, indicating that the three sub-ontologies
are equally difficult to predict from the literature. The
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 7 of 14
Table 2 Overall performance of literature features on
human proteins
Molecular function
Features F-max Precision Recall macro-AUC
Baseline (Original) 0.094 0.055 0.327 0.680
Baseline (Enhanced) 0.064 0.036 0.322 0.701
Co-mentions (Original) 0.386 0.302 0.533 0.769
Co-mentions (Enhanced) 0.377 0.336 0.447 0.764
BoW 0.394 0.376 0.414 0.768
Co-mentions + BoW 0.408 0.354 0.491 0.790
Biological process
Features F-max Precision Recall macro-AUC
Baseline (Original) 0.134 0.091 0.249 0.610
Baseline (Enhanced) 0.155 0.103 0.311 0.611
Co-mentions (Original) 0.424 0.426 0.422 0.750
Co-mentions (Enhanced) 0.429 0.427 0.430 0.752
BoW 0.461 0.467 0.455 0.768
Co-mentions + BoW 0.459 0.426 0.510 0.779
Cellular component
Features F-max Precision Recall macro-AUC
Baseline (Original) 0.086 0.050 0.305 0.640
Baseline (Enhanced) 0.073 0.041 0.317 0.642
Co-mentions (Original) 0.587 0.590 0.585 0.744
Co-mentions (Enhanced) 0.589 0.583 0.596 0.753
BoW 0.608 0.594 0.624 0.755
Co-mentions + BoW 0.607 0.592 0.622 0.773
Precision, Recall and F-max are micro-averaged across all proteins. Baseline
corresponds to using only the co-mentions mined from the literature as a
classifier. Macro-AUC is the average AUC per GO category. Co-mentions + BoW
utilizes original co-mentions and BoW features within a single classifier.
differences in performance as measured by F-max, which
is micro-averaged, are likely the result of the differences
in the distribution of terms across the different levels in
the three sub-ontologies. The similar performance across
the sub-ontologies is in contrast to what is observed
when using other types of data: MF accuracy is typ-
ically much higher than BP accuracy, especially when
using sequence data [1,8], with the exception of network
data such as protein-protein interactions that yields better
performance in BP.
Exploring differences between original and enhanced
co-mentions
Examining Table 1, we see that the enhanced dictionary
finds ?35% (?56 million) more unique co-mentions,
makes about 32,000 fewer predictions (Table 3) and
performs slightly better at the function prediction
task (Table 2). To elucidate the differences that GO
Table 3 Description of the gold standard human
annotations and predictions made by GOstruct from each
type of feature
Molecular Biological Cellular
function process component
Feature type # Predictions # Predictions # Predictions
Gold standard 36,349 264,631 79,631
Original 102,486 268,068 76,513
Enhanced 64,919 276,734 81,094
BoW 40,499 268,114 77,753
Combined 62,039 386,267 78,475
All numbers are counts based on the predictions broken down by sub-ontology;
these counts have the true path rule applied.
term recognition plays in the function prediction task,
co-mention features and predictions were examined for
individual proteins.
Examining individual predictions it appears that many
of the predictions made from enhanced co-mention fea-
tures are more specific than both the original dictionary
and the gold standard annotations; this is also supported
by further evidence presented in the functional analysis
in the Functional class analysis and Analysis of indi-
vidual Biological Process and Molecular Function classes
sections. For example, in GOstruct predictions using the
original dictionary, DIS3 (Q9Y2L1) is (correctly) anno-
tated with rRNA processing (GO:0006364). Using co-
mentions from the enhanced dictionary, the protein is
predicted to be involved with maturation of 5.8S rRNA
(GO:0000460), a direct child of rRNA processing. There
are 10 more unique sentence and 31 more unique non-
sentence GO term co-mentions provided as features by
the enhanced dictionary. Some of the co-mentions iden-
tified by the enhanced and not by the original dictionary
refer to mRNA cleavage, cell fate determination, and
dsRNA fragmentation. Even though none of these co-
mentions directly correspond to the more specific func-
tion predicted by GOstruct, it could be that the machine
learner is utilizing this extra information to make more
specific predictions. Interestingly, the human DIS3 pro-
tein is not currently known to be involved with the more
specific process, but the yeast DIS3 protein is. We did not
attempt to normalize proteins to specific species because
that is a separate problem in itself. It is probable that if we
normalized protein mentions to specific species or imple-
mented a cross-species evaluation utilizing homology the
results of the enhanced dictionary would show improved
performance.
We expected to see a bigger increase in performance
because we are able to recognize more specific GO
terms utilizing the enhanced dictionary. One possible rea-
son that we dont is due to increased ambiguity in the
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 8 of 14
dictionary. In the enhanced dictionary, for example, a syn-
onym of implantation is added to the term GO:0007566
- embryo implantation. While a majority of the time this
synonym correctly refers to that GO term, there are cases
such as . . . tumor cell implantation for which an incor-
rect co-mention will be added to the feature representa-
tion. These contextually incorrect features could limit the
usefulness of those GO terms and result in noisier fea-
tures. One way to address this may be to create a separate
feature set of only co-mentions based on synonyms so the
machine learner could differentiate or weight them dif-
ferently; this could help improve performance using the
enhanced dictionary co-mentions.
Functional class analysis
We nowmove to an analysis of functional classes to assess
how well different parts of GO are predicted by different
feature sets (Figure 3). We use two separate metrics, depth
within the GO hierarchy and information content (IC) of
the GO term derived from our gold standard annotations.
Because the GO is a graph with multiple inheritance and
depth can be a fuzzy concept [21], we define depth as the
length of the shortest path from the root to the term in the
GO hierarchy. We calculate an annotation-based infor-
mation content(IC) for each GO term based on the gold
standard annotations using the IC statistic described in
Resnik et al. [22].
Figure 3(a) shows the distribution of counts of GO terms
within the gold standard and predictions by both depth
and information content, Figure 3(b) shows the macro-
averaged performance (F-measure) for each feature set by
depth, and Figure 3(c) shows the macro-averaged perfor-
mance for each feature set binned by GO term informa-
tion content. Examining 3(a) we find that terms appear
to be normally distributed with mean depth of 4. Look-
ing at information content, we find that over two-thirds
of the terms have an information content score between
6 and 8, indicating that a majority of terms within the
gold standard set are annotated very few times. Overall,
for all sets of features, performance of concepts decreases
as the depth and information content increases; it is intu-
itive that terms that are more broad, and less informative,
would be easier to predict than terms that are specific and
more informative.
Examining performance by depth (Figure 3(b)) we see a
decrease in performance between depths 1-3, after which
performance levels off. As a function of information con-
tent we obtain a more detailed picture, with a much
larger decrease in performance with increased term speci-
ficity; all features are able to predict low information con-
tent, less interesting terms, such as binding (IC=0.20)
or biological regulation (IC=0.66) with high confidence
(F-measure > 0.8). Performance drops to its lowest for
terms that have information content between 7 and 9
indicating there still remains much work to be done to
accurately predict these specific and informative terms.
Interestingly, there is an increase in performance for
the most specific terms, especially using the BoW and
combined representations; however, there are very few
such terms as seen in (Figure 3(a)), representing very
few proteins, so its not clear if this is a real trend.
Finally, we observe that for both depth and IC analy-
sis the knowledge-free BoW features usually outperform
the knowledge-based co-mentions and that the enhanced
co-mentions usually produce slightly better performance
than the original co-mentions.
Analysis of individual biological process andmolecular
function classes
To further explore the impact of the different features
on predictions, we examined the best (Table 4) and
worst (Table 5) Biological Process andMolecular Function
categories.
Examining the top concepts predicted, it is reinforced
that the enhanced co-mentions are able to make more
informative predictions, in addition to increasing recall
without a loss in precision when compared to the origi-
nal co-mentions. All 12 of the top terms predicted by the
original co-mentions have an information content < 2 as
opposed to only 7 terms from the enhanced co-mentions.
We can compare the performance on specific functional
classes. For example, GO:0007076 - mitotic chromosome
condensation is the second highest predicted GO term
by the enhanced co-mentions (F=0.769) while it is ranked
581 for the original co-mentions (F=0.526). Granted, there
will always be specific cases where one performs better
than the other; from these and previous analyses, we find
that the enhanced co-mentions are able to predict more
informative terms for more proteins than the original co-
mention features (Figure 3 and Table 4). This shows that
improving GO term recognition leads to an improvement
in the specificity of function prediction.
Considering the top concepts predicted by the BoW
features, we see a pattern similar to the enhanced co-
mentions. Five out of the top twelve concepts predicted
have an information content score greater than 6; these
informative terms are different between the two fea-
ture sets. For the top functions predicted by all features
the combined classifier of co-mentions and BoW pro-
ducesmore predictions, leading to higher recall and better
F-measure. Even though some of the top terms predicted
are informative and interesting we still strive for better
performance on the most informative terms.
We also analyze the most difficult functional classes to
predict, results can be seen in Table 5. Between all features
we find some similar terms are difficult to predict; local-
ization and electron carrier activity are in the worst
five from all feature sets. It is interesting to note that
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 9 of 14
Figure 3 Functional class analysis of all GO term annotations and predictions. a) Distribution of the depth and information content of GO
term annotations. As IC values are real numbers, they are binned, and each bar represents a range, e.g. [1,2) includes all depth 1 terms and IC
between 1 and 2 (not including 2). b)Macro-averaged F-measure performance broken down by GO term depth. c)Macro-averaged F-measure
performance binned by GO term information content.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 10 of 14
Table 4 Top biological process andmolecular function classes predicted by each type of feature
Original co-mentions
GO ID Name # Predictions Precision Recall F-measure Depth IC
GO:0009987 cellular process 6,164 0.812 0.875 0.842 1 0.66
GO:0044699 single-organism process 4,849 0.743 0.765 0.754 1 0.96
GO:0044763 single-organism cellular process 4,295 0.681 0.714 0.697 2 1.20
GO:0008152 metabolic process 3,893 0.644 0.726 0.682 1 1.22
GO:0065007 biological regulation 3,615 0.691 0.629 0.658 1 0.90
GO:0071704 organic substance metabolic process 3,489 0.611 0.677 0.643 2 1.42
GO:0050789 regulation of biological process 3,350 0.668 0.601 0.633 2 0.97
GO:0044238 primary metabolic process 3,337 0.593 0.655 0.623 2 1.56
GO:0044237 cellular metabolic process 3,268 0.590 0.644 0.616 2 1.49
GO:0050794 regulation of cellular process 3,156 0.648 0.583 0.614 3 1.11
GO:0050896 response to stimulus 2,968 0.606 0.590 0.597 1 1.62
GO:0043170 macromolecule metabolic process 2,640 0.548 0.618 0.581 3 1.77
Enhanced co-mentions
GO ID Name # Predictions Precision Recall F-measure Depth IC
GO:0009987 cellular process 6,223 0.816 0.887 0.850 1 0.66
GO:0007076 mitotic chromosome condensation 6 0.833 0.714 0.769 4 8.58
GO:0006323 DNA packaging 6 0.833 0.714 0.769 3 7.81
GO:0044699 single-organism process 4,957 0.744 0.783 0.763 1 0.96
GO:0044763 single-organism cellular process 4,423 0.682 0.736 0.708 2 1.20
GO:0008152 metabolic process 3,887 0.643 0.723 0.681 1 1.22
GO:0065007 biological regulation 3,701 0.683 0.636 0.659 1 0.90
GO:0050789 regulation of biological process 3,453 0.662 0.613 0.637 2 0.97
GO:0071704 organic substance metabolic process 3,491 0.605 0.670 0.636 2 1.42
GO:0043252 sodium-independent organic anion transport 11 0.636 0.583 0.608 7 8.50
GO:0000398 mRNA splicing, via spliceosome 140 0.492 0.697 0.577 10 5.88
GO:0006607 NLS-bearing protein import into nucleus 15 0.533 0.571 0.551 6 8.50
Bag-of-words
GO ID Name # Predictions Precision Recall F-measure Depth IC
GO:0009987 cellular process 6,005 0.820 0.869 0.844 1 0.66
GO:0044699 single-organism process 4,940 0.754 0.799 0.776 1 0.96
GO:0044763 single-organism cellular process 4,449 0.696 0.764 0.728 2 1.20
GO:0043252 sodium-independent organic anion transport 8 0.875 0.583 0.700 7 8.50
GO:0065007 biological regulation 3,865 0.698 0.686 0.692 1 0.90
GO:0008152 metabolic process 3,870 0.647 0.733 0.688 1 1.22
GO:0050789 regulation of biological process 3,597 0.680 0.663 0.671 2 0.97
GO:0006479 protein methylation 13 0.615 0.727 0.666 8 6.52
GO:0051568 histone H3-K4 methylation 13 0.615 0.727 0.666 11 7.94
GO:0007076 mitotic chromosome condensation 5 0.800 0.571 0.666 4 8.58
GO:0050794 regulation of cellular process 3,440 0.657 0.651 0.654 3 1.11
GO:0006497 protein lipidation 9 0.889 0.500 0.640 7 6.79
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 11 of 14
Table 4 Top biological process andmolecular function classes predicted by each type of feature (Continued)
Co-mentions + Bag-of-words
GO ID Name # Predictions Precision Recall F-measure Depth IC
GO:0009987 cellular process 6,420 0.813 0.913 0.860 1 0.66
GO:0044699 single-organism process 5,338 0.736 0.834 0.782 1 0.96
GO:0044763 single-organism cellular process 4,862 0.674 0.800 0.731 2 1.20
GO:0065007 biological regulation 4,445 0.669 0.749 0.707 1 0.90
GO:0008152 metabolic process 4,252 0.638 0.785 0.704 1 1.22
GO:0050789 regulation of biological process 4,199 0.650 0.733 0.689 2 0.97
GO:0050794 regulation of cellular process 4,046 0.626 0.723 0.671 3 1.11
GO:0043252 sodium-independent organic anion transport 15 0.600 0.750 0.667 7 8.50
GO:0071704 organic substance metabolic process 3,883 0.602 0.743 0.665 2 1.42
GO:0043170 macromolecule metabolic process 3,007 0.540 0.694 0.607 3 1.77
GO:0051716 cellular response to stimulus 3,176 0.520 0.674 0.587 3 1.89
GO:0006386 termination of RNA polymerase III transcription 12 0.583 0.583 0.583 7 8.18
Table 5 Most difficult biological process andmolecular function classes
Original co-mentions
GO ID Name # Predictions Precision Recall F-measure IC
GO:0051179 localization 28 0.107 0.054 0.072 5.70
GO:0016247 channel regulator activity 115 0.043 0.208 0.071 6.53
GO:0009055 electron carrier activity 108 0.03 0.111 0.055 6.94
GO:0007067 mitosis 23 0.043 0.031 0.036 7.54
GO:0042056 chemoattractant activity 53 0.018 0.067 0.029 7.56
Enhanced co-mentions
GO ID Name # Predictions Precision Recall F-measure IC
GO:0009055 electron carrier activity 102 0.090 0.138 0.109 6.94
GO:0051179 localization 42 0.071 0.055 0.061 5.70
GO:0019838 growth factor binding 44 0.021 0.035 0.027 5.99
GO:0070888 E-box binding 99 0.010 0.066 0.019 7.49
GO:0030545 receptor regulator activity 152 0.007 0.020 0.010 7.63
Bag-of-words
GO ID Name # Predictions Precision Recall F-measure IC
GO:0051179 localization 18 0.277 0.090 0.137 5.70
GO:0009055 electron carrier activity 29 0.103 0.083 0.092 6.94
GO:0016042 lipid catabolic process 26 0.076 0.054 0.063 5.80
GO:0015992 proton transport 15 0.066 0.047 0.055 7.29
GO:0005516 calmodulin binding 14 0.071 0.033 0.045 7.25
Co-mentions + Bag-of-words
GO ID Name # Predictions Precision Recall F-measure IC
GO:0051179 localization 61 0.100 0.109 0.104 5.70
GO:0009055 electron carrier activity 62 0.079 0.138 0.101 6.94
GO:0030545 receptor regulator activity 63 0.064 0.080 0.071 7.63
GO:0042056 chemoattractant activity 24 0.041 0.066 0.051 7.56
GO:0040007 growth 27 0.030 0.066 0.047 7.33
IC represents information content of term.
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 12 of 14
the information content of these difficult to predict terms
lies around the median range for all predicted terms. We
might have expected that the most difficult terms to pre-
dict would be those most informative terms (IC around
10). We believe that these terms are difficult to predict
because the ontological term names are made up of com-
mon words that will be seen many times in the biomedical
literature, even when not related to protein function. This
ambiguity likely results in a high number of features cor-
responding to these terms which results in poor predictive
performance. There is still further work needed to address
these shortcomings of literature mined features.
Manual analysis of predictions
Manual analysis of individual predictions
We know that GO annotations are incomplete and there-
fore some predictions that are classified as false positives
could be actually correct. The predictionmay even be sup-
ported by an existing publication, however due to the slow
process of curation they are not yet in a database. We
manually examined false positive predictions that contain
sentence level co-mentions of the protein and predicted
function to identify a few examples of predictions that
look correct but are counted as incorrect:
 Protein GCNT1 (Q02742) was predicted to be
involved with carbohydrate metabolic process
(GO:0006959). In PMID:23646466 [23] we find
Genes related to carbohydrate metabolism include
PPP1R3C, B3GNT1, and GCNT1. . . .
 Protein CERS2 (Q96G23) was predicted to play a role
in ceramide biosynthetic process (GO:0046513). In
PMID:22144673 [24] we see . . .CerS2, which uses
C22-CoA for ceramide synthesis. . . .
These are just two examples taken from the co-
mentions, but there are most likely more, which could
mean that the true performance of the system is underes-
timated. Through these examples we show how the input
features can be used not only for prediction, but also for
validation. This is not possible when using features that
are not mined from the biomedical literature and illustrate
their importance.
Manual analysis of functional classes
In the previous section we explored individual co-
mentions that could serve as validation for an incorrect
GOstruct prediction. In addition to this one-off anal-
ysis, we can label subsets of co-mentions pertaining
to particular functional concepts for validation on
a medium-throughput scale. To identify functional
classes for additional exploration, all GO concepts
were examined for three criteria: 1) their involve-
ment in numerous co-mentions with human proteins
2) numerous predictions made with an overall aver-
age performance and 3) confidence in the ability to
extract the concept from text. The concepts chosen
for further annotation were GO:0009966  regulation
of signal transduction, GO:0022857  transmembrane
transporter, and GO:0008144 - drug binding. For each
of these classes all human co-mentions were manually
examined.
We identified 204 co-mentions between a human pro-
tein and GO:0008144 - drug binding (IC=6.63). Out of
204 co-mentions, 112 appeared to correctly related the
concept with the protein (precision of 0.554). 61 unique
proteins were linked to the correct 112 co-mentions. Of
these, only 4 contained annotations of drug binding
in GOA, while the other 57 are not currently known
to be involved with drug binding. When we exam-
ined the predictions made by GOstruct for these pro-
teins, unfortunately, none of them were predicted as drug
binding. After further examination of the co-mentions,
most appear to be from structure papers and refer to drug
binding pockets within specific residues or domains of the
proteins. It is unlikely that the specific drug could be iden-
tified from the context of the sentence and many refer
to a proposed binding site with no experimental data for
support.
The concept GO:0022857 - transmembrane trans-
porter (IC=4.17) co-occurred with a human protein 181
different times. 69 co-mentions appeared to correctly
relate the concept with the labeled protein (precision of
0.381). A total of 32 proteins could be annotated with
this concept; out of the 32 only 6 are not already anno-
tated with transmembrane transporter in GOA. When
we examine the predictions made from the enhanced fea-
tures, only 1 out of the 6 proteins are predicted to be
involved with transmembrane transporter.
There were a total of 134 human co-mentions con-
taining GO:0009966  regulation of signal transduction
(IC=3.30). 73 out of 134 co-mentions appeared to cor-
rectly relate the concept with the protein (precision of
0.543). A total of 58 proteins could be annotated based
upon these co-mentions. 21 proteins already contain
annotations conceptually related to regulation of signal
transduction, while the other 37 proteins do not contain
annotations related to regulation of signal transduction;
the later could represent true but uncurated functions.
When we examine the predictions made by GOstruct
using the enhanced co-mention features, 9 out of those 37
proteins were predicted to be involved with regulation of
signal transduction.
When a random subset of 1,500 human co-mentions
were labeled it was found that ?30% (441 out of 1,500)
correctly related the labeled protein and GO term. By
annotating co-mentions of specific functional concepts
we see that these categories have a higher proportion of
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 13 of 14
correct co-mentions than the random sample from all
co-mention; there will also be some categories where per-
formance of co-mentions is quite low. This information
can be used in multiple different ways. If we are more con-
fident that certain categories related to function can be
extracted from co-mentions, we can use this information
to inform the classifier by encoding the information into
the input features. Additionally, we show the importance
and ability of co-mentions to not only be used as input fea-
tures, but also for validation and enhancing the machine
learning results. We show that many of the predictions
made by our system could possibly be correct, but just not
curated in the gold standard annotations.
Impact of evaluationmetric on performance
In our initial experiments, we required predictions and
gold standard annotations to match exactly (data not
shown), but we found, through manual examination of
predictions, that many false positives are very close (in
terms of ontological distance) to the gold standard anno-
tations. This type of evaluation measures the ability of a
system to predict functions exactly, at the correct speci-
ficity in the hierarchy, but it doesnt accurately represent
the overall performance of the system. It is preferable to
score predictions that are close to gold standard annota-
tions higher than a far distant prediction. We are aware
of more sophisticated methods to calculate precision and
recall that take into account conceptual overlap for hier-
archical classification scenarios [25,26]. For the results
reported in Table 2, to take into account the hierarchy of
the Gene Ontology, we expanded both the predictions and
annotations via the true path rule to the root. By doing
this, we see a large increase in both precision and recall
of all features; this increase in performance suggests that
many of the predictions made are close to the actual anno-
tations and performance is better than previously thought.
A downside of our chosen comparison method is that
many false positives could be introduced via an incor-
rect prediction that is of a very specific functional class.
This could possibly explain why co-mentions from the
enhanced dictionary display a decrease in performance; a
single, very specific, incorrect prediction introduces many
false positives.
Conclusions
In this work we explored the use of protein-related fea-
tures derived from the published biomedical literature to
support protein function prediction. We evaluated two
different types of literature features, ontology concept co-
mentions and bag-of-words, and analyzed their impact
on the function prediction task. Both types of features
provided similar levels of performance. The advantage
of the bag-of-words approach is its simplicity. The addi-
tional effort required to identify GO term mentions in
text pays off by offering users the ability to validate pre-
dictions by viewing the specific literature context from
which an association is derived, as demonstrated in our
experiments.
In addition, we compared the value of concept
co-mentions considering two different spans of co-
occurrence: within a sentence (sentence co-mention)
and across a sentence boundary (sentence-external, or
non-sentence co-mention). Interestingly, we found that
sentence and non-sentence co-mentions are equally use-
ful, and that they are best used in conjunction as separate
feature sets. Combining co-mentions and bag-of-words
data provided only a marginal advantage, and in future
work we will explore ways to obtain better performance
from these features together. We also show that increas-
ing the ability to recognize GO terms from biomedical text
leads to more informative functional predictions. Addi-
tionally, the literature data we used provides performance
that is on par with other sources of data such as network
and sequence and has the advantage of being easy to verify
on the basis of the text.
Our experiment in medium-throughput manual inspec-
tion of protein-GO term co-mentions suggests that this
strategy can be used as a way of speeding up the process
of curation of protein function. The literature contains
millions of co-mentions, and a human-in-the-loop sys-
tem based on the detected co-mentions prioritized by
GOstruct can be a highly effective method to dramati-
cally speed up the rate at which proteins are currently
annotated.
Future work
This work marks only the beginning of incorporating text
mining for protein function prediction. There are always
other more sophisticated or semantic features to explore,
but based upon these results, there are some natural next
steps.
The first would be to incorporate larger spans for a bag-
of-words model due to the surprising performance of the
non-sentence co-mentions. By including words from sur-
rounding sentences, or an entire paragraph, more context
would be en-coded and the model might result in better
predictions.
Secondly, we found that an enhanced dictionary pro-
duced more individual co-mentions and fewer predic-
tions, resulting in slightly increased performance. We
explored several possible explanations as to why there is
not a greater impact. It could be due to a large num-
ber of competing co-mentions that prevent good patterns
from emerging or the possibility of introducing noise
through ambiguous protein mentions. A filter or classifier
that could identify a good co-mention would be pro-
viding much higher quality co-mentions as input, which
would in turn likely lead to better predictions. Another
Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 14 of 14
way to potentially improve performance is to separate
co-mentions found from synonyms from the original co-
mentions, thereby allowing the classifier to provide them
with different weights.
Additional files
Additional file 1: ConceptMapper parameters. A description of the
parameter values and impact for GO extraction through ConceptMapper.
Additional file 2: Yeast results. Function prediction performance
numbers for prediction on yeast proteins.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CSF developed NLP pipelines and generated literature features. IK performed
all runs using GOstruct and provided prediction output to CSF for analysis. KV
and ABH contributed to the design of methods and offered supervision at
every step. All authors read and approved the manuscript.
Acknowledgements
This work was funded by NIH grant 2T15LM009451 and NSF grants
DBI-0965616 and DBI-0965768 to ABH. KV was partially supported by NICTA,
which is funded by the Australian Government as represented by the
Department of Broadband, Communications and the Digital Economy and the
Australian Research Council through the ICT Centre of Excellence program.
Author details
1Computational Bioscience Program, University of Colorado School of
Medicine, 80045 Aurora, CO, USA. 2Department of Computer Science,
Colorado State University, 80523 Fort Collins, CO, USA. 3Department of
Computing and Information Systems, University of Melbourne, 3010 Parkville,
Victoria, Australia. 4Health and Biomedical Informatics Centre, University of
Melbourne, 3010 Parkville, Victoria, Australia.
Received: 3 November 2014 Accepted: 27 February 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Xiang et al. Journal of Biomedical Semantics 2015, 6:4
http://www.jbiomedsem.com/content/6/1/4SOFTWARE Open AccessOntorat: automatic generation of new ontology
terms, annotations, and axioms based on ontology
design patterns
Zuoshuang Xiang1, Jie Zheng2, Yu Lin1 and Yongqun He1*Abstract
Background: It is time-consuming to build an ontology with many terms and axioms. Thus it is desired to automate
the process of ontology development. Ontology Design Patterns (ODPs) provide a reusable solution to solve a
recurrent modeling problem in the context of ontology engineering. Because ontology terms often follow specific
ODPs, the Ontology for Biomedical Investigations (OBI) developers proposed a Quick Term Templates (QTTs)
process targeted at generating new ontology classes following the same pattern, using term templates in a
spreadsheet format.
Results: Inspired by the ODPs and QTTs, the Ontorat web application is developed to automatically generate new
ontology terms, annotations of terms, and logical axioms based on a specific ODP(s). The inputs of an Ontorat
execution include axiom expression settings, an input data file, ID generation settings, and a target ontology
(optional). The axiom expression settings can be saved as a predesigned Ontorat setting format text file for reuse.
The input data file is generated based on a template file created by a specific ODP (text or Excel format). Ontorat is
an efficient tool for ontology expansion. Different use cases are described. For example, Ontorat was applied to
automatically generate over 1,000 Japan RIKEN cell line cell terms with both logical axioms and rich annotation
axioms in the Cell Line Ontology (CLO). Approximately 800 licensed animal vaccines were represented and annotated
in the Vaccine Ontology (VO) by Ontorat. The OBI team used Ontorat to add assay and device terms required by
ENCODE project. Ontorat was also used to add missing annotations to all existing Biobank specific terms in the
Biobank Ontology. A collection of ODPs and templates with examples are provided on the Ontorat website and
can be reused to facilitate ontology development.
Conclusions: With ever increasing ontology development and applications, Ontorat provides a timely platform for
generating and annotating a large number of ontology terms by following design patterns.
Availability: http://ontorat.hegroup.org/
Keywords: Ontorat, Ontology design pattern, ODP, Quick term templates, QTT, Ontology developmentBackground
The Web Ontology Language (OWL) has been widely
used for ontology development. However, ontology devel-
opment and updating in OWL format is often time con-
suming and requires specialized knowledge of ontology
tools as well as specific scientific domains. Ways to im-
prove the process of ontology development are desirable.
It is frequently observed that a large number of new* Correspondence: yongqunh@med.umich.edu
1University of Michigan, Ann Arbor, MI, USA
Full list of author information is available at the end of the article
© 2015 Xiang et al.; licensee BioMed Central. T
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.ontology terms and term annotations follow the same
design patterns of logical definitions and axioms. An
ontology term refers to a term with a Uniform Resource
Identifier (URI) in the ontology. Even with the help of
the Protégé-OWL editor (http://protege.stanford.edu/),
manual adding and editing of these terms and annota-
tions is labor-intensive and time-consuming. To make
the ontology development more efficient, it is possible
to develop tools to automate the process of adding the
ontology contents with repetitive design patterns.
An OWL format ontology includes a set of axioms
that provides explicit logical assertions about three typeshis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 2 of 10
http://www.jbiomedsem.com/content/6/1/4of entities - classes, individuals and properties. As with
software design patterns for software engineering, an
Ontology Design Pattern (ODP) represents a reusable so-
lution to solve a recurrent modeling problem in the con-
text of ontology engineering. ODPs can be applied to
support ontology rational design and development, im-
prove ontology quality and reuse, disambiguate relations,
provide scalable representations of entities, and make on-
tologies more maintainable and understandable [1-5]. The
web portal of ODPs (http://ontologytdesignpatterns.org)
has collected many ODPs in different fields [6]. ODPs
have also been studied in biological and biomedical
fields [1-4,7,8]. ODPs can be represented using ontological
axioms or graphic diagrams.
Since many ontology terms (e.g., assays, vaccines) follow
the same design patterns, it is possible to apply specific
ODPs in new ontology term generations to support
ontology enrichment and expansion. To support quick
generation of new ontology classes, the developers of
the Ontology for Biomedical Investigations (OBI) [9]
proposed the usage of a Quick Term Template (QTT),
which is a spreadsheet template for populating terms
to define specific ontology classes [10]. The populated
template spreadsheet can then be converted into an
OWL file with newly generated ontology classes. The
generation of QTT templates relies on repeatable patterns
of to-be-generated ontology classes [10]. The conversion
of an input file generated using a QTT template to an
OWL output document could be implemented using
MappingMaster, a plugin program in the Protégé-OWL
editor [11,12]. The MappingMaster plugin works in
Protégé-OWL editor version 3.4 that only supports
OWL 1. However, the tool does not function in Protege
4.0 or higher versions that support OWL 2.0 and have
become the main choices of ontology developers.
Inspired by the ODP theories and OBI project QTT
operation, we developed Ontorat (http://ontorat.hegroup.
org/), a web application with the aim to automatically gen-
erate a large number of new ontology classes or add add-
itional axioms (e.g. annotations) to existing classes for a
specific target ontology. Ontorat offers a web-based plat-
form for writing up ontology axiom expressions with vari-
ables. Based on the axiom settings and a user-provided
input data file populated on a QTT-like template, Ontorat
is able to generate an OWL format output file, which can
be imported into a target ontology to enrich and expand
the ontology. Ontorat was first presented in the ICBO-
2012 conference as a software demo [13]. The tool has
been much improved during the past two years, including
bug fixes, web user interface improvements, and new fea-
ture additions. Ontorat has been used in enriching several
widely-used ontologies including the Vaccine Ontology
(VO) [14,15], the Ontology for Biomedical Investigations
(OBI) [9], and the Cell Line Ontology (CLO) [16]. Toallow users to better understand and use the tool, we pro-
vide systematic descriptions and use case examples of the
Ontorat in this paper.
Overall design
Based on the ODP concept and the Quick Term Templates
(QTT) procedure, we developed an overall strategy of ap-
plying these mechanisms to ontology expansion (Figure 1).
First, an ODP that covers a set of terms and their relations
needs to be identified (Figure 1a). Formal axioms that assert
logical relations among ontology terms and annotations
of these terms will then be specified based on the ODP
(Figure 1b). The ODP will guide the generation of a tab-
delimited text or Excel template file which would con-
tain all terms and annotations needed to define targeted
terms (Figure 1c). This template file will then be used to
populate specific contents (Figure 1d). By combining the
axiom settings and the input data file, an OWL format
output can be generated (Figure 1e).
We have developed the web-based Ontorat tool that
implements the ontology enrichment strategy shown in
Figure 1. Figure 2 lays out the Ontorat design and work-
flow pipeline. Specifically, on the Ontorat web page, a user
enters setting options and uploads the input data file via
the Ontorat web input form. The input data file is gener-
ated by populating a predesigned template file guided by
the ODP as mentioned above. After accepting the input
data file and setting options from the user, the web server
(via a PHP script) will be able to execute two operations: 1)
generation of new ontology classes with logical axioms
and annotations, or 2) addition of new axioms to existing
ontology terms. The Ontorat server will process the users
requests and generate either an Ontorat settings file or an
OWL output file. The Ontorat settings file can be stored
and reused later. For the OWL output generation, a
Manchester syntax file will be generated first and then
transferred to OWL format (Figure 2).
Implementation
Sever setup
The Ontorat server is a single HP server running the Red
Hat Linux operating system (Red Hat Enterprise Linux 6).
The Selinux program is enabled to improve the security
and stability of the server. The open source Apache HTTP
Server is installed as the HTTP application server. PHP
is used as the programming language in the web appli-
cation server. OWL API is used for OWL format data
operations.
Ontorat (http://ontorat.hegroup.org) provides a user-
friendly web form for data input (Figure 3).
Ontorat inputs
As guided by the general strategy shown in Figure 1, an
Ontorat execution requires two types of required inputs:
Figure 1 The strategy of applying ODPs into ontology term and annotation generation. An ODP is used to guide the generation of axiom
settings and a template file (text or Excel format). The template file is populated with specific contents to create an input data file. Based on the
axiom assertions and input data file, an OWL output can be generated by a software program to expand a targeted ontology.
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 3 of 10
http://www.jbiomedsem.com/content/6/1/4 Input data file, Figure 3 (2):
An Ontorat template file is usually generated first
based on the ODP including all term and annotation
types needed for defining a target term and then
filled up with specific terms and annotations for
each type. The file can be provided in an Excel or
tab-delimited text format.Figure 2 Ontorat software overall design and workflow. See the text for Axiom settings, Figure 3 (4)-(6):
The axioms are represented using Manchester OWL
Syntax [17] in Ontorat. The axiom settings can be added
one by one via the Ontorat web form or uploaded from
an Ontorat setting text file in an Ontorat-specific setting
file format. Ontorat can also generate the setting file
based on the setting inputs via the Ontorat web form.description.
Figure 3 The Ontorat web interface with explanation. The balloons represent components of the Ontorat web form for users to provide or
click. It is noted that some components are optional. The text notes inside boxes are the explanation notes for specific Ontorat components in
the web form.
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 4 of 10
http://www.jbiomedsem.com/content/6/1/4
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 5 of 10
http://www.jbiomedsem.com/content/6/1/4Three types of axiom assertions are allowed in
Ontorat:
a. Annotations. The annotations associate
information with an ontology class. Each
annotation includes an annotation property with
its value [18].
b. Equivalent classes. Equivalent classes provide
both sufficient and necessary axiom assertions to
define an ontology class.
c. Superclasses. Superclass axioms assert the parents
of an ontology class.
In Ontorat, the above ontology axioms are formatted
using the Manchester OWL Syntax, a logical
syntax designed for writing OWL class expressions
(http://www.w3.org/TR/owl2-manchester-syntax/)
[17]. An internally designed code is used to represent
different columns (i.e., variables). Specifically, we use
{$columnA} to represent the first column (or column
A), and use {$columnB} to represent column B, etc.
Each column represents a variable that will be used to
define an ontology class.
In addition, the URIs of terms, including many
commonly used properties (e.g., rdfs:label) shown
in the axiom settings (Figure 3 (7)), need to be
specified since the Ontorat program cannot know
they are ontology terms unless their URIs are
provided.In addition, Ontorat requests two other types of inputs
before execution.
 Operation type, Figure 3 (3):
Ontorat supports two kinds of operations based on
purposes: (1) generation of new ontology classes
with axioms, and (2) modification of existing
ontology classes with adding new axioms. An
Ontorat user is requested to specify the purpose of
an Ontorat operation.
 Inputs for assigning unique URIs to newly generated
terms:
When Ontorat generates new classes, unique
URIs will be assigned to newly generated terms.
To achieve this task, the following information is
needed:
a. Target ontology, Figure 3 (1):A user has an option to provide a target ontology
to ensure that unique ontology IDs will be
assigned to newly generated ontology terms.
Ontorat currently does not retrieve the
information of ontology from existing ontology
RDF triple store. To provide a target ontology, an
Ontorat user can either upload the target
ontology from a local computer or provide the
URL of the target ontology.b. Start portion of term URI, Figure 3 (8):
The start portion of term URI used for newly
added terms need to be specified. For example,
the string http://purl.obolibrary.org/obo/ is used
as the start portion of a URI of a term in an OBO
Foundry ontology.
c. Information of auto-generated term ID, Figure 3 (9):
Three data items are needed: prefix, number of
digits, and the start ID number. For example, the
Vaccine Ontology (VO) terms have the prefix of
VO_ that is followed by 7 digits. We can manually
specify the start ID from 1 or from another
number (e.g., 10000). This feature has a pitfall
since the incrementally assigned IDs from the start
ID may duplicate existing IDs in the target ontology.
To avoid this potential conflict, users may upload
the target ontology as described above. With the
target ontology provided, Ontorat will ensure the
automatic generation of non-replicated IDs.After the above information is provided manually,
Ontorat can generate an input setting text file for later
reuse (Figure 3), which is an important feature of Ontorat.
Ontorat outputs
Based on a users request, the Ontorat can generate two
kinds of outputs: an OWL file converted from a spread-
sheet data file based on axiom settings, and an input setting
file described above.
The Ontorat output OWL file can be visualized using
OWL ontology editors such as Protégé (http://protege.
stanford.edu/). The output OWL file can be imported to
a target ontology (e.g., VO) using the OWL import function
or merged to enrich the target ontology.
It is noted that a Manchester syntax file is generated
internally as an intermediate file which is used as the input
to generate a final OWL output file. When an error occurs
in translating the Manchester syntax to OWL format,
Ontorat will be able to provide the intermediate Manches-
ter syntax file for debugging.
Availability
The Ontorat program is freely available on the website:
http://ontorat.hegroup.org/. The source code of the
Ontorat software is released and available for downloading
on Github: https://github.com/ontoden/ontorat. The source
code is open source with the license of Apache License 2.0.
Features and usage
As described above, the Ontorat web application supports
two operations: generation of new ontology classes with
axioms, and adding new axioms to existing ontology
classes. Three types of axiom assertions (for asserting
annotations, equivalent classes, and superclasses) are
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 6 of 10
http://www.jbiomedsem.com/content/6/1/4allowed in Ontorat. In this section, we will use two spe-
cific examples to demonstrate how Ontorat supports
the above features, briefly summarize other use cases,
and then describe the Ontorat collection of different de-
sign patterns, templates, and examples.
Illustration of Ontorat features using CLO and Biobank
use cases
Cell lines are routinely used in various biological and
biomedical studies such as analysis of cell signalling
pathway studies and host-pathogen interactions [19,20].
The Cell Line Ontology (CLO) is a community-based
ontology that has logically represented over 38,000 cell
line cells [16]. For this Ontorat case study, an Excel file
containing information of over 1,000 cell line cells, which
was obtained from the Cell Bank of RIKEN BioResource
Center (BRC) in Japan, was used as input to add these cell
line cell terms and their annotations into CLO [16].
Figure 4 demonstrates an Ontorat example based on
the general strategy shown in Figure 1. Figure 4a shows
the design pattern used to define cell line cells obtained
from the RIKEN BioResource Center. Based on the de-
sign pattern, the following elements (terms or annota-
tions) are needed to define a cell line cell: (i) Cell line
resource (e.g. Japan RIKEN Cell Bank); (ii) Tissue in an
organism that a cell line cell is derived from; (iii) Person
(s) who registered the cell line (register); and (iv) Persons
who developed or maintained the cell line (originator).
As described in the Implementation section, different
assertion axioms were generated to represent the relations
of terms or annotations to targeted terms (e.g. cell line
cells) (Figure 4b). For example, cell line resource is repre-
sented as a superclass axiom expressed as follows:
is in cell line repository some RIKEN Cell Bank
This axiom specifies that the newly generated cell
line cell is in the RIKEN cell line repository. To ensure
that Ontorat correctly interpreted the axiom, the term
URIs for both is in cell line repository and RIKEN
Cell Bank should be specified in the web form as indi-
cated in Figure 3(7). With these specifications, Ontorat
will be able to translate the axiom into an OWL ex-
pression. The annotations of the term, such as label,
are represented as annotation axioms, as demonstrated
below (lower part of Figure 4b):
label {$columnA} cell
This axiom represents that the label of the newly
added cell line cell term is defined as the string shown
in the column A (represented by {$columnA}) of the in-
put data file followed by the word cell. The input tem-
plate file (Figure 4c) was populated with information fora specific cell line cell per row (Figure 4d). The string in
the column A of the first row is RCB2320. Based on
the above axiom setting, the label of the first cell line
cell term is RCB2320 cell (Figure 4e).
Using the same approach, Ontorat has added the infor-
mation of derived tissues, originators, and registers of in-
dividual cell line cells as annotation axioms of the newly
generated cell line cell terms (lower part of Figure 4b). It
is noted that in this case, we have added this information
as annotations of cell line cell terms. It is also possible to
add the same information as superclass axioms if we wish
to. For example, instead of defining the following annota-
tion axiom:
comment Derived from tissue: {$columnG} in animal:
{$columnF}.
We should add the following superclass axiom assertion:
derived from some ({$columnG} part of  some
{$columnF})
Where column G includes tissue information and col-
umn F includes animal information. In this case, the term
derived from should be an object property. Furthermore,
instead of simple strings, specific ontology term URIs
representing the tissue and animal should be provided in
column G and column F, respectively. Therefore, same
ODP could be represented by different OWL expressions.
The detailed Ontorat ODP, template, setting file, and
the example input and output files are available on the
Ontorat template web page: http://ontorat.hegroup.org/
designtemplates/cellline/clo-celllinecell.php.
The above CLO example involves the generation of
new ontology terms and addition of logical axioms and
annotation axioms at the same time using Ontorat.
Ontorat supports editing existing terms by addition of
new axioms (e.g. annotations). For example, Ontorat was
recently used to automatically add definition source and
term editor annotations to over 50 ontology classes in
the Biobank Ontology (https://code.google.com/p/bio-
bank-ontology/). The Biobank Ontology is developed for
representing and annotating entities related to Biobank
repositories. When new terms were initially added into
the ontology, definition source and term editor were not
specified. To add the annotations to biobank-specific
classes, the following settings were used in the Ontorat
annotations input section:
definition editor {$columnC},
definition source {$columnD}
Since the aim of this use case is to add annotations to
existing ontology terms, the edit existing classes  option
Figure 4 Demonstration of an Ontorat use case for ontology enrichment. This use case aimed to enrich the Cell Line Ontology (CLO) with
new over 1,000 cell line celles collected in Japan RIKEN Cell Bank. First the ODP was identified to define these cell line cells (a). As guided by the
ODP, a list of Ontorat settings was generated to specify axiom expressions with possible variables of terms and annotations (b). The template file
(c) was also generated and used to fill specific contents (d). Finally Ontorat generated an OWL format output file containing newly created
ontology terms together with their annotations. The output could be displayed using the Protégé-OWL editor (e). It is noted that only parts of
Ontorat settings and input data file are shown here. The full version of the files is available
on: http://ontorat.hegroup.org/designtemplates/cellline/clo-celllinecell.php.
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 7 of 10
http://www.jbiomedsem.com/content/6/1/4was chosen in the Ontorat Purpose input (Figure 3 (2)).
The Ontorat input files used to edit Biobank Ontology
and the output OWL file are available on: http://ontorat.
hegroup.org/designtemplates/biobank/index.php.
Brief summary of other Ontorat use cases
In the original Ontorat software demonstration in the
ICBO-2012 conference [13], Ontorat was used to add
approximately 800 US-licensed animal vaccines to theVaccine Ontology [14,15]. VO is a community-based
ontology in the domain of vaccine and vaccination. These
vaccines include 303 licensed vaccines against infections
of individual pathogens and 494 combination vaccines,
each of which protects against infections of two or more
pathogens. The data for these vaccines were originally ex-
tracted from the official USDA website and stored in the
VIOLIN vaccine database (http://www.violinet.org) [21].
Corresponding to the two sets of animal vaccines based
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 8 of 10
http://www.jbiomedsem.com/content/6/1/4on one or more pathogens targeted by a vaccine, two
Ontorat Excel template files were generated. In addition
to the generation of new classes of licensed animal vac-
cines, Ontorat was used to add annotations using annota-
tion properties (e.g., see_Also and term definition) [13].
To achieve multiple tasks, we performed multiple Ontorat
executions, each execution to achieve a specific task.
A large number of experimental assays have been used
in the biological and biomedical fields. The OBI consor-
tium has a major focus on modeling and representing these
assays [9]. OBI assays were defined by several elements in-
cluding: (i) assay inputs, such as materials to be evaluated
and devices used; (ii) assay output that is information about
some biological process or function (e.g., gene expression,
DNA methylation); (iii) assay aims, such as identification
of epigenetic modification, and (iv) main processes of an
assay, such as immunoprecipitation and sequencing. It is
often complicated to fully represent and annotate an assay
term in OWL expression. To manually generate assay
term with rich axioms is very time-consuming and has be-
come a bottleneck in OBI ontology expansion. To solve
this issue, Ontorat was applied.
Since the Excel template file format is generally friendly
and widely used by the public, domain experts without
ontology knowledge are able to add contents to the tem-
plate file. In the community-based ENCODE project [22],
the OBI team developed specific template files for adding
assay and device terms based on ODPs. The templates
were then provided to domain experts for them to submit
term requests. The requested terms with rich annotations
and logical axioms were then added into OBI using
Ontorat, and the ontology term IDs assigned by Ontorat
were provided to the end users for their usage.
Recently Ontorat has been utilized to add mouse strain
terms in the Beta Cell Genomics Ontology (BCGO) [23].
Although BCGO did not define mouse strain logically, it
contains rich annotations including MGI id, common
name, alternative term, definition, definition source, and
term editor. The Ontorat speeded up generation of these
terms. Moreover, since settings and templates can be
reused, it will be easy to add more mouse strain terms
in the future.
In addition to the use cases described above, Ontorat
has been applied to the development of the Ontology of
Vaccine Adverse Events (OVAE) [24] and the Ontology
of Biological and Clinical Statistics (OBCS) [25].
Collection of design patterns and templates
Since the ontology design pattern is a reusable modeling
solution for building an ontology, the Ontorat website
has provided a collection of design patterns and corre-
sponding templates for ontology developers to reuse. For
each collected case, Ontorat provides an ODP diagram,
an Excel template, a setting file, and an example withpopulated template data and output OWL file. The col-
lection supports the development of several ontologies,
including OBI, VO, CLO, and BCGO and available on:
http://ontorat.hegroup.org/designtemplates.
Discussion
Manually adding a large amount of terms or terms with
rich axioms into an ontology is a big challenge and be-
come a bottleneck of ontology development. It is time
consuming and error-prone to do it manually. Many
ontology terms were generated with the same ontology
design patterns (ODPs). Based on ODPs and inspired by
the Quick Term Template (QTT) procedure, the Ontorat
web application is developed to provide a robust and scal-
able platform for automatically generating new ontology
terms, axioms and annotations. Ontorat supports efficient
ontology enrichment and expansion. The design patterns
can be reused by ontology developers. The Ontorat
spreadsheet templates lower the technical barriers for
domain experts and data curators, so that they may con-
tribute actively to the ontology development without
knowing the specifics of OWL.
Tools with similar functions to Ontorat exist, includ-
ing MappingMaster [4], Populous [26], and TermGenie
(http://code.google.com/p/termgenie/). As introduced in
the Background section, as a Protégé plugin, Mapping-
Master can only be used with old version Protégé 3.4
and has not been updated to work for commonly used
Protégé 4 and 5 [4]. In addition, MappingMaster requires
writing template class expression using a M2 language, a
Domain Specific Language (DSL) based on the Manchester
OWL syntax. The programming with the language requires
a learning curve. In contrast to MappingMaster, Ontorat
can build axiom expressions from a web form using the
Manchester syntax. Ontorat has the capability of automat-
ically generating annotations of ontology terms. Populous
provides desktop standalone and user-friendly interface
[26]. However, it needs software installation. Populous
does not support the generation of term annotations.
Ontorat is implemented as a user-friendly web-based ap-
plication without the necessity of software download and
installation. TermGenie provides a web application that
creates new terms for an ontology using patterns (http://
code.google.com/p/termgenie/). TermGenie has been used
for the Gene Ontology (GO) and its cross products
(http://go.termgenie.org/). Based on predefined pat-
terns, TermGenie supports new ontology term gener-
ation and provides a user-friendly interface to domain
experts. Compared to Ontorat, TermGenie does not
allow the generation of new terms based on user-
provided patterns. Ontorat provides more flexibility in
allowing users to define patterns for different ontol-
ogies. TermGernie cannot be used to add new axioms
to existing terms.
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 9 of 10
http://www.jbiomedsem.com/content/6/1/4To ensure the generation of unique IDs for newly gen-
erated terms for a target ontology, the ontology is cur-
rently required to be loaded in Ontorat. URIGen is a
Java API and web service for managing ontology URI
creation (http://www.ebi.ac.uk/fgpt/sw/urigen/). URIGen
also provides a REST interface that interacts with the
URIGen server. It is possible to incorporate the URIGen
distributed ID management functionality into Ontorat
for unique ID assignment.
While Ontorat is primarily targeted for ontology de-
velopers with sufficient OWL ontology background,
Ontorat provides a way to separate the duties from the
ontology developers and domain experts who both partici-
pate in the development of a specific domain ontology.
Ontorat separates the Manchester syntax programming
from the template spreadsheet population. A domain ex-
pert who does not know programming can still work on
the ontology development project by working on populat-
ing the Excel spreadsheet. For example, in the OBI Assay
example described above, after receiving the Assay Excel
template file, the domain experts in the ENCODE project
[22] were able to independently provide definitions and
other information needed to define an assay term. After
obtaining the Excel file from the ENCODE group, the OBI
developers were able to use Ontoat to generate new ontol-
ogy terms and annotations separately. The logical axiom
expressions in Ontorat use the standard and widely-used
Manchester syntax, together with simple Ontorat rules for
representing ontology variables. Therefore, Ontorat pro-
vides a relatively straight forward platform for ontology
developers who are familiar with the Manchester syntax,
which is also used in the Protégé-OWL editor.
Ontorat implements the Quick Term Template (QTT)
procedure and more. An Ontorat template is equivalent
to a QTT template when the template is designed for
generating new ontology classes for ontology expansion.
In addition to new class generation, Ontorat can also
support the addition of new annotations to existing
ontology classes. In the future, Ontorat will also support
the generation of axioms that contain instances. Differ-
ent from the QTT approach, Ontorat emphasizes the
generation of machine-readable and reusable axiom set-
ting file. The Ontorat axiom expressions use the Man-
chester OWL syntax and easy-to-use Ontorat syntax of
variables. The Ontorat syntax provides a way to repre-
sent variables that are mapped to columns in the Excel
template spreadsheet. The Ontorat generated Ontorat
setting file is easily understandable and reusable.
Among software programs that support ontology
development, Ontorat is complementary to OntoFox
(http://ontofox.hegroup.org), another web application
developed by our group with the support from the OBO
Foundry community [27]. OntoFox supports the re-
trieval of a subset of ontology terms and axioms fromexisting ontologies [27]. Ontorat and OntoFox are com-
plementary in the sense that OntoFox supports the reuse
of existing ontology terms and Ontorat supports the
automatic generation of new ontology terms, axioms
and annotation of ontology terms. OntoFox and Ontorat
have been combined in use for development of new on-
tologies, such as the Cell Line Ontology (CLO) [16],
Vaccine Ontology [28], Ontology of Biological and Clin-
ical Statistics (OBCS) [25], and Beta Cell Genomics
Ontology (BCGO) [23]. In fact, Ontorat and OntoFox
are developed using similar web-based form and setting
file design. For example, the Ontorat setting file is simi-
lar in spirit to the OntoFox setting file that has been
proven to be very useful for reusability. We will seek
ways to better integrate these two software programs for
more efficient ontology development.
Furthermore, we plan to expand the Ontorat collec-
tion of ODPs, templates, and setting files together with
examples. Such a collection will support ontology design
pattern reuse, standardization, and various applications.
We encourage all parties to participate in contributing
their domain knowledge and expertise in this collabora-
tive movement.
Ontorat was introduced in an OBO Tutorial in the
International Conference on Biomedical Ontologies
(ICBO) in 2013. The tool was also demonstrated in an
OBO Tutorial and an OBO Technical Workshop in
ICBO-2014 (http://icbo14.com/), held at Houston, Texas,
USA. Given strong community demands and support,
Ontorat has provided a timely platform to support effi-
cient ontology development and applications.
Conclusions
Ontorat (http://ontorat.hegroup.org) is a web application
that supports automatic generation of new ontology terms,
term annotations, and logical axioms. Ontorat allows the
storage and reuse of axiom setting files and input template
files. Ontorat has also started the collection of reusable
ontology design patterns and templates.
Abbreviations
BCGO: Beta cell genomics ontology; CLO: Cell line ontology; GO: Gene
ontology; ICBO: International conference on biomedical ontology;
OBCS: Ontology of biological and clinical statistics; OBI: Ontology for
biomedical investigations; OBO: Open biology and biomedical ontologies;
OVAE: Ontology of vaccine adverse events; VO: Vaccine ontology.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
ZX was the primary software programmer of Ontorat. JZ generated many
use cases and provided a collection of design patterns and templates on
Ontorat. YL executed Ontorat use cases and participated in active
discussions. YH is the project manager and another programmer, designed
the Ontorat architecture, and tested use cases. YH and JZ drafted the
manuscript. All co-authors reviewed the manuscript and agreed on the
manuscript submission. All authors read and approved the final manuscript.
Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 10 of 10
http://www.jbiomedsem.com/content/6/1/4Acknowledgements
We thank Rebecca Racz for proofreading this manuscript and for her
valuable comments. The work described is funded in part by the National
Institutes of Health (NIH) grants 1R01AI081062 (YH) from the National
Institute of Allergy and Infectious Diseases (NIAID). The content of this paper
is solely the responsibility of the author and does not necessarily represent
the official views of the NIAID and the NIH or other funding organizations.
The article-processing fee for this article was paid by a bridge fund to YH at
the Unit for Laboratory Animal Medicine (ULAM) in the University of
Michigan Medical School.
Author details
1University of Michigan, Ann Arbor, MI, USA. 2University of Pennsylvania,
Philadelphia, PA, USA.
Received: 20 October 2014 Accepted: 25 December 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Kafkas et al. Journal of Biomedical Semantics 2015, 6:1
http://www.jbiomedsem.com/content/6/1/1RESEARCH Open AccessDatabase citation in supplementary data linked to
Europe PubMed Central full text biomedical
articles
?enay Kafkas*, Jee-Hyub Kim, Xingjun Pi and Johanna R McEntyreAbstract
Background: In this study, we present an analysis of data citation practices in full text research articles and their
corresponding supplementary data files, made available in the Open Access set of articles from Europe PubMed
Central. Our aim is to investigate whether supplementary data files should be considered as a source of information
for integrating the literature with biomolecular databases.
Results: Using text-mining methods to identify and extract a variety of core biological database accession numbers,
we found that the supplemental data files contain many more database citations than the body of the article, and
that those citations often take the form of a relatively small number of articles citing large collections of accession
numbers in text-based files. Moreover, citation of value-added databases derived from submission databases (such
as Pfam, UniProt or Ensembl) is common, demonstrating the reuse of these resources as datasets in themselves. All
the database accession numbers extracted from the supplementary data are publicly accessible from http://dx.doi.
org/10.5281/zenodo.11771.
Conclusions: Our study suggests that supplementary data should be considered when linking articles with data, in
curation pipelines, and in information retrieval tasks in order to make full use of the entire research article. These
observations highlight the need to improve the management of supplemental data in general, in order to make
this information more discoverable and useful.
Keywords: Text mining, Supplementary data, Accession number, Molecular biology databasesBackground
Biomolecular and literature databases are a vital resource
for the scientific community. Linking these resources
enables scientists to access, analyse and process the data
comprehensively. One way to link these resources is to
identify accession numbers as specific database citations
in text. Accession number annotation in full text has
been tackled in a variety of ways at various points in the
publication lifecycle. While some publishers tag (structur-
ally annotate) accession numbers in the text of articles as
a part of their production process, this is not something
done comprehensively across all publishers [1]. In the
absence of machine-actionable citation data, text min-
ing [1-4] can be used to annotate accession numbers* Correspondence: kafkas@ebi.ac.uk
European Molecular Biology Laboratory, European Bioinformatics Institute
(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,
UK
© 2015 Kafkas et al.; licensee BioMed Central.
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.automatically across large volumes of published research
articles. One such study is our recent work on the citation
of three major submission databases (ENA, UniProt,
PDBe) within the Open Access subset of Europe PMC
(http://europepmc.org/). In this study, we investigated to
what extent, (1) publishers provide structurally annotated
accession numbers in full text, (2) text mining extends
publisher annotations and (3) text mining contributes to
literaturedatabase cross links. Our results show that text
mining can significantly enrich publishers annotations
and contribute to literaturedatabase cross links (see [1]
for details).
Although the annotation of many types of accession
numbers is now part of the routine processing of full
text articles in Europe PMC, the extent of data citation in
supplementary data has yet to be explored. Supplementary
data is unstructured and therefore the content is basically
undiscoverable via the usual retrieval methods that operateThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 2 of 7
http://www.jbiomedsem.com/content/6/1/1only on the article narrative. However, finding database ci-
tations in supplementary data could be useful for the deep
integration of literature and databases and potentially
helpful for curators [5]. Moreover, as reported in a recent
study, which focuses on mining genetic variations from
literature, supplementary materials were identified as a
critical source of genetic mutations [6]. Here, we extend
our previous study on the analysis of database citation
in narrative of the full text articles to supplementary
data in order to understand whether supplementary
data is useful for linking articles to the biomolecular
databases. To the best of our knowledge, this is the first
study on the analysis of data citation in supplementary
data. In this study: We extended the Whatizit-Accession
Number Annotation (Whatizit-ANA) module to anno-
tate database citations to a total of ten biological data-
bases and revised the extraction rules and patterns. We
analysed and compared the distribution of the database
citations in the body of the Open Access article set
(OA-ePMC articles) and their associated supplementary
data files for the ten databases. All the database acces-
sion numbers extracted from the supplementary data
are publicly accessible from http://dx.doi.org/10.5281/
zenodo.11771.
Materials and methods
Literature and biomedical databases used
Literature
The full text articles and their supplementary files used
in this study were gathered from the OA-ePMC set. This
open access article set is available on the Europe
PMC FTP site and the linked supplementary data files
are available via the Europe PMC RESTful web ser-
vice (http://europepmc.org/restfulwebservice). We de-
cided to reanalyse the set of OA-ePMC articles that
we used in our previous study in order for the results
to be directly comparable (http://europepmc.org/ftp/oa/
AccNoAnalysisData/AnnotatedData/). This set contains
410,364 full text articles in XML format [1]. It is formed
by filtering out the articles which were published before
1990 since in this historical set, accession number cita-
tions are rare. We identified 361,937 supplementary files
that belong to these articles in various formats. The
distribution of these files according to the file formats is
shown in Figure 1. This shows that the majority of the
files have formats such as Portable Document Format
(PDF) and Microsoft Word (DOC) and Microsoft Excel
(XLS) that can be converted into text.
A three-step pre-processing was applied to the gath-
ered supplementary files: (1) screening out the supple-
mentary files that are not easily convertible to text
such as image, audio and movie file types (filtering is
done based on MIME types/subtypes that can be ex-
tracted from the file link in the full text XML, withinthe </supplementary-material > element). (2) screening
out text supplementary files that are unlikely to contain
accession numbers (e.g. source code files) (filtering is done
based using known file extensions for source code) and
(3) employing Apache Tika [7] to extract the text content
from the remaining files. The final set of supplementary
data included a total of 213,245 supplementary files either
in text or text convertible format linked to 68,995 of the
410,364 OA-ePMC articles.
Biological databases
We used ten major biological databases in this study.
Three of these databases are primary databases and
the other seven databases are added-value (secondary)
databases.
Primary databases
Primary databases accept direct submissions of de novo
data. The following primary databases were used in this
study:
 The European Nucleotide Archive (ENA, http://
www.ebi.ac.uk/ena/)
 ArrayExpress (http://www.ebi.ac.uk/arrayexpress/)
 Protein Data Bank, Europe (PDBe, http://www.ebi.
ac.uk/pdbe/)
Added-value databases
Added-value or secondary databases collect or present
data as curated sets or summaries based on primary data
submissions. The following added-value databases were
used in this study:
 The Protein families database (Pfam, http://pfam.
sanger.ac.uk/)
 Universal Protein knowledgebase (UniProt, http://
www.uniprot.org/)
 Reference Sequence (RefSeq, http://www.ncbi.nlm.
nih.gov/RefSeq/)
 Reference Single Nucleotide Polymorphism (RefSNP,
http://www.ncbi.nlm.nih.gov/SNP/)
 Ensembl (http://www.ensembl.org/index.html)
 Online Mendelian Inheritance in Man (OMIM,
http://www.omim.org/)
 InterPro (http://www.ebi.ac.uk/interpro/)
Annotating database citations
Database citations in publications were annotated by the
text-mining method used in our previous study [1].
This method mainly uses Whatizit-Accession Number
Annotation (Whatizit-ANA) module [1,8] where a set
of extraction rules and patterns were applied with con-
textual cues for recognising database citations. These
patterns are shown in Table 1.
Figure 1 Distribution of supplementary data by file formats. This figure describes distribution of supplementary files linked to the Europe
PMC open access full text articles by different file formats. The text convertible format covers the formats which can be convertible to text such
as pdf, xml, html and xsl.
Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 3 of 7
http://www.jbiomedsem.com/content/6/1/1For this study, we extended our annotation method by:
1. Adding extraction rules and patterns to include ten
database types (compared to the first version,
accession number annotation is extended to four
additional databases: Ensembl, RefSeq, RefSNP and
OMIM).
2. Revising the validation step by replacing the
previous accession number validator with a new one
based on the global EBI Search web service (http://
www.ebi.ac.uk/Tools/webservices/services/eb-eye).Table 1 Extraction patterns and contextual cues for
databases
Database Patterns Contextual cues
ENA [A-Z][09]{5}; [A-Z]{2}[09]{6}; [A-Z]
{3}[09]{5}; [A-Z]{4}[09]{8,10}; [A-Z]
{5}[09]{7}
genbank, gen, ddbj,
embl
UniProt [A-N,R-Z][09][A-Z][A-Z, 09][A-Z,
09][09]; [O,P,Q][09][A-Z, 09]
[A-Z, 09][A-Z, 09][09]
swissprot, sprot,
uniprot
PDBe [09][A-Z, 09]{3} pdb
InterPro IPR[09]{6} interpro
Pfam PF(AM)?[09]{5} hmm, family, pfam
ArrayExpress E-[A-Z]{4}-[09]+ arrayexpress
OMIM [09]{6} omim
Ensembl ENS[A-Z]*G[09]{11}+ ensembl
RefSeq (AC|AP|NC|NG|NM|NP|NR|NT|NW|
NZ|XM|XP|XR|YP|ZP|NS)_([A-Z]{4})*
[09]{6,9}(?:[.][09]+)?
refseq
RefSNP RS[09]{5,9} snpThis new validator covers more databases
(e.g. InterPro and Ensembl) and is more
robust.
This new Whatizit-ANA module has been integrated
into the core Europe PMC infrastructure and is available
via Whatizit web site and web service (http://www.ebi.
ac.uk/webservices/whatizit).
Results and discussion
Performance assessment of the Whatizit-ANA Module
We used the same sets of gold standards (for ENA,
UniProt and PDBe) used in our previous study for
assessing the performance differences between the pre-
vious and current versions of the Whatizit-ANA mod-
ule. The results presented in Table 2 show that the
current version of the module is better than the previ-
ous version (F-score values of > 96% for UniProt and
PDB and >77% for ENA) (please refer to [1] for our
performance evaluation). This is due to the improve-
ment in the validation component that we used in the
new version of our tool. This new validation compo-
nent is capable of validating accession numbers more
accurately, resulting in lower numbers of missed acces-
sion numbers [see Table 2, the new system misses
fewer accession numbers (false negatives) and hence
identifies higher number of accession numbers (true
positives) compared to the old one]. For example, in
the article with PMCID1892096, the UniProt citation
P09372 was missed (false negative) using the old ver-
sion of the tool, however it was annotated correctly
with the new version.
Table 2 Performance assessment results of the Whatizit ANA module
Database Evaluation #TP #FP #FN Precision (%) Recall (%) F-score (%)
New Old New Old New Old New Old New Old New Old
ENA Automatic 276 267 10 7 170 181 96.50 97.45 61.88 59.60 75.41 73.96
Manual 286 274 0 0 170 181 100 100 62.72 60.22 77.10 75.17
UniProt Automatic 574 569 28 8 39 39 95.35 98.61 93.64 93.59 94.49 96.03
Manual 601 577 1 0 39 39 99.83 100 93.91 93.67 96.78 96.73
PDBe Automatic 568 529 32 30 12 50 94.67 94.63 97.93 91.36 96.27 92.97
Manual 620 559 0 0 12 50 100 100 98.10 91.79 99.04 95.72
FP: False Positive, FN: False Negative, Old: Old Whatizit-ANA settings, New: New Whatizit-ANA settings.
Manual and automatic evaluation: In the automatic evaluation; we estimated the performance of the tool by assuming that publisher-supplied
accession numbers in the articles are a gold standard for annotation. However, when we manually analysed the false positive annotations provided
from our pipeline, we realised that the accession numbers provided in articles (the annotations that we assumed as gold standard in the automatic
evaluation) might not be always complete or correct. Therefore, the annotations made by our tool, which were not already annotated in the
article, were deemed false positives by the automatic evaluation, however, such annotations could be reassigned as true positives on
manual inspection.
Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 4 of 7
http://www.jbiomedsem.com/content/6/1/1Distribution of database citations
Figure 2 shows the distribution of database citation in
the 410,364 OA-ePMC articles and their supplementary
data. The analysis reveals that 16.8% of articles (68,995/
410,364; Figure 2 (c)) have supplementary data in either
text or text convertible format. Only, 3,365 of these
68,995 articles (3,365/410,364; 0.82%; Figure 2 (f )) con-
tain database citations in both their body and supple-
mentary data.Figure 2 Distribution of database citations in the OA-ePMC articles. T
PMC open access full text articles.Analysis of database citation in article body and
supplementary data
In the full set of 410,364 OA-ePMC articles, 28, 610
(6.97%, Figure 2 (g)) of the article bodies contain
database citations. Of the 213,245 supplementary files
that we can mine, 10,179 (4.77%) contain database ci-
tations. Table 3 shows the distribution of the database
citations in the bodies of these articles and supple-
mentary files.his figure describes distribution of database citations in the Europe
Table 3 Distribution of database citations in article body and supplementary data by databases in the OA-ePMC set
Database Supplementary data Article body Ratio Shared citations
Ensembl 1,292,198 1,152 1,121.70 23 (0.002%)
RefSeq 2,540,260 2,864 886.96 178 (0.007%)
InterPro 564,956 639 884.13 77 (0.014%)
UniProt 2,972,519 9,387 316.66 540 (0.018%)
Pfam 924,624 2,968 311.53 435 (0.047%)
RefSNP 2,443,679 31,061 78.67 3,849 (0.16%)
ENA 3,390,319 125,534 27.01 4,167 (0.12%)
PDBe 197,850 44,269 4.47 2,805 (1.42%)
ArrayExpress 2,377 1,565 1.52 53 (2.23%)
OMIM 2,400 2,779 0.86 19 (0.80%)
Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 5 of 7
JOURNAL OF
BIOMEDICAL SEMANTICS
Hur et al. Journal of Biomedical Semantics 2015, 6:2
http://www.jbiomedsem.com/content/6/1/2RESEARCH Open AccessDevelopment and application of an interaction
network ontology for literature mining of
vaccine-associated gene-gene interactions
Junguk Hur1, Arzucan Özgür2, Zuoshuang Xiang3 and Yongqun He3,4,5,6*Abstract
Background: Literature mining of gene-gene interactions has been enhanced by ontology-based name
classifications. However, in biomedical literature mining, interaction keywords have not been carefully studied
and used beyond a collection of keywords.
Methods: In this study, we report the development of a new Interaction Network Ontology (INO) that
classifies >800 interaction keywords and incorporates interaction terms from the PSI Molecular Interactions
(PSI-MI) and Gene Ontology (GO). Using INO-based literature mining results, a modified Fishers exact test was
established to analyze significantly over- and under-represented enriched gene-gene interaction types within a
specific area. Such a strategy was applied to study the vaccine-mediated gene-gene interactions using all
PubMed abstracts. The Vaccine Ontology (VO) and INO were used to support the retrieval of vaccine terms and
interaction keywords from the literature.
Results: INO is aligned with the Basic Formal Ontology (BFO) and imports terms from 10 other existing
ontologies. Current INO includes 540 terms. In terms of interaction-related terms, INO imports and aligns
PSI-MI and GO interaction terms and includes over 100 newly generated ontology terms with INO_ prefix. A
new annotation property, has literature mining keywords , was generated to allow the listing of different
keywords mapping to the interaction types in INO. Using all PubMed documents published as of 12/31/2013,
approximately 266,000 vaccine-associated documents were identified, and a total of 6,116 gene-pairs were
associated with at least one INO term. Out of 78 INO interaction terms associated with at least five gene-pairs
of the vaccine-associated sub-network, 14 terms were significantly over-represented (i.e., more frequently used)
and 17 under-represented based on our modified Fishers exact test. These over-represented and under-represented
terms share some common top-level terms but are distinct at the bottom levels of the INO hierarchy. The analysis of
these interaction types and their associated gene-gene pairs uncovered many scientific insights.
Conclusions: INO provides a novel approach for defining hierarchical interaction types and related keywords for
literature mining. The ontology-based literature mining, in combination with an INO-based statistical interaction
enrichment test, provides a new platform for efficient mining and analysis of topic-specific gene interaction networks.
Keywords: Biomedical ontology, Interaction network ontology, Literature mining, Interaction enrichment, Gene-gene
interaction* Correspondence: yongqunh@med.umich.edu
Equal contributors
3Unit for Laboratory Animal Medicine, University of Michigan, Ann Arbor, MI
48109, USA
4Department of Microbiology and Immunology, University of Michigan, Ann
Arbor, MI 48109, USA
Full list of author information is available at the end of the article
© 2015 Hur et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,
unless otherwise stated.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 2 of 10
http://www.jbiomedsem.com/content/6/1/2Background
Two common strategies of literature retrieval of re-
ported gene-gene interactions include gene-gene co-
occurrence and interaction keywords-based literature
mining. In this paper, the gene-gene interaction repre-
sents a broad interactive relation between two genes
or gene products [1]. Such a relation does not have to
be a direct physical interaction. The co-occurrence
strategy identifies two related genes both listed in the
same literature, or more specifically in the same title,
abstract, or sentence. An example of such a strategy is
PubGene, which extracts gene relationships based on
the co-occurrence of gene symbols in MEDLINE titles
and abstracts [2]. The other strategy relies on the identifi-
cation of two genes together with an interaction keyword
in the same sentence. Such a method may still generate
many false-positive results. To improve the interaction
keyword-based approach, machine learning algorithms
(e.g., support vector machine (SVM) [3]) with features
extracted from syntactic analysis of sentences (e.g., de-
pendency parse trees) can be used [4].
Ontologies can be applied to enhance literature mining
performance. For example, in our previous work, a
vaccine-specific sub-network was built by considering
only the interactions that were extracted from sen-
tences that contain the vaccine term (or its variants
like vaccines, vaccination, and vaccinated). This
strategy does not retrieve the sentences where more
specific vaccine names such as BCG (a commercial
tuberculosis vaccine) are mentioned. Such vaccine
names and their hierarchical relations are represented
in Vaccine Ontology (VO) [5]. We found that the ap-
plication of VO has significantly improved the analysis
of the vaccine-specific sub-networks [6].
An ontology that logically represents various inter-
action keywords/types and their semantic relations
would help address the challenge of retrieving and clas-
sifying the types of gene-gene interactions in the inter-
action keyword-based literature mining. The GENIA
ontology provides a semantically annotated corpus for
biological literature mining [7]. However, this ontology
does not specify various types of interactions between
genes or proteins. Initiated from the classification of >800
interaction keywords [6], we have developed the Inter-
action Network Ontology (INO) that ontologically repre-
sents various interaction types and their relations, and
collects and assigns interaction keywords to these different
interaction types. The details about the ontology will, for
the first time, be provided in this manuscript.
In addition to supporting the literature mining of
gene-gene interactions, INO can be used for interaction
type enrichment analysis. Gene Ontology (GO)-based
gene set enrichment analyses have been widely used to de-
termine over- or under-represented biological functions ina set of genes obtained from high-throughput Omics stud-
ies. GO provides controlled vocabulary of standard terms
for describing gene product characteristics in a hierarch-
ical structure. The input to the GO term enrichment ana-
lysis is a list of genes. Such a method does not classify
enriched gene-gene interactions. Since INO classifies
different interaction types into a structured ontology, it
becomes possible to perform a gene-gene interaction en-
richment study by comparing the INO-based literature-
mined data of gene-gene interactions in some specific
domain over the data from the broad background.
In this manuscript, we will first introduce the develop-
ment of INO with a focus on its representation of inter-
action types and keywords for literature mining. An
INO-based gene interaction enrichment method based
on a modified Fishers exact test will then be introduced.
We applied our approach to the analysis of the vaccine-
mediated gene-gene interactions. The resulting over-
and under-represented gene-gene interaction types and
gene-gene interactions will also be described in detail.
Methods
INO development
INO was developed by following the Open Biological
Ontology (OBO) Foundry ontology development princi-
ples, including openness and collaboration [8]. Its devel-
opment is aligned and integrated with existing OBO
Foundry library ontologies. INO imports existing terms
by using OntoFox [9]. New terms generated in INO use
the INO_ prefix. INO uses the format of W3C stand-
ard Web Ontology Language (OWL2) (http://www.w3.
org/TR/owl-guide/). For efficient editing of INO, the
Protégé 4.3 OWL ontology editor (http://protege.stanford.
edu/) was used.
The INO source is open freely under a Creative Com-
mons (CC) license for public and commercial usage.
INO has been deposited at the INO SourceForge project
page (http://sourceforge.net/projects/ino/). It is also
available in the ontology repositories of National Center
for Biomedical Ontology (NCBO) BioPortal (http://purl.
bioontology.org/ontology/INO) and Ontobee [10] (http://
www.ontobee.org/browser/index.php?o=INO).
INO-based literature mining of gene-gene interaction
pairs and interaction types
The sentences from the complete PubMed abstracts
(published up to 12/31/2013) were obtained from the
BioNLP database in the National Center for Integrative
Biomedical Informatics (http://ncibi.org/). Our in-house
literature mining tools, SciMiner [11] and VO-SciMiner
[12], were used to identify gene names/symbols and VO
and INO terms (interaction keywords) from these sen-
tences. Sentences with two gene names and at least one
INO term (e.g., interacts, binds, activates) were selected.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 3 of 10
http://www.jbiomedsem.com/content/6/1/2We obtained the dependency parse trees of the sen-
tences using the Stanford Parser [13] and extracted the
shortest dependency path between each pair of genes in
a sentence. We defined an edit distance-based kernel
function among these dependency paths and used SVM
[3] to classify whether a path describes an interaction
between a gene pair [6]. A confidence score calculated
based on SVM was used to measure the confidence of
association between two genes in a sentence in the lit-
erature. Positively-scored sentences were kept, and the
gene pairs together with the interaction keywords from
these sentences were extracted. The extracted inter-
action keywords were mapped to INO to define the
interaction types.
Development of INO-based statistical enrichment analysis
of literature mined gene-gene interaction data
A modified Fishers exact test has gained popularity over
the last decade in high-throughput gene expression
studies as a preferred method for identifying enriched
biological functions among given gene sets [14,15]. We
implemented the modified Fishers exact test in Perl
using the Ngram Statistics Package [16] to identify
enriched gene-gene interaction types, in terms of INO
terms, within a concept-specific sub-network. For each
INO term, a 2×2 contingency table is obtained on which
the Fishers test runs, as shown in Table 1. Both signifi-
cantly under-represented and over-represented terms are
selected as a significantly enriched INO term with a
p-value < 0.05 after Benjamini-Hochberg (BH) multiple
testing corrections. Here a significantly over-represented
or under-represented term indicates that the term was
significantly more or less frequently used in the vaccine
context compared to the whole literature background. In
the current study, a vaccine-associated gene-gene inter-
action network was defined based on the gene-gene inter-
actions obtained from the PubMed abstracts, including
those retrieved by a PubMed search of vaccine and those
identified by VO-SciMiner using 186 specific vaccine
terms extracted from the VO vaccine branch. These 186
vaccine terms (e.g., tuberculosis vaccine BCG) are easily
identified by natural language processing programs. This
vaccine-associated network was compared against the
complete gene-gene interaction network.Table 1 The 2x2 contingency table
# of gene-gene Concept- Whole
Interaction pairs specific sub-network Network
With the INO term 30  1 500
Without the INO term 150 30000
Note: The sub-network has 30 gene pairs associated with this INO term out
of a total of 180 gene pairs. A modified Fishers exact test, with the - 1
modification made to the typical Fishers exact test to make the statistical test
more conservative, was employed to identify significantly over-represented
terms (p-value of 6.9E-20).Results
The Interaction Network Ontology (INO)
(1) INO overall design and hierarchy
INO is a biomedical ontology in the domain of molecu-
lar interactions and interaction networks. INO is aligned
with the upper-level Basic Formal Ontology (BFO) [17]
(Figure 1). BFO contains two branches, continuant and
occurrent. The continuant branch represents time-
independent entities such as material entity, and the
occurrent branch represents time-related entities such
as process. BFO has currently been used by over 100
domain ontologies, including many (e.g., GO) within
the framework of the OBO Foundry [8]. By aligning
different domain ontologies under the two branches of
BFO, INO is able to efficiently use the terms from
other ontologies in representing signaling pathway
elements.
Three important INO terms are interaction, network,
and pathway. In INO, an interaction is defined as a pro-
cessual entity that has two or more participants (i.e.,
interactors) that have an effect upon one another under
a particular condition. An interactor (or called interact-
ant) is defined as a material entity that plays the role of
interactor role. With different roles, an interactor can be
an input interactor, output interactor, catalyst, positive
regulator, or negative regulator. An interaction con-
sumes its input interactors (but not the catalysts or regula-
tors) and generates its output interactors. A network is a
process that includes at least two connected interactions.
A network does not have to include a predefined start or
end entity. A pathway is a type of network that has speci-
fied distinct start(s) and end(s). Each of these three INO
terms includes many subclasses. Therefore, in addition to
the representation of various interaction types, INO has
also been developed to represent pathways and networks.
Furthermore, INO has been used as a species-neutral
ontology core and platform for generating human-specific
interaction network ontology (HINO) [18,19]. Since the
scope of this manuscript is the ontology-based literature
mining of gene-gene interactions, we will primarily focus
on the ontological representation of interactions in INO.
INO imports terms from other ontologies, particularly
from the Proteomics Standard Initiative-Molecular Inter-
action (PSI-MI), which is a standard molecular interaction
data exchange format established by the Human Proteome
Organization (HUPO) Proteomics Standard Initiative
(http://www.psidev.info). Their PSI-MI format has been
widely used in the proteomics community and PSI-MI is
also an OBO Foundry library ontology. To be compatible
with PSI-MI, we have imported the branch of the inter-
action type (MI_0190) to INO (Figures 1 and 2).
Compared to PSI-MI, GO Biological Processes (BP)
branch often has more detailed subclasses (or subtypes)
to specific interaction types. Using more general PSI-MI
interaction (INO)
process (BFO)
material
entity (BFO)
regulation
(INO)
entity (BFO)
occurrent (BFO)continuant (BFO)
organism
(OBI)
association (MI)
gene
(OGG)
dependent
continuant (BFO)
realizable entity
(BFO)
role (BFO)
interactor
role (INO)
independent
continuant (BFO)
direct interaction
(MI)
enzymatic reaction
(MI)
physical
association (MI)
interactor
(INO)
input
interactor
role (INO)
input
interactor
(INO)
has role
has role
positive
regulation
(INO)
negative
regulation
(INO)
phosphorylation
(GO)
Figure 1 INO hierarchy and selected INO key terms. INO is aligned with BFO. It imports most PSI-MI interaction type terms to represent
the various interaction types. Some bottom level interaction terms (e.g., phosphorylation) are replaced with corresponding GO terms. Many
INO-specific terms (e.g., regulation) that do not exist in PSI_MI or GO are also generated. Note that there are different interactors but only input
interactor is shown here. The network and pathway related terms are not shown.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 4 of 10
http://www.jbiomedsem.com/content/6/1/2terms (e.g., PSI-MI lipid addition) as parent terms, INO
has imported many specific GO subtypes of interactions
(e.g., GO protein myristoylation) to INO as subclasses
of the MI-based interaction terms (Figure 1). As a spe-
cific example, we have imported GO protein myristoyla-
tion and all of its GO subclasses to INO (Figure 2). The
GO term protein myristoylation has been used to re-
place the PSI-MI term myristoylation reaction. It is noted
that the top level GO Biological Processes hierarchy is not
used because many biological processes (e.g., metabolic
process) in GO are not interaction per se and thus can-
not be imported to INO for interaction representation.
While PSI-MI focuses on direct protein-protein inter-
actions, it does not include many other interaction types
such as regulation types. Therefore, INO also includes
interaction terms that are out of current PSI-MI scope,
especially different regulation types (Figure 1). Many of
these interaction types were generated by classifying the
over 800 interaction keywords used in our previous lit-
erature mining studies [1,6].
(2) Literature mining support in INO
The over 800 interaction keywords used in our previous
literature mining studies [1,6] do not correspond to thesame number of interaction types. While an interaction
type or term in INO has its ontology ID, such a term
may be associated with different synonyms or related
keywords that can be used for literature mining. To sup-
port identification of genetic interactions in literature,
synonyms and related keywords are needed. To meet
this need, we have generated an annotation property
called has literature mining keywords (Figure 2), which
allows the listing of different keywords mapping to the
interaction type.
For example, the term protein myristoylation in INO
has five related literature mining terms including myris-
toylate, myristoylates, myristoylated, myristoylating,
and myristoylation. These term variations are listed as
an annotation of the interaction type using the annota-
tion property has literature mining keywords (Figure 2).
The list of keywords can be easily extracted from the
ontology by SPARQL or other methods and used for lit-
erature mining.
(3) Statistics of INO terms and interaction keywords
As of October 2014, INO contains 540 terms, including
123 new INO terms and 317 terms imported from 11
existing ontologies. In addition to the aforementioned
Figure 2 The visualization of one term protein myristoylation (GO_0018377) in INO. Originated from GO, this term and its branch of child
terms are imported and placed with the framework of PSI-MI interaction types which are also imported into INO. The upper level terms are from
BFO. The OntoFox tool [9] was used for importing external ontology terms and their axioms. The image is a screenshot generated from Ontobee
[10]. To facilitate literature mining tagging, different synonyms of the term are collected under an annotation note.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 5 of 10
http://www.jbiomedsem.com/content/6/1/2ontologies, INO also has imported terms from other
authoritative domain ontologies such as the Chemical
Entities of Biological Interest (ChEBI) [20] and the
Ontology of Genes and Genomes (OGG) [21]. Proven-
ance and source ontology IDs are kept in our term
importing [9]. The detailed INO term statistics can be
found on the Ontobee INO statistics website (http://
www.ontobee.org/ontostat.php?ontology=INO).
Particularly, under the branch of INO interaction, INO
includes a total of 355 terms. In addition, approximately
700 keywords are defined using the annotation property
has literature mining keywords. These INO interaction
terms and their associated literature mining keywords can
be used for efficient literature text tagging and retrieval of
sentences containing these keywords. The usage of these
terms and keywords in our literature mining study is de-
scribed below.INO-based literature mining of gene-gene interactions
(1) Workflow and system design
The workflow of the ontology-based gene pair enrich-
ment analysis is illustrated in Figure 3. Specifically, all
publications from PubMed were first downloaded. The
sentences of article titles and abstracts were parsed and
pre-processed. Human gene names and interaction
keywords were tagged. To tag human gene names, the
HUGO human gene nomenclature assignments (http://
www.genenames.org/) were used. These human gene
names are also available in the OGG [21]. The INO
interaction types and associated keywords were used
for tagging interaction keywords. As detailed in the
Methods section, an INO-based modified Fishers exact
test was developed to identify statistically significantly
enriched gene-gene interaction types and associated
gene-gene pairs (Figure 3).
PubMed Literature
Sentence preprocessing
(titles, abstracts)
Literature mined sentences
containing two genes and
interaction keywords
Terms of a domain ontology
(e.g., VO)
Modified Fishers exact test
Enrichment of gene-gene interactions and
associated interaction types
HUGO human gene names;
INO ontology collections and
hierarchy of interaction words
Figure 3 The workflow of INO-based gene-gene interaction
enrichment analysis. This workflow illustrates the overall procedures
of ontology-based gene pair enrichment analysis.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 6 of 10
http://www.jbiomedsem.com/content/6/1/2The INO-based workflow for literature mining of
gene-gene interactions is applicable for different use case
studies. Below we introduce the application of such a
strategy for studying the gene-gene interactions in the
vaccine domain.
(2) INO-based literature enrichment analysis of vaccine-
associated gene-gene interaction data
Our literature mining analysis used all PubMed docu-
ments published as of 12/31/2013. A total of 23,481,042
PubMed documents were used as the background
data set in the analysis. Using this data set, SciMiner
identified 314,152 gene pairs, each of which was associ-
ated with at least one INO term.Table 2 Significantly over-represented INO terms among the
sub-network
INO_ID Reference term Enrichment
INO_0000140 Neutralization 6.6
INO_0000096 induction of production 6.2
INO_0000106 gene fusion 5.6
INO_0000103 accessory regulation 3.9
INO_0000062 Costimulation 3.7
INO_0000169 Synergization 3.0
INO_0000089 co-regulation 2.9
MI_0559 glycosylation reaction 2.9
MI_0195 covalent binding 2.5
MI_0208 genetic interaction 4.9
MI_0571 mRNA cleavage 23.2
MI_0902 RNA cleavage 16.2
MI_0910 nucleic acid cleavage 6.4
GO_0018377 protein myristoylation 2.3
*BH: Benjamini-Hochberg; **IFNG_IL12A (5): represents the IFNG and IL12A gene paWe applied our study to the vaccine domain. A
PubMed search for vaccine-related documents resulted
in 237,061 hits (as of 12/31/2013). VO-SciMiner add-
itionally identified 28,908 documents using VO terms,
resulting in a total of 265,969 documents to define the
vaccine-associated document sets. The gene-gene inter-
actions (i.e., gene pairs) with positive SVM scores and at
least one INO term at the same sentence level were
compiled from these 265,969 PubMed abstracts. A total
of 6,116 gene pairs were associated with at least one
INO term.
Out of 78 INO interaction terms associated with at
least five gene-pairs of the vaccine-associated sub-
network, 14 terms were significantly over-represented
(Benjamini-Hochberg (BH) p-value < 0.05 and a minimal
enrichment fold of 2) (Table 2). The results indicate that
these 14 interaction types are more extensively studied
in the vaccine context among the research of all the
gene-gene interaction types published in PubMed.
Furthermore, our gene-gene interaction enrichment
analysis was able to retrieve all the gene pairs associated
with each interaction type (last column in Table 2). For
example, as indicated in five publications (PubMed IDs:
19915058, 8557339, 15557182, 17517055, and 7525727),
the cytokines interferon-gamma (IFNG) and interleukin-
12A (IL12A) have been found to be closely related,
and the neutralization of one cytokine often leads to
decreased production of another one [22,23]. Such
neutralization-related research is typically found in
the field of vaccinology. In another example, associated
with the interaction type induction of production, the
production of one cytokine, TNF (or IFNG), was found to
be induced by another cytokine, IFNG (or TNF) [24]. Agene-gene interaction pairs of vaccine-associated
fold BH *P-value Most frequent gene-pair (#)
0 IFNG_IL12A (5)**
0 TNF_IFNG (2)
0 CD40LG_CD40 (3)
0 CD8A_CD4 (55)
0 CD40_CD8A (4)
0 CD8A_CD40 (5)
0 CD8A_CD40 (5)
0 IL17A_MUC6 (1)
0 CSF2_ACPP (2)
1.82E-10 CD40LG_CD40 (3)
2.58E-07 CFI_SUPT5H (1)
2.21E-06 CFI_SUPT5H (1)
6.11E-04 CFI_SUPT5H (1)
2.68E-03 CD4_S100B (2)
ir with the neutralization interaction keyword in five papers.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 7 of 10
http://www.jbiomedsem.com/content/6/1/2close examination of all the gene pairs recorded in Table 2
shows that they are all related to the vaccine and immun-
ology research. These results also confirm the specificity
of our INO-based enrichment analysis.
In addition, our study found 17 significantly under-
represented INO terms with a maximum enrichment
fold of 0.5 (equivalent to 2 fold in over-representation)
and BH P-value < 0.05 (Table 3). Compared to the gen-
eral gene-gene interaction research, these interaction
types are likely less studied in the vaccinology research
field. The reasons of these under-represented interaction
types may vary. It is likely that some of these under-
represented interactions represent new research oppor-
tunities in the vaccinology domain.
One advantage of INO-based study is that we can rely
on the INO hierarchy to identify the relations among
enriched interaction types. Such a strategy is used to
generate the hierarchies of enriched 14 over-represented
and 17 under-represented INO interaction types (Figure 4).
This study clearly shows the relations between many dif-
ferent interaction terms. For example, among the three
over-represented terms, mRNA cleavage, RNA cleavage,
and nucleic acid cleavage, there are two parentchild re-
lations as clearly shown in Figure 4. Interestingly, the term
cleavage reaction is one of the 17 under-represented
terms (Table 3). It is noted that the more general term
cleavage reaction is the parent term of nucleic acid cleav-
age, which is the parent term of RNA cleavage (Figure 4).
The term RNA cleavage has a child term mRNATable 3 Significantly under-represented INO terms among th
sub-network
INO_ID Reference term
MI_0203 dephosphorylation reaction
INO_0000178 tyrosine-phosphorylation
INO_0000044 gene expression regulation
INO_0000172 transactivation
INO_0000060 coprecipitation
GO_0016310 phosphorylation
MI_0403 colocalization
MI_0414 enzymatic reaction
MI_0194 cleavage reaction
MI_0213 methylation reaction
INO_0000092 dissociation
INO_0000048 coimmunoprecipitation
INO_0000115 hyperphosphorylation
INO_0000084 destabilization
GO_0006461 protein complex assembly
INO_0000088 protein dimerization
INO_0000171 Termination
*BH: Benjamini-Hochberg.cleavage. Besides these cleavage types, there are many
other specific cleavage reaction types, for example,
protein cleavage, DNA cleavage, and lipid cleavage. In
our calculation of the parent term cleavage reaction,
we included all its child terms. Therefore, the under-
represented cleavage reaction indicates that the whole
category of cleavage reaction is under-represented al-
though the above three specific reaction types are over-
represented.
Both sets of over-represented and under-represented
interaction terms share some common top-level terms
including regulation, direct interaction, association,
and interaction. Otherwise, specific profiles of the
two sets are in general distinct at the bottom levels
(Figure 4).
Discussion
This paper introduces two major contributions in the
area of ontology-based literature mining research. First,
we have for the first time systematically introduced the
development of the INO ontology targeting for robust
literature mining of gene-gene interaction types. It is
noted that in addition to literature mining, INO is also
being developed to model various interactions and net-
works among different molecules [18]. However, the
INO development was initiated from meeting our lite-
rature mining need [6]. Second, we have proposed and
implemented a novel INO-based gene-gene interaction
enrichment strategy. The INO-based gene pair enrichmente gene-gene interaction pairs of vaccine-associated
Enrichment fold BH* P-value
0.06 0
0.09 0
0.26 0
0.26 0
0.28 0
0.36 0
0.36 0
0.42 0
0.49 0
0.37 6.84E-16
0.28 6.27E-15
0.35 1.00E-13
0.27 2.54E-08
0.28 1.49E-05
0.24 1.97E-05
0.26 6.41E-05
0.42 3.98E-03
Figure 4 The hierarchies of over- and under-represented INO interaction terms. (A) The hierarchy of 14 over-represented INO interaction
terms. (B) The hierarchy of 17 under-represented INO interaction terms. The results were generated using OntoFox [9] with the OntoFox setting
includeComputedIntermediates, and visualized using the Protege-OWL editor (http://protege.stanford.edu/). The box-enclosed terms are over- or
under-represented interaction types directly identified in our program (see Tables 2 and 3). Other terms not enclosed in boxes are terms retrieved
by OntoFox to ensure the completeness of the hierarchies.
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 8 of 10
http://www.jbiomedsem.com/content/6/1/2analysis is novel in that the input of such analysis is the
literature mined gene-gene interaction types and gene
pairs. It differs from a typical GO enrichment analysis
where a list of genes is the input. Such a strategy was
further used to study the enriched gene-gene interaction
types and gene pairs in the domain of vaccinology. Our
results demonstrate that the INO offers a repository of
hierarchical interaction keywords and a semantic plat-
form for allowing systematical retrieval of interaction
types from the literature. The INO-based gene-gene
interaction enrichment method further provides a strategy
for analyzing the retrieved gene-gene interaction literature
mining results.
The coverage of the terms in INO for interaction key-
words in literature is wide and includes three sources:
(1) The Molecular Interactions (MI) ontology: INO has
imported all the interaction-related terms in MI; (2) The
Gene Ontology (GO): Many interaction-related GO
terms have been imported to INO and aligned with the
MI terms; and (3) Newly generated interaction terms in
INO: These new interaction-related terms are not avail-
able in MI or GO, and thus we generated them in INO
with the INO_ prefix. Furthermore, INO has included
many keywords that can be used for literature mining.
These literature mining-related keywords are often varia-
tions and synonyms of the ontology term labels. The in-
clusion of these keywords significantly increases our
coverage in literature mining. To better understand the
interaction term coverage of INO, we have compared
the INO system with the commonly used GENIA ter-
minology system [7]. The GENIA term annotation sys-
tem is grounded on the GENIA ontology that definesbiomedically meaningful nominal concepts. Our com-
parison found that INO covers all 17 interaction types in
the GENIA ontology.
To further examine the interaction term coverage of
INO, we have also compared our system with the inter-
action terminology collection from the BioNLP Shared
Task 2009, focusing on recognition of bio-molecular
events reported in the biomedical literature (http://www.
nactem.ac.uk/tsujii/GENIA/SharedTask/). Nine categories
of bio-events were covered: gene expression, transcription,
protein catabolism, localization, binding, phosphorylation,
regulation, positive regulation, and negative regulation
[25]. We used the BioNLP09 Shared Task training data
set that consists of 800 abstracts manually labeled for bio-
molecular events including the event trigger words (i.e.,
interaction keywords). These abstracts include 994 unique
interaction keywords that are shown for 6,607 times in the
data set. Our comparative analysis found that INO in-
cludes 279 of these 994 unique interaction keywords.
These 279 keywords are used for 4,448 times, which
corresponds to 67% of coverage if the keyword redun-
dancy is considered. It is noted that many keywords
(e.g., by, when, source, products, necessary, through)
listed in the BioNLP09 Shared Task training data are
not considered as interaction keywords in INO. We
will fully examine all the terms in the BioNLP09
Shared Task data set and hopefully expand INO to in-
clude more interaction keywords.
Our INO-based literature mining study found that
while it is relatively easy to describe the relation between
two genes when only one interaction keyword exists in
the sentence containing these two genes, it is difficult to
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 9 of 10
http://www.jbiomedsem.com/content/6/1/2describe the relation between the two genes if multiple
keywords exist. For example, in the IFNG-IL12A
neutralization-related interaction type (Table 2), we can
infer that these two genes participate in a neutralization-
related interaction(s). However, it does not mean that
IFNG neutralizes IL12A, or vice versa. We can only say
that these two genes interact somehow in a neutralization-
related pattern.
It is likely that multiple interaction-related keywords
co-exist in one sentence. For example, an IFNG-IL12
neutralization-related sentence is In vitro IL-12
neutralization dramatically impaired the IFN-gamma
response to S. typhimurium but not to ConA [26].
This sentence contains two interaction-related key-
words neutralization and impaired. This is a complex
relation where a neutralization of one gene impairs an-
other gene expression. It hints that one gene positively
regulates another. In this case, the neutralization is really
an experimental condition. Our literature mining program
retrieved both keywords independently without consider-
ing them together. Specifically, our current method identi-
fies all the interaction keywords and maps each of them
to corresponding INO interaction terms. However, we
have not systematically modeled and integrated these co-
existing terms into better understanding of the patterns of
corresponding literature text. It would be more advanced
if we could process these two keywords simultaneously
and assign a unique interaction type, such as impairment
after neutralization, which would be a subclass (or child
term) of the existing INO term positive regulation.
While this example demonstrates a new direction of
future research, such analysis does not undermine the
contributions of the new INO-based literature mining
strategy first reported in this manuscript. Indeed, our
strategy provides a new start point and platform for
further addressing these challenges.
The analysis of vaccine-associated interaction net-
works requires intensive research. The research reported
here uses INO-based literature mining to analyze the
vaccine-relevant gene-gene interactions. More research
can be conducted to study vaccine-gene interactions and
vaccine-associated adverse events. In addition to the
PubMed literature resource used in this study, additional
public resources such as Semantic MEDLINE, summar-
izing PubMed results into an interactive graph of seman-
tic predications [27], and The Vaccine Adverse Event
Reporting System (VAERS; https://vaers.hhs.gov), col-
lecting vaccine-associated adverse events following the
administrations with various licensed vaccines [28], may
further improve the INO-based analysis. While Semantic
MEDLINE and VAERS have been used in other vaccine-
related research [29,30], INO-based approaches are
expected to advance the research on the interaction
networks among vaccines, genes, and adverse events.The integrative research combining INO and different
resources would further facilitate our understanding of
vaccine mechanisms and support public health.Conclusions
INO provides a novel approach in ontologically defining
hierarchical interaction types and related interaction
keywords for literature mining. We have adopted a
modified Fishers exact test for statistically analyzing the
enriched interactions, in terms of INO. The input of
such a novel statistical test is the gene-gene interaction
pairs together with corresponding INO interaction terms.
Such a literature mining strategy was applied and evalu-
ated in the mining of vaccine-associated gene-gene inter-
actions. The results of our study demonstrate that the
ontology-based literature mining in combination with an
INO-based statistical interaction enrichment test is able to
efficiently mine and analyze different types of vaccine-
associated gene-gene interactions and corresponding
gene pairs.
Abbreviations
INO: Interaction network ontology; PSI-MI: Proteomics standards initiative-
molecular interaction; GO: Gene ontology; VO: Vaccine ontology; BFO: Basic
formal ontology; SVM: Support vector machine; CC: Creative commons;
NCBO: National center for biomedical ontology; BH: Benjamini-Hochberg;
OGG: Ontology of genes and genomes.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
JH developed the INO-based gene interaction enrichment analysis test and
generated data with the vaccine domain use case. AO developed the
SVM-based literature mining pipeline. ZX generated the script to execute the
literature mining pipeline. YH developed the INO and was the primary writer
of the manuscript. YH, JH, and AO all participated in the project design, result
interpretation, and manuscript writing. All authors read and approved the
final manuscript.
Acknowledgements
We thank Ms. Rebecca Racz for her valuable proofreading and comments.
This research was supported by grant R01AI081062 from the US NIH
National Institute of Allergy and Infectious Diseases (to YH) and Marie Curie
FP7-Reintegration-Grants within the 7th European Community Framework
Programme (to AO). JH was supported by the Juvenile Diabetes Research
Foundation post-doctoral research fellowship. The article-processing charge
for this article was paid by a bridge fund to YH at the Unit for Laboratory
Animal Medicine (ULAM) in the University of Michigan Medical School.
Author details
1Department of Neurology, University of Michigan, Ann Arbor MI 48109, USA.
2Department of Computer Engineering, Bogazici University, 34342 Istanbul,
Turkey. 3Unit for Laboratory Animal Medicine, University of Michigan, Ann
Arbor, MI 48109, USA. 4Department of Microbiology and Immunology,
University of Michigan, Ann Arbor, MI 48109, USA. 5Center for Computational
Medicine and Bioinformatics, University of Michigan, Ann Arbor MI 48109,
USA. 6Comprehensive Cancer Center, University of Michigan, Ann Arbor MI
48109, USA.
Received: 15 November 2014 Accepted: 17 December 2014
Published: 6 January 2015
Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 10 of 10
JOURNAL OF
BIOMEDICAL SEMANTICS
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 
DOI 10.1186/s13326-015-0026-0RESEARCH ARTICLE Open AccessAmbiguity and variability of database and
software names in bioinformatics
Geraint Duck1, Aleksandar Kovacevic2, David L. Robertson3, Robert Stevens1 and Goran Nenadic1,4*Abstract
Background: There are numerous options available to achieve various tasks in bioinformatics, but until recently,
there were no tools that could systematically identify mentions of databases and tools within the literature. In this
paper we explore the variability and ambiguity of database and software name mentions and compare dictionary
and machine learning approaches to their identification.
Results: Through the development and analysis of a corpus of 60 full-text documents manually annotated at the
mention level, we report high variability and ambiguity in database and software mentions. On a test set of 25
full-text documents, a baseline dictionary look-up achieved an F-score of 46 %, highlighting not only variability and
ambiguity but also the extensive number of new resources introduced. A machine learning approach achieved an
F-score of 63 % (with precision of 74 %) and 70 % (with precision of 83 %) for strict and lenient matching respectively.
We characterise the issues with various mention types and propose potential ways of capturing additional database
and software mentions in the literature.
Conclusions: Our analyses show that identification of mentions of databases and tools is a challenging task that
cannot be achieved by relying on current manually-curated resource repositories. Although machine learning shows
improvement and promise (primarily in precision), more contextual information needs to be taken into account to
achieve a good degree of accuracy.
Keywords: Bioinformatics, Computational biology, CRF, Dictionary, Resource extraction, Text-miningBackground
Bioinformatics and computational biology rely on do-
main databases and software to support data collection,
aggregation and analysis and, as such, have been re-
ported in research papers, typically as part of the
methods section. However, limited progress has been
made to systematically capture mentions of databases
and tools in order to explore the bioinformatics practice
of computational method on a large-scale. An evaluation
of the resources available could help bioinformaticians
to identify common usage patterns [1] and potentially
infer scientific best practice [2] based on a measure of
how often or where a particular resource is currently be-
ing used within an in silico workflow [3]. Although there* Correspondence: g.nenadic@manchester.ac.uk
1School of Computer Science, The University of Manchester, Oxford Road,
Manchester M13 9PL, UK
4Manchester Institute of Biotechnology, The University of Manchester, 131
Princess Street, Manchester M1 7DN, UK
Full list of author information is available at the end of the article
© 2015 Duck et al. This is an Open Access arti
(http://creativecommons.org/licenses/by/2.0),
provided the original work is properly credited
creativecommons.org/publicdomain/zero/1.0/are several inventories that list available database and
software resources (e.g., the NAR databases and web-
services special issues [4, 5], ExPASy [6], the Online
Bioinformatics Resources Collection [7], etc.), until re-
cently, to the best of our knowledge, there were no at-
tempts to systematically identify resource mentions in
the literature [8].
Biomedical text mining has seen wide usage in identi-
fying mentions of entities of different types in the litera-
ture in recent years. Named entity recognition (NER)
enables automated literature insights [9] and provides
input to other text-mining applications. For example,
within the fields of biology and bioinformatics, NER
systems have been developed to capture species [10],
proteins/genes [1113], chemicals [14], etc. Issues of
naming inconsistencies, numerous synonyms and acro-
nyms, and an inability to distinguish entity names from
common words in a natural language combined with
ambiguous definitions of concepts, make NER a difficult
task [15, 16]. Still, for some applications, NER toolscle distributed under the terms of the Creative Commons Attribution License
which permits unrestricted use, distribution, and reproduction in any medium,
. The Creative Commons Public Domain Dedication waiver (http://
) applies to the data made available in this article, unless otherwise stated
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 2 of 11achieve relatively high precision and recall scores. For
example, LINNAEUS achieved F-scores around the 95 %
mark for species name recognition and disambiguation
on the mention and document levels [10]. On the other
hand, gene names are known for their ambiguity and
variability, resulting in lower reported F-scores. For
example, ABNER [12] recorded an F-score of just under
73 % for strict-match gene name recognition (85 % with
some boundary error toleration), and GNAT [13] re-
ported an F-score of 81 % for the same task (up to a
maximum of 90 % for single species gene name recogni-
tion, e.g., for yeast).
Some previous work exists on automated identification
and harvesting of bioinformatics database and software
names from the literature. For example, OReFiL [17] uti-
lises the mentions of Unified Resource Locators (URLs)
in text to recognise new resources to update its own in-
ternal index. Similarly, BIRI (BioInformatics Resource
Inventory) uses a series of hand crafted regular expres-
sions to automatically capture resource names, their
functionality and classification from paper titles and ab-
stracts [18]. The reported quality of the identification
process was in line with other NER tasks. For example,
BIRI successfully extracted resource names in 94 % of
cases in a test corpus, which consisted of 392 abstracts
that matched a search for bioinformatics resource and
eight documents that were manually included to test do-
main robustness. However, both of these tools focused
on updates and have biased their evaluation to resource
rich text, which prevents full understanding of false
negative errors in the general bioinformatics literature.
This paper aims to analyse database and software
name mentions in the bioinformatics/computational
biology literature to assess challenges for automated ex-
traction. We analyse database and software names in the
computational biology literature using a set of 60 full-
text documents manually annotated at the mention level,
building on our previous work [19]. We analyse the
variability and ambiguity of bioinformatics resource
names and compare dictionary and machine learning
approaches for their identification based on the results
on an additional dataset of 25 full-text documents. Al-
though we focus here on bioinformatics resources, the
challenges and solutions encountered in database and
software recognition are generic, and thus not unique to
this domain [20].
Methods
Corpus annotation and analysis
For the purpose of this study, we define databases as
any electronic resource that stores records in a struc-
tured form, and provides unique identifiers to each rec-
ord. These include any database, ontology, repository or
classification resource, etc. Examples include SCOP (adatabase of protein structural classification) [21], Uni-
Prot (a database of protein sequences and functional in-
formation) [22], Gene Ontology (ontology that describes
gene product attributes) [23], PubMed (a repository of
abstracts) [24], etc. We adopt Wikipedias definition of
software [25]: a collection of computer programs  that
provides the instructions for telling a computer what to
do and how to do it. We use program and tool as syno-
nyms for software. Examples include BLAST (automated
sequence comparison) [26], eUtils (access to literature
data) [27], etc. We also include mentions of web-services
as well as package names (e.g., R packages from Biocon-
ductor [28, 29]). We explicitly exclude database record
numbers/identifiers (e.g., GO:0002474, Q8HWB0), file
formats (e.g., PDF), programming languages and their
libraries (e.g., Python, BioPython), operating systems
(e.g., Linux), algorithms (e.g., Merge-Sort), methods
(e.g., ANOVA, Random Forests) and approaches (e.g.,
Machine Learning, Dynamic Programming).
To explore the use of database and tool names, we
have developed an annotated set of 60 full-text articles
from the PubMed Central [30] open-access subset. The
articles were randomly selected from Genome Biology (5
articles), BMC Bioinformatics (36) and PLoS Computa-
tional Biology (19). These journals were selected as they
could provide a broad overview of the bioinformatics
and computational biology domain(s).
The articles were primarily annotated by a bioinforma-
tician (GD) with experience in text mining. The annota-
tion process included marking each database/software
name mention. We note that associated designators of
resources (e.g., words such as database, software) were
included only if part of the official name (e.g., Gene
Ontology). The inter-annotator agreement (IAA) [31] for
the annotation of database and software names was
calculated from five full-text articles randomly selected
from the annotated corpus, which were annotated by a
PhD student with bioinformatics and a text-mining
background.
To assess the complexity, composition, variability and
ambiguity of resource names, we performed an analysis of
the annotated mentions. The corpus was pre-processed
using a typical text-mining pipeline consisting of a tokeni-
ser, sentence splitter and part-of-speech (POS) tagger from
GATEs ANNIE [32]. We analysed the length of names,
their lexical (stemmed token-level) and structural com-
position (using POS tag patterns) and the level of variabil-
ity and ambiguity as compared to common English words,
acronyms and abbreviations.
In addition to the dataset of 60 articles that was used
for analysis and development of NER tools, an additional
dataset of 25 full-text annotated papers was created to
assess the quality of the proposed NER approaches (see
below).
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 3 of 11Dictionary-based approach (baseline)
We compiled an extensive dictionary of database and
software names from several existing sources (see
Table 1). Some well-known acronyms and spelling/
orthographic variants have also been added, resulting
in 7322 entries with 8169 variants (6929 after removing
repeats) for 6126 resources. The names collected in the
dictionary were also analysed using a similar approach
as used for the names appearing in the corpus (see
above). We then used LINNAEUS [10] to match these
names in text.
Machine learning approach
Given the availability of the manually annotated corpus,
a machine learning (ML) approach was explored for
identification of resource names. We approached the
task as a sequence-tagging problem as often adopted in
NER systems. We opted for Conditional Random Fields
(CRF) [33] and used features at the token-level that com-
prised the tokens own characteristics and the features of
the neighbouring tokens. We used the Beginning-Inside-
Outside (B-I-O) annotation.
The following features were engineered for each token:
1. Orthographic features captured the orthographic
patterns associated with biomedical resources
mentions. For example, a large percentage of
mentions are acronyms (e.g., GO, SCOP), capitalised
terms (e.g., Gene Ontology, Bioconductor) or words
that contain a combination of capital and lower cap
letters (e.g., MySQL, UniProt) etc. We engineered
two groups of orthographic features [34]. The first
group comprised shape (pattern) features that
mapped a given token to an abstract representation.
Each capital letter is replaced with X, lower caseTable 1 Sources from which the database and software name dictio
Type Entries Variants
DB 195 298
SW 263 278
PK 799 799
SW 2033 2087
SW 389 391
DB 379 379
DB 1452 1670
SW 135 135
SW 36 41
SW 1149 1183
SW, DB 171 231
Our dictionary (DB, SW, PK) 7322 6929
Note that entries and variants are not necessarily unique to a single resource list
DB databases, SW software, PK packages; data correct and accessible as of Februaryletter with x, a digit with d and any other
character with S. Two features were created in this
group: the first feature contained a mapping for each
character in a token (e.g., MySQL was mapped to
XxXXX); the second feature mapped a token to a
four character string that contained indicators of a
presence of a capital letter, a lower letter, a digit or
any other character (absence was mapped to a _),
e.g., MySQL was mapped to Xx_ _. The features in
the second group captured specific orthographic
characteristics (e.g., is the token capitalised, does it
consist of only capital letters, does it contain digits,
etc.  see Table 2 for the full list), which were
extracted by a set of regular expressions.
2. Dictionary features were represented by a single
binary feature that indicated if the given token was
contained within our biomedical resources
dictionary.
3. Lexical features included the token itself, its lemma
and part-of-speech (POS) tag.
4. Syntactic features were extracted from syntactic
relations in which the phrase was a governor or a
dependant, as returned by the Stanford parser [35,
36]; in cases where there were several relations, the
relation types were alphabetically sorted and
concatenated (e.g., pobj and advmod were
combined as advmod_pobj).
The experiments on the training data revealed that
two tokens before and one token after the current token
provided the best performance. The CRF model was
trained using CRF++ [37]. All pre-processing needed for
feature extraction was provided by the same text-mining
pipeline as used for the corpus analysis and dictionary-
based approach.nary is comprised
Source
databases.biomedcentral.com
www.bioinformatik.de
www.bioconductor.org
bioinformatics.ca/links_directory/
evolution.genetics.washington.edu/phylip/software.html
www.ebi.ac.uk/miriam/main/
www.oxfordjournals.org/nar/database/a/
www.netsci.org/Resources/Software/Bioinform/index.html
www.bioinf.manchester.ac.uk/recombination/programs.shtml
en.wikipedia.org/wiki/Wiki/<various>
Manually added entries
http://sourceforge.net/projects/bionerds/
28th, 2012
Table 2 Token-specific orthographic features extracted by
regular expressions
Name Description
isAcronym token is an acronym
containsAllCaps all the letters in the token are capitalised
isCapitalised token is capitalised
containsCapLetter token contains at least one capital letter
containsDigits token contains at least one digit
isAllDigits token is made up of digits only
Table 3 Statistics describing the manually annotated corpora
Development Test
Total number of documents 60 25
Total database and software mentions 2416 1479
Total unique resource mentions 401 301
Percentage of database mentions 36 % 28 %
Percentage of unique database mentions 27 % 30 %
Average mentions per document 40.3 70.0
Average unique mentions per document 8.1 13.4
Maximum mentions in a single document 227 217
Maximum unique mentions in a single
document
57 55
Resources with only a single lexicographic
mention
201 147
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 4 of 11Machine learning  post-processing
An analysis of the initial CRF results on the develop-
ment dataset revealed that a large portion of false nega-
tives were from resource mentions that were recognised
by the model at least once in a document, but missed
elsewhere within the same document. We have therefore
designed a two-pass post-processing approach. The first
pass collected and stored all the CRF tagging results.
These were then used to re-label the tokens in the sec-
ond pass. In order to avoid over-generation of labels
(i.e., possible false positives), we created a set of condi-
tions that each token had to meet if it was to be re-
labelled as a resource mention. First, it had to be labelled
as a (part of a) resource name in the first pass more
often than it was not, looking at the entire corpus that
was being tagged. If that was the case, the candidate
token also had to fulfil one of the following two condi-
tions: either it was contained within the biomedical re-
sources dictionary; or it was an acronym that had no
digits and was at least two characters long. Finally, the
following four tokens: analysis, genomes, cycle and
cell were never labelled as part of resource name in
the second round, as they were found to be the source
of a large percentage of false positives.
Evaluation
Standard text-mining performance statistics (precision,
recall, F-score) were used for evaluation. In particular,
we make use of 5-fold cross-validation across all 60 full-
text articles for both the dictionary and machine learn-
ing approaches. For a fair comparison, the dictionary-
based approach is only evaluated on the test set in each
fold, as it requires no prior training. We also test both
approaches directly on the test set of 25 articles without
additional training/adjustments.
Results and discussion
Corpus annotations
Table 3 gives an overview of the two corpora annotated
with resource mentions. We note that the IAA was rea-
sonably high: with lenient agreement (annotation offsets
overlap), an F-score of 86 % was calculated (93 % preci-
sion, 80 % recall). As expected, a decrease in IAA isobserved if strict agreement (offsets must exactly match)
is used instead (every score drops by 6 %).
In the development corpus, there were 401 lexically
unique resources mentioned 2416 times (6 mentions on
average per unique resource name), with an average of
40 resource mentions per document. The document
with the most mentions had 227 resource mentions
within it. Finally, 50 % of resource names were only
mentioned once in the corpus. A similar profile was
noted for the test corpus, although it contained notably
more resource mentions per document.
Database and software name composition
We first analysed the composition of resource names
both in the development corpus and dictionary. The lon-
gest database/software name in the annotated corpus
contained ten tokens (i.e., Search Tool for the Retrieval
of Interacting Genes/Proteins). However, there are longer
examples in the dictionary (e.g., Prediction of Protein
Sorting Signals and Localisation Sites in Amino Acid
Sequences).
To assess the composition of resource names within
our dictionary, we stemmed each token within each
name (using the Porter Stemming Algorithm [38]) and
counted the occurrences of each stemmed token.
Figure 1 displays the most frequent words: the two most
ones are database and ontology. A comparable lexical
distribution can be noted in the development set, with
database, gene, analysis, tool, genome, ontology featuring
as the most frequent ones (data not shown). This sug-
gests that some common head terms and some other
common bioinformatics relevant terms could aid recog-
nition. We also note that there is a long tailed curve in-
volved in the lexical decomposition of resource words.
As an initial structural analysis, we automatically col-
lected all the POS tags assigned to each unique database
and software name in the development corpus. These
Fig. 1 Top token frequencies within the manually compiled dictionary. The figure shows the most common stemmed tokens contained within
all the resource names found within our manually compiled dictionary. The top token is database with a count of 474, followed by ontology with
187 instances. Note that the scale is logarithmic (log base 2), and the y-axis crosses at eight rather than zero (for aesthetic reasons). The top terms
are labelled
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 5 of 11were then grouped to profile the structure of resource
names (see Table 4). We have identified a total of 405
patterns. The majority (79 %) of database and software
names comprise one, two or three proper nouns. An
additional 5 % were tagged a single common noun (e.g.,
affy). A roughly equivalent number of names contain
digits (e.g., S4, t2prhd). Nine patterns contain adjectives
(e.g., internal transcribed spacer 2) or prepositions/sub-
ordinating conjunctions (e.g., Structural Classification
Of Proteins). Finally, in two cases (SHAKE and dot), a
mention of software was tagged as a verb form. We note
that there are more patterns (405) than unique mentions
(401) because sometimes an equal resource name gets
tagged with differing patterns (e.g., R received both
NNP and NN POS tags). The analysis shows that there
is some variety in resource naming, and  as expected 
that recognition of simple noun phrases alone is notTable 4 Internal POS structure of database and software names
(the development corpus)
Pattern Count Frequency
NNP 258 63.7 %
NNP NNP 34 8.4 %
NNP NNP NNP 26 6.4 %
NN 20 4.9 %
NNP CD 16 4.0 %
NNP NNP NNP NNP 8 2.0 %
Other Patterns 43 10.6 %
NNP proper noun, NN singular noun, CD cardinal numbersufficient for identification of potential resource men-
tions. In particular, around 5 % of noun-phrases (as
extracted with the Stanford Parser) within the corpus
contain at least one resource mention.
Variability of resource names
To evaluate the variability of resource names within our
dictionary, we calculated the average number of name
variants for a given resource. As such, the variability of
resource names at the dictionary level is 1.13 (6929
unique variants over 6126 resources, after adjustment
for repeats). For the corpus analysis, we manually
grouped the names from the set of manually annotated
mentions that were referring to the same resource in
order to analyse name variability. Specifically, we grouped
variants based on spelling errors and orthographic dif-
ferences, and then grouped long and short form acro-
nym pairs based on our own background knowledge,
and the text from which they were initially extracted.
Of the 401 lexically unique names, 97 were variants of
other names, leaving 304 unique resources. In total,
231 resources had only a single name variant within
the corpus (76 %); 18 % of resources had two variants,
and the final 6 % had between three and five variants.
Of the 97 name variants, 36 were acronyms and most
of those were defined in text (and so could perhaps be
automatically expanded with available tools, e.g., [39]).
However, there were other cases where a resources
acronym was used without the expanded form for def-
inition (e.g., BLAST).
Table 6 Dictionary matching results on the development
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 6 of 11Ambiguity of resource names
As expected, a number of ambiguous resource names
exist within the bioinformatics domain. Interesting ex-
amples include Network [40] (a tool enabling network
inference from various biological datasets) and analysis
[41] (a package for DNA sequence analysis). We there-
fore analysed our dictionary of database and software
names to evaluate dictionary-level ambiguity when com-
pared to the entries in a full English words dictionary
derived from a publicly available list [42] (hereafter re-
ferred to as the English dictionary) and to a known
biomedical acronyms dictionary compiled from ADAM
[43] (hereafter referred to as the acronym dictionary),
consisting of 86,308 and 1933 terms, respectively. A total
of 52 names matched English words (e.g., analysis, cycle,
graph) and 77 names fully matched known acronyms
(e.g., DIP, distal interphalangeal and Database of Inter-
acting Proteins) when using case-sensitive matching. The
number of matches increases to 534 to the English dic-
tionary and to 96 for the acronym dictionary when case-
insensitive matching is used instead.
To evaluate the recognition-level ambiguity within the
annotated corpus, we also compared the annotated data-
base and software names to the English dictionary and
acronym dictionary. This resulted in four matches to the
English dictionary (ACT, blast, dot, R), and six to the
acronym dictionary (BBB, CMAP, DIP, IPA, MAS, VOCs)
using case-sensitive matching. This equates to roughly
3 % of the unique annotated names. The total increases
to 53 matches (17 %) if case-insensitive matching is used
instead.
Dictionary-based matching
Table 5 provides the standard text-mining performance
statistics for the dictionary matching approach. The
average lenient F-scores between 43 and 46 % highlight
the challenges for this approach, both in terms of match-
ing known ambiguous names (low precision), and from
the dictionary not being sufficiently comprehensive (lowTable 5 Evaluation results on the development and test
corpora
Development corpus Recall (%) Precision (%) F-score (%)
Dictionary 49 (47) 38 (37) 43 (41)
CRF with post-processing 58 (52) 76 (67) 65 (58)
CRF without post-processing 54 (49) 78 (70) 62 (57)
Test Corpus
Dictionary 46 (44) 46 (44) 46 (44)
CRF with post-processing 60 (54) 83 (74) 70 (63)
CRF without post-processing 53 (45) 71 (65) 62 (53)
Strict scores provided in brackets
P Precision, R Recall, F F-score evaluation on the development (5-cross validated)
and test corporarecall). Some common false positives were cycle, genomes
(potential mentions of Bioconductor packages) and GO
(which was frequently matched within GO database
identifiers (e.g., GO:0007089) because of inappropriate
tokenisation). Some common false negatives (i.e., missed
resource mentions) included Tabasco (PMC2242808),
MethMarker (PMC2784320), xPedPhase and i Linker
(both from PMC2691739). In each of these examples, the
name missed (numerous times) was the resource being in-
troduced in that paper. This shows that any NER for data-
base and software names must be able to capture newly
introduced resources to achieve high recall.
We note here the high variation in the different fold
scores (e.g., see the results for Fold 3 in Table 6), indi-
cate how challenging detection of resource names could
be, depending on the particular document. We also note
a difference between the results reported here (lenient
F-score of 4346 %) and those we obtained previously
[19] on a subset of 30 documents from the development
set (lenient F-score of 54 %). The drop in performance
can be partially contributed to the changes to both the
dataset (60 vs. 30 articles) and the underlying dictionar-
ies (updated), as well as the change in the evaluation ap-
proach (cross-fold vs. evaluating the entire dataset at
once; thus, a fold with an overrepresentation of false
negatives cannot be balanced out by another fold with
an overrepresentation of true positives (and the same for
false negatives)).
Machine-learning approach
The results of the application of the CRF model are
presented in Table 5. With post-processing, the average
F-scores of 6570 % for lenient and 5863 % for strict
matching present a considerable improvement over the
dictionary-based approach, but still leaves the task only
moderately solved. Table 7 shows the results of different
folds for the development corpus. It is interesting thatcorpus
Fold Recall (%) Precision (%) F-score (%)
1 46 (43) 41 (39) 43 (41)
2 34 (31) 37 (34) 36 (32)
3 36 (34) 24 (23) 29 (27)
4 55 (53) 46 (45) 50 (49)
5 76 (75) 44 (43) 56 (55)
Min 34 (31) 24 (23) 29 (27)
Max 76 (75) 46 (45) 56 (55)
Mean 49 (47) 38 (37) 43 (41)
Note that for Fold 3, a decrease in score (of about 8 % F-score) is observed if
the LINNAEUS abbreviation detected is disabled. Strict scores provided
in brackets
P Precision, R Recall, F F-score on the development set using dictionary
look-up
Table 7 Machine learning results with post-processing on the
development corpus
Fold Recall (%) Precision (%) F-score (%)
1 51 (44) 71 (60) 59 (51)
2 44 (35) 88 (71) 59 (47)
3 51 (44) 76 (66) 61 (53)
4 65 (60) 73 (67) 69 (63)
5 80 (76) 74 (70) 77 (73)
Min 44 (35) 71 (60) 59 (47)
Max 80 (76) 88 (71) 77 (73)
Mean 58 (52) 76 (67) 65 (58)
Micro Avg 56 (50) 76 (67) 65 (57)
Strict scores provided in brackets
P Precision, R Recall, F F-score on the development set using machine learning
with post-processing (5-cross fold)
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 7 of 11precision was relatively high (7683 %), while recall was
notably lower (5860 %). These results lead us to believe
that the current feature set is insufficient to capture lex-
ical variability in sentences with biomedical resource
mentions. The lenient matching scores were generally
higher than the strict scores (7 % on F-score, 6 % on re-
call and 9 % on precision), which indicates that bound-
ary adjustment of the recognised tokens is a challenging
task, similar to other biomedical NER tasks.
The application of the ML-model with post-processing
showed positive effects, as the results without post-
processing had consistently lower recall (drop of 47 %
for lenient and 39 % for strict matching). While the ef-
fect on precision was not stable, the overall F-score has
still increased (38 % for lenient and 110 % for strict
matching). Table 8 presents the details on the folds for
the development corpus. To further evaluate the loss in
recall when the post-processing step is omitted, we ana-
lysed the full list of false negative mentions to extract
what percentage of these were dictionary matches, but
had nevertheless been rejected by the ML approach. ItTable 8 Machine learning results without post-processing on
the development set
Fold Recall (%) Precision (%) F-score (%)
1 46 (41) 78 (69) 58 (51)
2 42 (35) 89 (75) 57 (48)
3 45 (41) 75 (70) 56 (52)
4 60 (55) 71 (66) 65 (60)
5 76 (74) 74 (72) 75 (73)
Min 42 (35) 71 (66) 56 (52)
Max 76 (74) 89 (75) 75 (73)
Mean 54 (49) 78 (70) 62 (57)
Micro Avg 52 (47) 77 (70) 62 (56)
P Precision, R Recall, F F-score on the development set using machine learning
without post-processing (5-cross fold). Strict scores provided in bracketsturns out that this occurred in 158 (15 %) of the false
negative mentions. While providing more training data
could help, this issue could perhaps be also addressed by
using additional features (for example, utilising some of
the rules we suggest in the next section), or by combin-
ing dictionary and ML-methods. We note, however, that
the direct merge of the dictionary and ML results is in-
sufficient due to the large number of false-positives that
dictionary matching introduces (see Table 9). Specific-
ally, combining both results gives an average increase in
recall of 5 % (across all folds), but a large reduction in
precision, resulting in an average reduction in F-score of
15 %.
Feature impact analysis for the ML model
We explored the impact that particular groups of fea-
tures have on the recognition of biomedical resource
names. During the 5-fold cross validation, each of the
feature groups was removed and the CRF models were
then trained and applied to the test fold enabling us to
evaluate the contribution of each group. The CRF
models were built without post-processing as we wanted
to avoid the contributions being biased by that step (es-
pecially because it uses the dictionary predictions). The
results are presented in Table 10.
Overall, the lexical features were beneficial: when this
group of features was removed, there was a drop of 8 %
in precision, 6 % in recall, resulting in a 7 % lower F-
score. The syntactic features had only a slight impact on
the performance: removing this group resulted in a 1 %
drop in both precision and recall and a 2 % in F-score.
The orthographic features had a similar effect as the lex-
ical features: when these were removed, there was an
8 % loss in precision, a 6 % loss in recall, resulting in a
7 % loss in F-score. Surprisingly, removing the dictionary
features did not result in a high decrease in performance
(there was a drop of 8 % in precision, a 5 % drop in re-
call and thus a 6 % drop in F-score), suggesting that theTable 9 Combined dictionary and machine learning results on
the development set
Fold Recall (%) Precision (%) F-score (%)
1 56 (49) 43 (38) 49 (42)
2 50 (41) 45 (37) 48 (39)
3 57 (52) 32 (29) 41 (37)
4 68 (64) 45 (42) 54 (51)
5 87 (84) 45 (43) 59 (57)
Min 50 (41) 32 (29) 41 (37)
Max 87 (84) 45 (43) 59 (57)
Mean 64 (58) 42 (38) 50 (45)
P Precision, R Recall, F F-score on the development set combining the
dictionary and machine learning annotations (5-cross fold). Strict scores
provided in brackets
Table 10 Feature impact analysis of the machine learning
model without post-processing on the development set
Feature group Recall (%) Precision (%) F-score (%)
All features 54 (49) 78 (70) 62 (57)
No lexical features 46 (43) 68 (62) 54 (50)
No syntactic features 53 (48) 77 (69) 61 (55)
No orthographic features 48 (43) 70 (62) 55 (50)
No dictionary features 49 (44) 70 (62) 57 (51)
P Precision, R Recall, F F-score feature contribution results comparison. Strict
scores provided in brackets
Table 12 Example clues and phrases appearing with specific
heads or in Hearst patterns
 the stochastic simulator Dizzy allows 
The MethMarker software was 
 tools: CLUSTALW, , and MUSCLE.
 programs such as Simlink, , and SimPed.
Database and software names are in italics, the associated clue is in bold
Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 8 of 11ML-model (without the aid of a dictionary), even with
the relatively limited amount of training data, managed
to capture a significant number of resource mentions.
Missed database and software mentions
We further analysed the database and software names
not picked up by our ML approach for any common
textual clues and patterns. Table 11 summarises different
clue categories and their potential relative contribution
to the overall recall. Overall, using all clues that we have
recognised (see below), final recall could be as high as
94 % (Table 11), though utilising all of these pointers will
likely have a detrimental effect on precision.
The first type of clue that seemed most discriminatory
was to associate potential names with head terms, i.e.,
terms that are explicit designators of the type of re-
source. In the most basic case, a resource name could
include a head term or be immediately followed by one
(see Table 12). Key head terms included database,
software, tool, program, simulator, system, library and
service. Still, we note that not all potential clues are fully
discriminatory. For example, we note that including sys-
tem as a head clue might be problematic as the word
can have other uses and meaning within biology (e.g.,
biological systems). Similarly, although module could be
a useful head for identification of software names,
the mention of module(s) in P and D modulesTable 11 Types of textual patterns and clues for identification
of database and software names
Type Contribution to total TPs
Machine learning matches 55.3 %
Heads and Hearst Patterns 9.8 %
Title appearances 0.5 %
JOURNAL OF
BIOMEDICAL SEMANTICS
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 
DOI 10.1186/s13326-015-0016-2
RESEARCH ARTICLE Open Access
Developing VISO: Vaccine Information
Statement Ontology for patient education
Muhammad Amith1, Yang Gong1, Rachel Cunningham2, Julie Boom2 and Cui Tao1*
Abstract
Objective: To construct a comprehensive vaccine information ontology that can support personal health
information applications using patient-consumer lexicon, and lead to outcomes that can improve patient education.
Methods: The authors composed the Vaccine Information Statement Ontology (VISO) using the web ontology
language (OWL). We started with 6 Vaccine Information Statement (VIS) documents collected from the Centers for
Disease Control and Prevention (CDC) website. Important and relevant selections from the documents were recorded,
and knowledge triples were derived. Based on the collection of knowledge triples, the meta-level formalization of the
vaccine information domain was developed. Relevant instances and their relationships were created to represent
vaccine domain knowledge
Results: The initial iteration of the VISO was realized, based on the 6 Vaccine Information Statements and coded into
OWL2 with Protégé. The ontology consisted of 132 concepts (classes and subclasses) with 33 types of relationships
between the concepts. The total number of instances from classes totaled at 460, along with 429 knowledge triples in
total. Semiotic-based metric scoring was applied to evaluate quality of the ontology.
Keywords: Biomedical informatics, Vaccines, Vaccine Information Statements, Knowledge based systems, Ontology,
Ontology construction
Introduction
In the present information age, patients are affordedmany
options to educate themselves on vaccines. Some of these
options included valid and reputable websites, books, and
other media sources while other options may appear rep-
utable but are not. Regardless of the sources credibility,
one researcher found that 70% of individuals who seek
vaccine information online are influenced by what they
find on the Internet [1]. Hopefully, patients and parents
are able to identify the most reliable resources such as
the Centers for Disease Control and Preventions (CDC)
Vaccine Information Statements (VIS). VISs were devel-
oped in response to the National Childhood Vaccine
Injury Act (NCVIA) which was passed in an effort to
minimize provider liability and respond to public health
concerns. The NCVIA requires healthcare providers to
provide a VIS to the person receiving the vaccine or
*Correspondence: cui.tao@uth.tmc.edu
1School of Biomedical Informatics, University of Texas Health Science Center,
7000 Fannin St, 77030 Houston, TX, USA
Full list of author information is available at the end of the article
his/her guardian. The VIS must be given each time a
vaccine is administered and provides information on the
benefits and risks of the vaccine as well as other rele-
vant disease information. They are usually provided to the
patient as a handout [2].
Some researchers have identified specific issues with
the dissemination of vaccine information through these
documents. Both Lieu, et al., and St. Amour, et al. have
revealed that vaccine documentation handed to patients
are rarely read or fully understood by the patient [3,4].
This kind of education is called passive education, because
the patients may or may not choose to read the materials.
In addition, while the purpose of the VIS is to inform par-
ents and initiate potential questions, limited clinical staff
time and resources may hinder the optimal interactions
ideal for vaccine education. This demonstrates a pervasive
issue with medical education for patients, where the deliv-
ery and presentation should bemore consumer-friendly in
order to effectively impact patient education and increase
knowledge [5-7].
© 2015 Amith et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 2 of 12
Biomedical-related ontologies have had an impact on
patient learning and decision-making by utilizing patient-
friendly terminology rather than confusing medical jar-
gon. Ontologies are understood to be a consensus-based
controlled vocabulary between terms and relationships,
while serving as a vehicle of interaction between humans
and computers. In lay terms, it is similar to concept
maps or graphs but with the capacity to link with other
graphs, and evoke reasoning and inferences. Some pub-
lished examples and ideas include:
 ontology-driven clinical decision support systems
for patients in regards to discharge medication [8],
 building medical ontology models for Italian
patients [9],
 an ontology-based coaching tool for physicians to
prepare dialogue with patients [10],
 and dialogue systems for patient planning [11]
Following in the examples described, we developed an
ontology-driven mobile application system to improve
patient learning and comprehension of vaccine knowl-
edge [12]. While a proof of concept prototype, the sys-
tem was limited by the test ontology which comprised
of 19 classes and 82 instances, a relatively small knowl-
edge base with which to interact. To further extend
this project, as a first step, we aimed to create a com-
prehensive vaccine ontology with which patients can
interact.
The objective of this paper is to formalize the vac-
cine knowledge for patient education using ontology tools
as a solution for vaccine education for the public. We
propose modeling vaccine knowledge using OWL2 (Web
Ontology Language) [13], which will be encoded with
Protégé [14]. We focus on the knowledge in the VISs
developed by the CDC. By publishing the Vaccine Infor-
mation Statement Ontology (VISO) in a domain ontology
and offering the benefit of a serialized format that can
be processed by a machine and reused [15], we intro-
duce opportunities to develop ontology-driven personal
health agents to improve patient learning and compre-
hension of vaccine knowledge. Overall, the overarching
goal is to develop a scalable ontological model that can
reliably cover any new and applicable vaccine knowledge
for a consumer audience. With a scalable conceptual-
level model, opportunities in natural language ontology
learning and population would be a future possibility to
investigate for automated upkeep and maintenance of the
ontology.
This paper will start by briefly discussing the source
material, the VISs. Afterwards, the paper will segue way
into describing the development of the class-level schema
and the instance level of VISO. This will include discus-
sion of the initial common design patterns encountered
and basic ontology metrics. The Results section will elab-
orate the quantitative and qualitative aspects of VISO, that
also includes an initial evaluation scoring. The paper will
then close with challenges in developing an ontology from
the CDC VISs and future direction.
Materials
The focus of this paper is the ontology representation of
the knowledge in the VISs developed by the CDC. Cur-
rently there are 25 VISs available from the CDCs website,
and these documents vary between 1 to 2 pages [16].
The VISs describe patient-level information about vac-
cines ranging from the historical burden of disease to the
clinical indications for the vaccine.
The current iteration of the VISO is derived from six
CDC VISs:
 Diphtheria, tetanus, and acellular pertussis vaccine
(DTaP) [17]
 Rotavirus vaccine [18]
 Hepatitis B vaccine [19]
 Haemophilus influenzae type B vaccine (Hib) [20]
 Measles, mumps, and rubella (MMR) [21]
 Pneumococcal conjugate vaccine (PCV13) [22]
All of the vaccines selected are recommended for chil-
dren on or before the first year of life. Each of the VIS
documents are available for download from the CDCs
website in both PDF and RTF format, along with the
HTML version.
The VISs are written at a 10th grade reading level [23],
and with no images or figures to supplement any passages.
Also from a subjective observation, there appears to be
some organized consistency of the content within each of
the documents, which also helped provide a skeletal struc-
ture for the VISO ontology. Some examples of consistent
dedicated sections included:
 General vaccine and disease information.
 Possible vaccine reactions and/or side effects.
 Populations for whom the vaccine is indicated.
 General vaccine recommendations and dose
information.
Method
Each of the six VISs acquired from the CDCs website
were examined, and sentences were identified as relevant
selections to inform the design of VISO. Ignoring headers
and standard text that appeared on VIS documents, the
relevant passages were transcribed onto a tracking spread-
sheet and coded for simple identification. Any fragment
with bullet points was also recorded. Each phrase in the
VIS was broken down into a knowledge triple, which is
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 3 of 12
a piece of factual information that is decomposed to a
subject-predicate-object format that can be visually rep-
resented or modeled in an ontology. Separate tracking
documents were used for individual VIS documents for
management purposes. Later, separate knowledge mod-
els for each of the VIS documents were realized, and
then collated to obtain a comprehensive model of vaccine
information. The proceeding subsections will discuss the
high-level conceptualization of the VISO model, detailing
the class-level organization and formalization of knowl-
edge, and the last subsection will discuss the encoding of
the instances and triples, along with reoccurring design
patterns in the VIS domain. The end goal was to develop
common classes that can accommodate a large corpus
of vaccine information, and have a formalized model to
cover an expansive vaccine domain that is relevant for
patients.
Meta-level conceptualization
For a presentation of the properties between the classes
see Figure 1. The oval shapes in the diagram depicts
the high-level classes in VISO. Between the oval-shaped
classes, a line connects two classes to signify a relation-
ship, and a dotted line depicts a relationship between
subclasses. Labels are placed beside both the classes
and the relationship connection to provide identification.
For clarification, some classes in the diagram have the
same name, which means they are referring to the same
class.
Class-level representation
Figure 1 illustrates the meta-level interpretation of the
VISO. The model represents a composite of the high
abstraction understanding of the six VISs with 23 unique
high-level parent classes. Table 1 outlines all of the high-
level classes employed for the six VIS document models.
The first column lists the formal name of the high-level
class, and its corresponding description is listed in the
next column. Also, examples of subclasses derived from
the class and sample instances are listed in another col-
umn, and a count of the number of instances using the
class (or using its subclasses) were registered. The most
salient classes found in all of the VIS documents wereVac-
cine, Target, Reaction, Dosage, and People. The Vaccine
class designated the vaccine that is the subject of the VIS
document. Target class referred to what the Vaccine class
is protecting against, e.g. hepatitis B, rotavirus, etc. The
Reaction class categorizes any possible side effect or reac-
tion following vaccination. The Dosage class is used for
any vaccine series and/or dose information from the VIS,
Figure 1 Vaccine Information Statement Ontology (VISO). Parent-level graph of VISO classes.
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 4 of 12
Table 1 Class table
Meta-level class Description Example subclasses or Instances (in italics) Instances
Vaccine A class description to categorize vaccines
documented by the CDCs VIS documentation
AlternateVaccine 9
Target Provides class specification for virus, bacteria,
diseases, etc. that are prevented by Vaccine
Disease
Virus, SeriousDisease, Bacteria 12
People Categorizes various types of patients or groups of
individuals impacted by vaccines, diseases,
reactions, and other health conditions
PeopleWithCondition, Infants, PeopleWithIllness,
PeopleWithModerateIllness, PeopleOfAge,
Children, Adults
93
Source Used to describe an origin of a vaccines target
(bacteria, disease). Can be reused in relations with
other classes.
bacteria, Hepatitis B virus 5
Channel Class of medium of travel for vaccine targets. PeopleChannel, ObjectChannel,
PeopleWithConditionChannel, DermalChannel,
HumanActivityChannel, FluidChannel s
19
Cause For a description of a condition as a result of a
vaccine target. E.g. infection, coughing spells.
ear infection, long-term illness, coughing spells, pneumonia 39
Location Type to categorize location, specifically area of the
body, affected by a heath condition or reaction
FacialLocation, ThroatLocation, ArmLocation 14
Probability Classification for types of probabilities
enumerated in VIS documents
QualitativeProbability, QuantitativeProbability,
ProbabilityInCases
38
Outcome Types of effects resulting from causation,
reactions, or chain of outcomes.
RareOutcome, AdultOutcome, ChildOutcome,
FatalOutcome
38
Duration Used for various types of descriptions for
qualitative length of time for effects of health
conditions or signs of conditions.
DurationInMinutes, DurationInWeeks 5
Substance Classification of kinds of substance for vaccines
and possibly other class groupings.
LiquidSubstance, GaseousSubstance,
SolidSubstance
1
Combination For various artifacts that interact with vaccines. SafeCombination, DangerousCombination 1
Method Groups and classifies inoculation methods for
vaccines.
InhaleMethod, InjectionMethod, OralMethod 1
NumberOfDoses Enumerates the maximum number of doses for
vaccine.
OptionalNumberOfDoses 6
Dosage Designates the types of dose or the dose interval.
E.g. 1st Dose, 3rd Dose.
OptionalDose, DoseIntervals 23
Component Categorizes types of elements of a vaccine. ViralComponent, NoninfectiousViralComponent 3
Age Enumerates the type of quantitative classification
of age ? years, months, weeks, etc.
AgeInMonths, AgeInYears 25
Date Enumerates the types of quantitative classification
of date ? days, months, weeks, etc.
DateInYears, DateInMonths, DateInDays 4
Occurrence Classification of types of events VaccinatedOccurence, TimedOccurence 24
Action Types of patient recourse in response to
reactions or actions required vaccine patients
before inoculation.
InactiveAction, ActiveAction, EmergencyAction 8
Allergen For classes of substances leading to an allergic
reaction.
VaccineAllergen, VaccineComponentAllergen 7
Reaction Used to categorizes types of reactions that may
result from vaccination.
MildReaction, TemporaryReaction, SeriousReaction,
ModerateReaction, AdultReaction
71
Sign Class of indicators for vaccine reaction effects. fast heartbeat, crying, stomach pain, hives 14
Total instances of classes 460
and the People class organizes various classes of patients
who may or may not be recommended to receive the
vaccine.
Subclass-level representation
The VISO models high-level classes utilizes extensive
subclassing to cover specific abstraction of entities. One
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 5 of 12
apparent use was for categorization purposes to organize
the knowledge for better identification, such as the case
for the People class. For example, the VIS documents often
refer to a specific population that may react differently to
vaccines, or may need a different vaccine type due to age
or certainmedical conditions, justifying the need to create
subclasses to refer to specific groups of people.
Another motivation for subclasses is to facilitate
descriptions from the text that are in the form of
adjectives or prepositions. If the documentation refers
to the number of optional doses of a vaccine, a sub-
class called OptionalNumberOfDoses, which is a child
of NumberOfDoses, was defined. Attention was given
to classes that have universal subclasses that may not
have been observed from the VIS documentation. Some
examples are Organ class (Heart, Liver, Lungs) or
Substance class (GaseousSubstance, LiquidSubstance,
SolidSubstance).
Value sets and partitions
To address quantitative, descriptive, or ranks in classes,
value set representations, as described in a W3C work-
ing draft [24], was utilized for the VISO meta-ontology.
More specifically, we use subclasses to represent per-
missible values in a value set. For example, the VIS
documentation alluded to a three-point scale, especially
when describing severity of conditions - mild, moder-
ate, severe (or serious). We created a Reaction class that
has the subclasses MildReaction, ModerateReaction, and
SeriousReaction to describe the degree of vaccine effect
severity. Other classes that employ value sets include
the People class and the Target class. For any other
units of measure revealed in the VIS documents, spe-
cific classes yielded subclasses that handled units of
measure, such as, AgeInYears and AgeInMonths for the
Age class or DateInMonths and DateInDays for the Date
class.
Properties
Knowledge triples evoked by the VIS documents sug-
gested common relationships or properties between
classes across the VIS corpus. In all of the 6 documents,
it was common to describe a vaccine preventing a bacte-
rial infection or virus, or a vaccine can potentially cause
a rare reaction following administration. Many of these
properties between the classes were identified and nor-
malized to a standard representation. One example in the
DTaP VIS, the evoked triple - Another vaccine, called Td,
protects against tetanus and diphtheria... - utilized the
predicate protect against, which essentially means Td
prevents tetanus and diphtheria. In result, Td protects
against tetanus was rendered as Td prevents tetanus,
where prevents is the standard property, or controlled
term, to describe that specific relationship.
Table 2 identifies all of the object properties utilized in
VISO. The first column list the domain classes, with the
properties and range classes in the subsequent columns.
Overall, 33 types of object properties exist in the current
iteration of VISO.
Table 3 list data properties. Similar in format to Table 2,
two types of data properties are used in VISO with the
domain listed as Protégés superclass Thing and a string
literal for its range. These data properties are meant to be
global to all of the classes. These two properties serve as
utility properties to accommodate information that either
provide an alternative name ( also known as) or infor-
mation that describe or define an object of the class ( is
described as).
Instances and triples
Afterwards, the meta-level ontology development led into
the encoding of instances from the collected VISs. Refer-
ring to Table 1, the People class accounted for most of
instances, with 93 instances of the People classes. Since the
initial set of VIS documents was small, some of the classes
had one instance stemming from its class. There is a pos-
sibility that the remaining VIS documents may add more
instances to these classes.
Earlier, we indicated that across the VIS documents
there exist some consistency that influence the represen-
tation of knowledge in the VISO. This was also reflected
in how the instances are conceptualized. The following
subsections list some noticeable design patterns observed
in the development of VISO - dosage pattern for vaccine
dose representation, target pattern for representing the
object of vaccine prevention, and the reaction pattern for
possible side effects of the vaccine.
Dosage pattern
Every vaccine documentation outlines the number of
doses, and when and who should receive the vaccine.
The three knowledge triples provided in the representa-
tion in Figure 2, where statement 1 is represented as a
ChildDose subclass, and their corresponding dose order
(statements 2 and 3) are depicted in subclasses of Age for
the the time of the vaccination. Most of the dose informa-
tion found in the VIS documents are modeled similarly in
VISO.
Target pattern
Pertinent information on what a vaccine protects against
is found in the six vaccine statements. Target instances
comprise of measles, tetanus, pneumococcal disease,
rotavirus, pertussis, etc. Figure 3 illustrates a typical
pattern forTarget instances. In the figure, the 9 statements
listed aremapped in accordance to the VISOmodel. State-
ment 1 reveals the type of Target (Serious Disease) that
diphtheria is. Statement 2 is abstracted with a Source
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 6 of 12
Table 2 Object properties
Domain Properties Range Triples
Target, Cause, Reaction affects People, Location 11
Dosage after Date 2
PeopleWithAllergicReaction, AllergicReaction allergic to Allergen 7
Dosage before Occurrence 1
PeopleBornFrom born from Date 1
Target causes Cause 40
Vaccine contains Component 3
Vaccine discouraged for People 19
NumberOfDoses for People 5
Cause, Outcome, Reaction happens Probability 42
Vaccine has alternate Alternate Vaccine 4
Vaccine has dosage Dosage 26
Vaccine has number of NumberOfDoses 6
Reaction has signs Sign 14
Vaccine is a substance of Substance 1
Vaccine is safe for People 6
Vaccine is safe with Combination 1
Vaccine is taken Method 1
Target, Reaction, Sign, ObjectChannel lasts Duration 5
Target, Cause, Outcome, Reaction leads Outcome 31
Cause, Reaction located Location 15
Vaccine may cause Reaction 54
Reaction need Action 3
Reaction, Sign occurs after Date, Occurrence 24
PeopleOfAge of age Age 3
Target originates Source 5
Vaccine prevents Target 11
Vaccine protects People 23
Target spreads through Channel 19
People take Action 5
Dosage taken at Age 22
Cause to People 2
HumanActivityChannel with People 2
Total number of triples 414
Table 3 Data properties
Domain Properties Range Triples
Thing also known as string data 7
Thing is described as string data 8
Total number of triples 15
subclass of Bacteria, with a relationship of originates
with theTarget instance. Likewise, statement 3 is shown as
PeopleChannel subclass of Channel. Statements 5 through
9 are facilitated by the Cause class with relationships
to instances of Location and Outcome classes. This is
an example is indicative of how Target class instances
and their commonly related relationships are modeled in
VISO.
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 7 of 12
Figure 2 Dosage modeling example. Example of Dosage instance representation with an excerpt from the MMR VIS.
Reaction pattern
Reaction patterns modeled possible reactions from vacci-
nations described in each of the VIS documents. Figure 4
shows a model of three sample statements. Statement 1
shown as an instance of Vaccine may cause MildReac-
tion triple. Statement 2 alludes to the left branch with an
instance of MildReaction located at Location, and state-
ment 3 represented in the right branch of a Probability
subclass of ProbabilityInPatients.
The patterns described in this section are the most
evident types in the mapping of the information, but addi-
tional vaccine information design patterns also exist. It is
also understood that more patterns may emerge with the
inclusion of the remaining VIS documents.
Quality evaluation
To evaluate the overall quality of VISO, the ontology
was scored using the ontology metrics suite proposed
by Burton-Jones, et al. [25]. Inspired by semiotics con-
cepts, this simple, yet extensive ontology scoring metric
evaluates an ontology based on four criteria - seman-
tic, syntactic, social, and pragmatic levels. The semantic
criteria evaluates the term meanings and word sense of
each of the terms. The syntactic criteria assesses syn-
tax of the ontology, while social criteria examines how
other ontologist use the ontology through links. The prag-
matic criteria evaluates the practicality of the ontology,
relating to its usage and its construction. Also, the evalu-
ation metric is flexible to allow individual tailoring if any
of the criteria does not apply, as demonstrated in refer-
ence [25]. The resulting final score is a value between 0
and 1.
Because of the initial design of the ontology, the scoring
metric needed to be tailored to provide a progress indica-
tor of its development. For example, social criteria which
is dependent on other ontologies linking to VISO was not
Figure 3 Target modeling example. Example of Target instance representation with an excerpt from the DTaP VIS.
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 8 of 12
Figure 4 Reaction modeling example. Example of Reaction instance representation with an excerpt from the Hepatitis B VIS.
possible to determine, therefore, it was excluded from the
complete calculation score.
However, the attainment of the other scores were
straightforward. The semantic quality score (E in
equation (1)) was attained by a normalized summation
of Interperatability (w1 as terms with a word sense in
WordNet/TC as total terms used in the ontology), Con-
sistency (misused terms as i/total classes, properties, and
instances as C), and Clarity (average number of word
senses for each term, w2/total classes, properties, and
instances, C). For w1 (Interperatability) and w2 (Clar-
ity), a simple Java GUI-based application utilizing the
ws4j library for WordNet [26] and the OWL API libray
[27] was developed to automate and retrieve the values
to calculate semantic scores. Equation (1) outlines the
semantic quality calculation.
E =
(1
3 ?
w1
TC
)
+
(1
3 ?
i
C
)
+
(1
3 ?
w2
C
)
(1)
The pragmatic score, P, was determined by the normal-
ized summation of Comprehensiveness (C/500 as total
number of classes and properties over 500, the maxi-
mumnumber for classes and properties for the calculation
[25]), and Accuracy (false statements as f /total number
of statements from the ontology as TS). Relevance scor-
ing (application of statements/total number of statements)
involves ontology usage in an application, which was omit-
ted. For Accuracy, vaccine subject matter experts were
given a list of statements from the ontology and were
asked to label whether the statement was true or false and
to provide any corrections.While the terminology and the
knowledge source was primarily from the VIS, our vac-
cine experts also interact directly with patients and their
concerns relating to vaccines which helped to focus on
the patient-centric goal of the VIS. Below, equation (2)
describes the calculation of P.
P =
(1
2 ?
C
500
)
+
(1
2 ?
f
TS
)
(2)
For syntactic quality, S, the Protégé editor provided
some of the values for the scores. Both the Lawful-
ness, number of syntactic violations over total number of
statements in the ontology (l/TS) and Richness, num-
ber of syntactic elements utlized over the total syntactic
elements (s/SF) accounted for the normalized sum of syn-
tactic quality. Equation (3) shows the syntactic quality
calculation.
S =
(1
2 ?
l
TS
)
+
(1
2 ?
s
SF
)
(3)
While the metric evaluation can be tailored specific to
the ontology, aspects like the social criteria and Rele-
vance were excluded. The modified final score is repre-
sented in equation (4), whereQ is the overall quality score.
Q =
(1
3 ? E
)
+
(1
3 ? P
)
+
(1
3 ? S
)
(4)
Results and discussion
Results
The VISOwas serialized in OWL2 with Protégé. Themet-
rics data collected from Protégé is shown in Table 4. The
first column displays the item and the second column
displays the value associated. The current VISO model
produced a class count of 132, including subclasses; 33
object properties; 2 data properties; and 419 unique indi-
vidual instances. In addition, 460 instances derived from
concepts were created, along with a total of 429 knowledge
triples.
Table 4 VISOmetrics data
Class count 132
Object property count 33
Data property count 2
Unique individual count 419
Instances asserted from classes 460
Total knowledge triples 429
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 9 of 12
As stated earlier, part of the process in engineering the
abstraction of the VIS involved obtaining passages to be
mapped into the model. Table 5 displays the number of
passages parsed for evaluation under the Total Passages
column. Also, the third column recorded the number of
instance triples produced. For all six of the VIS document
sources, a total of 244 sentences and passages were used,
and 427 instance triples were extrapolated from them, and
later merged and coded with Protégé to VISO.
Table 2 indicate the number of knowledge triples
present in the current VISO version. Most of the triples
were mainly of the Vaccine may cause Reaction type,
with 54 instances of that triple. Total number of instances
of triples in VISO numbered at 429 (derived from Tables 2
and 3), that includes the 15 instances of triples denoted by
data property relationships from the table in Table 3.
Both the HermiT (version 1.3.8) and the FaCT+ reason-
ers with the Protégé editor did not reveal any discrepan-
cies with this version of the model.
The final quality score for evaluation, Q, based on the
tailored equation (4) calculated to 0.54. Observing the
other components of the quality score, the semantic qual-
ity, E, amounted to 0.59. Pragmatic quality and syntactic
quality equated to 0.75 (P) and 0.27 (S), respectively.
Result analysis
While the construction of VISO is in the early stages and
covering 6 VISs, the quality score appears to reflect the
early developmental state of the ontology. However, look-
ing at the decomposition of the score can reveal some
insight to VISO and possibly inform the future direction
of the VISO development.
Observing specifics of the semantic quality, the Clarity
component score (0.89) reveals less ambiguity among the
terms. The Interpertability value (0.74) also indicates
the use of meaningful terms in the ontology, and the
inconsistency of the terms was low (0.04).
With pragmatic quality (0.75) of VISO, we ignored the
Relevance quality due to unavailability for deployment in
an application environment. Comprehensiveness quality
(1.2) was exceedingly high due to being a large ontology
with many classes and properties. Based on reports from
Table 5 Extraction Results
CDC VIS Total passages Instances of triples in VISO
Hepatitis B 45 76
Rotavirus 49 64
Hib 41 43
PCV13 32 50
DTaP 51 112
MMR 26 82
Total 244 427
two expert reviewers, VISOs inaccuracy was relatively
low at 0.30.
Syntactic quality revealed no violations with syntax,
however, the Richness quality computed at 0.54, reflect-
ing that VISOwas utilizing a little over half of the syntactic
features available.
Based on the results, the VISOs strength lies in its prag-
matic quality which indicates its overall usability based
on the two of the three components described earlier -
Accuracy and Comprehensiveness. The excluded com-
ponent, Relevance, could provide a more holistic score
in the future. Syntactic outcome was the weakest of the
three aspects of VISOs quality. This is partially due to
the minimal usage of ontology features available. Focus on
this aspect will warrant attention in future development of
VISO.
Relating to Consistency, some terms, according to
reviewers, were improperly used or required nuanced
descriptions. One example is the term Dosage which is
technically used to describemeasurement of a vaccine and
not a synonym for dose. Another example are property
labels like cause. Some instances in the ontology should
have been labeled may cause to imply a possible causa-
tion, rather than an expected outcome. Proper and precise
term usage will be another focus that the next version of
VISO will rectify.
Discussion
Modeling the CDCs VISs posed several challenges. One
challenge was determining relevant pieces in the corpus
that could be used for knowledge extraction. Most of
the documents had statements that were either repetitive,
or had literary flourishes that were deemed unnecessary.
Also, the documents may have a paragraph or a sentence
that summarizes preceding information with granular-
ity. In most cases, these were viewed as repetitive, yet
may serve a future purpose if the ontology were to be
used to construct dialogue with patients. Additionally, the
documentation comprised of some knowledge that were
historical statements. An example from the rotavirus VIS:
Because children are protected by the vaccine,
hospitalizations, and emergency visits for rotavirus
have dropped dramatically. [18]
It is debatable whether the historical informationmay be
useful to patients, or if summarized statements, which are
naturally repetitious in these documents, could be inte-
grated into the VISOmodel. In this initial version of VISO
the repetitive and historical texts were not mapped but
may be considered in future versions of the ontology.
Another challenge were gaps in the knowledge where
the information was incomplete, brief, or needed med-
ical understanding beyond the lay person. Some of the
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 10 of 12
language in the documents may not be readily evident to
a parent or patient, and would require a medical profes-
sional to provide interpretation. For example, if a certain
vaccine should not be given to a child who is moderately
ill, how does the reader of the VIS determine the exact
signs of a moderately ill child as opposed to a child who
may be mildly ill? If the documents refer to a sign as
physical weakness, how does the patient or the reader
determine features or indicators of physical weakness?
Issues like these limited the scope of the VISO knowledge-
base. In practice, the healthcare provider, rather than the
parent, will determine if the child is too ill to receive the
vaccine. It is the medical professionals responsibility to
provide judgment and guidance to the patient. Similarly,
there were issues with limited number of VIS documents
to develop the VISO models and to create instances. Lack
of additional information resulted in some of the classes
not having any subclasses to suggest.
Expressing the knowledge contained within the VIS
documents posed some challenges as well. While the
meta-level definition was designed with subclasses to han-
dle descriptive instances, there are often passages with
complex nouns and adjectives where each word carried
important meaning for the instance. Examples such as
painful tightening of muscles or difficult for infants to
breathe posed a predicament of whether these instances
should be decomposed to additional classes and rela-
tionships; use a subclass or create a new subclass; apply
polymorphism; or keep as is as an instance. In most cases,
they were realized as a single instance, until the meta-
level model is further developed to map difficult passages.
Moreover, given the historical nature of the VISs and con-
sidering that several VISs were originally developed more
than 30 years ago in response to NCVIA, it is assumed
that subsequent versions contain much of the original
language. This likely contributes to the variability in the
semantic language of the ontology.
Conclusion and future direction
We introduce the Vaccine Information Statement Ontol-
ogy, which could positively influence the development of
intelligent ontology-drive applications and mitigate the
knowledge gap that often exists in patients seeking accu-
rate and reliable information but encountering complex or
inaccurate sources. Possible future goals in continuing the
development of the VISO include:
 Expand the Vaccine Information Statement Domain.
This version of the VISO models 6 VIS documents as
an initial iteration to examine the ontology. The next
few iterations of VISO will include more domain
knowledge from the remaining 19 VIS documents,
available from the CDC website. There is also an
awareness that additional knowledge can be modeled
from outside the CDCs VIS documents. It is also
assumed that the meta-level design will mature as we
realize alternative interpretation of vaccine
information, or discover abstractions that could
integrate some of the ignored passages and phrase,
like negation statements or summary passages. With
an expanded version and throughout the
development cycle, we plan to evaluate the ontology
using semiotic-based metric suite, and also adapt the
suite to also include aspects previously excluded, like
social quality or Relevance.
 Link VISO with existing relevant ontologies to
expand the knowledge domain. Some third party
ontologies that could be aligned with the VISO may
include the Vaccine Ontology (VO) [28], Ontology of
Vaccine Adverse Effect (OVAE) [29] or Medical
Dictionary for Regulatory Activities (MedDRA) [30]
to address reaction or conditions in the VISO. This
approach would comprise of code-linking particular
classes with matching classes in the VISO model,
which would lead into providing a comprehensive
and expanded knowledge-base for patient learning.
However, because the knowledge-base is intended for
patient use, it will be essential to determine the
appropriate since the aforementioned third party
ontologies utilize professional vocabulary which may
not be understood by patients.
 Integrating patient-level synonymous terms and
multi-language equivalents into the lexicon. A
multi-lingual VISO would presumably expand to
reach potential patients who may be excluded
because of language or socio-economic barriers.
There is also an interest in using the Open Access,
Collaborative Consumer Health Vocabulary Initiative
[31] as a resource to integrate consumer-level terms
or synonyms.
 Applying natural language processing. We intend to
explore the possibility of applying natural language
processing (NLP) for both ontology learning and
knowledge retrieval. We will implement NLP
methods to facilitate automatic knowledge extraction
to expand the VISO. This will also provide intrinsic
value for applications using natural language
processing and dialogue systems.
 Intelligent mobile agents. Ontology-driven
applications could introduce the potential for
intelligent agents for learning. Realistic and engaging
agents are proven to be more effective for increasing
involvement and learning [32-35], persuasion [36],
and trustworthiness [37,38]. It provides a
cost-effective way to address patients concerns and
answer their questions about vaccination, which
otherwise requires healthcare professionals to
address in person. This will presumably improve the
Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 11 of 12
efficiency of healthcare delivery workflows and
reduce the cost. They also provide flexibilities for
vaccine education. Patients or parents can spend as
much time as they need with intelligent mobile
devices, and interact with intelligent devices anytime
they have access to a computer or tablet. Last but not
least, personalized agents can be automatically built
according to users preference to improve the
usability and acceptability of the system.
Examples like VAMATA, which is an
ontology-driven mobile application with a speech
interface designed for combat medical settings, reveal
applicable synergy between natural language
processing and ontology in mobile applications [39].
We have previously developed a proof-of-concept
ontology-driven mobile application with a natural lan-
guage interface to query a VISO knowledge-base [12]. The
ongoing evolution of VISO will assist in the continuing
development of that project.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MA wrote the initial draft and revised subsequent draft, developed the
ontology, and preformed the evaluation, collection of the metrics, and analysis
of the results. YG revised and approved early and final versions of the drafts
and contributed to review of the ontology. RC revised the draft and approved
final draft, contributed to the review and evaluation of the ontology. JB
contributed to the revision and approval of the final draft, contributed to the
review and evaluation of the ontology . CT was Principal Investigator for this
project, contributed to research design, provided substantive intellectual
contributions, and feedback on the manuscript. All authors read and approved
the final manuscript.
Acknowledgements
This project is partially supported by the National Library Of Medicine of the
National Institutes of Health under Award Number R01LM011829.
Author details
1School of Biomedical Informatics, University of Texas Health Science Center,
7000 Fannin St, 77030 Houston, TX, USA. 2Immunization Project, Texas
Childrens Hospital, 1102 Bates, 77030 Houston, TX, USA.
Received: 9 December 2014 Accepted: 28 March 2015
Published: 1 May 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Hastings et al. Journal of Biomedical Semantics  (2015) 6:10 
DOI 10.1186/s13326-015-0005-5
RESEARCH Open Access
eNanoMapper: harnessing ontologies to
enable data integration for nanomaterial risk
assessment
Janna Hastings1*, Nina Jeliazkova2, Gareth Owen1, Georgia Tsiliki3, Cristian R Munteanu4,5,
Christoph Steinbeck1 and Egon Willighagen5
Abstract
Engineered nanomaterials (ENMs) are being developed to meet specific application needs in diverse domains across
the engineering and biomedical sciences (e.g. drug delivery). However, accompanying the exciting proliferation of
novel nanomaterials is a challenging race to understand and predict their possibly detrimental effects on human
health and the environment. The eNanoMapper project (www.enanomapper.net) is creating a pan-European
computational infrastructure for toxicological data management for ENMs, based on semantic web standards and
ontologies. Here, we describe the development of the eNanoMapper ontology based on adopting and extending
existing ontologies of relevance for the nanosafety domain. The resulting eNanoMapper ontology is available at http://
purl.enanomapper.net/onto/enanomapper.owl. We aim to make the re-use of external ontology content seamless
and thus we have developed a library to automate the extraction of subsets of ontology content and the assembly of
the subsets into an integrated whole. The library is available (open source) at http://github.com/enanomapper/
slimmer/. Finally, we give a comprehensive survey of the domain content and identify gap areas. ENM safety is at the
boundary between engineering and the life sciences, and at the boundary between molecular granularity and bulk
granularity. This creates challenges for the definition of key entities in the domain, which we also discuss.
Keywords: Nanomaterial, Safety, Ontology
Background
Nanomaterials are materials in which the individual com-
ponents are sized roughly in the 1-100 nanometer range
in at least one dimension, although an exact definition is
still being debated [1,2]. Particles in this size range dis-
play special properties having to do with their very large
ratio of surface area to volume [3]. Natural nanomate-
rials include viral capsids and spider silk. Recent years
have seen an explosion in the development of engineered
nanomaterials (ENMs) aiming to exploit the special prop-
erties of these materials in various domains including
biomedicine (e.g. as vehicles for drug delivery), optics and
electronics [3].
*Correspondence: hastings@ebi.ac.uk
1European Molecular Biology Laboratory  European Bioinformatics Institute
(EMBL-EBI), Cambridge, United Kingdom
Full list of author information is available at the end of the article
Counterbalancing the many possible benefits of devel-
oped nanotechnology, nanoparticles also pose serious
risks to human and environmental health [4]. Recognising
these dangers, regulatory bodies are calling for system-
atic and thorough toxicological and safety investigations
into ENMs with the objective of feeding knowledge into
predictive tools which are able to assist researchers in
designing safe nanomaterials. Evaluating and predicting
the possible dangers of different nanomaterials requires
assembling a wealth of information on those materials 
the composition, shape and properties of the individual
nanoparticles, their interactions with biological systems
across different tissues and species, and their diffusion
behaviour into the natural environment. These data are
arising from different disciplines with highly heteroge-
neous requirements, methods, labelling and reporting
practices. Regulatory descriptions of ENMs are not like
those needed for nanoQSAR analyses. Safety require-
ments may also vary under different conditions, e.g. when
© 2015 Hastings et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Hastings et al. Journal of Biomedical Semantics  (2015) 6:10 Page 2 of 15
developing vehicles for drug delivery in life-threatening
diseases as compared to materials for use in the construc-
tion industry.
The eNanoMapper project (www.enanomapper.net) is
creating a pan-European computational infrastructure
for toxicological data management for ENMs, based on
semantic web standards and ontologies. eNanoMapper
aims to develop a comprehensive ontology and annotated
database for the nanosafety domain to address the chal-
lenge of supporting the unified annotation of nanomateri-
als and their relevant biological properties, experimental
model systems (e.g. cell lines), conditions, protocols, and
data about their environmental impact. Rather than start-
ing afresh, the developing ontology will build on existing
work, integrating existing ontologies in a flexible pipeline.
The establishment of a universal standardisation schema
and infrastructure for nanomaterials safety assessment
is a key project goal, which will support collaboration,
integrated analysis, and discoveries from data organised
within a knowledge-based framework.
In this paper, we survey the existing ontologies that were
integrated into the unified eNanoMapper ontology, focus-
ing on the challenges we experienced with the integration
of diverse sources and our automated solution for seam-
less modular re-use of external content. Furthermore,
we discuss challenges in the definition of key entities in
the domain and give harmonised definitions for the core
material and experimental entities in the domain.
Problem
The eNanoMapper ontology covers the following broad
content areas:
1. A categorisation of nanoparticle classes based on
their properties, constituency and shape.
2. Physicochemical properties for ENM
characterisation.
3. Biological characterisation that describes the
ENM-specific interactions with, for example,
proteins to form a corona.
4. Environmental characterisation.
5. Experimental design and encoding for experiments
in which nanosafety is assessed.
6. The full nanomaterial lifecycle including
manufacturing and environmental decay or
accumulation.
7. Known safety information about ENMs.
Table 1 gives a summary of the ontologies that have been
identified as already in part covering these domain areas.
The selection of ontologies is motivated by the require-
ment that the ontologies be (a) open, that is, licensed for
re-use without restriction other than attribution, (b) suit-
able for use in data annotation, i.e. using unique numeric
identifiers and offering textual labels and definitions, and
(c) be broadly mutually compatible (although with some
provisos as we will discuss in the section on our re-use
pipeline below).
These ontologies are described further in the Results
section below. However, our initial naïve attempt to re-
use these ontologies in their entirety in the development
of the integrated eNanoMapper ontology ran into several
challenges:
 Some content was duplicated across multiple
different ontologies, resulting in multiple classes with
different identifiers being included with the same
label  including cases where classes with the same
label were defined differently across the ontologies;
 Some classes which were multiply imported, i.e.
following the recommended re-use policy, in the
ontologies we imported, such as frequently used
upper-level classes or unit classes, subsequently were
found to have multiple copies of all associated
annotations and axioms in the resulting composite
ontology;
 It was difficult to reconcile the different upper level
ontologies that were used in these ontologies, and in
some cases even when the same upper level ontology
was used (BFO), different versions of that upper level
still caused incompatibilities;
 The presence of gaps in the imported content
necessitates the manual annotation of content
additions (which may later be submitted to various
source ontologies). It was not easy to seamlessly add
manual content to the imported ontologies without
needing to re-create the manual content every time
the source ontologies changed and were re-imported;
and
 Some of the definitions of the classes we wanted
to re-use were missing or not sufficiently clear
for unambiguous re-use as part of an integrated
whole.
Based on exact label matching only, the overlap between
the ChEBI ontology and theNPO is 395. This is a small but
nevertheless significant number of exactly shared labels.
Most of these are groups, atoms or chemical classes that
are included in NPO so as to support description of nano-
material composition. Some, but not all, of these are cross-
referenced to ChEBI via an additional annotation dbXref 
in NPO. Other classes with overlapping labels derive from
the fledgling nanoparticle classification that is included
in ChEBI. For this branch of NPO, there are no cross-
JOURNAL OF
BIOMEDICAL SEMANTICS
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 
DOI 10.1186/s13326-015-0010-8RESEARCH ARTICLE Open AccessDevelopment and validation of a classification
approach for extracting severity automatically
from electronic health records
Mary Regina Boland1,2*, Nicholas P Tatonetti1,2,3,4 and George Hripcsak1,2Abstract
Background: Electronic Health Records (EHRs) contain a wealth of information useful for studying clinical
phenotype-genotype relationships. Severity is important for distinguishing among phenotypes; however other
severity indices classify patient-level severity (e.g., mild vs. acute dermatitis) rather than phenotype-level severity (e.g.,
acne vs. myocardial infarction). Phenotype-level severity is independent of the individual patients state and is relative
to other phenotypes. Further, phenotype-level severity does not change based on the individual patient. For example,
acne is mild at the phenotype-level and relative to other phenotypes. Therefore, a given patient may have a severe
form of acne (this is the patient-level severity), but this does not effect its overall designation as a mild phenotype at
the phenotype-level.
Methods: We present a method for classifying severity at the phenotype-level that uses the Systemized Nomenclature
of Medicine  Clinical Terms. Our method is called the Classification Approach for Extracting Severity Automatically
from Electronic Health Records (CAESAR). CAESAR combines multiple severity measures  number of comorbidities,
medications, procedures, cost, treatment time, and a proportional index term. CAESAR employs a random forest
algorithm and these severity measures to discriminate between severe and mild phenotypes.
Results: Using a random forest algorithm and these severity measures as input, CAESAR differentiates between
severe and mild phenotypes (sensitivity = 91.67, specificity = 77.78) when compared to a manually evaluated
reference standard (k = 0.716).
Conclusions: CAESAR enables researchers to measure phenotype severity from EHRs to identify phenotypes that
are important for comparative effectiveness research.
Keywords: Electronic Health Records, Phenotype, Health status indicators, Data mining, Outcome assessment
(Health Care)Background
Recently, the Institute of Medicine has stressed the im-
portance of Comparative Effectiveness Research (CER)
in informing physician decision-making [1]. As a result,
many national and international organizations were formed
to study clinically meaningful Health Outcomes of Interest
(HOIs). This included the Observational Medical Out-
comes Partnership (OMOP), which standardized HOI* Correspondence: mb3402@columbia.edu
1Department of Biomedical Informatics, Columbia University, New York, NY,
USA
2Observational Health Data Sciences and Informatics (OHDSI), Columbia
University, 622 West 168th Street, PH-20, New York, NY, USA
Full list of author information is available at the end of the article
© 2015 Boland et al.; licensee BioMed Central.
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.identification and extraction from electronic data sources
for fewer than 50 phenotypes [2]. The Electronic Medical
Records and Genomics Network (eMERGE) [3] also
classified some 20 phenotypes, which were used to
perform Phenome-Wide Association Studies (PheWAS)
[4]. However, a short list of phenotypes of interest re-
mains lacking in part because of complexity in defining
the term phenotype for use in Electronic Health Records
(EHRs) and genetics [5].
EHRs contain a wealth of information for studying
phenotypes including longitudinal health information
from millions of patients. Extracting phenotypes from
EHRs involves many EHR-specific complexities includingThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 2 of 13data sparseness, low data quality [6], bias [7], and health-
care process effects [8].
Many machine-learning techniques that correlate EHR
phenotypes with genotypes encounter large false positive
rates [3]. Multiple hypothesis correction methods aim to
reduce the false positive rate. However, these methods
strongly penalize for a large phenotype selection space.
A method is needed that efficiently reduces the pheno-
type selection space to only include important pheno-
types. This would reduce the number of false positives
in our results and allow us to prioritize phenotypes for
CER and rank them by severity.
To extract phenotypes from EHRs, a specialized ontol-
ogy or terminology is needed that describes phenotypes,
their subtypes and the various relationships between
phenotypes. Several ontologies/terminologies have been
developed for studying human phenotypes including
the Human Phenotype Ontology (HPO) [9]. The HPO
contains phenotypes with at least some hereditary com-
ponent, e.g., Gaucher disease. However, EHRs contain
phenotypes that are recorded during the clinical encoun-
ter that are not necessarily hereditary. To capture a pa-
tients phenotype from EHRs, we will utilize an ontology
specifically designed for phenotype representation in
EHRs called the Systemized Nomenclature of Medicine 
Clinical Terms (SNOMED-CT) [10,11]. SNOMED-CT
captures phenotypes from EHRs, including injuries that
are not included in the HPO. Furthermore, SNOMED-CT
can be used to capture more clinical content then Inter-
national Classification of Diseases, version 9 (ICD-9) codes
[12], making SNOMED-CT ideal for phenotype classifica-
tion. Using SNOMED-CT enables development of a stan-
dardized approach that conforms to OMOPs guidelines
promoting data reuse.
Robust methods are needed that address these
challenges and reuse existing standards to support data
sharing across institutions. This would propel our un-
derstanding of phenotypes and allow for robust CER to
improve clinical care. This would also help pave the way
for truly translational discoveries and allow genotype-
phenotype associations to be explored for clinically im-
portant phenotypes of interest [13].
An important component when studying phenotypes
is phenotype severity. Green et al. demonstrate that a
patients disease severity at hospital admission was
crucial [14] when analyzing phenotype severity at the
patient-level. We are interested in classifying phenotypes
as either severe or mild at the phenotype-level, which
differs from the vast literature on patient-specific sever-
ity. Classifying severity at the phenotype-level involves
distinguishing acne as a mild condition from myocardial
infarction as a severe condition. Contrastingly, patient-
level severity assesses whether a given patient has a mild
or severe form of a phenotype (e.g., acne). Importantly,phenotype-level severity is independent of the individual
patients state and is relative to other phenotypes (e.g.,
acne vs. myocardial infarction). Further, phenotype-level
severity does not change based on the individual patient.
For example, acne is mild at the phenotype-level, which
is relative to other phenotypes. Therefore, a given pa-
tient may have a severe form of acne (i.e., patient-level
severity = severe), but the overall phenotype-level sever-
ity is mild because phenotype-level severity is relative to
other phenotypes and does not change based on an indi-
vidual patients patient-level severity.
Studying phenotype severity is complex. The plethora
of medical conditions is mirrored by an equally diverse
set of severity indices that run the full range of medical
condition complexity. For example, there is a severity
index specifically designed for nail psoriasis [15], insomnia
[16], addiction [17], and even fecal incontinence [18].
However, each of these indices focuses on classifying pa-
tients as being either a severe or mild case of a given con-
dition (e.g., psoriasis). They do not capture the difference
at the phenotype-level.
Other researchers developed methods to study patient-
specific phenotype severity at the organismal level. For
example, the Severity of Illness Index assesses patient
health using seven separate dimensions [19] consisting of:
1) the stage of the principal diagnosis at time of admission;
2) complications; 3) interactions (i.e., the number of pa-
tient comorbidities that are unrelated to the principal
diagnosis); 4) dependency (i.e., the amount of care re-
quired that is above the ordinary); 5) non-operating room
procedures (i.e., the type and number of procedures per-
formed); 6) rate of response to therapy; and 7) remission
of acute symptoms directly related to admission.
The Severity of Illness Index is useful for characterizing
patients as severe or mild types of a given disease pheno-
type. However, it does not measure severity at the
phenotype-level (e.g., acne vs. myocardial infarction),
which is required to reduce the phenotype selection space
to only the most severe phenotypes for CER.
In this paper, we describe the development and valid-
ation of a Classification Approach for Extracting Severity
Automatically from Electronic Health Records (CAESAR).
CAESAR incorporates the spirit of the Severity of Illness
Index, but measures phenotype-level severity rather than
patient-level severity. CAESAR was designed specifically
for use with EHR-derived phenotypes.
Methods
Measuring severity
We used five EHR-specific measures of condition severity
that are related to the 7 dimensions from Horns patient-
level severity index [19] because EHRs differ from research
databases [20]. Columbia University Medical Centers
(CUMC) Institutional Review Board approved this study.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 3 of 13Condition treatment time can be indicative of severity
and so it was included as a severity measure. Treatment
time is particularly indicative of severity for acute condi-
tions, e.g., fractures, wounds or burns, because minor
(less severe) fractures often heal more rapidly than major
fractures (more severe). However, treatment time is also
dependent on the chronicity of the disease [21], which is
separate from severity. Treatment time can also have
other effects when recorded in EHRs [22-24].
Because hospital duration time can be influenced by
many factors, e.g., patients other comorbidities, we de-
cided to analyze the condition treatment time. While
inter-dependent, hospital duration time is typically a
subset of the entire condition treatment time (which can
include multiple hospital visits).
Number of comorbidities is another useful measure for
assessing phenotype severity. A similar measure is found
in the Severity of Illness Index that measures the num-
ber of other conditions or problems that a given patientFigure 1 Example showing differences between ehr manifestations of s
Phenotype-level differences between severe and mild phenotypes are shown
phenotypes if you only look at the number of procedures, comorbidities or p
alone to identify severity, it would be difficult. However, if cost is used as a pr
infarction is more severe than acne and also costs more). But if you use the tr
severity will result (acne takes longer to treat as a result of chronicity, and the
severity). This underscores the importance of using multiple measures togethhas at the time of their principal diagnosis. Our EHR-
specific version looks at the number of distinct comor-
bidities per patient with a given phenotype and then
averages across all of the individuals in the database with
that phenotype. This average tells us the comorbidity
burden associated with a given phenotype. An example
is given in Figure 1 to illustrate how the number of
comorbidities, medications, and treatment time can dif-
fer by phenotype severity. Note that acne is an atypical
mild phenotype as its treatment time is longer than
myocardial infarction while most mild phenotypes have
shorter treatment times. Importantly, chronicity also
affects treatment time, which can negate the effect that
severity has on treatment time (Figure 1).
Number of medications is another useful measure for
assessing severity. This measure is related to the previ-
ous measure (i.e., the number of comorbidities). How-
ever, it differs because some phenotypes have a large
number of medications, but also a small number ofevere (Myocardial Infarction or MI) and mild (Acne) phenotypes.
in Figure 1. Notice that there is very little difference between the two
rescribed medications. Therefore, if you use any of those three measures
oxy for severity then the correct classification would be made (myocardial
eatment length then an incorrect classification of the phenotype-level
refore longer treatment length is not equal to increased phenotype-level
er as a proxy for severity, which is the approach employed by CAESAR.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 4 of 13comorbidities, e.g., burn injuries. Therefore, in many
cases these measures will be similar but in other import-
ant instances they will differ.
Number of procedures is also based on a measure from
the Severity of Illness Index. Because we are focused on
phenotype-level severity, we computed an average num-
ber of procedures associated with each phenotype. First,
we extracted the number of procedures performed per
phenotype and per patient. Then we computed the aver-
age across all patients in our database yielding the aver-
age number of procedures per phenotype.
Cost to treat phenotype is a commonly used metric for
assessing severity [25]. The Centers for Medicare and
Medicaid Services released the billable rate for each pro-
cedure code per minute [26]. They also released the
number of minutes each procedure typically requires.
Combining these data allows us to calculate the billable
amount for a given procedure [26]. The billable rates are
from 2004 and they are for each Healthcare Common
Procedure Coding System (HCPCS) code [26].
Since these data are only available for procedure codes
(HCPCS codes are procedure codes) we calculated the
total cost per patient using the procedures they were
given. We determined the cost per phenotype by taking
the average cost across all patients with that phenotype.
Measures of phenotype severity and E-PSI (Ehr-phenotype
severity index)
We first calculated the proportion of each measure. The
sum of the proportions (there are five proportions  one
for each measure) was divided by the total number of
proportions (i.e., five). This final value is E-PSI, an index
term based on all 5 measures given in Equation 1 where
x is a phenotype. Therefore, E-PSI is a proportional
index that incorporates treatment time, cost, number of
medications, procedures, and comorbidities.
Equation 1:
E-PSI (Phenotype x)
¼ xcost
max costð Þ þ
xtreatment length
max treatment lengthð Þ þ
xcomorbidities
max comorbiditiesð Þ
þ xmedications
max medicationsð Þ þ
xprocedures
max proceduresð Þ
For example the treatment time of Hemoglobin SS
disease with crisis is 1406 days. We divide this by the
max treatment length of any phenotype, which is also
1406 days. This gives us the proportional treatment
length of the disease or 1.00. Likewise, proportions are
calculated for each of the five measures. The sum of the
proportions is divided by the total number of propor-
tions, or 5. This is E-PSI, the proportional index, for the
phenotype.
We used Independent Components Analysis (ICA)
[27] to visualize the relationship between E-PSI and eachphenotype severity measure. Computations were per-
formed in R (v.3.1.1).
Reference standard development and evaluation
Development of the Reference Standard involved using
the CUMC Clinical Data Warehouse that was trans-
formed to the Clinical Data Model (CDM) outlined by
the OMOP consortium [2]. All low prevalence phenotypes
were removed, leaving behind a set of 4,683 phenotypes
(prevalence of at least 0.0001). Because we are studying
phenotypes manifested during the clinical encounter, we
treat each distinct SNOMED-CT code as a unique
phenotype. This was done because each SNOMED-CT
code indicates a unique aspect of the patient state [28].
To compare results between mild and severe pheno-
types, we required a reference-standard set of SNOMED-
CT codes that were labeled as mild and severe. In
addition, the set must be un-biased towards a particular
clinical subfield (e.g., oncology or nephrology). Therefore,
we developed a reference-standard set of 516 phenotypes
(out of the 4,683 phenotype super-set) using a set of heu-
ristics. All malignant cancers and accidents were labeled
as severe; all ulcers were labeled as mild; all carcin-
omas in situ were labeled as mild; and most labor and
delivery-related phenotypes were labeled as mild. Since
the reference standard was created manually, the final
judgment was left to the ontology expert regarding label-
ing a given phenotype as mild or severe. However, the
ontology expert consulted with medical experts to reduce
ambiguity.
Evaluation of the Reference Standard required soliciting
volunteers to manually evaluate a subset of the reference
standard (N = 7). Half of the evaluators held a Medical
Degree (MD) (N = 3) and completed residency while the
other half were graduate students with informatics train-
ing (N = 3) and one post-doctoral scientist. We asked
each evaluator to assign phenotypes as either mild or se-
vere. We provided each evaluator with instructions for
distinguishing between mild and severe phenotypes. For
example, severe conditions are conditions that are life-
threatening (e.g., stroke is immediately life-threatening) or
permanently disabling (congenital conditions are generally
considered severe unless they are easily corrected). Mild
conditions may still require treatment (e.g., benign neo-
plasms and cysts are generally considered mild and not
severe as they may not require surgery). To ascertain the
confidence that each evaluator had in making their sever-
ity assessments, we asked evaluators to denote their confi-
dence in each severity assignment using a modified Likert
scale [29] with the following 3 choices: very confident,
somewhat confident and not confident. All evaluators
were provided with two coded examples and 100 ran-
domly extracted phenotypes (from the reference stand-
ard). This evaluation set of 100 phenotypes contained 50
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 5 of 13mild and 50 severe (labels from the reference-standard).
Pair-wise agreement between each evaluator and the
reference-standard was calculated using Cohens kappa
[30,31]. Inter-rater agreement among all evaluators
and the reference standard was calculated using Fleisss
kappa [32,33].
Evaluation of Measures at Capturing Severity involved
comparing results from mild and severe phenotypes
for each severity measure. Severity measures were not
normally distributed so non-parametric measures (i.e.,
quartiles) were used for comparisons.
Learning phenotype-level severity classes
Development of the random forest classifier
CAESAR involved the unsupervised learning of classes
by calculating a proximity matrix [34]. The scaled 1-
proximity for each data point (in this case a phenotype)
was plotted [34]. The reference standard result was then
overlaid on top to determine if there was any significant
clustering based on a phenotypes class (in this case
severe or mild). Clusters of severe and mild phenotypes
can be used to set demarcation points for labeling a
phenotype.
Using the proximity matrix also allows for discrimin-
ation among levels of severity, in addition to the binary
classification of severe vs. mild. We used the random-
Forest package (v.4.6-10) in R (v.3.1.1) for calculations
[35] and we used 1000 trees in our model. The random
forest classifier, or CAESAR, takes all 5 severity mea-
sures and E-PSI (the proportional index term) as input
for the model.
Evaluation of the random forest classifier
CAESAR was evaluated using the 516-phenotype refer-
ence standard. Sensitivity and specificity were used to
assess CAESARs performance. The class errors for se-
vere and mild were measured using the randomForest
package [35] and compared against the out-of-bag
(OOB) error rate. The randomForest algorithm uses the
Gini index to measure node impurity for classification
trees. The Gini impurity measure sums the probability
of an item being chosen times the probability of misclas-
sifying that item. We can assess the importance of each
variable (i.e., the 5 measures and E-PSI) included in
CAESAR by looking at the mean decrease in Gini. Vari-
ables with larger decreases in Gini are more important
to include in CAESAR for accurate prediction.
Results
Assessment of phenotype severity
Severe phenotypes in general are more prevalent in
EHRs because in-patient records contain sicker indi-
viduals when compared to the general population, which
can introduce something called the Berkson bias [36].However, in the general population mild phenotypes are
often more prevalent than severe phenotypes.
For condition/phenotype information we used data
from CUMC EHRs, which was initially recorded using
ICD-9 codes. These ICD-9 codes were mapped to
SNOMED-CT codes using the OMOP CDM v.4 [2]. For
this paper, we used all phenotypes (each phenotype be-
ing a unique SNOMED-CT code) with prevalence of at
least 0.0001 in our hospital database. This constituted
4,683 phenotypes. We then analyzed the distribution of
each of the five measures and E-PSI among the 4,683
phenotypes. Figure 2 shows the correlation matrix
among the 5 severity measures and E-PSI.
Strong correlations exist between both the number of
procedures and the number of medications (r = 0.88),
and the number of comorbidities (r = 0.89). This indi-
cates that there is a high degree of inter-relatedness be-
tween the number of procedures and the other severity
measures. Cost was calculated using HCPCS codes
alone, whereas the number of procedures measure in-
cludes both HCPCS and the ICD-9 procedure codes as
defined in the OMOP CDM. Because cost was calcu-
lated using only HCPCS codes, the correlation between
cost and the number of procedures was only 0.63. Also
phenotype measures were increased for more severe
phenotypes. This could be useful for distinguishing
among subtypes of a given phenotype based on severity.
E-PSI versus other severity measures
We performed ICA on a data frame containing each
of the five severity measures and E-PSI. The result is
shown in Figure 3 with phenotypes colored by increas-
ing E-PSI score and size denoting cost. Notice that
phenotype cost is not directly related to the E-PSI
score. Also phenotypes with higher E-PSI seem to be
more severe (Figure 3). For example, complication of
transplanted heart, a severe phenotype, had a high
E-PSI score (and high cost).
Phenotypes can be ranked differently depending on
the severity measure used. To illustrate this, we ranked
the phenotypes using E-PSI, cost, and treatment length
and extracted the top 10 given in Table 1. When ranked
by E-PSI and cost, transplant complication phenotypes
appeared (4/10 phenotypes), which are generally consid-
ered to be highly severe. However, the top 10 pheno-
types when ranked by treatment time were also highly
severe phenotypes, e.g., Human Immunodeficiency Virus
and sickle cell. An ideal approach, used in CAESAR,
combines multiple severity measures into one classifier.
Complication of transplanted heart appears in the top
10 phenotypes when ranked by all three-severity measures
(italicized in Table 1). This is particularly interesting
because this phenotype is both a complication phenotype
and transplant phenotype. By being a complication the
Condition Length
20 60
r= 0.12
p<0.001
r= 0.17
p<0.001
5 15 25
r= 0.17
p<0.001
r= 0.17
p<0.001
0.2 0.6
0
40
0
10
00
r= 0.40
p<0.001
20
60
No. of Comorbidities
r= 0.75
p<0.001
r= 0.89
p<0.001
r= 0.49
p<0.001
r= 0.88
p<0.001
No. of Medications
r= 0.88
p<0.001
r= 0.69
p<0.001
0
40
80
12
0
r= 0.91
p<0.001
5
15
25
No. of Procedures
r= 0.63
p<0.001
r= 0.94
p<0.001
Cost
0
40
0
80
0
r= 0.71
p<0.001
0 400 1000
0.
2
0.
6
0 40 80 120 0 400 800
E?PSI
Figure 2 Severity measure correlation matrix. Histograms of each severity measure shown (along the diagonal) with pairwise correlation
graphs (lower triangle) and correlation coefficients and p-values (upper triangle). Notice the condition length is the least correlated with the other
measures while number of medications and number of procedures are highly correlated (r = 0.88, p < 0.001).
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 6 of 13phenotype is therefore a severe subtype of another pheno-
type, in this case a heart transplant (which is actually a
procedure). Heart transplants are only performed on sick
patients; therefore this phenotype is always a subtype of
another phenotype (e.g., coronary arteriosclerosis). Hence
complication of transplanted heart is a severe subtype of
multiple phenotypes (e.g., heart transplant, and the pre-
cursor phenotype that necessitated the heart transplant 
coronary arteriosclerosis).Evaluation of severity measures
Development of the Reference Standard severe and mild
SNOMED-CT codes involved using a set of heuristics
with medical guidance. Phenotypes were considered severe
if they were life threatening (e.g., stroke) or permanently
disabling (e.g., spina bifida). In general, congenital pheno-
types were considered severe unless easily correctable.
Phenotypes were considered mild if they generaly require
routine or non-surgical (e.g., throat soreness) treatment.
Figure 3 Independent component analysis of phenotypes illustrates relationship between E-PSI and cost. Independent Component
Analysis was performed using all five severity measures and E-PSI. Phenotypes are colored by increasing E-PSI score (higher score denoted by light
blue, lower score denoted by dark navy). The size indicates cost (large size indicates high cost). Phenotypes with higher E-PSI seem to be more
severe; for example, complication of transplanted heart, a severe phenotype, had a high E-PSI score (and high cost). However, phenotype cost is
not directly related to the E-PSI score.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 7 of 13Several heuristics were used: 1) all benign neoplasms
were labeled as mild; 2) all malignant neoplasms were la-
beled as severe; 3) all ulcers were labeled as mild; 4)
common symptoms and conditions that are generally of
a mild nature (e.g., single live birth, throat soreness,
vomiting) were labeled as mild; 5) phenotypes that were
known to be severe (e.g., myocardial infarction, stroke,
cerebral palsy) were labeled as severe. The ultimate
determination was left to the ontology expert forTable 1 Top 10 phenotypes ranked by severity measure
E-PSI Cost
Complication of transplanted heart Complication of
Transplant follow-up Transplant follo
Posttransplantation lymphoproliferative syndrome Disorder of imm
Complication of transplanted lung Post-transplanta
syndrome
Complication of hemodialysis Anterior horn c
Disorder of immune function Endocrine/meta
Complication of renal dialysis Myocardial deg
Disorder of transplanted bone marrow APL - Acute pro
Arrested development following proteincalorie malnutrition Isolated (Fiedle
Serratia septicaemia Complication odetermining the final classification of severe and mild
phenotypes. The ontology expert consulted with medical
experts when deemed appropriate. The final reference
standard consisted of 516 SNOMED-CT phenotypes (of
the 4,683 phenotypes). In the reference standard, 372
phenotypes were labeled as mild and 144 were labeled as
severe.
Evaluation of the Reference Standard was performed
using volunteers from the Department of BiomedicalTreatment length
transplanted heart Hemoglobin SS disease with crisis
w-up Complication of transplanted heart
une function Hemoglobin SS disease without crisis
tion lymphoproliferative Exstrophy of bladder sequence
ell disease Factor IX deficiency
bolic screening Complication of transplanted kidney
eneration Type II diabetes mellitus - poor control
myelocytic leukaemia Sickle cell-hemoglobin C disease without crisis
rs) myocarditis HIV - Human immunodeficiency virus infection
f transplanted lung Osteoarthritis
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 8 of 13Informatics at CUMC. Seven volunteers evaluated the
reference standard including three MDs with residency
training, three graduate students with informatics ex-
perience and one post-doc (non-MD). Compensation
was commensurate with experience (post-docs received
$15 and graduate students received $10 Starbucks gift
cards).
We excluded two evaluations from our analyses: one
because the evaluator had great difficulty with the med-
ical terminology, and the second because the evaluator
failed to use the drop-down menu provided as part of
the evaluation. We calculated the Fleiss kappa for inter-
rater agreement among the remaining 5 evaluations and
found evaluator agreement was high (k = 0.716). The in-
dividual results for agreement between each evaluator
and the reference standard were kappa equal to 0.66,
0.68, 0.70, 0.74, and 0.80. Overall, evaluator agreement
(k = 0.716) was sufficient for comparing two groups (i.e.,
mild and severe) and 100% agreement was observed be-
tween all five raters and the reference-standard for 77
phenotypes (of 100).
Evaluation of Measures at Capturing Severity was per-
formed by comparing the distributions of all 6 measures
between severe and mild phenotypes in our 516-phenotype
reference standard. Results are shown in Figure 4. Increases
were observed for severe phenotypes across all measures.
We performed the Wilcoxon Rank Sum Test to assess
significance of the differences between severe vs. mild
phenotypes shown in Figure 4. The p-values for each
comparison were <0.001.
Unsupervised learning of severity classes
Development of the random forest classifier
CAESAR used an unsupervised random forest algorithm
(randomForest package in R) that required E-PSI and all
5-severity measures as input. We ran CAESAR on all
4,683 phenotypes and then used the 516-phenotype
reference standard to measure the accuracy of the
classifier.
Evaluation of the random forest classifier
CAESAR achieved a sensitivity = 91.67 and specificity =
77.78 indicating that it was able to discriminate between
severe and mild phenotypes. CAESAR was able to detect
mild phenotypes better than severe phenotypes as shown
in Figure 5.
The Mean Decrease in Gini (MDG) measured the im-
portance of each severity measure in CAESAR. The
most important measure was the number of medications
(MDG = 54.83) followed by E-PSI (MDG = 40.40) and
the number of comorbidities (MDG = 30.92). Cost was
the least important measure (MDG = 24.35).
CAESAR used all 4,683 phenotypes plotted on the
scaled 1-proximity for each phenotype [34] shown inFigure 6 with the reference standard overlaid on top.
Notice that phenotypes cluster by severity class (i.e.,
mild or severe) with a mild space (lower left) and a
severe space (lower right), and phenotypes of inter-
mediate severity in between.
However, three phenotypes are in the mild space
(lower left) of the random forest model (Figure 6). These
phenotypes are allergy to peanuts, suicide-cut/stab, and
motor vehicle traffic accident involving collision be-
tween motor vehicle and animal-drawn vehicle, driver of
motor vehicle injured. These phenotypes are probably
misclassified because they are ambiguous (in the case of
the motor vehicle accident, and the suicide cut/stab) or
because the severity information may be contained in
unstructured EHR data elements (as could be the case
with allergies).
Using the proximity matrix also allows further dis-
crimination among severity levels beyond the binary
mild vs. severe classification. Phenotypes with am-
biguous severity classifications appear in the middle
of Figure 6. To identify highly severe phenotypes, we
can focus only on phenotypes contained in the lower
right hand portion of Figure 6. This reduces the
phenotype selection space from 4,683 to 1,395 pheno-
types (~70% reduction).
We are providing several CAESAR files for free down-
load online at http://caesar.tatonettilab.org. These in-
clude, the 516-phenotype reference-standard used to
evaluate CAESAR, the 100-phenotype evaluation set
given to the independent evaluators along with the in-
structions, and the 4,683 conditions with their E-PSI
scores and the first and second dimensions of the 1-
proximity matrix (shown in Figure 6). This last file also
contains two subset tables containing the automatically
classified mild and severe phenotypes and their scores.Discussion
Using the patient-specific severity index as a backbone
[19], we identified five measures of EHR-specific pheno-
type severity that we used as input for CAESAR.
Phenotype-level severity differs from patient-level sever-
ity because it is an attribute of the phenotype itself and
can be used to rank phenotypes. Using CAESAR, we
were able to reduce our 4,683-phenotype set (starting
point) to 1,395 phenotypes with high severity and preva-
lence (at least 0.0001) reducing the phenotype selection
space by ~70%. Severe phenotypes are highly important
for CER because they generally correlate with lower sur-
vival outcomes, lost-productivity, and have an increased
cost burden. In fact, patients with severe heart failure
tend to have bad outcomes regardless of the treatment
they receive [37]. Therefore understanding the severity
of each condition is important before performing CER
Mild Severe
0
20
0
40
0
60
0
80
0
12
00
Treatment Length
Mild Severe
20
40
60
80
Comorbidities
Mild Severe
0
20
40
60
80
10
0
12
0
Medications
Mild Severe
5
10
15
20
25
30
Procedures
Mild Severe
0
20
0
40
0
60
0
80
0
Cost
Mild Severe
0.
2
0.
4
0.
6
0.
8
E?PSI
Figure 4 Differences in severity measures and e-psi for mild vs. severe phenotypes. The distribution of each of the 6 measures used in
CAESAR is shown for severe and mild phenotypes. Severity assignments were from our reference standard. Using the Wilcoxon Rank Sum Test,
we found statistically significant differences between severe and mild phenotypes across all 6 measures (p < 0.001). Severe phenotypes (dark red)
having higher values for each of the six measures than mild phenotypes. The least dramatic differences were observed for cost and number of
comorbidities while the most dramatic difference was for the number of medications.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 9 of 13and having a complete list of severe phenotypes would
be greatly beneficial.
Additionally, developing a classification algorithm that
is biased towards identifying more severe over mild phe-
notypes is optimal, as it would enable detection of pheno-
types that are crucial for public health purposes. Active
learning methods that favor detection of severe pheno-
types were proven successful in a subsequent study [38].
CAESAR uses an integrated severity measure ap-
proach, which is better than using any of the othermeasures alone, e.g., cost, as each severity measure has
its own specific bias. It is well known that cosmetic pro-
cedures, which by definition treat mild phenotypes, are
high in cost. If cost is used as a proxy for severity it
could introduce many biases towards phenotypes that
require cosmetic procedures (e.g., crooked nose) that are
of little importance to public health. Also some cancers
are high in cost but low in mortality (and therefore
severity), a good example being non-melanoma skin
cancer [39]. Therefore, by including multiple severity
0 100 200 300 400 500
0.
10
0.
15
0.
20
0.
25
0.
30
Error Rates for Random Forest Classification
trees
E
rr
or
OOB
MILD
SEVERE
Figure 5 CAESAR error rates. Error rates for CAESARs random forest classified are depicted with severe denoted by the green line, mild denoted by
the red line and out-of-bag (OOB) error denoted by the black line. CAESAR achieved a sensitivity = 91.67 and specificity = 77.78 indicating that it was
able to discriminate between severe and mild phenotypes. CAESAR was able to detect mild phenotypes better than severe phenotypes.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 10 of 13measures in CAESAR we have developed a method that
is robust to these types of biases.
Another interesting finding was that cancer-screening
codes tend to be classified as severe phenotypes by CAE-
SAR even though they were generally considered as mild
in the reference standard. The probable cause for this is
that screening codes, e.g., screening for malignant neo-
plasm of respiratory tract, are generally only assigned by
physicians when cancer is one of the differential diagno-
ses. In this particular situation the screening code, while
not an indicator of the disease itself, is indicative of the
patient being in an abnormal state with some symptoms
of neoplastic presence. Although not diagnoses, screen-
ing codes are indicative of a particular manifestation of
the patient state, and therefore can be considered as
phenotypes. This finding is also an artifact of the EHR,
which records the patient state [8], which does not
always correlate with the true phenotype [5,28].
Importantly, CAESAR may be useful for distinguishing
among subtypes of a given phenotype if one of the char-
acteristics of a subtype involves severity. For example,
the severity of Gaucher disease subtypes are difficult to
capture at the patient-level [40]. This rare phenotype
would benefit greatly from study using EHRs where
more patient data exists. Using CAESAR may help in
capturing the phenotype-level severity aspect of this rarephenotype, which would help propel the utility of using
EHRs to study rare phenotypes [41] by providing accur-
ate severity-based subtyping.
CAESAR is directly relevant to the efforts of the
Observational Health Data Sciences and Informatics
consortium (OHDSI), which is a continuation of OMOP.
OHDSI is an international network focused on observa-
tional studies using EHRs and other health record systems.
Their original motivation was to study post-market effects
of pharmaceutical drugs [42] based on their pharmaceutical
partnerships. To this end, a severity-based list of ranked
phenotypes would be beneficial for assessing the relative
importance of various post-marketing effects (e.g., nausea is
mild, arrhythmia is severe).
Other phenotyping efforts would also benefit from
CAESAR including the eMERGE network [3], which
seeks to carefully define phenotypes of interest for use in
PheWAS studies. So far they have classified 20 pheno-
types. Having a ranked list of phenotypes would help
eMERGE to rank prospective phenotypes, thereby allow-
ing them to select more severe phenotypes for further
algorithm development efforts.
There are several limitations to this work. The first is
that we used CUMC data when calculating four of the
severity measures. Because we used only one institutions
data, we have an institution-specific bias. However, since
Figure 6 Classification result from CAESAR showing all 4,683 phenotypes (gray) with severe (red) and mild (pink) phenotype labels
from the reference standard. All 4,683 phenotypes plotted using CAESARs dimensions 1 and 2 of the scaled 1-proximity matrix. Severe phenotypes
are colored red, mild phenotypes are colored pink and phenotypes not in the reference standard are colored gray. Notice that most of the
severe phenotypes are in the lower right hand portion of the plot while the mild space is found in the lower left hand portion.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 11 of 13CAESAR was designed using the OMOP CDM, it is
portable for use at other institutions that conform to the
OMOP CDM. The second limitation is that we did not
use clinical notes to assess severity. Some phenotypes,
e.g., allergy to peanuts, may be mentioned more often in
notes than in structured data elements. For such pheno-
types, CAESAR would under estimate their severity. The
third limitation is that we only used procedure codes
to determine phenotype cost. Therefore, phenotypes
that do not require procedures will appear as low cost
phenotypes even though they may have other costs, e.g.,
medications.
Future work involves investigating the inter-relatedness
of our severity measures and determining the temporal
factors that affect these dependencies. We also plan to in-
vestigate the inter-dependency of phenotypes (e.g., blurred
vision is a symptom of stroke, but both are treated as
separate phenotypes) and determine the utility of our se-
verity measures for distinguishing between phenotypes
and their subtypes.
Another potentially interesting extension of our
work could involve utilizing the semantics of SNOMED,
specifically their phenotype/subtype relations, to explore
CAESARs severity results. Because we chose SNOMED
to represent each phenotype, we can leverage SNO-
MEDs semantics to further probe the relationshipbetween severity and disease. Perhaps some of the
phenotypes with ambiguous severity (middle of Figure 6)
occurred because their disease subtypes can be either
mild or severe (we can assess this using SNOMEDs
hierarchical structure). However, leveraging the seman-
tics of concepts for severity classification is a complex
area [43], which will likely require additional methods to
tackle. Hopefully these topics can be explored in future
by ourselves or others.
Conclusions
This paper presents CAESAR, a method for classifying
severity from EHRs. CAESAR takes several known mea-
sures of severity: cost, treatment time, number of co-
morbidities, medications, and procedures per phenotype,
and a proportional index term as input into a random
forest algorithm that classifies each phenotype as either
mild or severe. Using a reference standard that was
validated by medical experts (k = 0.716), we found that
CAESAR achieved a sensitivity of 91.67 and specificity
of 77.78 for severity detection. CAESAR reduced our
4,683-phenotype set (starting point) to 1,395 phenotypes
with high severity. By characterizing phenotype-level
severity using CAESAR, we can identify phenotypes
worthy of study from EHRs that are of particular im-
portance for CER and public health.
Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 12 of 13Abbreviations
CER: Comparative Effectiveness Research; HOI: Health Outcomes of Interest;
OMOP: Observational Medical Outcomes Partnership; eMERGE: The Electronic
Medical Records and Genomics Network; PheWAS: Phenome-Wide
Association; EHRs: Electronic Health Records; HPO: Human Phenotype
Ontology; SNOMED-CT: Systemized Nomenclature of Medicine  Clinical
Terms; CAESAR: Classification Approach for Extracting Severity Automatically
from Electronic Health Records; CUMC: Columbia University Medical Center;
HCPCS: Healthcare Common Procedure Coding System; E-PSI: Ehr-phenotype
severity index; ICA: Independent Components Analysis; CDM: Clinical Data
Model; MD: Medical Degree; OOB: Out-of-bag error rate; MDG: Mean
Decrease in Gini; OHDSI: Observational Health Data Sciences and Informatics
consortium; ICD-9: International classifcation of diseases, 9th revision.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MRB performed research, data analyses, and wrote the paper. NPT
contributed to statistical design procedures, and provided intellectual
contributions. GH was Principal Investigator for this project, contributed to
research design, provided substantive intellectual contributions, and feedback
on the manuscript. All authors read and approved the final manuscript.
Acknowledgments
We thank the OMOP consortium and OHDSI, Dr. Patrick Ryan, and Rohan
Bareja for their assistance with various facets of OMOP/OHDSI and CUMCs
data warehouse. Support for this research provided by R01 LM006910 (GH)
and MRB supported in part by training grant T15 LM00707.
Author details
1Department of Biomedical Informatics, Columbia University, New York, NY,
USA. 2Observational Health Data Sciences and Informatics (OHDSI), Columbia
University, 622 West 168th Street, PH-20, New York, NY, USA. 3Department of
Systems Biology, Columbia University, New York, NY, USA. 4Department of
Medicine, Columbia University, New York, NY, USA.
Received: 3 November 2014 Accepted: 3 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 
DOI 10.1186/s13326-015-0022-4RESEARCH ARTICLE Open AccessNetworks of neuroinjury semantic predications to
identify biomarkers for mild traumatic brain injury
Michael J Cairelli1*, Marcelo Fiszman1, Han Zhang2 and Thomas C Rindflesch1Abstract
Objective: Mild traumatic brain injury (mTBI) has high prevalence in the military, among athletes, and in the
general population worldwide (largely due to falls). Consequences can include a range of neuropsychological
disorders. Unfortunately, such neural injury often goes undiagnosed due to the difficulty in identifying symptoms,
so the discovery of an effective biomarker would greatly assist diagnosis; however, no single biomarker has been
identified. We identify several body substances as potential components of a panel of biomarkers to support the
diagnosis of mild traumatic brain injury.
Methods: Our approach to diagnostic biomarker discovery combines ideas and techniques from systems medicine,
natural language processing, and graph theory. We create a molecular interaction network that represents neural
injury and is composed of relationships automatically extracted from the literature. We retrieve citations related to
neurological injury and extract relationships (semantic predications) that contain potential biomarkers. After linking
all relationships together to create a network representing neural injury, we filter the network by relationship
frequency and concept connectivity to reduce the set to a manageable size of higher interest substances.
Results: 99,437 relevant citations yielded 26,441 unique relations. 18,085 of these contained a potential biomarker
as subject or object with a total of 6246 unique concepts. After filtering by graph metrics, the set was reduced to
1021 relationships with 49 unique concepts, including 17 potential biomarkers.
Conclusion: We created a network of relationships containing substances derived from 99,437 citations and filtered
using graph metrics to provide a set of 17 potential biomarkers. We discuss the interaction of several of these
(glutamate, glucose, and lactate) as the basis for more effective diagnosis than is currently possible. This method
provides an opportunity to focus the effort of wet bench research on those substances with the highest potential
as biomarkers for mTBI.
Keywords: Semantic predications, Semantic networks, Natural language processing, Degree centrality, Traumatic
brain injuryIntroduction
The diagnosis and treatment of traumatic brain injury
(TBI) has received considerable attention. The military
community may provide the biggest contribution to this
interest because the signature injury of the wars in Iraq
and Afghanistan is mild TBI (mTBI) [1]. mTBI is some-
times referred to as concussion, although the latter term
is becoming less common in clinical and research con-
texts. The athletic community is also concerned with
this condition, especially football and fighting sports, but* Correspondence: mike.cairelli@nih.gov
1National Institutes of Health, National Library of Medicine, 38A 9N912A,
8600 Rockville Pike, Bethesda, MD 20892, USA
Full list of author information is available at the end of the article
© 2015 Cairelli et al.; licensee BioMed Central.
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.also rugby, hockey, and soccer [2-8]. Although less
newsworthy, falls cause the majority of head injuries in
the US, with nearly 1.7 million TBI cases annually [9].
Worldwide, the annual incidence of mild TBI is esti-
mated to be above 600 per 100,000 and, in addition to
falls, motorcycle and bicycle accidents are also major
causes [10]. As important as improvements in care are
for veterans and athletes, such improvements can have a
much broader impact on the health of communities
around the world.
Although there is a need to improve the treatment of
brain injury, perhaps the most significant hurdle is diag-
nosing mTBI. Current diagnostic standards are adequateThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 2 of 14for moderate and severe TBI because their signs and
symptoms are more easily identifiable, but about 70-
90% of TBI is mild, also known as concussion, and
still difficult to recognize [10]. Additionally, the World
Health Organization estimates that many mild injuries
are not even seen by a health care practitioner because
this lack of obvious and urgent symptoms fails to mo-
tivate patients to seek care [10]. Unfortunately, this
does not mean that there are no long-term sequelae
resulting from mTBI. According to current clinical re-
search, mTBI sequelae include cognitive dysfunction,
post-traumatic stress disorder, depression, anxiety, and
dementia [2,11].
However, there are no currently accepted markers for
clinical diagnosis of mTBI. Different organizations have
created schematic tools for diagnosis, but these are sub-
jective and the organizations do not completely agree on
what constitutes a concussion [12]. For the greatest im-
pact for military applications and throughout the world,
as well as to minimize costs, a blood-based test would
be ideal. Thus far such a test has not been found.
There have been several candidate substances (S100B,
neuron-specific enolase, glial acidic fibrillary protein,
etc.), but none have succeeded for effective diagnosis of
mild injury [13]. Because the search for a single bio-
marker has not succeeded, a composite panel may be
an effective alternative. We present a method to help
facilitate the identification of substances that have
potential as biomarkers, which can then be validated
experimentally.
As demonstrated with systems biology [14], the mo-
lecular interactions that occur after neurological injury
are complex. There may be no serum value for any of
the individual components of this complicated interplay
that are specific to neural injury. However, some specific
combination of these values included in a panel has
much greater potential for diagnostic accuracy. The
first step in investigating which substances belong in
such a panel is to identify the potential candidates for
inclusion. In this paper, we describe a methodology to
provide a list of substances that is intended to estab-
lish a base of current knowledge and provide insight
into the development of a biomarker panel for mTBI
diagnosis. We apply natural language processing to
MEDLINE citations to extract semantic predications,
which we represent as a network of potentially rele-
vant substances interacting with their physiological
environment. These semantic predications are subject-
relation-object triples, where the subjects and objects
are UMLS concepts and the relation is derived from
the UMLS Semantic Network as appropriate for a given
concept pair. We then use network analysis techniques
to identify a list that is focused on highly significant
substances.Background
Systems medicine
Our approach to diagnostic biomarker discovery was in-
spired by systems medicine, the application of systems
biology to medicine. The underlying philosophy looks at
biology as information science and is concerned with
the network of molecular interactions that define bio-
logical processes [14,15]. Additionally, disease states are
viewed as a perturbation of these molecular networks
[15]. In the case of traditional TBI biomarker discovery,
the approach has been to seek an individual molecule to
represent a disease state, while disregarding any notion
of a network let alone its perturbation. Wang et al. de-
scribe this approach as pauciparameter, containing an
inadequate amount of information and resulting in inad-
equate characterization [15]. The network must be con-
sidered as a whole, because a network perturbation does
not necessitate that any of the individual molecules are
outside of their normal serum measures, especially at
early stages of disease, when prevention is still possible
or treatment is optimal. They give prostate specific anti-
gen for prostate cancer screening as an example of a fail-
ure of the traditional single marker, pauciparameter
approach [15].
Natural language processing
Natural language processing combines artificial intelligence
and linguistic theory to extract meaning from text, using
statistical machine-learning, hand-written rules, or a com-
bined approach [16]. The data utilized in this study were
provided by SemRep, which extracts semantic predications
from all titles and abstracts in MEDLINE [17]. These pred-
ications take the form of a subject-predicate-object triplet.
The subject and object are mapped to Unified Medical
Language System (UMLS) concepts using MetaMap [18]
and are stored with their UMLS semantic type, whereas
the predicate is mapped to the UMLS Semantic Network
[19]. This provides precise semantic meaning from the
source text. For example, from the sentence in (1), SemRep
extracts the predications in (2).
(1) Basic science and clinical observations supportive of
the role of endothelins in the spasm associated with
stroke and subarachnoid hemorrhage are presented.
(Pubmed ID 15281894)
(2) Endothelin ASSOCIATED_WITH Spasm
Spasm ASSOCIATED_WITH Cerebrovascular
accidentThe results of this process are stored in a predication
database, SemMedDB [20], which has been used to sup-
port a range of biomedical information management
research: identifying novel therapeutic approaches [21],
labeling extracted information from clinical text [22],
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 3 of 14literature-based discovery [23-26], clinical information
retrieval for physicians [27], retrieving clinical documents
[28], abstraction summarization of biomedical texts [29],
biological entity recognition [30], identifying disease candi-
date genes [31], support for cardiovascular clinical guide-
lines [32,33], interpreting microarray data [34], extracting
research findings from literature [35], and supporting for-
mal models of knowledge representation [36,22].
Networks of semantic predications
Any concept in a set of predications can serve as ei-
ther subject or object in various relationships. For
example, one can imagine the concept Glutamate
appearing in many predications similar to the following:
Glutamate ASSOCIATED_WITH Traumatic Brain Injury,
Glutamate INHIBITS Glutamate Synthase, or Glycine
STIMULATES Glutamate. Similarly, any concept can have
a set of relationships that include it as either the sub-
ject or object. Further, any set of predications can be
represented as a network with each concept symbolized
as a node and each relationship denoted by an edge
(or arc) between the two nodes that represent its
subject and object. A network containing the above
predications is contained in Figure 1.
One of the goals of network theory is to establish sig-
nificance of a given node or relationship. Degree central-
ity is based on the number of connections a node has
and Zhang et al. [37] have shown that it is effective for
identifying nodes in a graph that humans consider im-
portant. We have previously applied degree centrality to
SemRep generated semantic predications to successfully
summarize therapeutic studies [38]. For node (or vertex)
v, the degree centrality is calculated by dividing the total
number of nodes connected to v, deg(v), by the total
number of nodes in the network other than v, n-1:
Cd vð Þ ¼ deg vð Þn?1STIMULATES INHIBITS
ASSOCIATED_WITH 
Glycine Glutamate 
Synthase 
TBIGlutamate
Figure 1 Network of glutamate predications. Subject and object
concepts are represented as nodes and predicates are represented
as edges. Glutamate is common to all three predications and is,
therefore, the most highly connected node in the network.A simple means of judging the value of a given rela-
tionship is the frequency of the relationship, that is, a
simple count of how many times it occurs in a given set.
When using an automated tool, a single occurrence of a
predication is much more susceptible to computational
error than a predication with multiple instances. There-
fore, a higher frequency may provide more confidence in
the validity of the relationship, but at the same time, a
high frequency is reflective of an abundance of asser-
tions in the literature which is likely to be indicative of a
well-known fact and may be less desirable for novel
discovery.
Incorporation of systems medicine, natural language
processing, and network theory
This methodology combines ideas and techniques from
systems medicine, natural language processing, and net-
work theory. A network of relationships involving sub-
stances is created, but the data source is semantic
predications from MEDLINE citations rather than gen-
omic or other large-scale experimental data as have
often been used for systems medicine. These semantic
predications provide a computable form of the know-
ledge contained in MEDLINE that includes gene, pro-
tein, and metabolite relationships analogous to the
experimental data traditionally used in systems medi-
cine, as well as additional types of relations at the organ-
ism, system, organ, tissue, cell, and molecular level.
Statistical approaches are often used to establish correl-
ation and significance of different components in the ex-
perimental data of systems medicine, whereas a network
of semantic predications provided by SemRep naturally
expresses the network of interactions postulated by sys-
tems approaches. Network filtering techniques are used
to further suggest significance of the individual concepts
and their relationships. By coupling components from
these three fields, a novel method of biomarker discov-
ery is proposed.
Related work
Several manual reviews have been undertaken to survey
potential biomarkers for TBI [39,40] and more specific-
ally mTBI [41-43]. These authors search for citations
specifically detailing clinical research of mTBI bio-
markers and therefore contain only potential biomarkers
that have already been investigated. Another limitation
of the studies is the small number of citations reviewed
(ranging from 26 [42] to 107 [43]) due to the limitations
of human review. Although no automated detection of
potential TBI biomarkers exists in the literature, there
are automatic systems to help diagnose other disorders,
for example diabetes and obesity [44]. Although not re-
lated to mTBI, there is research related to the literature-
based discovery of other types of interaction networks
Figure 2 Overview of methodology. A PubMed search was used to
find citations related to nervous system trauma (NST). SemRep
extracted predications containing chemical substances from these
citations, which were then arranged into a network. The network
was filtered by connectedness (degree centrality) and frequency to
provide a summary view of the most significant relationships.
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 4 of 14(though not specifically for biomarkers). One automatic-
ally generates an interaction network detailing gene
involvement in vaccine-related fever using 170,000 cita-
tions from a PubMed search and a vaccinespecific
ontology [45]. Another used citations containing the
PubMed MeSH term human and containing sentences
related to interferon-gamma, from which relationships
were extracted and ranked using graph metrics [46].
Jordan et al. [47] present a keyword search method for
identifying putative biomarkers for breast and lung can-
cer by searching for genes and proteins associated with a
biological fluid keyword and either cancer. However,
none of this work has made use of semantic predica-
tions, as we have, in the formation of an interaction net-
work. There is a large body of work on literature-based
discovery approaches many of which use SemRep se-
mantic predications [26,48-54]. These approaches may
generate systems for discovery [55-58] or are specific
applications to predict various phenomena such as
interactions between genes and proteins [46,59], can-
cer treatments [60,61], adverse drug reactions [49],
drug-drug interactions [50], drug repurposing [51,62],
asthma gene associations [63], treatments for neovas-
cularization in diabetic retinopathy [52], relations be-
tween psychiatric and somatic diseases [64], genes
related to reactive oxygen species and diabetes [65],
and mechanisms for sleep disturbance [25] and the obesity
paradox [53].
Methods
As shown in Figure 2, citations related to nervous sys-
tem trauma were retrieved from PubMed. From these,
predications were extracted that contain a substance as
the subject or object. These predications were organized
into a single network which is then filtered to select for
the most highly connected and frequent components.
The substances included in this summary network serve
as the list of potential mTBI biomarkers.
Citation search
A PubMed search for all articles containing the MeSH
term Trauma, Nervous System was used to generate a
list of PubMed identification numbers (PMIDs). This
term is a parent to Brain Injuries in the MeSH hierarchy
and also includes terms such as Spinal Cord Injuries and
Cerebrovascular Trauma. The source publications were
limited only in requiring that they included neural injury
as a topic, with no limitations on journal, species, loca-
tion, or type of injury. Although this included non-TBI
injury and models, (e.g., stroke, spinal cord injury,
hypoglossal-nerve injury, etc.), the goal was to undertake
as wide a search as possible in order to retrieve remote
and ignored possibilities, with the assumption that a sig-
nificant level of commonality exists between the variousforms of injury included under this broad heading in
light of their inclusion of common injury pathways such
as inflammation and oxidative damage. 99,437 unique ci-
tations were returned by this search.
Semantic predication selection
Semantic predications were extracted from SemMedDB
using the PMIDs resulting from the above PubMed
search, which yielded 26,441 unique predications. Over-
all, this set contains 6246 unique concepts, including less
informative terms, such as rattus, injury, and patients as
well as more specific terms, such as glutamate, brain-
derived neurotrophic factor, and methylprednisolone.
We then required the predications to contain at least
one concept (subject or object) having a UMLS semantic
type with potential as a substance biomarker (amino acid
sequence; amino acid, peptide, or protein; biologically
active substance; body substance; carbohydrate; carbohy-
drate sequence; chemical; chemical viewed functionally;
chemical viewed structurally; eicosanoid; enzyme; gene
or gene product; gene or genome; hormone; immuno-
logic factor; inorganic chemical; lipid; neuroreactive
substance or biogenic amine; nucleic acid, nucleoside,
or nucleotide; nucleotide sequence; organic chemical;
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 5 of 14organophosphorus compound; receptor; steroid; sub-
stance). If only one of the arguments was of this type,
the other concept could be of any semantic type. This
resulted in the inclusion of some concepts that indi-
cate that the research was performed in animal models
such as Rattus and Animals. We did not discard these
nodes because they allow the inclusion of potential
biomarkers from basic research in the spirit of transla-
tional medicine. Although a given semantic type, for
instance Pharmaceutical Substance, was not included
in the list of target semantic types, it could still appear
in a resulting predication if the complimentary subject
or object met the requirements. As an example, in
the predication Dexamethasone INTERACTS_WITH
NF-kappa B, the subject, Dexamethasone, is of type
Pharmaceutical Substance and the object, NF-kappa
B, is of type Amino Acid, Peptide, or Protein. This
predication qualifies for inclusion because of the object,
not the subject. In the predication Dexamethasone
TREATS Rheumatoid Arthritis, the object, Rheumatoid
Arthritis, is of type Disease or Syndrome, so the
predication would not be selected because neither
subject nor object is of an included semantic type.
After applying this limitation, 18,085 unique predica-
tions remained.
Network of predications
These 18,085 predications extracted from neurological
injury MEDLINE citations and containing a potential
biomarker as subject or object were then linked together
as a network. This network represents all of the known
substance activity involved in neurotrauma, as indicated
by the semantic predications included in SemMedDB.
The nodes of the network represent arguments (subject
or object) from the predications, and the edges represent
the predicates or relationships between subjects and ob-
jects. Each subject-object pair might have multiple pred-
icates. For example, both Melatonin INHIBITS Free
Radicals and Melatonin COEXISTS_WITH Free Radicals
may have been asserted in the literature. When counting
edges in the network, each predicate between the same
subject and object in such predications was counted
separately. Additionally, each subject-predicate-object
triplet could have been asserted once in MEDLINE (and
thus in SemMedDB) or as many as dozens of times.
When taking into account each predication extracted
from multiple citations, the network has 6246 total
nodes and 18,085 total edges. When only unique
(different) predications are considered (regardless of the
number citations they were extracted from), the number
of nodes in the network remains 6246, but the number
of edges is 14,085. This is still a rather large network; to
reduce it to more manageable size, further filtering was
carried out.Network filtering: degree centrality
The first cutoff applied was degree centrality. After
attempting several thresholds, a node degree cutoff of
0.0000800641 was empirically chosen to provide a net-
work with more than 50 and fewer than 100 nodes,
thereby providing a humanly readable graph. This degree
correlates to a node having edges connecting to 50 other
nodes. For example, the concept Traumatic Brain Injury
is connected to 295 other nodes with a degree of
0.0004724 and, therefore, is maintained in the network
after degree filtering. However, the concept cyclooxygen-
ase 2 is connected to only 43 concepts with a degree of
0.00006886 and so is eliminated. The 20 most highly
connected concepts are shown in Figure 3, and 20 exam-
ples of the 2688 nodes which had only a single connec-
tion are provided in Figure 4.
Network filtering: frequency of occurrence
Frequency of occurrence was used in conjunction with
degree centrality to increase the saliency of the network.
A given edge (predicate) between highly connected
nodes (arguments) was required to have a frequency of
occurrence of 2 in an attempt to eliminate spurious
extractions while still including rare statements. As an
example, the predication Interleukin-3 DISRUPTS Cell
Death is maintained in the final network because it
occurs twice in the SemMedDB predication set. Be-
cause NADPH Dehydrogenase INTERACTS_WITH
Glial Fibrillary Acidic Protein occurs only a single
time, it is not included in the final network. The most
frequently occurring predications from this set are
provided in Figure 5. This refinement requiring a fre-
quency greater than or equal to 2 and a node degree
greater than or equal to 0.0000800641 (50 or more
connections) resulted in 1021 predications with 49 unique
concepts (see Figure 6).
Substance network visualization
The resulting network was visualized as a network in
Cytoscape [66]. Redundant edges between connected
nodes were reduced to a single edge for visual simplicity.
In addition to the substance concepts targeted, it also
contains non-substance concepts which are coupled to
the substances in the final predication set. An additional
network visualization was produced (Figure 7), reformat-
ted to focus on the resulting potential biomarkers. All
non-candidate concepts were reduced in size and labels
removed. A candidate subnetwork was formed consist-
ing of substance nodes, edges connecting them, and
directly intermediate nodes and edges (single nodes be-
tween two substances if no edge directly connected the
pair). Nodes and edges outside of the candidate subnet-
work were also colored gray to further reduce visual
prominence.
Figure 3 20 most connected nodes in unfiltered network.
Figure 4 20 from the 2688 nodes with only a single predication in the unfiltered network.
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 6 of 14
Figure 5 20 most frequent predications in the unfiltered network.
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 7 of 14Substance network semantic distribution
The final network was analyzed to outline the distribu-
tion of UMLS semantic types and predicates. The se-
mantic types of nodes were sorted and tallied as were
the predicate for each token of the edges.
Substance identification precision
SemMedDB maintains a reference to the specific sen-
tence in the original citation that was the source for each
predication. Each substance in Figure 6 was compared
against this source sentence and evaluated for con-
sistency with the sentence, not for truth value. In other
words, we evaluated only whether the substance occurs
in the text; whether or not the text provided a biomedi-
cally accurate statement was not evaluated at this stage
(however, truth value was addressed in Section Evalu-
ation of biomarker potential). Precision was calculated
for the resulting substance list using the number of cor-
rect substances in the final network and the total num-
ber of substances in the final network as follows:
Precision ¼ Correct Substancesð Þ= Total Identified Substancesð Þ :
Evaluation of biomarker potential
Each of the substances in the final, filtered network
was individually reviewed manually as a potential
mTBI biomarker. The evaluation was based on 3
questions: is there evidence of a change in the level
of this substance during traumatic brain injury, is
this change evidenced in blood, and has the substancebeen previously investigated as a biomarker for trau-
matic brain injury. We searched PubMed with the
query [substance name] AND traumatic brain injury
AND (serum OR blood) and the resulting articles
were explored to provide answers to the evaluation
questions.
Results
Filtered network
There are a total of 17 substances out of the 49 con-
cepts in the final network. The first version (Figure 6)
shows all concepts (49) and their connections (145),
while in the second (Figure 7), a candidate subnet-
work is emphasized in black containing 17 sub-
stances as labeled nodes and the 48 edges connecting
them. The candidate subnetwork also contains 12
unlabeled non-substance nodes. One node shown in
the complete network was incorrectly identified as
the substance SHAM (salicylhydroxamate) instead of
sham (meaning a false experimental action) while
the 17 other substances were correctly extracted, for a
precision of 0.94.
Substance network semantic distribution
As seen in Table 1, the most common predicate in the
final network is LOCATION_OF with 26 instances. This
represents 22% of the 209 total edges. The predicate
PREDISPOSES, which is a clear indicator of biomarker
potential, is significantly lower at 12 edges (5.7%). The
semantic type Amino Acid, Peptide, or Protein was by far
most common with 13 out of the 49 nodes (26.5%) as
Figure 6 Visualization of substance predication network. The network contains 49 nodes and 1021 edges. Multiple edges between a pair of
nodes are represented as a single edge for visual simplicity; therefore edge labels are not included. Abbreviations: FGF2 = fibroblast growth
factor 2, NGFs = nerve growth factors, BDNF = brain-derived neurotrophic factor, NaCl = sodium chloride, APP = amyloid-beta precursor protein,
SOD = superoxide dismutase, NSE = neuron specific enolase, GFAP = glial fibrillary acidic protein, TBI = traumatic brain injury, IL6 = interleukin 6,
NE = norepinephrin, DA = dopamine, SHAM = salicylhydroxamic acid.
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 8 of 14seen in Table 2. This semantic type was also dominant
within the subset of substance nodes (Table 3) with 8 of
the 17 (47.1%).Evaluation of biomarker potential
The results of the substance verification in Table 4
provide an estimate of level of interest for further re-
search as a member in a biomarker panel. In general,
the substances show evidence of change in TBI in
the literature, with two exceptions: amyloid beta-
protein precursor and calpain. (Although calpain
itself does not appear in the literature, the calpain-
derived NH2-terminal fragment of ?-spectrin frag-
ment does [67]). Timing and degree of change may
also be an issue regarding the effectiveness of some
substances as mTBI biomarkers. Reduced levels of
calcium appear to return to normal within as little as
4 hours of trauma [68]. Glutamate levels increase in
cerebral spinal fluid but there is no evidence for
measurable changes in blood [69-72]. And a conflict
exists in the literature for melatonin. One study re-
ports a decrease in serum melatonin after TBI [73]
while another reports no change in blood but an in-
crease in cerebral spinal fluid [74].Discussion
Most substances identified in this study as worthy of
consideration as mTBI biomarkers fall into four gen-
eral categories: previously studied biomarkers (amyl-
oid beta-protein precursor, brain-derived neurotrophic
factor, fibroblast growth factor 2, glial fibrillary acidic
protein, neuron-specific enolase, S100b); neurotransmitters
(glutamate, dopamine, norepinephrine); inflammation
and cell injury markers (interleukin-6, calpain break-
down products, malondialdehyde, superoxide dismutase);
and ubiquitous substances (glucose, lactate, calcium).
Although all of the resulting substances were reviewed
in depth during the methodology, the following illustrate
the information contained in the resulting mTBI bio-
marker network and the information retrieved during
the validation process. These examples suggest possible
implications for clinical practice retrieved directly from
the research literature.Glutamate
The well-known association between glutamate and TBI
is present in the network as Glutamate ASSOCIATED_
WITH Traumatic Brain Injury (PMID 17014847), but
relationships that focus on interconnectedness with
S100B
Calpain
Glutamate
Calcium
NSE
NE
DA
Malondialdehyde 
SOD
IL6
BDNF
APP
Melatonin
LactateGlucose
GFAP
FGF2
Figure 7 Visualization of interaction network of TBI substances. Only
substance nodes are labeled and paths between substance nodes
are colored black for lengths one or two edges. Abbreviations:
FGF2 = fibroblast growth factor 2, BDNF = brain-derived neurotrophic
factor, APP = amyloid-beta precursor protein, SOD = superoxide
dismutase, NSE = neuron specific enolase, GFAP = glial fibrillary acidic
protein, IL6 = interleukin 6, NE = norepinephrin, DA = dopamine.
Table 1 Predication frequency in final network
LOCATION_OF 46
ASSOCIATED_WITH 28
PART_OF 25
AUGMENTS 20
PREDISPOSES 12
ADMINISTERED_TO 11
ISA 11
AFFECTS 10
CAUSES 10
COMPARED_WITH 6
DISRUPTS 5
INTERACTS_WITH 5
TREATS 5
STIMULATES 4
INHIBITS 3
COEXISTS_WITH 2
NEG_ADMINISTERED_TO 2
NEG_INTERACTS_WITH 2
PRODUCES 2
Table 2 Semantic type frequency in final network
Amino acid, peptide, or protein 13
Body part, organ, or organ component 5
Biologically active substance 4
Cell 4
Hormone 4
Injury or poisoning 4
Organic chemical 3
Neuroreactive substance or biogenic amine 2
Animal 1
Cell component 1
Gene or genome 1
Inorganic chemical 1
Mammal 1
Pharmacologic substance 1
Patient or disabled group 1
Sign or symptom 1
Steroid 1
Tissue 1
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 9 of 14other substances in the network are also present.
For instance, Lactate INTERACTS_WITH Glutamate
is extracted from (3) which notes that glutamate is
produced from the metabolism of lactate in TBI,
and perhaps a less familiar relationship, Glutamate
STIMULATES Lactate is extracted from (4), highlighting
glutamates role in activating lactate production in a
potentially neuroprotective, estrogen receptor-dependent
manner.
(3) Infusion with  3-(13)C-lactate produced (13)C
signals for glutamine  indicating tricarboxylic acid
cycle operation followed by conversion of glutamate
to glutamine. (PMID 19700417)
(4) These results suggest a new neuroprotective
mechanism of 17beta-estradiol by activating
glutamate-stimulated lactate production, which is
estrogen receptor-dependent. (PMID 11368971)Glucose and lactate
Glucose and lactate are substances within the network
that (along with calcium) are ubiquitous in the human
system. Within the context of TBI a major concern is
the decrease of available glucose in the brain due to
Table 3 Semantic type frequency of substances in final
network
Amino acid, peptide, or protein 8
Organic chemical 3
Biologically active substance 2
Neuroreactive substance or biogenic amine 2
Gene or genome 1
Hormone 1
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 10 of 14ischemia and the subsequent increase in lactate. This is
included in our neural injury network as Glucose
COEXISTS_WITH Lactate, which is extracted from (5).
Sentence (6) is another source of the link between
lactate and glucose, but the source sentence provides
the additional knowledge that peripheral blood glu-
cose levels are not isolated from cerebral levels and
lactate production in the TBI brain, while (7) pre-
sents the opposite, that arterial lactate is connected
to cerebral lactate and subsequently cerebral glucose,
represented in our network as Lactate COEXISTS_
WITH Glucose. As suggested in (4) above, the ratio
of glucose to lactate is influenced by glutamate. It
has been suggested that this may be a result of astro-
cytes responding to increased extracellular glutamate
by increasing glycolysis and, thereby, lactate produc-
tion [75].Table 4 Verification of substances in TBI physiology and TBI b
Changes in trauma? Chang
1 Brain-derived neurotrophic factor Yes Yes
2 Fibroblast growth factor 2 Yes Yes
3 Glial fibrillary acidic protein Yes Yes
4 Neuron-specific enolase Yes Yes
5 S100B Yes Yes
6 Amyloid beta-protein precursor Yes No
7 Interleukin-6 Yes Yes
8 Malondialdehyde Yes Yes
9 Superoxide dismutase Yes Yes
10 Glucose Yes Yes
11 Lactate Yes Yes
12 Dopamine Yes Yes
13 Norepinephrine Yes Yes
14 Calcium Yes Yes
15 Melatonin Yes Yes*
16 Glutamate Yes Yes*
17 Calpain No+ No+
*Modest change or conflicting reports. +Although Calpain itself does not change in trau(5) Following TBI, neuron use initially increases, with
subsequent depletion of extracellular glucose,
resulting in increased levels of extracellular lactate
and pyruvate. (PMID 18826359)
(6) Arterial blood glucose significantly influenced signs
of cerebral metabolism reflected by increased
cerebral glucose uptake [and] decreased cerebral
lactate production (PMID 19196488)
(7) We conclude that arterial lactate augmentation can
increase brain dialysate lactate, and result in more
rapid recovery of dialysate glucose after FPI [fluid
percussion brain injury]. (PMID 10709871)
Biomarker panels
Although there have been a limited number of attempts
to include multiple biomarkers in panels for TBI
[67,76,77], these have not included some of the types of
substances returned in our results. To a large degree the
absence of consideration for such substances may be ex-
plained by their lack of specificity or their ubiquitous
nature. The level of specificity as an analyte for these
neglected substances is significantly higher for an indi-
vidual marker to stand on its own, and substances that
are frequent if not ubiquitous in normal physiology are
not obvious as candidates for TBI identification. Taken
on their own, glucose and calcium levels are not useful
as measures of brain injury. However, a panel of
markers could better represent the complex network of
molecular changes that occur during TBI and changeiomarker research
es in blood? Previously studied? PMIDs
Yes 11585248, 22528282, 20679891
Yes 11320217, 7696886
Yes 16266720, 22528282, 21079180
Yes 16716992, 22528282
Yes 19257803, 22528282, 21079180
Yes 8140894, 15258792
No 20850781, 20858121
No 11280646, 11466564
No 17869973
No 20889287, 9808254
No 18183032, 20889287
No 7584744
No 6886758, 3592639
No 10386980, 4637556
No 18183032, 17060154
No 20225002, 21878868
No+ 19811094
ma, its products do change and are found in the blood and have been studied.
a)
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 11 of 14the goal from an individual marker/single variable to a
panel ameliorates the lack of specificity  as long as the
panel as a whole provides adequate sensitivity and
specificity.b)
Figure 8 The relative distribution of predication frequency by year.
a) All frequencies. b) Predications that have at least 2 occurrences.
78.8% of predications occurred only once.Limitations of study
These resulting data provide a clinically relevant hypoth-
esis of potential mTBI biomarkers, which requires ex-
perimental validation. In our investigation into the
validity of the results, it was evident that for some of the
substances, especially the previously-studied biomarkers,
the background TBI model-based studies have already
been completed. For others, this is not the case and
basic exploration in models may need to be pursued be-
fore moving towards clinical research.
The current result set is limited to the uppermost ex-
treme of node connectedness and therefore potentially
overlooks less investigated substances that appear in
fewer publications. An elimination of the most frequent
predications may enrich the result set for substances less
familiar and thereby, potentially, more valuable. The
current threshold is principally set to provide a visually
comprehensible network in the result, though such a
visualization is not required. Reducing the threshold for
inclusion would expand the list with significant com-
pounds, including microRNA.
When we filter by frequency of occurrence with a
cutoff of 2 instances we eliminate 78.8% of the predica-
tions. This step risks eliminating predications that
occur only once because they are completely new and
have only been stated once. Figure 8a shows that 7.8%
of predications were from citations in 2010. As seen in
Figure 8b, when all predications that occur only one
time are removed, the 2010 fraction increases to 7.9%.
This shows that there is not a disproportionate elimin-
ation of predications from the most recent citations
and the loss of unique predications due to their novelty
likely plays a much less significant role than the elim-
ination of inaccurate extraction by SemRep. On the
other hand, as SemRep precision continues to improve,
additional attention to date of publication may be
required.Future directions
Creating a map of neural injury interactions offers sig-
nificant potential for basic science research. Additionally,
our refinement of the network to identify the most
significant interactions according to their degree central-
ity and frequency facilitates the quick translation of pub-
lished research data into clinical practice. The resulting
compound list is clearly interesting in the context of
clinical applicability and merits further study. This tech-
nique allows the investigation of potential biomarkers tobe focused, potentially reducing the wet-lab effort and
reducing the time of assay development.
Now that we have outlined a basic methodology, we
would like to compare this method with various other
methods combining information extraction and network
analysis to understand the advantages and disadvantages
to different approaches.
Our current methodology can be expanded as noted
above to include different subsets of substances in the
Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 12 of 14final result. Additionally, this methodology is not limited
to biomarker discovery but can also be applied to other
areas of medical discovery, including novel therapeutic
targets, drug repurposing, and others.
Conclusion
We have explored the creation of a molecular inter-
action network that represents neural injury and is
composed of semantic predications automatically ex-
tracted from the literature. We achieved our goal of
providing substances with potential as biomarkers to
support the diagnosis of mTBI. The methodology is
based on a network of semantic predications represent-
ing the interaction of substances observed subsequent
to neural insult. Combining semantic predications
of TBI substance interactions into a network in this way
correlates well with systems biology (and by extension,
systems medicine), which is concerned with the com-
plex network interplay of a biological unit and repre-
sents injury and illness as a perturbation to the
network.
Predications were extracted by SemRep and the com-
ponent subject or object concepts were mapped to
nodes and their relationships (predicates) mapped to
edges, creating a network of relations. This network rep-
resents a summary of the physiological and pharmacoge-
nomic space of neurological injury, as presented in the
literature included in MEDLINE. To identify clinically
significant candidates for mTBI biomarkers, the network
was then filtered by degree centrality and frequency,
greatly reducing the density of concepts and relation-
ships. The resulting network produced 17 compounds to
be considered as mTBI biomarkers, both previously
investigated and novel as TBI biomarker candidates. The
interaction of several of these is discussed as the basis
for a panel of biomarkers to more effectively diagnose
mTBI than is currently possible.
Availability of data and software
The predication data (SemMedDB) is available at
skr3.nlm.nih.gov. Degree and frequency filtering java
programs are available at skr3.nlm.nih.gov/mTBI.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MJC participated in the design of the study; performed the PubMed Searches;
queried the SemMedDB database; participated in the graph creation, filtering,
and visualization; and drafted the manuscript. MF participated in the design of
the study and helped to draft the manuscript. HZ participated in the graph
creation, filtering, and visualization and helped to draft the manuscript. TCR
participated in the design of the study and helped to draft the manuscript. All
authors read and approved the final manuscript.
Acknowledgments
This research was supported in part by an appointment to the National
Library of Medicine Research Participation Program administered by the OakRidge Institute for Science and Education through an inter-agency agreement
between the US Department of Energy and the National Library of Medicine.
This study was supported in part by the Intramural Research Program of the
National Institutes of Health, National Library of Medicine.
Author details
1National Institutes of Health, National Library of Medicine, 38A 9N912A,
8600 Rockville Pike, Bethesda, MD 20892, USA. 2Department of Medical
Informatics, China Medical University, Shenyang, Liaoning 110001, China.
JOURNAL OF
BIOMEDICAL SEMANTICS
Doing-Harris et al. Journal of Biomedical Semantics  (2015) 6:15 
DOI 10.1186/s13326-015-0011-7SOFTWARE Open AccessAutomated concept and relationship extraction
for the semi-automated ontology management
(SEAM) system
Kristina Doing-Harris1*, Yarden Livnat2 and Stephane Meystre1Abstract
Background: We develop medical-specialty specific ontologies that contain the settled science and common
term usage. We leverage current practices in information and relationship extraction to streamline the ontology
development process. Our system combines different text types with information and relationship extraction
techniques in a low overhead modifiable system. Our SEmi-Automated ontology Maintenance (SEAM) system
features a natural language processing pipeline for information extraction. Synonym and hierarchical groups are
identified using corpus-based semantics and lexico-syntactic patterns. The semantic vectors we use are term
frequency by inverse document frequency and context vectors.
Clinical documents contain the terms we want in an ontology. They also contain idiosyncratic usage and are
unlikely to contain the linguistic constructs associated with synonym and hierarchy identification. By including
both clinical and biomedical texts, SEAM can recommend terms from those appearing in both document types.
The set of recommended terms is then used to filter the synonyms and hierarchical relationships extracted from
the biomedical corpus.
We demonstrate the generality of the system across three use cases: ontologies for acute changes in mental status,
Medically Unexplained Syndromes, and echocardiogram summary statements.
Results: Across the three uses cases, we held the number of recommended terms relatively constant by changing
SEAMs parameters. Experts seem to find more than 300 recommended terms to be overwhelming. The approval
rate of recommended terms increased as the number and specificity of clinical documents in the corpus increased.
It was 60% when there were 199 clinical documents that were not specific to the ontology domain and 90% when
there were 2879 documents very specific to the target domain.
We found that fewer than 100 recommended synonym groups were also preferred. Approval rates for synonym
recommendations remained low varying from 43% to 25% as the number of journal articles increased from 19 to
47. Overall the number of recommended hierarchical relationships was very low although approval was good. It
varied between 67% and 31%.
Conclusion: SEAM produced a concise list of recommended clinical terms, synonyms and hierarchical relationships
regardless of medical domain.
Keywords: Ontology, Natural language processing, Terminology extraction* Correspondence: kristina.doing-harris@utah.edu
1University of Utah, Department of Biomedical Informatics, 421 Wakara Way,
Suite 140, Salt Lake City, UT 84112, USA
Full list of author information is available at the end of the article
© 2015 Doing-Harris et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,
unless otherwise stated.
Doing-Harris et al. Journal of Biomedical Semantics  (2015) 6:15 Page 2 of 15Introduction
We created an ontology development system, SEmi-Au-
tomated ontology Maintenance (SEAM) that leverages
current practices in information and relationship extrac-
tion from text to streamline the process of generating
knowledge structures. The knowledge structures that inter-
est us are medical specialty-specific ontologies. We will use
these ontologies for machine assisted clinical diagnostic de-
cision support (CDS). CDS requires knowledge structures
representing diagnostic criteria, a method for gathering pa-
tient information and the ability to reconcile the gathered
patient information with the diagnostic knowledge struc-
tures. These requirements stem from diagnostic decision-
making, which requires knowing at least two things: 1) the
criteria for a diagnosis and 2) if this particular patient
meets those criteria. The goal of the SEAM system is to fa-
cilitate the information acquisition necessary to construct
ontologies that represent the settled science and common
term usage with respect to either medical specialty or par-
ticular disease.
Background
Our current approach to building diagnostic knowledge
structures is to construct an application ontology of a spe-
cific disease or medical specialty. Here application ontol-
ogy is used to differentiate them from domain ontologies
like the one described in [1]. Ontology is an arrangement
for defining concepts, the relationships between them,
and rules relating to the combining of concepts and rela-
tions [2]. Conceptsa are roughly the ideas to which
words refer (i.e. what the words mean), which is also
called semantics. Concepts are often thought of as groups
of semantically equivalent terms (e.g. heart attack, myo-
cardial infarction, MI). These equivalences allow an auto-
matic system to map terms used in one setting on to
those used in another. Relationships between the terms
are required because people often use terms that are
semantically related, but not semantically equivalent, to
represent the same idea. For instance, a clinician may in
some situations refer interchangeably to bowels and intes-
JOURNAL OF
BIOMEDICAL SEMANTICS
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 
DOI 10.1186/s13326-015-0012-6
RESEARCH ARTICLE Open Access
Semantic enrichment of longitudinal clinical
study data using the CDISC standards and the
semantic statistics vocabularies
Hugo Leroux1* and Laurent Lefort2
Abstract
Background: There is an increasing recognition of the need for the data capture phase of clinical studies to be
improved and for more effective sharing of clinical data. The Health Care and Life Sciences community has embraced
semantic technologies to facilitate the integration of health data from electronic health records, clinical studies and
pharmaceutical research. This paper explores the integration of clinical study data exchange standards and semantic
statistic vocabularies to deliver clinical data as linked data in a format that is easier to enrich with links to
complementary data sources and consume by a broad user base.
Methods: We propose a Linked Clinical Data Cube (LCDC), which combines the strength of the RDF Data Cube and
DDI-RDF vocabulary to enrich clinical data based on the CDISC standards. The CDISC standards provide the
mechanisms for the data to be standardised, made more accessible and accountable whereas the RDF Data Cube and
DDI-RDF vocabularies provide novel approaches to managing large volumes of heterogeneous linked data resources.
Results: We validate our approach using a large-scale longitudinal clinical study into neurodegenerative diseases.
This dataset, comprising more than 1600 variables clustered in 25 different sub-domains, has been fully converted
into RDF forming one main data cube and one specialised cube for each sub-domain. One sub-domain, the
Medications specialised cube, has been linked to relevant external vocabularies, such as the Australian Medicines
Terminology and the ATC DDD taxonomy and DrugBank terminology. This provides new dimensions on which to
query the data that promote the exploration of drug-drug and drug-disease interactions.
Conclusions: This implementation highlights the effectiveness of the association of the semantic statistics
vocabularies for the publication of large heterogeneous data sets as linked data and the integration of the semantic
statistics vocabularies with the CDISC standards. In particular, it demonstrates the potential of the two vocabularies in
overcoming the monolithic nature of the underlying model and improving the navigation and querying of the data
from multiple angles to support richer data analysis of clinical study data. The forecasted benefits are more efficient
use of clinicians time and the potential to facilitate cross-study analysis.
Keywords: Ontology, Semantic enrichment, Longitudinal clinical study, RDF data cube, Medication mapping
*Correspondence: hugo.leroux@csiro.au
Equal contributors
1The Australian e-Health Research Centre, Digital Productivity Flagship, CSIRO,
Level 5 - UQ Health Sciences Building 901/16, Brisbane, Queensland 4029,
Australia
Full list of author information is available at the end of the article
© 2015 Leroux and Lefort; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication
waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise
stated.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 2 of 14
Background
In the last decade, the Health Care and Life Sciences com-
munity and pharmaceutical industry have wholeheartedly
adopted [1] clinical study data exchange technologies
based on XML to capture clinical study data. This is
largely due to the recent strategy [2] of the Food and
Drug Administration (FDA) in promoting the Clinical
Data Interchange Standards Consortium (CDISC) suite
of standards to facilitate data submission and exchange.
Furthermore, the move by EU and US regulating bod-
ies to open access to clinical data [3,4] will also foster
the adoption of tools supporting clinical data manage-
ment standards, especially those that can easily be linked
to methods and tools developed for Government Linked
Data and Linked Science Data.
CDISC has developed a set of platform-independent
data standards [5] for the collection and dissemination of
clinical trial data. The CDISC Operational Data Model
(ODM) is an XML format that facilitates the exchange
of clinical data captured during a clinical study. ODM-
based files contain the study data and the associated
descriptions of the data items, their groupings into Case
Report Forms (CRFs), which are electronic documents to
record the study data, and the associated questions and
code lists. Furthermore, the FDA has mandated the use
of other CDISC standards in clinical studies. In particu-
lar the CDISC Study Data Tabulation Model (SDTM) is
used to facilitate studymetadata submissions and improve
the accountability of the study data. The role of the
CDISC Clinical Data Acquisition Standards Harmoniza-
tion (CDASH) is to standardise the generation of CRFs for
clinical studies. The implementation of the ODM, STDM
and CDASH standards in Clinical Data Management
Systems (CDMS) has enabled larger and more diverse
longitudinal clinical research studies and increased the
capability of users to exchange and combine data [6].
Challenges relating to the cross-study analysis of clinical
study data
A number of limitations relating to the reporting of results
derived from current clinical trial endeavours were iden-
tified by van Valkenhoef et al. [7]. In particular, they
stress that: current infrastructure is focused on text-based
reports of single studies, whereas efficient evidence-based
medicine requires the automated integration of multiple
clinical trials from different information resources [7].
They specifically advocate for a comprehensive record of
clinical trials to be made available in a machine under-
standable format that would improve the efficiency of
evidence-based decision making but more importantly
that decisions could then finally be explicitly linked back
to the underlying data. Chief among their list of topics
for future research directions are: (i) the development of
a comprehensive data model for clinical trials and their
aggregate level results; and (ii) the development of a plat-
form to share structured systematic review data sets.
Our contribution: semantic enrichment
This research builds upon existing work [8] to semanti-
cally enrich longitudinal clinical study data, based on the
CDISC standards, using semantic statistic vocabularies,
namely the RDF Data Cube and DDI-RDF vocabularies.
We propose a Linked Clinical Data Cube, a set of mod-
ular data cubes that helps manage the multi-dimensional
and multi-disciplinary nature of the clinical data. The
RDF Data Cube vocabulary [9] is used to build multi-
dimensional data cubes and supports flexible access to the
data via thematic slices. The DDI-RDF Discovery vocab-
ulary [10] is effective at encoding the study-specific data
dictionary embedded in the CDISC ODM standard as
linked data and helps in managing the link between the
data cube variables and the data.
Our objective is to make the data captured within the
Australian, Imaging, Biomarker and Lifestyle study of
Ageing (AIBL) [11] seamlessly available to researchers
who wish to engage in cross-domain analysis of the data.
We achieve our goal by semantically enriching the data,
when possible, with external data sources. Our approach
is four-fold:
Phase 1: Integrating the CDISC ODM data model with
the semantic statistic vocabularies. We describe how the
clinical data available in CDISC ODM can be mapped to
the RDF Data Cube and DDI-RDF Discovery vocabulary
to form the Linked Clinical Data Cube.
Phase 2: Splitting the data into modularised cubes. We
outline the design principles of splitting the data intomore
modularised and manageable groupings to provide alter-
native mechanisms for accessing and querying the data.
The RDF Data Cube and DDI-RDF vocabularies are piv-
otal elements of our slicing strategy and of the URI scheme
defined for our implementation.
Phase 3: Enriching the LCDC with the CDISC standards.
We discuss how useful the benefits of clinical study data
to adhering to the CDISC CDASH and SDTM standards
then elaborate on guidelines to classify the data into the
broad categories.
Phase 4: Mapping the data to drug terminologies. We
demonstrate the utility of the LCDC bymapping the med-
ications data derived from the AIBL study to selected
online drug terminologies.
The AIBL study
AIBL is a prospective study of a large group (1112) of
individuals residing in two Australian cities, Perth and
Melbourne, aged over 60 years who are either clas-
sified as cognitively healthy, or meet clinical criteria
for mild cognitive impairment or Alzheimers Disease
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 3 of 14
and who have agreed to reassessment every 18 months.
Assessment comprises extensive study of cognitive func-
tion, neuroimaging, blood biomarkers and lifestyle (diet
and exercise) characteristics [11]. By combining these
investigations in a prospective fashion, the AIBL study
contributes to understanding the development and pro-
gression of Alzheimers Disease through the prodromal,
preclinical and clinical stages of the disease [12]. It is vital
for the clinical data to be reported at regular intervals as
the study progresses. To facilitate this task, the study data
is manually entered into the OpenClinica Clinical Data
Management System (CDMS) by study staff [13]. Figure 1
describes the AIBL study with the fivemain categories and
sub-categories.
OpenClinica [14] is an open-source CDMS for collect-
ing and managing clinical data. The AIBL study data was
successfully migrated to this platform in 2011 [13] and has
been live since August 2011. OpenClinica supports the
creation of customisable studies and the design of user-
defined Case Report Forms (CRFs) using an Excel spread-
sheet and adheres to the CDISC ODM standard. The
data collected for the AIBL study covers multiple domains
as shown in Figure 1. This dataset comprises more than
1600 variables clustered into 25 different sub-domains.
The study has been split into five themes: Study, Clinical,
Cognitive, Imaging and Lifestyle. The Study theme com-
prises administrative information that, for the most part,
is not shared within the cube. Table 1 depicts the total
number of instances for the various LCDC classes organ-
ised by theme. The total number of variables, in the
table, is smaller than 1600 because the generation to RDF
suppresses duplicates.
Article outline
In the remainder of this article, we outline an approach
to semantically enrich clinical study data, in particu-
lar patient-reported medication usage, and facilitate their
delivery to clinical researchers. In particular, we outline
how the use of semantic statistics vocabularies is effective
at organising the data into a LCDC. We also elaborate on
the approach taken to categorise the AIBL data set into
CDISC CDASH and SDTM domains and the work car-
ried out to translate the CDISC standards into RDF. This
leads into the discussion on the design principles for the
LCDC and of the benefits of splitting the data into more
modularised groupings.
Methods
The LCDC [15] comprises one main cube and several
specialised cubes, one for each domain within the study,
that integrates the CDISC ODM data model with the RDF
Data Cube and DDI-RDF vocabularies. We elaborate fur-
ther on the rationale behind this integration below. The
LCDC is designed around a set of cubes, slices, obser-
vation groups and observations and these are discussed
further below. The ability to standardise the clinical data
in order to facilitate cross-domain and, possibly, cross-
study analysis of the data is one of the salient objectives
of the LCDC. To this end, we describe how the study
variables have been enriched by the CDISC CDASH and
SDTM standards. Aside from providing a standardised
representation to the study variables and grouping them
along the various CDISC categories, this enrichment pro-
cess allows for seamless substitution of variable names in
the navigation and querying of the clinical study. Finally,
Figure 1 The Australian imaging biomarker and lifestyle study of ageing. Illustrates the logical organisation of the AIBL study. The AIBL study
(depicted as a rectangle in light green with thick border) is split into the five domains (depicted as rectangles in light blue). Each domain is further
categorised into sub-domains depicted by rounded rectangles.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 4 of 14
Table 1 Number of instances for the LCDC classes organised by theme
Theme Total Obs. Obs. Subject Sub-theme Sub-theme Variable
group section series slice
Clinical 1030430 4495 25210 1416 25 6452 506
Cognitive 761650 4612 9826 1415 19 4069 367
Imaging 58601 866 2136 365 12 941 59
Lifestyle 710594 4026 11953 1415 19 7360 391
Study 235566 5384 6218 1414 13 3292 155
we outline how the coupling of the study data with exter-
nal resources - in this case drug terminologies - can be
achieved within the LCDC and we elaborate on our pro-
cess to implement a linked medications data set and how
the patient-reported medication intake from the AIBL
study has been mapped to this data set.
Phase 1: Integrating the CDISC ODM data model with the
semantic statistic vocabularies
Clinical study data is extracted in CDISC ODM for-
mat. The primary dimensions of the CDISC ODM
data model are the Subject and Study Event of interest
within the study. The additional dimensions, including
the Study, Form, ItemGroup and Item, depend on the
study domains and are specified by the data dictionary
that defines the study. The strength of the RDF Data
Cube is that the original structure of the CDISC ODM
data model (Study-Subject-StudyEvent-Form-ItemGroup-
Item) lends itself to be replicated in the generated cube
with relative ease. A further contribution of the RDF
Data Cube is that it can help overcome the monolithic
nature of the ODM data model by facilitating the con-
struction of multi-dimensional cubes that offer access
points to the data via thematic slices. The LCDC is
organised into one main cube and several specialised
cubes corresponding to the various domains in the
study.
The RDF Data Cube model facilitates the grouping
of subsets of observations, within the dataset, whereby
all but one (or a small subset) of the dimensions are
fixed. Furthermore, it supports alternative methods of
accessing the data where the data is aggregated along
other dimensions or along the same dimension in differ-
ent order. The DDI-RDF Discovery vocabulary is used
to consistently manage the study-specific data dictionary
exported in CDISC ODM format enriched with CDISC
metadata resources (CDASH and SDTM). These two
vocabularies are supplemented by the Vocabulary of Inter-
linked Dataset (VoID). These allow the LCDC ontology
to be defined with more generalised classes and proper-
ties, such as the disco:Universe, disco:Variable
and disco:VariableDefinition [15] as depicted in
Figure 2.
Phase 2: Splitting the data into modularised cubes
The design of the LCDC is achieved in three steps. The
first step involves splitting the dataset into smaller, more
manageable specialised cubes. The second step is to define
several slice hierarchies that offer multiple access options
to the individual data records. The third step is to define a
URI scheme that supports access to the cube at all levels of
the slice hierarchy. These three steps are discussed below.
The LCDC defines three categories of slices. The time-
series slices address the longitudinal nature of the study
and organise the data into time-intervals and dated
and non-dated time points. Cross-section slices adopt a
subject-centric approach to the abstraction of the data
along some important concepts such as gender, genotype
and neurological classifications. The Theme slices cate-
gorise the data into the study domains and sub-domains
(disco:Universe in DDI-RDF) and help link the main
and specialised cubes together. This process enhances the
navigation and querying of the data in the LCDC because
we provide three direct links to nodes containing the data
instead of one through the Phase series (at the level of
the Study Event data in ODM), the Subject section (at the
Subject level) and the sub-theme slice (at the Item Group
level).
The slice hierarchy is provided primarily through the
use of the classes and properties from the RDF Data
Cube. Figure 3 highlights the LCDC slices that sub-
sume qb:Slice. We use the void:subset property
to describe the link between the main and specialised
cubes. Links between slices and observations are specified
using the qb:observation property, while the ones
between slices and observation groups are represented by
qb:observationGroup. The specialisedSeries
and specialisedSection properties manage the
links between the slices in the main and specialised cubes.
The specialisedObservation property, which is a
sub-property of qb:observation, handles the links
between the observation groups from the main cube to
corresponding observations in the specialised cubes.
The URI scheme describing the LCDC follows the
convention from the Linked Data API [16], which uses
URIs ending with an identifier to provide access to a
single instance (Item endpoint) and URIs ending with
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 5 of 14
Figure 2 Mapping the CDISC ODMmodel to the data cube and DDI vocabularies. Illustrates how the original CDISC ODMmodel (depicted by
rectangles in light gray) is overlaid with the RDF Data Cube (depicted by ellipses in green) and the DDI-RDF vocabularies (depicted by rounded
rectangles in blue). The Data section, depicted on the left of the model, comprises a hierarchical structure whereby each level is fully contained
within the preceding level. As the left side is more about structuring the clinical data, the Data section of the CDISC ODMmodel is more closely
related to qb. The Clinical Data node is mapped to qb:Dataset while qb:Slice is used to split the Subject, Study Event and
Form data nodes across the ODM hierarchy into slices, and the Item Data node is mapped to qb:Observation. The ODM node refers to the
entire data set and is mapped to disco:LogicalDataset. The right side comprises the metadata section, which contains one Study node,
which further comprises one MetaData node. The MetaData node contains a number of StudyEventDef, FormDef, ItemGroupDef and
ItemDef nodes, one corresponding to each of the Subject, Study Event, Form, Item Group and Item data nodes defined in the Data
section. The Metadata section shows how the variable definitions managed through discomatches ODMs ItemDef while the grouping of
variables via disco:Universe is applied at the FormDef level. Finally, Item Data is logically mapped to disco:Variable.
a keyword to provide access to a list of instances (List
endpoint).
Phase 3: Enriching the LCDC with the CDISC standards
The CDISC CDASH and SDTM standards provide the
means to standardise the clinical data. Despite not being
designed around the CDISC standards, there is a good
overlap between the AIBL study and the CDISC CDASH
and SDTM standards for categories such as Vital Signs,
Blood (represented by Laboratory Test in CDASH) and
Medical History. For some categories within AIBL, the
study data is clustered across many classes that do not
necessarily fit to single CDASH or SDTM categories. We
have chosen to map our medication data to the Concomi-
tant Medications (CM) class within CDASH. Regarding
CM, the approach taken by CDISC is to provide a frame-
work and allow the users the ability to define the terminol-
ogy of their choice. The AIBL Demographics data can be
mapped to the CDISC Demographics and Subject Char-
acteristics categories. SDTMs Trial Arms, Trial Summary,
Trial Visits and Subject Visits categories are appropriate
targets for mapping longitudinal aspects of the study. For
data items that are based on questionnaires, the method-
ology adopted by CDISC is to guide the user by providing
a Questionnaire Supplements (QS) template that the user
can mould to their needs. The SDTM standard provides
approximately 50 questionnaires within theQSmodel that
the user can use to model their study. The relatively low
number of publicly available questionnaires is due to the
fact that many of the questionnaires in clinical studies are
licensed.
We have coupled the AIBL-specific variables to existing
CDISC concepts, when possible, to allow a straightfor-
ward swap of variable names in a query. For exam-
ple, the AIBL property for systolic blood Pressure
(aiblvitalsigns:systolicBP) has been linked to
the CDISC Vital Sign concept (cdiscvs:systolicB-
loodPressure).
Phase 4: Mapping the data to drug terminologies
In addition to the direct coupling between AIBL and
CDISC definitions described above, we have mapped
the patient-reported medication intake of the AIBL par-
ticipants to three external terminologies: AMT, ATC
DDDa and DrugBank. Our goal is to provide multiple
links to hierarchical classifications of drugs. AMT pro-
vides unique codes and accurate standardised names to
unambiguously identify all commonly used medicines in
Australia with eight key top-level concepts [17]. We aug-
ment AMTs capabilities with links to ARTGb and UNIIc.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 6 of 14
Figure 3 Linked clinical data cube architecture aligned with the RDF data cube. Depicts the architecture of the LCDC. The Main cube (depicted as
a red cube) is split into modular Specialised cubes (depicted as a blue cube) and linked using the void:subset property. The Main cube is
organised into time-series, cross-section and theme slices using the qb:slice property. The slices are then divided into
Observations using the qb:observation property. The qb:dataset property is used to link the observations back to the cube. The
Specialised cubes are organised similarly to the Main cube with the exception of the theme slices. The dotted lines show how the slices from
all cubes interlink to the study observations through the use of ObservationGroups and the qb:observationGroup property. The
mainObservation property manages the link between the ObservationGroups and the Observations while the
specialisedObservation property handles the link between the ObservationGroups in the main cube and the corresponding
Observations in the specialised cubes.
ARTG contains the most comprehensive list of brand
names (Trade Product) in Australia, while UNII provides
a non-proprietary, unambiguous and unique list of sub-
stances as maintained by the FDA. DrugBank provides
a rich taxonomy of drug information alongside compre-
hensive drug, gene and food interactions. The appeal for
our project is in the exploration of drug-drug interac-
tions that provide some insight into the potential risks and
contraindications associated with the intake of the medi-
cation. Furthermore, by exploiting the gene-drug interac-
tions of medication targets, we can extend our framework
to support the discovery of biomarkers. Finally, the avail-
ability of the food interactions will be useful when we
explore the association between the participants drug
intake and type and amount of food consumed. Both ATC
DDD and DrugBank provide a supplementary means to
query the data. The five-level ATC DDD taxonomy of
medications provides an additional mechanism for the
data to be categorised and offers the means to aggregate
the study data for statistical purposes. This is complemen-
tary to what is possible with the help of the vocabularies
provided by AMT.
Medication mapping is challenging due to the quality,
accuracy and completeness of the information. Previous
studies [8,18] have identified numerous inconsistencies
linked to the naming of the medications with a mix of
trade name, active ingredients and informal name used to
describe the prescribed medications.
The processing pipeline for mapping the medications
data to the selected medication terminologies is sum-
marised below. The medication records are extracted
from OpenClinica, at the start of the pipeline, as an XML
document in CDISC ODM format. A data cleaning pro-
cess is conducted to manually address the inconsistencies
described above. This is followed by four mapping phases.
In Phase 1, we attempt a map of the cleaned medica-
tion names to the Trade Productd (TP) concept in AMT.
We use the list of brand names compiled by ARTG to
assist us in this task. In Phase 2, we try to map the same
medications to the Medicinal Producte (MP) concept in
AMT.We use the DrugBank terms to boost the number of
mapped concepts. The third phase attempts a map to the
substances (active ingredients) either entered by the par-
ticipants or contained within the medications recorded.
To this end, we use the list provided by UNII or the
Medicinal Substancef (MS) defined in AMT. In Phase 4,
we map the medications to the ATC DDD classification
hierarchy by taking advantage of the existing mapping
between the various terminologies (e.g. DrugBank and
ATC DDD). We have thus compiled a linked medications
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 7 of 14
data set that links AMT, DrugBank, ARTG, ATCDDD and
UNII with one another as depicted in Figure 4.
Results
The result of mapping the AIBL medications data to
the medication terminologies is illustrated in Table 2.
The first row discloses the total number of medications
extracted. The second row represents the mappings to
either a Medicinal Product, a Trade Product or a Sub-
stance in AMT. The third, fourth and fifth rows provide
the mapping count for these AMT concepts individually.
The Linked Clinical Data Cube has been evaluated using
the full AIBL data set to demonstrate its potential in
formulating queries across the broad spectrum of tests
and the categories within the clinical study. While simple
queries can be answered using a single data cube, more
complex queries need data from several cubes to be avail-
able. The clinical data is formalised into RDF prior to
being loaded in a Virtuoso triple-store.
SPARQL Queries
To demonstrate the utility of the LCDC, we have devised a
set of three questions that are typical of the questions that
the AIBL researchers are likely to ask of the study data.
We provide, below, a listing of the three queries. However,
due to privacy constraints, we have structured our queries
so that they only return aggregated counts because we are
unable to present the participants unique identifier as part
of the results of the queries.
Those SPARQL queries have been chosen in order to
demonstrate the breadth and depth of questions that may
be asked on the data set. They demonstrate how data from
Table 2 Medications mapping statistics
Mapped Count Percentage
Total 7942 100.00%
Medicinal product/trade product/substance 5536 69.71%
Trade product 5518 69.48%
Medicinal product 5266 66.31%
Substance 5382 67.77%
the AIBL study can be effortlessly combined with drug
information, for example, in order to facilitate queries that
answer questions based on drug classifications. Further-
more, we also demonstrate, through the integration of the
AIBL data set with terminologies from the CDISC stan-
dards, how the AIBL data set can be queried by using the
CDISC standardised terminology rather than the actual
test names used by the AIBL study. We believe that these
types of queries will drive the cross-study and cross-
domain benefits of the linked clinical data approaches
such as the LCDC.
Query 1: Using CDISC terms, find the number of participants
who have hypertension
Hypertension is defined as having systolic and diastolic
blood pressure readings above 140 and 90 respectively
(written as 140/90 mm Hg) [19], most of the time. This
query explores the use of the CDISC SDTM controlled
terminology to access the diastolic and systolic blood
pressure readings for participants in the AIBL study. It
allows the user to interchangeably use the variable name
from the AIBL study or from CDISC SDTM.
Figure 4 Linked Australian medications data set. Depicts the interlinking of the drug terminologies available, mostly, in Australia in order to
facilitate their navigation. For the sake of simplicity, all data item variables have been omitted from the Figure. The AMT concepts are depicted in
teal. The ATC DDD concepts are depicted in orange. UNII concepts are in light-blue while DrugBank concept is in light green and the ARTG
concept is in magenta. The Figure also introduces an xkos:ConceptAssociation predicate (depicted in yellow) to define many-to-many
relationships between amt:MedicinalProduct and artg:RegisteredMedicine concepts.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 8 of 14
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX lcdcobs: <http://purl.org/sstats/lcdc/def/obs#>
PREFIX cdiscvs: <http://purl.org/odm2rdf/sdtm/vs/def/cdiscvs#>
PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>
SELECT count(DISTINCT ?subject) as ?hypertension WHERE {
# find the AIBL variable corresponding to cdiscvs:systolicBP
?vs_sBP rdfs:subPropertyOf cdiscvs:systolicBloodPressure .
# find the AIBL variable corresponding to cdiscvs:diastolicBP
?vs_dBP rdfs:subPropertyOf cdiscvs:diastolicBloodPressure .
?observation rdf:type lcdcobs:Observation .
# get the observation for one AIBL subject
?observation lcdccore:subject ?subject .
# get the ?sysBP using cdisc vs alias
?observation ?vs_sBP ?sysBP .
?observation ?vs_dBP ?diasBP .
FILTER ((xsd:integer(?sysBP) > 140) && (xsd:integer(?diasBP) > 90) )
}
The query obtains the relevant test names from the ontology by performing a lookup of properties that are sub-
properties of the CDISC Vital Signs (prefix: vs) diastolic and systolic blood pressure variables. This is achieved by this
statement:
?vs_dBP rdfs:subPropertyOf cdiscvs:diastolicBloodPressure .
This query is possible because we have implemented a linked set that connects the variable name from the AIBL study
to the standardised terminology in CDISC SDTM vs domain as illustrated below.
aiblvitalsigns:diastolicBP
rdf:type owl:DatatypeProperty ;
rdfs:subPropertyOf cdiscvs:diastolicBloodPressure.
aiblvitalsigns:systolicBP
rdf:type owl:DatatypeProperty;
rdfs:subPropertyOf cdiscvs:systolicBloodPressure.
We believe that the use of linksets in this manner is important and useful because it adheres to the principles of
information hiding in that the user need not be aware of the exact wording of a variable. As long as the user knows the
corresponding standardised variable name, the user is able to successfully execute a query on the data set. We intend
to further develop this traceability mechanism with the help of the Provenance Ontology [20] to fully disclose how the
published data is derived from the originally captured data.
The result of Query 1 is displayed below:
hypertension
242
Query 2: Howmany participants are taking an anti-diabetic drug such asMetformin?
Some studies [21,22] have shown a possible link between type2 diabetes and early-stage AD. In this query, we retrieve
a list of anti-diabetic drugs to demonstrate the benefits of linking the patient-reported medications to standardised
external terminologies and the strength of the LCDC in using federated queries to facilitate cross-domain querying. The
first portion of this query obtains a list of anti-diabetic drugs from DrugBank (outlined in section A in the SPARQL).
The second part of the query utilises the mappings between the patient-reported medications and DrugBank entities to
link to the anti-diabetic drugs identified in section A.
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX drugbank: <http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource
/drugbank/>
PREFIX aiblmed: <http://aehrc-ci.it.csiro.au/aibl/lcdc/clinical/medication/def
/aibl-medication#>
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 9 of 14
PREFIX lcdcobs: <http://purl.org/sstats/lcdc/def/obs#>
PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>
PREFIX cm: <http://purl.org/sstats/lcdc/cm/def/cm#>
PREFIX amt: <http://nehta.gov/amt#>
SELECT count (distinct ?subject) as ?count ?mp_med
WHERE {
# Section A. find all instances of anti-diabetic drugs
SERVICE <http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql> {
# find the drug (?s) that has the name Metformin
?s drugbank:genericName "Metformin" .
# find the category of the drug (?s)
?s drugbank:drugCategory ?category .
# find all other instances of ?drug that has the same ?category
?drug drugbank:drugCategory ?category .
}
# Section B. find the participants who take the ?drug from A.
{ SELECT distinct ?drug ?med ?subject ?mp_med WHERE {
GRAPH <http://localhost/dataset/aibl/lcdc/clinical> {
# specifies that ?obs is an observation
?obs a lcdcobs:Observation .
# get the medicinal product code for this med
?obs cm:medicinalProduct ?cm_mp .
# lookup the drugbank entity linked to this MP
?cm_mp skos:exactMatch ?drug .
# get this drugs name
?cm_mp amt:synonym ?mp_med .
# find the participant associated with this observation
?obs lcdccore:subject ?subject .
}
} }
}
LIMIT 20
The linkset developed to map the AMT concepts to DrugBank has been inspired from the approach described in
[23,24] and uses the skos:exactMatch predicate.
<http://snomedtools.info/snomed/version/1/concept/rdfs/105271000036100>
rdf:type amt:MedicinalProduct ,
owl:NamedIndividual ;
skos:exactMatch
<http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugs/DB06655>.
The significance of this mapping is the provision of drug-drug, drug-gene and possibly drug-disease and gene-gene
information relating to the AIBL study to the researchers by fully utilising the links provided by DrugBank.
The result of Query 2 is displayed in Table 3:
Query 3: Are there participants whose classification has transitioned from healthy tomild cognitive impairment but whose
triglycerides level has remained normal?
Research has investigated the risk factors associated with low-density lipoproteins or triglycerides on the incidence and
progression of dementia and AD in later life [22]. With this in mind, we construct the query below to retrieve par-
ticipants records whose confirmed classification status have been updated from being healthy as subjective memory
complainer or non-memory complainer to having mild cognitive impairment but who have also maintained a normal
(< 1.7 mmol/L) level of triglycerides in their blood sample over the course of an 18-month period between the
baseline and 18-month time-points.
PREFIX qb: <http://purl.org/linked-data/cube#>
PREFIX lcdcsection: <http://purl.org/sstats/lcdc/def/cross-section#>
PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>
PREFIX aiblblood: <http://aehrc-ci.it.csiro.au/aibl/lcdc/clinical/blood/def
/aibl-blood#>
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 10 of 14
PREFIX aiblneuropsych: <http://aehrc-ci.it.csiro.au/aibl/lcdc/cognitive/neuropsych
/def/aibl-neuropsych#>
PREFIX aiblphase: <http://aehrc-ci.it.csiro.au/dataset/aibl/lcdc/id/phase/>
PREFIX aiblsubtheme: <http://aehrc-ci.it.csiro.au/dataset/aibl/lcdc/id/subtheme/>
SELECT DISTINCT count(?subject) as ?subjectCount ?class1 ?class2
WHERE {
{ select distinct ?subject ?obs1a ?trig1 where {
# retrieve the objects from the SubTheme cross-section slice
?nodeSect a lcdcsection:SubThemeSection .
# only get observations for the participants from the Blood domain
?nodeSect lcdcsection:subtheme aiblsubtheme:blood .
# get the observations from the slice
?nodeSect qb:observation ?obs1a .
# once we get the observations, get the subject
?obs1a lcdccore:subject ?subject .
# only select observations for the baseline phase
?obs1a lcdccore:phase aiblphase:baseline .
# get the triglycerides measurements
?obs1a aiblblood:trig ?trig1 .
} }
{ select distinct ?subject ?class1 ?obs1b where {
?nodeSect a lcdcsection:SubThemeSection .
# only get observations for the participants from the Neuropsych domain
?nodeSect lcdcsection:subtheme aiblsubtheme:neuropsych .
?nodeSect qb:observation ?obs1b .
?obs1b lcdccore:subject ?subject .
?obs1b lcdccore:phase aiblphase:baseline .
# get the subjects classifications
?obs1b aiblneuropsych:confirmedClassification ?class1 .
# only select healthy subjects
FILTER(?class1 =
aiblneuropsych:ConfirmClassification360-memoryComplainerHealthyControl
|| ?class1 =
aiblneuropsych:ConfirmClassification360-nonMemoryComplainerHealthyControl
)
} }
FILTER (xsd:float(?trig1) < 1.7)
{ select distinct ?subject ?obs2a ?trig2 where {
?nodeSect a lcdcsection:SubThemeSection .
?nodeSect lcdcsection:subtheme aiblsubtheme:blood .
?nodeSect qb:observation ?obs2a .
?obs2a lcdccore:subject ?subject .
# select observations for the 18-month phase
?obs2a lcdccore:phase aiblphase:18months .
?obs2a aiblblood:trig ?trig2 .
} }
{ select distinct ?subject ?class2 ?obs2b where {
?nodeSect a lcdcsection:SubThemeSection .
?nodeSect lcdcsection:subtheme aiblsubtheme:neuropsych .
?nodeSect qb:observation ?obs2b .
?obs2b lcdccore:subject ?subject .
?obs2b lcdccore:phase aiblphase:18months .
?obs2b aiblneuropsych:confirmedClassification ?class2 .
# ensure subject have transitioned to MCI
FILTER(?class2 = aiblneuropsych:ConfirmClassification360-mciPatient )
} }
# normal range for trig. is < 1.7 mmol/L
FILTER (xsd:float(?trig2) < 1.7)
}
The above query highlights the strength of the LCDC in facilitating cross-domain queries by fully exploiting the
potential of slices and observations within the specialised cubes. While the above query can be achieved without a data
cube, the use of slices and observations make the query more elegant and effective. It demonstrates the navigation of the
AIBL data set across two specialised cubes (Neuropsych and Blood) and four slices (two slices at each time points for
each cube). These are contained within the four observations (?obs1a, ?obs1b, ?obs2a, ?obs2b) within the above
query.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 11 of 14
Table 3 Participants taking anti-diabetic drugs
Count mp_med
1 Insulin glargine
3 Glimepiride
46 Metformin
4 Rosiglitazone
2 Glipizide
17 Gliclazide
4 Pioglitazone
1 Sitagliptin
The result of Query 3 is displayed in Table 4:
We provide an indication of the execution time of the
three queries in Table 5 below. These queries have been
executed on a Virtuoso 6.1 instance running on a vir-
tual machine with an AMDOpteron Processor 62xx CPU,
8GB of DDR3 RAM and running Ubuntu 13.04 LTS
(Raring Ringtail).
Discussion and related work
Our results demonstrate the effectiveness of integrating
semantic statistics vocabularies with the CDISC standards
in order to expedite the navigation and querying of
the data. Our contribution extends previous attempts
to semantically enrich biomedical research data using
ontologies [25] or linked data resources [26]. To the best
of our knowledge, no study has yet investigated the associ-
ation of semantic statistics vocabularies with clinical data
exchange standards. The design of the LCDCwas inspired
by the Translational Medicine Ontology [27] and our use
cases were motivated by similar objectives of providing
qualitative and pertinent clinical data to the researchers
and clinicians in the right format. This is what has driven
our resolve to split the data into onemain cube and several
specialised cubes corresponding to the various domains in
our study. The benefits of this approach are demonstrated
in the third query where data from two specialised cubes
are amalgamated to derive the results.
Observational clinical study data is patchy by nature,
mainly because of the various collection mechanisms
involved that often lead to information being inadver-
tently left out or inaccurately recorded. Furthermore, the
sheer volume of variables and the longitudinal nature of
the AIBL clinical study have given rise to an enormous vol-
ume of data that need to be analysed. This has led to the
second design decision that is to split the data into time-
series, cross-sections and themes in order to improve their
manageability during the generation process and facili-
tate their discovery and usability by end users. Moreover,
the addition of external standardised terminologies, such
as the CDISC standard terminologies and the various
drug vocabularies utilised, have contributed not only to
standardising the data and to removing ambiguities but
to enriching the data by providing links to relevant online
resources, such as genes and pathways definitions and
information about their interactions with the entities.
Challenges in the use of the CDISC standards as the
underlying model
While the CDISC models suit our immediate purpose,
they present a few shortcomings, mainly in relation to
the semantics associated with the clinical study data.
ODMs constrained hierarchical structure largely pro-
motes single-study explorations of clinical study data.
Furthermore, the inability to store domain information
alongside the user-defined data items in the customisable
CRFs is, in our view, very restrictive, thus impeding their
use outside of the study context [28]. However, this stems
more from the various failings in the implementation of
the CDISC standards by the vendors. The ODM stan-
dard allows for CDASH terms to be inserted through the
use of annotations within the ODM XML model. How-
ever, several vendors, such as OpenClinica, choose not to
offer this feature natively within their tool. Abler et al. [28]
make a passionate claim for the definition of a language of
forms that can effectively record the logical relationships
between questions or sets of questions asked in the forms.
On a more technical aspect, ODM also suffers from
a lack of established complex data type standards, thus
allowing a study coordinator to provide an alternative def-
inition for, say, the Physical Quantity data type. Further-
more, despite the provision of detailed Implementation
Guides describing the correct way of encoding data items,
the definition of very coarsely granulated meta-data cat-
egories, such as Medical History in SDTM, opens up the
possibility, for the user, to capture semantically identical
data in multiple domains. While the lack of data stan-
dards is a problem, the lack of mechanisms to enforce
adherence to these data standards is a greater problem. As
such, despite CDISC providing mechanisms, through its
SDTM and CDASHmodels, to define common semantics,
in our experience, very few study coordinators choose to
use them.
Our choice of the CDISC standard as the underlying
model for our architecture is influenced by three factors:
(i) since the FDA and other regulatory bodies mandate the
use of CDISC as the de facto standards for representing
and reporting clinical study data, a vast majority of the
clinical study data that we encounter is already in CDISC
format; (ii) several extensions to the CDISC standards
(such as the Therapeutic Area standard for Alzheimers
Disease) are appealing to us; and (iii) we have not yet found
a consistent and complete set of ontologies that we could
use instead.
In our approach to semantically enrich the clinical study
data, we need to address the study-specific nature of
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 12 of 14
Table 4 Participants classifications and triglycerides level
SubjectCount Class1 Class2
11 ConfirmClassification360- ConfirmClassification360-
memoryComplainerHealthyControl mciPatient
4 ConfirmClassification360- ConfirmClassification360-
nonMemoryComplainerHealthy mciPatient
Control
CDISC ODM datasets. We inherit many issues that have
been created in the previous steps of the data capture
chain, such as the use of user-defined questionnaires and
instruments that use their own language and the loss of
domain knowledge during the digitisation phase of the
data. Our solution is to reintroduce the loss of domain-
specific information by first trying to retrofit the study
variables to the SDTM and CDASH models, even though
they were not initially modelled that way. Concurrently,
we look to biomedical ontologies, such as the NCBO
Bioportal ontologies [29] and SNOMED CTg, to provide
alternative foundations for domain enrichment of the data
set. Several ontologies, in the context of clinical trials
[30-32], have been proposed recently and are partially
applicable to our needs. However, they do not adequately
cover the observational aspects that are required for our
data cubes. Furthermore, several of these ontologies have
a large number of dependencies to other ontologies that
do not meet our requirements. We overcome the limita-
tions related to the single-study nature of ODM by fitting
the study data to the RDF Data Cube. The introduction
of additional dimensions, through the integration to the
RDF Data Cube, opens up new access points to the data
through the use of the thematic slices.
Ultimately, our view is that regulatory bodies have a
pivotal role to play in encouraging the clinical study coor-
dinators to engage with data scientists at an earlier stage
in their clinical study to help with the design of their
study and associated artefacts. Too much emphasis is
placed on the data collection phase and not enough effort
is expended in clarifying what is needed to analyse the
data.
Related work
The Linked Open Drug Data (LODD) [33] and the Linked
Life Data (LLD) [34] projects provide additional resources
Table 5 Query performances
Query Execution time (msec)
1 22
2 36
3 270
that can be used to extend the Linked Clinical Data Cube.
Both projects aim to build a large scale knowledge cloud
that can be used for drug discovery. LODD federates the
efforts by participants of the W3C Semantic Web Health
Care and Life Sciences (HCLS) Interest group to con-
vert available resources into linked data. LLD provides
a semantic data integration platform for the biomedical
domain comprising many of the data sources belonging
to LODD. The resulting datasets contains more than 8
million triples representing the knowledge within over 2
millions links relating to medications, diseases, clinical
trials, gene information and pharmaceutical companies
among others. This was followed by efforts to convert
the ChEMBL database as linked open data [23]. This new
linked dataset combines the description of the biologi-
cal entities with links to Bio2RDF [35], ChemSpider [36],
OpenMoleculesRDF [37] and CrossRef [38] to allow deref-
erenceable access to a myriad of external datasets. We
have adopted a similar methodology in our approach to
map the medications specialised cube to AMT, DrugBank
and ATC DDD.
Among the various use cases reported via the W3C
HCLS Interest group are efforts to explore links to iden-
tify and verify genes linked to Alzheimers disease (AD).
Through the links between the drug, medications, dis-
ease and clinical trial repositories, we hope to leverage
on efforts by others to further explore the effects of pre-
scribed medications, for AD sufferers, on the various
genes comprising the pathways of interest. Other applica-
tions of LODD include the identification of potential side-
effects linked to the intake of drugs that have conflicting
stimuli on the disease pathways.
The SALUS project [39] is a former attempt to adapt
CDISC standards to build a Semantic Framework to
improve interoperability between clinical research and
clinical care domains. We adopt a similar approach to
them but their focus is on service mappings rather than
linked data sets. The Semantic Cockpit [40] project aims
to develop a data slicing framework comparable to what
we propose on the basis of the RDF Data Cube. The
goal of this project is to intelligently assist business
analysts by discriminating unimportant information and
using reasoning to only present useful information to the
analyst.
Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 13 of 14
The Linked Medical Data Access Control (LiMDAC)
project [41] has devised a framework to enable the inte-
gration of medical data without compromising its privacy,
security and integrity. It defines three linked data mod-
els that use the RDF Data Cube to build an access control
framework that restricts access to the aggregated data.
The Pharmaceutical Users Software Exchange [42] com-
munity, in concert with the FDA, has started work on RDF
representations of various CDISC models [43], includ-
ing the terminologies published by the National Cancer
Institute (NCI) Enterprise Vocabulary Services [44]. This
community has started to evaluate the RDF Data Cube
[45,46] for the publication of clinical study data. These
conversions of comma-separated-value files, however, do
not fully exploit the relationships between the data and
metadata structures embedded within the XML versions
of the CDISC standards and the patterns and concept
definitions included in the generated RDF content.
Conclusions
This paper has outlined the semantic enrichment of longi-
tudinal clinical study data based on the CDISC standards
with elements from the semantic statistics vocabularies,
namely the RDF Data Cube and the DDI-RDF Discov-
ery vocabularies. We have outlined how the Health Care
and Life Science community is likely to benefit from the
adoption of tools and techniques that will deliver clini-
cal data as linked data and advance its integration with
complementary data sources. In this regard, we have
proposed a Linked Clinical Data Cube, which integrates
one main and several specialised data cubes to provide
increased flexibility in the navigation of the clinical data
and allow the users to formulate the queries more effi-
ciently and effectively. The Linked Clinical Data Cube
combines the strength of the RDF Data Cube in defining
multi-dimensional data cubes and the DDI-RDF Discov-
ery vocabulary in encoding the CDISC metadata and
the study specific data dictionary as linked data. Our
approach was validated using data captured as part of
a longitudinal clinical study into neurodegenerative dis-
eases. This research has resulted in four contributions.
First, we have uncovered the complementarities of the
RDF Data Cube and DDI-RDF Discovery vocabularies for
the publication of large heterogeneous data sets as linked
data. Second, we have demonstrated the fit of the seman-
tic statistics vocabularies to enrich the CDISC ODM data
model for the publication of clinical study data as linked
data. Third, we have illustrated how the clinical study
data has been semantically enriched with links to external
resources and how they ultimately improve the navigation
and querying of the data. Fourth, we have built the foun-
dations of a framework supporting cross-domains and
cross-study analysis by adopting a more standardised data
structure. Our next step is to enrich the remaining study
data set with concepts from other domain ontologies,
such as Blood, Neuropsychological tests and Nutrition, to
name just three.
Endnotes
aAnatomical Therapeutic Chemical Defined Daily Dose.
bAustralian Register of Therapeutic Goods.
cUnique Ingredient Identifier.
d30560011000036108 | trade product |.
e30497011000036103 | medicinal
product|.
f30388011000036105 | medicinal
substance |.
gSystematized Nomenclature of Medicine Clinical
Terms.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
HL drafted the manuscript. HL and LL modelled the LCDC that LL
implemented. LL revised the manuscript. Both authors read and approved the
final manuscript.
Acknowledgements
The authors would like to express their gratitude to Drs Alejandro Metke and
Michael Lawley for their assistance in scoping the Medications case study and
along with Dr Bevan Koopman and Mr Simon McBride for reviewing the paper
and to Mr Simon Gibson and Mr Louis Delachat for their assistance in the
project.
Author details
1The Australian e-Health Research Centre, Digital Productivity Flagship, CSIRO,
Level 5 - UQ Health Sciences Building 901/16, Brisbane, Queensland 4029,
Australia. 2Digital Economy Program, Digital Productivity Flagship, CSIRO,
Canberra, ACT 2601, Australia.
Received: 8 August 2014 Accepted: 5 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 
DOI 10.1186/s13326-015-0009-1RESEARCH ARTICLE Open AccessExpanding the mammalian phenotype ontology
to support automated exchange of high
throughput mouse phenotyping data generated
by large-scale mouse knockout screens
Cynthia L Smith and Janan T Eppig*Abstract
Background: A vast array of data is about to emerge from the large scale high-throughput mouse knockout
phenotyping projects worldwide. It is critical that this information is captured in a standardized manner, made
accessible, and is fully integrated with other phenotype data sets for comprehensive querying and analysis across
all phenotype data types. The volume of data generated by the high-throughput phenotyping screens is expected
to grow exponentially, thus, automated methods and standards to exchange phenotype data are required.
Results: The IMPC (International Mouse Phenotyping Consortium) is using the Mammalian Phenotype (MP)
ontology in the automated annotation of phenodeviant data from high throughput phenotyping screens. 287
new term additions with additional hierarchy revisions were made in multiple branches of the MP ontology to
accurately describe the results generated by these high throughput screens.
Conclusions: Because these large scale phenotyping data sets will be reported using the MP as the common data
standard for annotation and data exchange, automated importation of these data to MGI (Mouse Genome Informatics)
and other resources is possible without curatorial effort. Maximum biomedical value of these mutant mice will come
from integrating primary high-throughput phenotyping data with secondary, comprehensive phenotypic analyses
combined with published phenotype details on these and related mutants at MGI and other resources.
Keywords: Phenotype, Ontology, Mouse, Data integration, DatabaseBackground
The accessibility of the mouse genome to genetic ma-
nipulation, biochemical and molecular experimentation,
and the availability of its full genomic sequence has
made the mouse indispensable in modeling human dis-
eases and complex syndromes arising from various eti-
ologies. A myriad of approaches have been taken to
create mutations in the mouse genome that mimic those
in human disorders. Forward genetics mutagenesis pro-
jects using various inducers (e.g., ENU, transposons)
have been and continue to be executed (Mutagenetix,
Australian Phenome Bank, etc. (reviewed in [1]). Many
of these screens are designed to look for deviants in one* Correspondence: janan.eppig@jax.org
Mouse Genome Informatics, The Jackson Laboratory, Bar Harbor, ME 04609,
USA
© 2015 Smith and Eppig; licensee BioMed Cen
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.or two specific phenotype areas, such as congenital heart
defects or neurobehavioral abnormalities. Once a pheno-
deviant is identified, mapping or sequencing studies aid
in identifying the molecular mutation. More recently,
large-scale gene targeted knockout screens have been
designed to analyze the phenotypic consequences of mu-
tating each protein-coding gene in mouse (International
Mouse Phenotyping Consortium, IMPC) [2]. Unlike pre-
vious induced mutation screens, these phenotyping pipe-
lines are designed to systematically screen every mutant
mouse line for defects in a wide array of physiological
systems. Because the gene mutation is already identified,
these phenotype data can be integrated immediately with
other information known about the genes function, ex-
pression and biological pathways.tral. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Table 1 MP terms assigned to IMPC parameters, by
systems
System Terms assigned New terms
Adipose tissue 6 3
Behavior/neurological 85 15
Cardiovascular system 59 9
Craniofacial 39 1
Digestive/alimentary 6 10
Embryogenesis 3 0
Endocrine/exocrine gland 13 3
Growth/size/body 16 3
Hearing/vestibular/ear 18 3
Hematopoietic system 82 25
Homeostasis/metabolism 216 129
Immune system 67 24
Integument 55 9
Limbs/digits/tail 42 6
Liver/biliary system 1 1
Mortality/aging 10 7
Muscle 5 0
Nervous system 5 4
Pigmentation 13 0
Renal/urinary system 6 2
Reproductive system 25 4
Respiratory system 9 4
Skeleton 70 11
Taste/olfaction 1 0
Vision/eye 55 14
MP terms used in annotations in postnatal tests in IMPC as of 10/10/2014.
Note: the total number in the second column is more than 752; this is due to
terms assigned to multiple systems, such as abnormal testis morphology
[MP:0001146], which occurs in both the endocrine/exocrine gland and
reproductive systems headings. Some new terms were added during the
Europhenome and Sanger Institutes Mouse Resource Portal pilot phenotyping
projects; others were added recently to describe IMPC pipeline parameters.
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 2 of 7The Mammalian Phenotype (MP) ontology [3] is a con-
trolled vocabulary that has been used at Mouse Genome
Informatics (MGI) to annotate phenotype data from large-
scale data sets, including mouse mutagenesis screens, and
from data described in published literature. The MP
ontology was first developed by iterative additions as cura-
tors required terms to describe published and imported
phenotype data sets, then later by additions and improve-
ments made via specific review with subject matter ex-
perts covering targeted areas of the ontology. Recently, we
undertook to add and revise many areas of the ontology
simultaneously to accommodate consistent reporting from
high-throughput data pipelines and support automated
data exchange with the IMPC, MGI and other resources.
Methods
Ontology editing and files
The Mammalian Phenotype Ontology in OWL format is
maintained and edited using Protégé-4.3 software.
Ontology files are available in OWL and converted OBO
formats from the MGI ftp site [4].
Retrieval of MGI data
Data in MGI version 5.20 were retrieved from the public
website update posted on 10/21/2014 at MGI [5] or via
MouseMine [6].
Results and discussion
Expanding and using the mammalian phenotype ontology
to annotate high-throughput mouse phenotype data
MP is used as a data standard to annotate published and
large scale mouse phenotype data sets [1]. MGI and the
Rat Genome Database [7] incorporate this tool to aid in
organizing, and analyzing data sets. Unlike other previ-
ously imported phenotype data sets to MGI, which re-
quired curator intervention to annotate or translate to
the MP ontology standard, the high throughput mouse
phenotyping pilot projects such as Europhenome [8] and
the Sanger Mouse Genetics Project (MGP) [9] are using
the MP to annotate data sets directly and the IMPC also
has adopted this standard [10]. These large-scale pheno-
typing projects use a standard series of phenotyping pa-
rameters called pipelines (described in detail at IMPC/
IMPReSS Pipelines [11]). The IMPC core phenotyping
pipeline includes the minimum required phenotype pa-
rameters that have been agreed by all IMPC participating
research groups. A minimum of seven male and seven fe-
male mice at ages of 916 weeks are subjected to a battery
of mandatory tests with some centers performing added
optional tests. Performing these tests and reporting result-
ing phenotype data in a standardized way allows data to
be compared and shared not only among mouse pheno-
typing centers, but also relative to other annotated pub-
lished data and contributed data sets.The accurate description of phenodeviant test results
in the IMPReSS pipelines required the addition of 287
new MP terms as of 10/10/2014 (Table 1). New terms
were added in multiple systems, with the majority of the
new terms (216) assigned in the homeostasis/metabol-
ism section to describe results of specific blood clinical
chemistry tests. For example, in Protocol FRUCTOSA-
MINE IMPC_CBC_020_001 [12] the ?mol/l of fructosa-
mine in the blood at 16 weeks of age is measured in one
test. This test is used to evaluate the long-term average
amount of glucose in blood, and deviations may indicate
a problem with regulation of glucose homeostasis. A sta-
tistically significant increase is assigned the newly cre-
ated MP term increased circulating fructosamine level
[MP:0010087] and a decrease is assigned decreased
circulating fructosamine level [MP:0010088]. Existing
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 3 of 7MGI annotations to mutant phenotypes were also updated
to use these newly created terms, when appropriate. Other
sections of the ontology requiring significant new terms
included the immune system, the hematopoietic system
and behavior, suggesting that these systems should be sub-
ject to further expert review for completeness.
However, recently reviewed sections of the ontology re-
quired fewer additional new terms. For example, the car-
diovascular system was recently revised to support the
phenotype descriptions of the ENU mutations generated
by the Cardiovascular Development Consortium (CvDC)
(C. Lo, manuscript submitted). Only 9 additional terms
were required to support the IMPC data. Likewise, terms
previously requested from members of the FaceBase con-
sortium [13] resulted in good coverage of craniofacial
terms, requiring only one new additional term for IMPC
in this section.
Many of the new terms created during this revision are
now being used in the IMPC tests and in existing MGI
mouse phenotype annotations from literature and other
resources. MGI phenotype annotations are updated when
new terms are added.
Existing ontology structures also were reviewed for con-
tent coverage and organization. For example, the term ab-
normal adaptive thermogenesis [MP:0011019] was added
as a sibling term to both abnormal body temperature
[MP:0005535] and abnormal body temperature homeosta-
sis [MP:0001777]. abnormal adaptive thermogenesis
became the parent of the new terms describing stress-
induced hyperthermia responses. Recently, new terms
covering abnormal alpha-beta T cell morphology
[MP:0012762] and abnormal alpha-beta T cell number
[MP:0012763] were added, which organized together
the terms describing CD4- and CD8-positive alpha-beta
intraepithelial, memory, cytotoxic and regulatory T cells
used by the consortium.
Assignment of MP terms to results of high throughput
pipelines
IMPReSS [14] is a database and web portal developed to
track phenotyping procedures used by the phenotyping
centers of the IMPC. Users can search for phenotype tests
such as Lens Opacity [IMPC_EYE_017_001] [15] that as-
sess a phenotype of interest, e.g., cataracts [MP:0001304].
The definition and assignment of these ontology terms is
captured in IMPReSS at the level of each parameter and
has been developed collaboratively by the data wranglers
(scientific support staff charged with assisting centers in
data capture and download), the phenotyping centers, and
ontology developers. For some parameters, the assignment
of phenotype terms by data wranglers of the IMPC was
straightforward and did not require further discussion
with ontology developers. For example, the significant
test results for Heart Weight [IMPC_HWT_001] will beassigned to the MP terms abnormal heart weight
[MP:0004857], increased heart weight [MP:0002833]
and decreased heart weight [MP:0002834]. For many pa-
rameters, a new MP term was requested by data wran-
glers, but the term assignment was also unambiguous.
Examples include many clinical chemistry terms such as
abnormal circulating lipase level [MP:0011885] and sub-
classes, abnormal circulating ferritin level [MP:0011889]
and subclasses or increased circulating magnesium level
[MP:0010092]. For several terms, clarification of a text def-
inition, or a split of concepts was required. The ontology
developer created the new terms abnormal fluid intake
[MP:0011947], increased fluid intake [MP:0011941] and
decreased fluid intake [MP:0011941] to be used in mul-
tiple IMPC parameters, in order to distinguish this pheno-
type from terms used to describe drinking frequency and
other consumption behaviors, for which text definitions
were also revised for clarity. Finally, for a subset of parame-
ters, a new term(s) assignment was suggested and created
by the ontology developer to describe the results of a test.
Such terms include abnormal bronchoconstrictive re-
sponse [MP:0012123] and subclasses, which were recom-
mended for annotation of results in the Enhanced pause
(Penh) [ICS_CHL_003_001] plethysmography test that
measures response to provocation challenge with antigens/
allergens.
752 MP terms have been assigned to protocols in the
IMPReSS database as of 10/10/2014, but final assign-
ments/protocols remain under review (Table 1). Existing
MGI phenotype annotations were revised to use the
newly created terms, when appropriate. However, with
some terms, we did not find.Use of MP ontology at IMPC
The IMPC web interface at the European Bioinformatics
Institute (EBI) [16] allows searching and browsing for
phenodeviant data using MP terms. For example, select-
ing the term cardiovascular system phenotype from
the phenotypes menu returns a page with the term,
definition, all pipeline procedures associated with a car-
diovascular system term and all gene variants with car-
diovascular system phenotype [17]. Search results may
be further refined using available filters. More specific
cardiovascular terms, e.g., abnormal heart weight can
be selected and phenotype data associated with this term
may be viewed.
To download and work with large data sets, the
phenotype data and MP calls are made available by EBI
at the IMPC RESTfulAPI [18]. MP terms associated to
the different mutant genotypes may be retrieved in con-
junction with the phenotyping center, pipeline, pheno-
typing procedure, gene symbol, allele symbol, strain
name, or any combination of these parameters [2]. MGI
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 4 of 7uses this interface to retrieve data sets for importation
and integration with other MGI data.
MP expansion to accommodate new IMPC prenatal
screens
Identifying genes that are essential during development
is required to understand the many processes driving di-
rected prenatal growth, differentiation and organogen-
esis. Mutations in such genes also can help identify
origins of developmental disease and congenital defects.
Data currently in MGI suggest that approximately 27%
(2669/10014) of genes have at least one knockout allele
made into mice that exhibits a prenatal or perinatal le-
thal phenotype (Table 2).
To study the large number of homozygous knockout
strains generated by the IMPC expected to exhibit a pre-
natal lethal phenotype, a phenotyping pipeline for the in-
vestigation of embryonic lethal knockout lines is being
developed. A series of prenatal screenings, lethality sta-
ging, gross morphology, and histopathology tests are be-
ing discussed by the IMPC to decide upon a logical
testing order and to identify additional MP terms spe-
cific to these tests [19].
Some tests will require the addition of new MP terms.
For example, new early lethality terms may be needed.
Existing terms cover windows commonly seen in pub-
lished literature and can correspond to broad time frames
(e.g. prenatal) or to narrow time points (e.g. implant-
ation) (Figure 1). The IMPC centers collectively have
chosen four specific prenatal points for lethality analysis,
but not all centers are analyzing each time point. New
terms describing embryonic lethality prior to organogen-
esis (approximately mouse E9.5), embryonic lethality
prior to tooth bud stage (approximately mouse E12-12.5),
and prenatal lethality prior to heart atrial septation (ap-
proximately mouse E14.5-E15.5) have been added and
placed in the hierarchy in relationship to the existing terms
to cover mouse lines that are not viable at this stage. Add-
itional terms are under discussion. As additional homozy-
gous lethal lines are analyzed, it is possible to identify thoseTable 2 Mouse genes with mutations causing pre- or perinata
Genes with lethality
annotation
Prenatal lethality 2017
Perinatal lethality 1076
Both pre- and perinatal lethality 424
Total unique objects 2669
Ratio of total objects annotated 2669/10014
Numbers of mouse genes and alleles involved in genotypes annotated to prenatal
the number of genes with at least one allele in a genotype annotated to a prenatal
alleles annotated to either term set and some genes have one allele annotated to b
differences or the nature of the mutant allele. The second column lists the total nu
perinatal lethality term. The third column lists the number of genes with additionalthat exhibit lethality at E12.5 but viability at E9.5; the win-
dow of lethality is somewhere between E9.5 and E12.5.
Other centers will only test the E12.5 time point, so a term
describing lethality prior to E12.5 may be needed since the
E9.5 time point will not be analyzed in this case. There will
be more variations of these developmental time windows
depending on the testing pipelines finally agreed upon.
The developers of the recently described Drosophila
Phenotype Ontology (DPO) [20] have constructed lethal-
ity and partial lethality terms for recording and reasoning
about the timing of death in populations. The approach
taken by the DPO combines the terms lethal and par-
tially lethal - majority die with a set of terms for life
stages from the Drosophila temporal stage ontology using
formal semantics in OWL. After reasoning, the resulting
list forms a nested classification.
For mouse, there exists defined prenatal stage classifi-
cations based on Theiler stages or time from plug after
mating, but these as well as postnatal stages are not for-
malized into a separate comprehensive stage ontology
and would be required for considering this approach.
Most mouse researchers use embryonic day terminology
and not Theiler stages when describing the time of pre-
natal lethality in mouse in published literature. Further
complications to this approach are the significant varia-
tions among different mouse inbred strains in their aver-
age gestational periods (e.g. 18.75 days in FVB/NJ and
20.5 days in A/J, [21]). Thus the MP uses developmental
hallmarks to describe developmental stages, such as im-
plantation and organogenesis, adding text definitions
suggesting an average prenatal age. In addition to the pre-
natal lethality stage terms, the MP ontology contains le-
thality terms describing neonatal lethality, early postnatal
lethality and lethality at juvenile stages. A temporal stage
ontology for mouse using these developmental and post-
natal hallmarks would need to be created for such an ap-
proach to be feasible for formal definitions within the MP
ontology, as well as relating these stages to other species.
To anticipate the need for new MP terms in gross
morphology and prenatal histopathology, we are proactivelyl lethality
Alleles with lethality
annotation
Genes with lethality annotation and
postnatal disease annotation
4393 611
2304 534
589 322
6108 823
6697/26894 823/10014
or perinatal lethality MP terms in MGI as of 10/21/2014. The first column lists
lethality MP term or a perinatal lethality term. Some genes have multiple
oth term sets, possibly due to incomplete penetrance, genetic background
mber of alleles in a genotype annotated to a prenatal lethality MP term or a
disease annotations suggesting postnatal phenotypes.
Figure 1 Mouse prenatal lethality stages. Defined mouse prenatal stages incorporated in Mammalian Phenotype lethality terms and new time
points required to support IMPC prenatal screening (Not drawn to scale).
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 5 of 7reviewing and adding prenatal MP phenotype terms. New
terms covering embryonic pattern formation, gastrulation
and organogenesis. We have added over 189 new terms to
describe these mutations with greater precision. For ex-
ample, new terms describing abnormal cardiac or cranial
neural crest cell morphology, migration, proliferation, dif-
ferentiation and apoptosis have been added. Terms describ-
ing abnormalities in embryonic neuroepithelium were
added. For many other terms, the definitions and synonyms
have been updated to include greater detail, including terms
describing neural tube defects, neuropore defects and spina
bifida.
The embryogenesis section of the MP has been slightly
reorganized, with many new and existing terms moved and
grouped such as abnormal gastrulation [MP:0001695]
now placed under abnormal developmental patterning
[MP:0002084] in the hierarchy, or the new term abnormal
morula morphology [MP:0012058] placed under abnor-
mal preimplantation embryo development [MP:0012103].
In addition to defects of the embryo proper, prenatal
lethality may also be due to an indirect result of placen-
tal defects. IMPC prenatal screens are also developing
tests to distinguish the case in which a placental insuffi-
ciency is responsible for lethality. MGI data (retrieved
10/24/2014) includes 356 genes with 593 alleles anno-
tated with terms covering both placental defects and pre
-or perinatal lethality. Such mutations may be subject to
additional conditional mutation analysis or tetraploid
rescue experiments to determine the effects of the muta-
tion on embryonic or adult tissue in absence of placenta
defects. We added 27 new placenta related terms to the
MP to describe the results of the placenta analysis, for
example placenta necrosis [MP:0013247].We will continue to refine and expand the embryogen-
esis and placenta sections of the ontology, as required
for reporting the data generated during the IMPC pre-
natal phenotype screening.
Importation of IMPC phenotype data and integration with
MGI data sets
The IMPC provides a RESTful interface to mouse alleles,
experimental results and genotypephenotype associa-
tions determined by statistical analysis [2]. Phenotyping
data were released starting in June, 2014. These data will
be retrieved automatically and integrated into all other
information in the MGI database. MGI has previously
incorporated high-throughput phenotyping data from
pilot projects including the EuroPhenome and Sanger
Mouse Genetics Project (MGP) pipelines (manuscript in
preparation) and new data from the IMPC will be
imported similarly. The inclusion of data from IMPC will
unify access to mouse phenotype data from many data re-
sources sets and from published data using the Mamma-
lian Phenotype terms as the unifying standard.
MGI will remain the source of global mouse phenotype
data integration from large and small scale data sets, con-
tributions and literature. Users will want to see the IMPC
knockout data, but also compare these data in context of
other types of mutations. Most human diseases are not
functional knock-out mutations, so to effectively model
human disease, phenotype data associated with all allele
types (e.g. induced point mutations (such as ENU), spon-
taneous mutations (some are recurring), in-dels, copy
number variants, conditional mutations, etc.) are required
for interspecies comparisons. Of the 3093 genes with an
allele annotated to pre- or perinatal lethal phenotypes,
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 6 of 7MGI data also includes postnatal disease data for 823 of
these genes (Table 2). For this set of genes, postnatal
annotations involved data from 1) conditional genotypes,
2) haploinsufficient or partially insufficient genotypes when
the homozygous knockout is lethal [22], 3) incomplete pre-
or perinatal lethality, 4) the influence of mouse genetic
background strain which can have dramatic effects on
mouse phenotype [23,24], and 5) additional alleles of the
gene that were not knockouts, but were small indels, point
mutations, etc. that caused altered expression or activity of
the gene product (e.g. hypomorphic and gain of function
mutations). An example of a gene with an allelic series
causing differing phenotypes is Fgfr2 (Figure 2). The
Fgfr2tm1Lni and the Fgfr2tm1.1Wrst functional targeted knock-
out mutations result in prenatal lethality. However, the
ENU-induced point mutation in Fgfr2m1Sgg results in a
mouse that models Crouzon syndrome. A targeted muta-
tion that introduces a different point mutation, Fgfr2tm1Ewj,
results in a mouse that models Apert Syndrome, and a tar-
geted mutation that knocks out only one isoform of Fgfr2,
Fgfr2tm1.1Dsn, results in a mouse that models Multiple In-
testinal Atresia.Figure 2 Allelic series for mouse Fgfr2 gene shows range of phenoty
twenty-seven known alleles of Fgfr2 that exist in mice. Different mutations
the homozygous and heterozygous states. An additional eighty-eight muta
also known.Conclusions
We describe an expansion of the Mammalian Phenotype
Ontology to support phenotype annotation of data gen-
erated during high-throughput phenotype screens in
mice. Unlike previous phenotyping projects, we have
worked with the IMPC and the pilot projects of the Wel-
come Trust Sanger Institute and Europhenome projects to
create and assign phenotype terms to phenodeviants when
the data sets are generated by these resources. This will
support automated loading of these data from the IMPC
to MGI and will also be interoperable with other database
resources and tools.
Previously imported small- and mid-scale mutagenesis
projects [1] used other system-specific vocabularies to
describe phenotypes or used text based phenotype de-
scriptions that required database curator intervention
and translation in order to import the phenotype data
into MGI using the Mammalian Phenotype Ontology
standard. The IMPC data will be loaded directly into
MGI and integrated immediately with all other allele
and data types to support knowledge discovery. Further-
more, the MP also is used by mouse repositories topes. Screenshot of MGI Allele Summary Page listing seven of the
in this gene result in a range of phenotypes and disease models in
tions that exist only in gene trapped or targeted ES cell lines are
Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 7 of 7enable searching and describing available mouse strains
and stocks that were originally generated for the high
throughput phenotyping screens. These include the Jackson
Laboratory Repository [25], the European Mouse Mutant
Archive [26], the Mutant Mouse Regional Resource Centers
[27], and the KOMP Repository [28] among others.
Abbreviations
IMPC: International Mouse Phenotyping Consortium; MP: Mammalian
phenotype; MGI: Mouse genome informatics; OWL: Ontology web language;
OBO: Open biomedical ontologies; RGD: Rat genome database; MGP: Mouse
Genetics Project, JAXMice, Jackson Laboratory Repository; IMPreSS: International
Mouse Phenotyping Resource of Standardised Screens; EMMA: European
mouse mutant archive; MMRRC: Mutant Mouse Regional Resource Centers,
KOMP, Knockout Mouse Repository.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
CS executed the ontology changes in coordination with IMPC, performed
the data analysis and drafted the manuscript. JT conceived of the study, and
participated in coordination with IMPC and helped to draft and edit the
manuscript. Both authors read and approved the final manuscript.
Acknowledgements
Anna Anagnostopolous has reviewed embryogenesis terms in the MP and
has made crucial recommendations for additions and revisions. Henrik
Westerberg and the data wranglers of the IMPC consortium have made
many requests for terms and have suggested revisions. We thank Susan Bello
for helpful comments on multiple versions of the manuscript.
Received: 3 November 2014 Accepted: 3 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 
DOI 10.1186/s13326-015-0015-3
RESEARCH ARTICLE Open Access
Formalizing biomedical concepts from textual
definitions
Alina Petrova1*, Yue Ma2, George Tsatsaronis1, Maria Kissa1, Felix Distel2, Franz Baader2
and Michael Schroeder1
Abstract
Background: Ontologies play a major role in life sciences, enabling a number of applications, from new data
integration to knowledge verification. SNOMED CT is a large medical ontology that is formally defined so that it
ensures global consistency and support of complex reasoning tasks. Most biomedical ontologies and taxonomies on
the other hand define concepts only textually, without the use of logic. Here, we investigate how to automatically
generate formal concept definitions from textual ones. We develop a method that uses machine learning in
combination with several types of lexical and semantic features and outputs formal definitions that follow the
structure of SNOMED CT concept definitions.
Results: We evaluate our method on three benchmarks and test both the underlying relation extraction component
as well as the overall quality of output concept definitions. In addition, we provide an analysis on the following
aspects: (1) How do definitions mined from the Web and literature differ from the ones mined from manually created
definitions, e.g., MESH? (2) How do different feature representations, e.g., the restrictions of relations domain and
range, impact on the generated definition quality?, (3) How do different machine learning algorithms compare to
each other for the task of formal definition generation?, and, (4) What is the influence of the learning data size to the
task? We discuss all of these settings in detail and show that the suggested approach can achieve success rates of
over 90%. In addition, the results show that the choice of corpora, lexical features, learning algorithm and data size do
not impact the performance as strongly as semantic types do. Semantic types limit the domain and range of a
predicted relation, and as long as relations domain and range pairs do not overlap, this information is most valuable
in formalizing textual definitions.
Conclusions: The analysis presented in this manuscript implies that automated methods can provide a valuable
contribution to the formalization of biomedical knowledge, thus paving the way for future applications that go
beyond retrieval and into complex reasoning. The method is implemented and accessible to the public from:
https://github.com/alifahsyamsiyah/learningDL.
Keywords: Formal definitions, Biomedical ontologies, Relation extraction, SNOMED CT, MeSH
Introduction
Research in the biomedical domain is characterized by
an exponential growth of the published scientific mate-
rials, e.g., articles, patents, datasets, technical reports.
Handling such a scale of information is a huge chal-
lenge, for the purpose of which multiple initiatives have
been launched in order to organize biomedical knowledge
*Correspondence: alina.v.petrova@gmail.com
1Biotechnology Center, Technische Universität Dresden, Dresden, Germany
Full list of author information is available at the end of the article
formally. The use of ontologies is one of the most promis-
ing key aspects in this direction that has attracted a lot
of interest [1]. An ontology is a complex formal struc-
ture that can be decomposed into a set of logical axioms
that state different relations between formal concepts.
Together the axiomsmodel the state of affairs in a domain.
With the advances in Description Logics (DL), the process
of designing, implementing and maintaining large-scale
ontologies has been considerably facilitated [2]. In fact,
DL has become the most widely used formalism underly-
ing ontologies. Several well-known biomedical ontologies,
such as GALEN [3] or SNOMED CT are based on DL.
© 2015 Petrova et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 2 of 17
With regards to the potential benefits of formal knowl-
edge in biomedical research, there exist already two
examples where such a formalization has helped towards
knowledge discovery. In the first case, a formal ontol-
ogy about human anatomy with over 70,000 concepts
(FMA) is used to infer potential internal and hidden
injuries from injuries that are visible in images [4]. In
the second case, the authors show how reasoning over
yeast metabolism can generate novel hypotheses [5]. The
necessary background knowledge and reasoning frame-
work form a crucial part of a robot scientist , which
autonomously executes and evaluates experiments in
yeast. Thus, formalizing biomedical knowledge can assist
important biomedical applications. However, the problem
of formalizing the knowledge in the domain is an open
problem, since most biomedical ontologies and vocabu-
laries, such as GO, MeSH, OBO, define concepts only
informally by text strings. Hence, the main problem is to
convert textual definitions into formal representations.
A key step in formalizing knowledge in biomedical
domain is to extract formal definitions for biomedi-
cal concepts. As explained in [6], concepts are for-
mally defined in terms of their relationships with other
concepts. These logical definitions give explicit mean-
ing which a computer can process and query on . In
Table 1, the second row gives an example of a formal
definition of a concept Baritosis. The definition reads
as follows: Baritosis is a sort of Pneumoconiosis and
relates to another concept Barium_dust via the relation
Causative_agent . The first row of Table 1 is the corre-
sponding textual definition. Indeed, almost all of the exist-
ing biomedical formal ontologies, such as OpenGalen,
SNOMED CT, FMA, contain only such kind of formal
knowledge due to the essence of real practice, though
DL theory allows for more expressive representation (e.g.
General Concept Inclusions [2]). Thus, in this paper, we
focus on learning formal definitions of concepts.
Unlike the taxonomy acquisition which seeks to iden-
tify parent-child relations in text and is usually based on
simple patterns [7], definition generation typically focuses
on highly expressive axioms containing various logical
connectives and non-taxonomic relation instances. In
Figure 1, a simple example illustrates the problem of for-
mal definition generation from unstructured text, along
with its important aspects. The figure outlines a typi-
cal text mining workflow based on supervised machine
learning: data acquisition, feature extraction, training and
Table 1 Textual and formal definitions of Baritosis
Textual definition Baritosis is a benign type of pneumoconiosis, which
is caused by long-term exposure to barium dust.
Formal definition Baritosis  Pneumoconiosis
?Causative_agent.Barium_dust
testing. The workflow is adapted to the task of formal
definition generation and contains steps, resources and
intermediate states that are needed to extract the formal
definition of Baritosis from its textual definition.
The workflow is not restricted to such textual resources
as Web articles or MeSH entries. Input textual defi-
nitions can be retrieved from a number of resources,
such as encyclopedias and terminologies, PubMed, plug-
ins to known ontology editors (e.g., Dog4Dag [8], that can
retrieve textual definitions from the web) and, in principle,
any resource that contains textual information relevant for
the domain.
The proposed workflow does not fully solve the prob-
lem of automatic formal definition generation. However,
is seeks to formalize biomedical knowledge in a way that is
well established by the life sciences community, i.e., using
the same representation as in the SNOMED CT ontology,
namely a description logic EL++ [2]. At the heart of this
representation lie the relations that are intersected and
existentially quantified. Hence, taxonomic and especially
non-taxonomic relation extraction form a very important
part of our work. Relation extraction is integrated into a
bigger end-to-end pipeline that takes as input biomedical
texts and outputs description logic axioms that formalize
the information in these texts.
For example, Figure 1 depicts a series of steps, com-
prising the annotation of the sentence with concepts from
a designated ontology, the representation of this textual
definition in a feature space and the application of a
trained model, e.g., classifier, that has learned to recog-
nize roles (relations) between biomedical concepts, can
lead to the final desired output, which is the formal defini-
tion of Baritosis. However, there are three main aspects
which comprise the focus of this work and which can give
insightful directions on how the task may be addressed
efficiently: (a) the modeling of the problem, i.e., the selec-
tion of the corpora and the relations that may participate
in the formal definitions, (b) the feature engineering, and,
(c) the actual machine learning process. Aspect (a) is
examined in a setup where the input unstructured text
is annotated and then aligned with knowledge about the
chosen relations. The analysis of this aspect can illus-
trate how the definitions mined from different types of
corpora influence the final outcome. Aspect (b) aims at
examining the importance of different feature types in the
learning process. Finally, aspect (c) is meant to provide an
insight on the impact that different learning algorithms
have, as well as on the number of training examples that
are needed per role from the learning process.
Related work
We start the discussion of the related work with relation
extraction. Relation extraction (RE) is the task of detect-
ing and classifying semantic relations that hold between
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 3 of 17
Figure 1 Overview of the main aspects related to automated extraction of formal concepts definitions, via a simple example of the definition of
Baritosis. The figure illustrates an established text mining workflow based on supervised machine learning to address the task. In this work we
analyze the impact to the overall performance of the different aspects, namely: modeling (selection of corpora and relations set), feature
engineering (selection of lexical and semantic features) and machine learning (selection of classifiers and number of training examples).
different entities. While it can be performed on both
structured and unstructured data, our interest is focused
on relation extraction from text.
Relation extraction for general domain
Textual relation extraction can be performed using differ-
ent types of linguistic data that one can get from the input
text. The most common way is to use the lexical repre-
sentation of the text in order to generate typical patterns
for the target relations. The patterns can either be con-
structed manually [9], or can be leant automatically using
machine learning techniques [10].
Certain systems explore the syntactic structure of the
source text. The motivation behind it is that the seman-
tic relations that hold between the two concepts should
be reflected by syntactic dependencies of these concepts.
Learning by Reading system [11] extracts propositions
from syntactic structures of type Subject  Predicate 
Object. For the arguments of a relation, i.e. for subjects
and objects, the lexical items are generalized to classes
(the classes themselves are automatically derived from the
corpus). The predicates remain in their lexical form.
Some systems incorporate semantic information into
the extraction process. The entities and potential rela-
tion mentions that have been annotated in the text are
assigned more general semantic classes. If a combination
of semantic types of the argument concepts and the type
of the relation match a certain pattern (which is either
induced from an existing ontology, pre-defined manually
or appear with a high frequency), the underlying lexi-
cal relation is extracted. Flati et al. [12] extract semantic
predicates using semantic classes of argument concepts
adopted from Wikipedia. Dahab et al. [13] integrate top-
level ontologies to semantically parse the input text and
to generate semantic patterns of concepts and relations.
In the work by Hovy et al. [11] the semantic classes are
constructed by the system itself.
With regards to the type of learning that is performed
over relations, the task of extracting relations can be done
in a supervised way, in an unsupervised way, or in a semi-
supervised way, e.g. bootstrapping when an initial seed of
relation instances is used. Traditional relation extraction
encompasses supervised learning techniques. Mohamed
et al. [14] state that traditional RE requires the user to
specify information about the relations to be learned.
The information about the relations can be encoded in
two ways: (a) for every relation the set of correspond-
ing patterns is manually tailored; (b) relational instances
are annotated in the text corpus, and the patterns are
acquired explicitly (based on frequent sequences of word
tokens) or implicitly (using machine learning). The new
relational instances are extracted by pattern-matching or
by running a trained machine learning model over the
input texts. The supervised approach usually gives high
precision of the retrieved relation instances, which can go
over 90%. This makes supervised learning an ideal tech-
nique for tasks that incorporate relation extraction as part
of the pipeline and need the RE component to output
high-quality relations so that the error would not accu-
mulate throughout the pipeline. This is the reason why
our method of generating formal definitions is based on
supervised RE.
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 4 of 17
Semi-supervised learning of relations usually has a core
of annotated material from which the learning is initialed,
and then the process of extraction proceeds in an unsuper-
vised manner. It is ideal for situations when the training
data is few. For example, the NELL system [15] starts with
an initial ontology (a form of prior knowledge) that con-
tains some categories, relations and relational instances.
The ontology helps building the first set of patterns that
are then used to populate the categories of the ontology
and to extract new facts, which are then used to retrain
the extraction system and to learn yet new facts etc.
The main distinctive feature of unsupervised relation
extraction systems is that they do not use any assisting
information during learning: they are not provided with
the seed examples, or background expressive ontologies,
or manually constructed patterns. The learning is per-
formed purely from the input data [11]. One of the pop-
ular unsupervised RE approaches is the so-called Open
Information Extraction [16]. It is a domain-independent
paradigm that uses web-scale-size input corpora of texts.
It tends to extract as many triples as possible, but they
are not always well-formed or abstract. Both precision
and recall of unsupervised RE systems are lower that of
the supervised ones. Banko et al. [16] are the pioneers of
Open Information Extraction. Their system TextRunner
works in three steps. First, a deep linguistic analysis is
performed over a small corpus of texts. The system itself
separates the parsed triples into positive and negative
ones. The triples are used for training a machine learn-
ing RE model. Secondly, the model classifies the rest of
the corpus (millions of sentences) and extracts positive
triples. The extraction is done in one pass over the cor-
pus and does not involve the deep processing any more.
Lastly, newly extracted triples are assigned a confidence
score based on the frequency count of the triple. The sys-
tem is completely unsupervised, taking raw texts as input
and outputting relational triples. Unfortunately, only 1
million out of 11million high confident triples were evalu-
ated as concrete versus abstract, underspecified facts, e.g.
Einstein  derived  the Theory of Relativity versus
Einstein  derived  theory.
Biomedical relation extraction
The majority of research work on biomedical relation
extraction focus on the relations between specific concept
types: genes, proteins, diseases and drugs. Heterogeneous
pieces of information are mined from various textual
sources and assembled together in a form of ontologies,
semantic networks, knowledge bases or other knowledge
representation structures.
Relation extraction in biomedical domain adopts the
methodologies of the general relation extraction. One of
the most common approaches is to use lexico-syntactic
patterns. A set of relevant relations is manually designed
by domain experts, and every relation is assigned to a
set of textual patterns that are also constructed manu-
ally or extracted automatically from texts. Huang et al.
[17] extract protein-protein interactions using lexical pat-
terns. Patterns are mined through the dynamic alignment
of relevant sentences that mention the interaction. Both
the precision and the recall of the system reach 80%.
Xu and Wang [18] use simple pattern-based approach to
extract drug-disease relation instances from MEDLINE
abstracts. The patterns are not complicated (e.g. DRUG-
induced DISEASE), thus the approach exhibits a typical
bias towards high precision at the expense of low recall:
90% precision and 13% recall. However, the majority of
extracted instances do not yet exist in a structured way
in biomedical databases, which proves the usefulness of
the approach. The majority of work on pattern-based
relation extraction rely on hand-crafted templates whose
construction is a laborious task. In some cases the patterns
are built automatically, nevertheless the approach lacks
the ability to extract relations that are not explicitly stated
in the text, i.e. the relation is not properly mentioned by a
verb, a deverbative noun etc, or the two interlinked enti-
ties are located to far from each other in the text, and the
pattern cannot cover them.
Another common relation extraction approach uses
co-occurrence information. The idea behind it is quite
intuitive: entities occurring in the same sentence signifi-
cantly often should be related [19]. The drawback of the
approach lies in that the correlation information per se
cannot capture the type of relation present, i.e. what the
formal semantics of the relation is. However, it can effi-
ciently identify potential relations and relation instances
that may be examined with other NLP techniques after-
wards.
Alternative approach to extract biomedical relations is
to use machine learning techniques. Firstly, the source
text is annotated with biomedical concepts; secondly, sen-
tences or phrases are labeled with relations using external
knowledge resources, manual annotation or exploiting the
concept types. Finally, a model is trained to discriminate
between instances of different classes, i.e. relations. Airola
et al. [20] focus on protein-protein interaction extraction
and utilize graph kernel based learning algorithm the F
score of 56.4%. Chun et al. [21] focus on the extraction
of gene-disease relations from manually annotated MED-
LINE abstracts that describe either pathophysiology, or
therapeutic significance of a gene or the use of a gene as
a marker for possible diagnosis and disease risks. Incor-
porating an NER pre-filtering step for gene and disease
names the classification performance yields 78.5% preci-
sion and 87.1% recall. Machine learning appears to be a
potential approach of relation extraction which does not
require to do the tedious work of pattern construction and
is able to generalize.
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 5 of 17
Formalizing information in textual form
There are several works that attempt to convert textual
representation of general knowledge into a structured
form. One approach is described in [22]. The authors
focus on automatic acquisition of ontology axioms. The
formalism of choice is SHOIN, an expressive DL that
is able to model negation, conjunction, disjunction, and
quantitative restrictions. The developed system has sev-
eral limitations in formalizing the definitional sentences.
The majority of limitations stems from the use of hand-
crafted rules. In contrast, in our work we attempt to solve
this issue by applying machine learning techniques to
learn the models of axioms, as shown in Figure 1, which
avoid hand-craft patterns on the lexicon or the syntactic
structure of a sentence.
An additional related approach that falls into the broad
area of ontology acquisition is described in [23]. Given
that ontologies consist of terminological axioms (TBox)
and assertional facts (ABox), in this paper, we focus on
acquiring a special but common TBox knowledge, named
formal definitions, from texts. Existing TBox generation
approaches are mainly based on syntax-based transfor-
mation, but they suffer from the unresolved reference
relations (e.g., ?Of ) and the lexical variant problems
(e.g., Causative_agent relation in SNOMED CT can be
expressed both by caused by and due to). Our method is
designed to remedy these problems.
To the best of our knowledge, the is no system that
does automatic ontology acquisition or definition gener-
ation for biomedical concepts. However, in the domain
of life sciences there exist several works that move into
that direction. A work by R. J. Kate [24] presents the first
step towards automated generation of formal definitions
for concepts that are not yet present in SNOMED CT.
The task is to build a relation identification model that
is able to recognize a certain SNOMED CT relation in
text. The textual data used in [24] are the clinical phrases
from SNOMED CT that describe the concept in natural
language (e.g., acute gastric ulcer with perforation). A
separate machine learning classifier is trained for every
typed version of every SNOMED CT relation, e.g., find-
ing_site(disorder, body_structure) and finding_site(finding,
body_structure) yield two separate models. There are
three main drawbacks of this work. Firstly, it uses only the
data from SNOMED CT clinical phrases, which are for-
mulated in a controlled language. However, the ultimate
goal of the system is to be able to identify relations in
various medical texts for new biomedical concepts, and
these texts are not written in a controlled language. Sec-
ondly, the system builds a separate classifier for every
relation and its typed version. The resulting system has
to run hundreds of models every time a new text passage
is processed, which is computationally expensive. Lastly,
the work does not discuss how the outputs of multiple
classifiers should be combined into a single definition.
In our approach we deal with texts of different origin
and quality, and we incorporate the information about
semantic types of concepts involved in a relation into the
feature space instead of training separate classifiers for
every combination of concept types and relations.
Okumura et al. [25] automatically process textual
descriptions of clinical findings. Every description belong
to one of ten categories: anomaly, symptom, examination,
physiology etc. Based on the analysis of 161 descriptions,
every category was manually assigned a set of typical
semantic-syntactic patterns, e.g., a typical way of express-
ing a pathology is a pattern substance + verb phrase for
phenomenon, as in some fibrosis persisted. The study sug-
gests that there are common ways in which biomedical
knowledge is expressed in natural language. Our work
uses this finding as one of the motivations to use machine
learning techniques and to encode such patterns automat-
ically into models.
Dentler and Cornet [26] eliminate redundant elements
in already existing SNOMED CT definitions. Using the
ELK reasoner [27], the authors eliminated redundant con-
cepts, existential restrictions and rolegroups. Here is an
example of the elimination rule for concepts: if a concept
is more general or equivalent to one of the other concepts
in the definition of the same concept or a superconcept.
This work is highly relevant to the task of formal definition
generation, as it provides a method for post-processing
that can improve the quality of generated axioms and to
make the resulting ontology easier to maintain, construct
and expand.
Methods
Adding new concepts to a formal ontology is a tedious,
costly and error-prone process, that needs to be per-
formed manually by specially trained knowledge engi-
neers. By contrast, textual information from the medical
domain is widely available from publicly accessible
resources, such as the web, textbooks and PubMed arti-
cles. In the following we present ourmethodology towards
the automation of formalizing concept definitions from
textual ones.
Problem formulation
Relation instances form the basis of a concept defini-
tion; they contain necessary and sufficient information
about the taxonomic and non-taxonomic links between
the concept to be defined and the other concepts. Table 1
illustrates the connection between a textual definition and
its formal representation.
Existing approaches for relation extraction mostly focus
on learning superclass or subclass relations [8] (e.g.
Baritosis - is_a - Pneumoconiosis as given in Table 1),
leaving out the non-taxonomic relations (e.g. Baritosis -
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 6 of 17
caused_by - Barium_dust). However, the latter are essen-
tial for the task of formal definition generation. Existing
ontologies in the biomedical domain that contain non-
hierarchical relations have the following properties: (1) the
set of relations is much smaller than the set of concepts,
e.g., SNOMED CT currently has 56 roles, but more than
311,000 concepts, (2) the set of relations remains relatively
stable while the concept set is expanded and modified
much more often, and, (3) the set of relational instances,
i.e. unique semantic links between concepts, is much big-
ger than the set of relations, e.g., SNOMED CT has more
than 1,360,000 relationships.
The observations above suggest that if we are able to
extract a relatively small set of relation types, this will
result in many relational instances that may populate a
knowledge base. Thus, we formulate the problem tar-
geted by the present work as follows: create a system, that
for a given set of input texts annotated with biomedical
concepts is able: (a) to find text strings that describe a rela-
tionship between these concepts, and to recognize, which
relationship it is, and (b) to combine these relationship
instances into concept definitions. For example, for the
target concept Baritosiswe expect the system to recognize
two relations, Causative_agent and Finding_site, from the
following two sentences: (1) Baritosis is a benign type of
pneumoconiosis, which is caused by long-term exposure
to barium dust . (2) Baritosis is due to inorganic dust lies
in the lungs . The corresponding relational instances are:
Baritosis - Causative_agent - Barium_dust and Baritosis -
Finding_site - Lung_structure.
Terminology used
The current work is done on the border of two research
areas, namely Text Mining and Description Logic. This
section bridges the gap between the terminologies of the
two communities, giving equivalent terms to all notions
used in the paper.
 relation
In this work we interchangeably use the terms
relation, relationship and role. The last term comes
from the ontology development research, while the
first two terms are used when ontology generation is
addressed from the natural language processing
viewpoint.
 triple
A binary relation instance is often called a triple,
since it can be specified by the types of the relation
and the two arguments. In linguistic, a triple often
refers to a lexical representation of the grammatical
structure Subject  Predicate  Object.
 domain and range
Each relation has a domain and a range, i.e., values
for the first and second arguments of the relation,
respectively. In linguistic triples, the domain specifies
the types of subject and the range specifies the types
of object a relation takes.
 semantic type
In this work we define the domain and range of
relations using semantic types. By them we refer to
categories of concepts that can either be classes of an
ontology (e.g., all the classes of an upper ontology, or
several top levels of classes in a an ontology or a
taxonomy), or some concept types with broad
semantics. In the experiments presented in this paper
we use three different semantic types: semantic types
from UMLS Semantic Network [28], SNOMED CT
classes and SNOMED CT categories.
Learning formal definitions
In the following, we describe analytically the aspects of
the suggested methodology towards learning formal defi-
nitions from unstructured text.
Corpora
Textual corpora and sets of formally defined relations may
stem from different sources. The choices are important
per se, as to their quality and volume, and in combination
with each other. A corpus should adequately represent the
domain of choice and should contain necessary and suf-
ficient information about the domain concepts. For the
biomedical domain, the following resources are taken into
consideration:
MeSH (Medical Subject Headings) [29]: Definitions
in natural language are produced manually by medical
experts and embedded inMeSH, and, thus, are considered
precise, scientifically valid, and high quality textual data.
MEDLINE: Journal abstracts for biomedical literature
are collected from around the world and can be accessed
via PubMed [30]. Since MEDLINE contains, among other
things, recent publications with cutting-edge advances in
biomedicine, it is of particular interest for the task at hand
since it enables the formalization and integration of newly
emerged biomedical concepts.
Wikipedia articles: Wikipedia provides fundamental
information about biomedical concepts, which can be eas-
ily retrieved by article titles, e.g., Alastrim, Iridodonesis.
Web articles: Besides Wikipedia, many other websites
provide relevant knowledge about biomedical conceptsa.
Such information should be filtered from the web pages by
selecting sentences of definitional structures. For instance,
the Dog4Dag system [8] can retrieve and rank textual
definitions from the web.
In this work, we construct the following corpora listed
below:
 MeSH: Out of 26,853 entries accompanied by textual
definitions in MeSH, we selected all concepts that
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 7 of 17
also have definitions in SNOMED CT. For this we
used the UMLS Metathesaurus [31] which contains
mappings of concepts from various knowledge
resources, including MeSH and SNOMED CT.
 SemRep: Collected from the SemRep project that
conducted a gold standard annotation study in which
500 sentences were selected from MEDLINE
abstracts and manually annotated with 26 semantic
relationships [32].
 WIKI is obtained by querying Wikipedia with
one-word SNOMED CT concept names and amounts
to 53,943 distinct sentences with 972,038 words.
 D4D contains textual definitions extracted by
querying Dog4Dag over concepts that have
relationships via the most frequent attributes used for
Disease, namely Associated_morphology,
Causative_agent, and Finding_site, obtaining 7,092
distinct sentences with 112,886 words.
Unlike the corpus SemRep, the other three corpora, i.e.,
MeSH, WIKI, and D4D are plain texts without annota-
tions. To use them for learning formal definitions, we
developed the alignment process as explained in details in
Section Alignment.
Relation sets
Relations in biomedical thesauri are selected and specified
by domain experts; therefore we assume that all rela-
tions are relevant in terms of semantics, hence they are
interesting to be modeled. However, statistically relations
are not equally distributed across domain texts; some
relations are dominant. For example, for the disease con-
cepts in SNOMED CT, among 48,076 axioms about non-
taxonomic relationships, 40,708 of them only use three
relations: Associated_morphology, Causative_agent, and
Finding_site.
The SemRep corpus contains 26 relations, the most fre-
quent ones being Process_of, Location_of, Part_of, Treats,
Isa, Affects, Causes etc. The statistical distribution of rela-
tions in SemRep gold standard corpus is illustrated in
Figure 2.
Based on the analysis above, in this work, we focus on
two groups of relations:
 The three SNOMED CT relations
(Associated_morphology, Causative_agent,
Finding_site);
 The 26 relations that occur in the SemRep corpus.
Once the relation sets are fixed, we need a set of rela-
tion instances to be used as training data. In the case of
SemRep, we take the instances that are annotated in the
corpus. In SNOMED CT, due to its formal semantics,
we can distinguish two cases: explicit and inferred rela-
tionships. The explicit relationship base (ExpRB) contains
all relationships among concepts that are explicitly given
in the description of concepts in SNOMED CT. For
instance, in Table 1, a human readable display of the
formal definition for the concept Baritosis, we have
Baritosis|Causative_agent|Barium_dust as an explicit one.
The inferred relationship base (InfRB) can be built through
a tractable Description Logic (DL) reasoning engine as
follows: InfRB = {A|R|B : SNOMED CT |= A 
?R.B}, where |= is the logical entailment under DL
semantics which is tractable for EL++ [33], the logic
language underlying SNOMED CT. By this, we have
Baritosis|Causative_agent|Dust as an inferred relationship
since Barium_dust is a subclass of Dust by SNOMED
CT. By the monotonicity of DL semantics, we have
ExpRB ? InfRB. The details of the two relationship bases
for SNOMED CT are summarized in Table 2.
Alignment
In our task, a fundamental requirement is the training
data from which a model can be learned to recognize
formal definitions from texts. When manually annotated
corpus is not available, a common case in our experi-
ments, the training data can be automatically created by
distant-supervision approach [34]. This consists of two
steps: (1) finding the mentions of biomedical concepts in
a sentence, and, (2) aligning the sentence with a relation
by the following principle: if the sentence contains a pair
of concepts, say A and B, and this pair are arguments of a
relation r according to a relationship RB set under consid-
eration, that is A|r|B is in RB, then the sentence fragment
between A and B will be aligned with the relation r. This
two-step process is illustrated in Table 3. For the given
sentence, the fragments Baritosis and barium dust are
the textual mentions of the concepts Baritosis_(disorder)
and Barium_Dust_(substance), respectively. By looking
up the relationship set, such as ExpRB or InfRB, we
know that these two concepts are related by the relation
Causative_agent. Thus, the string between these two con-
cepts, i.e. is pneumoconiosis caused by, is aligned with
the relation Causative_agent. Such an alignment process
is performed on our MeSH,WIKI and D4D corpora.
Feature engineering
The choice of features is key in classifying relations as
it directly influences the success rate of the classifica-
tion process. To this end, we explore two types of feature:
lexical and semantic.
Lexical Features: The lexical features represent specific
words, word sequences or word components that link two
concepts in a sentence and are located in-between the
concept mentions. With regards to the representation of
the lexical features we have utilized three approaches: (1)
bag-of-words (BOW), (2) word n-grams, and, (3) char-
acter n-grams. BOW is the most straightforward text
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 8 of 17
Figure 2 The distribution of relations in the SemRep corpus.
representation in which text is viewed as unordered set
of words, each unique word in the document collection
corresponding to a separate feature. In the word n-grams
representation, text is represented via all possible word
sequences of length up to n. Finally, in the character n-
grams representation, text is represented via all possible
character sequences of length up to n. All the three types
of lexical features separately can be used separately as well
as in combination with each other. In Tsatsaronis et al.
[10], the results did not prove that the combination of
word and character n-grams have a synergy effect on the
performance, hence we skip the combination of lexical
features in the experiments described below.
The lines in Table 3 starting with BoW, Word n-
grams, and Char. n-grams illustrate the lexical features
for the definition: Baritosis is pneumoconiosis caused by
barium dust. The basic assumption behind the choice of
features is that each relation has a characteristic way of
being expressed in natural language text, which may be
captured by the analysis of the words that occur between
the two concepts. The values of lexical features, i.e., the
three representations of text strings, are binary: the value
of a feature is 1, if the corresponding textual element is
present in the string, otherwise the value is 0.We have also
tried expanding these representations to their weighted
versions, assigning real values to features according to
Table 2 Sizes of the explicit and inferred relationships for
the relations: Associated_morphology, Causative_agent,
and Finding_site
Associated_morphology Causative_agent Finding_site
InfRB 503,306 91,794 1,306,354
ExpRB 32,454 13,225 43,079
their frequencies [10]. However, the weighting scheme of
choice turned out to be computationally expensive, but
did not yield considerable improvement to the perfor-
mance. Thus, in the present work we focus on boolean
features.
Table 4 gives an example of highly important lexical
features for the three SNOMED CT roles, when Word
n-grams are used as the feature.
Semantic Features: While lexical features reflect the
relation per se, i.e. its semantics and typical ways of
expression in the text, semantic features focus on what
types of concept arguments a relation can take. They
specify the domain and the range of a relation instance.
For instance, the relation Finding_site has the subject
type Disorder and the object type Body_structure. The
motivation behind the use of semantic features is quite
intuitive: since every relation has a domain and a range, it
can take only certain types of concepts as its arguments.
If we include these types into the feature representa-
tion of instances, we impose explicit constraints on the
arguments of every instance.
Semantic features can help distinguish different rela-
tions even though they share some similar lexical features.
For example, in Table 4, Causative_agent and Finding_site
have similar lexical features infection of and an infec-
tion of, respectively. However, they have different argu-
ment types. So for the sentence Baritosis is an infection of
lung, the relation Finding_site will be recognized instead
of Causative_agent, once we know lung is of the type
Body_structure which is an improper argument type for
Causative_agent.
There are several possibilities on how to define a seman-
tic type given a biomedical concept:
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 9 of 17
Table 3 Example alignment between sentences and relationships via semantic annotation, and lexical and semantic
features extracted from the alignment
Sentence Baritosis is pneumoconiosis caused by barium dust.
Annotated Sentence Baritosis is pneumoconiosis caused by barium dust.
Baritosis_(disorder) Barium_Dust_(substance)
SNOMED CT relationship Baritosis_(disorder) | Causative_agent | Barium_Dust_(substance)
Semantic Features left type between-words right type
disorder is pneumoconiosis caused by organism
BoW {is, pneumoconiosis, caused, by}
Word 2-grams {is pneumoconiosis, pneumoconiosis caused, caused by}
Char. 3-grams {is , s p, pn, pne, neu, eum, umo, moc, oco, con, oni, nio, ios, osi, sis, is , s c, ca, cau, aus, use, sed, ed ,
d b, by}
 UMLS (grouped) semantic types. The UMLS
Semantic Network [35] contains 134 manually built
concept types relevant for the biomedical domain.
Types are assigned to all the concepts of the UMLS
Metathesaurus. However, the modelling of the
domain offered by UMLS is not necessarily
compatible with that of desired relations, i.e., the
types may not fully correspond to the domain and
range of relations and, thus, will not form valid
patterns of type pairs. Moreover, there are 15
coarser-grained semantic types defined for certain
applications, providing a partition of the UMLS
Metathesaurus for 99.5% of the concepts.
 Upper level classes as types. Another approach is to
use the taxonomic structure of a domain ontology. If
the taxonomy forms a single tree of concept classes,
then the first n levels of it can be taken as semantic
types. If there are several independent trees, the tree
top classes can serve as types. For example, MeSH
has 16 taxonomic trees and SNOMED CT has 19 top
concepts. They can directly be used as types for their
sub-concepts. Indeed, there can be different
granularities in choosing a proper taxonomy level as
types. However, more fine-grained levels mean more
specific information that we know about the target
concept, which is often hard to obtain beforehand.
Therefore, we consider the level-one top concepts.
 SNOMED CT semantic types. Unlike the top
concepts, SNOMED CT has defined semantic types
for its concepts which can be read off from the names
of the concepts given in parentheses. For example, in
Table 4 Examples of highly weighted lexical features for
the three SNOMED CT roles: AM Associated_morphology
CA (Causative_agent), and FS (Finding_site)
AM displacement of, medical condition characterized
CA caused, cause, from the, by a, agent of, an infection of
FS of, in, affects only, infection of
Table 4, we have the SNOMED CT concepts
Baritosis_(disorder) whose type is disorder and
Barium_Dust_(substance) having type substance.
Unlike in UMLS Semantic Network, in SNOMED CT
a concept has precisely one semantic type.
Machine learning
We compared the performance of several classifiers with
respect to learning predictive models that can classify
new, unseen relation instances. The tested classifiers are:
Logistic Regression (LR), Support Vector Machines (SVM),
Multinomial Naive Bayes (MNB) and Random Forests
(RF). SVM yielded the highest performance in our exper-
iments on classifying relations, compared to the other
three classifiers. SVM is a linear classification algorithm
that automatically builds a hyperplane separating the
instances of different classes in such a way that the margin
(the distance between the hyperplane and the instances)
is maximized.
Formal definition generation and evaluation
From relationships discovered from texts, it is easy to
traverse to the EL style formal definitions by applying
the following transformations to a single (Equation 1) or
multiple (Equation 2) relationships, respectively [36]:
A|R|B ? A  ?R.B (1)
{A|Ri|Bi} ? A  i?Ri.Bi (2)
Besides evaluating the quality of relation extraction,
we also evaluate the percentage of candidate definitions
that are correct with respect to the formal SNOMED CT
definitions. One main problem is that concepts can be
defined with multiple ways under the DL semantics. For
example we can get a candidate ?Causative_agent.Dust
for the target concept Baritosis. When looking up at the
definition given in SNOMED CT, this candidate is not
explicitly mentioned. However, this does not affect the
definition, since we have ?Causative_agent.Barium_dust,
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 10 of 17
and because SNOMED CT|= Barium_dust  Dust holds,
it follows that Baritosis  ?Causative_agent.Dust.
The definition precision, DefPre, can then be defined as
follows, where Cands = {A|R|B : A,B are concept names
and Ris a relation name}.
Def Pre = |{A|R|B ? Cands : SNOMED CT |= A|R|B}||Cands| ,
(3)
Availability of data and software
All corpora used in this work are freely available. MeSH,
MEDLINE, SNOMED CT, as well as the UMLS resources
(the UMLS Semantic Network, the UMLSMetathesaurus)
can be accessed via the official NLM website: http://
nlm.nih.gov Annotated corpus from the SemRep project
(The SemRep Gold Standard corpus) can be obtained
from http://skr.nlm.nih.gov/SemRepGold/. The four cor-
pora that were used for training and testing the system,
namely WIKI, D4D, SemRep and MeSH, are also put in
open access in the form of machine-readable .arff files
and can be found here: http://www.db-net.aueb.gr/gbt/
download.html. For the implementation of the machine
learning approaches and the representation of the train-
ing and test instances in machine format, we used Weka
(Waikato Environment for Knowledge Analysis), which
can be obtained from http://www.cs.waikato.ac.nz/ml/
weka/. The implementation of the full pipeline of for-
mal definition generation is published online as a GitHub
project: https://github.com/alifahsyamsiyah/learningDL.
Implementation
For the purposes of the implementation and validation
of the suggested approach, we report in the following
the technical details with regards to the versions of the
resources and tools used. The WIKI corpus was collected
from Wikipedia of Nov. 7, 2012, and the D4D corpus was
collected using the Dog4Dag plugin on Nov. 9, 2012. With
regards to SNOMED CT, the version released as of Jan.
31, 2012 was used. The MeSH hierarchy version used is
the official MeSH 2013 release, that was officially released
by NLM during December 2012. The UMLS Metathe-
saurus was used in version 2012AB. The SemRep corpus
was last accessed on Sep. 15, 2013. The Weka version
used for both training and testing the approach was ver-
sion 3.6.5. Default settings were used for all of the tested
machine learning approaches. In particular, we usedWeka
implementation of Support Vector Machines, namely of
their sequential minimal optimization (SMO) version. For
all experiments we used the SMO setting with the lin-
ear kernel, the complexity parameter C = 1.0 and the
epsilon parameter  = 1.0E ? 12. The linear kernel can
be set up in Weka by choosing the PolyKernel kernel with
exponent parameter of 1.0. No feature selection was per-
formed. With regards to Metamap, the 2012 version was
used, which can be obtained from the following location:
http://metamap.nlm.nih.gov/. Default settings were used
with options -R SNOMEDCT to restrict the annotation
resource to SNOMED CT.
Results
We have conducted four different experiments that eval-
uate the task of formal definition generation in two
different levels: (1) learning roles (relations) between con-
cepts, and (2) learning the formal definitions of concepts
as a whole. The final definition of a concept consists
of relations combined together. Thus, we are interested
in evaluating both of these crucial aspects of definition
generation, i.e., the way relational instances are formed
and the way they are combined into a definition. The
first three experiments (Sections Problem formulation to
Availability of data and software) account for the level
of relations, and the last experiment (Section Quality of
generated formal definitions) corresponds to the level of
definitions.
More precisely, Experiment 1 is an initial attempt to
extract biomedical relations from text using machine
learning. It explores the potential of different classification
algorithms to correctly label instances of three frequent
SNOMED CT relationships using lexical features from
MeSH definitions.
In Experiment 2 we added a new feature type, namely
semantic features, to the learning process and we exam-
ined the scenario of learning a bigger set of distinct rela-
tions. For this purpose we used the SemRep corpus, that
comes with a set of textual definitions manually aligned
with 26 relations.
In Experiment 3, we switched the corpus to web-based
textual data with the aim to test the robustness of our
approach in this setting. The problem of data acquisition
is less relevant for Web sources, thus in this experiment
we also examined the influence of the data size on the
learning performance.
In the last experiment we estimated the quality of gen-
erated formal definitions compared to their original forms
given by SNOMED CT.
The first three experiments give us insights about all
major parameters of the relation extraction process that
we outlined in the abstract, i.e., the source of the input cor-
pus, its size, the number of distinct relations and feature
representation. Table 5 summarizes themain results of the
first three experiments. It shows that with themost impor-
tant lexical features (character 3-grams) and appropriate
semantic types, we achieved F-score larger than 90% on
all datasets using 10-fold cross-validation for evaluation.
Furthermore, free texts extracted from the Web proved to
give a competitive result. One may assume that it is due to
Petrova
etal.JournalofBiom
edicalSem
antics
 (2015) 6:22 
Page
11
of17
Table 5 Description of the setup of the three experiments
Modeling Feature engineering Data F-score
Corpora Relation set Relationship Lexical Semantic Size Without types With types
Exp1 MeSH SNOMED CT InfRB 3-grams  424 74% 99.1%
Exp2 SemRep SemRep SemRep 3-grams UMLS 1,357 51%54% 94%
Exp3 WIKI+D4D SNOMED CT InfRB 3-grams SNOMED CT 9,292 58%70% 100%
In all experiments Support Vector Machines are used.
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 12 of 17
the larger data size of the corpus (9,292 v.s. 1,357 or 424
instances).
The fourth experiment shows that our approach can
generate formal definitions with a precision that can reach
up to 81%, as it was defined by Equation 3.
Experiment 1: Lexical features and different classifiers
In the first experiment we used the MeSH corpus
described in Section Corpora as the source of input
texts. We aligned MeSH textual definitions with for-
mal definitions from SNOMED CT ontology (Section
Alignment) and labeled definition substrings with one
of the three SNOMED CT relations, i.e., Finding_site,
Associated_morphology, Causative_agent. We then con-
verted the textual instances of relations into feature rep-
resentation using lexical features. We experimented with
both word and character n-grams, varying the size param-
eter n from 1 to 4. Then the classification model was
trained and tested using 4 different algorithms: Logistic
Regression, Support Vector Machines, Multinomial Naive
Bayes and Random Forests.
We measured the performance of every combination
of features and classification algorithms using the macro-
average F-measure over the three relations in a 10-fold
cross-validation setting. A detailed description and statis-
tics over all settings can be found in [10]. The top perform-
ing setting uses character tri-grams and Support Vector
Machines, yielding an F-measure of 74%. This result illus-
trates that the signal lexical features carry is quite strong
for the relation classification purposes. However, in order
to reach better performance one needs to elaborate on
the experiment setting, which was conducted in the sub-
sequent experiments. In the following experiments we
report results by using only the SVM classifier setting,
since the difference in performance compared to the other
classifiers is negligible. The lexical features of choice are
character tri-grams.
Experiment 2: Semantic features and the number of
relations
In this experiment we aimed at expanding the relation
set from just three SNOMED CT roles to a larger set
of diverge, semantically rich relations. The process of
aligning MeSH and SNOMED CT definitions provided a
dataset of moderate size even for the most frequent rela-
tions, and the number of relation instances that we are
able to extract via the alignment for less populated rela-
tions is insufficient for the automatic learning. Thus we
switched to another corpus of definitions, namely SemRep
(Section Corpora).
The SemRep Gold Standard corpus contains both tex-
tual definitions and a set of relations and consists of 1,357
relation instances. In addition, we introduced semantic
features, reducing every argument concept to its UMLS
semantic type (Section Feature engineering).
We trained and tested the classification models for top
5 and top 10 most frequent SemRep relations as well as
for the whole set of 26 relations. The results are given in
Table 6.
As the results show, semantic types seem to offer a
big contribution to the overall performance. To answer
the question how much do they add to the learning, we
repeated the experiment, leaving out semantic features.
The results when only n-grams were used, are 54% and
51% for the top 5 and for all SemRep relations, respec-
tively. Compared to the results on the full feature set (94%
and 82.7% resp.), the difference in performance rate was
40%. So, semantic types as features are important.
In addition, we examined the effect of the lexical fea-
tures comparing the results of using both feature types,
and of leaving out the lexical features. This is translated
into comparing the first and the second line of Table 6. As
the results show, the lexical features cannot be neglected
as they do offer important contribution in the cases where
a relatively large number of relations is considered, e.g., 10
or more.
The second question that we would like to address is
whether semantic types are generally effective learning
features, or the performance boost was specific to SemRep
dataset. For this purpose, we have tried adding semantic
features to the Experiment 1, extending the feature rep-
resentation of MeSH instances with the same semantic
types from UMLS. The resulting F-measure of 73.9% is a
bit lower than the original one: the semantic types slightly
deteriorated the performance, serving as noise to the clas-
sifier. However, adding semantic types of different origin
had an opposite effect: upper level concepts of SNOMED
CT taken as types gave an F-measure of 99.1% for the
classification of the three SNOMED CT relations com-
pared to 74.5% with lexical features only. From this we
can conclude that semantic types are of great value for the
classification, given that their modeling is consistent with
the modeling of the relations.
Table 6 The performance of multi-class relational classifier
across three different SemRep datasets
Top 5 relations Top 10 relations All relations
F-measure
(with Types) 94% 89.1% 82.7%
F-measure
(only Types) 93.5% 79.2% 65.5%
Size 860 (63%) 1,144 (84%) 1,357 (100%)
The size of each dataset is specified by the absolute number of instances and by
the percentage of instances covered by the respective set of relations. The table
reports F-Measure for two settings: including semantic types in the feature
space, and excluding them.
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 13 of 17
Experiment 3: Web corpora and the dataset size
The third experiment introduces Web-based corpora as
the source of textual definitions from which formal def-
initions and relation instances are built. We used two
different corpora,WIKI and D4D, automatically extracted
from the Web and analyzed the impact of corpora for our
task. The results are reported in Table 7. In accordance to
the findings of the previous experiment, semantic types
(the SNOMED CT types) improved the results notably,
achieving an F-measure of 100% on both corpora. This is
largely due to the disjointness of argument types for the
three target roles from SNOMED CT. Moreover, Table 7
shows that the use of D4D always improved the final per-
formance compared to the use of WIKI. An explanation
for this can be that the sentences from D4D are filtered
beforehand by syntactical patterns [8].
Overall, WIKI and D4D corpora are much larger that
SemRrep and MeSH corpora (Section Corpora). In fact,
having a dataset of more than 60 thousand sentences, it
is possible to plot the performance of the relation classifi-
cation as a function of the dataset size and to analyze the
impact that the number of learning instances has on the
learning. Using the settings of Experiment 3, we discuss
this impact in Section Does data size matter?.
Quality of generated formal definitions
The final step is to translate the predicted relation-
ships into formal definitions. For this, we consider
SNOMED CT as a reference ontology, which has EL
as the underlying logical representation, and, hence,
no subjective judgement is involved in the analysis. In
particular, we consider the concepts that are descen-
dants of Disease(disorder). Although there is a total
of 65,073 descendants of Disease, not all of them
have textual mentions in the corpora we studied. We
examined 1,721 concepts, such as Contact_dermatitis
and Subdural_intracranial_hematoma, which have occur-
rences in the collection of WIKI and D4D corpora,
according to Metamap. For these 1,721 concepts, we
obtained an average precision of 66.5% (defined by
Equation 3). The low value is due to the fact that there
are 314 concepts that occur in the sentences which do not
contain suitable information for extracting their formal
definitions, thus, obtaining zero precision. Considering
Table 7 Main results on the web corporaWIKI and D4D,
where the lexical feature is character 3-grams and type is
the SNOMED CT semantic type as discussion in
Section Feature engineering
Char 3-gram Char 3-gram + Type
WIKI 58% 100%
D4D 70% 100%
merely the remaining 1,407 concepts, we achieved a pre-
cision of 81.3%.
We further investigated if formal reasoning can be help-
ful for our task. For this, instead of using InfRB as done in
all other experiments, we use ExpRB, as given in Table 2, to
construct training data. Note that ExpRB is a proper sub-
set of InfRB, so less training data can be obtained in this
setting compared to using InfRB. As a result, the average
precision of definitions decreased to 61.4% from 66.5% for
the 1,721 concepts. The 5.1 p.p. precision difference shows
that the dataset automatically enriched by formal reason-
ing (the use of InfRB) improves the systems quality in
predicting formal definitions of SNOMED CT concepts.
This is because the inferred relationship base brings more
training examples to boost the whole learning procedure.
Discussion
The above experiments show that automated conversion
of textual definitions into formal ones is a hard, but
feasible problem. In this section we briefly discuss the
choice of corpora, lexical and semantic features, learning
algorithms and data size and how they influence the per-
formance of the definition generation pipeline. Sections
Do corpora matter? to Does data size matter? directly
correspond to the questions we posed at the beginning of
the paper, and Section Chosen formalism sets an open
question of which logic better suits the task of formal
definition generation.
Do corpora matter?
At a first glance, for extracting formal definitions from
texts, the textual data should have a big effect on the
system. For example, MeSH contains manually edited
textual definitions for concepts, which should be of an
obvious advantage for this task. However, from Experi-
ment 3 and 4, the experiments on automatically extracted
web corpora WIKI and D4D, we can still achieve for-
mal definitions of a good quality based on both F-score
and the definition precision. Set aside MeSH, WIKI and
SemRep corpora, which are manually curated, this might
be explained by two facts: (1) the alignment process,
described in Section Alignment, does ensure that the
aligned examples are descriptive of the target relation set,
and, (2) the D4D plugin prioritizes the definitional sen-
tences from the Web, giving higher scores to trustworthy
resources. Overall, the selection of the corpus given the
four choices, did not affect much the final performance,
with the approach providing good generated definitions in
all cases.
Does feature representation matter?
Lexical features
We have tried two different lexical features, word and
character n-grams, of various size of up to n = 4 (word
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 14 of 17
unigrams constituting the basic bag of wordsmodel). Since
the character n-grams model consistently outperformed
the word n-grams model in the Experiment 1, given
the same value of n, in the subsequent experiments we
focused on the use of characters. The n-grammodel incor-
porates as features all possible combinations of characters
of length n that are present in the input corpus, hence
the size of the model is exponential to n. For this reason
the goal is to find the minimum value of the parameter
n that yields one of the highest performance. In Experi-
ment 1 we noticed that character 3-grams perform almost
as good the 4-grams while keeping the size of the model
computationally feasible [10]. The expansion of the model
to 5-grams and beyond is thus unnecessary. While uni-
grams and bi-grams do not convey information that is
statistically relevant for the classification, 3-grams, in con-
trast, are able to capture stems of the key words, important
morphemes, word order etc. They are shallow linguis-
tic features that are easy to generate and integrate into
the model, they lead to the top classification performance
among other lexical features and are used in all experi-
ments. Finally, in Experiment 2 we showed that, although
semantic types as features had the biggest influence on the
performance, lexical features are still of high value in cases
where the target relation set is relatively large.
Semantic features
Semantic features represent broad categories of con-
cepts serving as relation arguments. They are the features
that drastically influence the classification performance.
Semantic types impose category constraints on the argu-
ment values of the relation. Taken together, all instances
of the same relation form typical patterns of semantic
type pairs that are specific for this particular relation. As
features, semantic types convey a strong signal to the clas-
sifier which relation to choose. The discriminative power
of semantic types of relation arguments is the strongest
if the patterns for different relations do not overlap. The
more semantic type pairs are shared by more than one
relation, and the more instances are covered by those
pairs, the more erroneous is the classification based on
these features. In Table 6 we can see that the performance
slowly deteriorates with the growing number of distinct
relations. In fact, taken as the only features, semantic types
yield lower results than in combination with lexical fea-
tures for the number of relations of ten and more. Thus,
the combination of lexical and semantic features is utilized
for such cases.
How to choose the right semantic types?
Experiments 2 and 3 illustrated that involving semantic
types gave a great boost to the system. In our approach
to formalizing textual definitions, we selected predefined
relations from existing biomedical thesaurus, such as
UMLS and SNOMED. Note that each thesaurus has a dif-
ferent way to define the semantic type for a concept [37].
As in Experiment 2, the UMLS semantic types were used
for SemRep relations, and in Experiment 3, the SNOMED
CT types were used for SNOMED CT relations. A natu-
ral question would be if the consistency between relations
and semantic types matters. To this end, we performed
an experiment on MeSH corpus with SNOMED CT rela-
tions but with consistent (SNOMED CT) and inconsis-
tent (UMLS) semantic types, respectively, as features. We
achieved 99% F-measure in the consistency case, and only
74% for the inconsistency one. Compared to the baseline
75%, inconsistent matching did not improve, instead, it
even weakened the system. This illustrates that the con-
straints imposed by SNOMEDCT types are unambiguous
for SNOMED CT relations, but overlapping according to
UMLS types.
Example of the influence of semantic types
To examine thoroughly the effectiveness of the seman-
tic type feature for predicting relations, let us recall Table
4 which contains similar lexical features for two differ-
ent relations: an infection of  for the Causative_agent
relation, and infection of  for the Finding_site relation.
Indeed, the string an infection of virus denotes the
existence of a causative agent relation, while the string
infection of stomach gives the location of a disease,
and, thus, it denotes the existence of a finding site rela-
tion. Because the two strings are practically the same, i.e.,
infection of , by examining only the lexical information
in this case it is hard to decide which relations should
be assigned to the two strings. However, Causative_agent
and Finding_site have specific combinations of semantic
types with regards to the domain and range. As its range
argument, Causative_agent relation may have a concept
of the semantic type Organism. The Finding_site relation
may have Body_structure. In parallel, Causative_agent
cannot have Body_structure as a range argument, and
Finding_site cannot have Organism as its range argument.
Hence, since virus has Organism as its semantic type,
and Stomach has Body_structure, the machine learner
can easily assign correct relations for the strings an infec-
tion of virus and infection of stomach. Therefore, in this
case our approach identifies a Causative_agent relation in
the former case, and a Finding_site relation in the latter
case.
DoML algorithmsmatter?
The choice of the machine learning algorithm is sec-
ondary for the task at hand. While in [10] Support Vector
Machines consistently dominate over Logistic Regression,
Random Forests and Multinomial Naïve Bayes, the differ-
ence in performance rate values are partly due to the small
size of the dataset (424 instances). In several cases, SVM
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 15 of 17
have been shown to match or dominate the performance
of competitive techniques for major text mining exercises.
For example, in [38] it is shown that SVM, Naïve Bayes
and k-Nearest Neighbor are among the best performing
classifiers. A later work by Mladenic´ et al. [39] evaluates
several classifiers on the Reuters news datasets showing
that SVM tend to outperform other algorithms including
Naïve Bayes.
However, several studies on the use of Big Data sug-
gest that the performance rates of various algorithms tend
to converge given considerable amount of instances. In
[40], in the task of word sense disambiguation, the per-
formance of several algorithms (Naïve Bayes, Winnow,
Perceptron) gradually increases and eventually converges
as the training dataset size increases. Colas and Brazdil
[41] conclude that, although SVM are among the most
effective algorithms in the area of text processing, vari-
ous other algorithms, e.g., Naïve Bayes or k-NN, achieve
comparable performance. Hence, in our work we shift the
focus towards feature engineering and use only SVM.
Does data size matter?
For the experiments introduced in Section Results, the
corpora data sizes moved from 424 to 9,292, on which we
achieved consistent results as analyzed above. Indeed, we
conducted similar conclusions by sampling smaller data
sets from the large corporaWIKI andD4D: (a) The perfor-
mance increased with the increase of the data size if only
lexical features were used: on D4D, F-score ranged from
59% (data size 300500) to 63% (data size 7501,000),
and to 70% (data size 1,5003,100). On WIKI, F-score
ranged from 48% (data size 600700) to 57% (data size
8003,000), and to 67% (data size 4,0006,000). (b) The
performance stayed relatively constant if semantic fea-
tures were involved too, no matter the size of the data:
F-score fell into the range of 98% to 100% on all the sam-
pled smaller data sets. This again confirms that semantic
type is a key feature for the task.
Chosen formalism
In this work, we choose to learn EL style biomedical
ontologies, adopted by SNOMED CT, for the following
reasons: (a) the results can be directly compatible with and
integrated into the existing resources, (b) it is possible to
evaluate our work, using SNOMED CT as a benchmark,
(c) the generated definitions can be easily manipulated
by users who have previous experience with SNOMED
CT, (d) EL can deduce implicit hierarchies among the
SNOMED CT concepts.
However, there are many other Description Logic styles
that might be interesting to consider, for example, the
full version of GALEN [42] which was designed to
be a re-usable application-independent and language-
independent model of medical concepts. This opens our
future work on extending the proposed method for learn-
ing more expressive biomedical ontologies.
What kinds of definitions are generated by the method?
EL imposes strict constraints on the form of the defini-
tions that are generated:
 The left part is the concept to be defined, and the
right part is the intersection (conjunction) of
existentially quantified (restricted) roles. The two
parts can be linked into an axiom either by an
equivalence operator ? (making it a DL definition in
the strict sense), or by a subsumption (inclusion)
operator  (thus making it a primitive definition, that
only specifies the necessary condition). In the current
work we make all generated definitions to contain a
subsumption operator.
Ex: Arthritis is a form of joint disorder that results
from joint inflammation.
Arthritis  Joint_Disorder  ?results_from.Joint
_Inflammation
 The definition cannot contain negation or
disjunction of concepts, cardinality constraints,
universal quantification etc., as these are not part of
the EL syntax.
 The definitions have flat structure. If there is an
existential restriction of a concept in the right part of
the definition, then this concept should be a simple
concept and not of the form C1  C2 or ?R1.C1.
This, however, does not by any means prevent the defi-
nition from containing several relations, which is often the
case for biomedical concepts, as it was rightfully stated by
the reviewer.
Fox ? Fordyce_Disease  ?Finding_site.Apocrine_glands
?Finding_site.Intraepidermal_apocrine_ducts  (4)
?Causative_agent.Obstructure (5)
?Causative_agent.Rupture (6)
?Associative_morphology.Papular_eruptions (7)
Conclusion
In this work we explored the problem of formal def-
inition generation from textual definitions of biomedi-
cal concepts. We suggested a machine learning based
method that automatically generates Description Logic
axioms from textual data. At the core of the method
lies the relation extraction component. Once formal rela-
tion instances are generated from text, they are combined
into definitions. The method is implemented and is made
available to the public. We tested the method on three
benchmark data, evaluating it both at the level of relations
and at the level of definitions and achieving a success rate
Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 16 of 17
of over 90% and 80%, respectively. Moreover, we inves-
tigated in detail how different aspects of the method,
e.g., the source of textual definitions or the types of fea-
tures used for learning, affect its performance. Overall,
the choice of corpora, lexical features, learning algorithm
and data size do not impact the performance as strongly
as semantic types do. Semantic types limit the domain
and range of a predicted relation, and as long as relations
domain and range pairs do not overlap, this information is
most valuable in prediction. Our work demonstrated that
automated conversion of textual definitions into formal
ones is a hard, but feasible problem. It enables complex
reasoning over biomedical knowledge that still exists only
in unstructured textual form and can contribute to many
biomedical applications.
Endnote
aE.g., www.primehealthchannel.com or topdefinitions.
com
Competing interests
The authors declare that they have no competing interests.
Authors contributions
MS and FB had the original idea. AP, GT, MS and YM developed the idea. AP
and YM implemented it. AP, GT and MS performed feature enfineering and
introduced semantic features, while YM, FD and FB focused on the use of
explicit and implicit relations and on formal definition generation. AP, YM and
GT wrote the manuscript, ran the experiments and analysed the results. MK
and FD contributed to discussions and experiments. All authors read and
approved the final manuscript.
Acknowledgements
The current research work is funded by the Hybrid Reasoning for Intelligent
Systems Unit (HYBRIS B1) established by the Deutsche Forschungsgemeinschaft
(DFG).
Author details
1Biotechnology Center, Technische Universität Dresden, Dresden, Germany.
2Institute of Theoretical Computer Science, Technische Universität Dresden,
Dresden, Germany.
Received: 2 July 2014 Accepted: 23 March 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Kafkas et al. Journal of Biomedical Semantics  (2015) 6:7 
DOI 10.1186/s13326-015-0003-7SOFTWARE Open AccessSection level search functionality in Europe PMC
?enay Kafkas*, Xingjun Pi, Nikos Marinos, Francesco Talo, Andrew Morrison and Johanna R McEntyreAbstract
Background: As the availability of open access full text research articles increases, so does the need for
sophisticated search services that make the most of this new content. Here, we present a new feature available in
Europe PMC that allows selected sections of full text articles to be searched, including figures and reference lists. Users
can now search particular parts of an article, reducing noise and allowing fine-tuning of searches.
Results: To the best of our knowledge, Europe PMC is the first service that provides a granular literature search by
allowing users to target their search to particular sections of articles. This new functionality is based on a
heuristic algorithm that identifies and categorises article sections into 17 pre-defined categories based on the
section heading. The taggers performance is measured against a manually curated dataset consisting of 100 full text
articles with an F-score of 98.02%.
Conclusions: The section search is available from the advanced search within Europe PMC (http://europepmc.org).
The source code is freely available from http://europepmc.org/ftp/oa/SectionTagger/.
Keywords: Text mining, Section, Information retrievalBackground
Life science research articles are narrative accounts of
research findings, usually describing methods, experi-
mental results, and providing scientific context to the
new work reported. Most typical research articles are
structured into sections (segments), most often repre-
sented by a logical sequence, known as IMRAD -
Introduction, Materials & Methods, Results and
Discussion [1]. However, synonyms of these typical
section titles are frequently used in articles, according
to different journal styles. Furthermore, other types of sec-
tions are common, such as Case Report in clinical jour-
nals, or additional sections such as Funding Sources.
These sections provide useful context for the human
readers understanding of the findings described.
The availability of full text articles online provides the
opportunity to develop deep search over the complete
article, not just the abstract. While this extends the con-
tent available for searching, it can also unfortunately add
significant noise in the results returned. For example, for
searches that order results by publication date or* Correspondence: kafkas@ebi.ac.uk
Equal contributors
European Molecular Biology Laboratory, European Bioinformatics Institute
(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge, CB10
1SD, United Kingdom
© 2015 Kafkas et al.; licensee BioMed Central.
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.citation count, the results at the top of the list can have
little bearing on the original search term if that term is
found only in the Reference list.
There are a few free-to-use services that provide bio-
medical literature search services on full-text documents,
for example, PubMed Central (http://www.ncbi.nlm.nih.
gov/pmc), Google Scholar (http://scholar.google.co.uk/),
BioText Search Engine (http://biosearch.berkeley.edu) and
Yale Image Finder (YIF) (http://krauthammerlab.med.yale.
edu/imagefinder/). However, to the best of our knowledge,
neither PubMed Central nor Google Scholar allows users
to limit searches to, or exclude, particular sections of arti-
cles. BioText allows users to limit searches to figure cap-
tions and tables, and YIF only allows users to limit searches
to figure captions. At Europe PMC (http://europepmc.org)
[2], we have implemented a comprehensive section-level
search feature that is applied to incoming full text articles
daily, and have exposed it to users both within the de-
fault search on the Europe PMC website, and within the
Advanced Search form.Implementation
Implementation details
This section-level search feature has been implemented
as a component of the existing Europe PMC full text in-
frastructure. As the database is updated with new fullThis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Kafkas et al. Journal of Biomedical Semantics  (2015) 6:7 Page 2 of 5text content, a rule-based section tagger, developed to
identify the sections of full text articles, is deployed prior
to Lucene indexing (http://lucene.apache.org/). Further
implementation details of the section tagger are pro-
vided below.
Section categorisation
In total 17 section category types have been identified as
frequently occurring, based on an analysis of content of
structured section headers (section headers are tagged by
using the <title> XML element, e.g. <title>Methods</title>)
appearing in the XML of the open access (OA) set
of Europe PMC articles. The pre-selected categories
are: Introduction & Background, Materials & Methods,
Discussion, Conclusion & Future Work, Case Study,
Acknowledgement & Funding, Author Contribution,
Competing Interest, Supplementary Data, Abbreviations,
JOURNAL OF
BIOMEDICAL SEMANTICS
Cunningham et al. Journal of Biomedical Semantics  (2015) 6:32 
DOI 10.1186/s13326-015-0030-4SHORT REPORT Open AccessImproving the Sequence Ontology
terminology for genomic variant annotation
Fiona Cunningham1, Barry Moore2, Nicole Ruiz-Schultz3, Graham RS Ritchie1,4 and Karen Eilbeck3*Abstract
Background: The Genome Variant Format (GVF) uses the Sequence Ontology (SO) to enable detailed annotation of
sequence variation. The annotation includes SO terms for the type of sequence alteration, the genomic features
that are changed and the effect of the alteration. The SO maintains and updates the specification and provides the
underlying ontologicial structure.
Methods: A requirements analysis was undertaken to gather terms missing in the SO release at the time, but
needed to adequately describe the effects of sequence alteration on a set of variant genomic annotations. We have
extended and remodeled the SO to include and define all terms that describe the effect of variation upon
reference genomic features in the Ensembl variation databases.
Results: The new terminology was used to annotate the human reference genome with a set of variants from
both COSMIC and dbSNP. A GVF file containing 170,853 sequence alterations was generated using the SO
terminology to annotate the kinds of alteration, the effect of the alteration and the reference feature changed.
There are four kinds of alteration and 24 kinds of effect seen in this dataset. (Ensembl Variation annotates 34
different SO consequence terms: http://www.ensembl.org/info/docs/variation/predicted_data.html).
Conclusions: We explain the updates to the Sequence Ontology to describe the effect of variation on existing
reference features. We have provided a set of annotations using this terminology, and the well defined GVF
specification. We have also provided a provisional exploration of this large annotation dataset.Findings
Background
The Sequence Ontology (SO) [1] provides terminology
to define sequence features. These features are the build-
ing blocks of sequence annotation, and allow biologically
meaningful regions to be assigned between coordinates
of sequences such as genome assemblies and transcripts.
The relationships between the terms in SO provide for
the annotation of multi-part features such as gene
models, composed of multiple transcripts, exons, introns
and UTR features. Reference genome annotations are
often shared using a flat file format GFF3, developed by
the GMOD community [2], which stipulates that SO
terms describe each annotated feature, thus many gen-
ome annotation tools use SO to describe reference gen-
ome features. While terms to describe variants have long* Correspondence: keilbeck@genetics.utah.edu
3Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA
Full list of author information is available at the end of the article
© 2015 Cunningham et al. This is an Open Ac
License (http://creativecommons.org/licenses/
medium, provided the original work is proper
creativecommons.org/publicdomain/zero/1.0/been part of the Sequence Ontology, increased need for
new variation terms to describe the predicted effect of
sequence alterations on existing genomic features lead
to the development of new terms. This has been driven
by the proliferation of software tools that predict the ef-
fect of sequence alterations such as Ensembls Variant
Effect Predictor (VEP) [3] and the VAAST suite tool:
Variant Annotation Tool (VAT) [4]. In this mansucript,
SO terms are italicized and written without underscores.
Next generation sequencing (NGS) technologies have
provided an enormous expansion in our understanding
of the landscape of genetic variation [5, 6] as well as the
impact of that variation on human health [79]. These
datasets create a significant burden in computational
analysis and data storage, but established work-flows for
analysis are emerging [3] and well established data for-
mats exist for each stage of the process. The original
base calls from the sequencer are converted to FASTQ
files [10] that contain the sequence data; the SAM for-
mat [11] captures the alignment of the sequence to acess article distributed under the terms of the Creative Commons Attribution
by/4.0), which permits unrestricted use, distribution, and reproduction in any
ly credited. The Creative Commons Public Domain Dedication waiver (http://
) applies to the data made available in this article, unless otherwise stated.
Cunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 2 of 5reference genome and the Variant Call Format [12] has
become widely adopted by variant calling tools to report
variants and the information needed to call them. How-
ever, knowing the type and genomic location of a se-
quence change is just the first step in understanding its
clinical or biological consequences. Variant annotation
then begins the process of adding additional knowledge
about the structural and functional consequences of
those variants through the impact on reference sequence
features and ultimately on phenotype.
The Genome Variation Format (GVF) [13] is a variant
file format for the detailed annotation of genetic vari-
ation. GVF is a community supported format that uses
established ontologies such as the Sequence Ontology
[1] to describe the variant data. GVF does not replace
existing variant nomenclature systems such as HGVS
[14] and ISCN [15] that provide effective ways to unam-
biguously describe individual variants in the literature.
GVF provides the infrastructure to support inclusion of
these nomenclatures along with other detailed variant
annotations in a format capable of supporting genome
scale variant data. GVF is used in the community for ex-
change of variant annotations between Ensembl [16],
DGVa and dbVar [17] and is compatible with existing
GFF3 software [2, 18] as well as emerging domain spe-
cific tools [4, 19].Fig. 1 Hierarchical view of new and modified Sequence Ontology terms u
portion of the SO sequence variant subsumption hierarchy is shown, with t
where the sequence alteration occurs within or overlaps an annotated refe
ablation, feature amplification, define cases where an entire feature is altere
http://sequenceontology.org/browser/obob.cgi and http://ensembl.org/infoUser requirements and ontology development
Upon the release of the specification for variant genome
annotation, GVF used terms from the Sequence Ontol-
ogy release 2.4.3. While this resource provided 101
terms to describe the effects of a sequence alteration on
genomic features, it was still missing sufficiently special-
ized terms to fully capture the kinds of variation annotated
by the Ensembl variation pipeline [20]. A requirements
analysis was undertaken to establish the terminology and
relationships between terms to accomplish annotation and
facilitate queries of annotated datasets. Ensembl uses 34
terms [21] to describe the effect of variation, 21 of which
were new to SO, and 2 required an ammendement to
the name. Figure 1 shows a subset of the terms in SO
that describe sequence variants, with the Ensembl terms
highlighted.
In the SO, the sequence alteration and the effects of
the alteration are separated. A sequence alteration de-
fines the nucleotide change observed in an individual se-
quence, in relation to a reference sequence. Examples of
alterations are insertion, deletion, substitution and SNV.
The effect of a sequence alteration is the observed or
predicted change to annotated reference seqeunce fea-
tures. These effects of sequence alterations are defined
as sequence variants in SO and are outlined in Fig. 1.
Examples of these terms are missense variant, wherebysed by Ensembl to annotate the effects od sequence alteration. A
erms used by Ensembl in dark grey. Feature variant terms define cases
rence feature such as a transcript or exon, whereas the kinds of feature
d. Definitions for these terms are available from the miSO browser:
/genome/variation/predicted_data.html
Cunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 3 of 5codon bases are modified in such a way as the resuling
amino acid would change, and splice donor variant
where by the alteration changes the two-base pair region
at the 5? end of an intron.
One of the advantages of using an ontology for the an-
notation of data, is that given the related nature of the
terms, there are options to annotate data to the level of
detail afforded by the evidence. Under the sequence vari-
ant node, SO provides two high level nodes in the ontol-
ogy: structural variant and functional variant. Structural
variants pertain to changes with regard to annotated se-
quence features, and are the output of automated variant
effect predition tools such as VEP [3]. Functional variants
however describe the cellular effect of a sequence alter-
ation and are generally manually curated. These functional
terms have largely been absorbed into the Variation ontol-
ogy [22] and are not automatically assigned by variant ef-
fect prediction tools. With regards to structural variants,
the alteration can either internally modify a sequence fea-
ture, when the alteration falls within the extent of a refer-
ence sequence feature such as an exon (feature variant),
or the alteration can be greater than the extent of the se-
quence feature, causing the ablation or amplification of an
entire genomic feature such as a transcript.
The feature variant node in the ontology subsumes
the terms that describe changes internal to genomic fea-
tures such as those affecting genes, transcripts and in-
trons. The majority of the sequence alterations currently
annotated by Ensembl cause feature variants. These fea-
ture variant terms are shown in Fig. 1, where the terms
used in Ensembl annotations are highlighted in dark
grey. There are five subtypes: intergenic variant, geneFig. 2 Treemap of the proportion of variant affect atributed to each kind o
treemap displays hierarchcal data as nested rectangles. In this dataset there
substitution and SNV, each with a different color. For each sequence alterat
rectangle proportional to the number of occrurences of that annotation, an
generated using the IBM Manyeyes tool (http://www-958.ibm.com/)variant, feature truncation, feature elongation and regu-
latory region variant. Of these terms, gene variant has
77 direct and indirect subtypes and includes most of the
terms that describe structural sequence variants caused
by substitutions and small insertions and deletions. This
portion of the SO contains terms with multiple parents,
to allow for effective querying of the annotations. For
example, the term stop retained variant is both a syn-
onymous variant and a terminator codon variant. Users
are thus able to query the Ensembl data for all termin-
ator codon variants or all synonymous variants.
Annotated variants
GVF formated variant genome annotations for 19 organ-
isms, typed using SO are available within the Ensembl
databases [23] and for download (ftp://ftp.ensembl.org/
pub/release-69/variation/gvf/). Included in this set is a
GVF file of 170,853 human variant annotations, with
data from dbSNP [24] and COSMIC [25] using the de-
scribed terminology. There are four kinds of sequence al-
terations reported, corresponding to 158205 SNVs, 7575
deletions, 3097 insertions and 1876 substitutions. There
are 24 kinds of variant_effect reported in the file, and
five kinds of genomic feature affected (mRNA, miRNA,
transcript, primary_transcript and ncRNA). There are
1,485,317 reported variant effects with corresponding
genomic features, as a single alteration may perturb
many annotated genomic features. For example an SNV
may intersect two alternate transcripts, one in an exon,
the other in an intron. Figure 2 shows a tree map of the
proportion of variant effects annotated to each kind of
sequence alteration in this dataset. As can be seen, eachf sequence alteration in Ensembl human GVF dataset (release 69). A
are four kinds of sequence alteration annotated: insertion, deletion,
ion, the annotated variant effects are shown with the size of the
d the count is provided where space permits. The treemap was
Cunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 4 of 5kind of alteration causes proportionally different effects
upon the genome features; insertions and deletions
cause more frameshift variants, where as the SNVs and
other substitutions cause more missense variants.
Discussion and conclusions
Detailed annotation of sequence variation is complicated
because reference genome annotations are complex. Genes
may produce multiple transcripts, may overlap each other
on opposite strands, or even be nested within introns of
other genes, therefore a variant may influence multiple
genomic features. Capturing the effect of a sequence alter-
ation on the genomic features with which it intersects is
an important step towards understanding the implication
of the variant sequence. The terminology described here
provides a basis with whch to categorize and define se-
quence variation and the flexibility to annotate the effect
with respect to the feature intersected. This ontology pro-
vides very specific leaf terms, with which to automatically
annotate genomic sequence but also useful mid level terms
for querying.
Future developments to the ontology will include de-
veloping relationships between the sequence variant
terms and the sequence features that are affected. There
has been significant uptake of these variant effect terms
by the genomic variant annotation community. The
UCSC genomic browser uses this termnology in variant
annotation [26] as does the NCBIs ClinVar data diction-
ary and dbVar database [17]. New terms will be added as
required. New terms and updates to the ontology may
be requested using the term tracker (https://sourcefor-
ge.net/p/song/term-tracker/). Development of the SO is
collaborative, incorporating community discussion via
our mailing list and the term tracker as well as the re-
sults of focused working groups.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
FC and GR performed requirements analysis. KE, FC, NRS, GR and BM
contributed to ontology development and term definition. All authors
contributed to manuscript.
Acknowledgements
This work was supported by the National Human Genome Research Institute
[R01HG004341 to KE] and National Libarary of Medicine training grant [T15
LM007124-18, NRS]. Ensembl receives majority funding from the Wellcome Trust
(grant numbers WT095908 and WT098051) with additional funding for specific
project components from the National Human Genome Research Institute
(U41HG007234, 1R01HD074078, and U41HG007823), the Biotechnology and
Biological Sciences Research Council (BB/K009524/1, BB/L024225/1, BB/M018458/1
and BB/M020398/1), the Centre for Therapeutic Target Validation (CTTV) and the
European Molecular Biology Laboratory. The research leading to these results has
received funding from the European Union's Seventh Framework Programme
(FP7/2007-2013) under grant agreement n° 282510 (BLUEPRINT). The research
leading to these results has received funding from the European Union's
Seventh Framework Capacities Specific Programme under grant agreement n°
284209 (BioMedBridges). This project has received funding from the EuropeanUnions Horizon 2020 research and innovation programme under grant
agreement n° 634143 (MedBioinformatics)
Author details
1European Molecular Biology Laboratory, European Bioinformatics Institute,
Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK.
2Department of Human Genetics, University of Utah, Salt Lake City, UT, USA.
3Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,
USA. 4Wellcome Trust Sanger Institute, Hinxton, Cambridge, UK.
JOURNAL OF
BIOMEDICAL SEMANTICS
Mina et al. Journal of Biomedical Semantics 2015, 6:5
http://www.jbiomedsem.com/content/6/1/5
RESEARCH Open Access
Nanopublications for exposing experimental
data in the life-sciences: a Huntingtons
Disease case study
Eleni Mina1, Mark Thompson1, Rajaram Kaliyaperumal1, Jun Zhao2, Eelke van der Horst1,
Zuotian Tatum1, Kristina M Hettne1, Erik A Schultes1, Barend Mons1 and Marco Roos1*
Abstract
Data from high throughput experiments often produce far more results than can ever appear in the main text or tables
of a single research article. In these cases, the majority of new associations are often archived either as supplemental
information in an arbitrary format or in publisher-independent databases that can be difficult to find. These data are
not only lost from scientific discourse, but are also elusive to automated search, retrieval and processing. Here, we use
the nanopublication model to make scientific assertions that were concluded from a workflow analysis of
Huntingtons Disease data machine-readable, interoperable, and citable. We followed the nanopublication guidelines
to semantically model our assertions as well as their provenance metadata and authorship. We demonstrate
interoperability by linking nanopublication provenance to the Research Object model. These results indicate that
nanopublications can provide an incentive for researchers to expose data that is interoperable and machine-readable
for future use and preservation for which they can get credits for their effort. Nanopublications can have a leading role
into hypotheses generation offering opportunities to produce large-scale data integration.
Keywords: Huntingtons disease, Nanopublication, Provenance, Research object, Workflows, Interoperability,
Data integration
Background
The large amount of scientific literature in the field
of biomedical sciences makes it impossible to manually
access and extract all relevant information for a particular
study. This problem is mitigated somewhat by text min-
ing techniques on scientific literature and the availabil-
ity of public online databases containing (supplemental)
data. However, many problems remain with respect to the
availability, persistence and interpretation of the essential
knowledge and data of a study.
Text mining techniques allow scientists to mine rela-
tions from vast amounts of abstracts and extract explicitly
defined information [1] or even implicit information [2,3].
Because most of these techniques are limited to min-
ing abstracts, it is reasonable to assume that information
*Correspondence: m.roos@lumc.nl
1Department of Human Genetics, Leiden University Medical Center, PO Box
9600, 2300 RC Leiden, The Netherlands
Full list of author information is available at the end of the article
such as tables, figures and supplementary information are
overlooked. Moreover, recent attempts to mine literature
for mutations stored in databases, showed that there was
a very low coverage of mutations described in full text and
supplemental information [4].
This is partly remedied by making data public via online
databases. However, this by itself does not guarantee that
data can be readily found, understood and used in com-
putational experiments. This is particularly problematic
at a time when more, and larger, datasets are produced
that will never be fully published in traditional journals.
Moreover, there is no well-defined standard for scien-
tists to get credit for the curation effort that is typically
required to make a discovery and its supporting exper-
imental data available in an online database. We argue
that attribution and provenance are important to ensure
trust in the findings and interpretations that scientists
make public. Additionally, a sufficiently detailed level of
attribution provides an incentive for scientists, curators
and technicians to make experimental data available in
© 2015 Mina et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License
(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,
provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Mina et al. Journal of Biomedical Semantics 2015, 6:5 Page 2 of 12
http://www.jbiomedsem.com/content/6/1/5
an interoperable and re-usable way. The Nanopublica-
tion data model [5] was proposed to take all these issues
into consideration. The nanopublication guidelines docu-
ment [5] provides details of the nanopublication schema
and recommendations for constructing nanopublications
from Life Science data. Based on Semantic-web tech-
nology, the nanopublication model is a minimal model
for publishing an assertion, together with attribution and
provenance metadata.
The assertion graph contains the central statement that
the author considers valuable (publishable) and for which
she would like to be cited (attribution). It should be kept
as small as possible in accordance with the guidelines.
The provenance graph is used to provide evidence for the
assertion. It is up to the author to decide howmuch prove-
nance information to give, but in general, more prove-
nance will increase the trustworthiness of the assertion,
and thus the value of the nanopublication. The publication
info graph provides detailed information about the nanop-
ublication itself: creation date, licenses, authors and other
contributors can be listed there. Attribution to curators
and data modelers are part of the nanopublication design
to incentivize data publishing.
We used the nanopublication schema tomodel scientific
results from an in-silico experiment. Previously Beck et al.
[6] used GWAS data stored in the GWAS central database
to model as nanopublications and they demonstrated how
such valuable information can be incorporated within
the Linked Data web to assist the formation of new
hypotheses and interesting findings. In our experiment
we investigated the relation between gene deregulation in
Huntingtons disease and epigenetic features that might
be associated with transcriptional abnormalities (E. Mina
et al., manuscript in preparation).
We show how the results of this case study can be rep-
resented as nanopublications and how this promotes data
integration and interoperability.
Huntingtons Disease as case study for modelling scientific
results into nanopublications
Huntingtons Disease is a dominantly inherited neurode-
generative disease that affects 1 - 10/100.000 individuals
and thus making it the most common inherited neurode-
generative disorder [7]. Despite the fact that the genetic
cause for HD was already identified in 1993, no cure has
yet been found and the exact mechanisms that lead to
the HD phenotype are still not well known. Gene expres-
sion studies revealed massive changes in HD brain that
take place even before first symptoms arise [8]. There is
evidence for altered chromatin conformation in HD [9]
that might explain these changes. We selected to analyse
two datasets that are associated with epigenetic regula-
tion, concerning CpG islands in the human genome [10]
and chromatin marks mapped across nine cell types [11].
Identifying genes that are deregulated in HD and are asso-
ciated with these regions can give insight into chromatin-
associated mechanisms that are potentially at play in this
disease.
Our analysis has been implemented through the use of
workflows using the Taverna workflow management sys-
tem [12,13]. As input we used gene expression data from
three different brain regions from HD affected individuals
and age and sex matched controls [14]. We tested for gene
differential expression (DE) between controls and HD
samples in the most highly affected brain region, caudate
nucleus, and we integrated this data with the two epi-
genetic datasets discussed previously which are publicly
available via the genome browser [15,16].
HD is a devastating disease and no actual cure has
been found yet to treat or slow down disease progres-
sion. Therefore, research on this domain is mainly focus-
ing on the production of new data and investment on
expensive experiments. It is important to realize that shar-
ing information is essential in research for developing
new hypotheses that can tackle difficult use cases such
as HD. Because of the unavailability of previous exper-
iments to be found online using common biomedical
engines, expensive experiments become lost and unnec-
essarily replicated. For example in our case study, we
found that the association that we inferred between the
HTT gene, which mutant form causes Huntingtons Dis-
ease, and BAIAP2, a brain-specific angiogenesis inhibitor
(BAI1)-binding protein, was present in a table in a paper
by Kaltenbach et al. [17]. However, it is not explicitly in
any abstract which makes it hard to retrieve from systems
such as PubMed.
Results and Discussion
Nanopublication model design principles
We decided to model and expose as nanopublications two
assertions from the results of our workflow: 1) differ-
entially expressed genes in HD and 2) genes that over-
lap with a particular genomic region that is associated
with epigenetic regulation. Note that these natural lan-
guage statements would typically be used in a caption
for a figure, table or supplemental information section to
describe a dataset in a traditional publication. Considering
the problems with automatic retrieval and interpretation
of such data, we aim to expose these assertions in a
way that is more useful to other scientists (for example
to integrate our results with their own data). Moreover,
we provide provenance containing the origin and exper-
imental context for the data in order to increase trust
and confidence. Our nanopublications are stored in the
AllegroGraph triple store [18]. The link to the browsable
user interface and the SPARQL endpoint can be found on
the myExperiment link: http://www.myexperiment.org/
packs/622.html. The user can log in and browse through
Mina et al. Journal of Biomedical Semantics 2015, 6:5 Page 3 of 12
http://www.jbiomedsem.com/content/6/1/5
the nanopublications by logging in with username test
and password tester. The queries used in this paper are
stored under the menu Queries ? Saved.
Assertionmodel
We defined two natural language statements that we wish
to convert to RDF:
gene X is associated with HD, because it was found to
be deregulated in HD and gene Y is associated with a
promoter, and this promoter overlaps with a CpG island
and/or a particular chromatin state, and we wish to refer
to the experiment by which we found these associations.
We decided to model our results into two nanopublica-
tions. By further subdividing those statements, we see the
RDF triple relations appear naturally:
Nanopublication assertion 1:
1. There is a gene disease association that refers_to
gene X and Huntingtons Disease
Nanopublication assertion 2:
1. Gene Y is associated_with promoter Z
2. Promoter Z overlaps_with a biological regiona
The assertion models are shown in Figure 1 and
Figure 2.
For some of the terms in these statements we found sev-
eral ontologies that defined classes for them. For example,
promoter, gene, and CpG island appear (among oth-
ers) in the following ontologies: NIF Standard ontology
(NIFSTD), NCI Thesaurus (NCI) and the Gene Regu-
lation Ontology (GRO)b. We chose to use NIFSTD for
our case study, because it covers an appropriate domain
and it uses the Basic Formal Ontology (BFO), which can
benefit data interoperability and OWL reasoning (e.g. for
checking inconsistencies).
We chose to use bio2rdf instances for the associated
genes [19] because they provide RDF with resolvable
resource URIs formany different biomedical resources. To
describe the gene-disease association linked with altered
gene expression we used the class with that label from
the SemanticScience Integrated Ontology (SIO) [20]. The
SIO predicate refers to was used to associate each dif-
ferentially expressed gene with HD. There were also terms
that we did not find in an available ontology. These were
the ones that described the type of the chromatin state
that a promoter of a gene can be in, active promoter
state, weak promoter state, poised promoter state
and heterochromatic. We decided to create our own
classes to describe these terms. Being aware of interoper-
ability issues, we defined them as subtypes of classes in
the Sequence Ontology (SO). We defined the class chro-
matin_region as a subclass of biological_region in SO.
We defined another class chromatin_state as a subclass
of feature_attribute. Subclasses of chromatin_state are
the states active_promoter, weak_promoter, poised_
promoter and heterochromatic, Figure 3. In Table 1
we provide the definition for these classes, the reused SO
ontological terms and their corresponding URIs. We also
defined an object property has_state which has domain
chromatin and range chromatin_state.
For the predicates we considered the use of the Rela-
tion Ontology. Extending the Relation Ontology with the
appropriate predicates would support interoperability and
reasoning in the long term, because its use of BFO. How-
ever, we found that the OWL domain and range spec-
ifications did not match our statements. Therefore, we
decided to use predicates from the also popular Sequence
Ontology (SO) and Semanticscience Integrated Ontology
(SIO) [20] that also seemed appropriate for our assertions.
This is a typical trade-off between quality and effort that
we expect nanopublishers will have to make frequently.
We can justify this for two reasons: 1) releasing experi-
mental data as linked open data using any standard ontol-
ogy is already an important step forward from current
Figure 1 Differential gene expression assertion model. The template for the differential gene expression assertion. Orange diamonds refer to a
RDF resource that was defined by this nanopublication, whereas the gene (pink diamond) is defined by a bio2rdf resource. The Sequence Ontology
(SO) were used for the predicates refers_to. The classes for Huntingtons disease, gene and gene-disease association linked with altered gene
expression are defined by the nifstd, bio2rdf and SIO ontologies respectively.
Mina et al. Journal of Biomedical Semantics 2015, 6:5 Page 4 of 12
http://www.jbiomedsem.com/content/6/1/5
Figure 2 Genomic overlap assertion model. Orange diamonds refer also here to a RDF resource that was defined by this nanopublication,
whereas the gene (pink diamond) is defined by a bio2rdf resource. The classes promoter and biological region were defined by the nifstd ontology.
The Semanticscience Integrated Ontology (SIO) and Sequence Ontology (SO) were used for the predicates overlaps_with and associated_with.
practice and 2) interoperability issues at the ontology level
is a shared responsibility with ontology developers and
curators who provide mappings between ontologies and
with higher level ontologies.
The process of nanopublication modeling can be min-
imized when previous examples are used as templates
for similar data. For instance, the nanopublication mod-
els presented here can serve as templates for exposing
differentially expressed genes in a disease condition. We
demonstrate the reuse of our own template of Figure 2 by
exposing 5 types of nanopublications concerning genomic
overlap. The reuse of templates improves interoperability
of scientific results beyond the interoperability that RDF
already provides. It facilitates crafting assertions while
ensuring that the same URIs are used for the same type of
data.
Provenancemodel
Publishing information is meaningful only if there is
enough supporting information for reproducing them. For
example Ioannidis et al., pointed out that they could not
reproduce the majority of the 18 articles they investigated
describing results from microarray experiments, includ-
ing selected tables and figures [21]. Nanopublication does
not guarantee full reproducibility, but as a model for com-
bining data with attribution and provenance in a digital
format it at least makes it possible to trace the origin
of scientific results. The provenance section of a nanop-
ublication ties the results (the nanopub assertion) to a
description of an experiment and the associated mate-
rials, conditions and methods. The main purpose is to
capture as accurately as possible where the assertion came
from and what the conditions of our experiment were
by aggregating and annotating resources that were used
throughout the experiment.
In our case the experiment is in-silico: a workflow
process that combines existing data sources to expose
JOURNAL OF
BIOMEDICAL SEMANTICS
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 
DOI 10.1186/s13326-015-0023-3
RESEARCH ARTICLE Open Access
Sequential pattern mining for discovering
gene interactions and their contextual
information from biomedical texts
Peggy Cellier1*, Thierry Charnois2*, Marc Plantevit3, Christophe Rigotti4, Bruno Crémilleux5,
Olivier Gandrillon6, Jir?í Kléma7 and Jean-Luc Manguin5
Abstract
Background: Discovering gene interactions and their characterizations from biological text collections is a crucial
issue in bioinformatics. Indeed, text collections are large and it is very difficult for biologists to fully take benefit from
this amount of knowledge. Natural Language Processing (NLP) methods have been applied to extract background
knowledge from biomedical texts. Some of existing NLP approaches are based on handcrafted rules and thus are time
consuming and often devoted to a specific corpus. Machine learning based NLP methods, give good results but
generate outcomes that are not really understandable by a user.
Results: We take advantage of an hybridization of data mining and natural language processing to propose an
original symbolic method to automatically produce patterns conveying gene interactions and their characterizations.
Therefore, our method not only allows gene interactions but also semantics information on the extracted interactions
(e.g., modalities, biological contexts, interaction types) to be detected. Only limited resource is required: the text
collection that is used as a training corpus. Our approach gives results comparable to the results given by
state-of-the-art methods and is even better for the gene interaction detection in AIMed.
Conclusions: Experiments show how our approach enables to discover interactions and their characterizations. To
the best of our knowledge, there is few methods that automatically extract the interactions and also associated
semantics information. The extracted gene interactions from PubMed are available through a simple web interface at
https://bingotexte.greyc.fr/. The software is available at https://bingo2.greyc.fr/?q=node/22.
Keywords: Data mining, Sequential pattern mining, Natural language processing, Information extraction, Gene
interactions
Introduction
Literature on biology and medicine represents a huge
amount of knowledge: more than 24 million publica-
tions are currently listed in the PubMed repository [1].
These text collections are large and it is difficult for biol-
ogists to fully take benefit from this incredible amount
of knowledge. A critical challenge is then to extract rel-
evant and useful knowledge spread in such collections.
Text mining and Natural Language Processing (NLP)
*Correspondence: Peggy.Cellier@irisa.fr; Thierry.Charnois@lipn.univ-paris13.fr
1INSA de Rennes, IRISA, UMR6074, F-35042 Rennes, France
2Université de Paris 13, LIPN, UMR7030, F-93430 Villetaneuse, France
Full list of author information is available at the end of the article
are rapidly becoming an essential component of vari-
ous bio-applications. These techniques have widely been
applied to extract and exploit background knowledge
from biomedical texts.
Among many tasks, a crucial issue is the annotation of
a large amount of genetic information. NLP, and Informa-
tion Extraction (IE) in particular, aim to provide accurate
processing to extract specific knowledge such as named
entities (e.g., gene, protein) and relationships between
the recognized entities (e.g., gene-gene interactions, bio-
logical functions). Databases such as BioGRID [2] or
STRING [3] store a large collection of interactions
derived from different sources and indicate which gene
© 2015 Cellier et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction
in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 2 of 12
interacts with a specified gene. However, these databases
do not support more complex requests such as: which
genes inhibit gene X? what is the biological context (e.g.,
organism, biological information) associated to a gene-
gene interaction? what is the kind of interaction between
genes X and Y? what is the modality associated to the
extracted information (related work, experimental result,
etc.)? These requests are useful for biologists since they
enable to faster point out the piece of information they
look for. Unfortunately, to the best of our knowledge, no
work has been reported yet to support these kinds of
requests. That is why in this paper we propose a method
to retrieve that kind of information.
Our method automatically discovered a human man-
ageable set of patterns that are then validated by experts to
provide linguistic patterns. In other words, thanks to the
linguistic patterns, our method not only allows gene inter-
actions but also semantics information on the extracted
interactions (e.g., modalities, biological contexts, interac-
tion types) to be detected.
The need for linguistic resources (grammars or lin-
guistic rules) is a common feature of the information
extraction methods. Indeed, those NLP approaches apply
rules such as regular expressions [4] or syntactic pat-
terns [5,6]. However, these rules are handcrafted and thus
those methods are time consuming and often devoted to
a specific corpus [7].
In contrast, machine learning based methods, for exam-
ple support vector machines or conditional random fields
[8], are less time consuming than rule-based methods.
Machine learning methods for gene interaction detec-
tion usually tackle the task as a classification problem.
Best results are obtained with kernel methods [9-12] and
some NLP parsers can be used to provide some features
to the classifier [13]. Although they provide good results,
machine learning methods still need many features. Also,
their outcomes are not really understandable by a user, nor
they can be used as linguistic patterns in NLP systems.
Furthermore, the annotation process of training corpora
requires a substantial investment of time, and cannot be
reused in other domains (some new corpora must be
annotated for new domains) [7]. A good trade-off is the
cross-fertilization of information extraction and machine
learning techniques which aims at automatically learning
the linguistic rules [14,15]. However, in most cases the
learning process is done from text syntactic parsing. For
instance, BioContextt [16] or Turku Event Extraction Sys-
tem (TEES) [17] aim at extracting biological events with
contextual informations (e.g., species involved, localiza-
tion, modality) about the biological events. Those systems
are based on a syntactic analysis. Therefore, the quality
of the learned rules relies on syntactic process results.
Still some works such as [18] or [19] do not use syntactic
parsing.
For example, Abacha and al. [19] have a corpus based
strategy close to [20] and this line of research. They aim at
learning patterns from a list of seed terms corresponding
to pairs of entities known to be in some target rela-
tions. Other works based on pattern matching as AliBaba
[21-23], learn surface patterns using sequence alignment
of sentences to derive motifs. This method is based on
a list of terms that represent interactions. Only interac-
tion patterns are learned and no new term to symbolize
interaction can be discovered.With ourmethod, linguistic
patterns are automatically learned to detect interactions
(interaction patterns) and also, at the same time, to char-
acterize the interactions (characterization patterns). In
addition, the terms and the patterns do not need to be pro-
vided. They are automatically extracted by the method. It
thus provides new knowledge.
The key idea of our approach is to take advantage of
an hybridization of data mining and NLP for Biologi-
cal Natural Language Processing (BioNLP). Data min-
ing techniques, such as extraction of frequent sequential
patterns [24], enable the discovery of implicit, previ-
ously unknown, and potentially useful information from
data [25]. Our contribution is an original method to auto-
matically produce patterns (which can be seen as a kind of
linguistic rules) from text collections.
The problem of data mining techniques is that, in
general, too many patterns are generated. That is why,
our method is based on recursive sequential pattern min-
ing with constraints from the NLP field to tackle the
discovery of gene interactions. The patterns output con-
vey a model of the interactions that are enhanced with
semantics information (modalities, biological contexts,
interactions types).
Only limited resource is required: the text collection
(used as a training corpus) which only contains sentences
with interactions and where only gene names are tagged
but not the interaction. In particular, terms and patterns
are automatically discovered from texts without other
resources.
To the best of our knowledge, there are few methods
that extract the interactions and also provide associ-
ated semantics information on the extracted interactions
thanks to the discovered patterns which are understable
and can be manually modified by a human expert.
In addition, we propose to use background knowledge,
a well-established biomedical corpora and a gene inter-
action database, in order to assess the relevance of pat-
terns, used as linguistic rules, and to help an experta to
select them. We describe a validation method based on
the idea that the relevant rules convey information that
must be consistent with the background knowledge. This
method is interesting because the validation of rules is
currently widely based on human checking which is highly
time consuming. Last but not least, we conduct extensive
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 3 of 12
experiments highlighting how our approach enables to
discover interactions and their characterizations and we
present a discussion of the results.
Method
This section presents our method to produce linguistic
rules in order to discover interactions and their character-
izations. Figure 1 gives a global view of the process.
Background: sequential pattern mining
Sequential pattern mining is a well-known technique
introduced in [24] to find regularities in database of
sequences, and for which there are several efficient algo-
rithms (e.g. [26-30]). A sequence as used in our method
is an ordered list ?i1 . . . im?, where the elements of the list
i1 . . . im are called itemsb.
A sequence S1 = ?i1 . . . in? is included in a sequence
S2 = ?i?1 . . . i?m? if there exist integers 1 ? j1 < ... < jn ? m
such that i1 = i?j1 , ..., in = i?jn . The sequence S1 is called a
subsequence of S2, and we note S1  S2. For example, we
have ?b d?  ?a b c d?.
A sequence database SDB is a set of tuples (sid, S),
where sid is a sequence identifier and S a sequence.
For instance SDB1 = {(1, ?a b c d?), (2, ?b d e?),
(3, ?a c d e?), (4, ?a d c b?)} is a database of four
sequences.
A tuple (sid, S) contains a sequence S? , if S? is a subse-
quence of S. The support of a sequence S? in a sequence
database SDB, denoted sup(S?) is the number of tuples
in the database containing S? . For example, in SDB1
sup(?b d?) = 2, since sequences 1 and 2 contain ?b d?.
Notice that for notational convenience, sometimes the rel-
ative support is used. In this case, the support sup(S?) is
the relative number of tuples in the database that contain
S? , sup(S?) = |{(sid,S) | (sid,S)?SDB?(S?S)}||SDB| .
A frequent sequential pattern is a sequence such that its
support is greater or equal to a given support threshold
minsup.
Figure 1 General framework to extract gene interactions. Figure 1 presents the overall process to detect and characterize gene interactions. There
are two steps. The first step is the extraction of patterns. Sequential patterns are mined from a learning corpus that contains sentences representing
gene interactions. In order to reduce the number of extracted patterns, constraints and recursive mining are applied. At the end, few sequential
patterns corresponding to candidate linguistic interaction or characterization rules remain. A key point is that the sequence of words expressing the
interaction in a pattern is automatically discovered. As an example, the sequence of words <AGENE, "association"> in the pattern AGENE
association with AGENE (see Table 5) where a small gap may appear between AGENE and "association" are discovered by our method. This pattern
conveys an interaction between two genes (denoted AGENE) in association. The sequential patterns are then analyzed and validated by experts.
The ones that are not relevant for interaction detection or characterization are removed. The second step is the application of those validated
patterns as linguistic rules to discover and characterize interactions in new corpora.
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 4 of 12
Extraction of sequential patterns in texts
For the extraction of sequential patterns from biological
texts, we use a training corpus which is a set of sen-
tences that contain interactions (but not annotated) and
where the genes are identified. In this paper we consider
sentences containing interactions and at least two gene
names to avoid problems introduced by the anaphoric
structuresc [31]. The training corpus, with tagged gene
names, is selected by an expert. The items are combina-
tions of lemma and POS tagsd. POS tag information is
important to disambiguate words (e.g., form the noun vs
to form the verb). The sequences of the database are the
interaction sentences where each word is replaced by the
corresponding item. The order relation between items in
a sequence is the order of words within the sentence. For
example, let us consider two sentences that contain gene
interactions: Recent studies have suggested that c-myc
may be vital for regulation of hTERT mRNA expression
and telomerase activity. and Injection of frpHE mRNA
in Xenopus embryos inhibited the Wnt-8 mediated dorsal
axis duplication. All gene names are replaced by a spe-
cific item, AGENE, and the other words are replaced by
the combinations of their lemma and their POS tag. An
excerpt of the database that contains the sequences asso-
ciated to those two sentences is given in Table 1. The
sequential patterns are extracted from this database.
The choice of a support threshold minsup is a well-
known problem in data mining. With a highminsup, only
few very general patterns can be extracted. With a low
minsup, a lot of patterns can be found. Some interesting
words, for example interaction, are not very frequent
so that we set a low value of minsup. As a consequence,
a huge set of patterns is discovered and it needs to be
filtered in order to return only relevant patterns.
Constraints and recursive mining
To reduce the number of extracted patterns, we use a
combination of data mining methods. The constraint-
based pattern paradigm (e.g., [32]) enables discovering
patterns under user-defined constraints in order to drive
the mining process towards the user objectives. Recursive
mining [33] reduces the number of patterns by extracting
their common structures.
Linguistic constraints
In pattern mining, constraints allow the user to define
more precisely what should be considered as interesting.
The most commonly used constraint is the constraint of
frequency (minsup). However, it is possible to use different
constraints [34]. In our method, in order to extract gene
interaction patterns, we use three additional constraints.
The first constraint is that the pattern must contain two
gene names, i.e. two AGENE items.
The second constraint is that the pattern must contain
at least a verb or a noun.
Finally, among the patterns that satisfy the frequency
and the two other previous constraints, we retain only the
maximal ones with respect to the inclusion order . That
last constraint allows the redundancy between patterns to
be reduced.
The constraints can be gathered in only one constraint
CG which is the conjunction of the three constraints.
SAT(CG) is the set of patterns satisfying CG.
Recursivemining
Even if the new set of sequential patterns, SAT(CG), is
significantly smaller than the initial set of all extracted
sequential patterns without constraints, it can still be too
large to be analyzed and validated by experts. To find
a limited number of patterns corresponding to general
structures among the whole pattern collection, we use
the recursive mining technique of [33]. The key idea of
this post-processing is to reduce the size of the output by
successively repeating the mining process on the patterns
themselves in order to extract the structure shared by the
patterns. More precisely, at each step, the previous set of
sequential patterns is used as a new sequential database,
and a new extraction is made. The process stops when no
Table 1 Example of a sequence database
ID Sequence
... ...
S1 ?Recent@jjstudy@nnshave@vhpsuggest@vvnthat@in/thatAGENEmay@mdbe@vbvital@jj
for@in regulation@nn of@in AGENEmrna@np expression@nn and@cc telomerase@nn
activity@nn .@sent ?
S2 ?injection@nnof@inAGENEmrna@npin@inxenopus@npembryo@nnsinhibit@vvdthe@dt
AGENEmediate@vvd dorsal@jj axis@nn duplication@nn .@sent ?
... ...
Table 1 shows an excerpt of a sequence database which contains two interaction sentences:
S1: Recent studies have suggested that c-mycmay be vital for regulation of hTERTmRNA expression and telomerase activity. and
S2: Injection of frpHEmRNA in Xenopus embryos inhibited theWnt-8mediated dorsal axis duplication..
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 5 of 12
more than k patterns are obtained by the extraction, where
k is a parameter set by the user.
Our target is to identify at least one pattern by verb or
noun that appears in the patterns in SAT(CG). So, for each
verb or noun denoted Xi, that appears in SAT(CG), we
collect the set EXi of patterns containing Xi, EXi = {s ?
SAT(CG) | ?Xi?  s}. Note that some frequent patterns can
contain more than one noun and/or verb (so several Xi).
In this case, the pattern is duplicated in the EXi of each
noun and/or verb.
For a given value of k, we apply the recursive mining
post-processing technique on each EXi . At each extraction
step we select only the patterns that satisfy CG, and use a
relative minimum support threshold minsup = 1k . That
threshold value and the maximality constraint guarantee
that recursive mining process terminates in finite steps as
proved in [35].
At the end of this post-processing of all EXi , the number
of sequential patterns cannot exceed n × k where n is the
number of verbs and nouns occurring in SAT(CG).
Selection and categorization of patterns
The sequential patterns are then analyzed and validated
by experts. The ones that are not relevant for interaction
detection or characterization are removed. The remain-
ing ones are selected as linguistic extraction rules [36].
A selected pattern is classified with respect to the kind
of information conveyed by the pattern. There are two
main classes of patterns: interaction patterns and charac-
terization patterns. The first class indicates what kind of
interaction between genes is found (e.g., inhibition). The
second class is characterization patterns. It is built by the
experts and can be completed with other classes if other
kinds of information extraction rules are found. There are
two kinds of characterization patterns: modality patterns
and biological context patterns. Examples of patterns are
discussed in Section Extracted sequential patterns.
When the experts have validated and classified all pat-
terns in the different categories, they are applied as lin-
guistic rules to discover and characterize interactions in
new corpora.
In practice, this step is not time consuming as shown
in the following and can be helped by using background
knowledge to support pattern validation as proposed
in the Section About validation of sequential patterns
as linguistic extraction rules. Detection with sequential
patterns representing interactions, modalities or biolog-
ical contexts is much more elaborated than just a co-
occurrence detection. Indeed, the order of the words and
the context are important, they provide semantics infor-
mation. For instance, the sub-categorization of the verb
given by the POS tagging indicates the passive or active
verb and identifies the direction of the interaction. Prepo-
sitions can give this information when the pattern does
not contain a verb, for example: ?activation@nn of@in
AGENE by@in AGENE?.
Note the genericity of the approach, indeed the
extracted patterns allow genetic interactions to be discov-
ered as well as physical protein interactions.
Results
In this section, we present the experiments and results.
First, the training corpus is detailled. Then, the sequential
pattern extraction is described. Finally, the results of the
application of the extracted patterns on testing corpora
are presented.
Training corpus
Genes can interact with each other through the proteins
they synthesize. Moreover, although there are conven-
tions, the same word can represent a gene name and the
protein synthesized by the gene. Biologists know from
the context if the sentence is about protein or gene. To
discover the linguistic patterns of interactions between
genes, we merge two different corpora containing genes
and proteins, to create the training corpus. The first cor-
pus contains sentences from PubMed abstracts, selected
by Christine Brune as sentences containing gene interac-
tions. It contains 1,806 sentences. That corpus is available
as a secondary data source for the learning tasks Protein-
Protein Interaction Task (Interaction Award Sub-task,
ISS) from BioCreAtIvE Challenge II [8]. The second
corpus [37] contains 2,995 sentences mentionning inter-
actions between genes selected by an expert. The union
of those two corpora results in a dataset containing 4,801
sentences about gene interactions.
Sequential pattern extraction
Datamining task
As previously mentionned, the extraction of sequential
patterns from the training corpus needs the computation
of POS tags. For this task, we use the treetagger tool [38].
In addition, for the data mining task, minsup is set to
10. It means that a sequential pattern is frequent if it
appears in at least 10 sentences (i.e. in more than 0.2%
of sentences). Indeed, with that threshold some irrelevant
patterns are not taken into account while many patterns
of true gene interactions are discovered. Note that other
experiments, not reported here, have been conducted
with greaterminsup values (15 and 20).With those greater
minsup, some relevant patterns for interaction detection
are lost.
More than 32 million of frequent sequential patterns
are extracted, with minsup equals to 10. This number is
large but the extraction takes only 15 minutes (the extrac-
tion tool is dmt4sp [39]). The application of constraints
significantly reduces the number of sequential patterns.
Indeed, the number of sequential patterns satisfying the
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 6 of 12
constraints is about 65,000. Note that the application of
the constraints was not time consuming and takes less
than two minutes. However, the number of remaining
patterns is still prohibitive for analysis and validation by
human experts.
The recursive mining also reduces significantly the
number of sequential patterns. From the extracted pat-
terns, we build a subset of patterns for each noun or verb.
The number of built subsets is 515 (365 for nouns, 150
for verbs). The recursive mining of each subset exhibits
at most k sequential patterns to represent that subset. In
this experiment, we set the parameter k to 4. It allows sev-
eral patterns to be kept for each noun or verb in order
to cover a sufficient number of different cases (for exam-
ple 4 patterns corresponding to 4 syntactic constructions
with the verb inhibit@vvn are computed). At the end of
the recursivemining, there remain 667 sequential patterns
that can represent interactions or their characterizationsf.
That number, which is significantly smaller than the pre-
vious one, guarantees the feasibility of an analysis of those
patterns by experts. The recursive mining of those subsets
is also very fast and takes about 2 minutes.
Extracted sequential patterns
The 667 remaining sequential patterns were analyzed by
two experts in 90 minutes.
The patterns are grouped together by noun or verb, the
experts have thus to classify 380 groups. But some nouns
or verbs are repeated with different POS tagged informa-
tion (e.g., analyze@vvd and analyze@vvn), so these groups
are not considered independently by the experts and it
helps the validation task (for instance both versions of
verb "analyze"I? are pruned together). Actually, there are
285 different nouns and verbs. Moreover, at this point of
the validation, the patterns are roughly split into three sets
by the experts: "interaction patterns", "characterization
patterns"I? and "not relevant".
Finally, the experts validated 232 sequential patterns
for interaction detection, 231 patterns for characteriza-
tion of interactions and they removed the remaining (i.e.
204 unuseful patterns). Indeed, the latter do not con-
vey information about interactions, in particular there
are generic verbs like "appear"I? and "contain". Among the
first group of 232 patterns, some explicitly give the type
of the interactions. For example, ?AGENE interact@vvz
with@in AGENE?, ?AGENE bind@vvz to@to AGENE?,
?AGENE deplete@vvn AGENE? and ?activation@nn of@in
AGENE by@in AGENE? describe well-known interac-
tions (binding, inhibition, activation). Note that when
the patterns are applied, zero or several words may
appear between two consecutive items of the pattern.
For example, the pattern ?AGENE interact@vvz with@in
AGENE? matches the sentence <gene_name=MYC>
interacts with <gene_name=STAT3>. and also the sen-
tence <gene_name=MYC> interacts with genes in par-
ticular <gene_name=STAT3>g.
Other patterns represent more general interactions and
express the fact that a gene plays a role in an activity of
another one. Representative patterns of this kind are for
instance ? AGENE involve@vvn in@in AGENE?, ?AGENE
play@vvz role@nn in@in the@dt AGENE? and ?AGENE
play@vvz role@nn in@in of@in AGENE?. Note that the
involve verb and the play role in phrase were not
reported in [40,41] and [21].
The second group of 231 patterns for characterization
represents other kinds of semantics information: modal-
ities or biological context, for instance, ?in@in fibrob-
last@nns AGENE AGENE? or ?the@dt possibility@nn
that@in/that AGENE AGENE?. Figure 2 depicts the tax-
onomy that we define and use in our experiments for
the characterization patterns. That taxonomy was built
with the help of the extracted patterns. The modality
patterns express the confidence in the detected interac-
tions. Modality can be seen as a kind of uncertainty [42].
We define four levels of confidence: Assumption, Obser-
vation, Demonstration and Related work, and another
subclass representing the Negation (patterns denoting
evidence of absence of interaction). For example, the
sentence "It suggests that <gene_name=MYC> inter-
acts with <gene_name=STAT3>" has a lower confidence
than "It was demonstrated that <gene_name=MYC>
interacts with <gene_name =STAT3>". The biological
context patterns indicate information about the biolog-
ical context of interactions, for example the disease or
the organism involved in the interaction. That class is
split into four subclasses: organism, component, biological
situation and biological relation. The subclass organism
represents the organisms involved in the interaction (e.g.,
Figure 2 Taxonomy for characterization patterns. Figure 2 describes the taxonomy used to classified the extracted sequential patterns.
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 7 of 12
mouse, human). The subclass component represents
the anatomy/biological components (e.g. breast or
fibroblast). The subclass biological situation gives the
framework of interactions, for example, cancer, tumor
or in vitro. The last subclass gives, when applicable, the
biological relation (e.g., homology).
The sequential patterns obtained are linguistic extrac-
tion rules that can be used on biomedical texts to detect
and characterize interactions between genes. Note that
to be applied, those patterns do not need a full syntactic
analysis of a sentence.
Indeed, the matching process tries to instantiate each
element of the pattern in the given sentence. For each pat-
tern, every possible matching within the sentence is tested
and not only the first one.
Application: detection and characterization of gene
interactions
We have evaluated the quality of the sequential patterns
found in the previous section as information extraction
rules. In this section, we present the experimental settings
and the results.
Testing corpora and evaluation criteria
Testing Corpora We have considered three well-known
testing datasets (cf Table 2 and Table 3): AIMed [43],
BioInfer [44], HPRD50 [45] and a fourth testing corpus
extracted from PubMed [1] (more information is given
in the next section). Note that, in AIMed, BioInfer and
HPRD50, the names of genes are already identified and
tagged. More information about those corpora can be
found in [46].
Construction of the PubMed corpus In order to test the
sequential patterns extracted in the previous section as
linguistic extraction rules to characterize interactions, we
need a testing corpus.
We have built a testing corpus that is a subset of
abstracts from the PubMed database. It is built in two
Table 2 Results of the application of the extracted patterns
Corpus # Recall Precision f ? score f ? score
Sentences presented in [11]
details given in Table 3
AIMed 1955 78.6 35.6 49 [34.7, 41.5]
BioInfer 1100 46.5 25.3 32.8 [15.9, 40.6]
PubMed 200 75.0 83.0 78.7 ?
HPRD50 145 66.8 46.7 55.0 [38.3, 69.8]
Table 2 gives the list of the four testing corpora used to evaluate the proposed
approach, and the results of the evaluation. The meaning of the columns is: the
name of the corpus, the number of the sentences in the corpus, the recall score
of the proposed approach applied on the corpus, the precision score of the
proposed approach applied on the corpus, thef -score of the proposed approach
applied on the corpus. The last column indicates the range of the f -scores
presented in [11] with also a cross-corpus validation.
steps which are described below. The first step is the selec-
tion of abstracts from PubMed. In the PubMed database,
each paper has an identifier called PMID (PubMed IDen-
tifier). For each official acronym of gene in the HUGO [47]
dictionary, a request is sent to PubMed in order to get
all PMID of papers that contain the gene. An index of
genes and their associated papers is thus created. Then the
inverted index is computed, i.e. the index that associates
to each PMID the list of genes. From that second index,
the PMIDs that do not have at least two gene names in
their list are pruned. Indeed, as we are looking for inter-
actions between genes, it implies that at least two genes
are mentioned in the text. There remains 624,519 PMIDs.
The second step is the named-entity recognition. Some-
times, the gene name used to index an abstract and the
gene name that appears in the abstract text are different.
Indeed, a gene can be represented by different synony-
mous forms. It is thus important to identify the gene in the
text; that task is called Named-Entity Recognition (NER).
We propose to use a "dictionary-based" approach [48].
Although that kind of approaches usually has a good pre-
cision, it does not provide a good recall. We propose some
improvements to increase the recall.
First, all genes associated to the PMID of an abstract are
searched into that abstract using official acronyms from
the HUGO dictionary. With that approach only 48.1% of
abstracts have at least two recognized genes. In addition,
we identified 182 official acronyms as common English
words (e.g., AGO, AS, BAD)h. In order to reduce the num-
ber of mistakes, they are considered as gene names only
when they are in uppercase.
Second, in order to improve the number of recognized
genes in abstracts, other fields of the HUGO dictionary
are used: old acronyms, alias acronyms, and complete
names. With that improvement, 61.7% of abstracts have at
least two recognized genes. Note that this improvement is
mainly due to the alias acronyms (+ 9%).
The last improvement is the use of significant parts of
the complete official name. The official name is often long,
and authors do not write it completely. Instead of look-
ing for the complete name, we look for significant parts
of it. We identify three common kinds of significant parts:
a word ending by in (e.g., insulin), a word ending by
ase (e.g., transferase) and a word followed by protein
(e.g., AE binding protein 1). For instance, for gene alka-
line ceramidase 3, the significant part is ceramidase and
thus to recognize this gene name in texts, only cerami-
dase would be used. The plural forms are also taken into
account (e.g., caspases, kinases). With that improve-
ment, 66.1% of the 624,519 abstracts have at least two
recognized genes and form the testing corpus.
Evaluation criteria for the extraction of gene inter-
actions and their contextual information We evaluate
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 8 of 12
Table 3 Details of information presented in paper [11]
Method SL SL SpT SpT kBSPS kBSPS edit edit APG APG
Training corpus (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer)
AIMed - 41.5 - 34.7 - 40.3 - 39.6 - 37.9
BioInfer 40.6 - 24.3 - 24.8 - 15.9 - 22.5 -
HPRD50 59.0 61.8 43.2 51.3 51.0 69.8 38.3 62.4 61.6 62.1
The acronyms used in this table are the ones used in paper [11]: SL: Shallow linguistic kernel; SpT: Spectrum tree kernel; kBSPS: k-band shortest path spectrum kernel;
edit: Edit distance kernel; APG: All-paths graph kernel. See paper [11] for more details.
our approach with a cross-corpus evaluation to show the
genericity of the proposed approach. It means that we
extract the patterns from a corpus and apply them on the
other four corpora [11]. Note that in the literature many
approaches are evaluated with a cross-validation, which
means that a corpus is split in several parts, one part is
used to learn and the rest is used to apply.
It is thus much more difficult to get good results with a
cross-corpus evaluation than a cross-validation.
Indeed, there is more heterogeneity between corpora
(i.e., corpus characteristics are different) than between
parts of a single corpus [11].
We use the f -score function as an evaluation measure,
which is defined as f -score = 2×Precision×RecallPrecision+Recall .
Detection of gene interactions
We have applied the 232 extracted sequential patterns as
linguistic extraction rules to detect interactions on the
four corpora.
All corpora used for evaluation have all gene names
readily tagged. This means that our results only mea-
sure the performance of gene interaction extraction and
are not influenced by the issue of named entity recogni-
tion. Therefore, to compute the f-score, a true positive is
a couple of mentioned gene names in the sentence (i.e.
the gene names given in the tags) which are in interac-
tion and detected as an interaction by our method. Table 2
gives the results. We did not have any gold standard
reference to evaluate the results for the testing corpus
from PubMed. Since we cannot implement an automatic
validation, we randomly took 200 sentences among the
sentences of the PubMed testing corpus. Then, we car-
ried out a POS tagging and assessed the performances of
the extraction rules to detect interactions in the 200 sen-
tencesi. The f -score for the gene interaction detection for
the testing corpus is 78.7. In Table 2, the last column indi-
cates the range of the f -scores presented in [11] with a
cross-corpus validation. Several kernel-based approaches
are presented in [11], the range allows to show the worst
and the best results among all those methods. Note that
the best result of the ranges is not achieved in practice
by the same method. Our approach gives results com-
parable to the results given by state-of-the-art methods
and it is even better for the gene interaction detection in
AIMed. This last result is important because AIMed is the
largest corpus and the most commonly used in the liter-
ature. Moreover, our approach is simple and allows more
information that just the presence of an interaction to be
extracted. Indeed, thanks to the patterns, semantics infor-
mation can also be extracted, contextual information (see
next section) but also information about the kind of inter-
action (e.g., inhibition, binding) and the direction of the
interaction.
Characterization of gene interaction
The method also gives information about modality and
about the biological context: biological situation, compo-
nent, organism, biological relation. For that characteri-
zation task, there exist some methods dealing with the
subtask of the detection of sentences containing uncer-
tainty [42] (modality can be seen as a kind of uncertainty)
but few adress the biological characterization problem. It
was thus difficult to compare our result for the interac-
tion characterization with a gold standard. We randomly
took 200 sentences containing at least two gene names
among the sentences of the testing corpus extracted from
PubMed. Those sentences are not the same ones that are
used to evaluate the interaction detection but they come
from the same testing corpus PubMed. Out of 200 inter-
actions, there are 149 characterizations (71modalities and
78 biological context). The sentences have been annotated
by a computer scientist with specialisation in NLP and a
biologist. Then, we evaluated the precision and recall. The
characterization patterns are applied on a pair of genes
that is already detected as in interaction. We evaluate the
characterization at the interaction level. The precision is
88% and the recall is 69% (f -score= 77). Several reasons
explain why the recall is not greater and are discussed in
the next section.
Discussion
In this section, the results of the previous section are dis-
cussed from a qualitative point of view and we present a
process to support pattern validation.
About interaction detection
In the experiments a linguistic pattern is matched against
a whole sentence at a time. That wide scopemay introduce
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 9 of 12
ambiguities in the detection of interactions, and false pos-
itives, when more than two genes appear in a sentence.
For example, in sentence FGF-7 recognizes one FGFR iso-
form known as the FGFR2 IIIb isoform or keratinocyte
growth factor receptor (KGFR), whereas FGF-2 binds well
to FGFR1, FGFR2, and FGFR4 but interacts poorly with
KGFR. an interaction between FGFR2 IIIb and FGFR1 is
detected. Actually, there is no interaction between those
two genes, they only appear in two different propositions
of the same sentence. FGFR1 interacts with FGF-2 in the
second proposition but since there is no limitation of the
scope, an interaction between FGFR1 and FGFR2 IIIb is
also detected. Several cases are possible: when several
binary interactions are present in the sentence or when the
interaction is n-ary (n ? 3). The case of n-ary interactions
can be solved with a training data set containing n-ary
interactions. The other cases can be treated by introduc-
ing limitations of pattern scope, for example cue-phrases
(e.g., but, however).
False negatives depend on the absence of some nouns
or verbs of interaction in the patterns. For example, the
noun modulation is not discovered whereas the verb
modulate appears in sequential patterns. This suggests
that the use of linguistic resources (e.g. lexicon or dic-
tionary), manually or semi-automatically, would improve
interaction patterns and thus interaction detection.
About interaction characterization
The false negatives, which are dependent on the absence
of some patterns, are also an important problem for inter-
action characterization.
For example, in our experiments in the sentence
<gene_name=BRCA1> interacts in vivo and in vitro
with the Rb-binding proteins, <gene_name=RBBP7> and
<gene_name=RBBP4>[ ...] the biological situation in
vitro is detected whereas in vivo is not detected. Indeed,
there is no sequential pattern extracted from the training
corpus that contains in vivo. That case is considered as
true positive for in vitro interaction and as false negative
for in vivo interaction. The recall (69%) is strongly depen-
dent on the number of false negatives. Note that the false
negatives mainly come from biological contexts not suf-
ficiently represented (about 92%). It is explained by the
difficulty to have a training corpus that contains all bio-
logical context (e.g, body parts as liver, pituitary gland,
diseases). As for interaction detection, using a special-
ized lexicon would increase the vocabulary and thus the
number of patterns and would improve those results.
About validation of sequential patterns as linguistic
extraction rules
Section Method shows how the sequential patterns are
automatically extracted from a corpus. Those patterns are
then analyzed and validated by two experts as linguistic
extraction rules. But sometimes, the needed resources
(e.g., time, expert) can be missing or the number of
sequential patterns can be too large to be easily managed
by a human. In those cases, for the selection and validation
of patterns, we propose an automatic process based on the
use of background knowledge. The selection is thus less
accurate than a manual selection but can be automatic.
The automatic validation process is based on two steps.
First, each sequential pattern is applied on a corpus
called rule validation corpus. It provides for each pattern
the following information: the genes detected as interact-
ing and the associated sentences.
Second, a gene interaction database is used as an oracle
to assess the patterns. In our method, the rule valida-
tion corpus comes from the PubMed papers and the gene
interaction database is BioGRID. Our idea is that the rel-
evant patterns, when applied on the validation corpus,
retrieve interactions that must be consistent with the gene
interaction database. An interaction detected by a sequen-
tial pattern is considered as a false positive if the interac-
tion does not exist in the gene interaction database, else it
is a true positive (same gene names and same PMID)j.
A pattern with a high number of true positives is likely
to be interesting.
Table 4 gives an excerpt of the information provided
for each pattern. It contains the number of interactions
Table 4 Examples of information about the application of
information extraction rules
Number of Number of
Information retrieved interactions true positives
extraction rule
AGENE AGENE the@dt 6 1
response@nn
AGENE AGENE serine@nn 3 3
AGENE reveal@vvd AGENE 0 undefined
AGENE association@nn 6 4
with@in AGENE
AGENE bind@vvz to@to AGENE 8 5
Table 4 gives an excerpt of provided information about patterns extracted from
the PubMed corpus. The meaning of the columns is: sequential pattern, number
of interactions detected by the pattern and number of detected interactions that
are correct with respect to the oracle, i.e. interactions that also exist in BioGRID.
The first pattern can be read as a gene followed by a gene then by the word the
and the word response. This pattern detects 6 interactions and 1 is in BioGRID.
The second pattern can be read as a gene followed by a gene then by the word
serine. It detects 3 interactions that are all in BioGRID. The third pattern can be
read as a gene followed by the verb reveal in past tense, then by a gene. This
pattern does not detect interactions in the rule validation corpus, thus no
information is provided to evaluate it. The fourth pattern can be read as a gene
followed by the noun association, then by the word with and a gene name. It
detects 6 interactions out of which 4 are in BioGRID. The fifth pattern can be read
as a gene followed by the verb bind in present tense, then by the word to and a
gene name. This pattern detects 8 interactions and 5 of them are in BioGRID.
For example, it detects that the following complex sentence Cbl is a cytosolic
protein that is rapidly tyrosine phosphorylated in response to Fc receptor
activation and binds to the adaptor proteins Grb2, CrkL, and Nck. contains an
association between two signalling molecules (Cbl and Grb2).
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 10 of 12
detected by the pattern and the number of detected inter-
actions that are correct with respect to the oracle, i.e.
interactions that also exist in BioGRID. For example, the
fifth pattern can be read as a gene followed by the verb
bind in present tense, then by the word to and a gene
name. This pattern detects 8 interactions and 5 of them
are in BioGRID. For example, it detects that the follow-
ing complex sentence Cbl is a cytosolic protein that is
rapidly tyrosine phosphorylated in response to Fc receptor
activation and binds to the adaptor proteins Grb2, CrkL,
and Nck. contains an association between two signalling
molecules (CBL and GRB2).
Those measures can be used to automatically select
patterns as linguistic information extraction rules.
To end up with a more speculative note, this step could
also be interesting even when there is an expert to select
the patterns by providing more information to help them.
In addition, a pattern with low number of true positives
can retrieve sentences that really contain interactions.
This can be the case if the interaction is not reported
in BioGRID or is reported but the gene names in the
sentence are other gene names than the ones used in
BioGRID. Therefore, it is interesting to provide, for each
pattern, the detected interaction sentences. Table 5 gives
an excerpt of interactions detected by the sequential pat-
tern: AGENE association with AGENE. For instance, the
pattern detects that in the paper with PMID 10204582, an
interaction between genes SHC1 and CRKL is mentioned
(the pattern matches a sentence of the abstract) but
according to BioGRID, there is no interaction between
SHC1 and CRKL. The discovered interaction in the sen-
tences in paper 10204582 is thus unexpected in BioGRID.
The pattern also detects that in this paper, an interaction
between genes CBL and CRKL is mentioned, and indeed,
according to BioGRID, there is an interaction between
CBL and CRKL mentioned in paper 10204582. It is inter-
esting to note that the three genes involved harbour all
three similar biological functions (they are all signalling
molecules) and that their association is fully relevant as
exemplified by the strong functional connectivity detected
in the STRING database between those three genes.
Therefore the extracted interaction between genes SHC1
and CRKL is fully relevant even if it does not appear in
BioGRID. Of course, more systematic studies should be
undertaken to ascertain this, but this is beyond the scope
of the present paper.
Conclusions
We have proposed an original approach to help experts
to design linguistic information extraction rules by auto-
matically extract sequential patterns filtered by linguistic
constraints and recursive mining. Unlike existing meth-
ods, our approach is independent of syntactic pars-
ing and only requires the training corpus as external
resource to learn patterns (note that interaction clues
are not annotated in the training corpus). The patterns
representing interactions and their characterizations are
automatically discovered from texts. An advantage of
the use of sequential patterns as linguistic rules is that
they are understandable and manageable by an expert.
If needed, the expert can easily modify the proposed
rules or add new ones. To the best of our knowledge,
there are few methods that automatically extract inter-
action patterns and also characterization patterns (i.e.,
patterns for contextual information about the discovered
interactions).
Table 5 Example of information for a sequential pattern
PMID Gene 1 Gene 2 BioGRID verdict Sentence
10204582 SHC1 CRKL not in BioGRID These results suggest a fundamental role for the
tyrosine phosphorylation of Cbl, CrkL, SLP-76, and
<gene_name="SHC1"> and the association
of Cblwith<gene_name="CRKL">, SLP-76,
and Nck in Fc gammaRI signaling in human
macrophages.
10204582 CBL CRKL in BioGRID PP1, a specific inhibitor of Src kinases, inhibited
the Fc gammaRI-induced respiratory burst,
as well as the tyrosine phosphorylation of
<gene_name="CBL"> and its inducible
association with<gene_name="CRKL">.
Table 5 gives 2 interactions (highlighted in bold in the table) detected by the sequential pattern: AGENE association with AGENE.
The meaning of the columns is: the id number of the paper in PubMed, the genes that interact, the verdict of the oracle and the sentence where the interaction is
recognized. For instance, the pattern detects that in paper 10204582, an interaction between genes SHC1 and CRKL is mentioned, because the pattern matches a
sentence of the abstract of the paper, but according to BioGRID, there is no interaction between SHC1 and CRKL and the discovered interaction in the sentences in
paper 10204582 is unexpected because not in BioGRID and interesting. The pattern also detects that in this paper, an interaction between genes CBL and CRKL is
mentioned, and indeed, according to BioGRID, there is an interaction between CBL and CRKL mentioned in paper 10204582.
Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 11 of 12
The experiments show how our approach enables to
discover interactions and their characterizations. Our
approach gives results comparable to the results given by
state-of-the-art methods and is even better for the gene
interaction detection in AIMed. The main advantages
of our approach are that, first, semantics information
are extracted in addition to the information about the
presence of an interaction; second, the patterns, used as
extraction linguistic rules, are automatically discovered.
Further work will look how enhance the extracted pat-
terns thanks to other information sources (e.g., specialized
dictionaries).
Availability of software and supporting data
The extracted gene interactions from PubMed are
available at https://bingotexte.greyc.fr/. The evaluation
corpora from PubMed are available at https://cremilleux.
users.greyc.fr/jbms/. The software (SMBio) that allows
sequential patterns of gene interactions to be extracted is
available at https://bingo2.greyc.fr/?q=node/22. The list of
the 182 official acronyms identified as common English
words is available at: https://bingotexte.greyc.fr/ambig_
names.
Endnotes
aIn the rest of the paper, the term expert is used for a
linguist or a biologist; both skills are useful to validate
rules.
bNotice that this is a simplified form of sequences,
while in the general sequential pattern mining
framework, a sequence is a list of sets of items, and not
only a list of items.
cAn anaphoric structure is the use of a linguistic unit,
such as a pronoun, to refer back to a gene name.
dPOS (Part-Of-Speech) tags are grammatical
information about words. For example, nnmeans
common noun and vvpmeans verb in non-3rd personal
singular present. The exhaustive list of POS tags can be
found at: http://www.cis.uni-muenchen.de/~schmid/
tools/TreeTagger/.
eInstitut de Biologie du Développement de
Marseille-Luminy.
fThe maximum number of patterns per verb and noun
is 4, thus the maximum number of patterns after
applying recursive mining is 2060. The number of
patterns in the results is only 667 because some verbs
and nouns are represented by less than 4 patterns.
gNote that such a functional relationship between
MYC and STAT-3 can be illustrated by the fact that the
expression of the c-myc gene is under the control of the
STAT-3 signalling pathway (see [49] for a review).
hThe list is available at: https://bingotexte.greyc.fr/
ambig_names.
iDiscovered interactions for the whole testing corpus
are available at https://bingotexte.greyc.fr/.
jBioGRID provides information about gene
interactions and the PMID of the articles where the
interactions are mentioned. In order to measure the
accuracy of the patterns, we take into account these two
pieces of information.
Abbreviations
NLP: Natural language processing; NER: Named-entity recognition; IE:
Information exctraction; BioNLP: Biological natural language processing; POS:
Part-Of-Speech; minsup: support threshold; PMID: PubMed IDentifier.
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PC, TC, MP, CR and BC developped the interaction detection and
characterization method, and the validation step. Experiments were mainly
conducted by MP, PC and TC. JK helped to design the evaluation. JLM and TC
developped the named-entity recognition method. OG provided the
biological expertise. JLM built the website interface. All authors read and
approved the manuscript.
Acknowledgements
This work is partly supported by the ANR (French Research National Agency)
funded project Hybride ANR-11-BS002-002 and progam Investissements
dAvenir ANR-10-LABX-0083.
Author details
1INSA de Rennes, IRISA, UMR6074, F-35042 Rennes, France. 2Université de Paris
13, LIPN, UMR7030, F-93430 Villetaneuse, France. 3Université Lyon 1, LIRIS,
UMR5205, F-69622 Lyon, France. 4INSA de Lyon, LIRIS, UMR5205, F-69621
Lyon, France. 5Université de Caen, GREYC, UMR6072, F-14032 Caen, France.
6Université Lyon 1, CGMC, UMR5534, F-69622 Lyon, France. 7Faculty of
Electrical Engineering, Czech Technical University, Prague, Czech Republic.
Received: 25 July 2013 Accepted: 22 April 2015
JOURNAL OF
BIOMEDICAL SEMANTICS
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 
DOI 10.1186/s13326-015-0007-3RESEARCH Open AccessUsing description logics to evaluate the
consistency of drug-class membership relations in
NDF-RT
Rainer Winnenburg1,2, Jonathan M Mortensen1,2 and Olivier Bodenreider1*Abstract
Background: The NDF-RT (National Drug File Reference Terminology) is an ontology, which describes drugs and
their properties and supports computerized physician order entry systems. NDF-RTs classes are mostly specified
using only necessary conditions and lack sufficient conditions, making its use limited until recently, when asserted
drug-class relations were added. The addition of these asserted drug-class relations presents an opportunity to
compare them with drug-class relations that can be inferred using the properties of drugs and drug classes in NDF-RT.
Methods: We enriched NDF-RTs drug-classes with sufficient conditions, added property equivalences, and then used
an OWL reasoner to infer drug-class membership relations. We compared the inferred class relations to the recently
added asserted relations derived from FDA Structured Product Labels.
Results: The inferred and asserted relations only match in about 50% of the cases, due to incompleteness of the drug
descriptions and quality issues in the class definitions.
Conclusions: This investigation quantifies and categorizes the disparities between asserted and inferred drug-class
relations and illustrates issues with class definitions and drug descriptions. In addition, it serves as an example of the
benefits DL can add to ontology development and evaluation.
Keywords: Ontology, Description logics, Quality assurance, National drug file-reference terminologyIntroduction
We rely on ontologies throughout biomedicine, from the
life sciences to the clinic [1]. As Electronic Health
Record adoption increases in the clinic, so too will the
reliance on the ontologies that facilitate their meaningful
use. Clinical decision support and analytics are functions
supported by ontologies. For example, computerized
physician order entry (CPOE) systems typically leverage
drug ontologies to ensure that patients are safely
prescribed drugs in accordance with clinical guidelines
(e.g., [2]).
An example of such an ontology is the National Drug
File-Reference Terminology (NDF-RT), an extension to
the drug formulary used by the Veterans Administration
and developed using a description logics (DL) formalism.* Correspondence: olivier@nlm.nih.gov
1Lister Hill National Center for Biomedical Communications, National Library
of Medicine, National Institutes of Health, Bethesda, MD, USA
Full list of author information is available at the end of the article
© 2015 Winnenburg et al.; licensee BioMed Ce
Commons Attribution License (http://creativec
reproduction in any medium, provided the or
Dedication waiver (http://creativecommons.or
unless otherwise stated.It provides a rich description of pharmacologic classes in
reference to properties, such as mechanism of action,
physiologic effect, chemical structure and therapeutic
intent. NDF-RT can be leveraged to prevent a patient
allergic to penicillin drugs from being prescribed amoxi-
cillin, a penicillin antibacterial.
However, NDF-RT only specifies necessary conditions
for class membership to the pharmacologic classes, but
not sufficient conditions. (In DL parlance, these classes
are primitive, not defined.) As a consequence, a DL
reasoner is unable to classify automatically drugs as
members of a given pharmacologic class, even when
both drugs and pharmacologic classes are described in
terms of the same properties. The inability to classify
drugs into their classes limits the usefulness of NDF-RT
in systems like CPOE that rely on such information.
In previous work, where we overcame this limitation
by augmenting the pharmacologic classes with necessary
and sufficient conditions, we found that we could inferntral. This is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
iginal work is properly credited. The Creative Commons Public Domain
g/publicdomain/zero/1.0/) applies to the data made available in this article,
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 2 of 9drug-class membership relations effectively [3]. Specific-
ally, we demonstrated the use of a modified version of
NDF-RT for clinical decision purposes (patient classifica-
tion). One limitation of this work was that we did not
evaluate the inferred drug-class membership relations
beyond our proof-of-concept application.
NDF-RT recently integrated authoritative drug-class
membership assertions extracted from the Structured
Product Labels (package inserts) by the Food and Drug
Administration (FDA), along with a specification of the
drugs in terms of the same properties used for specifying
the classes. These assertions remove the drug-class
membership limitation we highlighted earlier, instead
providing explicit drug-class membership relations that
do not rely on DL reasoning. But precisely because these
asserted drug-class relations have been made independ-
ently of the logical definitions of the classes, there is the
possibility for the asserted and inferred drug-class mem-
bership relations to be inconsistent.
The objective of this work is to evaluate the consistency
of the drug-class membership relations that were inferred
from the pharmacologic class definitions and drug
descriptions, against the newly asserted, authoritative
drug-class membership relations. This evaluation is also
an indirect contribution to the assessment of the class def-
initions and the drug descriptions in terms of complete-
ness and consistency (i.e., agreement between information
sources).
Background
NDF-RT drugs and classes
The National Drug File Reference Terminology (NDF-
RT) is a resource developed by the Department of
Veterans Affairs (VA), Veterans Health Administration,
as an extension of the VA National Drug File [4]. Like
other modern biomedical terminologies, NDF-RT is de-
veloped using description logics and is available in native
XML format. The version used in this study is the latest
version available, dated November 3, 2014, downloaded
from [5], from which we derived our augmented
representation.
This version covers 7,287 active moieties (DRUG_
KIND, level = ingredient), as well as 543 Established
Pharmacologic Classes (EPCs) specified in reference to
some of the properties of the active moieties. NDF-RT
now contains several sources of relations between drugs
and their properties. The April 2014 version of NDF-RT
introduced a new set of relations between drugs and
their properties originating from the class indexing file
released as part of DailyMed, identified by the suffix
FDASPL. Moreover, this version also introduced au-
thoritative drug-class membership assertions from the
same source. Finally, NDF-RT also provides a specifica-
tion of the EPCs in reference to the same propertiesused for describing the drugs themselves, provided by
Federal Medication Terminologies subject matter ex-
perts and identified by the suffix FMTSME. In this
work, we focus on the drug-property assertions from
FDASPL, class-property assertions from FMTSME, and
drug-class assertions provided by the FDA.
Description logics
In short, Description Logics (DL) are a set of logical
constructs with which one can develop ontologies.
Krötzsch and colleagues provide a more formal intro-
duction to DL [6]. Like other knowledge representation
methods, DL allows one to specify, in a computable
fashion, the entities (i.e., classes) that exist in a given do-
main and the relationships (i.e., relations) between them.
In comparison to older methods of knowledge represen-
tation, DL ensures common, unambiguous semantics so
that the ontologys interpretation is consistent across
software and users. This consistent logical underpinning
enables the use of reasoners, which are programs that
compute (i.e., infer) the logical entailments (i.e., conclu-
sions) of a given ontology. For example, if Alprostadil
has physiologic effect Venous dilation and Venous dila-
tion is-a Vasodilation, a reasoner concludes that Alpros-
tadil has physiologic effect Vasodilation. A typical
approach to developing ontologies with DL is to specify
a set of properties that each class has (e.g., Penicillin
antibacterial has ingredient Penicillin and treats or pre-
vents Bacterial infection; Antiseptic treats or prevents
Bacterial infection) and then infer the additional rela-
tions among classes. With a set of specified classes, a
reasoner can then classify them into an inferred hier-
archy. In our example, the inferred hierarchy would
show that Penicillin antibacterial is-a Antiseptic. In the
context of this study, NDF-RT uses this same approach,
specifying EPCs in terms of their properties. Unlike the
example above, however, pharmacologic classes in NDF-
RT (EPCs) are primitive, in that they only specify the
necessary conditions of class membership, and therefore
prevent a reasoner from constructing a useful inferred
hierarchy. Later, we describe how we enrich NDF-RT
with sufficient conditions so that we can take full advan-
tage of a reasoner.
In this work, we use OWL, the web ontology language,
a web standard for developing ontologies that leverages
DL. OWL is the de facto standard for biomedical ontol-
ogies and there is a suite of tools for developing OWL
ontologies, including development environments such as
Protégé [7] and reasoners such as HermiT [8].
Related work
In addition to being used as a framework for building on-
tologies, DL has been shown to be useful for reasoning
with biomedical entities, including protein phosphatases
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 3 of 9[9] and penetrating injuries [10]. However, to our know-
ledge, DL reasoning has not yet been applied to the auto-
matic classification of drugs, except for our previous work
on anti-coagulants [3].
NDF-RT is used frequently as a resource for standard-
izing pharmacologic classes (e.g., [11,12]). However, in-
vestigators generally use the drug properties as classes
(e.g., drugs that have the physiologic effect decreased co-
agulation activity for anti-coagulants), rather than the
EPCs. Moreover, only asserted relations are used in most
investigations, as opposed to inferred drug-class relations.
The specific contribution of this paper is the augmen-
tation of the logical definitions of pharmacologic classes
in NDF-RT to enable the automatic inference of drug-
class membership relations using a DL reasoner. We
substantially extend our previous work on anticoagu-
lants, by generalizing it to all pharmacologic classes and
providing a comparison to authoritative, asserted drug-
class relations from the FDA.
Methods
Our approach to evaluating inferred drug-class member-
ship relations in NDF-RT is summarized as follows.
First, we converted the NDF-RT data from their original
format (XML) to a DL format (OWL). This conversion
process augments the EPCs with necessary and sufficient
conditions. These conditions allowed a DL reasoner to
classify drugs into their respective classes using the class
definitions and the properties of drugs. We created two
OWL datasets. One, used as a gold standard, only con-
tains the asserted, authoritative drug-class relations. In
contrast, these asserted relations have been removed
from the second dataset, so that only inferred drug-class
relations were present after the reasoner runs (i.e., in-
ferred by the reasoner). We ran a DL reasoner and then
compared inferred and asserted drug-class relations
from the perspective of drugs and from that of classes.
In order to restrict this investigation to clinically sig-
nificant drugs, we mapped all NDF-RT ingredients to
RxNorm and required that ingredients be linked to clin-
ical drugs. We further normalized all ingredients to base
ingredients in RxNorm, to abstract away from minor dif-
ferences in ingredients, including salts, esters and com-
plexes, which rarely affect drug-class membership. In
practice, we mapped the precise ingredients in RxNorm
(e.g., albuterol sulfate) to their base ingredient (albuterol).
Multi-ingredient drugs were ignored, because there is
often more variability in their classification.
Augmenting pharmacologic classes with sufficient
conditions
In order to produce the two OWL datasets used for
comparing asserted and inferred drug-class relations, we
started by creating a baseline OWL representationfrom the original XML dataset, which we used as our
asserted dataset (dataset A). Next, as previously de-
scribed in [3], we transformed the primitive EPCs into
defined classes by taking the existing set of properties
for each class (i.e., necessary conditions) and using them
to define the class. In particular, all properties are
folded into a single owl:equivalentClass (?)
axiom, thereby specifying necessary and sufficient condi-
tions of each class. For the purpose of this work, we
focus on the three main properties used for the descrip-
tion of the drugs (mechanism of action, physiologic
effect and chemical structure). Additionally, we lever-
aged the therapeutic intent relations (may_treat and
may_prevent) present in NDF-RT, because many EPCs
refer to them in their definitions. These relations link
drugs and EPCs to disease entities.
We further modified this OWL file by applying a series
of transformations that are necessary for enabling proper
inference (dataset I). We harmonized the names of
roles used in the definition of the classes (e.g., has_
MoA_FMTSME) with those used in the description of
the drugs (e.g., has_MoA_FDASPL) by creating owl:
equivalentProperty axioms between them. The
following equivalences are created:
 has_MoA_FMTSME ? has_MoA_FDASPL (for
mechanism of action),
 has_PE_FMTSME ? has_PE_FDASPL (for
physiologic effect),
 has_Chemical_Structure_FMTSME ?
has_Chemical_Structure_FDASPL,
 may_treat_FMTSME ?may_treat_NDFRT, and
 may_prevent_FMTSME ?may_prevent_NDFRT.
Inferring relations between drugs and EPCs
Next, we leveraged an OWL reasoner to infer the drug-
class membership relations from the class definitions
and the descriptions of drugs. Using the necessary and
sufficient conditions we created for the classes, an OWL
reasoner infers a subclass relation between a drug and a
pharmacologic class when the properties of the drug and
those of the pharmacologic class are shared. For ex-
ample, the class beta2-Adrenergic Agonist [EPC]
(N0000175779) is defined as equivalent to ('Pharmaceut-
ical Preparations' and (has_MoA_FMTSME some
'Adrenergic beta2-Agonists [MoA]')). The drug albuterol
(N0000147099) has the property has_MoA_FDASPL
some 'Adrenergic beta2-Agonists [MoA]', and is therefore
inferred as being a subclass of beta2-Adrenergic Agonist
[EPC]. (The inference will also occur if the property of
the drug is a subclass of the property used in the defin-
ition of the class). Figure 1 provides a schematic of the
above example.
Figure 1 Method overview. Relations between the drug albuterol and the class beta2-Adrenergic Agonist [EPC], with asserted and inferred drug-class
relations. Note that there is only one direct path from ingredients to pharmacologic classes through the recently added yellow asserted drug-class
relation. In this study, we compare how often inference using the properties, which produces the dashed orange line, recapitulates the solid yellow line.
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 4 of 9A secondary benefit of the classification with an OWL
reasoner is that it creates a hierarchy of the pharmaco-
logic classes themselves, based on their logical defini-
tions. For example, beta2-Adrenergic Agonist [EPC]
(N0000175779) is inferred to be a subclass of beta-
Adrenergic Agonist [EPC] (N0000175555), because the
definition of beta2-Adrenergic Agonist [EPC] shown
earlier is more specific than that of beta-Adrenergic
Agonist [EPC] ('Pharmaceutical Preparations' and
(has_MoA_FMTSME some 'Adrenergic beta-Agonists
[MoA]')). For this reason, we reclassified both OWL
datasets, although no inferred drug-class relations
were generated in dataset A.
Figure 2 provides a screenshot from Protégé of a
pharmacologic class before enrichment and Figure 3
shows its definition after. Before enrichment, the class
beta2-Adrenergic Agonist [EPC] has no sufficient condi-
tions (the section Equivalent To is empty) and the
EPCs are not hierarchically related (beta2-Adrenergic
Agonist [EPC] and beta-Adrenergic Agonist [EPC] are at
the same hierarchical level, i.e., part of a flat list of
EPCs). The drug albuterol is asserted to be a member of
the class beta2-Adrenergic Agonist [EPC]. In contrast,
after enrichment (and reclassification), the class beta2-
Adrenergic Agonist [EPC] has acquired sufficient condi-
tions (visible in the section Equivalent To) and the
EPCs are now hierarchically related (beta2-Adrenergic
Agonist [EPC] is a subclass of beta-Adrenergic Agonist
[EPC]). The drug albuterol is inferred to be a member of
the class beta2-Adrenergic Agonist [EPC].
Comparing asserted and inferred drug-class relations
We compared asserted (dataset A) and inferred (data-
set I) drug-class relations from the perspective of drugs
and pharmacologic classes, respectively. In both cases,we issued queries against the OWL datasets (after re-
classification). For each drug, we queried its set of
pharmacologic classes in each dataset and determined
which classes are common to both datasets vs. specific to
one dataset. For example, the drug albuterol
(N0000147099) has the same class in both datasets, beta2-
Adrenergic Agonist [EPC] (N0000175779). In contrast, the
drug hydrochlorothiazide (N0000145995) has an asserted
relation to Thiazide Diuretic [EPC] (N0000175419), but
an inferred relation to Thiazide-like Diuretic [EPC]
(N0000175420). For each pharmacologic class, we queried
its set of drugs in each dataset and determined which
drugs are common to both datasets vs. specific to one
dataset. In order to consider higher-level classes to which
drugs are not direct members, we used the transitive clos-
ure of the hierarchical relation rdfs:subClassOf. As a
consequence, a given class will have as members not only
its direct drugs, but also the members of all its subclasses.
For example, in both the A and I datasets, the class
beta-Adrenergic Agonist [EPC] has the base ingredient
albuterol as an indirect member through its subclass class
beta2-Adrenergic Agonist [EPC]. Of note, the salt ingre-
dient albuterol sulfate is ignored as a result of the
normalization to RxNorm base ingredients described
earlier.
Implementation
The modifications described above were performed
using an XSL (eXtensible Stylesheet Language) trans-
formation. The resulting OWL file was classified with
HermiT 1.2.2 [8]. Protégé 5.0 was used for visualization
purposes [7]. The OWL file containing the inferences
computed by the reasoner was loaded in the open source
triple store Virtuoso 7.10 [13]. The query language
SPARQL was used for querying drug-class relations
Figure 2 Primitive class Adrenergic Decongestant [EPC]. beta2-Adrenergic Agonist [EPC] appears as a primitive class in the default distribution
of NDF-RT.
Figure 3 Defined class Adrenergic Decongestant [EPC]. The appearance of beta2-Adrenergic Agonist [EPC]in Protégé after augmenting it with
sufficient conditions.
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 5 of 9
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 6 of 9Results
Asserted and inferred drug-class relations
Drugs
Of the 7,352 drugs (at the ingredient level) in NDF-RT,
3,351 are identifiable as clinically relevant ingredients in
RxNorm. After normalization to base ingredients, 2,247
drugs remain, of which 1,308 have at least one relation
to a pharmacologic class (EPC). As shown in Table 1, all
but 48 drugs (1,260) have asserted drug-class relations
and 1,011 drugs have inferred relations. 963 drugs have
both asserted and inferred relations.
Pharmacologic classes
Of the 553 pharmacologic classes (EPC) in NDF-RT, 463
have relations to drugs, of which all but five (458) have
asserted relations and 340 have inferred relations (as
shown in Table 2). In total, 335 of the 463 classes have
both asserted and inferred relations to drugs.
Drug-class relations
As shown in Figure 4, there are 1,396 asserted and 1,125
inferred direct drug-class relations, of which 825 (59%
and 77%, respectively) are in common. Of the asserted
relations, 571 (41%) could not be inferred, whereas 300
(27%) inferred relations are not present in the asserted
set. Considering the transitive closure of the hierarchical
relation rdfs:subClassOf (for the drug class per-
spective), we obtain 2,211 asserted and 1,513 inferred
drug-class relations, of which 1,332 (40% and 88%, re-
spectively) are in common. Of the asserted relations 879
(40%) could not be inferred, whereas 181 (12%) inferred
relations are not present in the asserted set.
Perspective of drugs
For each drug, we compare the set of (direct) pharmaco-
logic classes in datasets A and I. The various types of
differences observed between asserted and inferred
drug-class relations are presented in Table 1. The largest
category corresponds to drugs with identical sets of
asserted and inferred drug-class relations (50%). For ex-
ample, the drug imatinib has the same class KinaseTable 1 Drug-class relations (direct), drug perspective
Drugs related to drug classes
Drugs with identical sets of classes for the asserted and inferred drug-class re
Drugs with compatible sets of classes (each class from the asserted is identical t
Drugs with additional drug-class relations in the asserted set only
Drugs with additional drug-class relations in the inferred set only
Drugs with additional drug-class relations in both the asserted and inferred s
Drugs with asserted drug-class relations only (no inferred relations)
Drugs with inferred drug-class relations only (no asserted relations)
Total number of related drugsInhibitor [EPC] in both datasets. Drugs with asserted
drug-class relations, but lacking inferred drug-class rela-
tions represent 23% of the cases. For example, the drug
losartan has the class Angiotensin 2 Receptor Blocker
[EPC] in dataset A, but no class in dataset I.
Perspective of pharmacologic classes
For each pharmacologic class, we compare the set of
(direct and indirect) drug members in datasets A and
I. The various types of differences observed between
asserted and inferred drug-class relations are presented
in Table 2. As we observed for drugs, the largest cat-
egory corresponds to EPCs with identical sets of asserted
and inferred drug-class relations (52%). For example, the
class Monoamine Oxidase Inhibitor [EPC] has the same
five drugs in both datasets, including isocarboxazid and
rasagiline. EPCs with asserted drug-class relations, but
lacking inferred drug-class relations also represent
about 27% of the cases. For example, the class Quin-
olone Antibacterial [EPC] has eight drugs in dataset
A, including ofloxacin and levofloxacin, but no mem-
bers in dataset I.
Discussion
Disparities between asserted and inferred drug-class
relations
Missing inferences
As mentioned in the results, the largest category of dis-
parity is represented by missing inferred drug-class rela-
tions, including cases where there are no inferred
relations at all and cases where inferred relations only
cover part of the asserted relations. Missing inferences
should not be interpreted as an inherent failure of the
OWL reasoner to identify drug-class relations, but ra-
ther as issues with the completeness and quality of class
definitions and drug descriptions (see below for details).
For example, the reason why the drug lurasidone, a drug
indicated for the treatment of schizophrenia, has an
asserted, but not inferred drug-class relation to Atypical
Antipsychotic [EPC] is because the therapeutic intent of
lurasidone (Schizophrenia and Disorders with Psychotic# %
lations 660 50.46
o or hierarchically related to a class in the inferred set) 127 9.71
68 5.20
73 5.58
et 35 2.68
297 22.71
48 3.67
1308 100.00
Table 2 Drug-class relations (direct and indirect), class perspective
Drug classes related to drugs # %
Classes with identical sets of drugs for the asserted and inferred drug-class relations 242 52.27
Classes with additional drug-class relations in the asserted set only 55 11.88
Classes with additional drug-class relations in the inferred set only 20 4.32
Classes with additional drug-class relations in both the asserted and inferred set 18 3.89
Classes with asserted drug-class relations only (no inferred relations) 123 26.57
Classes with inferred drug-class relations only (no asserted relations) 5 1.08
Total number of related classes 463 100.00
Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 7 of 9Features) is not described in the dataset. In fact, there is
no drug property asserted for lurasidone by FDASPL.
Another example is the drug ofloxacin mentioned earl-
ier. In this case, the asserted EPC (Quinolone Antimicro-
bial [EPC]) is not inferred because its definition includes
both may_treat Infectious Diseases and may_prevent
Infectious Diseases, while the drug description only in-
cludes treatment, not prevention (e.g., may_treat 'Klebsi-
ella Infections). Similarly, the description of the drug
ipilimumab is too underspecified to match the definition
of its asserted class, CTLA-4-directed Blocking Antibody
[EPC]. In addition to has_MoA CTLA-4-directed Anti-
body Interactions, which is in the drug description, the
SOFTWARE Open Access
owlcpp: a C++ library for working with OWL
ontologies
Mikhail K. Levin and Lindsay G. Cowell*
Abstract
Background: The increasing use of ontologies highlights the need for a library for working with ontologies that is
efficient, accessible from various programming languages, and compatible with common computational platforms.
Results: We developed owlcpp, a library for storing and searching RDF triples, parsing RDF/XML documents,
converting triples into OWL axioms, and reasoning. The library is written in ISO-compliant C++ to facilitate efficiency,
portability, and accessibility from other programming languages. Internally, owlcpp uses the Raptor RDF Syntax library
for parsing RDF/XML and the FaCT++ library for reasoning. The current version of owlcpp is supported under Linux,
OSX, and Windows platforms and provides an API for Python.
Conclusions: The results of our evaluation show that, compared to other commonly used libraries, owlcpp is significantly
more efficient in terms of memory usage and searching RDF triple stores. owlcpp performs strict parsing and detects
errors ignored by other libraries, thus reducing the possibility of incorrect semantic interpretation of ontologies. owlcpp is
available at http://owl-cpp.sf.net/ under the Boost Software License, Version 1.0.
Background
Ontologies are being increasingly recognized as import-
ant information resources that capture descriptive infor-
mation in a standardized, structured, and computable
form. One of the most widely used approaches for repre-
senting ontologies is the family of languages referred to
as the Web Ontology Language (OWL) [1]. The OWL
languages were designed to represent ontologies for use
in the Semantic Web and were therefore built on the
W3C semantic web stack, which includes XML, XML
Schema, RDF, and RDF Schema [25].
Working with OWL ontologies involves several com-
mon procedures, including parsing ontology documents,
storing them as RDF triples and axioms, querying and
serializing their in-memory representation, passing the
axioms to a reasoner, and performing logical queries.
Given the increasing size of ontologies, it is extremely
important to have software for working with OWL on-
tologies that can perform these procedures efficiently.
During the last decade, many open-source libraries
useful for working with OWL ontologies written in RDF
+XML format have become available. These, however,
fail to fully meet the needs of software developers build-
ing software for working with OWL ontologies. First,
existing libraries do not scale well enough to support
ontologies of over a few million triples [6, 7]. In
addition, the majority of them are implemented in
non-native languages, which are usually less efficient
and involve significant overhead when accessed from
other languages [8]. For example, the libraries with the
most extensive functionality, OWL API [9, 10] and
Apache Jena [11, 12], are implemented in Java. Given
the difficulties of accessing Java from other languages,
it is not surprising that the recent Perl and Python li-
braries ONTO-PERL [13], RDFLIB [14], and FuXi [15],
replicate some of the functionality already present in
OWL API and Jena. Redland RDF framework is imple-
mented in C and provides utilities for parsing, storing,
and querying RDF triples [16, 17]. Its native code base
allows it to more easily expose its API in several other
languages and to be usable on virtually any platform,
including mobile devices [18]. The functionality of
Redland is limited, though, because it does not directly
support OWL.
We sought to fill this gap by developing a library with
the following key features: (i) supports fast loading and
searching of large ontologies, (ii) has a small memory
* Correspondence: lindsay.cowell@utsouthwestern.edu
Department of Clinical Sciences, University of Texas Southwestern Medical
Center, 5323 Harry Hines Boulevard, Dallas, TX, USA
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2015 Levin and Cowell. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 
DOI 10.1186/s13326-015-0035-z
footprint, (iii) provides cross-platform compatibility, and
(iv) can be accessed from multiple programming lan-
guages. The resulting library, owlcpp, is designed to sup-
port a common workflow where OWL ontologies
written in RDF/XML are loaded from the file system
and submitted to a reasoner for processing (Fig. 1).
owlcpp is implemented in standard C++ and is aimed
primarily at C++ and Python software developers. Here
we discuss major design features and describe the results
of an evaluation in which we compared the loading time,
query time, and memory footprint of owlcpp and several
other libraries across a set of ontologies of varying size
and composition (Table 1).
Implementation
The design choices in implementing owlcpp were based
on providing the four key features listed above. Specific-
ally, owlcpp is developed in ISO-compliant C++03 [19],
which ensures source-level portability, supports gener-
ation of language bindings, facilitates creation of concise
and expressive APIs [20], and compiles into efficient ex-
ecutables. The API for owlcpp was designed to be concise
without sacrificing usability and performance. In the inter-
est of clarity and thread-safety, class methods and function
arguments were declared const wherever possible.
The compatibility of owlcpp with different platforms was
verified by compiling the library and executing the unit
tests on the following platforms and compilers: Linux,
Ubuntu 14.04 64-bit (gcc v4.8, Clang 3.5); Windows 7
64-bit (Microsoft Visual C++ 13); Mac OS X 10.6.8
(i686-apple-darwin10-gcc 4.2.1); Windows XP 32-bit
(Microsoft Visual C++ 9, MinGW gcc 4.5.2).
Currently, owlcpp comprises three modules, rdf, for
storing and searching RDF terms and triples; io, for load-
ing ontology documents; and logic, for converting triples
into axioms and passing them to a reasoner. The io
module depends on the Redland Raptor [21], libXML2
[22], and iconv [23] libraries, and the logic module de-
pends on FaCT++ [24, 25]. The io and logic modules
have different external dependencies and can be built
and used separately from each other. owlcpp also uses
many of the Boost libraries, e.g., iterator, multi-index,
and file system [26].
The basic features of owlcpp, as well as those of other
key libraries, are shown in Table 2.
rdf module
The rdf module implements classes and methods as
needed to support the RDF standard. To accommodate
the demands of working with large ontologies, the de-
sign priorities for the module were compact in-memory
representation of RDF terms and triples, and their effi-
cient search and retrieval. The Triple_store class is
the main container provided by the module. It supports
storing, retrieving, and searching for prefix IRIs, RDF
terms, ontology document descriptions, and RDF triples
(Fig. 2). The library uses light-weight IDs to point to
prefix IRIs (Ns_id), terms (Node_id), and document
descriptions (Doc_id). The IDs for prefixes and terms
Fig. 1 Workflow of the owlcpp library. Owlcpp loads RDF/XML
documents from the file system, parses them using the Raptor RDF
Syntax library, stores RDF triples in a triple store, converts the triples
to OWL axioms, and passes these to FACT++ for reasoning
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 2 of 9
standardized by RDF and OWL are defined by the li-
brary as compile-time constants.
The RDF standard defines three types of terms imple-
mented in owlcpp as the Node_iri, Node_literal,
and Node_blank classes (Fig. 3) [4]. These are
accessed through the interface defined by the abstract
Node class. A Node_iri object represents a prefix IRI
with an optional fragment identifier. A Node_literal
object stores an ID pointing to a Node_iri defining
the datatype (e.g., xsd:date, rdf:PlainLiteral), a
value as an appropriate internal type defined by sub-
classes of Node_literal, and, in the case of a string-
valued literal, a language. A Node_blank object stores
the ID of the document in which it was defined and an
integer uniquely identifying the blank node within the
document. In this way, blank nodes from different docu-
ments added to the same triple store are always kept
distinct.
The ontology document descriptions (Doc_meta
class) store Node_ids for the ontology and version IRIs
and for the file system path for the ontology document.
The Triple class represents RDF triples by defining
a combination of subject, predicate, and object terms.
Since each term may appear in many triples, the
Triple class stores a light-weight ID rather than a
value for each of the terms. In addition, Triple stores
the ID of the source document.
Since term IDs do not distinguish between different types
of terms, the Triple class cannot, by itself, enforce the
type restrictions on its terms. For example, it is possible to
create a triple where the subject term ID refers to a literal
or where the predicate term ID refers to a blank node.
While such triples will not be created during normal ontol-
ogy parsing, it should still be noted that the Triple class
implements a generalized RDF triple [4].
Searching stored triples is a frequent, performance-
sensitive operation. The types of searches are application
dependent and may involve matching any combination
of subject, predicate, object, and document, while leav-
ing other elements unspecified. To efficiently perform
the required types of searches, owlcpp stores triples in
several indices. Within an index, triples are separated
Table 1 Ontologies used for evaluating owlcpp
Name Size Terms Triples Axioms
MB IRI Literal Blank
OBP [29] 3.0 935 2366 5056 25,924 6109
OBI [30] 6.1 4106 10,115 8709 75,666 32,800
Uberon [31] 62.0 32,078 118,935 87,713 579,388 56,956
OpenGALEN parta 130.5 45,404 8423 771,980 2,004,170 187,893
VTO [37] 149.2 110,418 502,521 103,801 1,358,341 829,796
MESH [38] 193.8 916,056 249,740 4345 1,667,128 1,654,092
DRON [39]b 214.8 344,403 322,903 322,902 2,281,817 1,313,110
Biomodels [40] 253.2 232,214 535,725 481,909 2,686,610 1,905,822
OpenGALEN [41] 546.3 127,042 56,469 3,508,389 8,724,486 555,740
aOpenGALEN8_DD_2_Chapters.owl and its imports
bdron- ndc.owl
Table 2 Basic features of owlcpp and other similar libraries
Feature owlcpp Redland Jena OWL API
Load RDF/XML ? ? ? ?
Serialize RDF/XML  ? ? ?
Turtle I/O  ? ? ?
OWL/XML, Functional, Manchester I/O    ?
Search RDF triples ? ? ? 
Convert RDF to axioms ?  ? ?
Access FaCT++ reasoner ?   ?
Access to other reasoners (Chainsaw, JFact, HermiT, Pellet, RacerPro)    ?
Axiom API    ?
C/C++ API ? ?  
Python API ? ?  
Java API  ? ?
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 3 of 9
into bins according to one term and sorted within each bin
according to the other terms. For example, an index in
which triples are binned by subject and sorted by predicate,
object, and document can help to efficiently identify the tri-
ples matching a subject and a predicate. On the other hand,
if matching an object and a predicate is required, an index
configured to bin by object and sort by predicate, subject,
and document is expected to perform better.
Since the indices have a significant memory footprint,
the user can define the number and type of indices dur-
ing compilation. By default, owlcpp uses two indices: (i)
bin by subject and sort by predicate, object, and docu-
ment, and (ii) bin by object and sort by predicate, sub-
ject, and document. These defaults were selected
because they were empirically found to perform best for
axiom generation, as described below.
Fig. 2 A diagram of the owlcpp Triple_store class, objects it stores, and some implemented methods. The Triple_store class serves as
a container for prefix IRIs (Ns_iri), different types of RDF terms, which are accessed through the abstract Node interface, ontology document
descriptions (Doc_meta), and RDF triples (Triple). Each object can be retrieved through an overloaded square bracket operator by supplying
the objects appropriately typed ID
Fig. 3 The RDF term type hierarchy as implemented in owlcpp. The RDF standard defines three types of terms implemented in owlcpp as
the Node_iri, Node_literal, and Node_blank classes, which are accessed through the abstract Node interface. The abstract
Node_literal class is further subtyped by concrete literal classes that represent the literal values as appropriate internal types, e.g.,
Node_bool for boolean types, Node_int for integers, etc. String-valued literals may also specify the language of their string values
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 4 of 9
Searching stored RDF triples is done using the
find_triple method provided by Triple_store.
The method is designed to automatically select the opti-
mal search procedure based on the type of query and
the available triple indices. The results of the search are
returned as an iterator range [27], which is both efficient
and convenient, because the range can be used to test
the success of the search, to obtain the first match, or to
iterate over all matching triples.
io module
The io module provides several methods for loading
RDF/XML ontology documents to a triple store. The
documents can be loaded from the C++ Standard Library
input streams using method load or directly from the
filesystem using load_file. Loading an ontology docu-
ment involves reading data from a stream, parsing XML
structures, interpreting them as RDF statements, convert-
ing the statements into RDF terms and triples, and insert-
ing them into a triple store. Currently, the Redland Raptor
library is used as an RDF parser [16], which, in turn, relies
on libxml [22] for parsing XML and iconv [23] for charac-
ter encoding support.
The io module is designed for early detection of and
recovery from errors. The errors may originate at differ-
ent levels of the document loading process, such as
opening a file or parsing XML, RDF, or OWL. An error
occurring for any reason during document loading
aborts the process by throwing an exception containing
detailed information about its causes. In addition, in-
stead of producing a triple store that contains an un-
certain number of triples from the document that
caused the error, the exception leaves the triple store
in a valid state with none of the documents content in
the triple store. This behavior allows the user to utilize
the existing triple store content or to attempt loading
the document again.
OWL documents often import the contents of other
documents, identifying them by their ontology IRIs and
version IRIs. The io module provides a mechanism to
automatically load the imports from the file system. To
be able to locate the documents, the module implements
a Catalog class that stores document descriptions and
maps the document, ontology and version IRIs to file
system paths. The module also provides a method for
scanning file system directories for ontology documents
and adding their metadata to the catalog. Supplying a
Catalog object to the load or load_file methods
causes the module to also load the imported documents
to the triple store. Automatically loading documents
from the Internet is not currently supported by the
module because this feature introduces a significant un-
certainty to the success and performance of ontology
loading and adds complex operating system-specific de-
pendencies to the library.
logic module
The logic module is responsible for translating RDF tri-
ples into OWL axioms and facilitating interaction with
reasoners. Translation of triples to axioms is imple-
mented by following the W3C Recommendation [28]. If
the triples do not meet some of the stated requirements,
the process is aborted by throwing an exception contain-
ing detailed information about its causes. Axioms can
be generated from the entire store, or from a subset of
the triples. Generating an axiom associated with a par-
ticular triple usually requires information stored in
other triples, which are found by searching the triple
store. The axiom generation algorithm searches the
triple store by subject, by subject and predicate, and by
predicate and object.
Frequent search operations make configuration of
triple indices an important factor affecting axiom gener-
ation performance. The optimal configuration was iden-
tified empirically by comparing axiom generation times
using eight hand-picked index configurations and three
different ontologies: the Ontology of Biological Pathways
(OBP) [29], the Ontology for Biomedical Investigations
(OBI) [30], and the Integrated Cross-species Anatomy
Ontology (Uberon) [31]. For Uberon, the largest ontol-
ogy of the three, selecting the best index configuration
reduced the axiom generation time by a factor of 2.5
thousand.
Currently, the logic module works with FaCT++, which
is, to our knowledge, the only open-source C/C++ reasoner
library for OWL DL [24]. Logical queries are currently per-
formed directly through the FaCT++ interface.
Concurrency
Although owlcpp does not provide explicit support for
concurrency, similar to C++ Standard Library con-
tainers, it is designed to maximize the number of
thread-safe operations without penalizing performance.
Operations that do not change the state of owlcpp con-
tainers (e.g., all const methods) are guaranteed to be
thread-safe. On the other hand, if multiple threads con-
currently access an owlcpp container object and at least
one of the threads modifies its state (e.g., inserts an RDF
triple), the behavior is undefined. Therefore the user is
expected to ensure that a modifying thread obtains ex-
clusive access to owlcpp containers.
Build system
The build system for owlcpp is based on Boost.Build and
is compatible with both Unix-like and Windows plat-
forms [32, 33]. It is responsible for compilation and link-
ing of static and shared variants of the library, as well as
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 5 of 9
the sample executables according to the configuration
provided by the user. The system also builds the re-
quired third-party dependencies from their sources, gen-
erates a Distutils module of owlcpp Python bindings,
and produces API documentation using Doxygen [34].
Unit tests
Unit tests comprise approximately 20 % of the library
source code and cover most of its functionality. A separ-
ate test suite is implemented for each of the owlcpp
modules. The tests for the io and logic modules make
use of many small sample ontology documents that are
part of the project source tree. Some of the documents
are designed to test error detection at the XML, RDF, or
OWL level. Some of the sample documents used by the
logic module tests were adapted from the OWL 2 Test
Cases [35]. The tests verify expected consistency of the
ontologies and perform more specific logical queries. All
unit tests are executed by the build system with a single
command.
Python bindings
The current version of owlcpp includes bindings for
Python developed using the Boost.Python library. The
functionality of the bindings is verified by a separate
unit test suite. The bindings and their dependencies
are packaged by the build system into a distributable
Python module. APIs for other programming lan-
guages can also be exposed from owlcpp relatively eas-
ily and with minimal overhead. In future versions of
owlcpp, these will be provided with the help of the
SWIG library [36].
Results
Evaluation
To evaluate owlcpp, we compared the document loading
time, triple query time, and memory foot print of owlcpp
with those of Redland, Jena, and OWL API. Evaluation
was conducted using five ontologies of varying size and
composition: the Vertebrate Taxonomy Ontology (VTO)
[37], Medical Subject Headings ontology (MESH) [38],
Drug Ontology (DRON) [39], Biomodels Ontology [40],
and OpenGALEN [41]. In addition, a one quarter por-
tion of OpenGALEN (OpenGALEN part) was used. The
statistics of the ontologies, including filesystem foot-
prints and the counts for RDF terms, triples, and axioms
are listed in Table 1.
To test the ability of the library to process large ontol-
ogies on off-the-shelf hardware, the tests were con-
ducted on an underpowered, by current standards,
computer. Performance was tested on a laptop with an
Intel Core2 Duo T7700 2.40GHz CPU, 3GB of RAM,
running Linux Ubuntu 14.04 64-bit. The performance of
Java libraries was tested using Oracle Java JDK v1.7.0_51
with a 2 GB maximum memory pool (-Xmx2048m). Un-
less noted otherwise, owlcpp was compiled with gcc v4.8
using the default triple index configuration. Redland v1.0.13
with a hashed in-memory store, Jena v2.12.1 with an in-
memory RDF store, and OWL API v4.0.1 were used for
comparison, also with their default settings. The source
code for performance tests can be found in Additional file
1. All testing was done using owlcpp v0.3.5.
Ontology loading performance
To evaluate ontology loading performance, each of the
six ontologies was loaded into each of the four libraries.
Some libraries were unable to load the larger ontologies.
Jena was unable to load the complete OpenGALEN
ontology, while Redland failed to load the complete
OpenGALEN, Biomodels, and DRON ontologies. An at-
tempt to load the complete OpenGALEN into Redland
on a system with 32GB of RAM was also unsuccessful.
The loading rates (size of the ontology file system foot-
print divided by the recorded loading time) is shown in
Fig. 4a. The ontology loading rate of owlcpp ranges from
3.1 to 6.9 MB/s, while the range for the other libraries is
from 2.6 to 7.2 MB/s. The owlcpp loading rate is faster
than that of Jena for the five ontologies Jena could load
and faster than that of Redland for two of the three on-
tologies Redland could load. Redland had a faster load-
ing rate than owlcpp for MeSH. In addition, the owlcpp
loading rate is faster than that of OWL API for four of
the six test ontologies. OWL API has a faster loading
rate for both OpenGALEN full and part.
Memory footprint
The amount of memory required by each library during
ontology loading was estimated by probing the resident
set size of the process virtual memory. The peak RAM
utilization normalized by the size of the ontology on the
file system is shown in Fig. 4b. Of the libraries tested,
owlcpp had the smallest memory footprint for all ontol-
ogies ranging from 1.8 to 3.2 bytes of RAM required for
each byte on the file system. The same ratio ranged from
7.0 to 11.5 for Redland, from 6.8 to 10.7 for Jena, and
from 2.6 to 7.0 for OWL API.
Triple search efficiency
Searching by subject and predicate is the most common
triples search during axiom generation. Therefore, the
triple search performance of the libraries was tested by
repeating queries where the subject was selected at ran-
dom, and the predicate was rdfs:subClassOf. For
each query, all matching triples were identified and
counted. The number of queries for each test was se-
lected so as to keep the test time at about one minute.
The number of queries performed by each library di-
vided by the elapsed time is shown on Fig. 4c. The
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 6 of 9
owlcpp library showed significantly higher search rates
ranging from 3.0 to 3.2 million queries per second (MQ/s).
Jena showed significantly lower rates from 0.53 to
0.83 MQ/s. The rates for Redland ranged from 0.2 to
0.69 MQ/s. Note that OWL API was not included in this
evaluation because it stores axioms rather than triples.
Accuracy and error detection
In addition to evaluating the performance of owlcpp, we
wanted to assess the accuracy of parsing and axiom gen-
eration. This was done using the OWL 2 Test Cases
[35], some of which are incorporated into owlcpp unit
tests. Further testing was done during the development
of the Ontology of Biological Pathways (OBP) [29] by
executing queries formulated by domain experts and
comparing the results with ones from Protégé running
with either FaCT++ or the HermiT reasoner plug-in
[42]. The results of the queries were always identical.
Strict error checking has proven to be an important
feature of owlcpp, helping to avoid incorrect semantic
interpretation of ontologies and facilitating their devel-
opment. Examples of errors detected by owlcpp but ig-
nored by OWL API and Jena are undeclared property
and annotation predicates and misspelled standard
OWL terms.
Discussion
owlcpp is a C++ library providing support for storing
and searching RDF terms and triples, for loading RDF/
XML documents along with their imports into a triple
store, for generating OWL axioms based on stored tri-
ples, and passing axioms to the FaCT++ reasoner. To
the best of our knowledge, owlcpp is the first C++ library
for working with OWL ontologies.
Our primary goal was to design a library for software
developers that would scale well for working with large
ontologies. To facilitate use by software developers, we
designed owlcpp to have a concise and expressive C++
API and an efficient Python API. For example, loading
an ontology file into an owlcpp triple store can be
Fig. 4 Performance comparison of the owlcpp, Redland, Jena, and OWL
API libraries. The measurements were done using the following
ontologies (by size, see Table 1): part of OpenGALEN (OG part), VTO,
MESH, DRON, Biomodels, and complete OpenGALEN (OG full). The bars
showing performance measurements are color-coded by library and
grouped by ontology. The standard deviations of the measurements are
shown as error bars. The corresponding bars are not shown if ontology
loading failed. a shows ontology loading ratesontology size divided
by loading time. b shows the RAM footprint of each library after
ontology loading normalized by the filesystem size of the ontology.
c shows triple store querying rates. Each query identifies all triples
matching a combination of a random subject and a constant
predicate. Triple querying rates for OWL API are not shown be-
cause this operation is not supported
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 7 of 9
accomplished with just two lines of code, whereas the
same operation through the Redland Raptor library API
requires over a dozen lines [43]. The API for RDF triple
store search is another example. In owlcpp, a single
method, Triple_store::find_triple(), can be
used to search for triples matching a specific subject,
predicate, object, document, or any combination thereof.
The search is performed without sacrificing performance
by selecting the most suitable triple index at compile
time. The result of the search, an iterator range, can be
used transparently to determine whether the triple store
contains a triple matching the specified condition, to re-
trieve the first matching triple, or to iterate over all
matching triples. On the other hand, the triple stores of
both Redland and Jena define over ten different methods
for searching triples.
Of critical importance to the utility of owlcpp is ensur-
ing its scalability for use with large ontologies. Thus, we
designed owlcpp to have a compact, in-memory storage
of RDF terms and triples, efficient indexing of stored tri-
ples, and no virtual machine requirement. The latter fa-
cilitates owlcpps deployment in HPC environments. To
evaluate the scalability of owlcpp, we compared its mem-
ory footprint, ontology document loading time, and
triple query time with those of Jena, Redland, and
OWLAPI. We found that owlcpp has a smaller memory
footprint than the other three libraries for all ontologies
tested (Fig. 4b), and we find that the ontology document
loading time is faster for owlcpp than the other libraries
for all tests with two exceptions (Fig. 4a): Redland was
faster loading MesH, and OWLAPI was faster loading
OpenGALEN or a part of OpenGALEN. The lower per-
formance of owlcpp with the MeSH ontology is probably
due to this ontologys low ratio of triples to IRIs. In
MeSH, each IRI appears, on average, in 1.8 triples,
whereas in other ontologies this ratio ranges from 6.6 to
69. This property of MeSH increases the relative cost of
IRI parsing, while diminishing the benefit of utilizing IRI
IDs. Ontology loading by owlcpp is slower for OpenGA-
LEN, either full or part, than for the other ontologies.
This is probably due to a 50 % greater number of triples
per megabyte in the OpenGALEN ontology.
While interpreting the performance measurements of
the owlcpp, Redland, Jena, and OWL API libraries, it is
important to note significant differences in their archi-
tecture. owlcpp and Redland are natively-compiled li-
braries, whereas Jena and OWL API run under Java
virtual machine and exhibit less deterministic perform-
ance and memory footprint due to just-in-time compil-
ation and garbage collection. Furthermore, while owlcpp,
Redland, and Jena store the documents in memory as a
set of RDF triples, OWL API immediately converts the
triples into axioms and annotations, which, arguably,
can be stored in memory more compactly. Nevertheless,
the performance comparison is useful because it helps
predict the hardware requirements for a task and reflects
on the overall user experience.
There are several limitations of owlcpp, which will be
addressed in future versions. First, RDF/XML is the only
OWL format currently supported by owlcpp. Future ver-
sions will introduce support for additional syntaxes, par-
ticularly Manchester, Turtle, and OWL/XML. Second,
owlcpp doesnt currently provide a Java API, and is
therefore not interoperable with most of the currently
available RDF/OWL tools. In future versions, we will
provide a Java API. Third, although it is possible to
manually add more nodes and triples to an owlcpp triple
store, it is not currently possible to save the new RDF
graph. Another important limitation of owlcpp is the
lack of a description logic expression and axiom inter-
face for axiom editing. Future versions will include this
and will also improve readability of error messages, pro-
vide options for less strict parsing and axiom generation,
and include a module for batch execution of OWL 2
Test Cases. Finally, future versions of owlcpp will pro-
vide an axiom-based in-memory data structure.
Conclusions
owlcpp presents a number of benefits for developers and
users. Its compact datamodel and efficient execution
make it possible to work with large ontologies using off-
the-shelf hardware. As a native library, owlcpp does not
depend on a virtual machine installation, facilitating its
deployment in HPC environments. The C++ and Python
APIs of owlcpp are concise and expressive and facilitate its
integration with other software modules. Currently, owlcpp
is used in many groups to work with biological ontologies
as well as in other fields including virtual reality, robotics,
image analysis, and answer set programming.
Availability and requirements
Project name: owlcpp
Project home page: http://owl-cpp.sourceforge.net/
Operating system(s): Cross-platform (tested: Linux,
Windows, Mac)
Programming language: C++, Python
Other requirements: Boost, libxml2, iconv (under
Windows), Raptor, FaCT++
License: Boost Software LicenseVersion 1.0
Any restrictions to use by non-academics: none
Additional file
Additional file 1: This file contains the code used for the
performance comparison. (PDF 49 kb)
Competing interests
The authors declare that they have no competing interests.
Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 8 of 9
Authors contributions
MKL developed the software, tested the performance, and prepared the first
draft of the manuscript. LGC provided expert guidance, revised the
manuscript, and wrote the final version. Both authors read and approved the
final manuscript.
Authors information
MKL is a Contractor Application Developer at the Bank of America, Charlotte,
NC. LGC is an Associate Professor in the Division of Biomedical Informatics,
Department of Clinical Sciences, UT Southwestern Medical Center. owlcpp
was developed while MLK was an Instructor at UTSW.
Acknowledgements
The authors would like to thank Dmitri Tsarkov for help with the FaCT++
library; Alan Ruttenberg and Wac?aw Ku?nierczyk for useful discussions; and
Anna Maria Masci, Federico Bozzo, Simone Miraglio, and Andrés Samuel for
testing owlcpp. This work was supported by an NIAID-funded R01 (AI077706)
and a Burroughs Wellcome Fund Career Award to LGC.
Received: 18 October 2014 Accepted: 4 September 2015
REVIEW Open Access
Special issue on bio-ontologies and
phenotypes
Larisa N. Soldatova1*, Nigel Collier2, Anika Oellrich3, Tudor Groza4, Karin Verspoor5, Philippe Rocca-Serra6,
Michel Dumontier7 and Nigam H. Shah7
Abstract
The bio-ontologies and phenotypes special issue includes eight papers selected from the 11 papers presented at
the Bio-Ontologies SIG (Special Interest Group) and the Phenotype Day at ISMB (Intelligent Systems for Molecular
Biology) conference in Boston in 2014. The selected papers span a wide range of topics including the automated
re-use and update of ontologies, quality assessment of ontological resources, and the systematic description of
phenotype variation, driven by manual, semi- and fully automatic means.
Introduction
The special issue on bio-ontologies and phenotypes in-
cludes selected papers that were presented at the Bio-
Ontologies SIG and the Phenotype Day at ISMB (Intelli-
gent Systems for Molecular Biology) conference in 2014.
Over the 17 years, the Bio-Ontologies SIG at ISMB
has provided an environment for discussion of ontol-
ogies, their applications to biology, and more generally
the organisation, presentation and dissemination of
knowledge in biomedicine and the life sciences. In 2014
the bio-ontologies SIG ran an extended event called the
Phenotype Day, which focused on the systematic de-
scription of phenotypic variation. The Phenotype Day
brought together researchers across many disciplines to
discuss phenotype-related issues and resources, and to
share their experience with defining, representing, pro-
cessing and using phenotype data.
The 2-day event on July 11th and 12th co-located with
ISMB 2014 in Boston, received 38 submissions, includ-
ing 26 papers, five flash updates and six poster abstracts
and one position paper. Of the 11 papers selected for
presentation at the meeting, the eight papers selected for
this special issue are extended versions of seven original
papers and one position paper.
Summary of selected papers
The selected for the special issue papers span a wide
range of topics including the automated re-use and up-
date of ontologies, quality assessment of ontological re-
sources, and the systematic description of phenotype
variation, driven by manual, semi- and fully automatic
means. From these articles, it is clear that systematic de-
scription of phenotypes plays a role when accessing and
mining medical records as well as in the analysis of
model organism data, genome sequence analysis and
translation of knowledge across species. Accurate pheno-
typing has the potential to bridge studies that aim to ad-
vance the science of medicine, such as a better
understanding of the genomic basis of diseases in the
Mouse Genome Informatics (MGI) database, and studies
that aim to advance the practice of medicine such as the
treatment of complex disorders like Chronic Obstructive
Pulmonary Disorder (COPD) [1].
Collier et al. in the paper titled Concept selection for
phenotypes and diseases using learn to rank [2] present
a supervised learning based approach for the recognition
of disorder-related descriptions in electronic health re-
cords (EHRs). The authors explore the potential of four
off-the-shelf concept recognition systems on the ShARE/
CLEF 2013 gold standard collection and combine the
systems using a variety of learn-to-rank algorithms. The
four systems are Apache cTAKES, the NCBO Annotator,
BeCAS and MetaMap. The proposed ensemble approach
leads to an improvement in harmonized mean for recall-
precision (F1 = 0.24) due primarily to an increase in
* Correspondence: larisa.soldatova@brunel.ac.uk
1Brunel University, London, UK
Full list of author information is available at the end of the article
© 2015 Soldatova et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Soldatova et al. Journal of Biomedical Semantics  (2015) 6:40 
DOI 10.1186/s13326-015-0040-2
recall for mentions of diseases and anatomical abnor-
malities. However performance across semantic types
varies widely. The authors conclude with a discussion of
the limitations offered by using off-the-shelf approaches
and consider the need of domain adaptation in an oper-
ational setting.
In Development and validation of a classification ap-
proach for extracting severity automatically from elec-
tronic health records Boland et al. [3] present a method
for classifying phenotype-level severity between severe
and mild conditions such as Acne, which would be con-
sidered mild, compared to say heart failure. The authors
report that previous machine learning approaches have
tended to yield high false positive rates due to the large
space of phenotypes. The random forest approach they
propose exploits several measures of severity such as
cost, treatment time, medications and procedure to yield
a sensitivity of 0.92 and a specificity of 0.78 in discerning
mild phenotypes from severe conditions when compared
to a gold standard corpus.
Funk et al. in the paper titled Evaluating a variety of
text-mined features for automatic protein function pre-
diction with GOstruct report the results of their ana-
lysis on the use of protein-related features extracted
from the biomedical literature for prediction of protein
functions [4]. The authors considered two approaches:
(1) a knowledge-based approach that uses ontology con-
cepts co-mentions and is based on co-occurences of an
entity and the corresponding ontology terms identified
in the literature; (2) a knowledge-free approach that uses
a bag-of-words as features; and where proteins are asso-
ciated to words from the sentences in which they are
mentioned. Two data sources were used in this study:
abstracts and titles from Medline, and full-text articles
from the PubMed Open Access Collection (PMCOA).
Gene Ontology (GO) annotations for human and yeast
genes were obtained from the GOA (Gene Ontology An-
notation) datasets. The authors analysed the impact of
using the two alternative approaches for feature con-
struction on the quality of prediction of protein func-
tions. Interestingly, both approaches provided similar
levels of performance. Overall, the best performance is
seen when using both co-mentions and the bag-of-
words features, but the advantage is marginal. Funk
et al. have demonstrated that the ability to recognize GO
terms in the literature text leads to more informative
functional predictions [4].
Smith et al. present a paper titled Expanding the
mammalian phenotype ontology to support automated
exchange of high throughput mouse phenotyping data
generated by large-scale mouse knockout screens [5].
The authors expand and exploit the Mammalian Pheno-
type (MP) ontology for the annotation and organization
of high throughput data from phenotype screening
experiments in the MGI database. The authors discuss
how recent additions and revisions have been under-
taken in many areas of the MP to support automated
data exchange with the International Mouse Phenotype
Consortium and other projects. In total 287 new terms
were added to the MP hierarchy during the present revi-
sion. The majority of the terms were added in the
homeostasis/metabolism section of the ontology.
Fu et al. [6] in the paper titled Supporting the annota-
tion of chronic obstructive pulmonary disease (COPD)
phenotypes with text mining workflows presents a
methodology for constructing a corpus of full-text clin-
ical documents for fine-grained COPD annotations. The
authors report that symptoms from COPD vary widely
between patients so that automated free-text analysis on
the EHR is necessary to bring pertinent conditions to
the attention of the clinician. However, gold standard
data for COPD related symptoms have so far been lack-
ing, hampering the development of text mining ap-
proaches. Annotation efforts by Fu et al., were
supported by expert-led guideline development and the
re-use of the Argo text mining platform yielding an F1
of 0.46 using relaxed matching.
eNanoMapper: harnessing ontologies to enable data
integration for nanomaterial risk assessment by Hast-
ings et al. presents an ontology which covers broad areas
such as a categorisation of nanoparticle classes based on
their properties, constituency and shape, physicochemi-
cal and biological properties of nanoparticles, environ-
mental aspects, experimental design, as well as safety
information [7]. Following the best practices in ontology
development, eNanoMapper re-uses existing ontologies,
i.e. ChEBI (Chemical Entities of Biological Interest),
NPO (NanoParticle Ontology), BAO (BioAssay Ontol-
ogy). To overcome the limitations of the manual imports
from third party ontologies, the authors developed a
dedicated library to facilitate ontology re-use by extract-
ing subsets of existing ontologies, which allows the
resulting branches and components of different ontol-
ogies to be automatically pieced together.
Winnenburg et al. in the paper titled Using descrip-
tion logics to evaluate the consistency of drug-class
membership relations in NDF-RT compared the
asserted and inferred class relations in the recently up-
dated NDF-RT (National Drug File Reference Termin-
ology) ontology [8]. NDF-RT integrated authoritative
drug-class membership assertions extracted from the
Structured Product Labels by FDA (the Food and Drug
Administration). The authors evaluated the consistency
of the drug-class membership relations inferred from the
pharmacologic class definitions and drug descriptions,
against the newly asserted, authoritative drug-class
membership relations. The enriched logic in NDF-RT
enables the evaluation of the quality and completeness
Soldatova et al. Journal of Biomedical Semantics  (2015) 6:40 Page 2 of 3
of newly added knowledge. The authors conclude that
the inferred and asserted relations matched only in
about 50 % of the cases. The results suggest that there is
an opportunity for quality assurance of NDF-RT content
(completeness of the drug descriptions and quality of
the class definitions).
In a commentary by Papatheodorou et al. titled Link-
ing gene expression to phenotypes via pathway informa-
tion the authors survey recent research efforts to
provide knowledge support for managing and integrating
data about genes, pathways and phenotypes [9]. The au-
thors argue that in order to exploit the full potential of
gene expression data to infer phenotypic consequence
due to changes in gene expression, the links between
gene expression and pathways as well as pathways and
phenotypes need to be improved. Furthermore, the au-
thors suggest that the current ontological representa-
tions of phenotypes needs to be extended, in order to
cover the complexity and variability of phenotypes in
and across species. They summarize that the current
state-of-the-art builds solid foundations for future work
to overcome these challenges.
Conclusion
In recent years the biological sciences have generated
very large, complex data sets whose management, ana-
lysis and sharing have created unprecedented challenges.
The development of bio-ontologies has been critical in
handling these data and enabling interoperability be-
tween databases and between applications [10]. The sys-
tematic description of phenotype variation is crucial for
elucidating the causal relationship between a genotype
placed in a certain environment and a phenotype. Sys-
tematic representation of phenotypes using ontologies is
essential when accessing and mining medical records as
well as for the analysis of model organism data, genome
sequence analysis and translation of knowledge across
species. The papers included in the bio-ontologies and
phenotypes special issue report on the development,
management and quality assessment of ontological re-
sources, and the systematic description of phenotype
variation, driven by manual, semi- and fully automatic
means.
Competing interests
The authors have no competing interests to declare.
Acknowledgements
As editors of this thematic issue, we thank all the authors who submitted
papers, the Program Committee members and the reviewers for their
excellent work. We are grateful for help from Goran Nenadic and Dietrich
Rebholz-Schuhmann from BioMed Central in putting this thematic issue
together.
Declarations
This article has been published as part of Journal of Biomedical Semantics
Volume 6/1, 2015: Proceedings of the Bio-Ontologies Special Interest Group
and the Phenotype Day 2014. The full contents of the special issue are avail-
able online at http://www.jbiomedsem.com/content/6/1/.
Author details
1Brunel University, London, UK. 2The University of Cambridge, Cambridge,
UK. 3The Wellcome Trust Sanger Institute, Hinxton, UK. 4The Garvan Institute
of Medical Research, Sydney, Australia. 5The University of Melbourne,
Melbourne, Australia. 6The University of Oxford, Oxford e-Research Centre,
Oxford, UK. 7Stanford University, Stanford, CA, USA.
Received: 19 April 2015 Accepted: 15 November 2015
RESEARCH ARTICLE Open Access
Identification of sex-associated network
patterns in Vaccine-Adverse Event
Association Network in VAERS
Yuji Zhang1,2*, Puqiang Wu3, Yi Luo4,5 and Cui Tao5*
Abstract
Background: Vaccines are one of the most important public health successes in last century. Besides
effectiveness in reducing the morbidity and mortality from many infectious diseases, a successful vaccine
program also requires a rigorous assessment on their safety. Due to the limitations of adverse event (AE) data
from clinical trials and post-approval surveillance systems, novel computational approaches are needed to
organize, visualize, and analyze such high-dimensional complex data.
Results: In this paper, we proposed a network-based approach to investigate the vaccine-AE association network from
the Vaccine AE Reporting System (VAERS) data. Statistical summary was calculated using the VAERS raw data and
represented in the Resource Description Framework (RDF). The RDF graph was leveraged for network analysis.
Specifically, we compared network properties of (1) vaccine - adverse event association network based on reports
collected over a 23 year period as well as each year; and (2) sex-specific vaccine-adverse event association network. We
observed that (1) network diameter and average path length dont change dramatically over a 23-year period, while
the average node degree of these networks changes due to the different number of reports during different periods of
time; (2) vaccine - adverse event associations derived from different sexes show sex-associated patterns in sex-specific
vaccine-AE association networks.
Conclusions: We have developed a network-based approach to investigate the vaccine-AE association network from
the VAERS data. To our knowledge, this is the first time that a network-based approach was used to identify sex-specific
association patterns in a spontaneous reporting system database. Due to unique limitations of such passive surveillance
systems, our proposed network-based approaches have the potential to summarize and analyze the associations in
passive surveillance systems by (1) identifying nodes of importance, irrespective of whether they are disproportionally
reported; (2) providing guidance on sex-specific recommendations in personalized vaccinology.
Background
Vaccines are one of the most cost-effective public health
interventions to date, leading to at least 9599 % de-
crease of most vaccine-preventable diseases in the
United States [1]. While their benefits far overweigh
their risks and costs, vaccines are accompanied with
specific adverse events (AEs). Assessment of vaccine
safety usually starts at the pre-approval stage, when in-
formation about AEs is collected during Phase I-IV of
clinical trials. However, there are several limitations of
such information. First, clinical trials usually have small
sample sizes which are insufficient to detect rare AEs. Sec-
ond, clinical trials are usually carried out in well-defined,
homogeneous populations within relatively short follow-
up periods, which may limit the generalizability of their ef-
fect in all populations. Therefore, the complete safety pro-
files associated with a vaccine cannot be fully established
only through clinical trials. Post-approval surveillance of
vaccine AEs is needed to assess the vaccine safety
throughout its life on the market.
The Vaccine AE Reporting System (VAERS) is a pas-
sive surveillance system to monitor vaccine safety after
the administration of vaccines licensed in the United
* Correspondence: yuzhang@som.umaryland.edu; Cui.Tao@uth.tmc.edu
1Division of Biostatistics and Bioinformatics, University of Maryland
Greenebaum Cancer Center, Baltimore, USA
5School of Biomedical Informatics, University of Texas Health Science Center
at Houston, Houston, Texas, USA
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2016 Zhang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a
link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this
article, unless otherwise stated.
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 
DOI 10.1186/s13326-015-0032-2
States [2]. The VAERS is co-managed by the United
States Food and Drug Administration (FDA) and the
Centers for Disease Control and Prevention (CDC). By
the end of 2013, the VAERS contains more than 200,000
reports in total, including 72 vaccine types and 7368
reported symptoms/AEs. However, there are several
limitations we need consider in the analyses of spontan-
eous reporting systems such as VAERS, including lack
of verification of reported diagnoses, lack of consistent
diagnostic criteria for all cases with a given diagnosis,
wide range of data quality, underreporting, inadequate
denominator data, and absence of an unvaccinated con-
trol group [3]. To address some of these limitations,
various data mining approaches have been developed to
identify potential signals in the data [4]. Most of these
approaches focus on disproportionality of reporting,
which aims to identify conditions that comprise a larger
proportion of reported events for a given vaccine, com-
pared to other vaccines in the same reporting system
[3]. However, such disproportionality methods still have
difficulties to identify potential vaccine-AE associations
due to the limitations of VAERS data. In Bate et al. 2009
[5], the authors suggested that a single drug-AE should
be analyzed in the context of all drug-AE associations.
Harpez et al. proposed a clustering approach to identify
drug groups that were reported to have same AEs [6].
However, this approach didnt account for all co-
administered drugs and co-occurring AEs. Since VAERS
receives more than 14,000 reports every year, there is a
pressing need to develop novel approaches to organize
these high-dimensional VAERS data and identify poten-
tial vaccine-AE associations.
In recent years, network analysis emerges as a very
promising approach for simultaneous representation of
complex high dimensional data. Specifically, these
network-based computational approaches gained popular-
ity and have become a new paradigm to investigate associ-
ations among biological entities (e.g., drugs, diseases, and
genes). Applications of these approaches include drug
repositioning [7, 8], disease gene prioritization [911],
and identification of disease relationships [12, 13].
These network analysis approaches are usually devel-
oped based on the observations from real-world net-
works. First, most real-world networks (e.g., WWW
network, protein-protein interaction network, and so-
cial network) are not randomly organized but are
driven by preferential attachment and growth (e.g.,
some nodes have more connections than others). Such
networks are called Scale-free networks. In the
scale-free network, the most highly connected nodes
are called hub nodes. Second, most real world net-
works are modular, comprised of small, densely con-
nected groups of nodes. Network analysis metrics and
algorithms have been designed to identify network hub
nodes and modules in a scale-free network. Ball and Botsis
proposed a network-based approach to aid visualization of
patterns in VAERS data that a medical expert might
recognize as clinically important [14]. In our previous
work, we developed a network analysis approach to den-
tify vaccine-related networks and their underlying struc-
tural information from PubMed literature abstracts, which
were consistent with that captured by the Vaccine Ontol-
ogy (VO) [15]. The modular structure and hub nodes of
these vaccine networks reveal important unidentified
knowledge critical to biomedical research and public
health and to generate testable hypotheses for future
experimental verification.
In this paper, we proposed a network-based approach
to investigate the vaccine-AE association network from
VAERS data. First, we extracted and represented data
summarized from VAERS database using Resource
Description Framework (RDF). We calculated overall
proportional reporting ratio (PRR), yearly PRR and sex-
specific PRR for each vaccine-AE association in the
VAERS. We then applied a series of network approaches
to the network consisting of significant vaccine-AE asso-
ciations (i.e., PRR > 1). Specifically, we compared network
properties of (1) vaccine-AE association network based on
reports collected over a 23 year period as well as each year;
(2) sex-specific vaccine-AE association network. We ob-
served that (1) network diameter and average path
length dont change dramatically over a 23-year period,
while the average node degree of these networks
changes due to the different number of reports during
different period of time; (2) vaccine-AE associations de-
rived from different sexes show sex-associated patterns
in sex-specific vaccine-AE association networks.
The rest of the paper is organized as follows. In
Section Materials and methods, we introduce our meth-
odology on data collection, summarization, representa-
tion, and analysis. In Section Results, we present the
result of our study. In Section Discussions, we discuss
the potential scientific contributions of this study. In
Section Conclusions and future work, we conclude the
paper and discuss future directions.
Materials and methods
In this section, we first describe the data resources and
preprocessing method in this work. We then introduce
our proposed network-based approach for investigating
vaccine-related associations derived from VAERS. Figure 1
illustrates the steps of the proposed approach.
VAERS database
We downloaded raw data from the VAERS system in
comma-separated value (CSV) format (https://vaers.hhs.-
gov/data/data). All the data from 19902013 was loaded
to a mySQL relational database for further processing.
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 2 of 8
The VAERS database contains three tables: Data, Data
sources and preprocessing Symptom, and Vaccine. The
Data table contains general information about each report
including VAERS report ID, date the report was received,
the state patient was in, age and sex of the patient, and de-
tailed description of the symptom (e.g., if the symptom
was life threatening, if the patient in the report died and
if-so the date of death, if the patient ever attend the ER for
treatment, and if so, how many days was the patient ad-
ministered at the hospital.). The Symptom table contains
a list of symptom terms (MedDRA terms) involved in
the report. Completed information about one report
can be jointed from the three tables using VAERS ID.
The Vaccine table includes information about the vac-
cine administered to the patient such as vaccine manu-
facturer, type of vaccine, dosage of the vaccine,
vaccination route, vaccination site, and vaccination
name.
Statistical summary of VAERS data
As we discussed above, the VAERS is a spontaneous
reporting system which contains unverified reports with
inconsistent data quality. Symptoms reported occurring
after vaccination do not necessarily indicate a causality
association with the vaccine. Therefore, we used statis-
tical methods to summarize meta-level features of
vaccine-symptom pairs. For each vaccine-symptom pair,
we calculated the following features (1) the number of
reports that contains the pair; (2) the number of reports
that contains the pair each year; (3) the demographic
distribution among the reports that contain the pair
(total and yearly) grouped by gender and age groups;
and (4) overall proportional reporting ratio (PRR) and
yearly PRRs [16]. A PRR is the ratio between the
frequency with which a specific symptom (e.g., AE)
occurs for a vaccine of interest (relative to all symptoms
reported for the vaccine) and the frequency with which
the same symptom occurs for all vaccines reported to
the VAERS (relative to all symptoms for all vaccines
reported to VAERS) [3]. A PRR greater than 1 suggests
that the post-vaccination symptom (AE) is more
commonly observed for individuals administrated with
the particular vaccine, relative to all other vaccines
reported to the VAERS.
Fig. 1 Overview of the proposed study. VAERS: Vaccine AE Reporting System; RDF: Resource Description Framework; VAE: Vaccine-AE; PRR: Proportional
Reporting Ratio
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 3 of 8
The overall PRR ratio of a vaccine (V) and a symptom
(S) association was calculated by (Numreports for V that con-
tainsS/Numall thereports for V)/(Numtotalreports that con-
tains S/Numtotal reports in VAERS).
The yearly PRR ratio of a vaccine (V) and a symp-
tom (S) association in Year (Y) was calculated by
(Numreports for V that contains Sin year Y/Numall the re-
ports for V in Year Y)/(Numtotalreports that con-
tains S in Year Y/Numtotalreports in VAERS in Year Y).
The sex-specific PRR ratio or a vaccine (V) and a symp-
tom (S) association in Gender (G) was calculated by
(Numreports for V that contains S forpatient with G/Numall there-
ports for V for patient with G)/(Numtotalreports that contains S for pa-
tient with G/Numtotalreports in VAERS for patient with G).
RDF conversion
The statistical summary introduced in the previous sec-
tion was stored in a relational database and converted to
the Resource Description Framework (RDF) format. We
have introduced detailed information about how to
represent vaccine symptom pairs with meta-information
in RDF and our vision on linking heterogeneous
vaccine-related data sets using linked data approach in
our previous work [17].
Figure 2 shows the meta-level RDF graph representation
of a vaccine symptom association. Each unique association
(vaccine-symptom pair) has an unique identifier. The cor-
responding vaccine, symptom, demographic distribution,
and PRR values are also represented in RDF. SPARQL
queries can be conducted to retrieve useful information
for network analysis which we will introduce in the next
section.
Network analysis
The analysis of network properties was performed using
the Network Analyzer plugin in Cytoscape [18]. Cytos-
cape is an open-source platform for integration,
visualization, and analysis of biological networks. Its
functionalities can be extended through Cytoscape plu-
gins. Scientists from different research fields have con-
tributed more than 160 useful plugins so far. These
comprehensive features allow us to perform thorough
network-level analyses, visualization of our association
tables, and integration with other biological networks in
the future. In this study, the average node degree,
average path length, and network diameter of one
network was calculated.
The vaccine-AE association network is a bipartite
network, which consists of interactions between two
different types of nodes (X-type and Y-type), with
edges connecting only nodes of different types. To cal-
culate the similarity of one type of nodes (e.g., X-type
nodes) based on their interactions with another type of
nodes (e.g., Y-type nodes), the Pearson correlation co-
efficient (PCC) was employed as an association index.
We assume that node A and B are X-type nodes, and
the PCC between node A and B is calculated by
PCCAB ¼ jNðAÞ?NðBÞj?ny?jNðAÞj?jNðBÞjffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
jNðAÞj?jNðBÞj?

ny?jNðAÞj

?

ny?jNðBÞj
r
ð1Þ
where A and B are nodes of same type, N(A) and N(B)
are their total number of interactions with A and B,
Fig. 2 Sample RDF graph representation of vaccine AE association
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 4 of 8
N(A)?N(B) is the total number of Y-type nodes that
interact with both A and B, and ny is the total the total
number of Y-type nodes in the network. A PCC of 1
indicates a perfect overlap, 0 corresponds to the number
of shared interactors expected by chance and ?1 depicts
perfect anti-correlation.
The hierarchical clustering analysis is used to identify
the similarities among vaccines using their association in-
dexes. Both the heatmap of the dendrogram are used to
visualize the clustering results. The clustering analysis and
visualization of vaccine-AE association network was per-
formed using the GAIN tool [19].
Results
Overview of the results
Overall, we extracted 2,346,367 vaccine-AE associations
from the VAERS system, with 83,148 distinct associations.
We defined that a vaccine-AE association is significant if
PRR for this association is greater than 1. Among all
vaccine-AE associations reported in the VAERS, we identi-
fied 277,698 vaccine-AE associations, 53,795 of which
have overall PRR greater than 1 between 1990 and 2013.
We also investigated yearly PRRs of these associations.
For one specific year, we define that a vaccine-AE associ-
ation is significant if the yearly PRR is greater than 1.
Table 1 presents the numbers of significant associations
for each year (Nlink Column).
Based on the significant yearly or overall associa-
tions, we further investigated these association net-
works using different network properties. Table 1
presents the general characteristics of the overall associ-
ation network as well as yearly-significant association net-
works, including average node degree, average path length
and network diameter. This demonstrates that vaccine-AE
network is dense network, with any given node connected
to all other nodes through an average of approximately
two other nodes and a maximum of 56 nodes. It is ex-
plained partly by that many vaccines are co-administered.
However, given that there are more AEs than vaccines in
the network, it is plausible that many AEs were reported
together. Another interesting observation was that across
23 years, the average path length and network diameter
for yearly vaccine-AE association networks dont change
dramatically. It indicated that in most cases, it is relatively
common that two vaccines sharing one AE or two AEs as-
sociate with one vaccine in the network. On the other
hand, the average node degree of these networks changes
over time, partly due to the increasing number of reports
received from 1990 to 2013 (Table 1). All the network
information in Table 1 were presented in Additional file 1.
Different AE association patterns in different genders
We further investigated whether vaccine-AE associations
are different between genders. We constructed sex-specific
vaccine-AE association networks by computing the PRR
based on reports only from female/male populations. There
are 49,616 and 51,578 significant vaccine-AE associations
(i.e., PRR > 1) in female and male populations, respectively.
The network properties of these two sex-specific associ-
ation networks are similar with overall association network
(Table 1). We clustered the vaccines based on their associ-
ation indexes calculated by their associations with AEs. In
Fig. 3a and b, we observed different similarity patterns in
female (Fig. 3a and male (Fig. 3b). For instance,
HBHEPB, ROTH1, PNC13, DTAP IPVHIB, PNC,
ROTHB5, DTAPHEPBIP, HIBV, DTAP, and IPV were
clustered together based on their associations with adverse
events in the female population. Besides most of the vac-
cines that were grouped in the female population, we also
found four more vaccines in the same group in the male
population, including PNC, HEP, VARCEL, and MMR.
Similarly, while DIPHIB, DTP, and OPV were tightly
Table 1 General characteristics of the networks
Nnode Nlink Average
degree
Average path
length
Network
diameter
1990 342 990 5.79 3.05 7
1991 634 2,756 8.69 2.69 5
1992 553 2,275 8.23 2.58 5
1993 517 2,263 8.75 2.45 5
1994 549 2,487 9.06 2.46 5
1995 637 3,080 9.67 2.54 6
1996 628 3,028 9.64 2.54 5
1997 660 3,270 9.91 2.51 5
1998 793 3,989 10.06 2.54 5
1999 1,047 5,245 10.21 2.67 5
2000 1,065 5,708 10.72 2.59 5
2001 906 4,904 10.83 2.53 5
2002 987 5,289 10.72 2.58 5
2003 1,430 8,197 11.46 2,52 5
2004 1,160 6,491 11.19 2.62 5
2005 1,315 7,290 11.08 2.66 6
2006 1,978 12,128 12.26 2.66 5
2007 3,103 20,214 13.03 2.59 5
2008 3,045 20,196 13.27 2.58 6
2009 3,351 22,054 13.16 2.57 5
2010 2,639 18,041 13.67 2.62 5
2011 2,142 13,533 12.64 2.69 5
2012 1,943 12,292 12.65 2.72 5
2013 826 4,886 11.83 2.72 5
Female 4,947 49,616 20.14 2.35 5
Male 4,519 51,578 22.83 2.35 5
Overall 5,938 53,742 18.10 2.48 5
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 5 of 8
clustered in the male population, RV was also grouped in
this cluster in the female population. The dendrograms
indicate the same differences between two populations
(Fig. 3c and d). These results indicate that there are indeed
sex-specific reponse differences after vaccine injection.
We also compared whether pairs of vaccines with similar
association profiles in female-specific association network
are also similar in male-specific network. In Fig. 4, the dis-
tributions of PCC indexes are different in two populations,
indicating that there are some sex-specific associations in
both populations, although majority of association relation-
ships can be identified in both populations. Specifically,
there were more vaccine pairs showing high similairties in
the male population than in the female population. The
underlying mechanisms need further investigation using
other types of biological data, such as genomic, metablo-
mic, and proteomic level measurement data.
Discussions
Most vaccine-preventable diseases have declined in the
United States by at least 9599 % [20, 21]. However, vac-
cines are pharmaceutical products that carry risks. Cer-
tain biomarkers or individual variations could implicate
different vaccine responses, which are essential for preci-
sion medicine. Identifying these associations is critical to
vaccine safety, which reassures public acceptance of vac-
cines. One way to address this question is the post-
approval surveillance of vaccine AEs. For instance, the
VAERS is a passive surveillance system to monitor vac-
cine safety after the administration of vaccines licensed
Fig. 3 Comparison of vaccine similarity in different sexes. a Hierarchical analysis of vaccines based on association information in female
reports; b Hierarchical analysis of vaccines based on association information in male reports; c Dendrogram of vaccine similarity in female
reports; d Dendrogram of vaccine similarity in male reports
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 6 of 8
in the United States [2]. Such surveillance data can com-
plement the original safety evaluation data generated
from the clinical trial phases and provide more compre-
hensive safety assessment in a much larger population.
We are one of the first research groups that investigates
sex-specific vaccine-AE association patterns by integrat-
ing traditional statistical signal detection and network
analysis approaches. Our findings indicated that saftety
signals present different patterns in female and male
population. This is consistent with previous studies in
the vaccine community [22, 23]. With high-throughput
technology advances such as next generation sequencing,
transcriptomics, epigenetics, proteomics, and new compu-
tational approaches to interpreting big data, we expect a
better understanding of associations and mechanisms of
vaccine AEs and immunogenicity. Network analysis ap-
proaches is one of the promising stratetigies to integrate
such heterogeneous big data, leading to a more
personalized or individual approach to vaccine practice in
the near future.
Conclusions and future work
In this paper, we proposed a network-based approach
to investigate the vaccine-AE association network from
VAERS. The results indicated that (1) network diameter
and average path length of vaccine-AE association
networks dont change dramatically over a 23-year
period, while the average node degree of these networks
changes due to the different number of reports during
different period of time; (2) vaccine-AE associations
derived from different genders show sex-associated
patterns in sex-specific vaccine-AE association net-
works. To our knowledge, this is the first time that a
network-based approach has been used to identify sex-
specific association patterns in a spontaneous reporting
system database. Due to unique limitations of such
Fig. 4 Density plot of PCC association indexes in female vs male populations (red: female; blue: male)
Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 7 of 8
passive surveillance systems, network-based approaches
have the potential to (1) identify nodes of importance,
irrespective of whether they are disproportionally
reported; (2) provide guidance on sex-specific recom-
mendations in personalized vaccinology.
Extensions of this work include: (1) integration of
other spontaneous reporting system databases (e.g., the
European Adverse events following immunization
(AEFI) system) to construct more complete vaccine-
AE association networks; (2) incorporation of other
complementary public databases such as Semantic
MEDLINE [24]; (3) development of advanced network-
based approaches taking the PRR values into account;
(4) investigation of other types of data mining methods
to assess the significance of vaccine-AE associations;
(5) focused investigation of examples based on the net-
work parameters; and (6) identification of sex-specific
subnetwork patterns of AE correlation networks.
Additional file
Additional file 1: Network file containing all the networks
presented in Table 1. (CYS 13422 kb)
Competing interests
The authors declare that they have no competing interests.
Authors' contributions
YZ and CT led the study design and analysis, and drafted the manuscript. PY
extracted associations from VAERS database and performed statitistical
analysis. YL converted the VAERS information into RDF format. All authors
read and approved the final manuscript.
Acknowledgements
This project was supported by the National Cancer Institute grant P30 CA
13427404 to the University of Maryland Baltimore, and the National Library
of Medicine of the National Institutes of Health under Award Number
R01LM011829 to C.T.
Author details
1Division of Biostatistics and Bioinformatics, University of Maryland
Greenebaum Cancer Center, Baltimore, USA. 2Department of Epidemiology
and Public Health, University of Maryland School of Medicine, Baltimore,
USA. 3University of Wisconsin-Madison, Madison, Winsconsin, USA.
4Department of Computer Science and Engineering, Lehigh University,
Bethlehem, Pennsylvania, USA. 5School of Biomedical Informatics, University
of Texas Health Science Center at Houston, Houston, Texas, USA.
Received: 10 December 2014 Accepted: 10 August 2015
Published: 19 August 2015
RESEARCH Open Access
My Corporis Fabrica Embryo: An
ontology-based 3D spatio-temporal
modeling of human embryo development
Pierre-Yves Rabattu1*, Benoit Massé2, Federico Ulliana3, Marie-Christine Rousset3, Damien Rohmer2,4,
Jean-Claude Léon2 and Olivier Palombi1,2
Abstract
Background: Embryology is a complex morphologic discipline involving a set of entangled mechanisms, sometime
difficult to understand and to visualize. Recent computer based techniques ranging from geometrical to physically
based modeling are used to assist the visualization and the simulation of virtual humans for numerous domains
such as surgical simulation and learning. On the other side, the ontology-based approach applied to knowledge
representation is more and more successfully adopted in the life-science domains to formalize biological entities
and phenomena, thanks to a declarative approach for expressing and reasoning over symbolic information.
3D models and ontologies are two complementary ways to describe biological entities that remain largely
separated. Indeed, while many ontologies providing a unified formalization of anatomy and embryology exist,
they remain only descriptive and make the access to anatomical content of complex 3D embryology models
and simulations difficult.
Results: In this work, we present a novel ontology describing the development of the human embryology deforming
3D models. Beyond describing how organs and structures are composed, our ontology integrates a procedural
description of their 3D representations, temporal deformation and relations with respect to their developments. We
also created inferences rules to express complex connections between entities. It results in a unified description of
both the knowledge of the organs deformation and their 3D representations enabling to visualize dynamically the
embryo deformation during the Carnegie stages. Through a simplified ontology, containing representative entities
which are linked to spatial position and temporal process information, we illustrate the added-value of such a
declarative approach for interactive simulation and visualization of 3D embryos.
Conclusions: Combining ontologies and 3D models enables a declarative description of different embryological
models that capture the complexity of human developmental anatomy. Visualizing embryos with 3D geometric
models and their animated deformations perhaps paves the way towards some kind of hypothesis-driven application.
These can also be used to assist the learning process of this complex knowledge.
Availability: http://www.mycorporisfabrica.org/
Background
Embryology has been studied for many centuries. This
morphologic discipline studies the transformation of a sin-
gle cell into a complete organism, which is composed of
almost 1014 cells. This discipline has benefited of progress
from imagery, histology, molecular biology and genetic.
However it remains a high level of complexity and lots of
physiological and pathological mechanisms stay unclear.
Computer modeling and simulation of human body allow
to express and to visualize complex physiological processes,
to make the steps of human embryology more accessible.
However, no unified model which allows integrating em-
bryological processes into 3D visualization exists.
An ontology is a formal description of a domain of
interest [1]. Ontologies are increasingly employed in the
life-science domains, and in particular in biology, for the
* Correspondence: PYRabattu@chu-grenoble.fr
1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2015 Rabattu et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 
DOI 10.1186/s13326-015-0034-0
sake of standardizing a common vocabulary of entities
and expressing the inherent complexity of biological sys-
tems. Moreover, ontologies formal systems can be read,
understood and exploited by computers to carry out tasks
that would be otherwise tedious and time-consuming for
the humans. The main biology ontologies are referenced
in OBO Foundry [2] and NCBO Bioportal [3].
Human anatomy ontologies are referenced by the
Foundational Model of Anatomy domain, FMA [4], which
contains the concepts and relationships that pertain to the
structural organization of the human body. Uberon [5] is
a multi-species ontology for anatomy (just as the gene
ontology (GO) project [6] is a multi-species ontology for
molecular functions, biological processes or cellular com-
ponents). It is integrated with species-specific ontologies,
but it does not dependent on them.
Regarding embryology, some interesting concepts have
been introduced in several anatomical ontologies like the
drosophila (DAO) one [7]. More specifically, Kaufman [8]
developed the Atlas of Mouse Development which has
been integrated into the Mouse Atlas Project (MAP) led by
Davidson [9], Bard [10], Baldock [11] and Ringwald [12]. It
is a study of mouse developmental anatomy through histo-
logical sections of mouse embryos reconstructed in a full-
grey image, for each developmental stage (Theiler stages).
Brune [13] has completed this project by defining 3D re-
gions (domains) of the embryo models, to map an ontology
to each of these spatial models. Through the 3D domains,
users can navigate from the spatial representation of the
embryo to the ontology and vice versa. It is important to
note that there is one ontology for each Theiler stage. Links
between ontologies and 3D images also exist in some other
atlas as in the Virtual Fly Brain project allowing users to
explore the structure of the Drosophila brain by browsing
3D images of a brain with subregions displayed as colored
overlays [14] and, in a more general setting, in Allen Brain
Atlas [15].
Burger [16] developed the concept of the Abstract
Mouse which was further developed by Hayamizu [17]
who created a non-stage specific representation of a
mouse developmental anatomy ontology (EMAPA, [18])
where each anatomical entity is linked to the first and last
stages of its existence.
Hunter [19] has built a human developmental anat-
omy ontology (EHDA), which is composed of several
ontologies, one for each Carnegie stage (CS) [120]
that only includes basic part_of data. Bard [20] devel-
oped an ontology of human developmental anatomy
(EHDAA2) with more than 2000 anatomical entities,
which are linked by part_of, is_a, develops_from
relationships and linked to the Carnegie stage by
starts_at and ends_at relations. He had initially
developed a work, as part of the Edinburgh Mouse
Atlas Project, to provide a structured and controlled
vocabulary of stage-specific anatomical structures for
the developing laboratory mouse.
Palombi et al. [21,22] have developed an ontology of hu-
man anatomy which includes 3D representations, where
each anatomical entity is linked to its corresponding 3D
model. They also considered the function of each entity to
assist the creation of complex 3D models for visualization
and simulation [23]. My Corporis Fabrica (MyCF) is
equipped with automatic reasoning capabilities that enable
model checking and complex queries answering. They
demonstrated that 3D graphical models are effective to
represent either anatomy or embryology.
This kind of 3D modeling is already used in Bio-
mechanics, Computer Aided Medicine and Computer
Graphics. It can be useful in plenty of domains like bio-
mechanics, ergonomics, diagnosis, treatment planning,
visualization, graphics, robotics, or, for example, helping
deduce the anatomic consequences of an injury [24].
Kerwin et al. have developed a three-dimensional atlas of
the human embryonic brain using anatomical landmarks
and gene expression data to define major subdivisions
through 12 stages of development (Carnegie Stages 1223).
Virtual 3D anatomical models were generated from intact
specimens using optical projection tomography. The 3D
models and a preliminary set of anatomical domains and
ontology are available on the atlas pages along with gene
expression data from approximately 100 genes in the HUD-
SEN Human Spatial Gene Expression Database [25,26].
Gasser [27] developed the Virtual Human Embryo [28],
which is an atlas of digitally captured images of serial
embryo sections corresponding to each of the 23 Carnegie
stages. Then, they were assembled to provide 3D images
and holistic views of development. Moreover, this database
is accessible to all researchers, teachers and students and
can be used acknowledging the source of the images, i.e.,
Virtual Human Embryo DREM Project.
Full exploitation of ontologies goes far beyond providing
standardized notations for explicit formalized knowledge
on domains of interest. It requires equipping ontological
statements with reasoning and querying capabilities in
order to efficiently retrieve relevant and precise informa-
tion. However, until now, only few existing works in
biomedical ontologies (e.g., [2124]) take advantage of
available automatic reasoners. Most of these works rely on
ontologies expressed in OWL and use OWL reasoners
such as Pellet [29] or Racer [30] that come with a high
computational complexity in the worst case (as they are
Description Logics reasoners). Recently, RDF-based seman-
tic environments such as Jena (http://jena.apache.org/) or
Cwm (http://www.w3.org/2000/10/swap/doc/cwm) have
included rules (a la Datalog) to perform inferences on top
of RDF datasets. Datalog rules [31] and Description Logics
[32] are two orthogonal decidable fragments of first-order
logics that have been extensively studied in knowledge
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 2 of 15
representation and in deductive databases. The interest of
Datalog rules is that they are easy to read and write for
practitioners and they have a polynomial data complexity
while allowing expressing complex interaction between
properties and recursivity. In addition, as it will be shown
in our examples, rules allows to capture in a uniform man-
ner OWL constraints that are useful in practice, such as
property transitivity or symmetry, but also domain-specific
rules with practical relevance for users in many domains of
interest.
In this paper, we describe a novel ontology of human
embryo developmental anatomy. Thanks to a RDF-
based uniform knowledge representation formalism, it
enables interoperability between anatomical models
(based on a stage-independent hierarchical structure
like Bards), 3D graphical models [21] and spatio-
temporal representations of development processes.
This integrated declarative approach facilitates the gen-
eration of animations of the embryological development
and opens new possibilities for learning and reasoning,
in particular for a better understanding of developmen-
tal abnormalities. The key issue to achieve an effective
interoperability is the rule-based inference allowing us
to express quite simply how properties interact to relate
the different models.
Our final goal is to include some malformation path-
ologies and represent them so as to increase the under-
standing of the mechanism of their pathogenesis. The
embryological entities we consider have been willingly
restricted in numbers and simplified in terminology to
ease the development and the test of our approach.
Results
The MyCF Embryo ontology has been designed so as to
accommodate the description of several embryos, male or
female, their 3D representations and their spatio-temporal
developmental processes.
MyCF Embryo ontology is made of 6 taxonomies of
classes related by relations, and by a set of 15 rules, that
we will describe now.
The top classes of the different taxonomies are: embryo-
logical_entity, temporal_entity, geometrical_entity, process,
spatio-temporal_representation and disease.
Each embryological entity is assigned to a set of spatio-
temporal representations describing a geometrical compo-
nent at a given time or during a process over a given
duration.
We now describe the main classes in the ontology and
the properties linking them. Additional file 1: Table S1
summarizes them and provides their definitions and the
correspondences with existing ontologies if any.
Our ontology is expressed in RDFS [33] which is broadly
used in Linked Data [34] to express so-called light-weight
ontologies with a rule-based semantics. Whereas OWL is
often seen as an extension of RDFS, this is not exactly the
case, mainly because RDFS allows to use the same identifier
as a class, instance or property, which is not possible in
Description Logics (at the basis of OWL profiles), in
which the sets of instances, classes and properties must be
disjoint. In addition, RDF allows to use blank nodes in class
or property position.
Similarly, the RDF query language SPARQL can query
simultaneously the data and the schema and allows vari-
ables to stand for classes and properties. This goes beyond
the first-order conjonctive queries typically considered in
Description Logics. It is worth emphasizing that OWL and
RDFS can interoperate, as there exists an RDF triple nota-
tion for most of the OWL constructs (in particular the
OWL2 profiles), and the corresponding axioms can be
expressed as logical rules. An important need was to
express fine-grained domain-specific rules, in particular for
describing the spatio-temporal development relations be-
tween subparts of the embryo. As it will be shown below,
such rules are quite easy to express on top of RDF facts,
while they are not expressible in Description Logics and
thus, in OWL.
Embryological_entity
Embryological entities model the anatomical structure of a
formal embryo as classes (e.g., kidney, left_kidney, embryo)
directly related by properties (that hold between classes and
not between instances in contrast with modeling using
OWL). Thus, declaring that left kidney is a part of embryo
will be simply expressed by a single RDF triple<left_kidney,
part_of, embryo>, as it is done in MyCF ontology [21].
Embryological entities can be material or immaterial. For
example, the kidney, gonad and cloaca are material entities
while the renal hilum and the ureterovesicular junction are
immaterial ones. The relationships between embryological
entities are described by standard ontology properties like
subClassOf (left kidney is a subclass of kidney) and part_of
(left kidney is a part of embryo) and its inverse has_part.
We use also the property develops_from presented in [20]
to describe the lineage between entities during the embryo
development. Some entities directly_develops_from an-
other, means that an organ is a direct outcome from
another organ. This allows us to express for instance that
the kidney directly_develops_from the metanephric blas-
tema, the metanephric blastema directly_develops_from
the metanephros and so, the kidney develops_from the
metanephros (see inference section below).
Temporal entity
Let us first note that we will use within this work the term
evolution in its animation and modeling meaning, i.e. a sur-
face undergoing geometrical deformation through time,
and not with respect to its biological sense.
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 3 of 15
Temporal entities represent Carnegie stages and gestation
weeks. These allow us to describe the evolution of the em-
bryo at different granularities. In particular, the 23 Carnegie
stages correspond to the first eight weeks of gestation. The
remaining gestation weeks (form 9 to 12) are represented
by single classes. Since Carnegie stages have different dur-
ation, we employ two new properties from_gestation_day to
to_gestation_day, in order to describe the first and last day
of each stage. For example, the stage 14 lasts two days
(from day 32 to day 34), while the stage 17 lasts three days
(from day 39 to day 42).
As we will discuss next, temporal entities are a crucial
aspect of our ontology model for the procedural modeling
of the embryo evolution. Moreover, each stage is described
as a stage following_stage the previous one. For example,
the stage te15 following_stage te14. This relation, follo-
wing_stage, is simply used to express the chronological
order between the gestation stages and it is similar to
immediately_preceded_by found in RO (see Additional
file 1: Table S1). A specific reasoning process over stages
is presented in Osumi-Sutherland et al. [35] for auto-
matic classification.
Geometrical component
Geometrical components are classes representing 3D geo-
metric shapes. These classes instances are paired with the
spatio-temporal representation of organs. They allow the
user to visualize the coarse representation of the organs at
a user-prescribed time, i.e., Carnegie stage or gestation
week. More specifically, we focus on being able to represent
the overall shape, position, orientation and size, of the
organs. We have defined the following 6 basic geometrical
primitives to represent an organ or a part of it: point, line,
plane, ovoid, cylinder and duct. We used only simple
geometrical primitives in this work to enable the procedural
generation of the geometry of organs and we explain in the
discussion how to extend such representations to produce
a more accurate visual representation of the organs.
Ducts represent the ureter and other tubular structures.
An example of such structure is given in Fig. 1a.
The planes are used to illustrate some immaterial em-
bryological entities such as a foramen or embryo sagittal
plane, or a flat side of an organ, e.g. the inferior side of
liver.
Cylinders are used to describe some solid organs such
as the diaphragm whose geometry is roughly cylindrical.
The line is used to describe a boundary of an organ or
a particular geometric feature, e.g. the vertebral line,
used to fix or to move an organ relatively to it.
Points are used to represent immaterial entities linking
different organs, e.g. the left uretero-vesicular junction link-
ing the ureter with the bladder. The contact area between
two organs is described with a unique point belonging to
both organs.
Of course, this set of geometric entities can be extended
towards more complex geometric shapes.
Each geometric object is defined by a barycenter position,
axis size, and vector orientation. For example, kidneys are
drawn as ovoids (e.g. Fig. 1b). AT stages 14 and 15, we have
two different instances of the ovoid class. Each instance
provides information about the size of the ovoid axes, the
position of the barycenter w.r.t. the embryo-axes and
vectors coordinates to orient the shape w.r.t. to the refer-
ence frame of the scene.
At the ontology level, these three informations are set
with three properties, namely axis_size, barycenter_posi-
tion, and x, y and z_axis_orientation that are subproper-
ties of vector_coordinates. These properties are
specialized further in order to process complex objects
like ducts or lines. These two shapes have two end-
points, called the rostral and the caudal ends that must
be described separately. Therefore we introduced some
subproperties, namely caudal_end_axis_size, rostral_-
end_axis_size, caudal_end_barycenter_position, rostral_end_
barycenter_position, caudal_end_vector_coordinates, rostral_
end_vector_coordinates.
Fig. 1 a Geometrical representation of a duct. The both rostral and caudal ends can be described and can have evolution separately. b Geometrical
representation of an ovoid. This ovoid can be described using a barycenter and three axes with specific size and orientation
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 4 of 15
Finally, the property has_geometrical_representation is
used to link the objects with the spatio-temporal represen-
tation of each structure, which are described next.
Spatio-temporal representation
Spatio-temporal representations are the central part of
our ontology. They establish a precise connection between
the world of anatomy and 3D graphics, along the space
and time dimensions.
The instances of this class link organs (e.g. the class
left_kidney) with their geometrical representation (e.g.
an instance of ovoid) and temporal entities (e.g. Carnegie
stage 14). The property describes connects a spatio-
temporal representation with an anatomical entity. Regard-
ing the Xtemporal aspects, the property at_stage connects
the spatio-temporal representation instance with a gesta-
tion moment. To describe the evolution of organs, spatio-
temporal representation instances must also relate to time-
intervals.
For example, to describe the process undergone by the
ureter between stage 14 and 18, the process instance is
explained by the property has_process that ties a spatio-
temporal representation instance with a process instance as
follows.
Process
Evolutionary processes represent the observed phenomenon
that can take place in the embryo during a gestation dur-
ation. We represented 6 kinds of processes: growth, migra-
tion, rotation, interaction, division and fixation.
Growth involves almost all organs. Migration involves
organs like the kidney that moves from the pelvic to the
lumbar region, or the gonad that does almost the same
in the opposite direction. Rotation involves organs like
the kidney that undergo a 90° rotation around two of its
reference axes during its migration towards the lumbar
region. Interaction between organs is necessary for their
development like the ureter-kidney interaction, which is
necessary for the kidney growth. Division can be either a
physiological or a pathological process that leads an
organ to abnormally split in different parts. At an ex-
ample, it can be used to model the kidney duplication,
occuring when a ureter (ureteric bud) splits too early
before joining a kidney (metanephric blastema). Fixation
describes the hard link of two anatomical entities.
We introduce some properties to describe the processes,
for instance, the direction of a migration or a rotation of or-
gans, the growth proportion, fixed-points for fixations, the
interacting objects involved in a process. These properties
are migration_direction (which is a subproperty of vector_-
coordinates), rotation_degree, growth_proportion (for rostral
and caudal ends), fixed_to (for rostral and caudal ends).
Rostral and caudal subproperties allow a variation of the
endpoints of an object like ducts and lines in a different
way.
To capture the dependency between the different pro-
cesses in the embryo evolution we also use the property
depends_on. For example, the kidney growth depends on
the interaction between ureter and kidney.
A spatio-temporal representation is linked to a process
by has_process and to duration between two temporal
entities using from_stage and to_stage.
It is important to note that the properties starts_at, end-
s_at, which are used to describe the existence of an organ
over a time interval differ from the properties from_stage
and to_stage which describe a process over a time interval.
For example, the existence of the cloaca starts_at the
Carnegie stage 11 and ends_at the Carnegie stage 14.
However, the cloaca can have a process from_stage 11
to_stage 12 and another one from_stage 12 to_stage 14.
Disease
The Disease class describes the embryo malformations
through the activation or inactivation of some processes
[36]. For instance, the hypoplastic kidney occurs when
there is no interaction between the ureter and the kid-
ney, and it inhibits the kidney growth. We use the prop-
erty impacts_processus to relate diseases with impacted
processes. We use the property impacts_entity to denote
the anatomical entities involved in a process or impacted
by a disease.
Running example
We now describe in detail how the evolution of an organ is
represented in our ontology. We will use the representation
of the evolution of the left kidney during the period be-
tween Carnegie stages 14 to 20.
An instance of the organ left kidney is described through
a set of spatio-temporal representations (3D). Below,
e00_left_kidney is an instance of left kidney, i.e., the left kid-
ney of a particular embryo (the embryo 00). The left kidney
class is a subClassOf kidney, which is a part_of embryo.
The graphical representation of these triples is given
in Fig. 2.
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 5 of 15
Concerning the 3D representation, the e00_left_kidney
has a set of spatio-temporal representation instances,
either at a given stage or for a given
duration .
This is depicted in Fig. 3.
The description of the kidney links its spatio-temporal
representation instance to a geometrical component
(has_geometrical_representation) during a specific stage
(at_stage). This geometrical component is an ovoid
which has some information about its barycenter_posi-
tion, its axis_size, its x, y and z_axis_orientation, as illus-
trated in Fig. 4.
The dynamic description of the left kidney is achieved
through a connection between a spatio-temporal repre-
sentation instance, a process (with property has_process)
and a duration period (with properties from_stage and
to_stage). For example, the fact that the left kidney has a
growth_proportion of 10 on each of its three axes during
the stages 14 to 20 of gestation is declared as in Fig. 5.
We now illustrate how we represent pathologies. We
consider the example of kidney hypoplastic pathology,
which impacts the interaction between ureter and kidney
during the embryo development, and thus the kidney
growth process. We use the property impacts_processus
to relate a pathology to the process it impacts (see
Fig. 6).
When querying the ontology, we can discover that the
kidney hypoplastic lacks a kidney growth.
Below we illustrate the whole resulting ontology fragment
(see Fig. 7).
Inference
The inference rules of MyCF Embryo express complex
connections between entities. These rules offer a uni-
form setting for expressing the semantics of most of the
OWL and all the RDFS constraints (such as transitivity
or symmetry of some generic properties like subClassOf
or subPropertyOf, and of more specific properties like
part_of or develops_from) but also domain-specific rules
that have to be declared by the ontology designer. These
rules capture in a very compact way implicit facts that
can be made explicit on demand or at query time by an
inference mechanism.
This mechanism is automatic and consists in applying
the rules on the explicit facts declared and stored as RDF
triples, in all the possible manners satisfying the conditions
of these rules. For each possible instantiation of the
variables (denoted by a name starting by ?) appearing in a
condition part of a given rule such that all its conditions
are satisfied by explicit facts, the new facts corresponding
to the (appropriately instantiated) conclusion of the rule are
added. This saturation process is iterated as long as new
facts can be produced. The termination is guaranteed by
the form of the rules that are considered. They correspond
to safe rules, also called Datalog rules, i.e., all the variables
Fig. 2 Representation of an organ in the ontology. Class and subclass
are represented in dark orange round-angle boxes. Instances are
represented in light orange right-angle boxes
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 6 of 15
appearing in the conclusion of a rule also appears in the
condition part.
The rules that are considered in the current version of
MyCF Embryo are summarized in Additional file 2:
Table S2.
The first group of rules enriches the description of
the embryological entities development. In particular,
it describes the dependency between anatomical en-
tities during the gestation by means of properties
directly_develops_from and develops_from that denote
direct and (possibly) indirect dependencies,
respectively.
The rule R1 defines directly_develops_from as a sub-
property of develops_from. This last one is a transitive
property, as defined by rule R2. As illustrated in Fig. 8,
rule R1 allows us to infer for instance that since the
kidney directly develops from the metanephric blastema,
then more generally it develops from that entity. Fur-
thermore, because the metanephric blastema develops
from the metanephros, by rule R2, we can infer that the
kidney develops from the metanephros. Rule R3 de-
scribes the development relations between subparts of
the embryo. As illustrated in Fig. 9, it allows for instance
to infer that since the metanephric blastema directly de-
velops from the metanephros, and this last one is a part of
the nephros, then the metanephric blastema develops from
the nephros. Rule R4, is the analogous of rule R3, for the
subClassOf property. It is also illustrated in Fig. 9.
The second group of rules describes the relations be-
tween embryological entities and geometrical entities
through their spatio-temporal representations at differ-
ent gestation stages.
Fig. 3 Links between spatio-temporal representation with organ instance and temporal entities. Each spatio-temporal representation is linked to a specific
stage (at_stage) or to duration (from_stage, to_stage). The organs classes and instances are represented with orange boxes. Spatio-temporal representations
are represented in purple boxes and temporal entities are represented with green boxes
Fig. 4 Static description of the left kidney at stage 14. Link between spatio-temporal representation and geometrical component (has_geometrical_
representation). Each spatio-temporal representation is linked to a geometrical representation which allows to give some information about
size, position and orientation of the organ. The double arrows show the link to dataproperties allowed to give some numerical information. The
geometrical components are represented with blue boxes
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 7 of 15
The rule R8 allows to tie embryological entities with
their geometric representations, so as to answer to queries
like "which geometrical representations of kidneys are
available?.
The effect of this rule is illustrated in Fig. 10. It allows us
to infer that a given instance e00_left_kidney of the left_kid-
ney entity can be described by a specific left_kidney_ovoid
geometrical model, associated to its spatio-temporal repre-
sentation st_representation_of_left_kidney_at_te14.
The rule R9 further describes geometrical represen-
tation of an embryo organ by explicitly inferring the
gestation stage of the organ that it represents. As
shown in Fig. 10 it allows us to infer that since st_re-
presentation_of_left_kidney_at_te14 corresponds to the
Carnegie stage 14 (te14), then the corresponding geo-
metrical model left_kidney_ovoid describes the corre-
sponding instance of left kidney at this stage 14.
At this point, it is also possible to query the ontol-
ogy and ask for "all geometrical representations (?g) of
entities (?ee) that develop from the Cloaca, starting
from stage 14". This translates to the following
SPARQL query
To impose a discrete order between the gestation
stages, we introduced the property following_stage. This
permits to state that stage 15 follows stage 14 and that
stage 16 follows stage 15. By means of the rule R10, we
can compute the total order between the gestation
stages, so as to infer also that stage 16 follows stage 14,
and have a complete answer to our query.
The last group of rules enriches the description of
the evolution processes of the embryo. Rule R11 de-
fines the indirect dependency between processes. As
Fig. 5 Dynamic description of the left kidney from stage 14 to stage 20. Each spatio-temporal representation for duration is linked to a process,
which have some evolutionary characteristics, represented by double arrows to numerical information. Processes are represented in yellow boxes
Fig. 6 The impaction of a disease on a processus. A pathology can impact a process to lead to a malformation like a malrotation of an organ, a
less of migration or growth. The pathologies are represented in dark purple boxes
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 8 of 15
illustrated in Fig. 11, as the kidney growth depends on
the interaction between kidney and ureter and this
last one depends on the fixation between kidney and
ureter, we can deduce that the kidney growth depends
on the fixation between kidney and ureter. Rule R12
connects processes and pathologies. For example, as
shown in Fig. 12, since the lack of kidney growth implies
the hypoplastic kidney, and that the kidney growth depends
on the interaction between kidney and ureter, we can also
deduce that the lack of interaction between kidney
and ureter implies the hypoplastic kidney. The last
two rules relate processes to the anatomical entities
they impact. As depicted in Fig. 13, rule R13 allows
us to infer that process left_kidney_growth_between_-
te14_and_te20 impacts entity left_kidney due to the
fact that the left_kidney_growth_between_te14_and_te20
is the process described by the spatio-temporal representa-
tion st_representation_of_left_kidney_from_te14_to_te20,
and this spatio-temporal representation describes an
instance of the left_kidney. Finally rule 14 and 15 are
used to infer the Carnegie stages where a process oc-
curs. These are illustrated in Fig. 14. For example, we
can infer that the process left_kidney_growth occurs
between stages 14 and 20, because its associated
spatio-temporal representation lasts during this
period.
3D visualization and modeling
Finally, the animated 3D model is built using the infor-
mation stored into the ontology. The scene creation re-
quires three steps. First, the user selects the organs and
the gestation period. Secondly, all the informations de-
scribing those organs are retrieved from the ontology.
Thirdly, these informations are used to create the meshes
corresponding to the organs, and to animate the scene.
Fig. 7 Overview of the left kidney static and dynamic descriptions in MyCF Embryo. For the kidney example, we can see the links with spatio-temporal
representation, the geometrical component and the process, and the impact of the disease. Note the importance of the spatio-temporal representation
which is the central part of our ontology
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 9 of 15
For example, the user can select a set of organs of the
uro-genital system (gonads, kidneys, ureters, cloaca,
bladder, rectum), and a period which goes from Carnegie
stage 10 to gestation week 12.
The following query extracts all geometrical re-
presentations and properties of the left kidney for the
reference model we built, named mcfe: e00_left_
kidney.
For each spatio-temporal representation (?str) and
geometrical representation (?ge) of the left kidney, this
query extracts the geometrical shape (?shape), the stage
(?stage) and all informations needed to describe the
shape of organs (?property and ?value) like their axis size
or their barycenter location.
Informations about processes are obtained in a similar
way.
The organ information extracted from the ontology is
procedurally interpreted. The first step is to create a static
scene containing all the selected organs. Each organ of the
scene is a mesh generated with the information obtained
from its geometrical entity in the ontology.
We now describe how the meshes are generated. When
the geometrical shape is an ovoid or a cylinder, the mesh
can be simply generated from the ovoid or cylinder bary-
center position, axis size and orientation. When the
geometrical shape is a duct, the mesh is generated as a
sequence of cylinders around its skeletal line. This skeleton
is defined by a Bézier curve, whose endpoints coordinates
and tangents are directly given in the ontology. In some
cases, a complex geometry may not be fully procedurally
described. In such a case, an existing mesh can be specified
as a link in the ontology. Therefore, the mesh can be in-
cluded directly in the 3D scene.
Fusion and division processes require specific model-
ing approaches because the associated shape of organs
must blend or split seamlessly. For example, the cloaca
divides itself into two parts becoming the bladder and
the rectum. These organs are represented using implicit
Fig. 9 Inference R3 and R4
Fig. 8 Inference R1 and R2. The inferred links are represented with
dotted arrows
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 10 of 15
surface modeling which are surfaces defined as the constant
value, called isovalue, of a field function defined in 3D
space. In our implementation, we define the field function
as a smooth decreasing function of the distance to a refer-
ence point, or a reference curve, called the skeleton of the
implicit shape. More specifically, we chose to use a polyno-
mial field function with compact support called meta-ball.
Finally, the implicit surfaces are converted into a mesh for
the visualization (Figs. 15, 16 and 17).
Lastly, the scene is procedurally animated, thanks to the
information given by the evolution processes. As the
Carnegie stages may have different durations, we consider
that the gestation day is the common unit of time to com-
pute the animation through all stages. As an example, stage
11 lasts one day whereas stage 17 lasts three. Consequently,
the animation of stage 11 lasts one time unit whereas stage
17 lasts three.
Given these settings, it is possible to generate an ani-
mated 3D scene representing the embryo development
given only high-level descriptions of the contributing or-
gans. To illustrate how a 3D scene and an animation of a
complex process (such a kidney and urogenital formation
and migration with ureter fixation) can be built and moni-
tored by the ontology. The complementary video (see Add-
itional file 3: movie 1) firstly shows each step described in
the ontology in an independent manner, and lastly the
resulting complex 3D animation. This video demonstrates
the effective monitoring of the ontology over the 3 D
models of organs displayed using a basic rendering. High
quality rendering of the scene could be explored in a future
work.
Discussion
This work proves that a 3D graphical model can be moni-
tored by an appropriate ontology. This ontology incorpo-
rates a subset devoted to spatial knowledge, which allows
the description of organs as 3D shapes, and another subset
focusing on temporal knowledge describing these organs at
a specific stage. This ontology is also deductive, i.e., from
an initial condition and some evolutionary process, it can
compute a final condition through a procedural approach.
As an example, an initial position and orientation of the
kidney at stage 14 with two evolutionary processes (growth
and rotation) effectively produces its rotational movement
and corresponding growth.
This work aims at making a step towards automatic and
adaptive 3D modeling and simulation of complex embryo-
logical processes. This distinguishes our work from existing
3D atlases in which ontologies are mainly used to navigate
through static, predefined 3D illustrations.
Our 3D geometrical representation of the organs is lim-
ited to simple 3D primitives. We explain hereafter how this
work could be extended to handle more accurate 3D
shapes.
Firstly, let us note that the organs can be described hier-
archically (see for instance Palombi et al. [21,22]). Secondly,
complex and detailed shapes can be defined and deformed
with the knowledge of a more simple one acting as proxy
Fig. 10 Inference R8 and R9. Str correspond to the spatio-temporal representation instance
Fig. 11 Inference R11
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 11 of 15
or bounding structure. This implies that a 3D scene de-
scribing the evolution of the organs can be described using
two levels of details.
At the scale of the overall scene, the purpose is the defin-
ition of the global interactions between different organs.
For instance, these interactions can be the relative position
and collisions between organs, or the branching structures.
At this level, all the organs can take part to the computa-
tion, and the 3D local geometry may not need to be accur-
ate. Note that this level is the one we described in this
paper using simple primitives.
At the complementary level, the organs are described
more accurately, possibly using lower levels of details.
These levels that we plan to address in a future work
can be handled more locally and may involve only a sin-
gle organ at a time, or the direct neighboring organs in
the case of collisions. We propose to handle this local
geometrical description in interleaving two approaches.
Firstly, using a parametric description incorporating the
use of more basic shapes associated with more parame-
ters handled within the ontology. Secondly, using a mesh
based representation to achieve accurate 3D description
and visualization. Such a mesh will use the knowledge of
the coarse representation acting as a guiding structure
to be deformed. The deformation itself can be imple-
mented using a smooth volumetric interpolation func-
tion such as mean value [37], harmonic [38] or Green
[39] coordinates functions, for instance. Note that
simulation-based approaches and self-collision handling
can also be used with the previous approaches to handle
small geometrical details.
We believe that using such an approach, i.e. interleaving
the basic guiding shapes with the complex mesh descrip-
tion in an iterative process applied through the different
level of details can converge toward a more accurate geo-
metrical description of the organs.
We can also note that, introducing a physically-based
simulator in the 3D model could extend the range of simu-
lated events for pathological studies. More specifically, the
research and teaching of teratology "in silico" could benefits
from the proposed approach. Indeed, the complexity of
these topics makes the proposed 3D representation essen-
tial for a good understanding.
Conclusion
We have demonstrated that an ontology can be used to
unify and organize medical knowledge in the domain of
embryo development together with 3D modeling processes
and animation.
Also we have shown the extensibility and scalability of
our declarative approach based on RDF triples and rules. In
fact, this work extends the ontology MyCF [21] which has
been developed and enriched by medical students who had
no difficulty after a short training to edit and update a
RDF-based knowledge base. The scalability of the approach
had already been shown for MyCF that contained about
74000 classes and properties as much as 11 rules for
describing 3D models of human body and its functions.
We intend to extend the ontology for all structures in
the embryo for the entire duration of gestation and to val-
idate the ontology using information from scans of real
embryos. All ontologies represent the knowledge of a
Fig. 12 Inference R12
Fig. 13 Inference R13
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 12 of 15
community at a given time, and must be continually
updated and improved to remain up to date with the latest
knowledge. A unique feature of our approach is the
declarative nature of the graphical models, which makes it
possible for domain experts to enrich the knowledge base
at any time, through simple edit operations without having
to modify the (domain-independent) reasoning algorithmic
machinery used for answering queries. Embryology is in
constant evolution, benefiting from other sciences progress.
We aim at making a step toward a collaborative tool that
will ultimately improve medical care.
Material and methods
The main purpose of this study was to generate an ontol-
ogy of human developmental anatomy unifying embryology
and 3D modeling processes.
Our ontology describes in a comprehensible way the
different geometrical informations, evolution and interac-
tions between various organs during gestation. These de-
scriptions must be formatted in a way such that a 3D
model can be procedurally generated using information
solely queried from the ontology. Our objective was not to
build another ontology as other already exist, rather it was
to add geometrical and dynamic information to existing on-
tologies enabling powerful applications in 3D graphics and
visualization domains.
In this work, two tasks have been addressed simultan-
eously. Firstly, we have built an ontology including all the
required information. To achieve this, we created different
categories that describe entities' geometrical states and
evolution, and a set of properties to link the different parts
together. Secondly, we developed a software tool that
generates a 3D graphical scene representing the embryo
development. To do so, it queries the ontology and
extracts all information necessary for modeling, visualiz-
ing and simulating the 3D scene.
We focused on the development of the urinary system.
We believe this system to be a good study case as its
development involves a diversity of mechanisms that are
representative of other evolution processes and a large
variety of diseases caused by anomalies of these processes.
We used the TopBraidComposer [40] tool which allows
the creation and edition of lightweight ontologies in RDF
format [41] format lightweight ontologies, and to query
them using SPARQL [42].
RDF and SPARQL are standards recommended by the
W3C for the semantic Web and Linked Data \footno-
te{http://linkeddata.org/}.
In addition, TopBraidComposer allow the addition of
inference rules on top of RDF datasets and supports the
application of these rules on RDF facts until saturation (i.e.,
until all the possible facts that can be inferred using the
Fig. 14 Inference R14 and 15
Fig. 15 3D geometrical modelization of the cloacas division process is driven by the ontology
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 13 of 15
rules have been obtained). RDF datasets equipped with
rules allow the capture of most of OWL \cite{owl} con-
straints that are useful in practice, such as the transitivity or
symmetry properties, as well as domain-specific rules with
practical relevance for users in many domains of interest.
The 3D graphical tool was developed using the 3D open
source software Blender [43]. All the 3D surfaces where
procedurally handled using Python [44] scripting within the
Blender framework. The ovoids were handled as Blender
parameterized primitives. The implicit surfaces were repre-
sented as a set of points along the trajectory of the ducts
while the field function and the representative mesh of the
isovalue was handled by Blender software. Finally, the shape
representing the boundary of the embryo was created inter-
actively using Blender interface. All the coordinates param-
eterizing the shapes and its animation were expressed
locally with respect to the embryo main axes and size. The
coordinates were queried within our Python script to be
directly usable within the Blender framework.
Our ontology is public available: [https://mybody.in-
rialpes.fr/mycfembryo/].
Additional files
Additional file 1: Table S1. It contains the detail of each used
properties, the number of properties, classes and instances of the
ontology. (DOCX 24 kb)
Additional file 2: Table S2. It contains the rules that are considered in
the current version of MyCF Embryo. (DOCX 14 kb)
Additional file 3: Movie 1. It demonstrates the control of 3D modeling
by the ontology. (MOV)
Competing interests
The authors declare that they have no competing interests.
Authors contributions
PY-R contributed to the ontological modeling and the design of the overall
architecture, participated to implement the rule engine and to the embryology
expertise. BM contributed to the models (3D models) and the link between
model and ontology, and is the main software developer. FU contributed to the
ontological modeling and to the design of the overall architecture, participated to
implement the rule engine. M-CR contributed to the ontological modeling and to
the design of the overall architecture. DR co-supervised this project. J-CL
contributed to the models (3D models and ontologies). OP conceived the
original idea, contributed to the temporo-spatial models and co-supervised
this project. All authors read and approved the final manuscript.
Acknowledgement
This work has been partially supported by the labEx PERSYVALLab
(ANR11-LABX-0025-01) and by the ANR project PAGODA
(ANR-12-JS02-007-01).
Author details
1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France.
2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, France.
3LIG (CNRS-UJF-INPG-UPMF), Université de Grenoble, Grenoble, France. 4CPE
Lyon, Université de Lyon, Lyon, France.
Received: 14 December 2014 Accepted: 2 September 2015
Fig. 16 Example of the growth and rotation process on two axes of the kidney
Fig. 17 Example of the migration, rotation, growth of kidneys and ureters
Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 14 of 15
RESEARCH Open Access
KneeTex: an ontologydriven system for
information extraction from MRI reports
Irena Spasi?1*, Bo Zhao1, Christopher B. Jones1 and Kate Button2
Abstract
Background: In the realm of knee pathology, magnetic resonance imaging (MRI) has the advantage of visualising
all structures within the knee joint, which makes it a valuable tool for increasing diagnostic accuracy and planning
surgical treatments. Therefore, clinical narratives found in MRI reports convey valuable diagnostic information. A
range of studies have proven the feasibility of natural language processing for information extraction from clinical
narratives. However, no study focused specifically on MRI reports in relation to knee pathology, possibly due to the
complexity of knee anatomy and a wide range of conditions that may be associated with different anatomical entities.
In this paper we describe KneeTex, an information extraction system that operates in this domain.
Methods: As an ontologydriven information extraction system, KneeTex makes active use of an ontology to strongly
guide and constrain text analysis. We used automatic term recognition to facilitate the development of a domainspecific
ontology with sufficient detail and coverage for text mining applications. In combination with the ontology,
high regularity of the sublanguage used in knee MRI reports allowed us to model its processing by a set of
sophisticated lexicosemantic rules with minimal syntactic analysis. The main processing steps involve named
entity recognition combined with coordination, enumeration, ambiguity and coreference resolution, followed by text
segmentation. Ontologybased semantic typing is then used to drive the template filling process.
Results: We adopted an existing ontology, TRAK (Taxonomy for RehAbilitation of Knee conditions), for use within
KneeTex. The original TRAK ontology expanded from 1,292 concepts, 1,720 synonyms and 518 relationship instances
to 1,621 concepts, 2,550 synonyms and 560 relationship instances. This provided KneeTex with a very finegrained
lexicosemantic knowledge base, which is highly attuned to the given sublanguage. Information extraction
results were evaluated on a test set of 100 MRI reports. A gold standard consisted of 1,259 filled template
records with the following slots: finding, finding qualifier, negation, certainty, anatomy and anatomy qualifier.
KneeTex extracted information with precision of 98.00 %, recall of 97.63 % and Fmeasure of 97.81 %, the
values of which are in line with humanlike performance.
Conclusions: KneeTex is an opensource, standalone application for information extraction from narrative
reports that describe an MRI scan of the knee. Given an MRI report as input, the system outputs the corresponding
clinical findings in the form of JavaScript Object Notation objects. The extracted information is mapped onto TRAK,
an ontology that formally models knowledge relevant for the rehabilitation of knee conditions. As a result, formally
structured and coded information allows for complex searches to be conducted efficiently over the original MRI
reports, thereby effectively supporting epidemiologic studies of knee conditions.
* Correspondence: I.Spasic@cs.cardiff.ac.uk
1School of Computer Science & Informatics, Cardiff University, Cardiff CF24
3AA, UK
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2015 Spasi? et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Spasi? et al. Journal of Biomedical Semantics  (2015) 6:34 
DOI 10.1186/s13326-015-0033-1
Background
Magnetic resonance imaging (MRI) is a technique used
to visualise internal body structure by recording radio
waves emitted by the tissues in the presence of a strong
magnetic field. MRI better differentiates between soft tis-
sues than does X-ray imaging, which uses high frequency
electromagnetic waves that pass through soft parts of the
human body to create a radiograph, an image resulting
from the different absorption rates of different tissues.
MRI can also produce three dimensional images. When it
comes to diagnosing knee pathology, MRI has the advan-
tage of visualising all structures within the knee joint, i.e.
both soft tissue and bone. When used in conjunction with
medical history and physical examination, this makes
MRI a valuable tool for increasing diagnostic accuracy
and planning surgical treatments [15]. For example,
meniscal tears are a relatively common knee injury, having
a prevalence of 22.4 % among all soft tissues injuries seen
in a trauma department [6]. The accuracy of diagnosing
meniscal tears using individual physical tests is reported
to be 74 %, but increases to 96 % when MRI is used [5].
When MRI results are combined with clinical assessments
(namely, locking, giving way and McMurrays test), then
their diagnostic performance increases respectively as fol-
lows: accuracy  88.3 %, 89.9 % and 89.4 %, sensitivity 
95.7 %, 97.4 % and 97.4 %, specificity  74.2 %, 75.8 % and
74.2 %, positive predictive value  87.5 %, 88.4 % and
87.7 %, and negative predictive value  90.2 %, 94.0 % and
93.9 % [4]. More recently, the importance of MRI in diag-
nosis and treatment planning for cases of symptomatic
early knee osteoarthritis has been highlighted. If an Xray
image of the knee is found to be normal, but clinical
examination produces specific findings, then MRI scan
can be performed to establish more accurate diagnosis. It
can be used to identify an appropriate surgical or nonsur-
gical treatment target and decrease the need for costly and
invasive diagnostic arthroscopy [1, 7].
In clinical practice, radiology images (e.g. produced by
Xray or MRI) are usually accompanied by imaging re-
ports (or radiology reports), which serve the purpose of
conveying a specialist interpretation of images and relate
it to the patients signs and symptoms in order to suggest
diagnosis [8]. This information is then used by clinicians
to support decision making on appropriate treatment.
In terms of research, MRI evidence is often used to sup-
port epidemiologic studies of knee pathology [9, 10]. In
particular, MRI findings are indispensible features of
longitudinal studies of knee osteoarthritis [11, 12], where
lesions detected by MRI were found to precede onset of
clinical symptoms. However, many of published research
findings are probably false due to sampling bias and low
statistical power [13]. Small sample size is often the cause
underlying these two concerns although the relationship is
not simple or proportional [14]. Unfortunately, sample
size is typically subject to funding and personnel con-
straints. Given the complexity and cost of manual inter-
pretation of MRI evidence, it is, therefore, not surprising
that the size of such epidemiologic studies has been lim-
ited to hundreds (e.g. 514 [9], 710 [10]) or even dozens of
cases (e.g. 20 [11], 36 [12]). If interpretation of evidence
described in MRI reports could be automated, then it
would overcome the size limitation in retrospective cohort
studies posed by the need to manually sort through the
evidence.
We recently provided a critical overview of the current
state of the art for natural language processing (NLP) re-
lated to cancer [15], where clinical narratives such as those
found in pathology and radiology reports convey valuable
diagnostic information that is predictive of the prognosis
and biological behaviour of a disease process [16]. The
review highlighted the fact that a range of studies have
proven the feasibility of NLP for extracting structured
information from free text reports (e.g. [1721]). For sim-
pler information extraction tasks, humanlike perform-
ance of automated systems can be expected. For example,
when evaluated for the extraction of American College of
Radiology utilisation review codes from radiology reports,
M+, a system for medical text analysis, achieved recall,
precision and specificity of 87, 85 and 98 % respectively
[22]. These results were comparable to average recall, pre-
cision and specificity recorded by physicians, namely 88,
86 and 98 %. Comparably good results were achieved for
more complex tasks such as translating radiology reports
into a large database [18], where the Medical Language
Extraction and Encoding (MedLEE) system achieved recall
of 81 % and specificity of 99 % with a total of 24 clinical
conditions (diseases, abnormalities and clinical states)
being the subject of the study. Again these results were
comparable to average recall (85 %87 %) and specificity
(98 %) achieved by expert human coders.
Typical processing steps taken in such NLP systems
include text segmentation into words, sentences, para-
graphs and/or sections, partofspeech tagging, parsing,
named entity recognition (NER), normalisation and neg-
ation annotation [17, 23, 24]. Recognition of named en-
tities, i.e. phrases that are used to differentiate between
entities of the same semantic type (e.g. Osgood-Schlatter
disease is a name used to refer to a specific disease),
followed by normalising the representation of their
meaning (e.g. Osgood-Schlatter disease is also known as
apophysitis of the tibial tubercle or OSD), is the crucial
step towards semantic interpretation of clinical narra-
tives. In order to disambiguate named entities and assert
relationships between them (e.g. relate disease/disorder,
sign/symptom or procedure to an anatomical site), do-
mainspecific knowledge needs to be available in a ma-
chinereadable form. For example, the domain knowledge
is specified in MedLEE using a table created manually
Spasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 2 of 26
based on domain expertise [17]. Similarly, Medical Text
Analysis System (MedTAS) utilises external knowledge re-
sources such as terminologies and ontologies [25]. Alterna-
tively, M+ uses Bayesian Networks to represent semantic
types and relations within a specific medical domain such
as that of chest radiology reports [22]. Ideally, when a suit-
able ontology is available it can be used to add an explicit
semantic layer over text data by linking domainspecific
terms, i.e. textual representation of concepts, to their
descriptions in the ontology [26]. This allows text to be
mined for interpretable information about domain
specific concepts and their relationships.
In our previous work, we developed TRAK (Taxonomy
for RehAbilitation of Knee conditions), an ontology that
formally models knowledge relevant for the rehabilitation
of knee conditions [27]. This knowledge resource allowed
us to implement an NLP system able to interpret kneere-
lated clinical findings from MRI reports. In this paper, we
describe KneeTex, an opensource, standalone applica-
tion developed to address the task of information extrac-
tion from narrative reports that describe an MRI scan of
the knee. KneeTex is an ontologydriven, rulebased sys-
tem. It takes an MRI report as an input and outputs the
corresponding clinical findings in the form of JavaScript
Object Notation (JSON) objects, a lightweight datainter-
change format [28]. KneeTex not only extracts, but also
codes the extracted information by mapping it onto the
TRAK ontology. The resulting formally structured and
coded information allows complex searches to be con-
ducted efficiently over the original MRI reports, thereby
effectively supporting epidemiologic studies of knee
conditions.
Methods
System specification
Information extraction (IE) is the task of automatically
selecting specific facts about prespecified types of entities
and relationships from freetext documents. In other
words, the goal of IE is to convert free text into a struc-
tured form by filling a template (a data structure with pre-
defined slots) with the relevant information extracted (slot
fillers) [29]. Figure 1 provides a graphical representation of
a template specific to our system, whose structure is illus-
trated using Unified Modelling Language (UML) [30]. The
template specifies the types of entities and relationships
we aim to extract in this particular study.
The goal of our system is to extract information about
clinical observations made from MRI scans. Information
extracted about individual observation is structured into
two major parts: finding and anatomy. Finding represents
a clinical manifestation (e.g. injury, disease, etc.) observed
by a radiologist. In other words, it corresponds to what is
observed. Information related to anatomy refers to a
specific part of human anatomy affected by the finding. In
other words, it corresponds to where the finding is
observed. Both finding and anatomy may have qualifiers,
Fig. 1 Information extraction template represented by UML diagram. Each slot has got the following properties: extracted text, concept identifier,
preferred concept name, start position of extracted text and its length
Spasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 3 of 26
which provide more specific information extracted about
them. In addition to general qualifiers, each finding is
associated with information about its certainty (as judged
by the radiologist) and negation (which specifies whether
the finding is positive or negative). Tables 1 and 2 provide
examples of a filled template based on information ex-
tracted automatically from the given sentences. The filled
template examples are represented using JSON.
Data
Between January 2001 and May 2012, a total of 6,382
individuals with an acute knee injury attended the Acute
Knee Screening Service at the Emergency Unit of the
Cardiff and Vale University Health Board (C&V UHB).
A subset of 1,657 individuals fulfilled locally agreed clin-
ical criteria for an MRI scan. Both the clinical assess-
ment and MRI findings for these individuals were stored
in a clinical database on a secure server within the C&V
UHB. This database was originally developed for the
purposes of service evaluation and auditing practice.
Out of 1,657 referred individuals, a total of 1,468 MRI
scan visits were identified retrospectively from the data-
base records. Following an MRI scan, the imaging results
were summarised by a radiologist (from a team of five)
in a diagnostic narrative report that conveys a specialist
interpretation of the MRI scan and relates it to the pa-
tients signs and symptoms. These MRI reports formed
the dataset used in this study.
All reports were anonymised by removing all identi-
fiable information related to either patient or radiolo-
gist together with the attendance date and the links
to the patients assessment and treatment details. The
anonymised reports were transferred to an encrypted
memory stick that was password protected and locked in
a filing cabinet in a lockable room. Ethical approval for
this study was obtained from the South East Wales
Research Ethics Committee (10/MRE09/29).
As part of their radiology reporting initiative whose aim
is to improve reporting practices by creating a library of
clear and consistent report templates, the Radiological
Society of North America provides a template for knee
MRI reports [31]. However, the reports in our dataset did
not follow any such predefined structure. The structure
varied across the reports, but they generally tended to or-
ganise information under the following headings: MRI OF
THE LEFT/RIGHT KNEE, INDICATION, HISTORY,
FINDINGS and CONCLUSION. Within the reports,
which were distributed in a plain text format, these sec-
tions were indicated with upper case (see Table 3 for an
example).
The size of the overall dataset was 1,002 KB with a total
of 13,991 sentences, 178,931 tokens, 3,277 distinct tokens
Table 1 An example of a filled template. Original text source:
There is a small undisplaced vertical radial tear of the posterior
horn of the lateral meniscus.
Table 2 An example of a filled template. Original text source:
A peripheral tear involving the body of the lateral meniscus
extending into the posterior third is seen.
Table 3 Knee MRI report. A sample from the training dataset
Spasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 4 of 26
and 2,681 distinct stems. On average, the size of an indi-
vidual MRI report was 0.68 KB (±0.40 KB) with a total of
9.53 (±5.13) sentences and 110.81 (±64.60) tokens.1 We
separated the data into training and testing sets. A test set
was created by randomly selecting a subset of 100 MRI
reports from the overall dataset. These reports were then
removed from consideration so that the performance of
the system could later be evaluated on unseen data. The
remaining 1,368 reports formed a training set, which was
used to inform system development.
For training and testing purposes, the data were manu-
ally annotated with labels that correspond to slots and
their relationships from the IE template (see Fig. 1). The
annotation was performed using BRAT, a webbased tool
for text annotation [32]. The process involved annotating
text spans with slot names (e.g. finding or anatomy) as
well as annotating dependencies between them (e.g. obser-
ved_in). Figure 2 provides a visualisation of an annotated
example. A total of 100 training documents and 100 test-
ing documents were annotated independently by two
annotators.
Ontology
We previously developed TRAK as an ontology that for-
mally models knowledge relevant for the rehabilitation
of knee conditions [27]. This information includes classi-
fication of knee conditions, detailed knowledge about
knee anatomy and an array of healthcare activities that
can be used to diagnose and treat knee conditions.
Therefore, TRAK provides a framework that can be used
to collect coded data in order to support epidemiologic
studies much in the way Read Codes, a coded thesaurus
of clinical terms [33], are used to record observational
data in the Clinical Practice Research Datalink (CPRD) 
formerly known as the General Practice Research Data-
base (GPRD) [34]. TRAK follows design principles rec-
ommended by the Open Biomedical Ontologies (OBO)
Foundry and is implemented in OBO [35], a format
widely used by this community. Its public release can be
accessed through BioPortal [36], a web portal that pro-
vides a uniform mechanism to access biomedical ontol-
ogies, where it can be browsed, searched and visualised.
TRAK was initially developed with a specific task in
mind  to formally define standard care for the rehabilita-
tion of knee conditions. At the same time, it was designed
to be extensible in order to support other tasks in the
domain. For example, the knowledge about knee anatomy,
which is crossreferenced to a total of 205 concepts in the
Foundational Model of Anatomy (FMA) [37], is directly
applicable to interpretation of reports describing knee
MRI scans. However, in order to fully support semantic
interpretation of this type of clinical narratives, the TRAK
ontology needed to be expanded with other types of
MRIspecific concepts.
Ontology expansion
In order to support semantic interpretation of the ter-
minological content found in knee MRI reports, we
needed to ensure that all relevant concepts are modelled
appropriately in the TRAK ontology. The main aspect of
this task was the expansion of a specific domain modelled
by the ontology, for example, MRIspecific observations
such as hyaline cartilage abnormality, bone bruise, cyclops
lesion, etc. In order to support NLP applications of the
ontology, its vocabulary also needed to be expanded to
include term variants commonly used in MRI reports.
Some term variants are confined to a specific clinical
sublanguage [18] and as such are typically underrepre-
sented in standardised medical dictionaries such as those
included in the Unified Medical Language System (UMLS)
[38]. For example, collateral ligament was found to have
no other synonyms in the UMLS. Yet, collateral ligaments
are colloquially referred to as collaterals in clinical narra-
RESEARCH Open Access
OntoStudyEdit: a new approach for
ontology-based representation and
management of metadata in clinical and
epidemiological research
Alexandr Uciteli* and Heinrich Herre*
Abstract
Background: The specification of metadata in clinical and epidemiological study projects absorbs significant
expense. The validity and quality of the collected data depend heavily on the precise and semantical correct
representation of their metadata.
In various research organizations, which are planning and coordinating studies, the required metadata are specified
differently, depending on many conditions, e.g., on the used study management software. The latter does not
always meet the needs of a particular research organization, e.g., with respect to the relevant metadata attributes
and structuring possibilities.
Methods: The objective of the research, set forth in this paper, is the development of a new approach for
ontology-based representation and management of metadata. The basic features of this approach are
demonstrated by the software tool OntoStudyEdit (OSE). The OSE is designed and developed according to the
three ontology method. This method for developing software is based on the interactions of three different kinds
of ontologies: a task ontology, a domain ontology and a top-level ontology.
Results: The OSE can be easily adapted to different requirements, and it supports an ontologically founded
representation and efficient management of metadata. The metadata specifications can by imported from various
sources; they can be edited with the OSE, and they can be exported in/to several formats, which are used, e.g., by
different study management software.
Conclusions: Advantages of this approach are the adaptability of the OSE by integrating suitable domain
ontologies, the ontological specification of mappings between the import/export formats and the DO, the
specification of the study metadata in a uniform manner and its reuse in different research projects, and an intuitive
data entry for non-expert users.
Introduction
There is a large variety of particular clinical and epidemio-
logical research projects, which typically produce a large
amount of data. The data stem from questionnaires, inter-
views but also from specific findings and from laboratory
analyses. Before these data can be collected, the needed
metadata must be precisely specified. The metadata in-
clude, in the context of this paper:
 The description of all data elements/items (e.g.,
questions of a questionnaire, measurements of an
investigation) by particular attributes (e.g., question
text, description of a measurement, unit of measure,
data type, codelist);
 The description of a study structure, i.e., the grouping
of the items and the description of the corresponding
groups by suitable attributes (e.g., title, commentary).
These groups may be modules within an assessment,
complete assessments (e.g., questionnaires, interviews,
physical examinations, laboratory analyses of taken
specimen), or assessment groups (e.g., according to
* Correspondence: alexander.uciteli@imise.uni-leipzig.de; heinrich.herre@
imise.uni-leipzig.de
Institute for Medical Informatics, Statistics and Epidemiology (IMISE),
University of Leipzig, Leipzig, Germany
© 2015 Uciteli and Herre. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 
DOI 10.1186/s13326-015-0042-0
study cohorts or to particular data acquisition time
points), as well as all items of the study.
The specification of the metadata in particular research
organizations must consider certain requirements, e.g.,
which item attributes are relevant (e.g., name, label, range,
data type, format, unit of measure), how the items should
be grouped (e.g., module, item group), or which study
management software or data entry tools (hereinafter
referred to as study software) are used (e.g., OpenClinica
[1], ERT [2]).
In this paper we present and discuss a new approach
for the ontology-based representation and management
of metadata in clinical and epidemiological research,
which is demonstrated by the software tool OntoStudyE-
dit (OSE). The OSE can easily be adapted to the needs
of a particular research organization by the use of a suit-
able domain ontology. Furthermore, it supports and
provides an ontology-based configuration of the im-
port/export functions in the desired formats without
the necessity to change the source code. The import/
export functions need only to be implemented once
for a format type (e.g., xml, excel, sql, pdf ), and can
be configured by an ontology-based definition of
mappings between a format type and the domain
ontology. This approach has the advantage that the
domain experts (e.g., biometrician, data manager,
study assistant) can specify the study metadata ac-
cording to the common usage in a particular research
organization by using the respective familiar termin-
ology and without dealing with technical issues. By
the provision of import from various sources and ex-
port to several formats the differently specified meta-
data can be represented on the same semantic basis;
hence, the once specified metadata can be reused in
various research projects and utilized by different
study software (or other tools).
Methods
Ontology-based representation of metadata
Metadata are used to describe data, hence, they add
more precise meaning to data, the semantics of which
remains often underspecified. Since the metadata itself
must be specified by some formal representation, the
meaning of which should be explained, we arrive at an
infinite regress, which must be brought to an end by
some basic principle. In our approach this infinite
regress is blocked by using the top-level ontology Gen-
eral Formal Ontology (GFO) [3]. GFO provides the most
basic layer for ontological foundation and represent a
well-established semantic basis for modelling metadata.
The generic method of reconstruction, or of modelling
the domain entities within the framework of a top-level
ontology is called the method of ontological reduction.
This method as well as the suitability of GFO for model-
ling metadata were presented in [4].
The OSE is a plug-in for Protégé-Frames [5], which is
conceptually based on the notion of a frame. We decided
to use Protégé-Frames for our implementation because it
supports the generation of data acquisition forms. The
forms can easily be adapted, i.e., the slot widgets (input
fields like text field, text area, checkbox, combobox) can
be selected and arranged in any given layout. Furthermore,
the specification of the slot facets (e.g., cardinality, mini-
mum, maximum, default values) allows to elegantly con-
trol the quality of the user input (e.g., alert on missing
data, prevent false data entries). In summary, the data
acquisition forms of Protégé-Frames permit an intuitive
data entry for non-expert users, in comparison, e.g., to the
OWL version of Protégé.
Frames are formal representational structures, which
are exhaustively classified into the following types: clas-
ses, slots, facets and individuals [6]. Together with
axioms they form the building blocks for Protégé-
Frames ontologies. Classes represent concepts related to
a domain. Slots represent properties or attributes of clas-
ses, whereas facets describe properties of slots. Slots
may be attached to frames, and then they describe prop-
erties of that frame. A slot, attached to a frame, can have
values, which might be constraint by facets. A slot can
be attached to a frame as a template slot or as an own
slot [6].
The concepts, represented by Protégé classes, are asso-
ciated in GFO to categories, and the slots attached to a
class frame describe properties of that class. A category
is defined in GFO as an entity being independent of time
and space, that can be instantiated. A category is repre-
sented by some symbolic structure, which denotes a
meaning, also called intension. The notion of a class - as
used, for example, in UML [7], OWL [8], or Frames -
captures relevant aspects of categories. Subsequently, we
use the term class in the sense of a symbolic represen-
tation of a category and the term slot - as a symbolic
representation of a property or a relation. A meta-class
in Protégé-Frames corresponds in GFO to a category the
instances of which are themselves categories. In Protégé-
Frames each class is an instance of a Standard-Meta-
Class. In GFO there exists a meta-category, denoted by
Category(2), the instances of which are all categories of
first order. A category is of first order if all of its
instances are individuals. The meta-classes in Protégé
correspond to the second-order categories in GFO, which
are extensional subcategories of the category Category(2).
The three ontology method
The OSE is designed and developed according to the
three ontology method [9]. This method for developing
software is based on the interactions of three different
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 2 of 9
kinds of ontologies: a task ontology (TO), a domain
ontology (DO) and a top-level ontology (TLO). The TO
is an ontology for the general problem that the software
is intended to solve. The DO provides the domain-
specific knowledge, whereas the TLO integrates the TO
and the DO and is used as the foundation of them. The
TLO also provides means for integrating data from
different domains. For integrating the TO and the DO we
use the TLO GFO because it is sufficiently expressive, in
particular, it contains an ontology of categories (that
allows categories of higher order), as well as an ontology
of properties and attributives [4].
Results
The ontological architecture of the OSE
The ontological architecture of the OSE is represented
by systems of categories of several levels of abstraction
and relations between them (Fig. 1). The TO is an upper
ontology with respect to the considered DOs, hence, the
DO categories are extensional subcategories of the TO
categories. The GFO is used as semantic foundation for
both TO and DO by classifying categories of TO and
DO under particular GFO categories (e.g., category,
process, individual).
We use classes for the representation of categories and
slots for the representation of properties and relations in
Protégé-Frames ontologies.
Task and domain ontology of the OSE
In this section we consider the classes which represent
categories of the TO and DO (Fig. 2).
The names of TO classes starts with the underline-
sign. The TO includes at the upmost level following
classes: _CONFIG, _ELEMENT, and _ANNOTATING_
ONTOLOGY_ROOT. The annotating ontologies are
inserted below the node _ANNOTATING_ONTOLO-
GY_ROOT (see Annotating ontologies and annotation).
The class _ELEMENT, its subclasses and instances are
visible for the user. The instances can be edited by the
user by means of a graphical user interface (GUI). On
the other hand, the class _CONFIG, and its sub-
classes and instances are hidden from the user; these
classes and its instances are used by the OSE in the
background.
The subclasses of _ELEMENT are: _CONSTANT, _OB-
JECT, _GROUP, _ANNOTATION, and _EXPORT. The
instances of _CONSTANT are used in expressions, the
class _OBJECT represents individual entities (e.g., items
or measurement units), whereas the class _GROUP stands
for lists, which might contain further elements (instances
of _ELEMENT). The instances of _ANNOTATION are
annotations of elements by concepts of the annotating
ontology (see Annotating ontologies and annotation). The
subclasses of _EXPORT represent export formats pro-
vided for the export function.
The following subclasses of _CONFIG are introduced:
_FORMAT_MAPPING, _ANNOTATION_TYPE, the former
of which is described in more detail in section Ontological
specification of mappings between import/export formats and
domain ontology and the latter  in section Annotating ontol-
ogies and annotation. In particular, as instances of _ANNO-
TATION_TYPE various annotation types can be defined. The
class _FORMAT_MAPPING and its subclasses are used to
specify ontologically various import and export formats.
The class _MATH and its subclasses are used by the
expression editor, displayed in the working area E (see Usage
of the OSE). The class _MATH_EXPRESSION_RELATOR
and its subclasses describe various mathematical operations
and functions (for example: AND, >, +). These relators
possess arguments, being numbers (_NUMBER), constants
(_CONSTANT), particular study elements (_STUDY_ELE-
MENT, e.g., Item), or further relators. With these means the
editor is able to build an expression in form of a tree. The TO
includes a number of slots, the most important of which are
the following: _contains and _HIERARCHY_SUBCLASS.
The _contains slot represents the relation between instances
(e.g., Page: B1 _contains Module: Socio-demographic data),
whereas the slot _HIERARCHY_SUBCLASS describes the
corresponding basic relation between classes (example: Page
has _HIERARCHY_SUBCLASS Module). The relation
_HIERARCHY_SUBCLASS is formally defined as follows:
Cat_1 _HIERARCHY_SUBCLASS Cat_2 :=
(? x y) (x :: Cat_1 ? _contains(x,y)? y :: Cat_2).
A further constituent of the ontological architecture is
the DO. This ontology is embedded into the TO, hence,
these classes are subclasses of TO classes. We developed
an example DO, based on the structure of clinical trials
which are carried out at the Clinical Trial Centre Leipzig
Fig. 1 Ontological architecture
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 3 of 9
[10]. Classes like Study or Module are placed below the
class _GROUP, whereas Item or Unit_of_Measure are
subclasses of _OBJECT. The slots of these classes can
freely be defined. The class Item, for example, can pos-
sess following slots: name, description, unit_of_measure,
range, codelist or rules.
Ontological specification of mappings between import/
export formats and domain ontology
In this section we outline the specification of mappings
between import/export formats and DO for the example
of an xml-based format, CDISC ODM [11]. For the
mapping specification the subclasses of _FORMAT_-
MAPPING are used.
The tag structure of an xml-based format can be
considered as a tree. As a first step the root tag (in our
example, <ODM>) must be specified as instance of
_ROOT_TAG. The other tags are defined as instances
of _TAG. For each tag its sub-tags and attributes must
be specified by the instance editor (Fig. 3). The tag
<Study> contains, e.g., the sub-tags <GlobalVariables>,
<BasicDefinitions>, and <MetaDataVersion> as well as
the attribute OID. The tags are mapped to the DO
classes, whereas their texts and attributes - to the DO
slots. In our example, the tag <Study> is mapped to the
class Study and the attribute OID - to the slot :NAME
of the class Study.
The whole tag tree must be specified only once; this is
done by declaration of the corresponding sub-tags and
attributes and by the specification of its mapping to the
classes resp. slots of the DO. Then, this mapping can be
used for the import/export of various metadata
specifications.
Annotating ontologies and annotation
By using Protégé it is possible to import an ontology
into another one. We could, e.g., import the ACGT
Master Ontology [12], phenotype or property ontol-
ogies, or LOINC [13] and may use their categories
for the annotation of the instances of the DO (e.g.,
concrete item instances like DYSPNEA_AT_REST).
Ontologies, intended to be used to annotate instances
of a DO within the OSE, are called in this paper an-
notating ontologies.
For this purpose, we introduced the following classes
into the TO:
Fig. 2 Task and domain ontology
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 4 of 9
 _ANNOTATION_TYPE (subclass of _CONFIG).
Within DO we may define various annotation
types, being instances of _ANNOTATION_TYPE
(e.g., annotated_with, risk_factor_of, symptom_of ).
 _ANNOTATION (subclass of _OBJECT). This class
has three slots: _annotated_elements,
_annotation_type und _annotating_concepts. Using
OSE we may create concrete annotations as
instances of _ANNOTATION. This is realized by
selecting the elements to be annotated, taken from
subclasses of _ELEMENT, by choosing an
annotation type from the instances of
_ANNOTATION_TYPE, and by selecting suitable
classes taken from the annotating ontology.
 _ANNOTATING_ONTOLOGY_ROOT. Below this
class annotating ontologies can be inserted.
 _ANNOTATING_ONTOLOGY_METACLASS
(subclass of :STANDARD-CLASS). This is a meta-
class, containing all classes of the annotating ontology
as instances. This class has an additional slot, denoted
by _annotations, which is defined as the inverse slot
of _annotating_concepts.
We may not only annotate single instances, but also
sets of instances. It is, e.g., not sufficient to annotate the
item ITEM:DYSPNEA_AT_REST as a symptom of
Congestive heart failure, taken from the Human
Phenotype Ontology (HPO) [14]. This item is associated
with a codelist that includes two possible values YES
or NO (depending on whether a patient has dyspnea
or not). Only if YES is selected as answer to the question
whether dyspnea at rest holds, this symptom is true. I.e.,
only the combination of the item ITEM:DYSPNEA_AT_R-
EST and the answer option YES can be annotated (Fig. 4).
In this way both conditions are AND-connected, express-
ing that the question was answered and that the answer is
yes. If an item does not possess a codelist, then also
ranges can be annotated. An example is the annotation of
the item ITEM:SYSTOLIC_BLOOD_PRESURE together
with Range [121;] (i.e., >= 121) with the concept Elevated
systolic blood pressure from HPO (Fig. 4).
By use of the inverse slot _annotations for classes
of the annotating ontology we may realize the fol-
lowing valuable feature: for a given concept all of its
annotations can be displayed. This functionality can
be very important in searching for items in certain
domains during the planning phase of a study. If we
are planning, e.g., a study for heart failure we may
ask for all annotations of the class Congestive heart
failure (and possibly of its subclasses). These anno-
tations will then be displayed. In this way one has a
quick access to items which can be used to query
the symptoms or risk factors of the heart failure
(Fig. 5).
The annotation of different instances of the domain
ontology (even of different domain ontologies) by the
Fig. 3 ODM format mapping
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 5 of 9
same class of the same annotating ontology establishes a
semantic connection between these instances. Such
annotations allow the semantic search for items over the
categories, which are used in these annotations, but they
also allow the comparability of data which are acquired
for various distinct studies.
Usage of the OSE
Subsequently, we sketch the graphical user interface
(GUI) and the main functions of the OSE: specification,
management, import and export of metadata, searching
and navigation.
The GUI is partitioned into five working regions A, B,
C, D and E (Fig. 6):
A: Study Elements. Within this region all study elements
(being subclasses of _ELEMENT) are represented and
displayed. Besides a class, the number of its instances is
shown (put in brackets). For a selected class its instances
are displayed in the working field C (Instance Browser).
Furthermore, a searching field is available. For the export
of the specified metadata an export format must be
selected (a subclass of _EXPORT) and the button exp
pressed. It is also possible to export a metadata
specification as an ontology that can be used as a Case
Report Form (CRF) preview (Fig. 7).
B: Study Hierarchy. This hierarchy shows the structure
of a study. This hierarchy is formed by instances which
are connected by the contains-relation. The user may
create new elements in a group, may change the ele-
ments order, and may remove elements from a group.
By choosing an element, a form for the acquisition of
its slot values is displayed in the working area D
(Instance Editor). Furthermore, a search field is
available.
C: Instance Browser. The instance browser shows
instances of the class which is selected in the working
area A. By choosing an instance, a form for capturing
Fig. 4 Annotation instance (examples)
Fig. 5 Concept annotations (example)
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 6 of 9
its slot values will be shown in the working area D
(Instance Editor). Instances may be deleted.
Furthermore, it is possible to associate instances from
this working area to groups from the working area B by
drag-and-drop. A search field is available.
D: Instance Editor. The instance editor provides forms
for capturing the slot values of instances.
E: Expression Editor. This editor supports the editing of
formulas, being represented in form of trees. Various
operators and numbers can be used, for example
arithmetical, logical operators, and other relations;
furthermore, study elements (for example items) and
constants can be referenced.
The functionalities of the working areas A: Study Ele-
ments, B: Study Hierarchy and E: Expression Editor were
implemented in the OSE plug-in, whereas the function-
alities of the working areas C: Instance Browser and D:
Instance Editor are already provided by Protégé as
standard features. The working area A was imple-
mented in such a way that only the subtree of
_ELEMENT is displayed for the user, whereas the
subtree of _CONFIG remains hidden. In this way we
may assure that the user of OSE (e.g., data manager,
researcher, study assistant) cannot change the config-
uration that was specified, e.g., by ontologists or IT-
specialists. Furthermore, we implemented for this
working area the import/export of the metadata spe-
cification. The working area B displays the study hier-
archy as a tree which connects the instances (i.e., the
study elements, as for example, module or item) by
the contains-relation, whereas the standard trees of
Protégé represent class hierarchies, based on the is-a
relation. Moreover, we implemented all functions,
needed for manipulating the study structure. Add-
itionally, the expression editor (E) was implemented;
this editor provides all needed functions for designing
and managing of formulas. We increased the usability
Fig. 6 GUI of the OSE. a Study Elements; b Study Hierarchy; c Instance Browser; d Instance Editor; e Expression Editor
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 7 of 9
of the tool by adapting the automatically generated
data acquisition forms (i.e., the slot widgets were
replaced and the form layout was changed).
Related work
There are few systems that pursue similar purposes as
OSE, notably TIM (Trial Item Manager) [15] and
ObTiMA (Ontology-based Managing of Clinical Trials)
[16], which are subsequently considered in more detail.
ObTiMa is a system for ontology-based management of
clinical trials, which is composed of the two components:
the Trial Builder for designing clinical trials and the
Patient Data Management System for handling patient
data within a trial. Trial Builder allows the creation of
CRF items, based on the concepts of an ontology (ACGT
Master Ontology). The main difference between the
ObTiMas Trial Builder and the OSE consists in that the
Trial Builder is based on ODM and the possible item
attributes (e.g., question, data type, measurement unit) are
fixed and cannot be changed or extended, whereas in OSE
item attributes are defined by domain ontologies and
can be flexibly handled. Hence, OSE may take into
account the needs of the diverse research organizations,
which usually differs with respect to the practiced specifi-
cation of metadata that typically use different metadata
types (e.g., items, codelists, modules), different attributes,
groupings, and hierarchical levels (e.g., study-event-
module-item). Furthermore, the flexible, ontology-based
development of mappings between the ontologies of OSE
and diverse import and export formats enables the reuse
of specified metadata in various research projects and
their utilization by different study software.
The TIM pursues aims, analogous to OSE, namely, to
support the specification of items in clinical trials.
Similarly as OSE, TIM is based on a semantic model
consisting of a fixed component (the meta-model and
the core types of the data model), and a flexible module
(domain-specific types of the data model). This structure
Fig. 7 Case report form preview (example)
Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 8 of 9
supports the adaption to user-specific needs. Though,
there are differences between TIM and OSE. In TIM the
fixed component and the flexible part are not clearly
separated, whereas in OSE both components (TO and
DO) are explicitly divided and endowed with an
ontologically-based semantic basis. Consequently, OSE
exhibits a higher flexibility with respect to the change and
adaption of the domain-specific constituents. Further-
more, OSE provides various additional functionalities,
among them, the ontologically-based creation of format
mappings, and the use of rule expressions. Finally, the
usage of Protégé-Frames supports the adaption of the data
acquisition forms, and allows for an extension of the
software by additional plug-ins.
Conclusions and future work
In this paper we presented and discussed a new ap-
proach for ontology-based representation and manage-
ment of metadata in clinical and epidemiological
research using the software tool OntoStudyEdit (OSE).
Advantages of this approach are: 1. the adaptability of
the OSE to intended aims and given needs by integrating
suitable domain ontologies in a modular way; 2. the
ontological specification of mappings between the im-
port/export formats and the DO, such that no changes
of the source code are needed by the replacement of the
DO; 3. the specification of the study metadata in a
uniform manner and reuse of which in different research
projects; 4. an intuitive data entry for non-expert users.
The OntoStudyEdit is a tab widget plug-in for
Protégé-Frames; this implies that all functionalities of
Protégé can be used. Of particular interest is the
adaption of the data acquisition forms. At present, we
are working on the implementation of further import/
export functions, e.g., related to annotated CRF in PDF
format and to specifications for the import in different
study software.
There is ongoing evaluation of the OSE which
started already some time ago. At first, metadata of
the LIFE study are entered with the OSE. For LIFE
we developed an ontology, called LIFE Investigation
Ontology (LIO), and a Protégé-Frames based tool,
called query generator [17]. The metadata part of LIO
was integrated into OSE as a domain ontology. LIO
is a frame ontology and its use as domain ontology in
OSE preserves the structure of the LIFE metadata.
For this reason it is rather simple to input and to
manage the LIFE metadata by using the OSE. We
already experienced that the non-expert users (e.g.,
data manager, researcher, study assistants) are able to
cope well with OSE, as well as with the LIFE query
generator, that is productively used since two years by
the same user group.
Competing interests
The authors declare that they have no competing interests.
Acknowledgements
This paper has been presented at ODLS 2014 (Ontologies and Data in Life
Sciences) in Freiburg.
We acknowledge support from the German Research Foundation (DFG) and
Universität Leipzig within the program of Open Access Publishing.
Received: 24 March 2015 Accepted: 15 December 2015
DATABASE Open Access
Colil: a database and search service for
citation contexts in the life sciences domain
Toyofumi Fujiwara1 and Yasunori Yamamoto2*
Abstract
Background: To promote research activities in a particular research area, it is important to efficiently identify
current research trends, advances, and issues in that area. Although review papers in the research area can suffice
for this purpose in general, researchers are not necessarily able to obtain these papers from research aspects of
their interests at the time they are required. Therefore, the utilization of the citation contexts of papers in a research
area has been considered as another approach. However, there are few search services to retrieve citation contexts
in the life sciences domain; furthermore, efficiently obtaining citation contexts is becoming difficult due to the large
volume and rapid growth of life sciences papers.
Results: Here, we introduce the Colil (Comments on Literature in Literature) database to store citation contexts in
the life sciences domain. By using the Resource Description Framework (RDF) and a newly compiled vocabulary, we
built the Colil database and made it available through the SPARQL endpoint. In addition, we developed a web-
based search service called Colil that searches for a cited paper in the Colil database and then returns a list of
citation contexts for it along with papers relevant to it based on co-citations. The citation contexts in the Colil
database were extracted from full-text papers of the PubMed Central Open Access Subset (PMC-OAS), which
includes 545,147 papers indexed in PubMed. These papers are distributed across 3,171 journals and cite 5,136,741
unique papers that correspond to approximately 25 % of total PubMed entries.
Conclusions: By utilizing Colil, researchers can easily refer to a set of citation contexts and relevant papers based
on co-citations for a target paper. Colil helps researchers to comprehend life sciences papers in a research area
more efficiently and makes their biological research more efficient.
Keywords: Life sciences paper, Citation, Citation context, Co-citation, PMC Open Access Subset, RDF, SPARQL
Background
The ability to efficiently identify current research trends,
advances, and issues in a research area is highly import-
ant to researchers to promote their research activities.
For example, in cases of international collaborative re-
search, researchers often need to read relevant papers
and summarize the current knowledge about the re-
search area beyond their own research fields [1]. Al-
though review papers in the research area can suffice for
this purpose in general, researchers are not necessarily
able to obtain these papers at the right time from view-
points of their interests. In addition, these papers reflect
only previously published papers at the time of writing a
review paper and viewpoints of its authors. To comple-
ment this issue, the utilization of the citation contexts
for papers in a research area has been considered as
another approach [2]. However, there are few search
services to retrieve citation contexts in the life sciences
domain, and to efficiently obtain the citation contexts
for a target paper is becoming difficult due to the large
volume and rapid growth of life sciences papers. Here, we
introduce the Colil (Comments on Literature in Literature)
database and a web-based search service called Colil for
citation contexts in the life sciences domain.
In life sciences papers, citations are widely used and typ-
JOURNAL OF
BIOMEDICAL SEMANTICS
van Dam et al. Journal of Biomedical Semantics  (2015) 6:39 
DOI 10.1186/s13326-015-0038-9
SOFTWARE Open Access
RDF2Graph a tool to recover, understand
and validate the ontology of an RDF resource
Jesse CJ van Dam1*, Jasper J Koehorst1, Peter J Schaap1, Vitor AP Martins dos Santos1,2
and Maria Suarez-Diez1
Abstract
Background: Semantic web technologies have a tremendous potential for the integration of heterogeneous data
sets. Therefore, an increasing number of widely used biological resources are becoming available in the RDF data
model. There are however, no tools available that provide structural overviews of these resources. Such structural
overviews are essential to efficiently query these resources and to assess their structural integrity and design, thereby
strengthening their use and potential.
Results: Here we present RDF2Graph, a tool that automatically recovers the structure of an RDF resource. The
generated overview allows to create complex queries on these resources and to structurally validate newly created
resources.
Conclusion: RDF2Graph facilitates the creation of complex queries thereby enabling access to knowledge stored
across multiple RDF resources. RDF2Graph facilitates creation of high quality resources and resource descriptions,
which in turn increases usability of the semantic web technologies.
Keywords: Data integration, RDF, Databases, Semantic web, Visualization, SPARQL, OWL, Ontology
Background
In the life sciences, high-throughput technologies deliver
ever-growing amounts of heterogeneous (meta) data at
different scales, which are produced, stored and analysed
in both structured and semi-structured formats. Systems
Biology is an integrative discipline that uses various inte-
gration strategies to model and discover properties of bio-
logical systems. Integration and analysis of heterogeneous
biological data and knowledge require efficient informa-
tion retrieval and management systems and Semantic
Web technologies are designed to meet this challenge [1].
The RDF data model is a mature W3C standard [2, 3]
designed for the integrated representation of heteroge-
neous information from disparate sources and it is proving
effective for creating and sharing biological data. RDF
is not a data format, but a data model for describing
resources in the form of self-descriptive subject, predi-
cate and object triples that can be linked in an RDF-graph.
*Correspondence: jesse.vandam@wur.nl
1Laboratory of Systems and Synthetic Biology, Wageningen University,
Dreijenplein 10, 6703 HB Wageningen, The Netherlands
Full list of author information is available at the end of the article
Integration of heterogeneous data from different sources
in a single graph relies on using retrievable controlled
vocabularies, which is essential to access and analyse inte-
grated data [4]. Once data sources are converted into the
semantic Web, SPARQL [5, 6] can be used to query mul-
tiple of these resources, simultaneously or consecutively,
without further modifying any of them.
Widely used biological resources such as Reactome,
ChEBI and UniProt, among others, [710] have been
transformed into the RDF data model and the Bio2RDF
[11] project has transformed a large set of additional
sources, such as the NCBI GenBank files [12], DrugBank
[13] and InterPro database [14]. Additionally, there are
on-going efforts to develop tools providing results in this
data model, such as the Semantic Annotation Platform
for Prokaryotes, SAPP, (J. Koehorst, J. van Dam et al. per-
sonal communication) that provides genome functional
annotation in the RDF data model.
These RDF resources can be readily queried with
SPARQL. Constructing SPARQL queries requires that the
user has a mental representation of the underlying struc-
ture of the resource. The structure of a resource is the
© 2015 van Dam et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
van Dam et al. Journal of Biomedical Semantics  (2015) 6:39 Page 2 of 12
set of object types and their relationships, i.e. the explicit
representation of the predicates linking different classes.
This structure represents the set of semantic constraints
embedded in the resource. In a biological database con-
taining information on biochemical reactions, genes and
their identifiers are linked to proteins; proteins are linked
to EC numbers; EC numbers are connected to reactions
that involve metabolites as products and substrates. To
retrieve information pertainingmetabolites and genes, the
SPARQL query has to obey the specific network topology
linking these types of objects. RDF data sources do not
need a predefined scheme so that new data types can be
added at any time expanding the underlying structure. If
themodifications in the underlying structure generated by
this new data are not known, linked information cannot be
retrieved. Not having a clear idea of the underlying struc-
ture makes querying an RDF resource inefficient, time
consuming, or even impossible.
The structure of a resource can be either retrieved
through manual queries or it can be provided by the data
publishers in the documentation of the resource. This
structural information can be encoded using Web Ontol-
ogy Language (OWL) files [15]. OWL was created as a
description logic language and it is intended for automatic
reasoning; nevertheless, its axioms can also be used to
construct a graphical overview of the described resource
[16]. However, the OWL standard does not require all
axioms necessary for such reconstruction. Examples of
necessary axioms not obligated by this standard are object
all values from and data all values from. In some of the
resources created by the Bio2RDF project these axioms
(object all values from and data all values from) are not
provided. Furthermore, the ontology generation process
is, at best, semi-automatic, time consuming and error-
prone. Errorsmight also accumulate due to the conversion
code used to generate the RDF resource, as the triple
generating code can contain lexical errors in predicate
definition such as typos, inconsistent usage of upper and
lower case, or misspelled words, thereby populating a
resource containing information on proteins with infor-
mation on porteins, which describes proteins associated
to transmembrane transport. These errors lead to descrip-
tions of the intended content of the resource rather than
of its actual content.
Shape Expressions (ShEx) is a standard to describe, val-
idate and transform RDF data. One of the goals of this
standard is to create an easy to read language for the vali-
dation of instance data, however, it is still being developed
and no final recommendation is yet available [1719].
Computational tools able to reconstruct the structure of
RDF resources are thus required to i) facilitate query writ-
ing and to ii) enable data providers to verify the structural
integrity of their resource. To our knowledge, no such tool,
able to automatically recover the structure of the resource
and the associated multiplicity of the predicates, exist.
Semscape [20] is an already existing Cytoscape [21] app
that is able to retrieve to some extent the structure of the
resource. However, it has limited recovery and simplifica-
tion capabilities, leading to unreadable hairballs for larger
structures. Furthermore, additional statistical information
about the classes and links is not retained. Here we present
RDF2Graph, a tool to automatically recover the structure
of an RDF resource and to generate a visualization, ShEx
file and/or an OWL ontology thereof. These can be used
to write SPARQL queries or to verify (generated) RDF
resources.
Implementation
RDF2Graph performs two distinct processes to retrieve
the structure of a resource. Initially, there is a recovery of
all classes, predicates and unique type links together with
their associated statistics. In the second stage there is a
simplification step to arrive to a neat structural overview.
A simplified overview of the complete process is given in
Fig. 1.
A type link is defined as a link joining a subject
class type to an object class or value data type, via a
predicate. A unique type link is defined as a unique
tuple: type of subject, predicate, (data)type of object. For
the triple <:BRCA1, :locatedOn, :chromosome17> the
type link is <:Gene, :locatedOn, :Chromosome>. When
considering the full resource, all type links <:Gene, :locate-
dOn, :Chromosome> correspond to the same unique type
link. In the triple <:Adam :hasSon :Bob> the type link is
<:Person, :hasSon, :Person>.
Themultiplicity of a unique type link describes the num-
ber of instances connected to each other. The forward
multiplicity can be: i) One-to-one (also denoted: 1..1) each
source instance has exactly one reference to the target;
ii) One-or-many (1..N) each source instance has one or
JOURNAL OF
BIOMEDICAL SEMANTICS
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 
DOI 10.1186/s13326-015-0036-yRESEARCH Open AccessOntology-based representation and
analysis of host-Brucella interactions
Yu Lin, Zuoshuang Xiang and Yongqun He*Abstract
Background: Biomedical ontologies are representations of classes of entities in the biomedical domain and how
these classes are related in computer- and human-interpretable formats. Ontologies support data standardization
and exchange and provide a basis for computer-assisted automated reasoning. IDOBRU is an ontology in the
domain of Brucella and brucellosis. Brucella is a Gram-negative intracellular bacterium that causes brucellosis, the
most common zoonotic disease in the world. In this study, IDOBRU is used as a platform to model and analyze
how the hosts, especially host macrophages, interact with virulent Brucella strains or live attenuated Brucella
vaccine strains. Such a study allows us to better integrate and understand intricate Brucella pathogenesis and
host immunity mechanisms.
Results: Different levels of host-Brucella interactions based on different host cell types and Brucella strains were
first defined ontologically. Three important processes of virulent Brucella interacting with host macrophages were
represented: Brucella entry into macrophage, intracellular trafficking, and intracellular replication. Two Brucella
pathogenesis mechanisms were ontologically represented: Brucella Type IV secretion system that supports intracellular
trafficking and replication, and Brucella erythritol metabolism that participates in Brucella intracellular survival and
pathogenesis. The host cell death pathway is critical to the outcome of host-Brucella interactions. For better survival
and replication, virulent Brucella prevents macrophage cell death. However, live attenuated B. abortus vaccine strain
RB51 induces caspase-2-mediated proinflammatory cell death. Brucella-associated cell death processes are represented
in IDOBRU. The gene and protein information of 432 manually annotated Brucella virulence factors were represented
using the Ontology of Genes and Genomes (OGG) and Protein Ontology (PRO), respectively. Seven inference rules
were defined to capture the knowledge of host-Brucella interactions and implemented in IDOBRU. Current IDOBRU
includes 3611 ontology terms. SPARQL queries identified many results that are critical to the host-Brucella interactions.
For example, out of 269 protein virulence factors related to macrophage-Brucella interactions, 81 are critical to Brucella
intracellular replication inside macrophages. A SPARQL query also identified 11 biological processes important for
Brucella virulence.
Conclusions: To systematically represent and analyze fundamental host-pathogen interaction mechanisms, we
provided for the first time comprehensive ontological modeling of host-pathogen interactions using Brucella as
the pathogen model. The methods and ontology representations used in our study are generic and can be
broadened to study the interactions between hosts and other pathogens.* Correspondence: yongqunh@med.umich.edu
Unit of Laboratory Animal Medicine, Department of Microbiology and
Immunology, Center for Computational Medicine and Bioinformatics, and
Comprehensive Cancer Center, University of Michigan Medical School, 1150 W.
Medical Center Dr, Ann Arbor, MI 48109, USA
© 2015 Lin et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to
the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 2 of 18Background
In the field of infectious diseases, the study of the inter-
active relationships between pathogens and their hosts is
critically important. An infectious disease is the result of
intense interactions between a pathogen and its host. Dur-
ing these interactions both the host and the pathogen at-
tempt to manipulate each other using a complex network
mechanism to maximize their respective survival probabil-
ities. For the goal of survival and replication, the pathogen
may adapt different pathogenesis strategies to infect the
host. On the other hand, the host may apply innate and
adaptive immune defense mechanisms to fight against the
invading pathogen. Different live attenuated vaccines may
stimulate sufficient host immunity but does not induce
damaging effects on the host. For decades, scientists have
conducted research to study the different aspects of host-
pathogen interactions. In order to obtain a full picture of
the host-pathogen interaction mechanisms, separate data
from those studies needs to be integrated. Thus, a strategy
of knowledge representation, management and reasoning
based on the huge data resources is in need. Such a strat-
egy will enable the knowledge integration, complicated
biological data analysis, and provide insights for biologists
to generate new hypotheses.
Brucella is a Gram-negative, non-spore-forming, facul-
tative, intracellular bacterium that causes chronic zoonotic
brucellosis in humans and a variety of animal species [1].
Human brucellosis remains the most common zoonotic
disease worldwide, with more than 500,000 new human
cases reported annually [2]. A safe and effective human
vaccine is required but does not yet exist. A rational vac-
cine design would benefit from insightful understanding
of the interactions between host and Brucella, specifically,
Brucella pathogenesis and host defense mechanisms. Bru-
cella infections are typically chronic in nature [1], suggest-
ing a continuous interaction between host and Brucella.
To promote its long-term intracellular survival, Brucella
minimizes the activation of host inflammatory mecha-
nisms. For example, Brucella lipopolysaccharide (LPS) has
100- to 1000-fold decreased capability to activate pro-
inflammatory TNF-? and IL-1 cytokines compared to
similar concentrations of E. coli LPS [3]. Hundreds of Bru-
cella protein virulence factors participate in the Brucella
pathogenesis and interacting with host immune systems
[4, 5]. The immune systems in various Brucella hosts re-
spond poorly against virulent Brucella strains but very
well against live attenuated Brucella vaccine strains [68].
Such a complex host-pathogen interaction system involves
a number of cells, molecules and biological processes.
Therefore, the host-Brucella interactions present a good
example for an ontology-based exploration of complex
bacterial pathogenesis and host immunity mechanisms.
As an extension ontology of the Infectious Disease Ontol-
ogy (IDO) [9], Brucellosis Ontology (IDOBRU) is developedpreviously at our lab [10]. Following the good practice of
the OBO Foundry principles [11], IDOBRU was developed
under the framework of the Basic Formal Ontology (BFO)
[12] and IDO. BFO contains two branches, continuant and
occurrent [11, 13]. The continuant branch represents time-
independent entity such as material entity, and the occur-
rent branch represents time-related entity such as process.
By aligning different domain ontologies under the two
branches of BFO, the knowledge from broad biological
areas could be captured and organized under a compre-
hensive ontology-level structure. Since IDO aligns with
BFO, IDOBRU automatically aligns with BFO. IDOBRU
exemplifies IDO in the case of brucellosis, which covers a
broad range of topics, including host infection, zoonotic
disease transmission, symptoms, virulence factors, patho-
genesis, diagnosis, intentional release, vaccine prevention,
and treatment [10].
Since the study of host-Brucella interactions has been a
major brucellosis research effort, this paper goes beyond
the simplified introduction of virulence factors and patho-
genesis in the previous IDOBRU paper [10], and provides
more detailed ontological representation on various as-
pects of the host-pathogen interactions. In this report,
different types of the pathogen-side pathogenesis mecha-
nisms and host-side immune defense strategies are de-
scribed with specific examples. Comparing with the 245
virulence factors in the original IDOBRU ontology [10],
current version includes 432 virulence factors in Brucella.
Furthermore, a list of computer-understandable logic in-
ference rules is defined in this study to make the virulence
factors, host-Brucella interactions and related processes
computable. Use cases are provided to demonstrate how
such ontological representations and inference rule-based
automated reasoning help data integration and query in
the area of host-pathogen/vaccine interactions.
Results
In what follows, italics are used to make ontological ax-
ioms (i.e., statements that say what is true in the domain)
(http://www.w3.org/TR/owl2-syntax/), simple bold words
represent ontology relations, single quotes are used to rep-
resent ontology terms, and double quotes are used for text
definitions or emphases.
Overall design of ontological representing of
host-Brucella interaction
Various disciplines dissect interactions into different tai-
lored meanings. The definition of an interaction in the
host-pathogen interaction area includes a two-way effect
(i.e., hosts effect upon pathogen, and pathogens effect
upon host). The interactions between host (e.g., human)
and Brucella exemplifies a host-pathogen interaction in
the context of biology. As an intracellular bacterium, Bru-
cella strains are able to invade, survive and replicate for
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 3 of 18prolonged periods within in vivo host cells or in vitro cul-
tured host cells. The GO term interspecies interaction be-
tween organisms (GO_0044419) is defined as any process
in which an organism has an effect on an organism of a
different species. Without capturing the granularity at the
host cell level, this GO term is not sufficient for our case.
The definition of host-Brucella interaction in IDOBRU is
an interspecies interaction that is the physical encounter
of two parties: Brucella and its host organism or host cell.
While interacting, Brucella and its host organism or host
cell have effects upon each other. The interaction in this
definition specifically emphasizes the physical encounter
and interaction of the two parties in an action. Also, given
that Brucella is an intracellular bacterium, it is important
to explicitly mention both of the host organism and the
host cell.
The host-Brucella interaction includes three main
parts: 1) Brucella entry into host cell (IDO_0101170)
(the interaction at the interface between Brucella and
host), 2) process of establishing Brucella infection in
host (IDO_0100426) (Brucella side response), and 3)
host anti-Brucella process (IDO_0100115) (host side
response). The IDOBRU term Brucella entry into host
cell is a child term of GO entry of bacterium into host
cell (GO_0035635). The IDOBRU term process of es-
tablishing Brucella infection in host is a child term of
the IDO-core term process of establishing an infection
(IDO_0000603), which is under GO biological process.
The host anti-Brucella process is a child term of GO
biological process.
As we mentioned before, IDOBRU adopted Basic For-
mal Ontology version 2 (BFO 2) as its top level ontology
[10]. Favoring BFO is due to the integrative nature of
IDOBRU, a representation of all aspects of brucellosis, as
it requires integrating with other OBO library ontologies,
including the Cell Type Ontology (CL) [14], Chemical En-
tities of Biological Interest (ChEBI) [15], Gene Ontology
(GO) [16], Information Artifact Ontology (IAO) [17],
Ontology of Biomedical Investigations [18], Ontology of
General Medical Science (OGMS) (https://code.google.
com/p/ogms/), Ontology of Genes and Genomes (OGG)
[19], and Protein Ontology (PRO) [20]. BFO serves as a
common structure and a formal framework to seamlessly
integrate existing terms from all OBO foundry ontologies.
Under this consideration, we adopt relations formally de-
fined by the community as much as possible. To represent
the relations between molecular entities and its related
host-Brucella interactions, we adopted the relation has_
agent formally defined by Smith et al. [21]. In the defin-
ition provided by Smith et al., a p has_agent c at t denotes
a primitive relation between a process p, a continuant c
and a time t at which the continuant c is causally ac-
tive in the process [21]. The notion of causally active
is aimed to capture the from-to directionality natureof a biological process, which provides the explicit meas-
ure for the two-way effect interactions. Applying this re-
lation to model host-Brucella interaction, we differentiate
the host-Brucella interaction into two categories: 1) Bru-
cella process towards host infection, and 2) host anti-Bru-
cella process. The first category has Brucella as its agent,
and the second category has a host as its agent. In another
word, Brucella actively causes establishing Brucella infec-
tion in host, and host actively causes host anti-Brucella
processes. The Brucella and host both play an agent role in
the processes of host-Brucella interaction.
To go beyond the textual definitions and logically define
the host-Brucella interaction and many other ontology
terms, two approaches were used. One approach was to
use an ontological axiom(s). An ontological axiom is a
statement that provides explicit logical assertions about
three types of things: classes, individuals and properties
(http://www.w3.org/TR/owl2-syntax/#Axioms). The other
facts implicitly contained in the ontology can be inferred
using a reasoning software program (i.e., a reasoner). An-
other approach is based on inference rules. On the Se-
mantic Web, the term inference means an automatic
procedure that can generate new relationship(s) based
on the data (e.g., ontology knowledge data) and some
additional information in the form of a vocabulary, for
example, a set of rules (http://www.w3.org/standards/
semanticweb/inference). An inference rule (IR) is a lo-
gical form consisting of a function that takes premises,
analyzes the syntax, and returns a conclusion(s).
To improve data integration and discover new relation-
ships and possible inconsistencies, we have defined seven
inference rules (IRs) in this report. We have first devel-
oped inference rules to capture the physical interaction of
the two entities (agents) in a host-Brucella interaction.
We use the IF p THEN q inference rules to state that,
given the truth of p, allows the truth of q to be inferred.
The IFTHEN rules are used as a knowledge representa-
tion format that can be easily understood and simple to
implement by a computer [22]. Here we formalized the
first inference rule (IR1) to define a host-Brucella inter-
action as following:
(IR1) IF (a agent_in p, ? b agent_in p), ? p is_a
process, ? (a part_of A, ? b part_of B) ? (A is_a
(host organism ? host cell), ? B is_a Brucella), THEN
p is_a host-Brucella interaction
IR1 gives three constrains sufficient to define a direct
host-Brucella interaction: 1) two entities a and b are
agents in a process; 2) these two entities are parts of entity
A and B respectively; 3) A is a host organism or host cell,
and B is a Brucella bacterium. A and B are disjoint with
each other. It is noted that while a direct host-Brucella
interaction involves both host and Brucella molecules, a
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 4 of 18process that participates in the host-pathogen inter-
action may be just a typical host or pathogen process
that is triggered by the direct host-Brucella interaction.
In this article, we will provide many examples showing
the pathogen-side or host-side processes following a
direct host-Brucella interaction.
Besides IR1, this article will provide six other IFTHEN
inference rules. The IR2 provides an inference that one
process is preceded by another process after a direct host-
Brucella interaction. IR3-IR7 is about the inference on a
virulence factor. Different from ontology axioms that
usually represent necessary or (necessary and sufficient)
conditions, these IFTHEN inference rules represent
sufficient criteria (IF conditions) for a specific inference
(THEN conclusion).
While ontology axioms behave like inference rules,
the Web Ontology Language (OWL) is unable to express
all relations and inference rules (http://dior.ics.muni.cz/
~makub/owl/#ontology). One common language that can
be used to define inference rules is the Semantic Web Rule
Language (SWRL) developed based on a combination of
the OWL language with the Rule Markup Language
(http://www.w3.org/Submission/SWRL/). While inference
rules do not have to be a part of the ontology, SWRL can
be used to represent the rules in an ontology like IDOBRU
as shown in Additional file 1. These inference rules de-
scribed in an ontology have at least two functions. First,
these rules in combination with a logic reasoner support
ontology consistency check. If an ontology has errors in
conflict with an inference rule, a reasoner like Hermit
(http://hermit-reasoner.com/) will be able to detect the
error. Second, an IFTHEN inference rule can gener-
ate a conclusion on an ontology instance based on spe-
cified conditions.
While inference rules are less frequently used in bio-
medical ontologies, the SPARQL Protocol and RDF Query
Language (SPARQL) has been widely used for querying
ontologies [23] and is familiar to the general readers in
biomedical semantics. In this article, we have also pro-
vided many SPARQL examples to illustrate the usage of
SPARQL in querying information related in host-Bru-
cella interactions.
Ontology modeling of different types of Brucella
Brucella strains can be separated into smooth strains and
rough Brucella strains depending on their lipopolysac-
charide (LPS) compositions. As a major component of the
outer membrane of Brucella, Brucella LPS is composed of
three parts: O polysaccharide (or called O-chain or O-
antigen), core oligosaccharide, and lipid A. The O polysac-
charide, a repetitive glycan polymer attached to the core
oligosaccharide, is the outermost domain of the LPS mol-
ecule. The presence or absence of the LPS O polysacchar-
ide determines whether a Brucella strain has the smoothor rough phenotype, respectively. The presence of full-
length O polysaccharides would render the bacterium
smooth, whereas the absence of O polysaccharides would
make the bacterium rough [24]. Rough Brucella is usually
attenuated, and it does not stimulate the production of
anti-O polysaccharide antibody in an infected host. In con-
trast, most smooth Brucella strains are virulent, and they
have intact O polysaccharides and can stimulate anti-O
polysaccharide antibody in host [25]. Virulent wild type B.
abortus, B. melitensis, B. neotomae, and B. suis are all
smooth strains, and their rough strains (including many
vaccine and vaccine candidate strains) are attenuated.
However, virulent wild type B. canis and B. ovis are
rough. Therefore, the virulent/attenuated and smooth/
rough characteristics of Brucella strains may not match
each other.
In order to define the smooth and rough characteristics
of Brucella, the negative statement of has no Brucella O
polysaccharide needs to be addressed. Ceusters et al. has
given a set of lacks relations to represent the negative
findings in electronic health records [26]. For example, the
relation lacks_part was defined in terms of the positive re-
lation part_of, holds between a particular p and a universal
u when p has no u as part [26]. However, all relations in
OWL are relations between particulars by default and can-
not represent the relation between a particular and a uni-
versal. It is possible to rely on the punning (an OWL2
feature) that allows OWL developers to use the URI of a
class for an individual (http://www.w3.org/TR/owl2-new-
features/#F12:_Punning). Another shortcoming about the
lacks_part relation in OWL is that the relation in OWL
expression C subClassOf: lacks-part some D implies the
presence of an instance of D where the relation itself
suggests the lack of such an instance [27]. Therefore,
although initially we used the lacks_part relation, we
have recently switched to the use of not has_part as
suggested in the article [27]. Here we adopt their strat-
egy to define the terms smooth Brucella strain and
rough Brucella strain as follows:
Smooth Brucella strain = def a Brucella has_part some
smooth Brucella lipopolysaccharide
Rough Brucella strain = def a Brucella has_part only
rough Brucella lipopolysaccharide
Smooth Brucella lipopolysaccharide = def a Brucella
lipopolysaccharide has_part some Brucella O
polysaccharide
Rough Brucella lipopolysaccharide = def a Brucella
lipopolysaccharide not has_part some Brucella O
polysaccharide
Ontology modeling of host-Brucella interaction subtypes
Brucella bacteria are able to invade and infect both profes-
sional and non-professional phagocytes. The interactions
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 5 of 18between Brucella and these host cells dictate the outcomes
of the infection [28]. At least three types of host cells are
recognized: macrophages, dendritic cells, and epithelial cells
(e.g., placental trophoblast cells) [28]. Some cell lines, such
as mouse macrophage cell line J774 [25] and human
epithelial cell line HeLa [29] are important models for
studying host-Brucella interactions. Those cell types
and cell lines have been imported into IDOBRU from
Cell Type Ontology (CL) [14] and Cell Line Ontology
(CLO) [30] respectively by using OntoFox, a web-based
tool for retrieving and extracting ontological terms and
axioms [31].
Given the above three types of host cells and two types
of Brucella strains, we have asserted several subtypes of
host-Brucella interactions using the format of host cell 
Brucella interaction. Specifically, six subtypes of host-
Brucella interactions were asserted:
1) macrophage  smooth Brucella interaction
2) macrophage  rough Brucella interaction
3) dendritic cell  smooth Brucella interaction
4) dendritic cell  rough Brucella interaction
5) epithelial cell  smooth Brucella interaction
6) epithelial cell  rough Brucella interaction.
Different agents participate in each of the host-Brucella
interactions. For example, the agents in macrophage-Bru-
cella interactions include:
1) Brucella agent in macrophage  Brucella
interaction
2) Brucella agent in process of establishing Brucella
infection in macrophage
3) (macrophage or macrophage cell line cell) agent in
macrophage  Brucella interaction
4) (macrophage or macrophage cell line cell) agent in
macrophage anti-Brucella processFig. 1 Ontological representation of various interactions between Brucella5) macrophage-Brucella interaction has_part process
of establishing Brucella infection in macrophage
6) macrophage-Brucella interaction has_part
macrophage anti-Brucella process
Figure 1 illustrates the above triples and gives six sub-
types of macrophage-Brucella interactions.
It is noted that the above categorizations do not count
on the interactions between Brucella and host organ
(e.g., spleen) or the whole host at an organism level. As
Brucella is an intracellular bacterium, the macrophage-
Brucella interaction is critical to the outcome of the
host-Brucella interaction [32]. In this article, we will pri-
marily use the macrophage-Brucella interaction processes
as an example for modeling the host-Brucella interactions.
Ontology representation of Brucella invasion, trafficking,
and replication inside host cells
As an intracellular bacterium, the invasion, survival and
replication of Brucella inside host cells are crucial to
Brucellas lifecycle. The ultimate goal of Brucella is to
propagate in their preferred niche in host cells (particu-
larly the macrophages), where they can reach extensive
replication and subsequently transmitted to new host
cells. The intracellular life of Brucella is a subject of in-
tensive scientific research [28, 32].
Specifically, through different modes of entry into a
macrophage (details given in the following section), a
smooth or rough Brucella cell will enter a Brucella-con-
taining vacuole (BCV) inside a macrophage. The BCVs
containing smooth and rough Brucella cells undergo differ-
ent intracellular trafficking pathways. Smooth BCVs be-
come mature replicative niche, where the bacteria undergo
extensive intracellular replications. In such a replicative
niche, programmed macrophage cell death is prevented,
which is beneficial for the intracellular Brucella. In con-
trast, rough bruellae are fused with lysosome and cannotand host cells
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 6 of 18replicate inside macrophages [28]. The rough BCV will
undergo programmed cell death [25, 3335]. The process
of rough Brucella interacting with macrophages precedes
the process of programmed macrophage cell death, which
is beneficial for the host.
Brucella: from entry into host cell to replication niche
Smooth and rough Brucella strains utilize different mech-
anisms of entry into host cells. The Brucella LPS O poly-
saccharide is a critical molecule for interaction with lipid
rafts within the plasma membrane of a host cell. The lipid
rafts mediate the internalization of Brucella by phagocytes
and nonprofessional phagocytes in a manner that leads to
the development of the replicative niche [28]. The onto-
logical representation of Brucella entry into macrophages
and other related processes is shown in Fig. 2.
As shown in Fig. 2a, a smooth Brucella LPS (part of Bru-
cella) and a lipid raft from a macrophage (part of macro-
phage) are both agents in the process of the interaction
between smooth LPS and macrophage lipid raft. A lipid
raft is composed of cholesterol (CHEBI_16113), ganglioside
GM1 (CHEBI_61048), and glycosylphosphatidylinositol
(CHEBI_24410). The LPS-lipid raft interaction leads to the
engulfment of smooth Brucella into a macrophage, pre-
ceded with the formation of an early phagosome contain-
ing smooth Brucella. This early phagosome does not fuse
with late endosome or lysosome [28]. The smooth Brucella
containing vacuole (BCV) is acidified and trafficked to anFig. 2 Ontological representation of the entrance into macrophage to replendoplasmic reticulum (ER), where the BCV is fused with
ER membrane. The fusion event leads to the formation of
ER-derived replication niche of smooth Brucella, where the
intracellular replication takes place. It is noted that only a
small percentage of all invading Brucella cells will survive
and achieve their goal of intracellular replications through
the trafficking pathway.
Compared to smooth Brucella, rough Brucella cells have
a different fate (Fig. 2b). Since rough Brucella does not
have LPS O polysaccharide, rough Brucella enters into a
macrophage via direct macrophage engulfment rather than
the lipid raft-dependent engulfment [36]. The early rough
Brucella-containing phagosome is fused with lysosome to
form a rough Brucella-containing phagolysosome, where
the invading bacterium is killed by the macrophage [36].
To formalize the representation of Brucella intracellular
trafficking pathways, five formal relations defined in BFO
were used: participates in, initially participates in, trans-
formation of, begins to exist during, and starts_during.
The terms participates_in and its subclasses initially par-
ticipates in and begins_to_exist_during are three rela-
tions between continuant and process. If a continuant c
begins to exist during a process p, it infers that continu-
ant c does not exist before p starts. If a continuant c ini-
tially participates in a process p, it infers that p cannot
start without the existence of continuant c. Transfor-
mation_of links continuants in a similar fashion as pre-
ceded_by linking processes. Specifically, if continuantication by smooth Brucella (a) and rough Brucella (b)
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 7 of 18c1 transformation_of continuant c2, it infers that c2
exists earlier than c1. The term starts_during represents a
relation between two processes. Taken these five relations
together, the following IR2 is developed to infer the tem-
poral relations between two processes:
(IR2) IF c1 participates in p1, ? c initiatively
participates in p2, ? c2 begins_to_exist_during p1,
? c2 transformation_of c1, THEN p2 starts_during p1
For example, based on the biological fact that an early
smooth Brucella-containing phagosome is transformed
to smooth Brucella-containing vacuole (Fig. 2), the fol-
lowing statements are represented in IDOBRU:
1. acidic phagosome containing smooth Brucella
participates in smooth Brucella containing
phagosome fusing with ER membrane
2. ER-derived replication niche of smooth Brucella
initiatively participates in smooth Brucella
intracellular replication in macrophage
3. ER-derived replication niche of smooth Brucella
begins_to_exist_during smooth Brucella containing
phagosome fusing with ER membrane
4. ER-derived replication niche of smooth Brucella
transformation_of acidic phagosome containing
smooth Brucella
Based on IR2, it is inferred that smooth Brucella intra-
cellular replication in macrophage starts_during smooth
Brucella containing phagosome fusing with ER membrane.
The inferred statement is biologically valid [6].
As described in the Methods section, IR2 has been added
to IDOBRU using the SWRL syntax (Additional file 1).
Representing intracellular survival of smooth Brucella inside
macrophages
While a smooth Brucella-containing vacuole is trafficking
within a host cell, the bacteria inside the vacuole encounter
formidable environmental stresses such as the exposures
to reactive oxygen (ROS) and nitrogen species (RNS),
acidic pH, nutritional deprivation, and lytic peptides con-
tained in lysosomes [28]. To withstand these environmen-
tal stresses, Brucella has developed different strategies.
IDOBRU uses several smooth Brucella resistance sub-
class terms to represent the resistant processes enabling
smooth Brucella to survive under the stressful environ-
ments within macrophages. Examples of such terms are:
smooth Brucella resistance to nutrient deprivation,
smooth Brucella resistance to antimicrobial peptide,
smooth Brucella resistance to nitrosative stress inside
BCV, smooth Brucella resistance to oxidative stress in-
side BCV, and smooth Brucella resistance to acidity
stress inside BCV. The part of  relation was used to linkabove processes to process of establishing smooth Bru-
cella survival in macrophage in the ontology.
The abilities of smooth Brucella resistance to those
stressful conditions were represented as different types of
disposition. The IDO term protective resistance is used
as their direct mother term in virtue of protecting Brucella
from different stresses. Smooth Brucella has the disposi-
tions including nutrient deprivation resistance disposition,
antimicrobial peptide resistance disposition, oxidative
stress resistance disposition, nitrosative stress resistance
disposition, and acidic stress resistance disposition. These
dispositions are realized in relevant resistance processes
and inhere in a smooth Brucella strain. For example, the
nutrient deprivation resistance disposition of smooth Bru-
cella is realized in the process of smooth Brucella resist-
ance to nutrient deprivation, and smooth Brucella strain
is the agent participating in the process (Fig. 3).
As the interactions are between host and pathogen,
the response actions of macrophage cells are represented
in IDOBRU using four biological process terms: nitric-
oxide synthase biosynthetic process (GO_0051767), re-
active oxygen species metabolic process (GO_0072593),
acidification of BCV in macrophage (IDO_0100758),
and macrophage antimicrobial peptide production (IDO_
0100759). All these processes are part of the macrophage
anti-Brucella process. The unfolds_in relation is used to
capture the fact that those processes take place inside a
macrophage cell (Fig. 3).
Representing Brucella pathogenesis mechanisms
In macrophages, the majority of BCVs fuse with lysosomes
and the bacteria are killed and degraded in the early hours
of internalization [36]. Approximately 1030 % of internal-
ized Brucella cells are able to survive and undergo the
intracellular trafficking of the host cell [37]. The molecular
mechanisms of Brucella pathogenesis are responsible for
all kinds of interactions with their mammalian hosts. Viru-
lent Brucella employs several strategies and uses many
virulence factors to establish and maintain persistent intra-
cellular residence in host cells. Intracellular Brucella also
alters biological functions of professional phagocytes,
resulting in the losing of their robust antigen-processing
capacity. In order to prevent more hostile extracellular en-
vironment, virulence Brucella is able to prevent the pro-
grammed cell death of infected macrophages [38]. Brucella
pathogenesis relies on the presence of many Brucella viru-
lence factors and their interactions with the host defense
system. Two examples are generated here to illustrate how
IDOBRU represents the interactions between host and
bacterial genes, proteins and pathways.
Representing type IV secretion system
Bacterial type IV secretion systems (T4SS) are often crit-
ical to selective translocation of proteins and DNA
Fig. 3 Ontological representation of Brucella intracellular survival inside macrophages
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 8 of 18protein complexes [39]. Brucella T4SS, encoded by the
virB operon, is a major virulence factor system [3841].
Brucella T4SS virB operon includes 12 genes whose ex-
pression is specifically induced by the phagosome acidifi-
cation after Brucella phagocytosis [40]. Brucella T4SS is
required for smooth Brucella trafficking to replication
niches and intracellular survival inside host cells. In
rough strain of Brucella, T4SS expression is important
for Brucella cytotoxicity in macrophages [38].
Figure 4a represents Brucella T4SS and its roles in the
pathogenesis of smooth Brucella. The Brucella virB op-
eron expression is preceded by acidification of smooth
Brucella containing phagosome. The Brucella VirB pro-
teins, encoded by the Brucella virB operon, start to exist
with the expression of the virB operon genes (begin_
to_exist_during). The process of binding a virB promoter
regulates the T4SS virB operon expressions. For example,
VjbR, a LuxR-type quorum-sensing regulator, binds on the
virB promoter, and activates Brucella virB operon expres-
sion [42]. Therefore, VjbR regulates Brucella T4SS directly
and subsequently has an impact on the effectors of T4SS
protein secretions. As shown in Fig. 4a, VjbR is an
agent in the Brucella virB promoter binding process,
which leads to positive regulation of the Brucella virB
operon expression.
Brucella T4SS acts as a translocator of Brucella proteins
or DNA into a macrophage [42]. Entities translocated or
secreted by Brucella T4SS are termed as Brucella T4SS
effectors, which regulate different processes essential toBrucella intracellular trafficking or replication. A Brucella
T4SS effector is defined as a molecular entity that bears
the Brucella T4SS effector role [40]. The Brucella T4SS
effector role is a new IDOBRU term that is defined as: a
role that inheres in a Brucella protein or a DNA upon
which the Brucella T4SS acts, and as a result, the protein
or DNA is secreted out of the bacterium.
As an example of a Brucella T4SS effector, Brucella pro-
tein RicA (Rab2 interacting conserved protein A), is repre-
sented in IDOBRU to illustrate the Brucella T4SS virulence
mechanism (Fig. 4). Brucella RicA, encoded by a Brucella
gene BMEI0736, binds to human small GTPase protein
Rab2 [43]. RicA is translocated from B. abortus to infected
macrophages. However, this phenomenon does not occur
when a Brucella virB mutant infects the macrophage cell.
Rab2 also coordinates the retrograde Golgi-to-ER transport
[44]. The Rab2 is essential for the formation of the fusion
between BCV and ER, which results in the Brucella replica-
tion niche [45]. As shown in Fig. 4b, Brucella T4SS partici-
pates in the process of translocation of RicA into a
macrophage as an agent. Brucella protein RicA bears Bru-
cella T4SS effector role, and it participates in the processes
of translocation of RicA into macrophage and RicA-
Rab2 binding. The process of RicA binding to Rab2
leads to the formation of RicA-Rab2 complex. Rab2
participates in the regulation of Golgi-to-ER transporta-
tion, which regulates the retrograde Golgi-to-ER trans-
port. Overall, Fig. 4b provides a detailed explanation to
an axiom stated in Fig. 4a: Brucella T4SS effector
Fig. 4 Ontological modeling of Brucella type IV secretion system and its effects. a The general Brucella type IV secretion in Brucella (a) and
example of RicA as T4SS effector (b)
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 9 of 18participates in the regulation of Brucella intracellular
trafficking.
Representing Brucella erythritol metabolism
As described earlier, one of the intracellular environmental
stresses that Brucella faces is nutritional deprivation. Bru-
cella uses alternative metabolism pathways to obtain car-
bon, nitrogen, oxygen, phosphorus, sulfur and metals
from its intracellular host [5]. For example, an alternative
pathway for Brucella to acquire carbon is through the
erythritol metabolism [28, 46]. The genome of attenuated
B. abortus vaccine strain S19 includes a 703 nucleotide de-
letion on its ery operon. The deletion affects the gene eryC
coding for an enzyme erythrulose-1-phosphate dehydro-
genase (EryC) and another gene eryD that encodes for
EryD, a regulator of ery operon expression [47]. The dele-
tion is a cause of the attenuation characteristic of strain 19
[48]. The eryCmutant of B. suis also reduces its intracellu-
lar replication in macrophage cells [46].To ontologically represent the virulent characteristic
of EryC, we started by representing the whole process
and its participants at the molecular level and organism
level (Fig. 5):
1) The erythritol metabolism pathway is important for
Brucella intracellular replication. The intracellular
Brucella in a macrophage has the disposition of
uptaking erythritol as the carbon source, which is
realized in the relevant uptaking erythritol process.
The process of uptaking erythritol as carbon source
has Brucella erythritol catabolic process as its part.
The enzyme substrate role of erythritol is realized
in the erythritol catabolic process. The term enzyme
substrate role is defined as A role that inheres in a
protein or a compound upon which enzyme
catalyzes. It is realized in the enzymatic reaction
processes, where the molecules at the beginning of
the process, called substrates, are converted into
Fig. 5 Ontological representation of Brucella erythiritol metabolism and its involvement in Brucella pathogenesis
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 10 of 18different molecules, namely products. The final
product of erythritol catabolic process is the
dihydroxyacetone phosphate (DHAP), which is
represented using the relation begins_to_exist_during
as: DHAP begins_to_exist_during erythritol catabolic
process. The process of uptaking erythritol as carbon
source is a partial process of Brucella resistance to
nutrient deprivation, which accordingly precedes and
is required for the process of Brucella intracellular
replication. The relation preceded_by is used to
denote the relation between these two processes. Both
Brucella response to nutrient deprivation and
Brucella intracellular replication are parts of the
host-Brucella interaction process (Fig. 5). It is noted
that we did not model the detailed chemical reactions
that are components of the whole erythritol catabolic
process, because it is out of the scope of current
IDOBRU development.
2) Brucella genes eryC and eryD involve in the
erythritol metabolism pathway.
Both eryC and eryD are parts of the Brucella ery
operon. The eryC gene encodes erythrulose-1-
phosphate-dehydrogenase enzyme (EryC), and it
participates in the erythritol catabolic process. The
eryD gene encodes for an EryD protein that regulates
the ery operon expression [47]. The object property
regulates is a relation between processes, and it is
used in GO to represent the fact that a process A has
a direct influence on another process B such that it
controls some aspects of how process B unfolds [49].Therefore, the protein EryD regulates the ery operon
expression (Fig. 5).
3) B. abortus vaccine strain S19 has a 703-nucleotide
deletion which interrupts both the coding regions of
eryC (BAB2_0370) and eryD (BAB2_0369) [48]. The
deletion affects the C terminal part of the Brucella
protein EryC and the N-terminal part of the Brucella
protein EryD [48]. Therefore, S19 lacks the intact
EryD protein and EryC protein as its parts. In the
other example, the eryC mutant of B. suis has no EryC
protein as its part, and it is an agent in the process of
reduced intracellular replication in macrophage [46].
Reduced intracellular replication in macrophage is a
compromised intracellular replication process in
macrophage.
Representing host immune responses to Brucella infection
Virulent Brucella is a stealthy bacterium that hijacks many
host immune mechanisms to serve its own survival and
replication inside a host [6]. As introduced above in the
Brucella pathogenesis section, virulent Brucella is able to
replicate inside macrophages which are typically powerful
innate immune cells. Brucella can survive in replicative
phagosomes inside macrophages where nutrients are diffi-
cult to obtain. The Brucella-containing phagosome does
not fuse with bactericidal lysosomes [6]. Furthermore, to
maintain the bacterial natural living niche, virulent Bru-
cella prevents the programmed macrophage cell death.
However, live attenuated Brucella strains, including Bru-
cella cattle vaccine RB51 [50], induce apoptosis or other
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 11 of 18types of programmed cell death of infected macrophages,
which destroys the Brucella living niche and exposes the
bacteria to the most hostile extracellular environment
[25, 34]. Therefore, the programmed cell death process
benefits the host. Below we ontologically represent and
analyze the process using the example of live attenu-
ated rough B. abortus strain RB51, which is a cattle
brucellosis vaccine licensed and used in the USA and
many other countries [50].
Our previous wet-lab studies have shown that RB51
and many other rough attenuated Brucella strains in-
duce caspase-2-mediated pro-inflammatory cell death
in macrophages through the release of cytochrome c from
mitochondria [34, 35]. RB51 has an insertion within wboA
gene that leads to the deficiency of Brucella LPS O poly-
saccharide and results in its rough phenotype [51]. Figure 6
illustrates ontological representation of RB51-induced
capspase-2-mediated macrophage cell death. In this repre-
sentation, RB51 has no intact wboA gene that encodes for
an enzyme involved in the biosynthesis of Brucella abortus
O-polysaccharide [52]; therefore, RB51 is lack of Brucella
abortus O-polysaccharide. RB51 is an agent in the process
of its infecting a macrophage, which is a part of the
macrophage-RB51 interaction process. The RB51 infec-
tion of macrophage triggers (RO:precedes) three pro-
cesses: activation of caspases-2, positive regulation of
macrophage programmed cell death, and positive regula-
tion of macrophage necrotic cell death. The activation of
caspases-2 leads to (RO:precedes) positive regulation of
macrophage programmed cell death. The positive regula-
tion of macrophage cell programmed death positively reg-
ulates the apoptotic macrophage cell programmed death,
which leads to necrotic macrophage cell death. The nec-
rotic macrophage cell death is positively regulated by the
positively regulation of macrophage necrotic cell death.
From a macrophages perspective, the macrophage has
two opposite dispositions: 1) the disposition of undergo-
ing programmed cell death that is realized by attenuated
RB51 infection of macrophage; and 2) the resistance toFig. 6 Ontological representation of Brucella vaccine strain RB51-induced mprogrammed cell death that is realized by virulent Bru-
cella infection of macrophage.Ontological representation and queries of virulence
factors and associated host-Brucella interactions
Ontology classification of Brucella host-Brucella interactions
involving virulence factors
We previously defined a Brucella virulence factor as viru-
lence factor that bears Brucella virulence factor disposition
[10]. The Brucella virulence factor disposition is defined
as a disposition borne by a biological macromolecule pro-
duced by Brucella spp. that is the disposition to improve
survival of the pathogen in a host, improve transmission of
the pathogen to a host, or cause pathological processes
in a host. To further expand the definition, virulence
factors are ontologically classified in this article to be
critical to five pathogen virulence (or microbial patho-
genesis) processes:
1) colonization of a niche in the host (this includes
adhesion to cells);
2) evasion of the hosts immune response;
3) inhibition of the hosts immune response;
4) entry into and exit out of cells (if the pathogen is an
intracellular one);
5) absorption of nutrition from the host.
As shown in the modeling of T4SS and eryC described
above, a Brucella virulence factor is involved in at least
one critical process as part of the host-Brucella inter-
action, or a process precedes the critical process. Although
many Brucella molecules participate in the host-Brucella
interaction processes, not all of them contribute to the
virulence of the Brucella. One method to confirm the sta-
tus of a molecule being a virulence factor is knock-out ex-
perimental evidence, where the pathogen without this
molecule realizes a reduced or abolished virulence dis-
position during the host-Brucella interaction.acrophage cell death
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 12 of 18Ontology representations of Brucella virulence gene
mutants
The major category of virulence factors are protein viru-
lence factors that are encoded by virulence genes. Com-
pared to the original IDOBRU that includes 245 Brucella
virulence factors [10], current IDOBRU has been ex-
panded to include 432 experimentally-verified Brucella
virulence factors. All these virulence factors are experi-
mentally verified. The gene mutation followed by experi-
mental examination of the virulence of the gene mutant
inside a host (i.e., the host organism or host cell) is the
major method to detect the status of a pathogen protein
as a virulence factor.
Figure 7 shows how IDOBRU represents a virulence gene
mutant that lacks an intact protein virulence factor. Specif-
ically, a gene mutant is represented in IDOBRU as a mutant
that does not has_part a gene, which also results in the
lack of an intact protein (Fig. 7a). The original IDOBRU
used IDO ontology identifiers with the IDO_ to represent
Brucella genes and proteins. However, Brucella genes and
proteins may be used in other ontologies such as the Vac-
cine Ontology (VO) [53, 54]. The usage of IDO-specific
identifiers does not support data integration and resource
interoperability. As detailed in the Methods section and
shown in Fig. 7, in the new version of IDOBRU, we have
imported the Ontology of Genes and Genomes (OGG) IDs
(Fig. 7b) and Protein Ontology (PRO) IDs (Fig. 7c) to repre-
sent the genes and proteins of Brucella virulence factors.
OGG is a relatively new ontology that represents specific
genes in different species [19]. PRO is an ontology of pro-
tein entities [20]. To link the gene and protein entities, weFig. 7 IDOBRU representation of a Brucella virulence gene mutant. The ex
mutant has a mutation of strain 2038 sodC gene (b), which encodes a pr
relation has_gene_template is used to link the protein to the gene. Thehave adopted the PRO relation has_gene_template to rep-
resent a protein encoded by a gene (Fig. 7c), and the rela-
tion encodes to represent a gene encoding a protein.
Description rules to define virulence factors
To establish logical reasoning for a virulence factor, we de-
veloped five description rules as defined below. In the for-
mulation of these rules, o denotes an organism O, g and g
denote genetic materials, e denotes a molecular entity, p
denotes a process, i denotes a host-pathogen interaction
process, and mo denotes a mutant of O.
(IR3) IF o has_part g, ? g encodes e, THEN o
has_part e
IR3 means that if an organism has part of a gene that
encodes for a molecular entity (i.e., gene product such
as protein), then this organism has part of the molecu-
lar entity.
(IR4) IF mo has_part g, ? g derives_from g, ? (g not
has_part part of g) ? genome of o has_part g, THEN
genome of mo not has_part g
IR4 means that if a mutant of an organism has an arti-
ficially altered gene g that is derived from g, either by an
insertion or partial deletion, then the genome of the mu-
tant has no intact g as its part. When the g is fully de-
leted (i.e., g gene knock-out) from mutant mo, the above
rule will not apply. In this case, we simply assert that
genome of mo not has_part g (see below).ample shown here is B. abortus strain 2308 sodC mutant (a). This
otein called Copper/Zinc Superoxide Dismutase (SOD) (c). The
screenshots came from the IDOBRU page in Ontobee [61]
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 13 of 18(IR5) IF genome of mo not has_part g, THEN mo not
has_part g
If the genome of mutant has no intact gene g as its
part, the mutant has no g as its part.
(IR6) IF genome of mo not has_part g, ? g encodes e,
THEN mo not has_part e
IR6 means that if the mutant has no intact gene g as
its part, and the gene g encodes the molecular entity e in
the non-mutated organism, then the mutant has no in-
tact e as its part.
IR7 was given as a final inference rule for inferring a
virulence factor:
(IR7) IF (mo has disposition at some time
attenuated disposition ? attenuated disposition
realized in i) ? mo not has_part e, ? (mo
agent_in_compromised_process p ? p is_a pathogen
virulence process), THEN e is_a virulence factor.
For example, as shown in Fig. 5, (B. abortus eryC mu-
tant not has_part EryC) AND (B. abortus eryC mutant
agent_in_compromised_process Brucella intracellular
replication in macrophage) AND (Brucella intracellular
replication in macrophage is_a pathogen virulence
process) means that EryC is_a Brucella virulence factor.
According to IR6, if a gene g encoding a protein e is mu-
tated from a mutant, the mutant does not have the in-
tact protein e any more. Since eryC is mutated from the
eryC mutant, we can infer that the eryC mutant does
not have EryC.
We can therefore identify the biological process import-
ant to the pathogen virulence during the host-Brucella
interaction. Using the same Fig. 5 example, (EryC partici-
pate_in Brucella erythritol catabolic process) AND (EryC
mutant of B. abortus agent_in_compromised_process
intracellular replication in macrophage), which means
that the Brucella erythritol catabolic process is crucial to
the intracellular replication in macrophage.
With the support of the above representations defined
by IR3-7, we have annotated 269 virulence factors asso-
ciated with various macrophage-Brucella interactions.
IR7 is included in IDOBRU (Additional file 1: Figure S1).
SPARQL queries of IDOBRU for virulence factors critical for
host-Brucella interactions
In this study, several SPARQL scripts were generated to
query the information related to host-Brucella interac-
tions. The details are provided below:
First, a simple SPARQL script was generated to query the
number of protein virulence factors collected in IDOBRU
(Additional file 2). Each virulence factor protein is bearerof at some time (BFO_0000053) some Brucella virulence
factor disposition (IDO_0100116). The query identified
432 protein virulence factors.
The second SPARQL query identifies the processes in
which Brucella virulence factors participate (Fig. 8). Specif-
ically, the script queries what compromised processes Bru-
cella virulence factor mutants get involved in. The relation
agent_in_compromised_process as described earlier is
used here. In total, 11 biological processes, for example,
Brucella entry into macrophage (IDO_0100610) and
Brucella intracellular trafficking (IDO_0100983), were
identified (Fig. 8). These processes are critical to Bru-
cella pathogenesis inside host cells.
Third, those Brucella mutants that are attenuated in-
side macrophages during various macrophage-Brucella
interactions were identified using SPARQL (Additional
file 3). Each of these mutants is associated with a par-
ticular gene (represented in OGG) and a corresponding
protein (represented in PRO) (Fig. 7). Therefore, the
queries also provide us a way to extract those virulence
genes and virulence factors important for macrophage-
Brucella interactions. In total, 269 gene mutants that are
associated with 269 genes and protein virulence factors
were found. The list of all these gene mutants is also
provided in Additional file 3.
Fourth, the Brucella protein virulence factors import-
ant for Brucella intracellular replication inside macro-
phages were detected using two SPARQL queries
(Additional file 4). A query identified 81 such virulence
factors, and the other query provided the detailed list
of these factors (Additional file 4).
It is noted that the inference rules IR3-7 provide the
logic clues on the second and third sets of queries. Spe-
cifically, to search Brucella virulence factors important
for intracellular replication in macrophages, first we re-
trieved all possible gene mutants. Each protein encoded
by a gene mutated in a mutant is an agent involved in the
compromised process of macrophage-Brucella interaction
(IDO_0100832) (Additional file 3: Figure S3) or Brucella
intracellular replication in macrophage (IDO_0100612)
(Additional file 4: Figure S4). The relation agent_in_com-
promised_process as described earlier is used here. Due to
the mutation event, the intact Brucella protein (virulence
factor) does not exist in the mutant organism (IR4 and
IR6). Therefore, we are able to retrieve all the correspond-
ing virulence factor proteins using the not has_part rela-
tion (Additional file 4: Figure S4).
Discussion
Compared to the original IDOBRU paper published in
2011 [10], current article included several novel contri-
butions. First, while the original paper only includes one
section with one figure in the topic of the host-Brucella
interaction (i.e., virulence factor and pathogenesis),
Fig. 8 SPARQL query of biological processes involving Brucella virulence factors. This script queries the compromised Brucella processes in which
Brucella virulence gene mutants participate. The query was performed using the Ontobee SPARQL web-interface (http://www.ontobee.org/sparql).
The IDO_0101168 in the script is agent_in_compromised_process
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 14 of 18current article focuses on the modeling and analysis of
different aspects of the host-Brucella interactions. Specific-
ally, this article first ontologically differentiates smooth and
rough Brucella phenotypes and how each phenotype is re-
lated to Brucella virulence. Six host-Brucella interaction
subtypes are categorized, and the agents participating in
any of the subtypes (i.e., macrophage-Brucella interaction)
are defined. IDOBRU is also used to represent detailed
processes of Brucella invasion, trafficking, and replication
inside host cells. Examples described in this article include
two major Brucella pathogenesis mechanisms: the Type IV
secretion system (T4SS) and the erythritol metabolism. In
terms of host immune response against Brucella infection,
the inhibition or promotion of host programmed cell death
is specifically modeled using IDOBRU. Regarding the viru-
lence factors, this article for the first time ontologically
classifies five different types of host-pathogen interaction
processes where virulence factors may play a critical role.
Approximately 200 more virulence factors have been in-
cluded in current IDOBRU since the original IDROBRU
publication. Second, seven specific inference rules are gen-
erated and described in this paper for reasoning related to
host-Brucella interactions and Brucella virulence factors.
The layout and implementation of these inference rules
provide more power in using IDOBRU for computer-
assisted reasoning. Third, this article introduces an updated
style of representing Brucella genes and proteins. Instead
of using IDO IDs to represent genes and proteins, theupdated IDOBRU imports OGG and PRO IDs for more
authentic representations of Brucella genes and proteins.
Such gene/protein representations support ontology reuse
and interoperability. Fourth, this article provides many
SPARQL scripts that demonstrate the applications of IDO-
BRU. Furthermore, many more ontology terms have been
added to IDOBRU. Compared to the original IDOBRU
version published in 2011 that includes 1503 terms,
current version 1.2.79 includes 3488 terms. We have
more than doubled the numbers of the terms in the
IDOBRU ontology, clearly showing our progress in the
IDOBRU development.
Other literature reports exist for ontological modelling
of host-pathogen interactions. The Plant-Associated Mi-
crobe Gene Ontology (PAMGO) Consortium uses GO for
modeling host-pathogen interactions based on the investi-
gation of plant-associated symbionts [55]. Their efforts
yielded a group of GO biological process terms that cap-
ture the processes occurring between hosts and symbionts
(from mutualists to pathogens). PAMGO is focused on
representing processes. Representation of host partici-
pants (e.g., organelle like ER, and cell membrane) and
pathogen details (e.g., LPS of Brucella, Brucella proteins)
was not covered. IDO-core provides many top level terms
in the area of host-pathogen interactions, such as estab-
lishment of localization in host (IDO_0000625). As an ex-
tension ontology of IDO-core, the Malaria Ontology team
developed several terms related to malaria-host interact
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 15 of 18processes, such as inhibition of invasion and responsive-
ness to host cue [56]. Another ontology named Host
Pathogen Interaction Ontology (HPIO) found on NCBO
BioPortal is current under development, which aims to
describe the host-pathogen interactions between Salmon-
ella bacteria and swine, also as an extension of IDO-core.
However, HPIO does not represent the interactions be-
tween host and pathogens extensively. None of the above
efforts covers the participants of the interaction processes
and relations between those processes. In contrast, IDO-
BRU represents various participants in the interaction
processes between Brucella and its hosts with details at
the organism, cell, and molecular levels. IDOBRU is used
as a framework to model different proteins, complexes
and interaction processes such as how virulence factors
play a role in the Brucella-host interactions and how the
interactions happened as a series of sequential events. To
support data standardization and exchange, formal rela-
tions are applied in the IDOBRU modeling. To the best of
our knowledge, our IDOBRU modeling and representa-
tion of various areas of host-Brucella interactions repre-
sent the first comprehensive host-pathogen interaction
ontological analysis.
The host-Brucella interaction is modeled in IDOBRU as
a process with many specific interaction subclasses. The
interactions between Brucella and hosts are Brucella-spe-
cific and host (e.g., host organism or cell) - specific. For
example, the pathway of Brucella entry into macrophage
differs from the bacterial entry into epithelial cell, and the
immune response and intracellular trafficking induced by
Brucella infection differ among professional and non-
professional phagocytes [57]. Therefore, it is important to
classify different subtypes of host-Brucella interactions by
host cells. The macrophages are emphasized in this study
since macrophage is likely the most critical host cell type
in terms of host-Brucella interactions [32]. More onto-
logical representations with other host cells are needed.
Similarly, we will need to classify the subtypes based on
host organism or host organs as well, since the infection
of any Brucella strain has its host preference. These as-
pects are critical to the development of host-specific Bru-
cella vaccines.
Compared to well-studied model organisms such as E.
coli, Brucella is less studied, and the coverage of Brucella
research is often unbalanced and limited. For example,
middle products and enzymes involved in the erythritol
catabolism pathway in Brucella are not available in PRO
or ChEBI. We communicated the PRO and ChEBI groups
and submitted related middle product terms to ChEBI
and enzyme terms to PR. For example, over 1000 Brucella
strain specific proteins were submitted to PR. Although
the enzyme databases and knowledge bases are well devel-
oped and contain authentic comprehensive information,
the erythritol catabolic pathway is not included in well-known enzyme databases such as BRENDA (http://
www.brenda-enzymes.org/). GO has very few gene prod-
ucts from Brucella annotated with experimental evidence
codes (http://www.geneontology.org/GO.evidence.shtml).
The inference rules (IR3-IR7) generated in this study pro-
vide a framework on inferring virulence factors. These rules
can be used for validating the ontology and populating the
ontology. These inference rules can also be generalized to
other pathogen. The inference rules represent knowledge
and provide a format for computer-understandable auto-
mated reasoning. These rules offer explicit and transparent
assumptions for representing virulence factors in the case
of the host-Brucella interaction. Based on the inference
rules and available data, a computer will be able to assess if
a material entity is a specific virulence factor. Since the
mechanisms of virulence are different from species to spe-
cies, more inference rules may be developed with different
types of pathogens (e.g., HIV virus).
SPARQL provides a powerful method for querying and
analyzing the data in an ontology [58]. For example, our
queries identified 269 protein virulence factors related to
macrophage-Brucella interactions; and among these pro-
teins, 81 are important for intracellular replication within
macrophage from the knowledge stored in current IDO-
BRU. Note that virB1, virB5, virB8, virB9 and virB10 are
in the above list, which validates the modeling of T4SS
mechanism in this paper. In addition, we have identified
11 biological processes important for Brucella virulence
(Fig. 8). These simple but powerful SPARQL queries dem-
onstrate the applications of IDOBRU and IDOBRU-based
SPARQL technology.
One important contribution of this paper is its first re-
port in co-representing genes and proteins using the
Ontology of Genes and Genomes (OGG) [19] and the
Protein Ontology (PRO) [20]. The majority of Brucella
virulence factors are proteins, which are encoded by spe-
cific genes in different Brucella strains. The practice of
using IDOBRU IDs in our original IDOBRU version was
not ideal since it does not support ontology reuse and in-
tegration. To address this issue, the new version of IDO-
BRU uses the gene and protein IDs from the OGG and
PRO, two ontologies in the OBO ontology library. The
OGG ontology was recently developed to represent genes
from specific organisms by reusing existing resources, pri-
marily the NCBI Gene resource [59]. Due to the large
numbers of genes available in different organisms, OGG
includes different subsets, each of which represents genes
from one or a few organisms. Using the OGG develop-
ment strategy, we first generated the OGG Brucella subset
that covers all genes of three major Brucella strains. All
virulence factor genes covered in IDOBRU are from these
three Brucella strains. The availability of the OGG Brucella
subset allows us to retrieve and reuse the OGG terms to
represent Brucella genes in IDOBRU. Similarly, specific
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 16 of 18Brucella proteins were generated in PRO and reused in
IDOBRU. Furthermore, using two object properties (i.e.,
has_gene_template and encodes), we were able to repre-
sent the relations between genes and proteins. Since genes
and proteins are two fundamental entities in biology, this
study provide a demonstration on how these can be onto-
logically represented and interlinked.
Conclusions
In this paper, we ontologically represent various Brucella-
host interactions primarily using the Brucella-macrophage
interaction as a use case. A formal definition of the Bru-
cella-host interaction was given in OWL format. After the
definitions on smooth and rough Brucella are given, six
subtypes of Brucella-host interactions are classified ac-
cording to the Brucella phenotypes and host cell types.
IDOBRU is further used as a platform to represent inter-
active processes including Brucella invasion, intracellular
trafficking and intracellular replication at an organism
level. By representing the Brucella pathogenesis mecha-
nisms using Brucella T4SS and EryC examples, we dem-
onstrate how to ontologically link biological processes
from the organism level down to the molecular level. De-
scription logical inference rules have also been defined to
infer: 1) the interaction process between two species (i.e.,
a host and a pathogen); 2) the temporal relations of bio-
logical processes; 3) relations between gene, protein, gen-
ome, and gene mutant; and 4) a virulence factor. For this
study, many new terms have been added into IDOBRU.
Using SPARQL queries generated based on inference
rules, out of the 269 virulence factors related to macro-
phage-Brucella interactions, 81 virulence factors were
found to be important for Brucella intracellular replication
inside macrophage. Eleven biological processes were also
found important for Brucella virulence.
Methods
Ontology editing
The format of W3C standard Web Ontology Language
(OWL2) (http://www.w3.org/TR/owl-guide/) was applied
for IDOBRU development. The Protégé OWL ontology
editor (http://protege.stanford.edu/) (versions 4.3 and 5.0
beta) was used to edit IDOBRU.
Existing ontology term import
The ontology development uses a hybrid bottom-up and
top-down method as described in our original IDOBRU
article [10]. For this host-Brucella interaction study, many
external ontology terms from existing ontologies, includ-
ing Cell Type Ontology (CL) [14], Chemical Entities of
Biological Interest (ChEBI) [15], Gene Ontology (GO)
[16], Protein Ontology (PRO) [20], were imported to IDO-
BRU using OntoFox (http://ontofox.hegroup.org/) [31].IDOBRU access and visualization
The latest version of IDOBRU is always available at Source-
forge website: (http://svn.code.sf.net/p/idobru/code/trunk/
src/ontology/brucellosis.owl). Not that this is an unmerged
OWL file, and it imports many other OWL files in the
same folder. Therefore, it would be best to get all the re-
lated ontology files via SVN. IDOBRU has also been depos-
ited in the NCBO BioPortal (http://bioportal.bioontology.
org/ontologies/IDOBRU) and the Ontobee linked ontology
browser system (http://www.ontobee.org/browser/index.
php?o=IDOBRU). Both NCBO BioPortal and Ontobee pro-
vide interactive search and visualization features for IDO-
BRU exploration and analysis.OGG and PRO representation of Brucella virulence factors
The Brucella subset of the Ontology of Genes and Ge-
nomes (OGG) was generated using a method described in
the OGG paper [19]. Specifically, a NCBITaxon subset
was generated to include three Brucella strains using
OntoFox [31]. These strains are B. abortus strain 2308, B.
suis strain 1330, and B. melitensis strain 16 M. All anno-
tated Brucella genes in IDOBRU come from these three
strains. Most of the information of all added Brucella
genes encoding protein virulence factors was obtained
from the manually annotated Victors database (http://
www.phidias.us/victors) in the PHDIAS resource [60].
The OGG Brucella subset was submitted to the He group
RDF triple store [61]. OntoFox was then used to retrieve
the Brucella genes covered in IDOBRU.
The corresponding proteins encoded by these Brucella
genes are represented by Protein Ontology (PRO) [20].
OntoFox was used to extract the information of these
proteins from PRO. The resulting PRO subset was then
imported to IDOBRU.Queries of IDOBRU
SPARQL scripts were developed to query IDOBRU using
the IDOBRU SPARQL query web page (http://www.
phidias.us/bbp/idobru/sparql/index.php) located in the
Brucella Bioinformatics Portal (BBP; http://www.phidias.
us/bbp) [60, 62].Implementation of inference rules
The reasoner HermiT 1.3.8 (http://hermit-reasoner.com/)
as a plugin in the Protégé OWL editor (http://protege.
stanford.edu/) was used to implement the inference
rules defined in this paper. The rule view editor in the
Protégé OWL editor was used to edit the rules. The
ontology rule view in Protégé is accessible from the
Protégé menu Window? Views?Ontology views?
Rules. The saved IDOBRU OWL file contains the rules
in the format of OWL with SWRL codes.
Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 17 of 18Additional files
Additional file 1: Implementation of inference rules using the
Protégé platform. (PDF 383 kb)
Additional file 2: SPARQL query of IDOBRU for the total number of
protein virulence factors in IDOBRU. (PDF 309 kb)
Additional file 3: SPARQL query of IDOBRU for Brucella mutants
that are attenuated inside macrophages during the macrophage-
Brucella interactions. Since each mutant is associated with one gene
and one protein, these queries also allow us extract those virulence
genes and protein virulence factors that participate in various
macrophage-Brucella interactions. (PDF 353 kb)
Additional file 4: SPARQL query of IDOBRU for Brucella protein
virulence factors important for the intracellular replication of
Brucella inside macrophages. (PDF 198 kb)
Competing interests
The authors declare that they have no competing interests.
Authors contributions
YL: Primary IDOBRU ontology developer, SPARQL analysis, and use case
testing. ZX: IDOBRU developer, SPARQL analysis, and manuscript editing. YH:
IDOBRU developer, project design and management, brucellosis domain
expert, and SPARQL analysis, and use case testing. The manuscript was
primarily drafted by YL and YH, and edited and approved for publications by
all authors.
Acknowledgements
This work has been supported by grant R01AI081062 from the NIH National
Institute of Allergy and Infectious Diseases (NIAID). The article-processing
charge for this article was paid by a bridge fund to YH from the Unit for
Laboratory Animal Medicine (ULAM) in the University of Michigan. We thank
Dr. Salwa Ali and Mr. Jiangan Xie for contributing to the IDOBRU generation.
We appreciate Drs. Darren A. Natale and Cathy H. Wu for their support on
adding related Brucella proteins to PRO and their comments and editing of
the manuscript.
Received: 31 December 2012 Accepted: 23 September 2015
