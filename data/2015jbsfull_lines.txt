JOURNAL OFBIOMEDICAL SEMANTICSPapatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 DOI 10.1186/s13326-015-0013-5REVIEW Open AccessLinking gene expression to phenotypes viapathway informationIrene Papatheodorou*, Anika Oellrich and Damian SmedleyAbstractEstablishing robust links among gene expression, pathways and phenotypes is critical for understanding diseases anddeveloping treatments. In recent years there have been many efforts to develop the computational means to traversefrom genes to gene expression, model pathways and classify phenotypes. Numerous ontologies and other controlledvocabularies have been developed, as well as computational methods to combine and mine these data sets andestablish connections. Here we discuss these efforts and identify areas of future work that could lead to a betterintegration of genes, pathways and phenotypes to provide insights into the mechanisms under which genemutations affect expression and pathways and how these effects are manifested onto the phenotype.Keywords: Phenotypes, Pathways, Gene expression, OntologiesIntroductionA fundamental aspect of disease research involves theunderstanding of biological processes that underpinobserved phenotypes. In order to achieve this level ofunderstanding, diseases need to be described as collec-tions of measured phenotypes and these phenotypes needto be analysed in relation to their genetic causes andgenomic effects and linked with information on molecu-lar interactions. One consequence of these efforts couldbe the ability to produce predictive models of phenotypesfrom genomic profiles with the aim of describing diseasesmore accurately. Such models will be helpful in under-standing the genetic basis and molecular mechanismsleading to complex or rare developmental diseases and theprocess of ageing, as well as the characterisation and pro-gression of cancer types. In particular, models built frommodel organism datasets can be translated into insightson humans in areas such as disease gene identification anddrug target testing.Methods for assigning genotypes to phenotypes havebeen developed and used intensively [1-3]. Thesemethodsinclude genome wide association studies (GWAS) that areapplied to identify causative genotypes for various con-ditions and phenotypes. For example, a case-controlled*Correspondence: ip8@sanger.ac.ukEqual contributorsMouse Developmental Genetics, Wellcome Trust Sanger Institute, WellcomeTrust Genome Campus, CB1 10SA, Hinxton, UKgenome wide association study identified five loci to beassociated with the susceptibility of osteoarthritis [4].However, the identification of loci (and with that the geno-type) still leaves a gap as to what molecular mechanismsare at play to yield the observed phenotype. As a conse-quence, GWAS studies are usually followed by functionalexperiments, trying to unravel the biological mechanismsthat could influence the phenotype given the identifiedgenotype.A functional follow-up experiment to fill the gapbetween genotype and phenotype is the assessment ofexpression levels of genes in the vicinity of the identifiedGWAS loci in one or more tissues. In the study concern-ing osteoarthritis [4], the authors investigated further thegene expression and the protein expression of associatedgenes using RT-PCR (genes) and immunohistochemicalstaining. Through these functional studies, they identifiedhigh levels of nucleostemin (encoded by the GNL3 gene)in osteoarthitis patients.A potential next step in connecting the identified geno-type with the phenotype is to establish a link between theexpression of genes and the observed phenotypes, whichhas been attempted by numerous studies [5-7]. A recentexample involves the characterisation of phenotypes inyeast using high-throughput transcriptomic analyses [5].Data classification methods have been used extensivelyto characterise healthy or diseased tissue [6] from thecontext of gene expression.© 2015 Papatheodorou et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 2 of 7In the examples of the GWAS and high-throughput geneexpression studies, although the genetic and genomic out-comes of the disease can be associated with phenotypes,the biological events leading to the phenotype at the sys-tems level are not discovered. Signalling and metabolicpathway analyses can inform on the specific mechanismsof the genetic causes of the phenotypes. Recent work byHarper et al. [8] presents a method for augmenting path-way data with phenotypes from high-throughput geneticscreens in bacteria in order to discover causative genes.To date, many databases (e.g. [9-11]) and ontologies (e.g.[12-14]) have been developed to describe genes, path-ways and phenotypes across different species (see Figure 1and Additional file 1). However, the semantic integra-tion of these resources needed to computationally anal-yse the experimental set-up described above, is still atits infancy. This hinders the development of generaliseddata analysis methods that combine genes, gene expres-sion, pathways and phenotypes. Here, we identified threeareas of research that need to be further developed inorder to facilitate computational prediction of the biolog-ical mechanisms that link genotypes to phenotypes: (i)the ontological characterisation of phenotypes, (ii) linkinggene expression and phenotypes and (iii) linking pathwaysand phenotypes. We describe the current state-of-the-artfor each of the three areas in the following sections indi-vidually and highlight potential future challenges whereidentified.Ontological characterisation of phenotypesDue to the availability of phenotype data from severalmodel organisms (see Figure 1, e.g., phenotypes fromthe International Mouse Phenotyping Consortium [15]),options are not limited to human systems but may includedata from several different species. Furthermore, dataobtained through different experiments and stored indifferent data resources can vary in the detail the infor-mation is represented with [16]. As a consequence, threemajor aspects of data integration need to be addressed:(i) the integration across the different levels of complex-ity within an organism, (ii) the integration across species,and (iii) the frequencies of occurrences of phenotypes(quantification). These three aspects are further illus-trated in Figure 2. To facilitate data integration, numerousFigure 1 Databases and ontologies for information on genes, pathways and phenotypes. The diagram shows the information flow fromgenes to phenotypes via pathways. There are a large number of databases storing gene expression and other genomic data, with most of themspecies specific that include links to a phenotype ontology term. In addition, there is a large number of phenotype ontologies that are not organismspecific, such as a mammalian phenotype ontology and the cellular phenotype ontology (CMPO). There exist a few databases providing genotypeto phenotype links, although most of this information is covered by species specific genomic databases. There are many small-scale species specificor pathway-type specific databases and a few large general pathway databases (KEGG, Pathway Commons, REACTOME). Pathway ontologies existbut are not widely used yet. Although in general there are good links among genes and pathways and genes and phenotypes, associationsbetween pathways and phenotypes are lacking.Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 3 of 7Figure 2 Three challenges when representing and comparing phenotypes. The diagram illustrates the three challenges that need to beovercome in order to link gene expression and phenotypes using pathways: (A) the integration across the different levels of complexity within anorganism, (B) the integration across species, and (C) the frequencies of occurrences of phenotypes (quantification)  purple colour representsindividuals possessing phenotype of interest (examples in parentheses taken from Angelman syndrome in OrphaNet).ontologies have been developed that define the meaningof biological concepts, such as the Gene Ontology (GO)[13].Integration across different levels of organismalcomplexityPhenotypes span different levels of complexity and rangefrom a molecular level to the entire organism, such asthe cellular level, the tissue level or the organ level (seeFigure 2(A)) [17]. Existing biomedical ontologies coverseveral levels of complexity, e.g., ontologies that repre-sent gene function (GO) as well as tissue information(e.g. BRaunschweig ENzyme DAtabase (BRENDA) tissueontology (BTO) [14]) or organism level (e.g. the Mam-malian Phenotype Ontology (MP) [12] or the HumanPhenotype Ontology (HPO) [18]). In order to facilitatereasoning over the different levels of complexity neededto describe an individual with ontologies, the ontolo-gies have to be aligned and mapped to one another.While mapping efforts are ongoing to align ontolo-gies across species covering the same level of com-plexity, e.g., the alignment of anatomy as provided byUBERON [19], the seamless integration of ontologiesacross the different levels of complexity is still ongoingwork.Integration across speciesIn order to computationally compare phenotypes acrossdifferent species, the existing phenotype data needs tobe semantically annotated in a way that would facilitatethe comparison. Traditionally, model organisms as wellas human data were semantically represented using pre-composed phenotype ontologies. In a pre-composed phe-notype ontology such as MP or HPO, one concept cor-responds to one phenotype and can directly be usedfor annotation. However, a comparison is only possibleas long as the same pre-composed ontology is used forannotation.To overcome this limitation of pre-composed pheno-type ontologies, post-composed phenotype representa-tions have been suggested. One approach that is broadlyused, for example to post-compose MP and HPO andrepresent zebrafish mutants in the Zebrafish ModelOrganism database [20], is the description of phenotypesusing Entity-Quality (EQ) statements. Entity-Quality (EQ)statements enable the composition of phenotypes usingspecies-independent ontologies [21], e.g. GO (for the rep-resentation of processes) or UBERON (a cross-speciesanatomy ontology). While some of the statements havebeen generated and verified automatically [22], manualverification is still needed to ensure the correct repre-sentation. How species can be compared based on pre-and post-composed phenotype annotations is illustratedin Figure 2(B).The applicability of the generated EQ statements isdemonstrated by their usage in a variety of projects,which e.g., predict the involvement of genes in diseasesand pathological processes [2,3] and gene function [23].Papatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 4 of 7Despite the successful applications of pre- and post-composed phenotype annotations, the harmonised appli-cation of phenotypes in conjunction with gene, expressionand pathway data is still very limited.Frequencies of occurring phenotypesThe quantification of phenotype data is beginning tobecome available: databases such as OrphaNet [24]describe disease phenotypes with additional quantifiers,e.g., the phenotype dwarfism (OrphaNet clinical signid: 53350) is very frequent in patients with a 12q14micro deletion syndrome (OrphaNet disorder id: 12544)or the phenotype strabismus (OrphaNet clinical sign id:5870) is occurring occasionally in patients with Angel-man syndrome (OrphaNet disorder id: 90). OrphaNetassigns phenotype annotations and frequency informa-tion represented with an OrphaNet-specific vocabulary(see Figure 2(C)).A similar strategy has been applied to annotate humangenetic disorders described in the Online MendelianInheritance inMan (OMIM) database [9]. Each disorder isdescribed using concepts of the HPO and, optionally, fre-quency information can be added to each of the assignedphenotype annotation [25]. Despite great efforts, fre-quency information is not available for all the annotationsassigned and only available via the download file.While clinical databases already work on the inclu-sion of quantified phenotype data, model organismdatabases lag behind by not providing this information.Thus, quantified phenotype information cannot yet beused for cross-species data analysis and computationalmodelling.Linking gene expression to phenotypesThe ease of obtaining whole genome expression datasetshas enabled more thorough classification of phenotypesassociated with the expression of sets of genes [6]. A largenumber of studies attempt to identify groups of geneswhose expression is responsible for a particular pheno-type, such as disease or tissue morphology [26]. Morecomplex experimental designs attempt to associate thephenotype with dynamic or systems views of gene expres-sion [27]. The techniques used to link the phenotypeto causative patterns of gene expression largely dependon the experimental design and the technology used toprofile gene expression.Gene expression signaturesIn order to characterise a phenotype in terms of geneexpression, most studies attempt to identify the minimumnumber of genes whose expression patterns determine thephenotype in question. This group of genes is referred toas a gene signature in the literature and once defined andvalidated has important practical applications to diseasediagnosis and prognosis, as well as the discovery of newtherapies. In cancer genomics, for example, classificationsof tumour types from high-throughput gene expressionand/or copy number profiles have helped unravel thecomplexity of different cancer types and have led to a bet-ter understanding of cancer progression and the identifi-cation of new diagnostic biomarkers. For example, Marisaet al. [6] produced a transcriptome-based classification of566 colon cancer samples to discover six different molec-ular subtypes of the disease, that associated with distinctclinicopathological characteristics and corresponded todifferent relapse times. Aravinthan et al. [28] defined asignature of 40 genes that appear upregulated in hepato-cyte senescence as opposed to controls and then validatedthis by finding enrichment of these genes in public datasets representing liver conditions such as steatohepatitis,alcohol-related hepatitis and HCV-related cirrhosis [28].Given enough data sets, existing data mining methodscan assign patterns of gene expression to the phenotypesunder study. Although the linkage of gene expression sig-natures to phenotype associations is an important stepin determining the causal link between genotypes andphenotypes, it is still difficult to establish the underly-ing biological mechanism from gene expression data setsalone.Complex experimental designsMore complex experimental designs are used in order torefine the mechanisms under which gene expression canlead to a certain phenotype. Here, the experimental designattempts to address issues such as the influence of envi-ronmental factors, time and interplay between tissues. Anindividual study example comes from the work by Äijöet al. [27] where statistical modelling based on Gaussianprocesses is used to analyse the differentiation of humanTh17 cells. The authors expose CD4+ T cells to two dif-ferent types of ligands and record the gene expressionusing RNA-seq over five different time-points. The anal-ysis can describe the dynamics and provide insight intothe kinetics of gene expression that lead to the differentoutcomes of T cell activation depending on the ligandsused.Tissue-specific and temporal based gene expressionwith matching phenotype measurements could be identi-fied by appropriate experimental controls. However, theseare often absent or impractical to implement in large-scale phenotyping assays or in cases ofmeta-analyses fromalready available data sets [7]. Generally, formore complexexperimental designs to be more case-specific, custom-made computational solutions are usually required inorder to analyse the gene expression data according tothe different variables. Efforts are being made to gener-alise these tools and resources so that analysis of complexdesigns can be made easier. Examples include softwarePapatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 5 of 7for the analysis of time-series data sets. The DyNB toolssuite in [27] and NextmaSigPro [29] are examples ofsoftware tools that enable analysis of time-series datasets.Efforts have also been made to tackle the complexitiesof tissue specific gene expression in whole organism geneexpression data sets by developing resources such as tis-sue specific gene expression atlases [30-32]. These datasets can be used as benchmarks to explore experimentson whole organisms. Small organisms such as DrosophilaMelanogaster are difficult to dissect on a large scale andsometimes tissue specific expression must be inferredfrom whole body profiles, rather than directly measured.Innocenti et al. [33] extracted tissue specific genes fromwhole fly gene expression by use of FlyAtlas [30,34]. FlyAt-las is a database that holds information on genes expressedin 25 adult fly tissues originally obtained by tissue specificmicroarray profiling in wild type flies.Gene expression analyses are very useful in identify-ing groups of genes that could characterise a pheno-type. Although they do not provide much detail on thespecific mechanisms under which the original stress ormutation leads to the observed phenotype, they can beused as a starting point for subsequent analyses thatcan narrow down candidate pathways and processes andgenerate hypotheses for more detailed experiments thatcan eventually shed light on the exact causes of thephenotype.Linking pathways to phenotypesDeriving the underlying mechanism of the phenotype,given the initial mutations and/or resulting gene expres-sion, involves the integration of knowledge on pro-tein interactions and pathways [35]. There are differenttypes of pathway analyses frequently used dependingon the nature of the pathways: protein-protein inter-actions; gene-regulatory pathways; quantitative reactionmodelling that includesmetabolic, pharmacokinetic mod-elling. Methods for analysing these types of pathways havebeen previously reviewed in [35,36]. Linking these typesof analyses with gene expression depends on whetherthere are already candidate pathways of interest and whatis their degree of annotation. It also depends on thehypotheses of the studies investigated and whether theyinvolve a small and specific pathway where knowledge ofquantitative reactions matter and are available or whetherthey involve a large integrational study where broadnessof pathway connections are important, usually at a cost ofusing detailed quantitative information on the kinetics ofthe interactions involved.PathwaymodelsData for quantitative pathway analyses usually come fromdirect protein level measurements, therefore enabling theuse of computational simulations for the formulation ofpredictive hypotheses that can subsequently be testedexperimentally. Such approaches have the potential toproduce predictive mathematical models describing theunderlying mechanisms at high-levels of detail [37].Panetta et al. [38] study the variations of methotrexateaccumulation in cells of acute lymphoblastic leukemiapatients using pharmacokinetic and pharmacodynamicmodels. By employing these methods they characterisehow perturbations in the folate pathway, target ofmethotrexate, vary across the tumour subtypes (pheno-types) and how they relate to genetic variation and geneexpression.Quantitative pathway modelling methods are not easyto implement on a large scale and are mostly usefulwhen there is already substantial knowledge of the bio-logical process involved. In cases where the underlyingbiological process is unknown or poorly defined, high-throughput protein interaction data or high-level pathwayinformation from pathway databases can help disentan-gle the mechanisms that are responsible for or inducedby the observed gene expression. Boolean logic and otherlogic-based approaches, such as [39,40], have been usedsuccessfully for qualitative pathway analyses, to gener-ate hypotheses that link gene expression, pathways andphenotypes.Knowledge integrationPathway databases such as REACTOME [10] or KEGG[41] contain a wide range of developmental, signalling,metabolic, as well as disease pathways. These are well-linked to other resources, such as Ensembl [11] andUniprot [42], for better integration with gene and proteininformation. Currently they support pathway enrichmentanalyses for a set of interesting genes or proteins andprovide tools for visualising the pathways in the contextof these interesting molecules. In addition, REACTOMEprovides ontological links between pathways, thereforeallowing the exploration of interactions and relationshipsacross different pathways. Often these pathway resourcesdo not contain exactly the same pathways and in orderto enable more comprehensive analyses, their data setsneed to be merged. Resources such as BioSystems [43]attempt to collect and disseminate all available pathwaysfrom the available databases. However, due to lack of awidely used controlled vocabulary describing the avail-able pathways, such attempts fail to fully semanticallyintegrate data from different pathway databases. Therehas been significant progress in developing ontologiesand standard formats for descriptions of pathway compo-nents and reactions (SBO, SBML, [44]). However, thesehave mainly been focused on describing the mathematicalinteractions within pathways in order to enable simula-tions. Therefore, they have not been widely adopted byPapatheodorou et al. Journal of Biomedical Semantics  (2015) 6:17 Page 6 of 7all pathway databases in order to enable more effectiveintegration.Further work also needs to focus on linking the differ-ent levels of information, protein levels, gene expressionandmetabolic and signalling pathways into computationalmodels that can handle qualitative and quantitative path-way parameters. Integrating different kinds of data setsfrom different species to solve a single, common biologi-cal process is an invaluable step in pathway analyses, butremains a difficult task. Advances in text-mining meth-ods, as well as more accurate orthologous relationshipsbetween the genes of different species will help overcomethese problems.A major remaining problem in the linkage of genes andtheir expression signatures to pathways to phenotypes isthe limited knowledge of the mapping between pathwaysand phenotypes. This is a difficult task mainly due to thelack of appropriate data sets that would enable the infer-ence of such connections on the large scale. However,high-throughput phenotyping projects such as the Inter-national Mouse Phenotyping Consortium [15] have thepotential to provide sufficient data sets for the inferenceof such links.Finally, recent efforts on multi-scale models of organsattempt to bridge the gap between molecular pathwaysand physiology through projects such as the Virtual Phys-iological Human [45] and the Virtual Liver, a collaborativeeffort to produce a physiological model of the liver thatinteracts with pathways and other molecular componentin order to support simulations and the understanding ofthe liver function in health and disease [46]. Such effortsare still in their initial steps but have the potential to facil-itate a better understanding of the relationship betweengenes and phenotypes.ConclusionsHigh-resolution gene expression data sets are provid-ing more insight into the functional consequences of thegenotype as well as clues into the mechanisms that mightcontrol the phenotype. At the same time, research utilisingpathway analysis and data integration has been increas-ingly important in explaining the biological mechanismsunder which genotypes (and gene expression) influencephenotypes. Some form of pathway analysis is routinelypart of gene expression studies. However, this is hinderedby the lack of detailed pathway maps and quantitativeinformation on the reactions. From the perspective ofphenotype characterisation, the development of differenttypes of ontologies and links between them is increas-ingly improving the integration of gene, tissue, anatomicaland disease data sets within and between species. Theseimprovements are creating the basis for more detailedassociations between genes, pathways and phenotypes inthe future.Additional fileAdditional file 1: Table detailing all databases and ontologies thatappear in the text.Competing interestsThe authors declare that they have no competing interests.Authors contributionsAll authors read and approved the final manuscript.AcknowledgementsThis work was supported by the Wellcome Trust grant [098051] and theNational Institutes of Health (NIH) grant [1 U54 HG006370-01].Received: 29 October 2014 Accepted: 19 March 2015JOURNAL OFBIOMEDICAL SEMANTICSChristen et al. Journal of Biomedical Semantics  (2015) 6:26 DOI 10.1186/s13326-015-0020-6RESEARCH ARTICLE Open AccessRegion Evolution eXplorer  A tool fordiscovering evolution trends in ontologyregionsVictor Christen1*, Michael Hartung1,2 and Anika Groß1,2AbstractBackground: A large number of life science ontologies has been developed to support different applicationscenarios such as gene annotation or functional analysis. The continuous accumulation of new insights andknowledge affects specific portions in ontologies and thus leads to their adaptation. Therefore, it is valuable to studywhich ontology parts have been extensively modified or remained unchanged. Users can monitor the evolution of anontology to improve its further development or apply the knowledge in their applications.Results: Here we present REX (Region Evolution eXplorer) a web-based system for exploring the evolution ofontology parts (regions). REX provides an analysis platform for currently about 1,000 versions of 16 well-known lifescience ontologies. Interactive workflows allow an explorative analysis of changing ontology regions and can be usedto study evolution trends for long-term periods.Conclusion: REX is a web application providing an interactive and user-friendly interface to identify (un)stableregions in large life science ontologies. It is available at http://www.izbi.de/rex.Keywords: Ontology evolution, Ontology visualization, OntologiesBackgroundIn recent years ontologies have become increasinglyimportant for annotating, sharing and analyzing datain the life sciences [1,2]. For instance, functional termenrichment analysis [3] use ontologies to propagate infor-mation along their structure to find over-representedterms w.r.t. a list of interesting genes. The heavy usageof ontologies leads to a steady modification of their con-tent [4,5]. In particular, ontologies are adapted to incor-porate new knowledge, eliminate initial design errors orachieve changed requirements. Tools like Protégé [6] sup-port the development and change of ontologies. Thisprocess is usually distributed since especially large ontolo-gies can not be maintained by single developers, such thatcollaborative work is performed [6,7]. Typically, the over-all development of an ontology is coordinated by a projectleader or consortium, and multiple developers contribute*Correspondence: christen@informatik.uni-leipzig.de1Department of Computer Science, Universität Leipzig, Augustusplatz 10,Leipzig, GermanyFull list of author information is available at the end of the articleknowledge in their field of expertise. Ontology providersrelease new versions on a regular basis or whenever a sig-nificant amount of changes were performed. Users shouldthus always consider the newest ontology version in theirapplications to avoid errors from previous versions and tobe up-to-date w.r.t. the modeled knowledge.Due to the ontologys size and complexity, the prob-lem arises that coordinators, developers and users want toknow whether specific parts (regions) of a large ontologyhave changed or not. We see different use cases where atool support is required: Region Evolution Analysis: Users may questionwhich regions have evolved in what way in a specificperiod of time. For instance, there can be regionsexhibiting a high degree of instability. These regionsmay have been in the focus of development andunderlay many modifications. This might be causedby the topics modeled within these regions, e.g.,current topics require permanent modifications to beup-to-date. By contrast, a stable region might bealready completed or was of low interest during© 2015 Christen et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 2 of 12recent ontology development. Furthermore,interesting insights come up when studying theevolution of a region over time, e.g., by consideringthe change intensity in the past five years. Anotheruse case would be the comparison of the evolution indifferent regions, e.g., a head-to-head comparison oftwo regions can provide information whether theseregions have evolved in a similar way or show adifferent evolution behavior. Ontology Development and ProjectCoordination: In ontology development projectscoordinators usually face the problem how to trackand measure the ongoing development in anontology. This especially holds for large anddistributed projects when the ontology to bedeveloped covers a number of different topics. Insuch cases project coordinators are interested in theevolution of different ontology parts. In particular,they like to see (1) how work has progressed and (2)like to detect potential for future development.Having a tool that can flexibly compute where, whenand how many changes occurred, an improvedproject controlling and decision management can beachieved. For instance, if work in an area did notprogress as planned, resources can be re-scheduledaccordingly in order to complete the work.The controlling is not limited to project coordinators.Also, developers can inform themselves about theevolution in different regions and may findinteresting starting points to participate, e.g., regionswith topics they are aware of. Dependent Data and Algorithms: Biomedicaldatasets like genes, images or electronic healthrecords are typically annotated with concepts ofontologies. Thus, they depend on the ontologycontent and exhibit another use case for REX. Forinstance, if a user considers the anatomy part of theNCI Thesaurus (NCIT) [8] for annotating local datasuch as radiology pictures, she would like to knowhow this part has evolved recently, i.e., is the partunstable or stable. Thus, one can estimate whether ornot an adaptation of the annotations would befeasible. Moreover, ontology-based algorithms orapplications might be affected by ontology changes.For instance, if results of a gene set enrichmentanalysis [3] are located in a strongly evolvingontology part, it should be re-done based on thenewest ontology version to see how results change.By contrast, results located within stable ontologyparts are likely to remain unchanged. In own previouswork [9] we already used such techniques to figureout how the results of real gene set enrichmentanalyses changed over time and how these changesare related to ontology modifications.A number of existing web applications provide queryfunctionalities for specific ontologies like the popularGene Ontology (GO) (e.g., [10,11]). Furthermore, life sci-ence ontologies can be accessed through platforms likeBioPortal [12] or OBO Foundry [13]. Although it is pos-sible to retrieve different versions of an ontology, suchplatforms rarely provide information about evolution, i.e.,users have the problem to figure out how an ontology hasevolved compared to their version in use. Recently, someweb tools offer access to information about the evolutionof the GeneOntology (GO). GOChase [14] allows to studythe history of individual GO concepts and Park et al. [15]propose graph-based visualization methods to view mod-ified GO terms. In own previous work we designed theOnEX web application [16] for versioning as well as quan-titative and concept-based evolution analysis of life sci-ence ontologies. Our tool CODEX [17] can be used todetermine a diff between two ontology versions coveringcomplex changes (e.g., concept merge or split). For a gen-eral overview on ontology and schema evolution includingdiff computation we refer to [4]. In summary, currentlyavailable tools lack the functionality to analyze and com-pare evolution in different ontology parts especially forlarge ontologies with several version releases.We therefore present the novel web application REX(Region Evolution eXplorer). REX can be used (1) todetermine differently changing regions for periodicallyupdated ontologies, and (2) to interactively explore thechange intensity of those regions. REX provides a com-parative trend analysis such that users and developerscan monitor the long-term evolution for their regions ofinterest, e.g., to track the work or coordinate future devel-opment. To show the applicability of REX, we evaluate thetool by analyzing evolution trends in four representativelife science ontologies. REX is online available at http://www.izbi.de/rex and provides a web service interface forprogrammatic access at http://dbs.uni-leipzig.de/wsrex.This paper is an extended version of [18] presented atDILS 2014. For this version REX has been improved andprovides additional features such as the specification ofindividual cost models and a web service interface forprogrammatic access. We further describe possible usecases for REX and outline opportunities for future workin more detail. New region evolution analyses have beenperformed on four representative life science ontologies.The base region discovery algorithm used by REX hasbeen published in [19]. This algorithm allows to detect(un)stable ontology regions for an arbitrary number ofontology versions. However, in this form the algorithm isonly applicable offline, i.e., the research community cannot make use of it. With the help of REX the algorithmis applicable in two ways: (1) by interactively analyz-ing region evolution via the web application and (2) byremotely accessing the web service interface. REX fitsChristen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 3 of 12into our tool suite for ontology evolution managementas follows. REX is build upon the OnEX repository [16]offering versioning capabilities for life science ontologies,i.e., ontologies and their versions available in OnEX canbe analyzed with REX as well. If someone is interestedin detailed changes between two particular ontology ver-sions we refer to the CODEX tool [17] which providesontology version comparison (diff ) facilities.MethodsThe region discovery method proposed in [19] enables thedetection of changing and stable ontology regions. Thebasic idea is to compute change intensities for regionsbased on changes between several succeeding versions ofan ontology within a specific time interval. First, we brieflydescribe the applied cost model and region measures. Wethen describe the region discovery method as well as analgorithm to identify trends in the evolution of ontolo-gies. We present the infrastructure of REX and describeits different workflows and features.Region discovery methodsChange costsAn ontology consists of a set of concepts which are inter-related by different relationships like is-a and part-of.Each ontology concept has an unambiguous identifier andis further defined by a set of attributes like its name,synonyms or definition. Discovering changing or stableontology parts requires the definition of a cost model tomeasure the influence of changes on ontology concepts.In general, ontology content can be added (addition),removed (deletion) or modified (update). Here we distin-guish between seven basic change operations for ontol-ogy concepts, their attributes and relationships betweenconcepts listed in Table 1. These basic change opera-tions cover all modifications that typically occur in anontology and are suitable to detect changing ontologyregions. More complex change operations (e.g., conceptmoves) are composed of these basic operations and can bederived by aggregating basic changes to a more compactrepresentation [20]. For instance, a move of a conceptwithin the ontology hierarchy is composed of an addition(addR) and a remove (delR) of a relationship. Further-more, typical changes like name and property changes arecovered by the change operation chgAttValue. Relation-ship changes with is-a or other semantics (e.g., part-of ) arerepresented by addR/delR. Our cost model now assignschange costs to each basic change operation, i.e., we canrepresent the impact of change operations by differentcosts (see change costs used in REX in Table 1). Forinstance, we can assign higher costs to deletions sincethey might have a higher impact on dependent applica-tions than additions. Note, that users can adapt the costmodel according to their application scenario. If a useris especially interested in regions that have been heavilyextended, she should rank additions higher than dele-tions. To reflect the impact of changes on concepts, weintroduce two types of concept costs: (1) local costs lc(c)cover the impact of change operations that directly influ-ence a concept c, e.g., the change of an attribute valueor the addition/deletion of a child concept have a directimpact, and (2) aggregated costs ac(c) are used to reflectall changes occurring in the is-a descendants of a conceptc, e.g., leaf additions/deletions indirectly influence ances-tor concepts. We will later describe how we assign localand aggregated costs to concepts.Regions andmeasuresAn ontology region OR consists of an ontology concept(region root rc) and its is-a subgraph, i.e., it covers all leafand inner concept changes within this region. The defi-nition of our regions covers the experience that changesoften occur in the boundary of an ontology, e.g., additionof leaves or subgraphs to extend the knowledge of a spe-cific topic. Of course our regions also cover changes oninner concepts since all intermediate concepts betweenthe root and the leaves are part of the region. As an exam-ple Figure 1 (left) illustrates part of an anatomy ontology.We can consider the regions lung and tonsil each con-sisting of three concepts. Note that the complete ontologycan also be regarded as a region defined by the ontologyroot organ.Table 1 Change operations and change cost modelChange operation Description Change costsAttributesaddC Addition of a new concept 1delC Deletion of a concept 2RelationshipsaddR Addition of a new relationship 0.5/0.5delR Deletion of a relationship 1.0/1.0ConceptsaddA Addition of a new attribute 0.5delA Deletion of an attribute 0.5chgAttValue Modification/change of an attribute value 0.5The table shows which change operations and corresponding change costs we utilize in REX. For relationships we split the costs and assign them to the source andtarget concept, respectively.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 4 of 12Figure 1 Example part of an anatomy ontology. The figure shows a small yet comprehensive example anatomy ontology to illustrate regions aswell as local (lc(c)) and aggregated (ac(c)) costs (left). For instance, the region lung consists of three concepts and has aggregated costs of four.The table on the right shows the corresponding results when applying the region measures (abs_size, abs_costs, avg_costs) in this example.So far, REX provides a set of measures to describe thechange intensity of ontology regions. For each OR onecan determine its absolute size (abs_size(OR)) w.r.t. thenumber of concepts. Absolute change costs of an OR(abs_costs(OR)) are represented by the aggregated costs ofits root ac(rc). The average change costs per concept inOR can be computed as the fraction of absolute changecosts and the region size: avg_costs(OR) = abs_costs(OR)abs_size(OR) .Applying these measures to our example results in the val-ues displayed in Figure 1 (right). The lung region changedmore intensively (avg_costs(?lung?) ? 1.33) compared totonsil (avg_costs(?tonsil?) ? 0.67). The overall changeintensity of the ontology is 67 ? 0.86.Our general aim is to determine (un)stable ontologyregions w.r.t. a specific time interval (tstart , tend), i.e.,changes between ontology versions released in this inter-val need to be considered. For this purpose we show firsthow we can determine local (lc) and aggregated costs (ac)for two versionsOold andOnew. Later we will describe howwe can generalize the two-version approach for an arbi-trary number of versions. For further details about bothalgorithms we refer to [19]. We will highlight the mainsteps since the REX application is the main contributionof this article.Region discovery for two versionsThe general procedure for two versions is depicted in thefollowing algorithm (computeAggregatedCosts):Algorithm 1: computeAggregatedCostsInput: ontology versions Oold and Onew, change costs ?Output: ontology version Onew with assigned aggregatedcosts1 Oold ? Onew ? diff(Oold ,Onew);2 assignLocalCosts(Oold ? Onew, ? ,Oold ,Onew);3 Oold ? aggregateCosts(Oold);4 Onew ? aggregateCosts(Onew);5 transferCosts(Oold ,Onew);6 return Onew;The algorithm accepts two versions Oold, Onew and acost model ? . Its four main steps are: (1) diff computation,(2) local cost assignment, (3) cost propagation and (4)cost transfer. We first need to determine the differencebetween both input versions (line 1). For this purpose wecan use existing Diff algorithms such as PromptDiff [21]or COntoDiff [20]. The result is the diffOold?Onew con-sisting of a set of change operations that occurred betweenOold and Onew.Using the diff and the change costs ? we next assignlocal costs to concepts which are involved in changes (line2). Depending on the type of change we assign local coststo concepts in the old or new version. Additions are reg-istered in the new version while deletions are covered inthe old version. The assignment further depends on thekind of ontology element that has been changed. Costsfrom changes on a concept or its attributes are assignedto the concept itself while costs for relationships are splitand assigned to the source and target concept of therelationship, respectively.We now use the two ontology versions annotated withlocal costs to derive the aggregated costs per concept (line3-4). In particular, we propagate local costs along is_apaths upwards to the root(s). Due to multi-inheritance wemay need to split costs during propagation. The aggre-gated costs ac(c) of a concept c can be determined asfollows:ac(c) =?c??children(c)ac(c?)|parents(c?)| + lc(c)The aggregated costs ac(c?) of each child c? are dividedby the number of parents the child has (|parents(c?)|).These costs are summed up for each child of the consid-ered concept c and added to its local costs lc(c) to finallyget its aggregated costs ac(c). We thus distribute costs inthe case of multiple inheritance and finally ensure thatthe root concept(s) of the ontology contain the overallsum of all assigned local costs. In our example in Figure 1(left) the aggregated costs of organ (ac(?organ?) = 6) arecomputed based on the aggregated costs of its childrenac(?lung?) = 4 and ac(?tonsil?) = 2 as well as its own localcosts lc(?organ?) = 0.In order to determine (un)stable regions in the new ver-sion, we need to transfer costs from Oold into Onew (lineChristen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 5 of 125). We therefore sum up aggregated costs which belong tothe same concept in the old/new version. After this stepwe can apply our region measures as defined earlier or usethe new ontology version with aggregated costs for furtherprocessing (see Multiple Version algorithm).Region discovery formultiple versionsWe generalize our basic algorithm for multiple releasedversions O1, . . . , On by executing it n ? 1 times so thatwe successively determine aggregated costs (for each ver-sion change Oi?1 ? Oi) and transfer them to the newestversion On. In On we can apply the previously describedregion measures. The overall algorithm findRegionslooks as follows:Algorithm 2: findRegionsInput: ontology versions O1, . . . ,On, change costs ?Output: newest version On with determined changeintensities (e.g., abs_costs, avg_costs)1 forall the succeeding versions Oi ? Oi+1 do2 Oi+1 ? computeAggregatedCosts(Oi,Oi+1, ?);3 computeRegionMeasures(On);4 return On;Trend discovery for regionsUsing the region discovery method (findRegions) onecan determine the most (un)stable regions for a spe-cific time interval. To better monitor region changes overlong periods of time and to figure out trends in theirevolution, we propose a further method for trend dis-covery based on sliding windows. The overall proceduretrendDiscovery looks as follows: Using the regiondiscovery method (findRegions) one can determinethe most (un)stable regions for a specific time interval. Tobetter monitor region changes over long periods of timeand to figure out trends in their evolution, we propose afurther method for trend discovery based on sliding win-dows. The overall procedure trendDiscovery looks asfollows:Algorithm 3: trendDiscoveryInput: time interval (tstart , tend), ontology O, ontology regionof interest OR ? O, change costs ? , window size ?, stepwidth Output: time-based stability valuesmeasuredCosts1 t ? tstart ; measuredCosts ? ?;2 while t + ? < tend do3 versions ? getReleasedVersions(O, (t ? ?, t));4 latestVersion ? discoverRegions(versions, ?);5 regionCosts ? getStabilityValuesForRegion6 (OR, latestVersion);7 measuredCosts.put((t, regionCosts));8 t ? t + ;9 returnmeasuredCosts;The algorithm works on an ontology O, a time interval(tstart , tend) and an ontology region of interest OR to bemonitored. We further use a sliding window of size ?, astep width  and change costs ? . In particular, we suc-cessively shift the window beginning at tstart ? ? over thetime interval until we reach its end tend . In each step wefirst determine the released ontology versions within thewindow (line 3). We then calculate and save the costs (e.g.,avg_costs) for OR by calling the region discovery algo-rithm (discoverRegions) for the versions within ?.We thus generate a time-based map (line 6) containinginformation about the change intensity of OR at specificpoints in time in the defined window. The results arevisualized for users in the Trend Analysis component ofREX.Web applicationArchitectural overviewREX is based on a three-layered architecture displayedin Figure 2. The back-end consists of the OnEX repos-itory [16] which currently provides access to more than1,000 versions of 16 popular life science ontologies. Notethat it supports the import of ontologies in different for-mats such asOWL andOBO. Users can analyze integratedversions with the offered facilities of REX. The serverlayer is implemented in Java and realizes different ser-vices to access ontology versions in OnEX. Moreover, itprovides services to calculate the region measures and toperform trend and quantitative analyses. Every service isencapsulated in its own module, such that it is possible tochange the region discovery algorithm independently ofthe other modules. Results are transformed such that theapplication can visualize ontologies and changing regionsin graphs. Moreover, we provide a web service for pro-grammatic access. So far, it computes the average costsper concept for a particular ontology and time interval.Ontology developers are thus able to integrate REX func-tionalities into their own applications. For instance, a setof annotations could be automatically rejected, if the aver-age costs of involved concepts exceed a given threshold.The front-end is a platform-independent web applicationbased on the Google Web Toolkit (GWT)[22] and thegraph library InfoVis[23]. In the following we discuss theanalysis facilities of REX, namely the Structural Analysis,Quantitative Change Analysis and Trend Analysis, as wellas the web service interface, in more detail.Structural analysisThe structural analysis component represents the evolu-tion of regions in an ontology for a specified time intervalas a graph (Figure 3). The component is mainly dividedinto a Browser View as well as a table to search and fil-ter results (Table View). First the user needs to specify theontology name and the time period to review in the InputChristen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 6 of 12Figure 2 Three-layered architecture of REX. The figure shows the architecture of REX consisting of three layers: (1) knowledge base layer, (2) serverlayer, (3) presentation layer.form. Moreover, users can adapt the applied change costmodel according to their analysis szenario (Change CostModel). The system then performs the region discoveryalgorithms and generates a graph to visualize the results(Browser View). Each node in the graph represents anontology concept, is-a relationships are displayed as edgesbetween the nodes. The layout is circular and displays aconcept and its near neighborhood, i.e., its descendantsand parent nodes (either with or without labels). Users caneasily identify interesting sub regions by selecting a con-cept in the graph (Browser View) or in the Table View.This concept is then shown as the central node in theBrowser View. It is possible to navigate in both directionsthrough the ontology. For instance, if one is interested ina specific sub region and its content, one clicks on thenode and the graph will display the sub region in moredetail. In contrast, one can also navigate to a more gen-eral concept (surrounded by blue circles) to see siblingregions of the current one. The colors signal the measuredchange intensity (avg_costs) of a region. Red stays for highchange intensity whereby green is used to mark stableregions. Thus, users can easily figure out where (un)stableregions are located. We provide two coloring schemes: (1)interval-based grouping or (2) equal distribution betweenFigure 3 Structural Analysis component. The figure shows the structural analysis component of REX.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 7 of 12min/max avg_costs. For each concept in the graph, smallinfo boxes (mouse over) provide further information likethe accession number, concept name/label or the mea-sured avg_costs.In general the number of concepts and relationshipsin an ontology is very high. Thus, it is difficult to rec-ognize interesting regions only by browsing through thegraph especially for large ontologies. Moreover, users maybe interested in the change intensity of specific regions.The Table View therefore allows users to filter and sortontology regions by their accession number, name andavg_costs. In particular, search criteria can be specifiedin the head of the table to find regions of interest. Forinstance, one can filter out all regions in the Adult MouseAnatomy Ontology containing the name heart. Users cansimply select their region of interest in the table and moveto the Browser View for its visualization. To get a moredetailed view of occurred changes, users can request thelocal Change History of a selected concept at the bottomof the table.Quantitative change analysisTo get information about how many changes occurredin an ontology for a specific time interval REX offersthe quantitative change analysis component (Figure 4left). Users can generate diagrams to see the differencesbetween released ontology versions in statistical (quan-titative) form, i.e., we count and visualize how manychanges (addC, delC, addR, delR) occurred. In particular,users can display the number of changes in one ontologyfor a specific time interval, e.g., GOBiological Processes in2013.Moreover, one can compare the evolution of two dif-ferent ontologies for a specified time interval or comparetwo different time intervals for the same ontology. Userscan thus identify interesting ontologies and time periodsfor later region analyses.Trend analysisThe trend analysis component can be used to studyand compare the long-term evolution of selected regions(Figure 4 right). Users first need to specify the ontology,the time interval (first and last version) and the windowsize and step width (number of versions). Next they areable to select regions of their interest either by search-ing the respective accession number/concept name or bychoosing from top-level concepts of the ontology. REXexecutes the proposed trendDiscovery algorithm tomeasure the avg_costs for the selected regions at differ-ent points in time. The results are converted into a linechart which displays the trend of the measured avg_costsfor each region over time. Users are thus able to comparethe change intensity for different regions of interest withinone diagram.Web serviceIn addition to the web application, we provide a JAX webservice for programmatic access to REX. The web serviceinterface is available at http://dbs.uni-leipzig.de/wsrex?wsdl. Programmers can apply the region discovery meth-ods for a specified ontology and a defined time interval.Using the provided WSDL description they can gener-ate the corresponding client classes to enable web serviceinteraction. We provide three methods building on eachother: getAvailableOntologies returns all existing ontologiesin our OnEX repository. getVersions returns a list of available versions for aspecified ontology. calculateRegions calculates the average costs for eachconcept in the specified ontology and time interval. Itreturns a list of concepts including accessionnumbers, concept names and the computed averagecosts for each concept.Results and discussionIn the following we will describe and discuss someselected results generated with REX. In particular, wewill present results for the following well-known lifescience ontologies: Gene Ontology (GO) with its subFigure 4 Quantitative Change and Trend Analysis components. The figure shows the quantitative change and trend analysis components of REX.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 8 of 12ontologiesMolecular Functions (GO-MF), Biological Pro-cesses (GO-BP) and Cellular Components (GO-CC), theThesaurus of the National Cancer Institute (NCIT), AdultMouse Anatomy ontology (MA) and Chemical Entities ofBiomedical Interest (ChEBI). We will focus on results forthe recent past (mainly 2012-2013). Note that users canflexibly use REX to explore evolution trends for regions inother available ontologies for arbitrary time intervals. Wefirst discuss the evolution in general (quantitative statis-tics) and show the change intensities for whole ontologies.We then describe the usage of the structural analysis andtrend analysis components of REX by different examples.Evolution in generalUsually, the evolution of an ontology can be described bythe number of basic changes (e.g, addC, delC, addR,delR) occurred. For a start, the quantity of change opera-tions provides an indication of how an ontology evolved,e.g., an ontology exhibiting a small number of changesover the time can be classified as stable. However, thelocation, i.e., information about the region where changesoccurred is missing. Table 2 shows the quantity of addi-tions and deletions of concepts and relationships for theconsidered ontologies in 2012 and 2013 generated withthe quantitative change analysis component of REX. Over-all, every ontology has been modified in the consideredtime intervals. An exception forms MA, where no (onlyone) version was released in 2012 (2013). In general theontologies grow, i.e., the quantity of insertions (add) ishigher than the quantity of deletions (del). Most changesoccurred in NCIT and ChEBI, e.g., more than 12,000 con-cepts have been added in both ontologies. However, therehas also been an increased number of deletions, i.e., theontologies were optimized by rearranging concepts in thehierarchy or by merging multiple redundant concepts intoa single one.We apply our region algorithm to measure the changeintensity of whole ontologies. In particular, we use the rootconcept(s) of an ontology as regions, i.e., we aggregateall costs in the root(s) and can thus estimate the over-all ontology change intensity for a specific time interval.Additional file 1: Table S1 displays the change intensi-ties (abs_size, abs_costs, avg_costs) for all ontologies underinvestigation in 2012 and 2013. The ontologies show dif-ferent behaviors in their change intensities. In both peri-ods ChEBI exhibits the highest absolute costs. Its changeintensity even increased from 2012 compared to 2013(avg_costs: 0.88 ?0.95). Similarly, other ontologies likeGO-CC or NCIT have been modified more extensively in2013. In contrast, the GO sub ontologies GO-BP and GO-MF show decreased change intensities in 2013 comparedto 2012, i.e., modification actions on these ontologies havebeen reduced. Regarding GO, GO-BP is the sub ontol-ogy with the most frequent changes in both years. MAis relatively stable since only slight changes occurred in2013.Structural analysisAfter focusing on the overall ontology change inten-sity, we will now show how one can use the structuralanalysis component to explore details about the evolu-tion in different regions of an ontology. We describe theusage of the structural analysis component for GO-MF in2013. GO-MF has two parts namely molecular_function(GO:0003674) which contains all active molecular func-tions and obsolete_molecular_function (GO:0008369)used to collect all obsolete (inactive) concepts. All mainregions are direct children of GO:0003674. The browserview shows, that the majority of these regions are unsta-ble (see red nodes next to the central node in Figure 5left). For instance, transporter activity (GO:0005215)has avg_costs of 0.4 which are greater than those ofmolecular_function (0.12). Furthermore, many children(sub regions) of transporter activity show high avg_costs(Figure 5 middle). This indicates that the whole region oftransporter activity has significantly changed comparedto other regions in 2013 that show low avg_costs since lessor even zero changes occurred. For instance, the channelTable 2 Quantitative analysis results2012 2013addC delC addR delR addC delC addR delRGO-BP 2,914 51 11,940 2,844 1,159 91 5,742 2,812GO-MF 461 62 1,159 379 126 6 431 179GO-CC 185 3 581 124 219 4 597 341ChEBI 7,961 60 15,803 1,713 4,323 70 17,010 2,830NCIT 4,878 109 6,064 1,115 8,327 174 9,183 958MA - - - - - - - -The table shows the quantity of changes occurred in the ontologies under investigation. We distinguish between addC, delC, addR and delR changes for two periodsnamely 2012 and 2013. We considered available versions (at least two) within a period. MA has released no (only one) version in 2012 (2013). Thus, no statistics areprovided for MA.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 9 of 12Figure 5 Structural analysis for GO-MF in 2013. The figure shows the sub graphs of the root concept GO:0003674 molecular_function (left),GO:0005215 transporter activity (middle) and GO:0016247 channel regulator activity (right). Measured change intensities (avg_costs) are displayedusing a red-green scale (green: stable, i.e., less avg_costs; red: unstable, i.e., increased avg_costs).regulator activity(GO:0016247) region has avg_costs ofzero, i.e., no concept in this region has been modified in2013 (Figure 5 right).Instead of browsing, one can use the table view to locateinteresting regions by specifying different filter criteria.For instance, to select all regions in GO-MF related tothe term protein, one can specify a filter condition onthe name column (Figure 6). REX selects and displays allregions that satisfy this criteria, e.g., for GO-MF in 2013we find 557 regions related to protein. Users can furtherspecify conditions on avg_costs to find strongly chang-ing or stable regions. In our case we may look for regionsrelated to protein having avg_costs > 1, i.e., we searchfor unstable regions related to protein (Figure 6). We canthus reduce the selection from 557 to 14 regions satisfy-ing both criteria. Based on this selection (and a possiblesorting) users can now select a region of interest to cre-ate a corresponding graph in the browser view for a moredetailed inspection.We further allow to modify the applied cost model.Dependent on the application scenario users mightbe mainly interested in one/some of the used changeoperations (e.g., addC, addR, . . . ) , i.e., they should rankthe respective costs higher. One user might like to knowwhich ontology parts were of high research interest andhave been strongly extended in the near past (manyadditions). Another user might be looking for regionswhere many deletions took place since she needs toknow whether her application is affected by many infor-mation reducing changes (many deletions). To visual-ize the impact of different cost models, we exemplaryassign high costs to deletions (delC, delR, delA) and addi-tions (addC, addR, addA), respectively. Figure 7 showsresults for the concept heart development in GO-BPbetween September 2012 and 2014. Red nodes on left(right) denote regions where predominantly deletionsFigure 6 Specification of a filter on the name column and avg_costsin the table view. The figure shows the specification of a filter on thename column for GO-MF in 2013. In particular, we search for allregions related to protein having avg_costs > 1. For GO-MF in 201314 regions satisfy this criteria.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 10 of 12Figure 7 Application of different cost models. The figure shows results for the application of different cost model specifications for the conceptheart development in GO-BP between 09-2012 and 09-2014. To visualize the impact of different cost models, we assign high costs to deletions(left) and additions (right), respectively. Red nodes on the left (right) denote regions where predominantly deletions (additions) took place.(additions) took place. The results show that a variationof the cost model impacts the computation of stable orunstable regions. Many subregions of heart developmenthave been mainly extended (red nodes on the right side)whereas only two subregions where affected by a highnumber of deletions (red nodes on the left side).Trend analysisAs an example, we will show results for a two-year trendanalysis in NCIT between 2012 and 2013. In particu-lar, we select the three regions Chemotherapy_Regimen(C12218), Molecular_Abnormality (C3910) and Activity(C43431) and measure their change intensity (avg_costs).We choose a sliding window of six versions (window size?) and shift the window by one version in each step(step width ). Figure 8 displays the generated resultchart. The three regions show a different behavior in theirchange intensity. The work on Molecular_Abnormalitywas mainly performed in the beginning of 2012 (avg_costsup to 0.9) before its change intensity decreased to nearlyzero, i.e., one might consider this region as one thatbecame stable over time. The Chemotherapy_Regimen(C12218) region was stable in the complete period(avg_costs <0.05), i.e., the development in this regionwas probably performed before 2013 and it seems thatthe region will be stable in the near future as well. Onthe other hand, such a long-term stable region mighthave just been of low interest in the past and needsfuture development. In contrast, the region on Activity(C43431) has been continuously adapted during the wholeanalysis period. It seems to be of high research interestand is still under development such that it is likely tobe further changed in the next months or years. Usersthat are especially interested in content of this region forFigure 8 Trend analysis for selected regions of NCIT between 2012-2013. We perform a trend analysis for three regions of NCIT between 2012-2013:Chemotherapy_Regimen (C12218), Molecular_Abnormality (C3910) and Activity (C43431). The figure shows how their change intensity(avg_costs) evolved over time when using a sliding window of length six months and a step width of one month.Christen et al. Journal of Biomedical Semantics  (2015) 6:26 Page 11 of 12their analyses/workflows need to take care of the ongoingevolution. In contrast those working within the Molec-ular_Abnormality and Chemotherapy_Regimen regionscan assume that their regions of interest will be relativelystable in the near future. The trend analysis of REX isvaluable to support ontology development since the evo-lution of ontologies can be monitored over longer periodsin time. Of course, the interpretation of trend results isup to the user and depends on their specific applicationscenario.Conclusions and future workREX provides interactive access to information about theevolution of life science ontologies. Users can explore(un)stable ontology regions by different workflows. Theknowledge about changing ontology regions can be usedto support ontology-based algorithms and analysis. Fur-thermore, the development of large life science ontologiescan be monitored with REX, i.e., developers and projectcoordinators can inform themselves about ongoing workin different ontology parts.For future work, we plan to extend REX such thatusers are able to perform region analysis on their indi-vidual ontologies. We will further extend the change costcomputation of REX by involving alternative metrics forchanging concepts. For instance, we can involve seman-tic similarities or distances between ontology concepts(see [24] for an overview) to include the near contextof a changed concept, i.e. changes on ancestor as wellas descendant concepts. Effects of dense local changesmight have more impact, and could by ranked higherduring change intensity computation. Moreover, we liketo perform a more detailed evaluation with ontologydevelopers to analyze how REX can be used in ontologydevelopment and application scenarios. In [9] we alreadyused the Region Discovery Algorithm to analyze GeneOntology changes in the context of the widely used termenrichment analyses. It would be further interesting to seeif specific evolution trends are in accordance with editorialpolicies or specific activities in sub-domains. It might behelpful to provide a suitable presentation of REX results,e.g., by integrating its functionalities into tools used bythe ontology developers or annotation curators. Currently,the GOA consortium uses the tool Protein2GO for anno-tation and emphasizes curation and quality control ofGO annotations [25]. So far, it does not involve informa-tion on ontology evolution. Curators could be supportedby presenting REX change intensities for newly createdand existing annotations to indicate whether further qual-ity control might be necessary, e.g., due to significantchanges in the considered ontology part. To better sup-port the ontology development process with informationabout the evolution in different ontology regions, we liketo provide REX plugins for common tools like Protégé [26]or OBO-Edit [27]. The plugins should be able to flex-ibly present ontologies and their changing regions. Forinstance, developers might prefer a reduced presentationof the hierarchies, e.g., by focusing on highly changingregions that cover frequently used concepts or by divid-ing concepts of an ontology into smaller, moremanageableunits [28].Additional fileAdditional file 1: Table S1. Change intensity of complete ontologies in2012 and 2013. The table shows the change intensity for each ontologyunder investigation in 2012 and 2013. The three columns per year displaythe ontology size (abs_size) and the measured absolute costs (abs_costs) aswell as average costs (avg_costs). The red-green scale for avg_costshighlights ontologies with high (red) and low (green) change costs. Weperformed the region discovery algorithm for released versions in 2012and 2013, and considered the root concept(s) as region(s). For ontologieswith multiple root concepts we summed up the absolute costs per rootconcept and calculated the average costs w.r.t. the overall ontology size.Competing interestsThe authors declare that they have no competing interests.Authors contributionsVC has implemented the web application. MH and AG designed the regionanalysis algorithms, integrated the ontology versions and participated in theGUI component design. All authors participated in the evaluation andcontributed to write, read and approve the final manuscript.AcknowledgementsWe acknowledge support from the German Research Foundation (DFG) andUniversität Leipzig within the program of Open Access Publishing. A shortversion of this publication has been published as application paper at theconference on Data Integration in the Life Sciences (DILS) 2014.Author details1Department of Computer Science, Universität Leipzig, Augustusplatz 10,Leipzig, Germany. 2Interdisciplinary Center for Bioinformatics, UniversitätLeipzig, Härtelstr. 16 - 18, Leipzig, Germany.Received: 30 September 2014 Accepted: 17 April 2015JOURNAL OFBIOMEDICAL SEMANTICSPeters et al. Journal of Biomedical Semantics  (2015) 6:19 DOI 10.1186/s13326-015-0018-0RESEARCH ARTICLE Open AccessEvaluating drug-drug interaction information inNDF-RT and DrugBankLee B Peters1, Nathan Bahr2 and Olivier Bodenreider1*AbstractBackground: There is limited consensus among drug information sources on what constitutes drug-drug interactions(DDIs). We investigate DDI information in two publicly available sources, NDF-RT and DrugBank.Methods: We acquire drug-drug interactions from NDF-RT and DrugBank, and normalize the drugs to RxNorm. Wecompare interactions between NDF-RT and DrugBank and evaluate both sources against a reference list of 360 criticalinteractions. We compare the interactions detected with NDF-RT and DrugBank on a large prescription dataset. Finally,we contrast NDF-RT and DrugBank against a commercial source.Results: DrugBank drug-drug interaction information has limited overlap with NDF-RT (24-30%). The coverage ofthe reference set by both sources is about 60%. Applied to a prescription dataset of 35.5M pairs of co-prescribedsystemic clinical drugs, NDF-RT would have identified 808,285 interactions, while DrugBank would have identified1,170,693. Of these, 382,833 are common. The commercial source Multum provides a more systematic coverage(91%) of the reference list.Conclusions: This investigation confirms the limited overlap of DDI information between NDF-RT and DrugBank.Additional research is required to determine which source is better, if any. Usage of any of these sources inclinical decision systems should disclose these limitations.Keywords: Drug-drug interactions, NDF-RT, DrugBankBackgroundMotivationAn important component of electronic health recordsystems is the use of clinical decision support (CDS) toimprove medication safety [1,2]. Preventable adversedrug reactions include those resulting from drug-druginteractions (DDIs) [3,4]. Among other things, CDS sys-tems leverage drug-drug interaction information to re-duce the possibility of adverse drug events [5,6]. Whilethere are many sources of DDI information, and manycommercially available systems which contain this infor-mation, there is limited consensus on what constitutescritical and non-critical DDI [7].One source of publicly available DDI information iscontained in the National Drug File Reference Termin-ology (NDF-RT) and available through the NDF-RT appli-cation programming interface (API). Several applications* Correspondence: obodenreider@mail.nih.gov1Lister Hill National Center for Biomedical Communications, National Libraryof Medicine, National Institutes of Health, Bethesda, Maryland, USAFull list of author information is available at the end of the article© 2016 Peters et al.; licensee BioMed Central.Commons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.developed for medication management take advantage ofthe API for checking interactions in medication lists (e.g.,the iOS app Dosage for medication information and re-minders). However, it was announced before the writingof this article that the DDI information would be soon re-moved from NDF-RT [8]. Our search for another publiclyavailable source of DDI information led us to evaluate theDrugBank data source as a possible replacement for theNDF-RT DDIs.In this paper, our objective is to evaluate the DDI in-formation in NDF-RT and DrugBank. More specifically,we contrast NDF-RT and DrugBank interactions againsteach other, and contrast both against a previously pub-lished reference set of critical DDIs [9]. We also contrastNDF-RT and DrugBank in their ability to detect DDIs ina large prescription dataset. Finally, we compare thecoverage of the reference set by DrugBank and NDF-RTto that of a commercial source.This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 2 of 10Drug information sourcesThe following sections detail the characteristics of thedrug information sources used in this research. We useRxNorm to harmonize drugs between the two sources ofDDIs under investigation, NDF-RT and DrugBank.NDF-RTThe National Drug File Reference Terminology (NDF-RT)is a resource developed by the Department of VeteransAffairs (VA) Veterans Health Administration, as an exten-sion of the VA National Drug File [10]. It is updatedmonthly. NDF-RT covers 7319 active moieties (level = in-gredient). In addition to providing information about indi-vidual drugs (e.g., mechanism of action, physiologic effect,therapeutic intent), NDF-RT also provides a set of 10,831drug-drug interactions (DDIs). DDIs are asserted at theingredient level and accompanied by a mention of severity(significant or critical). For example, NDF-RT asserts acritical interaction between omeprazole and clopidogreland asserts a significant interaction between diltiazem andlovastatin. The version used in this study is dated July 7,2014 and was accessed through the NDF-RT API [11].(Provision of DDI information in NDF-RT was discontin-ued in November 2014).DrugBankDeveloped with funding from Genome Canada, DrugBankis a knowledge base containing extensive biochemicaland pharmacological information about drugs, theirmechanisms and their targets [12]. DrugBank covers7683 active moieties. Although not primarily developedfor clinical use, DrugBank provides a set of 12,128 drug-drug interactions (DDIs), asserted at the ingredientlevel, along with a brief textual description of the inter-action, and information about the possible molecularbasis of the interaction (target-based, enzyme-based,transporter-based). For example, DrugBank asserts aninteraction between omeprazole and clopidogrel bisul-fate, described as Omeprazole may decrease serumconcentrations of the active metabolite(s) of clopido-grel. Clopidogrel prescribing information recommendsavoiding concurrent use with omeprazole, due to thepossibility that combined use may result in decreasedclopidogrel effectiveness. The possible molecular basisof the interaction is reported to be enzyme-based ortransporter-based. The version used in this study (4.0) wasdownloaded from the DrugBank website on July 1, 2014.RxNormRxNorm is a standardized nomenclature for medicationsproduced and maintained by the U.S. National Libraryof Medicine (NLM) [13]. While NDF-RT is one of thedrug information sources integrated in RxNorm, Drug-Bank is not. However, most ingredients from DrugBankare covered by RxNorm and RxNorm can be used tomap drugs between DrugBank and NDF-RT. Moreover,RxNorm provides a rich network of relations amongvarious types of drug entities. For example, RxNorm ex-plicitly asserts that clopidogrel bisulfate is the preciseingredient of the ingredient clopidogrel, making it pos-sible to normalize the various salts and esters of a drugto their base form. RxNorm also integrates the Anatom-ical Therapeutic Chemical (ATC) classification, whichmakes it possible to extract the ATC class for mostdrugs. For example ATC classifies clopidogrel as a Plate-let aggregation inhibitors excl. heparin. The July 2014version of RxNorm is used in this study and wasaccessed through the RxNorm API [14].Related workExtracting drug-drug interaction (DDI) information fromtextual resources, such as the biomedical literature orstructured product labels is an active field of research[15-18]. Other researchers predict DDIs from a variety ofresources [19].As mentioned earlier, researchers have shown thebenefit of integrating DDI information in CPOEs (e.g.,[1,2]). However, not all studies demonstrate improve-ment on medication safety, especially due to the largenumber of alerts produced by some systems [20], whichraises questions about the quality of the underlying DDIinformation.Given the limited consensus across sources of DDI in-formation [7], researchers have proposed criteria forassessing high-priority DDIs [21] and for calibratingCDS systems [22]. An expert panel was convened toidentify high-severity, clinically significant DDIs for theOffice of the National Coordinator for Health Informa-tion Technology (ONC) as part of the Meaningful Useincentive program [9]. Candidate DDIs were assessed bythe panel based on a number of factors, including sever-ity levels across medication knowledge bases, conse-quences of the interaction, predisposing factors, andavailability of therapeutic alternatives. The resulting listcontains 360 interacting pairs of individual drugs con-taining 86 unique drugs. This list will be referred to asthe reference set of DDIs in the following sections.Specific contributionThe specific contribution of our work is to contrast twopublicly available sources of DDI information, NDF-RTand DrugBank, through an assessment of the overlap oftheir content, and of their coverage of a reference set ofDDIs. Moreover, we compare the ability of these twosources to identify DDIs in a large prescription dataset,and contrast them against a commercial source. To thebest of our knowledge, this is the first such comparativeinvestigation of NDF-RT and DrugBank DDI information.Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 3 of 10MethodsOur approach to evaluating drug-drug interaction (DDI)information in NDF-RT and DrugBank can be summa-rized as follows. We acquire the list of drug-drug inter-actions in NDF-RT and DrugBank, as well as a referenceset of DDIs. We map all drugs from the three sets toRxNorm and further normalize them to ingredient en-tities. We then compare the lists of pairs of interactingdrugs across sources in order to determine the sharedcoverage between NDF-RT and DrugBank, as well as thecoverage of the reference set by both sources. Wecharacterize the differences among DDI sets in terms ofdrug classes. We also compare the interactions detectedwith NDF-RT and DrugBank in a large prescriptiondataset. Finally, we compare the coverage of the refer-ence set by DrugBank and NDF-RT to that of a com-mercial source.Acquiring DDI informationNDF-RTWe used the NDF-RT API [11] to first extract the fullset of DDIs (DRUG_INTERACTION_KIND concepts),then to extract each associated drug concept (level = in-gredient) in the pair.DrugBankThe DrugBank XML and schema definition files weredownloaded from the DrugBank web site. We extractedthe interaction data from the XML file and created atable of drug name pairs for the interacting drugs.Reference setThe reference set of DDIs was created from the drugnames listed in Table two of [9] by associating each ob-ject drug with all corresponding precipitant drug(s)within a given interaction class. One pair involving amulti-ingredient drug (azathioprine and mercaptopu-rine) was eliminated, because multi-ingredient drugs aregenerally not consistently represented across sources.Normalizing drugs in reference to RxNormAfter obtaining the drug name pairs for interactingdrugs, we mapped the drugs to RxNorm by retrievingthe RxNorm identifiers (RxCUIs). For NDF-RT, theRxCUI is part of the drugs concept properties, so weused the NDF-RT API to extract the RxCUI from thedrug properties. For DrugBank and the reference set, weused the RxNorm API [7] to find the RxCUI from thedrug name. More specifically, we used exact and nor-malized string matches to map drug names to RxNorm.We then normalized to RxNorm ingredients thosedrugs which mapped to RxNorm entities. Some of thedrugs corresponded to base ingredients (e.g., doxacur-ium), while others corresponded to salt forms thatRxNorm classifies as precise ingredients (e.g., doxacur-ium chloride). In order to establish a consistent drugrepresentation across all three data sets for comparison,we converted precise ingredients to RxNorm ingredients(e.g., doxacurium chloride to doxacurium) using theRxNorm API.We eliminated from the comparison those pairs forwhich at least one of the drugs could not be found inRxNorm. For example, the DrugBank drug cerivastatin,which was withdrawn from the U.S. market in 2001, isnot present in RxNorm.Comparing interactions across sourcesHaving normalized all drugs to RxNorm ingredients, wecompared the lists from DDIs of NDF-RT and DrugBankin order to determine the similarities and unique fea-tures of each source. In addition, we compared eachwith the reference set of DDIs to determine how manyof the reference DDIs each source covered.Characterizing differencesTo determine if there was a pattern of missing interac-tions, we abstracted the pairs of interacting drugs intopairs of classes from the Anatomical Therapeutic Chem-ical (ATC) Classification System. We mapped the drugsdirectly to their 4th-level ATC classes using the RxNormAPI and identified those pairs of ATC classes for whichthe proportion of DDIs in common between NDF-RTand DrugBank was low, using the Jaccard score. Of note,some NDF-RT and DrugBank drugs are not representedin ATC. So only DDIs for which both the object andprecipitant drugs are present in ATC were analysed.Comparing coverage of interactions from actualprescription dataIn order to assess the difference between NDF-RT andDrugBank interactions based on usage data, we collecteddrug pairs generated from actual patient prescription lists.Checking medication lists for drug-drug interactions repre-sents a practical use case and is offered as a service bymany medication list applications. It also provides somefrequency of usage of the co-prescribed drug pairs extractedfrom the lists. The data was acquired from SymphonyHealth Solutions (http://symphonyhealth.com/). It includedone year of prescription data from the Washington, D.C.area from July 1, 2011 through June 30, 2012. Prescrip-tion information included prescriber, de-identified pa-tient information, and specific medication information,including the drug name and strength. The drug infor-mation was mapped into RxNorm clinical drugs. From alist of co-prescribed drugs, we generated all possiblepairwise combinations within the drug list (ignoring theorder of drugs in the pair, since the distinction betweenobject and precipitant drugs is not required for DDIPeters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 4 of 10testing). We ignored topical drugs from the prescriptionlist, because DDI information represented at the ingre-dient level in NDF-RT and DrugBank usually refers tointeractions between systemic drugs. For example, wegenerated the following pair of RxNorm clinical drug,24 HR Diltiazem Hydrocloride 360 MG Extended Re-lease Oral Capsule (830795) and Lovastatin 20 MG OralTablet (197904). We mapped the clinical drugs to theiractive moieties in RxNorm (Diltiazem Hydrocloride andLovastatin, respectively) and further normalized thoseto RxNorm ingredients, as we did with all object andprecipitant drugs from the various DDI lists. Here, wenormalized Diltiazem Hydrocloride to Diltiazem. Insummary, we transformed the pair of RxNorm clinicaldrugs extracted from the prescription list into the pairof RxNorm ingredients (Diltiazem, Lovastatin) for com-parison to the DDIs in NDF-RT and DrugBank.Publicly available vs. commercial DDI sourcesTo compare the coverage of the reference set by Drug-Bank and NDF-RT to that of a commercial source, weinvestigated the Multum drug knowledge base throughthe interaction checker of the website Drugs.com(http://www.drugs.com/), against which we tested the360 DDIs of the reference set.ResultsDDI information in NDF-RT, DrugBank and the reference setTable 1 summarizes the number of DDIs in the threedata sets.Reference setThe reference set contained 360 DDIs; all the drugsmapped to RxNorm and were all classified as ingredi-ents. The 360 DDIs covered 86 RxNorm ingredients.DrugBankDrugBank contained 12,128 DDIs defined in the XMLfile. DDIs involving drugs with no mapping to RxNormwere discarded (418 DDIs involving 46 drugs). Analysisof all the discarded DDIs revealed several reasons whythe DDIs were eliminated. Some DDIs involved drugsthat were either withdrawn from public use (e.g., cerivas-tatin, ephedra, heptabarbital) or not approved by theU.S. Food and Drug Adminstration (e.g., cinolazepam,carbetocin) and those drugs could not be mapped toTable 1 DDI counts in the three datasetsData set DDI countsTotal from source Mapped to RxNorm NormalizedReference 360 360 360DrugBank 12128 11762 11552NDF-RT 10831 9452 9392RxNorm. Additionally, 518 DrugBank DDIs were elimi-nated through the ingredient normalization process. Forexample, the DDIs containing zuclopenthixol, zuclo-penthixol acetate and zuclopenthixol deconoate werenormalized to produce a single set with zuclopenthixolas the ingredient, eliminating the redundant pairs con-taining the salt forms. The resulting 11,552 normalizedDDIs covered 1153 RxNorm ingredients.NDF-RTNDF-RT contained 10,831 DDIs extracted from the dataset. DDIs involving drugs with no mapping to RxNormwere discarded (1379 DDIs involving 38 drugs). Analysisof all the discarded DDIs revealed that some DDIs wereassociated with drugs which referenced obsolete RxNormconcepts, many of these vaccine drugs that were recentlyremoved from RxNorm. Additionally, 60 NDF-RT DDIswere eliminated through the ingredient normalizationprocess. The resulting 9,392 normalized DDIs covered1079 RxNorm ingredients.In the remainder of this paper, DDIs refer to pairs ofobject and precipitant drugs normalized to RxNorm in-gredients. However, even after normalization to RxNormingredients, the coverage of drugs is not expected to bethe same in NDF-RT and DrugBank. For example, vac-cines and other biologicals are present in NDF-RT, butout of scope for DrugBank. When analysing DDIs acrossthe two sources, breakdown by pharmacological classeswill reflect such differences in drug coverage.Comparing interactions across sourcesThe matching DDIs between the three data sets areshown in Table 2.Overlap between DrugBank and NDF-RTOverall, the 2801 DDIs common to NDF-RT and Drug-Bank represent a 30% coverage rate of NDF-RT byDrugBank and a 24% coverage rate of DrugBank byNDF-RT. Example of common DDIs include diltiazem/lovastatin and itraconazole/sirolimus. Examples of DDIsin DrugBank only include acebutolol/Insulin Lispro andmetronidazole/terfenadine. Examples of DDIs in NDF-RT only include amiodarone/sotalol and meperidine/linezolid.When we only consider DDIs from the reference set,the overlap between DrugBank and NDF-RT is significantlyTable 2 Matching DDIs across data setsData set Number of matching DDIsReference DrugBank NDF-RTReference 360 211 207DrugBank 211 11552 2801NDF-RT 207 2801 9392Table 3 Reference Set DDI and coverage in NDF-RT and DrugBankGrp # DDI groupdescription# pairs NDF-RTmatchesDrugBankmatchesObject members Precipitant members3 Amphetamine andderivatives  MAOinhibitors60 30 (50%) 30 (50%) Dexmethylphenidate, Dextroamphetamine, Methylphenidate,Lisdexamfetamine, Phendimetrazine, Pseudoephedrine,Amphetamine, Benzphetamine, Diethylproprion, Phentermine,Atomoxetine, MethamphetamineTranylcypromine, Phenelzine, Isocarboxazid, Procarbazine,Selegiline4 Atazanavir  Protonpump inhibitors (PPIs)5 5 (100%) 5 (100%) Atazanavir Omeprazole, Lansoprazole, Pantoprazole, Rabeprazole, Esmoprazole8 Fluoxetine - MOAinhibitors55 39 (71%) 43 (78%) Fluoxetine, Paroxetine, Citalopram, Escitalopram,Sertraline, Fluvoxamine, Duloxetine, Nefazodone,Desvenlafaxine, Milnacipran, VenlafaxineTranylcypromine, Phenelzine, Isocarboxazid, Procarbazine, Selegiline11 Irinotecan Ketoconazole23 1 (4%) 5 (22%) Irinotecan Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,Darunavir, Lopinavir, Tipranavir, Fosamprenavir, Clarithromycin,Erythromycin, Telithromycin, Amiodarone, Verapamil, Diltiazem,Ketoconazole, Itraconazole, Fluconazole, Voriconazole, Nefazodone,Aprepitant, Cimetidine16 Narcotic analgesics MAO inhibitors30 25 (83%) 15 (50%) Meperidine, Methadone, Tapentadol, Fentanyl,Tramadol, DextromethorphanTranylcypromine, Phenelzine, Isocarboxazid, Procarbazine, Selegiline22 Ramelteon-fluvoxamine 4 0 (0%) 2 (50%) Ramelteon Fluvoxamine, Amiodarone, Ticlopidine, Ciprofloxacin23 Rifampin  ritonavir 60 41 (68%) 25 (42%) Bosentan, Rifapentine, Carbamazepine, Rifabutin,Rifampin, St. Johns wortRitonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,Darunavir, Lopinavir, Tipranavir, Fosamprenavir25 HMG Co-A reductaseinhibitors  proteaseinhibitors40 38 (95%) 33 (83%) Simvastatin, Lovastatin Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir, Amprenavir,Darunavir, Lopinavir, Tipranavir, Clarithromycin, Erythromycin,Telithromycin, Amiodarone, Verapamil, Diltiazem, Tranylcypromine,Phenelzine, Isocarboxazid, Procarbazine, Selegiline27 Telithromycin ergot alkaloids andderivatives60 13 (22%) 35 (58%) Ritonavir, Nelfinavir, Atazanavir, Indinavir, Saquinavir,Amprenavir, Darunavir, Lopinavir, Tipranavir,Clarithromycin, Erythromycin, Telithromycin,Ketoconazole, Itraconazole, VoriconazoleErgotamine, Methylergonovine, Dihydroergotamine, Ergonovine28 Tizanidine ciprofloxacin7 6 (86%) 6 (86%) Tizanidine Fluvoxamine, Amiodarone, Ticlopidine, Ciprofloxacin, Mexiletine,Propafenone, Zileuton30 Tranylcypromine procarbazine1 1 (100%) 1 (100%) Tranylcypromine Procarbazine31 Triptans  MAOinhibitors15 8 (53%) 11 (73%) Sumatriptan, Zolmitriptan, Rizatriptan Tranylcypromine, Phenelzine, Isocarboxazid, Moclobamide,Methylene blueTOTAL 207 (58%) 211 (59%)Petersetal.JournalofBiomedicalSemantics (2015) 6:19 Page5of10Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 6 of 10higher. The 146 reference DDIs common to NDF-RT andDrugBank represent 71% of the 207 reference DDIs cov-ered by NDF-RT and 69% of the 211 reference DDIs cov-ered by DrugBank.Reference set coverageTable 3 shows the breakdown by the reference setgroups of the DDI mapping for DrugBank and NDF-RT.DrugBank contained 211 DDIs (59%) from the referenceset, compared with 207 DDIs (58%) for NDF-RT. Therewere 146 DDIs (42%) from the reference set which wereboth in the NDF-RT and DrugBank, including diltia-zem/lovastatin, simvastatin/amiodarone and atazana-vir/omeprazole. Conversely, there were 88 DDIs (24%) inthe reference set which were not contained in eitherDrugBank or NDF-RT, including irinotecan/indinavir,ketoconazole/ergonovine and milnacipran/selegiline.DrugBank contained 65 DDIs from the reference setwhich were not in NDF-RT (e.g., lovastatin/tipranavirand ramelteon/fluvoxamine), and NDF-RT contained 61DDIs from the reference set not in DrugBank (e.g., flu-oxetine/procarbazine and simvastatin/saquinavir). Drug-Bank had coverage in all groups from the reference set,though only 100% coverage in two groups. NDF-RT hadcoverage of all but one group (#22 ramelteon-fluvox-amine). The coverage of the reference set of drug-druginteractions by DrugBank and NDF-RT is available asAdditional file 1.Of the 86 drugs contained in the reference set, one(moclobamide, involved in 3 DDIs) did not exist in NDF-RT and two (dexmethylphenidate, involved in 5 DDIs andmethylene blue, involved in 3 DDIs) did not exist in Drug-Bank. In addition, there were four other drugs (bosentan,lopinavir, methadone and zileuton) which were present inTable 4 Top ATC Class Counts in NDF-RT and DrugBankNDF-RTATC Class Name DDIAntibiotics 709Anticholinesterases 25Fluoroquinolones 401Hydantoin derivatives 266Macrolides 443Non-selective monoamine reuptake inhibitors 311Other antidepressants 309Protease inhibitors 1130Protein kinase inhibitors 759Selective immunosuppressants 307Selective serotonin reuptake inhibitors 341Triazole derivatives 427Vitamin K antagonists 292*Boldface values indicate the ten top categories in each source.DrugBank, but not involved in any of the DDIs from thereference set. DrugBank did have DDIs for these fourdrugs outside of the reference set.Pharmacologic classesWe mapped the DDIs from NDF-RT and DrugBank to4th-level ATC drug classes to see if there were distinctiveclass differences between the two sets of DDIs. A smallproportion of the drugs, such as avanafil, lopinavir andzileuton (14% in NDF-RT, 11% in DrugBank) were notrepresented in ATC and the corresponding DDIs wereexcluded from the analysis.Table 4 shows the top frequency count of ATC classesfor NDF-RT and DrugBank DDIs, along with the num-ber of drugs from each class represented in the sourceof DDI. For example, NDF-RT has 401 DDIs involving11 drugs for the fluoroquinolone class, while DrugBankhas 335 interactions involving 14 drugs for this class.Fluoroquinolones is in the top-10 classes for the fre-quency of DDIs in NDF-RT, but not in DrugBank. Twoclasses, protein kinase inhibitors and selective immuno-suppressants, seem underrepresented in DrugBank,while three classes, anticholinesterases, hydantoin deriv-atives and non-selective monoamine reuptake inhibitors,seem underrepresented in NDF-RT.Table 5 shows the ATC pairs containing the mostDDIs from NDF-RT and DrugBank, but a low propor-tion of shared DDIs between the two sources (evidencedby the low Jaccard scores). The Adrenergic and dopa-minergic agents - Non-selective monoamine reuptake in-hibitors class pair, for example, has no NDF-RT DDIsand 78 DrugBank DDIs. Examples in this class pair in-clude dopamine/amitriptyline and norepinephrine/amox-ipine. Conversely, in the Protease inhibitors - ProteinDrugBankdrugs DDI drugs22 744 254 411 711 335 144 406 46 502 910 736 1011 457 1011 751 1119 389 2013 150 146 380 64 502 43 427 3Table 5 Top ATC Class PairsATC Class Pair total DDIs NDF-RT only DrugBank only both JaccardProtease inhibitors - Protein kinase inhibitors 103 72 10 21 0.20Benzodiazepine derivatives - Protease inhibitors 88 5 52 28 0.32Adrenergic and dopaminergic agents - Non-selective monoamine reuptake inhibitors 78 0 78 0 0Antibiotics - Other quaternary ammonium compounds 71 35 8 28 0.39Penicillins with extended spectrum - Tetracyclines 63 0 58 5 0.08Protein kinase inhibitors - Triazole derivatives 60 32 10 18 0.30Beta blocking agents, non-selective - Sulfonamides, urea derivatives 56 10 14 32 0.57Macrolides - Protein kinase inhibitors 56 27 14 15 0.27Figure 1 Frequency of DDIs in prescription pairs.Peters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 7 of 10kinase inhibitors class pair, many more specific DDIsexist in NDF-RT, such as boceprevir/bosutinib, than inDrugBank. Finally, the Beta blocking agents, non-selective - Sulfonamides, urea derivatives class pair illus-trates a situation where both sources share a majority ofDDIs, but also have a significant number of specificDDIs. For example, sotalol/glyburide is common, butsotalol/tolazamide is specific to NDF-RT and sotalol/gli-clazide is specific to DrugBank.Comparing coverage of interactions from actualprescription dataFrom the prescription dataset, almost 35.9 million pairswere extracted, representing 816,258 unique pairs ofRxNorm clinical drugs. Restricted to systemic drugs, thedataset included 35.5 million pairs of clinical drugs(808,285 unique). Each clinical drug maps to at least oneingredient, and multi-ingredient drugs map to several in-gredients, resulting in multiple ingredient pairs for agiven pair of clinical drugs. For example, starting fromthe pair of clinical drugs Primidone 250 MG Oral Tabletand Carbidopa 25 MG/Levodopa 100 MG Oral Tablet,we generate the following two pairs of ingredients, (Pri-midone, Carbidopa) and (Primidone, Levodopa), becausethe clinical drug Carbidopa 25 MG/Levodopa 100 MGOral Tablet maps to two ingredients, Carbidopa andLevodopa. After mapping of the clinical drugs to ingredi-ents, there were 45.2 million pairs of co-prescribeddrugs, ranging in frequency between 1 and 158,515 (me-dian = 18). Of these, 808,285 pairs matched with NDF-RT DDIs (2153 unique), while 1,170,693 pairs matchedwith DrugBank DDIs (2823 unique). There were 382,833pairs that matched with both NDF-RT and DrugBank(988 unique). There were 26,672 matches with the refer-ence set (88 unique). Figure 1 shows the frequency ofthe DDIs (in parenthesis) with the unique number ofDDIs found in the prescription dataset.Examples of frequently co-prescribed drugs identifiedas a DDI by both NDF-RT and DrugBank include Diltia-zem/Lovastatin and Clarithromycin/Simvastatin. Co-prescribed drugs identified as a DDI by NDF-RT, butnot DrugBank include Amlodipine/Simvastatin and Dil-tiazem/Oxycodone. Conversely, DDIs identified by Drug-Bank, but not NDF-RT, include Amoxicillin/EthinylEstradiol and Hydrochlorothiazide/Insulin Lispro. Table 6shows the most frequently co-prescribed pairs recog-nized as a DDI by one or both of the sources.Publicly available vs. commercial DDI sourcesWe were able to match 328 (91%) DDIs of the referenceset to Multum DDIs, a significantly higher proportioncompared to NDF-RT and DrugBank. The level of sever-ity provided by Multum for these DDIs (on a 3-levelscale mild, moderate or major) was consistent with thenotion of high-severity claimed by the authors of thereference set, since 302 DDIs were rated major and theother 26 were rated moderate.DiscussionIn this section, we discuss some of the reasons for thelimited overlap of DDI information among sources. Wealso analyse its implications for our interactions API.Limited overlap between NDF-RT and DrugBank DDIsFindingsOne important finding of this investigation is the limitedoverlap (<30%) between DrugBank and NDF-RT DDIs,Table 6 Top DDI Pairs found in Prescription DataDrug pair Freq DrugBank DDI NDF-RT DDIAmlodipine-Simvastatin 76980 XGlipizide-Metoprolol 15469 X Xatorvastatin-Fenofibrate 14897 X XFenofibrate-Simvastatin 14060 X XAlbuterol-Metoprolol 14030 XInsulin Glargine-Metoprolol 13492 XMetoprolol-Sertraline 13311 Xatorvastatin-Diltiazem 13218 X XDigoxin-Furosemide 12958 XLisinopril-Triamterene 12399 X Xcyclobenzaprine-Tramadol 12211 XDiltiazem-Metoprolol 11738 XOxycodone-Sertraline 11710 XDiltiazem-Simvastatin 11484 X XAlprazolam-Fluoxetine 11194 XSertraline-Trazodone 11066 XAmoxicillin-Ethinyl Estradiol 11044 XLisinopril-Spironolactone 10248 X XClonazepam-Fluoxetine 10149 XFenofibrate-rosuvastatin 9954 X XClonidine-Metoprolol 9827 X XHydralazine-Metoprolol 9556 Xduloxetine-Trazodone 9236 Xclopidogrel-pantoprazole 9177 XEscitalopram-Metoprolol 8396 XLevofloxacin-Prednisone 8335 XEscitalopram-Oxycodone 8228 XCitalopram-Trazodone 8081 XCitalopram-Oxycodone 8016 XMetoprolol-salmeterol 7703 XAtenolol-Glipizide 7597 X XFluoxetine-Trazodone 7579 Xcarvedilol-Digoxin 7517 XFluoxetine-Oxycodone 7508 Xcarvedilol-Insulin Glargine 7496 XCitalopram-Metoprolol 7474 XEscitalopram-Trazodone 7421 XAlprazolam-Omeprazole 7339 XPeters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 8 of 10although both sources cover roughly the same numberof DDIs. We found 6591 DDIs specific to NDF-RT and8751 specific to DrugBank, while only 2801 DDIs werecommon to both sources. Differences in scope (e.g., thelack of vaccine DDIs in DrugBank) account for only asmall portion of the differences. Differential coveragepersisted after abstracting DDIs to the correspondingdrug classes from ATC. For example, NDF-RT had manymore DDIs than DrugBank for Protease inhibitor drugs,and had much fewer DDIs for Anticholinesterases drugs.Likewise, many ATC class pairs had a small proportionof DDIs in common, evidenced by low Jaccard scores.For example, among the 56 DDIs for the Macrolides -Protein kinase inhibitors class pair, only 15 are commonto NDF-RT and DrugBank (Jaccard = 0.27). The lowoverlap rate also applied to our more frequently pre-scribed drug pairs in the prescription data set. When weexamined the co-prescribed drugs from the prescriptiondata set, we found significant differences in identificationof DDIs between the two sources for the most frequentlyco-prescribed pairs. Of the top 10 most frequently pre-scribed drug pairs identified as potential DDIs by Drug-Bank, only 5 were recognized as NDF-RT DDIs (seeTable 6 for details).DDI severityWe wondered if the differential coverage observed be-tween NDF-RT and DrugBank could be due in part todifferent editorial guidelines for curating less severeDDIs, assuming the most severe DDIs would be coveredmore consistently by both sources. This did not seem tobe the case since the 360 high-severity DDIs from thereference set are only partially covered by NDF-RT andDrugBank. While NDF-RT provides an indication of se-verity (critical or significant) for its DDIs, DrugBankdoes not. Nevertheless, we tested if DrugBank wouldcover a larger proportion of NDF-RT critical DDIs. Infact, DrugBank DDIs accounted for 31% of the total crit-ical DDIs in NDF-RT and for 29% of the total significantDDIs. In other words, there is no difference in the cover-age of NDF-RT by DrugBank in terms of DDI severity.(Even when correcting for the absence of vaccine DDIsin DrugBank, the overlapping DDIs accounted for just40% of the critical DDIs in NDF-RT.)Differential coverage of DDIs among sourcesOur finding of limited overlap between NDF-RT andDrugBank should not be surprising given similar find-ings in studies comparing drug knowledge bases for crit-ically important DDIs [21]. There is no standardizationfor what constitutes a drug-drug interaction, and DDIcurators have to consider a variety of drug and patientspecific factors in their decision to include pairs of drugsin their DDI lists. These factors include the severity ofthe interaction, the probability of the interaction, patientcharacteristics (e.g., specific patient groups, such as eld-erly patients), and evidence supporting the interaction(quantity of evidence, as well as the quality of the evi-dence). The number of factors to consider makes this acomplex and daunting task. Moreover, the maintenancePeters et al. Journal of Biomedical Semantics  (2015) 6:19 Page 9 of 10of DDI lists itself is an issue, as additional evidence foror against DDIs becomes available over time. Finally, dif-ferent strategies for prioritizing the various factors alsoaccounts for some of the differences observed acrosssources.Implications for the interactions APIOverall, our investigation of NDF-RT and DrugBank assources of DDIs for our API provides a mixed picture.Not only do they both provide incomplete coverage ofthe reference set (about 60% each), but their overlap isalso limited (42%). A similar difference could be ob-served when we simulated interaction detection basedon actual usage data. We also confirmed that a commer-cial source, Multum, provided a more systematic cover-age of the reference set.With the provision of DDI information being discon-tinued in NDF-RT in November 2014, we decided to useDrugBank as a replacement for our interactions API.This solution is far from ideal, because this investigationdid not establish the accuracy of either source, but sim-ply assessed differences among them. On the other hand,DDI determination is not an exact science and bothNDF-RT and DrugBank provide valuable information tosupport medical decision. NDF-RT DDIs are associatedwith levels of severity, while DrugBanks come with ashort description. In practice, the absence of severitylevels in DrugBank is a disadvantage, as severity is animportant consideration for determining when to alertphysicians to a potential DDI. Severity is also a require-ment when checking DDI in the context of the Mean-ingful Use incentive program.It is important to keep in mind that no clinical deci-sion system, as good as it is, can be a substitute for med-ical advice. Our API not only clearly discloses the originof the information it provides, but also reminds ourusers to seek advice from health professionals beforemaking decisions about their medications. In the future,we plan to perform a more systematic and qualitative in-vestigation of publicly available and commercial DDIsources in order to better assess the differences amongthese sources.ConclusionsThis study is the first comparative investigation of DDIinformation in two publicly available sources, NDF-RTand DrugBank. We compared the two sources not onlyto themselves, but also to a reference set of DDIs, andassessed their ability to identify DDIs in a large prescrip-tion dataset. We also contrasted NDF-RT and DrugBankagainst the commercial source Multum.This investigation confirms the limited overlap be-tween DDI information between NDF-RT and Drug-Bank, even for the reference dataset. Additional researchis required to determine which source is better, if any.Usage of any of these sources in clinical decision systemsshould clearly disclose these limitations.Additional fileAdditional file 1: Coverage of the reference set of drug-drug inter-actions by DrugBank and NDF-RT.Competing interestsThe authors declare that they have no competing interests.Authors contributionsLP downloaded, normalized and compared the main datasets. OB createdand normalized the reference dataset. NB characterized the differencesagainst ATC drug classes and the commercial source of DDI information. Allthe authors contributed to writing the manuscript. All the authors have readand approved the final manuscript.AcknowledgementsThis work was supported by the Intramural Research Program of the NIH,National Library of Medicine.Author details1Lister Hill National Center for Biomedical Communications, National Libraryof Medicine, National Institutes of Health, Bethesda, Maryland, USA.2Department of Medical Informatics and Clinical Epidemiology, OregonHealth & Science University, Portland, Oregon, USA.Received: 22 November 2014 Accepted: 30 March 2015Published: 11 May 2015JOURNAL OFBIOMEDICAL SEMANTICSWinnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 DOI 10.1186/s13326-015-0017-1RESEARCH ARTICLE Open AccessExploring adverse drug events at the class levelRainer Winnenburg1, Alfred Sorbello2 and Olivier Bodenreider3*AbstractBackground: While the association between a drug and an adverse event (ADE) is generally detected at the levelof individual drugs, ADEs are often discussed at the class level, i.e., at the level of pharmacologic classes (e.g., indrug labels). We propose two approaches, one visual and one computational, to exploring the contribution ofindividual drugs to the class signal.Methods: Having established a dataset of ADEs from MEDLINE, we aggregate drugs into ATC classes and ADEs intohigh-level MeSH terms. We compute statistical associations between drugs and ADEs at the drug level and at theclass level. Finally, we visualize the signals at increasing levels of resolution using heat maps. We also automate theexploration of drug-ADE associations at the class level using clustering techniques.Results: Using our visual approach, we were able to uncover known associations, e.g., between fluoroquinolonesand tendon injuries, and between statins and rhabdomyolysis. Using our computational approach, we systematicallyanalyzed 488 associations between a drug class and an ADE.Conclusions: The findings gained from our exploratory techniques should be of interest to the curators of ADErepositories and drug safety professionals. Our approach can be applied to different drug-ADE datasets, using differentdrug classification systems and different signal detection algorithms.Keywords: Adverse drug events, Drug classes, Anatomical Therapeutic Chemical (ATC) drug classification system,Class effect, Heat maps, PharmacovigilanceBackgroundMotivationAccording to the Agency for Healthcare Research andQuality (AHRQ), adverse drug events (ADEs) result inmore than 770,000 injuries and deaths each year andcost up to $5.6 million per hospital [1]. Drug safety isaddressed through the drug development process, notonly during clinical trials [2], but also through postmarket-ing surveillance, by analyzing spontaneous reports [3],observational data [4] and the biomedical literature [5].While the association between a drug and an adverseevent is generally detected at the level of individual drugs(e.g., between aspirin and Reye syndrome [6]), ADEs areoften discussed at the level of pharmacologic classes.Examples include the ototoxicity of aminoglycosides [7],the association between statins and rhabdomyolysis [8],and between vaccines and Guillain-Barré syndrome [9].* Correspondence: obodenreider@mail.nih.gov3Lister Hill National Center for Biomedical Communications, National Libraryof Medicine, National Institutes of Health, Bethesda, MD, USAFull list of author information is available at the end of the article© 2016 Winnenburg et al.; licensee BioMed CeCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.These examples illustrate the need for investigating ADEsat the class level, i.e., after aggregating individual drugsinto pharmacologic classes.Some ADEs can be observed with every individualdrug in a class. This is often the case when the ADE isrelated to the physiologic effect of the drug. For ex-ample, bleeding is a common effect of anticoagulants,such as vitamin K antagonists [10]. Conversely, someADEs are associated with some class members, but notwith all of them. For example, a recent review reports adifferential risk of tendon injuries with various fluoro-quinolones, the highest risk being with ofloxacin [11].From an ontological perspective, it is interesting toexplore whether the ADE is an inherent property of theclass (inherited by every member of the class) or rathera property of some members only. In practice, whenthere is a high risk of an ADE for a class (i.e., a strongclass-level signal), one may want to drill down andinvestigate the drug-level signal for each individual drugin the class to discover if the class-level signal resultsfrom uniformly high drug-level signals, or is ratherdriven by an intense signal for a small number of drugs,ntral. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 2 of 10while the other drugs in the class would not exhibit a highrisk for this ADE. The former reflects a class propertyinherited by each drug, whereas the latter reflects a drugproperty, i.e., a property for some of the drugs only.The objective of this work is to explore the contributionof individual drugs to the class signal. More specifically, wepropose two approaches, one visual and one computa-tional, to identifying class effects, i.e., cases when alldrugs in a class have the same ADE (as opposed to caseswhere the class signal is driven by only a few drugs fromthe class).Drug and ADE terminologiesThe following sections detail the characteristics of theresources used in this research. We use MeSH for aggre-gating ADEs and ATC for drug classification purposes.We also use RxNorm to harmonize drugs betweenMeSH and ATC.MeSHThe MeSH thesaurus is the controlled vocabulary usedto index documents included in the MEDLINE database[12]. It contains over 27,000 descriptors (main headings)organized in sixteen hierarchical tree structures. Eachtree contains up to eleven levels denoting aboutnessrelationships between the terms. For example, the termRhabdomyolysis is classified under Muscular Diseasesin the Diseases tree. Version 2014 of MeSH is used inthis study.ATCThe Anatomical Therapeutic Chemical (ATC) classifica-tion [13], a system developed by the World HealthOrganization (WHO) Collaborating Centre for DrugStatistics Methodology, is recommended for worldwideuse to compile drug utilization statistics. The systemincludes drug classifications at 5 levels; anatomical, thera-peutic, pharmacological, chemical and drugs or ingredi-ents. For example, the 4th-level ATC class Vitamin Kantagonists (B01AA) has the following 5th-level drugs asmembers: acenocoumarol, dicumarol, fluindione, phe-nindione, phenprocoumon, tioclomarol and warfarin.The 2014 edition of ATC used in this study contains4,580 5th-level ATC drugs and 1,256 drug classes.RxNormRxNorm is a standardized nomenclature for medicationsproduced and maintained by the U.S. National Libraryof Medicine (NLM) [14]. Both ATC and MeSH are inte-grated in RxNorm, making it possible for us to useRxNorm to link MeSH drugs to their classes in ATC.Moreover, RxNorm provides a rich network of relationsamong various types of drug entities, making it possibleto normalize the various salts and esters of a drug(precise ingredients) to their base form (ingredient).The April 2014 version of RxNorm is used in this studyand was accessed through the RxNorm API [15].Related workADE extraction and predictionThere is a large body of research on the extraction ofdrug ADE associations from various sources (e.g.,[3-5,16]), in which terminologies are usually leveragedfor the normalization of drugs (e.g., to RxNorm andATC) and adverse reactions, for example to the Com-mon Terminology Criteria for Adverse Events (CTCAE)and the Medical Dictionary for Regulatory Activities(MedDRA). Researchers have also created repositories ofADEs, such as ADEpedia [17] and used network analysisto analyze and predict drug-ADE associations [18]. Inour effort to explore the ADEs at the class level, we usean existing dataset of drug-ADE pairs obtained fromprior work on extracting drug-ADE pairs from MED-LINE indexing.Research on class effectMany researchers have investigated whether a givenADE was specific to a drug or common to all drugs inthe corresponding class. Examples of such investigationsinclude the exploration of antiepileptic-induced suici-dality [19], association between anti-VEGF agents anddysthyroidism [20] or avascular necrosis of the femoralhead [21], association between dipeptidyl-peptidase-4inhibitors and heart failure [22] or angioedema [23], andatypical antipsychotic-induced somnambulism [24]. Asearch for class effect in the titles of PubMed articlesretrieves over one hundred citations. Such efforts, how-ever, generally investigate one specific drug class andone specific ADE. In contrast, we propose a methodfor assessing the class effect over a wide range of drugclasses and ADEs.Specific contributionThe specific contribution of our work is to combineexisting drug safety signal detection and visualizationtechniques, and to leverage drug terminologies for ex-ploring adverse drug events at the class level. We extendthe visual exploration with an automated computationalapproach to identifying class effects, allowing their system-atic detection from any dataset of drug-ADE associations.MethodsOur approach to exploring ADEs at the class level canbe summarized as follows. We first establish a dataset ofADEs by extracting drug-ADE pairs from MEDLINE.Then we aggregate drugs into ATC classes and ADEsinto high-level MeSH terms. We compute the associ-ation between drugs and ADEs at the drug level and atWinnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 3 of 10the class level. In our visual approach, we use heat mapsto visualize the signal at increasing levels of resolutionto distinguish between drug-level and class-level ADEs.In our computational approach, we achieve the sameresult by leveraging clustering techniques. While thevisual approach requires manual selection of the classesand ADEs of interest, the computational approach iscompletely automated and can be applied over a widerange of drug classes and ADEs.Extracting drug  adverse event pairs from the literatureOur dataset consists of pairs of drugs and ADEs ex-tracted from the MEDLINE database, using an approachsimilar to [5]. We use combinations of MeSH descrip-tors (and supplementary concepts) and qualifiers toidentify, on the one hand, drugs involved in ADEs (e.g.,ofloxacin/adverse effects) and, on the other, manifesta-tions reflecting an ADE (e.g., tendinopathy/chemicallyinduced). We improved upon [5] by also taking intoaccount those MeSH descriptors inherently indicative ofadverse events (e.g., Drug-induced liver injury). Wecollected the resulting list of drug-manifestation pairsfor each ADE (e.g., ofloxacin-tendinopathy).Linking MEDLINE drugs to ATC classesWe map all MeSH drugs extracted from MEDLINE to ourtarget terminology, ATC, for aggregation purposes, usingRxNorm.Table 1 Example of contingency table representingdrug-ADE associations in MEDLINEWith this ADE Without this ADEArticles mentioning this drug a bArticles not mentioning this drug c dMapping MeSH drugs to ATC drugs through RxNormingredientsBoth ATC and MeSH are integrated in RxNorm. For ex-ample, the RxNorm drug rosuvastatin (301542) is linkedto both the MeSH drug rosuvastatin (C422923) and the5th-level ATC drug rosuvastatin (C10AA07). Individualdrugs in MeSH correspond to ingredients (IN) and pre-cise ingredients (PIN) in RxNorm. We normalize thedrugs by mapping PINs to their corresponding INs. Forexample, RxNorm explicitly asserts that valproic acid isthe precise ingredient of the ingredient valproate.Of note, a given drug can be represented multipletimes in ATC. Typically, topical drugs and systemicdrugs have different ATC codes for the same activemoiety. For example, the anti-infective ofloxacin has twocodes in ATC, depending on whether it is classified asan antibacterial drug for systemic use (J01MA01) or asan ophthalmological drug (S01AE01). However, we con-sider unique drugs, not multiple codes, when we associ-ate drugs with their ADEs. We only use the codes tolink drugs to their classes. The individual MeSH drugsextracted from MEDLINE and which map to ATCconstitute the set of eligible drugs for this study.Establishing drug class membershipIn ATC, the 5th-level drugs are linked to one or more4th-level classes. For example, ofloxacin is a member ofthe two Fluoroquinolones drug classes (J01MA andS01AE). For the purpose of comparing class-level ADEsto drug-level ADE, we require that the classes contain asufficient number of members. In practice, we excludeall drug classes with fewer than 4 drug members in ourset of drugs. In this proof-of-concept investigation, thisthreshold was selected as a trade-off between retaining asufficient number of classes and getting a meaningfulinterpretation of the characteristics of the drugs in theseclasses.Aggregating adverse event terms in MeSHADEs can be expressed at different levels of granularity.The MeSH hierarchy has multiple levels, enabling MED-LINE indexers to capture information at the appropriatelevel of granularity. However, for analytical purposes, itis useful to aggregate detailed ADEs into coarser ADEclasses, similarly to what we do for the drugs. We usedescriptors at the second level of the MeSH hierarchy foraggregation purposes. For example, we would aggregateTendinopathy (tree number C05.651.869) and Rhabdo-myolysis (C05.651.807) to the second-level descriptorMuscular Diseases (C05.651).Computing adverse event signals at the drug levelIn pharmacovigilance, safety signal detection consists inthe identification of an association between a drug and anadverse event (AE). In this study, we use the traditionalproportional reporting ratio (PRR) [25] in computing stat-istical associations for unique drug- and drug class-AEpairs. PRR is a simple disproportionality method for signaldetection that is easy to compute and sufficient in the con-text of this study. Based on the frequencies shown inTable 1, the PRR is defined as follows:PRR ¼ a= aþ bð Þð Þ= c= cþ dð Þð Þ ð1ÞWe calculate signals for all possible combinations ofdrugs and ADEs that co-occur in at least one MEDLINEarticle. We apply the usual zero-cell correction to tableswhere b or c is equal to 0 (by adding 0.5 to each countin the 2 × 2 table). For all pairs that do not co-occur inthe literature, we set the PRR to a neutral value of 1.Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 4 of 10Computing adverse event signals at the class levelAt the class level, we compute the signal using a similarapproach. For drug classes, we count articles mentioningany drug from this drug class (a and b) and articles men-tioning any other drug (c and d). For ADE classes, wecount articles with any ADE from this ADE class (a andc) and articles with any other ADE (b and d).Exploring ADE signals at different levelsWe want to determine whether the class signal isdriven by the strong signal of only a few drugs or isdistributed among all drugs from that class. To thisend, we visually explore the signal at different levels ofgranularity, from drug class-ADE class, to individualdrug-ADE class, to individual drug-individual ADE.Visual patterns reflect the contribution of the drugsignal to the class signal. We draw on the techniquespopularized by gene expression data studies, combiningclustering and heat map visualization [26], for explor-ing the relations between drugs and ADEs. We rely onthe R statistical software package (version 3.1.2) forimplementation. More specifically, we use hclust forclustering (using complete linkage and Euclidean dis-tance) and heatmap for visualization.Drug class-ADE class signalWe start by plotting all ATC4 drug classes against allADE classes, using the drug class signal. To reduce theamplitude of the PRR signal, we plot the logn transformof the PRR for all eligible class pairs. We perform hier-archical clustering on both drug classes and ADE classesto group pairs of drug classes and ADE classes withsimilar signals. On the resulting heat map, strong signalswill appear in white and yellow, while weak signals willbe displayed in red.Drug-ADE class signalWhile a low-resolution map is sufficient to identifystrong class signals and the corresponding broad ADEclasses, a higher resolution is required to investigate thedistribution of the class signal among the individualdrugs members. Starting from the strongest signalsFigure 1 Patterns of associations between members of drug class CD(d1,..,dobserved in the previous step for a given drug class(e.g., PRR above 10), we plot the signal for each drug inthe class. Here again, we perform hierarchical clusteringof both individual drugs and ADE classes (based on thedrug-level PRR, as opposed to the class-level PRR usedin the previous step). This heat map exhibits the distri-bution of the class signal among the individual drugmembers. In some cases, we see the emergence ofcharacteristic patterns illustrated in Figure 1:1. A solid column (vertical bar) with medium intensity(bright orange/ yellow) reflects an ADE (class) thatis equally distributed among all members of theclass, corresponding to a class property.2. Several incomplete, non-overlapping vertical bars indifferent columns, with medium intensity, reflectADEs (ADE classes) associated with subsets of theclass members, but not all members. This patterncorresponds to the properties of sets of individualdrugs, rather than the property of the class itself.3. Isolated spots or small islands of high intensityreflect associations between one drug (or few drugs)from the class and an ADE (class), corresponding toindividual drug properties.Drug-ADE signalFinally, to assess individual ADEs, we plot the drug-level signal for each ADE in the ADE classes present atthe previous step. As before, we perform hierarchicalclustering on both drugs and ADEs (based on the drug-level PRR). This heat map exhibits the distributionof the ADE class signal among the individual ADEs.Patterns similar to those described above can also beobserved.Automating the detection of class effectsWhile the visual approach provides an intuitive explor-ation of the ADEs within a drug class, its manual naturerestricts its large applicability. Here we propose an auto-mated approach to identifying class effects in the samedataset.n) and the manifestation of an adverse event class CE(e1,..,em).Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 5 of 10IntuitionIn case of a class effect, the PRRs are expected to behomogeneous among all drug members in a class for agiven ADE, and we should not be able to identify dis-tinct subgroups among them. Conversely, if we can iden-tify subgroups among the drugs, it means that the classsignal is driven by some drugs more than others, whichis not characteristic of a class effect.ImplementationFor a given drug class and ADE pair, we have computedthe class-level signal (as described in section 2.5) andthe drug-level signal for each drug in the class (asdescribed in section 2.4). Only classes with at least fourdrug members are considered. Because PRRs are propor-tions, we use their logn-transformed value to approach anormal distribution.To examine the distribution of the PRRs for individualdrugs in the class, we use k-means clustering withEuclidean distance to identify two clusters (k = 2) amongthe (logn-transformed) PRRs. We then compare themeans between the two clusters using Welch's t-test,which accommodates unequal variances in samples. Ofnote, in some cases, when the PRRs for all drugs in aclass are very similar, k-means clustering only producesa single cluster. In this case, we assume that this clusteris homogeneous by design. When we obtain only onecluster or when the hypothesis of a difference betweenthe means of the two clusters is rejected (p-value > 0.05),we conclude to a class effect.For example, the 4th-level ATC class selective sero-tonin reuptake inhibitors (N06AB) has a (logn-trans-formed) PRR of 4.30 for the ADE sexual dysfunctions.We partition the PRRs for the individual drugs into twoclusters: {fluoxetine (4.25), fluvoxamine (3.85)} and {ser-traline (3.68), citalopram (3.57), paroxetine (3.77), escita-lopram (3.57)}. There is no significant difference betweenthe means of the two clusters (p-value 0.28). Thus weconclude that all the individual drugs contribute to thesignal for the drug class, which is the characteristic of aclass effect.ResultsDrug-ADE datasetWe collected 189,800 MEDLINE articles, from which weextracted 371,417 drug-ADE pairs. The 244,692 MeSHdrug instances mapped to 1,966 distinct 5th-level ATCdrugs, and were aggregated into 598 4th-level ATC clas-ses, of which 261 had at least four drugs. The 282,691adverse event instances (3,043 distinct MeSH terms)were aggregated into 314 2nd-level descriptors in MeSH.The coarse matrix (Figure 2) reflects the associationbetween each of the 261 drug classes of interest and the314 ADE classes. The dataset used for our computationalapproach includes all the 3,043 individual ADEs for eachof the 261 drug classes under investigation (794,223 pairs).Visual approachDrug class-ADE class signalFigure 2 represents the heat map of 261 drug classesand 314 ADE classes, with drug classes in rows andADE classes in columns. Because of the large number ofclasses, the labels are not legible at this resolution. (Ahigh-resolution version of the heat maps is availableas Additional file 1). However, bright yellow spots orislands are clearly visible. For example, the yellow rect-angle right at the center corresponds to the associationbetween fluoroquinolones and various kinds of tendoninjuries. Isolated bright spots are equally interesting. Forexample, the strong signal between statins and musculardiseases is represented by a single bright spot.Drug-ADE class signalThe left part of Figure 3 shows examples of interestingpatterns. There is a solid bar for all members of thestatins class and the ADE class muscular diseases. Andthere is an incomplete column involving 8 of the 14members of the fluoroquinolones class for the ADE classtendon injuries. Isolated spots are also visible, forexample, between rosuvastatin and chronic fatiguesyndrome, and between fleroxacin and radiation injuriesand radiation-induced neoplasms.Drug-ADE signalThe right part of Figure 3 also shows examples of inter-esting patterns, with higher resolution than before. Forexample, the solid bar between the statins class and theADE class muscular diseases, visible on the left, isconserved, but we can now see that its signal is drivenby the specific ADE rhabdomyolysis.Computational approachOf the 794,223 pairs of (drug class, ADE), the largemajority correspond to cases where at least one of thedrugs in the class has no reported association with theADE in the pair. In the visual approach, we assignedsuch combinations a neutral PRR of 1 for display pur-poses, resulting in many red areas on the heat map. Inthe computational approach, however, we ignored suchcases, because we cannot distinguish between absence ofevidence and evidence of absence for the drug-ADEassociation. As a consequence, only 488 drug class-ADEpairs could be explored for class effect. The class PRRsfor these pairs ranged from 0.11 to 373.97 (before logntransformation), with 134 pairs having a PRR above 10and 214 pairs having a PRR above 5.The clustering process yielded two clusters in 457cases (93%) and a single cluster in 31 cases (7%). WhenFigure 2 Heat map of drug classes and ADE classes (based on the class signals).Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 6 of 10two clusters were identified, the difference between theirmeans was not significant in 337 (74%) and significantin 120 (26%) of the 457 cases. Of note, a significantdifference between the clusters does not necessarily ruleout the possibility of a class effect, because the averagePRRs may be high in both clusters.Examples of pairs with a single cluster include (cortico-steroids, femur head necrosis) and (fibrates, musculardiseases). Examples of pairs with two clusters betweenwhich no difference could be found include (tetracyclineand derivatives, tooth discoloration), (statins, rhabdo-myolysis) and (selective serotonin reuptake inhibitors,sexual dysfunctions, psychological). In many of the pairswith two significantly different clusters, the PRRs werehigh in both clusters, suggesting a class effect despitethe presence of two distinct clusters. For example, in thepair (other aminoglycosides, labyrinth diseases) the aver-age PRR is 57 in the first cluster (7 drugs) and over 350in the second cluster (2 drugs). While drugs from thesecond cluster (arbekacin and dibekacin) show a higherrisk of ototoxicity, the risk for the drugs from the firstcluster seems high enough (PRR = 57) for labeling oto-toxicity a class effect. In contrast, there are pairs withtwo significantly different clusters where the PRRs areFigure 3 Detailed heap maps for individual drug classes (based on the individual drug signals); a) Fluoroquinolones, ADE classes and drugs;b) Fluoroquinolones, ADEs and drugs; c) Statins, ADE classes and drugs; d) Statins, ADEs and drugs.Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 7 of 10high in one cluster and low in the other. For example, inthe pair (selective serotonin reuptake inhibitors, long QTsyndrome), only the drugs citalopram and escitalopramexhibit a high PRR (about 20), while other drugs fromthis class have low PRRs (e.g., sertraline and paroxetinehave PRRs between 1 and 2).DiscussionFindingsVisual approachUsing our visual approach to exploring ADEs at theclass level, we were able to uncover known associations,e.g., between fluoroquinolones and tendon injuries, andbetween statins and rhabdomyolysis. More specifically,exploring the signal at increasingly higher levels of reso-lution revealed a difference between fluoroquinolonesand statins. Although both drug classes exhibit a strongclass-level signal for their respective ADEs, only 8 of the14 individual fluoroquinolones showed an associationwith tendon injuries, while all statins were associatedwith rhabdomyolysis. This difference illustrates the dis-tinction between a class effect (statins), i.e., inherited byall members, and the property of a subset of the classmembers.Computational approachThe computational approached proposed here auto-mates the interactive strategy for exploring the classsignal introduced with the visual approach. The patternsdetected on the heat map (Figure 1) correspond to caseswhere all drugs from the class have roughly similar PRRs(solid bar), or where groups of drug with different PRRlevels can be found (incomplete bar or isolated spot).Translated into clusters for automated processing, theWinnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 8 of 10solid bar corresponds to a single cluster or two clusterswith similar PRR levels (no significant difference be-tween the clusters), while the incomplete bar corre-sponds to two distinct clusters with significant differencebetween their average PRR levels. For example, for the pair(statins, rhabdomyolysis), we found two clusters with nosignificant difference. In contrast, the pair (fluoroquino-lones, tendon injuries) was excluded from automatic pro-cessing, because association with tendon injuries had beenreported for only four drugs (ciprofloxacin, fleroxacin,pefloxacin and ofloxacin), while no information wasavailable for the other ten fluoroquinolones in this class(e.g., trovafloxacin). In this case, expertise is required todistinguish between less toxic drugs and drugs recentlymarketed for which no ADEs have been reported as ofyet. For this reason, a proper determination of classeffect could be suggested for only 488 pairs based onthe dataset we exploited.ApplicationsThe findings gained from our exploratory techniquesshould be of interest to the curators of ADE repositoriesand drug safety professionals. One drug safety issue hasto do with the information found in drug labels, whereADEs can be labeled in reference to a specific drug or toan entire class of drugs. For example, the drug label forcitalopram includes a warning for QT prolongation (notfound in other SSRIs, such as sertraline). In contrast, thelabel for minocycline refers to an ADE for its class:THE USE OF DRUGS OF THE TETRACYCLINECLASS DURING TOOTH DEVELOPMENT [] MAYCAUSE PERMANENT DISCOLORATION OF THETEETH. To make this determination, drug safety offi-cers must be able to access not only safety informationfor a given drug, but also safety information for theother members of its class. The approaches we proposehere support effective review of safety information in thecontext of drug classes.To assess the relevance of our determination of apotential class effect with respect to information foundin the FDA-approved structured package labels availableas part of DailyMed [27], one of the authors (AS) witha drug safety background reviewed the top-20 pairsselected by our computational approach. These pairs are20 of the 488 pairs with the highest class-level PRR(>40), for which 2 clusters had been identified, but nosignificant difference between the clusters had beenfound. These pairs included well-known class effectsmentioned in drug labels, including (tetracycline and de-rivatives, tooth discoloration), (statins, rhabdomyolysis)and (selective serotonin reuptake inhibitors, sexual dys-functions, psychological) and (selective serotonin reuptakeinhibitors, serotonin syndrome). In five cases, the ADE ismentioned for all the drugs in the class, but the druglabel does not make explicit reference to the class in thewarning. In six other cases, it was not possible to verifythe information because there was no label available forsome of the drugs in the class (e.g., drugs not marketedin the U.S.). Finally, the remaining cases included falsepositives, where an ADE known to be associated with agiven systemic drug was wrongly associated with topicalforms of the drug (because our underlying dataset doesnot contain information about routes of administration).Overall, these results suggest that, while potentiallyhelpful to drug safety officers for exploring ADEs fordrugs in the context of their classes, our approaches toidentifying class effect should only be used to supportdeterminations made by domain experts.Limitations and future workA vast majority of the drug class-ADE pairs explored byour computational approach ended up not being amen-able to class effect determination, because no ADE infor-mation was retrieved for at least one of the drugs in theclass. Our class definitions are based on ATC and in-cluded drugs not marketed in the U.S., which made itdifficult to compare this information with warnings con-tained in the drug labels from DailyMed. Restricting thedefinition of drug classes to U.S. marketed drugs wouldhave led to a more meaningful comparison with Dai-lyMed information. Moreover, having additional infor-mation about the drugs would allow us to distinguishbetween older drugs for which no ADEs have been men-tioned (i.e., evidence of absence for the ADE) and drugsmore recently marketed for which there has not beenenough time for collecting safety information throughcase reports (i.e., absence of evidence for the ADE).Also missing from our current approach is an assess-ment of the strength of evidence for the drug-ADE sig-nal based on study design. For example, randomizedclinical trials could be given preference over non-comparative observational studies and case reports [28] .However, because our dataset is extracted from the bio-medical literature, we could easily provide supporting in-formation, such the number of articles in which theADE is reported for the drugs, as well as the publicationtype (e.g., case report vs. clinical trial).We are aware that our dataset of drug-ADE pairsextracted from the biomedical literature is biased (e.g.,towards case reports). However, our approach is agnosticto the source used to derive the signal. In future work, weare planning to apply it to the data from the FDA AdverseEvent Reporting System (FAERS). We could also leveragenatural language processing (NLP) techniques to extractADE pairs from text. Advanced NLP techniques would beable to extract the polarity of ADEs (i.e., negated ADEs),helping to assess evidence of absence of ADEs.Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:18 Page 9 of 10The signal detection method used in this investigationis extremely simple and may not be as robust as dispro-portionality score algorithms developed more recently.For example, limitations inherent in the use of PRRinclude inability to account for temporal trends andconfounding by age, sex, or concomitant drugs [29].Here again, our approach is agnostic to the methodsused for signal detection and could easily be adapted tomore sophisticated scores.Finally, while aggregation plays a central role in our ap-proach, ATC and MeSH are not the only terminologiesthat can support aggregation. For example, the EstablishedPharmacologic Classes distributed by FDA together withthe Structured Product Labels may offer an alternativedrug classification system. Our method for aggregatingADEs in MeSH was limited to one level across all subdo-mains and would benefit from refinement. Also, termin-ologies such as MedDRA offer not only an alternative, butgroupings of ADEs across hierarchical structures.ConclusionsWe presented two complementary approaches to exploringthe contribution of individual drugs to the class signal forADEs. The visual approach supports the interactiveexploration of the class signal at increasing levels ofresolution. We showed that specific visual patterns inheat maps are associated with class effects. Additionally,we presented a computational approach, complemen-tary to the visual approach, meant to assess the classeffect over a wide range of drug classes and ADEs sys-tematically and automatically. In both cases, we wereable to find support for multiple known class effects.Some of our findings were difficult to corroborateagainst drug labels of DailyMed for a variety of reasons.Our approach can be applied to other drug-ADE data-sets, using various drug classification systems and signaldetection algorithms. The findings gained from our ex-ploratory techniques should be of interest to the cura-tors of ADE repositories and drug safety professionals.Additional fileAdditional file 1: High-resolution heat maps of drugs and ADEs atdifferent levels of granularity.Competing interestsThe authors declare that they have no competing interests.Authors contributionsRW and OB designed the visual and computational frameworks. RWimplemented them. AS provided domain expertise and evaluated the results.RW and OB wrote the manuscript. All the authors have read and approvedthe final manuscript.AcknowledgementsA preliminary version of this work was presented to the Vaccine and DrugOntology Studies (VDOS-2014) workshop. This work was supported by theIntramural Research Program of the NIH, National Library of Medicine (NLM).This work also received support from the US Food and Drug Administration(FDA) through the Center for Drug Evaluation and Research (CDER) CriticalPath Program [interagency agreement with NLM (XLM12011 001)] and fromthe Office of Translational Sciences at CDER. While conducting this research,RW was supported by an appointment to the NLM Research ParticipationProgram administered by the Oak Ridge Institute for Science and Educationthrough an interagency agreement between the U.S. Department of Energyand the National Library of Medicine. The authors want to thank AnaSzarfman, Rave Harpaz and Anna Ripple for useful discussions.DisclaimerThe findings and conclusions expressed in this report are those of theauthors and do not necessarily represent the views of the US Food andDrug Administration or the US Government.Author details1Center for Biomedical Informatics Research, Stanford University, Stanford,CA, USA. 2Center for Drug Evaluation and Research, US Food and DrugAdministration, Silver Spring, MD, USA. 3Lister Hill National Center forBiomedical Communications, National Library of Medicine, National Institutesof Health, Bethesda, MD, USA.Received: 6 December 2014 Accepted: 30 March 2015JOURNAL OFBIOMEDICAL SEMANTICSAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 DOI 10.1186/s13326-015-0029-xRESEARCH Open AccessA framework for ontology-based questionanswering with application to parasiteimmunologyAmir H. Asiaee1, Todd Minning2, Prashant Doshi1* and Rick L. Tarleton2AbstractBackground: Large quantities of biomedical data are being produced at a rapid pace for a variety of organisms. Withontologies proliferating, data is increasingly being stored using the RDF data model and queried using RDF basedquerying languages. While existing systems facilitate the querying in various ways, the scientist must map thequestion in his or her mind to the interface used by the systems. The field of natural language processing has longinvestigated the challenges of designing natural language based retrieval systems. Recent efforts seek to bring theability to pose natural language questions to RDF data querying systems while leveraging the associated ontologies.These analyze the input question and extract triples (subject, relationship, object), if possible, mapping them to RDFtriples in the data. However, in the biomedical context, relationships between entities are not always explicit in thequestion and these are often complex involving many intermediate concepts.Results: We present a new framework, OntoNLQA, for querying RDF data annotated using ontologies which allowsposing questions in natural language. OntoNLQA offers five steps in order to answer natural language questions. Incomparison to previous systems, OntoNLQA differs in how some of themethods are realized. In particular, it introducesa novel approach for discovering the sophisticated semantic associations that may exist between the key terms of anatural language question, in order to build an intuitive query and retrieve precise answers. We apply this frameworkto the context of parasite immunology data, leading to a system called AskCuebee that allows parasitologists to posegenomic, proteomic and pathway questions in natural language related to the parasite, Trypanosoma cruzi. Weseparately evaluate the accuracy of each component of OntoNLQA as implemented in AskCuebee and the accuracy ofthe whole system. AskCuebee answers 68% of the questions in a corpus of 125 questions, and 60% of the questionsin a new previously unseen corpus. If we allow simple corrections by the scientists, this proportion increases to 92%.Conclusions: We introduce a novel framework for question answering and apply it to parasite immunology data.Evaluations of translating the questions to RDF triple queries by combining machine learning, lexical similaritymatching with ontology classes, properties and instances for specificity, and discovering associations between themdemonstrate that the approach performs well and improves on previous systems. Subsequently, OntoNLQA offers aviable framework for building question answering systems in other biomedical domains.Keywords: Chagas, Natural language, Ontology, Parasite data, Question answering*Correspondence: pdoshi@cs.uga.edu1THINC Lab, Department of Computer Science, University of Georgia, Athens,GA, USAFull list of author information is available at the end of the article© 2015 Asiaee et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Asiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 2 of 25BackgroundNew biomedical data is increasingly housed in resourcedescription framework (RDF) triple stores such asVirtuoso [1] and AllegroGraph [2], annotated using richontology schemas and queried using an RDF query lan-guage called SPARQL [3]. The RDF data model has theadvantage of making the relationships between the dataitems explicit, and provides a straightforward way forannotating data using ontologies. An example of thisis the semantic problem solving environment for theimmunology of the parasite,Trypanasoma cruzi (T. cruzi),which utilizes an RDF triple store for hosting the par-asites genomic (microarray), proteomic (transcriptome)and pathway data [4]. The data is annotated using theparasite experiment ontology (PEO) and queried usingthe open-source Cuebee [5] that provides an interfacefor facilitating the parasitologists formulation of SPARQLqueries. Another example is the translational medicineontology and knowledge base [6], which utilizes the uni-fying ontology to annotate integrated genomic, proteomicand disease data, along with patient electronic records.The data may be browsed in a RDF triple store.Simple Web-based forms that allow choosing attributeshave been the user interface of choice for traditionalbiomedical relational databases [7]. To promote ease ofquerying, systems that utilize ontology-based RDF datahave experimented with various interfaces: iSparql [8],NITELIGHT [9] and BioSPARQL [10] facilitate for-mulating SPARQL queries by allowing the biomedicalscientists to browse ontology concepts and pinpoint a sub-graph that pertains to the question in his or her mind.GINSENG [11], a guided-input natural language searchengine, and Cuebee [5] progressively guide the scientistsby suggesting concepts and relationships that decomposethe question into a RDF triple based query, which is theninternally translated into SPARQL. The triples are in theform of subject ? relationship ? object where sub-ject and object represent ontology concepts. As Asiaeeet al. [12] notes, such guidance is tightly coupled tothe particular ontology structure, and query formulationrequires comfort with the structure otherwise the finalquery is unintuitive to the user.In this article, we introduce a novel framework, OntoN-LQA, for querying RDF data annotated using ontologies.The specific contributions of this framework are:1. It allows posing queries as natural language questionsthereby requiring minimal translation between thequestion in users mind and the computer query.2. We present a new approach for answering naturallanguage questions on structured data that combinesmachine learning with semantic computing: use ofexisting ontologies, their structure and annotateddata, and triple-based queries.3. OntoNLQA is applied in the context of parasiteimmunology. The resulting system called AskCuebeeallows parasitologists to pose genomic, proteomicand pathway questions in natural language related tothe parasite, T. cruzi, for the first time.4. AskCuebee automatically answers 68% of a corpus of125 questions in 5-fold cross-validation, and 60% ofthe questions in a previously unseen corpus. Thislatter proportion increases to 82.5 % if we allowsimple corrections by the user to the output of someof the components.OntoNLQA is significant due to two reasons: First, itimproves on the disadvantages of existing biomedicaldata retrieval systems. In a systematic evaluation, Asiaeeet al. [12] demonstrate the benefits and limitations ofexisting ontology-driven query formulation systems. Amajor limitation is that scientists using these systemsrequire an understanding of the ontology structure inorder to quickly formulate queries. For example, queriesmay require using intermediate concepts in the ontologywhen there is no direct relationship between the conceptsthat scientists have in mind.To illustrate, consider this question in the context of T.cruzi immunology using the parasite experiment ontology(PEO) [13]:Which researchers study Neomycin drug resis-tance? PEO formalizes the experimentation processesin parasite research. Figure 1 illustrates the connectionbetween the two concepts researcher and Neomycin drugresistance in PEO. Notice that study corresponds to apath that includes two ontology properties, has agent andhas output value, and an intermediate ontology class,sequence extraction, which is not stated in the question.Because questions may not explicitly state how the tar-get concepts are related, the scientists RDF query is tieddown to the structure of the ontology and this prob-lem exaggerates when multiple intermediate concepts areneeded.Second, and the more important motivation derivesfrom the fact that a capability to pose questions in plainlanguage is a natural way of obtaining answers. It involvesminimal effort expended toward translating the questionin the scientists mind to a query acceptable to the system,which includes the effort involved in acquainting with thequery interface. In our informal discussions with biomed-ical scientists, this capability was consistently identified asthe one that is most preferred.OntoNLQA seeks to automatically translate a questioninto RDF triples, and build a SPARQL query to retrieve theanswers from data stored in the RDF data model. Towardthis, the framework utilizes a design comprised of fivecomponents working in a sequence: The first two iden-tify important entities in the scientists question. Theseare constituent words that are nouns and verbs, andAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 3 of 25Fig. 1 In order to formulate a query for the question above, the scientist needs to relate the two concepts, researcher and Neomycin drug resistance,using the intermediate concept in the ontology, sequence extraction, that connects the two. Realizing how these are related requires anunderstanding of the ontology design and its structurerelate to the concepts and relationships in the domain.Accuracy is important here because words erroneouslydeemed important get carried forward through all thecomponents. The third component matches the entitiesidentified previously to classes and properties of the ontol-ogy. The last two components receive a set of ontologyclasses and properties, and find semantic associationsbetween the entities. These associations could be multi-ple paths comprised of classes and properties representedas sequences of RDF triples, which are translated intoSPARQL to query the RDF data.OntoNLQA is not specific to a domain with multiplestrategies and methods possible to realize each compo-nent. We instantiate this framework in the context of par-asite immunology, and develop a system called AskCuebeethat allows parasitologists to pose genomic, proteomicand pathway questions in natural language related to theparasite, T. cruzi. A significant amount of data includinginternal laboratory data sets, KEGG pathway data, andgenomic data on orthologs such as Leishmania major andTrypanosoma brucei from TriTrypDB [7] is available in aRDF store for querying. The data is annotated using PEO.We evaluate the accuracy of each component ofOntoNLQA as implemented in AskCuebee, and the accu-racy of the whole system.AskCuebee has been deployed in the Tarleton lab foruse in their day-to-day research and replaces a previoustraditional relational database system1.While the field of natural language processing haslong investigated the challenges of designing systems thatrespond to questions in natural language [1418], thesedo not make use of ontologies or the RDF data model.Few recent ontology-based retrieval systems [19, 20] allowqueries as natural-language questions and seek to extractsubject ? predicate ? object triples directly from theinput question using pattern matching. However, a sig-nificant limitation is that the extracted triples may notbe present as is in the ontology because the scientistsquestion may not be cognizant of the ontologys struc-ture. Furthermore, as we illustrated previously, entitiesin the question may not be directly related motivatingsophisticated ways of connecting them to form an intu-itive query. Consequently, a large subset of the questionsare challenging to answer automatically, thereby necessi-tating user involvement to refine the triples. For example,Aqualog [19] could not answer 42% of the questions inits corpus automatically resorting to manual interventionfor these questions. A small subset of the systems [11, 21]refine the question in real-time  as it is being typed  bysuggesting concepts and relationships from the ontologyto the scientist. These occupy a middle ground betweenthose that truly allow questions in natural language andthose in which queries are RDF triples.Our driving biomedical domain pertains to theimmunology and pathogenesis of the parasite T. cruziinfection, which causes the Chagas disease. This dis-ease was recently labeled the new HIV/AIDS of theAmericas [22]. About 7 million people, mostly in LatinAmerica, are infected with this disease. Data availablefor querying by AskCuebee was collected in order tostudy how immune control and maintenance occurredduring a T. cruzi infection and how it managed to avoidimmune clearance. Data on DNA cloning steps for geneknockout studies, on generation of gene knockout strains,whole-genome transcript abundances for the four life-cycle stages of T. cruzi, orthologous genes in relatedorganisms and protein identifications based on peptidespectra are all included as RDF data.Article outlineNext, in the Methods section, we describe the design ofOntoNLQA, discuss the details of each component andhow each component is utilized in AskCuebee in the con-text of the parasite, T. cruzi, immunology research. Wereport on the results of evaluating the methods employedin AskCuebee as well as demonstrate the performance ofeach component and the performance of the whole sys-tem in the Results and discussion section. We also discussthe contributions and limitations of our framework basedon our evaluation results in this section. We present acomprehensive review of related work with a focus onontology-based retrieval systems in Related Works. Weconclude the article by summarizing our approach, mainfindings and providing thoughts on future directions inthe Conclusion section.MethodsOntoNLQA presents a new approach for answering ques-tions posed in natural language to RDF data annotatedusing an ontology. We begin by providing an overviewof the framework followed by details on each compo-nent and how it is applied in the context of a drivingAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 4 of 25biological domain, as well as its evaluation. As we discussbelow, multiple alternatives present themselves for real-izing each component of the framework, and we discusstheir benefits and limitations.Overview of OntoNLQABriefly, our approach in OntoNLQA is to identify theimportant entities present in the question, which are thenfound in the ontology and semantic associations betweenthe entities in the ontology are discovered. This approachencounters three main challenges:1. OntoNLQA needs to parse the question and identifythe important entities;2. It must find the ontology classes, properties andinstances (data) in the ontology(ies) that correspondto the identified entities; and3. Find semantic associations involving the ontologyclasses and properties, which need not be on a singledirected path. Express these in the form of RDFtriples that are translated into a computational queryfor the RDF data.While these challenges are common to some ofthe previous ontology-based natural language systems,OntoNLQA differs in its approach toward addressing them.These challenges suggest a pipeline of operations on thedata beginning with the question in natural language, asillustrated in Fig. 2. On receiving a question in naturallanguage, OntoNLQA performs linguistic pre-processing ofthe question during which punctuation symbols, quota-tion marks, parenthesis and any other character in thequestion generally deemed to be irrelevant to extractingthe important information, are filtered out. This resultsin a processed question. Words and phrases relevant tothe domain and of import to understanding the questionare deemed as important entities and extracted from theprocessed question by utilizing entity recognition tech-niques. These entities are then found in the ontology usinglexical matching. Ontology classes matched to the enti-ties form the end points of any semantic associations thatare additionally constrained to include matched ontologyproperties, if any. These associations are represented as asequence of RDF triples, which are then transformed intoSPARQL queries that retrieve the answer.Operations on the data in Fig. 2 are performed bythe components of the system. Subsequently, OntoNLQAis composed of five components as we show in Fig. 3.The first two components, which include linguistic pre-processing (box annotated 1 in Fig. 3) and entity recogni-tion (box annotated 2) address the first challenge, whichis similar to the well-known problem of named entityrecognition [23]. Our primary goal in extracting entitiesis to match them with their corresponding ontology ele-ments. Therefore, the labels in our context is a set ofontology classes and properties. A third component (boxannotated 3) matches each extracted entity from the pre-vious component to a specific ontology class, property orinstance.This component addresses the second challenge of find-ing corresponding ontology elements for the identifiedentities.The final two components handle the challenge of find-ing relationships between the ontology elements, repre-senting them as RDF triples, and translating these intoa computational query. Semantic association discovery isnontrivial when more than two ontology elements need toFig. 2 An illustration of the flow of data in OntoNLQA emphasizing the operation performed on the data at each step. Dotted lines show theoperation on the data. For example, lexical matching gives the ontology classes, properties and instances similar to the extracted entities. The directionof the arrows denotes the direction of flow of the dataAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 5 of 25Fig. 3 The design of OntoNLQA involving five general components that operate on the scientists question to eventually obtain the answerbe related (box annotated 4). Discovered semantic associ-ations may be represented as RDF triples. These are usedin generating a computational query for the RDF data bythe query formulation and answer retrieval component(box annotated 5).Components of OntoNLQA and their Implementation inAskCuebeeIn this subsection, we describe the components of theframework in detail. For each, we discuss various meth-ods for realizing the components functionality, whichmaybe beneficial in different contexts, and its utilization inAskCuebee.We apply OntoNLQA to the context of T. cruzi par-asite immunology data as illustrated in Fig. 4. We callthis application, AskCuebee (boxes annotated 2 and 3),which is assisting parasitologists at the Center for Trop-ical and Emerging Diseases at the University of Georgia,and their collaborators worldwide. The parasite, T. cruzi,is the agent of Chagas disease in humans. This disease isprevalent throughout Latin America and is often fatal.Linguistic pre-processingAll questions undergo linguistic pre-processing to filterconstituents that are not key toward a computationalunderstanding of the question. This pre-processing commonly utilized inmany question-answering systems is generally known to improve the accuracy of detectingimportant entities. The pre-processing starts with tok-enization: breaking down the string of characters intowords, phrases, symbols, or other meaningful elements.This is followed by removing stop words such as theFig. 4 OntoNLQA is a general framework and its realization in the context of our driving biological problem is called AskCuebeeAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 6 of 25definite articles, to, was, and many others. Standardlists of stop words are available [24]. In addition, punctu-ation symbols are removed, abbreviations are expanded,and comparative relationships in words are identfied usinggrammar dependencies [25] and parts of speech tag-ging [26], and replaced by their mathematical symbols; forexample, greater than 1 is replaced by > 1.The accuracy of linguistic pre-processing may beenhanced by using domain-specific lexicons or dictio-naries such as UMLS or MeSH, if these are relevant,though its use should be considered carefully due tothe concomitant increase in run time [27]. Much of thepreviously mentioned functionality for pre-processing isavailable in free computer applications such as StanfordCoreNLP [28], LingPipe [29] and OpenNLP [30].StanfordCoreNLP in AskCuebee Each question posedby the user is viewed by the system as a string of charac-ters. Therefore, common operations such as tokenization,extracting the roots of words (stemming), and removingthe punctuation symbols are essential. AskCuebee per-forms these using the standard operations found in theStanford CoreNLP library [28]. Furthermore, consider thefollowing two example questions:1. Show me the 3 prime forward sequences for all genesin metacyclic stage with log2 ratio higher than 1 andstandard deviation below 0.5.2. Which protein group numbers have spectral valuesbetween 40 and 50?In question (1), notice that while there are three num-bers mentioned, two of these are involved in comparativerelationships, 1 and 0.5. Thus, the comparative relation-ships we seek to identify are log2 ratio > 1 and standarddeviation < 0.5. In question (2), the comparative relation-ships are more complex as two relationships are combinedinto one using a conjunction. Therefore, we seek to extracttwo relationships, spectral values > 40 and spectral values< 50. These questions illustrate that we additionally needconversions between numbers and text, and extractionof comparative relationships. Both these require com-plex operations that include part-of-speech tagging suchas detecting the nouns, verbs and identifying grammardependencies, which are provided by Stanford CoreNLP.In addition, we detect abbreviations from a list that wemaintain.We introduce a simple method that uses dependencygrammar to detect the majority of the comparative rela-tionships. The first step is to detect the comparativephrases in the question and transform them into dis-tinct patterns. For example, standard deviation below 0.5from question (1) is transformed to standard deviationless than 0.5 and spectral values between 40 and 50 fromquestion (2) is converted to spectral values greater than40 and spectral values less than 50. Next step converts thedistinct patterns into a computational form by identifyingthe operands (standard deviation and 0.5) and operators(less than). Again, a dependency grammar is combinedwith part-of-speech tagging to create rules for detectingoperands and operators.Entity recognitionGiven the processed question, this component in theframework is tasked with identifying and labeling enti-ties that are relevant to obtaining the answer. Severalapproaches may be used toward entity recognition.These include supervised learning  a branch ofmachine learning  that utilizes statistical models for thetask. A classifier is trained using a large corpus of datarecords, each of which is labeled with the target entitynames. Entities in new data records are then identifiedand labeled by the classifiers. Potential classifiers includethe hidden Markov model [3134], maximum-entropyMarkov model [35, 36], support vector machines [37], andconditional random fields [38], all of which have beenutilized for entity recognition. Among these, conditionalrandom fields have distinguished themselves with theircomparatively more accurate performance [3941].Supervised learning usually requires a large trainingcorpus to learn a classifier that performs well. In theabsence of large data sets, the alternative method of semi-supervised learning uses a small collection of data to trainan initial classifier, which is then used to label new andpreviously unseen samples of data. These labeled data aresubsequently utilized to retrain the classifier. A commontechnique for semi-supervised learning is bootstrapping,which requires a small set of seed supervised data for theinitial learning [42].Other approaches not based on machine learning relyon dictionaries and rules. A simple approach is to locatelexically similar dictionary terms for each potential entityin the question [4346]. The approaches differ in how theysearch the dictionary with some using BLAST [47], andthe data sets that constitute the dictionary. For example,Krauthammer et al. [43] utilizes GeneBank as the dic-tionary. Alternately, general rules in the form of stringpatterns may be available. If a rule is satisfied by a termand its context in the question, the corresponding label isused to annotate the term [4851].Between the different approaches for entity recogni-tion, machine learning methods are currently receivingincreased attention in general [52, 53]. Regardless of semi-or fully-supervised methods, we need a set of labels forentity recognition. Presence of a domain ontology pro-vides a natural source for these labels. In this regard, animportant consideration is the number of labels that areneeded, which is often proportional to the size of the dataAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 7 of 25set. A large data set may permit better discrimination andtherefore more labels. On the other hand, a smaller dataset necessitates fewer labels. In this case, we may selectontology classes and properties that appear at a higherhierarchical level in the ontology graph. Let CO denotethis set from ontology, O. Such labels tend to be general,and each is useful toward annotating several terms in thequestion.Conditional random fields for entity recognition inAskCuebee Dictionary-based methods require domain-specific dictionaries. While substantial overarching dic-tionaries for biomedicine such as UMLS and MeSH areindeed available for use, these are not designed to be spe-cific to any particular organism. Biomedical ontologies,if available, serve to provide another source of dictionaryterms usually specific to a domain. In addition to findinga dictionary relevant to the domain of interest, a limita-tion of this approach is that dictionary look up could getexpensive if the dictionary is very large and unindexed. Onthe other hand, machine-learning based supervised clas-sification may need large training data in order to achievereasonable performance.Among supervised learning methods, conditional ran-dom fields (CRF) [38] demonstrate superior performancefor biomedical entity recognition. For example, CRFs wereutilized by the best performing system on the i2b2 medi-cal concept extraction task [41], by highly ranked systemson BioCreAtIve gene mention recognition tasks [39, 40](9 of 19 highest ranked systems use CRFs) and on JNLPBAbioentity recognition task [54]. This motivates consider-ing CRFs in AskCuebee as well. We briefly review CRFs inAppendix A.AskCuebee employs a linear-chain CRF and a pop-ular quasi-Newton method called limited memoryBroyden-Fletcher-Goldfarb-Shanno [55] for optimizingparameters. The parameters are the feature weights, ?j, ina CRF.Critical to the performance of CRFs is finding a setof feature functions. The simplest features of a naturallanguage question are the word tokens themselves. Inaddition, AskCuebee uses four different types of featuresfor training CRFs: orthographic, word shape, dictionaryand contextual features: Orthographic features: Biomedical entities oftenshare orthographic characteristics. These consist ofcapitalized letters, include digits and possibly somespecial characters as well. Thus, such features are notonly useful in detecting various types of biomedicalentities but are easily implemented using patterns orregular expressions. Appendix A includes a list of theorthographic features utilized in AskCuebee. Word shape: Words annotated with the same entitylabel may have the same shape. For example, a type ofabbreviation may not have numerical digits and geneIDs are a combination of digits and letters. Contextual features: These features take into accountthe properties of preceding and following tokens for acurrent token in order to determine the target label. Dictionary features: For each noun or verb phrase inthe input question we calculate their similarity scoreswith all ontology elements. If the highest similarityscore is higher than a threshold (for instance, 0.6), wefind the upper-level class or property of that specificontology element that is a training label. Then, weactivate a dictionary feature for the identified traininglabel. This feature is useful when the target entitiesbelong to more than one label.Ontology elementmatchingEntity labels are ontology classes and properties, whichcould be general and appear at the higher levels of theontology hierarchy. However, the RDF data annotated bythe ontology is often linked to more specific classes andproperties. Consequently, we may search the ontology formore specific matches with the recognized entities in thequestions. If an entity, e, is associated with a label, c ? CO,where CO is the set of all classes and properties in ontol-ogy, O, then, let Sc be the set of subclasses and propertiesin the ontology hierarchy rooted at c. Labeling the entitywith c allows us to limit our search for a more specificmatch to the elements of Sc. Importantly, this reduces thecomputational expense when the whole ontology may bevery large as is often the case with biomedical ontologies.A suitable approach for the matching is to use textsimilarity measures to calculate the degree of similaritybetween an entity and a specific ontology class or prop-erty. A similarity measure scores the degree of similaritybetween two text phrases by viewing them as sequences ofcharacters. Common measures that are utilized include: ISUB similarity [56] designed for aligningontologies [57]. This method identifies the longestcommon substring and records its length. It removesthis substring and searches for the next longestsubstring until no common substring is identified.The sum of the common substrings scaled with thelength of the original strings is the commonality inthe two strings. ISUB subtracts this commonalityfrom the difference of the two strings. The differenceis based on the length of the unmatched substrings. Levenshtein-based similarity (also known asNeedleman &Wunsch) [58] uses the Levenshteindistance [59] to determine the similarity of twosequences of characters. It calculates the bestalignment between two sequences of characters asAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 8 of 25the fewest number of mutations necessary to buildone sequence from the other. Smith and Waterman based similarity [60] looks forlongest common substrings between two phrases,and based on that produces the degree of similarity.This measure is similar to Needleman-Wunsch, andis commonly used in BLAST for aligning genome andprotein sequences. Cosine-based similarity [61] is a widely reportedmeasure for similarity between two vectors. Thismeasure models phrases as vectors of characters andcalculates the cosine between the two vectors. Thisprovides a score that is interpreted as the degree ofsimilarity between two chunks of texts. Jaccard-based similarity [62] calculates the degree ofsimilarity of two phrases by calculating the size of theset of intersection of the terms in the two phrasescompared to the size of the set of union of the terms.No particular measure in the above list dominates anyother measure in performance. Subsequently, we mayevaluate all of them for use in domain-specific systemssuch as AskCuebee. Classes and properties in Sc thatmatch sufficiently well with the entity, e, become a part ofthe candidate list. Based on the cardinality of the candi-date list, three situations arise as discussed below:Case (1): In the straightforward case where the candi-date list has only one member, the matched subclass orproperty is retained.Case (2): If the candidate list has multiple members, weneed to retain one among them. Here, we may considerthe context: the other entities identified in the questionand how each candidate relates with the ontology classesand properties that label the other entities. For example,we may rank order the candidates based on how manydirect paths each has with the other labels found in theontology. We may retain the candidate with the mostpaths, which is indicative of contextual consistency.Case (3): Finally, the candidate list could be empty.Because none of the ontology subclasses or propertieswere a close lexical match, our next step is to identify amatch in the RDF data. We may lookup the rdfs:type ofthe matched instances in the data set to obtain the corre-sponding ontology classes or properties. If multiple suchclasses obtain, the candidate list has multiple membersrequiring the approach in case (2) above to retain one.Semantic association discoverySpecific ontology classes and properties that label theidentified entities in the question now need to be relatedto each other. Two ontology elements have a seman-tic binary relationship if a directed or undirected pathconnects them in the ontology graph. However, scien-tists questions often include multiple entities. OntoNLQAdiffers from previous systems in how it handles this sit-uation. We must find an n-ary semantic relationshipbetween all of them. While pairwise binary relationshipsmay be found between each pair of labels, these pathsmust be linked with each other.An approach to relating them is to find the lowest com-mon ancestor (LCA). This is the ontology class that isthe ancestor of each entity label. An ancestor is any classthat lies on the path from the root of the ontology tothe label class. If there are multiple such common ances-tors, we pick the one that is most specific and is thereforelowest in the hierarchy. This ancestor would coincidewith Resniks most informative common ancestor [63] ifattention is limited to just the subclass taxonomy of theontology. However, the latter requires finding the proba-bilities of ontology classes typically using term frequenciesin domain texts. Furthermore, the LCA may be differentwhen named properties in an ontology are considered.An an illustration, consider Fig. 5 which shows thesemantic relationship between labels Cell Cloning andGene that appear in PEO. The binary relationship betweenthese two labels is a direct path in PEO. In this path, thereare several intermediate ontology concepts such as drugselection and transfection (marked differently), which area part of the relationship. Of course, the length of suchpaths depends on the design and structure of the partic-ular ontology. As there is one pair only in this example, asingle path is sufficient to obtain the semantic associationbetween the two labels.The graphs in Figs. 6 and 7 consider examples fromquestions containing more than two identified entities,resulting in more complex semantic associations betweenthe ontology elements. In Fig. 6, we are looking for seman-tic relationships between five prime forward region, spec-tral count and proteome analysis concepts, which werechosen as entity labels. This n-ary relationship may notbe a single path between the elements. However, thereare pairwise paths between each pair of the ontology ele-ments. Notice that proteome analysis is present on allthese paths and is the LCA.As a third example, consider the ontology classes, gene,spectral count, pathway, and experiment justification inFig. 7. It is straightforward to find the pairwise pathsbetween the classes in the ontology subgraph. However,unlike the previous example, none of these include a com-mon ancestor. As we show in Fig. 7, the LCA, process, isan intermediate concept and does not belong to the set oflabels for the entities in the question.In order to find semantic associations between multipleontology classes or properties, we discuss two methodsbelow:Method 1: Semantic association discovery based onthe LCA Notice that the presence of an LCA for theAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 9 of 25Fig. 5 The semantic path between the ontology concepts CellCloning and Gene ID Tc00.1047053509463.30. The lowest commonancestor is Cell Cloningmatched classes or properties in an ontology provides away to obtain the semantic association between them.From the LCA, we may obtain the shortest path thatconnects the LCA to each ontology class while includingany identified property. Consequently, we obtain multiplepaths each of which has the LCA at one end.This motivates finding an efficient way to compute theLCA. In Appendix B, we discuss an offline approach thatprecomputes the LCA for each pair of classes in theontology at hand and is currently the fastest. Wemay thensimply look up the LCA table to find all LCAs for everyontology entity pair. This process continues recursivelyuntil we identify a single LCA for all of the entity labels.Figure 8 illustrates this recursive algorithm. Note that thisrecursive procedure iterates over all LCAs for every pairuntil one of them leads to the final solution. If the algo-rithm fails to find any LCA for the entities, it concludesthat there is no semantic association between the ontologyclasses. In order to find the shortest path from the LCA toeach ontology class in the set of entity labels, we may usebidirectional search [64] to speed up the path finding.Method 2: Semantic association discovery based onpath finding An alternative approach for finding seman-tic associations is based on path queries. For example,SPARQL 1.1 provides facilities to find a path between twoelements in RDF data. We may use these path-findingqueries to find the semantic paths betweenmultiple ontol-ogy classes and properties. We present a simple methodthat includes finding all the paths between the ontol-ogy elements and selecting a common node among thesepaths. Specifically, We begin by finding pairwise paths: these are pathsbetween every pair of ontology elements in the set oflabels. We sort them based on their length inascending order. Note that multiple paths may exist between a pair ofontology elements. We create a set,{allPairwisePaths}, that contains sets of all thepairwise paths between every pair of the elements. In the next step, a Cartesian product of the sets inallPairwisePaths is obtained. Each member of theproduct set is itself a set of pairwise paths between allthe ontology elements. For each member of the product set, we identify anontology class that is common to all the paths, ifavailable, and store these common classes in a set,CommonNodes. Finally, this approach selects a class in the set,CommonNodes, that has the shortest paths to theontology elements.Both the above methods result in semantic paths, whichare then converted into sequences of RDF triples in astraightforward way.Association discovery in AskCuebee In order to dis-cover the associations between the matched PEO classesand properties, OntoNLQA suggests either precomputingthe LCA or running path queries between each pair ofmatched ontology elements and finding their intersection.While the former has an offline step of precomputing theAsiaee et al. Journal of Biomedical Semantics  (2015) 6:31 Page 10 of 25Fig. 6 This graph shows the semantic paths between the ontology concepts, five prime forward regions, proteome analysis, spectral value> 40 andspectral value< 50. The common node between all is proteome analysis, which forms the LCALCA between all pairs of classes in the ontology, the latteris fully online. We evaluate the two approaches and selectone for inclusion in AskCuebee.Query formulation and answer retrievalThe final component of OntoNLQA translates RDF triplesinto a computational query in the language of SPARQL.This translation is straightforward because the RDFtriples directly represent SPARQL graph patterns.If the RDF triple sequences constituting the semanticpaths need to be displayed, we may utilize any modal-ity including simply showing the sequences or markingthem on the ontology graph and displaying the subgraph.As an example, we may utilize the display of RDF triplesequences by a system such as Cuebee [5].The SPARQL query is then sent to any query endpointsuch as OpenLink Virtuoso [1], OpenRDF Sesame [65] orAllegroGraph [2], all which allow storing large amountsof annotated RDF data and query it using SPARQL. Theanswers may be displayed to end users in a tabular or anyother visual format depending on the context and scientistJOURNAL OFBIOMEDICAL SEMANTICSGroza et al. Journal of Biomedical Semantics  (2015) 6:21 DOI 10.1186/s13326-015-0008-2RESEARCH ARTICLE Open AccessCapturing domain knowledge frommultiplesources: the rare bone disorders use caseTudor Groza1*, Tania Tudorache2, Peter N Robinson3 and Andreas Zankl4AbstractBackground: Lately, ontologies have become a fundamental building block in the process of formalising and storingcomplex biomedical information. The community-driven ontology curation process, however, ignores the possibilityof multiple communities building, in parallel, conceptualisations of the same domain, and thus providing slightlydifferent perspectives on the same knowledge. The individual nature of this effort leads to the need of a mechanismto enable us to create an overarching and comprehensive overview of the different perspectives on the domainknowledge.Results: We introduce an approach that enables the loose integration of knowledge emerging from diverse sourcesunder a single coherent interoperable resource. To accurately track the original knowledge statements, we record theprovenance at very granular levels. We exemplify the approach in the rare bone disorders domain by proposing theRare Bone Disorders Ontology (RBDO). Using RBDO, researchers are able to answer queries, such as: What phenotypesdescribe a particular disorder and are common to all sources? or to understand similarities between disorders based ondivergent groupings (classifications) provided by the underlying sources.Availability: RBDO is available at http://purl.org/skeletome/rbdo. In order to support lightweight query andintegration, the knowledge captured by RBDO has also been made available as a SPARQL Endpoint at http://bio-lark.org/se_skeldys.html.Multiple perspectives over the same domainOntologies represent a formalised description of the con-cepts and relationships in a domain. For example, they canbe used to model and capture knowledge around a par-ticular set of hereditary disorders (e.g., bone dysplasias),in addition to their underlying genetic mechanisms (relat-ing disorders to genes) and their observable traits (relatingdisorders to phenotypes). Over the course of the pastdecade, they have become one of the main mechanismsused in building intelligent systems and algorithms to sup-port, among others, the study of cross-species phenotypenetworks [1-3], gene screening, prediction and prioritiza-tion [1,4] or disorder prediction [5,6]. This rising adop-tion, in particular by the biomedical community, has ledto the proliferation of the number of ontologies publishedopenly via repositories such as the NCBO BioPortal [7]or the Open Biological and Biomedical Ontology (OBO)*Correspondence: t.groza@garvan.org.au1School of ITEE, The University of Queensland, St Lucia, AustraliaFull list of author information is available at the end of the articleFoundry [8]. An effect of this uptake has been the need fora community-driven process [9], which requires a sharedunderstanding of the rules and primitives that govern thedomain under scrutiny. More concretely, experts requiredappropriate mechanisms to update and evolve ontologiesand the domain knowledge, in order to ensure an accurateknowledge transfer. A relevant example of such collab-orative knowledge curation is the development of theInternational Classification of Disorders (ICD-11) [10], orefforts like the Gene Ontology [11], the Human Pheno-type Ontology [12] or the International Classification forNursing Practice (ICNP) [13].Recognising the need for community-oriented knowl-edge curation does not, however, take into account thatmultiple communities may target, in parallel, the concep-tualisation of the same domain. This, in turn, leads toontologies that provide slightly different perspectives onthe same knowledge. These perspectives differ in:1. focus  the particular aspects of the domain  e.g.,given a particular set of disorders, a community may© 2015 Groza et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Groza et al. Journal of Biomedical Semantics  (2015) 6:21 Page 2 of 15focus more on the genetic mechanisms, whileanother one on the clinical presentation,2. granularity  the level of detail considered in theknowledge modelling process  e.g., given the samecontext, the knowledge captured by a communitymay include the prevalence of the disorders or theage of onset, while in other cases this knowledge maybe omitted, or3. in the underlying interests or priorities of thosecreating them  for example, a community may justbe interested in the clinical presentation of thedisorders because of its focus on clinical diagnosis.We are currently missing a key mechanism to allowus to create a comprehensive overview of the differentperspectives on the domain knowledge.Ontology integration is a field that has been extensivelystudied in the past. Although several definitions of ontol-ogy integration exist in the literature [14], it is usuallyreferred (as is in our case) to the process of combining twoor more ontologies about the same subject into a singleunified ontology. One step in the integration process is tofind the correspondences (a.k.a., mappings) between thesemantically-related entities, which can be done manu-ally or (semi-automatically). The ontology-matching topicthat has seen an enormous amount of research, as shownby several comprehensive and thorough surveys that areavailable in the literature [15-18]. All these approachesare focused on trying to identify in a (semi-)automaticmanner the correspondences between entities in differ-ent ontologies. This is not the focus of our work, as themappings between the two resources were identified in amanual fashion by a domain expert. Even more, existingwork on ontology integration does not keep track of theprovenance of the integrated entities. This novel aspect ofour work brings several benefits, which are discussed laterin the paper.In the biomedical domain, many ontologies createJOURNAL OFBIOMEDICAL SEMANTICSSuzuki et al. Journal of Biomedical Semantics  (2015) 6:30 DOI 10.1186/s13326-015-0028-yRESEARCH Open AccessDevelopment of an Ontology forPeriodontitisAsami Suzuki1,2, Takako Takai-Igarashi3,1*, Jun Nakaya3,4 and Hiroshi Tanaka3,1AbstractBackground: In the clinical dentists and periodontal researchers community, there is an obvious demand for asystems model capable of linking the clinical presentation of periodontitis to underlying molecular knowledge. Acomputer-readable representation of processes on disease development will give periodontal researchers opportunitiesto elucidate pathways and mechanisms of periodontitis. An ontology for periodontitis can be a model for integrationof large variety of factors relating to a complex disease such as chronic inflammation in different organs accompaniedby bone remodeling and immune system disorders, which has recently been referred to as osteoimmunology.Methods: Terms characteristic of descriptions related to the onset and progression of periodontitis were manuallyextracted from 194 review articles and PubMed abstracts by experts in periodontology. We specified all the relationsbetween the extracted terms and constructed them into an ontology for periodontitis. We also investigated matchingbetween classes of our ontology and that of Gene Ontology Biological Process.Results: We developed an ontology for periodontitis called Periodontitis-Ontology (PeriO). The pathologicalprogression of periodontitis is caused by complex, multi-factor interrelationships. PeriO consists of all the requiredconcepts to represent the pathological progression and clinical treatment of periodontitis. The pathological processeswere formalized with reference to Basic Formal Ontology and Relation Ontology, which accounts for participants in theprocesses realized by biological objects such as molecules and cells. We investigated the peculiarity of biologicalprocesses observed in pathological progression and medical treatments for the disease in comparison with GeneOntology Biological Process (GO-BP) annotations. The results indicated that peculiarities of Perio existed in 1)granularity and context dependency of both the conceptualizations, and 2) causality intrinsic to the pathologicalprocesses. PeriO defines more specific concepts than GO-BP, and thus can be added as descendants of GO-BPleaf nodes. PeriO defines causal relationships between the process concepts, which are not shown in GO-BP. Thedifference can be explained by the goal of conceptualization: PeriO focuses on mechanisms of the pathogenicprogress, while GO-BP focuses on cataloguing all of the biological processes observed in experiments. The goalof conceptualization in PeriO may reflect the domain knowledge where a consequence in the causal relationshipsis a primary interest. We believe the peculiarities can be shared among other diseases when comparing processesin disease against GO-BP.Conclusions: This is the first open biomedical ontology of periodontitis capable of providing a foundation for anontology-based model of aspects of molecular biology and pathological processes related to periodontitis, aswell as its relations with systemic diseases. PeriO is available at http://bio-omix.tmd.ac.jp/periodontitis/.Keywords: ontology, periodontitis, biomedical process, bone remodeling, Gene Ontology, osteoimmunology* Correspondence: takai@megabank.tohoku.ac.jp3Tohoku Medical Megabank Organization, Tohoku University, Sendai, Japan1Department of Computational Biology, Graduate School of BiomedicalScience, Tokyo Medical and Dental University, Bunky?, JapanFull list of author information is available at the end of the article© 2015 Suzuki et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Suzuki et al. Journal of Biomedical Semantics  (2015) 6:30 Page 2 of 13BackgroundBiological high throughput analysis generates hugeamounts of biomedical data that can be used for investi-gating disease mechanisms, and semantic technologiesare therefore expected to contribute to the effective useof these data. Many biomedical ontologies such as theGene Ontology (GO), the Disease Ontology (DO), theOntology for General Medical Science (OGMS), theHuman Disease Ontology (DOID) and the InfectiousDisease Ontology (IDO) have been developed to providesupport for sophisticated biomedical information sys-tems [1, 2]. These ontologies are providing a means forthe consistent representation of scientific data and thedomain entities made possible by these data [3].Disease has been one of the major targets for ontol-ogy development. The Unified Medical Language Sys-tem (UMLS) [4] and the Medical Subject Headings(MeSH) , a source vocabulary included in UMLS, [5]are long established thesauri that explicate numerousmedical terms. For many decades, the World HealthOrganization has provided International Classificationof Diseases (ICD) [6]. Terminologies and ontologies inbiology and medicine have also been reviewed byFreitas et al. [7]. Meanwhile, large-scale genomic projects,including the SNP consortium [8], the ENCODE pro-ject [9], the NIH Knockout Mouse Project [10], theWelcome Trust Case Control Consortium [11], and the1000 Genome Project [12], have tried to cataloguecomprehensive relationships between genes and dis-eases. These data collections require the developmentof ontologies that integrate genes with clinical out-comes [1320]. Masci et al. created comprehensivedefinitions of dendritic cells in order to distinguishmany derivatives of dendritic cells according to the pro-gression of immune responses [20]. Mungall et al. investi-gated ontological mapping of mutation phenotypes/diseases across species [14]. Rubin et al. integratedexisting ontologies for neuronal connectivity in order toexplicate abnormalities of neuronal diseases systematically[16]. Feltrin et al. and Lindeberg et al. expanded GO inorder to explicate muscle biology and plant pathology byadding specifications of pathological disorders and muta-tional phenotypes, respectively [1719].GO is an established ontology that consists of the fol-lowing three sub-ontologies: Cellular Component (CC),the parts of a cell or its extracellular environment; Mo-lecular Function (MF), the elemental activities of a geneproduct at the molecular level; and Biological Process(BP), operations or sets of molecular events with adefined beginning and end, pertinent to the functioningof integrated living units such as cells, tissues, organs,and organisms. While GO classifies internal processesin any biological phenomenon with external links toentries in the databases of genes by relationships ofassociated-to, a specific relationship to GO [21,22],disease-centered ontologies (DO, OGMS, DOID, andIDO) only describe relationships between the processesand external perturbations, including pathogens, drugs,environmental factors, and medical devices, on thediseases. As many bio-medical researchers strive tounderstand diseases in the context of networks andpathways in order to realize better and personalizeddiagnoses and treatments in clinical medicine, molecu-lar interpretations of both the external causes and theinternal processes of disease have demanded biologicalhigh throughput analyses in order to elucidate molecu-lar mechanisms of pathogenesis and progression. GO iscurrently used in biological high throughput analysesby detecting overrepresented GO terms in case groupsrelative to a control group [2325]; however, we do notconsider that GO covers all the required internal pro-cesses of disease, because internal disease processesmay be different from the processes observed underhealthy conditions. The difference can be observed notonly in the classification of the internal processes, butalso in the relationships between the internal processesand their participants and activators and/or suppressors tothese processes [26]. Some ontologies have attempted toinclude annotations of genes associated with internalprocesses specific to the diseases such as an ontology fordiabetes [2, 27]; however, those ontologies cover only asmall number of existent diseases.Periodontitis is a multifactorial disease causing in-flammation in periodontal tissue. The pathogenesis ofperiodontitis includes numerous biological entities suchas oral microorganisms and immune and genetic fac-tors, physical effects such as dental occlusion, drugs,and chemicals, environmental factors, and interactionswith systemic diseases such as diabetes and cardiovas-cular diseases.As systematic mechanisms underlying periodontitisare complex, it remains difficult to elucidate relation-ships and interactions between the multiple risk factorsthrough studies on individual molecules only. Analysesbased on pathways and networks are required in orderto elucidate relationships and interactions from omicsdata, such as genomics, transcriptomics, proteomics,and metabolomics observed in molecular pathways thatare involved in the pathogenesis and progression ofperiodontitis [28]. Actually, gene expression data fromperiodontal tissue has allowed the partial elucidation ofsuch molecular pathways [29]. However, previous ana-lyses of detecting disease-specific processes have notbeen so successful. This may be due to the complexityof periodontitis, as well as to the way results fromomics analyses have been semantically interpreted. Inomics analysis, GO is generally used in annotations of thedata; however, few processes specific to periodontitis areTable 1 The full texts and abstracts used in this studyReview articles No. of articlesFull texts 101Abstracts only 79Unavailable 14Total 194Suzuki et al. Journal of Biomedical Semantics  (2015) 6:30 Page 3 of 13included in GO. This was our motivation to enhance theGO by using a periodontitis-specific extension.Kornman proposed a systems model to link the clin-ical presentation of periodontitis to underlying molecu-lar knowledge and thus better clarify the pathogenesisof periodontal diseases [30].In the past decade, molecular details have been elucidatedin periodontitis comparison with other osteoimmunology-based diseases such as rheumatoid arthritis [31].Osteoimmunology is an interdisciplinary science inves-tigating the interplay between the skeletal and theimmune systems. The main contributors to osteoim-munology are bone effector cells such as osteoclasts orosteoblasts, and immune cells, particularly lymphocytesand monocytes [32]. Osteoimmunology has now becomeone of the most prominent research areas in clinical biol-ogy, and periodontitis is considered to be a good modelfor the study of the common mechanisms in osteoimmu-nology and the progression of target diseases. Severalstudies relating periodontitis to osteoimmunology have re-cently been reported [33, 34].In this paper, we report on Periodontitis-Ontology(PeriO), an ontology we developed for periodontitis.This ontology covers and formally describes a variety ofentities that stand in relation to periodontitis. PeriOdescribes relationships between molecular mechanismsof inner processes in periodontitis and pathogenesisand progression in a clinical view of the disease, as wellas relationships between molecular influences of drugsand environmental molecules and clinical medicationsand treatments.Content integrated into our PeriO includes the follow-ing: 1) functional classification of bacterial molecules inperiodontal lesions; 2) interactions between periodontitisand other systemic diseases; 3) environmental chemicalsaffecting periodontitis; and 4) processes of medical treat-ments for and the molecular pathogenesis of periodontitis.MethodsPeriO is based on our previous development of ontology,which specified the bone resorption response induced byperiodontitis [35]. Bone resorption is one part of a largerprocess in the onset and progression of periodontitis; theentire process is composed of many biological processesand clinical actions. We systematized the entire process inthis study as PeriO.Extraction of terms relating to periodontitisWe retrieved review articles for periodontitis by fromthe PubMed using keywords of periodontitis, biology,human and review on April 30, 2014. In order to col-lect mentions of molecules and cells participating inprocesses we formalized in PeriO, we investigatedJOURNAL OFBIOMEDICAL SEMANTICSAoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3http://www.jbiomedsem.com/content/6/1/3REVIEW Open AccessImplementation of linked data in the life sciencesat BioHackathon 2011Kiyoko F Aoki-Kinoshita1, Akira R Kinjo2, Mizuki Morita3, Yoshinobu Igarashi4, Yi-an Chen4, Yasumasa Shigemoto5,Takatomo Fujisawa5, Yukie Akune1, Takeo Katoda1, Anna Kokubu1, Takaaki Mori1, Mitsuteru Nakao6,Shuichi Kawashima7, Shinobu Okamoto7, Toshiaki Katayama7 and Soichi Ogishima8*AbstractBackground: Linked Data has gained some attention recently in the life sciences as an effective way to provideand share data. As a part of the Semantic Web, data are linked so that a person or machine can explore the web ofdata. Resource Description Framework (RDF) is the standard means of implementing Linked Data. In the process ofgenerating RDF data, not only are data simply linked to one another, the links themselves are characterized byontologies, thereby allowing the types of links to be distinguished. Although there is a high labor cost to define anontology for data providers, the merit lies in the higher level of interoperability with data analysis and visualizationsoftware. This increase in interoperability facilitates the multi-faceted retrieval of data, and the appropriate data canbe quickly extracted and visualized. Such retrieval is usually performed using the SPARQL (SPARQL Protocol andRDF Query Language) query language, which is used to query RDF data stores. For the database provider, suchinteroperability will surely lead to an increase in the number of users.Results: This manuscript describes the experiences and discussions shared among participants of the week-longBioHackathon 2011 who went through the development of RDF representations of their own data and developedspecific RDF and SPARQL use cases. Advice regarding considerations to take when developing RDF representationsof their data are provided for bioinformaticians considering making data available and interoperable.Conclusions: Participants of the BioHackathon 2011 were able to produce RDF representations of their data andgain a better understanding of the requirements for producing such data in a period of just five days. Wesummarize the work accomplished with the hope that it will be useful for researchers involved in developinglaboratory databases or data analysis, and those who are considering such technologies as RDF and Linked Data.Keywords: Semantic Web, Data integration, PDBj, DDBJ, Glycobiology, Alzheimers disease, Faceted search interfaceIntroductionAs technologies in the life sciences advance among vari-ous -omics fields in the post-genomic age, increasingamounts of a wide variety of data are being generated,making it difficult to query and find relationships betweenthe data. Currently, many of the databases that allow datato be downloaded often provide them in their own propri-etary format or as tab- or comma-delimited text files. In-corporating such data requires much data manipulationand integration, which is usually difficult for most* Correspondence: ogishima@sysmedbio.org8Department of Bioclinical informatics, Tohoku Medical MegabankOrganization, Tohoku University, Seiryo-cho 4-1, Aoba-ku, Sendai-shi Miyagi980-8575, JapanFull list of author information is available at the end of the article© 2014 Aoki-Kinoshita et al.; licensee BioMedCreative Commons Attribution License (http:/distribution, and reproduction in any mediumDomain Dedication waiver (http://creativecomarticle, unless otherwise stated.researchers. In order to process such data, most biologistswould use Excel and would probably need to write scriptsto find matching data across different data files, if notdone manually. Moreover, data matching may be difficultbecause of different levels of detail of the data provided,requiring disambiguation/clarifiation of data types, whichis a difficult process. Even for bioinformaticians, there is agreat amount of ad hoc data processing which becomesquite a burden. Moreover, high-activity databases oftenupdate their data on a regular basis, often increasing theburden to continuously import the necessary information.Another burden lies in the need to develop individualquery tools for each database, which may be limited infunctionality and focus solely on the database at hand, stillCentral. This is an Open Access article distributed under the terms of the/creativecommons.org/licenses/by/2.0), which permits unrestricted use,, provided the original work is properly credited. The Creative Commons Publicmons.org/publicdomain/zero/1.0/) applies to the data made available in thisAoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3 Page 2 of 13http://www.jbiomedsem.com/content/6/1/3requiring researchers to integrate data from multiplesources manually.In the midst of such activity, Linked Data has gainedsome attention recently in the life sciences as an effectiveway to provide and share data. As a part of the SemanticWeb, data are linked so that a person or machine can ex-plore the web of data. With Linked Data, when a user hassome data, he/she can find other, related, data [1] rathereasily. Resource Description Framework (RDF) is thestandard means of implementing Linked Data. By usingRDF, database providers can publish data contents that areaccessible via URIs. In addition, each data contains linksto other related data that are (preferably) provided in RDF.Thus, by crawling through the URIs that are linked to oneanother, a wide range of inter-related data can be retrievedusing Semantic Web technologies. As an example, the Sin-dice portal provides a search engine to query RDF dataacross all domains. Using existing web standards, Sindicecollects Semantic Web data, updated every five minutes,and allows users to search and query across this data [2].In the process of generating RDF data, not only aredata simply linked to one another, the links themselvesare characterized by ontologies, thereby allowing thetypes of links to be distinguished. Although it may re-quire a lot of effort for data providers to define an ontol-ogy, the merit lies in the higher level of interoperabilitywith data analysis and visualization software. That is, re-lated data are linked to one another via ontologies con-taining URIs, thus facilitating the multi-faceted retrievalof data, where the appropriate data can be quickly ex-tracted and visualized. Such retrieval is usually per-formed using the SPARQL (SPARQL Protocol and RDFQuery Language) query language, which is used againstRDF data stores, or triplestores [3]. For the databaseprovider, such interoperability will surely lead to an in-crease in the number of users.This manuscript will describe the experiences and dis-cussions shared among participants of BioHackathon2011 who went through the development of RDF repre-sentations of their own data and developed specific RDFand SPARQL use cases within a period of five days. Forbioinformaticians considering making data available andinteroperable, this manuscript will provide advice re-garding considerations to take when developing RDFrepresentations of their data.ReviewCurrent landscape of Semantic Web in the life sciencesLinking Open Data (LOD) is a recent movement encour-aging data providers to develop and to publish their datain a semantically connected manner. It is recommendedthat datasets are exposed and shared as Linked Data inRDF format, where URIs interlink resources on the Se-mantic Web. As shown in the LOD cloud diagram [4],life science data occupies one of the major domains ofLOD. This situation is primarily brought by the Bio2RDFproject [5] which translated major public bioinformaticsdatabases into RDF and provided them as SPARQL end-points. This pioneering work showed that distributeddatasets in the life sciences can be effectively integratedthrough Semantic Web technology.The semantics of RDF data is described by an ontology,which describes basic concepts in a domain and definesrelations among them. It provides the basic buildingblocks comprising its structure: classes or concepts, prop-erties, and restrictions on properties. As a result, an ontol-ogy provides a common vocabulary for researchers whoneed data integration, data sharing, semantic annotation,and extraction of information in the specific domain. Totake advantage of Linked Data, one will eventually need tomake use of ontologies. Several ontologies have alreadybeen carefully designed by experts in particular fields.BioPortal is a useful web resource for developers to finda particular ontology in the life sciences. It provides anopen repository and search engines for biological ontol-ogies [6]. Moreover, the BioPortal Ontology Recom-mender system uses a set of keywords describing adomain of interest and suggests appropriate ontologies forrepresenting the query [7]. The Open Biological and Bio-medical Ontologies (OBO) Foundry provides biomedicalontologies, such as the well-known Gene Ontology (GO),with the goal of creating a suite of orthogonal interoper-able reference ontologies in the biomedical domain [8].The BioGateway project [9] attempts to query complexbiological questions for obtaining scientific knowledgefrom RDF datasets in the semantic systems biology do-main. They integrated SwissProt [10] protein annotationsand taxonomic information with gene ontology annota-tions (GOA), ontologies provided by the open biologicaland biomedical ontologies (OBO) foundry and in-housedeveloped ontologies such as cell cycle ontology (CCO).This system presented an example of how SPARQL quer-ies can retrieve meaningful biological knowledge when theSemantic Web database contains rich information sup-ported by fine-grained ontological annotations.As the use of Semantic Web technologies increases, de-mand for SPARQL endpoints for major databases israised. In response to these demands, UniProt has re-leased their data in RDF and provides a publicly avail-able SPARQL endpoint (http://beta.sparql.uniprot.org/).European Bioinforamtics Institute in the European Mo-lecular Biology Laboratory (EMBL-EBI) recently startedto provide RDF and SPARQL endpoints for several da-tabases hosted at EBI (http://www.ebi.ac.uk/rdf/). Bio-Mart [11] is one of the de facto standard databasesintegrating various resources in biology, and the systemis widely used in many organizations [12]. A SPARQLquery interface has been implemented since the versionAoki-Kinoshita et al. Journal of Biomedical Semantics 2014, 6:3 Page 3 of 13http://www.jbiomedsem.com/content/6/1/30.8 release, enabling users to query the metadata of anyBioMart system from Semantic web applications [13].* LinkDBLinkDB is a database that compiles relationships be-tween database entries that have been serviced as thebackbone of GenomeNet for nearly 20 years. As ofAugust, 2011, a total of over 780,000,000 relationshipsbetween entries from over 160 life science databaseshave been registered. The data structure of LinkDB istriples, consisting of pairs of database entries and theirrelationships. Thus, it is very suitable for converting toRDF. The following three entry relationships are definedin LinkDB: equivalent (the same molecule but from dif-ferent databases), original (hyperlinks to target databaseentry provided in the subject database entry), and re-verse (opposite of original; subject database entry is ref-erenced by target). These relationships could be used aspredicates when generating RDF. During this BioHacka-thon, all of the LinkDB entries were converted to RDF.A manual describing how to use this data is available athttp://www.genome.jp/linkdb/linkdb_rdf.html.* PDBjThe Protein Data Bank Japan (PDBj), a member of theworldwide Protein Data Bank (wwPDB), is a database ofatomic structures of proteins and other biological mac-romolecules. PDBj has recently started providing thecontents of its entries in terms of RDF (http://rdf.wwpdb.org/). The RDF-formatted PDB entries are re-ferred to as PDB/RDF in the following. The originalPDB entries are provided in a format called macromol-ecular crystallographic information format (mmCIF),which is in turn defined by the PDB exchange (PDBx)dictionary [14,15]. The PDBx dictionary defines categor-ies and items for describing various aspects of macro-molecular structures. The OWL ontology of the PDB/RDF is essentially a direct translation of the PDBx dic-tionary augumented with additional classes and proper-ties to handle links between different data sources.In the PDB/RDF service, each PDB entry can beaccessed via a specific URL such as http://rdf.wwpdb.org/pdb/1GOF for the PDB entry 1GOF. This page con-tains mostly a list of links to the categories contained inthe entry. By following these links, for example, http://rdf.wwpdb.org/pdb/1GOF/entityCategory, one finds alist of links to entity category elements. Each categoryelement can be also accessed by a URL such as http://rdf.wwpdb.org/pdb/1GOF/entity/1 which contains thedata describing the molecular entity whose the primarykey is 1 (in this particular example, the entity is galact-ose oxidase. The PDBx dictionary also defines relationsbetween related categories, and this is reflected in PDB/JOURNAL OFBIOMEDICAL SEMANTICSCollier et al. Journal of Biomedical Semantics  (2015) 6:24 DOI 10.1186/s13326-015-0019-zRESEARCH ARTICLE Open AccessConcept selection for phenotypes anddiseases using learn to rankNigel Collier1,2*, Anika Oellrich3 and Tudor Groza4AbstractBackground: Phenotypes form the basis for determining the existence of a disease against the given evidence.Much of this evidence though remains locked away in text  scientific articles, clinical trial reports and electronicpatient records (EPR)  where authors use the full expressivity of human language to report their observations.Results: In this paper we exploit a combination of off-the-shelf tools for extracting a machine understandablerepresentation of phenotypes and other related concepts that concern the diagnosis and treatment of diseases.These are tested against a gold standard EPR collection that has been annotated with Unified Medical LanguageSystem (UMLS) concept identifiers: the ShARE/CLEF 2013 corpus for disorder detection. We evaluate four pipelines asstand-alone systems and then attempt to optimise semantic-type based performance using several learn-to-rank(LTR) approaches  three pairwise and one listwise. We observed that whilst overall Apache cTAKES tended tooutperform other stand-alone systems on a strong recall (R = 0.57), precision was low (P = 0.09) leading tolow-to-moderate F1 measure (F1 = 0.16). Moreover, there is substantial variation in system performance acrosssemantic types for disorders. For example, the concept Findings (T033) seemed to be very challenging for all systems.Combining systems within LTR improved F1 substantially (F1 = 0.24) particularly for Disease or syndrome (T047) andAnatomical abnormality (T190). Whilst recall is improved markedly, precision remains a challenge (P = 0.15, R = 0.59).IntroductionPhenotypes are generally regarded as the set of observ-able characteristics in an individual. Examples includebody weight loss and abnormal sinus rhythm. Pheno-types are important because they help to form the basis fordetermining the classification and treatment of a disease.Although coding systems such as the Human Pheno-type Ontology (HPO) [1] and the Mammalian PhenotypeOntology (MPO) [2] have made substantial progress inorganising the nomenclature of phenotypes, authors typ-ically report their observations using the full expressivityof human language. In order to fully exploit a machineunderstandable representation of phenotypic findings,it is necessary to develop techniques based on naturallanguage processing that can harmonise linguistic varia-tion [3-5]. Furthermore, such techniques need to operateon a range of text types such as scientific articles, clinical*Correspondence: nhc30@cam.ac.ukEqual contributors1University of Cambridge, Cambridge, UK2European Bioinformatics Institute (EMBL-EBI), Cambridge, UKFull list of author information is available at the end of the articletrials and patient records [6] in order to enable appli-cations that require inter-operable semantics. Use casesmight include automated cohort extraction to supportresearch into a particular rare genetic disorder or supportfor curating databases of human genetic diseases suchas the Online Mendelian Inheritance in Man database(OMIM) [7]. We envision the final result to be a represen-tation that decomposes the phenotype terms accordingto their elementary conceptual units (building block con-cepts) and harmonises them to ontologies such as theFoundationalModel of Anatomy (FMA) [8] for anatomicalstructures, the Phenotype Attribute and Trait Ontology(PATO) [9] for qualities and Gene Ontology (GO) [10]for biological processes. Our view is that the techniquesmust be able to support the capture of phenotypes fromboth physical objects and processes as well as cuttingacross levels of granularity from the molecular level to theorganism level.Finding the names of technical terms in life sciencetexts  known as named entity recognition  has beenthe topic of intensive study over the last decade. Ground-ing or normalising these terms to a logically structured© 2015 Collier et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 2 of 12domain vocabulary  an ontology  has proven to bea substantial challenge, e.g. [11,12], because of idiosyn-crasies in naming, the need to exploit syntactic structurein the case of disjoint terms, the paucity of annotatedcorpora for training and evaluation and the incomplete-ness of the target ontologies themselves. To accomplishthis task, concept identification systems have emergedwith different analytical goals. In this paper, we investi-gate the utility of four existing conceptual coding pipelines(i.e. MetaMap [13], Apache cTAKES [14], NCBO anno-tator [15] and BeCAS [16]) in order to identify and har-monise the phenotypes and other concepts related to thediagnosis and treatment of diseases. These tools do notexplicitly consider phenotypes as a conceptual categorybut rather provide groundings from text to a range ofbuilding block concepts which we hope to exploit. In orderto provide a basis for comparing these tools quantita-tively and qualitatively, we have chosen to harmonise theiroutputs to Unified Medical Language System (UMLS)concept unique identifiers (CUIs) and semantic types asthe common coding standard. Concept unique identifiersprovide a way to encode senses of words and phrases,e.g. culture as either anthropological culture or labora-tory culture [17]. UMLS semantic types provide a broadclassification of all the UMLS concepts contained in theMetaThesaurus as well as a structuring of those seman-tic types. There are approximately 133 semantic types and54 relationships between them. UMLS annotations wereassigned at the sentence level. Textual annotations usedthe ShARE/CLEF 2013 corpus [18] which we describelater. We have identified the concept classes which are themost promising building blocks  such as T184 Sign andSymptom - and evaluated based on these. Our approachaims to work towards the composition of phenotypes infuture work based on the building block outputs of thesystems reported here. We chose to focus on the uncus-tomised use-case of the four base systems as a way ofexploring their immediate utility to users who did not haveaccess or resources to build annotated training data or theability to build their own post-processing rules.In addition to evaluating the suitability of each individ-ual system on the ShARE/CLEF 2013 corpus, we inves-tigate possibilities to optimise the outputs of systemsusing an ensemble approach. In order to take advantageof the complementarity in concept recognition and gobeyond a simple voting mechanism, we have employedseveral learn-to-rank (LTR) methods  more specificallythree pairwise ranking approaches: SVMRank [19], Rank-Boost [20] and RankNet [21]; and one listwise rankingapproach: ListNet [22]. Such methods learn to optimisecontraints pairwise or list wise based on a set of featuresand a predefined ranking of the input. In our setting,each sentence, treated as an instance, is described via fivefeature blocks by the individual CR systems. Using theShARE/CLEF 2013 training data, the ranking of the sys-tems is assigned based on the ground truth and a modelis learned such that it maximises the ranking correla-tion. The final optimised ensemble and model is testedon the ShARE/CLEF 2013 test data set. The learn torank ensemble enables us to accept the choices of morethan one system in the event of a closely tied rank-ing. We found that combining systems within learn torank improved F1 substantially compared to stand-alonesystems.MethodsDataFor evaluation and training the re-ranker we chose to usethe ShARE/CLEF e-health 2013 Task 1 evaluation data setof 300 de-identified clinical records from theMultiparam-eter Intelligent Monitoring in Intensive Care (MIMIC)II data-base (http://mimic.physionet.org/database.html)with stand-off annotations for disorders. This is a mixedcorpus that includes discharge summaries, echo reportsand radiology reports used in an intensive care unit set-ting. 200 notes were designated for training and 100 fortesting. Annotation was done by two annotators plusopen adjudication. Access to the corpus required appro-priate registration with MIMIC II and the completion ofa US human subjects training certificate. The distributionof UMLS semantic types for disorder-related text spanscan be seen in Tables 1 and 2. Note that we removedminor classes with frequencies of 1 (i.e. T002, T031, T049,T058, T059, T121, T197). As can be seen in Tables 1and 2, the majority of semantic types relate to diseases,symptoms and pathological functions together with asubstantial minority of annotations for injuries, congen-ital and anatomical abnormalities and mental/behavioraldysfunctions.Table 1 ShARE/CLEF e-health training corpus semantictypesID UMLS Semantic type Freq. Unique Av. term lengthT047 Disease or syndrome 1803 410 1.97T184 Sign or symptom 842 163 1.56T046 Pathologic function 518 133 1.65T037 Injury or poisoning 213 96 2.00T019 Congenital abnormality 184 25 3.61T190 Anatomical abnormality 103 36 1.77T191 Neoplastic process 92 49 1.87T048 Mental or behavioraldysfunction84 32 1.76T033 Finding 45 15 2.90T020 Acquired abnormality 40 17 1.93Distribution of UMLS semantic types for annotations by frequency andfrequency without duplication as well as the average term length in tokens.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 3 of 12Table 2 ShARE/CLEF e-health test corpus semantic typesID UMLS Semantic type Freq. Unique Av. term lengthT047 Disease or syndrome 1723 371 1.88T184 Sign or symptom 816 149 1.51T046 Pathologic function 520 113 1.59T037 Injury or poisoning 106 33 1.75T019 Congenital abnormality 96 18 1.88T190 Anatomical abnormality 125 26 1.74T191 Neoplastic process 73 34 2.02T048 Mental or behavioraldysfunction137 32 1.67T033 Finding 13 6 1.11T020 Acquired abnormality 41 21 1.62Distribution of UMLS semantic types for annotations by frequency andfrequency without duplication as well as the average term length in tokens.An example source sentence from the corpus is shownin Figure 1 along with actual gold standard conceptannotations, harmonized semantic types and a potentialdecompositional mapping to PATO and FMA for one clin-ical phenotype (neck stiffness). Here C* annotationscorrespond to concept annotations and T* annotationscorrespond to harmonised semantic types.The distributions for train and test possess good agree-ment but, at the same time, also interesting differences:the average length of mentions of T019 congenital abnor-mality appears remarkably longer in the training corpus,and there are relatively fewer T037 injury or poisoning andT019 congenital abnormality instances in the testing set.Moreover, we observe a greater variety of T037 instancesin the testing corpus.Examples of what we might consider interesting phe-notypes occur across all anntoated UMLS semantictypes as well as for unannotated strings. For example,Right ventricular [is mildly] dilated (C0344893 | T019),wall motion abnormality (no CUI) and hypotension(C0520541 | T047). In other cases, the class shows adisease and not a phenotype, e.g. complex autonomousdisease (C0264956 | T046). We note that unannotatedstrings were not explicitly quantified in the present studyreported here and and are left for future study.Experimental setupWe follow standard metrics of evaluation for the taskusing F1, i.e. the harmonicmean of recall (R) and precision(P). This is the same metric used by participants of theShARE/CLEF 2013 Task 1. F1 is calculated as F1 = 2PR/(P + R), with P = TP/(TP + FP) and R = TP/(TP + FN)where TP is the number of system suggestions where thesemantic type and the CUI is the same as the gold stan-dard; FP is the number of system suggestions where thesemantic type and/or the CUI do not match the goldstandard; and FN is the number of spans in the goldstandard which the system failed to suggest. The majordifference between our evaluation and the ShARE/CLEFshared task is that we evaluate at the sentence level andnot the mention level, i.e. the focus is on predicting con-cept labels for the sentence as a whole and not the startingand ending positions of those annotations in the sentence.Consequently, our experimental results are not directlycomparable with those achieved by systems participat-ing in the ShARE/CLEF Tasks. Evaluation is conductedusing blind data not used in system development data ortraining.Different applications require a different approach todefining a true positive, false negative etc. In this case wehave considered a correct match to be recorded when acomplete match occurs between system output and goldstandard for both the identifier and the semantic type ofthat concept in UMLS. In line annotation is not consid-ered explicitly within this evaluation. Clearly any furtherapplication requiring the explicit annotation of relation-ships between concepts within the sentence would requirethis. The evaluation protocol reported here supportsuse cases such as statistical association analysis betweenthe co-occurring concepts and document indexing/retrieval.Individual system descriptionsThe problemwe consider is how to select a set of disorder-related SNOMED CT concepts for any given sentence.Disorder-related concepts are chosen because of theirrelevance to phenotype recognition. SNOMED-CT waschosen as the ontology for harmonisation because it offersa joint coding ontology for all the base systems. A numberof factors complicate the task including: (a) in line withour desire to test off-the-shelf performance, the systempipelines were not tuned in any way for predicting the spe-cific set of disorder-related semantic types appearing inthe corpus, (b) the annotation scheme allows for disjointFigure 1 Example of sentence annotations from the ShARE/CLEF corpus. The example shows concept annotations for headache; (C0018681 |T184), neck stiffness (CO151315 | T184) and unable to walk (C0560048 | T033). An example decomposition for neck stiffness is shown with anillustrative mapping to PATO:0001545 (inflexible) and FMA:Neck.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 4 of 12(e.g. Right ventricular . . . dilated) and overlapping anno-tation spans; and (c) clinical texts contain a high numberof abbreviations causing additional complications for termidentification and harmonisation.We consider four uncustomized base concept annota-tion systems based on clinical natural language process-ing: NCBO Annotator, BeCAS, cTAKES and MetaMap.With the exception of MetaMap, all the other systemswere used with their default parameters. Other systemsthat could have been applied here include ConceptMap-per [23], Whatizit [24] and Bio/MedLee [25]. These sys-tems were either difficult to access or did not providea route to UMLS concept harmonizations. The systemswe applied adopt a range of techniques but tend to avoiddeep parsing. Instead, they make use of a range of shal-low parsing, sequence-based machine learning (e.g. fornamed entity recognition and part of speech tagging) andpattern-based techniques, supplemented with restrictionsand inferences on source ontologies such as SNOMEDCT [26]. In all cases it should be noted that we dealt withblack box systems.NCBO Annotator (M1) The NCBO Annotator is anonline system that identifies and indexes biomedical con-cepts in unstructured text by exploiting a range of over300 ontologies in BioPortal. These ontologies includemany that have particular relevance to disorders andphenotypes such as SNOMED CT, LOINC (Logic Obser-vation Identifiers, Names and Codes) [27], the FMAand the International Classification of Diseases (ICD-10) [28]. NCBOAnnotator operates in two stages: conceptrecognition and semantic expansion. Concept recogni-tion performs lexical matching by pooling terms and theirsynonyms from across the ontologies and then apply-ing a multiline version of grep to match lexical variantsin free text. During semantic expansion, various rulessuch as transitive closure and semantic mapping using theUMLS Metathesaurus are used to suggest related con-cepts from within and across ontologies based on extantrelationships.BeCAS (M2) BeCAS (the BioMedical Concept Annota-tion System) is the newest integrated system of the fourthat we tried. The pipeline of processes involves the fol-lowing stages: sentence boundary detection, tokenization,lemmatization, part of speech (POS) tagging and chunk-ing, abbreviation disambiguation, and concept uniqueidentifier (CUI) tagging. The first four stages are per-formed by a dependency parser that incorporates domainadaptation using unlabelled data from the target domain.CUI tagging is conducted using regular expressions forspecific types such as anatomical entities and diseases.Dictionaries used as sources for the regular expressionsinclude the UMLS, LexEBI [29] and the Jochem jointchemical dictionary [30]. During development the con-cept recognition system was tested on abstracts and fulllength scientific articles using an overlapping matchingstrategy.Apache cTAKES (M3) cTAKES consists of a stagedpipeline of modules that are both statistical and rule-based. The order of processing is somewhat similar toMetaMap and consists of the following stages: sentenceboundary detection with OpenNLP, tokenization, lexicalnormalisation (SPECIALIST lexical tools), part of speechtagging and shallow parsing using OpenNLP trained in-domain on Mayo Clinic EPR concept recognition, nega-tion detection using NegEx [31] and temporal statusdetection. Concept recognition is conducted within theboundaries of noun phrases using dictionary matchingon a synonym-extended version of SNOMED CT andRxNORM [32] subset of UMLS. Evaluation was con-ducted with a focus on EPRs but also using corpora fromthe scientific literature.MetaMap (M4-M9) MetaMap is a widely used andtechnically mature system from the National Library ofMedicine (NLM) for finding mentions of clinical termsbased on CUI mappings to the UMLS Metathesaurus.The UMLS Metathesaurus forms the core of the UMLSand incorporates over 100 source vocabularies includingthe NCBI taxonomy, SNOMED CT and OMIM. Outputis to the 135 UMLS semantic types. The system exploitsa fusion of linguistic and statistical methods in a stagedanalysis pipeline. The first stages of processing performmundane but important tasks such as sentence bound-ary detection, tokenization, acronym/abbreviation iden-tification and POS tagging. In the next stages, candidatephrases are identified by dictionary lookup in the SPE-CIALIST lexicon and shallow parsing using the SPECIAL-IST parser. String matching then takes place on the UMLSMetathesaurus before candidates are mapped to theUMLS and compared for the amount of variation. A finalstage of word sense disambiguation uses local, contextualand domain-sensitive clues to arrive at the correct CUI.MetaMap is unique in providing a rich set ofoptions [33] to allow the user to customise the approachthe system takes to concept mapping.We chose to explorea range of options including what we considered a highprecision strict approach to matching as well as nega-tion detection with NegEx. The variations of MetaMap weexplored were: M4: MetaMap -A -negex  using strict matchingand negation detection M5: MetaMap -A -y  using strict matching andforcing MetaMap to perform word sensedisambiguation on equally scoring concepts M6: MetaMap -g  allowing concept gaps M7: MetaMap -i  ignoring word order M8: MetaMap  using the base version M9: MetaMap -A  using strict matching onlyCollier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 5 of 12Ensemble approachIn addition to the nine basic systems M1 to M9, we eval-uated several ranking approaches that rank the qualityof basic system outputs based on a sentence-level andconcept-level features. These features include individualsource sentence vocabulary, the semantic types suggestedby the system and the vocabulary for the suggested con-cept labels. More sophisticated features will be tested inthe future. We believe that the chosen features serve as auseful first step for evaluating the ranking approach. Theapproaches we tested make use of a scoring function torank each systems output set of concept labels against thetraining data. These rankings are used together with thefeatures to train a learn-to-rank (LTR) model. We evalu-ated four different ranking algorithms based on pairwiseand list wise comparisons to maximise the ranking corre-lation for all categories, where the categories represent thenine basic systems.We explore the underlying assumptionthat a set of features exists that can predict when one sys-tem will perform better on a given sentence than another.The ranking function we applied was the F1 metric thatwe used to evaluate each system described in detail in thesection below.Ranking essentially aims to establish which hypoth-esis about sentence-level concept annotations is mostlikely given the available evidence. Labelled instances areprovided during training as feature vectors. Each labeldenotes a single rank that is determined by comparing theF1 scores for each system based on the concepts they out-put on that sentence against the set of gold standard con-cepts. The goal of training each of the ranking approachesis to find a model that correctly determines the orderingof systems on a given sentence. Afterwards we can eitherchoose the predictions from the single highest rankingsystem or combine a group of highly ranking systems.The feature blocks used by the ensemble model arelisted in Table 3. During testing, a feature vector is pro-vided for each system (methods M1 . . . M9) and theLTR model determines a score which is then convertedto an ordered ranked list by the ensemble. In practice thesemantic types suggested by the top system are selected.If the first rank is shared between multiple systems, thetop outputs from the top ranking systems are combinedby taking the union.The LTR systems that we investigated include threepairwise LTR  SVMRank [19], RankNet [21], and Rank-Boost [20]  and one listwise LTR  ListNet [22]. Table 4provides a succinct comparative overview of the two typesof LTR, as initially described in [34].ResultsComparison of stand-alone systemsTable 5 presents results for each of the stand-alone sys-tems at a macro level, while Table 6 lists results structuredTable 3 Feature blocks used to build the ensemblemodelFeature block DescriptionFB1 A Boolean set of features for the system identifiers(i.e. M1 . . . M9);FB2 A Boolean set of features for the semantic typesthat are predicted by the system to appear and notappear in the sentence (i.e. T047, T184, . . . etc.);FB3 A set of integer valued features for the counts ofvocabulary terms appearing in UMLS concepts thatare predicted by the systems to appear in thesentence; In total the set consisted of 1,008 UMLSCUIs;FB4 A set of integer valued features for the counts ofvocabulary terms appearing in the sentence; Thevocabulary consisted of 13,565 terms;FB5 A set of integer valued features for the 45cluster distributed semantic classes which matchto FB3. The 45 cluster classes derived by RichardSocher and Christoph Manning from PubMedare available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gzaccording to semantic type. Note that we did not performany learning procedure at this stage on the gold stan-dard corpus. We can see several noteable results includingthe relatively better performance of systemM3 (cTAKES),both at the macro level (0.16 F1, compared to 0.08 F1achieve by the next system in line  M5), as well asacross most semantic types  with the exception of T190(Anatomical abnormality) where system M4 does best.No single system though achieves both winning recalland precision in the type-based setting. System M5 forexample (MetaMap -A -y) generally achieves the high-est precision. We can also note a wide disparity in F1 bysystems across semantic types.In general the stand-alone systems performed better onT047, T184 and T048. In contrast, performance on T037,Table 4 Brief comparative overview on the learn to rankapproaches, adapted from [34]Pairwise learn to rank Listwise learn to rankGoal Ranking by learning onobject pairsRanking by learning onobject listsLoss function pairwise loss, e.g., hingeloss, exponential loss,logistic losslistwise loss, e.g., crossentropy loss, cosine lossAdvantages Theoretical aspects arewell studiedConsiders the relationshipamong objects to their fullextentDisadvantages Considers only pairwiseorders; May be biasedtowards lists with moreobjectsTheoretical aspects areless well studiedAlgorithms SVMRank [19];RankNet [21];RankBoost [20]ListNet [22]Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 6 of 12Table 5 Comparison of stand-alone systems on trainingdataSystem P R F1M1: NCBO Annotator 0.0393 0.5044 0.0729M2: BeCAS 0.0146 0.0134 0.0140M3: Apache cTAKES 0.0933 0.5675 0.1602M4: MetaMap -A -negex 0.0389 0.2992 0.0689M5: MetaMap -A -y 0.0498 0.2505 0.0831M6: MetaMap -g 0.0387 0.2905 0.0683M7: MetaMap -i 0.0392 0.2994 0.0693M8: MetaMap 0.0389 0.2992 0.0689M9: MetaMap -A 0.0389 0.2992 0.0689Macro precision, recall and F1 of the individual systems on the training data. Thehighest scoring system F1 is shown in bold.T190, T033 and T019 tended to be weak. Stronger per-formance might be partly correlated with shorter averageterm length (see Table 2) but this is not an entirely satis-fying explanation. Another possible explanation is hintedat by the fact that the more challenging classes are at thelower end of frequencies in the EPR data. This might indi-cate that the semantic resources which the systems drawon have been less intensively developed and might notprovide such extensive lexical support as more frequentclasses.Learn-to-rank resultsUsing documents as the sampling unit, we performedrandomised 10-fold cross validation on the ShARE/CLEFtraining data. 9 parts of the data were selected withoutreplacement to train the four LTR models from scratchand 1 part was used to test. The 10 test parts were thenjoined together and recall, precision and F-score werecalculated as in the stand-alone evaluation.In the testing stage, we experimented with all combi-nations of feature blocks and also with different settingsfor LTR parameters. Best results were achieved using fea-ture blocks FB1, FB2 and FB4, in addition to the followingmodel parameters: SVMRank: a value of 30 for the trade-off betweentraining error and margin; RankNet: 100 epochs, 1 hidden layer with 10 nodesand a learning rate of 0.00005; RankBoost: 300 rounds to train and 10 thresholdcandidates to search; ListNet: 1500 epochs and a learning rate of 0.00001;Feature blocks FB3 and FB5 were not found to improveperformance in these experiments.Finally, in order to gain a deeper understanding inthe ensembles behaviour, we have experimented withdifferent tie-breaking strategies, at different top-K rankingTable 6 Comparison of stand-alone systems on trainingdataID Sys P R F1 ID Sys P R F1T047 M1 0.39 0.55 0.45 T191 M1 0.24 0.30 0.26M2 0.03 0.01 0.02 M2 0.05 0.03 0.04M3 0.44 0.63 0.52 M3 0.29 0.64 0.40M4 0.58 0.28 0.38 M4 0.21 0.25 0.23M5 0.72 0.22 0.34 M5 0.38 0.23 0.28M6 0.58 0.27 0.37 M6 0.21 0.25 0.23M7 0.58 0.28 0.38 M7 0.22 0.25 0.23M8 0.58 0.28 0.38 M8 0.21 0.25 0.23M9 0.58 0.28 0.38 M9 0.21 0.25 0.23T184 M1 0.35 0.61 0.45 T048 M1 0.28 0.49 0.35M2 0.02 0.01 0.01 M2 0.04 0.03 0.03M3 0.47 0.58 0.52 M3 0.45 0.55 0.50M4 0.62 0.41 0.49 M4 0.53 0.34 0.42M5 0.68 0.36 0.47 M5 0.67 0.27 0.38M6 0.61 0.40 0.49 M6 0.54 0.35 0.43M7 0.61 0.41 0.49 M7 0.54 0.34 0.42M8 0.62 0.41 0.49 M8 0.53 0.34 0.42M9 0.62 0.41 0.49 M9 0.53 0.34 0.42T046 M1 0.28 0.62 0.39 T033 M1 0.01 0.36 0.01M2 0.03 0.04 0.03 M2 0.00 0.00 0.00M3 0.30 0.69 0.42 M3 0.00 0.11 0.00M4 0.50 0.34 0.40 M4 0.00 0.13 0.01M5 0.50 0.26 0.34 M5 0.00 0.13 0.01M6 0.49 0.33 0.39 M6 0.00 0.13 0.01M7 0.50 0.34 0.41 M7 0.00 0.13 0.01M8 0.50 0.34 0.40 M8 0.00 0.13 0.01M9 0.50 0.34 0.40 M9 0.00 0.13 0.01T037 M1 0.19 0.24 0.21 T020 M1 0.36 0.50 0.42M2 0.00 0.00 0.00 M2 0.00 0.00 0.00M3 0.26 0.34 0.30 M3 0.33 0.57 0.42M4 0.38 0.22 0.28 M4 0.36 0.36 0.36M5 0.42 0.21 0.28 M5 0.36 0.21 0.27M6 0.36 0.20 0.25 M6 0.36 0.33 0.35M7 0.37 0.21 0.27 M7 0.36 0.36 0.36M8 0.38 0.22 0.28 M8 0.36 0.36 0.36M9 0.38 0.22 0.28 M9 0.36 0.36 0.36T190 M1 0.12 0.44 0.19 T019 M1 0.40 0.11 0.18M2 0.01 0.01 0.01 M2 0.00 0.00 0.00M3 0.12 0.55 0.19 M3 0.58 0.14 0.23M4 0.28 0.22 0.25 M4 0.27 0.07 0.11M5 0.32 0.19 0.24 M5 0.34 0.06 0.11M6 0.28 0.22 0.25 M6 0.25 0.07 0.11M7 0.28 0.22 0.25 M7 0.27 0.07 0.11M8 0.28 0.22 0.25 M8 0.27 0.07 0.11M9 0.28 0.22 0.25 M9 0.27 0.07 0.11Type-based micro precision, recall and F1 of the individual systems on thetraining data. The highest scoring system F1 for each semantic type is shown inbold. Note that italics scores indicate the highest level achieved for recall andprecision for each semantic type by any system.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 7 of 12levels. In our context, the outcome of applying LTR onan instance is a ranked list of the individual systems forthat instance, together with the associated weight. Hence,to compute the standard performance metrics such thatthe results are comparable to those of the stand-alonesystems, the LTR ranking has to be transformed into ahard classification outcome. This is realised by introduc-ing a cut-off at a desired top-K level, which entails thatthe systems ranked above K will participate in the classifi-cation outcome. In a setting where multiple systems maybe ranked above the threshold (possible even for K = 1), atie-breaking strategy is required. We have considered twostrategies: (i) a union strategy, where the individual anno-tations of all top-K ranked systems are merged via a setunion, and the union is considered the final classificationresult on that particular instance; and (ii) an oracle strat-egy, where using the ground truth, we aim to choose thesingle system among the top-K ranked that maximises theperformance metric.While the first strategy does not require any a prioriknowledge and is usable in a proper application sce-nario, the second is only usable when the ground truth isknown. Thus, it is not applicable for appropriate testing.However, we included this strategy in order to under-stand the actual contribution of the individual systems tothe ensemble result. Consequently, the tables listing themacro-performance metrics of the ensemble on both theten-fold cross validation (Table 7 and 8), as well as on theblind test data (Table 9) are accompanied by a measure ofindividual system contribution to the final outcome. Notethat, under normal circumstances, the sum of all indi-vidual contributions should be 1.0. However, this is trueonly for the oracle strategy, where a single system is cho-sen to represent the ensemble. The union strategy mayinvolve several systems, each of which will score points forcontributing to the ensemble result.Returning to the results, the overall macro performanceof the LTR approaches on ten-fold cross validation usingthe ShARE/CLEF training set is listed in Table 7. At top-1rank level, the best union strategy was achieved by SVM-Rank with an F1 of 0.24, while the Oracle strategy showsRankBoost to outperform the other models with an F1 of0.28. These compare to the best single system, as shownin Table 5, which was cTAKES (M3) with F1 = 0.16 representing a contribution of +8 and +11 points of F1,respectively. The decrease in ranking threshold leads to anatural decrease in F-Score for the union strategy (since itbecome more and more inclusive), and at the same timewith an increase in F-Score for the Oracle strategy (sinceit enlarges the pool from which it can choose the opti-mal solution)  from 0.24 (top-1) to 0.19 (top-2) and 0.18(top-3) for union and from 0.28 (top-1) to 0.36 (top-2) and0.38 (top-3) for Oracle. Independently of the threshold ormodel, however, the results of the stand-alone systems arereflected in the individual contributions of the systems inthe ensemble (as shown in Table 7).With a few exceptions,most of which are in the union strategy, M3 (cTakes) isthemost prominent contributor to the ensemble outcome,paired, subject to the LTR model, either with M1 (NCBOAnnotator) or M9 (MetaMap strict).It is interesting to note that, while using the Union strat-egy the LTR outcome is consistent across different top-Klevels  SVMRank achieving the best results  the samedoes not hold for the Oracle strategy, which shows threemodels achieving the best results at three top-K levels.There are, however, some patterns that emerge from theindividual system contributions. For example, RankNetshows a clear preference towards M1 and M3 only. SVM-Rank, ListNet and RankBoost use predominantly M1 andM3, augmented with M9, M4 and M7 respectively. Sur-prisingly M4 (MetaMap with Negex) appeared to haveminimal impact in the ensemble although it features moreprominently in several Oracle experiments.The ensemble approach improved performance for allsemantic types with the exception of two cases, wherethe performance was slightly reduced: T046 (F1: 0.42 to0.40), T020 (F1: 0.42 to 0.41). More importantly, in somecases, the improvement was substantial, e.g., 6% on T047and T048 or 5% on T190. In terms of LTR model, differ-ent models preferred different types  the results beingsplit between SVMRank and RankBoost. T047, T184 andT190 were dominated by SVMRank and T191 and T084by RankBoost.In order to show the generalizability of the ensembles,we ran them on the ShARE/CLEF held out set. The over-all results listed in Table 9 show an average improvementin performance of 2% across different tie-breaking strate-gies and top-K levels. Furthermore, the individual systemcontributions follow the same patterns as discussed onthe cross-validation results. Finally, as shown in Table 10,most semantic types achieved stronger performance onthe testing data with T019, T037, T046, T184 and T190showing strong gains. This indicates the potential variancein the data sample.DiscussionExamples of complicationsShort forms Whilst we still need to conduct a detaileddrill down analysis we can see from a preliminary sur-vey that one of the most significant sources of erroris the strong prevalence of undefined abbreviations inthe clinical texts, e.g. cp for C0008031: [chest pain], laenlargement for C0344720: [left atrium enlargement], nfor C0027497: [nausea]. Without pre-processing to nor-malise to full forms, the degree of ambiguity in the shortforms causes difficulties for the four systems which cannotbe solved in the ensemble. In contrast, full forms of shortforms were often found by the approaches employed.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 8 of 12Table 7 Learn to rank on training dataLearn to rank performance Individual system contributionTop-K Strategy Model P R F1 M1 M2 M3 M4 M5 M6 M7 M8 M9Top-1 Union SVMRank 0.1513 0.5960 0.2413 0.25 0.04 0.45 0.01 0.00 0.01 0.00 - 0.23ListNet 0.1153 0.5880 0.1928 0.73 0.03 0.21 0.02 - 0.00 - - -RankNet 0.0924 0.5206 0.1570 1.00 - - - - - - - -RankBoost 0.1296 0.6125 0.2139 0.46 0.05 0.50 - - 0.01 0.28 0.28 0.28Oracle SVMRank 0.1513 0.5960 0.2413 0.25 0.04 0.45 0.01 0.00 0.01 0.00 - 0.23ListNet 0.1153 0.5880 0.1928 0.73 0.03 0.21 0.02 0.00 - - -RankNet 0.0924 0.5206 0.1570 1.00 - - - - - - - -RankBoost 0.1791 0.6113 0.2770 0.27 0.04 0.49 - - 0.01 0.18 0.00 -Top-2 Union SVMRank 0.1122 0.6426 0.1911 0.58 0.07 0.66 0.03 0.00 0.02 0.00 0.23 0.40ListNet 0.0996 0.6566 0.1730 0.94 0.06 0.88 0.10 0.01 0.01 0.00 - -RankNet 0.0989 0.6477 0.1716 1.00 - 1.00 - - - - -RankBoost 0.1084 0.6469 0.1857 0.55 0.09 0.61 0.01 0.00 0.28 0.76 0.76 -Oracle SVMRank 0.2340 0.6316 0.3415 0.09 0.00 0.67 0.01 0.00 0.01 - - 0.23ListNet 0.2390 0.6439 0.3486 0.19 0.00 0.75 0.05 0.01 0.00 - - -RankNet 0.2533 0.6363 0.3624 0.17 - 0.83 - - - - - -RankBoost 0.2385 0.6359 0.3469 0.11 0.00 0.57 - - 0.02 0.29 0.00 -Top-3 Union SVMRank 0.1051 0.6545 0.1811 0.61 0.18 0.68 0.07 0.02 0.03 0.24 0.40 0.77ListNet 0.0921 0.6761 0.1621 0.97 0.60 0.96 0.40 0.03 0.03 0.01 - -RankNet 0.0943 0.6486 0.1647 1.00 1.00 1.00 - - - - -RankBoost 0.1048 0.6532 0.1806 0.56 0.11 0.63 0.29 0.23 0.75 0.97 0.97 -Oracle SVMRank 0.2469 0.6409 0.3565 0.07 0.00 0.62 0.02 0.01 0.01 0.00 - 0.28ListNet 0.2716 0.6596 0.3848 0.13 0.00 0.72 0.14 0.01 0.00 - - -RankNet 0.2536 0.6367 0.3627 0.17 0.00 0.83 - - - - - -RankBoost 0.2553 0.6397 0.3650 0.10 0.00 0.56 - 0.09 0.02 0.23 0.00 -Macro precision, recall and F1 at different top K levels. The highest scoring system F1 for each level (both union and oracle strategies) is shown in bold. The table alsoshows the individual contribution of the systems to the final score where italics scores indicate the highest contributing individual system(s) to each ensemble.Lack of context A common problem in clinical textsis known to be a lack of grammatical context. For exam-ple, a line in a record might consist only of a single nounphrase without end of line punctuation such as Left bun-dle branch block C0023211: [left bundle branch block].Whilst this should in theory be less of a problem foralgorithms that employ only local contextual patterns it,nevertheless, presents issues for sentence boundary detec-tion, which might introduce unexpected errors. In short-ened sentences, omission of the subject is often a problem,e.g. relative afferent defect can only be fully understoodin the context of the preceding sentence referring toocular discs and therefore achieving a normalisation onC0339663: [afferent pupillary defect].Complex grammatical structures and inferencesDisjoint concept mentions and inferences add an extralayer of difficulty to the task. An example includinga long distance relationship as well as an inference isshown in the following sentence: On motor exam, thereis generally decreased bulk and tone, decreased sym-metrically, there is generalised wasting . . . . Firstly, aninference is required to find the anatomical entity inquestion, which in this example is the muscle indicatedby motor exam and the context provided in the sen-tence decreased bulk and tone and wasting. Secondly,the inferred entity then needs to be connected withother distant text spans in the sentence such as gener-ally decreased bulk and tone and generalised wasting toyield the intended annotations C0026846: [muscle wast-ing] and C0026827: [decreased muscle tone]. However, wenote here that inference is not consistently handled in thegold standard. For example,. . . the gastrointestinal ser-vice felt that an upper gastrointestinal bleed secondary tonon-steroidal anti-inflammatory drugs was . . .  is anno-tated with C0413722: [non-steroidal anti-inflammatorydrugs] in the gold standard, suppressing the informationthat there is an adverse reaction (upper gastrointestinalbleed secondary to). If a system were to use matchingCollier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 9 of 12Table 8 Type-based learn to rank on training dataID Sys P R F1 ID Sys P R F1T019 SVMRank 0.46 0.15 0.23 T047 SVMRank 0.53 0.64 0.58ListNet 0.47 0.13 0.20 ListNet 0.42 0.65 0.51RankNet 0.47 0.11 0.18 RankNet 0.43 0.57 0.49RankBoost 0.43 0.16 0.23 RankBoost 0.46 0.68 0.55T020 SVMRank 0.32 0.57 0.41 T048 SVMRank 0.44 0.62 0.52ListNet 0.29 0.50 0.36 ListNet 0.42 0.54 0.47RankNet 0.33 0.42 0.37 RankNet 0.39 0.51 0.44RankBoost 0.21 0.50 0.30 RankBoost 0.48 0.67 0.56T033 SVMRank 0.01 0.32 0.02 T184 SVMRank 0.46 0.63 0.53ListNet 0.01 0.48 0.02 ListNet 0.40 0.66 0.50RankNet 0.01 0.48 0.03 RankNet 0.39 0.62 0.48RankBoost 0.01 0.45 0.02 RankBoost 0.45 0.64 0.53T037 SVMRank 0.34 0.33 0.33 T190 SVMRank 0.20 0.61 0.30ListNet 0.26 0.24 0.25 ListNet 0.17 0.58 0.27RankNet 0.22 0.21 0.22 RankNet 0.15 0.44 0.22RankBoost 0.30 0.29 0.29 RankBoost 0.16 0.60 0.25T046 SVMRank 0.29 0.66 0.40 T191 SVMRank 0.31 0.56 0.40ListNet 0.25 0.67 0.36 ListNet 0.37 0.44 0.40RankNet 0.27 0.61 0.37 RankNet 0.35 0.36 0.36RankBoost 0.24 0.67 0.35 RankBoost 0.35 0.56 0.43Note that the highest scoring system F1 for each semantic type is shown in bold.and local context rules, it may miss this annotation as itsinference system would expect to annotate secondary tonon-steroidal anti-inflammatory drugs, which, to the bestof our knowledge, does not exist as an ontology concept.Coordination Coordinating terms occur in a variety offorms, e.g. in comma lists or with and and or leadingto head sharing. For example, abdomen soft, non-tender,non-distended should give C0426663: [abdomen soft]and C0424826: [abdomen non-distended]. Whilst shortforms and coordination are known issues that are handledby state-of-the-art biomedical named entity recognitionpipelines, the lack of context in clinical reports and in par-ticular the disjointed nature of some complex phenotypeshas not yet been adequately consideredComparison with other ensemble approachesAlthough there has been quite a lot published on thesubject of concept normalisation and a large body of lit-erature on named entity recognition, there is relativelylittle work on comparing and combining existing systemsin ensemble approaches. In particular, learn-to-rank isa fairly recent technique for concept normalisation. Tothe best of our knowledge, it has only been applied oncebefore by Leaman et al. [35] for diseases, a subset of thesemantic types that we test here. Leaman et al. reportpromising results on a subset of the NCBI disease corpusand, in fact, their system came first in the ShARE/CLEFTask 1b.Ensembles have though been used before for the recog-nition of clinical concepts. Kang et al. [36] for exampleemployed dictionary and statistical pattern based tech-niques on the 2010 I2B2 corpus of EPRs, for term recog-nition (but not concept normalisation) achieving the thirdlevel of performance in the shared task. Xia et al. [37]show the effects of combining MetaMap and cTAKES forthe same ShARE/CLEF data we have shown here. Theircombination strategy is a simple rule-based approach thataccepts all outputs from the higher precision system andthen checks for conflicts in the output of the high recallsystem before accepting new CUIs.One line of investigation we want to pursue in futurework is to decouple the ranking of concepts from systembaskets, i.e. instead of treating the rank of a whole basketof concepts as the target we provide individual conceptsfor each system and then learn to rank these. This wouldpotentially allow us to better control for systems that arestrong on some concepts and weaker on others.LimitationsAll of the individual systems applied in our base studywere used without customization, e.g. training or specialpost-processing rules. This is in contrast to the systemsCollier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 10 of 12Table 9 Learn to rank on test dataLearn to rank performance Individual system contributionTop-K Strategy Model P R F1 M1 M2 M3 M4 M5 M6 M7 M8 M9Top-1 Union SVMRank 0.1712 0.6426 0.2703 0.17 0.04 0.50 - - 0.01 0.00 - 0.28ListNet 0.1271 0.6170 0.2108 0.65 0.04 0.26 0.05 - 0.00 - - -RankNet 0.0923 0.5096 0.1562 1.00 - - - - - - - -RankBoost 0.1408 0.6524 0.2316 0.43 0.05 0.50 - - 0.01 0.28 0.28 0.28Oracle SVMRank 0.1712 0.6426 0.2703 0.17 0.04 0.50 - - 0.01 0.00 - 0.28ListNet 0.1271 0.6170 0.2108 0.65 0.04 0.26 0.05 - 0.00 - - -RankNet 0.0923 0.5096 0.1562 1.00 - - - - - - - -RankBoost 0.1872 0.6504 0.2907 0.23 0.05 0.51 - - 0.01 0.20 0.00 -Top-2 Union SVMRank 0.1244 0.6986 0.2112 0.51 0.07 0.62 - - 0.01 0.00 0.28 0.50ListNet 0.1107 0.7109 0.1915 0.87 0.07 0.88 0.13 0.03 0.02 0.00 - -RankNet 0.1070 0.7028 0.1857 1.00 - 1.00 - - - - - -RankBoost 0.1188 0.7034 0.2032 0.53 0.09 0.62 0.01 0.01 0.28 0.76 0.76 0.76Oracle SVMRank 0.2350 0.6869 0.3501 0.07 0.00 0.63 - - 0.00 0.00 - 0.29ListNet 0.2534 0.6981 0.3718 0.14 0.00 0.77 0.06 0.02 0.00 0.00 - -RankNet 0.2629 0.6905 0.3808 0.15 - 0.85 - - - - - -RankBoost 0.2420 0.6908 0.3584 0.10 0.00 0.58 - 0.01 0.02 0.29 0.01 -Top-3 Union SVMRank 0.1157 0.7081 0.1990 0.53 0.10 0.63 - 0.01 0.28 0.50 0.93ListNet 0.1019 0.7287 0.1788 0.91 0.57 0.92 0.44 0.09 0.06 0.02 0.00 -RankNet 0.1029 0.7045 0.1796 1.00 1.00 1.00 - - - - - -RankBoost 0.1128 0.7109 0.1947 0.54 0.11 0.64 0.29 0.22 0.75 0.98 0.98 0.98Oracle SVMRank 0.2444 0.6933 0.3615 0.06 0.00 0.59 - 0.01 0.00 - 0.33ListNet 0.2773 0.7126 0.3993 0.11 0.00 0.72 0.12 0.04 0.01 0.00 - -RankNet 0.2643 0.6914 0.3824 0.15 0.01 0.85 - - - - - -RankBoost 0.2593 0.6956 0.3777 0.09 0.00 0.57 - 0.10 0.02 0.22 0.00 -Macro precision, recall and F1 at different top K levels. The highest scoring system F1 for each level (both union and oracle strategies) is shown in bold. The table alsoshows the individual contribution of the systems to the final score where italics scores indicate the highest contributing individual system(s) to each ensemble.in the ShARE/CLEF 2013 shared task which usuallyemployed machine learning on the labelled target domaindata to detect relevant spans of text for named entitiesand to filter the suggested concept identifiers so that theywere optimized for the detected spans. Both of these stepsled to substantial improvements on the results of theuncustomized individual systems that we report here. Webelieve that in particular the lack of a post-processing stepto filter concepts which did not directly appear in the textor were overlapping with other concepts led to substan-tially degraded precision than shared task participants.For example we found that our individual systems sug-gested many unannotated concepts related to the patientsuch as date of birth, gender, age and history of illnessas well as generic concepts that were part of more spe-cific ones. The best tuned system in the ShARE/CLEF2013 Task 1 (named entity recognition and normaliza-tion to SNOMED-CT at mention level) achieved an F1of 0.75 for named entity recognition and an accuracyscore of 0.59 for harmonization using strict matchingcriteria. Taken together with the F1 improvement weobserved in the ensemble approach, this finding rein-forces the generally held view that domain tuning is anecessary step to achieving high F1, even with relativelymature concept recognition tools such as the ones we haveemployed.Our choice of sentence-level concept harmonisationwas motivated by a use-case where the user requiresextraction of concepts from the document, e.g. for docu-ment or section classification, but does not require intra-sentential relationships between concepts, e.g. for textmining. The later would require mention-level harmon-isation by the four individual systems but our previousexperiments [38] have again indicated the challenge ofattempting this without some form of tuning. In futurework we would like to look at expanding our approach toexploit domain-adaptation methods, e.g. Latent DirichletAllocation (LDA), on mention-level annotation to allowCollier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 11 of 12Table 10 Type-based learn to rank on test dataID Sys P R F1 ID Sys P R F1T019 SVMRank 0.51 0.21 0.29 T047 SVMRank 0.52 0.66 0.58ListNet 0.59 0.18 0.28 ListNet 0.41 0.65 0.50RankNet 0.54 0.16 0.24 RankNet 0.38 0.51 0.44RankBoost 0.47 0.21 0.29 RankBoost 0.45 0.68 0.54T020 SVMRank 0.29 0.53 0.37 T048 SVMRank 0.48 0.68 0.56ListNet 0.34 0.50 0.41 ListNet 0.40 0.49 0.44RankNet 0.34 0.48 0.40 RankNet 0.38 0.48 0.43RankBoost 0.29 0.55 0.38 RankBoost 0.44 0.68 0.54T033 SVMRank 0.00 0.07 0.00 T184 SVMRank 0.52 0.62 0.57ListNet 0.00 0.27 0.00 ListNet 0.44 0.61 0.51RankNet 0.00 0.27 0.01 RankNet 0.40 0.58 0.47RankBoost 0.00 0.20 0.00 RankBoost 0.48 0.62 0.54T037 SVMRank 0.37 0.50 0.43 T190 SVMRank 0.28 0.69 0.40ListNet 0.35 0.46 0.40 ListNet 0.28 0.65 0.39RankNet 0.31 0.44 0.37 RankNet 0.25 0.55 0.34RankBoost 0.35 0.49 0.41 RankBoost 0.28 0.68 0.39T046 SVMRank 0.37 0.70 0.48 T191 SVMRank 0.27 0.54 0.36ListNet 0.37 0.70 0.48 ListNet 0.23 0.34 0.27RankNet 0.36 0.56 0.44 RankNet 0.17 0.24 0.20RankBoost 0.35 0.70 0.47 RankBoost 0.25 0.53 0.34Note that the highest scoring system F1 for each semantic type is shown in bold.direct comparison with the techniques employed inShARE/CLEF 2013.ConclusionsClinical phenotype recognition is essential for interpret-ing the evidence about human diseases in clinical recordsand the scientific literature. In this paper, we have eval-uated the F1 of four off-the-shelf concept recognitionsystems for identifying some of the building blocks inclinical phenotypes as well as disease-related concepts.Future work will have to develop additional filters forthis purpose. Our investigation of LTR techniques hasclearly shown that the methods we adopted are supe-rior to the off-the-shelf systems used separately but stillfall short of Oracle-based settings indicating that furtherenhancements are required in either feature selection orsampling.The tests have been run on the open gold-standardShARE/CLEF corpus harmonised to UMLS semantictypes. Findings indicate that cTAKES performs well com-pared to its peers but that annotation performance varieswidely across semantic types, and that MetaMap withstrict matching and word sense disambiguation can havesuperior precision. We presented an approach using sev-eral learn-to-rank methods that gave greatly improvedperformance across semantic types. The best ensemble- SVMRank - using the union tie-breaking strategy andthe oracle tie-breaking strategies achieved the Top-1 rank-ing level on training data. The results on the test datawere similar with both tie-breaking strategies at Top-1ranking.The results indicate the continued challenge of con-cept annotation and, in particular, the need to considerthe grammatical relations within phenotypementions.Wehave not yet tested the effectiveness of these approachesin an operational setting, e.g. for speed of processingor stability. We would like to extend our approach onfurther clinical benchmark data sets as they become avail-able in order to understand better the relative meritsof external feature sets such as FB3 and FB4. In theimmediate future, we plan on continuing to improve ourapproach by extending the distributed feature representa-tion employed in the meta-classifier, e.g. with LDA, andby exploring additional ways of sampling and combiningsystem outputs.Competing interestsThe authors declare that they have no competing interests.Authors contributionsNC, AO and TG formulated the experimental setup. AO and TG performed theexperiments. NC, AO and TG interpreted the results. NC, AO and TG wrote themanuscript. All authors read and approved the final manuscript.Collier et al. Journal of Biomedical Semantics  (2015) 6:24 Page 12 of 12AcknowledgementsWe gratefully acknowledge the kind permission of the ShARE/CLEF eHealthevaluation organisers for facilitating access to the ShARE/CLEF eHealth corpusused in our evaluation. Also we thank the anonymous reviewers for their kindcontribution to improving the final version of this paper. Nigel Colliersresearch is supported by the European Commission through the Marie CurieInternational Incoming Fellowship (IIF) programme (Project: Phenominer, Ref:301806). Tudor Grozas research is funded by the Australian Research Council(ARC) Discovery Early Career Researcher Award (DECRA)  DE120100508.Author details1University of Cambridge, Cambridge, UK. 2European Bioinformatics Institute(EMBL-EBI), Cambridge, UK. 3Wellcome Trust Sanger Institute, Cambridge, UK.4School of ITEE, the University of Queensland, St. Lucia, Australia. 5GarvanInstitute of Medical Research, Darlinghurst, Sydney, Australia.Received: 4 November 2014 Accepted: 1 April 2015JOURNAL OFBIOMEDICAL SEMANTICSLambrix et al. Journal of Biomedical Semantics  (2015) 6:12 DOI 10.1186/s13326-015-0002-8RESEARCH ARTICLE Open AccessCompleting the is-a structure in light-weightontologiesPatrick Lambrix1,2*, Fang Wei-Kleiner1 and Zlatan Dragisic1,2AbstractBackground: With the increasing presence of biomedical data sources on the Internet more and more researcheffort is put into finding possible ways for integrating and searching such often heterogeneous sources. Ontologiesare a key technology in this effort. However, developing ontologies is not an easy task and often the resultingontologies are not complete. In addition to being problematic for the correct modelling of a domain, suchincomplete ontologies, when used in semantically-enabled applications, can lead to valid conclusions being missed.Results: We consider the problem of repairing missing is-a relations in ontologies. We formalize the problem as ageneralized TBox abduction problem. Based on this abduction framework, we present complexity results for theexistence, relevance and necessity decision problems for the generalized TBox abduction problem with and withoutsome specific preference relations for ontologies that can be represented using a member of the EL family ofdescription logics. Further, we present algorithms for finding solutions, a system as well as experiments.Conclusions: Semantically-enabled applications need high quality ontologies and one key aspect is theircompleteness. We have introduced a framework and system that provides an environment for supporting domainexperts to complete the is-a structure of ontologies. We have shown the usefulness of the approach in differentexperiments. For the two Anatomy ontologies from the Ontology Alignment Evaluation Initiative, we repaired 94 and58 initial given missing is-a relations, respectively, and detected and repaired additionally, 47 and 10 missing is-arelations. In an experiment with BioTop without given missing is-a relations, we detected and repaired 40 newmissingis-a relations.Keywords: Ontologies, Ontology engineering, Ontology debuggingBackgroundWith the increasing presence of biomedical data sourceson the Internet more and more research effort is put intofinding possible ways for integrating and searching suchoften heterogeneous sources. Semantic Web technologiessuch as ontologies, are becoming a key technology inthis effort. Ontologies provide a means for modelling thedomain of interest and they allow for information reuse,portability and sharing across multiple platforms. Effortssuch as the Open Biological and Biomedical Ontologies(OBO) Foundry [1], BioPortal [2] and Unified MedicalLanguage System (UMLS) [3] aim at providing reposi-tories for biomedical ontologies and relations between*Correspondence: patrick.lambrix@liu.se1Department of Computer and Information Science, Linköping University,Linköping, Sweden2Swedish e-Science Research Centre, Linköping University, Linköping, Swedenthese ontologies thus providing means for annotating andsharing biomedical data sources. Many of the ontolo-gies in the biomedical domain, e.g., SNOMED [4] andGene Ontology [5], are, regarding knowledge represen-tation, light-weight ontologies. They are taxonomies orcan be represented using the EL description logic orsmall extensions thereof (e.g. [6] and the TONES Ontol-ogy Repository [7])a. Therefore, in this paper, we considerontologies that are represented by TBoxes in the EL fam-ily, which consist of axioms such as Carditis  Fracture,with the intended meaning that Carditis is a Fracture,where Carditis and Fracture are concepts and the relation-ship is an is-a relation. (For detailed syntax see SectionPreliminaries). A set of such terminological axioms is aTBox.Developing ontologies is not an easy task and often theresulting ontologies (including their is-a structures) are© 2015 Lambrix et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 2 of 26not complete. In addition to being problematic for the cor-rect modelling of a domain, such incomplete ontologiesalso influence the quality of semantically-enabled applica-tions. Incomplete ontologies when used in semantically-enabled applications can lead to valid conclusions beingmissed. For instance, in ontology-based search, queriesare refined and expanded by moving up and down thehierarchy of concepts. Incomplete structure in ontologiesinfluences the quality of the search results. As an exam-ple, suppose we want to find articles in PubMed [8] usingthe MeSH [9] term Scleral Disease. By default the querywill follow the hierarchy of MeSH and include more spe-cific terms for searching, such as Scleritis. If the relationbetween Scleral Disease and Scleritis is missing in MeSH,we will miss 922 articles in the search result, which isabout 57% of the original resultb. The structural informa-tion is also important information in ontology engineeringresearch. For instance, most current ontology alignmentsystems use structure-based strategies to find mappingsbetween the terms in different ontologies (e.g. overviewin [10]) and the modeling defects in the structure of theontologies have an important influence on the quality ofthe ontology alignment results.In this paper we tackle the problem of completing theis-a structure of ontologies. Completing the is-a structurerequires adding new correct is-a relations to the ontology.We identify two cases for finding relations which needto be added to an ontology. In case 1 missing is-a rela-tions have been detected and the task is to find ways ofmaking these detected is-a relations derivable in the ontol-ogy. There are many approaches to detect missing is-arelations, e.g., in ontology learning [11] or evolution [12],using linguistic [13] and logical [14,15] patterns, by usingknowledge intrinsic to an ontology network [16-21], or byusing machine learning and statistical methods [22-26].However, in general, these approaches do not detect allmissing is-a relations and in several cases even only few.Therefore, we assume that we have obtained a set of miss-ing is-a relations for a given ontology (but not necessarilyall). In the case where our set of missing is-a relations con-tains all missing is-a relations, completing the ontology iseasy. We just add all missing is-a relations to the ontol-ogy and a reasoner can compute all logical consequences.However, when the set of missing is-a relations does notcontain all missing is-a relations - and this is the commoncase - there are different ways to complete the ontology.The easiest way is still to just add the missing is-a relationsto the ontology. For instance, T in Figure 1 (and Figure 2)represents a small ontology inspired by Galen ontology(http://www.openclinical.org/prj_galen.html), that is rele-vant for our discussions. Assume that we have detectedthat Endocarditis  PathologicalPhenomenon and Gran-ulomaProcessNonNormalProcess are missing is-a rela-tions (M in Figure 1). Obviously, adding these relations tothe ontology will repair the missing is-a structure. How-ever, there are other more interesting possibilities. Forinstance, adding Carditis  CardioVascularDisease andGranulomaProcess  PathologicalProcess also repairs themissing is-a structure. Further, these is-a relations arecorrect according to the domain and constitute new is-a relations (e.g. Carditis  CardioVascularDisease) thatwere not derivable from the ontology and not originallydetected by the detection algorithmc. We also note thatfrom a logical point of view, adding Carditis  FractureandGranulomaProcessNonNormalProcess also repairsthe missing is-a structure. However, from the point ofview of the domain, this solution is not correct. Therefore,as it is the case for all approaches for dealing with model-ing defects, a domain expert needs to validate the logicalsolutions.In case 2 no missing is-a relations are given. In thiscase we investigate existing is-a relations in the ontologyand try to find new ways of deriving these existing is-arelations. This might pinpoint to the necessity of addingnew missing is-a relations to the ontology. As an exam-ple, let us assume that our ontology contains relationsT ? M in Figure 1. If we assume now that we wantto investigate new ways of deriving relations in M thenobviously adding Carditis  CardioVascularDisease andGranulomaProcess  PathologicalProcess would be onepossibility given that both are correct according to thedomain.The basic problem underlying the two cases can beformalized in the same way as a new kind of abduc-tion problem (formal definitions in Section Abductionframework). Abduction is a reasoning method to gen-erate explanations for observed symptoms and mani-festations. When the application domain is describedby a logical theory, it is called logic-based abduction[27]. Logic-based abduction is widely applied in diagno-sis, planning, and database updates [28], among others.Further, as we have seen above, there may be differ-ent ways to complete the is-a structure of ontologies.Therefore, we propose two preference criteria on the solu-tions for this new abduction problem as well as differentways to combine them and conduct complexity analy-sis on important decision problems regarding the variouspreference criteria for ontologies represented using ELor EL++.The contributions of this paper are the following. We formalize the repairing of the missing is-astructure in an ontology as a generalized version ofthe TBox abduction problem (GTAP). We present complexity results for the existence,relevance and necessity decision problems for GTAPin ontologies represented in EL and EL++ with andwithout the preference relations subset minimalityLambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 3 of 26Figure 1 Small EL example. (C is the set of atomic concepts in the ontology. T is a TBox representing the ontology. M is a set of missing is-arelations. Or is the oracle representing the domain expert).and semantic maximality as well as three ways ofcombining these (maxmin, minmax, skyline). Subsetminimality is a preference criterion that is often usedin abductive reasoning problems. Semanticmaximality is a new criterion that is important forGTAP. We provide algorithms for finding a skyline optimalsolution to GTAP in ontologies represented in ELand EL++. Although in theory, maxmin optimalsolutions are normally preferred, in practice, theycannot be guaranteed and skyline optimal solutionsare the best we can do. We provide a system and show its usefulness throughexperiments.MethodsPreliminaries - description logics EL and EL++Description logics are knowledge representation lan-guages. In description logics concept descriptions areconstructed inductively from a set NC of atomic conceptsand a set NR of atomic roles and (possibly) a set NI ofindividual names. The concept constructors for EL++ arethe top concept , the bottom concept ?, nominals, con-junction, existential restriction and a restricted form ofconcrete domains. In this paper, we consider the versionof EL++ without concrete domains. Note that this simpli-fication does not affect the complexity results presentedlater on. For the syntax of the different constructors seeTable 1.Figure 2 Graphical representation of the EL example in Figure 1. (Ovals represent concepts. Full arrows represent is-a relations between conceptsin the ontology. Dashed arrows represent missing is-a relations).Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 4 of 26Table 1 EL++ syntax and semanticsName Syntax SemanticsTop  IBottom ? ?Nominal {a} {aI }Conjunction C  D CI ? DIExistential ?r.C {x ? I |?y ? I :restriction (x, y) ? rI ? y ? CI}GCI CD CI ? DIRI r1 ? . . . ? rkr rI1 ? . . . ? rIk ? rIAn interpretation I consists of a non-empty set I andan interpretation function ·I which assigns to each atomicconcept A ? NC a subset AI ? I , to each atomic roler ? NR a relation rI ? I × I , and to each individ-ual name a ? NI an element aI ? I . The interpretationfunction is straightforwardly extended to complex con-cepts. An EL++ TBox (named CBox in [6]) is a finite set ofgeneral concept inclusions (GCIs) and role inclusions (RIs)whose syntax can be found in the lower part of Table 1.Note that a finite set of GCIs is called a general TBox. Aninterpretation I is a model of a TBox T if for each GCIand RI in T , the conditions given in the third column ofTable 1 are satisfied.EL has the restricted form of EL++ which allowsfor concept constructors of top concept , conjunctionand existential restriction. An EL TBox contains onlyGCIs.The main reasoning task for description logics is sub-sumption in which the problem is to decide for a TBoxT and concepts C and D whether T |= CD. Sub-sumption in EL++ is polynomial even w.r.t. generalTBoxes [6].Abduction frameworkIn the following we explain how the problem of find-ing possible ways to repair the missing is-a structure ina ontology is formalized as a generalized version of theTBox abduction problem as defined in [29]. We assumethat our ontology is represented using a TBox T in a lan-guage L which in this paper is EL or EL++. Further, wehave a set of missing is-a relations which are representedby a set M of atomic concept subsumptions. In case 1in Section Background, these missing is-a relations weredetected. In case 2 the elements in M are existing is-arelations in the ontology that are temporarily removed,and T represents the ontology that is obtained by remov-ing the elements in M from the original ontology. (Theycan later be added again after completing the ontology.)To complete the is-a structure of an ontology, the ontol-ogy should be extended with a set S of atomic conceptsubsumptions (repair) such that the extended ontology isconsistent and entails the missing is-a relations. However,the added atomic concept subsumptions should be correctaccording to the domain. In general, the set of all atomicconcept subsumptions that are correct according to thedomain are not known beforehand. Indeed, if this set weregiven then we would only have to add this to the ontol-ogy. The common case, however, is that we do not havethis set, but instead can rely on a domain expert that candecide whether an atomic concept subsumption is correctaccording to the domain. In our formalization the domainexpert is represented by an oracle Or that when given anatomic concept subsumption, returns true or false. It isthen required that for every atomic concept subsumptions ? S, we have that Or(s) = true. The following definitionformalizes this.Definition 1. (GENERALIZED TBOXABDUCTION) LetT be a TBox in language L and C be the set of all atomicconcepts in T. Let M = {Ai  Bi}ni=1 with Ai,Bi ? C be afinite set of TBox assertions. Let Or : {Ci Di | Ci,Di?C} ?{true, false}. A solution to the generalized TBox abductionproblem (GTAP) (T ,C,Or,M) is any finite set of TBoxassertions S = {Ei  Fi}ki=1 such that ?Ei, Fi : Ei, Fi ? C,?Ei, Fi : Or(Ei  Fi) = true, T ? S is consistent andT ? S |= M. The set of all such solutions is denoted asS(T ,C,Or,M).As an example, consider GTAP P as defined in Figure 1.Then {Carditis  CardioVascularDisease, Inflammation-Process  PathologicalProcess, GranulomaProcess InflammationProcess} is a solution for P . Anothersolution is {Carditis  CardioVascularDisease, Granu-lomaProcess  PathologicalProcess} as shown in SectionBackground.There can be many solutions for a GTAP and, asexplained in Section Background, not all solutions areequally interesting. Therefore, we propose two preferencecriteria on the solutions as well as different ways to com-bine them. The first criterion is a criterion that is notused in other abduction problems, but that is particularlyimportant for GTAP. In GTAP it is important to find solu-tions that add to the ontology as much information aspossible that is correct according to the domain. There-fore, the first criterion prefers solutions that imply moreinformation.Definition 2. (MORE INFORMATIVE) Let S and S? betwo solutions to the GTAP (T ,C,Or,M). S is said to bemore informative than S? iff T ? S |= S? and T ? S? |= S.Further, we say that S is equally informative as S? iff T ?S |= S? and T ? S? |= S.Lambrix et al. Journal of Biomedical Semantics  (2015) 6:12 Page 5 of 26Consider two solutions to P , S1 = {Inflammation-Process  PathologicalProcess, GranulomaProcess InflammationProcess}d and S2 = {InflammationProcess PathologicalProcess, GranulomaProcess  Pathologi-calProcess}. In this case solution S1 is more informativethan S2.Definition 3. (SEMANTIC MAXIMALITY) A solution Sto the GTAP (T ,C,Or,M) is said to be semantically max-imal iff there is no solution S? which is more informativethan S. The set of all semantically maximal solutions isdenoted as Smax(T ,C,Or,M).An example of a semantically maximal solution to Pis {InflammationProcess  PathologicalProcess, Granu-lomaProcess  InflammationProcess, Carditis  Cardio-VascularDisease}.The second criterion is a classical criterion in abduc-tion problems. It requires that no element in a solution isredundant.Definition 4. (SUBSET MINIMALITY) A solution S tothe GTAP (T ,C,Or,M) is said to be subset minimal iffthere is no proper subset S?  S such that S? is a solu-tion. The set of all subset minimal solutions is denoted asSmin(T ,C,Or,M).An example of a subset minimal solution for Pis {InflammationProcess  PathologicalProcess, Granu-lomaProcess InflammationProcess}. On the other hand,solution {Carditis  CardioVascularDisease, Inflamma-tionProcess  PathologicalProcess, GranulomaProcess InflammationProcess} is not subset minimal as it containsCarditis  CardioVascularDisease which is redundant forrepairing the missing is-a relations.In practice, both of the above two criteria are desir-able. We therefore define ways to combine these criteriadepending on what kind of priority we assign for the singleJOURNAL OFBIOMEDICAL SEMANTICSFu et al. Journal of Biomedical Semantics  (2015) 6:8 DOI 10.1186/s13326-015-0004-6RESEARCH ARTICLE Open AccessSupporting the annotation of chronic obstructivepulmonary disease (COPD) phenotypes with textmining workflowsXiao Fu1*, Riza Batista-Navarro1,2, Rafal Rak1 and Sophia Ananiadou1AbstractBackground: Chronic obstructive pulmonary disease (COPD) is a life-threatening lung disorder whose recentprevalence has led to an increasing burden on public healthcare. Phenotypic information in electronic clinicalrecords is essential in providing suitable personalised treatment to patients with COPD. However, as phenotypes areoften hidden within free text in clinical records, clinicians could benefit from text mining systems that facilitatetheir prompt recognition. This paper reports on a semi-automatic methodology for producing a corpus that canultimately support the development of text mining tools that, in turn, will expedite the process of identifyinggroups of COPD patients.Methods: A corpus of 30 full-text papers was formed based on selection criteria informed by the expertise of COPDspecialists. We developed an annotation scheme that is aimed at producing fine-grained, expressive andcomputable COPD annotations without burdening our curators with a highly complicated task. This wasimplemented in the Argo platform by means of a semi-automatic annotation workflow that integrates several textmining tools, including a graphical user interface for marking up documents.Results: When evaluated using gold standard (i.e., manually validated) annotations, the semi-automatic workflowwas shown to obtain a micro-averaged F-score of 45.70% (with relaxed matching). Utilising the gold standard datato train new concept recognisers, we demonstrated that our corpus, although still a work in progress, can fosterthe development of significantly better performing COPD phenotype extractors.Conclusions: We describe in this work the means by which we aim to eventually support the process of COPDphenotype curation, i.e., by the application of various text mining tools integrated into an annotation workflow.Although the corpus being described is still under development, our results thus far are encouraging and showgreat potential in stimulating the development of further automatic COPD phenotype extractors.Keywords: Corpus annotation, Phenotype curation, Automatic annotation workflows, Ontology linking, Corpora forclinical text mining, Chronic obstructive pulmonary diseaseBackgroundAn umbrella term for a range of lung abnormalities,chronic obstructive pulmonary disease (COPD) pertains tomedical conditions in which airflow from the lungs is re-peatedly impeded. This life-threatening disease, known tobe primarily caused by tobacco smoke, is not completely* Correspondence: xiao.fu-2@manchester.ac.ukEqual contributors1National Centre for Text Mining, School of Computer Science, University ofManchester, Manchester Institute of Biotechnology, 131 Princess Street,Manchester, UKFull list of author information is available at the end of the article© 2015 Fu et al.; licensee BioMed Central. ThisAttribution License (http://creativecommons.oreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.reversible and is incurable. COPD was ranked by theWorld Health Organization as the fifth leading cause ofdeath worldwide in 2002, and is predicted to become thethird by year 2030. Estimates have also shown that themortality rate for COPD could escalate by at least 30%within the next decade if preventive measures are not im-plemented [1].The disease and clinical manifestations of COPD areheterogeneous and widely vary from one patient toanother. As such, its treatment needs to be highly perso-nalised in order to ensure that the most suitable therapyis an Open Access article distributed under the terms of the Creative Commonsrg/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 2 of 11is provided to a patient. COPD phenotyping allows forwell-defined grouping of patients according to theirprognostic and therapeutic characteristics, and thus in-forms the development and provision of personalisedtherapy [2].The primary approach to recording phenotypic infor-mation is by means of electronic clinical records [3].However, as clinicians at the point of care use free textin describing phenotypes, such information can easilybecome obscured and inaccessible [4]. In order to ex-pedite the process of identifying a given patients COPDgroup, the phenotypic information locked away withinthese records needs to be automatically extracted anddistilled for the clinicians perusal.Capable of automatically distilling information expressedin natural language within documents, text mining can beapplied on clinical records in order to efficiently extractCOPD phenotypes of interest. However, the developmentof sophisticated text mining tools is reliant on the availabil-ity of gold standard annotated corpora, which serve asevaluation data as well as provide samples for training ma-chine learning-based approaches.This paper presents our ongoing efforts on the annota-tion of COPD phenotypes in a collection of scientific pa-pers. In our previous publication [5] on which this workis built upon, we proposed to form a corpus of clinicalrecords from the Multiparameter Intelligent Monitoringin Intensive Care II (MIMIC II) Clinical Database [6,7].However, our UK-based expert collaborators (i.e., stake-holders who will incorporate our text mining technologyinto their systems in the near future) recently pointedout that there are substantial discrepancies between thehospital system in the US (on which MIMIC II is fo-cussed) and that in the UK. After considering their ad-vice, we decided to utilise scientific articles from variousCOPD-relevant journals, rather than build a corpus ofclinical records which are highly US-specific. As previ-ous work demonstrated techniques which successfullyextracted information from unseen data even if thetraining/development data used was of a different docu-ment type [8], we believe that a gold standard corpus offull scientific articles should still allow for the develop-ment of phenotype extraction tools for clinical records.Nevertheless, our collaborators are still currently work-ing on obtaining a subset of clinical records from theirown hospital, which will also be annotated to becomepart of an augmented version of our corpus.In embarking on this effort, we are building a resourcethat will support the development of text mining methodsfor the automatic extraction of COPD phenotypes fromfree text. We envisage that such methods will ultimatelyfoster the development of applications which will enablepoint-of-care clinicians to more easily and confidentlyidentify a given COPD patients group, potentially leadingto the provision of the most appropriate personalisedtreatment. Furthermore, text mining methods can beemployed in order to facilitate the linking of COPD phe-notypes with genotypic information contained in pub-lished scientific literature.In the remainder of this paper, we firstly provide a re-view of the state of the art (Related Work). We proceedto describing our methods for corpus development(Methods), including our strategy for document selec-tion followed by our proposed annotation scheme. Adiscussion of our text mining-assisted annotation work-flow is also provided. We then share the results and ana-lysis of our evaluation (Results and Discussion). Lastly,we conclude the paper with a summary of our contribu-tions and an overview of ongoing and future work.Related workVarious corpora have been constructed to support thedevelopment of clinical natural language processing(NLP) methods. Some contain annotations formed onthe basis of document-level tags indicating the specificdiseases that clinical reports pertain to. In the 2007Computational Medicine Challenge data set [9], radi-ology reports were assigned codes from the ninthrevision of the International Classification of Diseases-Clinical Modification (ICD-9-CM) terminology [10]. Insimilar corpora, chest X-ray reports were manually la-belled with any of four pneumonia-related concepts [11]whilst any of 80 possible disease names were assigned todocuments in another collection of clinical records [12]with the assistance of automatic tools MetaMap Transfer(MMTx) [13] for concept recognition and NegEx [14]for negation detection. Whilst suitable for evaluating in-formation retrieval methods, such document-level anno-tations cannot sufficiently support the extraction ofphenotypic concepts which are described in clinical re-cords in largely variable ways, making it necessary forautomated methods to perform analysis by looking attheir actual mentions within text.Several other clinical corpora were thus enriched withtext-bound annotations, which serve as indicators ofspecific locations of phenotypic concept mentions withintext. For instance, all mentions of signs or symptoms,medications and procedures relevant to inflammatorybowel disease were marked up in the corpus developedby South et al [15]. Specific mentions of diseases andsigns or symptoms were similarly annotated under theShARe scheme [16,17] and additionally linked to termsin the SNOMED Clinical Terms vocabulary [18]. Whilstthe scheme developed by [19] had similar specifications,it is unique in terms of its employment of an automatictool to accelerate the annotation process. One difficultyencountered by annotators following such scheme, how-ever, is with manually mapping mentions of phenotypicFu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 3 of 11concepts to vocabulary terms, owing to the high degreeof variability in which these concepts are expressed intext. For instance, many signs or symptoms (e.g., gradualprogressive breathlessness), cannot be fully mapped toany of the existing terms in vocabularies.Alleviating this issue are schemes which were designedto enrich corpora with finer-grained text-bound annota-tions. The Clinical e-Science Framework (CLEF) annota-tion scheme [20] which defined several clinical concepttypes and relationships, required the decomposition ofphrases into their constituent concepts which were thenindividually assigned concept type labels and linkedusing any of their defined relationships. Also based on afine-grained annotation approach is the work by Mun-gall et al. [21] on the ontology-driven annotation ofinter-species phenotypic information based on the EQmodel [22]. Although their work was carried out withthe help of the Phenote software [23] for storing, man-aging and visualising annotations, the entire curationprocess was done manually, i.e., without the support ofany NLP tools. The effort we have undertaken, in con-trast, can be considered as a step towards automatingsuch EQ model-based fine-grained annotation of pheno-typic information.In this regard, our work is unique amongst annotationefforts within the clinical NLP community, but sharessimilarities with some phenotype curation pipelinesemployed in the domain of biological systematics. Cura-tors of the Phenoscape project [24] manually linkEQ-encoded phenotypes of fishes to the Zebrafish ModelOrganism Database using Phenex [25] which is a tool formanaging character-by-taxon matrices, a formal approachused by evolutionary biologists. To accelerate this process,Phenex has been recently enhanced with NLP capabilities[26] upon the integration of a text analytic known asCharaParser [27]. Based on a combination of bootstrap-ping and syntactic parsing approaches [28], CharaParsercan automatically annotate structured characteristics oforganisms (i.e., phenotypes) in text, but currently does nothave full support for linking concepts to ontologies [29].Also facilitating the semi-automatic curation of systemat-ics literature is GoldenGATE [30], a stand-alone applica-tion modelled after the GATE framework [31], whichallows for the combination of various NLP tools into textprocessing pipelines. It is functionally similar to our Web-based annotation platform Argo [32] in terms of itssupport for NLP workflow management and manual valid-ation of automatically generated annotations. However,the latter fosters interoperability to a higher degree byconforming to the industry-supported Unstructured Man-agement Information Architecture [33] and allowingworkflows to be invoked as Web services [34].By producing our proposed fine-grained phenotypeannotations which are linked to ontological concepts, weare representing them in a computable form thus mak-ing them suitable for computational applications such asinferencing and semantic search. The Phenomizer tool[35], for instance, has demonstrated the benefits of en-coding phenotypic information in a computable format.Leveraging the Human Phenotype Ontology (HPO) [36]whose terms are linked to diseases in the Online Men-delian Inheritance in Man (OMIM) vocabulary [37], itsupports clinicians in making diagnoses by semanticallysearching for the medical condition that best matchesthe HPO signs or symptoms given in a query. We envis-age that such an application, when integrated with a re-pository of phenotypes and corresponding clinicalrecommendations, e.g., Phenotype Portal [38] and thePhenotype KnowledgeBase [39], can ultimately assistpoint-of-care clinicians in more confidently providingpersonalised treatment to patients. Our work on the an-notation of COPD phenotypes aims to support the de-velopment of similar applications in the future.MethodsWe describe in this section our strategies for collectingdocuments for the corpus and our proposed annotationscheme. We also elaborate on the technology behind ourtext mining-assisted annotation methodology.Document selectionIn forming our corpus, we collected pertinent journal ar-ticles from the PubMed Central Open Access subset(PMC OA). As a preliminary step, we retrieved a list ofjournals which are most relevant to COPD by queryingPMC OA using the keywords chronic, obstructive,pulmonary, disease, respiratory and lung. This re-sulted in ten journal titles whose archives were thensearched for the keywords chronic obstructive pulmon-ary disease and COPD. A total of 974 full-text articleswere retrieved in this manner. The journal titles and art-icle distribution over them are shown in Figure 1.Upon consideration of our constraints in terms of re-sources such as time and personnel, we decided to trimdown the document set to 30 full articles. This was carriedout by compiling a list of COPD phenotypes based on thecombination of terms given by our domain experts andthose automatically extracted by Termine [40] from theCOPD guidelines published jointly by the American Thor-acic Society and the European Respiratory Society in 2004[41]. The resulting term list (provided as Additional file 1)contains 1,925 COPD phenotypes which were matchedagainst the content of the initial set of 974 articles. In orderto ensure that the documents in our corpus is representa-tive of the widest possible range of COPD phenotypes, weranked the documents according to decreasing number oftheir contained unique matches. We then selected the 30246135733962239BMC Pulmonary MedicineClinical Medicine Insights.Circulatory, Respiratory andPulmonary MedicineInternational Journal ofChronic ObstructivePulmonary DiseasePulmonary CirculationPulmonary MedicineHeart, Lung and VesselsPreventing Chronic DiseaseFigure 1 Distribution of COPD-relevant articles over COPD-focussed journals. A total of 974 full-text articles were retrieved from 10 journalsin the PubMed OpenAccess subset.Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 4 of 11top-ranked articles as the final document set for ourcorpus.A simple yet expressive annotation schemeTo capture and represent phenotypic information, wedeveloped a typology of clinical concepts (Table 1) tak-ing inspiration from the definition of COPD phenotypespreviously proposed [2], i.e., a single or combination ofdisease attributes that describe differences between indi-viduals with COPD as they relate to clinically meaning-ful outcomes (symptoms, exacerbations, response totherapy, rate of disease progression, or death). Afterreviewing the semantic representations used in previousclinical annotation efforts, we decided to adapt andTable 1 The proposed typology for capturing COPD phenotypType Description1) Problem an overall category for any COPD indications ofa) MedicalCondition* any disease or medical condition; includes COPcomorbiditiesb) RiskFactor* a phenotype signifying a patients increased chhaving COPDi) SignOrSymptom* an observable irregularity manifested by a COPii) IndividualBehaviour* a patients habits leading to susceptibility of haiii) TestOrMeasureResult* findings based on COPD-relevant examinations2) Treatment any medication, therapy or program for treating3) TestOrMeasure an overall category for any COPD-relevant examor measures/parametersa) RadiologicalTest any of the radiological tests for detecting COPDb) MicrobiologicalTest an examination of a COPD- relevant specimenc) PhysiologicalTest a measurement of a COPD patients capacity toTypes marked with an asterisk (*) were adapted from the PhenoCHF scheme.harmonise concept types from the annotation schemesapplied to the 2010 i2b2/VA Shared Task data set [42]and the PhenoCHF corpus [43]. In the former, conceptsof interest were categorised into broad types of problem,treatment and test/measure. However, it was determinedupon consultation with clinical experts that a finer-grained typology is necessary to better capture COPDphenotypes. For this, we looked into the semantic typesused in the annotation of phenotypes for congestiveheart failure in the PhenoCHF corpus, which are fine-grained yet generic enough to be applied to other med-ical conditions. We adapted some of those types andorganised them under the upper-level types of the i2b2/VA scheme.esExample(s)concern frequent exacerbatorD emphysema, pulmonary vascular disease, asthma,congestive heart failureances of increased levels of the c-reactive protein, alpha1 antitrypsindeficiencyD patient chronic cough, shortness of breath, purulent sputumproductionving COPD smoking for 25 yearsincreased white blood cell counts, FEV1 45% predictedCOPD oxygen therapy, pulmonary rehabilitation, pursed lipsbreathinginations increased compliance of the lung, FEV1, FEV1/FVC ratiocomputed tomography scanning, high resolution computedtomographycomplete blood countexercise 6-min walking distanceFu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 5 of 11Most phenotypes exemplified in Table 2 span fullphrases, especially in the case of risk factors such as in-creased compliance of the lung, chronic airways obstruc-tion and increased levels of the c-reactive protein. Someof the previously published schemes for annotating clin-ical text have proposed the encoding of phenotypesusing highly structured, expressive representations. Forthe symptom expressed as chronic airways obstruction,for example, the CLEF annotation scheme [20] recom-mends its annotation to consist of a has_location rela-tionship between chronic obstruction (a condition) andairways (locus). The EQ model for representing pheno-types [21], similarly, would decompose this phenotypeinto the following elements: airways as entity (E) andchronic obstruction as quality (Q). Whilst we recognisethat such granular representations are ideal for the pur-poses of knowledge representation and automatedknowledge inference, we feel that requiring them as partof the manual annotation of free-text documents signifi-cantly complicates the task for domain experts who maylack the necessary background in linguistics.We therefore propose an annotation methodology thatstrikes a balance between simplicity and granularity ofannotations. On the one hand, our scheme renders theannotation task highly intuitive by asking for only simpletext span selections, and not requiring the creation ofrelations nor the filling in of template slots. On the otherhand, we also introduce granularity into the annotationsby exploiting various semantic analytic tools, describedin the next section, which automatically identify con-stituent ontological concepts. The contribution of apply-ing automated concept identifiers is two-fold. Firstly,automatic concept identification as a pre-annotationstep helps accelerate the manual annotation process bysupplying visual cues to the annotators. For instance, thesymptom expressed within text as increased resistance ofthe small airways becomes easier for an annotator toTable 2 Examples of phenotypic information represented usinCOPD Phenotypes Automatically recognizedunderlying conceptschronic airways obstruction chronic airways obstructionparenchymal destruction parenchymal destructiondecrease in rate of lung function decrease in rate lung functionchronic bronchitis N/Amyocardial infarction N/Aenhanced response to inhaledcorticosteroidsenhanced response to corticosteroFEV1 45% predicted FEV1alpha1 antitrypsin deficiency alpha1 antitrypsin deficiencyrecognise, seeing that the elementary concepts resistanceand airways have been pre-annotated. Secondly, as theconstituent concepts will be linked to pertinent ontol-ogies, the semantics of the expression signifying thesymptom, which will be manually annotated as a simpletext span, is nevertheless encoded in a fine-grained andcomputable manner. Shown in Table 2 are some exam-ples of annotated phenotypes resulting from the applica-tion of our scheme.Text mining-assisted annotation with ArgoOur proposed methodology employs a number of textanalytics to realise its aims of reducing the manual effortrequired from annotators and providing granular com-putable annotations of COPD phenotypes. After analys-ing several documents, we established that treatmentsare often composed of drug names (e.g., Coumadin inCoumadin dosing) whilst problems typically containmentions of diseases/medical conditions (e.g., myocar-dial infarction), anatomical concepts (e.g., airways inchronic airways obstruction), proteins (e.g., alpha1 anti-trypsin in alpha1 antitrypsin deficiency), qualities (e.g.,destruction in parenchymal destruction) and tests (e.g.,FEV1 in FEV1 45% predicted). These observations, con-firmed by COPD experts, guided us in selecting theautomatic tools for recognising the above-mentionedtypes and for linking them to relevant ontologies.We used Argo [32], an interoperable Web-based textmining platform, to both integrate our elementary analyticsinto a processing workflow and to manage its execution.Argos rich library of processing components gives its usersaccess to various text analytics ranging from data readersand writers to syntactic tools and concept recognisers.From these, we selected the components which are mostsuitable for our tasks requirements, and arranged them ina multi-branch automatic annotation workflow, depictedin Figure 2. The workflow begins with a Document Readerg our proposed annotation schemeAutomatically linked ontological conceptschronic (PATO:0001863) respiratory airway (UBERON:0001005)obstructed (PATO:0000648)parenchyma (UBERON:0000353) damaged (PATO:0001167)decreased rate (PATO:0000911) lung (UBERON:0002048)function (PATO:0000173)chronic bronchitis (DOID:6132)myocardial infarction (DOID:5844)ids enhanced (PATO:0001589) response to (PATO:0000077)corticosteroid (ChEBI:50858)Forced Expiratory Volume 1 Test (NCIT:C38084)alpha-1-antitrypsin (PR:000014678) decreased amount(PATO:0001997)Figure 2 Our semi-automatic annotation workflow in Argo.Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 6 of 11that reads the records from our corpus, followed by theCafetiere Sentence Splitter which detects sentence bound-aries. Resulting sentences are then segmented into tokensby the GENIA Tagger which also provides part-of-speech(POS) and chunk tags, and additionally recognises proteinmentions [44].After running the syntactic tools, the workflow splitsinto four branches. The first branch performs joint anno-tation of concepts pertaining to Problem, Treatment andTestOrMeasure by means of the NERsuite [45] compo-nent, a named entity recogniser (NER) based on an imple-mentation of conditional random fields [46]. Suppliedwith a model trained on the 2010 i2b2/VA challenge train-ing set [47], this NER is employed to provide domain ex-perts with automatically generated cues which could aidthem in marking up full phrases describing COPD pheno-types. Meanwhile, the NERsuite component in the secondbranch is configured to recognise disease mentions usinga model trained on the NCBI Disease corpus [48]. Thethird branch performs drug name recognition using theChemical Entity Recogniser, an adaptation of NERsuiteemploying chemistry-specific features and heuristics [49]which was parameterised with a model trained on theDrug-Drug Interaction (DDI) corpus [50]. Finally, bymeans of the Truecase Asciifier, Brown, OBO Anatomyand UMLS Dictionary Feature Extractors, the last branchextracts various features required by the Anatomical En-tity Tagger which is capable of recognising anatomicalconcepts [51]. The Annotation Merger component col-lects annotations produced by the various concept recog-nisers whilst the Manual Annotation Editor allows humanannotators to manually correct, add or remove automatic-ally generated annotations via its rich graphical user inter-face (Figure 3).Finally, the workflows last component, the XMI Writer,stores the annotated documents in the XML MetadataInterchange standard format, which allows us to reuse theoutput in other workflows if necessary. Eventually, the an-notations can be made available in several other standardformats, such as RDF and BioC [52], which will be accom-plished directly in Argo through its various serialisationcomponents. We note that the automatic tool for recog-nising qualities is still under development, as are the com-ponents for linking mentions to concepts in ontologies.Nevertheless, we describe below our proposed strategy forontological concept identification.Linking phenotypic mentions to ontologiesIn order to identify the ontological concepts underlyingCOPD phenotypic information, the mentions automatic-ally annotated by our concept recognisers will be nor-malised to entries in various ontologies, namely, thePhenotype and Trait Ontology (PATO) [53] for qualities,Human Disease Ontology (DO) [54] for medical condi-tions, Uber Anatomy Ontology (UBERON) [55] for ana-tomical entities, Chemical Entities of Biological Interest(ChEBI) [56] for drugs, Protein Ontology (PRO) [57] forproteins and the National Cancer Institute Thesaurus(NCIT) [58] for tests/measures.The NCBO Annotator [59], formerly Open BiomedicalAnnotator, offers a solution to this problem by employ-ing a Web service that automatically matches textagainst specific ontologies. It is, however, not sufficientfor the requirements of our task as it is very limited interms of variant-matching [60], obtaining only exactstring matches against terms and synonyms contained inontologies. As observed from the examples in Table 2,there is a large variation in the expressions comprisingCOPD phenotypes. Consequently, many of these expres-sions do not exist in ontologies in the same form. Moresuitable, therefore, is a sophisticated normalisationmethod that takes into consideration morphological var-iations (e.g., alpha1 antitrypsin vs. alpha-1-antitrypsin),inflections (e.g., obstruction vs. obstructed), syntacticFigure 3 The user interface for linking mentions to ontologies.Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 7 of 11variations (e.g., decrease in rate vs. decreased rate) andsynonym sets (e.g., deficiency vs. decreased amount anddestruction vs. damage).Argos library includes several automatic ontology-linking components employing approximate string match-ing algorithms [61]. Furthermore, the Manual AnnotationEditor provides a user-friendly interface for manuallysupplying or correcting links to ontological concepts(Figure 4). Ongoing development work on improving thisontology-linking tool includes: (a) enhancement of thenormalisation method by the incorporation of algorithmsfor measuring syntactic and semantic similarity, and (b)shifting from Argos currently existing ontology-specificlinker components to a generic one that allows for linkingmentions against any ontology (from a specified set). Onceready, the new component will be added to Argos library.Instances of the component will then be integrated intoour semi-automatic workflow to facilitate the linking ofannotated mentions to the respective ontologies.Results and discussionAfter applying the Argo workflow described above on the30 articles in our corpus, we asked one of our collaboratingFigure 4 The Manual Annotation Editors graphical user interface. Thefiner-grained COPD phenotype annotations.domain experts to manually validate the automatically gen-erated annotations. In this section, we present the resultsof two types of evaluation. Firstly, the quality of the Argo-generated concept annotations was measured by compar-ing them against gold standard data, i.e., the annotationsmanually validated by the domain expert. Secondly, we car-ried out a preliminary evaluation of the gold standard an-notations that we have obtained thus far by utilising themin the development of machine learning-based conceptrecognisers. It is worth noting that our gold standard datais currently limited to our experts annotations on onlynine out of the 30 papers that she has examined thus far(equivalent to 1,701 sentences). Table 3 presents the num-ber of unique concepts for each type, as manually anno-tated by our domain expert. One can see that the mostprevalent types are Treatment, RiskFactor, MedicalCondi-tion, TestOrMeasure, Drug and AnatomicalConcept (inorder of decreasing frequency).Table 4 depicts the evaluation of Argos automaticallygenerated annotations against the gold standard, pre-sented by concept type. We note that only the five mostfrequently occurring concept types (which are commonbetween the manually validated annotations we have atarticle excerpt shown is annotated using our proposed scheme forTable 3 Number of unique concepts for each type, basedon the nine manually annotated articlesConcept type Number of uniqueconceptsTreatment 430RiskFactor 415MedicalCondition 371TestOrMeasure 282Drug 192AnatomicalConcept 96Quality 59Protein 40Total 1,885Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 8 of 11hand and the automatically generated annotations) wereincluded in the evaluation. Two different modes ofmatching were applied: exact matching, which considersa system annotation as correct only if it has the sameconcept type label and exactly the same boundaries as agold standard annotation; and relaxed matching, whichcounts even a partially overlapping system annotation ascorrect as long the non-overlapping tokens consist ofonly articles and modifiers (i.e., they have only DT, JJor RB as POS tags). We note that for a given pheno-typic expression, not only the full string is being evalu-ated, but also each of its subsumed concepts. It can beobserved from Table 4 that in general, the semi-automatic workflow obtains unsatisfactory performanceusing exact matching. After performing some error ana-lysis, we observed that majority of discrepancies werebrought about by the incorrect inclusion or exclusion ofarticles or modifiers in noun phrases, e.g., phospho-diesterase inhibitor (for a nonselective phosphodiesteraseinhibitor), an acute exacerbation (for acute exacerba-tion). Thus we next employed relaxed matching, whichrevealed that the semi-automatic workflow obtains mod-erate performance over all evaluated concept types (ex-cept for TestOrMeasure).Table 4 Evaluation of annotations automatically generated bdataExact matchingPrecision RecallAnatomicalConcept 0.1923 0.7527Drug 0.5861 0.2744MedicalCondition 0.0290 0.2842TestOrMeasure 0.1425 0.0680Treatment 0.3080 0.1494Micro-average 0.2670 0.2283Macro-average 0.3037 0.3057Results are reported for only nine full-text papers.It is obviously more desirable for a semi-automaticworkflow to approximate the gold standard annotations(i.e., to produce exact matches rather than partial ones).Nevertheless, Argos automatically generated annotationsproved to be helpful in a number of cases. For example,the automatic workflow was able to correctly annotatepartially correct annotations such as sputum (for sputumsmear), pulmonary (for pulmonary TB) and COPD-sta-ging (for COPD) served as visual cues to the annotator.Based on her experience in annotating our corpus, shefeels that having pre-supplied annotations, albeit incom-plete or incorrect, is preferable over not having any an-notations at all. We are, however, aware of the potentialbias that having pre-supplied annotations may bringabout, i.e., failure to annotate concepts completelymissed by automatic annotation due to reliance on visualcues. To avoid this scenario, the annotator has beenasked to read all of the sentences thoroughly and to keepin mind that the cues are not to be relied on. She hasadhered to this guideline throughout her annotations.Applying the gold standard annotations to an informa-tion extraction task, we employed NERsuite, an imple-mentation of the conditional random fields (CRFs)algorithm, to develop a new set of concept recognisers.Samples were represented using features which are by de-fault extracted by NERsuite, including character, token,lemma and part-of-speech tag n-grams (within a distanceof 2 from the token under consideration), chunk tags, aswell as a comprehensive set of orthographic features (e.g.,presence of uppercase or lowercase letters, digits, specialcharacters). The resulting models were then evaluated intwo ways. Firstly, for each concept type, models weretrained and subsequently evaluated in a 10-fold cross-validation manner, whose results are presented in Table 5alongside those obtained by the Argo components. In gen-erating the folds, the articles were split at the paragraphlevel, giving a total of 381 shorter documents. Secondly, tofacilitate evaluation on unseen data, each of the automat-ically and manually annotated subset of nine papers wassubdivided into training (75% or 286 paragraphs) andy the text mining-assisted workflow against gold standardRelaxed matchingF-score Precision Recall F-score0.3063 0.2814 0.9038 0.42920.3738 0.7921 0.6463 0.71180.2868 0.3697 0.6313 0.46630.0920 0.1914 0.1039 0.13470.2012 0.4688 0.4015 0.43250.2462 0.4050 0.5243 0.45700.3047 0.4207 0.5374 0.4719Table 5 Results of 10-fold cross validation of concept recognisers, using exact matchingConcept recognisers currently in Argo Concept recognisers trained on our corpusPrecision Recall F-score Precision Recall F-scoreAnatomicalConcept 0.2361 0.6617 0.3428 0.7602 0.4990 0.5912Drug 0.7318 0.2161 0.3283 0.8576 0.4499 0.5873MedicalCondition 0.3986 0.2436 0.3010 0.8510 0.4590 0.5932TestOrMeasure 0.0766 0.0182 0.0289 0.6850 0.3190 0.4332Treatment 0.4330 0.1021 0.1635 0.8276 0.3458 0.4829Micro-average 0.3305 0.1776 0.2310 0.7929 0.3970 0.5291Macro-average 0.3752 0.2483 0.2988 0.7963 0.4145 0.5452Performance is compared with that of the components utilised in the text mining-assisted workflow.Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 9 of 11held-out data (25% or 95 paragraphs). Models trained onthe former were then evaluated using annotations con-tained in the latter. Table 6 presents the evaluation resultsunder this setting.We show that by using our gold standard annotations astraining data, we were able to develop concept recogniserswhose performance is drastically better than those weemployed in our semi-automatic workflow. This signifi-cant improvement ranged from 24.84 (for Anatomical-Concept) to 40.43 (for TestOrMeasure) percentage pointsaccording to 10-fold cross validation, and from 19.49 (forAnatomicalConcept) to 40.45 (for TestOrMeasure) ac-cording to the fixed split evaluation. This implies that ourcorpus can stimulate the development of more suitableautomatic COPD phenotype extractors. We expect that asmore gold standard annotations become available to us (i.e., as our domain expert completes the validation of moredocuments in our corpus), the better equipped we will bein boosting the performance of our automatic COPD con-cept recognisers.ConclusionsIn this paper, we elucidate our proposed text mining-assisted methodology for the gold-standard annotation ofCOPD phenotypes in a corpus of full-text scientific arti-cles. We demonstrate with the proposed scheme that theannotation task can be kept simple for curators whilstTable 6 Results of evaluation using a fixed split over 381 parset: 25% or 95 paragraphs), using exact matchingConcept recognisers currently in ArgoPrecision Recall F-scorAnatomicalConcept 0.2602 0.6145 0.3656Drug 0.6885 0.1900 0.2979MedicalCondition 0.4494 0.2492 0.3206TestOrMeasure 0.0250 0.0041 0.0070Treatment 0.4111 0.0847 0.1404Micro-average 0.3735 0.1614 0.2254Macro-average 0.3669 0.2285 0.2816producing expressive and computable annotations. Byconstructing a semi-automatic annotation workflow inArgo, we seamlessly integrate and take advantage of sev-eral automatic NLP tools for the task. Furthermore, weare providing the domain experts with a highly intuitiveinterface for creating and manipulating annotations. Thecomparison of annotations automatically generated by theworkflow against manually validated ones (i.e., gold stand-ard) reveals an F-score of 45.70% using relaxed matching.New concept recognisers trained on these gold standardannotations demonstrate dramatically better performance(i.e., with a 20- to 30-percentage point margin in terms ofF-scores) over the off-the-shelf components used in theArgo workflow.Manual expert validation of the text mining-generatedannotations on the remaining 21 papers in the corpus isstill ongoing. In the meantime, we are enhancing ourontology concept linkers, which, once ready, will be appliedon the gold standard concepts to enrich our corpus withcomputable annotations. Our expert collaborators are alsoworking hard on obtaining a subset of clinical records fromtheir hospital, which will then be used to augment our cor-pus. With the resulting resource, which will be made pub-licly available upon completion, we aim to support thedevelopment and evaluation of text mining systems thatcan ultimately be applied to evidence-based healthcare andclinical decision support systems.agraphs (training set: 75% or 286 paragraphs; held-outConcept recognisers trained on our corpuse Precision Recall F-score0.8000 0.4314 0.56050.7966 0.4196 0.54970.8673 0.3899 0.53800.6719 0.2966 0.41150.8400 0.2903 0.43150.8034 0.3552 0.49260.7952 0.3656 0.5009Fu et al. Journal of Biomedical Semantics  (2015) 6:8 Page 10 of 11Additional fileAdditional file 1: List of COPD phenotypes used to retrieve articlesfrom the PubMed OpenAccess subset.Competing interestsThe authors declare that they have no competing interests.Authors contributionsXF carried out the collection of documents for the corpus, the design of theannotation scheme and evaluation of the automatic tools. RB participated inthe design of the annotation scheme, supervised the annotation andformulated the evaluation strategies. RR contributed towards thedevelopment of the text mining-assisted workflow and in enriching the Argolibrary with necessary components. XF, RB and RR drafted the manuscript. SAprovided research direction and supervised all steps of the work. All authorsread and approved the final manuscript.AcknowledgementsThe authors would like to thank Drs. Nawar Bakerly and Andrea Short of theSalford Royal NHS Foundation Trust and University of Manchester, who haveprovided their expertise on COPD to guide the clinical aspects of this work.Special mention goes to Andrea who has graciously contributed a significantamount of her time to provide us with annotations.The first author is financially supported by the University of Manchesters2013 President's Doctoral Scholar Award. This work is also partially supportedby the Medical Research Council (Supporting Evidence-based Public HealthInterventions using Text Mining [Grant MR/L01078X/1]) and by the DefenseAdvanced Research Projects Agency (Big Mechanism [Grant DARPA-BAA-14-14]).Author details1National Centre for Text Mining, School of Computer Science, University ofManchester, Manchester Institute of Biotechnology, 131 Princess Street,Manchester, UK. 2Department of Computer Science, University of thePhilippines Diliman, Quezon City 1101, Philippines.Received: 11 November 2014 Accepted: 22 February 2015JOURNAL OFBIOMEDICAL SEMANTICSWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 DOI 10.1186/s13326-015-0021-5RESEARCH ARTICLE Open AccessDiscovering relations between indirectlyconnected biomedical conceptsDirk Weissenborn1,2*, Michael Schroeder2 and George Tsatsaronis2AbstractBackground: The complexity and scale of the knowledge in the biomedical domain has motivated research worktowards mining heterogeneous data from both structured and unstructured knowledge bases. Towards this direction,it is necessary to combine facts in order to formulate hypotheses or draw conclusions about the domain concepts.This work addresses this problem by using indirect knowledge connecting two concepts in a knowledge graph todiscover hidden relations between them. The graph represents concepts as vertices and relations as edges, stemmingfrom structured (ontologies) and unstructured (textual) data. In this graph, path patterns, i.e. sequences of relations,are mined using distant supervision that potentially characterize a biomedical relation.Results: It is possible to identify characteristic path patterns of biomedical relations from this representation usingmachine learning. For experimental evaluation two frequent biomedical relations, namely has target, and may treat,are chosen. Results suggest that relation discovery using indirect knowledge is possible, with an AUC that can reachup to 0.8, a result which is a great improvement compared to the random classification, and which shows that goodpredictions can be prioritized by following the suggested approach.Conclusions: Analysis of the results indicates that the models can successfully learn expressive path patterns for theexamined relations. Furthermore, this work demonstrates that the constructed graph allows for the easy integration ofheterogeneous information and discovery of indirect connections between biomedical concepts.Keywords: Relation discovery, Biomedical concepts, Text miningBackgroundMotivation and objectivesKnowledge discovery is an important field of research,especially in the biomedical domain, in which the scaleand growth of accumulated knowledge of all kinds isalready beyond the capabilities of a single human to keepup with. This has motivated research towards miningknowledge from heterogeneous data of both structuredand unstructured knowledge bases (KBs). The parallel useof structured and unstructured data is important becausethey are complementary. Structured KBs contain explicitbut inadequately covered knowledge. In contrast, unstruc-tured KBs contain nearly all of the domain specific knowl-edge but lack in simplicity with regards to automatedanalysis.*Correspondence: dirk.weissenborn@dfki.de1DFKI Projektbüro Berlin, Alt-Moabit 91c, 10559 Berlin, Germany2Biotechnology Center, Technische Universität Dresden, Tatzberg 47/49,01307 Dresden, GermanyAn example of how fast the reporting of scientificfindings grows in this domain is illustrated in Figure 1,where the number of scientific publications indexed byPubMed is shown to be increasing in an exponentialfashion over the past decades. Similar findings can beobserved for structured data by examining the growth of arepresentative database in the biomedical domain, namelythe Unified Medical Language System (UMLS), shown inFigure 2.Besides the obstacles that the large scale of the databrings into the task of extracting information there is alsothe issue of combining pieces of knowledge together tocover as many aspects as possible which can potentiallylead to new knowledge. For example, typical informationextraction techniques focusing on drugs aim at extract-ing targets, adverse effects and indications, which cannotsucceed by limiting the applied methods to a small frag-ment of drug related information. Hence, it is necessaryto combine facts in order to formulate hypotheses ordraw conclusions about the domain concepts. This work© 2015 Weissenborn et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 2 of 19Figure 1 Growth of PubMed indexed scientific literature since 1965.The figure plots the number of PubMed indexed articles per year, forthe period 1965-2010. The plot shows that the indexed literaturegrows exponentially (blue line). In parallel, the annotation of thePubMed articles with MeSH terms has so far managed to follow thisgrowth (red line)a.attempts to address this problem by using indirect knowl-edge connecting two concepts to discover hidden relationsbetween them.In contrast to relation extraction, which aims at rec-ognizing direct mentions of relations within a sentenceor document between two concepts, relation discoverybetween indirectly connected concepts attempts to findFigure 2 Growth of UMLS Metathesaurus in the past decade. In thisplot, the growth of the UMLS metathesaurus in terms of number ofincluded concepts is presented. The plot refers to the period from2002 until the present. For this past period, the growth curve is steep,approximating an exponential tendencyb.hidden, yet unknown relations that can be derived fromsequences of already known and established facts. Thefirst reported and most famous discovery of this kind wasthe finding of Swanson in 1986 that fish oil may treatRaynauds syndrome [1]. He came to this conclusion bycombining the two simple facts from different scientificstudies that on the one hand fish oil has beneficial effectson blood viscosity and on the other hand patients suf-fering from Raynauds syndrom demonstrate increasedblood viscosity. Until that time there was no direct con-nection between the concepts of fish oil and Raynaudssyndrome, but only an indirect connection through theconcept of blood viscosity, which indicated that fish oilmay treat Raynauds syndrome which was indeed verifiedin 1989.To be able to extract indirect connections between con-cepts, knowledge from all sources is represented by agraph comprising concepts as vertices and labelled edgesconnecting the concepts. Edges are created by eitherextracting explicit knowledge from structured databasesin form of triples or by analysing unstructured textualdata. The idea of using graphs to represent knowledgeto find connections between concepts is not novel. Ithas been exploited in both ontology based [2] and liter-ature based approaches [3]. Representing knowledge insuch a way provides a simple framework that is poten-tially easy to interpret, makes the integration of hetero-geneous data straightforward and is useful for findingindirect connections, i.e. paths, in the graph betweenconcepts.The task of finding connections between two conceptsand identifying their meaning is called relation discov-ery. Besides being able to recognize that some connec-tion exists it is also important to understand what kindof relation is expressed to discover the hidden relationsrepresented by the given connections.Relation discovery is performed on top of the aforemen-tioned graph representation by using supervised machinelearning to learn path patterns that frequently occurbetween concept pairs of a specific relation and cantherefore be considered characteristic for that relation.A trained model can in turn be used to discover aspecific relation between indirectly connected conceptpairs.This work extends the study for DILS 2014 by theintroduction of a new approach and the validation ona manually created drug repositioning dataset. Further-more, the approach is explained and discussed in greatdetail with additional inspections into the clustering ofrelations by LDA. Our main contributions lie in thejoint exploitation of linguistic information and struc-tured knowledge in a simple, extensible graph repre-sentation for fully automatized, indirect discovery ofrelations.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 3 of 19Related workMost work on knowledge discovery from unstructured,textual data focuses on extracting relations betweentwo concepts mentioned in one sentence. This is veryimportant for many applications such as the curationof databases. However, in his famous work Swansonhas shown the potential of combining facts from dif-ferent sources to discover new, yet unknown knowledge[1].Recently, many studies have been conducted on find-ing hidden relations between concepts indirectly. Most ofthese works are purely based on statistical analysis of con-cept co-occurrence profiles fromMEDLINE, which differsfrom our approach in that they do not take any linguisticinformation into account, e.g., Frijters et al. [4] and Cohenet al. [5].Srinivasan P. et al. [6] developed a system that discov-ers relations by searching for interesting paths betweentwo concepts from a start concept of interest througha set of co-occurring concepts of predefined types fromMEDLINE that in turn co-occur with a set of potentialtarget concepts of predefined types without exploitingexisting linguistic information. The main difference isthat the whole process is manually guided and intendedto aid scientists in the search of new relations whereasour approach is completely automated. Furthermore, nomachine learning is applied to find interesting connec-tions, but only a hand-made weighting scheme based onthe ideas of TF-IDF. More sophisticated studies buildingupon this idea include the work of Hristovski et al. [7] andVidal et al. [8].BioLiterate, a system developed by Goertzel et al. [9],is designed to discover relations which are not containedin any individual abstract using probabilistic inference.In contrast to this work their approach is based on acollection of hand-built rules, that map linguistic con-structs onto a probabilistic reasoning system. Further-more, it does not make use of any structured knowledgebase.Arguably the most similar work to ours is the work ofLao et al. [10]. As in the current work, the authors usea combination of structured and unstructured knowledgeto infer relations between concepts using a sequence ofrelated concepts. They use an open domain, web-scalecorpus to train a classifier based on logistic regressionwith a huge amount of training examples represented byvectors of a very large feature space. However, the require-ments of this work, namely a limited amount of trainingdata and amuch smaller textual corpus, require a differentway of modeling and training.Table 1 provides an overview of the aforementionedworks in comparison to the current approach with respectto different aspects concerning the requirements and usedmethodologies.Table 1 Comparison of related work with respect to: use ofliguistic information, use of manually designed rules,application in restricted domain, possibility of usingsparse training dataWork Linguistic Manual Restricted SparseGoertzel et. al (2006) [9] x x x xFrijters et al. (2010) [4] x (x)Cohen et al. (2010) [5]Lao et al. (2012) [10] xSrinivasan et al. (2004) [6] x x xCurrent work x x xMethodsTerminologyIn this work an atomic piece of knowledge is consid-ered as a triple (ci, l, cj), consisting of a pair of concepts(ci, cj), e.g., (aspirin, inflammation), and a label l, e.g.,maytreat, representing a relation Rl to which the pair (ci, cj)belongs. Furthermore, indirect knowledge connecting twoconcepts cs and ct is defined as a sequence of triples start-ing with concept cs and ending in concept ct , where thesecond concept of each triple must be equal to the firstconcept of its following triple. Table 2 summarizes thenotation used in this article.Utilized biomedical knowledge sourcesNowadays, plenty of data is freely available and easy toaccess, but each data source has a different knowledgerepresentation, called a schema. The schema defines howconcepts can be described and how they can relate to eachother. For structured knowledge sources such as databasesTable 2 Summary of the terminology and notation usedthroughout themanuscriptSymbol Explanationci a conceptC a set of conceptsl a label representing a relationRl binary relation with label l(ci , l, cj)/triple a pair of concepts (ci , cj) connected by relation with label lR a set of triplesG the knowledge graphP a path in Gf a feature vectorE+/E? a set of positive/negative examples? set of model parametersX observable variables or observations defined by a modelH hidden or latent variables defined by a modelWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 4 of 19the set of relations and concepts as well as their rep-resentations are well defined, whereas for unstructuredknowledge sources like text this is not the case. Naturallanguage is far more expressive than the schemas of anystructured knowledge source because it is not restrictedto a fixed set of concepts and relations, but at the sametime it is much harder to interpret because natural lan-guage can express different pieces of knowledge with thesame representation (polysemy), and one piece of knowl-edge in many different ways (synonymy). In the following,we describe the different biomedical knowledge sourcesthat are used in this work, and how they have beenutilized.UnifiedMedical Language SystemThe most popular structured knowledge base for biomed-ical text mining is the Unified Medical Language Sys-tem (UMLS), which consists of 3 different resources,namely the Metathesaurus, the Semantic Network andthe Specialist Lexicon. The Metathesaurus is a multi-lingual vocabulary database which combines knowledgefrom many different structured knowledge sources. Itcontains a large amount of biomedical concepts, infor-mation about them (e.g., their semantic type, a descrip-tion, etc.) and how they are related to each other. TheSemantic Network comprises a set of semantic typesand relations connecting the semantic types to eachother. It provides a consistent, semantic categorizationof Metathesaurus concepts. The Specialist Lexicon is ageneral English lexicon consisting of biomedical termswhich is not used in this work. The roughly 3 millionconcepts contained in the Metathesaurus of the 2013ABRelease form the basis of the knowledge representation inthis work which means that concepts from other knowl-edge sources have to be mapped to concepts of theMetathesaurus.DrugBankDrugBank [11] is an open drug and drug-target database.A target of a drug refers to a protein that a drug isable to bind to. DrugBank is not yet part of the UMLS.Therefore, a mapping fromDrugBank to UMLS ids is nec-essary, which can partially be achieved by mapping theirrespective concept names to each other. By following thisapproach it is possible to map 1125 targets (proteins orgenes) and 2663 drugs from DrugBank to UMLS, whichresults in a total of 1228 distinct drug-target pairs mappedfrom all FDA-approved drug-target pairs documented inDrugBank.MEDLINEAs unstructured knowledge source MEDLINE is used.It is the collection of all publication abstracts fromall life-science journals indexed by PubMed. MEDLINEis the most widely used, freely available textual cor-pus for biomedical text mining. Furthermore, an alreadyannotated version of MEDLINE exists. The annota-tion is performed frequently by the National Libraryof Medicine (NLM) using the MetaMap program [12],which annotates natural language text with conceptsof the UMLS Metathesaurus. The 2012 MetaMappedMEDLINE corpus is used as unstructured textual knowl-edge source containing all publications until November18, 2011.When using the MetaMapped MEDLINE corpus careshould be taken. E.g., MetaMap has problems annotatinggenes with aliases which are common english words suchas impact or rare. In this work we exclude gene/proteinannotations for common english words. Furthermore,we only consider annotations of UMLS concepts of thefollowing semantic types or their respective subtypes:Organisms, Clinical Drug, Substances, Sign or Symptom,Anatomical Structure, Molecular Sequence, Body Spaceor Junction, Body Location or Region, Pathologic Func-tion, Injury or Poisening.Dependency treesDependency trees [13] are syntactic constructs of sen-tences in which each node of the tree represents a token(word or symbol) of the underlying sentence and eacharch represents a dependency between two tokens ofthat sentence. In dependency grammars (DG) the verbalways takes the central role of the sentence and istherefore always the root of the tree independent fromthe rest. Furthermore DGs do not require any order-ing of the sentence words and are thus also applicableto languages in which the order of words in a sen-tence is all the same (e.g., in Czech or Turkish). Unlikephrase structure grammars (constituency grammars) DGsdo not explicitly structure sentences into phrases butrely only on dependencies between words in a sentence[14]. An example of a dependency tree is shown inFigure 3.Knowledge representationIntegrating knowledge from heterogeneous data sourcesinto one coherent representation (schema) is a complexprocess. One of the main difficulties is the mapping ofthe concepts in each data source to each other. This hasbeen done for all used knowledge sources in this work asexplained in their respective description. Mapping rela-tions from different knowledge sources to each other iseven more complicated and can potentially result in a lossof information. To circumvent this problem relations arekept explicitly in the form they occurred in the sourceswhich results in a huge relation space with a lot of redun-dancy. The relation space, however, can be reduced byusing semantic vector representations for the relationsWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 5 of 19Figure 3 An example of a dependency tree of a sentence. The dependency tree of the following sentence is illustrated: Aspirin is used in thetreatment of inflammation and not nasal polyps .obtained by applying co-occurrence based dimensional-ity reduction algorithms as will be described later in thedescription of the encoding.Knowledge graphIn order to find indirect connections between conceptsquickly, both structured and unstructured knowledge isrepresented by a graph. Similar to the work of [10] adirected, edge-labelled graph G = (C,R) is used, com-prising a set of concepts C as vertices and a set of labellededges (triples) R = C × L × C between them, whereL denotes the set of all possible relation labels. If thereis a pair (ci, cj) ? Rl in one of the knowledge sources,an edge (ci, l, cj) is added to R. In other words, only if apair of concepts is known to be in a relation with labell, then there is an edge labelled with l in G connect-ing this pair of concepts. Note that a triple can occurmore than once in R, which means that R is actually amultiset. A path of concepts P in G of length n is an n-tuple of vertices P = (c1, ..., cn), where ?i, 1 ? i < n :?l ? L : (ci, l, ci+1) ? R, meaning that there must beat least one edge between the concepts ci and ci+1 forevery i.Knowledge extractionStructured knowledge sources, such as UMLS and Drug-Bank, already contain labelled relations Rl ? C × C. Theinformation of all relations Rl, i.e. its concept pairs (ci, cj)together with its label l, can directly be inserted into thegraph by adding all concepts ci and cj of all pairs to Cas vertices and all corresponding triples (ci, l, cj) to R asedges.Extracting triples from unstructured, textual datarequires a more elaborate strategy. Since MEDLINE, theused textual data, is already annotated with biomedicalconcepts of the UMLS Metathesaurus, this task reducesto extracting only the relations between concepts foundin one sentence. Previous work on relation extraction hasshown that the dependency path between two conceptsin a sentence typically contains all necessary informa-tion to recognize a specific underlying relation betweenthem (e.g., [15-18]). A dependency path is a path in adependency tree, which is a syntactic construct of a sen-tence as explained in the previous section. It is importantnot to confuse the notion of dependency path, which areedges in G, with the notion of a path in the knowledgegraph G.Triples are only extracted from sentences when a pairof concepts, or more precisely their headwords in thedependency tree, connected by a dependency path arefound that contains at least one verb form. If the depen-dency path does not contain any verb form, it is assumedthat there is no relation present in this sentence. Onthe other hand, if two or more verb forms are found onthe dependency path which are part of two distinct sub-sentences connected by some conjunction, it is assumedthat there is no direct relation in the sentence betweensuch concept pairs present and these triples are discardedas well. Furthermore, conjunction and apposition edgesare removed from the dependency paths together withtheir head words because in most cases they representsimple enumerations which do not effect the semanticsof the relation being expressed between the two con-cepts in question. If there is a negated noun or verbform present on the dependency path, the whole pathwill be treated as negated as well. Furthermore, thereis also the issue of extracting triples connected by verylong dependency paths. Long dependency paths can beWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 6 of 19very unspecific and confusing, and they are more likely tocontain parsing errors. Moreover, such paths occur usu-ally very rarely in the corpus which makes them hard tointerpret when using statistical methods. After manualinspection a maximum length of 6 was chosen to pre-vent that. Once a pair of concepts (ci, cj) is extracted froma sentence together with its post-processed dependencypath p, a triple (ci, p, cj) can be inserted into the knowledgegraph G the same way as for structured knowledge. Notethat there is no mapping from the extracted dependencypaths to any specific predefined relation label. Therefore,every possible dependency path can be viewed as a singlerelation.As an example of this procedure it is possible to extractthe following triples from the sentence shown in Figure 3: (aspirin, nsubjpass? use prep? in pobj? treatment prep?of pobj? , inflammation) (aspirin, neg nsubjpass? use prep? in pobj? treatment prep?of pobj? ,nasal polyps)In the second triple the conj-sequence is removed fromthe path and the overall path is negated because it containsa negated noun phrase.For both unstructured and structured triples (ci, l, cj)in the knowledge graph, there is always an inverse triple(cj, l?1, ci). During extraction these inverses are excludedand only one triple (ci, l, cj) is added to the graph toavoid including redundant information. However, duringpath search we also consider triple (cj, l?1, ci) to beexistent.An example sub-graph of the resulting knowledge graphcan be found Figure 4.ModellingIn order to discover that a pair of concepts cs (source)and ct (target) is in a relation q in question using indi-rect connections between them, i.e., paths in G of lengthgreater than 2, a model must be trained to recognizetypical graph path patterns for q from positive and neg-ative training pairs. Direct connections are excluded toavoid explicit inference of the relation in question. Dur-ing the application of the model, a set of graph pathsextracted between cs and ct are presented to the modelwhich in turn calculates a confidence score between 0 and1 of assigning label q to the concept pair (cs, ct) in ques-tion. In the following we describe these steps in moredetail.Two types of models are considered for modelling theproblem of discovering a relation between a pair of con-cepts given a set of paths connecting them. The first typeof modelling directly extracts features from the set ofpaths between a pair and uses the resulting feature vec-tor as input for any kind of vector classifier. This methodwill be referred to as the pair-based approach. A secondapproach is based on a model that assigns confidencescores to graph paths rather than to the pairs themselves.Given all graph paths and their respective scores betweentwo concepts, the final confidence score for the con-cept pair is calculated by averaging its best k graph pathFigure 4 A sub-graph of the knowledge graph. This sub-graph consists of example paths connecting the two concepts C0000545(Eicosapentaenoic Acid) and C1825292 (FFAR1 gene) which are part of the has target relation.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 7 of 19scores. The problem of considering all paths and averag-ing them to form the final score is that nearly all graphpaths are uninformative with respect to the relation thatexists between the source and the target concept. Thiswould introduce a lot of noise and disturb the resultingscores.Relation label encodingEncoding a graph path P = (c1, ..., cn) requires an encod-ing of each connection (ci, ci+1) in P as a feature vectorf(ci,ci+1), resulting in a sequence of feature vectors of length(n ? 1) which is used by the model for training of infer-ence, respectively. f(ci,ci+1) is defined as the sum of allfeature vectors of each relation label l occurring betweenci and ci+1 (see Equation 1). Feature vectors for all possi-ble relation labels l can be created in different ways. Thefollowing two sections explain how this is achieved in thiswork.f(ci,ci+1) =?(ci,l,ci+1)?Rfl (1)One-of-N encoding. The simplest way of encoding arelation label l is the one-of-N encoding, where only thel-dimension of the feature vector has value 1 and all oth-ers are 0, as the name of suggests. This encoding, however,is very poor because it does not take any semantic simi-larities or even synonymy among the relation labels intoaccount, which leads to an explosion of the feature spacegrowing as large as there are different relations. Especiallyfor unstructured relation labels (i.e., dependency paths)there are many ways of expressing the same underlyingrelation, resulting in a lot of redundancy. With a largenumber of training examples this can be handled by themodel, but if training examples are sparse, there is a needof encoding relations in a much smaller semantic featurespace or otherwise the model will overfit to the trainingdata.Semantic encoding. Mapping relation labels into asemantic space has already been done in other studiessuch as the work of Yao et al. [19]. Extracting seman-tic vectors for words co-occurring in documents is a wellstudied problem and thus, there are numerous algorithmsthat solve this task. Examples are latent semantic analysis(LSA, [20]), reflective random indexing (RRI, [5]), the gen-eralization of principle component analysis (gPCA, [21])or latent Dirichlet allocation (LDA, [22]). The basic ideafor constructing a semantic space of relations is to con-sider a pair of connected vertices, i.e. concepts, (ci, cj) inG as a document di,j and the label of each edge betweenthem as a word occurring in di,j. Using this transformationfor all connected concept pairs of G, the above mentionedalgorithms can be used natively to construct semantic fea-ture vectors of a specified size for each relation label or incase of LDA even for each concept pair at the same time.In the experiments LDA is used because its underlyingmodel fits well to this problem.By transforming pairs of concepts to documents andrelation labels to words, LDAs latent topics can be con-sidered as the true but hidden relations between apair of concepts. Each true relation has many differ-ent forms of representations in natural language textor databases. At the same time, the number of possi-ble true relations between a pair of concepts is usuallyvery low and in many cases even one, thus they are alsovery sparse. These two aspects can be reflected in LDAby setting the hyper-parameters of the model to some-thing well below 1. The idea of modeling relations withLDA was already investigated in a similar form by Yaoet al. [23].In case of using LDA features, we define fl as the con-ditional probability distribution over all possible latenttopics t given relation label l:f tl = p(t|l)p(t|l) ? p(t) · p(l|t), (2)where f tl is the value of the t-th dimension of fl.p(t) and p(l|t) are directly extracted from the trainedLDA model. Furthermore, in case of LDA, all pair fea-ture vectors f(ci,cj) are also normalized after summingover all feature vectors of labels occurring on edgesbetween (ci, cj).In order to validate that LDA is able to learn semanticvector representations of relations, the 15 most occurringdependency paths of the 100 semantically most similardependency paths to themay treat and has target relationwere extracted. The resulting sets of relations are shown inTable 3. From the examples, it can be seen that most of theextracted dependency paths for both of the relations areactually textual representations of them, which supportsthe claim that semantic vectors of relations can indeed belearned using LDA.Pair-based approachFor pair-based classification a vector classifier must betrained which takes as input a feature vector. Similar tothe previous work [10] a logistic regression model is usedin this approach. Given a set of graph paths Pcs,ct , thefeature vector fcs,ct for (cs, ct) is defined as the normal-ized sum of the feature vectors representing the pathsP ? Pcs,ct . The feature vector of a path P = (c1, ..., cn)is calculated from its corresponding sequence f(ci,ci+1) ?RN of feature vectors by transforming their outer prod-Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 8 of 19Table 3 The 15most popular relations taken from the 100semantically closest relations to the has target andmaytreat relationRelation Most similar relationshas targetdobj??? form prep??? with pobj???nsubjpass?????? degrade agent???? by pobj???nn?? activity nsubjpass?????? inhibit agent???? by pobj???nsubj??? inhibit dobj??? phosphorylation prep??? of pobj???nsubj??? show dobj??? affinity prep??? for pobj???nsubjpass?????? show xcomp???? interact prep??? with pobj???dep??? form dobj???nsubj??? inhibit prep??? in pobj??? presence prep??? of pobj???nsubjpass?????? cross ? linked prep??? to pobj???dep??? form nsubjpass??????dobj??? inhibit prep??? with pobj???nsubj??? potentiate dobj??? activity prep??? of pobj???nsubjpass?????? prepare agent???? by pobj??? reaction prep??? of pobj???nn?? substrate prep??? include pobj???nsubj??? act prep??? by pobj???may treatpobj??? with prep??? patient nsubjpass?????? treat prep??? with pobj???nsubj??? be prep??? in pobj??? treatment prep??? of pobj???nsubj??? be attr?? treatment prep??? for pobj???nn?? patient partmod????? treat prep??? with pobj???nsubjpass?????? use prep??? in pobj??? treatment prep??? of pobj???nsubjpass?????? use prep??? for pobj??? treatment prep??? of pobj???pobj??? with prep??? treat prep??? for pobj???dobj??? receive prep??? for pobj???attr?? be prep??? in pobj??? treatment prep??? of pobj???pobj??? with prep??? patient rcmod???? treat prep??? with pobj???nsubjpass?????? administer prep??? to pobj??? patient prep??? with pobj???nsubjpass?????? use prep??? in pobj??? patient prep??? with pobj???dobj??? use prep??? in pobj??? patient prep??? with pobj???nsubj??? improve prep??? in pobj??? patient prep??? with pobj???nsubj??? have prep??? in pobj??? patient prep??? with pobj???uct which is an (n ? 1)-dimensional tensor into a vectorrepresentation.fcs,ct =?P?Pcs ,ct fP????P?Pcs ,ct fP??? (3)fP = ?(f(c1,c2) ? · · · ? f(cn?1,cn))(4)The resulting vector consists of Nn?1 dimensions,where each dimension corresponds to a tuple in{1, 2, · · · ,N}(n?1) which represents a position in the for-mer tensor. The following equation is an example of trans-forming the outer product of two 4-dimensional vectors uand v into a vector representation.? (u ? v) = ?(uvT)= ?????????u1v1 u1v2 u1v3 u1v4u2v1 u2v2 u2v3 u2v4u3v1 u3v2 u3v3 u3v4u4v1 u4v2 u4v3 u4v4????????=????????????u1v1u1v2u1v3u1v4u2v1· · ·u4v3u4v4????????????One problem of using this type of encoding is theexponentially growing feature space depending on themaximumpath lengthm. The pair-based approach in con-junction with the plain one-of-N encoding has a featurespace in which each dimension corresponds to a sequenceof relation labels. The model learns which of those rela-tion label sequences are characteristic for the relation themodel is being trained on which is reflected by a high pos-itive weight for the corresponding dimension. This is veryuseful for the interpretation of what themodel has learnedbecause the extraction of highly weighted relation labelsequences from the trained model is very easy.Path-based approachFor path-based classification a binary sequence classifiermust be trained, which takes as input a sequence of fea-ture vectors f(ci,ci+1) ? RN (see Equation 1) constructedfrom the graph path P = (c1, ..., cn) in question and out-puts a confidence score. This can be modelled by usingany kind of vector-classifier which takes as input a featurevector of length (m · N), if the maximum possible lengthm of a sequence is known, or it is possible to use a propersequence classifier. For the former logistic regression andfor the latter a combination of two hidden markov mod-els, one trained on positive example paths for q (pHMM)and the other only trained on negative example paths(nHMM), were chosen. In case of the HMMs a path P isapplied to both HMMs during inference and the probabil-ity of sequence P given the respective HMM is computed.The confidence score of assigning label q to path P isfinally calculated by combining the two probabilities in thefollowing way:pHMM(q|P) = p(P, q)p(P, q) + p(P,¬q)= p(P|q)p(P|q) + p(P|¬q) (5)Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 9 of 19where p(P|q) = ppHMM(P) denotes the probability ofP calculated by the positive HMM and p(P|¬q) =pnHMM(P) the probability calculated by the negativeHMM, assuming p(q) = p(¬q), which is a strong assump-tion. In reality p(¬q) >> p(q), however, this would puttoo much weight on the outcome of the negative HMM.In practice, we are typically more interested in finding agood ordering of tuple candidates for a relation instead ofreal probabilities for single candidate tuples based on theoutput of both models.The advantage of this path-based approach over thepair-based model is that the feature space grows only lin-early with the maximum path length m in the case oflogistic regression and it is even constant with increasingm for the HMM approach.Graph path discoveryTo extract paths for a concept pair (cs, ct) to a maximumpath length m, a bidirectional search [24] is performed,i.e., searching is done by starting in both vertices cs andct until a maximum path length of?m+12?from each sideis reached. Although search is still exponential in timeand space complexity, it requires only the square root ofresources compared to naive search from the source tothe target vertex. Finally, similar to the work of [10], theprobability of expanding the search to a neighbor ver-tex cj from the current vertex ci is given by the followingformula:pexplore(cj|ci) = min(1,?h + |N(ci)||N(ci)|)(6)where N(c) denotes the set of neighbors of vertex c andh is a big number (e.g., 100,000 in this work). Usually thenumber of neighbors is not very high, which means thatin most cases every neighbor will be explored.Example paths of different sizes (2-4) can be seen inFigure 4, which illustrates a sub-graph of the knowl-edge graph containing paths between a drug and itstarget.TrainingModels are trained using distant supervision, whichassumes that paths between a pair of concepts of relationq are representing q and can therefore be considered posi-tive training examples for q. Even though this assumptionis strong it has been shown to be very effective in previousstudies [25,26].Training examples for a specific relation q can directlybe extracted from its relation Rq ? C × C contained in atleast one of the structured knowledge sources (e.g., Drug-Bank and/or UMLS). Amodel for relation label q is trainedwith a set of positive training examples E+q ? Rq and aset negative training examples E?q ? C × C, which is con-structed from E+q by pairing all source concepts of E+q witha random target concept of E+q , ensuring that Rq?E?q = Ø.By using the same concepts in both the positive and thenegative training set, it is ensured that the model doeslearn only about the paths between the pairs rather thanalso learning characteristics about the different conceptsof the two training sets.Given a set of positive (E+q ) and negative training exam-ples (E?q ) for relation label q, a graph path classifier istrained on all extracted graph paths for each concept pairof E+q and E?q . For HMMs, standard EM training (Baum-Welch algorithm) is applied, and for logistic regression,training is performed using gradient ascent on the likeli-hood function using LBFGS with L2-regularization.As described previously, only very few of the graphpaths extracted between concepts of a positive pair arereal indicators for the relation label q. This is a problemwhen training the path-based classifier model, because itmeans that most of the extracted positive example graphpaths, which are the paths between concepts of a positiveconcept pair are actually negative or noisy examples. Todeal with this problem most of the noise from the pos-itive path training examples can be removed as follows.First, a model is trained on the initial, noisy examples.Subsequently, the trained model is used to score all pos-itive graph paths in order to eliminate noisy paths byonly keeping those positive graph paths that were scoredhigher than a specific threshold (e.g., 0.5 in our case). Inturn, a completely new classifier model can be trainedon the pruned set of positive graph paths and the orig-inal set of negative graph paths. This procedure can berepeated several times, though once was already enoughin our experiments. Training the path-based HMM clas-sifier in such a way has shown to be more effectivein the conducted experiments and a clearer separationbetween the distribution of the confidence scores of thepositive compared to the negative training examples wasobserved.Finally, for learning semantic relation vectors the LDAmodel was trained using the efficient sparse stochasticinference algorithm developed by [27], which is particu-larly useful when dealing with huge amounts of trainingdata.Results and discussionGraph generationFor the already annotated MEDLINE corpus of 2012,triples were extracted by extracting dependency pathsbetween two annotated concepts in each sentence.ClearNLP [28] was used for dependency parsing, becauseWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 10 of 19it is very fast and provides existing models trained onmedical text. The resulting set of triples was stored in atitan [29] graph database. During extraction only depen-dency paths of length up to 6 were considered. The result-ing graph contains 278,061 vertices (i.e., concepts) withan average degree of 600 in- and outgoing edges, result-ing in 83 million edges (i.e., extracted triples) of around16 million different labels (i.e., dependency paths), whereeach label thus occurs on average 5.2 times. In total, 29.7million pairs of vertices are connected to each other. Bothvertex degrees and edge label occurrences follow a veryheavy tailed distribution (see Figure 5), i.e., most of thevertices and edge labels only occur very scarcely. Becausethere is so little data for those concepts and dependencypaths, there is no value in keeping those for statisticallearning methods. Therefore, the graph was pruned at atotal concept occurrence of at least 40 for vertices and atotal label occurrence of at least 50 for edges, after man-ual inspection of the occurrence statistics (see Figure 5).The pruned, unstructured part of the knowledge graphcontains 84,635 vertices and around 39 million edges with104,953 different labels between around 9 million con-nected concept pairs. Another 2.8 million pairs for rela-tions stemming from UMLS and DrugBank were addedto the graph as edges, but no new concepts were intro-duced, because the graph would have grown too large ifFigure 5 Distribution of vertex degree and edge labels in unpruned, unstructured part of the knowledge graph, in log-scale. Figure (a) shows thedistribution of vertex degrees. Similarly, Figure (b) shows the distribution of edge labels.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 11 of 19all concepts of the UMLS would have been included asvertices.Path searchFinding paths between two concepts in such a highlyconnected graph is computationally challenging, becausesearch time increases exponentially with the specifiedmaximum path length. Thus, given a pair of concepts(cs, ct), only paths up to a certain maximum length m =4 were extracted by performing a bidirectional search.During search, synonymsc of concepts on the currentlyexplored path were not allowed to be explored in the nextstep. One problem that arises is the fact that some ver-tices, called hubs, are connected to many concepts (e.g.,the concept of cell), which lets the search space explode ifhubs are explored. However, paths running through suchhubs can be considered less informative than paths run-ning through scarcely connected vertices, because hubsare very general concepts. Therefore, to avoid this prob-lem and to make the search algorithm faster, highly con-nected vertices (degree greater than 100,000) are excludedfrom search. Furthermore, in some cases the number of allpossible paths gets very large even with a maximum pathlength m = 4. Therefore, search time was limited to 40seconds per pair.Datasets and trainingExperiments were conducted on two different datasets,pertaining to two different relations, though the approachis applicable for learning any new relation, provided that itcomprises concepts from the UMLS metathesaurus. Thefirst dataset contains 438 concept pairs of the may treatrelation taken from the UMLS. It was constructed withtwo restrictions in mind. First, it was ensured that nodrug or disease concept occurred more than once in thewhole dataset and second, every concept in that datasethad to be part of the pruned graph. The former restrictionassured that the diseases are not dominated by one diseasetype (e.g., neoplasms, cardiovascular diseases etc.), butthat many types of diseases are represented proportionallyin each category. The latter restriction was made becausefor the extraction of paths the pair of concepts in ques-tion has to be part of the graph. Figures 6 and 7 show thedistribution of drug and disease types, respectively, con-tained in that dataset. The second dataset consists of 744pairs of the has target relation extracted from DrugBankFigure 6 Distribution of drug types in themay treat dataset. The distribution of the drug types occurrences in themay treat dataset is shown.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 12 of 19Figure 7 Distribution of disease types in themay treat dataset. Thedistribution of the disease types occurrences in themay treat datasetis shown.and mapped to UMLS. As for themay treat dataset it wasensured that all concepts are part of the pruned knowl-edge graph but multiple occurrences of one concept wereallowed. Figures 8 and 9 show the distribution of drugand disease types, respectively, contained in that dataset.Both datasets were constructed by extracting all conceptpairs that are contained in the respective relation fromthe UMLS and afterwards the pairs were filtered with theaforementioned restrictions in mind. Negative exampleswere constructed as described in the previous section.Note that ensuring the exclusiveness of positive and nega-tive examples can lead to a slightly smaller set of negativeexamples. The used datasets are publicly available and canbe found as Additional file 1.During path extraction, edges labelled with may treator has target, respectively, and the sibling (SIB) label wereignored. The sibling relation expresses that two conceptshave the same parent concept. We found that sibling con-cepts usually have very similar relations. For example,drugs of the same family often treat the same diseases.Thus paths like csSIB??? cx may treat?????? ct and cs SIB???cxhas target?????? ct occurred frequently as positive trainingexamples for themay treat and has target relation, respec-tively. Those obvious connections could potentially distortthe results. If not stated otherwise, concept pairs, forwhich no paths of the specified lengths could be found,were excluded in the experimental evaluation. The num-ber of exclusions depends on the maximum allowed pathlength. E.g., only around 36% of all has target pairs havepaths of length 2 (i.e., direct connections). Finally, all mod-els and training algorithms mentioned in the previoussection were implemented using the FACTORIE toolkit[30], version 1.0.0-RC1.ResultsAll results were obtained by evaluating the proposedmod-els on the datasets using 10-fold cross validation, if notstated otherwise. Classification performance was evalu-ated by the area under the curve (AUC) value of theROC-curve, a common classification evaluation methodfor information retrieval systems. Other evaluation met-rics based on the precision of the system are not use-ful in this context because the datasets consist of anequal number of positive and negative examples, whichis not the case in reality, where there are much morenegative example pairs (e.g., consider all possible drug-disease combinations from which only small fraction isin a may treat relation). Sensitivity (true positive rate)and specificity (false positive rate), which make up theROC curve, are independent of the prior distribution ofpositive and negative examples. Special focus should begiven to the steepness of the ROC curves at their begin-ning, because it can indicate that the models learned somevery characteristic path-patterns for a relation (e.g., seeTables 4 and 5). Note that example pairs for which nopaths were found were excluded in the evaluation of theexperiment.Comparison ofmodels and feature typesTable 6 shows the performance of the different mod-els on the two datasets encoded with both plain one-of-N and LDA features using only paths of length 3.The first finding is that the pair-based approach consis-tently outperforms the path-based approach, for whichlogistic regression seems to be the better model. ThisWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 13 of 19Figure 8 Distribution of drug types in the has target dataset. The distribution of the drug types occurrences in the has target dataset is shown.outcome can be explained by considering the fact thatthe pair-based approach relies on a much larger fea-ture space (exponential in the maximum path lengthm) compared to the two path-based approaches (lin-ear and constant in m), providing more information tothe model that seems to be necessary for sophisticatedclassification.Results on the has target dataset show that the AUC canreach up to 0.8 compared against a random baseline with0.5 AUC, which picks a class label at random. Thus, ourapproach demonstrates its ability to recognize the signalof the relation.Another interesting finding is that the ROC-curves ofthe pair-based approach are very steep at the beginning upto a recall level of around 0.6 (see Figure 10). In particular,this can be observed, when using plain features on the hastarget dataset. This indicates that there are some commonpath patterns which can be learned by the model and beused to infer the has target relation. Table 4 shows somehighly weighted example patterns learned by the model.Figure 9 Distribution of target types in the has target dataset. The distribution of the target types occurrences in the has target dataset is shown.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 14 of 19Table 4 Example plain path patterns of length 3 for the hastarget relation with high feature weights learned bypair-based logistic regressionHighly weighted feature Explanation(dep??? induce prep??? in pobj???),(pobj??? in prep??? express nsubjpass??????) The substance is inducedinto something, in whichthe target (gene/protein)is expressed.(nn?? level nsubjpass?????? measure prep??? in pobj???),(pobj??? with prep??? associate nsubjpass??????) Some levels of thesubstance were measuredin something thatis associated with thetarget.(nn?? level nsubjpass?????? measure prep??? in pobj???),(pobj??? to prep??? susceptible acomp???? be nsubj???) Some levels of thesubstanceweremeasuredin something, to whichthe target is susceptible.The impact of the different feature types cannot directlybe inferred from Table 6. In the may treat dataset theLDA encoding seems to help a lot, but on the has tar-get dataset, which contains about double the amount oftraining examples, it does not. To evaluate the impactof the different feature types, experiments with differentamounts of training examples of the has target-datasetwere conducted. The results were obtained using cross-validation and are presented in Figure 11.Models trained with one-of-N features depend highlyon the amount of supplied training data, whereas modelstrained on examples with LDA features do not. This showsTable 5 Example plain path patterns of length 3 for themay treat relation with high feature weights learned bypair-based logistic regressionHighly weighted feature Explanation(pobj??? with prep??? treat dobj???),(nsubjpass?????? diagnose prep??? in pobj???) The drug treatssomething (e.g., asymptom) that isdiagnosedtogether withthe disease.(pobj??? by agent???? suppress nsubjpass??????),(nsubj??? increase prep??? at pobj???) The drugsuppressessomething thatis increased bythe disease.(nsubj??? mimic dobj??? effect prep??? of pobj???),(nsubj??? appear xcomp???? have dobj??? effects prep??? on pobj???) The drugsbehavior mimicsthe effect ofsomethingwhichseems to have aneffect on thedisease.Table 6 Results using different models and encoding (pathlength 3)Dataset Model AUCPlain LDAmay treat LRpair 0.61 0.73LRpath 0.62 0.71HMMpath 0.48 0.68has target LRpair 0.78 0.72LRpath 0.64 0.67HMMpath 0.59 0.60LR logistic regression, HMM Hidden Markov Model, path - path- based featureencoding; pair - pair-based feature encoding.With bold, the best AUC values for Plain and LDA are highlighted.the potential of encoding relations with LDA, as it trans-fers them into a much lower-dimensional, semantic space,which reduces the amount of required training data. How-ever, it can also be seen that information is lost in thatprocess which explains the lower performance achievedwith a larger training set.Impact of path lengthIn order to evaluate the impact of the maximum pathlength on the overall performance on the two datasets,experiments were conducted with the pair-based logisticregression model on all paths up to length 3 and 4, respec-tively. Table 7 shows that using paths of length 4 doesnot improve the overall performance on the classificationtask. This could be due to the fact, that with increasingmaximum length the number of additional informativepaths gets lower, while the total number of extracted pathsgets exponentially bigger and so does the noise and fea-ture space. This can lead to overfitting of the model tothe training data, because training data is too sparse com-pared to the large feature space. Figure 10 summarizes theresults of the previous two sections by showing the ROC-curves for the two datasets with different feature types andmaximal lengths.Temporal impact of established knowledgeFrom the previously presented results it is not clear howmuch the classifiers depend on the maturity of the respec-tive knowledge that relates two concepts. It might be thecase that the trained models are only able to discoverrelations between pairs that are known to be in that rela-tion for a long time, which should be reflected in theamount of literature that implicitly relates these conceptsto each other. A validation dataset consisting of 42 drugrepositioning cases, that were collected manually from lit-erature, has been used to validate the performance of theclassifier trained on the entire may treat dataset in thatrespect. Drug repositioning refers to the application ofknown drugs to new diseases. It is an interesting use caseWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 15 of 19Figure 10 ROC curves for the has target andmay treat datasets. Figure (a) shows the ROC curves produced based on the validation conducted onthe has target dataset. Similarly, Figure (b) shows the ROC curves produced based on the validation conducted on themay treat dataset.Weissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 16 of 19Figure 11 Change of classification performance using different amounts of training data. The difference in classification performance is plottedwhen a varying number of training examples is used for the LDA and the plain feature extraction method respectively.scenario because these drugs and diseases are usually wellknown and described in the literature individually, eventhough their connection might have only been establishedrecently.The resulting scores are ordered by year of FDAapproval and are presented in Figure 12. The first find-ing is that the scores seem to be independent from theyear of approval. The classifier is able to classify evenmostof the very recent repositioning cases with a high score.These results show that recently established knowledgecan be discovered by this approach and suggest that eventhe discovery of new knowledge might be possible. It isnoticeable that the confidence scores of the classifier arein general very high on the repositioning dataset, consid-ering that the average classification score of negative pairsfor this classifier is 0.57 with only little variation amongthe scores of those negative examples.Note, that 4 of the 42 examples in the drug repositioningdataset are also contained in the training set for the maytreat relation, namely 1967-0, 1999-0, 2001-3, 2002-0.However, they account for less than 10% and therefore donot affect the qualitative observations of this experiment.Using indirect connections for relation discoveryIn many approaches to knowledge discovery (e.g., fordatabase curation), only direct mentions of two conceptsTable 7 Impact of maximum path lengths using pair-basedlogistic regressionDataset Length AUCPlain LDAmay treat 3-3 0.61 0.733-4 0.62 0.75has target 3-3 0.78 0.723-4 0.80 0.70The notation n-mmeans that only paths of minimum length n and maximumlengthm are allowed.in one sentence are being considered to assert a spe-cific relation between two concepts. This approach can bereflected in our setting by only considering paths of length2 (i.e. only direct connections), which were excluded forall previous experiments. The exclusion from the previ-ous experiments follows the rationale that this approachaims to find new, unknown facts, based on indirect con-nections between concepts. Furthermore, the problem ofonly using direct connections is that only around 36% ofthe has target pairs and 46% of the may treat pairs havedirect connections in the graph, which means that it isnot possible to classify more than those correctly. Theimprovements of adding indirect connections as featurescan be seen in Figure 13. By using indirect connectionsalmost twice the number of positive examples can beranked highly compared to the case of only using directconnections. Note that pairs of the has target datasetwhich do not have any connections of length 2 or 3,respectively, were also included in this experiment to illus-trate the recall improvements when indirect connectionsare included as features.DiscussionThe results of the experiments show the potential ofthe suggested approach. By considering indirect knowl-edge, models can be trained to discover hidden rela-tions between concepts that cannot be extracted directly.This has several potential applications. One application isthe curation of databases, where new knowledge can beinferred by combining already established facts. Anotherexample is the inference of completely new knowledge,like the task of drug repositioning. A model can learnfrom examples typical patterns of indirect connectionsbetween a drug that has been repositioned to a disease.This requires a simple adoption of the current approach toonly consider knowledge that has been established priorto the first mention of a drug being a potential reposition-ing candidate for a disease. Moreover, a trained model canWeissenborn et al. Journal of Biomedical Semantics  (2015) 6:28 Page 17 of 19Figure 12 Confidence scores of trainedmay treat classifier using LDA features on a drug repositioning dataset. The figure shows the results of theapplication of the trainedmay treat classifier, to a drug repositioning dataset, with real case studies of repositioning collected from the period 1955to 2013. The average classification score of negative training pairs is included as baseline at 0.57.be used to find interesting indirect connections betweentwo concepts with respect to a specific relation providedthat a curated gold standard of this information can begenerated. Predefined relations are not necessarily a pre-requisite of the approach, but only a set of concept pairs isneeded to learn characteristic path patterns. In general, itis very simple to integrate new knowledge sources or learnpath patterns of any relation in the knowledge graph. TheFigure 13 ROC curves using a varying number of path lengths. Thefigure shows the ROC curves for using paths of only length 2 andpaths of both length 2 and 3 on the has target dataset.simplicity in the design of the approach is a great advan-tage that offers a lot of flexibility regarding the knowledgesources that can be included in the knowledge graph, andwhich has many potential applications.However, besides all the positive aspects of the sug-gested approach, there are also problems some of whichare not easily solvable whereas others could be resolvedin future work. The construction of the unstructuredknowledge graph consists of several stages in which errorsoccur that accumulate in the resulting graph. For exam-ple, the concept annotation using MetaMap is in somecases very poor, especially for genes. The simple wordan gets very frequently annotated with the DIAPH3gene which has the alias AN. Other examples include theword impact annotated with the IMPACT gene, andrare with the Retinoic Acid Response Element (shortalias RARE). Moreover, in scientific articles sentences aremore complicated than in other texts because they tendto consist of many nested sub-sentences which makes thelinguistic analysis, especially for the dependency parser,more difficult.Another issue is incomplete knowledge. For example,the extraction of information from text does not take co-JOURNAL OFBIOMEDICAL SEMANTICSHoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 DOI 10.1186/s13326-015-0001-9SOFTWARE Open AccessSimilarity-based search of model organism,disease and drug effect phenotypesRobert Hoehndorf1,2*, Michael Gruenberger3, Georgios V Gkoutos4 and Paul N Schofield3AbstractBackground: Semantic similarity measures over phenotype ontologies have been demonstrated to provide apowerful approach for the analysis of model organism phenotypes, the discovery of animal models of human disease,novel pathways, gene functions, druggable therapeutic targets, and determination of pathogenicity.Results: We have developed PhenomeNET 2, a system that enables similarity-based searches over a large repositoryof phenotypes in real-time. It can be used to identify strains of model organisms that are phenotypically similar tohuman patients, diseases that are phenotypically similar to model organism phenotypes, or drug effect profiles thatare similar to the phenotypes observed in a patient or model organism. PhenomeNET 2 is available at http://aber-owl.net/phenomenet.Conclusions: Phenotype-similarity searches can provide a powerful tool for the discovery and investigation ofmolecular mechanisms underlying an observed phenotypic manifestation. PhenomeNET 2 facilitates user-definedsimilarity searches and allows researchers to analyze their data within a large repository of human, mouse and ratphenotypes.Keywords: Phenotype, Semantic similarity, OntologyBackgroundOur increasing ability to phenotypically characterizegenetic variants of model organisms, coupled with sys-tematic and hypothesis-driven mutagenesis efforts, isresulting in a wealth of information about phenotypes.Increasingly, phenotype associated information is repre-sented using ontologies [1], and methods for systematicanalysis of phenotypes need to utilize the knowledge con-tained in these ontologies [2]. One successful analysisapproach, leveraging ontologies, is the use of semanticsimilarity, which applies a similarity measure betweenterms in phenotype ontologies so as to compute the phe-notypic similarity between entities that are representedby them [3]. Phenotypic similarity between different bio-logical entities can be indicative of a large number of*Correspondence: robert.hoehndorf@kaust.edu.sa1Computational Bioscience Research Center, King Abdullah University ofScience and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia2Computer, Electrical and Mathematical Sciences & Engineering Division, KingAbdullah University of Science and Technology, 4700 KAUST, 23955-6900Thuwal, Saudi ArabiaFull list of author information is available at the end of the articlebiological relations that span multiple scales, and can beeffectively utilised so as to reveal gene function [4], muta-tions underlying genetically-based diseases [5-8] as well asdrug-target relationships [9].One challenge in making these analysis methods andresults available to a wide range of researchers is the com-plexity involved in preparing the underlying data and thetime required to perform the analysis. We have devel-oped PhenomeNET 2, a system that provides a web-basedinterface to perform similarity-based searches over a largerepository of phenotypes. PhenomeNET 2 is based onthe PhenomeNET platform which pre-computes similar-ity between a wide range of model organisms, diseasesand drug effect profiles, but does not allow searches basedon user-specified phenotype profiles. PhenomeNET 2 cannow be used tomeasure semantic similarity between user-specified phenotypic profiles and phenotypes observedin rat, mouse, nematode worm, slime mold and fruitflystrains and variants, human diseases and drug-associatedbiological effects. The PhenomeNET 2 public webserveris available at http://aber-owl.net/phenomenet.© 2015 Hoehndorf et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 2 of 6ImplementationOverviewFigure 1 provides a high-level overview of the compo-nents of PhenomeNET 2. These consist of a frontend,implemented in PHP, and a backend consisting of twoparts: an ontology-based phenotype integration servicethat integrates and translates phenotype ontologies ofmultiple species, and a similarity service that computesthe semantic (phenotypic) similarity between phenotypedescriptions.It was previously only possible to explore the Phe-nomeNet using genes or their identifiers, or labels oridentifiers of diseases that were already included in thenetwork. A key use case for PhenomeNET 2 is the dis-covery of phenotypically related mutants and diseasesusing investigators own phenotype profiles for search-ing the network. In order to achieve this, PhenomeNET 2implements several updates in comparison to the originalPhenomeNET system [5]: PhenomeNET 2 has a completely novel and updateduser interface, which facilitates search of animalmodel phenotypes, disease phenotypes or drug effectprofiles based on combinations of user-specifiedterms from the MP or HPO; PhenomeNET 2 contains a revised phenotypeknowledge base over which similarity is computed:additions include phenotypes from the rat modelorganism database [10] and the slime mold modelorganism database [11], drug effect profiles [9], anddisease phenotypes from Orphanet [6]; yeast andzebrafish phenotypes, which were included in theoriginal PhenomeNET knowledge base, wereFigure 1 PhenomeNET 2 analysis and architecture overview.removed in PhenomeNET 2 as they do not use apre-composed phenotype ontology for characterizingabnormalities in mutants; similarity computation has been reimplemented inC++ to improve query performance and reduce thememory footprint.Cross-species integrationPhenomeNET 2 accepts phenotype descriptions that cor-respond to terms that are available from either the HumanPhenotype Ontology (HPO) [12] or the Mammalian Phe-notype Ontology (MP) [13]. Using the definitions cre-ated for phenotype ontologies [14], we have previouslydeveloped a method to integrate phenotype ontologiesof multiple species into a single framework that can beused to translate phenotypes between different species[5]. For this purpose, we integrate species-specific phe-notype ontologies based on the formal definitions thathave been created for these ontologies [14]. Cross-speciesintegration is achieved by using the species-independentanatomy ontology Uberon [15] and the Gene Ontol-ogy [16] to integrate anatomical entities and biologicalprocesses and functions across species, and the species-independent ontology of qualities PATO [17] to charac-terize the type of abnormal phenotypes observed. Theseontologies are combined with anatomy ontologies suchas the Mouse Anatomy ontology [18] and the Founda-tional Model of Anatomy [19] using a knowledge-basedapproach for combining anatomy and phenotype ontolo-gies [20]. A description logic reasoner can then be usedto infer sub- and super-class relations across mouse andhuman phenotype ontologies.As a new addition, we have added the DictyosteliumPhenotype Ontology [11] to the set of ontologies in Phe-nomeNET 2. To integrate this ontology, we have addedformal PATO-based entity-quality definitions [17] to 505classes. The definitions we created are available at http://aber-owl.net/aber-owl/dicty/dicty-xp.obo.In PhenomeNET 2, the integration and inferencemethod is implemented in Java and relies on the OWLAPI [21] and the ELK OWL reasoner [22]. The integratedphenotype ontology used by PhenomeNET 2, and thesource code for performing the ontology integration andreasoning, is freely available from the projects website.Phenotype knowledge basePhenomeNET 2 utilizes a knowledge base that consists ofanimal model phenotypes (slime mold, nematode worm,fruitfly, rat, mouse), disease phenotypes (Orphanet andOMIM), and drug effects (SIDER). In comparison toPhenomeNET, we have added drug effect phenotypes(described previously [9]), slime mold and rat pheno-types. To add rat phenotypes, we downloaded the pheno-Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 3 of 6type annotations of rat genes with the MP from the RatGenome Database ftp://rgd.mcw.edu/pub/data_release/annotated_rgd_objects_by_ontology/rattus_genes_mp andincorporated them in PhenomeNET 2 similarly to mousephenotypes. In particular, we conjunctively combine theindividual phenotype classes and treat this conjunctionas a phenotypic representation of the gene within Phe-nomeNET 2. Using this method, we incorporated 6,464MP phenotypes annotations to 1,057 rat strains, 1,545genes and 1,860 rat QTLs.Similarly, we obtain slime mold phenotypes annotatedwith the Dictyostelium Phenotype Ontology from Dicty-Base (http://dictybase.org/db/cgi-bin/dictyBase/download/download.pl?area=mutant_phenotypes&ID=all-mutants.txt)and represent the slime mold mutants as a conjunction ofphenotypes.Genedisease association datasetsWe use several curated datasets to evaluate the per-formance of PhenomeNET 2 for prioritizing candidategenes of disease. We use the curated set of genediseaseassociations from the Rat Genome Database availableat ftp://rgd.mcw.edu/pub/data_release/annotated_rgd_objects_by_ontology/rattus_genes_rdo, where we filterthe genedisease associations and use only those thathave a direct annotation with an OMIM identifier. Wefurther use OMIMs genedisease associations, and iden-tify the rat ortholog using the orthologs provided bythe Rat Genome Database (ftp://rgd.mcw.edu/pub/data_release/RGD_ORTHOLOGS.txt). Finally, we also use thecurated mouse disease models from the Mouse GenomeInformatics (MGI) database (ftp://ftp.informatics.jax.org/pub/reports/MGI_Geno_Disease.rpt), excluding condi-tional mutations and assigning a genedisease associationbetween gene G and disease D if the genotype annotatedwith D involves a mutation in G.Similarity-based searchThe similarity computation in PhenomeNET 2 is imple-mented in C++ to improve performance over Java-basedimplementations. For similarity computation, we use thegroupwise similarity measure SimGIC [23], i.e., the Jac-card index weighted with information content of eachclass. Specifically, information content I(C) of an ontologyclass C is based on the probability P(X = C) that a geno-type or disease annotation X in the phenotype knowledgebase is C:I(C) = ? log(P(X = C)) (1)Given two complex phenotypes P and R, where P ischaracterized by the ontology classes Cl(P) = P1, . . . ,Pnand R is characterized by the classes Cl(R) = R1, . . . ,Rm,we define the similarity between P and R as:sim(P,R) =?x?Cl(R)?Cl(P)I(x)?y?Cl(R)?Cl(P)I(y) (2)where Cl(X) is the smallest set containing X that is closedagainst the super-class relation in MP, i.e., Cl(X) = {x|x ?Xor ?y : y ? X ? y MP x} (where y MP xmeans that y isa subclass of x in MP).Phenotype similarity is computed using only MP termsdue to the higher performance in prioritizing candidategenes for diseases using MP [24]. The repository of phe-notype descriptions over which similarity is computedconsists of the phenotype descriptions available from theMouse Genome Informatics (MGI) [25], Rat GenomeDatabase [10], WormBase [26], DictyBase [11], Saccha-romyces Genome Database [27], OnlineMendelian Inher-itance in Man (OMIM) [28], Orphanet [29] and SIDERdatabases [30].The PhenomeNET 2 interface is implemented in PHPusing the Bootstrap CSS stylesheets, and the Phe-nomeNET 2 interface employs webservices from theOntology Lookup Service [31,32] at the European Bioin-formatics Institute to display ontology structures of theMP and HPO. Information is processed on the webserverin PHP which forwards the user-based query to the Javabackend through a Unix socket connection, and receivesthe response from the Java backend also through a Unixsocket connection.Results and discussionWe have developed PhenomeNET 2 which extendsthe PhenomeNET platform and enables similarity-based searches for user-specified phenotype profilesover a repository of animal model phenotypes, humanMendelian diseases and drug effect profiles. Our imple-mentation of PhenomeNET 2 is available at http://aber-owl.net/phenomenet.We evaluated the performance of PhenomeNET 2 forprioritizing candidate genes of disease using rat pheno-types. As rat models are ranked based on their phenotypicsimilarity to the disease, we use a receiver operating char-acteristic (ROC) curve [33] to evaluate the results. A ROCcurve is a plot of the true positive rate as a function ofthe false positive rate, and is derived by comparing pre-dicted associations against those asserted in the cognatemodel organism database. The ROC curve for prioritiz-ing rat disease models as well as mouse disease modelsis shown in Figure 2. The area under the ROC curve is0.65 when using genedisease associations from the RatGenome Database as evaluation set and 0.68 when usingOMIMs genedisease associations as evaluation set.Hoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 4 of 6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1True Positive RateFalse Positive RateCandidate gene prioritizationRGD disease annotationsOMIM disease annotationsMGI disease annotationsxFigure 2 Performance of candidate gene prediction inPhenomeNET 2. RGD disease annotations prioritize rat models anduse RGDs disease model annotations as true positives. OMIM diseaseannotations prioritize rat models and use OMIMs diseasegeneassociations as true positives; OMIM genes are mapped to rat genesthrough orthology. MGI disease annotations prioritize mouse modelsand use MGIs disease models as true positives. The ROCAUCs are0.65, 0.68 and 0.86, respectively.The low recovery of disease annotations from rat mod-els is likely a consequence of the method of annotationused by the Rat Genome Database and the inclusionof very large numbers of olfactory receptor genes inthe annotated gene corpus. Of the total 1,545 rat genesannotated to MP, 1,265 are olfactory receptors whicheach bear a single annotation to taste/olfaction pheno-type (MP:0005394). Furthermore, the extensive use ofelectronic inference through orthology, and the sepa-rate criteria used for disease and phenotype annotationmeans that the disease phenotypes and the annotatedphenotypes of individual rat models often do not match,i.e., it would be impossible to infer even the domain ofthe asserted human or mouse diseases from the pheno-type annotations for many genes. For example, Col2a1(RGD:2375) is annotated only to the Chondrodystrophy(MP:0002657) phenotype but to 30 disease classes asvaried as Stickler syndrome, Femur head necrosis, hypothy-roidism and myopia using a disparate range of humandisease associations and types of evidence.To further evaluate query performance and its suitabil-ity for real-time user queries, we constructed 1,000 ran-dom queries, each consisting of 10 randomly selected MPclasses, and performed a similarity-based search acrossour phenotype repository using the PhenomeNET 2 sys-tem. An average query using PhenomeNET 2 systemwith 10 phenotype terms in the query takes 5.1 secondsto complete. Compared to the Groovy-based implemen-tation of PhenomeNET, this is a 12-time improvementin performance, and this improved performance enablesreal-time user-specified queries.There are several further related tools that use similaralgorithms and perform similar analyses. In particular,the Phenomizer [34] is a tool for diagnosing patientsbased on semantic similarity searchers over OMIM dis-eases using the Human Phenotype Ontology. Phenomizeris implemented in Java and can also perform real-timeand user-specified searches. However, it currently usesthe Human Phenotype Ontology and is limited to search-ing diseases available in the OMIM repository, whilePhenomeNET 2 uses a larger repository and can searchphenotypes across multiple model organism species,diseases and drug effect profiles.Another related software is PhenoDigm [35], a systemsimilar to PhenomeNET in that it precomputes similaritybetween model organisms and diseases. PhenoDigm doesnot currently support user-defined queries over its repos-itory of phenotypes. Finally, functionally the most similartool to PhenomeNET 2 is the search interface providedby the Monarch Initiative (http://monarchinitiative.org/analyze/phenotypes/). The Monarch Initiative providesthe possibility to search mouse and zebrafish models aswell as human diseases based on a set of user-specifiedphenotypes. The main differences to PhenomeNET 2 arethe choice of similarity measure and the underlying phe-notype knowledge base: the Monarch search tool utilizesthe OWLSim tools [7] to compute semantic similarityinstead of simGIC used by PhenomeNET 2, uses a sin-gle integrated phenotype ontology (the Monarch ontol-ogy) instead of a combination of multiple species-specificphenotype ontologies used by PhenomeNET 2, and incor-porates zebrafish phenotypes but no fly, worm, slimemoldor drug effect phenotypes.In the future, we plan to incorporate different similaritymeasures. For example, we intend to experiment withusing the Semantic Measures Library (SML) [36] andallow users to select multiple different similarity measuresfor their search. However, the use of a generic librarywritten in Java will require careful evaluation of queryperformance.ConclusionsWhilst PhenomeNET provides a powerful means toexplore the phenomic space occupied by model organ-isms, human genetic diseases, and pharmacological agentscaptured in major data resources, PhenomeNET 2 pro-vides the ability to take a newly-derived phenotypic pro-file from the experimental or genetic manipulation of anorganism, or an un-diagnosed patient, and conduct thephenotypic equivalent of a user-defined BLAST-typeHoehndorf et al. Journal of Biomedical Semantics  (2015) 6:6 Page 5 of 6search across a repository of phenotypes. Such a tool isof interest to many communities concerned with phe-nomics and the analysis of phenotypes. For example, theresults of a PhenomeNET 2 search will allow investigatorsto construct hypotheses about the pathways in which thegene under investigation is involved by looking for closelyrelated phenotypes [37], or, in phenotype-driven stud-ies, prioritize candidate genes in either human or mouse.The ability to search through drug-related phenotypes willalso help in the formulation of hypotheses about potentialgenetic underpinnings of otherwise uncharacterized phe-notypes through knowledge of drug targets, or in estab-lishing potential therapeutic strategies where loss of genefunction and drug induced phenotypes are concordant.Availability and requirements Project name: PhenomeNET 2 Project home page: http://aber-owl.net/phenomenet and https://code.google.com/p/phenomeblast Operating system(s): Platform independent Programming language: Groovy, Java, C++, PHP Other requirements: Boost library, OWLAPI, ELKreasoner License: New BSD license Any restrictions to use by non-academics: noneCompeting interestsThe authors declare that they have no competing interests.Authors contributionsRH, GVG and PNS conceived of the study, evaluated the results and wrote thepaper. MG implemented the interface, RH implemented the backend andevaluation software. All authors read and approved the final version of themanuscript.AcknowledgmentsNo special funding was received for this study.Author details1Computational Bioscience Research Center, King Abdullah University ofScience and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia.2Computer, Electrical and Mathematical Sciences & Engineering Division, KingAbdullah University of Science and Technology, 4700 KAUST, 23955-6900Thuwal, Saudi Arabia. 3Department of Computer Science, AberystwythUniversity, Llandinam Building, SY23 3DB Aberystwyth, UK. 4Department ofPhysiology, Development & Neuroscience, University of Cambridge, DowningStreet, CB2 3EG Cambridge, UK.Received: 12 September 2014 Accepted: 24 January 2015JOURNAL OFBIOMEDICAL SEMANTICSAlm et al. Journal of Biomedical Semantics  (2015) 6:20 DOI 10.1186/s13326-015-0014-4RESEARCH ARTICLE Open AccessAnnotation-based feature extraction from setsof SBMLmodelsRebekka Alm1,2*, Dagmar Waltemath3, Markus Wolfien3, Olaf Wolkenhauer3,5 and Ron Henkel4AbstractBackground: Model repositories such as BioModels Database provide computational models of biological systemsfor the scientific community. These models contain rich semantic annotations that link model entities to concepts inwell-established bio-ontologies such as Gene Ontology. Consequently, thematically similar models are likely to sharesimilar annotations. Based on this assumption, we argue that semantic annotations are a suitable tool to characterizesets of models. These characteristics improve model classification, allow to identify additional features for modelretrieval tasks, and enable the comparison of sets of models.Results: In this paper we discuss four methods for annotation-based feature extraction from model sets. We testedall methods on sets of models in SBML format which were composed from BioModels Database. To characterize eachof these sets, we analyzed and extracted concepts from three frequently used ontologies, namely Gene Ontology,ChEBI and SBO. We find that three out of the methods are suitable to determine characteristic features for arbitrarysets of models: The selected features vary depending on the underlying model set, and they are also specific to thechosen model set. We show that the identified features map on concepts that are higher up in the hierarchy of theontologies than the concepts used for model annotations. Our analysis also reveals that the information content ofconcepts in ontologies and their usage for model annotation do not correlate.Conclusions: Annotation-based feature extraction enables the comparison of model sets, as opposed to existingmethods for model-to-keyword comparison, or model-to-model comparison.Keywords: Feature extraction, Model similarity, Bio-ontologies, SBMLIntroductionThanks to standardization efforts in Systems Biology [1],modelers today have access to high-quality, curated mod-els in standard formats. The Systems Biology MarkupLanguage (SBML) [2] is an XML-based standard formatto encode models as interactions between biological enti-ties. The emerging networks are furthermore enrichedwith semantic annotations [3] which link model parts toexternal knowledge in domain-specific ontologies (bio-ontologies) [4]. Many SBML models live in open modelrepositories such as BioModels Database [5], the Phys-iome Model Repository [6], or JWS Online [7]. These*Correspondence: rebekka.alm@igd-r.fraunhofer.de1Department of Multimedia Communication, University of Rostock,Joachim-Jungius-Str. 11, 18051, Rostock, Germany2Fraunhofer Institute for Computer Graphics Research IGD,Joachim-Jungius-Str. 11, 18059, Rostock, GermanyFull list of author information is available at the end of the articlerepositories distribute computational models and asso-ciated data in standard formats. They support neces-sary management tasks, including curation, annotation,search, version control, data visualization etc. to differentextents.BioModels Database implements a native, SQL-basedsearch [5]. An alternative search is the ranked modelretrieval [8]. Here, models and their annotations aremapped on pre-defined model features (e. g., modelorganism, author, biological entity), leading to a charac-teristic term vector for each model. The properties ofthis vector are numeric values mostly describing term fre-quency and inverse document frequency (TF-IDF) [9].The ranking is determined by the comparison of searchterms (i. e. provided keywords) with the extracted char-acteristic term vector per model. Current approaches aresolely capable of comparing a set of keywords against anindexed corpus of models and retrieve matching models.In addition, it is possible to create a characteristic term© 2015 Alm et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Alm et al. Journal of Biomedical Semantics  (2015) 6:20 Page 2 of 13vector directly from a model and, subsequently, query acorpus by example.For example, a standard search for the keywords cellcycle in BioModels Database retrieves all models in thecorpus that are relevant to the term cell cycle. Together,all models returned by this search can be seen as a new,cell cycle focused, model set (or corpus). The same ispossible for keywords such as apoptosis, calcium oscil-lation or NF-?B. At this point, we end up with differentsets of thematically related models. To characterize sucha set and, later on, compare them, features describing thisspecific model set will be helpful. However, it is prob-lematic to identify suitable characteristics for arbitrary orthematically focused sets of models.In this paper we present four methods for annotation-based feature extraction from arbitrary sets of SBMLmodels. Our methods build on combinations of existingapproaches for feature extraction [10-13]. We exemplifyour methods by comparing the characteristic features ofthematic sets to the features of arbitrary sets of SBMLmodels. The thematic sets were extracted fromBioModelsDatabase and represent the cell cycle, apoptosis, calciumoscillation, and NF-?B. Concepts, i. e. terms in the ontol-ogy, were extracted from three major bio-ontologies usedto semantically enrich models (GO, ChEBI, SBO). Weargue that our methods contribute to the determinationof similarity between sets of SBML models. They alsoprovide statistics on the use of ontology terms in SBMLmodels, and on the relation between ontology terms andmodels.BackgroundBio-ontologiesSBML is an XML format. It uses an RDF scheme to addsemantic annotations to model parts [14]. Among theontologies that are used to enrich SBMLmodels, we chosehere the following three ontologies, which we believe arethe most relevant in model annotation: An ontology ofgene and gene product attributes, theGene Ontology (GO)[15]; an ontology of chemical entities, the Chemical Enti-ties in BIology (ChEBI) [16]; and an ontology for modelingin biology, the Systems Biology Ontology (SBO) [3].The GO is proposed andmaintained by theGene Ontol-ogy Consortium. It aims at standardizing the representa-tion of gene and gene product attributes across speciesand databases by a structured, precisely defined, com-mon, controlled vocabulary. GO covers three domains.The most important relationships within each domain areis-a and part-of. Additionally, each concept is linkedto other kinds of information, including many gene andprotein keyword databases.ChEBI is an ontology of chemical entities of biologicalinterest. All database entries are is_a linked within theontology. Chemical classifications of ChEBI are alignedwith the classification of chemical processes in the GO,and the majority of chemical processes in GO are definedin terms of the ChEBI entities that participate in them.The SBO provides a set of controlled vocabularies ofterms commonly used in Systems Biology. It consists ofseven orthogonal branches. Terms within each branch arelinked by standard is_a relationships. Formal ties to SBOhave been developed for several representation formats inSystems Biology. SBML elementsa, for example, carry anoptional sboTerm attribute, which allows for a precisedefinition of the meaning of encoded model entities andtheir relationships.Feature extraction from ontologiesFor feature extraction it is important to group similaritems and to find categories that represent the content ofthe objects.Several techniques to determine similarity use distancemeasures as a basis. Common techniques are euclidian orcosinus distance in vector space [17] or the editing dis-tance for text [9,17-19]. In the context of this work thetechniques to distances in ontologies and tree structuresare of significance.The hierarchical structure of the ontology can be used todetermine the (semantic) similarity between objects [17].A distinction is made between two approaches; the graph-theoretic and information-theoretic approach.Examples for the graph-theoretic approach are theworks of Bernstein et al. [17] and Wang et al. [20]. Theydescribe the traditional approach for distance determi-nation in ontologies using the number of edges betweenthe nodes. The inheritance structure is represented ina directed acyclic graph in which the specialization ofobjects increases with each level. In such a graph theontology distance can be described as the shortest pathbetween two nodes. The shorter the distance betweentwo nodes, the more similar they are. The problem withthis approach is the assumption that the edges representuniform distances within a taxonomy; i.e. the semanticconnections are of equal weight. Li et al. therefore inves-tigate in [21] how path length, depth and local semanticdensity influence the quality of the similarity function.They come to the conclusion, that for a semantic knowl-edge base especially path length and depth are importantto get similarity results that compare to the human per-ception of similarity. The similarity values are used incluster analysis approaches for hierarchical clustering [22].Applied to the feature extraction task, we group conceptsbased on their distance in the ontology graph for one bio-ontology at a time. The top-down approach starts with acluster containing all concepts and then splits this clusterinto smaller groups. The bottom-up approach starts withclusters only containing one concept. Those clusters aremerged into larger clusters.Alm et al. Journal of Biomedical Semantics  (2015) 6:20 Page 3 of 13The most prominent representative of the information-theoretic approach is Resnik [12,13]. This approachexploits the information content of objects to compare.The more information two objects have in common, themore similar they are. The information content of a con-cept c is dependent on the concepts probability. The prob-ability p(c) is calculated by the frequency freq(c) of theconcept and the count N of all concepts of the ontology. Itis formally defined by Resnik [12]:p(c) = freq(c)N (1)If all concepts in an ontology are subordinate to oneitem, then this item has the greatest probability of 1,because its classification always applies. However, thesmaller the probability of a concept is, the higher is itsinformation content. The information content IC can becalculated by the negative logarithm of the likelihood:IC(c) = ? log2 p(c) (2)For example, the root term of the Gene Ontology sum-marizes all concepts of the ontology and consequentlyhas an information content of zero. A child concept suchas establishment of localization (GO_0051234) that sum-marizes 1408 concepts has a higher information contentof 3.34 and a leaf concept such as natural killer cellmediated cytotoxicity directed against tumor cell target(GO_0002420) has the highest information content of10.59.In order to determine the common information con-tent of two objects, one considers the deepest elementthat classifies both objects together. The information con-tent of this element is the degree of mutual informationcontent.The Information Content can be used to address theproblem of overgeneralization when using parent con-cepts as representatives for child concepts [23]. The chal-lenge of feature extraction in ontologies is to find sum-marizing features that do not generalize too strongly.Concepts further up in the ontology are less specific thanconcepts further down in the ontology and, thus, haveless information content. Counting the number of refer-ences of a concept and its successor concepts would rankthe general concept always highest, as it has more refer-ences. The counting approach does not consider the lossof specificity when moving up the ontology. Trißl et al.propose a similarity-based scoring function where a gen-JOURNAL OFBIOMEDICAL SEMANTICSFunk et al. Journal of Biomedical Semantics  (2015) 6:9 DOI 10.1186/s13326-015-0006-4RESEARCH Open AccessEvaluating a variety of text-mined features forautomatic protein function prediction withGOstructChristopher S Funk1*, Indika Kahanda2, Asa Ben-Hur2 and Karin M Verspoor3,4AbstractMost computational methods that predict protein function do not take advantage of the large amount of informationcontained in the biomedical literature. In this work we evaluate both ontology term co-mention and bag-of-wordsfeatures mined from the biomedical literature and analyze their impact in the context of a structured output supportvector machine model, GOstruct. We find that even simple literature based features are useful for predicting humanprotein function (F-max: Molecular Function = 0.408, Biological Process = 0.461, Cellular Component = 0.608). Oneadvantage of using literature features is their ability to offer easy verification of automated predictions. We findthrough manual inspection of misclassifications that some false positive predictions could be biologically validpredictions based upon support extracted from the literature. Additionally, we present a medium-throughputpipeline that was used to annotate a large subset of co-mentions; we suggest that this strategy could help to speedup the rate at which proteins are curated.Keywords: Text mining, Protein function prediction, Biomedical concept recognitionIntroductionCharacterizing the functions of proteins is an importanttask in bioinformatics today. In recent years, many com-putational methods to predict protein function have beendeveloped to help understand functions without perform-ing costly experiments. Most computational methods usefeatures derived from sequence, structure or protein inter-action databases [1]; very few take advantage of the wealthof unstructured information contained in the biomedicalliterature. Because little work has been conducted usingthe literature for function prediction, it is not clear whattype of text-derived information will be useful for this taskor the best way to incorporate it. In this work, we evaluatetwo different types of literature features, co-occurrencesof specific concepts of interest as well as a bag-of-wordsmodel, and assess the most effective way to combinethem for automated function prediction. We also providemany examples of the usefulness of literature features forverification or validation of automated predictions.*Correspondence: christopher.funk@ucdenver.edu1Computational Bioscience Program, University of Colorado School ofMedicine, 80045 Aurora, CO, USAFull list of author information is available at the end of the articleBackgroundLiterature mining has been shown to have substantialpromise in the context of automated function prediction,although there has been limited exploration to date [2].The literature is a potentially important resource for thistask, as it is well known that the published literature isthe most current repository of biological knowledge andcuration of information into structured resources has notkept up with the explosion in publication [3]. A few teamsfrom the first Critical Assessment of Functional Anno-tation (CAFA) experiments [1] used text-based featuresto support prediction of Gene Ontology (GO) functionalannotations [4].Wong and Shatkay [5] was the only team in CAFA thatused exclusively literature-derived features for functionprediction. They utilized a k-nearest neighbor classifierwith each protein related to a set of predetermined char-acteristic terms. In order to have enough training data foreach functional class, they condensed information fromall terms to those GO terms in the second level of thehierarchy, which results in only predicting 34 terms outof the thousands in the Molecular Function and Biologi-cal Process sub-ontologies. Recently, there has been more© 2015 Funk et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 2 of 14in-depth analysis into how to use text-based features torepresent proteins from the literature without relying onmanually annotated data or information extraction algo-rithms [6]. This work explored using abstracts along withunigram/bigram feature representation of proteins.Another team, Björne and Salakoski [7], utilizedevents, specifically molecular interactions, extracted frombiomedical literature along with other types of biologicalinformation from databases; they focused on predictingthe 385 most common GO terms.The work we presented in the first CAFA [8] is ona different scale from these previous efforts, and inte-grates information relevant for predicting protein func-tion from a range of sources. We utilize as much of thebiomedical literature as possible and are able to make pre-dictions for the entire Gene Ontology, thanks to a struc-tured output support vector machine (SVM) approachcalled GOstruct [9]. We found in that previous workthat features extracted from the literature alone approachperformance of many commonly used features from non-literature sources, such as protein-protein interactionsderived from a curated resource. However, we used onlyconcept co-occurrence features  focusing on simple,scalable features  leaving open many questions about thebest strategy for representing the literature for the task ofautomated protein function prediction.In this work, we therefore explore a variety of text-mined features, and different ways of combining thesefeatures, in order to understand better the most effectiveway to use literature features for protein function pre-diction. We have extended our workshop paper [10] byrefining the enhanced GO extraction rules, performingmore extensive analysis of the data at the functional classlevel, and extending validation through manual curationusing a medium-throughput curation pipeline.We againexplore these questions in the context of the structuredoutput SVMmodel, GOstruct.MethodsAn overview of our experimental setup can be seen inFigure 1 with more specific details about each processfollowing.DataWe extracted text features from two different literaturesources: (1) 13,530,032 abstracts available from Medlineon October 23, 2013 with both a title and abstract textand (2) 595,010 full-text articles from the PubMed OpenAccess Collection (PMCOA) downloaded on November6, 2013. These literature collections were processedidentically and features obtained from both were com-bined. Gold standard Gene Ontology annotations forboth human and yeast genes were obtained from theGene Ontology Annotation (GOA) data sets [11]. Onlyannotations derived experimentally were considered (evi-dence codes EXP, IDA, IPI, IMP, IGI, IEP, TAS). Fur-thermore, the term Protein Binding (GO:0005515) wasremoved due to its broadness and overabundance ofannotations. The human gold standard set consists ofover 13,400 proteins annotated with over 11,000 func-tional classes while the yeast gold standard set consistsof over 4,500 proteins annotated with over 6,500 func-tional classes. Even though the gold standard sets arelarge, only proteins where there is enough training datawill produce predictions. Additionally, to produce mean-ingful area under the curve (AUC) scores only GO termsFigure 1 Overview of the experimental setup used for function prediction.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 3 of 14with at least 10 annotations in the gold standard are con-sidered as possible prediction targets; this corresponds to509 Molecular Function classes, 2,088 Biological Processclasses, and 345 Cellular Component classes.Literature featuresTwo different types of literature features were extractedand evaluated, co-mentions and bag of words. Co-mentions are mentions of both a specific protein andconcept from the Gene Ontology that co-occur with aspecified span of text; they represent a simple knowledge-directed approach to represent the information containedwithin the biomedical literature. Another representa-tion of biomedical information is to relate proteins towords mentioned in the surrounding context; this is aknowledge-free approach because we are not groundingwhat we relate to proteins into some ontology, but onlystrings.Text-mining pipelineA pipeline was created to automatically extract the twodifferent types of literature features using Apache UIMAversion 2.4 [12]. Whole abstracts were provided as inputand full-text documents were provided one paragraph ata time. The pipeline consists of splitting the input doc-uments into sentences, tokenization, and protein entitydetection through LingPipe trained on CRAFT [13], fol-lowed by mapping of protein mentions to UniProt identi-fiers through a protein dictionary. Then, Gene Ontology(GO) terms are recognized through dictionaries providedto ConceptMapper [14]. Finally, counts of GO termsassociated with proteins, and sentences containing pro-teins, are output. A modified pipeline to extract pro-teins, GO terms, or any entity from an ontology filefrom text is available at http://bionlp.sourceforge.net/nlp-pipelines/. Details of the individual steps are providedbelow.Proteinmention extractionThe protein dictionary consists of over 100,000 proteintargets from 27 different species, all protein targets fromthe CAFA2 competition (http://biofunctionprediction.org). To increase the ability to identify proteins in text,synonyms for proteins were added from UniProt (UniProtConsortium 2008) and BioThesaurus version 0.7 [15].Gene ontology term extractionThe best performing dictionary-based system and param-eter combination for GO term recognition identifiedin previous work was used [16]. ConceptMapper (CM)is highly configurable dictionary lookup system that isa native UIMA component. CM is highly configurablethrough the use of many parameters. The list of parame-ters used to extract GO terms is in Additional file 1.Two different dictionaries were provided to CM toextract Gene Ontology mentions from text: original andenhanced. Both dictionaries are based on GO from 2013-11-13. The original directly utilizes GO terms and syn-onyms, with the exception that the word activity wasremoved from the end of ontology terms. The enhanceddictionary augments the original dictionary with addi-tional synonyms for many GO concepts. Rules were man-ually created by examining variation between ontologyterms and the annotated examples in a natural languagecorpus. This enhanced dictionary improved GO recogni-tion F-measure performance on CRAFT corpus [13,17]by 0.1 (from 0.49 to 0.59), through application of termtransformation rules to generate synonyms.A simple rule deals with the many GO terms of the formX metabolic process, which we have observed often donot occur literally in published texts. For example, forterm GO:0043705, cyanophycin metabolic process syn-onyms of cyanophycin metabolism and metabolism ofcyanophycin are generated. It is also noted that most ofthe terms in GO are nominals, so it is important to gener-ate part of speech variants. There are also many positiveregulation of X terms; not only will we generate syn-onyms of positive regulation of such as stimulationand pro, but if there exist inflectional and derivationalvariants of X we can also substitute that in. For example,apoptotic stimulation and pro-apoptotic are added forpositive regulation of apoptosis (GO:0043065). The ver-sion of the enhanced dictionary differs from the dictionaryoriginally used for CAFA2, as described in [10].Co-mentionsCo-mentions are based on co-occurrences of entityand ontology concepts identified in the literature text.This approach represents a targeted knowledge-basedapproach to feature extraction. The co-mentions we usehere consist of a protein and Gene Ontology term thatco-occur anywhere together in a specified span. Whilethis approach does not capture relations as specific as anevent extraction strategy [7], it is more targeted to the pro-tein function prediction context as it directly looks for theGO concepts of the target prediction space. It also hashigher recall since it doesnt require an explicit connec-tion to be detected between the protein and the functionterm.For these experiments, we considered two spans: sen-tence and non-sentence. Sentence co-mentions are twoentities of interest seen within a single sentence whilenon-sentence co-mentions are those that are mentionedwithin the same paragraph/abstract, but not within thesame sentence. The number of co-mentions extracted forhuman and yeast proteins using both dictionaries can beseen in Table 1. For human proteins, the enhanced dic-tionary identifies 1,500 more GO terms than the originalFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 4 of 14Table 1 Statistics of co-mentions extracted from bothMedline and PMCOA using the different dictionaries for identifyingGO termsHumanDictionary Span Unique proteins Unique GO terms Unique co-mentions Total co-mentionsOriginalsentence 12,826 14,102 1,473,579 25,765,168non-sentence 13,459 17,231 3,070,466 147,524,964combined 13,492 17,424 3,222,619 173,289,862Enhancedsentence 12,998 15,415 1,839,360 33,199,284non-sentence 13,513 18,713 3,725,450 196,761,554combined 13,536 18,920 3,897,951 229,960,838YeastDictionary Span Unique proteins Unique GO terms Unique co-mentions Total co-mentionsOriginalsentence 5,016 9,471 317,715 2,945,833non-sentence 5,148 12,582 715,363 18,142,448combined 5,160 12,819 748,427 21,088,281Enhancedsentence 5,063 12,877 414,322 3,853,994non-sentence 5,160 13,769 901,123 23,986,761combined 5,167 14,018 939,743 27,840,755dictionary, which, leads to a 35% increase in the numberof co-mentions identified (?56 million more).Bag-of-wordsBag-of-words (BoW) features are commonly used in manytext classification tasks. They represent a knowledge-freeapproach to feature extraction. For these experiments,proteins are associated to words from sentences in whichthey were mentioned. All words were lowercased andstop words were removed, but no type of stemming orlemmatization was applied.Feature representationThe extracted literature information is provided to themachine learning framework as sets of features. Each pro-tein is represented as a list of terms, either Gene Ontologyor words, along with the number of times the term co-occurs with that protein in all of the biomedical literature.An example entry from the co-mention features is as fol-lows: Q9ZPY7, co_GO:0003675=6, co_GO:0005623=2,co_GO:0009986=2, co_GO:0016020=2. . . . We utilize asparse feature representation and only explicitly state thenon-zero features for both co-mentions and BoW.Experimental setupWe evaluate the performance of literature features usingthe structured output SVM approach GOstruct [9].GOstruct models the problem of predicting GO terms asa hierarchical multi-label classification task using a sin-gle classifier. As input, we provide GOstruct with differentsets of literature features for each protein, as describedabove, along with the gold standard GO term associa-tions of that protein, used for training. From these featuresets, GOstruct learns patterns associating the literaturefeatures to the known functional labels for all proteinsin the training set. Given a set of co-occurring termsfor a single protein, a full set of relevant Gene Ontologyterms can be predicted. In these experiments, we useno additional resource beyond the literature to representproteins.GOstruct provides confidence scores for each predic-tion; therefore, all results presented in this paper arebased upon the highest F-measure over all sets of con-fidence scores, F-max [1]. Precision, recall, and F-maxare reported based on evaluation using 5-fold cross val-idation. To take into account the structure of the GeneOntology, all gold standard annotations and predictionsare expanded via the true path rule to the root node ofGO. The true path rule states that the pathway froma child term all the way up to its top-level parent(s)must always be true. We then compare the expandedset of terms. (This choice of comparison impacts theinterpretations of our results, which is discussed furtherbelow). All experiments were conducted on both yeast andhuman.Note that the true path rule is only utilized during theevaluation of features through machine learning system(as discussed in Impact of evaluation metric on perfor-mance). All numbers reported about the performance andpredictionsmade by themachine learning system have therule applied, while numbers strictly referring to counts ofco-mentions mined from the literature do not.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 5 of 14Human evaluation of co-mentionsTo support evaluation of the accuracy of the co-mentionfeatures, we sampled a number of them and asked ahuman assessor to rate each one as good (True Positive)or bad (False Positive), i.e., whether or not it cap-tures a valid relationship. To assess accuracy of co-mentions as a whole, 1,500 sentence co-mentions wererandomly sampled from the 33.2 million co-mentionsfor annotation. Additionally, three smaller subsets of co-mentions of specific functional classes, totaling about500 co-mentions, were selected for annotation to assessaccuracy of sentence co-mentions for specific functionalclasses. In total, there were around 3,000 full sentencesannotated.To enable fast annotation of this rating, we developedan approach that allows for medium-throughputmanualannotation of co-mentions, about 60-100 per hour. Thesentence co-mentions are transformed to brat rapid anno-tation tool (http://brat.nlplab.org/) format. The annotatorviews both the identified protein and functional conceptin differing colors within the context of the entire sen-tence. The annotator is only required to connect themwith a single relationship, either Good-Comention orBad-Comention. The annotator was instructed to viewthe labeled protein and GO concept as correct and toonly annotate Good-Comention when there exists arelationship between the specified entities. While a rela-tionship may exist between the annotated GO categoryand another exact mention of the labeled protein, thatwould be considered incorrect for the purposes of thisannotation, i.e., it is a decision relative to individual men-tions of the protein in a specific textual context. Weutilized these annotations to assess quality of a random setof co-mentions and also to label subsets of co-mentionscontaining particular functional concepts.Results and discussionExploring the use of co-mention featuresWe mined co-mentions from two different text spans andexplore four different ways to use them.1. only using sentence co-mentions2. only using non-sentence co-mentions3. combining counts from sentence and non-sentenceco-mentions into one feature set in the inputrepresentation4. using two separate feature sets for sentence andnon-sentence co-mentionsThe spans were explained in more detail above, under theCo-mentions section.The performance of these four different strategies forcombining the co-mention features for the enhanceddictionary can be seen in Figure 2. Each branch of GOis predicted and evaluated separately, but the way tocombine features is the same for all branches. Similartrends are seen with the original dictionary (data notshown).Using the two types of co-mentions as two separate fea-ture sets provide the best performance on all branches ofGO (see green shapes in Figure 2). These two types ofco-mentions encode different but complementary infor-mation and the classifier is able to build a better model byconsidering them separately.We utilized our medium-throughput human annota-tion pipeline and curated 1,500 randomly sampled sen-tence co-mentions; we found that?30% (441 out of 1,500)appeared to correctly relate the labeled protein with thelabeled function. From these results it seems that sentenceco-mentions contain a high false positive rate, most likelydue tomanymentions of proteins or GO concepts within asingle sentence. Methods for filtering sentences that con-tain ambiguous mentions, due to both ambiguous proteinnames and many annotations within sentences containingcomplex syntactic structure, are still to be explored. Addi-tionally, more complicated relationship or event detectionwould reduce the number of false positives seen andprovide the classifier with higher quality sentence co-mentions, but significantly reduce the total number ofidentified co-mentions. It is unclear which method wouldbe preferred for function prediction features.Interestingly, non-sentence co-mentions perform betterthan sentence co-mentions. This goes against intuition, asco-mentions within a sentence boundary act as a proxy toa relationship between the protein and its function. How-ever, it was seen in Bada et al. [18] that often functionannotations do not occur within a sentence boundary withthe corresponding protein. While coreference resolutionmay be required to correctly resolve such relationships,capturing function concepts in close proximity to a pro-tein appears to be a useful approximation. This could bethe reason why non-sentence co-mentions perform bet-ter. Based upon these results, from now on, when we sayco-mention features we are referring to using both sen-tence and non-sentence as separate feature sets but withinthe same classifier.To establish a baseline we utilized the co-mentionsthemselves as a classifier; the co-mentions are used asthe final predictions of the system. We performed eval-uations using both original and enhanced co-mentions.Results from combining counts between sentence andnon-sentence co-mentions are presented in Table 2. Thebaseline leads to very low precision for all branches butwe do see impressive levels of recall. This signifies thatinformation from the literature is able to capture relevantbiological information, but because we are able to identifymany different co-mentions the false positive rate is fairlyhigh.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 6 of 14Figure 2 Precision, recall, and F-max performance of four different co-mention feature sets on function prediction. Better performance is tothe upper-right and the grey iso bars represent balance between precision and recall. Diamonds  Cellular Component, Circle  Biological Process,Square  Molecular Function.Performance on human proteinsWe report performance of all four feature sets on humanproteins in Table 2. Comparing the performance of theco-mention features, we find that the original co-mentionfeatures produce the better performance on MolecularFunction (MF), while the enhanced co-mentions performslightly better on both Biological Process (BP) and Cellu-lar Component (CC). The most surprising result is thatbag of words performed as well as it did, considering thecomplexity of the Gene Ontology with its many thousandsof terms. Many text classification tasks utilize BoW andachieve very good performance while some have tried torecognize functional classes from text with BoW modelswith poorer results [19,20]. Their applicability to functionprediction has only begun to be studied in this work andWong et al. [6]. One explanation for their performancecould be due to their higher utilization of the biomedi-cal literature; co-mentions only capture information whenboth a protein and GO term are recognized together whileBoW only relies on a protein to be recognized. In otherwords, the knowledge-based co-mentions are limited bythe performance of automatic GO concept recognition, achallenging task in itself [16], while the BoW features haveno such limitation. In support of that, we note that onaverage, there are 2,375 non-zero BoW features per pro-tein, whereas there are an average of 135 sentence and250 non-sentence non-zero co-mention features per pro-tein. The results reported here are for human proteins; inAdditional file 2 we provide results in yeast exhibiting thesame trends observed in human.Overall, best performance for all branches of the GeneOntology is seen when using both co-mentions and thebag-of-words features. This suggests that all types of fea-tures provide complementary information. In view of thisobservation, we explored an alternative to using the fea-tures in combination to train a single classifier, which isto train separate classifiers and combine their scores. Thisapproach gave similar results to those reported here (datanot shown). It can be difficult to understand the impactof each type of feature solely by looking at the overallperformance, since it is obtained by averaging across allproteins; we dive deeper in the following sections andprovide examples that indicate that using co-mentionsproduces higher recall than precision.Another observation to make is that performance for allthree branches of GO as measured using the macro-AUCis very similar, indicating that the three sub-ontologiesare equally difficult to predict from the literature. TheFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 7 of 14Table 2 Overall performance of literature features onhuman proteinsMolecular functionFeatures F-max Precision Recall macro-AUCBaseline (Original) 0.094 0.055 0.327 0.680Baseline (Enhanced) 0.064 0.036 0.322 0.701Co-mentions (Original) 0.386 0.302 0.533 0.769Co-mentions (Enhanced) 0.377 0.336 0.447 0.764BoW 0.394 0.376 0.414 0.768Co-mentions + BoW 0.408 0.354 0.491 0.790Biological processFeatures F-max Precision Recall macro-AUCBaseline (Original) 0.134 0.091 0.249 0.610Baseline (Enhanced) 0.155 0.103 0.311 0.611Co-mentions (Original) 0.424 0.426 0.422 0.750Co-mentions (Enhanced) 0.429 0.427 0.430 0.752BoW 0.461 0.467 0.455 0.768Co-mentions + BoW 0.459 0.426 0.510 0.779Cellular componentFeatures F-max Precision Recall macro-AUCBaseline (Original) 0.086 0.050 0.305 0.640Baseline (Enhanced) 0.073 0.041 0.317 0.642Co-mentions (Original) 0.587 0.590 0.585 0.744Co-mentions (Enhanced) 0.589 0.583 0.596 0.753BoW 0.608 0.594 0.624 0.755Co-mentions + BoW 0.607 0.592 0.622 0.773Precision, Recall and F-max are micro-averaged across all proteins. Baselinecorresponds to using only the co-mentions mined from the literature as aclassifier. Macro-AUC is the average AUC per GO category. Co-mentions + BoWutilizes original co-mentions and BoW features within a single classifier.differences in performance as measured by F-max, whichis micro-averaged, are likely the result of the differencesin the distribution of terms across the different levels inthe three sub-ontologies. The similar performance acrossthe sub-ontologies is in contrast to what is observedwhen using other types of data: MF accuracy is typ-ically much higher than BP accuracy, especially whenusing sequence data [1,8], with the exception of networkdata such as protein-protein interactions that yields betterperformance in BP.Exploring differences between original and enhancedco-mentionsExamining Table 1, we see that the enhanced dictionaryfinds ?35% (?56 million) more unique co-mentions,makes about 32,000 fewer predictions (Table 3) andperforms slightly better at the function predictiontask (Table 2). To elucidate the differences that GOTable 3 Description of the gold standard humanannotations and predictions made by GOstruct from eachtype of featureMolecular Biological Cellularfunction process componentFeature type # Predictions # Predictions # PredictionsGold standard 36,349 264,631 79,631Original 102,486 268,068 76,513Enhanced 64,919 276,734 81,094BoW 40,499 268,114 77,753Combined 62,039 386,267 78,475All numbers are counts based on the predictions broken down by sub-ontology;these counts have the true path rule applied.term recognition plays in the function prediction task,co-mention features and predictions were examined forindividual proteins.Examining individual predictions it appears that manyof the predictions made from enhanced co-mention fea-tures are more specific than both the original dictionaryand the gold standard annotations; this is also supportedby further evidence presented in the functional analysisin the Functional class analysis and Analysis of indi-vidual Biological Process and Molecular Function classessections. For example, in GOstruct predictions using theoriginal dictionary, DIS3 (Q9Y2L1) is (correctly) anno-tated with rRNA processing (GO:0006364). Using co-mentions from the enhanced dictionary, the protein ispredicted to be involved with maturation of 5.8S rRNA(GO:0000460), a direct child of rRNA processing. Thereare 10 more unique sentence and 31 more unique non-sentence GO term co-mentions provided as features bythe enhanced dictionary. Some of the co-mentions iden-tified by the enhanced and not by the original dictionaryrefer to mRNA cleavage, cell fate determination, anddsRNA fragmentation. Even though none of these co-mentions directly correspond to the more specific func-tion predicted by GOstruct, it could be that the machinelearner is utilizing this extra information to make morespecific predictions. Interestingly, the human DIS3 pro-tein is not currently known to be involved with the morespecific process, but the yeast DIS3 protein is. We did notattempt to normalize proteins to specific species becausethat is a separate problem in itself. It is probable that if wenormalized protein mentions to specific species or imple-mented a cross-species evaluation utilizing homology theresults of the enhanced dictionary would show improvedperformance.We expected to see a bigger increase in performancebecause we are able to recognize more specific GOterms utilizing the enhanced dictionary. One possible rea-son that we dont is due to increased ambiguity in theFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 8 of 14dictionary. In the enhanced dictionary, for example, a syn-onym of implantation is added to the term GO:0007566- embryo implantation. While a majority of the time thissynonym correctly refers to that GO term, there are casessuch as . . . tumor cell implantation for which an incor-rect co-mention will be added to the feature representa-tion. These contextually incorrect features could limit theusefulness of those GO terms and result in noisier fea-tures. One way to address this may be to create a separatefeature set of only co-mentions based on synonyms so themachine learner could differentiate or weight them dif-ferently; this could help improve performance using theenhanced dictionary co-mentions.Functional class analysisWe nowmove to an analysis of functional classes to assesshow well different parts of GO are predicted by differentfeature sets (Figure 3). We use two separate metrics, depthwithin the GO hierarchy and information content (IC) ofthe GO term derived from our gold standard annotations.Because the GO is a graph with multiple inheritance anddepth can be a fuzzy concept [21], we define depth as thelength of the shortest path from the root to the term in theGO hierarchy. We calculate an annotation-based infor-mation content(IC) for each GO term based on the goldstandard annotations using the IC statistic described inResnik et al. [22].Figure 3(a) shows the distribution of counts of GO termswithin the gold standard and predictions by both depthand information content, Figure 3(b) shows the macro-averaged performance (F-measure) for each feature set bydepth, and Figure 3(c) shows the macro-averaged perfor-mance for each feature set binned by GO term informa-tion content. Examining 3(a) we find that terms appearto be normally distributed with mean depth of 4. Look-ing at information content, we find that over two-thirdsof the terms have an information content score between6 and 8, indicating that a majority of terms within thegold standard set are annotated very few times. Overall,for all sets of features, performance of concepts decreasesas the depth and information content increases; it is intu-itive that terms that are more broad, and less informative,would be easier to predict than terms that are specific andmore informative.Examining performance by depth (Figure 3(b)) we see adecrease in performance between depths 1-3, after whichperformance levels off. As a function of information con-tent we obtain a more detailed picture, with a muchlarger decrease in performance with increased term speci-ficity; all features are able to predict low information con-tent, less interesting terms, such as binding (IC=0.20)or biological regulation (IC=0.66) with high confidence(F-measure > 0.8). Performance drops to its lowest forterms that have information content between 7 and 9indicating there still remains much work to be done toaccurately predict these specific and informative terms.Interestingly, there is an increase in performance forthe most specific terms, especially using the BoW andcombined representations; however, there are very fewsuch terms as seen in (Figure 3(a)), representing veryfew proteins, so its not clear if this is a real trend.Finally, we observe that for both depth and IC analy-sis the knowledge-free BoW features usually outperformthe knowledge-based co-mentions and that the enhancedco-mentions usually produce slightly better performancethan the original co-mentions.Analysis of individual biological process andmolecularfunction classesTo further explore the impact of the different featureson predictions, we examined the best (Table 4) andworst (Table 5) Biological Process andMolecular Functioncategories.Examining the top concepts predicted, it is reinforcedthat the enhanced co-mentions are able to make moreinformative predictions, in addition to increasing recallwithout a loss in precision when compared to the origi-nal co-mentions. All 12 of the top terms predicted by theoriginal co-mentions have an information content < 2 asopposed to only 7 terms from the enhanced co-mentions.We can compare the performance on specific functionalclasses. For example, GO:0007076 - mitotic chromosomecondensation is the second highest predicted GO termby the enhanced co-mentions (F=0.769) while it is ranked581 for the original co-mentions (F=0.526). Granted, therewill always be specific cases where one performs betterthan the other; from these and previous analyses, we findthat the enhanced co-mentions are able to predict moreinformative terms for more proteins than the original co-mention features (Figure 3 and Table 4). This shows thatimproving GO term recognition leads to an improvementin the specificity of function prediction.Considering the top concepts predicted by the BoWfeatures, we see a pattern similar to the enhanced co-mentions. Five out of the top twelve concepts predictedhave an information content score greater than 6; theseinformative terms are different between the two fea-ture sets. For the top functions predicted by all featuresthe combined classifier of co-mentions and BoW pro-ducesmore predictions, leading to higher recall and betterF-measure. Even though some of the top terms predictedare informative and interesting we still strive for betterperformance on the most informative terms.We also analyze the most difficult functional classes topredict, results can be seen in Table 5. Between all featureswe find some similar terms are difficult to predict; local-ization and electron carrier activity are in the worstfive from all feature sets. It is interesting to note thatFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 9 of 14Figure 3 Functional class analysis of all GO term annotations and predictions. a) Distribution of the depth and information content of GOterm annotations. As IC values are real numbers, they are binned, and each bar represents a range, e.g. [1,2) includes all depth 1 terms and ICbetween 1 and 2 (not including 2). b)Macro-averaged F-measure performance broken down by GO term depth. c)Macro-averaged F-measureperformance binned by GO term information content.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 10 of 14Table 4 Top biological process andmolecular function classes predicted by each type of featureOriginal co-mentionsGO ID Name # Predictions Precision Recall F-measure Depth ICGO:0009987 cellular process 6,164 0.812 0.875 0.842 1 0.66GO:0044699 single-organism process 4,849 0.743 0.765 0.754 1 0.96GO:0044763 single-organism cellular process 4,295 0.681 0.714 0.697 2 1.20GO:0008152 metabolic process 3,893 0.644 0.726 0.682 1 1.22GO:0065007 biological regulation 3,615 0.691 0.629 0.658 1 0.90GO:0071704 organic substance metabolic process 3,489 0.611 0.677 0.643 2 1.42GO:0050789 regulation of biological process 3,350 0.668 0.601 0.633 2 0.97GO:0044238 primary metabolic process 3,337 0.593 0.655 0.623 2 1.56GO:0044237 cellular metabolic process 3,268 0.590 0.644 0.616 2 1.49GO:0050794 regulation of cellular process 3,156 0.648 0.583 0.614 3 1.11GO:0050896 response to stimulus 2,968 0.606 0.590 0.597 1 1.62GO:0043170 macromolecule metabolic process 2,640 0.548 0.618 0.581 3 1.77Enhanced co-mentionsGO ID Name # Predictions Precision Recall F-measure Depth ICGO:0009987 cellular process 6,223 0.816 0.887 0.850 1 0.66GO:0007076 mitotic chromosome condensation 6 0.833 0.714 0.769 4 8.58GO:0006323 DNA packaging 6 0.833 0.714 0.769 3 7.81GO:0044699 single-organism process 4,957 0.744 0.783 0.763 1 0.96GO:0044763 single-organism cellular process 4,423 0.682 0.736 0.708 2 1.20GO:0008152 metabolic process 3,887 0.643 0.723 0.681 1 1.22GO:0065007 biological regulation 3,701 0.683 0.636 0.659 1 0.90GO:0050789 regulation of biological process 3,453 0.662 0.613 0.637 2 0.97GO:0071704 organic substance metabolic process 3,491 0.605 0.670 0.636 2 1.42GO:0043252 sodium-independent organic anion transport 11 0.636 0.583 0.608 7 8.50GO:0000398 mRNA splicing, via spliceosome 140 0.492 0.697 0.577 10 5.88GO:0006607 NLS-bearing protein import into nucleus 15 0.533 0.571 0.551 6 8.50Bag-of-wordsGO ID Name # Predictions Precision Recall F-measure Depth ICGO:0009987 cellular process 6,005 0.820 0.869 0.844 1 0.66GO:0044699 single-organism process 4,940 0.754 0.799 0.776 1 0.96GO:0044763 single-organism cellular process 4,449 0.696 0.764 0.728 2 1.20GO:0043252 sodium-independent organic anion transport 8 0.875 0.583 0.700 7 8.50GO:0065007 biological regulation 3,865 0.698 0.686 0.692 1 0.90GO:0008152 metabolic process 3,870 0.647 0.733 0.688 1 1.22GO:0050789 regulation of biological process 3,597 0.680 0.663 0.671 2 0.97GO:0006479 protein methylation 13 0.615 0.727 0.666 8 6.52GO:0051568 histone H3-K4 methylation 13 0.615 0.727 0.666 11 7.94GO:0007076 mitotic chromosome condensation 5 0.800 0.571 0.666 4 8.58GO:0050794 regulation of cellular process 3,440 0.657 0.651 0.654 3 1.11GO:0006497 protein lipidation 9 0.889 0.500 0.640 7 6.79Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 11 of 14Table 4 Top biological process andmolecular function classes predicted by each type of feature (Continued)Co-mentions + Bag-of-wordsGO ID Name # Predictions Precision Recall F-measure Depth ICGO:0009987 cellular process 6,420 0.813 0.913 0.860 1 0.66GO:0044699 single-organism process 5,338 0.736 0.834 0.782 1 0.96GO:0044763 single-organism cellular process 4,862 0.674 0.800 0.731 2 1.20GO:0065007 biological regulation 4,445 0.669 0.749 0.707 1 0.90GO:0008152 metabolic process 4,252 0.638 0.785 0.704 1 1.22GO:0050789 regulation of biological process 4,199 0.650 0.733 0.689 2 0.97GO:0050794 regulation of cellular process 4,046 0.626 0.723 0.671 3 1.11GO:0043252 sodium-independent organic anion transport 15 0.600 0.750 0.667 7 8.50GO:0071704 organic substance metabolic process 3,883 0.602 0.743 0.665 2 1.42GO:0043170 macromolecule metabolic process 3,007 0.540 0.694 0.607 3 1.77GO:0051716 cellular response to stimulus 3,176 0.520 0.674 0.587 3 1.89GO:0006386 termination of RNA polymerase III transcription 12 0.583 0.583 0.583 7 8.18Table 5 Most difficult biological process andmolecular function classesOriginal co-mentionsGO ID Name # Predictions Precision Recall F-measure ICGO:0051179 localization 28 0.107 0.054 0.072 5.70GO:0016247 channel regulator activity 115 0.043 0.208 0.071 6.53GO:0009055 electron carrier activity 108 0.03 0.111 0.055 6.94GO:0007067 mitosis 23 0.043 0.031 0.036 7.54GO:0042056 chemoattractant activity 53 0.018 0.067 0.029 7.56Enhanced co-mentionsGO ID Name # Predictions Precision Recall F-measure ICGO:0009055 electron carrier activity 102 0.090 0.138 0.109 6.94GO:0051179 localization 42 0.071 0.055 0.061 5.70GO:0019838 growth factor binding 44 0.021 0.035 0.027 5.99GO:0070888 E-box binding 99 0.010 0.066 0.019 7.49GO:0030545 receptor regulator activity 152 0.007 0.020 0.010 7.63Bag-of-wordsGO ID Name # Predictions Precision Recall F-measure ICGO:0051179 localization 18 0.277 0.090 0.137 5.70GO:0009055 electron carrier activity 29 0.103 0.083 0.092 6.94GO:0016042 lipid catabolic process 26 0.076 0.054 0.063 5.80GO:0015992 proton transport 15 0.066 0.047 0.055 7.29GO:0005516 calmodulin binding 14 0.071 0.033 0.045 7.25Co-mentions + Bag-of-wordsGO ID Name # Predictions Precision Recall F-measure ICGO:0051179 localization 61 0.100 0.109 0.104 5.70GO:0009055 electron carrier activity 62 0.079 0.138 0.101 6.94GO:0030545 receptor regulator activity 63 0.064 0.080 0.071 7.63GO:0042056 chemoattractant activity 24 0.041 0.066 0.051 7.56GO:0040007 growth 27 0.030 0.066 0.047 7.33IC represents information content of term.Funk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 12 of 14the information content of these difficult to predict termslies around the median range for all predicted terms. Wemight have expected that the most difficult terms to pre-dict would be those most informative terms (IC around10). We believe that these terms are difficult to predictbecause the ontological term names are made up of com-mon words that will be seen many times in the biomedicalliterature, even when not related to protein function. Thisambiguity likely results in a high number of features cor-responding to these terms which results in poor predictiveperformance. There is still further work needed to addressthese shortcomings of literature mined features.Manual analysis of predictionsManual analysis of individual predictionsWe know that GO annotations are incomplete and there-fore some predictions that are classified as false positivescould be actually correct. The predictionmay even be sup-ported by an existing publication, however due to the slowprocess of curation they are not yet in a database. Wemanually examined false positive predictions that containsentence level co-mentions of the protein and predictedfunction to identify a few examples of predictions thatlook correct but are counted as incorrect: Protein GCNT1 (Q02742) was predicted to beinvolved with carbohydrate metabolic process(GO:0006959). In PMID:23646466 [23] we findGenes related to carbohydrate metabolism includePPP1R3C, B3GNT1, and GCNT1. . . . Protein CERS2 (Q96G23) was predicted to play a rolein ceramide biosynthetic process (GO:0046513). InPMID:22144673 [24] we see . . .CerS2, which usesC22-CoA for ceramide synthesis. . . .These are just two examples taken from the co-mentions, but there are most likely more, which couldmean that the true performance of the system is underes-timated. Through these examples we show how the inputfeatures can be used not only for prediction, but also forvalidation. This is not possible when using features thatare not mined from the biomedical literature and illustratetheir importance.Manual analysis of functional classesIn the previous section we explored individual co-mentions that could serve as validation for an incorrectGOstruct prediction. In addition to this one-off anal-ysis, we can label subsets of co-mentions pertainingto particular functional concepts for validation ona medium-throughput scale. To identify functionalclasses for additional exploration, all GO conceptswere examined for three criteria: 1) their involve-ment in numerous co-mentions with human proteins2) numerous predictions made with an overall aver-age performance and 3) confidence in the ability toextract the concept from text. The concepts chosenfor further annotation were GO:0009966  regulationof signal transduction, GO:0022857  transmembranetransporter, and GO:0008144 - drug binding. For eachof these classes all human co-mentions were manuallyexamined.We identified 204 co-mentions between a human pro-tein and GO:0008144 - drug binding (IC=6.63). Out of204 co-mentions, 112 appeared to correctly related theconcept with the protein (precision of 0.554). 61 uniqueproteins were linked to the correct 112 co-mentions. Ofthese, only 4 contained annotations of drug bindingin GOA, while the other 57 are not currently knownto be involved with drug binding. When we exam-ined the predictions made by GOstruct for these pro-teins, unfortunately, none of them were predicted as drugbinding. After further examination of the co-mentions,most appear to be from structure papers and refer to drugbinding pockets within specific residues or domains of theproteins. It is unlikely that the specific drug could be iden-tified from the context of the sentence and many referto a proposed binding site with no experimental data forsupport.The concept GO:0022857 - transmembrane trans-porter (IC=4.17) co-occurred with a human protein 181different times. 69 co-mentions appeared to correctlyrelate the concept with the labeled protein (precision of0.381). A total of 32 proteins could be annotated withthis concept; out of the 32 only 6 are not already anno-tated with transmembrane transporter in GOA. Whenwe examine the predictions made from the enhanced fea-tures, only 1 out of the 6 proteins are predicted to beinvolved with transmembrane transporter.There were a total of 134 human co-mentions con-taining GO:0009966  regulation of signal transduction(IC=3.30). 73 out of 134 co-mentions appeared to cor-rectly relate the concept with the protein (precision of0.543). A total of 58 proteins could be annotated basedupon these co-mentions. 21 proteins already containannotations conceptually related to regulation of signaltransduction, while the other 37 proteins do not containannotations related to regulation of signal transduction;the later could represent true but uncurated functions.When we examine the predictions made by GOstructusing the enhanced co-mention features, 9 out of those 37proteins were predicted to be involved with regulation ofsignal transduction.When a random subset of 1,500 human co-mentionswere labeled it was found that ?30% (441 out of 1,500)correctly related the labeled protein and GO term. Byannotating co-mentions of specific functional conceptswe see that these categories have a higher proportion ofFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 13 of 14correct co-mentions than the random sample from allco-mention; there will also be some categories where per-formance of co-mentions is quite low. This informationcan be used in multiple different ways. If we are more con-fident that certain categories related to function can beextracted from co-mentions, we can use this informationto inform the classifier by encoding the information intothe input features. Additionally, we show the importanceand ability of co-mentions to not only be used as input fea-tures, but also for validation and enhancing the machinelearning results. We show that many of the predictionsmade by our system could possibly be correct, but just notcurated in the gold standard annotations.Impact of evaluationmetric on performanceIn our initial experiments, we required predictions andgold standard annotations to match exactly (data notshown), but we found, through manual examination ofpredictions, that many false positives are very close (interms of ontological distance) to the gold standard anno-tations. This type of evaluation measures the ability of asystem to predict functions exactly, at the correct speci-ficity in the hierarchy, but it doesnt accurately representthe overall performance of the system. It is preferable toscore predictions that are close to gold standard annota-tions higher than a far distant prediction. We are awareof more sophisticated methods to calculate precision andrecall that take into account conceptual overlap for hier-archical classification scenarios [25,26]. For the resultsreported in Table 2, to take into account the hierarchy ofthe Gene Ontology, we expanded both the predictions andannotations via the true path rule to the root. By doingthis, we see a large increase in both precision and recallof all features; this increase in performance suggests thatmany of the predictions made are close to the actual anno-tations and performance is better than previously thought.A downside of our chosen comparison method is thatmany false positives could be introduced via an incor-rect prediction that is of a very specific functional class.This could possibly explain why co-mentions from theenhanced dictionary display a decrease in performance; asingle, very specific, incorrect prediction introduces manyfalse positives.ConclusionsIn this work we explored the use of protein-related fea-tures derived from the published biomedical literature tosupport protein function prediction. We evaluated twodifferent types of literature features, ontology concept co-mentions and bag-of-words, and analyzed their impacton the function prediction task. Both types of featuresprovided similar levels of performance. The advantageof the bag-of-words approach is its simplicity. The addi-tional effort required to identify GO term mentions intext pays off by offering users the ability to validate pre-dictions by viewing the specific literature context fromwhich an association is derived, as demonstrated in ourexperiments.In addition, we compared the value of conceptco-mentions considering two different spans of co-occurrence: within a sentence (sentence co-mention)and across a sentence boundary (sentence-external, ornon-sentence co-mention). Interestingly, we found thatsentence and non-sentence co-mentions are equally use-ful, and that they are best used in conjunction as separatefeature sets. Combining co-mentions and bag-of-wordsdata provided only a marginal advantage, and in futurework we will explore ways to obtain better performancefrom these features together. We also show that increas-ing the ability to recognize GO terms from biomedical textleads to more informative functional predictions. Addi-tionally, the literature data we used provides performancethat is on par with other sources of data such as networkand sequence and has the advantage of being easy to verifyon the basis of the text.Our experiment in medium-throughput manual inspec-tion of protein-GO term co-mentions suggests that thisstrategy can be used as a way of speeding up the processof curation of protein function. The literature containsmillions of co-mentions, and a human-in-the-loop sys-tem based on the detected co-mentions prioritized byGOstruct can be a highly effective method to dramati-cally speed up the rate at which proteins are currentlyannotated.Future workThis work marks only the beginning of incorporating textmining for protein function prediction. There are alwaysother more sophisticated or semantic features to explore,but based upon these results, there are some natural nextsteps.The first would be to incorporate larger spans for a bag-of-words model due to the surprising performance of thenon-sentence co-mentions. By including words from sur-rounding sentences, or an entire paragraph, more contextwould be en-coded and the model might result in betterpredictions.Secondly, we found that an enhanced dictionary pro-duced more individual co-mentions and fewer predic-tions, resulting in slightly increased performance. Weexplored several possible explanations as to why there isnot a greater impact. It could be due to a large num-ber of competing co-mentions that prevent good patternsfrom emerging or the possibility of introducing noisethrough ambiguous protein mentions. A filter or classifierthat could identify a good co-mention would be pro-viding much higher quality co-mentions as input, whichwould in turn likely lead to better predictions. AnotherFunk et al. Journal of Biomedical Semantics  (2015) 6:9 Page 14 of 14way to potentially improve performance is to separateco-mentions found from synonyms from the original co-mentions, thereby allowing the classifier to provide themwith different weights.Additional filesAdditional file 1: ConceptMapper parameters. A description of theparameter values and impact for GO extraction through ConceptMapper.Additional file 2: Yeast results. Function prediction performancenumbers for prediction on yeast proteins.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCSF developed NLP pipelines and generated literature features. IK performedall runs using GOstruct and provided prediction output to CSF for analysis. KVand ABH contributed to the design of methods and offered supervision atevery step. All authors read and approved the manuscript.AcknowledgementsThis work was funded by NIH grant 2T15LM009451 and NSF grantsDBI-0965616 and DBI-0965768 to ABH. KV was partially supported by NICTA,which is funded by the Australian Government as represented by theDepartment of Broadband, Communications and the Digital Economy and theAustralian Research Council through the ICT Centre of Excellence program.Author details1Computational Bioscience Program, University of Colorado School ofMedicine, 80045 Aurora, CO, USA. 2Department of Computer Science,Colorado State University, 80523 Fort Collins, CO, USA. 3Department ofComputing and Information Systems, University of Melbourne, 3010 Parkville,Victoria, Australia. 4Health and Biomedical Informatics Centre, University ofMelbourne, 3010 Parkville, Victoria, Australia.Received: 3 November 2014 Accepted: 27 February 2015JOURNAL OFBIOMEDICAL SEMANTICSXiang et al. Journal of Biomedical Semantics 2015, 6:4http://www.jbiomedsem.com/content/6/1/4SOFTWARE Open AccessOntorat: automatic generation of new ontologyterms, annotations, and axioms based on ontologydesign patternsZuoshuang Xiang1, Jie Zheng2, Yu Lin1 and Yongqun He1*AbstractBackground: It is time-consuming to build an ontology with many terms and axioms. Thus it is desired to automatethe process of ontology development. Ontology Design Patterns (ODPs) provide a reusable solution to solve arecurrent modeling problem in the context of ontology engineering. Because ontology terms often follow specificODPs, the Ontology for Biomedical Investigations (OBI) developers proposed a Quick Term Templates (QTTs)process targeted at generating new ontology classes following the same pattern, using term templates in aspreadsheet format.Results: Inspired by the ODPs and QTTs, the Ontorat web application is developed to automatically generate newontology terms, annotations of terms, and logical axioms based on a specific ODP(s). The inputs of an Ontoratexecution include axiom expression settings, an input data file, ID generation settings, and a target ontology(optional). The axiom expression settings can be saved as a predesigned Ontorat setting format text file for reuse.The input data file is generated based on a template file created by a specific ODP (text or Excel format). Ontorat isan efficient tool for ontology expansion. Different use cases are described. For example, Ontorat was applied toautomatically generate over 1,000 Japan RIKEN cell line cell terms with both logical axioms and rich annotationaxioms in the Cell Line Ontology (CLO). Approximately 800 licensed animal vaccines were represented and annotatedin the Vaccine Ontology (VO) by Ontorat. The OBI team used Ontorat to add assay and device terms required byENCODE project. Ontorat was also used to add missing annotations to all existing Biobank specific terms in theBiobank Ontology. A collection of ODPs and templates with examples are provided on the Ontorat website andcan be reused to facilitate ontology development.Conclusions: With ever increasing ontology development and applications, Ontorat provides a timely platform forgenerating and annotating a large number of ontology terms by following design patterns.Availability: http://ontorat.hegroup.org/Keywords: Ontorat, Ontology design pattern, ODP, Quick term templates, QTT, Ontology developmentBackgroundThe Web Ontology Language (OWL) has been widelyused for ontology development. However, ontology devel-opment and updating in OWL format is often time con-suming and requires specialized knowledge of ontologytools as well as specific scientific domains. Ways to im-prove the process of ontology development are desirable.It is frequently observed that a large number of new* Correspondence: yongqunh@med.umich.edu1University of Michigan, Ann Arbor, MI, USAFull list of author information is available at the end of the article© 2015 Xiang et al.; licensee BioMed Central. TCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.ontology terms and term annotations follow the samedesign patterns of logical definitions and axioms. Anontology term refers to a term with a Uniform ResourceIdentifier (URI) in the ontology. Even with the help ofthe Protégé-OWL editor (http://protege.stanford.edu/),manual adding and editing of these terms and annota-tions is labor-intensive and time-consuming. To makethe ontology development more efficient, it is possibleto develop tools to automate the process of adding theontology contents with repetitive design patterns.An OWL format ontology includes a set of axiomsthat provides explicit logical assertions about three typeshis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 2 of 10http://www.jbiomedsem.com/content/6/1/4of entities - classes, individuals and properties. As withsoftware design patterns for software engineering, anOntology Design Pattern (ODP) represents a reusable so-lution to solve a recurrent modeling problem in the con-text of ontology engineering. ODPs can be applied tosupport ontology rational design and development, im-prove ontology quality and reuse, disambiguate relations,provide scalable representations of entities, and make on-tologies more maintainable and understandable [1-5]. Theweb portal of ODPs (http://ontologytdesignpatterns.org)has collected many ODPs in different fields [6]. ODPshave also been studied in biological and biomedicalfields [1-4,7,8]. ODPs can be represented using ontologicalaxioms or graphic diagrams.Since many ontology terms (e.g., assays, vaccines) followthe same design patterns, it is possible to apply specificODPs in new ontology term generations to supportontology enrichment and expansion. To support quickgeneration of new ontology classes, the developers ofthe Ontology for Biomedical Investigations (OBI) [9]proposed the usage of a Quick Term Template (QTT),which is a spreadsheet template for populating termsto define specific ontology classes [10]. The populatedtemplate spreadsheet can then be converted into anOWL file with newly generated ontology classes. Thegeneration of QTT templates relies on repeatable patternsof to-be-generated ontology classes [10]. The conversionof an input file generated using a QTT template to anOWL output document could be implemented usingMappingMaster, a plugin program in the Protégé-OWLeditor [11,12]. The MappingMaster plugin works inProtégé-OWL editor version 3.4 that only supportsOWL 1. However, the tool does not function in Protege4.0 or higher versions that support OWL 2.0 and havebecome the main choices of ontology developers.Inspired by the ODP theories and OBI project QTToperation, we developed Ontorat (http://ontorat.hegroup.org/), a web application with the aim to automatically gen-erate a large number of new ontology classes or add add-itional axioms (e.g. annotations) to existing classes for aspecific target ontology. Ontorat offers a web-based plat-form for writing up ontology axiom expressions with vari-ables. Based on the axiom settings and a user-providedinput data file populated on a QTT-like template, Ontoratis able to generate an OWL format output file, which canbe imported into a target ontology to enrich and expandthe ontology. Ontorat was first presented in the ICBO-2012 conference as a software demo [13]. The tool hasbeen much improved during the past two years, includingbug fixes, web user interface improvements, and new fea-ture additions. Ontorat has been used in enriching severalwidely-used ontologies including the Vaccine Ontology(VO) [14,15], the Ontology for Biomedical Investigations(OBI) [9], and the Cell Line Ontology (CLO) [16]. Toallow users to better understand and use the tool, we pro-vide systematic descriptions and use case examples of theOntorat in this paper.Overall designBased on the ODP concept and the Quick Term Templates(QTT) procedure, we developed an overall strategy of ap-plying these mechanisms to ontology expansion (Figure 1).First, an ODP that covers a set of terms and their relationsneeds to be identified (Figure 1a). Formal axioms that assertlogical relations among ontology terms and annotationsof these terms will then be specified based on the ODP(Figure 1b). The ODP will guide the generation of a tab-delimited text or Excel template file which would con-tain all terms and annotations needed to define targetedterms (Figure 1c). This template file will then be used topopulate specific contents (Figure 1d). By combining theaxiom settings and the input data file, an OWL formatoutput can be generated (Figure 1e).We have developed the web-based Ontorat tool thatimplements the ontology enrichment strategy shown inFigure 1. Figure 2 lays out the Ontorat design and work-flow pipeline. Specifically, on the Ontorat web page, a userenters setting options and uploads the input data file viathe Ontorat web input form. The input data file is gener-ated by populating a predesigned template file guided bythe ODP as mentioned above. After accepting the inputdata file and setting options from the user, the web server(via a PHP script) will be able to execute two operations: 1)generation of new ontology classes with logical axiomsand annotations, or 2) addition of new axioms to existingontology terms. The Ontorat server will process the usersrequests and generate either an Ontorat settings file or anOWL output file. The Ontorat settings file can be storedand reused later. For the OWL output generation, aManchester syntax file will be generated first and thentransferred to OWL format (Figure 2).ImplementationSever setupThe Ontorat server is a single HP server running the RedHat Linux operating system (Red Hat Enterprise Linux 6).The Selinux program is enabled to improve the securityand stability of the server. The open source Apache HTTPServer is installed as the HTTP application server. PHPis used as the programming language in the web appli-cation server. OWL API is used for OWL format dataoperations.Ontorat (http://ontorat.hegroup.org) provides a user-friendly web form for data input (Figure 3).Ontorat inputsAs guided by the general strategy shown in Figure 1, anOntorat execution requires two types of required inputs:Figure 1 The strategy of applying ODPs into ontology term and annotation generation. An ODP is used to guide the generation of axiomsettings and a template file (text or Excel format). The template file is populated with specific contents to create an input data file. Based on theaxiom assertions and input data file, an OWL output can be generated by a software program to expand a targeted ontology.Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 3 of 10http://www.jbiomedsem.com/content/6/1/4 Input data file, Figure 3 (2):An Ontorat template file is usually generated firstbased on the ODP including all term and annotationtypes needed for defining a target term and thenfilled up with specific terms and annotations foreach type. The file can be provided in an Excel ortab-delimited text format.Figure 2 Ontorat software overall design and workflow. See the text for Axiom settings, Figure 3 (4)-(6):The axioms are represented using Manchester OWLSyntax [17] in Ontorat. The axiom settings can be addedone by one via the Ontorat web form or uploaded froman Ontorat setting text file in an Ontorat-specific settingfile format. Ontorat can also generate the setting filebased on the setting inputs via the Ontorat web form.description.Figure 3 The Ontorat web interface with explanation. The balloons represent components of the Ontorat web form for users to provide orclick. It is noted that some components are optional. The text notes inside boxes are the explanation notes for specific Ontorat components inthe web form.Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 4 of 10http://www.jbiomedsem.com/content/6/1/4Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 5 of 10http://www.jbiomedsem.com/content/6/1/4Three types of axiom assertions are allowed inOntorat:a. Annotations. The annotations associateinformation with an ontology class. Eachannotation includes an annotation property withits value [18].b. Equivalent classes. Equivalent classes provideboth sufficient and necessary axiom assertions todefine an ontology class.c. Superclasses. Superclass axioms assert the parentsof an ontology class.In Ontorat, the above ontology axioms are formattedusing the Manchester OWL Syntax, a logicalsyntax designed for writing OWL class expressions(http://www.w3.org/TR/owl2-manchester-syntax/)[17]. An internally designed code is used to representdifferent columns (i.e., variables). Specifically, we use{$columnA} to represent the first column (or columnA), and use {$columnB} to represent column B, etc.Each column represents a variable that will be used todefine an ontology class.In addition, the URIs of terms, including manycommonly used properties (e.g., rdfs:label) shownin the axiom settings (Figure 3 (7)), need to bespecified since the Ontorat program cannot knowthey are ontology terms unless their URIs areprovided.In addition, Ontorat requests two other types of inputsbefore execution. Operation type, Figure 3 (3):Ontorat supports two kinds of operations based onpurposes: (1) generation of new ontology classeswith axioms, and (2) modification of existingontology classes with adding new axioms. AnOntorat user is requested to specify the purpose ofan Ontorat operation. Inputs for assigning unique URIs to newly generatedterms:When Ontorat generates new classes, uniqueURIs will be assigned to newly generated terms.To achieve this task, the following information isneeded:a. Target ontology, Figure 3 (1):A user has an option to provide a target ontologyto ensure that unique ontology IDs will beassigned to newly generated ontology terms.Ontorat currently does not retrieve theinformation of ontology from existing ontologyRDF triple store. To provide a target ontology, anOntorat user can either upload the targetontology from a local computer or provide theURL of the target ontology.b. Start portion of term URI, Figure 3 (8):The start portion of term URI used for newlyadded terms need to be specified. For example,the string http://purl.obolibrary.org/obo/ is usedas the start portion of a URI of a term in an OBOFoundry ontology.c. Information of auto-generated term ID, Figure 3 (9):Three data items are needed: prefix, number ofdigits, and the start ID number. For example, theVaccine Ontology (VO) terms have the prefix ofVO_ that is followed by 7 digits. We can manuallyspecify the start ID from 1 or from anothernumber (e.g., 10000). This feature has a pitfallsince the incrementally assigned IDs from the startID may duplicate existing IDs in the target ontology.To avoid this potential conflict, users may uploadthe target ontology as described above. With thetarget ontology provided, Ontorat will ensure theautomatic generation of non-replicated IDs.After the above information is provided manually,Ontorat can generate an input setting text file for laterreuse (Figure 3), which is an important feature of Ontorat.Ontorat outputsBased on a users request, the Ontorat can generate twokinds of outputs: an OWL file converted from a spread-sheet data file based on axiom settings, and an input settingfile described above.The Ontorat output OWL file can be visualized usingOWL ontology editors such as Protégé (http://protege.stanford.edu/). The output OWL file can be imported toa target ontology (e.g., VO) using the OWL import functionor merged to enrich the target ontology.It is noted that a Manchester syntax file is generatedinternally as an intermediate file which is used as the inputto generate a final OWL output file. When an error occursin translating the Manchester syntax to OWL format,Ontorat will be able to provide the intermediate Manches-ter syntax file for debugging.AvailabilityThe Ontorat program is freely available on the website:http://ontorat.hegroup.org/. The source code of theOntorat software is released and available for downloadingon Github: https://github.com/ontoden/ontorat. The sourcecode is open source with the license of Apache License 2.0.Features and usageAs described above, the Ontorat web application supportstwo operations: generation of new ontology classes withaxioms, and adding new axioms to existing ontologyclasses. Three types of axiom assertions (for assertingannotations, equivalent classes, and superclasses) areXiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 6 of 10http://www.jbiomedsem.com/content/6/1/4allowed in Ontorat. In this section, we will use two spe-cific examples to demonstrate how Ontorat supportsthe above features, briefly summarize other use cases,and then describe the Ontorat collection of different de-sign patterns, templates, and examples.Illustration of Ontorat features using CLO and Biobankuse casesCell lines are routinely used in various biological andbiomedical studies such as analysis of cell signallingpathway studies and host-pathogen interactions [19,20].The Cell Line Ontology (CLO) is a community-basedontology that has logically represented over 38,000 cellline cells [16]. For this Ontorat case study, an Excel filecontaining information of over 1,000 cell line cells, whichwas obtained from the Cell Bank of RIKEN BioResourceCenter (BRC) in Japan, was used as input to add these cellline cell terms and their annotations into CLO [16].Figure 4 demonstrates an Ontorat example based onthe general strategy shown in Figure 1. Figure 4a showsthe design pattern used to define cell line cells obtainedfrom the RIKEN BioResource Center. Based on the de-sign pattern, the following elements (terms or annota-tions) are needed to define a cell line cell: (i) Cell lineresource (e.g. Japan RIKEN Cell Bank); (ii) Tissue in anorganism that a cell line cell is derived from; (iii) Person(s) who registered the cell line (register); and (iv) Personswho developed or maintained the cell line (originator).As described in the Implementation section, differentassertion axioms were generated to represent the relationsof terms or annotations to targeted terms (e.g. cell linecells) (Figure 4b). For example, cell line resource is repre-sented as a superclass axiom expressed as follows:is in cell line repository some RIKEN Cell BankThis axiom specifies that the newly generated cellline cell is in the RIKEN cell line repository. To ensurethat Ontorat correctly interpreted the axiom, the termURIs for both is in cell line repository and RIKENCell Bank should be specified in the web form as indi-cated in Figure 3(7). With these specifications, Ontoratwill be able to translate the axiom into an OWL ex-pression. The annotations of the term, such as label,are represented as annotation axioms, as demonstratedbelow (lower part of Figure 4b):label {$columnA} cellThis axiom represents that the label of the newlyadded cell line cell term is defined as the string shownin the column A (represented by {$columnA}) of the in-put data file followed by the word cell. The input tem-plate file (Figure 4c) was populated with information fora specific cell line cell per row (Figure 4d). The string inthe column A of the first row is RCB2320. Based onthe above axiom setting, the label of the first cell linecell term is RCB2320 cell (Figure 4e).Using the same approach, Ontorat has added the infor-mation of derived tissues, originators, and registers of in-dividual cell line cells as annotation axioms of the newlygenerated cell line cell terms (lower part of Figure 4b). Itis noted that in this case, we have added this informationas annotations of cell line cell terms. It is also possible toadd the same information as superclass axioms if we wishto. For example, instead of defining the following annota-tion axiom:comment Derived from tissue: {$columnG} in animal:{$columnF}.We should add the following superclass axiom assertion:derived from some ({$columnG} part of  some{$columnF})Where column G includes tissue information and col-umn F includes animal information. In this case, the termderived from should be an object property. Furthermore,instead of simple strings, specific ontology term URIsrepresenting the tissue and animal should be provided incolumn G and column F, respectively. Therefore, sameODP could be represented by different OWL expressions.The detailed Ontorat ODP, template, setting file, andthe example input and output files are available on theOntorat template web page: http://ontorat.hegroup.org/designtemplates/cellline/clo-celllinecell.php.The above CLO example involves the generation ofnew ontology terms and addition of logical axioms andannotation axioms at the same time using Ontorat.Ontorat supports editing existing terms by addition ofnew axioms (e.g. annotations). For example, Ontorat wasrecently used to automatically add definition source andterm editor annotations to over 50 ontology classes inthe Biobank Ontology (https://code.google.com/p/bio-bank-ontology/). The Biobank Ontology is developed forrepresenting and annotating entities related to Biobankrepositories. When new terms were initially added intothe ontology, definition source and term editor were notspecified. To add the annotations to biobank-specificclasses, the following settings were used in the Ontoratannotations input section:definition editor {$columnC},definition source {$columnD}Since the aim of this use case is to add annotations toexisting ontology terms, the edit existing classes  optionFigure 4 Demonstration of an Ontorat use case for ontology enrichment. This use case aimed to enrich the Cell Line Ontology (CLO) withnew over 1,000 cell line celles collected in Japan RIKEN Cell Bank. First the ODP was identified to define these cell line cells (a). As guided by theODP, a list of Ontorat settings was generated to specify axiom expressions with possible variables of terms and annotations (b). The template file(c) was also generated and used to fill specific contents (d). Finally Ontorat generated an OWL format output file containing newly createdontology terms together with their annotations. The output could be displayed using the Protégé-OWL editor (e). It is noted that only parts ofOntorat settings and input data file are shown here. The full version of the files is availableon: http://ontorat.hegroup.org/designtemplates/cellline/clo-celllinecell.php.Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 7 of 10http://www.jbiomedsem.com/content/6/1/4was chosen in the Ontorat Purpose input (Figure 3 (2)).The Ontorat input files used to edit Biobank Ontologyand the output OWL file are available on: http://ontorat.hegroup.org/designtemplates/biobank/index.php.Brief summary of other Ontorat use casesIn the original Ontorat software demonstration in theICBO-2012 conference [13], Ontorat was used to addapproximately 800 US-licensed animal vaccines to theVaccine Ontology [14,15]. VO is a community-basedontology in the domain of vaccine and vaccination. Thesevaccines include 303 licensed vaccines against infectionsof individual pathogens and 494 combination vaccines,each of which protects against infections of two or morepathogens. The data for these vaccines were originally ex-tracted from the official USDA website and stored in theVIOLIN vaccine database (http://www.violinet.org) [21].Corresponding to the two sets of animal vaccines basedXiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 8 of 10http://www.jbiomedsem.com/content/6/1/4on one or more pathogens targeted by a vaccine, twoOntorat Excel template files were generated. In additionto the generation of new classes of licensed animal vac-cines, Ontorat was used to add annotations using annota-tion properties (e.g., see_Also and term definition) [13].To achieve multiple tasks, we performed multiple Ontoratexecutions, each execution to achieve a specific task.A large number of experimental assays have been usedin the biological and biomedical fields. The OBI consor-tium has a major focus on modeling and representing theseassays [9]. OBI assays were defined by several elements in-cluding: (i) assay inputs, such as materials to be evaluatedand devices used; (ii) assay output that is information aboutsome biological process or function (e.g., gene expression,DNA methylation); (iii) assay aims, such as identificationof epigenetic modification, and (iv) main processes of anassay, such as immunoprecipitation and sequencing. It isoften complicated to fully represent and annotate an assayterm in OWL expression. To manually generate assayterm with rich axioms is very time-consuming and has be-come a bottleneck in OBI ontology expansion. To solvethis issue, Ontorat was applied.Since the Excel template file format is generally friendlyand widely used by the public, domain experts withoutontology knowledge are able to add contents to the tem-plate file. In the community-based ENCODE project [22],the OBI team developed specific template files for addingassay and device terms based on ODPs. The templateswere then provided to domain experts for them to submitterm requests. The requested terms with rich annotationsand logical axioms were then added into OBI usingOntorat, and the ontology term IDs assigned by Ontoratwere provided to the end users for their usage.Recently Ontorat has been utilized to add mouse strainterms in the Beta Cell Genomics Ontology (BCGO) [23].Although BCGO did not define mouse strain logically, itcontains rich annotations including MGI id, commonname, alternative term, definition, definition source, andterm editor. The Ontorat speeded up generation of theseterms. Moreover, since settings and templates can bereused, it will be easy to add more mouse strain termsin the future.In addition to the use cases described above, Ontorathas been applied to the development of the Ontology ofVaccine Adverse Events (OVAE) [24] and the Ontologyof Biological and Clinical Statistics (OBCS) [25].Collection of design patterns and templatesSince the ontology design pattern is a reusable modelingsolution for building an ontology, the Ontorat websitehas provided a collection of design patterns and corre-sponding templates for ontology developers to reuse. Foreach collected case, Ontorat provides an ODP diagram,an Excel template, a setting file, and an example withpopulated template data and output OWL file. The col-lection supports the development of several ontologies,including OBI, VO, CLO, and BCGO and available on:http://ontorat.hegroup.org/designtemplates.DiscussionManually adding a large amount of terms or terms withrich axioms into an ontology is a big challenge and be-come a bottleneck of ontology development. It is timeconsuming and error-prone to do it manually. Manyontology terms were generated with the same ontologydesign patterns (ODPs). Based on ODPs and inspired bythe Quick Term Template (QTT) procedure, the Ontoratweb application is developed to provide a robust and scal-able platform for automatically generating new ontologyterms, axioms and annotations. Ontorat supports efficientontology enrichment and expansion. The design patternscan be reused by ontology developers. The Ontoratspreadsheet templates lower the technical barriers fordomain experts and data curators, so that they may con-tribute actively to the ontology development withoutknowing the specifics of OWL.Tools with similar functions to Ontorat exist, includ-ing MappingMaster [4], Populous [26], and TermGenie(http://code.google.com/p/termgenie/). As introduced inthe Background section, as a Protégé plugin, Mapping-Master can only be used with old version Protégé 3.4and has not been updated to work for commonly usedProtégé 4 and 5 [4]. In addition, MappingMaster requireswriting template class expression using a M2 language, aDomain Specific Language (DSL) based on the ManchesterOWL syntax. The programming with the language requiresa learning curve. In contrast to MappingMaster, Ontoratcan build axiom expressions from a web form using theManchester syntax. Ontorat has the capability of automat-ically generating annotations of ontology terms. Populousprovides desktop standalone and user-friendly interface[26]. However, it needs software installation. Populousdoes not support the generation of term annotations.Ontorat is implemented as a user-friendly web-based ap-plication without the necessity of software download andinstallation. TermGenie provides a web application thatcreates new terms for an ontology using patterns (http://code.google.com/p/termgenie/). TermGenie has been usedfor the Gene Ontology (GO) and its cross products(http://go.termgenie.org/). Based on predefined pat-terns, TermGenie supports new ontology term gener-ation and provides a user-friendly interface to domainexperts. Compared to Ontorat, TermGenie does notallow the generation of new terms based on user-provided patterns. Ontorat provides more flexibility inallowing users to define patterns for different ontol-ogies. TermGernie cannot be used to add new axiomsto existing terms.Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 9 of 10http://www.jbiomedsem.com/content/6/1/4To ensure the generation of unique IDs for newly gen-erated terms for a target ontology, the ontology is cur-rently required to be loaded in Ontorat. URIGen is aJava API and web service for managing ontology URIcreation (http://www.ebi.ac.uk/fgpt/sw/urigen/). URIGenalso provides a REST interface that interacts with theURIGen server. It is possible to incorporate the URIGendistributed ID management functionality into Ontoratfor unique ID assignment.While Ontorat is primarily targeted for ontology de-velopers with sufficient OWL ontology background,Ontorat provides a way to separate the duties from theontology developers and domain experts who both partici-pate in the development of a specific domain ontology.Ontorat separates the Manchester syntax programmingfrom the template spreadsheet population. A domain ex-pert who does not know programming can still work onthe ontology development project by working on populat-ing the Excel spreadsheet. For example, in the OBI Assayexample described above, after receiving the Assay Exceltemplate file, the domain experts in the ENCODE project[22] were able to independently provide definitions andother information needed to define an assay term. Afterobtaining the Excel file from the ENCODE group, the OBIdevelopers were able to use Ontoat to generate new ontol-ogy terms and annotations separately. The logical axiomexpressions in Ontorat use the standard and widely-usedManchester syntax, together with simple Ontorat rules forrepresenting ontology variables. Therefore, Ontorat pro-vides a relatively straight forward platform for ontologydevelopers who are familiar with the Manchester syntax,which is also used in the Protégé-OWL editor.Ontorat implements the Quick Term Template (QTT)procedure and more. An Ontorat template is equivalentto a QTT template when the template is designed forgenerating new ontology classes for ontology expansion.In addition to new class generation, Ontorat can alsosupport the addition of new annotations to existingontology classes. In the future, Ontorat will also supportthe generation of axioms that contain instances. Differ-ent from the QTT approach, Ontorat emphasizes thegeneration of machine-readable and reusable axiom set-ting file. The Ontorat axiom expressions use the Man-chester OWL syntax and easy-to-use Ontorat syntax ofvariables. The Ontorat syntax provides a way to repre-sent variables that are mapped to columns in the Exceltemplate spreadsheet. The Ontorat generated Ontoratsetting file is easily understandable and reusable.Among software programs that support ontologydevelopment, Ontorat is complementary to OntoFox(http://ontofox.hegroup.org), another web applicationdeveloped by our group with the support from the OBOFoundry community [27]. OntoFox supports the re-trieval of a subset of ontology terms and axioms fromexisting ontologies [27]. Ontorat and OntoFox are com-plementary in the sense that OntoFox supports the reuseof existing ontology terms and Ontorat supports theautomatic generation of new ontology terms, axiomsand annotation of ontology terms. OntoFox and Ontorathave been combined in use for development of new on-tologies, such as the Cell Line Ontology (CLO) [16],Vaccine Ontology [28], Ontology of Biological and Clin-ical Statistics (OBCS) [25], and Beta Cell GenomicsOntology (BCGO) [23]. In fact, Ontorat and OntoFoxare developed using similar web-based form and settingfile design. For example, the Ontorat setting file is simi-lar in spirit to the OntoFox setting file that has beenproven to be very useful for reusability. We will seekways to better integrate these two software programs formore efficient ontology development.Furthermore, we plan to expand the Ontorat collec-tion of ODPs, templates, and setting files together withexamples. Such a collection will support ontology designpattern reuse, standardization, and various applications.We encourage all parties to participate in contributingtheir domain knowledge and expertise in this collabora-tive movement.Ontorat was introduced in an OBO Tutorial in theInternational Conference on Biomedical Ontologies(ICBO) in 2013. The tool was also demonstrated in anOBO Tutorial and an OBO Technical Workshop inICBO-2014 (http://icbo14.com/), held at Houston, Texas,USA. Given strong community demands and support,Ontorat has provided a timely platform to support effi-cient ontology development and applications.ConclusionsOntorat (http://ontorat.hegroup.org) is a web applicationthat supports automatic generation of new ontology terms,term annotations, and logical axioms. Ontorat allows thestorage and reuse of axiom setting files and input templatefiles. Ontorat has also started the collection of reusableontology design patterns and templates.AbbreviationsBCGO: Beta cell genomics ontology; CLO: Cell line ontology; GO: Geneontology; ICBO: International conference on biomedical ontology;OBCS: Ontology of biological and clinical statistics; OBI: Ontology forbiomedical investigations; OBO: Open biology and biomedical ontologies;OVAE: Ontology of vaccine adverse events; VO: Vaccine ontology.Competing interestsThe authors declare that they have no competing interests.Authors contributionsZX was the primary software programmer of Ontorat. JZ generated manyuse cases and provided a collection of design patterns and templates onOntorat. YL executed Ontorat use cases and participated in activediscussions. YH is the project manager and another programmer, designedthe Ontorat architecture, and tested use cases. YH and JZ drafted themanuscript. All co-authors reviewed the manuscript and agreed on themanuscript submission. All authors read and approved the final manuscript.Xiang et al. Journal of Biomedical Semantics 2015, 6:4 Page 10 of 10http://www.jbiomedsem.com/content/6/1/4AcknowledgementsWe thank Rebecca Racz for proofreading this manuscript and for hervaluable comments. The work described is funded in part by the NationalInstitutes of Health (NIH) grants 1R01AI081062 (YH) from the NationalInstitute of Allergy and Infectious Diseases (NIAID). The content of this paperis solely the responsibility of the author and does not necessarily representthe official views of the NIAID and the NIH or other funding organizations.The article-processing fee for this article was paid by a bridge fund to YH atthe Unit for Laboratory Animal Medicine (ULAM) in the University ofMichigan Medical School.Author details1University of Michigan, Ann Arbor, MI, USA. 2University of Pennsylvania,Philadelphia, PA, USA.Received: 20 October 2014 Accepted: 25 December 2014JOURNAL OFBIOMEDICAL SEMANTICSKafkas et al. Journal of Biomedical Semantics 2015, 6:1http://www.jbiomedsem.com/content/6/1/1RESEARCH Open AccessDatabase citation in supplementary data linked toEurope PubMed Central full text biomedicalarticles?enay Kafkas*, Jee-Hyub Kim, Xingjun Pi and Johanna R McEntyreAbstractBackground: In this study, we present an analysis of data citation practices in full text research articles and theircorresponding supplementary data files, made available in the Open Access set of articles from Europe PubMedCentral. Our aim is to investigate whether supplementary data files should be considered as a source of informationfor integrating the literature with biomolecular databases.Results: Using text-mining methods to identify and extract a variety of core biological database accession numbers,we found that the supplemental data files contain many more database citations than the body of the article, andthat those citations often take the form of a relatively small number of articles citing large collections of accessionnumbers in text-based files. Moreover, citation of value-added databases derived from submission databases (suchas Pfam, UniProt or Ensembl) is common, demonstrating the reuse of these resources as datasets in themselves. Allthe database accession numbers extracted from the supplementary data are publicly accessible from http://dx.doi.org/10.5281/zenodo.11771.Conclusions: Our study suggests that supplementary data should be considered when linking articles with data, incuration pipelines, and in information retrieval tasks in order to make full use of the entire research article. Theseobservations highlight the need to improve the management of supplemental data in general, in order to makethis information more discoverable and useful.Keywords: Text mining, Supplementary data, Accession number, Molecular biology databasesBackgroundBiomolecular and literature databases are a vital resourcefor the scientific community. Linking these resourcesenables scientists to access, analyse and process the datacomprehensively. One way to link these resources is toidentify accession numbers as specific database citationsin text. Accession number annotation in full text hasbeen tackled in a variety of ways at various points in thepublication lifecycle. While some publishers tag (structur-ally annotate) accession numbers in the text of articles asa part of their production process, this is not somethingdone comprehensively across all publishers [1]. In theabsence of machine-actionable citation data, text min-ing [1-4] can be used to annotate accession numbers* Correspondence: kafkas@ebi.ac.ukEuropean Molecular Biology Laboratory, European Bioinformatics Institute(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UK© 2015 Kafkas et al.; licensee BioMed Central.Commons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.automatically across large volumes of published researcharticles. One such study is our recent work on the citationof three major submission databases (ENA, UniProt,PDBe) within the Open Access subset of Europe PMC(http://europepmc.org/). In this study, we investigated towhat extent, (1) publishers provide structurally annotatedaccession numbers in full text, (2) text mining extendspublisher annotations and (3) text mining contributes toliteraturedatabase cross links. Our results show that textmining can significantly enrich publishers annotationsand contribute to literaturedatabase cross links (see [1]for details).Although the annotation of many types of accessionnumbers is now part of the routine processing of fulltext articles in Europe PMC, the extent of data citation insupplementary data has yet to be explored. Supplementarydata is unstructured and therefore the content is basicallyundiscoverable via the usual retrieval methods that operateThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 2 of 7http://www.jbiomedsem.com/content/6/1/1only on the article narrative. However, finding database ci-tations in supplementary data could be useful for the deepintegration of literature and databases and potentiallyhelpful for curators [5]. Moreover, as reported in a recentstudy, which focuses on mining genetic variations fromliterature, supplementary materials were identified as acritical source of genetic mutations [6]. Here, we extendour previous study on the analysis of database citationin narrative of the full text articles to supplementarydata in order to understand whether supplementarydata is useful for linking articles to the biomoleculardatabases. To the best of our knowledge, this is the firststudy on the analysis of data citation in supplementarydata. In this study: We extended the Whatizit-AccessionNumber Annotation (Whatizit-ANA) module to anno-tate database citations to a total of ten biological data-bases and revised the extraction rules and patterns. Weanalysed and compared the distribution of the databasecitations in the body of the Open Access article set(OA-ePMC articles) and their associated supplementarydata files for the ten databases. All the database acces-sion numbers extracted from the supplementary dataare publicly accessible from http://dx.doi.org/10.5281/zenodo.11771.Materials and methodsLiterature and biomedical databases usedLiteratureThe full text articles and their supplementary files usedin this study were gathered from the OA-ePMC set. Thisopen access article set is available on the EuropePMC FTP site and the linked supplementary data filesare available via the Europe PMC RESTful web ser-vice (http://europepmc.org/restfulwebservice). We de-cided to reanalyse the set of OA-ePMC articles thatwe used in our previous study in order for the resultsto be directly comparable (http://europepmc.org/ftp/oa/AccNoAnalysisData/AnnotatedData/). This set contains410,364 full text articles in XML format [1]. It is formedby filtering out the articles which were published before1990 since in this historical set, accession number cita-tions are rare. We identified 361,937 supplementary filesthat belong to these articles in various formats. Thedistribution of these files according to the file formats isshown in Figure 1. This shows that the majority of thefiles have formats such as Portable Document Format(PDF) and Microsoft Word (DOC) and Microsoft Excel(XLS) that can be converted into text.A three-step pre-processing was applied to the gath-ered supplementary files: (1) screening out the supple-mentary files that are not easily convertible to textsuch as image, audio and movie file types (filtering isdone based on MIME types/subtypes that can be ex-tracted from the file link in the full text XML, withinthe </supplementary-material > element). (2) screeningout text supplementary files that are unlikely to containaccession numbers (e.g. source code files) (filtering is donebased using known file extensions for source code) and(3) employing Apache Tika [7] to extract the text contentfrom the remaining files. The final set of supplementarydata included a total of 213,245 supplementary files eitherin text or text convertible format linked to 68,995 of the410,364 OA-ePMC articles.Biological databasesWe used ten major biological databases in this study.Three of these databases are primary databases andthe other seven databases are added-value (secondary)databases.Primary databasesPrimary databases accept direct submissions of de novodata. The following primary databases were used in thisstudy: The European Nucleotide Archive (ENA, http://www.ebi.ac.uk/ena/) ArrayExpress (http://www.ebi.ac.uk/arrayexpress/) Protein Data Bank, Europe (PDBe, http://www.ebi.ac.uk/pdbe/)Added-value databasesAdded-value or secondary databases collect or presentdata as curated sets or summaries based on primary datasubmissions. The following added-value databases wereused in this study: The Protein families database (Pfam, http://pfam.sanger.ac.uk/) Universal Protein knowledgebase (UniProt, http://www.uniprot.org/) Reference Sequence (RefSeq, http://www.ncbi.nlm.nih.gov/RefSeq/) Reference Single Nucleotide Polymorphism (RefSNP,http://www.ncbi.nlm.nih.gov/SNP/) Ensembl (http://www.ensembl.org/index.html) Online Mendelian Inheritance in Man (OMIM,http://www.omim.org/) InterPro (http://www.ebi.ac.uk/interpro/)Annotating database citationsDatabase citations in publications were annotated by thetext-mining method used in our previous study [1].This method mainly uses Whatizit-Accession NumberAnnotation (Whatizit-ANA) module [1,8] where a setof extraction rules and patterns were applied with con-textual cues for recognising database citations. Thesepatterns are shown in Table 1.Figure 1 Distribution of supplementary data by file formats. This figure describes distribution of supplementary files linked to the EuropePMC open access full text articles by different file formats. The text convertible format covers the formats which can be convertible to text suchas pdf, xml, html and xsl.Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 3 of 7http://www.jbiomedsem.com/content/6/1/1For this study, we extended our annotation method by:1. Adding extraction rules and patterns to include tendatabase types (compared to the first version,accession number annotation is extended to fouradditional databases: Ensembl, RefSeq, RefSNP andOMIM).2. Revising the validation step by replacing theprevious accession number validator with a new onebased on the global EBI Search web service (http://www.ebi.ac.uk/Tools/webservices/services/eb-eye).Table 1 Extraction patterns and contextual cues fordatabasesDatabase Patterns Contextual cuesENA [A-Z][09]{5}; [A-Z]{2}[09]{6}; [A-Z]{3}[09]{5}; [A-Z]{4}[09]{8,10}; [A-Z]{5}[09]{7}genbank, gen, ddbj,emblUniProt [A-N,R-Z][09][A-Z][A-Z, 09][A-Z,09][09]; [O,P,Q][09][A-Z, 09][A-Z, 09][A-Z, 09][09]swissprot, sprot,uniprotPDBe [09][A-Z, 09]{3} pdbInterPro IPR[09]{6} interproPfam PF(AM)?[09]{5} hmm, family, pfamArrayExpress E-[A-Z]{4}-[09]+ arrayexpressOMIM [09]{6} omimEnsembl ENS[A-Z]*G[09]{11}+ ensemblRefSeq (AC|AP|NC|NG|NM|NP|NR|NT|NW|NZ|XM|XP|XR|YP|ZP|NS)_([A-Z]{4})*[09]{6,9}(?:[.][09]+)?refseqRefSNP RS[09]{5,9} snpThis new validator covers more databases(e.g. InterPro and Ensembl) and is morerobust.This new Whatizit-ANA module has been integratedinto the core Europe PMC infrastructure and is availablevia Whatizit web site and web service (http://www.ebi.ac.uk/webservices/whatizit).Results and discussionPerformance assessment of the Whatizit-ANA ModuleWe used the same sets of gold standards (for ENA,UniProt and PDBe) used in our previous study forassessing the performance differences between the pre-vious and current versions of the Whatizit-ANA mod-ule. The results presented in Table 2 show that thecurrent version of the module is better than the previ-ous version (F-score values of > 96% for UniProt andPDB and >77% for ENA) (please refer to [1] for ourperformance evaluation). This is due to the improve-ment in the validation component that we used in thenew version of our tool. This new validation compo-nent is capable of validating accession numbers moreaccurately, resulting in lower numbers of missed acces-sion numbers [see Table 2, the new system missesfewer accession numbers (false negatives) and henceidentifies higher number of accession numbers (truepositives) compared to the old one]. For example, inthe article with PMCID1892096, the UniProt citationP09372 was missed (false negative) using the old ver-sion of the tool, however it was annotated correctlywith the new version.Table 2 Performance assessment results of the Whatizit ANA moduleDatabase Evaluation #TP #FP #FN Precision (%) Recall (%) F-score (%)New Old New Old New Old New Old New Old New OldENA Automatic 276 267 10 7 170 181 96.50 97.45 61.88 59.60 75.41 73.96Manual 286 274 0 0 170 181 100 100 62.72 60.22 77.10 75.17UniProt Automatic 574 569 28 8 39 39 95.35 98.61 93.64 93.59 94.49 96.03Manual 601 577 1 0 39 39 99.83 100 93.91 93.67 96.78 96.73PDBe Automatic 568 529 32 30 12 50 94.67 94.63 97.93 91.36 96.27 92.97Manual 620 559 0 0 12 50 100 100 98.10 91.79 99.04 95.72FP: False Positive, FN: False Negative, Old: Old Whatizit-ANA settings, New: New Whatizit-ANA settings.Manual and automatic evaluation: In the automatic evaluation; we estimated the performance of the tool by assuming that publisher-suppliedaccession numbers in the articles are a gold standard for annotation. However, when we manually analysed the false positive annotations providedfrom our pipeline, we realised that the accession numbers provided in articles (the annotations that we assumed as gold standard in the automaticevaluation) might not be always complete or correct. Therefore, the annotations made by our tool, which were not already annotated in thearticle, were deemed false positives by the automatic evaluation, however, such annotations could be reassigned as true positives onmanual inspection.Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 4 of 7http://www.jbiomedsem.com/content/6/1/1Distribution of database citationsFigure 2 shows the distribution of database citation inthe 410,364 OA-ePMC articles and their supplementarydata. The analysis reveals that 16.8% of articles (68,995/410,364; Figure 2 (c)) have supplementary data in eithertext or text convertible format. Only, 3,365 of these68,995 articles (3,365/410,364; 0.82%; Figure 2 (f )) con-tain database citations in both their body and supple-mentary data.Figure 2 Distribution of database citations in the OA-ePMC articles. TPMC open access full text articles.Analysis of database citation in article body andsupplementary dataIn the full set of 410,364 OA-ePMC articles, 28, 610(6.97%, Figure 2 (g)) of the article bodies containdatabase citations. Of the 213,245 supplementary filesthat we can mine, 10,179 (4.77%) contain database ci-tations. Table 3 shows the distribution of the databasecitations in the bodies of these articles and supple-mentary files.his figure describes distribution of database citations in the EuropeTable 3 Distribution of database citations in article body and supplementary data by databases in the OA-ePMC setDatabase Supplementary data Article body Ratio Shared citationsEnsembl 1,292,198 1,152 1,121.70 23 (0.002%)RefSeq 2,540,260 2,864 886.96 178 (0.007%)InterPro 564,956 639 884.13 77 (0.014%)UniProt 2,972,519 9,387 316.66 540 (0.018%)Pfam 924,624 2,968 311.53 435 (0.047%)RefSNP 2,443,679 31,061 78.67 3,849 (0.16%)ENA 3,390,319 125,534 27.01 4,167 (0.12%)PDBe 197,850 44,269 4.47 2,805 (1.42%)ArrayExpress 2,377 1,565 1.52 53 (2.23%)OMIM 2,400 2,779 0.86 19 (0.80%)Kafkas et al. Journal of Biomedical Semantics 2015, 6:1 Page 5 of 7JOURNAL OFBIOMEDICAL SEMANTICSHur et al. Journal of Biomedical Semantics 2015, 6:2http://www.jbiomedsem.com/content/6/1/2RESEARCH Open AccessDevelopment and application of an interactionnetwork ontology for literature mining ofvaccine-associated gene-gene interactionsJunguk Hur1, Arzucan Özgür2, Zuoshuang Xiang3 and Yongqun He3,4,5,6*AbstractBackground: Literature mining of gene-gene interactions has been enhanced by ontology-based nameclassifications. However, in biomedical literature mining, interaction keywords have not been carefully studiedand used beyond a collection of keywords.Methods: In this study, we report the development of a new Interaction Network Ontology (INO) thatclassifies >800 interaction keywords and incorporates interaction terms from the PSI Molecular Interactions(PSI-MI) and Gene Ontology (GO). Using INO-based literature mining results, a modified Fishers exact test wasestablished to analyze significantly over- and under-represented enriched gene-gene interaction types within aspecific area. Such a strategy was applied to study the vaccine-mediated gene-gene interactions using allPubMed abstracts. The Vaccine Ontology (VO) and INO were used to support the retrieval of vaccine terms andinteraction keywords from the literature.Results: INO is aligned with the Basic Formal Ontology (BFO) and imports terms from 10 other existingontologies. Current INO includes 540 terms. In terms of interaction-related terms, INO imports and alignsPSI-MI and GO interaction terms and includes over 100 newly generated ontology terms with INO_ prefix. Anew annotation property, has literature mining keywords , was generated to allow the listing of differentkeywords mapping to the interaction types in INO. Using all PubMed documents published as of 12/31/2013,approximately 266,000 vaccine-associated documents were identified, and a total of 6,116 gene-pairs wereassociated with at least one INO term. Out of 78 INO interaction terms associated with at least five gene-pairsof the vaccine-associated sub-network, 14 terms were significantly over-represented (i.e., more frequently used)and 17 under-represented based on our modified Fishers exact test. These over-represented and under-representedterms share some common top-level terms but are distinct at the bottom levels of the INO hierarchy. The analysis ofthese interaction types and their associated gene-gene pairs uncovered many scientific insights.Conclusions: INO provides a novel approach for defining hierarchical interaction types and related keywords forliterature mining. The ontology-based literature mining, in combination with an INO-based statistical interactionenrichment test, provides a new platform for efficient mining and analysis of topic-specific gene interaction networks.Keywords: Biomedical ontology, Interaction network ontology, Literature mining, Interaction enrichment, Gene-geneinteraction* Correspondence: yongqunh@med.umich.eduEqual contributors3Unit for Laboratory Animal Medicine, University of Michigan, Ann Arbor, MI48109, USA4Department of Microbiology and Immunology, University of Michigan, AnnArbor, MI 48109, USAFull list of author information is available at the end of the article© 2015 Hur et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public DomainDedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,unless otherwise stated.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 2 of 10http://www.jbiomedsem.com/content/6/1/2BackgroundTwo common strategies of literature retrieval of re-ported gene-gene interactions include gene-gene co-occurrence and interaction keywords-based literaturemining. In this paper, the gene-gene interaction repre-sents a broad interactive relation between two genesor gene products [1]. Such a relation does not have tobe a direct physical interaction. The co-occurrencestrategy identifies two related genes both listed in thesame literature, or more specifically in the same title,abstract, or sentence. An example of such a strategy isPubGene, which extracts gene relationships based onthe co-occurrence of gene symbols in MEDLINE titlesand abstracts [2]. The other strategy relies on the identifi-cation of two genes together with an interaction keywordin the same sentence. Such a method may still generatemany false-positive results. To improve the interactionkeyword-based approach, machine learning algorithms(e.g., support vector machine (SVM) [3]) with featuresextracted from syntactic analysis of sentences (e.g., de-pendency parse trees) can be used [4].Ontologies can be applied to enhance literature miningperformance. For example, in our previous work, avaccine-specific sub-network was built by consideringonly the interactions that were extracted from sen-tences that contain the vaccine term (or its variantslike vaccines, vaccination, and vaccinated). Thisstrategy does not retrieve the sentences where morespecific vaccine names such as BCG (a commercialtuberculosis vaccine) are mentioned. Such vaccinenames and their hierarchical relations are representedin Vaccine Ontology (VO) [5]. We found that the ap-plication of VO has significantly improved the analysisof the vaccine-specific sub-networks [6].An ontology that logically represents various inter-action keywords/types and their semantic relationswould help address the challenge of retrieving and clas-sifying the types of gene-gene interactions in the inter-action keyword-based literature mining. The GENIAontology provides a semantically annotated corpus forbiological literature mining [7]. However, this ontologydoes not specify various types of interactions betweengenes or proteins. Initiated from the classification of >800interaction keywords [6], we have developed the Inter-action Network Ontology (INO) that ontologically repre-sents various interaction types and their relations, andcollects and assigns interaction keywords to these differentinteraction types. The details about the ontology will, forthe first time, be provided in this manuscript.In addition to supporting the literature mining ofgene-gene interactions, INO can be used for interactiontype enrichment analysis. Gene Ontology (GO)-basedgene set enrichment analyses have been widely used to de-termine over- or under-represented biological functions ina set of genes obtained from high-throughput Omics stud-ies. GO provides controlled vocabulary of standard termsfor describing gene product characteristics in a hierarch-ical structure. The input to the GO term enrichment ana-lysis is a list of genes. Such a method does not classifyenriched gene-gene interactions. Since INO classifiesdifferent interaction types into a structured ontology, itbecomes possible to perform a gene-gene interaction en-richment study by comparing the INO-based literature-mined data of gene-gene interactions in some specificdomain over the data from the broad background.In this manuscript, we will first introduce the develop-ment of INO with a focus on its representation of inter-action types and keywords for literature mining. AnINO-based gene interaction enrichment method basedon a modified Fishers exact test will then be introduced.We applied our approach to the analysis of the vaccine-mediated gene-gene interactions. The resulting over-and under-represented gene-gene interaction types andgene-gene interactions will also be described in detail.MethodsINO developmentINO was developed by following the Open BiologicalOntology (OBO) Foundry ontology development princi-ples, including openness and collaboration [8]. Its devel-opment is aligned and integrated with existing OBOFoundry library ontologies. INO imports existing termsby using OntoFox [9]. New terms generated in INO usethe INO_ prefix. INO uses the format of W3C stand-ard Web Ontology Language (OWL2) (http://www.w3.org/TR/owl-guide/). For efficient editing of INO, theProtégé 4.3 OWL ontology editor (http://protege.stanford.edu/) was used.The INO source is open freely under a Creative Com-mons (CC) license for public and commercial usage.INO has been deposited at the INO SourceForge projectpage (http://sourceforge.net/projects/ino/). It is alsoavailable in the ontology repositories of National Centerfor Biomedical Ontology (NCBO) BioPortal (http://purl.bioontology.org/ontology/INO) and Ontobee [10] (http://www.ontobee.org/browser/index.php?o=INO).INO-based literature mining of gene-gene interactionpairs and interaction typesThe sentences from the complete PubMed abstracts(published up to 12/31/2013) were obtained from theBioNLP database in the National Center for IntegrativeBiomedical Informatics (http://ncibi.org/). Our in-houseliterature mining tools, SciMiner [11] and VO-SciMiner[12], were used to identify gene names/symbols and VOand INO terms (interaction keywords) from these sen-tences. Sentences with two gene names and at least oneINO term (e.g., interacts, binds, activates) were selected.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 3 of 10http://www.jbiomedsem.com/content/6/1/2We obtained the dependency parse trees of the sen-tences using the Stanford Parser [13] and extracted theshortest dependency path between each pair of genes ina sentence. We defined an edit distance-based kernelfunction among these dependency paths and used SVM[3] to classify whether a path describes an interactionbetween a gene pair [6]. A confidence score calculatedbased on SVM was used to measure the confidence ofassociation between two genes in a sentence in the lit-erature. Positively-scored sentences were kept, and thegene pairs together with the interaction keywords fromthese sentences were extracted. The extracted inter-action keywords were mapped to INO to define theinteraction types.Development of INO-based statistical enrichment analysisof literature mined gene-gene interaction dataA modified Fishers exact test has gained popularity overthe last decade in high-throughput gene expressionstudies as a preferred method for identifying enrichedbiological functions among given gene sets [14,15]. Weimplemented the modified Fishers exact test in Perlusing the Ngram Statistics Package [16] to identifyenriched gene-gene interaction types, in terms of INOterms, within a concept-specific sub-network. For eachINO term, a 2×2 contingency table is obtained on whichthe Fishers test runs, as shown in Table 1. Both signifi-cantly under-represented and over-represented terms areselected as a significantly enriched INO term with ap-value < 0.05 after Benjamini-Hochberg (BH) multipletesting corrections. Here a significantly over-representedor under-represented term indicates that the term wassignificantly more or less frequently used in the vaccinecontext compared to the whole literature background. Inthe current study, a vaccine-associated gene-gene inter-action network was defined based on the gene-gene inter-actions obtained from the PubMed abstracts, includingthose retrieved by a PubMed search of vaccine and thoseidentified by VO-SciMiner using 186 specific vaccineterms extracted from the VO vaccine branch. These 186vaccine terms (e.g., tuberculosis vaccine BCG) are easilyidentified by natural language processing programs. Thisvaccine-associated network was compared against thecomplete gene-gene interaction network.Table 1 The 2x2 contingency table# of gene-gene Concept- WholeInteraction pairs specific sub-network NetworkWith the INO term 30  1 500Without the INO term 150 30000Note: The sub-network has 30 gene pairs associated with this INO term outof a total of 180 gene pairs. A modified Fishers exact test, with the - 1modification made to the typical Fishers exact test to make the statistical testmore conservative, was employed to identify significantly over-representedterms (p-value of 6.9E-20).ResultsThe Interaction Network Ontology (INO)(1) INO overall design and hierarchyINO is a biomedical ontology in the domain of molecu-lar interactions and interaction networks. INO is alignedwith the upper-level Basic Formal Ontology (BFO) [17](Figure 1). BFO contains two branches, continuant andoccurrent. The continuant branch represents time-independent entities such as material entity, and theoccurrent branch represents time-related entities suchas process. BFO has currently been used by over 100domain ontologies, including many (e.g., GO) withinthe framework of the OBO Foundry [8]. By aligningdifferent domain ontologies under the two branches ofBFO, INO is able to efficiently use the terms fromother ontologies in representing signaling pathwayelements.Three important INO terms are interaction, network,and pathway. In INO, an interaction is defined as a pro-cessual entity that has two or more participants (i.e.,interactors) that have an effect upon one another undera particular condition. An interactor (or called interact-ant) is defined as a material entity that plays the role ofinteractor role. With different roles, an interactor can bean input interactor, output interactor, catalyst, positiveregulator, or negative regulator. An interaction con-sumes its input interactors (but not the catalysts or regula-tors) and generates its output interactors. A network is aprocess that includes at least two connected interactions.A network does not have to include a predefined start orend entity. A pathway is a type of network that has speci-fied distinct start(s) and end(s). Each of these three INOterms includes many subclasses. Therefore, in addition tothe representation of various interaction types, INO hasalso been developed to represent pathways and networks.Furthermore, INO has been used as a species-neutralontology core and platform for generating human-specificinteraction network ontology (HINO) [18,19]. Since thescope of this manuscript is the ontology-based literaturemining of gene-gene interactions, we will primarily focuson the ontological representation of interactions in INO.INO imports terms from other ontologies, particularlyfrom the Proteomics Standard Initiative-Molecular Inter-action (PSI-MI), which is a standard molecular interactiondata exchange format established by the Human ProteomeOrganization (HUPO) Proteomics Standard Initiative(http://www.psidev.info). Their PSI-MI format has beenwidely used in the proteomics community and PSI-MI isalso an OBO Foundry library ontology. To be compatiblewith PSI-MI, we have imported the branch of the inter-action type (MI_0190) to INO (Figures 1 and 2).Compared to PSI-MI, GO Biological Processes (BP)branch often has more detailed subclasses (or subtypes)to specific interaction types. Using more general PSI-MIinteraction (INO)process (BFO)materialentity (BFO)regulation(INO)entity (BFO)occurrent (BFO)continuant (BFO)organism(OBI)association (MI)gene(OGG)dependentcontinuant (BFO)realizable entity(BFO)role (BFO)interactorrole (INO)independentcontinuant (BFO)direct interaction(MI)enzymatic reaction(MI)physicalassociation (MI)interactor(INO)inputinteractorrole (INO)inputinteractor(INO)has rolehas rolepositiveregulation(INO)negativeregulation(INO)phosphorylation(GO)Figure 1 INO hierarchy and selected INO key terms. INO is aligned with BFO. It imports most PSI-MI interaction type terms to representthe various interaction types. Some bottom level interaction terms (e.g., phosphorylation) are replaced with corresponding GO terms. ManyINO-specific terms (e.g., regulation) that do not exist in PSI_MI or GO are also generated. Note that there are different interactors but only inputinteractor is shown here. The network and pathway related terms are not shown.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 4 of 10http://www.jbiomedsem.com/content/6/1/2terms (e.g., PSI-MI lipid addition) as parent terms, INOhas imported many specific GO subtypes of interactions(e.g., GO protein myristoylation) to INO as subclassesof the MI-based interaction terms (Figure 1). As a spe-cific example, we have imported GO protein myristoyla-tion and all of its GO subclasses to INO (Figure 2). TheGO term protein myristoylation has been used to re-place the PSI-MI term myristoylation reaction. It is notedthat the top level GO Biological Processes hierarchy is notused because many biological processes (e.g., metabolicprocess) in GO are not interaction per se and thus can-not be imported to INO for interaction representation.While PSI-MI focuses on direct protein-protein inter-actions, it does not include many other interaction typessuch as regulation types. Therefore, INO also includesinteraction terms that are out of current PSI-MI scope,especially different regulation types (Figure 1). Many ofthese interaction types were generated by classifying theover 800 interaction keywords used in our previous lit-erature mining studies [1,6].(2) Literature mining support in INOThe over 800 interaction keywords used in our previousliterature mining studies [1,6] do not correspond to thesame number of interaction types. While an interactiontype or term in INO has its ontology ID, such a termmay be associated with different synonyms or relatedkeywords that can be used for literature mining. To sup-port identification of genetic interactions in literature,synonyms and related keywords are needed. To meetthis need, we have generated an annotation propertycalled has literature mining keywords (Figure 2), whichallows the listing of different keywords mapping to theinteraction type.For example, the term protein myristoylation in INOhas five related literature mining terms including myris-toylate, myristoylates, myristoylated, myristoylating,and myristoylation. These term variations are listed asan annotation of the interaction type using the annota-tion property has literature mining keywords (Figure 2).The list of keywords can be easily extracted from theontology by SPARQL or other methods and used for lit-erature mining.(3) Statistics of INO terms and interaction keywordsAs of October 2014, INO contains 540 terms, including123 new INO terms and 317 terms imported from 11existing ontologies. In addition to the aforementionedFigure 2 The visualization of one term protein myristoylation (GO_0018377) in INO. Originated from GO, this term and its branch of childterms are imported and placed with the framework of PSI-MI interaction types which are also imported into INO. The upper level terms are fromBFO. The OntoFox tool [9] was used for importing external ontology terms and their axioms. The image is a screenshot generated from Ontobee[10]. To facilitate literature mining tagging, different synonyms of the term are collected under an annotation note.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 5 of 10http://www.jbiomedsem.com/content/6/1/2ontologies, INO also has imported terms from otherauthoritative domain ontologies such as the ChemicalEntities of Biological Interest (ChEBI) [20] and theOntology of Genes and Genomes (OGG) [21]. Proven-ance and source ontology IDs are kept in our termimporting [9]. The detailed INO term statistics can befound on the Ontobee INO statistics website (http://www.ontobee.org/ontostat.php?ontology=INO).Particularly, under the branch of INO interaction, INOincludes a total of 355 terms. In addition, approximately700 keywords are defined using the annotation propertyhas literature mining keywords. These INO interactionterms and their associated literature mining keywords canbe used for efficient literature text tagging and retrieval ofsentences containing these keywords. The usage of theseterms and keywords in our literature mining study is de-scribed below.INO-based literature mining of gene-gene interactions(1) Workflow and system designThe workflow of the ontology-based gene pair enrich-ment analysis is illustrated in Figure 3. Specifically, allpublications from PubMed were first downloaded. Thesentences of article titles and abstracts were parsed andpre-processed. Human gene names and interactionkeywords were tagged. To tag human gene names, theHUGO human gene nomenclature assignments (http://www.genenames.org/) were used. These human genenames are also available in the OGG [21]. The INOinteraction types and associated keywords were usedfor tagging interaction keywords. As detailed in theMethods section, an INO-based modified Fishers exacttest was developed to identify statistically significantlyenriched gene-gene interaction types and associatedgene-gene pairs (Figure 3).PubMed LiteratureSentence preprocessing(titles, abstracts)Literature mined sentencescontaining two genes andinteraction keywordsTerms of a domain ontology(e.g., VO)Modified Fishers exact testEnrichment of gene-gene interactions andassociated interaction typesHUGO human gene names;INO ontology collections andhierarchy of interaction wordsFigure 3 The workflow of INO-based gene-gene interactionenrichment analysis. This workflow illustrates the overall proceduresof ontology-based gene pair enrichment analysis.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 6 of 10http://www.jbiomedsem.com/content/6/1/2The INO-based workflow for literature mining ofgene-gene interactions is applicable for different use casestudies. Below we introduce the application of such astrategy for studying the gene-gene interactions in thevaccine domain.(2) INO-based literature enrichment analysis of vaccine-associated gene-gene interaction dataOur literature mining analysis used all PubMed docu-ments published as of 12/31/2013. A total of 23,481,042PubMed documents were used as the backgrounddata set in the analysis. Using this data set, SciMineridentified 314,152 gene pairs, each of which was associ-ated with at least one INO term.Table 2 Significantly over-represented INO terms among thesub-networkINO_ID Reference term EnrichmentINO_0000140 Neutralization 6.6INO_0000096 induction of production 6.2INO_0000106 gene fusion 5.6INO_0000103 accessory regulation 3.9INO_0000062 Costimulation 3.7INO_0000169 Synergization 3.0INO_0000089 co-regulation 2.9MI_0559 glycosylation reaction 2.9MI_0195 covalent binding 2.5MI_0208 genetic interaction 4.9MI_0571 mRNA cleavage 23.2MI_0902 RNA cleavage 16.2MI_0910 nucleic acid cleavage 6.4GO_0018377 protein myristoylation 2.3*BH: Benjamini-Hochberg; **IFNG_IL12A (5): represents the IFNG and IL12A gene paWe applied our study to the vaccine domain. APubMed search for vaccine-related documents resultedin 237,061 hits (as of 12/31/2013). VO-SciMiner add-itionally identified 28,908 documents using VO terms,resulting in a total of 265,969 documents to define thevaccine-associated document sets. The gene-gene inter-actions (i.e., gene pairs) with positive SVM scores and atleast one INO term at the same sentence level werecompiled from these 265,969 PubMed abstracts. A totalof 6,116 gene pairs were associated with at least oneINO term.Out of 78 INO interaction terms associated with atleast five gene-pairs of the vaccine-associated sub-network, 14 terms were significantly over-represented(Benjamini-Hochberg (BH) p-value < 0.05 and a minimalenrichment fold of 2) (Table 2). The results indicate thatthese 14 interaction types are more extensively studiedin the vaccine context among the research of all thegene-gene interaction types published in PubMed.Furthermore, our gene-gene interaction enrichmentanalysis was able to retrieve all the gene pairs associatedwith each interaction type (last column in Table 2). Forexample, as indicated in five publications (PubMed IDs:19915058, 8557339, 15557182, 17517055, and 7525727),the cytokines interferon-gamma (IFNG) and interleukin-12A (IL12A) have been found to be closely related,and the neutralization of one cytokine often leads todecreased production of another one [22,23]. Suchneutralization-related research is typically found inthe field of vaccinology. In another example, associatedwith the interaction type induction of production, theproduction of one cytokine, TNF (or IFNG), was found tobe induced by another cytokine, IFNG (or TNF) [24]. Agene-gene interaction pairs of vaccine-associatedfold BH *P-value Most frequent gene-pair (#)0 IFNG_IL12A (5)**0 TNF_IFNG (2)0 CD40LG_CD40 (3)0 CD8A_CD4 (55)0 CD40_CD8A (4)0 CD8A_CD40 (5)0 CD8A_CD40 (5)0 IL17A_MUC6 (1)0 CSF2_ACPP (2)1.82E-10 CD40LG_CD40 (3)2.58E-07 CFI_SUPT5H (1)2.21E-06 CFI_SUPT5H (1)6.11E-04 CFI_SUPT5H (1)2.68E-03 CD4_S100B (2)ir with the neutralization interaction keyword in five papers.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 7 of 10http://www.jbiomedsem.com/content/6/1/2close examination of all the gene pairs recorded in Table 2shows that they are all related to the vaccine and immun-ology research. These results also confirm the specificityof our INO-based enrichment analysis.In addition, our study found 17 significantly under-represented INO terms with a maximum enrichmentfold of 0.5 (equivalent to 2 fold in over-representation)and BH P-value < 0.05 (Table 3). Compared to the gen-eral gene-gene interaction research, these interactiontypes are likely less studied in the vaccinology researchfield. The reasons of these under-represented interactiontypes may vary. It is likely that some of these under-represented interactions represent new research oppor-tunities in the vaccinology domain.One advantage of INO-based study is that we can relyon the INO hierarchy to identify the relations amongenriched interaction types. Such a strategy is used togenerate the hierarchies of enriched 14 over-representedand 17 under-represented INO interaction types (Figure 4).This study clearly shows the relations between many dif-ferent interaction terms. For example, among the threeover-represented terms, mRNA cleavage, RNA cleavage,and nucleic acid cleavage, there are two parentchild re-lations as clearly shown in Figure 4. Interestingly, the termcleavage reaction is one of the 17 under-representedterms (Table 3). It is noted that the more general termcleavage reaction is the parent term of nucleic acid cleav-age, which is the parent term of RNA cleavage (Figure 4).The term RNA cleavage has a child term mRNATable 3 Significantly under-represented INO terms among thsub-networkINO_ID Reference termMI_0203 dephosphorylation reactionINO_0000178 tyrosine-phosphorylationINO_0000044 gene expression regulationINO_0000172 transactivationINO_0000060 coprecipitationGO_0016310 phosphorylationMI_0403 colocalizationMI_0414 enzymatic reactionMI_0194 cleavage reactionMI_0213 methylation reactionINO_0000092 dissociationINO_0000048 coimmunoprecipitationINO_0000115 hyperphosphorylationINO_0000084 destabilizationGO_0006461 protein complex assemblyINO_0000088 protein dimerizationINO_0000171 Termination*BH: Benjamini-Hochberg.cleavage. Besides these cleavage types, there are manyother specific cleavage reaction types, for example,protein cleavage, DNA cleavage, and lipid cleavage. Inour calculation of the parent term cleavage reaction,we included all its child terms. Therefore, the under-represented cleavage reaction indicates that the wholecategory of cleavage reaction is under-represented al-though the above three specific reaction types are over-represented.Both sets of over-represented and under-representedinteraction terms share some common top-level termsincluding regulation, direct interaction, association,and interaction. Otherwise, specific profiles of thetwo sets are in general distinct at the bottom levels(Figure 4).DiscussionThis paper introduces two major contributions in thearea of ontology-based literature mining research. First,we have for the first time systematically introduced thedevelopment of the INO ontology targeting for robustliterature mining of gene-gene interaction types. It isnoted that in addition to literature mining, INO is alsobeing developed to model various interactions and net-works among different molecules [18]. However, theINO development was initiated from meeting our lite-rature mining need [6]. Second, we have proposed andimplemented a novel INO-based gene-gene interactionenrichment strategy. The INO-based gene pair enrichmente gene-gene interaction pairs of vaccine-associatedEnrichment fold BH* P-value0.06 00.09 00.26 00.26 00.28 00.36 00.36 00.42 00.49 00.37 6.84E-160.28 6.27E-150.35 1.00E-130.27 2.54E-080.28 1.49E-050.24 1.97E-050.26 6.41E-050.42 3.98E-03Figure 4 The hierarchies of over- and under-represented INO interaction terms. (A) The hierarchy of 14 over-represented INO interactionterms. (B) The hierarchy of 17 under-represented INO interaction terms. The results were generated using OntoFox [9] with the OntoFox settingincludeComputedIntermediates, and visualized using the Protege-OWL editor (http://protege.stanford.edu/). The box-enclosed terms are over- orunder-represented interaction types directly identified in our program (see Tables 2 and 3). Other terms not enclosed in boxes are terms retrievedby OntoFox to ensure the completeness of the hierarchies.Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 8 of 10http://www.jbiomedsem.com/content/6/1/2analysis is novel in that the input of such analysis is theliterature mined gene-gene interaction types and genepairs. It differs from a typical GO enrichment analysiswhere a list of genes is the input. Such a strategy wasfurther used to study the enriched gene-gene interactiontypes and gene pairs in the domain of vaccinology. Ourresults demonstrate that the INO offers a repository ofhierarchical interaction keywords and a semantic plat-form for allowing systematical retrieval of interactiontypes from the literature. The INO-based gene-geneinteraction enrichment method further provides a strategyfor analyzing the retrieved gene-gene interaction literaturemining results.The coverage of the terms in INO for interaction key-words in literature is wide and includes three sources:(1) The Molecular Interactions (MI) ontology: INO hasimported all the interaction-related terms in MI; (2) TheGene Ontology (GO): Many interaction-related GOterms have been imported to INO and aligned with theMI terms; and (3) Newly generated interaction terms inINO: These new interaction-related terms are not avail-able in MI or GO, and thus we generated them in INOwith the INO_ prefix. Furthermore, INO has includedmany keywords that can be used for literature mining.These literature mining-related keywords are often varia-tions and synonyms of the ontology term labels. The in-clusion of these keywords significantly increases ourcoverage in literature mining. To better understand theinteraction term coverage of INO, we have comparedthe INO system with the commonly used GENIA ter-minology system [7]. The GENIA term annotation sys-tem is grounded on the GENIA ontology that definesbiomedically meaningful nominal concepts. Our com-parison found that INO covers all 17 interaction types inthe GENIA ontology.To further examine the interaction term coverage ofINO, we have also compared our system with the inter-action terminology collection from the BioNLP SharedTask 2009, focusing on recognition of bio-molecularevents reported in the biomedical literature (http://www.nactem.ac.uk/tsujii/GENIA/SharedTask/). Nine categoriesof bio-events were covered: gene expression, transcription,protein catabolism, localization, binding, phosphorylation,regulation, positive regulation, and negative regulation[25]. We used the BioNLP09 Shared Task training dataset that consists of 800 abstracts manually labeled for bio-molecular events including the event trigger words (i.e.,interaction keywords). These abstracts include 994 uniqueinteraction keywords that are shown for 6,607 times in thedata set. Our comparative analysis found that INO in-cludes 279 of these 994 unique interaction keywords.These 279 keywords are used for 4,448 times, whichcorresponds to 67% of coverage if the keyword redun-dancy is considered. It is noted that many keywords(e.g., by, when, source, products, necessary, through)listed in the BioNLP09 Shared Task training data arenot considered as interaction keywords in INO. Wewill fully examine all the terms in the BioNLP09Shared Task data set and hopefully expand INO to in-clude more interaction keywords.Our INO-based literature mining study found thatwhile it is relatively easy to describe the relation betweentwo genes when only one interaction keyword exists inthe sentence containing these two genes, it is difficult toHur et al. Journal of Biomedical Semantics 2015, 6:2 Page 9 of 10http://www.jbiomedsem.com/content/6/1/2describe the relation between the two genes if multiplekeywords exist. For example, in the IFNG-IL12Aneutralization-related interaction type (Table 2), we caninfer that these two genes participate in a neutralization-related interaction(s). However, it does not mean thatIFNG neutralizes IL12A, or vice versa. We can only saythat these two genes interact somehow in a neutralization-related pattern.It is likely that multiple interaction-related keywordsco-exist in one sentence. For example, an IFNG-IL12neutralization-related sentence is In vitro IL-12neutralization dramatically impaired the IFN-gammaresponse to S. typhimurium but not to ConA [26].This sentence contains two interaction-related key-words neutralization and impaired. This is a complexrelation where a neutralization of one gene impairs an-other gene expression. It hints that one gene positivelyregulates another. In this case, the neutralization is reallyan experimental condition. Our literature mining programretrieved both keywords independently without consider-ing them together. Specifically, our current method identi-fies all the interaction keywords and maps each of themto corresponding INO interaction terms. However, wehave not systematically modeled and integrated these co-existing terms into better understanding of the patterns ofcorresponding literature text. It would be more advancedif we could process these two keywords simultaneouslyand assign a unique interaction type, such as impairmentafter neutralization, which would be a subclass (or childterm) of the existing INO term positive regulation.While this example demonstrates a new direction offuture research, such analysis does not undermine thecontributions of the new INO-based literature miningstrategy first reported in this manuscript. Indeed, ourstrategy provides a new start point and platform forfurther addressing these challenges.The analysis of vaccine-associated interaction net-works requires intensive research. The research reportedhere uses INO-based literature mining to analyze thevaccine-relevant gene-gene interactions. More researchcan be conducted to study vaccine-gene interactions andvaccine-associated adverse events. In addition to thePubMed literature resource used in this study, additionalpublic resources such as Semantic MEDLINE, summar-izing PubMed results into an interactive graph of seman-tic predications [27], and The Vaccine Adverse EventReporting System (VAERS; https://vaers.hhs.gov), col-lecting vaccine-associated adverse events following theadministrations with various licensed vaccines [28], mayfurther improve the INO-based analysis. While SemanticMEDLINE and VAERS have been used in other vaccine-related research [29,30], INO-based approaches areexpected to advance the research on the interactionnetworks among vaccines, genes, and adverse events.The integrative research combining INO and differentresources would further facilitate our understanding ofvaccine mechanisms and support public health.ConclusionsINO provides a novel approach in ontologically defininghierarchical interaction types and related interactionkeywords for literature mining. We have adopted amodified Fishers exact test for statistically analyzing theenriched interactions, in terms of INO. The input ofsuch a novel statistical test is the gene-gene interactionpairs together with corresponding INO interaction terms.Such a literature mining strategy was applied and evalu-ated in the mining of vaccine-associated gene-gene inter-actions. The results of our study demonstrate that theontology-based literature mining in combination with anINO-based statistical interaction enrichment test is able toefficiently mine and analyze different types of vaccine-associated gene-gene interactions and correspondinggene pairs.AbbreviationsINO: Interaction network ontology; PSI-MI: Proteomics standards initiative-molecular interaction; GO: Gene ontology; VO: Vaccine ontology; BFO: Basicformal ontology; SVM: Support vector machine; CC: Creative commons;NCBO: National center for biomedical ontology; BH: Benjamini-Hochberg;OGG: Ontology of genes and genomes.Competing interestsThe authors declare that they have no competing interests.Authors contributionsJH developed the INO-based gene interaction enrichment analysis test andgenerated data with the vaccine domain use case. AO developed theSVM-based literature mining pipeline. ZX generated the script to execute theliterature mining pipeline. YH developed the INO and was the primary writerof the manuscript. YH, JH, and AO all participated in the project design, resultinterpretation, and manuscript writing. All authors read and approved thefinal manuscript.AcknowledgementsWe thank Ms. Rebecca Racz for her valuable proofreading and comments.This research was supported by grant R01AI081062 from the US NIHNational Institute of Allergy and Infectious Diseases (to YH) and Marie CurieFP7-Reintegration-Grants within the 7th European Community FrameworkProgramme (to AO). JH was supported by the Juvenile Diabetes ResearchFoundation post-doctoral research fellowship. The article-processing chargefor this article was paid by a bridge fund to YH at the Unit for LaboratoryAnimal Medicine (ULAM) in the University of Michigan Medical School.Author details1Department of Neurology, University of Michigan, Ann Arbor MI 48109, USA.2Department of Computer Engineering, Bogazici University, 34342 Istanbul,Turkey. 3Unit for Laboratory Animal Medicine, University of Michigan, AnnArbor, MI 48109, USA. 4Department of Microbiology and Immunology,University of Michigan, Ann Arbor, MI 48109, USA. 5Center for ComputationalMedicine and Bioinformatics, University of Michigan, Ann Arbor MI 48109,USA. 6Comprehensive Cancer Center, University of Michigan, Ann Arbor MI48109, USA.Received: 15 November 2014 Accepted: 17 December 2014Published: 6 January 2015Hur et al. Journal of Biomedical Semantics 2015, 6:2 Page 10 of 10JOURNAL OFBIOMEDICAL SEMANTICSDuck et al. Journal of Biomedical Semantics  (2015) 6:29 DOI 10.1186/s13326-015-0026-0RESEARCH ARTICLE Open AccessAmbiguity and variability of database andsoftware names in bioinformaticsGeraint Duck1, Aleksandar Kovacevic2, David L. Robertson3, Robert Stevens1 and Goran Nenadic1,4*AbstractBackground: There are numerous options available to achieve various tasks in bioinformatics, but until recently,there were no tools that could systematically identify mentions of databases and tools within the literature. In thispaper we explore the variability and ambiguity of database and software name mentions and compare dictionaryand machine learning approaches to their identification.Results: Through the development and analysis of a corpus of 60 full-text documents manually annotated at themention level, we report high variability and ambiguity in database and software mentions. On a test set of 25full-text documents, a baseline dictionary look-up achieved an F-score of 46 %, highlighting not only variability andambiguity but also the extensive number of new resources introduced. A machine learning approach achieved anF-score of 63 % (with precision of 74 %) and 70 % (with precision of 83 %) for strict and lenient matching respectively.We characterise the issues with various mention types and propose potential ways of capturing additional databaseand software mentions in the literature.Conclusions: Our analyses show that identification of mentions of databases and tools is a challenging task thatcannot be achieved by relying on current manually-curated resource repositories. Although machine learning showsimprovement and promise (primarily in precision), more contextual information needs to be taken into account toachieve a good degree of accuracy.Keywords: Bioinformatics, Computational biology, CRF, Dictionary, Resource extraction, Text-miningBackgroundBioinformatics and computational biology rely on do-main databases and software to support data collection,aggregation and analysis and, as such, have been re-ported in research papers, typically as part of themethods section. However, limited progress has beenmade to systematically capture mentions of databasesand tools in order to explore the bioinformatics practiceof computational method on a large-scale. An evaluationof the resources available could help bioinformaticiansto identify common usage patterns [1] and potentiallyinfer scientific best practice [2] based on a measure ofhow often or where a particular resource is currently be-ing used within an in silico workflow [3]. Although there* Correspondence: g.nenadic@manchester.ac.uk1School of Computer Science, The University of Manchester, Oxford Road,Manchester M13 9PL, UK4Manchester Institute of Biotechnology, The University of Manchester, 131Princess Street, Manchester M1 7DN, UKFull list of author information is available at the end of the article© 2015 Duck et al. This is an Open Access arti(http://creativecommons.org/licenses/by/2.0),provided the original work is properly creditedcreativecommons.org/publicdomain/zero/1.0/are several inventories that list available database andsoftware resources (e.g., the NAR databases and web-services special issues [4, 5], ExPASy [6], the OnlineBioinformatics Resources Collection [7], etc.), until re-cently, to the best of our knowledge, there were no at-tempts to systematically identify resource mentions inthe literature [8].Biomedical text mining has seen wide usage in identi-fying mentions of entities of different types in the litera-ture in recent years. Named entity recognition (NER)enables automated literature insights [9] and providesinput to other text-mining applications. For example,within the fields of biology and bioinformatics, NERsystems have been developed to capture species [10],proteins/genes [1113], chemicals [14], etc. Issues ofnaming inconsistencies, numerous synonyms and acro-nyms, and an inability to distinguish entity names fromcommon words in a natural language combined withambiguous definitions of concepts, make NER a difficulttask [15, 16]. Still, for some applications, NER toolscle distributed under the terms of the Creative Commons Attribution Licensewhich permits unrestricted use, distribution, and reproduction in any medium,. The Creative Commons Public Domain Dedication waiver (http://) applies to the data made available in this article, unless otherwise statedDuck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 2 of 11achieve relatively high precision and recall scores. Forexample, LINNAEUS achieved F-scores around the 95 %mark for species name recognition and disambiguationon the mention and document levels [10]. On the otherhand, gene names are known for their ambiguity andvariability, resulting in lower reported F-scores. Forexample, ABNER [12] recorded an F-score of just under73 % for strict-match gene name recognition (85 % withsome boundary error toleration), and GNAT [13] re-ported an F-score of 81 % for the same task (up to amaximum of 90 % for single species gene name recogni-tion, e.g., for yeast).Some previous work exists on automated identificationand harvesting of bioinformatics database and softwarenames from the literature. For example, OReFiL [17] uti-lises the mentions of Unified Resource Locators (URLs)in text to recognise new resources to update its own in-ternal index. Similarly, BIRI (BioInformatics ResourceInventory) uses a series of hand crafted regular expres-sions to automatically capture resource names, theirfunctionality and classification from paper titles and ab-stracts [18]. The reported quality of the identificationprocess was in line with other NER tasks. For example,BIRI successfully extracted resource names in 94 % ofcases in a test corpus, which consisted of 392 abstractsthat matched a search for bioinformatics resource andeight documents that were manually included to test do-main robustness. However, both of these tools focusedon updates and have biased their evaluation to resourcerich text, which prevents full understanding of falsenegative errors in the general bioinformatics literature.This paper aims to analyse database and softwarename mentions in the bioinformatics/computationalbiology literature to assess challenges for automated ex-traction. We analyse database and software names in thecomputational biology literature using a set of 60 full-text documents manually annotated at the mention level,building on our previous work [19]. We analyse thevariability and ambiguity of bioinformatics resourcenames and compare dictionary and machine learningapproaches for their identification based on the resultson an additional dataset of 25 full-text documents. Al-though we focus here on bioinformatics resources, thechallenges and solutions encountered in database andsoftware recognition are generic, and thus not unique tothis domain [20].MethodsCorpus annotation and analysisFor the purpose of this study, we define databases asany electronic resource that stores records in a struc-tured form, and provides unique identifiers to each rec-ord. These include any database, ontology, repository orclassification resource, etc. Examples include SCOP (adatabase of protein structural classification) [21], Uni-Prot (a database of protein sequences and functional in-formation) [22], Gene Ontology (ontology that describesgene product attributes) [23], PubMed (a repository ofabstracts) [24], etc. We adopt Wikipedias definition ofsoftware [25]: a collection of computer programs  thatprovides the instructions for telling a computer what todo and how to do it. We use program and tool as syno-nyms for software. Examples include BLAST (automatedsequence comparison) [26], eUtils (access to literaturedata) [27], etc. We also include mentions of web-servicesas well as package names (e.g., R packages from Biocon-ductor [28, 29]). We explicitly exclude database recordnumbers/identifiers (e.g., GO:0002474, Q8HWB0), fileformats (e.g., PDF), programming languages and theirlibraries (e.g., Python, BioPython), operating systems(e.g., Linux), algorithms (e.g., Merge-Sort), methods(e.g., ANOVA, Random Forests) and approaches (e.g.,Machine Learning, Dynamic Programming).To explore the use of database and tool names, wehave developed an annotated set of 60 full-text articlesfrom the PubMed Central [30] open-access subset. Thearticles were randomly selected from Genome Biology (5articles), BMC Bioinformatics (36) and PLoS Computa-tional Biology (19). These journals were selected as theycould provide a broad overview of the bioinformaticsand computational biology domain(s).The articles were primarily annotated by a bioinforma-tician (GD) with experience in text mining. The annota-tion process included marking each database/softwarename mention. We note that associated designators ofresources (e.g., words such as database, software) wereincluded only if part of the official name (e.g., GeneOntology). The inter-annotator agreement (IAA) [31] forthe annotation of database and software names wascalculated from five full-text articles randomly selectedfrom the annotated corpus, which were annotated by aPhD student with bioinformatics and a text-miningbackground.To assess the complexity, composition, variability andambiguity of resource names, we performed an analysis ofthe annotated mentions. The corpus was pre-processedusing a typical text-mining pipeline consisting of a tokeni-ser, sentence splitter and part-of-speech (POS) tagger fromGATEs ANNIE [32]. We analysed the length of names,their lexical (stemmed token-level) and structural com-position (using POS tag patterns) and the level of variabil-ity and ambiguity as compared to common English words,acronyms and abbreviations.In addition to the dataset of 60 articles that was usedfor analysis and development of NER tools, an additionaldataset of 25 full-text annotated papers was created toassess the quality of the proposed NER approaches (seebelow).Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 3 of 11Dictionary-based approach (baseline)We compiled an extensive dictionary of database andsoftware names from several existing sources (seeTable 1). Some well-known acronyms and spelling/orthographic variants have also been added, resultingin 7322 entries with 8169 variants (6929 after removingrepeats) for 6126 resources. The names collected in thedictionary were also analysed using a similar approachas used for the names appearing in the corpus (seeabove). We then used LINNAEUS [10] to match thesenames in text.Machine learning approachGiven the availability of the manually annotated corpus,a machine learning (ML) approach was explored foridentification of resource names. We approached thetask as a sequence-tagging problem as often adopted inNER systems. We opted for Conditional Random Fields(CRF) [33] and used features at the token-level that com-prised the tokens own characteristics and the features ofthe neighbouring tokens. We used the Beginning-Inside-Outside (B-I-O) annotation.The following features were engineered for each token:1. Orthographic features captured the orthographicpatterns associated with biomedical resourcesmentions. For example, a large percentage ofmentions are acronyms (e.g., GO, SCOP), capitalisedterms (e.g., Gene Ontology, Bioconductor) or wordsthat contain a combination of capital and lower capletters (e.g., MySQL, UniProt) etc. We engineeredtwo groups of orthographic features [34]. The firstgroup comprised shape (pattern) features thatmapped a given token to an abstract representation.Each capital letter is replaced with X, lower caseTable 1 Sources from which the database and software name dictioType Entries VariantsDB 195 298SW 263 278PK 799 799SW 2033 2087SW 389 391DB 379 379DB 1452 1670SW 135 135SW 36 41SW 1149 1183SW, DB 171 231Our dictionary (DB, SW, PK) 7322 6929Note that entries and variants are not necessarily unique to a single resource listDB databases, SW software, PK packages; data correct and accessible as of Februaryletter with x, a digit with d and any othercharacter with S. Two features were created in thisgroup: the first feature contained a mapping for eachcharacter in a token (e.g., MySQL was mapped toXxXXX); the second feature mapped a token to afour character string that contained indicators of apresence of a capital letter, a lower letter, a digit orany other character (absence was mapped to a _),e.g., MySQL was mapped to Xx_ _. The features inthe second group captured specific orthographiccharacteristics (e.g., is the token capitalised, does itconsist of only capital letters, does it contain digits,etc.  see Table 2 for the full list), which wereextracted by a set of regular expressions.2. Dictionary features were represented by a singlebinary feature that indicated if the given token wascontained within our biomedical resourcesdictionary.3. Lexical features included the token itself, its lemmaand part-of-speech (POS) tag.4. Syntactic features were extracted from syntacticrelations in which the phrase was a governor or adependant, as returned by the Stanford parser [35,36]; in cases where there were several relations, therelation types were alphabetically sorted andconcatenated (e.g., pobj and advmod werecombined as advmod_pobj).The experiments on the training data revealed thattwo tokens before and one token after the current tokenprovided the best performance. The CRF model wastrained using CRF++ [37]. All pre-processing needed forfeature extraction was provided by the same text-miningpipeline as used for the corpus analysis and dictionary-based approach.nary is comprisedSourcedatabases.biomedcentral.comwww.bioinformatik.dewww.bioconductor.orgbioinformatics.ca/links_directory/evolution.genetics.washington.edu/phylip/software.htmlwww.ebi.ac.uk/miriam/main/www.oxfordjournals.org/nar/database/a/www.netsci.org/Resources/Software/Bioinform/index.htmlwww.bioinf.manchester.ac.uk/recombination/programs.shtmlen.wikipedia.org/wiki/Wiki/<various>Manually added entrieshttp://sourceforge.net/projects/bionerds/28th, 2012Table 2 Token-specific orthographic features extracted byregular expressionsName DescriptionisAcronym token is an acronymcontainsAllCaps all the letters in the token are capitalisedisCapitalised token is capitalisedcontainsCapLetter token contains at least one capital lettercontainsDigits token contains at least one digitisAllDigits token is made up of digits onlyTable 3 Statistics describing the manually annotated corporaDevelopment TestTotal number of documents 60 25Total database and software mentions 2416 1479Total unique resource mentions 401 301Percentage of database mentions 36 % 28 %Percentage of unique database mentions 27 % 30 %Average mentions per document 40.3 70.0Average unique mentions per document 8.1 13.4Maximum mentions in a single document 227 217Maximum unique mentions in a singledocument57 55Resources with only a single lexicographicmention201 147Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 4 of 11Machine learning  post-processingAn analysis of the initial CRF results on the develop-ment dataset revealed that a large portion of false nega-tives were from resource mentions that were recognisedby the model at least once in a document, but missedelsewhere within the same document. We have thereforedesigned a two-pass post-processing approach. The firstpass collected and stored all the CRF tagging results.These were then used to re-label the tokens in the sec-ond pass. In order to avoid over-generation of labels(i.e., possible false positives), we created a set of condi-tions that each token had to meet if it was to be re-labelled as a resource mention. First, it had to be labelledas a (part of a) resource name in the first pass moreoften than it was not, looking at the entire corpus thatwas being tagged. If that was the case, the candidatetoken also had to fulfil one of the following two condi-tions: either it was contained within the biomedical re-sources dictionary; or it was an acronym that had nodigits and was at least two characters long. Finally, thefollowing four tokens: analysis, genomes, cycle andcell were never labelled as part of resource name inthe second round, as they were found to be the sourceof a large percentage of false positives.EvaluationStandard text-mining performance statistics (precision,recall, F-score) were used for evaluation. In particular,we make use of 5-fold cross-validation across all 60 full-text articles for both the dictionary and machine learn-ing approaches. For a fair comparison, the dictionary-based approach is only evaluated on the test set in eachfold, as it requires no prior training. We also test bothapproaches directly on the test set of 25 articles withoutadditional training/adjustments.Results and discussionCorpus annotationsTable 3 gives an overview of the two corpora annotatedwith resource mentions. We note that the IAA was rea-sonably high: with lenient agreement (annotation offsetsoverlap), an F-score of 86 % was calculated (93 % preci-sion, 80 % recall). As expected, a decrease in IAA isobserved if strict agreement (offsets must exactly match)is used instead (every score drops by 6 %).In the development corpus, there were 401 lexicallyunique resources mentioned 2416 times (6 mentions onaverage per unique resource name), with an average of40 resource mentions per document. The documentwith the most mentions had 227 resource mentionswithin it. Finally, 50 % of resource names were onlymentioned once in the corpus. A similar profile wasnoted for the test corpus, although it contained notablymore resource mentions per document.Database and software name compositionWe first analysed the composition of resource namesboth in the development corpus and dictionary. The lon-gest database/software name in the annotated corpuscontained ten tokens (i.e., Search Tool for the Retrievalof Interacting Genes/Proteins). However, there are longerexamples in the dictionary (e.g., Prediction of ProteinSorting Signals and Localisation Sites in Amino AcidSequences).To assess the composition of resource names withinour dictionary, we stemmed each token within eachname (using the Porter Stemming Algorithm [38]) andcounted the occurrences of each stemmed token.Figure 1 displays the most frequent words: the two mostones are database and ontology. A comparable lexicaldistribution can be noted in the development set, withdatabase, gene, analysis, tool, genome, ontology featuringas the most frequent ones (data not shown). This sug-gests that some common head terms and some othercommon bioinformatics relevant terms could aid recog-nition. We also note that there is a long tailed curve in-volved in the lexical decomposition of resource words.As an initial structural analysis, we automatically col-lected all the POS tags assigned to each unique databaseand software name in the development corpus. TheseFig. 1 Top token frequencies within the manually compiled dictionary. The figure shows the most common stemmed tokens contained withinall the resource names found within our manually compiled dictionary. The top token is database with a count of 474, followed by ontology with187 instances. Note that the scale is logarithmic (log base 2), and the y-axis crosses at eight rather than zero (for aesthetic reasons). The top termsare labelledDuck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 5 of 11were then grouped to profile the structure of resourcenames (see Table 4). We have identified a total of 405patterns. The majority (79 %) of database and softwarenames comprise one, two or three proper nouns. Anadditional 5 % were tagged a single common noun (e.g.,affy). A roughly equivalent number of names containdigits (e.g., S4, t2prhd). Nine patterns contain adjectives(e.g., internal transcribed spacer 2) or prepositions/sub-ordinating conjunctions (e.g., Structural ClassificationOf Proteins). Finally, in two cases (SHAKE and dot), amention of software was tagged as a verb form. We notethat there are more patterns (405) than unique mentions(401) because sometimes an equal resource name getstagged with differing patterns (e.g., R received bothNNP and NN POS tags). The analysis shows that thereis some variety in resource naming, and  as expected that recognition of simple noun phrases alone is notTable 4 Internal POS structure of database and software names(the development corpus)Pattern Count FrequencyNNP 258 63.7 %NNP NNP 34 8.4 %NNP NNP NNP 26 6.4 %NN 20 4.9 %NNP CD 16 4.0 %NNP NNP NNP NNP 8 2.0 %Other Patterns 43 10.6 %NNP proper noun, NN singular noun, CD cardinal numbersufficient for identification of potential resource men-tions. In particular, around 5 % of noun-phrases (asextracted with the Stanford Parser) within the corpuscontain at least one resource mention.Variability of resource namesTo evaluate the variability of resource names within ourdictionary, we calculated the average number of namevariants for a given resource. As such, the variability ofresource names at the dictionary level is 1.13 (6929unique variants over 6126 resources, after adjustmentfor repeats). For the corpus analysis, we manuallygrouped the names from the set of manually annotatedmentions that were referring to the same resource inorder to analyse name variability. Specifically, we groupedvariants based on spelling errors and orthographic dif-ferences, and then grouped long and short form acro-nym pairs based on our own background knowledge,and the text from which they were initially extracted.Of the 401 lexically unique names, 97 were variants ofother names, leaving 304 unique resources. In total,231 resources had only a single name variant withinthe corpus (76 %); 18 % of resources had two variants,and the final 6 % had between three and five variants.Of the 97 name variants, 36 were acronyms and mostof those were defined in text (and so could perhaps beautomatically expanded with available tools, e.g., [39]).However, there were other cases where a resourcesacronym was used without the expanded form for def-inition (e.g., BLAST).Table 6 Dictionary matching results on the developmentDuck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 6 of 11Ambiguity of resource namesAs expected, a number of ambiguous resource namesexist within the bioinformatics domain. Interesting ex-amples include Network [40] (a tool enabling networkinference from various biological datasets) and analysis[41] (a package for DNA sequence analysis). We there-fore analysed our dictionary of database and softwarenames to evaluate dictionary-level ambiguity when com-pared to the entries in a full English words dictionaryderived from a publicly available list [42] (hereafter re-ferred to as the English dictionary) and to a knownbiomedical acronyms dictionary compiled from ADAM[43] (hereafter referred to as the acronym dictionary),consisting of 86,308 and 1933 terms, respectively. A totalof 52 names matched English words (e.g., analysis, cycle,graph) and 77 names fully matched known acronyms(e.g., DIP, distal interphalangeal and Database of Inter-acting Proteins) when using case-sensitive matching. Thenumber of matches increases to 534 to the English dic-tionary and to 96 for the acronym dictionary when case-insensitive matching is used instead.To evaluate the recognition-level ambiguity within theannotated corpus, we also compared the annotated data-base and software names to the English dictionary andacronym dictionary. This resulted in four matches to theEnglish dictionary (ACT, blast, dot, R), and six to theacronym dictionary (BBB, CMAP, DIP, IPA, MAS, VOCs)using case-sensitive matching. This equates to roughly3 % of the unique annotated names. The total increasesto 53 matches (17 %) if case-insensitive matching is usedinstead.Dictionary-based matchingTable 5 provides the standard text-mining performancestatistics for the dictionary matching approach. Theaverage lenient F-scores between 43 and 46 % highlightthe challenges for this approach, both in terms of match-ing known ambiguous names (low precision), and fromthe dictionary not being sufficiently comprehensive (lowTable 5 Evaluation results on the development and testcorporaDevelopment corpus Recall (%) Precision (%) F-score (%)Dictionary 49 (47) 38 (37) 43 (41)CRF with post-processing 58 (52) 76 (67) 65 (58)CRF without post-processing 54 (49) 78 (70) 62 (57)Test CorpusDictionary 46 (44) 46 (44) 46 (44)CRF with post-processing 60 (54) 83 (74) 70 (63)CRF without post-processing 53 (45) 71 (65) 62 (53)Strict scores provided in bracketsP Precision, R Recall, F F-score evaluation on the development (5-cross validated)and test corporarecall). Some common false positives were cycle, genomes(potential mentions of Bioconductor packages) and GO(which was frequently matched within GO databaseidentifiers (e.g., GO:0007089) because of inappropriatetokenisation). Some common false negatives (i.e., missedresource mentions) included Tabasco (PMC2242808),MethMarker (PMC2784320), xPedPhase and i Linker(both from PMC2691739). In each of these examples, thename missed (numerous times) was the resource being in-troduced in that paper. This shows that any NER for data-base and software names must be able to capture newlyintroduced resources to achieve high recall.We note here the high variation in the different foldscores (e.g., see the results for Fold 3 in Table 6), indi-cate how challenging detection of resource names couldbe, depending on the particular document. We also notea difference between the results reported here (lenientF-score of 4346 %) and those we obtained previously[19] on a subset of 30 documents from the developmentset (lenient F-score of 54 %). The drop in performancecan be partially contributed to the changes to both thedataset (60 vs. 30 articles) and the underlying dictionar-ies (updated), as well as the change in the evaluation ap-proach (cross-fold vs. evaluating the entire dataset atonce; thus, a fold with an overrepresentation of falsenegatives cannot be balanced out by another fold withan overrepresentation of true positives (and the same forfalse negatives)).Machine-learning approachThe results of the application of the CRF model arepresented in Table 5. With post-processing, the averageF-scores of 6570 % for lenient and 5863 % for strictmatching present a considerable improvement over thedictionary-based approach, but still leaves the task onlymoderately solved. Table 7 shows the results of differentfolds for the development corpus. It is interesting thatcorpusFold Recall (%) Precision (%) F-score (%)1 46 (43) 41 (39) 43 (41)2 34 (31) 37 (34) 36 (32)3 36 (34) 24 (23) 29 (27)4 55 (53) 46 (45) 50 (49)5 76 (75) 44 (43) 56 (55)Min 34 (31) 24 (23) 29 (27)Max 76 (75) 46 (45) 56 (55)Mean 49 (47) 38 (37) 43 (41)Note that for Fold 3, a decrease in score (of about 8 % F-score) is observed ifthe LINNAEUS abbreviation detected is disabled. Strict scores providedin bracketsP Precision, R Recall, F F-score on the development set using dictionarylook-upTable 7 Machine learning results with post-processing on thedevelopment corpusFold Recall (%) Precision (%) F-score (%)1 51 (44) 71 (60) 59 (51)2 44 (35) 88 (71) 59 (47)3 51 (44) 76 (66) 61 (53)4 65 (60) 73 (67) 69 (63)5 80 (76) 74 (70) 77 (73)Min 44 (35) 71 (60) 59 (47)Max 80 (76) 88 (71) 77 (73)Mean 58 (52) 76 (67) 65 (58)Micro Avg 56 (50) 76 (67) 65 (57)Strict scores provided in bracketsP Precision, R Recall, F F-score on the development set using machine learningwith post-processing (5-cross fold)Duck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 7 of 11precision was relatively high (7683 %), while recall wasnotably lower (5860 %). These results lead us to believethat the current feature set is insufficient to capture lex-ical variability in sentences with biomedical resourcementions. The lenient matching scores were generallyhigher than the strict scores (7 % on F-score, 6 % on re-call and 9 % on precision), which indicates that bound-ary adjustment of the recognised tokens is a challengingtask, similar to other biomedical NER tasks.The application of the ML-model with post-processingshowed positive effects, as the results without post-processing had consistently lower recall (drop of 47 %for lenient and 39 % for strict matching). While the ef-fect on precision was not stable, the overall F-score hasstill increased (38 % for lenient and 110 % for strictmatching). Table 8 presents the details on the folds forthe development corpus. To further evaluate the loss inrecall when the post-processing step is omitted, we ana-lysed the full list of false negative mentions to extractwhat percentage of these were dictionary matches, buthad nevertheless been rejected by the ML approach. ItTable 8 Machine learning results without post-processing onthe development setFold Recall (%) Precision (%) F-score (%)1 46 (41) 78 (69) 58 (51)2 42 (35) 89 (75) 57 (48)3 45 (41) 75 (70) 56 (52)4 60 (55) 71 (66) 65 (60)5 76 (74) 74 (72) 75 (73)Min 42 (35) 71 (66) 56 (52)Max 76 (74) 89 (75) 75 (73)Mean 54 (49) 78 (70) 62 (57)Micro Avg 52 (47) 77 (70) 62 (56)P Precision, R Recall, F F-score on the development set using machine learningwithout post-processing (5-cross fold). Strict scores provided in bracketsturns out that this occurred in 158 (15 %) of the falsenegative mentions. While providing more training datacould help, this issue could perhaps be also addressed byusing additional features (for example, utilising some ofthe rules we suggest in the next section), or by combin-ing dictionary and ML-methods. We note, however, thatthe direct merge of the dictionary and ML results is in-sufficient due to the large number of false-positives thatdictionary matching introduces (see Table 9). Specific-ally, combining both results gives an average increase inrecall of 5 % (across all folds), but a large reduction inprecision, resulting in an average reduction in F-score of15 %.Feature impact analysis for the ML modelWe explored the impact that particular groups of fea-tures have on the recognition of biomedical resourcenames. During the 5-fold cross validation, each of thefeature groups was removed and the CRF models werethen trained and applied to the test fold enabling us toevaluate the contribution of each group. The CRFmodels were built without post-processing as we wantedto avoid the contributions being biased by that step (es-pecially because it uses the dictionary predictions). Theresults are presented in Table 10.Overall, the lexical features were beneficial: when thisgroup of features was removed, there was a drop of 8 %in precision, 6 % in recall, resulting in a 7 % lower F-score. The syntactic features had only a slight impact onthe performance: removing this group resulted in a 1 %drop in both precision and recall and a 2 % in F-score.The orthographic features had a similar effect as the lex-ical features: when these were removed, there was an8 % loss in precision, a 6 % loss in recall, resulting in a7 % loss in F-score. Surprisingly, removing the dictionaryfeatures did not result in a high decrease in performance(there was a drop of 8 % in precision, a 5 % drop in re-call and thus a 6 % drop in F-score), suggesting that theTable 9 Combined dictionary and machine learning results onthe development setFold Recall (%) Precision (%) F-score (%)1 56 (49) 43 (38) 49 (42)2 50 (41) 45 (37) 48 (39)3 57 (52) 32 (29) 41 (37)4 68 (64) 45 (42) 54 (51)5 87 (84) 45 (43) 59 (57)Min 50 (41) 32 (29) 41 (37)Max 87 (84) 45 (43) 59 (57)Mean 64 (58) 42 (38) 50 (45)P Precision, R Recall, F F-score on the development set combining thedictionary and machine learning annotations (5-cross fold). Strict scoresprovided in bracketsTable 10 Feature impact analysis of the machine learningmodel without post-processing on the development setFeature group Recall (%) Precision (%) F-score (%)All features 54 (49) 78 (70) 62 (57)No lexical features 46 (43) 68 (62) 54 (50)No syntactic features 53 (48) 77 (69) 61 (55)No orthographic features 48 (43) 70 (62) 55 (50)No dictionary features 49 (44) 70 (62) 57 (51)P Precision, R Recall, F F-score feature contribution results comparison. Strictscores provided in bracketsTable 12 Example clues and phrases appearing with specificheads or in Hearst patterns the stochastic simulator Dizzy allows The MethMarker software was  tools: CLUSTALW, , and MUSCLE. programs such as Simlink, , and SimPed.Database and software names are in italics, the associated clue is in boldDuck et al. Journal of Biomedical Semantics  (2015) 6:29 Page 8 of 11ML-model (without the aid of a dictionary), even withthe relatively limited amount of training data, managedto capture a significant number of resource mentions.Missed database and software mentionsWe further analysed the database and software namesnot picked up by our ML approach for any commontextual clues and patterns. Table 11 summarises differentclue categories and their potential relative contributionto the overall recall. Overall, using all clues that we haverecognised (see below), final recall could be as high as94 % (Table 11), though utilising all of these pointers willlikely have a detrimental effect on precision.The first type of clue that seemed most discriminatorywas to associate potential names with head terms, i.e.,terms that are explicit designators of the type of re-source. In the most basic case, a resource name couldinclude a head term or be immediately followed by one(see Table 12). Key head terms included database,software, tool, program, simulator, system, library andservice. Still, we note that not all potential clues are fullydiscriminatory. For example, we note that including sys-tem as a head clue might be problematic as the wordcan have other uses and meaning within biology (e.g.,biological systems). Similarly, although module could bea useful head for identification of software names,the mention of module(s) in P and D modulesTable 11 Types of textual patterns and clues for identificationof database and software namesType Contribution to total TPsMachine learning matches 55.3 %Heads and Hearst Patterns 9.8 %Title appearances 0.5 %JOURNAL OFBIOMEDICAL SEMANTICSAmith et al. Journal of Biomedical Semantics  (2015) 6:23 DOI 10.1186/s13326-015-0016-2RESEARCH ARTICLE Open AccessDeveloping VISO: Vaccine InformationStatement Ontology for patient educationMuhammad Amith1, Yang Gong1, Rachel Cunningham2, Julie Boom2 and Cui Tao1*AbstractObjective: To construct a comprehensive vaccine information ontology that can support personal healthinformation applications using patient-consumer lexicon, and lead to outcomes that can improve patient education.Methods: The authors composed the Vaccine Information Statement Ontology (VISO) using the web ontologylanguage (OWL). We started with 6 Vaccine Information Statement (VIS) documents collected from the Centers forDisease Control and Prevention (CDC) website. Important and relevant selections from the documents were recorded,and knowledge triples were derived. Based on the collection of knowledge triples, the meta-level formalization of thevaccine information domain was developed. Relevant instances and their relationships were created to representvaccine domain knowledgeResults: The initial iteration of the VISO was realized, based on the 6 Vaccine Information Statements and coded intoOWL2 with Protégé. The ontology consisted of 132 concepts (classes and subclasses) with 33 types of relationshipsbetween the concepts. The total number of instances from classes totaled at 460, along with 429 knowledge triples intotal. Semiotic-based metric scoring was applied to evaluate quality of the ontology.Keywords: Biomedical informatics, Vaccines, Vaccine Information Statements, Knowledge based systems, Ontology,Ontology constructionIntroductionIn the present information age, patients are affordedmanyoptions to educate themselves on vaccines. Some of theseoptions included valid and reputable websites, books, andother media sources while other options may appear rep-utable but are not. Regardless of the sources credibility,one researcher found that 70% of individuals who seekvaccine information online are influenced by what theyfind on the Internet [1]. Hopefully, patients and parentsare able to identify the most reliable resources such asthe Centers for Disease Control and Preventions (CDC)Vaccine Information Statements (VIS). VISs were devel-oped in response to the National Childhood VaccineInjury Act (NCVIA) which was passed in an effort tominimize provider liability and respond to public healthconcerns. The NCVIA requires healthcare providers toprovide a VIS to the person receiving the vaccine or*Correspondence: cui.tao@uth.tmc.edu1School of Biomedical Informatics, University of Texas Health Science Center,7000 Fannin St, 77030 Houston, TX, USAFull list of author information is available at the end of the articlehis/her guardian. The VIS must be given each time avaccine is administered and provides information on thebenefits and risks of the vaccine as well as other rele-vant disease information. They are usually provided to thepatient as a handout [2].Some researchers have identified specific issues withthe dissemination of vaccine information through thesedocuments. Both Lieu, et al., and St. Amour, et al. haverevealed that vaccine documentation handed to patientsare rarely read or fully understood by the patient [3,4].This kind of education is called passive education, becausethe patients may or may not choose to read the materials.In addition, while the purpose of the VIS is to inform par-ents and initiate potential questions, limited clinical stafftime and resources may hinder the optimal interactionsideal for vaccine education. This demonstrates a pervasiveissue with medical education for patients, where the deliv-ery and presentation should bemore consumer-friendly inorder to effectively impact patient education and increaseknowledge [5-7].© 2015 Amith et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 2 of 12Biomedical-related ontologies have had an impact onpatient learning and decision-making by utilizing patient-friendly terminology rather than confusing medical jar-gon. Ontologies are understood to be a consensus-basedcontrolled vocabulary between terms and relationships,while serving as a vehicle of interaction between humansand computers. In lay terms, it is similar to conceptmaps or graphs but with the capacity to link with othergraphs, and evoke reasoning and inferences. Some pub-lished examples and ideas include: ontology-driven clinical decision support systemsfor patients in regards to discharge medication [8], building medical ontology models for Italianpatients [9], an ontology-based coaching tool for physicians toprepare dialogue with patients [10], and dialogue systems for patient planning [11]Following in the examples described, we developed anontology-driven mobile application system to improvepatient learning and comprehension of vaccine knowl-edge [12]. While a proof of concept prototype, the sys-tem was limited by the test ontology which comprisedof 19 classes and 82 instances, a relatively small knowl-edge base with which to interact. To further extendthis project, as a first step, we aimed to create a com-prehensive vaccine ontology with which patients caninteract.The objective of this paper is to formalize the vac-cine knowledge for patient education using ontology toolsas a solution for vaccine education for the public. Wepropose modeling vaccine knowledge using OWL2 (WebOntology Language) [13], which will be encoded withProtégé [14]. We focus on the knowledge in the VISsdeveloped by the CDC. By publishing the Vaccine Infor-mation Statement Ontology (VISO) in a domain ontologyand offering the benefit of a serialized format that canbe processed by a machine and reused [15], we intro-duce opportunities to develop ontology-driven personalhealth agents to improve patient learning and compre-hension of vaccine knowledge. Overall, the overarchinggoal is to develop a scalable ontological model that canreliably cover any new and applicable vaccine knowledgefor a consumer audience. With a scalable conceptual-level model, opportunities in natural language ontologylearning and population would be a future possibility toinvestigate for automated upkeep and maintenance of theontology.This paper will start by briefly discussing the sourcematerial, the VISs. Afterwards, the paper will segue wayinto describing the development of the class-level schemaand the instance level of VISO. This will include discus-sion of the initial common design patterns encounteredand basic ontology metrics. The Results section will elab-orate the quantitative and qualitative aspects of VISO, thatalso includes an initial evaluation scoring. The paper willthen close with challenges in developing an ontology fromthe CDC VISs and future direction.MaterialsThe focus of this paper is the ontology representation ofthe knowledge in the VISs developed by the CDC. Cur-rently there are 25 VISs available from the CDCs website,and these documents vary between 1 to 2 pages [16].The VISs describe patient-level information about vac-cines ranging from the historical burden of disease to theclinical indications for the vaccine.The current iteration of the VISO is derived from sixCDC VISs: Diphtheria, tetanus, and acellular pertussis vaccine(DTaP) [17] Rotavirus vaccine [18] Hepatitis B vaccine [19] Haemophilus influenzae type B vaccine (Hib) [20] Measles, mumps, and rubella (MMR) [21] Pneumococcal conjugate vaccine (PCV13) [22]All of the vaccines selected are recommended for chil-dren on or before the first year of life. Each of the VISdocuments are available for download from the CDCswebsite in both PDF and RTF format, along with theHTML version.The VISs are written at a 10th grade reading level [23],and with no images or figures to supplement any passages.Also from a subjective observation, there appears to besome organized consistency of the content within each ofthe documents, which also helped provide a skeletal struc-ture for the VISO ontology. Some examples of consistentdedicated sections included: General vaccine and disease information. Possible vaccine reactions and/or side effects. Populations for whom the vaccine is indicated. General vaccine recommendations and doseinformation.MethodEach of the six VISs acquired from the CDCs websitewere examined, and sentences were identified as relevantselections to inform the design of VISO. Ignoring headersand standard text that appeared on VIS documents, therelevant passages were transcribed onto a tracking spread-sheet and coded for simple identification. Any fragmentwith bullet points was also recorded. Each phrase in theVIS was broken down into a knowledge triple, which isAmith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 3 of 12a piece of factual information that is decomposed to asubject-predicate-object format that can be visually rep-resented or modeled in an ontology. Separate trackingdocuments were used for individual VIS documents formanagement purposes. Later, separate knowledge mod-els for each of the VIS documents were realized, andthen collated to obtain a comprehensive model of vaccineinformation. The proceeding subsections will discuss thehigh-level conceptualization of the VISO model, detailingthe class-level organization and formalization of knowl-edge, and the last subsection will discuss the encoding ofthe instances and triples, along with reoccurring designpatterns in the VIS domain. The end goal was to developcommon classes that can accommodate a large corpusof vaccine information, and have a formalized model tocover an expansive vaccine domain that is relevant forpatients.Meta-level conceptualizationFor a presentation of the properties between the classessee Figure 1. The oval shapes in the diagram depictsthe high-level classes in VISO. Between the oval-shapedclasses, a line connects two classes to signify a relation-ship, and a dotted line depicts a relationship betweensubclasses. Labels are placed beside both the classesand the relationship connection to provide identification.For clarification, some classes in the diagram have thesame name, which means they are referring to the sameclass.Class-level representationFigure 1 illustrates the meta-level interpretation of theVISO. The model represents a composite of the highabstraction understanding of the six VISs with 23 uniquehigh-level parent classes. Table 1 outlines all of the high-level classes employed for the six VIS document models.The first column lists the formal name of the high-levelclass, and its corresponding description is listed in thenext column. Also, examples of subclasses derived fromthe class and sample instances are listed in another col-umn, and a count of the number of instances using theclass (or using its subclasses) were registered. The mostsalient classes found in all of the VIS documents wereVac-cine, Target, Reaction, Dosage, and People. The Vaccineclass designated the vaccine that is the subject of the VISdocument. Target class referred to what the Vaccine classis protecting against, e.g. hepatitis B, rotavirus, etc. TheReaction class categorizes any possible side effect or reac-tion following vaccination. The Dosage class is used forany vaccine series and/or dose information from the VIS,Figure 1 Vaccine Information Statement Ontology (VISO). Parent-level graph of VISO classes.Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 4 of 12Table 1 Class tableMeta-level class Description Example subclasses or Instances (in italics) InstancesVaccine A class description to categorize vaccinesdocumented by the CDCs VIS documentationAlternateVaccine 9Target Provides class specification for virus, bacteria,diseases, etc. that are prevented by VaccineDiseaseVirus, SeriousDisease, Bacteria 12People Categorizes various types of patients or groups ofindividuals impacted by vaccines, diseases,reactions, and other health conditionsPeopleWithCondition, Infants, PeopleWithIllness,PeopleWithModerateIllness, PeopleOfAge,Children, Adults93Source Used to describe an origin of a vaccines target(bacteria, disease). Can be reused in relations withother classes.bacteria, Hepatitis B virus 5Channel Class of medium of travel for vaccine targets. PeopleChannel, ObjectChannel,PeopleWithConditionChannel, DermalChannel,HumanActivityChannel, FluidChannel s19Cause For a description of a condition as a result of avaccine target. E.g. infection, coughing spells.ear infection, long-term illness, coughing spells, pneumonia 39Location Type to categorize location, specifically area of thebody, affected by a heath condition or reactionFacialLocation, ThroatLocation, ArmLocation 14Probability Classification for types of probabilitiesenumerated in VIS documentsQualitativeProbability, QuantitativeProbability,ProbabilityInCases38Outcome Types of effects resulting from causation,reactions, or chain of outcomes.RareOutcome, AdultOutcome, ChildOutcome,FatalOutcome38Duration Used for various types of descriptions forqualitative length of time for effects of healthconditions or signs of conditions.DurationInMinutes, DurationInWeeks 5Substance Classification of kinds of substance for vaccinesand possibly other class groupings.LiquidSubstance, GaseousSubstance,SolidSubstance1Combination For various artifacts that interact with vaccines. SafeCombination, DangerousCombination 1Method Groups and classifies inoculation methods forvaccines.InhaleMethod, InjectionMethod, OralMethod 1NumberOfDoses Enumerates the maximum number of doses forvaccine.OptionalNumberOfDoses 6Dosage Designates the types of dose or the dose interval.E.g. 1st Dose, 3rd Dose.OptionalDose, DoseIntervals 23Component Categorizes types of elements of a vaccine. ViralComponent, NoninfectiousViralComponent 3Age Enumerates the type of quantitative classificationof age ? years, months, weeks, etc.AgeInMonths, AgeInYears 25Date Enumerates the types of quantitative classificationof date ? days, months, weeks, etc.DateInYears, DateInMonths, DateInDays 4Occurrence Classification of types of events VaccinatedOccurence, TimedOccurence 24Action Types of patient recourse in response toreactions or actions required vaccine patientsbefore inoculation.InactiveAction, ActiveAction, EmergencyAction 8Allergen For classes of substances leading to an allergicreaction.VaccineAllergen, VaccineComponentAllergen 7Reaction Used to categorizes types of reactions that mayresult from vaccination.MildReaction, TemporaryReaction, SeriousReaction,ModerateReaction, AdultReaction71Sign Class of indicators for vaccine reaction effects. fast heartbeat, crying, stomach pain, hives 14Total instances of classes 460and the People class organizes various classes of patientswho may or may not be recommended to receive thevaccine.Subclass-level representationThe VISO models high-level classes utilizes extensivesubclassing to cover specific abstraction of entities. OneAmith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 5 of 12apparent use was for categorization purposes to organizethe knowledge for better identification, such as the casefor the People class. For example, the VIS documents oftenrefer to a specific population that may react differently tovaccines, or may need a different vaccine type due to ageor certainmedical conditions, justifying the need to createsubclasses to refer to specific groups of people.Another motivation for subclasses is to facilitatedescriptions from the text that are in the form ofadjectives or prepositions. If the documentation refersto the number of optional doses of a vaccine, a sub-class called OptionalNumberOfDoses, which is a childof NumberOfDoses, was defined. Attention was givento classes that have universal subclasses that may nothave been observed from the VIS documentation. Someexamples are Organ class (Heart, Liver, Lungs) orSubstance class (GaseousSubstance, LiquidSubstance,SolidSubstance).Value sets and partitionsTo address quantitative, descriptive, or ranks in classes,value set representations, as described in a W3C work-ing draft [24], was utilized for the VISO meta-ontology.More specifically, we use subclasses to represent per-missible values in a value set. For example, the VISdocumentation alluded to a three-point scale, especiallywhen describing severity of conditions - mild, moder-ate, severe (or serious). We created a Reaction class thathas the subclasses MildReaction, ModerateReaction, andSeriousReaction to describe the degree of vaccine effectseverity. Other classes that employ value sets includethe People class and the Target class. For any otherunits of measure revealed in the VIS documents, spe-cific classes yielded subclasses that handled units ofmeasure, such as, AgeInYears and AgeInMonths for theAge class or DateInMonths and DateInDays for the Dateclass.PropertiesKnowledge triples evoked by the VIS documents sug-gested common relationships or properties betweenclasses across the VIS corpus. In all of the 6 documents,it was common to describe a vaccine preventing a bacte-rial infection or virus, or a vaccine can potentially causea rare reaction following administration. Many of theseproperties between the classes were identified and nor-malized to a standard representation. One example in theDTaP VIS, the evoked triple - Another vaccine, called Td,protects against tetanus and diphtheria... - utilized thepredicate protect against, which essentially means Tdprevents tetanus and diphtheria. In result, Td protectsagainst tetanus was rendered as Td prevents tetanus,where prevents is the standard property, or controlledterm, to describe that specific relationship.Table 2 identifies all of the object properties utilized inVISO. The first column list the domain classes, with theproperties and range classes in the subsequent columns.Overall, 33 types of object properties exist in the currentiteration of VISO.Table 3 list data properties. Similar in format to Table 2,two types of data properties are used in VISO with thedomain listed as Protégés superclass Thing and a stringliteral for its range. These data properties are meant to beglobal to all of the classes. These two properties serve asutility properties to accommodate information that eitherprovide an alternative name ( also known as) or infor-mation that describe or define an object of the class ( isdescribed as).Instances and triplesAfterwards, the meta-level ontology development led intothe encoding of instances from the collected VISs. Refer-ring to Table 1, the People class accounted for most ofinstances, with 93 instances of the People classes. Since theinitial set of VIS documents was small, some of the classeshad one instance stemming from its class. There is a pos-sibility that the remaining VIS documents may add moreinstances to these classes.Earlier, we indicated that across the VIS documentsthere exist some consistency that influence the represen-tation of knowledge in the VISO. This was also reflectedin how the instances are conceptualized. The followingsubsections list some noticeable design patterns observedin the development of VISO - dosage pattern for vaccinedose representation, target pattern for representing theobject of vaccine prevention, and the reaction pattern forpossible side effects of the vaccine.Dosage patternEvery vaccine documentation outlines the number ofdoses, and when and who should receive the vaccine.The three knowledge triples provided in the representa-tion in Figure 2, where statement 1 is represented as aChildDose subclass, and their corresponding dose order(statements 2 and 3) are depicted in subclasses of Age forthe the time of the vaccination. Most of the dose informa-tion found in the VIS documents are modeled similarly inVISO.Target patternPertinent information on what a vaccine protects againstis found in the six vaccine statements. Target instancescomprise of measles, tetanus, pneumococcal disease,rotavirus, pertussis, etc. Figure 3 illustrates a typicalpattern forTarget instances. In the figure, the 9 statementslisted aremapped in accordance to the VISOmodel. State-ment 1 reveals the type of Target (Serious Disease) thatdiphtheria is. Statement 2 is abstracted with a SourceAmith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 6 of 12Table 2 Object propertiesDomain Properties Range TriplesTarget, Cause, Reaction affects People, Location 11Dosage after Date 2PeopleWithAllergicReaction, AllergicReaction allergic to Allergen 7Dosage before Occurrence 1PeopleBornFrom born from Date 1Target causes Cause 40Vaccine contains Component 3Vaccine discouraged for People 19NumberOfDoses for People 5Cause, Outcome, Reaction happens Probability 42Vaccine has alternate Alternate Vaccine 4Vaccine has dosage Dosage 26Vaccine has number of NumberOfDoses 6Reaction has signs Sign 14Vaccine is a substance of Substance 1Vaccine is safe for People 6Vaccine is safe with Combination 1Vaccine is taken Method 1Target, Reaction, Sign, ObjectChannel lasts Duration 5Target, Cause, Outcome, Reaction leads Outcome 31Cause, Reaction located Location 15Vaccine may cause Reaction 54Reaction need Action 3Reaction, Sign occurs after Date, Occurrence 24PeopleOfAge of age Age 3Target originates Source 5Vaccine prevents Target 11Vaccine protects People 23Target spreads through Channel 19People take Action 5Dosage taken at Age 22Cause to People 2HumanActivityChannel with People 2Total number of triples 414Table 3 Data propertiesDomain Properties Range TriplesThing also known as string data 7Thing is described as string data 8Total number of triples 15subclass of Bacteria, with a relationship of originateswith theTarget instance. Likewise, statement 3 is shown asPeopleChannel subclass of Channel. Statements 5 through9 are facilitated by the Cause class with relationshipsto instances of Location and Outcome classes. This isan example is indicative of how Target class instancesand their commonly related relationships are modeled inVISO.Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 7 of 12Figure 2 Dosage modeling example. Example of Dosage instance representation with an excerpt from the MMR VIS.Reaction patternReaction patterns modeled possible reactions from vacci-nations described in each of the VIS documents. Figure 4shows a model of three sample statements. Statement 1shown as an instance of Vaccine may cause MildReac-tion triple. Statement 2 alludes to the left branch with aninstance of MildReaction located at Location, and state-ment 3 represented in the right branch of a Probabilitysubclass of ProbabilityInPatients.The patterns described in this section are the mostevident types in the mapping of the information, but addi-tional vaccine information design patterns also exist. It isalso understood that more patterns may emerge with theinclusion of the remaining VIS documents.Quality evaluationTo evaluate the overall quality of VISO, the ontologywas scored using the ontology metrics suite proposedby Burton-Jones, et al. [25]. Inspired by semiotics con-cepts, this simple, yet extensive ontology scoring metricevaluates an ontology based on four criteria - seman-tic, syntactic, social, and pragmatic levels. The semanticcriteria evaluates the term meanings and word sense ofeach of the terms. The syntactic criteria assesses syn-tax of the ontology, while social criteria examines howother ontologist use the ontology through links. The prag-matic criteria evaluates the practicality of the ontology,relating to its usage and its construction. Also, the evalu-ation metric is flexible to allow individual tailoring if anyof the criteria does not apply, as demonstrated in refer-ence [25]. The resulting final score is a value between 0and 1.Because of the initial design of the ontology, the scoringmetric needed to be tailored to provide a progress indica-tor of its development. For example, social criteria whichis dependent on other ontologies linking to VISO was notFigure 3 Target modeling example. Example of Target instance representation with an excerpt from the DTaP VIS.Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 8 of 12Figure 4 Reaction modeling example. Example of Reaction instance representation with an excerpt from the Hepatitis B VIS.possible to determine, therefore, it was excluded from thecomplete calculation score.However, the attainment of the other scores werestraightforward. The semantic quality score (E inequation (1)) was attained by a normalized summationof Interperatability (w1 as terms with a word sense inWordNet/TC as total terms used in the ontology), Con-sistency (misused terms as i/total classes, properties, andinstances as C), and Clarity (average number of wordsenses for each term, w2/total classes, properties, andinstances, C). For w1 (Interperatability) and w2 (Clar-ity), a simple Java GUI-based application utilizing thews4j library for WordNet [26] and the OWL API libray[27] was developed to automate and retrieve the valuesto calculate semantic scores. Equation (1) outlines thesemantic quality calculation.E =(13 ?w1TC)+(13 ?iC)+(13 ?w2C)(1)The pragmatic score, P, was determined by the normal-ized summation of Comprehensiveness (C/500 as totalnumber of classes and properties over 500, the maxi-mumnumber for classes and properties for the calculation[25]), and Accuracy (false statements as f /total numberof statements from the ontology as TS). Relevance scor-ing (application of statements/total number of statements)involves ontology usage in an application, which was omit-ted. For Accuracy, vaccine subject matter experts weregiven a list of statements from the ontology and wereasked to label whether the statement was true or false andto provide any corrections.While the terminology and theknowledge source was primarily from the VIS, our vac-cine experts also interact directly with patients and theirconcerns relating to vaccines which helped to focus onthe patient-centric goal of the VIS. Below, equation (2)describes the calculation of P.P =(12 ?C500)+(12 ?fTS)(2)For syntactic quality, S, the Protégé editor providedsome of the values for the scores. Both the Lawful-ness, number of syntactic violations over total number ofstatements in the ontology (l/TS) and Richness, num-ber of syntactic elements utlized over the total syntacticelements (s/SF) accounted for the normalized sum of syn-tactic quality. Equation (3) shows the syntactic qualitycalculation.S =(12 ?lTS)+(12 ?sSF)(3)While the metric evaluation can be tailored specific tothe ontology, aspects like the social criteria and Rele-vance were excluded. The modified final score is repre-sented in equation (4), whereQ is the overall quality score.Q =(13 ? E)+(13 ? P)+(13 ? S)(4)Results and discussionResultsThe VISOwas serialized in OWL2 with Protégé. Themet-rics data collected from Protégé is shown in Table 4. Thefirst column displays the item and the second columndisplays the value associated. The current VISO modelproduced a class count of 132, including subclasses; 33object properties; 2 data properties; and 419 unique indi-vidual instances. In addition, 460 instances derived fromconcepts were created, along with a total of 429 knowledgetriples.Table 4 VISOmetrics dataClass count 132Object property count 33Data property count 2Unique individual count 419Instances asserted from classes 460Total knowledge triples 429Amith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 9 of 12As stated earlier, part of the process in engineering theabstraction of the VIS involved obtaining passages to bemapped into the model. Table 5 displays the number ofpassages parsed for evaluation under the Total Passagescolumn. Also, the third column recorded the number ofinstance triples produced. For all six of the VIS documentsources, a total of 244 sentences and passages were used,and 427 instance triples were extrapolated from them, andlater merged and coded with Protégé to VISO.Table 2 indicate the number of knowledge triplespresent in the current VISO version. Most of the tripleswere mainly of the Vaccine may cause Reaction type,with 54 instances of that triple. Total number of instancesof triples in VISO numbered at 429 (derived from Tables 2and 3), that includes the 15 instances of triples denoted bydata property relationships from the table in Table 3.Both the HermiT (version 1.3.8) and the FaCT+ reason-ers with the Protégé editor did not reveal any discrepan-cies with this version of the model.The final quality score for evaluation, Q, based on thetailored equation (4) calculated to 0.54. Observing theother components of the quality score, the semantic qual-ity, E, amounted to 0.59. Pragmatic quality and syntacticquality equated to 0.75 (P) and 0.27 (S), respectively.Result analysisWhile the construction of VISO is in the early stages andcovering 6 VISs, the quality score appears to reflect theearly developmental state of the ontology. However, look-ing at the decomposition of the score can reveal someinsight to VISO and possibly inform the future directionof the VISO development.Observing specifics of the semantic quality, the Claritycomponent score (0.89) reveals less ambiguity among theterms. The Interpertability value (0.74) also indicatesthe use of meaningful terms in the ontology, and theinconsistency of the terms was low (0.04).With pragmatic quality (0.75) of VISO, we ignored theRelevance quality due to unavailability for deployment inan application environment. Comprehensiveness quality(1.2) was exceedingly high due to being a large ontologywith many classes and properties. Based on reports fromTable 5 Extraction ResultsCDC VIS Total passages Instances of triples in VISOHepatitis B 45 76Rotavirus 49 64Hib 41 43PCV13 32 50DTaP 51 112MMR 26 82Total 244 427two expert reviewers, VISOs inaccuracy was relativelylow at 0.30.Syntactic quality revealed no violations with syntax,however, the Richness quality computed at 0.54, reflect-ing that VISOwas utilizing a little over half of the syntacticfeatures available.Based on the results, the VISOs strength lies in its prag-matic quality which indicates its overall usability basedon the two of the three components described earlier -Accuracy and Comprehensiveness. The excluded com-ponent, Relevance, could provide a more holistic scorein the future. Syntactic outcome was the weakest of thethree aspects of VISOs quality. This is partially due tothe minimal usage of ontology features available. Focus onthis aspect will warrant attention in future development ofVISO.Relating to Consistency, some terms, according toreviewers, were improperly used or required nuanceddescriptions. One example is the term Dosage which istechnically used to describemeasurement of a vaccine andnot a synonym for dose. Another example are propertylabels like cause. Some instances in the ontology shouldhave been labeled may cause to imply a possible causa-tion, rather than an expected outcome. Proper and preciseterm usage will be another focus that the next version ofVISO will rectify.DiscussionModeling the CDCs VISs posed several challenges. Onechallenge was determining relevant pieces in the corpusthat could be used for knowledge extraction. Most ofthe documents had statements that were either repetitive,or had literary flourishes that were deemed unnecessary.Also, the documents may have a paragraph or a sentencethat summarizes preceding information with granular-ity. In most cases, these were viewed as repetitive, yetmay serve a future purpose if the ontology were to beused to construct dialogue with patients. Additionally, thedocumentation comprised of some knowledge that werehistorical statements. An example from the rotavirus VIS:Because children are protected by the vaccine,hospitalizations, and emergency visits for rotavirushave dropped dramatically. [18]It is debatable whether the historical informationmay beuseful to patients, or if summarized statements, which arenaturally repetitious in these documents, could be inte-grated into the VISOmodel. In this initial version of VISOthe repetitive and historical texts were not mapped butmay be considered in future versions of the ontology.Another challenge were gaps in the knowledge wherethe information was incomplete, brief, or needed med-ical understanding beyond the lay person. Some of theAmith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 10 of 12language in the documents may not be readily evident toa parent or patient, and would require a medical profes-sional to provide interpretation. For example, if a certainvaccine should not be given to a child who is moderatelyill, how does the reader of the VIS determine the exactsigns of a moderately ill child as opposed to a child whomay be mildly ill? If the documents refer to a sign asphysical weakness, how does the patient or the readerdetermine features or indicators of physical weakness?Issues like these limited the scope of the VISO knowledge-base. In practice, the healthcare provider, rather than theparent, will determine if the child is too ill to receive thevaccine. It is the medical professionals responsibility toprovide judgment and guidance to the patient. Similarly,there were issues with limited number of VIS documentsto develop the VISO models and to create instances. Lackof additional information resulted in some of the classesnot having any subclasses to suggest.Expressing the knowledge contained within the VISdocuments posed some challenges as well. While themeta-level definition was designed with subclasses to han-dle descriptive instances, there are often passages withcomplex nouns and adjectives where each word carriedimportant meaning for the instance. Examples such aspainful tightening of muscles or difficult for infants tobreathe posed a predicament of whether these instancesshould be decomposed to additional classes and rela-tionships; use a subclass or create a new subclass; applypolymorphism; or keep as is as an instance. In most cases,they were realized as a single instance, until the meta-level model is further developed to map difficult passages.Moreover, given the historical nature of the VISs and con-sidering that several VISs were originally developed morethan 30 years ago in response to NCVIA, it is assumedthat subsequent versions contain much of the originallanguage. This likely contributes to the variability in thesemantic language of the ontology.Conclusion and future directionWe introduce the Vaccine Information Statement Ontol-ogy, which could positively influence the development ofintelligent ontology-drive applications and mitigate theknowledge gap that often exists in patients seeking accu-rate and reliable information but encountering complex orinaccurate sources. Possible future goals in continuing thedevelopment of the VISO include: Expand the Vaccine Information Statement Domain.This version of the VISO models 6 VIS documents asan initial iteration to examine the ontology. The nextfew iterations of VISO will include more domainknowledge from the remaining 19 VIS documents,available from the CDC website. There is also anawareness that additional knowledge can be modeledfrom outside the CDCs VIS documents. It is alsoassumed that the meta-level design will mature as werealize alternative interpretation of vaccineinformation, or discover abstractions that couldintegrate some of the ignored passages and phrase,like negation statements or summary passages. Withan expanded version and throughout thedevelopment cycle, we plan to evaluate the ontologyusing semiotic-based metric suite, and also adapt thesuite to also include aspects previously excluded, likesocial quality or Relevance. Link VISO with existing relevant ontologies toexpand the knowledge domain. Some third partyontologies that could be aligned with the VISO mayinclude the Vaccine Ontology (VO) [28], Ontology ofVaccine Adverse Effect (OVAE) [29] or MedicalDictionary for Regulatory Activities (MedDRA) [30]to address reaction or conditions in the VISO. Thisapproach would comprise of code-linking particularclasses with matching classes in the VISO model,which would lead into providing a comprehensiveand expanded knowledge-base for patient learning.However, because the knowledge-base is intended forpatient use, it will be essential to determine theappropriate since the aforementioned third partyontologies utilize professional vocabulary which maynot be understood by patients. Integrating patient-level synonymous terms andmulti-language equivalents into the lexicon. Amulti-lingual VISO would presumably expand toreach potential patients who may be excludedbecause of language or socio-economic barriers.There is also an interest in using the Open Access,Collaborative Consumer Health Vocabulary Initiative[31] as a resource to integrate consumer-level termsor synonyms. Applying natural language processing. We intend toexplore the possibility of applying natural languageprocessing (NLP) for both ontology learning andknowledge retrieval. We will implement NLPmethods to facilitate automatic knowledge extractionto expand the VISO. This will also provide intrinsicvalue for applications using natural languageprocessing and dialogue systems. Intelligent mobile agents. Ontology-drivenapplications could introduce the potential forintelligent agents for learning. Realistic and engagingagents are proven to be more effective for increasinginvolvement and learning [32-35], persuasion [36],and trustworthiness [37,38]. It provides acost-effective way to address patients concerns andanswer their questions about vaccination, whichotherwise requires healthcare professionals toaddress in person. This will presumably improve theAmith et al. Journal of Biomedical Semantics  (2015) 6:23 Page 11 of 12efficiency of healthcare delivery workflows andreduce the cost. They also provide flexibilities forvaccine education. Patients or parents can spend asmuch time as they need with intelligent mobiledevices, and interact with intelligent devices anytimethey have access to a computer or tablet. Last but notleast, personalized agents can be automatically builtaccording to users preference to improve theusability and acceptability of the system.Examples like VAMATA, which is anontology-driven mobile application with a speechinterface designed for combat medical settings, revealapplicable synergy between natural languageprocessing and ontology in mobile applications [39].We have previously developed a proof-of-conceptontology-driven mobile application with a natural lan-guage interface to query a VISO knowledge-base [12]. Theongoing evolution of VISO will assist in the continuingdevelopment of that project.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMA wrote the initial draft and revised subsequent draft, developed theontology, and preformed the evaluation, collection of the metrics, and analysisof the results. YG revised and approved early and final versions of the draftsand contributed to review of the ontology. RC revised the draft and approvedfinal draft, contributed to the review and evaluation of the ontology. JBcontributed to the revision and approval of the final draft, contributed to thereview and evaluation of the ontology . CT was Principal Investigator for thisproject, contributed to research design, provided substantive intellectualcontributions, and feedback on the manuscript. All authors read and approvedthe final manuscript.AcknowledgementsThis project is partially supported by the National Library Of Medicine of theNational Institutes of Health under Award Number R01LM011829.Author details1School of Biomedical Informatics, University of Texas Health Science Center,7000 Fannin St, 77030 Houston, TX, USA. 2Immunization Project, TexasChildrens Hospital, 1102 Bates, 77030 Houston, TX, USA.Received: 9 December 2014 Accepted: 28 March 2015Published: 1 May 2015JOURNAL OFBIOMEDICAL SEMANTICSHastings et al. Journal of Biomedical Semantics  (2015) 6:10 DOI 10.1186/s13326-015-0005-5RESEARCH Open AccesseNanoMapper: harnessing ontologies toenable data integration for nanomaterial riskassessmentJanna Hastings1*, Nina Jeliazkova2, Gareth Owen1, Georgia Tsiliki3, Cristian R Munteanu4,5,Christoph Steinbeck1 and Egon Willighagen5AbstractEngineered nanomaterials (ENMs) are being developed to meet specific application needs in diverse domains acrossthe engineering and biomedical sciences (e.g. drug delivery). However, accompanying the exciting proliferation ofnovel nanomaterials is a challenging race to understand and predict their possibly detrimental effects on humanhealth and the environment. The eNanoMapper project (www.enanomapper.net) is creating a pan-Europeancomputational infrastructure for toxicological data management for ENMs, based on semantic web standards andontologies. Here, we describe the development of the eNanoMapper ontology based on adopting and extendingexisting ontologies of relevance for the nanosafety domain. The resulting eNanoMapper ontology is available at http://purl.enanomapper.net/onto/enanomapper.owl. We aim to make the re-use of external ontology content seamlessand thus we have developed a library to automate the extraction of subsets of ontology content and the assembly ofthe subsets into an integrated whole. The library is available (open source) at http://github.com/enanomapper/slimmer/. Finally, we give a comprehensive survey of the domain content and identify gap areas. ENM safety is at theboundary between engineering and the life sciences, and at the boundary between molecular granularity and bulkgranularity. This creates challenges for the definition of key entities in the domain, which we also discuss.Keywords: Nanomaterial, Safety, OntologyBackgroundNanomaterials are materials in which the individual com-ponents are sized roughly in the 1-100 nanometer rangein at least one dimension, although an exact definition isstill being debated [1,2]. Particles in this size range dis-play special properties having to do with their very largeratio of surface area to volume [3]. Natural nanomate-rials include viral capsids and spider silk. Recent yearshave seen an explosion in the development of engineerednanomaterials (ENMs) aiming to exploit the special prop-erties of these materials in various domains includingbiomedicine (e.g. as vehicles for drug delivery), optics andelectronics [3].*Correspondence: hastings@ebi.ac.uk1European Molecular Biology Laboratory  European Bioinformatics Institute(EMBL-EBI), Cambridge, United KingdomFull list of author information is available at the end of the articleCounterbalancing the many possible benefits of devel-oped nanotechnology, nanoparticles also pose seriousrisks to human and environmental health [4]. Recognisingthese dangers, regulatory bodies are calling for system-atic and thorough toxicological and safety investigationsinto ENMs with the objective of feeding knowledge intopredictive tools which are able to assist researchers indesigning safe nanomaterials. Evaluating and predictingthe possible dangers of different nanomaterials requiresassembling a wealth of information on those materials the composition, shape and properties of the individualnanoparticles, their interactions with biological systemsacross different tissues and species, and their diffusionbehaviour into the natural environment. These data arearising from different disciplines with highly heteroge-neous requirements, methods, labelling and reportingpractices. Regulatory descriptions of ENMs are not likethose needed for nanoQSAR analyses. Safety require-ments may also vary under different conditions, e.g. when© 2015 Hastings et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Hastings et al. Journal of Biomedical Semantics  (2015) 6:10 Page 2 of 15developing vehicles for drug delivery in life-threateningdiseases as compared to materials for use in the construc-tion industry.The eNanoMapper project (www.enanomapper.net) iscreating a pan-European computational infrastructurefor toxicological data management for ENMs, based onsemantic web standards and ontologies. eNanoMapperaims to develop a comprehensive ontology and annotateddatabase for the nanosafety domain to address the chal-lenge of supporting the unified annotation of nanomateri-als and their relevant biological properties, experimentalmodel systems (e.g. cell lines), conditions, protocols, anddata about their environmental impact. Rather than start-ing afresh, the developing ontology will build on existingwork, integrating existing ontologies in a flexible pipeline.The establishment of a universal standardisation schemaand infrastructure for nanomaterials safety assessmentis a key project goal, which will support collaboration,integrated analysis, and discoveries from data organisedwithin a knowledge-based framework.In this paper, we survey the existing ontologies that wereintegrated into the unified eNanoMapper ontology, focus-ing on the challenges we experienced with the integrationof diverse sources and our automated solution for seam-less modular re-use of external content. Furthermore,we discuss challenges in the definition of key entities inthe domain and give harmonised definitions for the corematerial and experimental entities in the domain.ProblemThe eNanoMapper ontology covers the following broadcontent areas:1. A categorisation of nanoparticle classes based ontheir properties, constituency and shape.2. Physicochemical properties for ENMcharacterisation.3. Biological characterisation that describes theENM-specific interactions with, for example,proteins to form a corona.4. Environmental characterisation.5. Experimental design and encoding for experimentsin which nanosafety is assessed.6. The full nanomaterial lifecycle includingmanufacturing and environmental decay oraccumulation.7. Known safety information about ENMs.Table 1 gives a summary of the ontologies that have beenidentified as already in part covering these domain areas.The selection of ontologies is motivated by the require-ment that the ontologies be (a) open, that is, licensed forre-use without restriction other than attribution, (b) suit-able for use in data annotation, i.e. using unique numericidentifiers and offering textual labels and definitions, and(c) be broadly mutually compatible (although with someprovisos as we will discuss in the section on our re-usepipeline below).These ontologies are described further in the Resultssection below. However, our initial naïve attempt to re-use these ontologies in their entirety in the developmentof the integrated eNanoMapper ontology ran into severalchallenges: Some content was duplicated across multipledifferent ontologies, resulting in multiple classes withdifferent identifiers being included with the samelabel  including cases where classes with the samelabel were defined differently across the ontologies; Some classes which were multiply imported, i.e.following the recommended re-use policy, in theontologies we imported, such as frequently usedupper-level classes or unit classes, subsequently werefound to have multiple copies of all associatedannotations and axioms in the resulting compositeontology; It was difficult to reconcile the different upper levelontologies that were used in these ontologies, and insome cases even when the same upper level ontologywas used (BFO), different versions of that upper levelstill caused incompatibilities; The presence of gaps in the imported contentnecessitates the manual annotation of contentadditions (which may later be submitted to varioussource ontologies). It was not easy to seamlessly addmanual content to the imported ontologies withoutneeding to re-create the manual content every timethe source ontologies changed and were re-imported;and Some of the definitions of the classes we wantedto re-use were missing or not sufficiently clearfor unambiguous re-use as part of an integratedwhole.Based on exact label matching only, the overlap betweenthe ChEBI ontology and theNPO is 395. This is a small butnevertheless significant number of exactly shared labels.Most of these are groups, atoms or chemical classes thatare included in NPO so as to support description of nano-material composition. Some, but not all, of these are cross-referenced to ChEBI via an additional annotation dbXref in NPO. Other classes with overlapping labels derive fromthe fledgling nanoparticle classification that is includedin ChEBI. For this branch of NPO, there are no cross-JOURNAL OFBIOMEDICAL SEMANTICSBoland et al. Journal of Biomedical Semantics  (2015) 6:14 DOI 10.1186/s13326-015-0010-8RESEARCH ARTICLE Open AccessDevelopment and validation of a classificationapproach for extracting severity automaticallyfrom electronic health recordsMary Regina Boland1,2*, Nicholas P Tatonetti1,2,3,4 and George Hripcsak1,2AbstractBackground: Electronic Health Records (EHRs) contain a wealth of information useful for studying clinicalphenotype-genotype relationships. Severity is important for distinguishing among phenotypes; however otherseverity indices classify patient-level severity (e.g., mild vs. acute dermatitis) rather than phenotype-level severity (e.g.,acne vs. myocardial infarction). Phenotype-level severity is independent of the individual patients state and is relativeto other phenotypes. Further, phenotype-level severity does not change based on the individual patient. For example,acne is mild at the phenotype-level and relative to other phenotypes. Therefore, a given patient may have a severeform of acne (this is the patient-level severity), but this does not effect its overall designation as a mild phenotype atthe phenotype-level.Methods: We present a method for classifying severity at the phenotype-level that uses the Systemized Nomenclatureof Medicine  Clinical Terms. Our method is called the Classification Approach for Extracting Severity Automaticallyfrom Electronic Health Records (CAESAR). CAESAR combines multiple severity measures  number of comorbidities,medications, procedures, cost, treatment time, and a proportional index term. CAESAR employs a random forestalgorithm and these severity measures to discriminate between severe and mild phenotypes.Results: Using a random forest algorithm and these severity measures as input, CAESAR differentiates betweensevere and mild phenotypes (sensitivity = 91.67, specificity = 77.78) when compared to a manually evaluatedreference standard (k = 0.716).Conclusions: CAESAR enables researchers to measure phenotype severity from EHRs to identify phenotypes thatare important for comparative effectiveness research.Keywords: Electronic Health Records, Phenotype, Health status indicators, Data mining, Outcome assessment(Health Care)BackgroundRecently, the Institute of Medicine has stressed the im-portance of Comparative Effectiveness Research (CER)in informing physician decision-making [1]. As a result,many national and international organizations were formedto study clinically meaningful Health Outcomes of Interest(HOIs). This included the Observational Medical Out-comes Partnership (OMOP), which standardized HOI* Correspondence: mb3402@columbia.edu1Department of Biomedical Informatics, Columbia University, New York, NY,USA2Observational Health Data Sciences and Informatics (OHDSI), ColumbiaUniversity, 622 West 168th Street, PH-20, New York, NY, USAFull list of author information is available at the end of the article© 2015 Boland et al.; licensee BioMed Central.Commons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.identification and extraction from electronic data sourcesfor fewer than 50 phenotypes [2]. The Electronic MedicalRecords and Genomics Network (eMERGE) [3] alsoclassified some 20 phenotypes, which were used toperform Phenome-Wide Association Studies (PheWAS)[4]. However, a short list of phenotypes of interest re-mains lacking in part because of complexity in definingthe term phenotype for use in Electronic Health Records(EHRs) and genetics [5].EHRs contain a wealth of information for studyingphenotypes including longitudinal health informationfrom millions of patients. Extracting phenotypes fromEHRs involves many EHR-specific complexities includingThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 2 of 13data sparseness, low data quality [6], bias [7], and health-care process effects [8].Many machine-learning techniques that correlate EHRphenotypes with genotypes encounter large false positiverates [3]. Multiple hypothesis correction methods aim toreduce the false positive rate. However, these methodsstrongly penalize for a large phenotype selection space.A method is needed that efficiently reduces the pheno-type selection space to only include important pheno-types. This would reduce the number of false positivesin our results and allow us to prioritize phenotypes forCER and rank them by severity.To extract phenotypes from EHRs, a specialized ontol-ogy or terminology is needed that describes phenotypes,their subtypes and the various relationships betweenphenotypes. Several ontologies/terminologies have beendeveloped for studying human phenotypes includingthe Human Phenotype Ontology (HPO) [9]. The HPOcontains phenotypes with at least some hereditary com-ponent, e.g., Gaucher disease. However, EHRs containphenotypes that are recorded during the clinical encoun-ter that are not necessarily hereditary. To capture a pa-tients phenotype from EHRs, we will utilize an ontologyspecifically designed for phenotype representation inEHRs called the Systemized Nomenclature of Medicine Clinical Terms (SNOMED-CT) [10,11]. SNOMED-CTcaptures phenotypes from EHRs, including injuries thatare not included in the HPO. Furthermore, SNOMED-CTcan be used to capture more clinical content then Inter-national Classification of Diseases, version 9 (ICD-9) codes[12], making SNOMED-CT ideal for phenotype classifica-tion. Using SNOMED-CT enables development of a stan-dardized approach that conforms to OMOPs guidelinespromoting data reuse.Robust methods are needed that address thesechallenges and reuse existing standards to support datasharing across institutions. This would propel our un-derstanding of phenotypes and allow for robust CER toimprove clinical care. This would also help pave the wayfor truly translational discoveries and allow genotype-phenotype associations to be explored for clinically im-portant phenotypes of interest [13].An important component when studying phenotypesis phenotype severity. Green et al. demonstrate that apatients disease severity at hospital admission wascrucial [14] when analyzing phenotype severity at thepatient-level. We are interested in classifying phenotypesas either severe or mild at the phenotype-level, whichdiffers from the vast literature on patient-specific sever-ity. Classifying severity at the phenotype-level involvesdistinguishing acne as a mild condition from myocardialinfarction as a severe condition. Contrastingly, patient-level severity assesses whether a given patient has a mildor severe form of a phenotype (e.g., acne). Importantly,phenotype-level severity is independent of the individualpatients state and is relative to other phenotypes (e.g.,acne vs. myocardial infarction). Further, phenotype-levelseverity does not change based on the individual patient.For example, acne is mild at the phenotype-level, whichis relative to other phenotypes. Therefore, a given pa-tient may have a severe form of acne (i.e., patient-levelseverity = severe), but the overall phenotype-level sever-ity is mild because phenotype-level severity is relative toother phenotypes and does not change based on an indi-vidual patients patient-level severity.Studying phenotype severity is complex. The plethoraof medical conditions is mirrored by an equally diverseset of severity indices that run the full range of medicalcondition complexity. For example, there is a severityindex specifically designed for nail psoriasis [15], insomnia[16], addiction [17], and even fecal incontinence [18].However, each of these indices focuses on classifying pa-tients as being either a severe or mild case of a given con-dition (e.g., psoriasis). They do not capture the differenceat the phenotype-level.Other researchers developed methods to study patient-specific phenotype severity at the organismal level. Forexample, the Severity of Illness Index assesses patienthealth using seven separate dimensions [19] consisting of:1) the stage of the principal diagnosis at time of admission;2) complications; 3) interactions (i.e., the number of pa-tient comorbidities that are unrelated to the principaldiagnosis); 4) dependency (i.e., the amount of care re-quired that is above the ordinary); 5) non-operating roomprocedures (i.e., the type and number of procedures per-formed); 6) rate of response to therapy; and 7) remissionof acute symptoms directly related to admission.The Severity of Illness Index is useful for characterizingpatients as severe or mild types of a given disease pheno-type. However, it does not measure severity at thephenotype-level (e.g., acne vs. myocardial infarction),which is required to reduce the phenotype selection spaceto only the most severe phenotypes for CER.In this paper, we describe the development and valid-ation of a Classification Approach for Extracting SeverityAutomatically from Electronic Health Records (CAESAR).CAESAR incorporates the spirit of the Severity of IllnessIndex, but measures phenotype-level severity rather thanpatient-level severity. CAESAR was designed specificallyfor use with EHR-derived phenotypes.MethodsMeasuring severityWe used five EHR-specific measures of condition severitythat are related to the 7 dimensions from Horns patient-level severity index [19] because EHRs differ from researchdatabases [20]. Columbia University Medical Centers(CUMC) Institutional Review Board approved this study.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 3 of 13Condition treatment time can be indicative of severityand so it was included as a severity measure. Treatmenttime is particularly indicative of severity for acute condi-tions, e.g., fractures, wounds or burns, because minor(less severe) fractures often heal more rapidly than majorfractures (more severe). However, treatment time is alsodependent on the chronicity of the disease [21], which isseparate from severity. Treatment time can also haveother effects when recorded in EHRs [22-24].Because hospital duration time can be influenced bymany factors, e.g., patients other comorbidities, we de-cided to analyze the condition treatment time. Whileinter-dependent, hospital duration time is typically asubset of the entire condition treatment time (which caninclude multiple hospital visits).Number of comorbidities is another useful measure forassessing phenotype severity. A similar measure is foundin the Severity of Illness Index that measures the num-ber of other conditions or problems that a given patientFigure 1 Example showing differences between ehr manifestations of sPhenotype-level differences between severe and mild phenotypes are shownphenotypes if you only look at the number of procedures, comorbidities or palone to identify severity, it would be difficult. However, if cost is used as a prinfarction is more severe than acne and also costs more). But if you use the trseverity will result (acne takes longer to treat as a result of chronicity, and theseverity). This underscores the importance of using multiple measures togethhas at the time of their principal diagnosis. Our EHR-specific version looks at the number of distinct comor-bidities per patient with a given phenotype and thenaverages across all of the individuals in the database withthat phenotype. This average tells us the comorbidityburden associated with a given phenotype. An exampleis given in Figure 1 to illustrate how the number ofcomorbidities, medications, and treatment time can dif-fer by phenotype severity. Note that acne is an atypicalmild phenotype as its treatment time is longer thanmyocardial infarction while most mild phenotypes haveshorter treatment times. Importantly, chronicity alsoaffects treatment time, which can negate the effect thatseverity has on treatment time (Figure 1).Number of medications is another useful measure forassessing severity. This measure is related to the previ-ous measure (i.e., the number of comorbidities). How-ever, it differs because some phenotypes have a largenumber of medications, but also a small number ofevere (Myocardial Infarction or MI) and mild (Acne) phenotypes.in Figure 1. Notice that there is very little difference between the tworescribed medications. Therefore, if you use any of those three measuresoxy for severity then the correct classification would be made (myocardialeatment length then an incorrect classification of the phenotype-levelrefore longer treatment length is not equal to increased phenotype-leveler as a proxy for severity, which is the approach employed by CAESAR.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 4 of 13comorbidities, e.g., burn injuries. Therefore, in manycases these measures will be similar but in other import-ant instances they will differ.Number of procedures is also based on a measure fromthe Severity of Illness Index. Because we are focused onphenotype-level severity, we computed an average num-ber of procedures associated with each phenotype. First,we extracted the number of procedures performed perphenotype and per patient. Then we computed the aver-age across all patients in our database yielding the aver-age number of procedures per phenotype.Cost to treat phenotype is a commonly used metric forassessing severity [25]. The Centers for Medicare andMedicaid Services released the billable rate for each pro-cedure code per minute [26]. They also released thenumber of minutes each procedure typically requires.Combining these data allows us to calculate the billableamount for a given procedure [26]. The billable rates arefrom 2004 and they are for each Healthcare CommonProcedure Coding System (HCPCS) code [26].Since these data are only available for procedure codes(HCPCS codes are procedure codes) we calculated thetotal cost per patient using the procedures they weregiven. We determined the cost per phenotype by takingthe average cost across all patients with that phenotype.Measures of phenotype severity and E-PSI (Ehr-phenotypeseverity index)We first calculated the proportion of each measure. Thesum of the proportions (there are five proportions  onefor each measure) was divided by the total number ofproportions (i.e., five). This final value is E-PSI, an indexterm based on all 5 measures given in Equation 1 wherex is a phenotype. Therefore, E-PSI is a proportionalindex that incorporates treatment time, cost, number ofmedications, procedures, and comorbidities.Equation 1:E-PSI (Phenotype x)¼ xcostmax costð Þ þxtreatment lengthmax treatment lengthð Þ þxcomorbiditiesmax comorbiditiesð Þþ xmedicationsmax medicationsð Þ þxproceduresmax proceduresð ÞFor example the treatment time of Hemoglobin SSdisease with crisis is 1406 days. We divide this by themax treatment length of any phenotype, which is also1406 days. This gives us the proportional treatmentlength of the disease or 1.00. Likewise, proportions arecalculated for each of the five measures. The sum of theproportions is divided by the total number of propor-tions, or 5. This is E-PSI, the proportional index, for thephenotype.We used Independent Components Analysis (ICA)[27] to visualize the relationship between E-PSI and eachphenotype severity measure. Computations were per-formed in R (v.3.1.1).Reference standard development and evaluationDevelopment of the Reference Standard involved usingthe CUMC Clinical Data Warehouse that was trans-formed to the Clinical Data Model (CDM) outlined bythe OMOP consortium [2]. All low prevalence phenotypeswere removed, leaving behind a set of 4,683 phenotypes(prevalence of at least 0.0001). Because we are studyingphenotypes manifested during the clinical encounter, wetreat each distinct SNOMED-CT code as a uniquephenotype. This was done because each SNOMED-CTcode indicates a unique aspect of the patient state [28].To compare results between mild and severe pheno-types, we required a reference-standard set of SNOMED-CT codes that were labeled as mild and severe. Inaddition, the set must be un-biased towards a particularclinical subfield (e.g., oncology or nephrology). Therefore,we developed a reference-standard set of 516 phenotypes(out of the 4,683 phenotype super-set) using a set of heu-ristics. All malignant cancers and accidents were labeledas severe; all ulcers were labeled as mild; all carcin-omas in situ were labeled as mild; and most labor anddelivery-related phenotypes were labeled as mild. Sincethe reference standard was created manually, the finaljudgment was left to the ontology expert regarding label-ing a given phenotype as mild or severe. However, theontology expert consulted with medical experts to reduceambiguity.Evaluation of the Reference Standard required solicitingvolunteers to manually evaluate a subset of the referencestandard (N = 7). Half of the evaluators held a MedicalDegree (MD) (N = 3) and completed residency while theother half were graduate students with informatics train-ing (N = 3) and one post-doctoral scientist. We askedeach evaluator to assign phenotypes as either mild or se-vere. We provided each evaluator with instructions fordistinguishing between mild and severe phenotypes. Forexample, severe conditions are conditions that are life-threatening (e.g., stroke is immediately life-threatening) orpermanently disabling (congenital conditions are generallyconsidered severe unless they are easily corrected). Mildconditions may still require treatment (e.g., benign neo-plasms and cysts are generally considered mild and notsevere as they may not require surgery). To ascertain theconfidence that each evaluator had in making their sever-ity assessments, we asked evaluators to denote their confi-dence in each severity assignment using a modified Likertscale [29] with the following 3 choices: very confident,somewhat confident and not confident. All evaluatorswere provided with two coded examples and 100 ran-domly extracted phenotypes (from the reference stand-ard). This evaluation set of 100 phenotypes contained 50Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 5 of 13mild and 50 severe (labels from the reference-standard).Pair-wise agreement between each evaluator and thereference-standard was calculated using Cohens kappa[30,31]. Inter-rater agreement among all evaluatorsand the reference standard was calculated using Fleissskappa [32,33].Evaluation of Measures at Capturing Severity involvedcomparing results from mild and severe phenotypesfor each severity measure. Severity measures were notnormally distributed so non-parametric measures (i.e.,quartiles) were used for comparisons.Learning phenotype-level severity classesDevelopment of the random forest classifierCAESAR involved the unsupervised learning of classesby calculating a proximity matrix [34]. The scaled 1-proximity for each data point (in this case a phenotype)was plotted [34]. The reference standard result was thenoverlaid on top to determine if there was any significantclustering based on a phenotypes class (in this casesevere or mild). Clusters of severe and mild phenotypescan be used to set demarcation points for labeling aphenotype.Using the proximity matrix also allows for discrimin-ation among levels of severity, in addition to the binaryclassification of severe vs. mild. We used the random-Forest package (v.4.6-10) in R (v.3.1.1) for calculations[35] and we used 1000 trees in our model. The randomforest classifier, or CAESAR, takes all 5 severity mea-sures and E-PSI (the proportional index term) as inputfor the model.Evaluation of the random forest classifierCAESAR was evaluated using the 516-phenotype refer-ence standard. Sensitivity and specificity were used toassess CAESARs performance. The class errors for se-vere and mild were measured using the randomForestpackage [35] and compared against the out-of-bag(OOB) error rate. The randomForest algorithm uses theGini index to measure node impurity for classificationtrees. The Gini impurity measure sums the probabilityof an item being chosen times the probability of misclas-sifying that item. We can assess the importance of eachvariable (i.e., the 5 measures and E-PSI) included inCAESAR by looking at the mean decrease in Gini. Vari-ables with larger decreases in Gini are more importantto include in CAESAR for accurate prediction.ResultsAssessment of phenotype severitySevere phenotypes in general are more prevalent inEHRs because in-patient records contain sicker indi-viduals when compared to the general population, whichcan introduce something called the Berkson bias [36].However, in the general population mild phenotypes areoften more prevalent than severe phenotypes.For condition/phenotype information we used datafrom CUMC EHRs, which was initially recorded usingICD-9 codes. These ICD-9 codes were mapped toSNOMED-CT codes using the OMOP CDM v.4 [2]. Forthis paper, we used all phenotypes (each phenotype be-ing a unique SNOMED-CT code) with prevalence of atleast 0.0001 in our hospital database. This constituted4,683 phenotypes. We then analyzed the distribution ofeach of the five measures and E-PSI among the 4,683phenotypes. Figure 2 shows the correlation matrixamong the 5 severity measures and E-PSI.Strong correlations exist between both the number ofprocedures and the number of medications (r = 0.88),and the number of comorbidities (r = 0.89). This indi-cates that there is a high degree of inter-relatedness be-tween the number of procedures and the other severitymeasures. Cost was calculated using HCPCS codesalone, whereas the number of procedures measure in-cludes both HCPCS and the ICD-9 procedure codes asdefined in the OMOP CDM. Because cost was calcu-lated using only HCPCS codes, the correlation betweencost and the number of procedures was only 0.63. Alsophenotype measures were increased for more severephenotypes. This could be useful for distinguishingamong subtypes of a given phenotype based on severity.E-PSI versus other severity measuresWe performed ICA on a data frame containing eachof the five severity measures and E-PSI. The result isshown in Figure 3 with phenotypes colored by increas-ing E-PSI score and size denoting cost. Notice thatphenotype cost is not directly related to the E-PSIscore. Also phenotypes with higher E-PSI seem to bemore severe (Figure 3). For example, complication oftransplanted heart, a severe phenotype, had a highE-PSI score (and high cost).Phenotypes can be ranked differently depending onthe severity measure used. To illustrate this, we rankedthe phenotypes using E-PSI, cost, and treatment lengthand extracted the top 10 given in Table 1. When rankedby E-PSI and cost, transplant complication phenotypesappeared (4/10 phenotypes), which are generally consid-ered to be highly severe. However, the top 10 pheno-types when ranked by treatment time were also highlysevere phenotypes, e.g., Human Immunodeficiency Virusand sickle cell. An ideal approach, used in CAESAR,combines multiple severity measures into one classifier.Complication of transplanted heart appears in the top10 phenotypes when ranked by all three-severity measures(italicized in Table 1). This is particularly interestingbecause this phenotype is both a complication phenotypeand transplant phenotype. By being a complication theCondition Length20 60r= 0.12p<0.001r= 0.17p<0.0015 15 25r= 0.17p<0.001r= 0.17p<0.0010.2 0.604001000r= 0.40p<0.0012060No. of Comorbiditiesr= 0.75p<0.001r= 0.89p<0.001r= 0.49p<0.001r= 0.88p<0.001No. of Medicationsr= 0.88p<0.001r= 0.69p<0.00104080120r= 0.91p<0.00151525No. of Proceduresr= 0.63p<0.001r= 0.94p<0.001Cost0400800r= 0.71p<0.0010 400 10000.20.60 40 80 120 0 400 800E?PSIFigure 2 Severity measure correlation matrix. Histograms of each severity measure shown (along the diagonal) with pairwise correlationgraphs (lower triangle) and correlation coefficients and p-values (upper triangle). Notice the condition length is the least correlated with the othermeasures while number of medications and number of procedures are highly correlated (r = 0.88, p < 0.001).Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 6 of 13phenotype is therefore a severe subtype of another pheno-type, in this case a heart transplant (which is actually aprocedure). Heart transplants are only performed on sickpatients; therefore this phenotype is always a subtype ofanother phenotype (e.g., coronary arteriosclerosis). Hencecomplication of transplanted heart is a severe subtype ofmultiple phenotypes (e.g., heart transplant, and the pre-cursor phenotype that necessitated the heart transplant coronary arteriosclerosis).Evaluation of severity measuresDevelopment of the Reference Standard severe and mildSNOMED-CT codes involved using a set of heuristicswith medical guidance. Phenotypes were considered severeif they were life threatening (e.g., stroke) or permanentlydisabling (e.g., spina bifida). In general, congenital pheno-types were considered severe unless easily correctable.Phenotypes were considered mild if they generaly requireroutine or non-surgical (e.g., throat soreness) treatment.Figure 3 Independent component analysis of phenotypes illustrates relationship between E-PSI and cost. Independent ComponentAnalysis was performed using all five severity measures and E-PSI. Phenotypes are colored by increasing E-PSI score (higher score denoted by lightblue, lower score denoted by dark navy). The size indicates cost (large size indicates high cost). Phenotypes with higher E-PSI seem to be moresevere; for example, complication of transplanted heart, a severe phenotype, had a high E-PSI score (and high cost). However, phenotype cost isnot directly related to the E-PSI score.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 7 of 13Several heuristics were used: 1) all benign neoplasmswere labeled as mild; 2) all malignant neoplasms were la-beled as severe; 3) all ulcers were labeled as mild; 4)common symptoms and conditions that are generally ofa mild nature (e.g., single live birth, throat soreness,vomiting) were labeled as mild; 5) phenotypes that wereknown to be severe (e.g., myocardial infarction, stroke,cerebral palsy) were labeled as severe. The ultimatedetermination was left to the ontology expert forTable 1 Top 10 phenotypes ranked by severity measureE-PSI CostComplication of transplanted heart Complication ofTransplant follow-up Transplant folloPosttransplantation lymphoproliferative syndrome Disorder of immComplication of transplanted lung Post-transplantasyndromeComplication of hemodialysis Anterior horn cDisorder of immune function Endocrine/metaComplication of renal dialysis Myocardial degDisorder of transplanted bone marrow APL - Acute proArrested development following proteincalorie malnutrition Isolated (FiedleSerratia septicaemia Complication odetermining the final classification of severe and mildphenotypes. The ontology expert consulted with medicalexperts when deemed appropriate. The final referencestandard consisted of 516 SNOMED-CT phenotypes (ofthe 4,683 phenotypes). In the reference standard, 372phenotypes were labeled as mild and 144 were labeled assevere.Evaluation of the Reference Standard was performedusing volunteers from the Department of BiomedicalTreatment lengthtransplanted heart Hemoglobin SS disease with crisisw-up Complication of transplanted heartune function Hemoglobin SS disease without crisistion lymphoproliferative Exstrophy of bladder sequenceell disease Factor IX deficiencybolic screening Complication of transplanted kidneyeneration Type II diabetes mellitus - poor controlmyelocytic leukaemia Sickle cell-hemoglobin C disease without crisisrs) myocarditis HIV - Human immunodeficiency virus infectionf transplanted lung OsteoarthritisBoland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 8 of 13Informatics at CUMC. Seven volunteers evaluated thereference standard including three MDs with residencytraining, three graduate students with informatics ex-perience and one post-doc (non-MD). Compensationwas commensurate with experience (post-docs received$15 and graduate students received $10 Starbucks giftcards).We excluded two evaluations from our analyses: onebecause the evaluator had great difficulty with the med-ical terminology, and the second because the evaluatorfailed to use the drop-down menu provided as part ofthe evaluation. We calculated the Fleiss kappa for inter-rater agreement among the remaining 5 evaluations andfound evaluator agreement was high (k = 0.716). The in-dividual results for agreement between each evaluatorand the reference standard were kappa equal to 0.66,0.68, 0.70, 0.74, and 0.80. Overall, evaluator agreement(k = 0.716) was sufficient for comparing two groups (i.e.,mild and severe) and 100% agreement was observed be-tween all five raters and the reference-standard for 77phenotypes (of 100).Evaluation of Measures at Capturing Severity was per-formed by comparing the distributions of all 6 measuresbetween severe and mild phenotypes in our 516-phenotypereference standard. Results are shown in Figure 4. Increaseswere observed for severe phenotypes across all measures.We performed the Wilcoxon Rank Sum Test to assesssignificance of the differences between severe vs. mildphenotypes shown in Figure 4. The p-values for eachcomparison were <0.001.Unsupervised learning of severity classesDevelopment of the random forest classifierCAESAR used an unsupervised random forest algorithm(randomForest package in R) that required E-PSI and all5-severity measures as input. We ran CAESAR on all4,683 phenotypes and then used the 516-phenotypereference standard to measure the accuracy of theclassifier.Evaluation of the random forest classifierCAESAR achieved a sensitivity = 91.67 and specificity =77.78 indicating that it was able to discriminate betweensevere and mild phenotypes. CAESAR was able to detectmild phenotypes better than severe phenotypes as shownin Figure 5.The Mean Decrease in Gini (MDG) measured the im-portance of each severity measure in CAESAR. Themost important measure was the number of medications(MDG = 54.83) followed by E-PSI (MDG = 40.40) andthe number of comorbidities (MDG = 30.92). Cost wasthe least important measure (MDG = 24.35).CAESAR used all 4,683 phenotypes plotted on thescaled 1-proximity for each phenotype [34] shown inFigure 6 with the reference standard overlaid on top.Notice that phenotypes cluster by severity class (i.e.,mild or severe) with a mild space (lower left) and asevere space (lower right), and phenotypes of inter-mediate severity in between.However, three phenotypes are in the mild space(lower left) of the random forest model (Figure 6). Thesephenotypes are allergy to peanuts, suicide-cut/stab, andmotor vehicle traffic accident involving collision be-tween motor vehicle and animal-drawn vehicle, driver ofmotor vehicle injured. These phenotypes are probablymisclassified because they are ambiguous (in the case ofthe motor vehicle accident, and the suicide cut/stab) orbecause the severity information may be contained inunstructured EHR data elements (as could be the casewith allergies).Using the proximity matrix also allows further dis-crimination among severity levels beyond the binarymild vs. severe classification. Phenotypes with am-biguous severity classifications appear in the middleof Figure 6. To identify highly severe phenotypes, wecan focus only on phenotypes contained in the lowerright hand portion of Figure 6. This reduces thephenotype selection space from 4,683 to 1,395 pheno-types (~70% reduction).We are providing several CAESAR files for free down-load online at http://caesar.tatonettilab.org. These in-clude, the 516-phenotype reference-standard used toevaluate CAESAR, the 100-phenotype evaluation setgiven to the independent evaluators along with the in-structions, and the 4,683 conditions with their E-PSIscores and the first and second dimensions of the 1-proximity matrix (shown in Figure 6). This last file alsocontains two subset tables containing the automaticallyclassified mild and severe phenotypes and their scores.DiscussionUsing the patient-specific severity index as a backbone[19], we identified five measures of EHR-specific pheno-type severity that we used as input for CAESAR.Phenotype-level severity differs from patient-level sever-ity because it is an attribute of the phenotype itself andcan be used to rank phenotypes. Using CAESAR, wewere able to reduce our 4,683-phenotype set (startingpoint) to 1,395 phenotypes with high severity and preva-lence (at least 0.0001) reducing the phenotype selectionspace by ~70%. Severe phenotypes are highly importantfor CER because they generally correlate with lower sur-vival outcomes, lost-productivity, and have an increasedcost burden. In fact, patients with severe heart failuretend to have bad outcomes regardless of the treatmentthey receive [37]. Therefore understanding the severityof each condition is important before performing CERMild Severe02004006008001200Treatment LengthMild Severe20406080ComorbiditiesMild Severe020406080100120MedicationsMild Severe51015202530ProceduresMild Severe0200400600800CostMild Severe0.20.40.60.8E?PSIFigure 4 Differences in severity measures and e-psi for mild vs. severe phenotypes. The distribution of each of the 6 measures used inCAESAR is shown for severe and mild phenotypes. Severity assignments were from our reference standard. Using the Wilcoxon Rank Sum Test,we found statistically significant differences between severe and mild phenotypes across all 6 measures (p < 0.001). Severe phenotypes (dark red)having higher values for each of the six measures than mild phenotypes. The least dramatic differences were observed for cost and number ofcomorbidities while the most dramatic difference was for the number of medications.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 9 of 13and having a complete list of severe phenotypes wouldbe greatly beneficial.Additionally, developing a classification algorithm thatis biased towards identifying more severe over mild phe-notypes is optimal, as it would enable detection of pheno-types that are crucial for public health purposes. Activelearning methods that favor detection of severe pheno-types were proven successful in a subsequent study [38].CAESAR uses an integrated severity measure ap-proach, which is better than using any of the othermeasures alone, e.g., cost, as each severity measure hasits own specific bias. It is well known that cosmetic pro-cedures, which by definition treat mild phenotypes, arehigh in cost. If cost is used as a proxy for severity itcould introduce many biases towards phenotypes thatrequire cosmetic procedures (e.g., crooked nose) that areof little importance to public health. Also some cancersare high in cost but low in mortality (and thereforeseverity), a good example being non-melanoma skincancer [39]. Therefore, by including multiple severity0 100 200 300 400 5000.100.150.200.250.30Error Rates for Random Forest ClassificationtreesErrorOOBMILDSEVEREFigure 5 CAESAR error rates. Error rates for CAESARs random forest classified are depicted with severe denoted by the green line, mild denoted bythe red line and out-of-bag (OOB) error denoted by the black line. CAESAR achieved a sensitivity = 91.67 and specificity = 77.78 indicating that it wasable to discriminate between severe and mild phenotypes. CAESAR was able to detect mild phenotypes better than severe phenotypes.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 10 of 13measures in CAESAR we have developed a method thatis robust to these types of biases.Another interesting finding was that cancer-screeningcodes tend to be classified as severe phenotypes by CAE-SAR even though they were generally considered as mildin the reference standard. The probable cause for this isthat screening codes, e.g., screening for malignant neo-plasm of respiratory tract, are generally only assigned byphysicians when cancer is one of the differential diagno-ses. In this particular situation the screening code, whilenot an indicator of the disease itself, is indicative of thepatient being in an abnormal state with some symptomsof neoplastic presence. Although not diagnoses, screen-ing codes are indicative of a particular manifestation ofthe patient state, and therefore can be considered asphenotypes. This finding is also an artifact of the EHR,which records the patient state [8], which does notalways correlate with the true phenotype [5,28].Importantly, CAESAR may be useful for distinguishingamong subtypes of a given phenotype if one of the char-acteristics of a subtype involves severity. For example,the severity of Gaucher disease subtypes are difficult tocapture at the patient-level [40]. This rare phenotypewould benefit greatly from study using EHRs wheremore patient data exists. Using CAESAR may help incapturing the phenotype-level severity aspect of this rarephenotype, which would help propel the utility of usingEHRs to study rare phenotypes [41] by providing accur-ate severity-based subtyping.CAESAR is directly relevant to the efforts of theObservational Health Data Sciences and Informaticsconsortium (OHDSI), which is a continuation of OMOP.OHDSI is an international network focused on observa-tional studies using EHRs and other health record systems.Their original motivation was to study post-market effectsof pharmaceutical drugs [42] based on their pharmaceuticalpartnerships. To this end, a severity-based list of rankedphenotypes would be beneficial for assessing the relativeimportance of various post-marketing effects (e.g., nausea ismild, arrhythmia is severe).Other phenotyping efforts would also benefit fromCAESAR including the eMERGE network [3], whichseeks to carefully define phenotypes of interest for use inPheWAS studies. So far they have classified 20 pheno-types. Having a ranked list of phenotypes would helpeMERGE to rank prospective phenotypes, thereby allow-ing them to select more severe phenotypes for furtheralgorithm development efforts.There are several limitations to this work. The first isthat we used CUMC data when calculating four of theseverity measures. Because we used only one institutionsdata, we have an institution-specific bias. However, sinceFigure 6 Classification result from CAESAR showing all 4,683 phenotypes (gray) with severe (red) and mild (pink) phenotype labelsfrom the reference standard. All 4,683 phenotypes plotted using CAESARs dimensions 1 and 2 of the scaled 1-proximity matrix. Severe phenotypesare colored red, mild phenotypes are colored pink and phenotypes not in the reference standard are colored gray. Notice that most of thesevere phenotypes are in the lower right hand portion of the plot while the mild space is found in the lower left hand portion.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 11 of 13CAESAR was designed using the OMOP CDM, it isportable for use at other institutions that conform to theOMOP CDM. The second limitation is that we did notuse clinical notes to assess severity. Some phenotypes,e.g., allergy to peanuts, may be mentioned more often innotes than in structured data elements. For such pheno-types, CAESAR would under estimate their severity. Thethird limitation is that we only used procedure codesto determine phenotype cost. Therefore, phenotypesthat do not require procedures will appear as low costphenotypes even though they may have other costs, e.g.,medications.Future work involves investigating the inter-relatednessof our severity measures and determining the temporalfactors that affect these dependencies. We also plan to in-vestigate the inter-dependency of phenotypes (e.g., blurredvision is a symptom of stroke, but both are treated asseparate phenotypes) and determine the utility of our se-verity measures for distinguishing between phenotypesand their subtypes.Another potentially interesting extension of ourwork could involve utilizing the semantics of SNOMED,specifically their phenotype/subtype relations, to exploreCAESARs severity results. Because we chose SNOMEDto represent each phenotype, we can leverage SNO-MEDs semantics to further probe the relationshipbetween severity and disease. Perhaps some of thephenotypes with ambiguous severity (middle of Figure 6)occurred because their disease subtypes can be eithermild or severe (we can assess this using SNOMEDshierarchical structure). However, leveraging the seman-tics of concepts for severity classification is a complexarea [43], which will likely require additional methods totackle. Hopefully these topics can be explored in futureby ourselves or others.ConclusionsThis paper presents CAESAR, a method for classifyingseverity from EHRs. CAESAR takes several known mea-sures of severity: cost, treatment time, number of co-morbidities, medications, and procedures per phenotype,and a proportional index term as input into a randomforest algorithm that classifies each phenotype as eithermild or severe. Using a reference standard that wasvalidated by medical experts (k = 0.716), we found thatCAESAR achieved a sensitivity of 91.67 and specificityof 77.78 for severity detection. CAESAR reduced our4,683-phenotype set (starting point) to 1,395 phenotypeswith high severity. By characterizing phenotype-levelseverity using CAESAR, we can identify phenotypesworthy of study from EHRs that are of particular im-portance for CER and public health.Boland et al. Journal of Biomedical Semantics  (2015) 6:14 Page 12 of 13AbbreviationsCER: Comparative Effectiveness Research; HOI: Health Outcomes of Interest;OMOP: Observational Medical Outcomes Partnership; eMERGE: The ElectronicMedical Records and Genomics Network; PheWAS: Phenome-WideAssociation; EHRs: Electronic Health Records; HPO: Human PhenotypeOntology; SNOMED-CT: Systemized Nomenclature of Medicine  ClinicalTerms; CAESAR: Classification Approach for Extracting Severity Automaticallyfrom Electronic Health Records; CUMC: Columbia University Medical Center;HCPCS: Healthcare Common Procedure Coding System; E-PSI: Ehr-phenotypeseverity index; ICA: Independent Components Analysis; CDM: Clinical DataModel; MD: Medical Degree; OOB: Out-of-bag error rate; MDG: MeanDecrease in Gini; OHDSI: Observational Health Data Sciences and Informaticsconsortium; ICD-9: International classifcation of diseases, 9th revision.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMRB performed research, data analyses, and wrote the paper. NPTcontributed to statistical design procedures, and provided intellectualcontributions. GH was Principal Investigator for this project, contributed toresearch design, provided substantive intellectual contributions, and feedbackon the manuscript. All authors read and approved the final manuscript.AcknowledgmentsWe thank the OMOP consortium and OHDSI, Dr. Patrick Ryan, and RohanBareja for their assistance with various facets of OMOP/OHDSI and CUMCsdata warehouse. Support for this research provided by R01 LM006910 (GH)and MRB supported in part by training grant T15 LM00707.Author details1Department of Biomedical Informatics, Columbia University, New York, NY,USA. 2Observational Health Data Sciences and Informatics (OHDSI), ColumbiaUniversity, 622 West 168th Street, PH-20, New York, NY, USA. 3Department ofSystems Biology, Columbia University, New York, NY, USA. 4Department ofMedicine, Columbia University, New York, NY, USA.Received: 3 November 2014 Accepted: 3 March 2015JOURNAL OFBIOMEDICAL SEMANTICSCairelli et al. Journal of Biomedical Semantics  (2015) 6:25 DOI 10.1186/s13326-015-0022-4RESEARCH ARTICLE Open AccessNetworks of neuroinjury semantic predications toidentify biomarkers for mild traumatic brain injuryMichael J Cairelli1*, Marcelo Fiszman1, Han Zhang2 and Thomas C Rindflesch1AbstractObjective: Mild traumatic brain injury (mTBI) has high prevalence in the military, among athletes, and in thegeneral population worldwide (largely due to falls). Consequences can include a range of neuropsychologicaldisorders. Unfortunately, such neural injury often goes undiagnosed due to the difficulty in identifying symptoms,so the discovery of an effective biomarker would greatly assist diagnosis; however, no single biomarker has beenidentified. We identify several body substances as potential components of a panel of biomarkers to support thediagnosis of mild traumatic brain injury.Methods: Our approach to diagnostic biomarker discovery combines ideas and techniques from systems medicine,natural language processing, and graph theory. We create a molecular interaction network that represents neuralinjury and is composed of relationships automatically extracted from the literature. We retrieve citations related toneurological injury and extract relationships (semantic predications) that contain potential biomarkers. After linkingall relationships together to create a network representing neural injury, we filter the network by relationshipfrequency and concept connectivity to reduce the set to a manageable size of higher interest substances.Results: 99,437 relevant citations yielded 26,441 unique relations. 18,085 of these contained a potential biomarkeras subject or object with a total of 6246 unique concepts. After filtering by graph metrics, the set was reduced to1021 relationships with 49 unique concepts, including 17 potential biomarkers.Conclusion: We created a network of relationships containing substances derived from 99,437 citations and filteredusing graph metrics to provide a set of 17 potential biomarkers. We discuss the interaction of several of these(glutamate, glucose, and lactate) as the basis for more effective diagnosis than is currently possible. This methodprovides an opportunity to focus the effort of wet bench research on those substances with the highest potentialas biomarkers for mTBI.Keywords: Semantic predications, Semantic networks, Natural language processing, Degree centrality, Traumaticbrain injuryIntroductionThe diagnosis and treatment of traumatic brain injury(TBI) has received considerable attention. The militarycommunity may provide the biggest contribution to thisinterest because the signature injury of the wars in Iraqand Afghanistan is mild TBI (mTBI) [1]. mTBI is some-times referred to as concussion, although the latter termis becoming less common in clinical and research con-texts. The athletic community is also concerned withthis condition, especially football and fighting sports, but* Correspondence: mike.cairelli@nih.gov1National Institutes of Health, National Library of Medicine, 38A 9N912A,8600 Rockville Pike, Bethesda, MD 20892, USAFull list of author information is available at the end of the article© 2015 Cairelli et al.; licensee BioMed Central.Commons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.also rugby, hockey, and soccer [2-8]. Although lessnewsworthy, falls cause the majority of head injuries inthe US, with nearly 1.7 million TBI cases annually [9].Worldwide, the annual incidence of mild TBI is esti-mated to be above 600 per 100,000 and, in addition tofalls, motorcycle and bicycle accidents are also majorcauses [10]. As important as improvements in care arefor veterans and athletes, such improvements can have amuch broader impact on the health of communitiesaround the world.Although there is a need to improve the treatment ofbrain injury, perhaps the most significant hurdle is diag-nosing mTBI. Current diagnostic standards are adequateThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 2 of 14for moderate and severe TBI because their signs andsymptoms are more easily identifiable, but about 70-90% of TBI is mild, also known as concussion, andstill difficult to recognize [10]. Additionally, the WorldHealth Organization estimates that many mild injuriesare not even seen by a health care practitioner becausethis lack of obvious and urgent symptoms fails to mo-tivate patients to seek care [10]. Unfortunately, thisdoes not mean that there are no long-term sequelaeresulting from mTBI. According to current clinical re-search, mTBI sequelae include cognitive dysfunction,post-traumatic stress disorder, depression, anxiety, anddementia [2,11].However, there are no currently accepted markers forclinical diagnosis of mTBI. Different organizations havecreated schematic tools for diagnosis, but these are sub-jective and the organizations do not completely agree onwhat constitutes a concussion [12]. For the greatest im-pact for military applications and throughout the world,as well as to minimize costs, a blood-based test wouldbe ideal. Thus far such a test has not been found.There have been several candidate substances (S100B,neuron-specific enolase, glial acidic fibrillary protein,etc.), but none have succeeded for effective diagnosis ofmild injury [13]. Because the search for a single bio-marker has not succeeded, a composite panel may bean effective alternative. We present a method to helpfacilitate the identification of substances that havepotential as biomarkers, which can then be validatedexperimentally.As demonstrated with systems biology [14], the mo-lecular interactions that occur after neurological injuryare complex. There may be no serum value for any ofthe individual components of this complicated interplaythat are specific to neural injury. However, some specificcombination of these values included in a panel hasmuch greater potential for diagnostic accuracy. Thefirst step in investigating which substances belong insuch a panel is to identify the potential candidates forinclusion. In this paper, we describe a methodology toprovide a list of substances that is intended to estab-lish a base of current knowledge and provide insightinto the development of a biomarker panel for mTBIdiagnosis. We apply natural language processing toMEDLINE citations to extract semantic predications,which we represent as a network of potentially rele-vant substances interacting with their physiologicalenvironment. These semantic predications are subject-relation-object triples, where the subjects and objectsare UMLS concepts and the relation is derived fromthe UMLS Semantic Network as appropriate for a givenconcept pair. We then use network analysis techniquesto identify a list that is focused on highly significantsubstances.BackgroundSystems medicineOur approach to diagnostic biomarker discovery was in-spired by systems medicine, the application of systemsbiology to medicine. The underlying philosophy looks atbiology as information science and is concerned withthe network of molecular interactions that define bio-logical processes [14,15]. Additionally, disease states areviewed as a perturbation of these molecular networks[15]. In the case of traditional TBI biomarker discovery,the approach has been to seek an individual molecule torepresent a disease state, while disregarding any notionof a network let alone its perturbation. Wang et al. de-scribe this approach as pauciparameter, containing aninadequate amount of information and resulting in inad-equate characterization [15]. The network must be con-sidered as a whole, because a network perturbation doesnot necessitate that any of the individual molecules areoutside of their normal serum measures, especially atearly stages of disease, when prevention is still possibleor treatment is optimal. They give prostate specific anti-gen for prostate cancer screening as an example of a fail-ure of the traditional single marker, pauciparameterapproach [15].Natural language processingNatural language processing combines artificial intelligenceand linguistic theory to extract meaning from text, usingstatistical machine-learning, hand-written rules, or a com-bined approach [16]. The data utilized in this study wereprovided by SemRep, which extracts semantic predicationsfrom all titles and abstracts in MEDLINE [17]. These pred-ications take the form of a subject-predicate-object triplet.The subject and object are mapped to Unified MedicalLanguage System (UMLS) concepts using MetaMap [18]and are stored with their UMLS semantic type, whereasthe predicate is mapped to the UMLS Semantic Network[19]. This provides precise semantic meaning from thesource text. For example, from the sentence in (1), SemRepextracts the predications in (2).(1) Basic science and clinical observations supportive ofthe role of endothelins in the spasm associated withstroke and subarachnoid hemorrhage are presented.(Pubmed ID 15281894)(2) Endothelin ASSOCIATED_WITH SpasmSpasm ASSOCIATED_WITH CerebrovascularaccidentThe results of this process are stored in a predicationdatabase, SemMedDB [20], which has been used to sup-port a range of biomedical information managementresearch: identifying novel therapeutic approaches [21],labeling extracted information from clinical text [22],Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 3 of 14literature-based discovery [23-26], clinical informationretrieval for physicians [27], retrieving clinical documents[28], abstraction summarization of biomedical texts [29],biological entity recognition [30], identifying disease candi-date genes [31], support for cardiovascular clinical guide-lines [32,33], interpreting microarray data [34], extractingresearch findings from literature [35], and supporting for-mal models of knowledge representation [36,22].Networks of semantic predicationsAny concept in a set of predications can serve as ei-ther subject or object in various relationships. Forexample, one can imagine the concept Glutamateappearing in many predications similar to the following:Glutamate ASSOCIATED_WITH Traumatic Brain Injury,Glutamate INHIBITS Glutamate Synthase, or GlycineSTIMULATES Glutamate. Similarly, any concept can havea set of relationships that include it as either the sub-ject or object. Further, any set of predications can berepresented as a network with each concept symbolizedas a node and each relationship denoted by an edge(or arc) between the two nodes that represent itssubject and object. A network containing the abovepredications is contained in Figure 1.One of the goals of network theory is to establish sig-nificance of a given node or relationship. Degree central-ity is based on the number of connections a node hasand Zhang et al. [37] have shown that it is effective foridentifying nodes in a graph that humans consider im-portant. We have previously applied degree centrality toSemRep generated semantic predications to successfullysummarize therapeutic studies [38]. For node (or vertex)v, the degree centrality is calculated by dividing the totalnumber of nodes connected to v, deg(v), by the totalnumber of nodes in the network other than v, n-1:Cd vð Þ ¼ deg vð Þn?1STIMULATES INHIBITSASSOCIATED_WITH Glycine Glutamate Synthase TBIGlutamateFigure 1 Network of glutamate predications. Subject and objectconcepts are represented as nodes and predicates are representedas edges. Glutamate is common to all three predications and is,therefore, the most highly connected node in the network.A simple means of judging the value of a given rela-tionship is the frequency of the relationship, that is, asimple count of how many times it occurs in a given set.When using an automated tool, a single occurrence of apredication is much more susceptible to computationalerror than a predication with multiple instances. There-fore, a higher frequency may provide more confidence inthe validity of the relationship, but at the same time, ahigh frequency is reflective of an abundance of asser-tions in the literature which is likely to be indicative of awell-known fact and may be less desirable for noveldiscovery.Incorporation of systems medicine, natural languageprocessing, and network theoryThis methodology combines ideas and techniques fromsystems medicine, natural language processing, and net-work theory. A network of relationships involving sub-stances is created, but the data source is semanticpredications from MEDLINE citations rather than gen-omic or other large-scale experimental data as haveoften been used for systems medicine. These semanticpredications provide a computable form of the know-ledge contained in MEDLINE that includes gene, pro-tein, and metabolite relationships analogous to theexperimental data traditionally used in systems medi-cine, as well as additional types of relations at the organ-ism, system, organ, tissue, cell, and molecular level.Statistical approaches are often used to establish correl-ation and significance of different components in the ex-perimental data of systems medicine, whereas a networkof semantic predications provided by SemRep naturallyexpresses the network of interactions postulated by sys-tems approaches. Network filtering techniques are usedto further suggest significance of the individual conceptsand their relationships. By coupling components fromthese three fields, a novel method of biomarker discov-ery is proposed.Related workSeveral manual reviews have been undertaken to surveypotential biomarkers for TBI [39,40] and more specific-ally mTBI [41-43]. These authors search for citationsspecifically detailing clinical research of mTBI bio-markers and therefore contain only potential biomarkersthat have already been investigated. Another limitationof the studies is the small number of citations reviewed(ranging from 26 [42] to 107 [43]) due to the limitationsof human review. Although no automated detection ofpotential TBI biomarkers exists in the literature, thereare automatic systems to help diagnose other disorders,for example diabetes and obesity [44]. Although not re-lated to mTBI, there is research related to the literature-based discovery of other types of interaction networksFigure 2 Overview of methodology. A PubMed search was used tofind citations related to nervous system trauma (NST). SemRepextracted predications containing chemical substances from thesecitations, which were then arranged into a network. The networkwas filtered by connectedness (degree centrality) and frequency toprovide a summary view of the most significant relationships.Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 4 of 14(though not specifically for biomarkers). One automatic-ally generates an interaction network detailing geneinvolvement in vaccine-related fever using 170,000 cita-tions from a PubMed search and a vaccinespecificontology [45]. Another used citations containing thePubMed MeSH term human and containing sentencesrelated to interferon-gamma, from which relationshipswere extracted and ranked using graph metrics [46].Jordan et al. [47] present a keyword search method foridentifying putative biomarkers for breast and lung can-cer by searching for genes and proteins associated with abiological fluid keyword and either cancer. However,none of this work has made use of semantic predica-tions, as we have, in the formation of an interaction net-work. There is a large body of work on literature-baseddiscovery approaches many of which use SemRep se-mantic predications [26,48-54]. These approaches maygenerate systems for discovery [55-58] or are specificapplications to predict various phenomena such asinteractions between genes and proteins [46,59], can-cer treatments [60,61], adverse drug reactions [49],drug-drug interactions [50], drug repurposing [51,62],asthma gene associations [63], treatments for neovas-cularization in diabetic retinopathy [52], relations be-tween psychiatric and somatic diseases [64], genesrelated to reactive oxygen species and diabetes [65],and mechanisms for sleep disturbance [25] and the obesityparadox [53].MethodsAs shown in Figure 2, citations related to nervous sys-tem trauma were retrieved from PubMed. From these,predications were extracted that contain a substance asthe subject or object. These predications were organizedinto a single network which is then filtered to select forthe most highly connected and frequent components.The substances included in this summary network serveas the list of potential mTBI biomarkers.Citation searchA PubMed search for all articles containing the MeSHterm Trauma, Nervous System was used to generate alist of PubMed identification numbers (PMIDs). Thisterm is a parent to Brain Injuries in the MeSH hierarchyand also includes terms such as Spinal Cord Injuries andCerebrovascular Trauma. The source publications werelimited only in requiring that they included neural injuryas a topic, with no limitations on journal, species, loca-tion, or type of injury. Although this included non-TBIinjury and models, (e.g., stroke, spinal cord injury,hypoglossal-nerve injury, etc.), the goal was to undertakeas wide a search as possible in order to retrieve remoteand ignored possibilities, with the assumption that a sig-nificant level of commonality exists between the variousforms of injury included under this broad heading inlight of their inclusion of common injury pathways suchas inflammation and oxidative damage. 99,437 unique ci-tations were returned by this search.Semantic predication selectionSemantic predications were extracted from SemMedDBusing the PMIDs resulting from the above PubMedsearch, which yielded 26,441 unique predications. Over-all, this set contains 6246 unique concepts, including lessinformative terms, such as rattus, injury, and patients aswell as more specific terms, such as glutamate, brain-derived neurotrophic factor, and methylprednisolone.We then required the predications to contain at leastone concept (subject or object) having a UMLS semantictype with potential as a substance biomarker (amino acidsequence; amino acid, peptide, or protein; biologicallyactive substance; body substance; carbohydrate; carbohy-drate sequence; chemical; chemical viewed functionally;chemical viewed structurally; eicosanoid; enzyme; geneor gene product; gene or genome; hormone; immuno-logic factor; inorganic chemical; lipid; neuroreactivesubstance or biogenic amine; nucleic acid, nucleoside,or nucleotide; nucleotide sequence; organic chemical;Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 5 of 14organophosphorus compound; receptor; steroid; sub-stance). If only one of the arguments was of this type,the other concept could be of any semantic type. Thisresulted in the inclusion of some concepts that indi-cate that the research was performed in animal modelssuch as Rattus and Animals. We did not discard thesenodes because they allow the inclusion of potentialbiomarkers from basic research in the spirit of transla-tional medicine. Although a given semantic type, forinstance Pharmaceutical Substance, was not includedin the list of target semantic types, it could still appearin a resulting predication if the complimentary subjector object met the requirements. As an example, inthe predication Dexamethasone INTERACTS_WITHNF-kappa B, the subject, Dexamethasone, is of typePharmaceutical Substance and the object, NF-kappaB, is of type Amino Acid, Peptide, or Protein. Thispredication qualifies for inclusion because of the object,not the subject. In the predication DexamethasoneTREATS Rheumatoid Arthritis, the object, RheumatoidArthritis, is of type Disease or Syndrome, so thepredication would not be selected because neithersubject nor object is of an included semantic type.After applying this limitation, 18,085 unique predica-tions remained.Network of predicationsThese 18,085 predications extracted from neurologicalinjury MEDLINE citations and containing a potentialbiomarker as subject or object were then linked togetheras a network. This network represents all of the knownsubstance activity involved in neurotrauma, as indicatedby the semantic predications included in SemMedDB.The nodes of the network represent arguments (subjector object) from the predications, and the edges representthe predicates or relationships between subjects and ob-jects. Each subject-object pair might have multiple pred-icates. For example, both Melatonin INHIBITS FreeRadicals and Melatonin COEXISTS_WITH Free Radicalsmay have been asserted in the literature. When countingedges in the network, each predicate between the samesubject and object in such predications was countedseparately. Additionally, each subject-predicate-objecttriplet could have been asserted once in MEDLINE (andthus in SemMedDB) or as many as dozens of times.When taking into account each predication extractedfrom multiple citations, the network has 6246 totalnodes and 18,085 total edges. When only unique(different) predications are considered (regardless of thenumber citations they were extracted from), the numberof nodes in the network remains 6246, but the numberof edges is 14,085. This is still a rather large network; toreduce it to more manageable size, further filtering wascarried out.Network filtering: degree centralityThe first cutoff applied was degree centrality. Afterattempting several thresholds, a node degree cutoff of0.0000800641 was empirically chosen to provide a net-work with more than 50 and fewer than 100 nodes,thereby providing a humanly readable graph. This degreecorrelates to a node having edges connecting to 50 othernodes. For example, the concept Traumatic Brain Injuryis connected to 295 other nodes with a degree of0.0004724 and, therefore, is maintained in the networkafter degree filtering. However, the concept cyclooxygen-ase 2 is connected to only 43 concepts with a degree of0.00006886 and so is eliminated. The 20 most highlyconnected concepts are shown in Figure 3, and 20 exam-ples of the 2688 nodes which had only a single connec-tion are provided in Figure 4.Network filtering: frequency of occurrenceFrequency of occurrence was used in conjunction withdegree centrality to increase the saliency of the network.A given edge (predicate) between highly connectednodes (arguments) was required to have a frequency ofoccurrence of 2 in an attempt to eliminate spuriousextractions while still including rare statements. As anexample, the predication Interleukin-3 DISRUPTS CellDeath is maintained in the final network because itoccurs twice in the SemMedDB predication set. Be-cause NADPH Dehydrogenase INTERACTS_WITHGlial Fibrillary Acidic Protein occurs only a singletime, it is not included in the final network. The mostfrequently occurring predications from this set areprovided in Figure 5. This refinement requiring a fre-quency greater than or equal to 2 and a node degreegreater than or equal to 0.0000800641 (50 or moreconnections) resulted in 1021 predications with 49 uniqueconcepts (see Figure 6).Substance network visualizationThe resulting network was visualized as a network inCytoscape [66]. Redundant edges between connectednodes were reduced to a single edge for visual simplicity.In addition to the substance concepts targeted, it alsocontains non-substance concepts which are coupled tothe substances in the final predication set. An additionalnetwork visualization was produced (Figure 7), reformat-ted to focus on the resulting potential biomarkers. Allnon-candidate concepts were reduced in size and labelsremoved. A candidate subnetwork was formed consist-ing of substance nodes, edges connecting them, anddirectly intermediate nodes and edges (single nodes be-tween two substances if no edge directly connected thepair). Nodes and edges outside of the candidate subnet-work were also colored gray to further reduce visualprominence.Figure 3 20 most connected nodes in unfiltered network.Figure 4 20 from the 2688 nodes with only a single predication in the unfiltered network.Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 6 of 14Figure 5 20 most frequent predications in the unfiltered network.Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 7 of 14Substance network semantic distributionThe final network was analyzed to outline the distribu-tion of UMLS semantic types and predicates. The se-mantic types of nodes were sorted and tallied as werethe predicate for each token of the edges.Substance identification precisionSemMedDB maintains a reference to the specific sen-tence in the original citation that was the source for eachpredication. Each substance in Figure 6 was comparedagainst this source sentence and evaluated for con-sistency with the sentence, not for truth value. In otherwords, we evaluated only whether the substance occursin the text; whether or not the text provided a biomedi-cally accurate statement was not evaluated at this stage(however, truth value was addressed in Section Evalu-ation of biomarker potential). Precision was calculatedfor the resulting substance list using the number of cor-rect substances in the final network and the total num-ber of substances in the final network as follows:Precision ¼ Correct Substancesð Þ= Total Identified Substancesð Þ :Evaluation of biomarker potentialEach of the substances in the final, filtered networkwas individually reviewed manually as a potentialmTBI biomarker. The evaluation was based on 3questions: is there evidence of a change in the levelof this substance during traumatic brain injury, isthis change evidenced in blood, and has the substancebeen previously investigated as a biomarker for trau-matic brain injury. We searched PubMed with thequery [substance name] AND traumatic brain injuryAND (serum OR blood) and the resulting articleswere explored to provide answers to the evaluationquestions.ResultsFiltered networkThere are a total of 17 substances out of the 49 con-cepts in the final network. The first version (Figure 6)shows all concepts (49) and their connections (145),while in the second (Figure 7), a candidate subnet-work is emphasized in black containing 17 sub-stances as labeled nodes and the 48 edges connectingthem. The candidate subnetwork also contains 12unlabeled non-substance nodes. One node shown inthe complete network was incorrectly identified asthe substance SHAM (salicylhydroxamate) instead ofsham (meaning a false experimental action) whilethe 17 other substances were correctly extracted, for aprecision of 0.94.Substance network semantic distributionAs seen in Table 1, the most common predicate in thefinal network is LOCATION_OF with 26 instances. Thisrepresents 22% of the 209 total edges. The predicatePREDISPOSES, which is a clear indicator of biomarkerpotential, is significantly lower at 12 edges (5.7%). Thesemantic type Amino Acid, Peptide, or Protein was by farmost common with 13 out of the 49 nodes (26.5%) asFigure 6 Visualization of substance predication network. The network contains 49 nodes and 1021 edges. Multiple edges between a pair ofnodes are represented as a single edge for visual simplicity; therefore edge labels are not included. Abbreviations: FGF2 = fibroblast growthfactor 2, NGFs = nerve growth factors, BDNF = brain-derived neurotrophic factor, NaCl = sodium chloride, APP = amyloid-beta precursor protein,SOD = superoxide dismutase, NSE = neuron specific enolase, GFAP = glial fibrillary acidic protein, TBI = traumatic brain injury, IL6 = interleukin 6,NE = norepinephrin, DA = dopamine, SHAM = salicylhydroxamic acid.Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 8 of 14seen in Table 2. This semantic type was also dominantwithin the subset of substance nodes (Table 3) with 8 ofthe 17 (47.1%).Evaluation of biomarker potentialThe results of the substance verification in Table 4provide an estimate of level of interest for further re-search as a member in a biomarker panel. In general,the substances show evidence of change in TBI inthe literature, with two exceptions: amyloid beta-protein precursor and calpain. (Although calpainitself does not appear in the literature, the calpain-derived NH2-terminal fragment of ?-spectrin frag-ment does [67]). Timing and degree of change mayalso be an issue regarding the effectiveness of somesubstances as mTBI biomarkers. Reduced levels ofcalcium appear to return to normal within as little as4 hours of trauma [68]. Glutamate levels increase incerebral spinal fluid but there is no evidence formeasurable changes in blood [69-72]. And a conflictexists in the literature for melatonin. One study re-ports a decrease in serum melatonin after TBI [73]while another reports no change in blood but an in-crease in cerebral spinal fluid [74].DiscussionMost substances identified in this study as worthy ofconsideration as mTBI biomarkers fall into four gen-eral categories: previously studied biomarkers (amyl-oid beta-protein precursor, brain-derived neurotrophicfactor, fibroblast growth factor 2, glial fibrillary acidicprotein, neuron-specific enolase, S100b); neurotransmitters(glutamate, dopamine, norepinephrine); inflammationand cell injury markers (interleukin-6, calpain break-down products, malondialdehyde, superoxide dismutase);and ubiquitous substances (glucose, lactate, calcium).Although all of the resulting substances were reviewedin depth during the methodology, the following illustratethe information contained in the resulting mTBI bio-marker network and the information retrieved duringthe validation process. These examples suggest possibleimplications for clinical practice retrieved directly fromthe research literature.GlutamateThe well-known association between glutamate and TBIis present in the network as Glutamate ASSOCIATED_WITH Traumatic Brain Injury (PMID 17014847), butrelationships that focus on interconnectedness withS100BCalpainGlutamateCalciumNSENEDAMalondialdehyde SODIL6BDNFAPPMelatoninLactateGlucoseGFAPFGF2Figure 7 Visualization of interaction network of TBI substances. Onlysubstance nodes are labeled and paths between substance nodesare colored black for lengths one or two edges. Abbreviations:FGF2 = fibroblast growth factor 2, BDNF = brain-derived neurotrophicfactor, APP = amyloid-beta precursor protein, SOD = superoxidedismutase, NSE = neuron specific enolase, GFAP = glial fibrillary acidicprotein, IL6 = interleukin 6, NE = norepinephrin, DA = dopamine.Table 1 Predication frequency in final networkLOCATION_OF 46ASSOCIATED_WITH 28PART_OF 25AUGMENTS 20PREDISPOSES 12ADMINISTERED_TO 11ISA 11AFFECTS 10CAUSES 10COMPARED_WITH 6DISRUPTS 5INTERACTS_WITH 5TREATS 5STIMULATES 4INHIBITS 3COEXISTS_WITH 2NEG_ADMINISTERED_TO 2NEG_INTERACTS_WITH 2PRODUCES 2Table 2 Semantic type frequency in final networkAmino acid, peptide, or protein 13Body part, organ, or organ component 5Biologically active substance 4Cell 4Hormone 4Injury or poisoning 4Organic chemical 3Neuroreactive substance or biogenic amine 2Animal 1Cell component 1Gene or genome 1Inorganic chemical 1Mammal 1Pharmacologic substance 1Patient or disabled group 1Sign or symptom 1Steroid 1Tissue 1Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 9 of 14other substances in the network are also present.For instance, Lactate INTERACTS_WITH Glutamateis extracted from (3) which notes that glutamate isproduced from the metabolism of lactate in TBI,and perhaps a less familiar relationship, GlutamateSTIMULATES Lactate is extracted from (4), highlightingglutamates role in activating lactate production in apotentially neuroprotective, estrogen receptor-dependentmanner.(3) Infusion with  3-(13)C-lactate produced (13)Csignals for glutamine  indicating tricarboxylic acidcycle operation followed by conversion of glutamateto glutamine. (PMID 19700417)(4) These results suggest a new neuroprotectivemechanism of 17beta-estradiol by activatingglutamate-stimulated lactate production, which isestrogen receptor-dependent. (PMID 11368971)Glucose and lactateGlucose and lactate are substances within the networkthat (along with calcium) are ubiquitous in the humansystem. Within the context of TBI a major concern isthe decrease of available glucose in the brain due toTable 3 Semantic type frequency of substances in finalnetworkAmino acid, peptide, or protein 8Organic chemical 3Biologically active substance 2Neuroreactive substance or biogenic amine 2Gene or genome 1Hormone 1Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 10 of 14ischemia and the subsequent increase in lactate. This isincluded in our neural injury network as GlucoseCOEXISTS_WITH Lactate, which is extracted from (5).Sentence (6) is another source of the link betweenlactate and glucose, but the source sentence providesthe additional knowledge that peripheral blood glu-cose levels are not isolated from cerebral levels andlactate production in the TBI brain, while (7) pre-sents the opposite, that arterial lactate is connectedto cerebral lactate and subsequently cerebral glucose,represented in our network as Lactate COEXISTS_WITH Glucose. As suggested in (4) above, the ratioof glucose to lactate is influenced by glutamate. Ithas been suggested that this may be a result of astro-cytes responding to increased extracellular glutamateby increasing glycolysis and, thereby, lactate produc-tion [75].Table 4 Verification of substances in TBI physiology and TBI bChanges in trauma? Chang1 Brain-derived neurotrophic factor Yes Yes2 Fibroblast growth factor 2 Yes Yes3 Glial fibrillary acidic protein Yes Yes4 Neuron-specific enolase Yes Yes5 S100B Yes Yes6 Amyloid beta-protein precursor Yes No7 Interleukin-6 Yes Yes8 Malondialdehyde Yes Yes9 Superoxide dismutase Yes Yes10 Glucose Yes Yes11 Lactate Yes Yes12 Dopamine Yes Yes13 Norepinephrine Yes Yes14 Calcium Yes Yes15 Melatonin Yes Yes*16 Glutamate Yes Yes*17 Calpain No+ No+*Modest change or conflicting reports. +Although Calpain itself does not change in trau(5) Following TBI, neuron use initially increases, withsubsequent depletion of extracellular glucose,resulting in increased levels of extracellular lactateand pyruvate. (PMID 18826359)(6) Arterial blood glucose significantly influenced signsof cerebral metabolism reflected by increasedcerebral glucose uptake [and] decreased cerebrallactate production (PMID 19196488)(7) We conclude that arterial lactate augmentation canincrease brain dialysate lactate, and result in morerapid recovery of dialysate glucose after FPI [fluidpercussion brain injury]. (PMID 10709871)Biomarker panelsAlthough there have been a limited number of attemptsto include multiple biomarkers in panels for TBI[67,76,77], these have not included some of the types ofsubstances returned in our results. To a large degree theabsence of consideration for such substances may be ex-plained by their lack of specificity or their ubiquitousnature. The level of specificity as an analyte for theseneglected substances is significantly higher for an indi-vidual marker to stand on its own, and substances thatare frequent if not ubiquitous in normal physiology arenot obvious as candidates for TBI identification. Takenon their own, glucose and calcium levels are not usefulas measures of brain injury. However, a panel ofmarkers could better represent the complex network ofmolecular changes that occur during TBI and changeiomarker researches in blood? Previously studied? PMIDsYes 11585248, 22528282, 20679891Yes 11320217, 7696886Yes 16266720, 22528282, 21079180Yes 16716992, 22528282Yes 19257803, 22528282, 21079180Yes 8140894, 15258792No 20850781, 20858121No 11280646, 11466564No 17869973No 20889287, 9808254No 18183032, 20889287No 7584744No 6886758, 3592639No 10386980, 4637556No 18183032, 17060154No 20225002, 21878868No+ 19811094ma, its products do change and are found in the blood and have been studied.a)Cairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 11 of 14the goal from an individual marker/single variable to apanel ameliorates the lack of specificity  as long as thepanel as a whole provides adequate sensitivity andspecificity.b)Figure 8 The relative distribution of predication frequency by year.a) All frequencies. b) Predications that have at least 2 occurrences.78.8% of predications occurred only once.Limitations of studyThese resulting data provide a clinically relevant hypoth-esis of potential mTBI biomarkers, which requires ex-perimental validation. In our investigation into thevalidity of the results, it was evident that for some of thesubstances, especially the previously-studied biomarkers,the background TBI model-based studies have alreadybeen completed. For others, this is not the case andbasic exploration in models may need to be pursued be-fore moving towards clinical research.The current result set is limited to the uppermost ex-treme of node connectedness and therefore potentiallyoverlooks less investigated substances that appear infewer publications. An elimination of the most frequentpredications may enrich the result set for substances lessfamiliar and thereby, potentially, more valuable. Thecurrent threshold is principally set to provide a visuallycomprehensible network in the result, though such avisualization is not required. Reducing the threshold forinclusion would expand the list with significant com-pounds, including microRNA.When we filter by frequency of occurrence with acutoff of 2 instances we eliminate 78.8% of the predica-tions. This step risks eliminating predications thatoccur only once because they are completely new andhave only been stated once. Figure 8a shows that 7.8%of predications were from citations in 2010. As seen inFigure 8b, when all predications that occur only onetime are removed, the 2010 fraction increases to 7.9%.This shows that there is not a disproportionate elimin-ation of predications from the most recent citationsand the loss of unique predications due to their noveltylikely plays a much less significant role than the elim-ination of inaccurate extraction by SemRep. On theother hand, as SemRep precision continues to improve,additional attention to date of publication may berequired.Future directionsCreating a map of neural injury interactions offers sig-nificant potential for basic science research. Additionally,our refinement of the network to identify the mostsignificant interactions according to their degree central-ity and frequency facilitates the quick translation of pub-lished research data into clinical practice. The resultingcompound list is clearly interesting in the context ofclinical applicability and merits further study. This tech-nique allows the investigation of potential biomarkers tobe focused, potentially reducing the wet-lab effort andreducing the time of assay development.Now that we have outlined a basic methodology, wewould like to compare this method with various othermethods combining information extraction and networkanalysis to understand the advantages and disadvantagesto different approaches.Our current methodology can be expanded as notedabove to include different subsets of substances in theCairelli et al. Journal of Biomedical Semantics  (2015) 6:25 Page 12 of 14final result. Additionally, this methodology is not limitedto biomarker discovery but can also be applied to otherareas of medical discovery, including novel therapeutictargets, drug repurposing, and others.ConclusionWe have explored the creation of a molecular inter-action network that represents neural injury and iscomposed of semantic predications automatically ex-tracted from the literature. We achieved our goal ofproviding substances with potential as biomarkers tosupport the diagnosis of mTBI. The methodology isbased on a network of semantic predications represent-ing the interaction of substances observed subsequentto neural insult. Combining semantic predicationsof TBI substance interactions into a network in this waycorrelates well with systems biology (and by extension,systems medicine), which is concerned with the com-plex network interplay of a biological unit and repre-sents injury and illness as a perturbation to thenetwork.Predications were extracted by SemRep and the com-ponent subject or object concepts were mapped tonodes and their relationships (predicates) mapped toedges, creating a network of relations. This network rep-resents a summary of the physiological and pharmacoge-nomic space of neurological injury, as presented in theliterature included in MEDLINE. To identify clinicallysignificant candidates for mTBI biomarkers, the networkwas then filtered by degree centrality and frequency,greatly reducing the density of concepts and relation-ships. The resulting network produced 17 compounds tobe considered as mTBI biomarkers, both previouslyinvestigated and novel as TBI biomarker candidates. Theinteraction of several of these is discussed as the basisfor a panel of biomarkers to more effectively diagnosemTBI than is currently possible.Availability of data and softwareThe predication data (SemMedDB) is available atskr3.nlm.nih.gov. Degree and frequency filtering javaprograms are available at skr3.nlm.nih.gov/mTBI.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMJC participated in the design of the study; performed the PubMed Searches;queried the SemMedDB database; participated in the graph creation, filtering,and visualization; and drafted the manuscript. MF participated in the design ofthe study and helped to draft the manuscript. HZ participated in the graphcreation, filtering, and visualization and helped to draft the manuscript. TCRparticipated in the design of the study and helped to draft the manuscript. Allauthors read and approved the final manuscript.AcknowledgmentsThis research was supported in part by an appointment to the NationalLibrary of Medicine Research Participation Program administered by the OakRidge Institute for Science and Education through an inter-agency agreementbetween the US Department of Energy and the National Library of Medicine.This study was supported in part by the Intramural Research Program of theNational Institutes of Health, National Library of Medicine.Author details1National Institutes of Health, National Library of Medicine, 38A 9N912A,8600 Rockville Pike, Bethesda, MD 20892, USA. 2Department of MedicalInformatics, China Medical University, Shenyang, Liaoning 110001, China.JOURNAL OFBIOMEDICAL SEMANTICSDoing-Harris et al. Journal of Biomedical Semantics  (2015) 6:15 DOI 10.1186/s13326-015-0011-7SOFTWARE Open AccessAutomated concept and relationship extractionfor the semi-automated ontology management(SEAM) systemKristina Doing-Harris1*, Yarden Livnat2 and Stephane Meystre1AbstractBackground: We develop medical-specialty specific ontologies that contain the settled science and commonterm usage. We leverage current practices in information and relationship extraction to streamline the ontologydevelopment process. Our system combines different text types with information and relationship extractiontechniques in a low overhead modifiable system. Our SEmi-Automated ontology Maintenance (SEAM) systemfeatures a natural language processing pipeline for information extraction. Synonym and hierarchical groups areidentified using corpus-based semantics and lexico-syntactic patterns. The semantic vectors we use are termfrequency by inverse document frequency and context vectors.Clinical documents contain the terms we want in an ontology. They also contain idiosyncratic usage and areunlikely to contain the linguistic constructs associated with synonym and hierarchy identification. By includingboth clinical and biomedical texts, SEAM can recommend terms from those appearing in both document types.The set of recommended terms is then used to filter the synonyms and hierarchical relationships extracted fromthe biomedical corpus.We demonstrate the generality of the system across three use cases: ontologies for acute changes in mental status,Medically Unexplained Syndromes, and echocardiogram summary statements.Results: Across the three uses cases, we held the number of recommended terms relatively constant by changingSEAMs parameters. Experts seem to find more than 300 recommended terms to be overwhelming. The approvalrate of recommended terms increased as the number and specificity of clinical documents in the corpus increased.It was 60% when there were 199 clinical documents that were not specific to the ontology domain and 90% whenthere were 2879 documents very specific to the target domain.We found that fewer than 100 recommended synonym groups were also preferred. Approval rates for synonymrecommendations remained low varying from 43% to 25% as the number of journal articles increased from 19 to47. Overall the number of recommended hierarchical relationships was very low although approval was good. Itvaried between 67% and 31%.Conclusion: SEAM produced a concise list of recommended clinical terms, synonyms and hierarchical relationshipsregardless of medical domain.Keywords: Ontology, Natural language processing, Terminology extraction* Correspondence: kristina.doing-harris@utah.edu1University of Utah, Department of Biomedical Informatics, 421 Wakara Way,Suite 140, Salt Lake City, UT 84112, USAFull list of author information is available at the end of the article© 2015 Doing-Harris et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public DomainDedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,unless otherwise stated.Doing-Harris et al. Journal of Biomedical Semantics  (2015) 6:15 Page 2 of 15IntroductionWe created an ontology development system, SEmi-Au-tomated ontology Maintenance (SEAM) that leveragescurrent practices in information and relationship extrac-tion from text to streamline the process of generatingknowledge structures. The knowledge structures that inter-est us are medical specialty-specific ontologies. We will usethese ontologies for machine assisted clinical diagnostic de-cision support (CDS). CDS requires knowledge structuresrepresenting diagnostic criteria, a method for gathering pa-tient information and the ability to reconcile the gatheredpatient information with the diagnostic knowledge struc-tures. These requirements stem from diagnostic decision-making, which requires knowing at least two things: 1) thecriteria for a diagnosis and 2) if this particular patientmeets those criteria. The goal of the SEAM system is to fa-cilitate the information acquisition necessary to constructontologies that represent the settled science and commonterm usage with respect to either medical specialty or par-ticular disease.BackgroundOur current approach to building diagnostic knowledgestructures is to construct an application ontology of a spe-cific disease or medical specialty. Here application ontol-ogy is used to differentiate them from domain ontologieslike the one described in [1]. Ontology is an arrangementfor defining concepts, the relationships between them,and rules relating to the combining of concepts and rela-tions [2]. Conceptsa are roughly the ideas to whichwords refer (i.e. what the words mean), which is alsocalled semantics. Concepts are often thought of as groupsof semantically equivalent terms (e.g. heart attack, myo-cardial infarction, MI). These equivalences allow an auto-matic system to map terms used in one setting on tothose used in another. Relationships between the termsare required because people often use terms that aresemantically related, but not semantically equivalent, torepresent the same idea. For instance, a clinician may insome situations refer interchangeably to bowels and intes-JOURNAL OFBIOMEDICAL SEMANTICSLeroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 DOI 10.1186/s13326-015-0012-6RESEARCH ARTICLE Open AccessSemantic enrichment of longitudinal clinicalstudy data using the CDISC standards and thesemantic statistics vocabulariesHugo Leroux1* and Laurent Lefort2AbstractBackground: There is an increasing recognition of the need for the data capture phase of clinical studies to beimproved and for more effective sharing of clinical data. The Health Care and Life Sciences community has embracedsemantic technologies to facilitate the integration of health data from electronic health records, clinical studies andpharmaceutical research. This paper explores the integration of clinical study data exchange standards and semanticstatistic vocabularies to deliver clinical data as linked data in a format that is easier to enrich with links tocomplementary data sources and consume by a broad user base.Methods: We propose a Linked Clinical Data Cube (LCDC), which combines the strength of the RDF Data Cube andDDI-RDF vocabulary to enrich clinical data based on the CDISC standards. The CDISC standards provide themechanisms for the data to be standardised, made more accessible and accountable whereas the RDF Data Cube andDDI-RDF vocabularies provide novel approaches to managing large volumes of heterogeneous linked data resources.Results: We validate our approach using a large-scale longitudinal clinical study into neurodegenerative diseases.This dataset, comprising more than 1600 variables clustered in 25 different sub-domains, has been fully convertedinto RDF forming one main data cube and one specialised cube for each sub-domain. One sub-domain, theMedications specialised cube, has been linked to relevant external vocabularies, such as the Australian MedicinesTerminology and the ATC DDD taxonomy and DrugBank terminology. This provides new dimensions on which toquery the data that promote the exploration of drug-drug and drug-disease interactions.Conclusions: This implementation highlights the effectiveness of the association of the semantic statisticsvocabularies for the publication of large heterogeneous data sets as linked data and the integration of the semanticstatistics vocabularies with the CDISC standards. In particular, it demonstrates the potential of the two vocabularies inovercoming the monolithic nature of the underlying model and improving the navigation and querying of the datafrom multiple angles to support richer data analysis of clinical study data. The forecasted benefits are more efficientuse of clinicians time and the potential to facilitate cross-study analysis.Keywords: Ontology, Semantic enrichment, Longitudinal clinical study, RDF data cube, Medication mapping*Correspondence: hugo.leroux@csiro.auEqual contributors1The Australian e-Health Research Centre, Digital Productivity Flagship, CSIRO,Level 5 - UQ Health Sciences Building 901/16, Brisbane, Queensland 4029,AustraliaFull list of author information is available at the end of the article© 2015 Leroux and Lefort; licensee BioMed Central. This is an Open Access article distributed under the terms of the CreativeCommons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andreproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 2 of 14BackgroundIn the last decade, the Health Care and Life Sciences com-munity and pharmaceutical industry have wholeheartedlyadopted [1] clinical study data exchange technologiesbased on XML to capture clinical study data. This islargely due to the recent strategy [2] of the Food andDrug Administration (FDA) in promoting the ClinicalData Interchange Standards Consortium (CDISC) suiteof standards to facilitate data submission and exchange.Furthermore, the move by EU and US regulating bod-ies to open access to clinical data [3,4] will also fosterthe adoption of tools supporting clinical data manage-ment standards, especially those that can easily be linkedto methods and tools developed for Government LinkedData and Linked Science Data.CDISC has developed a set of platform-independentdata standards [5] for the collection and dissemination ofclinical trial data. The CDISC Operational Data Model(ODM) is an XML format that facilitates the exchangeof clinical data captured during a clinical study. ODM-based files contain the study data and the associateddescriptions of the data items, their groupings into CaseReport Forms (CRFs), which are electronic documents torecord the study data, and the associated questions andcode lists. Furthermore, the FDA has mandated the useof other CDISC standards in clinical studies. In particu-lar the CDISC Study Data Tabulation Model (SDTM) isused to facilitate studymetadata submissions and improvethe accountability of the study data. The role of theCDISC Clinical Data Acquisition Standards Harmoniza-tion (CDASH) is to standardise the generation of CRFs forclinical studies. The implementation of the ODM, STDMand CDASH standards in Clinical Data ManagementSystems (CDMS) has enabled larger and more diverselongitudinal clinical research studies and increased thecapability of users to exchange and combine data [6].Challenges relating to the cross-study analysis of clinicalstudy dataA number of limitations relating to the reporting of resultsderived from current clinical trial endeavours were iden-tified by van Valkenhoef et al. [7]. In particular, theystress that: current infrastructure is focused on text-basedreports of single studies, whereas efficient evidence-basedmedicine requires the automated integration of multipleclinical trials from different information resources [7].They specifically advocate for a comprehensive record ofclinical trials to be made available in a machine under-standable format that would improve the efficiency ofevidence-based decision making but more importantlythat decisions could then finally be explicitly linked backto the underlying data. Chief among their list of topicsfor future research directions are: (i) the development ofa comprehensive data model for clinical trials and theiraggregate level results; and (ii) the development of a plat-form to share structured systematic review data sets.Our contribution: semantic enrichmentThis research builds upon existing work [8] to semanti-cally enrich longitudinal clinical study data, based on theCDISC standards, using semantic statistic vocabularies,namely the RDF Data Cube and DDI-RDF vocabularies.We propose a Linked Clinical Data Cube, a set of mod-ular data cubes that helps manage the multi-dimensionaland multi-disciplinary nature of the clinical data. TheRDF Data Cube vocabulary [9] is used to build multi-dimensional data cubes and supports flexible access to thedata via thematic slices. The DDI-RDF Discovery vocab-ulary [10] is effective at encoding the study-specific datadictionary embedded in the CDISC ODM standard aslinked data and helps in managing the link between thedata cube variables and the data.Our objective is to make the data captured within theAustralian, Imaging, Biomarker and Lifestyle study ofAgeing (AIBL) [11] seamlessly available to researcherswho wish to engage in cross-domain analysis of the data.We achieve our goal by semantically enriching the data,when possible, with external data sources. Our approachis four-fold:Phase 1: Integrating the CDISC ODM data model withthe semantic statistic vocabularies. We describe how theclinical data available in CDISC ODM can be mapped tothe RDF Data Cube and DDI-RDF Discovery vocabularyto form the Linked Clinical Data Cube.Phase 2: Splitting the data into modularised cubes. Weoutline the design principles of splitting the data intomoremodularised and manageable groupings to provide alter-native mechanisms for accessing and querying the data.The RDF Data Cube and DDI-RDF vocabularies are piv-otal elements of our slicing strategy and of the URI schemedefined for our implementation.Phase 3: Enriching the LCDC with the CDISC standards.We discuss how useful the benefits of clinical study datato adhering to the CDISC CDASH and SDTM standardsthen elaborate on guidelines to classify the data into thebroad categories.Phase 4: Mapping the data to drug terminologies. Wedemonstrate the utility of the LCDC bymapping the med-ications data derived from the AIBL study to selectedonline drug terminologies.The AIBL studyAIBL is a prospective study of a large group (1112) ofindividuals residing in two Australian cities, Perth andMelbourne, aged over 60 years who are either clas-sified as cognitively healthy, or meet clinical criteriafor mild cognitive impairment or Alzheimers DiseaseLeroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 3 of 14and who have agreed to reassessment every 18 months.Assessment comprises extensive study of cognitive func-tion, neuroimaging, blood biomarkers and lifestyle (dietand exercise) characteristics [11]. By combining theseinvestigations in a prospective fashion, the AIBL studycontributes to understanding the development and pro-gression of Alzheimers Disease through the prodromal,preclinical and clinical stages of the disease [12]. It is vitalfor the clinical data to be reported at regular intervals asthe study progresses. To facilitate this task, the study datais manually entered into the OpenClinica Clinical DataManagement System (CDMS) by study staff [13]. Figure 1describes the AIBL study with the fivemain categories andsub-categories.OpenClinica [14] is an open-source CDMS for collect-ing and managing clinical data. The AIBL study data wassuccessfully migrated to this platform in 2011 [13] and hasbeen live since August 2011. OpenClinica supports thecreation of customisable studies and the design of user-defined Case Report Forms (CRFs) using an Excel spread-sheet and adheres to the CDISC ODM standard. Thedata collected for the AIBL study covers multiple domainsas shown in Figure 1. This dataset comprises more than1600 variables clustered into 25 different sub-domains.The study has been split into five themes: Study, Clinical,Cognitive, Imaging and Lifestyle. The Study theme com-prises administrative information that, for the most part,is not shared within the cube. Table 1 depicts the totalnumber of instances for the various LCDC classes organ-ised by theme. The total number of variables, in thetable, is smaller than 1600 because the generation to RDFsuppresses duplicates.Article outlineIn the remainder of this article, we outline an approachto semantically enrich clinical study data, in particu-lar patient-reported medication usage, and facilitate theirdelivery to clinical researchers. In particular, we outlinehow the use of semantic statistics vocabularies is effectiveat organising the data into a LCDC. We also elaborate onthe approach taken to categorise the AIBL data set intoCDISC CDASH and SDTM domains and the work car-ried out to translate the CDISC standards into RDF. Thisleads into the discussion on the design principles for theLCDC and of the benefits of splitting the data into moremodularised groupings.MethodsThe LCDC [15] comprises one main cube and severalspecialised cubes, one for each domain within the study,that integrates the CDISC ODM data model with the RDFData Cube and DDI-RDF vocabularies. We elaborate fur-ther on the rationale behind this integration below. TheLCDC is designed around a set of cubes, slices, obser-vation groups and observations and these are discussedfurther below. The ability to standardise the clinical datain order to facilitate cross-domain and, possibly, cross-study analysis of the data is one of the salient objectivesof the LCDC. To this end, we describe how the studyvariables have been enriched by the CDISC CDASH andSDTM standards. Aside from providing a standardisedrepresentation to the study variables and grouping themalong the various CDISC categories, this enrichment pro-cess allows for seamless substitution of variable names inthe navigation and querying of the clinical study. Finally,Figure 1 The Australian imaging biomarker and lifestyle study of ageing. Illustrates the logical organisation of the AIBL study. The AIBL study(depicted as a rectangle in light green with thick border) is split into the five domains (depicted as rectangles in light blue). Each domain is furthercategorised into sub-domains depicted by rounded rectangles.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 4 of 14Table 1 Number of instances for the LCDC classes organised by themeTheme Total Obs. Obs. Subject Sub-theme Sub-theme Variablegroup section series sliceClinical 1030430 4495 25210 1416 25 6452 506Cognitive 761650 4612 9826 1415 19 4069 367Imaging 58601 866 2136 365 12 941 59Lifestyle 710594 4026 11953 1415 19 7360 391Study 235566 5384 6218 1414 13 3292 155we outline how the coupling of the study data with exter-nal resources - in this case drug terminologies - can beachieved within the LCDC and we elaborate on our pro-cess to implement a linked medications data set and howthe patient-reported medication intake from the AIBLstudy has been mapped to this data set.Phase 1: Integrating the CDISC ODM data model with thesemantic statistic vocabulariesClinical study data is extracted in CDISC ODM for-mat. The primary dimensions of the CDISC ODMdata model are the Subject and Study Event of interestwithin the study. The additional dimensions, includingthe Study, Form, ItemGroup and Item, depend on thestudy domains and are specified by the data dictionarythat defines the study. The strength of the RDF DataCube is that the original structure of the CDISC ODMdata model (Study-Subject-StudyEvent-Form-ItemGroup-Item) lends itself to be replicated in the generated cubewith relative ease. A further contribution of the RDFData Cube is that it can help overcome the monolithicnature of the ODM data model by facilitating the con-struction of multi-dimensional cubes that offer accesspoints to the data via thematic slices. The LCDC isorganised into one main cube and several specialisedcubes corresponding to the various domains in thestudy.The RDF Data Cube model facilitates the groupingof subsets of observations, within the dataset, wherebyall but one (or a small subset) of the dimensions arefixed. Furthermore, it supports alternative methods ofaccessing the data where the data is aggregated alongother dimensions or along the same dimension in differ-ent order. The DDI-RDF Discovery vocabulary is usedto consistently manage the study-specific data dictionaryexported in CDISC ODM format enriched with CDISCmetadata resources (CDASH and SDTM). These twovocabularies are supplemented by the Vocabulary of Inter-linked Dataset (VoID). These allow the LCDC ontologyto be defined with more generalised classes and proper-ties, such as the disco:Universe, disco:Variableand disco:VariableDefinition [15] as depicted inFigure 2.Phase 2: Splitting the data into modularised cubesThe design of the LCDC is achieved in three steps. Thefirst step involves splitting the dataset into smaller, moremanageable specialised cubes. The second step is to defineseveral slice hierarchies that offer multiple access optionsto the individual data records. The third step is to define aURI scheme that supports access to the cube at all levels ofthe slice hierarchy. These three steps are discussed below.The LCDC defines three categories of slices. The time-series slices address the longitudinal nature of the studyand organise the data into time-intervals and datedand non-dated time points. Cross-section slices adopt asubject-centric approach to the abstraction of the dataalong some important concepts such as gender, genotypeand neurological classifications. The Theme slices cate-gorise the data into the study domains and sub-domains(disco:Universe in DDI-RDF) and help link the mainand specialised cubes together. This process enhances thenavigation and querying of the data in the LCDC becausewe provide three direct links to nodes containing the datainstead of one through the Phase series (at the level ofthe Study Event data in ODM), the Subject section (at theSubject level) and the sub-theme slice (at the Item Grouplevel).The slice hierarchy is provided primarily through theuse of the classes and properties from the RDF DataCube. Figure 3 highlights the LCDC slices that sub-sume qb:Slice. We use the void:subset propertyto describe the link between the main and specialisedcubes. Links between slices and observations are specifiedusing the qb:observation property, while the onesbetween slices and observation groups are represented byqb:observationGroup. The specialisedSeriesand specialisedSection properties manage thelinks between the slices in the main and specialised cubes.The specialisedObservation property, which is asub-property of qb:observation, handles the linksbetween the observation groups from the main cube tocorresponding observations in the specialised cubes.The URI scheme describing the LCDC follows theconvention from the Linked Data API [16], which usesURIs ending with an identifier to provide access to asingle instance (Item endpoint) and URIs ending withLeroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 5 of 14Figure 2 Mapping the CDISC ODMmodel to the data cube and DDI vocabularies. Illustrates how the original CDISC ODMmodel (depicted byrectangles in light gray) is overlaid with the RDF Data Cube (depicted by ellipses in green) and the DDI-RDF vocabularies (depicted by roundedrectangles in blue). The Data section, depicted on the left of the model, comprises a hierarchical structure whereby each level is fully containedwithin the preceding level. As the left side is more about structuring the clinical data, the Data section of the CDISC ODMmodel is more closelyrelated to qb. The Clinical Data node is mapped to qb:Dataset while qb:Slice is used to split the Subject, Study Event andForm data nodes across the ODM hierarchy into slices, and the Item Data node is mapped to qb:Observation. The ODM node refers to theentire data set and is mapped to disco:LogicalDataset. The right side comprises the metadata section, which contains one Study node,which further comprises one MetaData node. The MetaData node contains a number of StudyEventDef, FormDef, ItemGroupDef andItemDef nodes, one corresponding to each of the Subject, Study Event, Form, Item Group and Item data nodes defined in the Datasection. The Metadata section shows how the variable definitions managed through discomatches ODMs ItemDef while the grouping ofvariables via disco:Universe is applied at the FormDef level. Finally, Item Data is logically mapped to disco:Variable.a keyword to provide access to a list of instances (Listendpoint).Phase 3: Enriching the LCDC with the CDISC standardsThe CDISC CDASH and SDTM standards provide themeans to standardise the clinical data. Despite not beingdesigned around the CDISC standards, there is a goodoverlap between the AIBL study and the CDISC CDASHand SDTM standards for categories such as Vital Signs,Blood (represented by Laboratory Test in CDASH) andMedical History. For some categories within AIBL, thestudy data is clustered across many classes that do notnecessarily fit to single CDASH or SDTM categories. Wehave chosen to map our medication data to the Concomi-tant Medications (CM) class within CDASH. RegardingCM, the approach taken by CDISC is to provide a frame-work and allow the users the ability to define the terminol-ogy of their choice. The AIBL Demographics data can bemapped to the CDISC Demographics and Subject Char-acteristics categories. SDTMs Trial Arms, Trial Summary,Trial Visits and Subject Visits categories are appropriatetargets for mapping longitudinal aspects of the study. Fordata items that are based on questionnaires, the method-ology adopted by CDISC is to guide the user by providinga Questionnaire Supplements (QS) template that the usercan mould to their needs. The SDTM standard providesapproximately 50 questionnaires within theQSmodel thatthe user can use to model their study. The relatively lownumber of publicly available questionnaires is due to thefact that many of the questionnaires in clinical studies arelicensed.We have coupled the AIBL-specific variables to existingCDISC concepts, when possible, to allow a straightfor-ward swap of variable names in a query. For exam-ple, the AIBL property for systolic blood Pressure(aiblvitalsigns:systolicBP) has been linked tothe CDISC Vital Sign concept (cdiscvs:systolicB-loodPressure).Phase 4: Mapping the data to drug terminologiesIn addition to the direct coupling between AIBL andCDISC definitions described above, we have mappedthe patient-reported medication intake of the AIBL par-ticipants to three external terminologies: AMT, ATCDDDa and DrugBank. Our goal is to provide multiplelinks to hierarchical classifications of drugs. AMT pro-vides unique codes and accurate standardised names tounambiguously identify all commonly used medicines inAustralia with eight key top-level concepts [17]. We aug-ment AMTs capabilities with links to ARTGb and UNIIc.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 6 of 14Figure 3 Linked clinical data cube architecture aligned with the RDF data cube. Depicts the architecture of the LCDC. The Main cube (depicted asa red cube) is split into modular Specialised cubes (depicted as a blue cube) and linked using the void:subset property. The Main cube isorganised into time-series, cross-section and theme slices using the qb:slice property. The slices are then divided intoObservations using the qb:observation property. The qb:dataset property is used to link the observations back to the cube. TheSpecialised cubes are organised similarly to the Main cube with the exception of the theme slices. The dotted lines show how the slices fromall cubes interlink to the study observations through the use of ObservationGroups and the qb:observationGroup property. ThemainObservation property manages the link between the ObservationGroups and the Observations while thespecialisedObservation property handles the link between the ObservationGroups in the main cube and the correspondingObservations in the specialised cubes.ARTG contains the most comprehensive list of brandnames (Trade Product) in Australia, while UNII providesa non-proprietary, unambiguous and unique list of sub-stances as maintained by the FDA. DrugBank providesa rich taxonomy of drug information alongside compre-hensive drug, gene and food interactions. The appeal forour project is in the exploration of drug-drug interac-tions that provide some insight into the potential risks andcontraindications associated with the intake of the medi-cation. Furthermore, by exploiting the gene-drug interac-tions of medication targets, we can extend our frameworkto support the discovery of biomarkers. Finally, the avail-ability of the food interactions will be useful when weexplore the association between the participants drugintake and type and amount of food consumed. Both ATCDDD and DrugBank provide a supplementary means toquery the data. The five-level ATC DDD taxonomy ofmedications provides an additional mechanism for thedata to be categorised and offers the means to aggregatethe study data for statistical purposes. This is complemen-tary to what is possible with the help of the vocabulariesprovided by AMT.Medication mapping is challenging due to the quality,accuracy and completeness of the information. Previousstudies [8,18] have identified numerous inconsistencieslinked to the naming of the medications with a mix oftrade name, active ingredients and informal name used todescribe the prescribed medications.The processing pipeline for mapping the medicationsdata to the selected medication terminologies is sum-marised below. The medication records are extractedfrom OpenClinica, at the start of the pipeline, as an XMLdocument in CDISC ODM format. A data cleaning pro-cess is conducted to manually address the inconsistenciesdescribed above. This is followed by four mapping phases.In Phase 1, we attempt a map of the cleaned medica-tion names to the Trade Productd (TP) concept in AMT.We use the list of brand names compiled by ARTG toassist us in this task. In Phase 2, we try to map the samemedications to the Medicinal Producte (MP) concept inAMT.We use the DrugBank terms to boost the number ofmapped concepts. The third phase attempts a map to thesubstances (active ingredients) either entered by the par-ticipants or contained within the medications recorded.To this end, we use the list provided by UNII or theMedicinal Substancef (MS) defined in AMT. In Phase 4,we map the medications to the ATC DDD classificationhierarchy by taking advantage of the existing mappingbetween the various terminologies (e.g. DrugBank andATC DDD). We have thus compiled a linked medicationsLeroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 7 of 14data set that links AMT, DrugBank, ARTG, ATCDDD andUNII with one another as depicted in Figure 4.ResultsThe result of mapping the AIBL medications data tothe medication terminologies is illustrated in Table 2.The first row discloses the total number of medicationsextracted. The second row represents the mappings toeither a Medicinal Product, a Trade Product or a Sub-stance in AMT. The third, fourth and fifth rows providethe mapping count for these AMT concepts individually.The Linked Clinical Data Cube has been evaluated usingthe full AIBL data set to demonstrate its potential informulating queries across the broad spectrum of testsand the categories within the clinical study. While simplequeries can be answered using a single data cube, morecomplex queries need data from several cubes to be avail-able. The clinical data is formalised into RDF prior tobeing loaded in a Virtuoso triple-store.SPARQL QueriesTo demonstrate the utility of the LCDC, we have devised aset of three questions that are typical of the questions thatthe AIBL researchers are likely to ask of the study data.We provide, below, a listing of the three queries. However,due to privacy constraints, we have structured our queriesso that they only return aggregated counts because we areunable to present the participants unique identifier as partof the results of the queries.Those SPARQL queries have been chosen in order todemonstrate the breadth and depth of questions that maybe asked on the data set. They demonstrate how data fromTable 2 Medications mapping statisticsMapped Count PercentageTotal 7942 100.00%Medicinal product/trade product/substance 5536 69.71%Trade product 5518 69.48%Medicinal product 5266 66.31%Substance 5382 67.77%the AIBL study can be effortlessly combined with druginformation, for example, in order to facilitate queries thatanswer questions based on drug classifications. Further-more, we also demonstrate, through the integration of theAIBL data set with terminologies from the CDISC stan-dards, how the AIBL data set can be queried by using theCDISC standardised terminology rather than the actualtest names used by the AIBL study. We believe that thesetypes of queries will drive the cross-study and cross-domain benefits of the linked clinical data approachessuch as the LCDC.Query 1: Using CDISC terms, find the number of participantswho have hypertensionHypertension is defined as having systolic and diastolicblood pressure readings above 140 and 90 respectively(written as 140/90 mm Hg) [19], most of the time. Thisquery explores the use of the CDISC SDTM controlledterminology to access the diastolic and systolic bloodpressure readings for participants in the AIBL study. Itallows the user to interchangeably use the variable namefrom the AIBL study or from CDISC SDTM.Figure 4 Linked Australian medications data set. Depicts the interlinking of the drug terminologies available, mostly, in Australia in order tofacilitate their navigation. For the sake of simplicity, all data item variables have been omitted from the Figure. The AMT concepts are depicted inteal. The ATC DDD concepts are depicted in orange. UNII concepts are in light-blue while DrugBank concept is in light green and the ARTGconcept is in magenta. The Figure also introduces an xkos:ConceptAssociation predicate (depicted in yellow) to define many-to-manyrelationships between amt:MedicinalProduct and artg:RegisteredMedicine concepts.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 8 of 14PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX lcdcobs: <http://purl.org/sstats/lcdc/def/obs#>PREFIX cdiscvs: <http://purl.org/odm2rdf/sdtm/vs/def/cdiscvs#>PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>SELECT count(DISTINCT ?subject) as ?hypertension WHERE {# find the AIBL variable corresponding to cdiscvs:systolicBP?vs_sBP rdfs:subPropertyOf cdiscvs:systolicBloodPressure .# find the AIBL variable corresponding to cdiscvs:diastolicBP?vs_dBP rdfs:subPropertyOf cdiscvs:diastolicBloodPressure .?observation rdf:type lcdcobs:Observation .# get the observation for one AIBL subject?observation lcdccore:subject ?subject .# get the ?sysBP using cdisc vs alias?observation ?vs_sBP ?sysBP .?observation ?vs_dBP ?diasBP .FILTER ((xsd:integer(?sysBP) > 140) && (xsd:integer(?diasBP) > 90) )}The query obtains the relevant test names from the ontology by performing a lookup of properties that are sub-properties of the CDISC Vital Signs (prefix: vs) diastolic and systolic blood pressure variables. This is achieved by thisstatement:?vs_dBP rdfs:subPropertyOf cdiscvs:diastolicBloodPressure .This query is possible because we have implemented a linked set that connects the variable name from the AIBL studyto the standardised terminology in CDISC SDTM vs domain as illustrated below.aiblvitalsigns:diastolicBPrdf:type owl:DatatypeProperty ;rdfs:subPropertyOf cdiscvs:diastolicBloodPressure.aiblvitalsigns:systolicBPrdf:type owl:DatatypeProperty;rdfs:subPropertyOf cdiscvs:systolicBloodPressure.We believe that the use of linksets in this manner is important and useful because it adheres to the principles ofinformation hiding in that the user need not be aware of the exact wording of a variable. As long as the user knows thecorresponding standardised variable name, the user is able to successfully execute a query on the data set. We intendto further develop this traceability mechanism with the help of the Provenance Ontology [20] to fully disclose how thepublished data is derived from the originally captured data.The result of Query 1 is displayed below:hypertension242Query 2: Howmany participants are taking an anti-diabetic drug such asMetformin?Some studies [21,22] have shown a possible link between type2 diabetes and early-stage AD. In this query, we retrievea list of anti-diabetic drugs to demonstrate the benefits of linking the patient-reported medications to standardisedexternal terminologies and the strength of the LCDC in using federated queries to facilitate cross-domain querying. Thefirst portion of this query obtains a list of anti-diabetic drugs from DrugBank (outlined in section A in the SPARQL).The second part of the query utilises the mappings between the patient-reported medications and DrugBank entities tolink to the anti-diabetic drugs identified in section A.PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX drugbank: <http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugbank/>PREFIX aiblmed: <http://aehrc-ci.it.csiro.au/aibl/lcdc/clinical/medication/def/aibl-medication#>Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 9 of 14PREFIX lcdcobs: <http://purl.org/sstats/lcdc/def/obs#>PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>PREFIX cm: <http://purl.org/sstats/lcdc/cm/def/cm#>PREFIX amt: <http://nehta.gov/amt#>SELECT count (distinct ?subject) as ?count ?mp_medWHERE {# Section A. find all instances of anti-diabetic drugsSERVICE <http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql> {# find the drug (?s) that has the name Metformin?s drugbank:genericName "Metformin" .# find the category of the drug (?s)?s drugbank:drugCategory ?category .# find all other instances of ?drug that has the same ?category?drug drugbank:drugCategory ?category .}# Section B. find the participants who take the ?drug from A.{ SELECT distinct ?drug ?med ?subject ?mp_med WHERE {GRAPH <http://localhost/dataset/aibl/lcdc/clinical> {# specifies that ?obs is an observation?obs a lcdcobs:Observation .# get the medicinal product code for this med?obs cm:medicinalProduct ?cm_mp .# lookup the drugbank entity linked to this MP?cm_mp skos:exactMatch ?drug .# get this drugs name?cm_mp amt:synonym ?mp_med .# find the participant associated with this observation?obs lcdccore:subject ?subject .}} }}LIMIT 20The linkset developed to map the AMT concepts to DrugBank has been inspired from the approach described in[23,24] and uses the skos:exactMatch predicate.<http://snomedtools.info/snomed/version/1/concept/rdfs/105271000036100>rdf:type amt:MedicinalProduct ,owl:NamedIndividual ;skos:exactMatch<http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugs/DB06655>.The significance of this mapping is the provision of drug-drug, drug-gene and possibly drug-disease and gene-geneinformation relating to the AIBL study to the researchers by fully utilising the links provided by DrugBank.The result of Query 2 is displayed in Table 3:Query 3: Are there participants whose classification has transitioned from healthy tomild cognitive impairment but whosetriglycerides level has remained normal?Research has investigated the risk factors associated with low-density lipoproteins or triglycerides on the incidence andprogression of dementia and AD in later life [22]. With this in mind, we construct the query below to retrieve par-ticipants records whose confirmed classification status have been updated from being healthy as subjective memorycomplainer or non-memory complainer to having mild cognitive impairment but who have also maintained a normal(< 1.7 mmol/L) level of triglycerides in their blood sample over the course of an 18-month period between thebaseline and 18-month time-points.PREFIX qb: <http://purl.org/linked-data/cube#>PREFIX lcdcsection: <http://purl.org/sstats/lcdc/def/cross-section#>PREFIX lcdccore: <http://purl.org/sstats/lcdc/def/core#>PREFIX aiblblood: <http://aehrc-ci.it.csiro.au/aibl/lcdc/clinical/blood/def/aibl-blood#>Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 10 of 14PREFIX aiblneuropsych: <http://aehrc-ci.it.csiro.au/aibl/lcdc/cognitive/neuropsych/def/aibl-neuropsych#>PREFIX aiblphase: <http://aehrc-ci.it.csiro.au/dataset/aibl/lcdc/id/phase/>PREFIX aiblsubtheme: <http://aehrc-ci.it.csiro.au/dataset/aibl/lcdc/id/subtheme/>SELECT DISTINCT count(?subject) as ?subjectCount ?class1 ?class2WHERE {{ select distinct ?subject ?obs1a ?trig1 where {# retrieve the objects from the SubTheme cross-section slice?nodeSect a lcdcsection:SubThemeSection .# only get observations for the participants from the Blood domain?nodeSect lcdcsection:subtheme aiblsubtheme:blood .# get the observations from the slice?nodeSect qb:observation ?obs1a .# once we get the observations, get the subject?obs1a lcdccore:subject ?subject .# only select observations for the baseline phase?obs1a lcdccore:phase aiblphase:baseline .# get the triglycerides measurements?obs1a aiblblood:trig ?trig1 .} }{ select distinct ?subject ?class1 ?obs1b where {?nodeSect a lcdcsection:SubThemeSection .# only get observations for the participants from the Neuropsych domain?nodeSect lcdcsection:subtheme aiblsubtheme:neuropsych .?nodeSect qb:observation ?obs1b .?obs1b lcdccore:subject ?subject .?obs1b lcdccore:phase aiblphase:baseline .# get the subjects classifications?obs1b aiblneuropsych:confirmedClassification ?class1 .# only select healthy subjectsFILTER(?class1 =aiblneuropsych:ConfirmClassification360-memoryComplainerHealthyControl|| ?class1 =aiblneuropsych:ConfirmClassification360-nonMemoryComplainerHealthyControl)} }FILTER (xsd:float(?trig1) < 1.7){ select distinct ?subject ?obs2a ?trig2 where {?nodeSect a lcdcsection:SubThemeSection .?nodeSect lcdcsection:subtheme aiblsubtheme:blood .?nodeSect qb:observation ?obs2a .?obs2a lcdccore:subject ?subject .# select observations for the 18-month phase?obs2a lcdccore:phase aiblphase:18months .?obs2a aiblblood:trig ?trig2 .} }{ select distinct ?subject ?class2 ?obs2b where {?nodeSect a lcdcsection:SubThemeSection .?nodeSect lcdcsection:subtheme aiblsubtheme:neuropsych .?nodeSect qb:observation ?obs2b .?obs2b lcdccore:subject ?subject .?obs2b lcdccore:phase aiblphase:18months .?obs2b aiblneuropsych:confirmedClassification ?class2 .# ensure subject have transitioned to MCIFILTER(?class2 = aiblneuropsych:ConfirmClassification360-mciPatient )} }# normal range for trig. is < 1.7 mmol/LFILTER (xsd:float(?trig2) < 1.7)}The above query highlights the strength of the LCDC in facilitating cross-domain queries by fully exploiting thepotential of slices and observations within the specialised cubes. While the above query can be achieved without a datacube, the use of slices and observations make the query more elegant and effective. It demonstrates the navigation of theAIBL data set across two specialised cubes (Neuropsych and Blood) and four slices (two slices at each time points foreach cube). These are contained within the four observations (?obs1a, ?obs1b, ?obs2a, ?obs2b) within the abovequery.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 11 of 14Table 3 Participants taking anti-diabetic drugsCount mp_med1 Insulin glargine3 Glimepiride46 Metformin4 Rosiglitazone2 Glipizide17 Gliclazide4 Pioglitazone1 SitagliptinThe result of Query 3 is displayed in Table 4:We provide an indication of the execution time of thethree queries in Table 5 below. These queries have beenexecuted on a Virtuoso 6.1 instance running on a vir-tual machine with an AMDOpteron Processor 62xx CPU,8GB of DDR3 RAM and running Ubuntu 13.04 LTS(Raring Ringtail).Discussion and related workOur results demonstrate the effectiveness of integratingsemantic statistics vocabularies with the CDISC standardsin order to expedite the navigation and querying ofthe data. Our contribution extends previous attemptsto semantically enrich biomedical research data usingontologies [25] or linked data resources [26]. To the bestof our knowledge, no study has yet investigated the associ-ation of semantic statistics vocabularies with clinical dataexchange standards. The design of the LCDCwas inspiredby the Translational Medicine Ontology [27] and our usecases were motivated by similar objectives of providingqualitative and pertinent clinical data to the researchersand clinicians in the right format. This is what has drivenour resolve to split the data into onemain cube and severalspecialised cubes corresponding to the various domains inour study. The benefits of this approach are demonstratedin the third query where data from two specialised cubesare amalgamated to derive the results.Observational clinical study data is patchy by nature,mainly because of the various collection mechanismsinvolved that often lead to information being inadver-tently left out or inaccurately recorded. Furthermore, thesheer volume of variables and the longitudinal nature ofthe AIBL clinical study have given rise to an enormous vol-ume of data that need to be analysed. This has led to thesecond design decision that is to split the data into time-series, cross-sections and themes in order to improve theirmanageability during the generation process and facili-tate their discovery and usability by end users. Moreover,the addition of external standardised terminologies, suchas the CDISC standard terminologies and the variousdrug vocabularies utilised, have contributed not only tostandardising the data and to removing ambiguities butto enriching the data by providing links to relevant onlineresources, such as genes and pathways definitions andinformation about their interactions with the entities.Challenges in the use of the CDISC standards as theunderlying modelWhile the CDISC models suit our immediate purpose,they present a few shortcomings, mainly in relation tothe semantics associated with the clinical study data.ODMs constrained hierarchical structure largely pro-motes single-study explorations of clinical study data.Furthermore, the inability to store domain informationalongside the user-defined data items in the customisableCRFs is, in our view, very restrictive, thus impeding theiruse outside of the study context [28]. However, this stemsmore from the various failings in the implementation ofthe CDISC standards by the vendors. The ODM stan-dard allows for CDASH terms to be inserted through theuse of annotations within the ODM XML model. How-ever, several vendors, such as OpenClinica, choose not tooffer this feature natively within their tool. Abler et al. [28]make a passionate claim for the definition of a language offorms that can effectively record the logical relationshipsbetween questions or sets of questions asked in the forms.On a more technical aspect, ODM also suffers froma lack of established complex data type standards, thusallowing a study coordinator to provide an alternative def-inition for, say, the Physical Quantity data type. Further-more, despite the provision of detailed ImplementationGuides describing the correct way of encoding data items,the definition of very coarsely granulated meta-data cat-egories, such as Medical History in SDTM, opens up thepossibility, for the user, to capture semantically identicaldata in multiple domains. While the lack of data stan-dards is a problem, the lack of mechanisms to enforceadherence to these data standards is a greater problem. Assuch, despite CDISC providing mechanisms, through itsSDTM and CDASHmodels, to define common semantics,in our experience, very few study coordinators choose touse them.Our choice of the CDISC standard as the underlyingmodel for our architecture is influenced by three factors:(i) since the FDA and other regulatory bodies mandate theuse of CDISC as the de facto standards for representingand reporting clinical study data, a vast majority of theclinical study data that we encounter is already in CDISCformat; (ii) several extensions to the CDISC standards(such as the Therapeutic Area standard for AlzheimersDisease) are appealing to us; and (iii) we have not yet founda consistent and complete set of ontologies that we coulduse instead.In our approach to semantically enrich the clinical studydata, we need to address the study-specific nature ofLeroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 12 of 14Table 4 Participants classifications and triglycerides levelSubjectCount Class1 Class211 ConfirmClassification360- ConfirmClassification360-memoryComplainerHealthyControl mciPatient4 ConfirmClassification360- ConfirmClassification360-nonMemoryComplainerHealthy mciPatientControlCDISC ODM datasets. We inherit many issues that havebeen created in the previous steps of the data capturechain, such as the use of user-defined questionnaires andinstruments that use their own language and the loss ofdomain knowledge during the digitisation phase of thedata. Our solution is to reintroduce the loss of domain-specific information by first trying to retrofit the studyvariables to the SDTM and CDASH models, even thoughthey were not initially modelled that way. Concurrently,we look to biomedical ontologies, such as the NCBOBioportal ontologies [29] and SNOMED CTg, to providealternative foundations for domain enrichment of the dataset. Several ontologies, in the context of clinical trials[30-32], have been proposed recently and are partiallyapplicable to our needs. However, they do not adequatelycover the observational aspects that are required for ourdata cubes. Furthermore, several of these ontologies havea large number of dependencies to other ontologies thatdo not meet our requirements. We overcome the limita-tions related to the single-study nature of ODM by fittingthe study data to the RDF Data Cube. The introductionof additional dimensions, through the integration to theRDF Data Cube, opens up new access points to the datathrough the use of the thematic slices.Ultimately, our view is that regulatory bodies have apivotal role to play in encouraging the clinical study coor-dinators to engage with data scientists at an earlier stagein their clinical study to help with the design of theirstudy and associated artefacts. Too much emphasis isplaced on the data collection phase and not enough effortis expended in clarifying what is needed to analyse thedata.Related workThe Linked Open Drug Data (LODD) [33] and the LinkedLife Data (LLD) [34] projects provide additional resourcesTable 5 Query performancesQuery Execution time (msec)1 222 363 270that can be used to extend the Linked Clinical Data Cube.Both projects aim to build a large scale knowledge cloudthat can be used for drug discovery. LODD federates theefforts by participants of the W3C Semantic Web HealthCare and Life Sciences (HCLS) Interest group to con-vert available resources into linked data. LLD providesa semantic data integration platform for the biomedicaldomain comprising many of the data sources belongingto LODD. The resulting datasets contains more than 8million triples representing the knowledge within over 2millions links relating to medications, diseases, clinicaltrials, gene information and pharmaceutical companiesamong others. This was followed by efforts to convertthe ChEMBL database as linked open data [23]. This newlinked dataset combines the description of the biologi-cal entities with links to Bio2RDF [35], ChemSpider [36],OpenMoleculesRDF [37] and CrossRef [38] to allow deref-erenceable access to a myriad of external datasets. Wehave adopted a similar methodology in our approach tomap the medications specialised cube to AMT, DrugBankand ATC DDD.Among the various use cases reported via the W3CHCLS Interest group are efforts to explore links to iden-tify and verify genes linked to Alzheimers disease (AD).Through the links between the drug, medications, dis-ease and clinical trial repositories, we hope to leverageon efforts by others to further explore the effects of pre-scribed medications, for AD sufferers, on the variousgenes comprising the pathways of interest. Other applica-tions of LODD include the identification of potential side-effects linked to the intake of drugs that have conflictingstimuli on the disease pathways.The SALUS project [39] is a former attempt to adaptCDISC standards to build a Semantic Framework toimprove interoperability between clinical research andclinical care domains. We adopt a similar approach tothem but their focus is on service mappings rather thanlinked data sets. The Semantic Cockpit [40] project aimsto develop a data slicing framework comparable to whatwe propose on the basis of the RDF Data Cube. Thegoal of this project is to intelligently assist businessanalysts by discriminating unimportant information andusing reasoning to only present useful information to theanalyst.Leroux and Lefort Journal of Biomedical Semantics  (2015) 6:16 Page 13 of 14The Linked Medical Data Access Control (LiMDAC)project [41] has devised a framework to enable the inte-gration of medical data without compromising its privacy,security and integrity. It defines three linked data mod-els that use the RDF Data Cube to build an access controlframework that restricts access to the aggregated data.The Pharmaceutical Users Software Exchange [42] com-munity, in concert with the FDA, has started work on RDFrepresentations of various CDISC models [43], includ-ing the terminologies published by the National CancerInstitute (NCI) Enterprise Vocabulary Services [44]. Thiscommunity has started to evaluate the RDF Data Cube[45,46] for the publication of clinical study data. Theseconversions of comma-separated-value files, however, donot fully exploit the relationships between the data andmetadata structures embedded within the XML versionsof the CDISC standards and the patterns and conceptdefinitions included in the generated RDF content.ConclusionsThis paper has outlined the semantic enrichment of longi-tudinal clinical study data based on the CDISC standardswith elements from the semantic statistics vocabularies,namely the RDF Data Cube and the DDI-RDF Discov-ery vocabularies. We have outlined how the Health Careand Life Science community is likely to benefit from theadoption of tools and techniques that will deliver clini-cal data as linked data and advance its integration withcomplementary data sources. In this regard, we haveproposed a Linked Clinical Data Cube, which integratesone main and several specialised data cubes to provideincreased flexibility in the navigation of the clinical dataand allow the users to formulate the queries more effi-ciently and effectively. The Linked Clinical Data Cubecombines the strength of the RDF Data Cube in definingmulti-dimensional data cubes and the DDI-RDF Discov-ery vocabulary in encoding the CDISC metadata andthe study specific data dictionary as linked data. Ourapproach was validated using data captured as part ofa longitudinal clinical study into neurodegenerative dis-eases. This research has resulted in four contributions.First, we have uncovered the complementarities of theRDF Data Cube and DDI-RDF Discovery vocabularies forthe publication of large heterogeneous data sets as linkeddata. Second, we have demonstrated the fit of the seman-tic statistics vocabularies to enrich the CDISC ODM datamodel for the publication of clinical study data as linkeddata. Third, we have illustrated how the clinical studydata has been semantically enriched with links to externalresources and how they ultimately improve the navigationand querying of the data. Fourth, we have built the foun-dations of a framework supporting cross-domains andcross-study analysis by adopting a more standardised datastructure. Our next step is to enrich the remaining studydata set with concepts from other domain ontologies,such as Blood, Neuropsychological tests and Nutrition, toname just three.EndnotesaAnatomical Therapeutic Chemical Defined Daily Dose.bAustralian Register of Therapeutic Goods.cUnique Ingredient Identifier.d30560011000036108 | trade product |.e30497011000036103 | medicinalproduct|.f30388011000036105 | medicinalsubstance |.gSystematized Nomenclature of Medicine ClinicalTerms.Competing interestsThe authors declare that they have no competing interests.Authors contributionsHL drafted the manuscript. HL and LL modelled the LCDC that LLimplemented. LL revised the manuscript. Both authors read and approved thefinal manuscript.AcknowledgementsThe authors would like to express their gratitude to Drs Alejandro Metke andMichael Lawley for their assistance in scoping the Medications case study andalong with Dr Bevan Koopman and Mr Simon McBride for reviewing the paperand to Mr Simon Gibson and Mr Louis Delachat for their assistance in theproject.Author details1The Australian e-Health Research Centre, Digital Productivity Flagship, CSIRO,Level 5 - UQ Health Sciences Building 901/16, Brisbane, Queensland 4029,Australia. 2Digital Economy Program, Digital Productivity Flagship, CSIRO,Canberra, ACT 2601, Australia.Received: 8 August 2014 Accepted: 5 March 2015JOURNAL OFBIOMEDICAL SEMANTICSSmith and Eppig Journal of Biomedical Semantics  (2015) 6:11 DOI 10.1186/s13326-015-0009-1RESEARCH ARTICLE Open AccessExpanding the mammalian phenotype ontologyto support automated exchange of highthroughput mouse phenotyping data generatedby large-scale mouse knockout screensCynthia L Smith and Janan T Eppig*AbstractBackground: A vast array of data is about to emerge from the large scale high-throughput mouse knockoutphenotyping projects worldwide. It is critical that this information is captured in a standardized manner, madeaccessible, and is fully integrated with other phenotype data sets for comprehensive querying and analysis acrossall phenotype data types. The volume of data generated by the high-throughput phenotyping screens is expectedto grow exponentially, thus, automated methods and standards to exchange phenotype data are required.Results: The IMPC (International Mouse Phenotyping Consortium) is using the Mammalian Phenotype (MP)ontology in the automated annotation of phenodeviant data from high throughput phenotyping screens. 287new term additions with additional hierarchy revisions were made in multiple branches of the MP ontology toaccurately describe the results generated by these high throughput screens.Conclusions: Because these large scale phenotyping data sets will be reported using the MP as the common datastandard for annotation and data exchange, automated importation of these data to MGI (Mouse Genome Informatics)and other resources is possible without curatorial effort. Maximum biomedical value of these mutant mice will comefrom integrating primary high-throughput phenotyping data with secondary, comprehensive phenotypic analysescombined with published phenotype details on these and related mutants at MGI and other resources.Keywords: Phenotype, Ontology, Mouse, Data integration, DatabaseBackgroundThe accessibility of the mouse genome to genetic ma-nipulation, biochemical and molecular experimentation,and the availability of its full genomic sequence hasmade the mouse indispensable in modeling human dis-eases and complex syndromes arising from various eti-ologies. A myriad of approaches have been taken tocreate mutations in the mouse genome that mimic thosein human disorders. Forward genetics mutagenesis pro-jects using various inducers (e.g., ENU, transposons)have been and continue to be executed (Mutagenetix,Australian Phenome Bank, etc. (reviewed in [1]). Manyof these screens are designed to look for deviants in one* Correspondence: janan.eppig@jax.orgMouse Genome Informatics, The Jackson Laboratory, Bar Harbor, ME 04609,USA© 2015 Smith and Eppig; licensee BioMed CenCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.or two specific phenotype areas, such as congenital heartdefects or neurobehavioral abnormalities. Once a pheno-deviant is identified, mapping or sequencing studies aidin identifying the molecular mutation. More recently,large-scale gene targeted knockout screens have beendesigned to analyze the phenotypic consequences of mu-tating each protein-coding gene in mouse (InternationalMouse Phenotyping Consortium, IMPC) [2]. Unlike pre-vious induced mutation screens, these phenotyping pipe-lines are designed to systematically screen every mutantmouse line for defects in a wide array of physiologicalsystems. Because the gene mutation is already identified,these phenotype data can be integrated immediately withother information known about the genes function, ex-pression and biological pathways.tral. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Table 1 MP terms assigned to IMPC parameters, bysystemsSystem Terms assigned New termsAdipose tissue 6 3Behavior/neurological 85 15Cardiovascular system 59 9Craniofacial 39 1Digestive/alimentary 6 10Embryogenesis 3 0Endocrine/exocrine gland 13 3Growth/size/body 16 3Hearing/vestibular/ear 18 3Hematopoietic system 82 25Homeostasis/metabolism 216 129Immune system 67 24Integument 55 9Limbs/digits/tail 42 6Liver/biliary system 1 1Mortality/aging 10 7Muscle 5 0Nervous system 5 4Pigmentation 13 0Renal/urinary system 6 2Reproductive system 25 4Respiratory system 9 4Skeleton 70 11Taste/olfaction 1 0Vision/eye 55 14MP terms used in annotations in postnatal tests in IMPC as of 10/10/2014.Note: the total number in the second column is more than 752; this is due toterms assigned to multiple systems, such as abnormal testis morphology[MP:0001146], which occurs in both the endocrine/exocrine gland andreproductive systems headings. Some new terms were added during theEurophenome and Sanger Institutes Mouse Resource Portal pilot phenotypingprojects; others were added recently to describe IMPC pipeline parameters.Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 2 of 7The Mammalian Phenotype (MP) ontology [3] is a con-trolled vocabulary that has been used at Mouse GenomeInformatics (MGI) to annotate phenotype data from large-scale data sets, including mouse mutagenesis screens, andfrom data described in published literature. The MPontology was first developed by iterative additions as cura-tors required terms to describe published and importedphenotype data sets, then later by additions and improve-ments made via specific review with subject matter ex-perts covering targeted areas of the ontology. Recently, weundertook to add and revise many areas of the ontologysimultaneously to accommodate consistent reporting fromhigh-throughput data pipelines and support automateddata exchange with the IMPC, MGI and other resources.MethodsOntology editing and filesThe Mammalian Phenotype Ontology in OWL format ismaintained and edited using Protégé-4.3 software.Ontology files are available in OWL and converted OBOformats from the MGI ftp site [4].Retrieval of MGI dataData in MGI version 5.20 were retrieved from the publicwebsite update posted on 10/21/2014 at MGI [5] or viaMouseMine [6].Results and discussionExpanding and using the mammalian phenotype ontologyto annotate high-throughput mouse phenotype dataMP is used as a data standard to annotate published andlarge scale mouse phenotype data sets [1]. MGI and theRat Genome Database [7] incorporate this tool to aid inorganizing, and analyzing data sets. Unlike other previ-ously imported phenotype data sets to MGI, which re-quired curator intervention to annotate or translate tothe MP ontology standard, the high throughput mousephenotyping pilot projects such as Europhenome [8] andthe Sanger Mouse Genetics Project (MGP) [9] are usingthe MP to annotate data sets directly and the IMPC alsohas adopted this standard [10]. These large-scale pheno-typing projects use a standard series of phenotyping pa-rameters called pipelines (described in detail at IMPC/IMPReSS Pipelines [11]). The IMPC core phenotypingpipeline includes the minimum required phenotype pa-rameters that have been agreed by all IMPC participatingresearch groups. A minimum of seven male and seven fe-male mice at ages of 916 weeks are subjected to a batteryof mandatory tests with some centers performing addedoptional tests. Performing these tests and reporting result-ing phenotype data in a standardized way allows data tobe compared and shared not only among mouse pheno-typing centers, but also relative to other annotated pub-lished data and contributed data sets.The accurate description of phenodeviant test resultsin the IMPReSS pipelines required the addition of 287new MP terms as of 10/10/2014 (Table 1). New termswere added in multiple systems, with the majority of thenew terms (216) assigned in the homeostasis/metabol-ism section to describe results of specific blood clinicalchemistry tests. For example, in Protocol FRUCTOSA-MINE IMPC_CBC_020_001 [12] the ?mol/l of fructosa-mine in the blood at 16 weeks of age is measured in onetest. This test is used to evaluate the long-term averageamount of glucose in blood, and deviations may indicatea problem with regulation of glucose homeostasis. A sta-tistically significant increase is assigned the newly cre-ated MP term increased circulating fructosamine level[MP:0010087] and a decrease is assigned decreasedcirculating fructosamine level [MP:0010088]. ExistingSmith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 3 of 7MGI annotations to mutant phenotypes were also updatedto use these newly created terms, when appropriate. Othersections of the ontology requiring significant new termsincluded the immune system, the hematopoietic systemand behavior, suggesting that these systems should be sub-ject to further expert review for completeness.However, recently reviewed sections of the ontology re-quired fewer additional new terms. For example, the car-diovascular system was recently revised to support thephenotype descriptions of the ENU mutations generatedby the Cardiovascular Development Consortium (CvDC)(C. Lo, manuscript submitted). Only 9 additional termswere required to support the IMPC data. Likewise, termspreviously requested from members of the FaceBase con-sortium [13] resulted in good coverage of craniofacialterms, requiring only one new additional term for IMPCin this section.Many of the new terms created during this revision arenow being used in the IMPC tests and in existing MGImouse phenotype annotations from literature and otherresources. MGI phenotype annotations are updated whennew terms are added.Existing ontology structures also were reviewed for con-tent coverage and organization. For example, the term ab-normal adaptive thermogenesis [MP:0011019] was addedas a sibling term to both abnormal body temperature[MP:0005535] and abnormal body temperature homeosta-sis [MP:0001777]. abnormal adaptive thermogenesisbecame the parent of the new terms describing stress-induced hyperthermia responses. Recently, new termscovering abnormal alpha-beta T cell morphology[MP:0012762] and abnormal alpha-beta T cell number[MP:0012763] were added, which organized togetherthe terms describing CD4- and CD8-positive alpha-betaintraepithelial, memory, cytotoxic and regulatory T cellsused by the consortium.Assignment of MP terms to results of high throughputpipelinesIMPReSS [14] is a database and web portal developed totrack phenotyping procedures used by the phenotypingcenters of the IMPC. Users can search for phenotype testssuch as Lens Opacity [IMPC_EYE_017_001] [15] that as-sess a phenotype of interest, e.g., cataracts [MP:0001304].The definition and assignment of these ontology terms iscaptured in IMPReSS at the level of each parameter andhas been developed collaboratively by the data wranglers(scientific support staff charged with assisting centers indata capture and download), the phenotyping centers, andontology developers. For some parameters, the assignmentof phenotype terms by data wranglers of the IMPC wasstraightforward and did not require further discussionwith ontology developers. For example, the significanttest results for Heart Weight [IMPC_HWT_001] will beassigned to the MP terms abnormal heart weight[MP:0004857], increased heart weight [MP:0002833]and decreased heart weight [MP:0002834]. For many pa-rameters, a new MP term was requested by data wran-glers, but the term assignment was also unambiguous.Examples include many clinical chemistry terms such asabnormal circulating lipase level [MP:0011885] and sub-classes, abnormal circulating ferritin level [MP:0011889]and subclasses or increased circulating magnesium level[MP:0010092]. For several terms, clarification of a text def-inition, or a split of concepts was required. The ontologydeveloper created the new terms abnormal fluid intake[MP:0011947], increased fluid intake [MP:0011941] anddecreased fluid intake [MP:0011941] to be used in mul-tiple IMPC parameters, in order to distinguish this pheno-type from terms used to describe drinking frequency andother consumption behaviors, for which text definitionswere also revised for clarity. Finally, for a subset of parame-ters, a new term(s) assignment was suggested and createdby the ontology developer to describe the results of a test.Such terms include abnormal bronchoconstrictive re-sponse [MP:0012123] and subclasses, which were recom-mended for annotation of results in the Enhanced pause(Penh) [ICS_CHL_003_001] plethysmography test thatmeasures response to provocation challenge with antigens/allergens.752 MP terms have been assigned to protocols in theIMPReSS database as of 10/10/2014, but final assign-ments/protocols remain under review (Table 1). ExistingMGI phenotype annotations were revised to use thenewly created terms, when appropriate. However, withsome terms, we did not find.Use of MP ontology at IMPCThe IMPC web interface at the European BioinformaticsInstitute (EBI) [16] allows searching and browsing forphenodeviant data using MP terms. For example, select-ing the term cardiovascular system phenotype fromthe phenotypes menu returns a page with the term,definition, all pipeline procedures associated with a car-diovascular system term and all gene variants with car-diovascular system phenotype [17]. Search results maybe further refined using available filters. More specificcardiovascular terms, e.g., abnormal heart weight canbe selected and phenotype data associated with this termmay be viewed.To download and work with large data sets, thephenotype data and MP calls are made available by EBIat the IMPC RESTfulAPI [18]. MP terms associated tothe different mutant genotypes may be retrieved in con-junction with the phenotyping center, pipeline, pheno-typing procedure, gene symbol, allele symbol, strainname, or any combination of these parameters [2]. MGISmith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 4 of 7uses this interface to retrieve data sets for importationand integration with other MGI data.MP expansion to accommodate new IMPC prenatalscreensIdentifying genes that are essential during developmentis required to understand the many processes driving di-rected prenatal growth, differentiation and organogen-esis. Mutations in such genes also can help identifyorigins of developmental disease and congenital defects.Data currently in MGI suggest that approximately 27%(2669/10014) of genes have at least one knockout allelemade into mice that exhibits a prenatal or perinatal le-thal phenotype (Table 2).To study the large number of homozygous knockoutstrains generated by the IMPC expected to exhibit a pre-natal lethal phenotype, a phenotyping pipeline for the in-vestigation of embryonic lethal knockout lines is beingdeveloped. A series of prenatal screenings, lethality sta-ging, gross morphology, and histopathology tests are be-ing discussed by the IMPC to decide upon a logicaltesting order and to identify additional MP terms spe-cific to these tests [19].Some tests will require the addition of new MP terms.For example, new early lethality terms may be needed.Existing terms cover windows commonly seen in pub-lished literature and can correspond to broad time frames(e.g. prenatal) or to narrow time points (e.g. implant-ation) (Figure 1). The IMPC centers collectively havechosen four specific prenatal points for lethality analysis,but not all centers are analyzing each time point. Newterms describing embryonic lethality prior to organogen-esis (approximately mouse E9.5), embryonic lethalityprior to tooth bud stage (approximately mouse E12-12.5),and prenatal lethality prior to heart atrial septation (ap-proximately mouse E14.5-E15.5) have been added andplaced in the hierarchy in relationship to the existing termsto cover mouse lines that are not viable at this stage. Add-itional terms are under discussion. As additional homozy-gous lethal lines are analyzed, it is possible to identify thoseTable 2 Mouse genes with mutations causing pre- or perinataGenes with lethalityannotationPrenatal lethality 2017Perinatal lethality 1076Both pre- and perinatal lethality 424Total unique objects 2669Ratio of total objects annotated 2669/10014Numbers of mouse genes and alleles involved in genotypes annotated to prenatalthe number of genes with at least one allele in a genotype annotated to a prenatalalleles annotated to either term set and some genes have one allele annotated to bdifferences or the nature of the mutant allele. The second column lists the total nuperinatal lethality term. The third column lists the number of genes with additionalthat exhibit lethality at E12.5 but viability at E9.5; the win-dow of lethality is somewhere between E9.5 and E12.5.Other centers will only test the E12.5 time point, so a termdescribing lethality prior to E12.5 may be needed since theE9.5 time point will not be analyzed in this case. There willbe more variations of these developmental time windowsdepending on the testing pipelines finally agreed upon.The developers of the recently described DrosophilaPhenotype Ontology (DPO) [20] have constructed lethal-ity and partial lethality terms for recording and reasoningabout the timing of death in populations. The approachtaken by the DPO combines the terms lethal and par-tially lethal - majority die with a set of terms for lifestages from the Drosophila temporal stage ontology usingformal semantics in OWL. After reasoning, the resultinglist forms a nested classification.For mouse, there exists defined prenatal stage classifi-cations based on Theiler stages or time from plug aftermating, but these as well as postnatal stages are not for-malized into a separate comprehensive stage ontologyand would be required for considering this approach.Most mouse researchers use embryonic day terminologyand not Theiler stages when describing the time of pre-natal lethality in mouse in published literature. Furthercomplications to this approach are the significant varia-tions among different mouse inbred strains in their aver-age gestational periods (e.g. 18.75 days in FVB/NJ and20.5 days in A/J, [21]). Thus the MP uses developmentalhallmarks to describe developmental stages, such as im-plantation and organogenesis, adding text definitionssuggesting an average prenatal age. In addition to the pre-natal lethality stage terms, the MP ontology contains le-thality terms describing neonatal lethality, early postnatallethality and lethality at juvenile stages. A temporal stageontology for mouse using these developmental and post-natal hallmarks would need to be created for such an ap-proach to be feasible for formal definitions within the MPontology, as well as relating these stages to other species.To anticipate the need for new MP terms in grossmorphology and prenatal histopathology, we are proactivelyl lethalityAlleles with lethalityannotationGenes with lethality annotation andpostnatal disease annotation4393 6112304 534589 3226108 8236697/26894 823/10014or perinatal lethality MP terms in MGI as of 10/21/2014. The first column listslethality MP term or a perinatal lethality term. Some genes have multipleoth term sets, possibly due to incomplete penetrance, genetic backgroundmber of alleles in a genotype annotated to a prenatal lethality MP term or adisease annotations suggesting postnatal phenotypes.Figure 1 Mouse prenatal lethality stages. Defined mouse prenatal stages incorporated in Mammalian Phenotype lethality terms and new timepoints required to support IMPC prenatal screening (Not drawn to scale).Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 5 of 7reviewing and adding prenatal MP phenotype terms. Newterms covering embryonic pattern formation, gastrulationand organogenesis. We have added over 189 new terms todescribe these mutations with greater precision. For ex-ample, new terms describing abnormal cardiac or cranialneural crest cell morphology, migration, proliferation, dif-ferentiation and apoptosis have been added. Terms describ-ing abnormalities in embryonic neuroepithelium wereadded. For many other terms, the definitions and synonymshave been updated to include greater detail, including termsdescribing neural tube defects, neuropore defects and spinabifida.The embryogenesis section of the MP has been slightlyreorganized, with many new and existing terms moved andgrouped such as abnormal gastrulation [MP:0001695]now placed under abnormal developmental patterning[MP:0002084] in the hierarchy, or the new term abnormalmorula morphology [MP:0012058] placed under abnor-mal preimplantation embryo development [MP:0012103].In addition to defects of the embryo proper, prenatallethality may also be due to an indirect result of placen-tal defects. IMPC prenatal screens are also developingtests to distinguish the case in which a placental insuffi-ciency is responsible for lethality. MGI data (retrieved10/24/2014) includes 356 genes with 593 alleles anno-tated with terms covering both placental defects and pre-or perinatal lethality. Such mutations may be subject toadditional conditional mutation analysis or tetraploidrescue experiments to determine the effects of the muta-tion on embryonic or adult tissue in absence of placentadefects. We added 27 new placenta related terms to theMP to describe the results of the placenta analysis, forexample placenta necrosis [MP:0013247].We will continue to refine and expand the embryogen-esis and placenta sections of the ontology, as requiredfor reporting the data generated during the IMPC pre-natal phenotype screening.Importation of IMPC phenotype data and integration withMGI data setsThe IMPC provides a RESTful interface to mouse alleles,experimental results and genotypephenotype associa-tions determined by statistical analysis [2]. Phenotypingdata were released starting in June, 2014. These data willbe retrieved automatically and integrated into all otherinformation in the MGI database. MGI has previouslyincorporated high-throughput phenotyping data frompilot projects including the EuroPhenome and SangerMouse Genetics Project (MGP) pipelines (manuscript inpreparation) and new data from the IMPC will beimported similarly. The inclusion of data from IMPC willunify access to mouse phenotype data from many data re-sources sets and from published data using the Mamma-lian Phenotype terms as the unifying standard.MGI will remain the source of global mouse phenotypedata integration from large and small scale data sets, con-tributions and literature. Users will want to see the IMPCknockout data, but also compare these data in context ofother types of mutations. Most human diseases are notfunctional knock-out mutations, so to effectively modelhuman disease, phenotype data associated with all alleletypes (e.g. induced point mutations (such as ENU), spon-taneous mutations (some are recurring), in-dels, copynumber variants, conditional mutations, etc.) are requiredfor interspecies comparisons. Of the 3093 genes with anallele annotated to pre- or perinatal lethal phenotypes,Smith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 6 of 7MGI data also includes postnatal disease data for 823 ofthese genes (Table 2). For this set of genes, postnatalannotations involved data from 1) conditional genotypes,2) haploinsufficient or partially insufficient genotypes whenthe homozygous knockout is lethal [22], 3) incomplete pre-or perinatal lethality, 4) the influence of mouse geneticbackground strain which can have dramatic effects onmouse phenotype [23,24], and 5) additional alleles of thegene that were not knockouts, but were small indels, pointmutations, etc. that caused altered expression or activity ofthe gene product (e.g. hypomorphic and gain of functionmutations). An example of a gene with an allelic seriescausing differing phenotypes is Fgfr2 (Figure 2). TheFgfr2tm1Lni and the Fgfr2tm1.1Wrst functional targeted knock-out mutations result in prenatal lethality. However, theENU-induced point mutation in Fgfr2m1Sgg results in amouse that models Crouzon syndrome. A targeted muta-tion that introduces a different point mutation, Fgfr2tm1Ewj,results in a mouse that models Apert Syndrome, and a tar-geted mutation that knocks out only one isoform of Fgfr2,Fgfr2tm1.1Dsn, results in a mouse that models Multiple In-testinal Atresia.Figure 2 Allelic series for mouse Fgfr2 gene shows range of phenotytwenty-seven known alleles of Fgfr2 that exist in mice. Different mutationsthe homozygous and heterozygous states. An additional eighty-eight mutaalso known.ConclusionsWe describe an expansion of the Mammalian PhenotypeOntology to support phenotype annotation of data gen-erated during high-throughput phenotype screens inmice. Unlike previous phenotyping projects, we haveworked with the IMPC and the pilot projects of the Wel-come Trust Sanger Institute and Europhenome projects tocreate and assign phenotype terms to phenodeviants whenthe data sets are generated by these resources. This willsupport automated loading of these data from the IMPCto MGI and will also be interoperable with other databaseresources and tools.Previously imported small- and mid-scale mutagenesisprojects [1] used other system-specific vocabularies todescribe phenotypes or used text based phenotype de-scriptions that required database curator interventionand translation in order to import the phenotype datainto MGI using the Mammalian Phenotype Ontologystandard. The IMPC data will be loaded directly intoMGI and integrated immediately with all other alleleand data types to support knowledge discovery. Further-more, the MP also is used by mouse repositories topes. Screenshot of MGI Allele Summary Page listing seven of thein this gene result in a range of phenotypes and disease models intions that exist only in gene trapped or targeted ES cell lines areSmith and Eppig Journal of Biomedical Semantics  (2015) 6:11 Page 7 of 7enable searching and describing available mouse strainsand stocks that were originally generated for the highthroughput phenotyping screens. These include the JacksonLaboratory Repository [25], the European Mouse MutantArchive [26], the Mutant Mouse Regional Resource Centers[27], and the KOMP Repository [28] among others.AbbreviationsIMPC: International Mouse Phenotyping Consortium; MP: Mammalianphenotype; MGI: Mouse genome informatics; OWL: Ontology web language;OBO: Open biomedical ontologies; RGD: Rat genome database; MGP: MouseGenetics Project, JAXMice, Jackson Laboratory Repository; IMPreSS: InternationalMouse Phenotyping Resource of Standardised Screens; EMMA: Europeanmouse mutant archive; MMRRC: Mutant Mouse Regional Resource Centers,KOMP, Knockout Mouse Repository.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCS executed the ontology changes in coordination with IMPC, performedthe data analysis and drafted the manuscript. JT conceived of the study, andparticipated in coordination with IMPC and helped to draft and edit themanuscript. Both authors read and approved the final manuscript.AcknowledgementsAnna Anagnostopolous has reviewed embryogenesis terms in the MP andhas made crucial recommendations for additions and revisions. HenrikWesterberg and the data wranglers of the IMPC consortium have mademany requests for terms and have suggested revisions. We thank Susan Bellofor helpful comments on multiple versions of the manuscript.Received: 3 November 2014 Accepted: 3 March 2015JOURNAL OFBIOMEDICAL SEMANTICSPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 DOI 10.1186/s13326-015-0015-3RESEARCH ARTICLE Open AccessFormalizing biomedical concepts from textualdefinitionsAlina Petrova1*, Yue Ma2, George Tsatsaronis1, Maria Kissa1, Felix Distel2, Franz Baader2and Michael Schroeder1AbstractBackground: Ontologies play a major role in life sciences, enabling a number of applications, from new dataintegration to knowledge verification. SNOMED CT is a large medical ontology that is formally defined so that itensures global consistency and support of complex reasoning tasks. Most biomedical ontologies and taxonomies onthe other hand define concepts only textually, without the use of logic. Here, we investigate how to automaticallygenerate formal concept definitions from textual ones. We develop a method that uses machine learning incombination with several types of lexical and semantic features and outputs formal definitions that follow thestructure of SNOMED CT concept definitions.Results: We evaluate our method on three benchmarks and test both the underlying relation extraction componentas well as the overall quality of output concept definitions. In addition, we provide an analysis on the followingaspects: (1) How do definitions mined from the Web and literature differ from the ones mined from manually createddefinitions, e.g., MESH? (2) How do different feature representations, e.g., the restrictions of relations domain andrange, impact on the generated definition quality?, (3) How do different machine learning algorithms compare toeach other for the task of formal definition generation?, and, (4) What is the influence of the learning data size to thetask? We discuss all of these settings in detail and show that the suggested approach can achieve success rates ofover 90%. In addition, the results show that the choice of corpora, lexical features, learning algorithm and data size donot impact the performance as strongly as semantic types do. Semantic types limit the domain and range of apredicted relation, and as long as relations domain and range pairs do not overlap, this information is most valuablein formalizing textual definitions.Conclusions: The analysis presented in this manuscript implies that automated methods can provide a valuablecontribution to the formalization of biomedical knowledge, thus paving the way for future applications that gobeyond retrieval and into complex reasoning. The method is implemented and accessible to the public from:https://github.com/alifahsyamsiyah/learningDL.Keywords: Formal definitions, Biomedical ontologies, Relation extraction, SNOMED CT, MeSHIntroductionResearch in the biomedical domain is characterized byan exponential growth of the published scientific mate-rials, e.g., articles, patents, datasets, technical reports.Handling such a scale of information is a huge chal-lenge, for the purpose of which multiple initiatives havebeen launched in order to organize biomedical knowledge*Correspondence: alina.v.petrova@gmail.com1Biotechnology Center, Technische Universität Dresden, Dresden, GermanyFull list of author information is available at the end of the articleformally. The use of ontologies is one of the most promis-ing key aspects in this direction that has attracted a lotof interest [1]. An ontology is a complex formal struc-ture that can be decomposed into a set of logical axiomsthat state different relations between formal concepts.Together the axiomsmodel the state of affairs in a domain.With the advances in Description Logics (DL), the processof designing, implementing and maintaining large-scaleontologies has been considerably facilitated [2]. In fact,DL has become the most widely used formalism underly-ing ontologies. Several well-known biomedical ontologies,such as GALEN [3] or SNOMED CT are based on DL.© 2015 Petrova et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 2 of 17With regards to the potential benefits of formal knowl-edge in biomedical research, there exist already twoexamples where such a formalization has helped towardsknowledge discovery. In the first case, a formal ontol-ogy about human anatomy with over 70,000 concepts(FMA) is used to infer potential internal and hiddeninjuries from injuries that are visible in images [4]. Inthe second case, the authors show how reasoning overyeast metabolism can generate novel hypotheses [5]. Thenecessary background knowledge and reasoning frame-work form a crucial part of a robot scientist , whichautonomously executes and evaluates experiments inyeast. Thus, formalizing biomedical knowledge can assistimportant biomedical applications. However, the problemof formalizing the knowledge in the domain is an openproblem, since most biomedical ontologies and vocabu-laries, such as GO, MeSH, OBO, define concepts onlyinformally by text strings. Hence, the main problem is toconvert textual definitions into formal representations.A key step in formalizing knowledge in biomedicaldomain is to extract formal definitions for biomedi-cal concepts. As explained in [6], concepts are for-mally defined in terms of their relationships with otherconcepts. These logical definitions give explicit mean-ing which a computer can process and query on . InTable 1, the second row gives an example of a formaldefinition of a concept Baritosis. The definition readsas follows: Baritosis is a sort of Pneumoconiosis andrelates to another concept Barium_dust via the relationCausative_agent . The first row of Table 1 is the corre-sponding textual definition. Indeed, almost all of the exist-ing biomedical formal ontologies, such as OpenGalen,SNOMED CT, FMA, contain only such kind of formalknowledge due to the essence of real practice, thoughDL theory allows for more expressive representation (e.g.General Concept Inclusions [2]). Thus, in this paper, wefocus on learning formal definitions of concepts.Unlike the taxonomy acquisition which seeks to iden-tify parent-child relations in text and is usually based onsimple patterns [7], definition generation typically focuseson highly expressive axioms containing various logicalconnectives and non-taxonomic relation instances. InFigure 1, a simple example illustrates the problem of for-mal definition generation from unstructured text, alongwith its important aspects. The figure outlines a typi-cal text mining workflow based on supervised machinelearning: data acquisition, feature extraction, training andTable 1 Textual and formal definitions of BaritosisTextual definition Baritosis is a benign type of pneumoconiosis, whichis caused by long-term exposure to barium dust.Formal definition Baritosis  Pneumoconiosis?Causative_agent.Barium_dusttesting. The workflow is adapted to the task of formaldefinition generation and contains steps, resources andintermediate states that are needed to extract the formaldefinition of Baritosis from its textual definition.The workflow is not restricted to such textual resourcesas Web articles or MeSH entries. Input textual defi-nitions can be retrieved from a number of resources,such as encyclopedias and terminologies, PubMed, plug-ins to known ontology editors (e.g., Dog4Dag [8], that canretrieve textual definitions from the web) and, in principle,any resource that contains textual information relevant forthe domain.The proposed workflow does not fully solve the prob-lem of automatic formal definition generation. However,is seeks to formalize biomedical knowledge in a way that iswell established by the life sciences community, i.e., usingthe same representation as in the SNOMED CT ontology,namely a description logic EL++ [2]. At the heart of thisrepresentation lie the relations that are intersected andexistentially quantified. Hence, taxonomic and especiallynon-taxonomic relation extraction form a very importantpart of our work. Relation extraction is integrated into abigger end-to-end pipeline that takes as input biomedicaltexts and outputs description logic axioms that formalizethe information in these texts.For example, Figure 1 depicts a series of steps, com-prising the annotation of the sentence with concepts froma designated ontology, the representation of this textualdefinition in a feature space and the application of atrained model, e.g., classifier, that has learned to recog-nize roles (relations) between biomedical concepts, canlead to the final desired output, which is the formal defini-tion of Baritosis. However, there are three main aspectswhich comprise the focus of this work and which can giveinsightful directions on how the task may be addressedefficiently: (a) the modeling of the problem, i.e., the selec-tion of the corpora and the relations that may participatein the formal definitions, (b) the feature engineering, and,(c) the actual machine learning process. Aspect (a) isexamined in a setup where the input unstructured textis annotated and then aligned with knowledge about thechosen relations. The analysis of this aspect can illus-trate how the definitions mined from different types ofcorpora influence the final outcome. Aspect (b) aims atexamining the importance of different feature types in thelearning process. Finally, aspect (c) is meant to provide aninsight on the impact that different learning algorithmshave, as well as on the number of training examples thatare needed per role from the learning process.Related workWe start the discussion of the related work with relationextraction. Relation extraction (RE) is the task of detect-ing and classifying semantic relations that hold betweenPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 3 of 17Figure 1 Overview of the main aspects related to automated extraction of formal concepts definitions, via a simple example of the definition ofBaritosis. The figure illustrates an established text mining workflow based on supervised machine learning to address the task. In this work weanalyze the impact to the overall performance of the different aspects, namely: modeling (selection of corpora and relations set), featureengineering (selection of lexical and semantic features) and machine learning (selection of classifiers and number of training examples).different entities. While it can be performed on bothstructured and unstructured data, our interest is focusedon relation extraction from text.Relation extraction for general domainTextual relation extraction can be performed using differ-ent types of linguistic data that one can get from the inputtext. The most common way is to use the lexical repre-sentation of the text in order to generate typical patternsfor the target relations. The patterns can either be con-structed manually [9], or can be leant automatically usingmachine learning techniques [10].Certain systems explore the syntactic structure of thesource text. The motivation behind it is that the seman-tic relations that hold between the two concepts shouldbe reflected by syntactic dependencies of these concepts.Learning by Reading system [11] extracts propositionsfrom syntactic structures of type Subject  Predicate Object. For the arguments of a relation, i.e. for subjectsand objects, the lexical items are generalized to classes(the classes themselves are automatically derived from thecorpus). The predicates remain in their lexical form.Some systems incorporate semantic information intothe extraction process. The entities and potential rela-tion mentions that have been annotated in the text areassigned more general semantic classes. If a combinationof semantic types of the argument concepts and the typeof the relation match a certain pattern (which is eitherinduced from an existing ontology, pre-defined manuallyor appear with a high frequency), the underlying lexi-cal relation is extracted. Flati et al. [12] extract semanticpredicates using semantic classes of argument conceptsadopted from Wikipedia. Dahab et al. [13] integrate top-level ontologies to semantically parse the input text andto generate semantic patterns of concepts and relations.In the work by Hovy et al. [11] the semantic classes areconstructed by the system itself.With regards to the type of learning that is performedover relations, the task of extracting relations can be donein a supervised way, in an unsupervised way, or in a semi-supervised way, e.g. bootstrapping when an initial seed ofrelation instances is used. Traditional relation extractionencompasses supervised learning techniques. Mohamedet al. [14] state that traditional RE requires the user tospecify information about the relations to be learned.The information about the relations can be encoded intwo ways: (a) for every relation the set of correspond-ing patterns is manually tailored; (b) relational instancesare annotated in the text corpus, and the patterns areacquired explicitly (based on frequent sequences of wordtokens) or implicitly (using machine learning). The newrelational instances are extracted by pattern-matching orby running a trained machine learning model over theinput texts. The supervised approach usually gives highprecision of the retrieved relation instances, which can goover 90%. This makes supervised learning an ideal tech-nique for tasks that incorporate relation extraction as partof the pipeline and need the RE component to outputhigh-quality relations so that the error would not accu-mulate throughout the pipeline. This is the reason whyour method of generating formal definitions is based onsupervised RE.Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 4 of 17Semi-supervised learning of relations usually has a coreof annotated material from which the learning is initialed,and then the process of extraction proceeds in an unsuper-vised manner. It is ideal for situations when the trainingdata is few. For example, the NELL system [15] starts withan initial ontology (a form of prior knowledge) that con-tains some categories, relations and relational instances.The ontology helps building the first set of patterns thatare then used to populate the categories of the ontologyand to extract new facts, which are then used to retrainthe extraction system and to learn yet new facts etc.The main distinctive feature of unsupervised relationextraction systems is that they do not use any assistinginformation during learning: they are not provided withthe seed examples, or background expressive ontologies,or manually constructed patterns. The learning is per-formed purely from the input data [11]. One of the pop-ular unsupervised RE approaches is the so-called OpenInformation Extraction [16]. It is a domain-independentparadigm that uses web-scale-size input corpora of texts.It tends to extract as many triples as possible, but theyare not always well-formed or abstract. Both precisionand recall of unsupervised RE systems are lower that ofthe supervised ones. Banko et al. [16] are the pioneers ofOpen Information Extraction. Their system TextRunnerworks in three steps. First, a deep linguistic analysis isperformed over a small corpus of texts. The system itselfseparates the parsed triples into positive and negativeones. The triples are used for training a machine learn-ing RE model. Secondly, the model classifies the rest ofthe corpus (millions of sentences) and extracts positivetriples. The extraction is done in one pass over the cor-pus and does not involve the deep processing any more.Lastly, newly extracted triples are assigned a confidencescore based on the frequency count of the triple. The sys-tem is completely unsupervised, taking raw texts as inputand outputting relational triples. Unfortunately, only 1million out of 11million high confident triples were evalu-ated as concrete versus abstract, underspecified facts, e.g.Einstein  derived  the Theory of Relativity versusEinstein  derived  theory.Biomedical relation extractionThe majority of research work on biomedical relationextraction focus on the relations between specific concepttypes: genes, proteins, diseases and drugs. Heterogeneouspieces of information are mined from various textualsources and assembled together in a form of ontologies,semantic networks, knowledge bases or other knowledgerepresentation structures.Relation extraction in biomedical domain adopts themethodologies of the general relation extraction. One ofthe most common approaches is to use lexico-syntacticpatterns. A set of relevant relations is manually designedby domain experts, and every relation is assigned to aset of textual patterns that are also constructed manu-ally or extracted automatically from texts. Huang et al.[17] extract protein-protein interactions using lexical pat-terns. Patterns are mined through the dynamic alignmentof relevant sentences that mention the interaction. Boththe precision and the recall of the system reach 80%.Xu and Wang [18] use simple pattern-based approach toextract drug-disease relation instances from MEDLINEabstracts. The patterns are not complicated (e.g. DRUG-induced DISEASE), thus the approach exhibits a typicalbias towards high precision at the expense of low recall:90% precision and 13% recall. However, the majority ofextracted instances do not yet exist in a structured wayin biomedical databases, which proves the usefulness ofthe approach. The majority of work on pattern-basedrelation extraction rely on hand-crafted templates whoseconstruction is a laborious task. In some cases the patternsare built automatically, nevertheless the approach lacksthe ability to extract relations that are not explicitly statedin the text, i.e. the relation is not properly mentioned by averb, a deverbative noun etc, or the two interlinked enti-ties are located to far from each other in the text, and thepattern cannot cover them.Another common relation extraction approach usesco-occurrence information. The idea behind it is quiteintuitive: entities occurring in the same sentence signifi-cantly often should be related [19]. The drawback of theapproach lies in that the correlation information per secannot capture the type of relation present, i.e. what theformal semantics of the relation is. However, it can effi-ciently identify potential relations and relation instancesthat may be examined with other NLP techniques after-wards.Alternative approach to extract biomedical relations isto use machine learning techniques. Firstly, the sourcetext is annotated with biomedical concepts; secondly, sen-tences or phrases are labeled with relations using externalknowledge resources, manual annotation or exploiting theconcept types. Finally, a model is trained to discriminatebetween instances of different classes, i.e. relations. Airolaet al. [20] focus on protein-protein interaction extractionand utilize graph kernel based learning algorithm the Fscore of 56.4%. Chun et al. [21] focus on the extractionof gene-disease relations from manually annotated MED-LINE abstracts that describe either pathophysiology, ortherapeutic significance of a gene or the use of a gene asa marker for possible diagnosis and disease risks. Incor-porating an NER pre-filtering step for gene and diseasenames the classification performance yields 78.5% preci-sion and 87.1% recall. Machine learning appears to be apotential approach of relation extraction which does notrequire to do the tedious work of pattern construction andis able to generalize.Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 5 of 17Formalizing information in textual formThere are several works that attempt to convert textualrepresentation of general knowledge into a structuredform. One approach is described in [22]. The authorsfocus on automatic acquisition of ontology axioms. Theformalism of choice is SHOIN, an expressive DL thatis able to model negation, conjunction, disjunction, andquantitative restrictions. The developed system has sev-eral limitations in formalizing the definitional sentences.The majority of limitations stems from the use of hand-crafted rules. In contrast, in our work we attempt to solvethis issue by applying machine learning techniques tolearn the models of axioms, as shown in Figure 1, whichavoid hand-craft patterns on the lexicon or the syntacticstructure of a sentence.An additional related approach that falls into the broadarea of ontology acquisition is described in [23]. Giventhat ontologies consist of terminological axioms (TBox)and assertional facts (ABox), in this paper, we focus onacquiring a special but common TBox knowledge, namedformal definitions, from texts. Existing TBox generationapproaches are mainly based on syntax-based transfor-mation, but they suffer from the unresolved referencerelations (e.g., ?Of ) and the lexical variant problems(e.g., Causative_agent relation in SNOMED CT can beexpressed both by caused by and due to). Our method isdesigned to remedy these problems.To the best of our knowledge, the is no system thatdoes automatic ontology acquisition or definition gener-ation for biomedical concepts. However, in the domainof life sciences there exist several works that move intothat direction. A work by R. J. Kate [24] presents the firststep towards automated generation of formal definitionsfor concepts that are not yet present in SNOMED CT.The task is to build a relation identification model thatis able to recognize a certain SNOMED CT relation intext. The textual data used in [24] are the clinical phrasesfrom SNOMED CT that describe the concept in naturallanguage (e.g., acute gastric ulcer with perforation). Aseparate machine learning classifier is trained for everytyped version of every SNOMED CT relation, e.g., find-ing_site(disorder, body_structure) and finding_site(finding,body_structure) yield two separate models. There arethree main drawbacks of this work. Firstly, it uses only thedata from SNOMED CT clinical phrases, which are for-mulated in a controlled language. However, the ultimategoal of the system is to be able to identify relations invarious medical texts for new biomedical concepts, andthese texts are not written in a controlled language. Sec-ondly, the system builds a separate classifier for everyrelation and its typed version. The resulting system hasto run hundreds of models every time a new text passageis processed, which is computationally expensive. Lastly,the work does not discuss how the outputs of multipleclassifiers should be combined into a single definition.In our approach we deal with texts of different originand quality, and we incorporate the information aboutsemantic types of concepts involved in a relation into thefeature space instead of training separate classifiers forevery combination of concept types and relations.Okumura et al. [25] automatically process textualdescriptions of clinical findings. Every description belongto one of ten categories: anomaly, symptom, examination,physiology etc. Based on the analysis of 161 descriptions,every category was manually assigned a set of typicalsemantic-syntactic patterns, e.g., a typical way of express-ing a pathology is a pattern substance + verb phrase forphenomenon, as in some fibrosis persisted. The study sug-gests that there are common ways in which biomedicalknowledge is expressed in natural language. Our workuses this finding as one of the motivations to use machinelearning techniques and to encode such patterns automat-ically into models.Dentler and Cornet [26] eliminate redundant elementsin already existing SNOMED CT definitions. Using theELK reasoner [27], the authors eliminated redundant con-cepts, existential restrictions and rolegroups. Here is anexample of the elimination rule for concepts: if a conceptis more general or equivalent to one of the other conceptsin the definition of the same concept or a superconcept.This work is highly relevant to the task of formal definitiongeneration, as it provides a method for post-processingthat can improve the quality of generated axioms and tomake the resulting ontology easier to maintain, constructand expand.MethodsAdding new concepts to a formal ontology is a tedious,costly and error-prone process, that needs to be per-formed manually by specially trained knowledge engi-neers. By contrast, textual information from the medicaldomain is widely available from publicly accessibleresources, such as the web, textbooks and PubMed arti-cles. In the following we present ourmethodology towardsthe automation of formalizing concept definitions fromtextual ones.Problem formulationRelation instances form the basis of a concept defini-tion; they contain necessary and sufficient informationabout the taxonomic and non-taxonomic links betweenthe concept to be defined and the other concepts. Table 1illustrates the connection between a textual definition andits formal representation.Existing approaches for relation extraction mostly focuson learning superclass or subclass relations [8] (e.g.Baritosis - is_a - Pneumoconiosis as given in Table 1),leaving out the non-taxonomic relations (e.g. Baritosis -Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 6 of 17caused_by - Barium_dust). However, the latter are essen-tial for the task of formal definition generation. Existingontologies in the biomedical domain that contain non-hierarchical relations have the following properties: (1) theset of relations is much smaller than the set of concepts,e.g., SNOMED CT currently has 56 roles, but more than311,000 concepts, (2) the set of relations remains relativelystable while the concept set is expanded and modifiedmuch more often, and, (3) the set of relational instances,i.e. unique semantic links between concepts, is much big-ger than the set of relations, e.g., SNOMED CT has morethan 1,360,000 relationships.The observations above suggest that if we are able toextract a relatively small set of relation types, this willresult in many relational instances that may populate aknowledge base. Thus, we formulate the problem tar-geted by the present work as follows: create a system, thatfor a given set of input texts annotated with biomedicalconcepts is able: (a) to find text strings that describe a rela-tionship between these concepts, and to recognize, whichrelationship it is, and (b) to combine these relationshipinstances into concept definitions. For example, for thetarget concept Baritosiswe expect the system to recognizetwo relations, Causative_agent and Finding_site, from thefollowing two sentences: (1) Baritosis is a benign type ofpneumoconiosis, which is caused by long-term exposureto barium dust . (2) Baritosis is due to inorganic dust liesin the lungs . The corresponding relational instances are:Baritosis - Causative_agent - Barium_dust and Baritosis -Finding_site - Lung_structure.Terminology usedThe current work is done on the border of two researchareas, namely Text Mining and Description Logic. Thissection bridges the gap between the terminologies of thetwo communities, giving equivalent terms to all notionsused in the paper. relationIn this work we interchangeably use the termsrelation, relationship and role. The last term comesfrom the ontology development research, while thefirst two terms are used when ontology generation isaddressed from the natural language processingviewpoint. tripleA binary relation instance is often called a triple,since it can be specified by the types of the relationand the two arguments. In linguistic, a triple oftenrefers to a lexical representation of the grammaticalstructure Subject  Predicate  Object. domain and rangeEach relation has a domain and a range, i.e., valuesfor the first and second arguments of the relation,respectively. In linguistic triples, the domain specifiesthe types of subject and the range specifies the typesof object a relation takes. semantic typeIn this work we define the domain and range ofrelations using semantic types. By them we refer tocategories of concepts that can either be classes of anontology (e.g., all the classes of an upper ontology, orseveral top levels of classes in a an ontology or ataxonomy), or some concept types with broadsemantics. In the experiments presented in this paperwe use three different semantic types: semantic typesfrom UMLS Semantic Network [28], SNOMED CTclasses and SNOMED CT categories.Learning formal definitionsIn the following, we describe analytically the aspects ofthe suggested methodology towards learning formal defi-nitions from unstructured text.CorporaTextual corpora and sets of formally defined relations maystem from different sources. The choices are importantper se, as to their quality and volume, and in combinationwith each other. A corpus should adequately represent thedomain of choice and should contain necessary and suf-ficient information about the domain concepts. For thebiomedical domain, the following resources are taken intoconsideration:MeSH (Medical Subject Headings) [29]: Definitionsin natural language are produced manually by medicalexperts and embedded inMeSH, and, thus, are consideredprecise, scientifically valid, and high quality textual data.MEDLINE: Journal abstracts for biomedical literatureare collected from around the world and can be accessedvia PubMed [30]. Since MEDLINE contains, among otherthings, recent publications with cutting-edge advances inbiomedicine, it is of particular interest for the task at handsince it enables the formalization and integration of newlyemerged biomedical concepts.Wikipedia articles: Wikipedia provides fundamentalinformation about biomedical concepts, which can be eas-ily retrieved by article titles, e.g., Alastrim, Iridodonesis.Web articles: Besides Wikipedia, many other websitesprovide relevant knowledge about biomedical conceptsa.Such information should be filtered from the web pages byselecting sentences of definitional structures. For instance,the Dog4Dag system [8] can retrieve and rank textualdefinitions from the web.In this work, we construct the following corpora listedbelow: MeSH: Out of 26,853 entries accompanied by textualdefinitions in MeSH, we selected all concepts thatPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 7 of 17also have definitions in SNOMED CT. For this weused the UMLS Metathesaurus [31] which containsmappings of concepts from various knowledgeresources, including MeSH and SNOMED CT. SemRep: Collected from the SemRep project thatconducted a gold standard annotation study in which500 sentences were selected from MEDLINEabstracts and manually annotated with 26 semanticrelationships [32]. WIKI is obtained by querying Wikipedia withone-word SNOMED CT concept names and amountsto 53,943 distinct sentences with 972,038 words. D4D contains textual definitions extracted byquerying Dog4Dag over concepts that haverelationships via the most frequent attributes used forDisease, namely Associated_morphology,Causative_agent, and Finding_site, obtaining 7,092distinct sentences with 112,886 words.Unlike the corpus SemRep, the other three corpora, i.e.,MeSH, WIKI, and D4D are plain texts without annota-tions. To use them for learning formal definitions, wedeveloped the alignment process as explained in details inSection Alignment.Relation setsRelations in biomedical thesauri are selected and specifiedby domain experts; therefore we assume that all rela-tions are relevant in terms of semantics, hence they areinteresting to be modeled. However, statistically relationsare not equally distributed across domain texts; somerelations are dominant. For example, for the disease con-cepts in SNOMED CT, among 48,076 axioms about non-taxonomic relationships, 40,708 of them only use threerelations: Associated_morphology, Causative_agent, andFinding_site.The SemRep corpus contains 26 relations, the most fre-quent ones being Process_of, Location_of, Part_of, Treats,Isa, Affects, Causes etc. The statistical distribution of rela-tions in SemRep gold standard corpus is illustrated inFigure 2.Based on the analysis above, in this work, we focus ontwo groups of relations: The three SNOMED CT relations(Associated_morphology, Causative_agent,Finding_site); The 26 relations that occur in the SemRep corpus.Once the relation sets are fixed, we need a set of rela-tion instances to be used as training data. In the case ofSemRep, we take the instances that are annotated in thecorpus. In SNOMED CT, due to its formal semantics,we can distinguish two cases: explicit and inferred rela-tionships. The explicit relationship base (ExpRB) containsall relationships among concepts that are explicitly givenin the description of concepts in SNOMED CT. Forinstance, in Table 1, a human readable display of theformal definition for the concept Baritosis, we haveBaritosis|Causative_agent|Barium_dust as an explicit one.The inferred relationship base (InfRB) can be built througha tractable Description Logic (DL) reasoning engine asfollows: InfRB = {A|R|B : SNOMED CT |= A ?R.B}, where |= is the logical entailment under DLsemantics which is tractable for EL++ [33], the logiclanguage underlying SNOMED CT. By this, we haveBaritosis|Causative_agent|Dust as an inferred relationshipsince Barium_dust is a subclass of Dust by SNOMEDCT. By the monotonicity of DL semantics, we haveExpRB ? InfRB. The details of the two relationship basesfor SNOMED CT are summarized in Table 2.AlignmentIn our task, a fundamental requirement is the trainingdata from which a model can be learned to recognizeformal definitions from texts. When manually annotatedcorpus is not available, a common case in our experi-ments, the training data can be automatically created bydistant-supervision approach [34]. This consists of twosteps: (1) finding the mentions of biomedical concepts ina sentence, and, (2) aligning the sentence with a relationby the following principle: if the sentence contains a pairof concepts, say A and B, and this pair are arguments of arelation r according to a relationship RB set under consid-eration, that is A|r|B is in RB, then the sentence fragmentbetween A and B will be aligned with the relation r. Thistwo-step process is illustrated in Table 3. For the givensentence, the fragments Baritosis and barium dust arethe textual mentions of the concepts Baritosis_(disorder)and Barium_Dust_(substance), respectively. By lookingup the relationship set, such as ExpRB or InfRB, weknow that these two concepts are related by the relationCausative_agent. Thus, the string between these two con-cepts, i.e. is pneumoconiosis caused by, is aligned withthe relation Causative_agent. Such an alignment processis performed on our MeSH,WIKI and D4D corpora.Feature engineeringThe choice of features is key in classifying relations asit directly influences the success rate of the classifica-tion process. To this end, we explore two types of feature:lexical and semantic.Lexical Features: The lexical features represent specificwords, word sequences or word components that link twoconcepts in a sentence and are located in-between theconcept mentions. With regards to the representation ofthe lexical features we have utilized three approaches: (1)bag-of-words (BOW), (2) word n-grams, and, (3) char-acter n-grams. BOW is the most straightforward textPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 8 of 17Figure 2 The distribution of relations in the SemRep corpus.representation in which text is viewed as unordered setof words, each unique word in the document collectioncorresponding to a separate feature. In the word n-gramsrepresentation, text is represented via all possible wordsequences of length up to n. Finally, in the character n-grams representation, text is represented via all possiblecharacter sequences of length up to n. All the three typesof lexical features separately can be used separately as wellas in combination with each other. In Tsatsaronis et al.[10], the results did not prove that the combination ofword and character n-grams have a synergy effect on theperformance, hence we skip the combination of lexicalfeatures in the experiments described below.The lines in Table 3 starting with BoW, Word n-grams, and Char. n-grams illustrate the lexical featuresfor the definition: Baritosis is pneumoconiosis caused bybarium dust. The basic assumption behind the choice offeatures is that each relation has a characteristic way ofbeing expressed in natural language text, which may becaptured by the analysis of the words that occur betweenthe two concepts. The values of lexical features, i.e., thethree representations of text strings, are binary: the valueof a feature is 1, if the corresponding textual element ispresent in the string, otherwise the value is 0.We have alsotried expanding these representations to their weightedversions, assigning real values to features according toTable 2 Sizes of the explicit and inferred relationships forthe relations: Associated_morphology, Causative_agent,and Finding_siteAssociated_morphology Causative_agent Finding_siteInfRB 503,306 91,794 1,306,354ExpRB 32,454 13,225 43,079their frequencies [10]. However, the weighting scheme ofchoice turned out to be computationally expensive, butdid not yield considerable improvement to the perfor-mance. Thus, in the present work we focus on booleanfeatures.Table 4 gives an example of highly important lexicalfeatures for the three SNOMED CT roles, when Wordn-grams are used as the feature.Semantic Features: While lexical features reflect therelation per se, i.e. its semantics and typical ways ofexpression in the text, semantic features focus on whattypes of concept arguments a relation can take. Theyspecify the domain and the range of a relation instance.For instance, the relation Finding_site has the subjecttype Disorder and the object type Body_structure. Themotivation behind the use of semantic features is quiteintuitive: since every relation has a domain and a range, itcan take only certain types of concepts as its arguments.If we include these types into the feature representa-tion of instances, we impose explicit constraints on thearguments of every instance.Semantic features can help distinguish different rela-tions even though they share some similar lexical features.For example, in Table 4, Causative_agent and Finding_sitehave similar lexical features infection of and an infec-tion of, respectively. However, they have different argu-ment types. So for the sentence Baritosis is an infection oflung, the relation Finding_site will be recognized insteadof Causative_agent, once we know lung is of the typeBody_structure which is an improper argument type forCausative_agent.There are several possibilities on how to define a seman-tic type given a biomedical concept:Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 9 of 17Table 3 Example alignment between sentences and relationships via semantic annotation, and lexical and semanticfeatures extracted from the alignmentSentence Baritosis is pneumoconiosis caused by barium dust.Annotated Sentence Baritosis is pneumoconiosis caused by barium dust.Baritosis_(disorder) Barium_Dust_(substance)SNOMED CT relationship Baritosis_(disorder) | Causative_agent | Barium_Dust_(substance)Semantic Features left type between-words right typedisorder is pneumoconiosis caused by organismBoW {is, pneumoconiosis, caused, by}Word 2-grams {is pneumoconiosis, pneumoconiosis caused, caused by}Char. 3-grams {is , s p, pn, pne, neu, eum, umo, moc, oco, con, oni, nio, ios, osi, sis, is , s c, ca, cau, aus, use, sed, ed ,d b, by} UMLS (grouped) semantic types. The UMLSSemantic Network [35] contains 134 manually builtconcept types relevant for the biomedical domain.Types are assigned to all the concepts of the UMLSMetathesaurus. However, the modelling of thedomain offered by UMLS is not necessarilycompatible with that of desired relations, i.e., thetypes may not fully correspond to the domain andrange of relations and, thus, will not form validpatterns of type pairs. Moreover, there are 15coarser-grained semantic types defined for certainapplications, providing a partition of the UMLSMetathesaurus for 99.5% of the concepts. Upper level classes as types. Another approach is touse the taxonomic structure of a domain ontology. Ifthe taxonomy forms a single tree of concept classes,then the first n levels of it can be taken as semantictypes. If there are several independent trees, the treetop classes can serve as types. For example, MeSHhas 16 taxonomic trees and SNOMED CT has 19 topconcepts. They can directly be used as types for theirsub-concepts. Indeed, there can be differentgranularities in choosing a proper taxonomy level astypes. However, more fine-grained levels mean morespecific information that we know about the targetconcept, which is often hard to obtain beforehand.Therefore, we consider the level-one top concepts. SNOMED CT semantic types. Unlike the topconcepts, SNOMED CT has defined semantic typesfor its concepts which can be read off from the namesof the concepts given in parentheses. For example, inTable 4 Examples of highly weighted lexical features forthe three SNOMED CT roles: AM Associated_morphologyCA (Causative_agent), and FS (Finding_site)AM displacement of, medical condition characterizedCA caused, cause, from the, by a, agent of, an infection ofFS of, in, affects only, infection ofTable 4, we have the SNOMED CT conceptsBaritosis_(disorder) whose type is disorder andBarium_Dust_(substance) having type substance.Unlike in UMLS Semantic Network, in SNOMED CTa concept has precisely one semantic type.Machine learningWe compared the performance of several classifiers withrespect to learning predictive models that can classifynew, unseen relation instances. The tested classifiers are:Logistic Regression (LR), Support Vector Machines (SVM),Multinomial Naive Bayes (MNB) and Random Forests(RF). SVM yielded the highest performance in our exper-iments on classifying relations, compared to the otherthree classifiers. SVM is a linear classification algorithmthat automatically builds a hyperplane separating theinstances of different classes in such a way that the margin(the distance between the hyperplane and the instances)is maximized.Formal definition generation and evaluationFrom relationships discovered from texts, it is easy totraverse to the EL style formal definitions by applyingthe following transformations to a single (Equation 1) ormultiple (Equation 2) relationships, respectively [36]:A|R|B ? A  ?R.B (1){A|Ri|Bi} ? A  i?Ri.Bi (2)Besides evaluating the quality of relation extraction,we also evaluate the percentage of candidate definitionsthat are correct with respect to the formal SNOMED CTdefinitions. One main problem is that concepts can bedefined with multiple ways under the DL semantics. Forexample we can get a candidate ?Causative_agent.Dustfor the target concept Baritosis. When looking up at thedefinition given in SNOMED CT, this candidate is notexplicitly mentioned. However, this does not affect thedefinition, since we have ?Causative_agent.Barium_dust,Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 10 of 17and because SNOMED CT|= Barium_dust  Dust holds,it follows that Baritosis  ?Causative_agent.Dust.The definition precision, DefPre, can then be defined asfollows, where Cands = {A|R|B : A,B are concept namesand Ris a relation name}.Def Pre = |{A|R|B ? Cands : SNOMED CT |= A|R|B}||Cands| ,(3)Availability of data and softwareAll corpora used in this work are freely available. MeSH,MEDLINE, SNOMED CT, as well as the UMLS resources(the UMLS Semantic Network, the UMLSMetathesaurus)can be accessed via the official NLM website: http://nlm.nih.gov Annotated corpus from the SemRep project(The SemRep Gold Standard corpus) can be obtainedfrom http://skr.nlm.nih.gov/SemRepGold/. The four cor-pora that were used for training and testing the system,namely WIKI, D4D, SemRep and MeSH, are also put inopen access in the form of machine-readable .arff filesand can be found here: http://www.db-net.aueb.gr/gbt/download.html. For the implementation of the machinelearning approaches and the representation of the train-ing and test instances in machine format, we used Weka(Waikato Environment for Knowledge Analysis), whichcan be obtained from http://www.cs.waikato.ac.nz/ml/weka/. The implementation of the full pipeline of for-mal definition generation is published online as a GitHubproject: https://github.com/alifahsyamsiyah/learningDL.ImplementationFor the purposes of the implementation and validationof the suggested approach, we report in the followingthe technical details with regards to the versions of theresources and tools used. The WIKI corpus was collectedfrom Wikipedia of Nov. 7, 2012, and the D4D corpus wascollected using the Dog4Dag plugin on Nov. 9, 2012. Withregards to SNOMED CT, the version released as of Jan.31, 2012 was used. The MeSH hierarchy version used isthe official MeSH 2013 release, that was officially releasedby NLM during December 2012. The UMLS Metathe-saurus was used in version 2012AB. The SemRep corpuswas last accessed on Sep. 15, 2013. The Weka versionused for both training and testing the approach was ver-sion 3.6.5. Default settings were used for all of the testedmachine learning approaches. In particular, we usedWekaimplementation of Support Vector Machines, namely oftheir sequential minimal optimization (SMO) version. Forall experiments we used the SMO setting with the lin-ear kernel, the complexity parameter C = 1.0 and theepsilon parameter  = 1.0E ? 12. The linear kernel canbe set up in Weka by choosing the PolyKernel kernel withexponent parameter of 1.0. No feature selection was per-formed. With regards to Metamap, the 2012 version wasused, which can be obtained from the following location:http://metamap.nlm.nih.gov/. Default settings were usedwith options -R SNOMEDCT to restrict the annotationresource to SNOMED CT.ResultsWe have conducted four different experiments that eval-uate the task of formal definition generation in twodifferent levels: (1) learning roles (relations) between con-cepts, and (2) learning the formal definitions of conceptsas a whole. The final definition of a concept consistsof relations combined together. Thus, we are interestedin evaluating both of these crucial aspects of definitiongeneration, i.e., the way relational instances are formedand the way they are combined into a definition. Thefirst three experiments (Sections Problem formulation toAvailability of data and software) account for the levelof relations, and the last experiment (Section Quality ofgenerated formal definitions) corresponds to the level ofdefinitions.More precisely, Experiment 1 is an initial attempt toextract biomedical relations from text using machinelearning. It explores the potential of different classificationalgorithms to correctly label instances of three frequentSNOMED CT relationships using lexical features fromMeSH definitions.In Experiment 2 we added a new feature type, namelysemantic features, to the learning process and we exam-ined the scenario of learning a bigger set of distinct rela-tions. For this purpose we used the SemRep corpus, thatcomes with a set of textual definitions manually alignedwith 26 relations.In Experiment 3, we switched the corpus to web-basedtextual data with the aim to test the robustness of ourapproach in this setting. The problem of data acquisitionis less relevant for Web sources, thus in this experimentwe also examined the influence of the data size on thelearning performance.In the last experiment we estimated the quality of gen-erated formal definitions compared to their original formsgiven by SNOMED CT.The first three experiments give us insights about allmajor parameters of the relation extraction process thatwe outlined in the abstract, i.e., the source of the input cor-pus, its size, the number of distinct relations and featurerepresentation. Table 5 summarizes themain results of thefirst three experiments. It shows that with themost impor-tant lexical features (character 3-grams) and appropriatesemantic types, we achieved F-score larger than 90% onall datasets using 10-fold cross-validation for evaluation.Furthermore, free texts extracted from the Web proved togive a competitive result. One may assume that it is due toPetrovaetal.JournalofBiomedicalSemantics (2015) 6:22 Page11of17Table 5 Description of the setup of the three experimentsModeling Feature engineering Data F-scoreCorpora Relation set Relationship Lexical Semantic Size Without types With typesExp1 MeSH SNOMED CT InfRB 3-grams  424 74% 99.1%Exp2 SemRep SemRep SemRep 3-grams UMLS 1,357 51%54% 94%Exp3 WIKI+D4D SNOMED CT InfRB 3-grams SNOMED CT 9,292 58%70% 100%In all experiments Support Vector Machines are used.Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 12 of 17the larger data size of the corpus (9,292 v.s. 1,357 or 424instances).The fourth experiment shows that our approach cangenerate formal definitions with a precision that can reachup to 81%, as it was defined by Equation 3.Experiment 1: Lexical features and different classifiersIn the first experiment we used the MeSH corpusdescribed in Section Corpora as the source of inputtexts. We aligned MeSH textual definitions with for-mal definitions from SNOMED CT ontology (SectionAlignment) and labeled definition substrings with oneof the three SNOMED CT relations, i.e., Finding_site,Associated_morphology, Causative_agent. We then con-verted the textual instances of relations into feature rep-resentation using lexical features. We experimented withboth word and character n-grams, varying the size param-eter n from 1 to 4. Then the classification model wastrained and tested using 4 different algorithms: LogisticRegression, Support Vector Machines, Multinomial NaiveBayes and Random Forests.We measured the performance of every combinationof features and classification algorithms using the macro-average F-measure over the three relations in a 10-foldcross-validation setting. A detailed description and statis-tics over all settings can be found in [10]. The top perform-ing setting uses character tri-grams and Support VectorMachines, yielding an F-measure of 74%. This result illus-trates that the signal lexical features carry is quite strongfor the relation classification purposes. However, in orderto reach better performance one needs to elaborate onthe experiment setting, which was conducted in the sub-sequent experiments. In the following experiments wereport results by using only the SVM classifier setting,since the difference in performance compared to the otherclassifiers is negligible. The lexical features of choice arecharacter tri-grams.Experiment 2: Semantic features and the number ofrelationsIn this experiment we aimed at expanding the relationset from just three SNOMED CT roles to a larger setof diverge, semantically rich relations. The process ofaligning MeSH and SNOMED CT definitions provided adataset of moderate size even for the most frequent rela-tions, and the number of relation instances that we areable to extract via the alignment for less populated rela-tions is insufficient for the automatic learning. Thus weswitched to another corpus of definitions, namely SemRep(Section Corpora).The SemRep Gold Standard corpus contains both tex-tual definitions and a set of relations and consists of 1,357relation instances. In addition, we introduced semanticfeatures, reducing every argument concept to its UMLSsemantic type (Section Feature engineering).We trained and tested the classification models for top5 and top 10 most frequent SemRep relations as well asfor the whole set of 26 relations. The results are given inTable 6.As the results show, semantic types seem to offer abig contribution to the overall performance. To answerthe question how much do they add to the learning, werepeated the experiment, leaving out semantic features.The results when only n-grams were used, are 54% and51% for the top 5 and for all SemRep relations, respec-tively. Compared to the results on the full feature set (94%and 82.7% resp.), the difference in performance rate was40%. So, semantic types as features are important.In addition, we examined the effect of the lexical fea-tures comparing the results of using both feature types,and of leaving out the lexical features. This is translatedinto comparing the first and the second line of Table 6. Asthe results show, the lexical features cannot be neglectedas they do offer important contribution in the cases wherea relatively large number of relations is considered, e.g., 10or more.The second question that we would like to address iswhether semantic types are generally effective learningfeatures, or the performance boost was specific to SemRepdataset. For this purpose, we have tried adding semanticfeatures to the Experiment 1, extending the feature rep-resentation of MeSH instances with the same semantictypes from UMLS. The resulting F-measure of 73.9% is abit lower than the original one: the semantic types slightlydeteriorated the performance, serving as noise to the clas-sifier. However, adding semantic types of different originhad an opposite effect: upper level concepts of SNOMEDCT taken as types gave an F-measure of 99.1% for theclassification of the three SNOMED CT relations com-pared to 74.5% with lexical features only. From this wecan conclude that semantic types are of great value for theclassification, given that their modeling is consistent withthe modeling of the relations.Table 6 The performance of multi-class relational classifieracross three different SemRep datasetsTop 5 relations Top 10 relations All relationsF-measure(with Types) 94% 89.1% 82.7%F-measure(only Types) 93.5% 79.2% 65.5%Size 860 (63%) 1,144 (84%) 1,357 (100%)The size of each dataset is specified by the absolute number of instances and bythe percentage of instances covered by the respective set of relations. The tablereports F-Measure for two settings: including semantic types in the featurespace, and excluding them.Petrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 13 of 17Experiment 3: Web corpora and the dataset sizeThe third experiment introduces Web-based corpora asthe source of textual definitions from which formal def-initions and relation instances are built. We used twodifferent corpora,WIKI and D4D, automatically extractedfrom the Web and analyzed the impact of corpora for ourtask. The results are reported in Table 7. In accordance tothe findings of the previous experiment, semantic types(the SNOMED CT types) improved the results notably,achieving an F-measure of 100% on both corpora. This islargely due to the disjointness of argument types for thethree target roles from SNOMED CT. Moreover, Table 7shows that the use of D4D always improved the final per-formance compared to the use of WIKI. An explanationfor this can be that the sentences from D4D are filteredbeforehand by syntactical patterns [8].Overall, WIKI and D4D corpora are much larger thatSemRrep and MeSH corpora (Section Corpora). In fact,having a dataset of more than 60 thousand sentences, itis possible to plot the performance of the relation classifi-cation as a function of the dataset size and to analyze theimpact that the number of learning instances has on thelearning. Using the settings of Experiment 3, we discussthis impact in Section Does data size matter?.Quality of generated formal definitionsThe final step is to translate the predicted relation-ships into formal definitions. For this, we considerSNOMED CT as a reference ontology, which has ELas the underlying logical representation, and, hence,no subjective judgement is involved in the analysis. Inparticular, we consider the concepts that are descen-dants of Disease(disorder). Although there is a totalof 65,073 descendants of Disease, not all of themhave textual mentions in the corpora we studied. Weexamined 1,721 concepts, such as Contact_dermatitisand Subdural_intracranial_hematoma, which have occur-rences in the collection of WIKI and D4D corpora,according to Metamap. For these 1,721 concepts, weobtained an average precision of 66.5% (defined byEquation 3). The low value is due to the fact that thereare 314 concepts that occur in the sentences which do notcontain suitable information for extracting their formaldefinitions, thus, obtaining zero precision. ConsideringTable 7 Main results on the web corporaWIKI and D4D,where the lexical feature is character 3-grams and type isthe SNOMED CT semantic type as discussion inSection Feature engineeringChar 3-gram Char 3-gram + TypeWIKI 58% 100%D4D 70% 100%merely the remaining 1,407 concepts, we achieved a pre-cision of 81.3%.We further investigated if formal reasoning can be help-ful for our task. For this, instead of using InfRB as done inall other experiments, we use ExpRB, as given in Table 2, toconstruct training data. Note that ExpRB is a proper sub-set of InfRB, so less training data can be obtained in thissetting compared to using InfRB. As a result, the averageprecision of definitions decreased to 61.4% from 66.5% forthe 1,721 concepts. The 5.1 p.p. precision difference showsthat the dataset automatically enriched by formal reason-ing (the use of InfRB) improves the systems quality inpredicting formal definitions of SNOMED CT concepts.This is because the inferred relationship base brings moretraining examples to boost the whole learning procedure.DiscussionThe above experiments show that automated conversionof textual definitions into formal ones is a hard, butfeasible problem. In this section we briefly discuss thechoice of corpora, lexical and semantic features, learningalgorithms and data size and how they influence the per-formance of the definition generation pipeline. SectionsDo corpora matter? to Does data size matter? directlycorrespond to the questions we posed at the beginning ofthe paper, and Section Chosen formalism sets an openquestion of which logic better suits the task of formaldefinition generation.Do corpora matter?At a first glance, for extracting formal definitions fromtexts, the textual data should have a big effect on thesystem. For example, MeSH contains manually editedtextual definitions for concepts, which should be of anobvious advantage for this task. However, from Experi-ment 3 and 4, the experiments on automatically extractedweb corpora WIKI and D4D, we can still achieve for-mal definitions of a good quality based on both F-scoreand the definition precision. Set aside MeSH, WIKI andSemRep corpora, which are manually curated, this mightbe explained by two facts: (1) the alignment process,described in Section Alignment, does ensure that thealigned examples are descriptive of the target relation set,and, (2) the D4D plugin prioritizes the definitional sen-tences from the Web, giving higher scores to trustworthyresources. Overall, the selection of the corpus given thefour choices, did not affect much the final performance,with the approach providing good generated definitions inall cases.Does feature representation matter?Lexical featuresWe have tried two different lexical features, word andcharacter n-grams, of various size of up to n = 4 (wordPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 14 of 17unigrams constituting the basic bag of wordsmodel). Sincethe character n-grams model consistently outperformedthe word n-grams model in the Experiment 1, giventhe same value of n, in the subsequent experiments wefocused on the use of characters. The n-grammodel incor-porates as features all possible combinations of charactersof length n that are present in the input corpus, hencethe size of the model is exponential to n. For this reasonthe goal is to find the minimum value of the parametern that yields one of the highest performance. In Experi-ment 1 we noticed that character 3-grams perform almostas good the 4-grams while keeping the size of the modelcomputationally feasible [10]. The expansion of the modelto 5-grams and beyond is thus unnecessary. While uni-grams and bi-grams do not convey information that isstatistically relevant for the classification, 3-grams, in con-trast, are able to capture stems of the key words, importantmorphemes, word order etc. They are shallow linguis-tic features that are easy to generate and integrate intothe model, they lead to the top classification performanceamong other lexical features and are used in all experi-ments. Finally, in Experiment 2 we showed that, althoughsemantic types as features had the biggest influence on theperformance, lexical features are still of high value in caseswhere the target relation set is relatively large.Semantic featuresSemantic features represent broad categories of con-cepts serving as relation arguments. They are the featuresthat drastically influence the classification performance.Semantic types impose category constraints on the argu-ment values of the relation. Taken together, all instancesof the same relation form typical patterns of semantictype pairs that are specific for this particular relation. Asfeatures, semantic types convey a strong signal to the clas-sifier which relation to choose. The discriminative powerof semantic types of relation arguments is the strongestif the patterns for different relations do not overlap. Themore semantic type pairs are shared by more than onerelation, and the more instances are covered by thosepairs, the more erroneous is the classification based onthese features. In Table 6 we can see that the performanceslowly deteriorates with the growing number of distinctrelations. In fact, taken as the only features, semantic typesyield lower results than in combination with lexical fea-tures for the number of relations of ten and more. Thus,the combination of lexical and semantic features is utilizedfor such cases.How to choose the right semantic types?Experiments 2 and 3 illustrated that involving semantictypes gave a great boost to the system. In our approachto formalizing textual definitions, we selected predefinedrelations from existing biomedical thesaurus, such asUMLS and SNOMED. Note that each thesaurus has a dif-ferent way to define the semantic type for a concept [37].As in Experiment 2, the UMLS semantic types were usedfor SemRep relations, and in Experiment 3, the SNOMEDCT types were used for SNOMED CT relations. A natu-ral question would be if the consistency between relationsand semantic types matters. To this end, we performedan experiment on MeSH corpus with SNOMED CT rela-tions but with consistent (SNOMED CT) and inconsis-tent (UMLS) semantic types, respectively, as features. Weachieved 99% F-measure in the consistency case, and only74% for the inconsistency one. Compared to the baseline75%, inconsistent matching did not improve, instead, iteven weakened the system. This illustrates that the con-straints imposed by SNOMEDCT types are unambiguousfor SNOMED CT relations, but overlapping according toUMLS types.Example of the influence of semantic typesTo examine thoroughly the effectiveness of the seman-tic type feature for predicting relations, let us recall Table4 which contains similar lexical features for two differ-ent relations: an infection of  for the Causative_agentrelation, and infection of  for the Finding_site relation.Indeed, the string an infection of virus denotes theexistence of a causative agent relation, while the stringinfection of stomach gives the location of a disease,and, thus, it denotes the existence of a finding site rela-tion. Because the two strings are practically the same, i.e.,infection of , by examining only the lexical informationin this case it is hard to decide which relations shouldbe assigned to the two strings. However, Causative_agentand Finding_site have specific combinations of semantictypes with regards to the domain and range. As its rangeargument, Causative_agent relation may have a conceptof the semantic type Organism. The Finding_site relationmay have Body_structure. In parallel, Causative_agentcannot have Body_structure as a range argument, andFinding_site cannot have Organism as its range argument.Hence, since virus has Organism as its semantic type,and Stomach has Body_structure, the machine learnercan easily assign correct relations for the strings an infec-tion of virus and infection of stomach. Therefore, in thiscase our approach identifies a Causative_agent relation inthe former case, and a Finding_site relation in the lattercase.DoML algorithmsmatter?The choice of the machine learning algorithm is sec-ondary for the task at hand. While in [10] Support VectorMachines consistently dominate over Logistic Regression,Random Forests and Multinomial Naïve Bayes, the differ-ence in performance rate values are partly due to the smallsize of the dataset (424 instances). In several cases, SVMPetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 15 of 17have been shown to match or dominate the performanceof competitive techniques for major text mining exercises.For example, in [38] it is shown that SVM, Naïve Bayesand k-Nearest Neighbor are among the best performingclassifiers. A later work by Mladenic´ et al. [39] evaluatesseveral classifiers on the Reuters news datasets showingthat SVM tend to outperform other algorithms includingNaïve Bayes.However, several studies on the use of Big Data sug-gest that the performance rates of various algorithms tendto converge given considerable amount of instances. In[40], in the task of word sense disambiguation, the per-formance of several algorithms (Naïve Bayes, Winnow,Perceptron) gradually increases and eventually convergesas the training dataset size increases. Colas and Brazdil[41] conclude that, although SVM are among the mosteffective algorithms in the area of text processing, vari-ous other algorithms, e.g., Naïve Bayes or k-NN, achievecomparable performance. Hence, in our work we shift thefocus towards feature engineering and use only SVM.Does data size matter?For the experiments introduced in Section Results, thecorpora data sizes moved from 424 to 9,292, on which weachieved consistent results as analyzed above. Indeed, weconducted similar conclusions by sampling smaller datasets from the large corporaWIKI andD4D: (a) The perfor-mance increased with the increase of the data size if onlylexical features were used: on D4D, F-score ranged from59% (data size 300500) to 63% (data size 7501,000),and to 70% (data size 1,5003,100). On WIKI, F-scoreranged from 48% (data size 600700) to 57% (data size8003,000), and to 67% (data size 4,0006,000). (b) Theperformance stayed relatively constant if semantic fea-tures were involved too, no matter the size of the data:F-score fell into the range of 98% to 100% on all the sam-pled smaller data sets. This again confirms that semantictype is a key feature for the task.Chosen formalismIn this work, we choose to learn EL style biomedicalontologies, adopted by SNOMED CT, for the followingreasons: (a) the results can be directly compatible with andintegrated into the existing resources, (b) it is possible toevaluate our work, using SNOMED CT as a benchmark,(c) the generated definitions can be easily manipulatedby users who have previous experience with SNOMEDCT, (d) EL can deduce implicit hierarchies among theSNOMED CT concepts.However, there are many other Description Logic stylesthat might be interesting to consider, for example, thefull version of GALEN [42] which was designed tobe a re-usable application-independent and language-independent model of medical concepts. This opens ourfuture work on extending the proposed method for learn-ing more expressive biomedical ontologies.What kinds of definitions are generated by the method?EL imposes strict constraints on the form of the defini-tions that are generated: The left part is the concept to be defined, and theright part is the intersection (conjunction) ofexistentially quantified (restricted) roles. The twoparts can be linked into an axiom either by anequivalence operator ? (making it a DL definition inthe strict sense), or by a subsumption (inclusion)operator  (thus making it a primitive definition, thatonly specifies the necessary condition). In the currentwork we make all generated definitions to contain asubsumption operator.Ex: Arthritis is a form of joint disorder that resultsfrom joint inflammation.Arthritis  Joint_Disorder  ?results_from.Joint_Inflammation The definition cannot contain negation ordisjunction of concepts, cardinality constraints,universal quantification etc., as these are not part ofthe EL syntax. The definitions have flat structure. If there is anexistential restriction of a concept in the right part ofthe definition, then this concept should be a simpleconcept and not of the form C1  C2 or ?R1.C1.This, however, does not by any means prevent the defi-nition from containing several relations, which is often thecase for biomedical concepts, as it was rightfully stated bythe reviewer.Fox ? Fordyce_Disease  ?Finding_site.Apocrine_glands?Finding_site.Intraepidermal_apocrine_ducts  (4)?Causative_agent.Obstructure (5)?Causative_agent.Rupture (6)?Associative_morphology.Papular_eruptions (7)ConclusionIn this work we explored the problem of formal def-inition generation from textual definitions of biomedi-cal concepts. We suggested a machine learning basedmethod that automatically generates Description Logicaxioms from textual data. At the core of the methodlies the relation extraction component. Once formal rela-tion instances are generated from text, they are combinedinto definitions. The method is implemented and is madeavailable to the public. We tested the method on threebenchmark data, evaluating it both at the level of relationsand at the level of definitions and achieving a success ratePetrova et al. Journal of Biomedical Semantics  (2015) 6:22 Page 16 of 17of over 90% and 80%, respectively. Moreover, we inves-tigated in detail how different aspects of the method,e.g., the source of textual definitions or the types of fea-tures used for learning, affect its performance. Overall,the choice of corpora, lexical features, learning algorithmand data size do not impact the performance as stronglyas semantic types do. Semantic types limit the domainand range of a predicted relation, and as long as relationsdomain and range pairs do not overlap, this information ismost valuable in prediction. Our work demonstrated thatautomated conversion of textual definitions into formalones is a hard, but feasible problem. It enables complexreasoning over biomedical knowledge that still exists onlyin unstructured textual form and can contribute to manybiomedical applications.EndnoteaE.g., www.primehealthchannel.com or topdefinitions.comCompeting interestsThe authors declare that they have no competing interests.Authors contributionsMS and FB had the original idea. AP, GT, MS and YM developed the idea. APand YM implemented it. AP, GT and MS performed feature enfineering andintroduced semantic features, while YM, FD and FB focused on the use ofexplicit and implicit relations and on formal definition generation. AP, YM andGT wrote the manuscript, ran the experiments and analysed the results. MKand FD contributed to discussions and experiments. All authors read andapproved the final manuscript.AcknowledgementsThe current research work is funded by the Hybrid Reasoning for IntelligentSystems Unit (HYBRIS B1) established by the Deutsche Forschungsgemeinschaft(DFG).Author details1Biotechnology Center, Technische Universität Dresden, Dresden, Germany.2Institute of Theoretical Computer Science, Technische Universität Dresden,Dresden, Germany.Received: 2 July 2014 Accepted: 23 March 2015JOURNAL OFBIOMEDICAL SEMANTICSKafkas et al. Journal of Biomedical Semantics  (2015) 6:7 DOI 10.1186/s13326-015-0003-7SOFTWARE Open AccessSection level search functionality in Europe PMC?enay Kafkas*, Xingjun Pi, Nikos Marinos, Francesco Talo, Andrew Morrison and Johanna R McEntyreAbstractBackground: As the availability of open access full text research articles increases, so does the need forsophisticated search services that make the most of this new content. Here, we present a new feature available inEurope PMC that allows selected sections of full text articles to be searched, including figures and reference lists. Userscan now search particular parts of an article, reducing noise and allowing fine-tuning of searches.Results: To the best of our knowledge, Europe PMC is the first service that provides a granular literature search byallowing users to target their search to particular sections of articles. This new functionality is based on aheuristic algorithm that identifies and categorises article sections into 17 pre-defined categories based on thesection heading. The taggers performance is measured against a manually curated dataset consisting of 100 full textarticles with an F-score of 98.02%.Conclusions: The section search is available from the advanced search within Europe PMC (http://europepmc.org).The source code is freely available from http://europepmc.org/ftp/oa/SectionTagger/.Keywords: Text mining, Section, Information retrievalBackgroundLife science research articles are narrative accounts ofresearch findings, usually describing methods, experi-mental results, and providing scientific context to thenew work reported. Most typical research articles arestructured into sections (segments), most often repre-sented by a logical sequence, known as IMRAD -Introduction, Materials & Methods, Results andDiscussion [1]. However, synonyms of these typicalsection titles are frequently used in articles, accordingto different journal styles. Furthermore, other types of sec-tions are common, such as Case Report in clinical jour-nals, or additional sections such as Funding Sources.These sections provide useful context for the humanreaders understanding of the findings described.The availability of full text articles online provides theopportunity to develop deep search over the completearticle, not just the abstract. While this extends the con-tent available for searching, it can also unfortunately addsignificant noise in the results returned. For example, forsearches that order results by publication date or* Correspondence: kafkas@ebi.ac.ukEqual contributorsEuropean Molecular Biology Laboratory, European Bioinformatics Institute(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge, CB101SD, United Kingdom© 2015 Kafkas et al.; licensee BioMed Central.Commons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.citation count, the results at the top of the list can havelittle bearing on the original search term if that term isfound only in the Reference list.There are a few free-to-use services that provide bio-medical literature search services on full-text documents,for example, PubMed Central (http://www.ncbi.nlm.nih.gov/pmc), Google Scholar (http://scholar.google.co.uk/),BioText Search Engine (http://biosearch.berkeley.edu) andYale Image Finder (YIF) (http://krauthammerlab.med.yale.edu/imagefinder/). However, to the best of our knowledge,neither PubMed Central nor Google Scholar allows usersto limit searches to, or exclude, particular sections of arti-cles. BioText allows users to limit searches to figure cap-tions and tables, and YIF only allows users to limit searchesto figure captions. At Europe PMC (http://europepmc.org)[2], we have implemented a comprehensive section-levelsearch feature that is applied to incoming full text articlesdaily, and have exposed it to users both within the de-fault search on the Europe PMC website, and within theAdvanced Search form.ImplementationImplementation detailsThis section-level search feature has been implementedas a component of the existing Europe PMC full text in-frastructure. As the database is updated with new fullThis is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Kafkas et al. Journal of Biomedical Semantics  (2015) 6:7 Page 2 of 5text content, a rule-based section tagger, developed toidentify the sections of full text articles, is deployed priorto Lucene indexing (http://lucene.apache.org/). Furtherimplementation details of the section tagger are pro-vided below.Section categorisationIn total 17 section category types have been identified asfrequently occurring, based on an analysis of content ofstructured section headers (section headers are tagged byusing the <title> XML element, e.g. <title>Methods</title>)appearing in the XML of the open access (OA) setof Europe PMC articles. The pre-selected categoriesare: Introduction & Background, Materials & Methods,Discussion, Conclusion & Future Work, Case Study,Acknowledgement & Funding, Author Contribution,Competing Interest, Supplementary Data, Abbreviations,JOURNAL OFBIOMEDICAL SEMANTICSCunningham et al. Journal of Biomedical Semantics  (2015) 6:32 DOI 10.1186/s13326-015-0030-4SHORT REPORT Open AccessImproving the Sequence Ontologyterminology for genomic variant annotationFiona Cunningham1, Barry Moore2, Nicole Ruiz-Schultz3, Graham RS Ritchie1,4 and Karen Eilbeck3*AbstractBackground: The Genome Variant Format (GVF) uses the Sequence Ontology (SO) to enable detailed annotation ofsequence variation. The annotation includes SO terms for the type of sequence alteration, the genomic featuresthat are changed and the effect of the alteration. The SO maintains and updates the specification and provides theunderlying ontologicial structure.Methods: A requirements analysis was undertaken to gather terms missing in the SO release at the time, butneeded to adequately describe the effects of sequence alteration on a set of variant genomic annotations. We haveextended and remodeled the SO to include and define all terms that describe the effect of variation uponreference genomic features in the Ensembl variation databases.Results: The new terminology was used to annotate the human reference genome with a set of variants fromboth COSMIC and dbSNP. A GVF file containing 170,853 sequence alterations was generated using the SOterminology to annotate the kinds of alteration, the effect of the alteration and the reference feature changed.There are four kinds of alteration and 24 kinds of effect seen in this dataset. (Ensembl Variation annotates 34different SO consequence terms: http://www.ensembl.org/info/docs/variation/predicted_data.html).Conclusions: We explain the updates to the Sequence Ontology to describe the effect of variation on existingreference features. We have provided a set of annotations using this terminology, and the well defined GVFspecification. We have also provided a provisional exploration of this large annotation dataset.FindingsBackgroundThe Sequence Ontology (SO) [1] provides terminologyto define sequence features. These features are the build-ing blocks of sequence annotation, and allow biologicallymeaningful regions to be assigned between coordinatesof sequences such as genome assemblies and transcripts.The relationships between the terms in SO provide forthe annotation of multi-part features such as genemodels, composed of multiple transcripts, exons, intronsand UTR features. Reference genome annotations areoften shared using a flat file format GFF3, developed bythe GMOD community [2], which stipulates that SOterms describe each annotated feature, thus many gen-ome annotation tools use SO to describe reference gen-ome features. While terms to describe variants have long* Correspondence: keilbeck@genetics.utah.edu3Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USAFull list of author information is available at the end of the article© 2015 Cunningham et al. This is an Open AcLicense (http://creativecommons.org/licenses/medium, provided the original work is propercreativecommons.org/publicdomain/zero/1.0/been part of the Sequence Ontology, increased need fornew variation terms to describe the predicted effect ofsequence alterations on existing genomic features leadto the development of new terms. This has been drivenby the proliferation of software tools that predict the ef-fect of sequence alterations such as Ensembls VariantEffect Predictor (VEP) [3] and the VAAST suite tool:Variant Annotation Tool (VAT) [4]. In this mansucript,SO terms are italicized and written without underscores.Next generation sequencing (NGS) technologies haveprovided an enormous expansion in our understandingof the landscape of genetic variation [5, 6] as well as theimpact of that variation on human health [79]. Thesedatasets create a significant burden in computationalanalysis and data storage, but established work-flows foranalysis are emerging [3] and well established data for-mats exist for each stage of the process. The originalbase calls from the sequencer are converted to FASTQfiles [10] that contain the sequence data; the SAM for-mat [11] captures the alignment of the sequence to acess article distributed under the terms of the Creative Commons Attributionby/4.0), which permits unrestricted use, distribution, and reproduction in anyly credited. The Creative Commons Public Domain Dedication waiver (http://) applies to the data made available in this article, unless otherwise stated.Cunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 2 of 5reference genome and the Variant Call Format [12] hasbecome widely adopted by variant calling tools to reportvariants and the information needed to call them. How-ever, knowing the type and genomic location of a se-quence change is just the first step in understanding itsclinical or biological consequences. Variant annotationthen begins the process of adding additional knowledgeabout the structural and functional consequences ofthose variants through the impact on reference sequencefeatures and ultimately on phenotype.The Genome Variation Format (GVF) [13] is a variantfile format for the detailed annotation of genetic vari-ation. GVF is a community supported format that usesestablished ontologies such as the Sequence Ontology[1] to describe the variant data. GVF does not replaceexisting variant nomenclature systems such as HGVS[14] and ISCN [15] that provide effective ways to unam-biguously describe individual variants in the literature.GVF provides the infrastructure to support inclusion ofthese nomenclatures along with other detailed variantannotations in a format capable of supporting genomescale variant data. GVF is used in the community for ex-change of variant annotations between Ensembl [16],DGVa and dbVar [17] and is compatible with existingGFF3 software [2, 18] as well as emerging domain spe-cific tools [4, 19].Fig. 1 Hierarchical view of new and modified Sequence Ontology terms uportion of the SO sequence variant subsumption hierarchy is shown, with twhere the sequence alteration occurs within or overlaps an annotated refeablation, feature amplification, define cases where an entire feature is alterehttp://sequenceontology.org/browser/obob.cgi and http://ensembl.org/infoUser requirements and ontology developmentUpon the release of the specification for variant genomeannotation, GVF used terms from the Sequence Ontol-ogy release 2.4.3. While this resource provided 101terms to describe the effects of a sequence alteration ongenomic features, it was still missing sufficiently special-ized terms to fully capture the kinds of variation annotatedby the Ensembl variation pipeline [20]. A requirementsanalysis was undertaken to establish the terminology andrelationships between terms to accomplish annotation andfacilitate queries of annotated datasets. Ensembl uses 34terms [21] to describe the effect of variation, 21 of whichwere new to SO, and 2 required an ammendement tothe name. Figure 1 shows a subset of the terms in SOthat describe sequence variants, with the Ensembl termshighlighted.In the SO, the sequence alteration and the effects ofthe alteration are separated. A sequence alteration de-fines the nucleotide change observed in an individual se-quence, in relation to a reference sequence. Examples ofalterations are insertion, deletion, substitution and SNV.The effect of a sequence alteration is the observed orpredicted change to annotated reference seqeunce fea-tures. These effects of sequence alterations are definedas sequence variants in SO and are outlined in Fig. 1.Examples of these terms are missense variant, wherebysed by Ensembl to annotate the effects od sequence alteration. Aerms used by Ensembl in dark grey. Feature variant terms define casesrence feature such as a transcript or exon, whereas the kinds of featured. Definitions for these terms are available from the miSO browser:/genome/variation/predicted_data.htmlCunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 3 of 5codon bases are modified in such a way as the resulingamino acid would change, and splice donor variantwhere by the alteration changes the two-base pair regionat the 5? end of an intron.One of the advantages of using an ontology for the an-notation of data, is that given the related nature of theterms, there are options to annotate data to the level ofdetail afforded by the evidence. Under the sequence vari-ant node, SO provides two high level nodes in the ontol-ogy: structural variant and functional variant. Structuralvariants pertain to changes with regard to annotated se-quence features, and are the output of automated varianteffect predition tools such as VEP [3]. Functional variantshowever describe the cellular effect of a sequence alter-ation and are generally manually curated. These functionalterms have largely been absorbed into the Variation ontol-ogy [22] and are not automatically assigned by variant ef-fect prediction tools. With regards to structural variants,the alteration can either internally modify a sequence fea-ture, when the alteration falls within the extent of a refer-ence sequence feature such as an exon (feature variant),or the alteration can be greater than the extent of the se-quence feature, causing the ablation or amplification of anentire genomic feature such as a transcript.The feature variant node in the ontology subsumesthe terms that describe changes internal to genomic fea-tures such as those affecting genes, transcripts and in-trons. The majority of the sequence alterations currentlyannotated by Ensembl cause feature variants. These fea-ture variant terms are shown in Fig. 1, where the termsused in Ensembl annotations are highlighted in darkgrey. There are five subtypes: intergenic variant, geneFig. 2 Treemap of the proportion of variant affect atributed to each kind otreemap displays hierarchcal data as nested rectangles. In this dataset theresubstitution and SNV, each with a different color. For each sequence alteratrectangle proportional to the number of occrurences of that annotation, angenerated using the IBM Manyeyes tool (http://www-958.ibm.com/)variant, feature truncation, feature elongation and regu-latory region variant. Of these terms, gene variant has77 direct and indirect subtypes and includes most of theterms that describe structural sequence variants causedby substitutions and small insertions and deletions. Thisportion of the SO contains terms with multiple parents,to allow for effective querying of the annotations. Forexample, the term stop retained variant is both a syn-onymous variant and a terminator codon variant. Usersare thus able to query the Ensembl data for all termin-ator codon variants or all synonymous variants.Annotated variantsGVF formated variant genome annotations for 19 organ-isms, typed using SO are available within the Ensembldatabases [23] and for download (ftp://ftp.ensembl.org/pub/release-69/variation/gvf/). Included in this set is aGVF file of 170,853 human variant annotations, withdata from dbSNP [24] and COSMIC [25] using the de-scribed terminology. There are four kinds of sequence al-terations reported, corresponding to 158205 SNVs, 7575deletions, 3097 insertions and 1876 substitutions. Thereare 24 kinds of variant_effect reported in the file, andfive kinds of genomic feature affected (mRNA, miRNA,transcript, primary_transcript and ncRNA). There are1,485,317 reported variant effects with correspondinggenomic features, as a single alteration may perturbmany annotated genomic features. For example an SNVmay intersect two alternate transcripts, one in an exon,the other in an intron. Figure 2 shows a tree map of theproportion of variant effects annotated to each kind ofsequence alteration in this dataset. As can be seen, eachf sequence alteration in Ensembl human GVF dataset (release 69). Aare four kinds of sequence alteration annotated: insertion, deletion,ion, the annotated variant effects are shown with the size of thed the count is provided where space permits. The treemap wasCunningham et al. Journal of Biomedical Semantics  (2015) 6:32 Page 4 of 5kind of alteration causes proportionally different effectsupon the genome features; insertions and deletionscause more frameshift variants, where as the SNVs andother substitutions cause more missense variants.Discussion and conclusionsDetailed annotation of sequence variation is complicatedbecause reference genome annotations are complex. Genesmay produce multiple transcripts, may overlap each otheron opposite strands, or even be nested within introns ofother genes, therefore a variant may influence multiplegenomic features. Capturing the effect of a sequence alter-ation on the genomic features with which it intersects isan important step towards understanding the implicationof the variant sequence. The terminology described hereprovides a basis with whch to categorize and define se-quence variation and the flexibility to annotate the effectwith respect to the feature intersected. This ontology pro-vides very specific leaf terms, with which to automaticallyannotate genomic sequence but also useful mid level termsfor querying.Future developments to the ontology will include de-veloping relationships between the sequence variantterms and the sequence features that are affected. Therehas been significant uptake of these variant effect termsby the genomic variant annotation community. TheUCSC genomic browser uses this termnology in variantannotation [26] as does the NCBIs ClinVar data diction-ary and dbVar database [17]. New terms will be added asrequired. New terms and updates to the ontology maybe requested using the term tracker (https://sourcefor-ge.net/p/song/term-tracker/). Development of the SO iscollaborative, incorporating community discussion viaour mailing list and the term tracker as well as the re-sults of focused working groups.Competing interestsThe authors declare that they have no competing interests.Authors contributionsFC and GR performed requirements analysis. KE, FC, NRS, GR and BMcontributed to ontology development and term definition. All authorscontributed to manuscript.AcknowledgementsThis work was supported by the National Human Genome Research Institute[R01HG004341 to KE] and National Libarary of Medicine training grant [T15LM007124-18, NRS]. Ensembl receives majority funding from the Wellcome Trust(grant numbers WT095908 and WT098051) with additional funding for specificproject components from the National Human Genome Research Institute(U41HG007234, 1R01HD074078, and U41HG007823), the Biotechnology andBiological Sciences Research Council (BB/K009524/1, BB/L024225/1, BB/M018458/1and BB/M020398/1), the Centre for Therapeutic Target Validation (CTTV) and theEuropean Molecular Biology Laboratory. The research leading to these results hasreceived funding from the European Union's Seventh Framework Programme(FP7/2007-2013) under grant agreement n° 282510 (BLUEPRINT). The researchleading to these results has received funding from the European Union'sSeventh Framework Capacities Specific Programme under grant agreement n°284209 (BioMedBridges). This project has received funding from the EuropeanUnions Horizon 2020 research and innovation programme under grantagreement n° 634143 (MedBioinformatics)Author details1European Molecular Biology Laboratory, European Bioinformatics Institute,Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK.2Department of Human Genetics, University of Utah, Salt Lake City, UT, USA.3Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USA. 4Wellcome Trust Sanger Institute, Hinxton, Cambridge, UK.JOURNAL OFBIOMEDICAL SEMANTICSMina et al. Journal of Biomedical Semantics 2015, 6:5http://www.jbiomedsem.com/content/6/1/5RESEARCH Open AccessNanopublications for exposing experimentaldata in the life-sciences: a HuntingtonsDisease case studyEleni Mina1, Mark Thompson1, Rajaram Kaliyaperumal1, Jun Zhao2, Eelke van der Horst1,Zuotian Tatum1, Kristina M Hettne1, Erik A Schultes1, Barend Mons1 and Marco Roos1*AbstractData from high throughput experiments often produce far more results than can ever appear in the main text or tablesof a single research article. In these cases, the majority of new associations are often archived either as supplementalinformation in an arbitrary format or in publisher-independent databases that can be difficult to find. These data arenot only lost from scientific discourse, but are also elusive to automated search, retrieval and processing. Here, we usethe nanopublication model to make scientific assertions that were concluded from a workflow analysis ofHuntingtons Disease data machine-readable, interoperable, and citable. We followed the nanopublication guidelinesto semantically model our assertions as well as their provenance metadata and authorship. We demonstrateinteroperability by linking nanopublication provenance to the Research Object model. These results indicate thatnanopublications can provide an incentive for researchers to expose data that is interoperable and machine-readablefor future use and preservation for which they can get credits for their effort. Nanopublications can have a leading roleinto hypotheses generation offering opportunities to produce large-scale data integration.Keywords: Huntingtons disease, Nanopublication, Provenance, Research object, Workflows, Interoperability,Data integrationBackgroundThe large amount of scientific literature in the fieldof biomedical sciences makes it impossible to manuallyaccess and extract all relevant information for a particularstudy. This problem is mitigated somewhat by text min-ing techniques on scientific literature and the availabil-ity of public online databases containing (supplemental)data. However, many problems remain with respect to theavailability, persistence and interpretation of the essentialknowledge and data of a study.Text mining techniques allow scientists to mine rela-tions from vast amounts of abstracts and extract explicitlydefined information [1] or even implicit information [2,3].Because most of these techniques are limited to min-ing abstracts, it is reasonable to assume that information*Correspondence: m.roos@lumc.nl1Department of Human Genetics, Leiden University Medical Center, PO Box9600, 2300 RC Leiden, The NetherlandsFull list of author information is available at the end of the articlesuch as tables, figures and supplementary information areoverlooked. Moreover, recent attempts to mine literaturefor mutations stored in databases, showed that there wasa very low coverage of mutations described in full text andsupplemental information [4].This is partly remedied by making data public via onlinedatabases. However, this by itself does not guarantee thatdata can be readily found, understood and used in com-putational experiments. This is particularly problematicat a time when more, and larger, datasets are producedthat will never be fully published in traditional journals.Moreover, there is no well-defined standard for scien-tists to get credit for the curation effort that is typicallyrequired to make a discovery and its supporting exper-imental data available in an online database. We arguethat attribution and provenance are important to ensuretrust in the findings and interpretations that scientistsmake public. Additionally, a sufficiently detailed level ofattribution provides an incentive for scientists, curatorsand technicians to make experimental data available in© 2015 Mina et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License(http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium,provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mina et al. Journal of Biomedical Semantics 2015, 6:5 Page 2 of 12http://www.jbiomedsem.com/content/6/1/5an interoperable and re-usable way. The Nanopublica-tion data model [5] was proposed to take all these issuesinto consideration. The nanopublication guidelines docu-ment [5] provides details of the nanopublication schemaand recommendations for constructing nanopublicationsfrom Life Science data. Based on Semantic-web tech-nology, the nanopublication model is a minimal modelfor publishing an assertion, together with attribution andprovenance metadata.The assertion graph contains the central statement thatthe author considers valuable (publishable) and for whichshe would like to be cited (attribution). It should be keptas small as possible in accordance with the guidelines.The provenance graph is used to provide evidence for theassertion. It is up to the author to decide howmuch prove-nance information to give, but in general, more prove-nance will increase the trustworthiness of the assertion,and thus the value of the nanopublication. The publicationinfo graph provides detailed information about the nanop-ublication itself: creation date, licenses, authors and othercontributors can be listed there. Attribution to curatorsand data modelers are part of the nanopublication designto incentivize data publishing.We used the nanopublication schema tomodel scientificresults from an in-silico experiment. Previously Beck et al.[6] used GWAS data stored in the GWAS central databaseto model as nanopublications and they demonstrated howsuch valuable information can be incorporated withinthe Linked Data web to assist the formation of newhypotheses and interesting findings. In our experimentwe investigated the relation between gene deregulation inHuntingtons disease and epigenetic features that mightbe associated with transcriptional abnormalities (E. Minaet al., manuscript in preparation).We show how the results of this case study can be rep-resented as nanopublications and how this promotes dataintegration and interoperability.Huntingtons Disease as case study for modelling scientificresults into nanopublicationsHuntingtons Disease is a dominantly inherited neurode-generative disease that affects 1 - 10/100.000 individualsand thus making it the most common inherited neurode-generative disorder [7]. Despite the fact that the geneticcause for HD was already identified in 1993, no cure hasyet been found and the exact mechanisms that lead tothe HD phenotype are still not well known. Gene expres-sion studies revealed massive changes in HD brain thattake place even before first symptoms arise [8]. There isevidence for altered chromatin conformation in HD [9]that might explain these changes. We selected to analysetwo datasets that are associated with epigenetic regula-tion, concerning CpG islands in the human genome [10]and chromatin marks mapped across nine cell types [11].Identifying genes that are deregulated in HD and are asso-ciated with these regions can give insight into chromatin-associated mechanisms that are potentially at play in thisdisease.Our analysis has been implemented through the use ofworkflows using the Taverna workflow management sys-tem [12,13]. As input we used gene expression data fromthree different brain regions from HD affected individualsand age and sex matched controls [14]. We tested for genedifferential expression (DE) between controls and HDsamples in the most highly affected brain region, caudatenucleus, and we integrated this data with the two epi-genetic datasets discussed previously which are publiclyavailable via the genome browser [15,16].HD is a devastating disease and no actual cure hasbeen found yet to treat or slow down disease progres-sion. Therefore, research on this domain is mainly focus-ing on the production of new data and investment onexpensive experiments. It is important to realize that shar-ing information is essential in research for developingnew hypotheses that can tackle difficult use cases suchas HD. Because of the unavailability of previous exper-iments to be found online using common biomedicalengines, expensive experiments become lost and unnec-essarily replicated. For example in our case study, wefound that the association that we inferred between theHTT gene, which mutant form causes Huntingtons Dis-ease, and BAIAP2, a brain-specific angiogenesis inhibitor(BAI1)-binding protein, was present in a table in a paperby Kaltenbach et al. [17]. However, it is not explicitly inany abstract which makes it hard to retrieve from systemssuch as PubMed.Results and DiscussionNanopublication model design principlesWe decided to model and expose as nanopublications twoassertions from the results of our workflow: 1) differ-entially expressed genes in HD and 2) genes that over-lap with a particular genomic region that is associatedwith epigenetic regulation. Note that these natural lan-guage statements would typically be used in a captionfor a figure, table or supplemental information section todescribe a dataset in a traditional publication. Consideringthe problems with automatic retrieval and interpretationof such data, we aim to expose these assertions in away that is more useful to other scientists (for exampleto integrate our results with their own data). Moreover,we provide provenance containing the origin and exper-imental context for the data in order to increase trustand confidence. Our nanopublications are stored in theAllegroGraph triple store [18]. The link to the browsableuser interface and the SPARQL endpoint can be found onthe myExperiment link: http://www.myexperiment.org/packs/622.html. The user can log in and browse throughMina et al. Journal of Biomedical Semantics 2015, 6:5 Page 3 of 12http://www.jbiomedsem.com/content/6/1/5the nanopublications by logging in with username testand password tester. The queries used in this paper arestored under the menu Queries ? Saved.AssertionmodelWe defined two natural language statements that we wishto convert to RDF:gene X is associated with HD, because it was found tobe deregulated in HD and gene Y is associated with apromoter, and this promoter overlaps with a CpG islandand/or a particular chromatin state, and we wish to referto the experiment by which we found these associations.We decided to model our results into two nanopublica-tions. By further subdividing those statements, we see theRDF triple relations appear naturally:Nanopublication assertion 1:1. There is a gene disease association that refers_togene X and Huntingtons DiseaseNanopublication assertion 2:1. Gene Y is associated_with promoter Z2. Promoter Z overlaps_with a biological regionaThe assertion models are shown in Figure 1 andFigure 2.For some of the terms in these statements we found sev-eral ontologies that defined classes for them. For example,promoter, gene, and CpG island appear (among oth-ers) in the following ontologies: NIF Standard ontology(NIFSTD), NCI Thesaurus (NCI) and the Gene Regu-lation Ontology (GRO)b. We chose to use NIFSTD forour case study, because it covers an appropriate domainand it uses the Basic Formal Ontology (BFO), which canbenefit data interoperability and OWL reasoning (e.g. forchecking inconsistencies).We chose to use bio2rdf instances for the associatedgenes [19] because they provide RDF with resolvableresource URIs formany different biomedical resources. Todescribe the gene-disease association linked with alteredgene expression we used the class with that label fromthe SemanticScience Integrated Ontology (SIO) [20]. TheSIO predicate refers to was used to associate each dif-ferentially expressed gene with HD. There were also termsthat we did not find in an available ontology. These werethe ones that described the type of the chromatin statethat a promoter of a gene can be in, active promoterstate, weak promoter state, poised promoter stateand heterochromatic. We decided to create our ownclasses to describe these terms. Being aware of interoper-ability issues, we defined them as subtypes of classes inthe Sequence Ontology (SO). We defined the class chro-matin_region as a subclass of biological_region in SO.We defined another class chromatin_state as a subclassof feature_attribute. Subclasses of chromatin_state arethe states active_promoter, weak_promoter, poised_promoter and heterochromatic, Figure 3. In Table 1we provide the definition for these classes, the reused SOontological terms and their corresponding URIs. We alsodefined an object property has_state which has domainchromatin and range chromatin_state.For the predicates we considered the use of the Rela-tion Ontology. Extending the Relation Ontology with theappropriate predicates would support interoperability andreasoning in the long term, because its use of BFO. How-ever, we found that the OWL domain and range spec-ifications did not match our statements. Therefore, wedecided to use predicates from the also popular SequenceOntology (SO) and Semanticscience Integrated Ontology(SIO) [20] that also seemed appropriate for our assertions.This is a typical trade-off between quality and effort thatwe expect nanopublishers will have to make frequently.We can justify this for two reasons: 1) releasing experi-mental data as linked open data using any standard ontol-ogy is already an important step forward from currentFigure 1 Differential gene expression assertion model. The template for the differential gene expression assertion. Orange diamonds refer to aRDF resource that was defined by this nanopublication, whereas the gene (pink diamond) is defined by a bio2rdf resource. The Sequence Ontology(SO) were used for the predicates refers_to. The classes for Huntingtons disease, gene and gene-disease association linked with altered geneexpression are defined by the nifstd, bio2rdf and SIO ontologies respectively.Mina et al. Journal of Biomedical Semantics 2015, 6:5 Page 4 of 12http://www.jbiomedsem.com/content/6/1/5Figure 2 Genomic overlap assertion model. Orange diamonds refer also here to a RDF resource that was defined by this nanopublication,whereas the gene (pink diamond) is defined by a bio2rdf resource. The classes promoter and biological region were defined by the nifstd ontology.The Semanticscience Integrated Ontology (SIO) and Sequence Ontology (SO) were used for the predicates overlaps_with and associated_with.practice and 2) interoperability issues at the ontology levelis a shared responsibility with ontology developers andcurators who provide mappings between ontologies andwith higher level ontologies.The process of nanopublication modeling can be min-imized when previous examples are used as templatesfor similar data. For instance, the nanopublication mod-els presented here can serve as templates for exposingdifferentially expressed genes in a disease condition. Wedemonstrate the reuse of our own template of Figure 2 byexposing 5 types of nanopublications concerning genomicoverlap. The reuse of templates improves interoperabilityof scientific results beyond the interoperability that RDFalready provides. It facilitates crafting assertions whileensuring that the same URIs are used for the same type ofdata.ProvenancemodelPublishing information is meaningful only if there isenough supporting information for reproducing them. Forexample Ioannidis et al., pointed out that they could notreproduce the majority of the 18 articles they investigateddescribing results from microarray experiments, includ-ing selected tables and figures [21]. Nanopublication doesnot guarantee full reproducibility, but as a model for com-bining data with attribution and provenance in a digitalformat it at least makes it possible to trace the originof scientific results. The provenance section of a nanop-ublication ties the results (the nanopub assertion) to adescription of an experiment and the associated mate-rials, conditions and methods. The main purpose is tocapture as accurately as possible where the assertion camefrom and what the conditions of our experiment wereby aggregating and annotating resources that were usedthroughout the experiment.In our case the experiment is in-silico: a workflowprocess that combines existing data sources to exposeJOURNAL OFBIOMEDICAL SEMANTICSCellier et al. Journal of Biomedical Semantics  (2015) 6:27 DOI 10.1186/s13326-015-0023-3RESEARCH ARTICLE Open AccessSequential pattern mining for discoveringgene interactions and their contextualinformation from biomedical textsPeggy Cellier1*, Thierry Charnois2*, Marc Plantevit3, Christophe Rigotti4, Bruno Crémilleux5,Olivier Gandrillon6, Jir?í Kléma7 and Jean-Luc Manguin5AbstractBackground: Discovering gene interactions and their characterizations from biological text collections is a crucialissue in bioinformatics. Indeed, text collections are large and it is very difficult for biologists to fully take benefit fromthis amount of knowledge. Natural Language Processing (NLP) methods have been applied to extract backgroundknowledge from biomedical texts. Some of existing NLP approaches are based on handcrafted rules and thus are timeconsuming and often devoted to a specific corpus. Machine learning based NLP methods, give good results butgenerate outcomes that are not really understandable by a user.Results: We take advantage of an hybridization of data mining and natural language processing to propose anoriginal symbolic method to automatically produce patterns conveying gene interactions and their characterizations.Therefore, our method not only allows gene interactions but also semantics information on the extracted interactions(e.g., modalities, biological contexts, interaction types) to be detected. Only limited resource is required: the textcollection that is used as a training corpus. Our approach gives results comparable to the results given bystate-of-the-art methods and is even better for the gene interaction detection in AIMed.Conclusions: Experiments show how our approach enables to discover interactions and their characterizations. Tothe best of our knowledge, there is few methods that automatically extract the interactions and also associatedsemantics information. The extracted gene interactions from PubMed are available through a simple web interface athttps://bingotexte.greyc.fr/. The software is available at https://bingo2.greyc.fr/?q=node/22.Keywords: Data mining, Sequential pattern mining, Natural language processing, Information extraction, GeneinteractionsIntroductionLiterature on biology and medicine represents a hugeamount of knowledge: more than 24 million publica-tions are currently listed in the PubMed repository [1].These text collections are large and it is difficult for biol-ogists to fully take benefit from this incredible amountof knowledge. A critical challenge is then to extract rel-evant and useful knowledge spread in such collections.Text mining and Natural Language Processing (NLP)*Correspondence: Peggy.Cellier@irisa.fr; Thierry.Charnois@lipn.univ-paris13.fr1INSA de Rennes, IRISA, UMR6074, F-35042 Rennes, France2Université de Paris 13, LIPN, UMR7030, F-93430 Villetaneuse, FranceFull list of author information is available at the end of the articleare rapidly becoming an essential component of vari-ous bio-applications. These techniques have widely beenapplied to extract and exploit background knowledgefrom biomedical texts.Among many tasks, a crucial issue is the annotation ofa large amount of genetic information. NLP, and Informa-tion Extraction (IE) in particular, aim to provide accurateprocessing to extract specific knowledge such as namedentities (e.g., gene, protein) and relationships betweenthe recognized entities (e.g., gene-gene interactions, bio-logical functions). Databases such as BioGRID [2] orSTRING [3] store a large collection of interactionsderived from different sources and indicate which gene© 2015 Cellier et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproductionin any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 2 of 12interacts with a specified gene. However, these databasesdo not support more complex requests such as: whichgenes inhibit gene X? what is the biological context (e.g.,organism, biological information) associated to a gene-gene interaction? what is the kind of interaction betweengenes X and Y? what is the modality associated to theextracted information (related work, experimental result,etc.)? These requests are useful for biologists since theyenable to faster point out the piece of information theylook for. Unfortunately, to the best of our knowledge, nowork has been reported yet to support these kinds ofrequests. That is why in this paper we propose a methodto retrieve that kind of information.Our method automatically discovered a human man-ageable set of patterns that are then validated by experts toprovide linguistic patterns. In other words, thanks to thelinguistic patterns, our method not only allows gene inter-actions but also semantics information on the extractedinteractions (e.g., modalities, biological contexts, interac-tion types) to be detected.The need for linguistic resources (grammars or lin-guistic rules) is a common feature of the informationextraction methods. Indeed, those NLP approaches applyrules such as regular expressions [4] or syntactic pat-terns [5,6]. However, these rules are handcrafted and thusthose methods are time consuming and often devoted toa specific corpus [7].In contrast, machine learning based methods, for exam-ple support vector machines or conditional random fields[8], are less time consuming than rule-based methods.Machine learning methods for gene interaction detec-tion usually tackle the task as a classification problem.Best results are obtained with kernel methods [9-12] andsome NLP parsers can be used to provide some featuresto the classifier [13]. Although they provide good results,machine learning methods still need many features. Also,their outcomes are not really understandable by a user, northey can be used as linguistic patterns in NLP systems.Furthermore, the annotation process of training corporarequires a substantial investment of time, and cannot bereused in other domains (some new corpora must beannotated for new domains) [7]. A good trade-off is thecross-fertilization of information extraction and machinelearning techniques which aims at automatically learningthe linguistic rules [14,15]. However, in most cases thelearning process is done from text syntactic parsing. Forinstance, BioContextt [16] or Turku Event Extraction Sys-tem (TEES) [17] aim at extracting biological events withcontextual informations (e.g., species involved, localiza-tion, modality) about the biological events. Those systemsare based on a syntactic analysis. Therefore, the qualityof the learned rules relies on syntactic process results.Still some works such as [18] or [19] do not use syntacticparsing.For example, Abacha and al. [19] have a corpus basedstrategy close to [20] and this line of research. They aim atlearning patterns from a list of seed terms correspondingto pairs of entities known to be in some target rela-tions. Other works based on pattern matching as AliBaba[21-23], learn surface patterns using sequence alignmentof sentences to derive motifs. This method is based ona list of terms that represent interactions. Only interac-tion patterns are learned and no new term to symbolizeinteraction can be discovered.With ourmethod, linguisticpatterns are automatically learned to detect interactions(interaction patterns) and also, at the same time, to char-acterize the interactions (characterization patterns). Inaddition, the terms and the patterns do not need to be pro-vided. They are automatically extracted by the method. Itthus provides new knowledge.The key idea of our approach is to take advantage ofan hybridization of data mining and NLP for Biologi-cal Natural Language Processing (BioNLP). Data min-ing techniques, such as extraction of frequent sequentialpatterns [24], enable the discovery of implicit, previ-ously unknown, and potentially useful information fromdata [25]. Our contribution is an original method to auto-matically produce patterns (which can be seen as a kind oflinguistic rules) from text collections.The problem of data mining techniques is that, ingeneral, too many patterns are generated. That is why,our method is based on recursive sequential pattern min-ing with constraints from the NLP field to tackle thediscovery of gene interactions. The patterns output con-vey a model of the interactions that are enhanced withsemantics information (modalities, biological contexts,interactions types).Only limited resource is required: the text collection(used as a training corpus) which only contains sentenceswith interactions and where only gene names are taggedbut not the interaction. In particular, terms and patternsare automatically discovered from texts without otherresources.To the best of our knowledge, there are few methodsthat extract the interactions and also provide associ-ated semantics information on the extracted interactionsthanks to the discovered patterns which are understableand can be manually modified by a human expert.In addition, we propose to use background knowledge,a well-established biomedical corpora and a gene inter-action database, in order to assess the relevance of pat-terns, used as linguistic rules, and to help an experta toselect them. We describe a validation method based onthe idea that the relevant rules convey information thatmust be consistent with the background knowledge. Thismethod is interesting because the validation of rules iscurrently widely based on human checking which is highlytime consuming. Last but not least, we conduct extensiveCellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 3 of 12experiments highlighting how our approach enables todiscover interactions and their characterizations and wepresent a discussion of the results.MethodThis section presents our method to produce linguisticrules in order to discover interactions and their character-izations. Figure 1 gives a global view of the process.Background: sequential pattern miningSequential pattern mining is a well-known techniqueintroduced in [24] to find regularities in database ofsequences, and for which there are several efficient algo-rithms (e.g. [26-30]). A sequence as used in our methodis an ordered list ?i1 . . . im?, where the elements of the listi1 . . . im are called itemsb.A sequence S1 = ?i1 . . . in? is included in a sequenceS2 = ?i?1 . . . i?m? if there exist integers 1 ? j1 < ... < jn ? msuch that i1 = i?j1 , ..., in = i?jn . The sequence S1 is called asubsequence of S2, and we note S1  S2. For example, wehave ?b d?  ?a b c d?.A sequence database SDB is a set of tuples (sid, S),where sid is a sequence identifier and S a sequence.For instance SDB1 = {(1, ?a b c d?), (2, ?b d e?),(3, ?a c d e?), (4, ?a d c b?)} is a database of foursequences.A tuple (sid, S) contains a sequence S? , if S? is a subse-quence of S. The support of a sequence S? in a sequencedatabase SDB, denoted sup(S?) is the number of tuplesin the database containing S? . For example, in SDB1sup(?b d?) = 2, since sequences 1 and 2 contain ?b d?.Notice that for notational convenience, sometimes the rel-ative support is used. In this case, the support sup(S?) isthe relative number of tuples in the database that containS? , sup(S?) = |{(sid,S) | (sid,S)?SDB?(S?S)}||SDB| .A frequent sequential pattern is a sequence such that itssupport is greater or equal to a given support thresholdminsup.Figure 1 General framework to extract gene interactions. Figure 1 presents the overall process to detect and characterize gene interactions. Thereare two steps. The first step is the extraction of patterns. Sequential patterns are mined from a learning corpus that contains sentences representinggene interactions. In order to reduce the number of extracted patterns, constraints and recursive mining are applied. At the end, few sequentialpatterns corresponding to candidate linguistic interaction or characterization rules remain. A key point is that the sequence of words expressing theinteraction in a pattern is automatically discovered. As an example, the sequence of words <AGENE, "association"> in the pattern AGENEassociation with AGENE (see Table 5) where a small gap may appear between AGENE and "association" are discovered by our method. This patternconveys an interaction between two genes (denoted AGENE) in association. The sequential patterns are then analyzed and validated by experts.The ones that are not relevant for interaction detection or characterization are removed. The second step is the application of those validatedpatterns as linguistic rules to discover and characterize interactions in new corpora.Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 4 of 12Extraction of sequential patterns in textsFor the extraction of sequential patterns from biologicaltexts, we use a training corpus which is a set of sen-tences that contain interactions (but not annotated) andwhere the genes are identified. In this paper we considersentences containing interactions and at least two genenames to avoid problems introduced by the anaphoricstructuresc [31]. The training corpus, with tagged genenames, is selected by an expert. The items are combina-tions of lemma and POS tagsd. POS tag information isimportant to disambiguate words (e.g., form the noun vsto form the verb). The sequences of the database are theinteraction sentences where each word is replaced by thecorresponding item. The order relation between items ina sequence is the order of words within the sentence. Forexample, let us consider two sentences that contain geneinteractions: Recent studies have suggested that c-mycmay be vital for regulation of hTERT mRNA expressionand telomerase activity. and Injection of frpHE mRNAin Xenopus embryos inhibited the Wnt-8 mediated dorsalaxis duplication. All gene names are replaced by a spe-cific item, AGENE, and the other words are replaced bythe combinations of their lemma and their POS tag. Anexcerpt of the database that contains the sequences asso-ciated to those two sentences is given in Table 1. Thesequential patterns are extracted from this database.The choice of a support threshold minsup is a well-known problem in data mining. With a highminsup, onlyfew very general patterns can be extracted. With a lowminsup, a lot of patterns can be found. Some interestingwords, for example interaction, are not very frequentso that we set a low value of minsup. As a consequence,a huge set of patterns is discovered and it needs to befiltered in order to return only relevant patterns.Constraints and recursive miningTo reduce the number of extracted patterns, we use acombination of data mining methods. The constraint-based pattern paradigm (e.g., [32]) enables discoveringpatterns under user-defined constraints in order to drivethe mining process towards the user objectives. Recursivemining [33] reduces the number of patterns by extractingtheir common structures.Linguistic constraintsIn pattern mining, constraints allow the user to definemore precisely what should be considered as interesting.The most commonly used constraint is the constraint offrequency (minsup). However, it is possible to use differentconstraints [34]. In our method, in order to extract geneinteraction patterns, we use three additional constraints.The first constraint is that the pattern must contain twogene names, i.e. two AGENE items.The second constraint is that the pattern must containat least a verb or a noun.Finally, among the patterns that satisfy the frequencyand the two other previous constraints, we retain only themaximal ones with respect to the inclusion order . Thatlast constraint allows the redundancy between patterns tobe reduced.The constraints can be gathered in only one constraintCG which is the conjunction of the three constraints.SAT(CG) is the set of patterns satisfying CG.RecursiveminingEven if the new set of sequential patterns, SAT(CG), issignificantly smaller than the initial set of all extractedsequential patterns without constraints, it can still be toolarge to be analyzed and validated by experts. To finda limited number of patterns corresponding to generalstructures among the whole pattern collection, we usethe recursive mining technique of [33]. The key idea ofthis post-processing is to reduce the size of the output bysuccessively repeating the mining process on the patternsthemselves in order to extract the structure shared by thepatterns. More precisely, at each step, the previous set ofsequential patterns is used as a new sequential database,and a new extraction is made. The process stops when noTable 1 Example of a sequence databaseID Sequence... ...S1 ?Recent@jjstudy@nnshave@vhpsuggest@vvnthat@in/thatAGENEmay@mdbe@vbvital@jjfor@in regulation@nn of@in AGENEmrna@np expression@nn and@cc telomerase@nnactivity@nn .@sent ?S2 ?injection@nnof@inAGENEmrna@npin@inxenopus@npembryo@nnsinhibit@vvdthe@dtAGENEmediate@vvd dorsal@jj axis@nn duplication@nn .@sent ?... ...Table 1 shows an excerpt of a sequence database which contains two interaction sentences:S1: Recent studies have suggested that c-mycmay be vital for regulation of hTERTmRNA expression and telomerase activity. andS2: Injection of frpHEmRNA in Xenopus embryos inhibited theWnt-8mediated dorsal axis duplication..Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 5 of 12more than k patterns are obtained by the extraction, wherek is a parameter set by the user.Our target is to identify at least one pattern by verb ornoun that appears in the patterns in SAT(CG). So, for eachverb or noun denoted Xi, that appears in SAT(CG), wecollect the set EXi of patterns containing Xi, EXi = {s ?SAT(CG) | ?Xi?  s}. Note that some frequent patterns cancontain more than one noun and/or verb (so several Xi).In this case, the pattern is duplicated in the EXi of eachnoun and/or verb.For a given value of k, we apply the recursive miningpost-processing technique on each EXi . At each extractionstep we select only the patterns that satisfy CG, and use arelative minimum support threshold minsup = 1k . Thatthreshold value and the maximality constraint guaranteethat recursive mining process terminates in finite steps asproved in [35].At the end of this post-processing of all EXi , the numberof sequential patterns cannot exceed n × k where n is thenumber of verbs and nouns occurring in SAT(CG).Selection and categorization of patternsThe sequential patterns are then analyzed and validatedby experts. The ones that are not relevant for interactiondetection or characterization are removed. The remain-ing ones are selected as linguistic extraction rules [36].A selected pattern is classified with respect to the kindof information conveyed by the pattern. There are twomain classes of patterns: interaction patterns and charac-terization patterns. The first class indicates what kind ofinteraction between genes is found (e.g., inhibition). Thesecond class is characterization patterns. It is built by theexperts and can be completed with other classes if otherkinds of information extraction rules are found. There aretwo kinds of characterization patterns: modality patternsand biological context patterns. Examples of patterns arediscussed in Section Extracted sequential patterns.When the experts have validated and classified all pat-terns in the different categories, they are applied as lin-guistic rules to discover and characterize interactions innew corpora.In practice, this step is not time consuming as shownin the following and can be helped by using backgroundknowledge to support pattern validation as proposedin the Section About validation of sequential patternsas linguistic extraction rules. Detection with sequentialpatterns representing interactions, modalities or biolog-ical contexts is much more elaborated than just a co-occurrence detection. Indeed, the order of the words andthe context are important, they provide semantics infor-mation. For instance, the sub-categorization of the verbgiven by the POS tagging indicates the passive or activeverb and identifies the direction of the interaction. Prepo-sitions can give this information when the pattern doesnot contain a verb, for example: ?activation@nn of@inAGENE by@in AGENE?.Note the genericity of the approach, indeed theextracted patterns allow genetic interactions to be discov-ered as well as physical protein interactions.ResultsIn this section, we present the experiments and results.First, the training corpus is detailled. Then, the sequentialpattern extraction is described. Finally, the results of theapplication of the extracted patterns on testing corporaare presented.Training corpusGenes can interact with each other through the proteinsthey synthesize. Moreover, although there are conven-tions, the same word can represent a gene name and theprotein synthesized by the gene. Biologists know fromthe context if the sentence is about protein or gene. Todiscover the linguistic patterns of interactions betweengenes, we merge two different corpora containing genesand proteins, to create the training corpus. The first cor-pus contains sentences from PubMed abstracts, selectedby Christine Brune as sentences containing gene interac-tions. It contains 1,806 sentences. That corpus is availableas a secondary data source for the learning tasks Protein-Protein Interaction Task (Interaction Award Sub-task,ISS) from BioCreAtIvE Challenge II [8]. The secondcorpus [37] contains 2,995 sentences mentionning inter-actions between genes selected by an expert. The unionof those two corpora results in a dataset containing 4,801sentences about gene interactions.Sequential pattern extractionDatamining taskAs previously mentionned, the extraction of sequentialpatterns from the training corpus needs the computationof POS tags. For this task, we use the treetagger tool [38].In addition, for the data mining task, minsup is set to10. It means that a sequential pattern is frequent if itappears in at least 10 sentences (i.e. in more than 0.2%of sentences). Indeed, with that threshold some irrelevantpatterns are not taken into account while many patternsof true gene interactions are discovered. Note that otherexperiments, not reported here, have been conductedwith greaterminsup values (15 and 20).With those greaterminsup, some relevant patterns for interaction detectionare lost.More than 32 million of frequent sequential patternsare extracted, with minsup equals to 10. This number islarge but the extraction takes only 15 minutes (the extrac-tion tool is dmt4sp [39]). The application of constraintssignificantly reduces the number of sequential patterns.Indeed, the number of sequential patterns satisfying theCellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 6 of 12constraints is about 65,000. Note that the application ofthe constraints was not time consuming and takes lessthan two minutes. However, the number of remainingpatterns is still prohibitive for analysis and validation byhuman experts.The recursive mining also reduces significantly thenumber of sequential patterns. From the extracted pat-terns, we build a subset of patterns for each noun or verb.The number of built subsets is 515 (365 for nouns, 150for verbs). The recursive mining of each subset exhibitsat most k sequential patterns to represent that subset. Inthis experiment, we set the parameter k to 4. It allows sev-eral patterns to be kept for each noun or verb in orderto cover a sufficient number of different cases (for exam-ple 4 patterns corresponding to 4 syntactic constructionswith the verb inhibit@vvn are computed). At the end ofthe recursivemining, there remain 667 sequential patternsthat can represent interactions or their characterizationsf.That number, which is significantly smaller than the pre-vious one, guarantees the feasibility of an analysis of thosepatterns by experts. The recursive mining of those subsetsis also very fast and takes about 2 minutes.Extracted sequential patternsThe 667 remaining sequential patterns were analyzed bytwo experts in 90 minutes.The patterns are grouped together by noun or verb, theexperts have thus to classify 380 groups. But some nounsor verbs are repeated with different POS tagged informa-tion (e.g., analyze@vvd and analyze@vvn), so these groupsare not considered independently by the experts and ithelps the validation task (for instance both versions ofverb "analyze"I? are pruned together). Actually, there are285 different nouns and verbs. Moreover, at this point ofthe validation, the patterns are roughly split into three setsby the experts: "interaction patterns", "characterizationpatterns"I? and "not relevant".Finally, the experts validated 232 sequential patternsfor interaction detection, 231 patterns for characteriza-tion of interactions and they removed the remaining (i.e.204 unuseful patterns). Indeed, the latter do not con-vey information about interactions, in particular thereare generic verbs like "appear"I? and "contain". Among thefirst group of 232 patterns, some explicitly give the typeof the interactions. For example, ?AGENE interact@vvzwith@in AGENE?, ?AGENE bind@vvz to@to AGENE?,?AGENE deplete@vvn AGENE? and ?activation@nn of@inAGENE by@in AGENE? describe well-known interac-tions (binding, inhibition, activation). Note that whenthe patterns are applied, zero or several words mayappear between two consecutive items of the pattern.For example, the pattern ?AGENE interact@vvz with@inAGENE? matches the sentence <gene_name=MYC>interacts with <gene_name=STAT3>. and also the sen-tence <gene_name=MYC> interacts with genes in par-ticular <gene_name=STAT3>g.Other patterns represent more general interactions andexpress the fact that a gene plays a role in an activity ofanother one. Representative patterns of this kind are forinstance ? AGENE involve@vvn in@in AGENE?, ?AGENEplay@vvz role@nn in@in the@dt AGENE? and ?AGENEplay@vvz role@nn in@in of@in AGENE?. Note that theinvolve verb and the play role in phrase were notreported in [40,41] and [21].The second group of 231 patterns for characterizationrepresents other kinds of semantics information: modal-ities or biological context, for instance, ?in@in fibrob-last@nns AGENE AGENE? or ?the@dt possibility@nnthat@in/that AGENE AGENE?. Figure 2 depicts the tax-onomy that we define and use in our experiments forthe characterization patterns. That taxonomy was builtwith the help of the extracted patterns. The modalitypatterns express the confidence in the detected interac-tions. Modality can be seen as a kind of uncertainty [42].We define four levels of confidence: Assumption, Obser-vation, Demonstration and Related work, and anothersubclass representing the Negation (patterns denotingevidence of absence of interaction). For example, thesentence "It suggests that <gene_name=MYC> inter-acts with <gene_name=STAT3>" has a lower confidencethan "It was demonstrated that <gene_name=MYC>interacts with <gene_name =STAT3>". The biologicalcontext patterns indicate information about the biolog-ical context of interactions, for example the disease orthe organism involved in the interaction. That class issplit into four subclasses: organism, component, biologicalsituation and biological relation. The subclass organismrepresents the organisms involved in the interaction (e.g.,Figure 2 Taxonomy for characterization patterns. Figure 2 describes the taxonomy used to classified the extracted sequential patterns.Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 7 of 12mouse, human). The subclass component representsthe anatomy/biological components (e.g. breast orfibroblast). The subclass biological situation gives theframework of interactions, for example, cancer, tumoror in vitro. The last subclass gives, when applicable, thebiological relation (e.g., homology).The sequential patterns obtained are linguistic extrac-tion rules that can be used on biomedical texts to detectand characterize interactions between genes. Note thatto be applied, those patterns do not need a full syntacticanalysis of a sentence.Indeed, the matching process tries to instantiate eachelement of the pattern in the given sentence. For each pat-tern, every possible matching within the sentence is testedand not only the first one.Application: detection and characterization of geneinteractionsWe have evaluated the quality of the sequential patternsfound in the previous section as information extractionrules. In this section, we present the experimental settingsand the results.Testing corpora and evaluation criteriaTesting Corpora We have considered three well-knowntesting datasets (cf Table 2 and Table 3): AIMed [43],BioInfer [44], HPRD50 [45] and a fourth testing corpusextracted from PubMed [1] (more information is givenin the next section). Note that, in AIMed, BioInfer andHPRD50, the names of genes are already identified andtagged. More information about those corpora can befound in [46].Construction of the PubMed corpus In order to test thesequential patterns extracted in the previous section aslinguistic extraction rules to characterize interactions, weneed a testing corpus.We have built a testing corpus that is a subset ofabstracts from the PubMed database. It is built in twoTable 2 Results of the application of the extracted patternsCorpus # Recall Precision f ? score f ? scoreSentences presented in [11]details given in Table 3AIMed 1955 78.6 35.6 49 [34.7, 41.5]BioInfer 1100 46.5 25.3 32.8 [15.9, 40.6]PubMed 200 75.0 83.0 78.7 ?HPRD50 145 66.8 46.7 55.0 [38.3, 69.8]Table 2 gives the list of the four testing corpora used to evaluate the proposedapproach, and the results of the evaluation. The meaning of the columns is: thename of the corpus, the number of the sentences in the corpus, the recall scoreof the proposed approach applied on the corpus, the precision score of theproposed approach applied on the corpus, thef -score of the proposed approachapplied on the corpus. The last column indicates the range of the f -scorespresented in [11] with also a cross-corpus validation.steps which are described below. The first step is the selec-tion of abstracts from PubMed. In the PubMed database,each paper has an identifier called PMID (PubMed IDen-tifier). For each official acronym of gene in the HUGO [47]dictionary, a request is sent to PubMed in order to getall PMID of papers that contain the gene. An index ofgenes and their associated papers is thus created. Then theinverted index is computed, i.e. the index that associatesto each PMID the list of genes. From that second index,the PMIDs that do not have at least two gene names intheir list are pruned. Indeed, as we are looking for inter-actions between genes, it implies that at least two genesare mentioned in the text. There remains 624,519 PMIDs.The second step is the named-entity recognition. Some-times, the gene name used to index an abstract and thegene name that appears in the abstract text are different.Indeed, a gene can be represented by different synony-mous forms. It is thus important to identify the gene in thetext; that task is called Named-Entity Recognition (NER).We propose to use a "dictionary-based" approach [48].Although that kind of approaches usually has a good pre-cision, it does not provide a good recall. We propose someimprovements to increase the recall.First, all genes associated to the PMID of an abstract aresearched into that abstract using official acronyms fromthe HUGO dictionary. With that approach only 48.1% ofabstracts have at least two recognized genes. In addition,we identified 182 official acronyms as common Englishwords (e.g., AGO, AS, BAD)h. In order to reduce the num-ber of mistakes, they are considered as gene names onlywhen they are in uppercase.Second, in order to improve the number of recognizedgenes in abstracts, other fields of the HUGO dictionaryare used: old acronyms, alias acronyms, and completenames. With that improvement, 61.7% of abstracts have atleast two recognized genes. Note that this improvement ismainly due to the alias acronyms (+ 9%).The last improvement is the use of significant parts ofthe complete official name. The official name is often long,and authors do not write it completely. Instead of look-ing for the complete name, we look for significant partsof it. We identify three common kinds of significant parts:a word ending by in (e.g., insulin), a word ending byase (e.g., transferase) and a word followed by protein(e.g., AE binding protein 1). For instance, for gene alka-line ceramidase 3, the significant part is ceramidase andthus to recognize this gene name in texts, only cerami-dase would be used. The plural forms are also taken intoaccount (e.g., caspases, kinases). With that improve-ment, 66.1% of the 624,519 abstracts have at least tworecognized genes and form the testing corpus.Evaluation criteria for the extraction of gene inter-actions and their contextual information We evaluateCellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 8 of 12Table 3 Details of information presented in paper [11]Method SL SL SpT SpT kBSPS kBSPS edit edit APG APGTraining corpus (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer) (AIMed) (BioInfer)AIMed - 41.5 - 34.7 - 40.3 - 39.6 - 37.9BioInfer 40.6 - 24.3 - 24.8 - 15.9 - 22.5 -HPRD50 59.0 61.8 43.2 51.3 51.0 69.8 38.3 62.4 61.6 62.1The acronyms used in this table are the ones used in paper [11]: SL: Shallow linguistic kernel; SpT: Spectrum tree kernel; kBSPS: k-band shortest path spectrum kernel;edit: Edit distance kernel; APG: All-paths graph kernel. See paper [11] for more details.our approach with a cross-corpus evaluation to show thegenericity of the proposed approach. It means that weextract the patterns from a corpus and apply them on theother four corpora [11]. Note that in the literature manyapproaches are evaluated with a cross-validation, whichmeans that a corpus is split in several parts, one part isused to learn and the rest is used to apply.It is thus much more difficult to get good results with across-corpus evaluation than a cross-validation.Indeed, there is more heterogeneity between corpora(i.e., corpus characteristics are different) than betweenparts of a single corpus [11].We use the f -score function as an evaluation measure,which is defined as f -score = 2×Precision×RecallPrecision+Recall .Detection of gene interactionsWe have applied the 232 extracted sequential patterns aslinguistic extraction rules to detect interactions on thefour corpora.All corpora used for evaluation have all gene namesreadily tagged. This means that our results only mea-sure the performance of gene interaction extraction andare not influenced by the issue of named entity recogni-tion. Therefore, to compute the f-score, a true positive isa couple of mentioned gene names in the sentence (i.e.the gene names given in the tags) which are in interac-tion and detected as an interaction by our method. Table 2gives the results. We did not have any gold standardreference to evaluate the results for the testing corpusfrom PubMed. Since we cannot implement an automaticvalidation, we randomly took 200 sentences among thesentences of the PubMed testing corpus. Then, we car-ried out a POS tagging and assessed the performances ofthe extraction rules to detect interactions in the 200 sen-tencesi. The f -score for the gene interaction detection forthe testing corpus is 78.7. In Table 2, the last column indi-cates the range of the f -scores presented in [11] with across-corpus validation. Several kernel-based approachesare presented in [11], the range allows to show the worstand the best results among all those methods. Note thatthe best result of the ranges is not achieved in practiceby the same method. Our approach gives results com-parable to the results given by state-of-the-art methodsand it is even better for the gene interaction detection inAIMed. This last result is important because AIMed is thelargest corpus and the most commonly used in the liter-ature. Moreover, our approach is simple and allows moreinformation that just the presence of an interaction to beextracted. Indeed, thanks to the patterns, semantics infor-mation can also be extracted, contextual information (seenext section) but also information about the kind of inter-action (e.g., inhibition, binding) and the direction of theinteraction.Characterization of gene interactionThe method also gives information about modality andabout the biological context: biological situation, compo-nent, organism, biological relation. For that characteri-zation task, there exist some methods dealing with thesubtask of the detection of sentences containing uncer-tainty [42] (modality can be seen as a kind of uncertainty)but few adress the biological characterization problem. Itwas thus difficult to compare our result for the interac-tion characterization with a gold standard. We randomlytook 200 sentences containing at least two gene namesamong the sentences of the testing corpus extracted fromPubMed. Those sentences are not the same ones that areused to evaluate the interaction detection but they comefrom the same testing corpus PubMed. Out of 200 inter-actions, there are 149 characterizations (71modalities and78 biological context). The sentences have been annotatedby a computer scientist with specialisation in NLP and abiologist. Then, we evaluated the precision and recall. Thecharacterization patterns are applied on a pair of genesthat is already detected as in interaction. We evaluate thecharacterization at the interaction level. The precision is88% and the recall is 69% (f -score= 77). Several reasonsexplain why the recall is not greater and are discussed inthe next section.DiscussionIn this section, the results of the previous section are dis-cussed from a qualitative point of view and we present aprocess to support pattern validation.About interaction detectionIn the experiments a linguistic pattern is matched againsta whole sentence at a time. That wide scopemay introduceCellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 9 of 12ambiguities in the detection of interactions, and false pos-itives, when more than two genes appear in a sentence.For example, in sentence FGF-7 recognizes one FGFR iso-form known as the FGFR2 IIIb isoform or keratinocytegrowth factor receptor (KGFR), whereas FGF-2 binds wellto FGFR1, FGFR2, and FGFR4 but interacts poorly withKGFR. an interaction between FGFR2 IIIb and FGFR1 isdetected. Actually, there is no interaction between thosetwo genes, they only appear in two different propositionsof the same sentence. FGFR1 interacts with FGF-2 in thesecond proposition but since there is no limitation of thescope, an interaction between FGFR1 and FGFR2 IIIb isalso detected. Several cases are possible: when severalbinary interactions are present in the sentence or when theinteraction is n-ary (n ? 3). The case of n-ary interactionscan be solved with a training data set containing n-aryinteractions. The other cases can be treated by introduc-ing limitations of pattern scope, for example cue-phrases(e.g., but, however).False negatives depend on the absence of some nounsor verbs of interaction in the patterns. For example, thenoun modulation is not discovered whereas the verbmodulate appears in sequential patterns. This suggeststhat the use of linguistic resources (e.g. lexicon or dic-tionary), manually or semi-automatically, would improveinteraction patterns and thus interaction detection.About interaction characterizationThe false negatives, which are dependent on the absenceof some patterns, are also an important problem for inter-action characterization.For example, in our experiments in the sentence<gene_name=BRCA1> interacts in vivo and in vitrowith the Rb-binding proteins, <gene_name=RBBP7> and<gene_name=RBBP4>[ ...] the biological situation invitro is detected whereas in vivo is not detected. Indeed,there is no sequential pattern extracted from the trainingcorpus that contains in vivo. That case is considered astrue positive for in vitro interaction and as false negativefor in vivo interaction. The recall (69%) is strongly depen-dent on the number of false negatives. Note that the falsenegatives mainly come from biological contexts not suf-ficiently represented (about 92%). It is explained by thedifficulty to have a training corpus that contains all bio-logical context (e.g, body parts as liver, pituitary gland,diseases). As for interaction detection, using a special-ized lexicon would increase the vocabulary and thus thenumber of patterns and would improve those results.About validation of sequential patterns as linguisticextraction rulesSection Method shows how the sequential patterns areautomatically extracted from a corpus. Those patterns arethen analyzed and validated by two experts as linguisticextraction rules. But sometimes, the needed resources(e.g., time, expert) can be missing or the number ofsequential patterns can be too large to be easily managedby a human. In those cases, for the selection and validationof patterns, we propose an automatic process based on theuse of background knowledge. The selection is thus lessaccurate than a manual selection but can be automatic.The automatic validation process is based on two steps.First, each sequential pattern is applied on a corpuscalled rule validation corpus. It provides for each patternthe following information: the genes detected as interact-ing and the associated sentences.Second, a gene interaction database is used as an oracleto assess the patterns. In our method, the rule valida-tion corpus comes from the PubMed papers and the geneinteraction database is BioGRID. Our idea is that the rel-evant patterns, when applied on the validation corpus,retrieve interactions that must be consistent with the geneinteraction database. An interaction detected by a sequen-tial pattern is considered as a false positive if the interac-tion does not exist in the gene interaction database, else itis a true positive (same gene names and same PMID)j.A pattern with a high number of true positives is likelyto be interesting.Table 4 gives an excerpt of the information providedfor each pattern. It contains the number of interactionsTable 4 Examples of information about the application ofinformation extraction rulesNumber of Number ofInformation retrieved interactions true positivesextraction ruleAGENE AGENE the@dt 6 1response@nnAGENE AGENE serine@nn 3 3AGENE reveal@vvd AGENE 0 undefinedAGENE association@nn 6 4with@in AGENEAGENE bind@vvz to@to AGENE 8 5Table 4 gives an excerpt of provided information about patterns extracted fromthe PubMed corpus. The meaning of the columns is: sequential pattern, numberof interactions detected by the pattern and number of detected interactions thatare correct with respect to the oracle, i.e. interactions that also exist in BioGRID.The first pattern can be read as a gene followed by a gene then by the word theand the word response. This pattern detects 6 interactions and 1 is in BioGRID.The second pattern can be read as a gene followed by a gene then by the wordserine. It detects 3 interactions that are all in BioGRID. The third pattern can beread as a gene followed by the verb reveal in past tense, then by a gene. Thispattern does not detect interactions in the rule validation corpus, thus noinformation is provided to evaluate it. The fourth pattern can be read as a genefollowed by the noun association, then by the word with and a gene name. Itdetects 6 interactions out of which 4 are in BioGRID. The fifth pattern can be readas a gene followed by the verb bind in present tense, then by the word to and agene name. This pattern detects 8 interactions and 5 of them are in BioGRID.For example, it detects that the following complex sentence Cbl is a cytosolicprotein that is rapidly tyrosine phosphorylated in response to Fc receptoractivation and binds to the adaptor proteins Grb2, CrkL, and Nck. contains anassociation between two signalling molecules (Cbl and Grb2).Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 10 of 12detected by the pattern and the number of detected inter-actions that are correct with respect to the oracle, i.e.interactions that also exist in BioGRID. For example, thefifth pattern can be read as a gene followed by the verbbind in present tense, then by the word to and a genename. This pattern detects 8 interactions and 5 of themare in BioGRID. For example, it detects that the follow-ing complex sentence Cbl is a cytosolic protein that israpidly tyrosine phosphorylated in response to Fc receptoractivation and binds to the adaptor proteins Grb2, CrkL,and Nck. contains an association between two signallingmolecules (CBL and GRB2).Those measures can be used to automatically selectpatterns as linguistic information extraction rules.To end up with a more speculative note, this step couldalso be interesting even when there is an expert to selectthe patterns by providing more information to help them.In addition, a pattern with low number of true positivescan retrieve sentences that really contain interactions.This can be the case if the interaction is not reportedin BioGRID or is reported but the gene names in thesentence are other gene names than the ones used inBioGRID. Therefore, it is interesting to provide, for eachpattern, the detected interaction sentences. Table 5 givesan excerpt of interactions detected by the sequential pat-tern: AGENE association with AGENE. For instance, thepattern detects that in the paper with PMID 10204582, aninteraction between genes SHC1 and CRKL is mentioned(the pattern matches a sentence of the abstract) butaccording to BioGRID, there is no interaction betweenSHC1 and CRKL. The discovered interaction in the sen-tences in paper 10204582 is thus unexpected in BioGRID.The pattern also detects that in this paper, an interactionbetween genes CBL and CRKL is mentioned, and indeed,according to BioGRID, there is an interaction betweenCBL and CRKL mentioned in paper 10204582. It is inter-esting to note that the three genes involved harbour allthree similar biological functions (they are all signallingmolecules) and that their association is fully relevant asexemplified by the strong functional connectivity detectedin the STRING database between those three genes.Therefore the extracted interaction between genes SHC1and CRKL is fully relevant even if it does not appear inBioGRID. Of course, more systematic studies should beundertaken to ascertain this, but this is beyond the scopeof the present paper.ConclusionsWe have proposed an original approach to help expertsto design linguistic information extraction rules by auto-matically extract sequential patterns filtered by linguisticconstraints and recursive mining. Unlike existing meth-ods, our approach is independent of syntactic pars-ing and only requires the training corpus as externalresource to learn patterns (note that interaction cluesare not annotated in the training corpus). The patternsrepresenting interactions and their characterizations areautomatically discovered from texts. An advantage ofthe use of sequential patterns as linguistic rules is thatthey are understandable and manageable by an expert.If needed, the expert can easily modify the proposedrules or add new ones. To the best of our knowledge,there are few methods that automatically extract inter-action patterns and also characterization patterns (i.e.,patterns for contextual information about the discoveredinteractions).Table 5 Example of information for a sequential patternPMID Gene 1 Gene 2 BioGRID verdict Sentence10204582 SHC1 CRKL not in BioGRID These results suggest a fundamental role for thetyrosine phosphorylation of Cbl, CrkL, SLP-76, and<gene_name="SHC1"> and the associationof Cblwith<gene_name="CRKL">, SLP-76,and Nck in Fc gammaRI signaling in humanmacrophages.10204582 CBL CRKL in BioGRID PP1, a specific inhibitor of Src kinases, inhibitedthe Fc gammaRI-induced respiratory burst,as well as the tyrosine phosphorylation of<gene_name="CBL"> and its inducibleassociation with<gene_name="CRKL">.Table 5 gives 2 interactions (highlighted in bold in the table) detected by the sequential pattern: AGENE association with AGENE.The meaning of the columns is: the id number of the paper in PubMed, the genes that interact, the verdict of the oracle and the sentence where the interaction isrecognized. For instance, the pattern detects that in paper 10204582, an interaction between genes SHC1 and CRKL is mentioned, because the pattern matches asentence of the abstract of the paper, but according to BioGRID, there is no interaction between SHC1 and CRKL and the discovered interaction in the sentences inpaper 10204582 is unexpected because not in BioGRID and interesting. The pattern also detects that in this paper, an interaction between genes CBL and CRKL ismentioned, and indeed, according to BioGRID, there is an interaction between CBL and CRKL mentioned in paper 10204582.Cellier et al. Journal of Biomedical Semantics  (2015) 6:27 Page 11 of 12The experiments show how our approach enables todiscover interactions and their characterizations. Ourapproach gives results comparable to the results given bystate-of-the-art methods and is even better for the geneinteraction detection in AIMed. The main advantagesof our approach are that, first, semantics informationare extracted in addition to the information about thepresence of an interaction; second, the patterns, used asextraction linguistic rules, are automatically discovered.Further work will look how enhance the extracted pat-terns thanks to other information sources (e.g., specializeddictionaries).Availability of software and supporting dataThe extracted gene interactions from PubMed areavailable at https://bingotexte.greyc.fr/. The evaluationcorpora from PubMed are available at https://cremilleux.users.greyc.fr/jbms/. The software (SMBio) that allowssequential patterns of gene interactions to be extracted isavailable at https://bingo2.greyc.fr/?q=node/22. The list ofthe 182 official acronyms identified as common Englishwords is available at: https://bingotexte.greyc.fr/ambig_names.EndnotesaIn the rest of the paper, the term expert is used for alinguist or a biologist; both skills are useful to validaterules.bNotice that this is a simplified form of sequences,while in the general sequential pattern miningframework, a sequence is a list of sets of items, and notonly a list of items.cAn anaphoric structure is the use of a linguistic unit,such as a pronoun, to refer back to a gene name.dPOS (Part-Of-Speech) tags are grammaticalinformation about words. For example, nnmeanscommon noun and vvpmeans verb in non-3rd personalsingular present. The exhaustive list of POS tags can befound at: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/.eInstitut de Biologie du Développement deMarseille-Luminy.fThe maximum number of patterns per verb and nounis 4, thus the maximum number of patterns afterapplying recursive mining is 2060. The number ofpatterns in the results is only 667 because some verbsand nouns are represented by less than 4 patterns.gNote that such a functional relationship betweenMYC and STAT-3 can be illustrated by the fact that theexpression of the c-myc gene is under the control of theSTAT-3 signalling pathway (see [49] for a review).hThe list is available at: https://bingotexte.greyc.fr/ambig_names.iDiscovered interactions for the whole testing corpusare available at https://bingotexte.greyc.fr/.jBioGRID provides information about geneinteractions and the PMID of the articles where theinteractions are mentioned. In order to measure theaccuracy of the patterns, we take into account these twopieces of information.AbbreviationsNLP: Natural language processing; NER: Named-entity recognition; IE:Information exctraction; BioNLP: Biological natural language processing; POS:Part-Of-Speech; minsup: support threshold; PMID: PubMed IDentifier.Competing interestsThe authors declare that they have no competing interests.Authors contributionsPC, TC, MP, CR and BC developped the interaction detection andcharacterization method, and the validation step. Experiments were mainlyconducted by MP, PC and TC. JK helped to design the evaluation. JLM and TCdevelopped the named-entity recognition method. OG provided thebiological expertise. JLM built the website interface. All authors read andapproved the manuscript.AcknowledgementsThis work is partly supported by the ANR (French Research National Agency)funded project Hybride ANR-11-BS002-002 and progam InvestissementsdAvenir ANR-10-LABX-0083.Author details1INSA de Rennes, IRISA, UMR6074, F-35042 Rennes, France. 2Université de Paris13, LIPN, UMR7030, F-93430 Villetaneuse, France. 3Université Lyon 1, LIRIS,UMR5205, F-69622 Lyon, France. 4INSA de Lyon, LIRIS, UMR5205, F-69621Lyon, France. 5Université de Caen, GREYC, UMR6072, F-14032 Caen, France.6Université Lyon 1, CGMC, UMR5534, F-69622 Lyon, France. 7Faculty ofElectrical Engineering, Czech Technical University, Prague, Czech Republic.Received: 25 July 2013 Accepted: 22 April 2015JOURNAL OFBIOMEDICAL SEMANTICSWinnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 DOI 10.1186/s13326-015-0007-3RESEARCH Open AccessUsing description logics to evaluate theconsistency of drug-class membership relations inNDF-RTRainer Winnenburg1,2, Jonathan M Mortensen1,2 and Olivier Bodenreider1*AbstractBackground: The NDF-RT (National Drug File Reference Terminology) is an ontology, which describes drugs andtheir properties and supports computerized physician order entry systems. NDF-RTs classes are mostly specifiedusing only necessary conditions and lack sufficient conditions, making its use limited until recently, when asserteddrug-class relations were added. The addition of these asserted drug-class relations presents an opportunity tocompare them with drug-class relations that can be inferred using the properties of drugs and drug classes in NDF-RT.Methods: We enriched NDF-RTs drug-classes with sufficient conditions, added property equivalences, and then usedan OWL reasoner to infer drug-class membership relations. We compared the inferred class relations to the recentlyadded asserted relations derived from FDA Structured Product Labels.Results: The inferred and asserted relations only match in about 50% of the cases, due to incompleteness of the drugdescriptions and quality issues in the class definitions.Conclusions: This investigation quantifies and categorizes the disparities between asserted and inferred drug-classrelations and illustrates issues with class definitions and drug descriptions. In addition, it serves as an example of thebenefits DL can add to ontology development and evaluation.Keywords: Ontology, Description logics, Quality assurance, National drug file-reference terminologyIntroductionWe rely on ontologies throughout biomedicine, from thelife sciences to the clinic [1]. As Electronic HealthRecord adoption increases in the clinic, so too will thereliance on the ontologies that facilitate their meaningfuluse. Clinical decision support and analytics are functionssupported by ontologies. For example, computerizedphysician order entry (CPOE) systems typically leveragedrug ontologies to ensure that patients are safelyprescribed drugs in accordance with clinical guidelines(e.g., [2]).An example of such an ontology is the National DrugFile-Reference Terminology (NDF-RT), an extension tothe drug formulary used by the Veterans Administrationand developed using a description logics (DL) formalism.* Correspondence: olivier@nlm.nih.gov1Lister Hill National Center for Biomedical Communications, National Libraryof Medicine, National Institutes of Health, Bethesda, MD, USAFull list of author information is available at the end of the article© 2015 Winnenburg et al.; licensee BioMed CeCommons Attribution License (http://creativecreproduction in any medium, provided the orDedication waiver (http://creativecommons.orunless otherwise stated.It provides a rich description of pharmacologic classes inreference to properties, such as mechanism of action,physiologic effect, chemical structure and therapeuticintent. NDF-RT can be leveraged to prevent a patientallergic to penicillin drugs from being prescribed amoxi-cillin, a penicillin antibacterial.However, NDF-RT only specifies necessary conditionsfor class membership to the pharmacologic classes, butnot sufficient conditions. (In DL parlance, these classesare primitive, not defined.) As a consequence, a DLreasoner is unable to classify automatically drugs asmembers of a given pharmacologic class, even whenboth drugs and pharmacologic classes are described interms of the same properties. The inability to classifydrugs into their classes limits the usefulness of NDF-RTin systems like CPOE that rely on such information.In previous work, where we overcame this limitationby augmenting the pharmacologic classes with necessaryand sufficient conditions, we found that we could inferntral. This is an Open Access article distributed under the terms of the Creativeommons.org/licenses/by/4.0), which permits unrestricted use, distribution, andiginal work is properly credited. The Creative Commons Public Domaing/publicdomain/zero/1.0/) applies to the data made available in this article,Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 2 of 9drug-class membership relations effectively [3]. Specific-ally, we demonstrated the use of a modified version ofNDF-RT for clinical decision purposes (patient classifica-tion). One limitation of this work was that we did notevaluate the inferred drug-class membership relationsbeyond our proof-of-concept application.NDF-RT recently integrated authoritative drug-classmembership assertions extracted from the StructuredProduct Labels (package inserts) by the Food and DrugAdministration (FDA), along with a specification of thedrugs in terms of the same properties used for specifyingthe classes. These assertions remove the drug-classmembership limitation we highlighted earlier, insteadproviding explicit drug-class membership relations thatdo not rely on DL reasoning. But precisely because theseasserted drug-class relations have been made independ-ently of the logical definitions of the classes, there is thepossibility for the asserted and inferred drug-class mem-bership relations to be inconsistent.The objective of this work is to evaluate the consistencyof the drug-class membership relations that were inferredfrom the pharmacologic class definitions and drugdescriptions, against the newly asserted, authoritativedrug-class membership relations. This evaluation is alsoan indirect contribution to the assessment of the class def-initions and the drug descriptions in terms of complete-ness and consistency (i.e., agreement between informationsources).BackgroundNDF-RT drugs and classesThe National Drug File Reference Terminology (NDF-RT) is a resource developed by the Department ofVeterans Affairs (VA), Veterans Health Administration,as an extension of the VA National Drug File [4]. Likeother modern biomedical terminologies, NDF-RT is de-veloped using description logics and is available in nativeXML format. The version used in this study is the latestversion available, dated November 3, 2014, downloadedfrom [5], from which we derived our augmentedrepresentation.This version covers 7,287 active moieties (DRUG_KIND, level = ingredient), as well as 543 EstablishedPharmacologic Classes (EPCs) specified in reference tosome of the properties of the active moieties. NDF-RTnow contains several sources of relations between drugsand their properties. The April 2014 version of NDF-RTintroduced a new set of relations between drugs andtheir properties originating from the class indexing filereleased as part of DailyMed, identified by the suffixFDASPL. Moreover, this version also introduced au-thoritative drug-class membership assertions from thesame source. Finally, NDF-RT also provides a specifica-tion of the EPCs in reference to the same propertiesused for describing the drugs themselves, provided byFederal Medication Terminologies subject matter ex-perts and identified by the suffix FMTSME. In thiswork, we focus on the drug-property assertions fromFDASPL, class-property assertions from FMTSME, anddrug-class assertions provided by the FDA.Description logicsIn short, Description Logics (DL) are a set of logicalconstructs with which one can develop ontologies.Krötzsch and colleagues provide a more formal intro-duction to DL [6]. Like other knowledge representationmethods, DL allows one to specify, in a computablefashion, the entities (i.e., classes) that exist in a given do-main and the relationships (i.e., relations) between them.In comparison to older methods of knowledge represen-tation, DL ensures common, unambiguous semantics sothat the ontologys interpretation is consistent acrosssoftware and users. This consistent logical underpinningenables the use of reasoners, which are programs thatcompute (i.e., infer) the logical entailments (i.e., conclu-sions) of a given ontology. For example, if Alprostadilhas physiologic effect Venous dilation and Venous dila-tion is-a Vasodilation, a reasoner concludes that Alpros-tadil has physiologic effect Vasodilation. A typicalapproach to developing ontologies with DL is to specifya set of properties that each class has (e.g., Penicillinantibacterial has ingredient Penicillin and treats or pre-vents Bacterial infection; Antiseptic treats or preventsBacterial infection) and then infer the additional rela-tions among classes. With a set of specified classes, areasoner can then classify them into an inferred hier-archy. In our example, the inferred hierarchy wouldshow that Penicillin antibacterial is-a Antiseptic. In thecontext of this study, NDF-RT uses this same approach,specifying EPCs in terms of their properties. Unlike theexample above, however, pharmacologic classes in NDF-RT (EPCs) are primitive, in that they only specify thenecessary conditions of class membership, and thereforeprevent a reasoner from constructing a useful inferredhierarchy. Later, we describe how we enrich NDF-RTwith sufficient conditions so that we can take full advan-tage of a reasoner.In this work, we use OWL, the web ontology language,a web standard for developing ontologies that leveragesDL. OWL is the de facto standard for biomedical ontol-ogies and there is a suite of tools for developing OWLontologies, including development environments such asProtégé [7] and reasoners such as HermiT [8].Related workIn addition to being used as a framework for building on-tologies, DL has been shown to be useful for reasoningwith biomedical entities, including protein phosphatasesWinnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 3 of 9[9] and penetrating injuries [10]. However, to our know-ledge, DL reasoning has not yet been applied to the auto-matic classification of drugs, except for our previous workon anti-coagulants [3].NDF-RT is used frequently as a resource for standard-izing pharmacologic classes (e.g., [11,12]). However, in-vestigators generally use the drug properties as classes(e.g., drugs that have the physiologic effect decreased co-agulation activity for anti-coagulants), rather than theEPCs. Moreover, only asserted relations are used in mostinvestigations, as opposed to inferred drug-class relations.The specific contribution of this paper is the augmen-tation of the logical definitions of pharmacologic classesin NDF-RT to enable the automatic inference of drug-class membership relations using a DL reasoner. Wesubstantially extend our previous work on anticoagu-lants, by generalizing it to all pharmacologic classes andproviding a comparison to authoritative, asserted drug-class relations from the FDA.MethodsOur approach to evaluating inferred drug-class member-ship relations in NDF-RT is summarized as follows.First, we converted the NDF-RT data from their originalformat (XML) to a DL format (OWL). This conversionprocess augments the EPCs with necessary and sufficientconditions. These conditions allowed a DL reasoner toclassify drugs into their respective classes using the classdefinitions and the properties of drugs. We created twoOWL datasets. One, used as a gold standard, only con-tains the asserted, authoritative drug-class relations. Incontrast, these asserted relations have been removedfrom the second dataset, so that only inferred drug-classrelations were present after the reasoner runs (i.e., in-ferred by the reasoner). We ran a DL reasoner and thencompared inferred and asserted drug-class relationsfrom the perspective of drugs and from that of classes.In order to restrict this investigation to clinically sig-nificant drugs, we mapped all NDF-RT ingredients toRxNorm and required that ingredients be linked to clin-ical drugs. We further normalized all ingredients to baseingredients in RxNorm, to abstract away from minor dif-ferences in ingredients, including salts, esters and com-plexes, which rarely affect drug-class membership. Inpractice, we mapped the precise ingredients in RxNorm(e.g., albuterol sulfate) to their base ingredient (albuterol).Multi-ingredient drugs were ignored, because there isoften more variability in their classification.Augmenting pharmacologic classes with sufficientconditionsIn order to produce the two OWL datasets used forcomparing asserted and inferred drug-class relations, westarted by creating a baseline OWL representationfrom the original XML dataset, which we used as ourasserted dataset (dataset A). Next, as previously de-scribed in [3], we transformed the primitive EPCs intodefined classes by taking the existing set of propertiesfor each class (i.e., necessary conditions) and using themto define the class. In particular, all properties arefolded into a single owl:equivalentClass (?)axiom, thereby specifying necessary and sufficient condi-tions of each class. For the purpose of this work, wefocus on the three main properties used for the descrip-tion of the drugs (mechanism of action, physiologiceffect and chemical structure). Additionally, we lever-aged the therapeutic intent relations (may_treat andmay_prevent) present in NDF-RT, because many EPCsrefer to them in their definitions. These relations linkdrugs and EPCs to disease entities.We further modified this OWL file by applying a seriesof transformations that are necessary for enabling properinference (dataset I). We harmonized the names ofroles used in the definition of the classes (e.g., has_MoA_FMTSME) with those used in the description ofthe drugs (e.g., has_MoA_FDASPL) by creating owl:equivalentProperty axioms between them. Thefollowing equivalences are created: has_MoA_FMTSME ? has_MoA_FDASPL (formechanism of action), has_PE_FMTSME ? has_PE_FDASPL (forphysiologic effect), has_Chemical_Structure_FMTSME ?has_Chemical_Structure_FDASPL, may_treat_FMTSME ?may_treat_NDFRT, and may_prevent_FMTSME ?may_prevent_NDFRT.Inferring relations between drugs and EPCsNext, we leveraged an OWL reasoner to infer the drug-class membership relations from the class definitionsand the descriptions of drugs. Using the necessary andsufficient conditions we created for the classes, an OWLreasoner infers a subclass relation between a drug and apharmacologic class when the properties of the drug andthose of the pharmacologic class are shared. For ex-ample, the class beta2-Adrenergic Agonist [EPC](N0000175779) is defined as equivalent to ('Pharmaceut-ical Preparations' and (has_MoA_FMTSME some'Adrenergic beta2-Agonists [MoA]')). The drug albuterol(N0000147099) has the property has_MoA_FDASPLsome 'Adrenergic beta2-Agonists [MoA]', and is thereforeinferred as being a subclass of beta2-Adrenergic Agonist[EPC]. (The inference will also occur if the property ofthe drug is a subclass of the property used in the defin-ition of the class). Figure 1 provides a schematic of theabove example.Figure 1 Method overview. Relations between the drug albuterol and the class beta2-Adrenergic Agonist [EPC], with asserted and inferred drug-classrelations. Note that there is only one direct path from ingredients to pharmacologic classes through the recently added yellow asserted drug-classrelation. In this study, we compare how often inference using the properties, which produces the dashed orange line, recapitulates the solid yellow line.Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 4 of 9A secondary benefit of the classification with an OWLreasoner is that it creates a hierarchy of the pharmaco-logic classes themselves, based on their logical defini-tions. For example, beta2-Adrenergic Agonist [EPC](N0000175779) is inferred to be a subclass of beta-Adrenergic Agonist [EPC] (N0000175555), because thedefinition of beta2-Adrenergic Agonist [EPC] shownearlier is more specific than that of beta-AdrenergicAgonist [EPC] ('Pharmaceutical Preparations' and(has_MoA_FMTSME some 'Adrenergic beta-Agonists[MoA]')). For this reason, we reclassified both OWLdatasets, although no inferred drug-class relationswere generated in dataset A.Figure 2 provides a screenshot from Protégé of apharmacologic class before enrichment and Figure 3shows its definition after. Before enrichment, the classbeta2-Adrenergic Agonist [EPC] has no sufficient condi-tions (the section Equivalent To is empty) and theEPCs are not hierarchically related (beta2-AdrenergicAgonist [EPC] and beta-Adrenergic Agonist [EPC] are atthe same hierarchical level, i.e., part of a flat list ofEPCs). The drug albuterol is asserted to be a member ofthe class beta2-Adrenergic Agonist [EPC]. In contrast,after enrichment (and reclassification), the class beta2-Adrenergic Agonist [EPC] has acquired sufficient condi-tions (visible in the section Equivalent To) and theEPCs are now hierarchically related (beta2-AdrenergicAgonist [EPC] is a subclass of beta-Adrenergic Agonist[EPC]). The drug albuterol is inferred to be a member ofthe class beta2-Adrenergic Agonist [EPC].Comparing asserted and inferred drug-class relationsWe compared asserted (dataset A) and inferred (data-set I) drug-class relations from the perspective of drugsand pharmacologic classes, respectively. In both cases,we issued queries against the OWL datasets (after re-classification). For each drug, we queried its set ofpharmacologic classes in each dataset and determinedwhich classes are common to both datasets vs. specific toone dataset. For example, the drug albuterol(N0000147099) has the same class in both datasets, beta2-Adrenergic Agonist [EPC] (N0000175779). In contrast, thedrug hydrochlorothiazide (N0000145995) has an assertedrelation to Thiazide Diuretic [EPC] (N0000175419), butan inferred relation to Thiazide-like Diuretic [EPC](N0000175420). For each pharmacologic class, we queriedits set of drugs in each dataset and determined whichdrugs are common to both datasets vs. specific to onedataset. In order to consider higher-level classes to whichdrugs are not direct members, we used the transitive clos-ure of the hierarchical relation rdfs:subClassOf. As aconsequence, a given class will have as members not onlyits direct drugs, but also the members of all its subclasses.For example, in both the A and I datasets, the classbeta-Adrenergic Agonist [EPC] has the base ingredientalbuterol as an indirect member through its subclass classbeta2-Adrenergic Agonist [EPC]. Of note, the salt ingre-dient albuterol sulfate is ignored as a result of thenormalization to RxNorm base ingredients describedearlier.ImplementationThe modifications described above were performedusing an XSL (eXtensible Stylesheet Language) trans-formation. The resulting OWL file was classified withHermiT 1.2.2 [8]. Protégé 5.0 was used for visualizationpurposes [7]. The OWL file containing the inferencescomputed by the reasoner was loaded in the open sourcetriple store Virtuoso 7.10 [13]. The query languageSPARQL was used for querying drug-class relationsFigure 2 Primitive class Adrenergic Decongestant [EPC]. beta2-Adrenergic Agonist [EPC] appears as a primitive class in the default distributionof NDF-RT.Figure 3 Defined class Adrenergic Decongestant [EPC]. The appearance of beta2-Adrenergic Agonist [EPC]in Protégé after augmenting it withsufficient conditions.Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 5 of 9Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 6 of 9ResultsAsserted and inferred drug-class relationsDrugsOf the 7,352 drugs (at the ingredient level) in NDF-RT,3,351 are identifiable as clinically relevant ingredients inRxNorm. After normalization to base ingredients, 2,247drugs remain, of which 1,308 have at least one relationto a pharmacologic class (EPC). As shown in Table 1, allbut 48 drugs (1,260) have asserted drug-class relationsand 1,011 drugs have inferred relations. 963 drugs haveboth asserted and inferred relations.Pharmacologic classesOf the 553 pharmacologic classes (EPC) in NDF-RT, 463have relations to drugs, of which all but five (458) haveasserted relations and 340 have inferred relations (asshown in Table 2). In total, 335 of the 463 classes haveboth asserted and inferred relations to drugs.Drug-class relationsAs shown in Figure 4, there are 1,396 asserted and 1,125inferred direct drug-class relations, of which 825 (59%and 77%, respectively) are in common. Of the assertedrelations, 571 (41%) could not be inferred, whereas 300(27%) inferred relations are not present in the assertedset. Considering the transitive closure of the hierarchicalrelation rdfs:subClassOf (for the drug class per-spective), we obtain 2,211 asserted and 1,513 inferreddrug-class relations, of which 1,332 (40% and 88%, re-spectively) are in common. Of the asserted relations 879(40%) could not be inferred, whereas 181 (12%) inferredrelations are not present in the asserted set.Perspective of drugsFor each drug, we compare the set of (direct) pharmaco-logic classes in datasets A and I. The various types ofdifferences observed between asserted and inferreddrug-class relations are presented in Table 1. The largestcategory corresponds to drugs with identical sets ofasserted and inferred drug-class relations (50%). For ex-ample, the drug imatinib has the same class KinaseTable 1 Drug-class relations (direct), drug perspectiveDrugs related to drug classesDrugs with identical sets of classes for the asserted and inferred drug-class reDrugs with compatible sets of classes (each class from the asserted is identical tDrugs with additional drug-class relations in the asserted set onlyDrugs with additional drug-class relations in the inferred set onlyDrugs with additional drug-class relations in both the asserted and inferred sDrugs with asserted drug-class relations only (no inferred relations)Drugs with inferred drug-class relations only (no asserted relations)Total number of related drugsInhibitor [EPC] in both datasets. Drugs with asserteddrug-class relations, but lacking inferred drug-class rela-tions represent 23% of the cases. For example, the druglosartan has the class Angiotensin 2 Receptor Blocker[EPC] in dataset A, but no class in dataset I.Perspective of pharmacologic classesFor each pharmacologic class, we compare the set of(direct and indirect) drug members in datasets A andI. The various types of differences observed betweenasserted and inferred drug-class relations are presentedin Table 2. As we observed for drugs, the largest cat-egory corresponds to EPCs with identical sets of assertedand inferred drug-class relations (52%). For example, theclass Monoamine Oxidase Inhibitor [EPC] has the samefive drugs in both datasets, including isocarboxazid andrasagiline. EPCs with asserted drug-class relations, butlacking inferred drug-class relations also representabout 27% of the cases. For example, the class Quin-olone Antibacterial [EPC] has eight drugs in datasetA, including ofloxacin and levofloxacin, but no mem-bers in dataset I.DiscussionDisparities between asserted and inferred drug-classrelationsMissing inferencesAs mentioned in the results, the largest category of dis-parity is represented by missing inferred drug-class rela-tions, including cases where there are no inferredrelations at all and cases where inferred relations onlycover part of the asserted relations. Missing inferencesshould not be interpreted as an inherent failure of theOWL reasoner to identify drug-class relations, but ra-ther as issues with the completeness and quality of classdefinitions and drug descriptions (see below for details).For example, the reason why the drug lurasidone, a drugindicated for the treatment of schizophrenia, has anasserted, but not inferred drug-class relation to AtypicalAntipsychotic [EPC] is because the therapeutic intent oflurasidone (Schizophrenia and Disorders with Psychotic# %lations 660 50.46o or hierarchically related to a class in the inferred set) 127 9.7168 5.2073 5.58et 35 2.68297 22.7148 3.671308 100.00Table 2 Drug-class relations (direct and indirect), class perspectiveDrug classes related to drugs # %Classes with identical sets of drugs for the asserted and inferred drug-class relations 242 52.27Classes with additional drug-class relations in the asserted set only 55 11.88Classes with additional drug-class relations in the inferred set only 20 4.32Classes with additional drug-class relations in both the asserted and inferred set 18 3.89Classes with asserted drug-class relations only (no inferred relations) 123 26.57Classes with inferred drug-class relations only (no asserted relations) 5 1.08Total number of related classes 463 100.00Winnenburg et al. Journal of Biomedical Semantics  (2015) 6:13 Page 7 of 9Features) is not described in the dataset. In fact, there isno drug property asserted for lurasidone by FDASPL.Another example is the drug ofloxacin mentioned earl-ier. In this case, the asserted EPC (Quinolone Antimicro-bial [EPC]) is not inferred because its definition includesboth may_treat Infectious Diseases and may_preventInfectious Diseases, while the drug description only in-cludes treatment, not prevention (e.g., may_treat 'Klebsi-ella Infections). Similarly, the description of the drugipilimumab is too underspecified to match the definitionof its asserted class, CTLA-4-directed Blocking Antibody[EPC]. In addition to has_MoA CTLA-4-directed Anti-body Interactions, which is in the drug description, theSOFTWARE Open Accessowlcpp: a C++ library for working with OWLontologiesMikhail K. Levin and Lindsay G. Cowell*AbstractBackground: The increasing use of ontologies highlights the need for a library for working with ontologies that isefficient, accessible from various programming languages, and compatible with common computational platforms.Results: We developed owlcpp, a library for storing and searching RDF triples, parsing RDF/XML documents,converting triples into OWL axioms, and reasoning. The library is written in ISO-compliant C++ to facilitate efficiency,portability, and accessibility from other programming languages. Internally, owlcpp uses the Raptor RDF Syntax libraryfor parsing RDF/XML and the FaCT++ library for reasoning. The current version of owlcpp is supported under Linux,OSX, and Windows platforms and provides an API for Python.Conclusions: The results of our evaluation show that, compared to other commonly used libraries, owlcpp is significantlymore efficient in terms of memory usage and searching RDF triple stores. owlcpp performs strict parsing and detectserrors ignored by other libraries, thus reducing the possibility of incorrect semantic interpretation of ontologies. owlcpp isavailable at http://owl-cpp.sf.net/ under the Boost Software License, Version 1.0.BackgroundOntologies are being increasingly recognized as import-ant information resources that capture descriptive infor-mation in a standardized, structured, and computableform. One of the most widely used approaches for repre-senting ontologies is the family of languages referred toas the Web Ontology Language (OWL) [1]. The OWLlanguages were designed to represent ontologies for usein the Semantic Web and were therefore built on theW3C semantic web stack, which includes XML, XMLSchema, RDF, and RDF Schema [25].Working with OWL ontologies involves several com-mon procedures, including parsing ontology documents,storing them as RDF triples and axioms, querying andserializing their in-memory representation, passing theaxioms to a reasoner, and performing logical queries.Given the increasing size of ontologies, it is extremelyimportant to have software for working with OWL on-tologies that can perform these procedures efficiently.During the last decade, many open-source librariesuseful for working with OWL ontologies written in RDF+XML format have become available. These, however,fail to fully meet the needs of software developers build-ing software for working with OWL ontologies. First,existing libraries do not scale well enough to supportontologies of over a few million triples [6, 7]. Inaddition, the majority of them are implemented innon-native languages, which are usually less efficientand involve significant overhead when accessed fromother languages [8]. For example, the libraries with themost extensive functionality, OWL API [9, 10] andApache Jena [11, 12], are implemented in Java. Giventhe difficulties of accessing Java from other languages,it is not surprising that the recent Perl and Python li-braries ONTO-PERL [13], RDFLIB [14], and FuXi [15],replicate some of the functionality already present inOWL API and Jena. Redland RDF framework is imple-mented in C and provides utilities for parsing, storing,and querying RDF triples [16, 17]. Its native code baseallows it to more easily expose its API in several otherlanguages and to be usable on virtually any platform,including mobile devices [18]. The functionality ofRedland is limited, though, because it does not directlysupport OWL.We sought to fill this gap by developing a library withthe following key features: (i) supports fast loading andsearching of large ontologies, (ii) has a small memory* Correspondence: lindsay.cowell@utsouthwestern.eduDepartment of Clinical Sciences, University of Texas Southwestern MedicalCenter, 5323 Harry Hines Boulevard, Dallas, TX, USAJOURNAL OFBIOMEDICAL SEMANTICS© 2015 Levin and Cowell. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 DOI 10.1186/s13326-015-0035-zfootprint, (iii) provides cross-platform compatibility, and(iv) can be accessed from multiple programming lan-guages. The resulting library, owlcpp, is designed to sup-port a common workflow where OWL ontologieswritten in RDF/XML are loaded from the file systemand submitted to a reasoner for processing (Fig. 1).owlcpp is implemented in standard C++ and is aimedprimarily at C++ and Python software developers. Herewe discuss major design features and describe the resultsof an evaluation in which we compared the loading time,query time, and memory footprint of owlcpp and severalother libraries across a set of ontologies of varying sizeand composition (Table 1).ImplementationThe design choices in implementing owlcpp were basedon providing the four key features listed above. Specific-ally, owlcpp is developed in ISO-compliant C++03 [19],which ensures source-level portability, supports gener-ation of language bindings, facilitates creation of conciseand expressive APIs [20], and compiles into efficient ex-ecutables. The API for owlcpp was designed to be concisewithout sacrificing usability and performance. In the inter-est of clarity and thread-safety, class methods and functionarguments were declared const wherever possible.The compatibility of owlcpp with different platforms wasverified by compiling the library and executing the unittests on the following platforms and compilers: Linux,Ubuntu 14.04 64-bit (gcc v4.8, Clang 3.5); Windows 764-bit (Microsoft Visual C++ 13); Mac OS X 10.6.8(i686-apple-darwin10-gcc 4.2.1); Windows XP 32-bit(Microsoft Visual C++ 9, MinGW gcc 4.5.2).Currently, owlcpp comprises three modules, rdf, forstoring and searching RDF terms and triples; io, for load-ing ontology documents; and logic, for converting triplesinto axioms and passing them to a reasoner. The iomodule depends on the Redland Raptor [21], libXML2[22], and iconv [23] libraries, and the logic module de-pends on FaCT++ [24, 25]. The io and logic moduleshave different external dependencies and can be builtand used separately from each other. owlcpp also usesmany of the Boost libraries, e.g., iterator, multi-index,and file system [26].The basic features of owlcpp, as well as those of otherkey libraries, are shown in Table 2.rdf moduleThe rdf module implements classes and methods asneeded to support the RDF standard. To accommodatethe demands of working with large ontologies, the de-sign priorities for the module were compact in-memoryrepresentation of RDF terms and triples, and their effi-cient search and retrieval. The Triple_store class isthe main container provided by the module. It supportsstoring, retrieving, and searching for prefix IRIs, RDFterms, ontology document descriptions, and RDF triples(Fig. 2). The library uses light-weight IDs to point toprefix IRIs (Ns_id), terms (Node_id), and documentdescriptions (Doc_id). The IDs for prefixes and termsFig. 1 Workflow of the owlcpp library. Owlcpp loads RDF/XMLdocuments from the file system, parses them using the Raptor RDFSyntax library, stores RDF triples in a triple store, converts the triplesto OWL axioms, and passes these to FACT++ for reasoningLevin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 2 of 9standardized by RDF and OWL are defined by the li-brary as compile-time constants.The RDF standard defines three types of terms imple-mented in owlcpp as the Node_iri, Node_literal,and Node_blank classes (Fig. 3) [4]. These areaccessed through the interface defined by the abstractNode class. A Node_iri object represents a prefix IRIwith an optional fragment identifier. A Node_literalobject stores an ID pointing to a Node_iri definingthe datatype (e.g., xsd:date, rdf:PlainLiteral), avalue as an appropriate internal type defined by sub-classes of Node_literal, and, in the case of a string-valued literal, a language. A Node_blank object storesthe ID of the document in which it was defined and aninteger uniquely identifying the blank node within thedocument. In this way, blank nodes from different docu-ments added to the same triple store are always keptdistinct.The ontology document descriptions (Doc_metaclass) store Node_ids for the ontology and version IRIsand for the file system path for the ontology document.The Triple class represents RDF triples by defininga combination of subject, predicate, and object terms.Since each term may appear in many triples, theTriple class stores a light-weight ID rather than avalue for each of the terms. In addition, Triple storesthe ID of the source document.Since term IDs do not distinguish between different typesof terms, the Triple class cannot, by itself, enforce thetype restrictions on its terms. For example, it is possible tocreate a triple where the subject term ID refers to a literalor where the predicate term ID refers to a blank node.While such triples will not be created during normal ontol-ogy parsing, it should still be noted that the Triple classimplements a generalized RDF triple [4].Searching stored triples is a frequent, performance-sensitive operation. The types of searches are applicationdependent and may involve matching any combinationof subject, predicate, object, and document, while leav-ing other elements unspecified. To efficiently performthe required types of searches, owlcpp stores triples inseveral indices. Within an index, triples are separatedTable 1 Ontologies used for evaluating owlcppName Size Terms Triples AxiomsMB IRI Literal BlankOBP [29] 3.0 935 2366 5056 25,924 6109OBI [30] 6.1 4106 10,115 8709 75,666 32,800Uberon [31] 62.0 32,078 118,935 87,713 579,388 56,956OpenGALEN parta 130.5 45,404 8423 771,980 2,004,170 187,893VTO [37] 149.2 110,418 502,521 103,801 1,358,341 829,796MESH [38] 193.8 916,056 249,740 4345 1,667,128 1,654,092DRON [39]b 214.8 344,403 322,903 322,902 2,281,817 1,313,110Biomodels [40] 253.2 232,214 535,725 481,909 2,686,610 1,905,822OpenGALEN [41] 546.3 127,042 56,469 3,508,389 8,724,486 555,740aOpenGALEN8_DD_2_Chapters.owl and its importsbdron- ndc.owlTable 2 Basic features of owlcpp and other similar librariesFeature owlcpp Redland Jena OWL APILoad RDF/XML ? ? ? ?Serialize RDF/XML  ? ? ?Turtle I/O  ? ? ?OWL/XML, Functional, Manchester I/O    ?Search RDF triples ? ? ? Convert RDF to axioms ?  ? ?Access FaCT++ reasoner ?   ?Access to other reasoners (Chainsaw, JFact, HermiT, Pellet, RacerPro)    ?Axiom API    ?C/C++ API ? ?  Python API ? ?  Java API  ? ?Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 3 of 9into bins according to one term and sorted within each binaccording to the other terms. For example, an index inwhich triples are binned by subject and sorted by predicate,object, and document can help to efficiently identify the tri-ples matching a subject and a predicate. On the other hand,if matching an object and a predicate is required, an indexconfigured to bin by object and sort by predicate, subject,and document is expected to perform better.Since the indices have a significant memory footprint,the user can define the number and type of indices dur-ing compilation. By default, owlcpp uses two indices: (i)bin by subject and sort by predicate, object, and docu-ment, and (ii) bin by object and sort by predicate, sub-ject, and document. These defaults were selectedbecause they were empirically found to perform best foraxiom generation, as described below.Fig. 2 A diagram of the owlcpp Triple_store class, objects it stores, and some implemented methods. The Triple_store class serves asa container for prefix IRIs (Ns_iri), different types of RDF terms, which are accessed through the abstract Node interface, ontology documentdescriptions (Doc_meta), and RDF triples (Triple). Each object can be retrieved through an overloaded square bracket operator by supplyingthe objects appropriately typed IDFig. 3 The RDF term type hierarchy as implemented in owlcpp. The RDF standard defines three types of terms implemented in owlcpp asthe Node_iri, Node_literal, and Node_blank classes, which are accessed through the abstract Node interface. The abstractNode_literal class is further subtyped by concrete literal classes that represent the literal values as appropriate internal types, e.g.,Node_bool for boolean types, Node_int for integers, etc. String-valued literals may also specify the language of their string valuesLevin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 4 of 9Searching stored RDF triples is done using thefind_triple method provided by Triple_store.The method is designed to automatically select the opti-mal search procedure based on the type of query andthe available triple indices. The results of the search arereturned as an iterator range [27], which is both efficientand convenient, because the range can be used to testthe success of the search, to obtain the first match, or toiterate over all matching triples.io moduleThe io module provides several methods for loadingRDF/XML ontology documents to a triple store. Thedocuments can be loaded from the C++ Standard Libraryinput streams using method load or directly from thefilesystem using load_file. Loading an ontology docu-ment involves reading data from a stream, parsing XMLstructures, interpreting them as RDF statements, convert-ing the statements into RDF terms and triples, and insert-ing them into a triple store. Currently, the Redland Raptorlibrary is used as an RDF parser [16], which, in turn, relieson libxml [22] for parsing XML and iconv [23] for charac-ter encoding support.The io module is designed for early detection of andrecovery from errors. The errors may originate at differ-ent levels of the document loading process, such asopening a file or parsing XML, RDF, or OWL. An erroroccurring for any reason during document loadingaborts the process by throwing an exception containingdetailed information about its causes. In addition, in-stead of producing a triple store that contains an un-certain number of triples from the document thatcaused the error, the exception leaves the triple storein a valid state with none of the documents content inthe triple store. This behavior allows the user to utilizethe existing triple store content or to attempt loadingthe document again.OWL documents often import the contents of otherdocuments, identifying them by their ontology IRIs andversion IRIs. The io module provides a mechanism toautomatically load the imports from the file system. Tobe able to locate the documents, the module implementsa Catalog class that stores document descriptions andmaps the document, ontology and version IRIs to filesystem paths. The module also provides a method forscanning file system directories for ontology documentsand adding their metadata to the catalog. Supplying aCatalog object to the load or load_file methodscauses the module to also load the imported documentsto the triple store. Automatically loading documentsfrom the Internet is not currently supported by themodule because this feature introduces a significant un-certainty to the success and performance of ontologyloading and adds complex operating system-specific de-pendencies to the library.logic moduleThe logic module is responsible for translating RDF tri-ples into OWL axioms and facilitating interaction withreasoners. Translation of triples to axioms is imple-mented by following the W3C Recommendation [28]. Ifthe triples do not meet some of the stated requirements,the process is aborted by throwing an exception contain-ing detailed information about its causes. Axioms canbe generated from the entire store, or from a subset ofthe triples. Generating an axiom associated with a par-ticular triple usually requires information stored inother triples, which are found by searching the triplestore. The axiom generation algorithm searches thetriple store by subject, by subject and predicate, and bypredicate and object.Frequent search operations make configuration oftriple indices an important factor affecting axiom gener-ation performance. The optimal configuration was iden-tified empirically by comparing axiom generation timesusing eight hand-picked index configurations and threedifferent ontologies: the Ontology of Biological Pathways(OBP) [29], the Ontology for Biomedical Investigations(OBI) [30], and the Integrated Cross-species AnatomyOntology (Uberon) [31]. For Uberon, the largest ontol-ogy of the three, selecting the best index configurationreduced the axiom generation time by a factor of 2.5thousand.Currently, the logic module works with FaCT++, whichis, to our knowledge, the only open-source C/C++ reasonerlibrary for OWL DL [24]. Logical queries are currently per-formed directly through the FaCT++ interface.ConcurrencyAlthough owlcpp does not provide explicit support forconcurrency, similar to C++ Standard Library con-tainers, it is designed to maximize the number ofthread-safe operations without penalizing performance.Operations that do not change the state of owlcpp con-tainers (e.g., all const methods) are guaranteed to bethread-safe. On the other hand, if multiple threads con-currently access an owlcpp container object and at leastone of the threads modifies its state (e.g., inserts an RDFtriple), the behavior is undefined. Therefore the user isexpected to ensure that a modifying thread obtains ex-clusive access to owlcpp containers.Build systemThe build system for owlcpp is based on Boost.Build andis compatible with both Unix-like and Windows plat-forms [32, 33]. It is responsible for compilation and link-ing of static and shared variants of the library, as well asLevin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 5 of 9the sample executables according to the configurationprovided by the user. The system also builds the re-quired third-party dependencies from their sources, gen-erates a Distutils module of owlcpp Python bindings,and produces API documentation using Doxygen [34].Unit testsUnit tests comprise approximately 20 % of the librarysource code and cover most of its functionality. A separ-ate test suite is implemented for each of the owlcppmodules. The tests for the io and logic modules makeuse of many small sample ontology documents that arepart of the project source tree. Some of the documentsare designed to test error detection at the XML, RDF, orOWL level. Some of the sample documents used by thelogic module tests were adapted from the OWL 2 TestCases [35]. The tests verify expected consistency of theontologies and perform more specific logical queries. Allunit tests are executed by the build system with a singlecommand.Python bindingsThe current version of owlcpp includes bindings forPython developed using the Boost.Python library. Thefunctionality of the bindings is verified by a separateunit test suite. The bindings and their dependenciesare packaged by the build system into a distributablePython module. APIs for other programming lan-guages can also be exposed from owlcpp relatively eas-ily and with minimal overhead. In future versions ofowlcpp, these will be provided with the help of theSWIG library [36].ResultsEvaluationTo evaluate owlcpp, we compared the document loadingtime, triple query time, and memory foot print of owlcppwith those of Redland, Jena, and OWL API. Evaluationwas conducted using five ontologies of varying size andcomposition: the Vertebrate Taxonomy Ontology (VTO)[37], Medical Subject Headings ontology (MESH) [38],Drug Ontology (DRON) [39], Biomodels Ontology [40],and OpenGALEN [41]. In addition, a one quarter por-tion of OpenGALEN (OpenGALEN part) was used. Thestatistics of the ontologies, including filesystem foot-prints and the counts for RDF terms, triples, and axiomsare listed in Table 1.To test the ability of the library to process large ontol-ogies on off-the-shelf hardware, the tests were con-ducted on an underpowered, by current standards,computer. Performance was tested on a laptop with anIntel Core2 Duo T7700 2.40GHz CPU, 3GB of RAM,running Linux Ubuntu 14.04 64-bit. The performance ofJava libraries was tested using Oracle Java JDK v1.7.0_51with a 2 GB maximum memory pool (-Xmx2048m). Un-less noted otherwise, owlcpp was compiled with gcc v4.8using the default triple index configuration. Redland v1.0.13with a hashed in-memory store, Jena v2.12.1 with an in-memory RDF store, and OWL API v4.0.1 were used forcomparison, also with their default settings. The sourcecode for performance tests can be found in Additional file1. All testing was done using owlcpp v0.3.5.Ontology loading performanceTo evaluate ontology loading performance, each of thesix ontologies was loaded into each of the four libraries.Some libraries were unable to load the larger ontologies.Jena was unable to load the complete OpenGALENontology, while Redland failed to load the completeOpenGALEN, Biomodels, and DRON ontologies. An at-tempt to load the complete OpenGALEN into Redlandon a system with 32GB of RAM was also unsuccessful.The loading rates (size of the ontology file system foot-print divided by the recorded loading time) is shown inFig. 4a. The ontology loading rate of owlcpp ranges from3.1 to 6.9 MB/s, while the range for the other libraries isfrom 2.6 to 7.2 MB/s. The owlcpp loading rate is fasterthan that of Jena for the five ontologies Jena could loadand faster than that of Redland for two of the three on-tologies Redland could load. Redland had a faster load-ing rate than owlcpp for MeSH. In addition, the owlcpploading rate is faster than that of OWL API for four ofthe six test ontologies. OWL API has a faster loadingrate for both OpenGALEN full and part.Memory footprintThe amount of memory required by each library duringontology loading was estimated by probing the residentset size of the process virtual memory. The peak RAMutilization normalized by the size of the ontology on thefile system is shown in Fig. 4b. Of the libraries tested,owlcpp had the smallest memory footprint for all ontol-ogies ranging from 1.8 to 3.2 bytes of RAM required foreach byte on the file system. The same ratio ranged from7.0 to 11.5 for Redland, from 6.8 to 10.7 for Jena, andfrom 2.6 to 7.0 for OWL API.Triple search efficiencySearching by subject and predicate is the most commontriples search during axiom generation. Therefore, thetriple search performance of the libraries was tested byrepeating queries where the subject was selected at ran-dom, and the predicate was rdfs:subClassOf. Foreach query, all matching triples were identified andcounted. The number of queries for each test was se-lected so as to keep the test time at about one minute.The number of queries performed by each library di-vided by the elapsed time is shown on Fig. 4c. TheLevin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 6 of 9owlcpp library showed significantly higher search ratesranging from 3.0 to 3.2 million queries per second (MQ/s).Jena showed significantly lower rates from 0.53 to0.83 MQ/s. The rates for Redland ranged from 0.2 to0.69 MQ/s. Note that OWL API was not included in thisevaluation because it stores axioms rather than triples.Accuracy and error detectionIn addition to evaluating the performance of owlcpp, wewanted to assess the accuracy of parsing and axiom gen-eration. This was done using the OWL 2 Test Cases[35], some of which are incorporated into owlcpp unittests. Further testing was done during the developmentof the Ontology of Biological Pathways (OBP) [29] byexecuting queries formulated by domain experts andcomparing the results with ones from Protégé runningwith either FaCT++ or the HermiT reasoner plug-in[42]. The results of the queries were always identical.Strict error checking has proven to be an importantfeature of owlcpp, helping to avoid incorrect semanticinterpretation of ontologies and facilitating their devel-opment. Examples of errors detected by owlcpp but ig-nored by OWL API and Jena are undeclared propertyand annotation predicates and misspelled standardOWL terms.Discussionowlcpp is a C++ library providing support for storingand searching RDF terms and triples, for loading RDF/XML documents along with their imports into a triplestore, for generating OWL axioms based on stored tri-ples, and passing axioms to the FaCT++ reasoner. Tothe best of our knowledge, owlcpp is the first C++ libraryfor working with OWL ontologies.Our primary goal was to design a library for softwaredevelopers that would scale well for working with largeontologies. To facilitate use by software developers, wedesigned owlcpp to have a concise and expressive C++API and an efficient Python API. For example, loadingan ontology file into an owlcpp triple store can beFig. 4 Performance comparison of the owlcpp, Redland, Jena, and OWLAPI libraries. The measurements were done using the followingontologies (by size, see Table 1): part of OpenGALEN (OG part), VTO,MESH, DRON, Biomodels, and complete OpenGALEN (OG full). The barsshowing performance measurements are color-coded by library andgrouped by ontology. The standard deviations of the measurements areshown as error bars. The corresponding bars are not shown if ontologyloading failed. a shows ontology loading ratesontology size dividedby loading time. b shows the RAM footprint of each library afterontology loading normalized by the filesystem size of the ontology.c shows triple store querying rates. Each query identifies all triplesmatching a combination of a random subject and a constantpredicate. Triple querying rates for OWL API are not shown be-cause this operation is not supportedLevin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 7 of 9accomplished with just two lines of code, whereas thesame operation through the Redland Raptor library APIrequires over a dozen lines [43]. The API for RDF triplestore search is another example. In owlcpp, a singlemethod, Triple_store::find_triple(), can beused to search for triples matching a specific subject,predicate, object, document, or any combination thereof.The search is performed without sacrificing performanceby selecting the most suitable triple index at compiletime. The result of the search, an iterator range, can beused transparently to determine whether the triple storecontains a triple matching the specified condition, to re-trieve the first matching triple, or to iterate over allmatching triples. On the other hand, the triple stores ofboth Redland and Jena define over ten different methodsfor searching triples.Of critical importance to the utility of owlcpp is ensur-ing its scalability for use with large ontologies. Thus, wedesigned owlcpp to have a compact, in-memory storageof RDF terms and triples, efficient indexing of stored tri-ples, and no virtual machine requirement. The latter fa-cilitates owlcpps deployment in HPC environments. Toevaluate the scalability of owlcpp, we compared its mem-ory footprint, ontology document loading time, andtriple query time with those of Jena, Redland, andOWLAPI. We found that owlcpp has a smaller memoryfootprint than the other three libraries for all ontologiestested (Fig. 4b), and we find that the ontology documentloading time is faster for owlcpp than the other librariesfor all tests with two exceptions (Fig. 4a): Redland wasfaster loading MesH, and OWLAPI was faster loadingOpenGALEN or a part of OpenGALEN. The lower per-formance of owlcpp with the MeSH ontology is probablydue to this ontologys low ratio of triples to IRIs. InMeSH, each IRI appears, on average, in 1.8 triples,whereas in other ontologies this ratio ranges from 6.6 to69. This property of MeSH increases the relative cost ofIRI parsing, while diminishing the benefit of utilizing IRIIDs. Ontology loading by owlcpp is slower for OpenGA-LEN, either full or part, than for the other ontologies.This is probably due to a 50 % greater number of triplesper megabyte in the OpenGALEN ontology.While interpreting the performance measurements ofthe owlcpp, Redland, Jena, and OWL API libraries, it isimportant to note significant differences in their archi-tecture. owlcpp and Redland are natively-compiled li-braries, whereas Jena and OWL API run under Javavirtual machine and exhibit less deterministic perform-ance and memory footprint due to just-in-time compil-ation and garbage collection. Furthermore, while owlcpp,Redland, and Jena store the documents in memory as aset of RDF triples, OWL API immediately converts thetriples into axioms and annotations, which, arguably,can be stored in memory more compactly. Nevertheless,the performance comparison is useful because it helpspredict the hardware requirements for a task and reflectson the overall user experience.There are several limitations of owlcpp, which will beaddressed in future versions. First, RDF/XML is the onlyOWL format currently supported by owlcpp. Future ver-sions will introduce support for additional syntaxes, par-ticularly Manchester, Turtle, and OWL/XML. Second,owlcpp doesnt currently provide a Java API, and istherefore not interoperable with most of the currentlyavailable RDF/OWL tools. In future versions, we willprovide a Java API. Third, although it is possible tomanually add more nodes and triples to an owlcpp triplestore, it is not currently possible to save the new RDFgraph. Another important limitation of owlcpp is thelack of a description logic expression and axiom inter-face for axiom editing. Future versions will include thisand will also improve readability of error messages, pro-vide options for less strict parsing and axiom generation,and include a module for batch execution of OWL 2Test Cases. Finally, future versions of owlcpp will pro-vide an axiom-based in-memory data structure.Conclusionsowlcpp presents a number of benefits for developers andusers. Its compact datamodel and efficient executionmake it possible to work with large ontologies using off-the-shelf hardware. As a native library, owlcpp does notdepend on a virtual machine installation, facilitating itsdeployment in HPC environments. The C++ and PythonAPIs of owlcpp are concise and expressive and facilitate itsintegration with other software modules. Currently, owlcppis used in many groups to work with biological ontologiesas well as in other fields including virtual reality, robotics,image analysis, and answer set programming.Availability and requirementsProject name: owlcppProject home page: http://owl-cpp.sourceforge.net/Operating system(s): Cross-platform (tested: Linux,Windows, Mac)Programming language: C++, PythonOther requirements: Boost, libxml2, iconv (underWindows), Raptor, FaCT++License: Boost Software LicenseVersion 1.0Any restrictions to use by non-academics: noneAdditional fileAdditional file 1: This file contains the code used for theperformance comparison. (PDF 49 kb)Competing interestsThe authors declare that they have no competing interests.Levin and Cowell Journal of Biomedical Semantics  (2015) 6:35 Page 8 of 9Authors contributionsMKL developed the software, tested the performance, and prepared the firstdraft of the manuscript. LGC provided expert guidance, revised themanuscript, and wrote the final version. Both authors read and approved thefinal manuscript.Authors informationMKL is a Contractor Application Developer at the Bank of America, Charlotte,NC. LGC is an Associate Professor in the Division of Biomedical Informatics,Department of Clinical Sciences, UT Southwestern Medical Center. owlcppwas developed while MLK was an Instructor at UTSW.AcknowledgementsThe authors would like to thank Dmitri Tsarkov for help with the FaCT++library; Alan Ruttenberg and Wac?aw Ku?nierczyk for useful discussions; andAnna Maria Masci, Federico Bozzo, Simone Miraglio, and Andrés Samuel fortesting owlcpp. This work was supported by an NIAID-funded R01 (AI077706)and a Burroughs Wellcome Fund Career Award to LGC.Received: 18 October 2014 Accepted: 4 September 2015REVIEW Open AccessSpecial issue on bio-ontologies andphenotypesLarisa N. Soldatova1*, Nigel Collier2, Anika Oellrich3, Tudor Groza4, Karin Verspoor5, Philippe Rocca-Serra6,Michel Dumontier7 and Nigam H. Shah7AbstractThe bio-ontologies and phenotypes special issue includes eight papers selected from the 11 papers presented atthe Bio-Ontologies SIG (Special Interest Group) and the Phenotype Day at ISMB (Intelligent Systems for MolecularBiology) conference in Boston in 2014. The selected papers span a wide range of topics including the automatedre-use and update of ontologies, quality assessment of ontological resources, and the systematic description ofphenotype variation, driven by manual, semi- and fully automatic means.IntroductionThe special issue on bio-ontologies and phenotypes in-cludes selected papers that were presented at the Bio-Ontologies SIG and the Phenotype Day at ISMB (Intelli-gent Systems for Molecular Biology) conference in 2014.Over the 17 years, the Bio-Ontologies SIG at ISMBhas provided an environment for discussion of ontol-ogies, their applications to biology, and more generallythe organisation, presentation and dissemination ofknowledge in biomedicine and the life sciences. In 2014the bio-ontologies SIG ran an extended event called thePhenotype Day, which focused on the systematic de-scription of phenotypic variation. The Phenotype Daybrought together researchers across many disciplines todiscuss phenotype-related issues and resources, and toshare their experience with defining, representing, pro-cessing and using phenotype data.The 2-day event on July 11th and 12th co-located withISMB 2014 in Boston, received 38 submissions, includ-ing 26 papers, five flash updates and six poster abstractsand one position paper. Of the 11 papers selected forpresentation at the meeting, the eight papers selected forthis special issue are extended versions of seven originalpapers and one position paper.Summary of selected papersThe selected for the special issue papers span a widerange of topics including the automated re-use and up-date of ontologies, quality assessment of ontological re-sources, and the systematic description of phenotypevariation, driven by manual, semi- and fully automaticmeans. From these articles, it is clear that systematic de-scription of phenotypes plays a role when accessing andmining medical records as well as in the analysis ofmodel organism data, genome sequence analysis andtranslation of knowledge across species. Accurate pheno-typing has the potential to bridge studies that aim to ad-vance the science of medicine, such as a betterunderstanding of the genomic basis of diseases in theMouse Genome Informatics (MGI) database, and studiesthat aim to advance the practice of medicine such as thetreatment of complex disorders like Chronic ObstructivePulmonary Disorder (COPD) [1].Collier et al. in the paper titled Concept selection forphenotypes and diseases using learn to rank [2] presenta supervised learning based approach for the recognitionof disorder-related descriptions in electronic health re-cords (EHRs). The authors explore the potential of fouroff-the-shelf concept recognition systems on the ShARE/CLEF 2013 gold standard collection and combine thesystems using a variety of learn-to-rank algorithms. Thefour systems are Apache cTAKES, the NCBO Annotator,BeCAS and MetaMap. The proposed ensemble approachleads to an improvement in harmonized mean for recall-precision (F1 = 0.24) due primarily to an increase in* Correspondence: larisa.soldatova@brunel.ac.uk1Brunel University, London, UKFull list of author information is available at the end of the article© 2015 Soldatova et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Soldatova et al. Journal of Biomedical Semantics  (2015) 6:40 DOI 10.1186/s13326-015-0040-2recall for mentions of diseases and anatomical abnor-malities. However performance across semantic typesvaries widely. The authors conclude with a discussion ofthe limitations offered by using off-the-shelf approachesand consider the need of domain adaptation in an oper-ational setting.In Development and validation of a classification ap-proach for extracting severity automatically from elec-tronic health records Boland et al. [3] present a methodfor classifying phenotype-level severity between severeand mild conditions such as Acne, which would be con-sidered mild, compared to say heart failure. The authorsreport that previous machine learning approaches havetended to yield high false positive rates due to the largespace of phenotypes. The random forest approach theypropose exploits several measures of severity such ascost, treatment time, medications and procedure to yielda sensitivity of 0.92 and a specificity of 0.78 in discerningmild phenotypes from severe conditions when comparedto a gold standard corpus.Funk et al. in the paper titled Evaluating a variety oftext-mined features for automatic protein function pre-diction with GOstruct report the results of their ana-lysis on the use of protein-related features extractedfrom the biomedical literature for prediction of proteinfunctions [4]. The authors considered two approaches:(1) a knowledge-based approach that uses ontology con-cepts co-mentions and is based on co-occurences of anentity and the corresponding ontology terms identifiedin the literature; (2) a knowledge-free approach that usesa bag-of-words as features; and where proteins are asso-ciated to words from the sentences in which they arementioned. Two data sources were used in this study:abstracts and titles from Medline, and full-text articlesfrom the PubMed Open Access Collection (PMCOA).Gene Ontology (GO) annotations for human and yeastgenes were obtained from the GOA (Gene Ontology An-notation) datasets. The authors analysed the impact ofusing the two alternative approaches for feature con-struction on the quality of prediction of protein func-tions. Interestingly, both approaches provided similarlevels of performance. Overall, the best performance isseen when using both co-mentions and the bag-of-words features, but the advantage is marginal. Funket al. have demonstrated that the ability to recognize GOterms in the literature text leads to more informativefunctional predictions [4].Smith et al. present a paper titled Expanding themammalian phenotype ontology to support automatedexchange of high throughput mouse phenotyping datagenerated by large-scale mouse knockout screens [5].The authors expand and exploit the Mammalian Pheno-type (MP) ontology for the annotation and organizationof high throughput data from phenotype screeningexperiments in the MGI database. The authors discusshow recent additions and revisions have been under-taken in many areas of the MP to support automateddata exchange with the International Mouse PhenotypeConsortium and other projects. In total 287 new termswere added to the MP hierarchy during the present revi-sion. The majority of the terms were added in thehomeostasis/metabolism section of the ontology.Fu et al. [6] in the paper titled Supporting the annota-tion of chronic obstructive pulmonary disease (COPD)phenotypes with text mining workflows presents amethodology for constructing a corpus of full-text clin-ical documents for fine-grained COPD annotations. Theauthors report that symptoms from COPD vary widelybetween patients so that automated free-text analysis onthe EHR is necessary to bring pertinent conditions tothe attention of the clinician. However, gold standarddata for COPD related symptoms have so far been lack-ing, hampering the development of text mining ap-proaches. Annotation efforts by Fu et al., weresupported by expert-led guideline development and there-use of the Argo text mining platform yielding an F1of 0.46 using relaxed matching.eNanoMapper: harnessing ontologies to enable dataintegration for nanomaterial risk assessment by Hast-ings et al. presents an ontology which covers broad areassuch as a categorisation of nanoparticle classes based ontheir properties, constituency and shape, physicochemi-cal and biological properties of nanoparticles, environ-mental aspects, experimental design, as well as safetyinformation [7]. Following the best practices in ontologydevelopment, eNanoMapper re-uses existing ontologies,i.e. ChEBI (Chemical Entities of Biological Interest),NPO (NanoParticle Ontology), BAO (BioAssay Ontol-ogy). To overcome the limitations of the manual importsfrom third party ontologies, the authors developed adedicated library to facilitate ontology re-use by extract-ing subsets of existing ontologies, which allows theresulting branches and components of different ontol-ogies to be automatically pieced together.Winnenburg et al. in the paper titled Using descrip-tion logics to evaluate the consistency of drug-classmembership relations in NDF-RT compared theasserted and inferred class relations in the recently up-dated NDF-RT (National Drug File Reference Termin-ology) ontology [8]. NDF-RT integrated authoritativedrug-class membership assertions extracted from theStructured Product Labels by FDA (the Food and DrugAdministration). The authors evaluated the consistencyof the drug-class membership relations inferred from thepharmacologic class definitions and drug descriptions,against the newly asserted, authoritative drug-classmembership relations. The enriched logic in NDF-RTenables the evaluation of the quality and completenessSoldatova et al. Journal of Biomedical Semantics  (2015) 6:40 Page 2 of 3of newly added knowledge. The authors conclude thatthe inferred and asserted relations matched only inabout 50 % of the cases. The results suggest that there isan opportunity for quality assurance of NDF-RT content(completeness of the drug descriptions and quality ofthe class definitions).In a commentary by Papatheodorou et al. titled Link-ing gene expression to phenotypes via pathway informa-tion the authors survey recent research efforts toprovide knowledge support for managing and integratingdata about genes, pathways and phenotypes [9]. The au-thors argue that in order to exploit the full potential ofgene expression data to infer phenotypic consequencedue to changes in gene expression, the links betweengene expression and pathways as well as pathways andphenotypes need to be improved. Furthermore, the au-thors suggest that the current ontological representa-tions of phenotypes needs to be extended, in order tocover the complexity and variability of phenotypes inand across species. They summarize that the currentstate-of-the-art builds solid foundations for future workto overcome these challenges.ConclusionIn recent years the biological sciences have generatedvery large, complex data sets whose management, ana-lysis and sharing have created unprecedented challenges.The development of bio-ontologies has been critical inhandling these data and enabling interoperability be-tween databases and between applications [10]. The sys-tematic description of phenotype variation is crucial forelucidating the causal relationship between a genotypeplaced in a certain environment and a phenotype. Sys-tematic representation of phenotypes using ontologies isessential when accessing and mining medical records aswell as for the analysis of model organism data, genomesequence analysis and translation of knowledge acrossspecies. The papers included in the bio-ontologies andphenotypes special issue report on the development,management and quality assessment of ontological re-sources, and the systematic description of phenotypevariation, driven by manual, semi- and fully automaticmeans.Competing interestsThe authors have no competing interests to declare.AcknowledgementsAs editors of this thematic issue, we thank all the authors who submittedpapers, the Program Committee members and the reviewers for theirexcellent work. We are grateful for help from Goran Nenadic and DietrichRebholz-Schuhmann from BioMed Central in putting this thematic issuetogether.DeclarationsThis article has been published as part of Journal of Biomedical SemanticsVolume 6/1, 2015: Proceedings of the Bio-Ontologies Special Interest Groupand the Phenotype Day 2014. The full contents of the special issue are avail-able online at http://www.jbiomedsem.com/content/6/1/.Author details1Brunel University, London, UK. 2The University of Cambridge, Cambridge,UK. 3The Wellcome Trust Sanger Institute, Hinxton, UK. 4The Garvan Instituteof Medical Research, Sydney, Australia. 5The University of Melbourne,Melbourne, Australia. 6The University of Oxford, Oxford e-Research Centre,Oxford, UK. 7Stanford University, Stanford, CA, USA.Received: 19 April 2015 Accepted: 15 November 2015RESEARCH ARTICLE Open AccessIdentification of sex-associated networkpatterns in Vaccine-Adverse EventAssociation Network in VAERSYuji Zhang1,2*, Puqiang Wu3, Yi Luo4,5 and Cui Tao5*AbstractBackground: Vaccines are one of the most important public health successes in last century. Besideseffectiveness in reducing the morbidity and mortality from many infectious diseases, a successful vaccineprogram also requires a rigorous assessment on their safety. Due to the limitations of adverse event (AE) datafrom clinical trials and post-approval surveillance systems, novel computational approaches are needed toorganize, visualize, and analyze such high-dimensional complex data.Results: In this paper, we proposed a network-based approach to investigate the vaccine-AE association network fromthe Vaccine AE Reporting System (VAERS) data. Statistical summary was calculated using the VAERS raw data andrepresented in the Resource Description Framework (RDF). The RDF graph was leveraged for network analysis.Specifically, we compared network properties of (1) vaccine - adverse event association network based on reportscollected over a 23 year period as well as each year; and (2) sex-specific vaccine-adverse event association network. Weobserved that (1) network diameter and average path length dont change dramatically over a 23-year period, whilethe average node degree of these networks changes due to the different number of reports during different periods oftime; (2) vaccine - adverse event associations derived from different sexes show sex-associated patterns in sex-specificvaccine-AE association networks.Conclusions: We have developed a network-based approach to investigate the vaccine-AE association network fromthe VAERS data. To our knowledge, this is the first time that a network-based approach was used to identify sex-specificassociation patterns in a spontaneous reporting system database. Due to unique limitations of such passive surveillancesystems, our proposed network-based approaches have the potential to summarize and analyze the associations inpassive surveillance systems by (1) identifying nodes of importance, irrespective of whether they are disproportionallyreported; (2) providing guidance on sex-specific recommendations in personalized vaccinology.BackgroundVaccines are one of the most cost-effective public healthinterventions to date, leading to at least 9599 % de-crease of most vaccine-preventable diseases in theUnited States [1]. While their benefits far overweightheir risks and costs, vaccines are accompanied withspecific adverse events (AEs). Assessment of vaccinesafety usually starts at the pre-approval stage, when in-formation about AEs is collected during Phase I-IV ofclinical trials. However, there are several limitations ofsuch information. First, clinical trials usually have smallsample sizes which are insufficient to detect rare AEs. Sec-ond, clinical trials are usually carried out in well-defined,homogeneous populations within relatively short follow-up periods, which may limit the generalizability of their ef-fect in all populations. Therefore, the complete safety pro-files associated with a vaccine cannot be fully establishedonly through clinical trials. Post-approval surveillance ofvaccine AEs is needed to assess the vaccine safetythroughout its life on the market.The Vaccine AE Reporting System (VAERS) is a pas-sive surveillance system to monitor vaccine safety afterthe administration of vaccines licensed in the United* Correspondence: yuzhang@som.umaryland.edu; Cui.Tao@uth.tmc.edu1Division of Biostatistics and Bioinformatics, University of MarylandGreenebaum Cancer Center, Baltimore, USA5School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, Texas, USAFull list of author information is available at the end of the articleJOURNAL OFBIOMEDICAL SEMANTICS© 2016 Zhang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide alink to the Creative Commons license, and indicate if changes were made. The Creative Commons Public DomainDedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in thisarticle, unless otherwise stated.Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 DOI 10.1186/s13326-015-0032-2States [2]. The VAERS is co-managed by the UnitedStates Food and Drug Administration (FDA) and theCenters for Disease Control and Prevention (CDC). Bythe end of 2013, the VAERS contains more than 200,000reports in total, including 72 vaccine types and 7368reported symptoms/AEs. However, there are severallimitations we need consider in the analyses of spontan-eous reporting systems such as VAERS, including lackof verification of reported diagnoses, lack of consistentdiagnostic criteria for all cases with a given diagnosis,wide range of data quality, underreporting, inadequatedenominator data, and absence of an unvaccinated con-trol group [3]. To address some of these limitations,various data mining approaches have been developed toidentify potential signals in the data [4]. Most of theseapproaches focus on disproportionality of reporting,which aims to identify conditions that comprise a largerproportion of reported events for a given vaccine, com-pared to other vaccines in the same reporting system[3]. However, such disproportionality methods still havedifficulties to identify potential vaccine-AE associationsdue to the limitations of VAERS data. In Bate et al. 2009[5], the authors suggested that a single drug-AE shouldbe analyzed in the context of all drug-AE associations.Harpez et al. proposed a clustering approach to identifydrug groups that were reported to have same AEs [6].However, this approach didnt account for all co-administered drugs and co-occurring AEs. Since VAERSreceives more than 14,000 reports every year, there is apressing need to develop novel approaches to organizethese high-dimensional VAERS data and identify poten-tial vaccine-AE associations.In recent years, network analysis emerges as a verypromising approach for simultaneous representation ofcomplex high dimensional data. Specifically, thesenetwork-based computational approaches gained popular-ity and have become a new paradigm to investigate associ-ations among biological entities (e.g., drugs, diseases, andgenes). Applications of these approaches include drugrepositioning [7, 8], disease gene prioritization [911],and identification of disease relationships [12, 13].These network analysis approaches are usually devel-oped based on the observations from real-world net-works. First, most real-world networks (e.g., WWWnetwork, protein-protein interaction network, and so-cial network) are not randomly organized but aredriven by preferential attachment and growth (e.g.,some nodes have more connections than others). Suchnetworks are called Scale-free networks. In thescale-free network, the most highly connected nodesare called hub nodes. Second, most real world net-works are modular, comprised of small, densely con-nected groups of nodes. Network analysis metrics andalgorithms have been designed to identify network hubnodes and modules in a scale-free network. Ball and Botsisproposed a network-based approach to aid visualization ofpatterns in VAERS data that a medical expert mightrecognize as clinically important [14]. In our previouswork, we developed a network analysis approach to den-tify vaccine-related networks and their underlying struc-tural information from PubMed literature abstracts, whichwere consistent with that captured by the Vaccine Ontol-ogy (VO) [15]. The modular structure and hub nodes ofthese vaccine networks reveal important unidentifiedknowledge critical to biomedical research and publichealth and to generate testable hypotheses for futureexperimental verification.In this paper, we proposed a network-based approachto investigate the vaccine-AE association network fromVAERS data. First, we extracted and represented datasummarized from VAERS database using ResourceDescription Framework (RDF). We calculated overallproportional reporting ratio (PRR), yearly PRR and sex-specific PRR for each vaccine-AE association in theVAERS. We then applied a series of network approachesto the network consisting of significant vaccine-AE asso-ciations (i.e., PRR > 1). Specifically, we compared networkproperties of (1) vaccine-AE association network based onreports collected over a 23 year period as well as each year;(2) sex-specific vaccine-AE association network. We ob-served that (1) network diameter and average pathlength dont change dramatically over a 23-year period,while the average node degree of these networkschanges due to the different number of reports duringdifferent period of time; (2) vaccine-AE associations de-rived from different sexes show sex-associated patternsin sex-specific vaccine-AE association networks.The rest of the paper is organized as follows. InSection Materials and methods, we introduce our meth-odology on data collection, summarization, representa-tion, and analysis. In Section Results, we present theresult of our study. In Section Discussions, we discussthe potential scientific contributions of this study. InSection Conclusions and future work, we conclude thepaper and discuss future directions.Materials and methodsIn this section, we first describe the data resources andpreprocessing method in this work. We then introduceour proposed network-based approach for investigatingvaccine-related associations derived from VAERS. Figure 1illustrates the steps of the proposed approach.VAERS databaseWe downloaded raw data from the VAERS system incomma-separated value (CSV) format (https://vaers.hhs.-gov/data/data). All the data from 19902013 was loadedto a mySQL relational database for further processing.Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 2 of 8The VAERS database contains three tables: Data, Datasources and preprocessing Symptom, and Vaccine. TheData table contains general information about each reportincluding VAERS report ID, date the report was received,the state patient was in, age and sex of the patient, and de-tailed description of the symptom (e.g., if the symptomwas life threatening, if the patient in the report died andif-so the date of death, if the patient ever attend the ER fortreatment, and if so, how many days was the patient ad-ministered at the hospital.). The Symptom table containsa list of symptom terms (MedDRA terms) involved inthe report. Completed information about one reportcan be jointed from the three tables using VAERS ID.The Vaccine table includes information about the vac-cine administered to the patient such as vaccine manu-facturer, type of vaccine, dosage of the vaccine,vaccination route, vaccination site, and vaccinationname.Statistical summary of VAERS dataAs we discussed above, the VAERS is a spontaneousreporting system which contains unverified reports withinconsistent data quality. Symptoms reported occurringafter vaccination do not necessarily indicate a causalityassociation with the vaccine. Therefore, we used statis-tical methods to summarize meta-level features ofvaccine-symptom pairs. For each vaccine-symptom pair,we calculated the following features (1) the number ofreports that contains the pair; (2) the number of reportsthat contains the pair each year; (3) the demographicdistribution among the reports that contain the pair(total and yearly) grouped by gender and age groups;and (4) overall proportional reporting ratio (PRR) andyearly PRRs [16]. A PRR is the ratio between thefrequency with which a specific symptom (e.g., AE)occurs for a vaccine of interest (relative to all symptomsreported for the vaccine) and the frequency with whichthe same symptom occurs for all vaccines reported tothe VAERS (relative to all symptoms for all vaccinesreported to VAERS) [3]. A PRR greater than 1 suggeststhat the post-vaccination symptom (AE) is morecommonly observed for individuals administrated withthe particular vaccine, relative to all other vaccinesreported to the VAERS.Fig. 1 Overview of the proposed study. VAERS: Vaccine AE Reporting System; RDF: Resource Description Framework; VAE: Vaccine-AE; PRR: ProportionalReporting RatioZhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 3 of 8The overall PRR ratio of a vaccine (V) and a symptom(S) association was calculated by (Numreports for V that con-tainsS/Numall thereports for V)/(Numtotalreports that con-tains S/Numtotal reports in VAERS).The yearly PRR ratio of a vaccine (V) and a symp-tom (S) association in Year (Y) was calculated by(Numreports for V that contains Sin year Y/Numall the re-ports for V in Year Y)/(Numtotalreports that con-tains S in Year Y/Numtotalreports in VAERS in Year Y).The sex-specific PRR ratio or a vaccine (V) and a symp-tom (S) association in Gender (G) was calculated by(Numreports for V that contains S forpatient with G/Numall there-ports for V for patient with G)/(Numtotalreports that contains S for pa-tient with G/Numtotalreports in VAERS for patient with G).RDF conversionThe statistical summary introduced in the previous sec-tion was stored in a relational database and converted tothe Resource Description Framework (RDF) format. Wehave introduced detailed information about how torepresent vaccine symptom pairs with meta-informationin RDF and our vision on linking heterogeneousvaccine-related data sets using linked data approach inour previous work [17].Figure 2 shows the meta-level RDF graph representationof a vaccine symptom association. Each unique association(vaccine-symptom pair) has an unique identifier. The cor-responding vaccine, symptom, demographic distribution,and PRR values are also represented in RDF. SPARQLqueries can be conducted to retrieve useful informationfor network analysis which we will introduce in the nextsection.Network analysisThe analysis of network properties was performed usingthe Network Analyzer plugin in Cytoscape [18]. Cytos-cape is an open-source platform for integration,visualization, and analysis of biological networks. Itsfunctionalities can be extended through Cytoscape plu-gins. Scientists from different research fields have con-tributed more than 160 useful plugins so far. Thesecomprehensive features allow us to perform thoroughnetwork-level analyses, visualization of our associationtables, and integration with other biological networks inthe future. In this study, the average node degree,average path length, and network diameter of onenetwork was calculated.The vaccine-AE association network is a bipartitenetwork, which consists of interactions between twodifferent types of nodes (X-type and Y-type), withedges connecting only nodes of different types. To cal-culate the similarity of one type of nodes (e.g., X-typenodes) based on their interactions with another type ofnodes (e.g., Y-type nodes), the Pearson correlation co-efficient (PCC) was employed as an association index.We assume that node A and B are X-type nodes, andthe PCC between node A and B is calculated byPCCAB ¼ jNðAÞ?NðBÞj?ny?jNðAÞj?jNðBÞjffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffijNðAÞj?jNðBÞj?ny?jNðAÞj?ny?jNðBÞjrð1Þwhere A and B are nodes of same type, N(A) and N(B)are their total number of interactions with A and B,Fig. 2 Sample RDF graph representation of vaccine AE associationZhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 4 of 8N(A)?N(B) is the total number of Y-type nodes thatinteract with both A and B, and ny is the total the totalnumber of Y-type nodes in the network. A PCC of 1indicates a perfect overlap, 0 corresponds to the numberof shared interactors expected by chance and ?1 depictsperfect anti-correlation.The hierarchical clustering analysis is used to identifythe similarities among vaccines using their association in-dexes. Both the heatmap of the dendrogram are used tovisualize the clustering results. The clustering analysis andvisualization of vaccine-AE association network was per-formed using the GAIN tool [19].ResultsOverview of the resultsOverall, we extracted 2,346,367 vaccine-AE associationsfrom the VAERS system, with 83,148 distinct associations.We defined that a vaccine-AE association is significant ifPRR for this association is greater than 1. Among allvaccine-AE associations reported in the VAERS, we identi-fied 277,698 vaccine-AE associations, 53,795 of whichhave overall PRR greater than 1 between 1990 and 2013.We also investigated yearly PRRs of these associations.For one specific year, we define that a vaccine-AE associ-ation is significant if the yearly PRR is greater than 1.Table 1 presents the numbers of significant associationsfor each year (Nlink Column).Based on the significant yearly or overall associa-tions, we further investigated these association net-works using different network properties. Table 1presents the general characteristics of the overall associ-ation network as well as yearly-significant association net-works, including average node degree, average path lengthand network diameter. This demonstrates that vaccine-AEnetwork is dense network, with any given node connectedto all other nodes through an average of approximatelytwo other nodes and a maximum of 56 nodes. It is ex-plained partly by that many vaccines are co-administered.However, given that there are more AEs than vaccines inthe network, it is plausible that many AEs were reportedtogether. Another interesting observation was that across23 years, the average path length and network diameterfor yearly vaccine-AE association networks dont changedramatically. It indicated that in most cases, it is relativelycommon that two vaccines sharing one AE or two AEs as-sociate with one vaccine in the network. On the otherhand, the average node degree of these networks changesover time, partly due to the increasing number of reportsreceived from 1990 to 2013 (Table 1). All the networkinformation in Table 1 were presented in Additional file 1.Different AE association patterns in different gendersWe further investigated whether vaccine-AE associationsare different between genders. We constructed sex-specificvaccine-AE association networks by computing the PRRbased on reports only from female/male populations. Thereare 49,616 and 51,578 significant vaccine-AE associations(i.e., PRR > 1) in female and male populations, respectively.The network properties of these two sex-specific associ-ation networks are similar with overall association network(Table 1). We clustered the vaccines based on their associ-ation indexes calculated by their associations with AEs. InFig. 3a and b, we observed different similarity patterns infemale (Fig. 3a and male (Fig. 3b). For instance,HBHEPB, ROTH1, PNC13, DTAP IPVHIB, PNC,ROTHB5, DTAPHEPBIP, HIBV, DTAP, and IPV wereclustered together based on their associations with adverseevents in the female population. Besides most of the vac-cines that were grouped in the female population, we alsofound four more vaccines in the same group in the malepopulation, including PNC, HEP, VARCEL, and MMR.Similarly, while DIPHIB, DTP, and OPV were tightlyTable 1 General characteristics of the networksNnode Nlink AveragedegreeAverage pathlengthNetworkdiameter1990 342 990 5.79 3.05 71991 634 2,756 8.69 2.69 51992 553 2,275 8.23 2.58 51993 517 2,263 8.75 2.45 51994 549 2,487 9.06 2.46 51995 637 3,080 9.67 2.54 61996 628 3,028 9.64 2.54 51997 660 3,270 9.91 2.51 51998 793 3,989 10.06 2.54 51999 1,047 5,245 10.21 2.67 52000 1,065 5,708 10.72 2.59 52001 906 4,904 10.83 2.53 52002 987 5,289 10.72 2.58 52003 1,430 8,197 11.46 2,52 52004 1,160 6,491 11.19 2.62 52005 1,315 7,290 11.08 2.66 62006 1,978 12,128 12.26 2.66 52007 3,103 20,214 13.03 2.59 52008 3,045 20,196 13.27 2.58 62009 3,351 22,054 13.16 2.57 52010 2,639 18,041 13.67 2.62 52011 2,142 13,533 12.64 2.69 52012 1,943 12,292 12.65 2.72 52013 826 4,886 11.83 2.72 5Female 4,947 49,616 20.14 2.35 5Male 4,519 51,578 22.83 2.35 5Overall 5,938 53,742 18.10 2.48 5Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 5 of 8clustered in the male population, RV was also grouped inthis cluster in the female population. The dendrogramsindicate the same differences between two populations(Fig. 3c and d). These results indicate that there are indeedsex-specific reponse differences after vaccine injection.We also compared whether pairs of vaccines with similarassociation profiles in female-specific association networkare also similar in male-specific network. In Fig. 4, the dis-tributions of PCC indexes are different in two populations,indicating that there are some sex-specific associations inboth populations, although majority of association relation-ships can be identified in both populations. Specifically,there were more vaccine pairs showing high similairties inthe male population than in the female population. Theunderlying mechanisms need further investigation usingother types of biological data, such as genomic, metablo-mic, and proteomic level measurement data.DiscussionsMost vaccine-preventable diseases have declined in theUnited States by at least 9599 % [20, 21]. However, vac-cines are pharmaceutical products that carry risks. Cer-tain biomarkers or individual variations could implicatedifferent vaccine responses, which are essential for preci-sion medicine. Identifying these associations is critical tovaccine safety, which reassures public acceptance of vac-cines. One way to address this question is the post-approval surveillance of vaccine AEs. For instance, theVAERS is a passive surveillance system to monitor vac-cine safety after the administration of vaccines licensedFig. 3 Comparison of vaccine similarity in different sexes. a Hierarchical analysis of vaccines based on association information in femalereports; b Hierarchical analysis of vaccines based on association information in male reports; c Dendrogram of vaccine similarity in femalereports; d Dendrogram of vaccine similarity in male reportsZhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 6 of 8in the United States [2]. Such surveillance data can com-plement the original safety evaluation data generatedfrom the clinical trial phases and provide more compre-hensive safety assessment in a much larger population.We are one of the first research groups that investigatessex-specific vaccine-AE association patterns by integrat-ing traditional statistical signal detection and networkanalysis approaches. Our findings indicated that saftetysignals present different patterns in female and malepopulation. This is consistent with previous studies inthe vaccine community [22, 23]. With high-throughputtechnology advances such as next generation sequencing,transcriptomics, epigenetics, proteomics, and new compu-tational approaches to interpreting big data, we expect abetter understanding of associations and mechanisms ofvaccine AEs and immunogenicity. Network analysis ap-proaches is one of the promising stratetigies to integratesuch heterogeneous big data, leading to a morepersonalized or individual approach to vaccine practice inthe near future.Conclusions and future workIn this paper, we proposed a network-based approachto investigate the vaccine-AE association network fromVAERS. The results indicated that (1) network diameterand average path length of vaccine-AE associationnetworks dont change dramatically over a 23-yearperiod, while the average node degree of these networkschanges due to the different number of reports duringdifferent period of time; (2) vaccine-AE associationsderived from different genders show sex-associatedpatterns in sex-specific vaccine-AE association net-works. To our knowledge, this is the first time that anetwork-based approach has been used to identify sex-specific association patterns in a spontaneous reportingsystem database. Due to unique limitations of suchFig. 4 Density plot of PCC association indexes in female vs male populations (red: female; blue: male)Zhang et al. Journal of Biomedical Semantics  (2015) 6:33 Page 7 of 8passive surveillance systems, network-based approacheshave the potential to (1) identify nodes of importance,irrespective of whether they are disproportionallyreported; (2) provide guidance on sex-specific recom-mendations in personalized vaccinology.Extensions of this work include: (1) integration ofother spontaneous reporting system databases (e.g., theEuropean Adverse events following immunization(AEFI) system) to construct more complete vaccine-AE association networks; (2) incorporation of othercomplementary public databases such as SemanticMEDLINE [24]; (3) development of advanced network-based approaches taking the PRR values into account;(4) investigation of other types of data mining methodsto assess the significance of vaccine-AE associations;(5) focused investigation of examples based on the net-work parameters; and (6) identification of sex-specificsubnetwork patterns of AE correlation networks.Additional fileAdditional file 1: Network file containing all the networkspresented in Table 1. (CYS 13422 kb)Competing interestsThe authors declare that they have no competing interests.Authors' contributionsYZ and CT led the study design and analysis, and drafted the manuscript. PYextracted associations from VAERS database and performed statitisticalanalysis. YL converted the VAERS information into RDF format. All authorsread and approved the final manuscript.AcknowledgementsThis project was supported by the National Cancer Institute grant P30 CA13427404 to the University of Maryland Baltimore, and the National Libraryof Medicine of the National Institutes of Health under Award NumberR01LM011829 to C.T.Author details1Division of Biostatistics and Bioinformatics, University of MarylandGreenebaum Cancer Center, Baltimore, USA. 2Department of Epidemiologyand Public Health, University of Maryland School of Medicine, Baltimore,USA. 3University of Wisconsin-Madison, Madison, Winsconsin, USA.4Department of Computer Science and Engineering, Lehigh University,Bethlehem, Pennsylvania, USA. 5School of Biomedical Informatics, Universityof Texas Health Science Center at Houston, Houston, Texas, USA.Received: 10 December 2014 Accepted: 10 August 2015Published: 19 August 2015RESEARCH Open AccessMy Corporis Fabrica Embryo: Anontology-based 3D spatio-temporalmodeling of human embryo developmentPierre-Yves Rabattu1*, Benoit Massé2, Federico Ulliana3, Marie-Christine Rousset3, Damien Rohmer2,4,Jean-Claude Léon2 and Olivier Palombi1,2AbstractBackground: Embryology is a complex morphologic discipline involving a set of entangled mechanisms, sometimedifficult to understand and to visualize. Recent computer based techniques ranging from geometrical to physicallybased modeling are used to assist the visualization and the simulation of virtual humans for numerous domainssuch as surgical simulation and learning. On the other side, the ontology-based approach applied to knowledgerepresentation is more and more successfully adopted in the life-science domains to formalize biological entitiesand phenomena, thanks to a declarative approach for expressing and reasoning over symbolic information.3D models and ontologies are two complementary ways to describe biological entities that remain largelyseparated. Indeed, while many ontologies providing a unified formalization of anatomy and embryology exist,they remain only descriptive and make the access to anatomical content of complex 3D embryology modelsand simulations difficult.Results: In this work, we present a novel ontology describing the development of the human embryology deforming3D models. Beyond describing how organs and structures are composed, our ontology integrates a proceduraldescription of their 3D representations, temporal deformation and relations with respect to their developments. Wealso created inferences rules to express complex connections between entities. It results in a unified description ofboth the knowledge of the organs deformation and their 3D representations enabling to visualize dynamically theembryo deformation during the Carnegie stages. Through a simplified ontology, containing representative entitieswhich are linked to spatial position and temporal process information, we illustrate the added-value of such adeclarative approach for interactive simulation and visualization of 3D embryos.Conclusions: Combining ontologies and 3D models enables a declarative description of different embryologicalmodels that capture the complexity of human developmental anatomy. Visualizing embryos with 3D geometricmodels and their animated deformations perhaps paves the way towards some kind of hypothesis-driven application.These can also be used to assist the learning process of this complex knowledge.Availability: http://www.mycorporisfabrica.org/BackgroundEmbryology has been studied for many centuries. Thismorphologic discipline studies the transformation of a sin-gle cell into a complete organism, which is composed ofalmost 1014 cells. This discipline has benefited of progressfrom imagery, histology, molecular biology and genetic.However it remains a high level of complexity and lots ofphysiological and pathological mechanisms stay unclear.Computer modeling and simulation of human body allowto express and to visualize complex physiological processes,to make the steps of human embryology more accessible.However, no unified model which allows integrating em-bryological processes into 3D visualization exists.An ontology is a formal description of a domain ofinterest [1]. Ontologies are increasingly employed in thelife-science domains, and in particular in biology, for the* Correspondence: PYRabattu@chu-grenoble.fr1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, FranceFull list of author information is available at the end of the articleJOURNAL OFBIOMEDICAL SEMANTICS© 2015 Rabattu et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 DOI 10.1186/s13326-015-0034-0sake of standardizing a common vocabulary of entitiesand expressing the inherent complexity of biological sys-tems. Moreover, ontologies formal systems can be read,understood and exploited by computers to carry out tasksthat would be otherwise tedious and time-consuming forthe humans. The main biology ontologies are referencedin OBO Foundry [2] and NCBO Bioportal [3].Human anatomy ontologies are referenced by theFoundational Model of Anatomy domain, FMA [4], whichcontains the concepts and relationships that pertain to thestructural organization of the human body. Uberon [5] isa multi-species ontology for anatomy (just as the geneontology (GO) project [6] is a multi-species ontology formolecular functions, biological processes or cellular com-ponents). It is integrated with species-specific ontologies,but it does not dependent on them.Regarding embryology, some interesting concepts havebeen introduced in several anatomical ontologies like thedrosophila (DAO) one [7]. More specifically, Kaufman [8]developed the Atlas of Mouse Development which hasbeen integrated into the Mouse Atlas Project (MAP) led byDavidson [9], Bard [10], Baldock [11] and Ringwald [12]. Itis a study of mouse developmental anatomy through histo-logical sections of mouse embryos reconstructed in a full-grey image, for each developmental stage (Theiler stages).Brune [13] has completed this project by defining 3D re-gions (domains) of the embryo models, to map an ontologyto each of these spatial models. Through the 3D domains,users can navigate from the spatial representation of theembryo to the ontology and vice versa. It is important tonote that there is one ontology for each Theiler stage. Linksbetween ontologies and 3D images also exist in some otheratlas as in the Virtual Fly Brain project allowing users toexplore the structure of the Drosophila brain by browsing3D images of a brain with subregions displayed as coloredoverlays [14] and, in a more general setting, in Allen BrainAtlas [15].Burger [16] developed the concept of the AbstractMouse which was further developed by Hayamizu [17]who created a non-stage specific representation of amouse developmental anatomy ontology (EMAPA, [18])where each anatomical entity is linked to the first and laststages of its existence.Hunter [19] has built a human developmental anat-omy ontology (EHDA), which is composed of severalontologies, one for each Carnegie stage (CS) [120]that only includes basic part_of data. Bard [20] devel-oped an ontology of human developmental anatomy(EHDAA2) with more than 2000 anatomical entities,which are linked by part_of, is_a, develops_fromrelationships and linked to the Carnegie stage bystarts_at and ends_at relations. He had initiallydeveloped a work, as part of the Edinburgh MouseAtlas Project, to provide a structured and controlledvocabulary of stage-specific anatomical structures forthe developing laboratory mouse.Palombi et al. [21,22] have developed an ontology of hu-man anatomy which includes 3D representations, whereeach anatomical entity is linked to its corresponding 3Dmodel. They also considered the function of each entity toassist the creation of complex 3D models for visualizationand simulation [23]. My Corporis Fabrica (MyCF) isequipped with automatic reasoning capabilities that enablemodel checking and complex queries answering. Theydemonstrated that 3D graphical models are effective torepresent either anatomy or embryology.This kind of 3D modeling is already used in Bio-mechanics, Computer Aided Medicine and ComputerGraphics. It can be useful in plenty of domains like bio-mechanics, ergonomics, diagnosis, treatment planning,visualization, graphics, robotics, or, for example, helpingdeduce the anatomic consequences of an injury [24].Kerwin et al. have developed a three-dimensional atlas ofthe human embryonic brain using anatomical landmarksand gene expression data to define major subdivisionsthrough 12 stages of development (Carnegie Stages 1223).Virtual 3D anatomical models were generated from intactspecimens using optical projection tomography. The 3Dmodels and a preliminary set of anatomical domains andontology are available on the atlas pages along with geneexpression data from approximately 100 genes in the HUD-SEN Human Spatial Gene Expression Database [25,26].Gasser [27] developed the Virtual Human Embryo [28],which is an atlas of digitally captured images of serialembryo sections corresponding to each of the 23 Carnegiestages. Then, they were assembled to provide 3D imagesand holistic views of development. Moreover, this databaseis accessible to all researchers, teachers and students andcan be used acknowledging the source of the images, i.e.,Virtual Human Embryo DREM Project.Full exploitation of ontologies goes far beyond providingstandardized notations for explicit formalized knowledgeon domains of interest. It requires equipping ontologicalstatements with reasoning and querying capabilities inorder to efficiently retrieve relevant and precise informa-tion. However, until now, only few existing works inbiomedical ontologies (e.g., [2124]) take advantage ofavailable automatic reasoners. Most of these works rely onontologies expressed in OWL and use OWL reasonerssuch as Pellet [29] or Racer [30] that come with a highcomputational complexity in the worst case (as they areDescription Logics reasoners). Recently, RDF-based seman-tic environments such as Jena (http://jena.apache.org/) orCwm (http://www.w3.org/2000/10/swap/doc/cwm) haveincluded rules (a la Datalog) to perform inferences on topof RDF datasets. Datalog rules [31] and Description Logics[32] are two orthogonal decidable fragments of first-orderlogics that have been extensively studied in knowledgeRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 2 of 15representation and in deductive databases. The interest ofDatalog rules is that they are easy to read and write forpractitioners and they have a polynomial data complexitywhile allowing expressing complex interaction betweenproperties and recursivity. In addition, as it will be shownin our examples, rules allows to capture in a uniform man-ner OWL constraints that are useful in practice, such asproperty transitivity or symmetry, but also domain-specificrules with practical relevance for users in many domains ofinterest.In this paper, we describe a novel ontology of humanembryo developmental anatomy. Thanks to a RDF-based uniform knowledge representation formalism, itenables interoperability between anatomical models(based on a stage-independent hierarchical structurelike Bards), 3D graphical models [21] and spatio-temporal representations of development processes.This integrated declarative approach facilitates the gen-eration of animations of the embryological developmentand opens new possibilities for learning and reasoning,in particular for a better understanding of developmen-tal abnormalities. The key issue to achieve an effectiveinteroperability is the rule-based inference allowing usto express quite simply how properties interact to relatethe different models.Our final goal is to include some malformation path-ologies and represent them so as to increase the under-standing of the mechanism of their pathogenesis. Theembryological entities we consider have been willinglyrestricted in numbers and simplified in terminology toease the development and the test of our approach.ResultsThe MyCF Embryo ontology has been designed so as toaccommodate the description of several embryos, male orfemale, their 3D representations and their spatio-temporaldevelopmental processes.MyCF Embryo ontology is made of 6 taxonomies ofclasses related by relations, and by a set of 15 rules, thatwe will describe now.The top classes of the different taxonomies are: embryo-logical_entity, temporal_entity, geometrical_entity, process,spatio-temporal_representation and disease.Each embryological entity is assigned to a set of spatio-temporal representations describing a geometrical compo-nent at a given time or during a process over a givenduration.We now describe the main classes in the ontology andthe properties linking them. Additional file 1: Table S1summarizes them and provides their definitions and thecorrespondences with existing ontologies if any.Our ontology is expressed in RDFS [33] which is broadlyused in Linked Data [34] to express so-called light-weightontologies with a rule-based semantics. Whereas OWL isoften seen as an extension of RDFS, this is not exactly thecase, mainly because RDFS allows to use the same identifieras a class, instance or property, which is not possible inDescription Logics (at the basis of OWL profiles), inwhich the sets of instances, classes and properties must bedisjoint. In addition, RDF allows to use blank nodes in classor property position.Similarly, the RDF query language SPARQL can querysimultaneously the data and the schema and allows vari-ables to stand for classes and properties. This goes beyondthe first-order conjonctive queries typically considered inDescription Logics. It is worth emphasizing that OWL andRDFS can interoperate, as there exists an RDF triple nota-tion for most of the OWL constructs (in particular theOWL2 profiles), and the corresponding axioms can beexpressed as logical rules. An important need was toexpress fine-grained domain-specific rules, in particular fordescribing the spatio-temporal development relations be-tween subparts of the embryo. As it will be shown below,such rules are quite easy to express on top of RDF facts,while they are not expressible in Description Logics andthus, in OWL.Embryological_entityEmbryological entities model the anatomical structure of aformal embryo as classes (e.g., kidney, left_kidney, embryo)directly related by properties (that hold between classes andnot between instances in contrast with modeling usingOWL). Thus, declaring that left kidney is a part of embryowill be simply expressed by a single RDF triple<left_kidney,part_of, embryo>, as it is done in MyCF ontology [21].Embryological entities can be material or immaterial. Forexample, the kidney, gonad and cloaca are material entitieswhile the renal hilum and the ureterovesicular junction areimmaterial ones. The relationships between embryologicalentities are described by standard ontology properties likesubClassOf (left kidney is a subclass of kidney) and part_of(left kidney is a part of embryo) and its inverse has_part.We use also the property develops_from presented in [20]to describe the lineage between entities during the embryodevelopment. Some entities directly_develops_from an-other, means that an organ is a direct outcome fromanother organ. This allows us to express for instance thatthe kidney directly_develops_from the metanephric blas-tema, the metanephric blastema directly_develops_fromthe metanephros and so, the kidney develops_from themetanephros (see inference section below).Temporal entityLet us first note that we will use within this work the termevolution in its animation and modeling meaning, i.e. a sur-face undergoing geometrical deformation through time,and not with respect to its biological sense.Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 3 of 15Temporal entities represent Carnegie stages and gestationweeks. These allow us to describe the evolution of the em-bryo at different granularities. In particular, the 23 Carnegiestages correspond to the first eight weeks of gestation. Theremaining gestation weeks (form 9 to 12) are representedby single classes. Since Carnegie stages have different dur-ation, we employ two new properties from_gestation_day toto_gestation_day, in order to describe the first and last dayof each stage. For example, the stage 14 lasts two days(from day 32 to day 34), while the stage 17 lasts three days(from day 39 to day 42).As we will discuss next, temporal entities are a crucialaspect of our ontology model for the procedural modelingof the embryo evolution. Moreover, each stage is describedas a stage following_stage the previous one. For example,the stage te15 following_stage te14. This relation, follo-wing_stage, is simply used to express the chronologicalorder between the gestation stages and it is similar toimmediately_preceded_by found in RO (see Additionalfile 1: Table S1). A specific reasoning process over stagesis presented in Osumi-Sutherland et al. [35] for auto-matic classification.Geometrical componentGeometrical components are classes representing 3D geo-metric shapes. These classes instances are paired with thespatio-temporal representation of organs. They allow theuser to visualize the coarse representation of the organs ata user-prescribed time, i.e., Carnegie stage or gestationweek. More specifically, we focus on being able to representthe overall shape, position, orientation and size, of theorgans. We have defined the following 6 basic geometricalprimitives to represent an organ or a part of it: point, line,plane, ovoid, cylinder and duct. We used only simplegeometrical primitives in this work to enable the proceduralgeneration of the geometry of organs and we explain in thediscussion how to extend such representations to producea more accurate visual representation of the organs.Ducts represent the ureter and other tubular structures.An example of such structure is given in Fig. 1a.The planes are used to illustrate some immaterial em-bryological entities such as a foramen or embryo sagittalplane, or a flat side of an organ, e.g. the inferior side ofliver.Cylinders are used to describe some solid organs suchas the diaphragm whose geometry is roughly cylindrical.The line is used to describe a boundary of an organ ora particular geometric feature, e.g. the vertebral line,used to fix or to move an organ relatively to it.Points are used to represent immaterial entities linkingdifferent organs, e.g. the left uretero-vesicular junction link-ing the ureter with the bladder. The contact area betweentwo organs is described with a unique point belonging toboth organs.Of course, this set of geometric entities can be extendedtowards more complex geometric shapes.Each geometric object is defined by a barycenter position,axis size, and vector orientation. For example, kidneys aredrawn as ovoids (e.g. Fig. 1b). AT stages 14 and 15, we havetwo different instances of the ovoid class. Each instanceprovides information about the size of the ovoid axes, theposition of the barycenter w.r.t. the embryo-axes andvectors coordinates to orient the shape w.r.t. to the refer-ence frame of the scene.At the ontology level, these three informations are setwith three properties, namely axis_size, barycenter_posi-tion, and x, y and z_axis_orientation that are subproper-ties of vector_coordinates. These properties arespecialized further in order to process complex objectslike ducts or lines. These two shapes have two end-points, called the rostral and the caudal ends that mustbe described separately. Therefore we introduced somesubproperties, namely caudal_end_axis_size, rostral_-end_axis_size, caudal_end_barycenter_position, rostral_end_barycenter_position, caudal_end_vector_coordinates, rostral_end_vector_coordinates.Fig. 1 a Geometrical representation of a duct. The both rostral and caudal ends can be described and can have evolution separately. b Geometricalrepresentation of an ovoid. This ovoid can be described using a barycenter and three axes with specific size and orientationRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 4 of 15Finally, the property has_geometrical_representation isused to link the objects with the spatio-temporal represen-tation of each structure, which are described next.Spatio-temporal representationSpatio-temporal representations are the central part ofour ontology. They establish a precise connection betweenthe world of anatomy and 3D graphics, along the spaceand time dimensions.The instances of this class link organs (e.g. the classleft_kidney) with their geometrical representation (e.g.an instance of ovoid) and temporal entities (e.g. Carnegiestage 14). The property describes connects a spatio-temporal representation with an anatomical entity. Regard-ing the Xtemporal aspects, the property at_stage connectsthe spatio-temporal representation instance with a gesta-tion moment. To describe the evolution of organs, spatio-temporal representation instances must also relate to time-intervals.For example, to describe the process undergone by theureter between stage 14 and 18, the process instance isexplained by the property has_process that ties a spatio-temporal representation instance with a process instance asfollows.ProcessEvolutionary processes represent the observed phenomenonthat can take place in the embryo during a gestation dur-ation. We represented 6 kinds of processes: growth, migra-tion, rotation, interaction, division and fixation.Growth involves almost all organs. Migration involvesorgans like the kidney that moves from the pelvic to thelumbar region, or the gonad that does almost the samein the opposite direction. Rotation involves organs likethe kidney that undergo a 90° rotation around two of itsreference axes during its migration towards the lumbarregion. Interaction between organs is necessary for theirdevelopment like the ureter-kidney interaction, which isnecessary for the kidney growth. Division can be either aphysiological or a pathological process that leads anorgan to abnormally split in different parts. At an ex-ample, it can be used to model the kidney duplication,occuring when a ureter (ureteric bud) splits too earlybefore joining a kidney (metanephric blastema). Fixationdescribes the hard link of two anatomical entities.We introduce some properties to describe the processes,for instance, the direction of a migration or a rotation of or-gans, the growth proportion, fixed-points for fixations, theinteracting objects involved in a process. These propertiesare migration_direction (which is a subproperty of vector_-coordinates), rotation_degree, growth_proportion (for rostraland caudal ends), fixed_to (for rostral and caudal ends).Rostral and caudal subproperties allow a variation of theendpoints of an object like ducts and lines in a differentway.To capture the dependency between the different pro-cesses in the embryo evolution we also use the propertydepends_on. For example, the kidney growth depends onthe interaction between ureter and kidney.A spatio-temporal representation is linked to a processby has_process and to duration between two temporalentities using from_stage and to_stage.It is important to note that the properties starts_at, end-s_at, which are used to describe the existence of an organover a time interval differ from the properties from_stageand to_stage which describe a process over a time interval.For example, the existence of the cloaca starts_at theCarnegie stage 11 and ends_at the Carnegie stage 14.However, the cloaca can have a process from_stage 11to_stage 12 and another one from_stage 12 to_stage 14.DiseaseThe Disease class describes the embryo malformationsthrough the activation or inactivation of some processes[36]. For instance, the hypoplastic kidney occurs whenthere is no interaction between the ureter and the kid-ney, and it inhibits the kidney growth. We use the prop-erty impacts_processus to relate diseases with impactedprocesses. We use the property impacts_entity to denotethe anatomical entities involved in a process or impactedby a disease.Running exampleWe now describe in detail how the evolution of an organ isrepresented in our ontology. We will use the representationof the evolution of the left kidney during the period be-tween Carnegie stages 14 to 20.An instance of the organ left kidney is described througha set of spatio-temporal representations (3D). Below,e00_left_kidney is an instance of left kidney, i.e., the left kid-ney of a particular embryo (the embryo 00). The left kidneyclass is a subClassOf kidney, which is a part_of embryo.The graphical representation of these triples is givenin Fig. 2.Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 5 of 15Concerning the 3D representation, the e00_left_kidneyhas a set of spatio-temporal representation instances,either at a given stage or for a givenduration .This is depicted in Fig. 3.The description of the kidney links its spatio-temporalrepresentation instance to a geometrical component(has_geometrical_representation) during a specific stage(at_stage). This geometrical component is an ovoidwhich has some information about its barycenter_posi-tion, its axis_size, its x, y and z_axis_orientation, as illus-trated in Fig. 4.The dynamic description of the left kidney is achievedthrough a connection between a spatio-temporal repre-sentation instance, a process (with property has_process)and a duration period (with properties from_stage andto_stage). For example, the fact that the left kidney has agrowth_proportion of 10 on each of its three axes duringthe stages 14 to 20 of gestation is declared as in Fig. 5.We now illustrate how we represent pathologies. Weconsider the example of kidney hypoplastic pathology,which impacts the interaction between ureter and kidneyduring the embryo development, and thus the kidneygrowth process. We use the property impacts_processusto relate a pathology to the process it impacts (seeFig. 6).When querying the ontology, we can discover that thekidney hypoplastic lacks a kidney growth.Below we illustrate the whole resulting ontology fragment(see Fig. 7).InferenceThe inference rules of MyCF Embryo express complexconnections between entities. These rules offer a uni-form setting for expressing the semantics of most of theOWL and all the RDFS constraints (such as transitivityor symmetry of some generic properties like subClassOfor subPropertyOf, and of more specific properties likepart_of or develops_from) but also domain-specific rulesthat have to be declared by the ontology designer. Theserules capture in a very compact way implicit facts thatcan be made explicit on demand or at query time by aninference mechanism.This mechanism is automatic and consists in applyingthe rules on the explicit facts declared and stored as RDFtriples, in all the possible manners satisfying the conditionsof these rules. For each possible instantiation of thevariables (denoted by a name starting by ?) appearing in acondition part of a given rule such that all its conditionsare satisfied by explicit facts, the new facts correspondingto the (appropriately instantiated) conclusion of the rule areadded. This saturation process is iterated as long as newfacts can be produced. The termination is guaranteed bythe form of the rules that are considered. They correspondto safe rules, also called Datalog rules, i.e., all the variablesFig. 2 Representation of an organ in the ontology. Class and subclassare represented in dark orange round-angle boxes. Instances arerepresented in light orange right-angle boxesRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 6 of 15appearing in the conclusion of a rule also appears in thecondition part.The rules that are considered in the current version ofMyCF Embryo are summarized in Additional file 2:Table S2.The first group of rules enriches the description ofthe embryological entities development. In particular,it describes the dependency between anatomical en-tities during the gestation by means of propertiesdirectly_develops_from and develops_from that denotedirect and (possibly) indirect dependencies,respectively.The rule R1 defines directly_develops_from as a sub-property of develops_from. This last one is a transitiveproperty, as defined by rule R2. As illustrated in Fig. 8,rule R1 allows us to infer for instance that since thekidney directly develops from the metanephric blastema,then more generally it develops from that entity. Fur-thermore, because the metanephric blastema developsfrom the metanephros, by rule R2, we can infer that thekidney develops from the metanephros. Rule R3 de-scribes the development relations between subparts ofthe embryo. As illustrated in Fig. 9, it allows for instanceto infer that since the metanephric blastema directly de-velops from the metanephros, and this last one is a part ofthe nephros, then the metanephric blastema develops fromthe nephros. Rule R4, is the analogous of rule R3, for thesubClassOf property. It is also illustrated in Fig. 9.The second group of rules describes the relations be-tween embryological entities and geometrical entitiesthrough their spatio-temporal representations at differ-ent gestation stages.Fig. 3 Links between spatio-temporal representation with organ instance and temporal entities. Each spatio-temporal representation is linked to a specificstage (at_stage) or to duration (from_stage, to_stage). The organs classes and instances are represented with orange boxes. Spatio-temporal representationsare represented in purple boxes and temporal entities are represented with green boxesFig. 4 Static description of the left kidney at stage 14. Link between spatio-temporal representation and geometrical component (has_geometrical_representation). Each spatio-temporal representation is linked to a geometrical representation which allows to give some information aboutsize, position and orientation of the organ. The double arrows show the link to dataproperties allowed to give some numerical information. Thegeometrical components are represented with blue boxesRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 7 of 15The rule R8 allows to tie embryological entities withtheir geometric representations, so as to answer to querieslike "which geometrical representations of kidneys areavailable?.The effect of this rule is illustrated in Fig. 10. It allows usto infer that a given instance e00_left_kidney of the left_kid-ney entity can be described by a specific left_kidney_ovoidgeometrical model, associated to its spatio-temporal repre-sentation st_representation_of_left_kidney_at_te14.The rule R9 further describes geometrical represen-tation of an embryo organ by explicitly inferring thegestation stage of the organ that it represents. Asshown in Fig. 10 it allows us to infer that since st_re-presentation_of_left_kidney_at_te14 corresponds to theCarnegie stage 14 (te14), then the corresponding geo-metrical model left_kidney_ovoid describes the corre-sponding instance of left kidney at this stage 14.At this point, it is also possible to query the ontol-ogy and ask for "all geometrical representations (?g) ofentities (?ee) that develop from the Cloaca, startingfrom stage 14". This translates to the followingSPARQL queryTo impose a discrete order between the gestationstages, we introduced the property following_stage. Thispermits to state that stage 15 follows stage 14 and thatstage 16 follows stage 15. By means of the rule R10, wecan compute the total order between the gestationstages, so as to infer also that stage 16 follows stage 14,and have a complete answer to our query.The last group of rules enriches the description ofthe evolution processes of the embryo. Rule R11 de-fines the indirect dependency between processes. AsFig. 5 Dynamic description of the left kidney from stage 14 to stage 20. Each spatio-temporal representation for duration is linked to a process,which have some evolutionary characteristics, represented by double arrows to numerical information. Processes are represented in yellow boxesFig. 6 The impaction of a disease on a processus. A pathology can impact a process to lead to a malformation like a malrotation of an organ, aless of migration or growth. The pathologies are represented in dark purple boxesRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 8 of 15illustrated in Fig. 11, as the kidney growth depends onthe interaction between kidney and ureter and thislast one depends on the fixation between kidney andureter, we can deduce that the kidney growth dependson the fixation between kidney and ureter. Rule R12connects processes and pathologies. For example, asshown in Fig. 12, since the lack of kidney growth impliesthe hypoplastic kidney, and that the kidney growth dependson the interaction between kidney and ureter, we can alsodeduce that the lack of interaction between kidneyand ureter implies the hypoplastic kidney. The lasttwo rules relate processes to the anatomical entitiesthey impact. As depicted in Fig. 13, rule R13 allowsus to infer that process left_kidney_growth_between_-te14_and_te20 impacts entity left_kidney due to thefact that the left_kidney_growth_between_te14_and_te20is the process described by the spatio-temporal representa-tion st_representation_of_left_kidney_from_te14_to_te20,and this spatio-temporal representation describes aninstance of the left_kidney. Finally rule 14 and 15 areused to infer the Carnegie stages where a process oc-curs. These are illustrated in Fig. 14. For example, wecan infer that the process left_kidney_growth occursbetween stages 14 and 20, because its associatedspatio-temporal representation lasts during thisperiod.3D visualization and modelingFinally, the animated 3D model is built using the infor-mation stored into the ontology. The scene creation re-quires three steps. First, the user selects the organs andthe gestation period. Secondly, all the informations de-scribing those organs are retrieved from the ontology.Thirdly, these informations are used to create the meshescorresponding to the organs, and to animate the scene.Fig. 7 Overview of the left kidney static and dynamic descriptions in MyCF Embryo. For the kidney example, we can see the links with spatio-temporalrepresentation, the geometrical component and the process, and the impact of the disease. Note the importance of the spatio-temporal representationwhich is the central part of our ontologyRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 9 of 15For example, the user can select a set of organs of theuro-genital system (gonads, kidneys, ureters, cloaca,bladder, rectum), and a period which goes from Carnegiestage 10 to gestation week 12.The following query extracts all geometrical re-presentations and properties of the left kidney for thereference model we built, named mcfe: e00_left_kidney.For each spatio-temporal representation (?str) andgeometrical representation (?ge) of the left kidney, thisquery extracts the geometrical shape (?shape), the stage(?stage) and all informations needed to describe theshape of organs (?property and ?value) like their axis sizeor their barycenter location.Informations about processes are obtained in a similarway.The organ information extracted from the ontology isprocedurally interpreted. The first step is to create a staticscene containing all the selected organs. Each organ of thescene is a mesh generated with the information obtainedfrom its geometrical entity in the ontology.We now describe how the meshes are generated. Whenthe geometrical shape is an ovoid or a cylinder, the meshcan be simply generated from the ovoid or cylinder bary-center position, axis size and orientation. When thegeometrical shape is a duct, the mesh is generated as asequence of cylinders around its skeletal line. This skeletonis defined by a Bézier curve, whose endpoints coordinatesand tangents are directly given in the ontology. In somecases, a complex geometry may not be fully procedurallydescribed. In such a case, an existing mesh can be specifiedas a link in the ontology. Therefore, the mesh can be in-cluded directly in the 3D scene.Fusion and division processes require specific model-ing approaches because the associated shape of organsmust blend or split seamlessly. For example, the cloacadivides itself into two parts becoming the bladder andthe rectum. These organs are represented using implicitFig. 9 Inference R3 and R4Fig. 8 Inference R1 and R2. The inferred links are represented withdotted arrowsRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 10 of 15surface modeling which are surfaces defined as the constantvalue, called isovalue, of a field function defined in 3Dspace. In our implementation, we define the field functionas a smooth decreasing function of the distance to a refer-ence point, or a reference curve, called the skeleton of theimplicit shape. More specifically, we chose to use a polyno-mial field function with compact support called meta-ball.Finally, the implicit surfaces are converted into a mesh forthe visualization (Figs. 15, 16 and 17).Lastly, the scene is procedurally animated, thanks to theinformation given by the evolution processes. As theCarnegie stages may have different durations, we considerthat the gestation day is the common unit of time to com-pute the animation through all stages. As an example, stage11 lasts one day whereas stage 17 lasts three. Consequently,the animation of stage 11 lasts one time unit whereas stage17 lasts three.Given these settings, it is possible to generate an ani-mated 3D scene representing the embryo developmentgiven only high-level descriptions of the contributing or-gans. To illustrate how a 3D scene and an animation of acomplex process (such a kidney and urogenital formationand migration with ureter fixation) can be built and moni-tored by the ontology. The complementary video (see Add-itional file 3: movie 1) firstly shows each step described inthe ontology in an independent manner, and lastly theresulting complex 3D animation. This video demonstratesthe effective monitoring of the ontology over the 3 Dmodels of organs displayed using a basic rendering. Highquality rendering of the scene could be explored in a futurework.DiscussionThis work proves that a 3D graphical model can be moni-tored by an appropriate ontology. This ontology incorpo-rates a subset devoted to spatial knowledge, which allowsthe description of organs as 3D shapes, and another subsetfocusing on temporal knowledge describing these organs ata specific stage. This ontology is also deductive, i.e., froman initial condition and some evolutionary process, it cancompute a final condition through a procedural approach.As an example, an initial position and orientation of thekidney at stage 14 with two evolutionary processes (growthand rotation) effectively produces its rotational movementand corresponding growth.This work aims at making a step towards automatic andadaptive 3D modeling and simulation of complex embryo-logical processes. This distinguishes our work from existing3D atlases in which ontologies are mainly used to navigatethrough static, predefined 3D illustrations.Our 3D geometrical representation of the organs is lim-ited to simple 3D primitives. We explain hereafter how thiswork could be extended to handle more accurate 3Dshapes.Firstly, let us note that the organs can be described hier-archically (see for instance Palombi et al. [21,22]). Secondly,complex and detailed shapes can be defined and deformedwith the knowledge of a more simple one acting as proxyFig. 10 Inference R8 and R9. Str correspond to the spatio-temporal representation instanceFig. 11 Inference R11Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 11 of 15or bounding structure. This implies that a 3D scene de-scribing the evolution of the organs can be described usingtwo levels of details.At the scale of the overall scene, the purpose is the defin-ition of the global interactions between different organs.For instance, these interactions can be the relative positionand collisions between organs, or the branching structures.At this level, all the organs can take part to the computa-tion, and the 3D local geometry may not need to be accur-ate. Note that this level is the one we described in thispaper using simple primitives.At the complementary level, the organs are describedmore accurately, possibly using lower levels of details.These levels that we plan to address in a future workcan be handled more locally and may involve only a sin-gle organ at a time, or the direct neighboring organs inthe case of collisions. We propose to handle this localgeometrical description in interleaving two approaches.Firstly, using a parametric description incorporating theuse of more basic shapes associated with more parame-ters handled within the ontology. Secondly, using a meshbased representation to achieve accurate 3D descriptionand visualization. Such a mesh will use the knowledge ofthe coarse representation acting as a guiding structureto be deformed. The deformation itself can be imple-mented using a smooth volumetric interpolation func-tion such as mean value [37], harmonic [38] or Green[39] coordinates functions, for instance. Note thatsimulation-based approaches and self-collision handlingcan also be used with the previous approaches to handlesmall geometrical details.We believe that using such an approach, i.e. interleavingthe basic guiding shapes with the complex mesh descrip-tion in an iterative process applied through the differentlevel of details can converge toward a more accurate geo-metrical description of the organs.We can also note that, introducing a physically-basedsimulator in the 3D model could extend the range of simu-lated events for pathological studies. More specifically, theresearch and teaching of teratology "in silico" could benefitsfrom the proposed approach. Indeed, the complexity ofthese topics makes the proposed 3D representation essen-tial for a good understanding.ConclusionWe have demonstrated that an ontology can be used tounify and organize medical knowledge in the domain ofembryo development together with 3D modeling processesand animation.Also we have shown the extensibility and scalability ofour declarative approach based on RDF triples and rules. Infact, this work extends the ontology MyCF [21] which hasbeen developed and enriched by medical students who hadno difficulty after a short training to edit and update aRDF-based knowledge base. The scalability of the approachhad already been shown for MyCF that contained about74000 classes and properties as much as 11 rules fordescribing 3D models of human body and its functions.We intend to extend the ontology for all structures inthe embryo for the entire duration of gestation and to val-idate the ontology using information from scans of realembryos. All ontologies represent the knowledge of aFig. 12 Inference R12Fig. 13 Inference R13Rabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 12 of 15community at a given time, and must be continuallyupdated and improved to remain up to date with the latestknowledge. A unique feature of our approach is thedeclarative nature of the graphical models, which makes itpossible for domain experts to enrich the knowledge baseat any time, through simple edit operations without havingto modify the (domain-independent) reasoning algorithmicmachinery used for answering queries. Embryology is inconstant evolution, benefiting from other sciences progress.We aim at making a step toward a collaborative tool thatwill ultimately improve medical care.Material and methodsThe main purpose of this study was to generate an ontol-ogy of human developmental anatomy unifying embryologyand 3D modeling processes.Our ontology describes in a comprehensible way thedifferent geometrical informations, evolution and interac-tions between various organs during gestation. These de-scriptions must be formatted in a way such that a 3Dmodel can be procedurally generated using informationsolely queried from the ontology. Our objective was not tobuild another ontology as other already exist, rather it wasto add geometrical and dynamic information to existing on-tologies enabling powerful applications in 3D graphics andvisualization domains.In this work, two tasks have been addressed simultan-eously. Firstly, we have built an ontology including all therequired information. To achieve this, we created differentcategories that describe entities' geometrical states andevolution, and a set of properties to link the different partstogether. Secondly, we developed a software tool thatgenerates a 3D graphical scene representing the embryodevelopment. To do so, it queries the ontology andextracts all information necessary for modeling, visualiz-ing and simulating the 3D scene.We focused on the development of the urinary system.We believe this system to be a good study case as itsdevelopment involves a diversity of mechanisms that arerepresentative of other evolution processes and a largevariety of diseases caused by anomalies of these processes.We used the TopBraidComposer [40] tool which allowsthe creation and edition of lightweight ontologies in RDFformat [41] format lightweight ontologies, and to querythem using SPARQL [42].RDF and SPARQL are standards recommended by theW3C for the semantic Web and Linked Data \footno-te{http://linkeddata.org/}.In addition, TopBraidComposer allow the addition ofinference rules on top of RDF datasets and supports theapplication of these rules on RDF facts until saturation (i.e.,until all the possible facts that can be inferred using theFig. 14 Inference R14 and 15Fig. 15 3D geometrical modelization of the cloacas division process is driven by the ontologyRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 13 of 15rules have been obtained). RDF datasets equipped withrules allow the capture of most of OWL \cite{owl} con-straints that are useful in practice, such as the transitivity orsymmetry properties, as well as domain-specific rules withpractical relevance for users in many domains of interest.The 3D graphical tool was developed using the 3D opensource software Blender [43]. All the 3D surfaces whereprocedurally handled using Python [44] scripting within theBlender framework. The ovoids were handled as Blenderparameterized primitives. The implicit surfaces were repre-sented as a set of points along the trajectory of the ductswhile the field function and the representative mesh of theisovalue was handled by Blender software. Finally, the shaperepresenting the boundary of the embryo was created inter-actively using Blender interface. All the coordinates param-eterizing the shapes and its animation were expressedlocally with respect to the embryo main axes and size. Thecoordinates were queried within our Python script to bedirectly usable within the Blender framework.Our ontology is public available: [https://mybody.in-rialpes.fr/mycfembryo/].Additional filesAdditional file 1: Table S1. It contains the detail of each usedproperties, the number of properties, classes and instances of theontology. (DOCX 24 kb)Additional file 2: Table S2. It contains the rules that are considered inthe current version of MyCF Embryo. (DOCX 14 kb)Additional file 3: Movie 1. It demonstrates the control of 3D modelingby the ontology. (MOV)Competing interestsThe authors declare that they have no competing interests.Authors contributionsPY-R contributed to the ontological modeling and the design of the overallarchitecture, participated to implement the rule engine and to the embryologyexpertise. BM contributed to the models (3D models) and the link betweenmodel and ontology, and is the main software developer. FU contributed to theontological modeling and to the design of the overall architecture, participated toimplement the rule engine. M-CR contributed to the ontological modeling and tothe design of the overall architecture. DR co-supervised this project. J-CLcontributed to the models (3D models and ontologies). OP conceived theoriginal idea, contributed to the temporo-spatial models and co-supervisedthis project. All authors read and approved the final manuscript.AcknowledgementThis work has been partially supported by the labEx PERSYVALLab(ANR11-LABX-0025-01) and by the ANR project PAGODA(ANR-12-JS02-007-01).Author details1Department of Anatomy, LADAF, Université Joseph Fourier, Grenoble, France.2LJK (CNRS-UJF-INPG-UPMF), INRIA, Université de Grenoble, Grenoble, France.3LIG (CNRS-UJF-INPG-UPMF), Université de Grenoble, Grenoble, France. 4CPELyon, Université de Lyon, Lyon, France.Received: 14 December 2014 Accepted: 2 September 2015Fig. 16 Example of the growth and rotation process on two axes of the kidneyFig. 17 Example of the migration, rotation, growth of kidneys and uretersRabattu et al. Journal of Biomedical Semantics  (2015) 6:36 Page 14 of 15RESEARCH Open AccessKneeTex: an ontologydriven system forinformation extraction from MRI reportsIrena Spasi?1*, Bo Zhao1, Christopher B. Jones1 and Kate Button2AbstractBackground: In the realm of knee pathology, magnetic resonance imaging (MRI) has the advantage of visualisingall structures within the knee joint, which makes it a valuable tool for increasing diagnostic accuracy and planningsurgical treatments. Therefore, clinical narratives found in MRI reports convey valuable diagnostic information. Arange of studies have proven the feasibility of natural language processing for information extraction from clinicalnarratives. However, no study focused specifically on MRI reports in relation to knee pathology, possibly due to thecomplexity of knee anatomy and a wide range of conditions that may be associated with different anatomical entities.In this paper we describe KneeTex, an information extraction system that operates in this domain.Methods: As an ontologydriven information extraction system, KneeTex makes active use of an ontology to stronglyguide and constrain text analysis. We used automatic term recognition to facilitate the development of a domainspecificontology with sufficient detail and coverage for text mining applications. In combination with the ontology,high regularity of the sublanguage used in knee MRI reports allowed us to model its processing by a set ofsophisticated lexicosemantic rules with minimal syntactic analysis. The main processing steps involve namedentity recognition combined with coordination, enumeration, ambiguity and coreference resolution, followed by textsegmentation. Ontologybased semantic typing is then used to drive the template filling process.Results: We adopted an existing ontology, TRAK (Taxonomy for RehAbilitation of Knee conditions), for use withinKneeTex. The original TRAK ontology expanded from 1,292 concepts, 1,720 synonyms and 518 relationship instancesto 1,621 concepts, 2,550 synonyms and 560 relationship instances. This provided KneeTex with a very finegrainedlexicosemantic knowledge base, which is highly attuned to the given sublanguage. Information extractionresults were evaluated on a test set of 100 MRI reports. A gold standard consisted of 1,259 filled templaterecords with the following slots: finding, finding qualifier, negation, certainty, anatomy and anatomy qualifier.KneeTex extracted information with precision of 98.00 %, recall of 97.63 % and Fmeasure of 97.81 %, thevalues of which are in line with humanlike performance.Conclusions: KneeTex is an opensource, standalone application for information extraction from narrativereports that describe an MRI scan of the knee. Given an MRI report as input, the system outputs the correspondingclinical findings in the form of JavaScript Object Notation objects. The extracted information is mapped onto TRAK,an ontology that formally models knowledge relevant for the rehabilitation of knee conditions. As a result, formallystructured and coded information allows for complex searches to be conducted efficiently over the original MRIreports, thereby effectively supporting epidemiologic studies of knee conditions.* Correspondence: I.Spasic@cs.cardiff.ac.uk1School of Computer Science & Informatics, Cardiff University, Cardiff CF243AA, UKFull list of author information is available at the end of the articleJOURNAL OFBIOMEDICAL SEMANTICS© 2015 Spasi? et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Spasi? et al. Journal of Biomedical Semantics  (2015) 6:34 DOI 10.1186/s13326-015-0033-1BackgroundMagnetic resonance imaging (MRI) is a technique usedto visualise internal body structure by recording radiowaves emitted by the tissues in the presence of a strongmagnetic field. MRI better differentiates between soft tis-sues than does X-ray imaging, which uses high frequencyelectromagnetic waves that pass through soft parts of thehuman body to create a radiograph, an image resultingfrom the different absorption rates of different tissues.MRI can also produce three dimensional images. When itcomes to diagnosing knee pathology, MRI has the advan-tage of visualising all structures within the knee joint, i.e.both soft tissue and bone. When used in conjunction withmedical history and physical examination, this makesMRI a valuable tool for increasing diagnostic accuracyand planning surgical treatments [15]. For example,meniscal tears are a relatively common knee injury, havinga prevalence of 22.4 % among all soft tissues injuries seenin a trauma department [6]. The accuracy of diagnosingmeniscal tears using individual physical tests is reportedto be 74 %, but increases to 96 % when MRI is used [5].When MRI results are combined with clinical assessments(namely, locking, giving way and McMurrays test), thentheir diagnostic performance increases respectively as fol-lows: accuracy  88.3 %, 89.9 % and 89.4 %, sensitivity 95.7 %, 97.4 % and 97.4 %, specificity  74.2 %, 75.8 % and74.2 %, positive predictive value  87.5 %, 88.4 % and87.7 %, and negative predictive value  90.2 %, 94.0 % and93.9 % [4]. More recently, the importance of MRI in diag-nosis and treatment planning for cases of symptomaticearly knee osteoarthritis has been highlighted. If an Xrayimage of the knee is found to be normal, but clinicalexamination produces specific findings, then MRI scancan be performed to establish more accurate diagnosis. Itcan be used to identify an appropriate surgical or nonsur-gical treatment target and decrease the need for costly andinvasive diagnostic arthroscopy [1, 7].In clinical practice, radiology images (e.g. produced byXray or MRI) are usually accompanied by imaging re-ports (or radiology reports), which serve the purpose ofconveying a specialist interpretation of images and relateit to the patients signs and symptoms in order to suggestdiagnosis [8]. This information is then used by cliniciansto support decision making on appropriate treatment.In terms of research, MRI evidence is often used to sup-port epidemiologic studies of knee pathology [9, 10]. Inparticular, MRI findings are indispensible features oflongitudinal studies of knee osteoarthritis [11, 12], wherelesions detected by MRI were found to precede onset ofclinical symptoms. However, many of published researchfindings are probably false due to sampling bias and lowstatistical power [13]. Small sample size is often the causeunderlying these two concerns although the relationship isnot simple or proportional [14]. Unfortunately, samplesize is typically subject to funding and personnel con-straints. Given the complexity and cost of manual inter-pretation of MRI evidence, it is, therefore, not surprisingthat the size of such epidemiologic studies has been lim-ited to hundreds (e.g. 514 [9], 710 [10]) or even dozens ofcases (e.g. 20 [11], 36 [12]). If interpretation of evidencedescribed in MRI reports could be automated, then itwould overcome the size limitation in retrospective cohortstudies posed by the need to manually sort through theevidence.We recently provided a critical overview of the currentstate of the art for natural language processing (NLP) re-lated to cancer [15], where clinical narratives such as thosefound in pathology and radiology reports convey valuablediagnostic information that is predictive of the prognosisand biological behaviour of a disease process [16]. Thereview highlighted the fact that a range of studies haveproven the feasibility of NLP for extracting structuredinformation from free text reports (e.g. [1721]). For sim-pler information extraction tasks, humanlike perform-ance of automated systems can be expected. For example,when evaluated for the extraction of American College ofRadiology utilisation review codes from radiology reports,M+, a system for medical text analysis, achieved recall,precision and specificity of 87, 85 and 98 % respectively[22]. These results were comparable to average recall, pre-cision and specificity recorded by physicians, namely 88,86 and 98 %. Comparably good results were achieved formore complex tasks such as translating radiology reportsinto a large database [18], where the Medical LanguageExtraction and Encoding (MedLEE) system achieved recallof 81 % and specificity of 99 % with a total of 24 clinicalconditions (diseases, abnormalities and clinical states)being the subject of the study. Again these results werecomparable to average recall (85 %87 %) and specificity(98 %) achieved by expert human coders.Typical processing steps taken in such NLP systemsinclude text segmentation into words, sentences, para-graphs and/or sections, partofspeech tagging, parsing,named entity recognition (NER), normalisation and neg-ation annotation [17, 23, 24]. Recognition of named en-tities, i.e. phrases that are used to differentiate betweenentities of the same semantic type (e.g. Osgood-Schlatterdisease is a name used to refer to a specific disease),followed by normalising the representation of theirmeaning (e.g. Osgood-Schlatter disease is also known asapophysitis of the tibial tubercle or OSD), is the crucialstep towards semantic interpretation of clinical narra-tives. In order to disambiguate named entities and assertrelationships between them (e.g. relate disease/disorder,sign/symptom or procedure to an anatomical site), do-mainspecific knowledge needs to be available in a ma-chinereadable form. For example, the domain knowledgeis specified in MedLEE using a table created manuallySpasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 2 of 26based on domain expertise [17]. Similarly, Medical TextAnalysis System (MedTAS) utilises external knowledge re-sources such as terminologies and ontologies [25]. Alterna-tively, M+ uses Bayesian Networks to represent semantictypes and relations within a specific medical domain suchas that of chest radiology reports [22]. Ideally, when a suit-able ontology is available it can be used to add an explicitsemantic layer over text data by linking domainspecificterms, i.e. textual representation of concepts, to theirdescriptions in the ontology [26]. This allows text to bemined for interpretable information about domainspecific concepts and their relationships.In our previous work, we developed TRAK (Taxonomyfor RehAbilitation of Knee conditions), an ontology thatformally models knowledge relevant for the rehabilitationof knee conditions [27]. This knowledge resource allowedus to implement an NLP system able to interpret kneere-lated clinical findings from MRI reports. In this paper, wedescribe KneeTex, an opensource, standalone applica-tion developed to address the task of information extrac-tion from narrative reports that describe an MRI scan ofthe knee. KneeTex is an ontologydriven, rulebased sys-tem. It takes an MRI report as an input and outputs thecorresponding clinical findings in the form of JavaScriptObject Notation (JSON) objects, a lightweight datainter-change format [28]. KneeTex not only extracts, but alsocodes the extracted information by mapping it onto theTRAK ontology. The resulting formally structured andcoded information allows complex searches to be con-ducted efficiently over the original MRI reports, therebyeffectively supporting epidemiologic studies of kneeconditions.MethodsSystem specificationInformation extraction (IE) is the task of automaticallyselecting specific facts about prespecified types of entitiesand relationships from freetext documents. In otherwords, the goal of IE is to convert free text into a struc-tured form by filling a template (a data structure with pre-defined slots) with the relevant information extracted (slotfillers) [29]. Figure 1 provides a graphical representation ofa template specific to our system, whose structure is illus-trated using Unified Modelling Language (UML) [30]. Thetemplate specifies the types of entities and relationshipswe aim to extract in this particular study.The goal of our system is to extract information aboutclinical observations made from MRI scans. Informationextracted about individual observation is structured intotwo major parts: finding and anatomy. Finding representsa clinical manifestation (e.g. injury, disease, etc.) observedby a radiologist. In other words, it corresponds to what isobserved. Information related to anatomy refers to aspecific part of human anatomy affected by the finding. Inother words, it corresponds to where the finding isobserved. Both finding and anatomy may have qualifiers,Fig. 1 Information extraction template represented by UML diagram. Each slot has got the following properties: extracted text, concept identifier,preferred concept name, start position of extracted text and its lengthSpasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 3 of 26which provide more specific information extracted aboutthem. In addition to general qualifiers, each finding isassociated with information about its certainty (as judgedby the radiologist) and negation (which specifies whetherthe finding is positive or negative). Tables 1 and 2 provideexamples of a filled template based on information ex-tracted automatically from the given sentences. The filledtemplate examples are represented using JSON.DataBetween January 2001 and May 2012, a total of 6,382individuals with an acute knee injury attended the AcuteKnee Screening Service at the Emergency Unit of theCardiff and Vale University Health Board (C&V UHB).A subset of 1,657 individuals fulfilled locally agreed clin-ical criteria for an MRI scan. Both the clinical assess-ment and MRI findings for these individuals were storedin a clinical database on a secure server within the C&VUHB. This database was originally developed for thepurposes of service evaluation and auditing practice.Out of 1,657 referred individuals, a total of 1,468 MRIscan visits were identified retrospectively from the data-base records. Following an MRI scan, the imaging resultswere summarised by a radiologist (from a team of five)in a diagnostic narrative report that conveys a specialistinterpretation of the MRI scan and relates it to the pa-tients signs and symptoms. These MRI reports formedthe dataset used in this study.All reports were anonymised by removing all identi-fiable information related to either patient or radiolo-gist together with the attendance date and the linksto the patients assessment and treatment details. Theanonymised reports were transferred to an encryptedmemory stick that was password protected and locked ina filing cabinet in a lockable room. Ethical approval forthis study was obtained from the South East WalesResearch Ethics Committee (10/MRE09/29).As part of their radiology reporting initiative whose aimis to improve reporting practices by creating a library ofclear and consistent report templates, the RadiologicalSociety of North America provides a template for kneeMRI reports [31]. However, the reports in our dataset didnot follow any such predefined structure. The structurevaried across the reports, but they generally tended to or-ganise information under the following headings: MRI OFTHE LEFT/RIGHT KNEE, INDICATION, HISTORY,FINDINGS and CONCLUSION. Within the reports,which were distributed in a plain text format, these sec-tions were indicated with upper case (see Table 3 for anexample).The size of the overall dataset was 1,002 KB with a totalof 13,991 sentences, 178,931 tokens, 3,277 distinct tokensTable 1 An example of a filled template. Original text source:There is a small undisplaced vertical radial tear of the posteriorhorn of the lateral meniscus.Table 2 An example of a filled template. Original text source:A peripheral tear involving the body of the lateral meniscusextending into the posterior third is seen.Table 3 Knee MRI report. A sample from the training datasetSpasi? et al. Journal of Biomedical Semantics  (2015) 6:34 Page 4 of 26and 2,681 distinct stems. On average, the size of an indi-vidual MRI report was 0.68 KB (±0.40 KB) with a total of9.53 (±5.13) sentences and 110.81 (±64.60) tokens.1 Weseparated the data into training and testing sets. A test setwas created by randomly selecting a subset of 100 MRIreports from the overall dataset. These reports were thenremoved from consideration so that the performance ofthe system could later be evaluated on unseen data. Theremaining 1,368 reports formed a training set, which wasused to inform system development.For training and testing purposes, the data were manu-ally annotated with labels that correspond to slots andtheir relationships from the IE template (see Fig. 1). Theannotation was performed using BRAT, a webbased toolfor text annotation [32]. The process involved annotatingtext spans with slot names (e.g. finding or anatomy) aswell as annotating dependencies between them (e.g. obser-ved_in). Figure 2 provides a visualisation of an annotatedexample. A total of 100 training documents and 100 test-ing documents were annotated independently by twoannotators.OntologyWe previously developed TRAK as an ontology that for-mally models knowledge relevant for the rehabilitationof knee conditions [27]. This information includes classi-fication of knee conditions, detailed knowledge aboutknee anatomy and an array of healthcare activities thatcan be used to diagnose and treat knee conditions.Therefore, TRAK provides a framework that can be usedto collect coded data in order to support epidemiologicstudies much in the way Read Codes, a coded thesaurusof clinical terms [33], are used to record observationaldata in the Clinical Practice Research Datalink (CPRD) formerly known as the General Practice Research Data-base (GPRD) [34]. TRAK follows design principles rec-ommended by the Open Biomedical Ontologies (OBO)Foundry and is implemented in OBO [35], a formatwidely used by this community. Its public release can beaccessed through BioPortal [36], a web portal that pro-vides a uniform mechanism to access biomedical ontol-ogies, where it can be browsed, searched and visualised.TRAK was initially developed with a specific task inmind  to formally define standard care for the rehabilita-tion of knee conditions. At the same time, it was designedto be extensible in order to support other tasks in thedomain. For example, the knowledge about knee anatomy,which is crossreferenced to a total of 205 concepts in theFoundational Model of Anatomy (FMA) [37], is directlyapplicable to interpretation of reports describing kneeMRI scans. However, in order to fully support semanticinterpretation of this type of clinical narratives, the TRAKontology needed to be expanded with other types ofMRIspecific concepts.Ontology expansionIn order to support semantic interpretation of the ter-minological content found in knee MRI reports, weneeded to ensure that all relevant concepts are modelledappropriately in the TRAK ontology. The main aspect ofthis task was the expansion of a specific domain modelledby the ontology, for example, MRIspecific observationssuch as hyaline cartilage abnormality, bone bruise, cyclopslesion, etc. In order to support NLP applications of theontology, its vocabulary also needed to be expanded toinclude term variants commonly used in MRI reports.Some term variants are confined to a specific clinicalsublanguage [18] and as such are typically underrepre-sented in standardised medical dictionaries such as thoseincluded in the Unified Medical Language System (UMLS)[38]. For example, collateral ligament was found to haveno other synonyms in the UMLS. Yet, collateral ligamentsare colloquially referred to as collaterals in clinical narra-RESEARCH Open AccessOntoStudyEdit: a new approach forontology-based representation andmanagement of metadata in clinical andepidemiological researchAlexandr Uciteli* and Heinrich Herre*AbstractBackground: The specification of metadata in clinical and epidemiological study projects absorbs significantexpense. The validity and quality of the collected data depend heavily on the precise and semantical correctrepresentation of their metadata.In various research organizations, which are planning and coordinating studies, the required metadata are specifieddifferently, depending on many conditions, e.g., on the used study management software. The latter does notalways meet the needs of a particular research organization, e.g., with respect to the relevant metadata attributesand structuring possibilities.Methods: The objective of the research, set forth in this paper, is the development of a new approach forontology-based representation and management of metadata. The basic features of this approach aredemonstrated by the software tool OntoStudyEdit (OSE). The OSE is designed and developed according to thethree ontology method. This method for developing software is based on the interactions of three different kindsof ontologies: a task ontology, a domain ontology and a top-level ontology.Results: The OSE can be easily adapted to different requirements, and it supports an ontologically foundedrepresentation and efficient management of metadata. The metadata specifications can by imported from varioussources; they can be edited with the OSE, and they can be exported in/to several formats, which are used, e.g., bydifferent study management software.Conclusions: Advantages of this approach are the adaptability of the OSE by integrating suitable domainontologies, the ontological specification of mappings between the import/export formats and the DO, thespecification of the study metadata in a uniform manner and its reuse in different research projects, and an intuitivedata entry for non-expert users.IntroductionThere is a large variety of particular clinical and epidemio-logical research projects, which typically produce a largeamount of data. The data stem from questionnaires, inter-views but also from specific findings and from laboratoryanalyses. Before these data can be collected, the neededmetadata must be precisely specified. The metadata in-clude, in the context of this paper: The description of all data elements/items (e.g.,questions of a questionnaire, measurements of aninvestigation) by particular attributes (e.g., questiontext, description of a measurement, unit of measure,data type, codelist); The description of a study structure, i.e., the groupingof the items and the description of the correspondinggroups by suitable attributes (e.g., title, commentary).These groups may be modules within an assessment,complete assessments (e.g., questionnaires, interviews,physical examinations, laboratory analyses of takenspecimen), or assessment groups (e.g., according to* Correspondence: alexander.uciteli@imise.uni-leipzig.de; heinrich.herre@imise.uni-leipzig.deInstitute for Medical Informatics, Statistics and Epidemiology (IMISE),University of Leipzig, Leipzig, Germany© 2015 Uciteli and Herre. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 DOI 10.1186/s13326-015-0042-0study cohorts or to particular data acquisition timepoints), as well as all items of the study.The specification of the metadata in particular researchorganizations must consider certain requirements, e.g.,which item attributes are relevant (e.g., name, label, range,data type, format, unit of measure), how the items shouldbe grouped (e.g., module, item group), or which studymanagement software or data entry tools (hereinafterreferred to as study software) are used (e.g., OpenClinica[1], ERT [2]).In this paper we present and discuss a new approachfor the ontology-based representation and managementof metadata in clinical and epidemiological research,which is demonstrated by the software tool OntoStudyE-dit (OSE). The OSE can easily be adapted to the needsof a particular research organization by the use of a suit-able domain ontology. Furthermore, it supports andprovides an ontology-based configuration of the im-port/export functions in the desired formats withoutthe necessity to change the source code. The import/export functions need only to be implemented oncefor a format type (e.g., xml, excel, sql, pdf ), and canbe configured by an ontology-based definition ofmappings between a format type and the domainontology. This approach has the advantage that thedomain experts (e.g., biometrician, data manager,study assistant) can specify the study metadata ac-cording to the common usage in a particular researchorganization by using the respective familiar termin-ology and without dealing with technical issues. Bythe provision of import from various sources and ex-port to several formats the differently specified meta-data can be represented on the same semantic basis;hence, the once specified metadata can be reused invarious research projects and utilized by differentstudy software (or other tools).MethodsOntology-based representation of metadataMetadata are used to describe data, hence, they addmore precise meaning to data, the semantics of whichremains often underspecified. Since the metadata itselfmust be specified by some formal representation, themeaning of which should be explained, we arrive at aninfinite regress, which must be brought to an end bysome basic principle. In our approach this infiniteregress is blocked by using the top-level ontology Gen-eral Formal Ontology (GFO) [3]. GFO provides the mostbasic layer for ontological foundation and represent awell-established semantic basis for modelling metadata.The generic method of reconstruction, or of modellingthe domain entities within the framework of a top-levelontology is called the method of ontological reduction.This method as well as the suitability of GFO for model-ling metadata were presented in [4].The OSE is a plug-in for Protégé-Frames [5], which isconceptually based on the notion of a frame. We decidedto use Protégé-Frames for our implementation because itsupports the generation of data acquisition forms. Theforms can easily be adapted, i.e., the slot widgets (inputfields like text field, text area, checkbox, combobox) canbe selected and arranged in any given layout. Furthermore,the specification of the slot facets (e.g., cardinality, mini-mum, maximum, default values) allows to elegantly con-trol the quality of the user input (e.g., alert on missingdata, prevent false data entries). In summary, the dataacquisition forms of Protégé-Frames permit an intuitivedata entry for non-expert users, in comparison, e.g., to theOWL version of Protégé.Frames are formal representational structures, whichare exhaustively classified into the following types: clas-ses, slots, facets and individuals [6]. Together withaxioms they form the building blocks for Protégé-Frames ontologies. Classes represent concepts related toa domain. Slots represent properties or attributes of clas-ses, whereas facets describe properties of slots. Slotsmay be attached to frames, and then they describe prop-erties of that frame. A slot, attached to a frame, can havevalues, which might be constraint by facets. A slot canbe attached to a frame as a template slot or as an ownslot [6].The concepts, represented by Protégé classes, are asso-ciated in GFO to categories, and the slots attached to aclass frame describe properties of that class. A categoryis defined in GFO as an entity being independent of timeand space, that can be instantiated. A category is repre-sented by some symbolic structure, which denotes ameaning, also called intension. The notion of a class - asused, for example, in UML [7], OWL [8], or Frames -captures relevant aspects of categories. Subsequently, weuse the term class in the sense of a symbolic represen-tation of a category and the term slot - as a symbolicrepresentation of a property or a relation. A meta-classin Protégé-Frames corresponds in GFO to a category theinstances of which are themselves categories. In Protégé-Frames each class is an instance of a Standard-Meta-Class. In GFO there exists a meta-category, denoted byCategory(2), the instances of which are all categories offirst order. A category is of first order if all of itsinstances are individuals. The meta-classes in Protégécorrespond to the second-order categories in GFO, whichare extensional subcategories of the category Category(2).The three ontology methodThe OSE is designed and developed according to thethree ontology method [9]. This method for developingsoftware is based on the interactions of three differentUciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 2 of 9kinds of ontologies: a task ontology (TO), a domainontology (DO) and a top-level ontology (TLO). The TOis an ontology for the general problem that the softwareis intended to solve. The DO provides the domain-specific knowledge, whereas the TLO integrates the TOand the DO and is used as the foundation of them. TheTLO also provides means for integrating data fromdifferent domains. For integrating the TO and the DO weuse the TLO GFO because it is sufficiently expressive, inparticular, it contains an ontology of categories (thatallows categories of higher order), as well as an ontologyof properties and attributives [4].ResultsThe ontological architecture of the OSEThe ontological architecture of the OSE is representedby systems of categories of several levels of abstractionand relations between them (Fig. 1). The TO is an upperontology with respect to the considered DOs, hence, theDO categories are extensional subcategories of the TOcategories. The GFO is used as semantic foundation forboth TO and DO by classifying categories of TO andDO under particular GFO categories (e.g., category,process, individual).We use classes for the representation of categories andslots for the representation of properties and relations inProtégé-Frames ontologies.Task and domain ontology of the OSEIn this section we consider the classes which representcategories of the TO and DO (Fig. 2).The names of TO classes starts with the underline-sign. The TO includes at the upmost level followingclasses: _CONFIG, _ELEMENT, and _ANNOTATING_ONTOLOGY_ROOT. The annotating ontologies areinserted below the node _ANNOTATING_ONTOLO-GY_ROOT (see Annotating ontologies and annotation).The class _ELEMENT, its subclasses and instances arevisible for the user. The instances can be edited by theuser by means of a graphical user interface (GUI). Onthe other hand, the class _CONFIG, and its sub-classes and instances are hidden from the user; theseclasses and its instances are used by the OSE in thebackground.The subclasses of _ELEMENT are: _CONSTANT, _OB-JECT, _GROUP, _ANNOTATION, and _EXPORT. Theinstances of _CONSTANT are used in expressions, theclass _OBJECT represents individual entities (e.g., itemsor measurement units), whereas the class _GROUP standsfor lists, which might contain further elements (instancesof _ELEMENT). The instances of _ANNOTATION areannotations of elements by concepts of the annotatingontology (see Annotating ontologies and annotation). Thesubclasses of _EXPORT represent export formats pro-vided for the export function.The following subclasses of _CONFIG are introduced:_FORMAT_MAPPING, _ANNOTATION_TYPE, the formerof which is described in more detail in section Ontologicalspecification of mappings between import/export formats anddomain ontology and the latter  in section Annotating ontol-ogies and annotation. In particular, as instances of _ANNO-TATION_TYPE various annotation types can be defined. Theclass _FORMAT_MAPPING and its subclasses are used tospecify ontologically various import and export formats.The class _MATH and its subclasses are used by theexpression editor, displayed in the working area E (see Usageof the OSE). The class _MATH_EXPRESSION_RELATORand its subclasses describe various mathematical operationsand functions (for example: AND, >, +). These relatorspossess arguments, being numbers (_NUMBER), constants(_CONSTANT), particular study elements (_STUDY_ELE-MENT, e.g., Item), or further relators. With these means theeditor is able to build an expression in form of a tree. The TOincludes a number of slots, the most important of which arethe following: _contains and _HIERARCHY_SUBCLASS.The _contains slot represents the relation between instances(e.g., Page: B1 _contains Module: Socio-demographic data),whereas the slot _HIERARCHY_SUBCLASS describes thecorresponding basic relation between classes (example: Pagehas _HIERARCHY_SUBCLASS Module). The relation_HIERARCHY_SUBCLASS is formally defined as follows:Cat_1 _HIERARCHY_SUBCLASS Cat_2 :=(? x y) (x :: Cat_1 ? _contains(x,y)? y :: Cat_2).A further constituent of the ontological architecture isthe DO. This ontology is embedded into the TO, hence,these classes are subclasses of TO classes. We developedan example DO, based on the structure of clinical trialswhich are carried out at the Clinical Trial Centre LeipzigFig. 1 Ontological architectureUciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 3 of 9[10]. Classes like Study or Module are placed below theclass _GROUP, whereas Item or Unit_of_Measure aresubclasses of _OBJECT. The slots of these classes canfreely be defined. The class Item, for example, can pos-sess following slots: name, description, unit_of_measure,range, codelist or rules.Ontological specification of mappings between import/export formats and domain ontologyIn this section we outline the specification of mappingsbetween import/export formats and DO for the exampleof an xml-based format, CDISC ODM [11]. For themapping specification the subclasses of _FORMAT_-MAPPING are used.The tag structure of an xml-based format can beconsidered as a tree. As a first step the root tag (in ourexample, <ODM>) must be specified as instance of_ROOT_TAG. The other tags are defined as instancesof _TAG. For each tag its sub-tags and attributes mustbe specified by the instance editor (Fig. 3). The tag<Study> contains, e.g., the sub-tags <GlobalVariables>,<BasicDefinitions>, and <MetaDataVersion> as well asthe attribute OID. The tags are mapped to the DOclasses, whereas their texts and attributes - to the DOslots. In our example, the tag <Study> is mapped to theclass Study and the attribute OID - to the slot :NAMEof the class Study.The whole tag tree must be specified only once; this isdone by declaration of the corresponding sub-tags andattributes and by the specification of its mapping to theclasses resp. slots of the DO. Then, this mapping can beused for the import/export of various metadataspecifications.Annotating ontologies and annotationBy using Protégé it is possible to import an ontologyinto another one. We could, e.g., import the ACGTMaster Ontology [12], phenotype or property ontol-ogies, or LOINC [13] and may use their categoriesfor the annotation of the instances of the DO (e.g.,concrete item instances like DYSPNEA_AT_REST).Ontologies, intended to be used to annotate instancesof a DO within the OSE, are called in this paper an-notating ontologies.For this purpose, we introduced the following classesinto the TO:Fig. 2 Task and domain ontologyUciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 4 of 9 _ANNOTATION_TYPE (subclass of _CONFIG).Within DO we may define various annotationtypes, being instances of _ANNOTATION_TYPE(e.g., annotated_with, risk_factor_of, symptom_of ). _ANNOTATION (subclass of _OBJECT). This classhas three slots: _annotated_elements,_annotation_type und _annotating_concepts. UsingOSE we may create concrete annotations asinstances of _ANNOTATION. This is realized byselecting the elements to be annotated, taken fromsubclasses of _ELEMENT, by choosing anannotation type from the instances of_ANNOTATION_TYPE, and by selecting suitableclasses taken from the annotating ontology. _ANNOTATING_ONTOLOGY_ROOT. Below thisclass annotating ontologies can be inserted. _ANNOTATING_ONTOLOGY_METACLASS(subclass of :STANDARD-CLASS). This is a meta-class, containing all classes of the annotating ontologyas instances. This class has an additional slot, denotedby _annotations, which is defined as the inverse slotof _annotating_concepts.We may not only annotate single instances, but alsosets of instances. It is, e.g., not sufficient to annotate theitem ITEM:DYSPNEA_AT_REST as a symptom ofCongestive heart failure, taken from the HumanPhenotype Ontology (HPO) [14]. This item is associatedwith a codelist that includes two possible values YESor NO (depending on whether a patient has dyspneaor not). Only if YES is selected as answer to the questionwhether dyspnea at rest holds, this symptom is true. I.e.,only the combination of the item ITEM:DYSPNEA_AT_R-EST and the answer option YES can be annotated (Fig. 4).In this way both conditions are AND-connected, express-ing that the question was answered and that the answer isyes. If an item does not possess a codelist, then alsoranges can be annotated. An example is the annotation ofthe item ITEM:SYSTOLIC_BLOOD_PRESURE togetherwith Range [121;] (i.e., >= 121) with the concept Elevatedsystolic blood pressure from HPO (Fig. 4).By use of the inverse slot _annotations for classesof the annotating ontology we may realize the fol-lowing valuable feature: for a given concept all of itsannotations can be displayed. This functionality canbe very important in searching for items in certaindomains during the planning phase of a study. If weare planning, e.g., a study for heart failure we mayask for all annotations of the class Congestive heartfailure (and possibly of its subclasses). These anno-tations will then be displayed. In this way one has aquick access to items which can be used to querythe symptoms or risk factors of the heart failure(Fig. 5).The annotation of different instances of the domainontology (even of different domain ontologies) by theFig. 3 ODM format mappingUciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 5 of 9same class of the same annotating ontology establishes asemantic connection between these instances. Suchannotations allow the semantic search for items over thecategories, which are used in these annotations, but theyalso allow the comparability of data which are acquiredfor various distinct studies.Usage of the OSESubsequently, we sketch the graphical user interface(GUI) and the main functions of the OSE: specification,management, import and export of metadata, searchingand navigation.The GUI is partitioned into five working regions A, B,C, D and E (Fig. 6):A: Study Elements. Within this region all study elements(being subclasses of _ELEMENT) are represented anddisplayed. Besides a class, the number of its instances isshown (put in brackets). For a selected class its instancesare displayed in the working field C (Instance Browser).Furthermore, a searching field is available. For the exportof the specified metadata an export format must beselected (a subclass of _EXPORT) and the button exppressed. It is also possible to export a metadataspecification as an ontology that can be used as a CaseReport Form (CRF) preview (Fig. 7).B: Study Hierarchy. This hierarchy shows the structureof a study. This hierarchy is formed by instances whichare connected by the contains-relation. The user maycreate new elements in a group, may change the ele-ments order, and may remove elements from a group.By choosing an element, a form for the acquisition ofits slot values is displayed in the working area D(Instance Editor). Furthermore, a search field isavailable.C: Instance Browser. The instance browser showsinstances of the class which is selected in the workingarea A. By choosing an instance, a form for capturingFig. 4 Annotation instance (examples)Fig. 5 Concept annotations (example)Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 6 of 9its slot values will be shown in the working area D(Instance Editor). Instances may be deleted.Furthermore, it is possible to associate instances fromthis working area to groups from the working area B bydrag-and-drop. A search field is available.D: Instance Editor. The instance editor provides formsfor capturing the slot values of instances.E: Expression Editor. This editor supports the editing offormulas, being represented in form of trees. Variousoperators and numbers can be used, for examplearithmetical, logical operators, and other relations;furthermore, study elements (for example items) andconstants can be referenced.The functionalities of the working areas A: Study Ele-ments, B: Study Hierarchy and E: Expression Editor wereimplemented in the OSE plug-in, whereas the function-alities of the working areas C: Instance Browser and D:Instance Editor are already provided by Protégé asstandard features. The working area A was imple-mented in such a way that only the subtree of_ELEMENT is displayed for the user, whereas thesubtree of _CONFIG remains hidden. In this way wemay assure that the user of OSE (e.g., data manager,researcher, study assistant) cannot change the config-uration that was specified, e.g., by ontologists or IT-specialists. Furthermore, we implemented for thisworking area the import/export of the metadata spe-cification. The working area B displays the study hier-archy as a tree which connects the instances (i.e., thestudy elements, as for example, module or item) bythe contains-relation, whereas the standard trees ofProtégé represent class hierarchies, based on the is-arelation. Moreover, we implemented all functions,needed for manipulating the study structure. Add-itionally, the expression editor (E) was implemented;this editor provides all needed functions for designingand managing of formulas. We increased the usabilityFig. 6 GUI of the OSE. a Study Elements; b Study Hierarchy; c Instance Browser; d Instance Editor; e Expression EditorUciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 7 of 9of the tool by adapting the automatically generateddata acquisition forms (i.e., the slot widgets werereplaced and the form layout was changed).Related workThere are few systems that pursue similar purposes asOSE, notably TIM (Trial Item Manager) [15] andObTiMA (Ontology-based Managing of Clinical Trials)[16], which are subsequently considered in more detail.ObTiMa is a system for ontology-based management ofclinical trials, which is composed of the two components:the Trial Builder for designing clinical trials and thePatient Data Management System for handling patientdata within a trial. Trial Builder allows the creation ofCRF items, based on the concepts of an ontology (ACGTMaster Ontology). The main difference between theObTiMas Trial Builder and the OSE consists in that theTrial Builder is based on ODM and the possible itemattributes (e.g., question, data type, measurement unit) arefixed and cannot be changed or extended, whereas in OSEitem attributes are defined by domain ontologies andcan be flexibly handled. Hence, OSE may take intoaccount the needs of the diverse research organizations,which usually differs with respect to the practiced specifi-cation of metadata that typically use different metadatatypes (e.g., items, codelists, modules), different attributes,groupings, and hierarchical levels (e.g., study-event-module-item). Furthermore, the flexible, ontology-baseddevelopment of mappings between the ontologies of OSEand diverse import and export formats enables the reuseof specified metadata in various research projects andtheir utilization by different study software.The TIM pursues aims, analogous to OSE, namely, tosupport the specification of items in clinical trials.Similarly as OSE, TIM is based on a semantic modelconsisting of a fixed component (the meta-model andthe core types of the data model), and a flexible module(domain-specific types of the data model). This structureFig. 7 Case report form preview (example)Uciteli and Herre Journal of Biomedical Semantics  (2015) 6:41 Page 8 of 9supports the adaption to user-specific needs. Though,there are differences between TIM and OSE. In TIM thefixed component and the flexible part are not clearlyseparated, whereas in OSE both components (TO andDO) are explicitly divided and endowed with anontologically-based semantic basis. Consequently, OSEexhibits a higher flexibility with respect to the change andadaption of the domain-specific constituents. Further-more, OSE provides various additional functionalities,among them, the ontologically-based creation of formatmappings, and the use of rule expressions. Finally, theusage of Protégé-Frames supports the adaption of the dataacquisition forms, and allows for an extension of thesoftware by additional plug-ins.Conclusions and future workIn this paper we presented and discussed a new ap-proach for ontology-based representation and manage-ment of metadata in clinical and epidemiologicalresearch using the software tool OntoStudyEdit (OSE).Advantages of this approach are: 1. the adaptability ofthe OSE to intended aims and given needs by integratingsuitable domain ontologies in a modular way; 2. theontological specification of mappings between the im-port/export formats and the DO, such that no changesof the source code are needed by the replacement of theDO; 3. the specification of the study metadata in auniform manner and reuse of which in different researchprojects; 4. an intuitive data entry for non-expert users.The OntoStudyEdit is a tab widget plug-in forProtégé-Frames; this implies that all functionalities ofProtégé can be used. Of particular interest is theadaption of the data acquisition forms. At present, weare working on the implementation of further import/export functions, e.g., related to annotated CRF in PDFformat and to specifications for the import in differentstudy software.There is ongoing evaluation of the OSE whichstarted already some time ago. At first, metadata ofthe LIFE study are entered with the OSE. For LIFEwe developed an ontology, called LIFE InvestigationOntology (LIO), and a Protégé-Frames based tool,called query generator [17]. The metadata part of LIOwas integrated into OSE as a domain ontology. LIOis a frame ontology and its use as domain ontology inOSE preserves the structure of the LIFE metadata.For this reason it is rather simple to input and tomanage the LIFE metadata by using the OSE. Wealready experienced that the non-expert users (e.g.,data manager, researcher, study assistants) are able tocope well with OSE, as well as with the LIFE querygenerator, that is productively used since two years bythe same user group.Competing interestsThe authors declare that they have no competing interests.AcknowledgementsThis paper has been presented at ODLS 2014 (Ontologies and Data in LifeSciences) in Freiburg.We acknowledge support from the German Research Foundation (DFG) andUniversität Leipzig within the program of Open Access Publishing.Received: 24 March 2015 Accepted: 15 December 2015DATABASE Open AccessColil: a database and search service forcitation contexts in the life sciences domainToyofumi Fujiwara1 and Yasunori Yamamoto2*AbstractBackground: To promote research activities in a particular research area, it is important to efficiently identifycurrent research trends, advances, and issues in that area. Although review papers in the research area can sufficefor this purpose in general, researchers are not necessarily able to obtain these papers from research aspects oftheir interests at the time they are required. Therefore, the utilization of the citation contexts of papers in a researcharea has been considered as another approach. However, there are few search services to retrieve citation contextsin the life sciences domain; furthermore, efficiently obtaining citation contexts is becoming difficult due to the largevolume and rapid growth of life sciences papers.Results: Here, we introduce the Colil (Comments on Literature in Literature) database to store citation contexts inthe life sciences domain. By using the Resource Description Framework (RDF) and a newly compiled vocabulary, webuilt the Colil database and made it available through the SPARQL endpoint. In addition, we developed a web-based search service called Colil that searches for a cited paper in the Colil database and then returns a list ofcitation contexts for it along with papers relevant to it based on co-citations. The citation contexts in the Colildatabase were extracted from full-text papers of the PubMed Central Open Access Subset (PMC-OAS), whichincludes 545,147 papers indexed in PubMed. These papers are distributed across 3,171 journals and cite 5,136,741unique papers that correspond to approximately 25 % of total PubMed entries.Conclusions: By utilizing Colil, researchers can easily refer to a set of citation contexts and relevant papers basedon co-citations for a target paper. Colil helps researchers to comprehend life sciences papers in a research areamore efficiently and makes their biological research more efficient.Keywords: Life sciences paper, Citation, Citation context, Co-citation, PMC Open Access Subset, RDF, SPARQLBackgroundThe ability to efficiently identify current research trends,advances, and issues in a research area is highly import-ant to researchers to promote their research activities.For example, in cases of international collaborative re-search, researchers often need to read relevant papersand summarize the current knowledge about the re-search area beyond their own research fields [1]. Al-though review papers in the research area can suffice forthis purpose in general, researchers are not necessarilyable to obtain these papers at the right time from view-points of their interests. In addition, these papers reflectonly previously published papers at the time of writing areview paper and viewpoints of its authors. To comple-ment this issue, the utilization of the citation contextsfor papers in a research area has been considered asanother approach [2]. However, there are few searchservices to retrieve citation contexts in the life sciencesdomain, and to efficiently obtain the citation contextsfor a target paper is becoming difficult due to the largevolume and rapid growth of life sciences papers. Here, weintroduce the Colil (Comments on Literature in Literature)database and a web-based search service called Colil forcitation contexts in the life sciences domain.In life sciences papers, citations are widely used and typ-JOURNAL OFBIOMEDICAL SEMANTICSvan Dam et al. Journal of Biomedical Semantics  (2015) 6:39 DOI 10.1186/s13326-015-0038-9SOFTWARE Open AccessRDF2Graph a tool to recover, understandand validate the ontology of an RDF resourceJesse CJ van Dam1*, Jasper J Koehorst1, Peter J Schaap1, Vitor AP Martins dos Santos1,2and Maria Suarez-Diez1AbstractBackground: Semantic web technologies have a tremendous potential for the integration of heterogeneous datasets. Therefore, an increasing number of widely used biological resources are becoming available in the RDF datamodel. There are however, no tools available that provide structural overviews of these resources. Such structuraloverviews are essential to efficiently query these resources and to assess their structural integrity and design, therebystrengthening their use and potential.Results: Here we present RDF2Graph, a tool that automatically recovers the structure of an RDF resource. Thegenerated overview allows to create complex queries on these resources and to structurally validate newly createdresources.Conclusion: RDF2Graph facilitates the creation of complex queries thereby enabling access to knowledge storedacross multiple RDF resources. RDF2Graph facilitates creation of high quality resources and resource descriptions,which in turn increases usability of the semantic web technologies.Keywords: Data integration, RDF, Databases, Semantic web, Visualization, SPARQL, OWL, OntologyBackgroundIn the life sciences, high-throughput technologies deliverever-growing amounts of heterogeneous (meta) data atdifferent scales, which are produced, stored and analysedin both structured and semi-structured formats. SystemsBiology is an integrative discipline that uses various inte-gration strategies to model and discover properties of bio-logical systems. Integration and analysis of heterogeneousbiological data and knowledge require efficient informa-tion retrieval and management systems and SemanticWeb technologies are designed to meet this challenge [1].The RDF data model is a mature W3C standard [2, 3]designed for the integrated representation of heteroge-neous information from disparate sources and it is provingeffective for creating and sharing biological data. RDFis not a data format, but a data model for describingresources in the form of self-descriptive subject, predi-cate and object triples that can be linked in an RDF-graph.*Correspondence: jesse.vandam@wur.nl1Laboratory of Systems and Synthetic Biology, Wageningen University,Dreijenplein 10, 6703 HB Wageningen, The NetherlandsFull list of author information is available at the end of the articleIntegration of heterogeneous data from different sourcesin a single graph relies on using retrievable controlledvocabularies, which is essential to access and analyse inte-grated data [4]. Once data sources are converted into thesemantic Web, SPARQL [5, 6] can be used to query mul-tiple of these resources, simultaneously or consecutively,without further modifying any of them.Widely used biological resources such as Reactome,ChEBI and UniProt, among others, [710] have beentransformed into the RDF data model and the Bio2RDF[11] project has transformed a large set of additionalsources, such as the NCBI GenBank files [12], DrugBank[13] and InterPro database [14]. Additionally, there areon-going efforts to develop tools providing results in thisdata model, such as the Semantic Annotation Platformfor Prokaryotes, SAPP, (J. Koehorst, J. van Dam et al. per-sonal communication) that provides genome functionalannotation in the RDF data model.These RDF resources can be readily queried withSPARQL. Constructing SPARQL queries requires that theuser has a mental representation of the underlying struc-ture of the resource. The structure of a resource is the© 2015 van Dam et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.van Dam et al. Journal of Biomedical Semantics  (2015) 6:39 Page 2 of 12set of object types and their relationships, i.e. the explicitrepresentation of the predicates linking different classes.This structure represents the set of semantic constraintsembedded in the resource. In a biological database con-taining information on biochemical reactions, genes andtheir identifiers are linked to proteins; proteins are linkedto EC numbers; EC numbers are connected to reactionsthat involve metabolites as products and substrates. Toretrieve information pertainingmetabolites and genes, theSPARQL query has to obey the specific network topologylinking these types of objects. RDF data sources do notneed a predefined scheme so that new data types can beadded at any time expanding the underlying structure. Ifthemodifications in the underlying structure generated bythis new data are not known, linked information cannot beretrieved. Not having a clear idea of the underlying struc-ture makes querying an RDF resource inefficient, timeconsuming, or even impossible.The structure of a resource can be either retrievedthrough manual queries or it can be provided by the datapublishers in the documentation of the resource. Thisstructural information can be encoded using Web Ontol-ogy Language (OWL) files [15]. OWL was created as adescription logic language and it is intended for automaticreasoning; nevertheless, its axioms can also be used toconstruct a graphical overview of the described resource[16]. However, the OWL standard does not require allaxioms necessary for such reconstruction. Examples ofnecessary axioms not obligated by this standard are objectall values from and data all values from. In some of theresources created by the Bio2RDF project these axioms(object all values from and data all values from) are notprovided. Furthermore, the ontology generation processis, at best, semi-automatic, time consuming and error-prone. Errorsmight also accumulate due to the conversioncode used to generate the RDF resource, as the triplegenerating code can contain lexical errors in predicatedefinition such as typos, inconsistent usage of upper andlower case, or misspelled words, thereby populating aresource containing information on proteins with infor-mation on porteins, which describes proteins associatedto transmembrane transport. These errors lead to descrip-tions of the intended content of the resource rather thanof its actual content.Shape Expressions (ShEx) is a standard to describe, val-idate and transform RDF data. One of the goals of thisstandard is to create an easy to read language for the vali-dation of instance data, however, it is still being developedand no final recommendation is yet available [1719].Computational tools able to reconstruct the structure ofRDF resources are thus required to i) facilitate query writ-ing and to ii) enable data providers to verify the structuralintegrity of their resource. To our knowledge, no such tool,able to automatically recover the structure of the resourceand the associated multiplicity of the predicates, exist.Semscape [20] is an already existing Cytoscape [21] appthat is able to retrieve to some extent the structure of theresource. However, it has limited recovery and simplifica-tion capabilities, leading to unreadable hairballs for largerstructures. Furthermore, additional statistical informationabout the classes and links is not retained. Here we presentRDF2Graph, a tool to automatically recover the structureof an RDF resource and to generate a visualization, ShExfile and/or an OWL ontology thereof. These can be usedto write SPARQL queries or to verify (generated) RDFresources.ImplementationRDF2Graph performs two distinct processes to retrievethe structure of a resource. Initially, there is a recovery ofall classes, predicates and unique type links together withtheir associated statistics. In the second stage there is asimplification step to arrive to a neat structural overview.A simplified overview of the complete process is given inFig. 1.A type link is defined as a link joining a subjectclass type to an object class or value data type, via apredicate. A unique type link is defined as a uniquetuple: type of subject, predicate, (data)type of object. Forthe triple <:BRCA1, :locatedOn, :chromosome17> thetype link is <:Gene, :locatedOn, :Chromosome>. Whenconsidering the full resource, all type links <:Gene, :locate-dOn, :Chromosome> correspond to the same unique typelink. In the triple <:Adam :hasSon :Bob> the type link is<:Person, :hasSon, :Person>.Themultiplicity of a unique type link describes the num-ber of instances connected to each other. The forwardmultiplicity can be: i) One-to-one (also denoted: 1..1) eachsource instance has exactly one reference to the target;ii) One-or-many (1..N) each source instance has one orJOURNAL OFBIOMEDICAL SEMANTICSLin et al. Journal of Biomedical Semantics  (2015) 6:37 DOI 10.1186/s13326-015-0036-yRESEARCH Open AccessOntology-based representation andanalysis of host-Brucella interactionsYu Lin, Zuoshuang Xiang and Yongqun He*AbstractBackground: Biomedical ontologies are representations of classes of entities in the biomedical domain and howthese classes are related in computer- and human-interpretable formats. Ontologies support data standardizationand exchange and provide a basis for computer-assisted automated reasoning. IDOBRU is an ontology in thedomain of Brucella and brucellosis. Brucella is a Gram-negative intracellular bacterium that causes brucellosis, themost common zoonotic disease in the world. In this study, IDOBRU is used as a platform to model and analyzehow the hosts, especially host macrophages, interact with virulent Brucella strains or live attenuated Brucellavaccine strains. Such a study allows us to better integrate and understand intricate Brucella pathogenesis andhost immunity mechanisms.Results: Different levels of host-Brucella interactions based on different host cell types and Brucella strains werefirst defined ontologically. Three important processes of virulent Brucella interacting with host macrophages wererepresented: Brucella entry into macrophage, intracellular trafficking, and intracellular replication. Two Brucellapathogenesis mechanisms were ontologically represented: Brucella Type IV secretion system that supports intracellulartrafficking and replication, and Brucella erythritol metabolism that participates in Brucella intracellular survival andpathogenesis. The host cell death pathway is critical to the outcome of host-Brucella interactions. For better survivaland replication, virulent Brucella prevents macrophage cell death. However, live attenuated B. abortus vaccine strainRB51 induces caspase-2-mediated proinflammatory cell death. Brucella-associated cell death processes are representedin IDOBRU. The gene and protein information of 432 manually annotated Brucella virulence factors were representedusing the Ontology of Genes and Genomes (OGG) and Protein Ontology (PRO), respectively. Seven inference ruleswere defined to capture the knowledge of host-Brucella interactions and implemented in IDOBRU. Current IDOBRUincludes 3611 ontology terms. SPARQL queries identified many results that are critical to the host-Brucella interactions.For example, out of 269 protein virulence factors related to macrophage-Brucella interactions, 81 are critical to Brucellaintracellular replication inside macrophages. A SPARQL query also identified 11 biological processes important forBrucella virulence.Conclusions: To systematically represent and analyze fundamental host-pathogen interaction mechanisms, weprovided for the first time comprehensive ontological modeling of host-pathogen interactions using Brucella asthe pathogen model. The methods and ontology representations used in our study are generic and can bebroadened to study the interactions between hosts and other pathogens.* Correspondence: yongqunh@med.umich.eduUnit of Laboratory Animal Medicine, Department of Microbiology andImmunology, Center for Computational Medicine and Bioinformatics, andComprehensive Cancer Center, University of Michigan Medical School, 1150 W.Medical Center Dr, Ann Arbor, MI 48109, USA© 2015 Lin et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 2 of 18BackgroundIn the field of infectious diseases, the study of the inter-active relationships between pathogens and their hosts iscritically important. An infectious disease is the result ofintense interactions between a pathogen and its host. Dur-ing these interactions both the host and the pathogen at-tempt to manipulate each other using a complex networkmechanism to maximize their respective survival probabil-ities. For the goal of survival and replication, the pathogenmay adapt different pathogenesis strategies to infect thehost. On the other hand, the host may apply innate andadaptive immune defense mechanisms to fight against theinvading pathogen. Different live attenuated vaccines maystimulate sufficient host immunity but does not inducedamaging effects on the host. For decades, scientists haveconducted research to study the different aspects of host-pathogen interactions. In order to obtain a full picture ofthe host-pathogen interaction mechanisms, separate datafrom those studies needs to be integrated. Thus, a strategyof knowledge representation, management and reasoningbased on the huge data resources is in need. Such a strat-egy will enable the knowledge integration, complicatedbiological data analysis, and provide insights for biologiststo generate new hypotheses.Brucella is a Gram-negative, non-spore-forming, facul-tative, intracellular bacterium that causes chronic zoonoticbrucellosis in humans and a variety of animal species [1].Human brucellosis remains the most common zoonoticdisease worldwide, with more than 500,000 new humancases reported annually [2]. A safe and effective humanvaccine is required but does not yet exist. A rational vac-cine design would benefit from insightful understandingof the interactions between host and Brucella, specifically,Brucella pathogenesis and host defense mechanisms. Bru-cella infections are typically chronic in nature [1], suggest-ing a continuous interaction between host and Brucella.To promote its long-term intracellular survival, Brucellaminimizes the activation of host inflammatory mecha-nisms. For example, Brucella lipopolysaccharide (LPS) has100- to 1000-fold decreased capability to activate pro-inflammatory TNF-? and IL-1 cytokines compared tosimilar concentrations of E. coli LPS [3]. Hundreds of Bru-cella protein virulence factors participate in the Brucellapathogenesis and interacting with host immune systems[4, 5]. The immune systems in various Brucella hosts re-spond poorly against virulent Brucella strains but verywell against live attenuated Brucella vaccine strains [68].Such a complex host-pathogen interaction system involvesa number of cells, molecules and biological processes.Therefore, the host-Brucella interactions present a goodexample for an ontology-based exploration of complexbacterial pathogenesis and host immunity mechanisms.As an extension ontology of the Infectious Disease Ontol-ogy (IDO) [9], Brucellosis Ontology (IDOBRU) is developedpreviously at our lab [10]. Following the good practice ofthe OBO Foundry principles [11], IDOBRU was developedunder the framework of the Basic Formal Ontology (BFO)[12] and IDO. BFO contains two branches, continuant andoccurrent [11, 13]. The continuant branch represents time-independent entity such as material entity, and the occur-rent branch represents time-related entity such as process.By aligning different domain ontologies under the twobranches of BFO, the knowledge from broad biologicalareas could be captured and organized under a compre-hensive ontology-level structure. Since IDO aligns withBFO, IDOBRU automatically aligns with BFO. IDOBRUexemplifies IDO in the case of brucellosis, which covers abroad range of topics, including host infection, zoonoticdisease transmission, symptoms, virulence factors, patho-genesis, diagnosis, intentional release, vaccine prevention,and treatment [10].Since the study of host-Brucella interactions has been amajor brucellosis research effort, this paper goes beyondthe simplified introduction of virulence factors and patho-genesis in the previous IDOBRU paper [10], and providesmore detailed ontological representation on various as-pects of the host-pathogen interactions. In this report,different types of the pathogen-side pathogenesis mecha-nisms and host-side immune defense strategies are de-scribed with specific examples. Comparing with the 245virulence factors in the original IDOBRU ontology [10],current version includes 432 virulence factors in Brucella.Furthermore, a list of computer-understandable logic in-ference rules is defined in this study to make the virulencefactors, host-Brucella interactions and related processescomputable. Use cases are provided to demonstrate howsuch ontological representations and inference rule-basedautomated reasoning help data integration and query inthe area of host-pathogen/vaccine interactions.ResultsIn what follows, italics are used to make ontological ax-ioms (i.e., statements that say what is true in the domain)(http://www.w3.org/TR/owl2-syntax/), simple bold wordsrepresent ontology relations, single quotes are used to rep-resent ontology terms, and double quotes are used for textdefinitions or emphases.Overall design of ontological representing ofhost-Brucella interactionVarious disciplines dissect interactions into different tai-lored meanings. The definition of an interaction in thehost-pathogen interaction area includes a two-way effect(i.e., hosts effect upon pathogen, and pathogens effectupon host). The interactions between host (e.g., human)and Brucella exemplifies a host-pathogen interaction inthe context of biology. As an intracellular bacterium, Bru-cella strains are able to invade, survive and replicate forLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 3 of 18prolonged periods within in vivo host cells or in vitro cul-tured host cells. The GO term interspecies interaction be-tween organisms (GO_0044419) is defined as any processin which an organism has an effect on an organism of adifferent species. Without capturing the granularity at thehost cell level, this GO term is not sufficient for our case.The definition of host-Brucella interaction in IDOBRU isan interspecies interaction that is the physical encounterof two parties: Brucella and its host organism or host cell.While interacting, Brucella and its host organism or hostcell have effects upon each other. The interaction in thisdefinition specifically emphasizes the physical encounterand interaction of the two parties in an action. Also, giventhat Brucella is an intracellular bacterium, it is importantto explicitly mention both of the host organism and thehost cell.The host-Brucella interaction includes three mainparts: 1) Brucella entry into host cell (IDO_0101170)(the interaction at the interface between Brucella andhost), 2) process of establishing Brucella infection inhost (IDO_0100426) (Brucella side response), and 3)host anti-Brucella process (IDO_0100115) (host sideresponse). The IDOBRU term Brucella entry into hostcell is a child term of GO entry of bacterium into hostcell (GO_0035635). The IDOBRU term process of es-tablishing Brucella infection in host is a child term ofthe IDO-core term process of establishing an infection(IDO_0000603), which is under GO biological process.The host anti-Brucella process is a child term of GObiological process.As we mentioned before, IDOBRU adopted Basic For-mal Ontology version 2 (BFO 2) as its top level ontology[10]. Favoring BFO is due to the integrative nature ofIDOBRU, a representation of all aspects of brucellosis, asit requires integrating with other OBO library ontologies,including the Cell Type Ontology (CL) [14], Chemical En-tities of Biological Interest (ChEBI) [15], Gene Ontology(GO) [16], Information Artifact Ontology (IAO) [17],Ontology of Biomedical Investigations [18], Ontology ofGeneral Medical Science (OGMS) (https://code.google.com/p/ogms/), Ontology of Genes and Genomes (OGG)[19], and Protein Ontology (PRO) [20]. BFO serves as acommon structure and a formal framework to seamlesslyintegrate existing terms from all OBO foundry ontologies.Under this consideration, we adopt relations formally de-fined by the community as much as possible. To representthe relations between molecular entities and its relatedhost-Brucella interactions, we adopted the relation has_agent formally defined by Smith et al. [21]. In the defin-ition provided by Smith et al., a p has_agent c at t denotesa primitive relation between a process p, a continuant cand a time t at which the continuant c is causally ac-tive in the process [21]. The notion of causally activeis aimed to capture the from-to directionality natureof a biological process, which provides the explicit meas-ure for the two-way effect interactions. Applying this re-lation to model host-Brucella interaction, we differentiatethe host-Brucella interaction into two categories: 1) Bru-cella process towards host infection, and 2) host anti-Bru-cella process. The first category has Brucella as its agent,and the second category has a host as its agent. In anotherword, Brucella actively causes establishing Brucella infec-tion in host, and host actively causes host anti-Brucellaprocesses. The Brucella and host both play an agent role inthe processes of host-Brucella interaction.To go beyond the textual definitions and logically definethe host-Brucella interaction and many other ontologyterms, two approaches were used. One approach was touse an ontological axiom(s). An ontological axiom is astatement that provides explicit logical assertions aboutthree types of things: classes, individuals and properties(http://www.w3.org/TR/owl2-syntax/#Axioms). The otherfacts implicitly contained in the ontology can be inferredusing a reasoning software program (i.e., a reasoner). An-other approach is based on inference rules. On the Se-mantic Web, the term inference means an automaticprocedure that can generate new relationship(s) basedon the data (e.g., ontology knowledge data) and someadditional information in the form of a vocabulary, forexample, a set of rules (http://www.w3.org/standards/semanticweb/inference). An inference rule (IR) is a lo-gical form consisting of a function that takes premises,analyzes the syntax, and returns a conclusion(s).To improve data integration and discover new relation-ships and possible inconsistencies, we have defined seveninference rules (IRs) in this report. We have first devel-oped inference rules to capture the physical interaction ofthe two entities (agents) in a host-Brucella interaction.We use the IF p THEN q inference rules to state that,given the truth of p, allows the truth of q to be inferred.The IFTHEN rules are used as a knowledge representa-tion format that can be easily understood and simple toimplement by a computer [22]. Here we formalized thefirst inference rule (IR1) to define a host-Brucella inter-action as following:(IR1) IF (a agent_in p, ? b agent_in p), ? p is_aprocess, ? (a part_of A, ? b part_of B) ? (A is_a(host organism ? host cell), ? B is_a Brucella), THENp is_a host-Brucella interactionIR1 gives three constrains sufficient to define a directhost-Brucella interaction: 1) two entities a and b areagents in a process; 2) these two entities are parts of entityA and B respectively; 3) A is a host organism or host cell,and B is a Brucella bacterium. A and B are disjoint witheach other. It is noted that while a direct host-Brucellainteraction involves both host and Brucella molecules, aLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 4 of 18process that participates in the host-pathogen inter-action may be just a typical host or pathogen processthat is triggered by the direct host-Brucella interaction.In this article, we will provide many examples showingthe pathogen-side or host-side processes following adirect host-Brucella interaction.Besides IR1, this article will provide six other IFTHENinference rules. The IR2 provides an inference that oneprocess is preceded by another process after a direct host-Brucella interaction. IR3-IR7 is about the inference on avirulence factor. Different from ontology axioms thatusually represent necessary or (necessary and sufficient)conditions, these IFTHEN inference rules representsufficient criteria (IF conditions) for a specific inference(THEN conclusion).While ontology axioms behave like inference rules,the Web Ontology Language (OWL) is unable to expressall relations and inference rules (http://dior.ics.muni.cz/~makub/owl/#ontology). One common language that canbe used to define inference rules is the Semantic Web RuleLanguage (SWRL) developed based on a combination ofthe OWL language with the Rule Markup Language(http://www.w3.org/Submission/SWRL/). While inferencerules do not have to be a part of the ontology, SWRL canbe used to represent the rules in an ontology like IDOBRUas shown in Additional file 1. These inference rules de-scribed in an ontology have at least two functions. First,these rules in combination with a logic reasoner supportontology consistency check. If an ontology has errors inconflict with an inference rule, a reasoner like Hermit(http://hermit-reasoner.com/) will be able to detect theerror. Second, an IFTHEN inference rule can gener-ate a conclusion on an ontology instance based on spe-cified conditions.While inference rules are less frequently used in bio-medical ontologies, the SPARQL Protocol and RDF QueryLanguage (SPARQL) has been widely used for queryingontologies [23] and is familiar to the general readers inbiomedical semantics. In this article, we have also pro-vided many SPARQL examples to illustrate the usage ofSPARQL in querying information related in host-Bru-cella interactions.Ontology modeling of different types of BrucellaBrucella strains can be separated into smooth strains andrough Brucella strains depending on their lipopolysac-charide (LPS) compositions. As a major component of theouter membrane of Brucella, Brucella LPS is composed ofthree parts: O polysaccharide (or called O-chain or O-antigen), core oligosaccharide, and lipid A. The O polysac-charide, a repetitive glycan polymer attached to the coreoligosaccharide, is the outermost domain of the LPS mol-ecule. The presence or absence of the LPS O polysacchar-ide determines whether a Brucella strain has the smoothor rough phenotype, respectively. The presence of full-length O polysaccharides would render the bacteriumsmooth, whereas the absence of O polysaccharides wouldmake the bacterium rough [24]. Rough Brucella is usuallyattenuated, and it does not stimulate the production ofanti-O polysaccharide antibody in an infected host. In con-trast, most smooth Brucella strains are virulent, and theyhave intact O polysaccharides and can stimulate anti-Opolysaccharide antibody in host [25]. Virulent wild type B.abortus, B. melitensis, B. neotomae, and B. suis are allsmooth strains, and their rough strains (including manyvaccine and vaccine candidate strains) are attenuated.However, virulent wild type B. canis and B. ovis arerough. Therefore, the virulent/attenuated and smooth/rough characteristics of Brucella strains may not matcheach other.In order to define the smooth and rough characteristicsof Brucella, the negative statement of has no Brucella Opolysaccharide needs to be addressed. Ceusters et al. hasgiven a set of lacks relations to represent the negativefindings in electronic health records [26]. For example, therelation lacks_part was defined in terms of the positive re-lation part_of, holds between a particular p and a universalu when p has no u as part [26]. However, all relations inOWL are relations between particulars by default and can-not represent the relation between a particular and a uni-versal. It is possible to rely on the punning (an OWL2feature) that allows OWL developers to use the URI of aclass for an individual (http://www.w3.org/TR/owl2-new-features/#F12:_Punning). Another shortcoming about thelacks_part relation in OWL is that the relation in OWLexpression C subClassOf: lacks-part some D implies thepresence of an instance of D where the relation itselfsuggests the lack of such an instance [27]. Therefore,although initially we used the lacks_part relation, wehave recently switched to the use of not has_part assuggested in the article [27]. Here we adopt their strat-egy to define the terms smooth Brucella strain andrough Brucella strain as follows:Smooth Brucella strain = def a Brucella has_part somesmooth Brucella lipopolysaccharideRough Brucella strain = def a Brucella has_part onlyrough Brucella lipopolysaccharideSmooth Brucella lipopolysaccharide = def a Brucellalipopolysaccharide has_part some Brucella OpolysaccharideRough Brucella lipopolysaccharide = def a Brucellalipopolysaccharide not has_part some Brucella OpolysaccharideOntology modeling of host-Brucella interaction subtypesBrucella bacteria are able to invade and infect both profes-sional and non-professional phagocytes. The interactionsLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 5 of 18between Brucella and these host cells dictate the outcomesof the infection [28]. At least three types of host cells arerecognized: macrophages, dendritic cells, and epithelial cells(e.g., placental trophoblast cells) [28]. Some cell lines, suchas mouse macrophage cell line J774 [25] and humanepithelial cell line HeLa [29] are important models forstudying host-Brucella interactions. Those cell typesand cell lines have been imported into IDOBRU fromCell Type Ontology (CL) [14] and Cell Line Ontology(CLO) [30] respectively by using OntoFox, a web-basedtool for retrieving and extracting ontological terms andaxioms [31].Given the above three types of host cells and two typesof Brucella strains, we have asserted several subtypes ofhost-Brucella interactions using the format of host cell Brucella interaction. Specifically, six subtypes of host-Brucella interactions were asserted:1) macrophage  smooth Brucella interaction2) macrophage  rough Brucella interaction3) dendritic cell  smooth Brucella interaction4) dendritic cell  rough Brucella interaction5) epithelial cell  smooth Brucella interaction6) epithelial cell  rough Brucella interaction.Different agents participate in each of the host-Brucellainteractions. For example, the agents in macrophage-Bru-cella interactions include:1) Brucella agent in macrophage  Brucellainteraction2) Brucella agent in process of establishing Brucellainfection in macrophage3) (macrophage or macrophage cell line cell) agent inmacrophage  Brucella interaction4) (macrophage or macrophage cell line cell) agent inmacrophage anti-Brucella processFig. 1 Ontological representation of various interactions between Brucella5) macrophage-Brucella interaction has_part processof establishing Brucella infection in macrophage6) macrophage-Brucella interaction has_partmacrophage anti-Brucella processFigure 1 illustrates the above triples and gives six sub-types of macrophage-Brucella interactions.It is noted that the above categorizations do not counton the interactions between Brucella and host organ(e.g., spleen) or the whole host at an organism level. AsBrucella is an intracellular bacterium, the macrophage-Brucella interaction is critical to the outcome of thehost-Brucella interaction [32]. In this article, we will pri-marily use the macrophage-Brucella interaction processesas an example for modeling the host-Brucella interactions.Ontology representation of Brucella invasion, trafficking,and replication inside host cellsAs an intracellular bacterium, the invasion, survival andreplication of Brucella inside host cells are crucial toBrucellas lifecycle. The ultimate goal of Brucella is topropagate in their preferred niche in host cells (particu-larly the macrophages), where they can reach extensivereplication and subsequently transmitted to new hostcells. The intracellular life of Brucella is a subject of in-tensive scientific research [28, 32].Specifically, through different modes of entry into amacrophage (details given in the following section), asmooth or rough Brucella cell will enter a Brucella-con-taining vacuole (BCV) inside a macrophage. The BCVscontaining smooth and rough Brucella cells undergo differ-ent intracellular trafficking pathways. Smooth BCVs be-come mature replicative niche, where the bacteria undergoextensive intracellular replications. In such a replicativeniche, programmed macrophage cell death is prevented,which is beneficial for the intracellular Brucella. In con-trast, rough bruellae are fused with lysosome and cannotand host cellsLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 6 of 18replicate inside macrophages [28]. The rough BCV willundergo programmed cell death [25, 3335]. The processof rough Brucella interacting with macrophages precedesthe process of programmed macrophage cell death, whichis beneficial for the host.Brucella: from entry into host cell to replication nicheSmooth and rough Brucella strains utilize different mech-anisms of entry into host cells. The Brucella LPS O poly-saccharide is a critical molecule for interaction with lipidrafts within the plasma membrane of a host cell. The lipidrafts mediate the internalization of Brucella by phagocytesand nonprofessional phagocytes in a manner that leads tothe development of the replicative niche [28]. The onto-logical representation of Brucella entry into macrophagesand other related processes is shown in Fig. 2.As shown in Fig. 2a, a smooth Brucella LPS (part of Bru-cella) and a lipid raft from a macrophage (part of macro-phage) are both agents in the process of the interactionbetween smooth LPS and macrophage lipid raft. A lipidraft is composed of cholesterol (CHEBI_16113), gangliosideGM1 (CHEBI_61048), and glycosylphosphatidylinositol(CHEBI_24410). The LPS-lipid raft interaction leads to theengulfment of smooth Brucella into a macrophage, pre-ceded with the formation of an early phagosome contain-ing smooth Brucella. This early phagosome does not fusewith late endosome or lysosome [28]. The smooth Brucellacontaining vacuole (BCV) is acidified and trafficked to anFig. 2 Ontological representation of the entrance into macrophage to replendoplasmic reticulum (ER), where the BCV is fused withER membrane. The fusion event leads to the formation ofER-derived replication niche of smooth Brucella, where theintracellular replication takes place. It is noted that only asmall percentage of all invading Brucella cells will surviveand achieve their goal of intracellular replications throughthe trafficking pathway.Compared to smooth Brucella, rough Brucella cells havea different fate (Fig. 2b). Since rough Brucella does nothave LPS O polysaccharide, rough Brucella enters into amacrophage via direct macrophage engulfment rather thanthe lipid raft-dependent engulfment [36]. The early roughBrucella-containing phagosome is fused with lysosome toform a rough Brucella-containing phagolysosome, wherethe invading bacterium is killed by the macrophage [36].To formalize the representation of Brucella intracellulartrafficking pathways, five formal relations defined in BFOwere used: participates in, initially participates in, trans-formation of, begins to exist during, and starts_during.The terms participates_in and its subclasses initially par-ticipates in and begins_to_exist_during are three rela-tions between continuant and process. If a continuant cbegins to exist during a process p, it infers that continu-ant c does not exist before p starts. If a continuant c ini-tially participates in a process p, it infers that p cannotstart without the existence of continuant c. Transfor-mation_of links continuants in a similar fashion as pre-ceded_by linking processes. Specifically, if continuantication by smooth Brucella (a) and rough Brucella (b)Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 7 of 18c1 transformation_of continuant c2, it infers that c2exists earlier than c1. The term starts_during represents arelation between two processes. Taken these five relationstogether, the following IR2 is developed to infer the tem-poral relations between two processes:(IR2) IF c1 participates in p1, ? c initiativelyparticipates in p2, ? c2 begins_to_exist_during p1,? c2 transformation_of c1, THEN p2 starts_during p1For example, based on the biological fact that an earlysmooth Brucella-containing phagosome is transformedto smooth Brucella-containing vacuole (Fig. 2), the fol-lowing statements are represented in IDOBRU:1. acidic phagosome containing smooth Brucellaparticipates in smooth Brucella containingphagosome fusing with ER membrane2. ER-derived replication niche of smooth Brucellainitiatively participates in smooth Brucellaintracellular replication in macrophage3. ER-derived replication niche of smooth Brucellabegins_to_exist_during smooth Brucella containingphagosome fusing with ER membrane4. ER-derived replication niche of smooth Brucellatransformation_of acidic phagosome containingsmooth BrucellaBased on IR2, it is inferred that smooth Brucella intra-cellular replication in macrophage starts_during smoothBrucella containing phagosome fusing with ER membrane.The inferred statement is biologically valid [6].As described in the Methods section, IR2 has been addedto IDOBRU using the SWRL syntax (Additional file 1).Representing intracellular survival of smooth Brucella insidemacrophagesWhile a smooth Brucella-containing vacuole is traffickingwithin a host cell, the bacteria inside the vacuole encounterformidable environmental stresses such as the exposuresto reactive oxygen (ROS) and nitrogen species (RNS),acidic pH, nutritional deprivation, and lytic peptides con-tained in lysosomes [28]. To withstand these environmen-tal stresses, Brucella has developed different strategies.IDOBRU uses several smooth Brucella resistance sub-class terms to represent the resistant processes enablingsmooth Brucella to survive under the stressful environ-ments within macrophages. Examples of such terms are:smooth Brucella resistance to nutrient deprivation,smooth Brucella resistance to antimicrobial peptide,smooth Brucella resistance to nitrosative stress insideBCV, smooth Brucella resistance to oxidative stress in-side BCV, and smooth Brucella resistance to aciditystress inside BCV. The part of  relation was used to linkabove processes to process of establishing smooth Bru-cella survival in macrophage in the ontology.The abilities of smooth Brucella resistance to thosestressful conditions were represented as different types ofdisposition. The IDO term protective resistance is usedas their direct mother term in virtue of protecting Brucellafrom different stresses. Smooth Brucella has the disposi-tions including nutrient deprivation resistance disposition,antimicrobial peptide resistance disposition, oxidativestress resistance disposition, nitrosative stress resistancedisposition, and acidic stress resistance disposition. Thesedispositions are realized in relevant resistance processesand inhere in a smooth Brucella strain. For example, thenutrient deprivation resistance disposition of smooth Bru-cella is realized in the process of smooth Brucella resist-ance to nutrient deprivation, and smooth Brucella strainis the agent participating in the process (Fig. 3).As the interactions are between host and pathogen,the response actions of macrophage cells are representedin IDOBRU using four biological process terms: nitric-oxide synthase biosynthetic process (GO_0051767), re-active oxygen species metabolic process (GO_0072593),acidification of BCV in macrophage (IDO_0100758),and macrophage antimicrobial peptide production (IDO_0100759). All these processes are part of the macrophageanti-Brucella process. The unfolds_in relation is used tocapture the fact that those processes take place inside amacrophage cell (Fig. 3).Representing Brucella pathogenesis mechanismsIn macrophages, the majority of BCVs fuse with lysosomesand the bacteria are killed and degraded in the early hoursof internalization [36]. Approximately 1030 % of internal-ized Brucella cells are able to survive and undergo theintracellular trafficking of the host cell [37]. The molecularmechanisms of Brucella pathogenesis are responsible forall kinds of interactions with their mammalian hosts. Viru-lent Brucella employs several strategies and uses manyvirulence factors to establish and maintain persistent intra-cellular residence in host cells. Intracellular Brucella alsoalters biological functions of professional phagocytes,resulting in the losing of their robust antigen-processingcapacity. In order to prevent more hostile extracellular en-vironment, virulence Brucella is able to prevent the pro-grammed cell death of infected macrophages [38]. Brucellapathogenesis relies on the presence of many Brucella viru-lence factors and their interactions with the host defensesystem. Two examples are generated here to illustrate howIDOBRU represents the interactions between host andbacterial genes, proteins and pathways.Representing type IV secretion systemBacterial type IV secretion systems (T4SS) are often crit-ical to selective translocation of proteins and DNAFig. 3 Ontological representation of Brucella intracellular survival inside macrophagesLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 8 of 18protein complexes [39]. Brucella T4SS, encoded by thevirB operon, is a major virulence factor system [3841].Brucella T4SS virB operon includes 12 genes whose ex-pression is specifically induced by the phagosome acidifi-cation after Brucella phagocytosis [40]. Brucella T4SS isrequired for smooth Brucella trafficking to replicationniches and intracellular survival inside host cells. Inrough strain of Brucella, T4SS expression is importantfor Brucella cytotoxicity in macrophages [38].Figure 4a represents Brucella T4SS and its roles in thepathogenesis of smooth Brucella. The Brucella virB op-eron expression is preceded by acidification of smoothBrucella containing phagosome. The Brucella VirB pro-teins, encoded by the Brucella virB operon, start to existwith the expression of the virB operon genes (begin_to_exist_during). The process of binding a virB promoterregulates the T4SS virB operon expressions. For example,VjbR, a LuxR-type quorum-sensing regulator, binds on thevirB promoter, and activates Brucella virB operon expres-sion [42]. Therefore, VjbR regulates Brucella T4SS directlyand subsequently has an impact on the effectors of T4SSprotein secretions. As shown in Fig. 4a, VjbR is anagent in the Brucella virB promoter binding process,which leads to positive regulation of the Brucella virBoperon expression.Brucella T4SS acts as a translocator of Brucella proteinsor DNA into a macrophage [42]. Entities translocated orsecreted by Brucella T4SS are termed as Brucella T4SSeffectors, which regulate different processes essential toBrucella intracellular trafficking or replication. A BrucellaT4SS effector is defined as a molecular entity that bearsthe Brucella T4SS effector role [40]. The Brucella T4SSeffector role is a new IDOBRU term that is defined as: arole that inheres in a Brucella protein or a DNA uponwhich the Brucella T4SS acts, and as a result, the proteinor DNA is secreted out of the bacterium.As an example of a Brucella T4SS effector, Brucella pro-tein RicA (Rab2 interacting conserved protein A), is repre-sented in IDOBRU to illustrate the Brucella T4SS virulencemechanism (Fig. 4). Brucella RicA, encoded by a Brucellagene BMEI0736, binds to human small GTPase proteinRab2 [43]. RicA is translocated from B. abortus to infectedmacrophages. However, this phenomenon does not occurwhen a Brucella virB mutant infects the macrophage cell.Rab2 also coordinates the retrograde Golgi-to-ER transport[44]. The Rab2 is essential for the formation of the fusionbetween BCV and ER, which results in the Brucella replica-tion niche [45]. As shown in Fig. 4b, Brucella T4SS partici-pates in the process of translocation of RicA into amacrophage as an agent. Brucella protein RicA bears Bru-cella T4SS effector role, and it participates in the processesof translocation of RicA into macrophage and RicA-Rab2 binding. The process of RicA binding to Rab2leads to the formation of RicA-Rab2 complex. Rab2participates in the regulation of Golgi-to-ER transporta-tion, which regulates the retrograde Golgi-to-ER trans-port. Overall, Fig. 4b provides a detailed explanation toan axiom stated in Fig. 4a: Brucella T4SS effectorFig. 4 Ontological modeling of Brucella type IV secretion system and its effects. a The general Brucella type IV secretion in Brucella (a) andexample of RicA as T4SS effector (b)Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 9 of 18participates in the regulation of Brucella intracellulartrafficking.Representing Brucella erythritol metabolismAs described earlier, one of the intracellular environmentalstresses that Brucella faces is nutritional deprivation. Bru-cella uses alternative metabolism pathways to obtain car-bon, nitrogen, oxygen, phosphorus, sulfur and metalsfrom its intracellular host [5]. For example, an alternativepathway for Brucella to acquire carbon is through theerythritol metabolism [28, 46]. The genome of attenuatedB. abortus vaccine strain S19 includes a 703 nucleotide de-letion on its ery operon. The deletion affects the gene eryCcoding for an enzyme erythrulose-1-phosphate dehydro-genase (EryC) and another gene eryD that encodes forEryD, a regulator of ery operon expression [47]. The dele-tion is a cause of the attenuation characteristic of strain 19[48]. The eryCmutant of B. suis also reduces its intracellu-lar replication in macrophage cells [46].To ontologically represent the virulent characteristicof EryC, we started by representing the whole processand its participants at the molecular level and organismlevel (Fig. 5):1) The erythritol metabolism pathway is important forBrucella intracellular replication. The intracellularBrucella in a macrophage has the disposition ofuptaking erythritol as the carbon source, which isrealized in the relevant uptaking erythritol process.The process of uptaking erythritol as carbon sourcehas Brucella erythritol catabolic process as its part.The enzyme substrate role of erythritol is realizedin the erythritol catabolic process. The term enzymesubstrate role is defined as A role that inheres in aprotein or a compound upon which enzymecatalyzes. It is realized in the enzymatic reactionprocesses, where the molecules at the beginning ofthe process, called substrates, are converted intoFig. 5 Ontological representation of Brucella erythiritol metabolism and its involvement in Brucella pathogenesisLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 10 of 18different molecules, namely products. The finalproduct of erythritol catabolic process is thedihydroxyacetone phosphate (DHAP), which isrepresented using the relation begins_to_exist_duringas: DHAP begins_to_exist_during erythritol catabolicprocess. The process of uptaking erythritol as carbonsource is a partial process of Brucella resistance tonutrient deprivation, which accordingly precedes andis required for the process of Brucella intracellularreplication. The relation preceded_by is used todenote the relation between these two processes. BothBrucella response to nutrient deprivation andBrucella intracellular replication are parts of thehost-Brucella interaction process (Fig. 5). It is notedthat we did not model the detailed chemical reactionsthat are components of the whole erythritol catabolicprocess, because it is out of the scope of currentIDOBRU development.2) Brucella genes eryC and eryD involve in theerythritol metabolism pathway.Both eryC and eryD are parts of the Brucella eryoperon. The eryC gene encodes erythrulose-1-phosphate-dehydrogenase enzyme (EryC), and itparticipates in the erythritol catabolic process. TheeryD gene encodes for an EryD protein that regulatesthe ery operon expression [47]. The object propertyregulates is a relation between processes, and it isused in GO to represent the fact that a process A hasa direct influence on another process B such that itcontrols some aspects of how process B unfolds [49].Therefore, the protein EryD regulates the ery operonexpression (Fig. 5).3) B. abortus vaccine strain S19 has a 703-nucleotidedeletion which interrupts both the coding regions oferyC (BAB2_0370) and eryD (BAB2_0369) [48]. Thedeletion affects the C terminal part of the Brucellaprotein EryC and the N-terminal part of the Brucellaprotein EryD [48]. Therefore, S19 lacks the intactEryD protein and EryC protein as its parts. In theother example, the eryC mutant of B. suis has no EryCprotein as its part, and it is an agent in the process ofreduced intracellular replication in macrophage [46].Reduced intracellular replication in macrophage is acompromised intracellular replication process inmacrophage.Representing host immune responses to Brucella infectionVirulent Brucella is a stealthy bacterium that hijacks manyhost immune mechanisms to serve its own survival andreplication inside a host [6]. As introduced above in theBrucella pathogenesis section, virulent Brucella is able toreplicate inside macrophages which are typically powerfulinnate immune cells. Brucella can survive in replicativephagosomes inside macrophages where nutrients are diffi-cult to obtain. The Brucella-containing phagosome doesnot fuse with bactericidal lysosomes [6]. Furthermore, tomaintain the bacterial natural living niche, virulent Bru-cella prevents the programmed macrophage cell death.However, live attenuated Brucella strains, including Bru-cella cattle vaccine RB51 [50], induce apoptosis or otherLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 11 of 18types of programmed cell death of infected macrophages,which destroys the Brucella living niche and exposes thebacteria to the most hostile extracellular environment[25, 34]. Therefore, the programmed cell death processbenefits the host. Below we ontologically represent andanalyze the process using the example of live attenu-ated rough B. abortus strain RB51, which is a cattlebrucellosis vaccine licensed and used in the USA andmany other countries [50].Our previous wet-lab studies have shown that RB51and many other rough attenuated Brucella strains in-duce caspase-2-mediated pro-inflammatory cell deathin macrophages through the release of cytochrome c frommitochondria [34, 35]. RB51 has an insertion within wboAgene that leads to the deficiency of Brucella LPS O poly-saccharide and results in its rough phenotype [51]. Figure 6illustrates ontological representation of RB51-inducedcapspase-2-mediated macrophage cell death. In this repre-sentation, RB51 has no intact wboA gene that encodes foran enzyme involved in the biosynthesis of Brucella abortusO-polysaccharide [52]; therefore, RB51 is lack of Brucellaabortus O-polysaccharide. RB51 is an agent in the processof its infecting a macrophage, which is a part of themacrophage-RB51 interaction process. The RB51 infec-tion of macrophage triggers (RO:precedes) three pro-cesses: activation of caspases-2, positive regulation ofmacrophage programmed cell death, and positive regula-tion of macrophage necrotic cell death. The activation ofcaspases-2 leads to (RO:precedes) positive regulation ofmacrophage programmed cell death. The positive regula-tion of macrophage cell programmed death positively reg-ulates the apoptotic macrophage cell programmed death,which leads to necrotic macrophage cell death. The nec-rotic macrophage cell death is positively regulated by thepositively regulation of macrophage necrotic cell death.From a macrophages perspective, the macrophage hastwo opposite dispositions: 1) the disposition of undergo-ing programmed cell death that is realized by attenuatedRB51 infection of macrophage; and 2) the resistance toFig. 6 Ontological representation of Brucella vaccine strain RB51-induced mprogrammed cell death that is realized by virulent Bru-cella infection of macrophage.Ontological representation and queries of virulencefactors and associated host-Brucella interactionsOntology classification of Brucella host-Brucella interactionsinvolving virulence factorsWe previously defined a Brucella virulence factor as viru-lence factor that bears Brucella virulence factor disposition[10]. The Brucella virulence factor disposition is definedas a disposition borne by a biological macromolecule pro-duced by Brucella spp. that is the disposition to improvesurvival of the pathogen in a host, improve transmission ofthe pathogen to a host, or cause pathological processesin a host. To further expand the definition, virulencefactors are ontologically classified in this article to becritical to five pathogen virulence (or microbial patho-genesis) processes:1) colonization of a niche in the host (this includesadhesion to cells);2) evasion of the hosts immune response;3) inhibition of the hosts immune response;4) entry into and exit out of cells (if the pathogen is anintracellular one);5) absorption of nutrition from the host.As shown in the modeling of T4SS and eryC describedabove, a Brucella virulence factor is involved in at leastone critical process as part of the host-Brucella inter-action, or a process precedes the critical process. Althoughmany Brucella molecules participate in the host-Brucellainteraction processes, not all of them contribute to thevirulence of the Brucella. One method to confirm the sta-tus of a molecule being a virulence factor is knock-out ex-perimental evidence, where the pathogen without thismolecule realizes a reduced or abolished virulence dis-position during the host-Brucella interaction.acrophage cell deathLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 12 of 18Ontology representations of Brucella virulence genemutantsThe major category of virulence factors are protein viru-lence factors that are encoded by virulence genes. Com-pared to the original IDOBRU that includes 245 Brucellavirulence factors [10], current IDOBRU has been ex-panded to include 432 experimentally-verified Brucellavirulence factors. All these virulence factors are experi-mentally verified. The gene mutation followed by experi-mental examination of the virulence of the gene mutantinside a host (i.e., the host organism or host cell) is themajor method to detect the status of a pathogen proteinas a virulence factor.Figure 7 shows how IDOBRU represents a virulence genemutant that lacks an intact protein virulence factor. Specif-ically, a gene mutant is represented in IDOBRU as a mutantthat does not has_part a gene, which also results in thelack of an intact protein (Fig. 7a). The original IDOBRUused IDO ontology identifiers with the IDO_ to representBrucella genes and proteins. However, Brucella genes andproteins may be used in other ontologies such as the Vac-cine Ontology (VO) [53, 54]. The usage of IDO-specificidentifiers does not support data integration and resourceinteroperability. As detailed in the Methods section andshown in Fig. 7, in the new version of IDOBRU, we haveimported the Ontology of Genes and Genomes (OGG) IDs(Fig. 7b) and Protein Ontology (PRO) IDs (Fig. 7c) to repre-sent the genes and proteins of Brucella virulence factors.OGG is a relatively new ontology that represents specificgenes in different species [19]. PRO is an ontology of pro-tein entities [20]. To link the gene and protein entities, weFig. 7 IDOBRU representation of a Brucella virulence gene mutant. The exmutant has a mutation of strain 2038 sodC gene (b), which encodes a prrelation has_gene_template is used to link the protein to the gene. Thehave adopted the PRO relation has_gene_template to rep-resent a protein encoded by a gene (Fig. 7c), and the rela-tion encodes to represent a gene encoding a protein.Description rules to define virulence factorsTo establish logical reasoning for a virulence factor, we de-veloped five description rules as defined below. In the for-mulation of these rules, o denotes an organism O, g and gdenote genetic materials, e denotes a molecular entity, pdenotes a process, i denotes a host-pathogen interactionprocess, and mo denotes a mutant of O.(IR3) IF o has_part g, ? g encodes e, THEN ohas_part eIR3 means that if an organism has part of a gene thatencodes for a molecular entity (i.e., gene product suchas protein), then this organism has part of the molecu-lar entity.(IR4) IF mo has_part g, ? g derives_from g, ? (g nothas_part part of g) ? genome of o has_part g, THENgenome of mo not has_part gIR4 means that if a mutant of an organism has an arti-ficially altered gene g that is derived from g, either by aninsertion or partial deletion, then the genome of the mu-tant has no intact g as its part. When the g is fully de-leted (i.e., g gene knock-out) from mutant mo, the aboverule will not apply. In this case, we simply assert thatgenome of mo not has_part g (see below).ample shown here is B. abortus strain 2308 sodC mutant (a). Thisotein called Copper/Zinc Superoxide Dismutase (SOD) (c). Thescreenshots came from the IDOBRU page in Ontobee [61]Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 13 of 18(IR5) IF genome of mo not has_part g, THEN mo nothas_part gIf the genome of mutant has no intact gene g as itspart, the mutant has no g as its part.(IR6) IF genome of mo not has_part g, ? g encodes e,THEN mo not has_part eIR6 means that if the mutant has no intact gene g asits part, and the gene g encodes the molecular entity e inthe non-mutated organism, then the mutant has no in-tact e as its part.IR7 was given as a final inference rule for inferring avirulence factor:(IR7) IF (mo has disposition at some timeattenuated disposition ? attenuated dispositionrealized in i) ? mo not has_part e, ? (moagent_in_compromised_process p ? p is_a pathogenvirulence process), THEN e is_a virulence factor.For example, as shown in Fig. 5, (B. abortus eryC mu-tant not has_part EryC) AND (B. abortus eryC mutantagent_in_compromised_process Brucella intracellularreplication in macrophage) AND (Brucella intracellularreplication in macrophage is_a pathogen virulenceprocess) means that EryC is_a Brucella virulence factor.According to IR6, if a gene g encoding a protein e is mu-tated from a mutant, the mutant does not have the in-tact protein e any more. Since eryC is mutated from theeryC mutant, we can infer that the eryC mutant doesnot have EryC.We can therefore identify the biological process import-ant to the pathogen virulence during the host-Brucellainteraction. Using the same Fig. 5 example, (EryC partici-pate_in Brucella erythritol catabolic process) AND (EryCmutant of B. abortus agent_in_compromised_processintracellular replication in macrophage), which meansthat the Brucella erythritol catabolic process is crucial tothe intracellular replication in macrophage.With the support of the above representations definedby IR3-7, we have annotated 269 virulence factors asso-ciated with various macrophage-Brucella interactions.IR7 is included in IDOBRU (Additional file 1: Figure S1).SPARQL queries of IDOBRU for virulence factors critical forhost-Brucella interactionsIn this study, several SPARQL scripts were generated toquery the information related to host-Brucella interac-tions. The details are provided below:First, a simple SPARQL script was generated to query thenumber of protein virulence factors collected in IDOBRU(Additional file 2). Each virulence factor protein is bearerof at some time (BFO_0000053) some Brucella virulencefactor disposition (IDO_0100116). The query identified432 protein virulence factors.The second SPARQL query identifies the processes inwhich Brucella virulence factors participate (Fig. 8). Specif-ically, the script queries what compromised processes Bru-cella virulence factor mutants get involved in. The relationagent_in_compromised_process as described earlier isused here. In total, 11 biological processes, for example,Brucella entry into macrophage (IDO_0100610) andBrucella intracellular trafficking (IDO_0100983), wereidentified (Fig. 8). These processes are critical to Bru-cella pathogenesis inside host cells.Third, those Brucella mutants that are attenuated in-side macrophages during various macrophage-Brucellainteractions were identified using SPARQL (Additionalfile 3). Each of these mutants is associated with a par-ticular gene (represented in OGG) and a correspondingprotein (represented in PRO) (Fig. 7). Therefore, thequeries also provide us a way to extract those virulencegenes and virulence factors important for macrophage-Brucella interactions. In total, 269 gene mutants that areassociated with 269 genes and protein virulence factorswere found. The list of all these gene mutants is alsoprovided in Additional file 3.Fourth, the Brucella protein virulence factors import-ant for Brucella intracellular replication inside macro-phages were detected using two SPARQL queries(Additional file 4). A query identified 81 such virulencefactors, and the other query provided the detailed listof these factors (Additional file 4).It is noted that the inference rules IR3-7 provide thelogic clues on the second and third sets of queries. Spe-cifically, to search Brucella virulence factors importantfor intracellular replication in macrophages, first we re-trieved all possible gene mutants. Each protein encodedby a gene mutated in a mutant is an agent involved in thecompromised process of macrophage-Brucella interaction(IDO_0100832) (Additional file 3: Figure S3) or Brucellaintracellular replication in macrophage (IDO_0100612)(Additional file 4: Figure S4). The relation agent_in_com-promised_process as described earlier is used here. Due tothe mutation event, the intact Brucella protein (virulencefactor) does not exist in the mutant organism (IR4 andIR6). Therefore, we are able to retrieve all the correspond-ing virulence factor proteins using the not has_part rela-tion (Additional file 4: Figure S4).DiscussionCompared to the original IDOBRU paper published in2011 [10], current article included several novel contri-butions. First, while the original paper only includes onesection with one figure in the topic of the host-Brucellainteraction (i.e., virulence factor and pathogenesis),Fig. 8 SPARQL query of biological processes involving Brucella virulence factors. This script queries the compromised Brucella processes in whichBrucella virulence gene mutants participate. The query was performed using the Ontobee SPARQL web-interface (http://www.ontobee.org/sparql).The IDO_0101168 in the script is agent_in_compromised_processLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 14 of 18current article focuses on the modeling and analysis ofdifferent aspects of the host-Brucella interactions. Specific-ally, this article first ontologically differentiates smooth andrough Brucella phenotypes and how each phenotype is re-lated to Brucella virulence. Six host-Brucella interactionsubtypes are categorized, and the agents participating inany of the subtypes (i.e., macrophage-Brucella interaction)are defined. IDOBRU is also used to represent detailedprocesses of Brucella invasion, trafficking, and replicationinside host cells. Examples described in this article includetwo major Brucella pathogenesis mechanisms: the Type IVsecretion system (T4SS) and the erythritol metabolism. Interms of host immune response against Brucella infection,the inhibition or promotion of host programmed cell deathis specifically modeled using IDOBRU. Regarding the viru-lence factors, this article for the first time ontologicallyclassifies five different types of host-pathogen interactionprocesses where virulence factors may play a critical role.Approximately 200 more virulence factors have been in-cluded in current IDOBRU since the original IDROBRUpublication. Second, seven specific inference rules are gen-erated and described in this paper for reasoning related tohost-Brucella interactions and Brucella virulence factors.The layout and implementation of these inference rulesprovide more power in using IDOBRU for computer-assisted reasoning. Third, this article introduces an updatedstyle of representing Brucella genes and proteins. Insteadof using IDO IDs to represent genes and proteins, theupdated IDOBRU imports OGG and PRO IDs for moreauthentic representations of Brucella genes and proteins.Such gene/protein representations support ontology reuseand interoperability. Fourth, this article provides manySPARQL scripts that demonstrate the applications of IDO-BRU. Furthermore, many more ontology terms have beenadded to IDOBRU. Compared to the original IDOBRUversion published in 2011 that includes 1503 terms,current version 1.2.79 includes 3488 terms. We havemore than doubled the numbers of the terms in theIDOBRU ontology, clearly showing our progress in theIDOBRU development.Other literature reports exist for ontological modellingof host-pathogen interactions. The Plant-Associated Mi-crobe Gene Ontology (PAMGO) Consortium uses GO formodeling host-pathogen interactions based on the investi-gation of plant-associated symbionts [55]. Their effortsyielded a group of GO biological process terms that cap-ture the processes occurring between hosts and symbionts(from mutualists to pathogens). PAMGO is focused onrepresenting processes. Representation of host partici-pants (e.g., organelle like ER, and cell membrane) andpathogen details (e.g., LPS of Brucella, Brucella proteins)was not covered. IDO-core provides many top level termsin the area of host-pathogen interactions, such as estab-lishment of localization in host (IDO_0000625). As an ex-tension ontology of IDO-core, the Malaria Ontology teamdeveloped several terms related to malaria-host interactLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 15 of 18processes, such as inhibition of invasion and responsive-ness to host cue [56]. Another ontology named HostPathogen Interaction Ontology (HPIO) found on NCBOBioPortal is current under development, which aims todescribe the host-pathogen interactions between Salmon-ella bacteria and swine, also as an extension of IDO-core.However, HPIO does not represent the interactions be-tween host and pathogens extensively. None of the aboveefforts covers the participants of the interaction processesand relations between those processes. In contrast, IDO-BRU represents various participants in the interactionprocesses between Brucella and its hosts with details atthe organism, cell, and molecular levels. IDOBRU is usedas a framework to model different proteins, complexesand interaction processes such as how virulence factorsplay a role in the Brucella-host interactions and how theinteractions happened as a series of sequential events. Tosupport data standardization and exchange, formal rela-tions are applied in the IDOBRU modeling. To the best ofour knowledge, our IDOBRU modeling and representa-tion of various areas of host-Brucella interactions repre-sent the first comprehensive host-pathogen interactionontological analysis.The host-Brucella interaction is modeled in IDOBRU asa process with many specific interaction subclasses. Theinteractions between Brucella and hosts are Brucella-spe-cific and host (e.g., host organism or cell) - specific. Forexample, the pathway of Brucella entry into macrophagediffers from the bacterial entry into epithelial cell, and theimmune response and intracellular trafficking induced byBrucella infection differ among professional and non-professional phagocytes [57]. Therefore, it is important toclassify different subtypes of host-Brucella interactions byhost cells. The macrophages are emphasized in this studysince macrophage is likely the most critical host cell typein terms of host-Brucella interactions [32]. More onto-logical representations with other host cells are needed.Similarly, we will need to classify the subtypes based onhost organism or host organs as well, since the infectionof any Brucella strain has its host preference. These as-pects are critical to the development of host-specific Bru-cella vaccines.Compared to well-studied model organisms such as E.coli, Brucella is less studied, and the coverage of Brucellaresearch is often unbalanced and limited. For example,middle products and enzymes involved in the erythritolcatabolism pathway in Brucella are not available in PROor ChEBI. We communicated the PRO and ChEBI groupsand submitted related middle product terms to ChEBIand enzyme terms to PR. For example, over 1000 Brucellastrain specific proteins were submitted to PR. Althoughthe enzyme databases and knowledge bases are well devel-oped and contain authentic comprehensive information,the erythritol catabolic pathway is not included in well-known enzyme databases such as BRENDA (http://www.brenda-enzymes.org/). GO has very few gene prod-ucts from Brucella annotated with experimental evidencecodes (http://www.geneontology.org/GO.evidence.shtml).The inference rules (IR3-IR7) generated in this study pro-vide a framework on inferring virulence factors. These rulescan be used for validating the ontology and populating theontology. These inference rules can also be generalized toother pathogen. The inference rules represent knowledgeand provide a format for computer-understandable auto-mated reasoning. These rules offer explicit and transparentassumptions for representing virulence factors in the caseof the host-Brucella interaction. Based on the inferencerules and available data, a computer will be able to assess ifa material entity is a specific virulence factor. Since themechanisms of virulence are different from species to spe-cies, more inference rules may be developed with differenttypes of pathogens (e.g., HIV virus).SPARQL provides a powerful method for querying andanalyzing the data in an ontology [58]. For example, ourqueries identified 269 protein virulence factors related tomacrophage-Brucella interactions; and among these pro-teins, 81 are important for intracellular replication withinmacrophage from the knowledge stored in current IDO-BRU. Note that virB1, virB5, virB8, virB9 and virB10 arein the above list, which validates the modeling of T4SSmechanism in this paper. In addition, we have identified11 biological processes important for Brucella virulence(Fig. 8). These simple but powerful SPARQL queries dem-onstrate the applications of IDOBRU and IDOBRU-basedSPARQL technology.One important contribution of this paper is its first re-port in co-representing genes and proteins using theOntology of Genes and Genomes (OGG) [19] and theProtein Ontology (PRO) [20]. The majority of Brucellavirulence factors are proteins, which are encoded by spe-cific genes in different Brucella strains. The practice ofusing IDOBRU IDs in our original IDOBRU version wasnot ideal since it does not support ontology reuse and in-tegration. To address this issue, the new version of IDO-BRU uses the gene and protein IDs from the OGG andPRO, two ontologies in the OBO ontology library. TheOGG ontology was recently developed to represent genesfrom specific organisms by reusing existing resources, pri-marily the NCBI Gene resource [59]. Due to the largenumbers of genes available in different organisms, OGGincludes different subsets, each of which represents genesfrom one or a few organisms. Using the OGG develop-ment strategy, we first generated the OGG Brucella subsetthat covers all genes of three major Brucella strains. Allvirulence factor genes covered in IDOBRU are from thesethree Brucella strains. The availability of the OGG Brucellasubset allows us to retrieve and reuse the OGG terms torepresent Brucella genes in IDOBRU. Similarly, specificLin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 16 of 18Brucella proteins were generated in PRO and reused inIDOBRU. Furthermore, using two object properties (i.e.,has_gene_template and encodes), we were able to repre-sent the relations between genes and proteins. Since genesand proteins are two fundamental entities in biology, thisstudy provide a demonstration on how these can be onto-logically represented and interlinked.ConclusionsIn this paper, we ontologically represent various Brucella-host interactions primarily using the Brucella-macrophageinteraction as a use case. A formal definition of the Bru-cella-host interaction was given in OWL format. After thedefinitions on smooth and rough Brucella are given, sixsubtypes of Brucella-host interactions are classified ac-cording to the Brucella phenotypes and host cell types.IDOBRU is further used as a platform to represent inter-active processes including Brucella invasion, intracellulartrafficking and intracellular replication at an organismlevel. By representing the Brucella pathogenesis mecha-nisms using Brucella T4SS and EryC examples, we dem-onstrate how to ontologically link biological processesfrom the organism level down to the molecular level. De-scription logical inference rules have also been defined toinfer: 1) the interaction process between two species (i.e.,a host and a pathogen); 2) the temporal relations of bio-logical processes; 3) relations between gene, protein, gen-ome, and gene mutant; and 4) a virulence factor. For thisstudy, many new terms have been added into IDOBRU.Using SPARQL queries generated based on inferencerules, out of the 269 virulence factors related to macro-phage-Brucella interactions, 81 virulence factors werefound to be important for Brucella intracellular replicationinside macrophage. Eleven biological processes were alsofound important for Brucella virulence.MethodsOntology editingThe format of W3C standard Web Ontology Language(OWL2) (http://www.w3.org/TR/owl-guide/) was appliedfor IDOBRU development. The Protégé OWL ontologyeditor (http://protege.stanford.edu/) (versions 4.3 and 5.0beta) was used to edit IDOBRU.Existing ontology term importThe ontology development uses a hybrid bottom-up andtop-down method as described in our original IDOBRUarticle [10]. For this host-Brucella interaction study, manyexternal ontology terms from existing ontologies, includ-ing Cell Type Ontology (CL) [14], Chemical Entities ofBiological Interest (ChEBI) [15], Gene Ontology (GO)[16], Protein Ontology (PRO) [20], were imported to IDO-BRU using OntoFox (http://ontofox.hegroup.org/) [31].IDOBRU access and visualizationThe latest version of IDOBRU is always available at Source-forge website: (http://svn.code.sf.net/p/idobru/code/trunk/src/ontology/brucellosis.owl). Not that this is an unmergedOWL file, and it imports many other OWL files in thesame folder. Therefore, it would be best to get all the re-lated ontology files via SVN. IDOBRU has also been depos-ited in the NCBO BioPortal (http://bioportal.bioontology.org/ontologies/IDOBRU) and the Ontobee linked ontologybrowser system (http://www.ontobee.org/browser/index.php?o=IDOBRU). Both NCBO BioPortal and Ontobee pro-vide interactive search and visualization features for IDO-BRU exploration and analysis.OGG and PRO representation of Brucella virulence factorsThe Brucella subset of the Ontology of Genes and Ge-nomes (OGG) was generated using a method described inthe OGG paper [19]. Specifically, a NCBITaxon subsetwas generated to include three Brucella strains usingOntoFox [31]. These strains are B. abortus strain 2308, B.suis strain 1330, and B. melitensis strain 16 M. All anno-tated Brucella genes in IDOBRU come from these threestrains. Most of the information of all added Brucellagenes encoding protein virulence factors was obtainedfrom the manually annotated Victors database (http://www.phidias.us/victors) in the PHDIAS resource [60].The OGG Brucella subset was submitted to the He groupRDF triple store [61]. OntoFox was then used to retrievethe Brucella genes covered in IDOBRU.The corresponding proteins encoded by these Brucellagenes are represented by Protein Ontology (PRO) [20].OntoFox was used to extract the information of theseproteins from PRO. The resulting PRO subset was thenimported to IDOBRU.Queries of IDOBRUSPARQL scripts were developed to query IDOBRU usingthe IDOBRU SPARQL query web page (http://www.phidias.us/bbp/idobru/sparql/index.php) located in theBrucella Bioinformatics Portal (BBP; http://www.phidias.us/bbp) [60, 62].Implementation of inference rulesThe reasoner HermiT 1.3.8 (http://hermit-reasoner.com/)as a plugin in the Protégé OWL editor (http://protege.stanford.edu/) was used to implement the inferencerules defined in this paper. The rule view editor in theProtégé OWL editor was used to edit the rules. Theontology rule view in Protégé is accessible from theProtégé menu Window? Views?Ontology views?Rules. The saved IDOBRU OWL file contains the rulesin the format of OWL with SWRL codes.Lin et al. Journal of Biomedical Semantics  (2015) 6:37 Page 17 of 18Additional filesAdditional file 1: Implementation of inference rules using theProtégé platform. (PDF 383 kb)Additional file 2: SPARQL query of IDOBRU for the total number ofprotein virulence factors in IDOBRU. (PDF 309 kb)Additional file 3: SPARQL query of IDOBRU for Brucella mutantsthat are attenuated inside macrophages during the macrophage-Brucella interactions. Since each mutant is associated with one geneand one protein, these queries also allow us extract those virulencegenes and protein virulence factors that participate in variousmacrophage-Brucella interactions. (PDF 353 kb)Additional file 4: SPARQL query of IDOBRU for Brucella proteinvirulence factors important for the intracellular replication ofBrucella inside macrophages. (PDF 198 kb)Competing interestsThe authors declare that they have no competing interests.Authors contributionsYL: Primary IDOBRU ontology developer, SPARQL analysis, and use casetesting. ZX: IDOBRU developer, SPARQL analysis, and manuscript editing. YH:IDOBRU developer, project design and management, brucellosis domainexpert, and SPARQL analysis, and use case testing. The manuscript wasprimarily drafted by YL and YH, and edited and approved for publications byall authors.AcknowledgementsThis work has been supported by grant R01AI081062 from the NIH NationalInstitute of Allergy and Infectious Diseases (NIAID). The article-processingcharge for this article was paid by a bridge fund to YH from the Unit forLaboratory Animal Medicine (ULAM) in the University of Michigan. We thankDr. Salwa Ali and Mr. Jiangan Xie for contributing to the IDOBRU generation.We appreciate Drs. Darren A. Natale and Cathy H. Wu for their support onadding related Brucella proteins to PRO and their comments and editing ofthe manuscript.Received: 31 December 2012 Accepted: 23 September 2015