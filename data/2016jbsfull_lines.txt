Zhang et al. Journal of Biomedical Semantics  (2016) 7:21 DOI 10.1186/s13326-016-0063-3ERRATUM Open AccessErratum to: Extracting drug-enzyme relationfrom literature as evidence for drug druginteractionYaoyun Zhang1, Heng-Yi Wu2, Jingcheng Du1, Jun Xu1, Jingqi Wang1, Cui Tao1, Lang Li2 and Hua Xu1*Following the publication of the original article [1] itwas brought to our attention that an important acknow-ledgement from co-author Jingcheng Du was missingfrom the manuscript. The authors would like to apolo-gise for this oversight and now take the opportunity togratefully acknowledge the support from the UTHealthInnovation for Cancer Prevention Research TrainingProgram Pre-doctoral Fellowship (Cancer Preventionand Research Institute of Texas grant # RP140103).Author details1School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USA. 2School of Medicine, Indiana University,Indianapolis, IN, USA.Received: 5 April 2016 Accepted: 5 April 2016Reference1. Zhang Y et al. Extracting drug-enzyme relation from literature as evidencefor drug drug interaction. J Biomed Semantics. 2016;7:11.* Correspondence: hua.xu@uth.tmc.edu1School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USAFull list of author information is available at the end of the article© 2016 Zhang et al. Open Access This articleInternational License (http://creativecommonsreproduction in any medium, provided you gthe Creative Commons license, and indicate if(http://creativecommons.org/publicdomain/ze  We accept pre-submission inquiries   Our selector tool helps you to find the most relevant journal  We provide round the clock customer support   Convenient online submission  Thorough peer review  Inclusion in PubMed and all major indexing services   Maximum visibility for your researchSubmit your manuscript atwww.biomedcentral.com/submitSubmit your next manuscript to BioMed Central and we will help you at every step:is distributed under the terms of the Creative Commons Attribution 4.0.org/licenses/by/4.0/), which permits unrestricted use, distribution, andive appropriate credit to the original author(s) and the source, provide a link tochanges were made. The Creative Commons Public Domain Dedication waiverro/1.0/) applies to the data made available in this article, unless otherwise stated.Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 DOI 10.1186/s13326-016-0076-yRESEARCH Open AccessFunctional coherence metrics in proteinfamiliesHugo P. Bastos1, Lisete Sousa2, Luka A. Clarke3 and Francisco M. Couto1*AbstractBackground: Biological sequences, such as proteins, have been provided with annotations that assign functionalinformation. These functional annotations are associations of proteins (or other biological sequences) with descriptorscharacterizing their biological roles. However, not all proteins are fully (or even at all) annotated. This annotationincompleteness limits our ability to make sound assertions about the functional coherence within sets of proteins.Annotation incompleteness is a problematic issue when measuring semantic functional similarity of biologicalsequences since they can only capture a limited amount of all the semantic aspects the sequences may encompass.Methods: Instead of relying uniquely on single (reductive) metrics, this work proposes a comprehensive approachfor assessing functional coherence within protein sets. The approach entails using visualization and term enrichmenttechniques anchored in specific domain knowledge, such as a protein family. For that purpose we evaluate two novelfunctional coherence metrics, mUI and mGIC that combine aspects of semantic similarity measures and termenrichment.Results: These metrics were used to effectively capture and measure the local similarity cores within protein sets.Hence, these metrics coupled with visualization tools allow an improved grasp on three important functionalannotation aspects: completeness, agreement and coherence.Conclusions: Measuring the functional similarity between proteins based on their annotations is a non trivial task.Several metrics exist but due both to characteristics intrinsic to the nature of graphs and extrinsic natures related tothe process of annotation each measure can only capture certain functional annotation aspects of proteins. Hence,when trying to measure the functional coherence of a set of proteins a single metric is too reductive. Therefore, it isvaluable to be aware of how each employed similarity metric works and what similarity aspects it can best capture.Here we test the behaviour and resilience of some similarity metrics.BackgroundOver the last two decades functional annotation systemshave been providing annotations for numerous proteinsas well as other gene products. One of the most com-mon steps used in functional annotation is the use ofsequence alignment algorithms to compare sequences andfind homologies from which functions can be extrapo-lated. Usually, lists of proteins (or other gene products)result from the output of many high-throughput tech-nologies. Therefore, not only is it important to identifycommon functions in those sets of proteins but also toquantify how functionally related the proteins are in order*Correspondence: fcouto@di.fc.ul.pt1LaSIGE, Faculdade de Ciências, Universidade de Lisboa, Lisboa, PortugalFull list of author information is available at the end of the articleto increase understanding of the involvement of biologicalsystems [1, 2].The GeneOntology (GO) project aims to provide gener-ically consistent descriptions for the molecular phenom-ena in which gene products are involved [3]. For over adecade the increasing popularity and consequent growthof GO has led to its adoption and prevalent use inannotation projects. Consequently, this pervasiveness hasenabled andmotivated the development of several seman-tic similarity metrics [46]. Semantic similarity can bedefined as the quantity that reflects the closeness inmean-ing of two concepts in an ontology. However, the semanticsimilarity between two proteins, which can be annotatedwith several GO terms is commonly called functionalsimilarity since it is the functional annotation terms that© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 2 of 11are being compared. More recently, several metrics focus-ing specifically on measuring the functional cohesivenessof a set of proteins (or gene products) through theirannotations have been developed. These metrics for theassessment of functional coherence using annotations arecommonly based on the previously developed groupwisesemantic similarity approaches.One of those metrics, GS2 [7], uses a set-based approachand was developed with computational efficiency in mindto measure gene set functional similarity based on GOterms. The GS2 algorithm ranks annotation terms usinga simple gene counting method and then compares eachgene with the remaining genes with respect to the dis-tribution of functional annotations. This simple measurecan only capture similarity trends within gene sets and cannot precisely assess similarity. Despite that, GS2 has per-formed well when compared with the semantic similaritypairwise measure of [8].On the other hand, another set of three different met-rics: average seed degree, total length and relative seeddegree were developed by [9], for the assessment of func-tional coherence in gene sets based on the topologicalproperties of GO-derived graphs. The procedure leadingto these metrics relies on building GO subgraphs thatsubsume each gene set annotation (for each GO aspect),whereas each node is a GO term and each edge is an is_arelationship between terms. Subsequently, those graphsare further enriched by adding genes, as a new type ofnode, associated to the original GO nodes, and additionalnew edges are created between GO terms whenever theseshare gene annotations. The original term-to-term edgesare weighted using the Information Content [10] differ-ence between both terms while the new edges createdafter addition of the gene nodes to the graph are statis-tically weighted based on the total number of edges inthe graph and the number of supporting genes for eachparticular edge. Hence, this approach handles the issueat hand both from an annotation enrichment perspectiveand an annotation relationship perspective. Steiner treesare then extracted from the graphs and the sum of alledge lengths is minimized for all possible subgraphs. Theaforementioned three metrics are then applied to thesetrees. The average seed degree averages, for a full tree,the counts of the number of genes associated to the seedterms thus reflecting a global measure of enrichment. Onthe other hand the total length metric reflects the over-all relatedness of functions by performing the sum of thelength of all edges in a tree. The relative seed degree met-ric combines the aspects described above as a ratio. Themethodology performs well, but like other GO-evaluationmethodologies, its metrics are dependent on the geneannotation state.The GO-based functional dissimilarity (GFD) metric[11] approaches the problem of functional coherence ingene sets by considering that each gene can encode severalproteins with different functions. In this metric, for eachgene set, only the most common and specific function ischosen as being the most globally cohesive function. Inthis approach, genes are represented as sets to which asimple counting edge-based measure ratio is applied andthat aims at equating both gene relatedness and specificity.The actual GFD is then the minimum of dissimilaritypossible for all representations of a given set of genes.Like the previous metrics this one also depends on thecompleteness of the annotations used in order to pro-vide accuratemeasurements. Furthermore, by consideringonly the most common and specific function in a geneset the authors are effectively discarding potential non-related functions that would cause noise, however at thecost of disregarding multi-functional associations in genesets.Furthermore, and despite not being exactly a system formeasuring functional coherence in gene sets, RuleGO [12]provides a service that statistically compares and char-acterizes two disjointed gene sets. Underneath it runs arule-based system that incrementally iterates the list ofGO terms annotating the two input gene sets and verifiesat each step if a new co-occurrence rule can be created.Much like the typical gene enrichment systems, this sys-tem also performs over-representation tests on the rulescreated and only rules corresponding to a p-value belowa given statistical significance threshold (after multipletesting correction) are considered. This process resultsin multi-attribute rules containing annotation terms andrespective support indexes and evaluation parameters thatcan be used in the characterization of the disjointed genesets. In this methodology rules are evaluated by length(number of genes in a rule premise) representing support,by depth (normalized sum of the GO graph levels whereterms in the rule appear) representing specificity and byan additional quality measure.A different approach is taken by [13] where functionalcoherence in gene sets is assessed with the help of thebiological literature. Here, term-by-genematrices are con-structed with entries derived from weighted frequenciesof the terms across a collection of abstracts (biologi-cal literature). The genes are then represented as vec-tors and the similarity between them is calculated as thecosine of the vector angles. Thus, a pair of genes wouldhave a cosine score of 1.0 if they shared the exact sameabstracts in the collection. Gene sets in this method weredeemed functionally coherent when cosine values above agiven threshold (0.6) were often found with significancesmeasured by a statistical test (Fishers exact test). Thisthreshold was chosen based on the distribution of sim-ilarity cosine scores in 1,000 random gene sets. Hence,functional coherence here is derived essentially from thesupporting literature, thus making the method sensitive toBastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 3 of 11the quality of the document corpus used. Regardless, themethod was used to obtain results similar to those pro-duced by another literature-based functional coherenceassessing method [14].Since functional annotation quality is paramount, [15]developed a system to provide an annotation confidencescore for genome annotations. The system operates onthe basis of a genome comparison approach wherebyannotations in a target genome are scored in compari-son with a reference genome. The gene alignments acrossgenomes are made via the BLAST tool with adjustmentsfor expected number of genes (different organisms havedifferent gene counts) and phylogenetic distance (closergenomes typically share more genes than distant ones).However, actual annotation similarity is derived fromfree-text annotations which are converted into word vec-tors that enable the calculation of a simple cosine simi-larity measure. Both sequence similarity and annotationsimilarity are combined into a single metric by applyingstatistical techniques.Despite the existence of these types of metrics the pro-tein annotation landscape is often very heterogeneous interms of quality, specificity and completeness. Annota-tion quality is related with the annotation method andsource used, e.g. defined by the different GO evidencecodes associated to each annotation. Annotation speci-ficity relates to how specific or general an annotationterm is, and when in a protein set there is a clear dis-proportion between general term annotations and specificannotations, that set can be said to suffer from annotationincompleteness.In this work we concern ourselves mostly with theaspects of annotation completeness and specificity. Giventhat functional similarity is derived from semantic sim-ilarity approaches over the annotation terms, it is alsorelevant to define the concept of annotation agreement asa measure of annotation homogeneity for a given set ofproteins. This metric, will naively measure the coherenceof a given set based on the fraction of shared annotationterms between all proteins in the set, and thus will behighly susceptible to the lack of annotation completeness.We use this measure as a baseline whereas we introduceother metrics to characterize the state of known func-tional similarity of a given set and gauge the potentialstate of annotation incompleteness. Hence, in this workfunctional coherence is defined as a measure of functionalcloseness (similarity) among all proteins in a set given thecurrent functional annotations within that protein set.MethodsA functional annotation is defined as a pairing betweena gene product (protein) identifier and a term providingsome functional description. In this study, only themolec-ular function term annotations from GO were consideredbecause the aim of this work lies closer to studyingone-dimensional annotation (as proposed by [16]) at themolecular functional level in enzymes. Ideally, the func-tional annotations over a given protein set should allow usto infer biological relationships within the set. In order toachieve that, it is convenient to have metrics that enableus to compare how similar (or dissimilar) annotationsare within a given protein set. However, considering theGO DAG structure it becomes apparent that measur-ing functional relatedness via annotation is not a trivialmatter. Therefore, in order to help make such assertionsregarding functional relatedness, three main annotationaspects were considered: completeness, agreement andcoherence.CompletenessAny set of functionally related proteins, in which not allproteins are annotated to the same specificity level, canbe considered to incur in a form of annotation incom-pleteness. Figure 1a) illustrates such a situation. For ahypothetical set of one hundred proteins, only one of thehypothetical annotation terms (besides the root) anno-tates all the proteins in that set. As the nodes get furtheraway from the root term, it can be seen that the num-ber of annotations dwindles until it reaches the leaf terms.And while any given protein does not need to have itsmost specific function represented by a leaf term, it isunlikely that a very generic term (such as a direct child ofthe root term) is a full descriptor of its activity. However,it is not trivial to determine this kind of incompleteness,and only after determining or predicting new functionalactivities can we definitively say that any given protein(or set) was incompletely annotated. Thus, in this workwe limit the definition of completeness to the minimalset of annotations evenly distributed among the pro-teins in a set that characterizes the unifying functions ofthat set.This kind of annotation incompleteness can derivefrom the fact that different protein annotation methodsare used, which provide different degrees of annotationconfidence. Therefore, annotation heterogeneity is cre-ated accordingly to the annotation confidence level givenby each annotation method. For instance, a majorityof the automatic annotation methods typically createmore generic annotations. On the other hand, man-ual curation is more likely to lead to more highly spe-cific annotations. Additionally, the inherent research biastowards more intensively studied model organisms andbiological processes can also help further this state ofincompleteness.AgreementAnnotation agreement can be defined as the fraction ofannotations that are shared in a set of proteins. Hence,Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 4 of 11Fig. 1 Hypothetical GO graph. Terms are represented by nodes where the number within is the number of proteins (of a given set of 100)annotated to that term. There are three situations represented: a annotation incompleteness, b annotation agreement and c annotation coherencethe annotation agreement of a given protein set (S) can becomputed using Eq. 1 as shown below:annotation agreement (S) =( t?i=1xi)× tN (1)with xi as the number of annotations for a term i, N as thetotal number of proteins annotated and t the total num-ber of distinct annotation terms. Therefore, the greaterthe amount of shared annotations the greater is the anno-tation agreement. Figure 1b) illustrates a hypothetical fullannotation agreement situation. In this situation, each oneof the one hundred proteins is annotated to the sameexact annotation term set and thus that hypothetical setachieves maximum or total annotation agreement. How-ever, this is a naive metric that is also overly sensitiveto annotation incompleteness and even small amounts ofnoise.CoherenceNaturally, a set of proteins having a total annotation agree-ment is also functionally similar, to the extent of its mostspecific annotation terms. On the other hand, functionalsimilarity may not need to be so strictly defined. Addition-ally, due to the above mentioned incompleteness issue andthe multi-functional nature of proteins, when measuringfunctional similarity through annotation, it may be usefulto consider just some of the annotations as being func-tionally characteristic of a given protein set. Furthermore,for the purposes of this work, the concept of annotationcoherence is further refined and defined as the fractionof shared annotations that define the core of the func-tional activities that is common and most relevant andthus able to characterize a given protein set, as a func-tional cohesive group. Figure 1c) illustrates a hypotheticalfull annotation coherence situation, where the grey shadednodes represent the functionally more relevant terms, orthe central functional cohesiveness of that set. However,a single metric is too reductive in assessing these (andother) different aspects of annotation that can dictate thefunctional coherence of the annotation space in proteinsets. Therefore, in this work, we use a set of metrics andrespective interpretation strategies relating to these threeaspects of annotation described above in order to exploreprotein (enzyme) annotation spaces.MethodologyWhen it comes to capturing the relationship betweenfunctional and sequence similarity, the different semanticsimilarity metrics often present a similar behaviour, withthe main distinction among them being their resolution.A comparison of several GO-based semantic similaritymetrics [17], found the graph-basedmeasure simGIC con-sistently showing a high resolution (and providing about19-44% increased resolution over the metric it derivesfrom, the simUI metric). In the work presented here, boththe simUI [18] and the simGIC [19] metrics are usedfor assessing functional coherence and establishing sim-ilarity baselines. Both metrics are pairwise, and as suchcalculate the similarity between protein pairs throughBastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 5 of 11their extended set of annotations (direct annotations andancestral terms). Therefore, for a pair of proteins A andB with their extended GO term annotation sets beingGO(A) and GO(B), respectively, simUI is computed bydividing the number of terms in the intersection of GO(A)with GO(B) by the number of terms in their union asshown by Eq. 2.simUI (A,B) = COUNTt?{GO(A)?GO(B)}COUNTt?{GO(A)?GO(B)} (2)Equation 3 is used to compute simGIC which henceis obtained by summing the Information Content (IC)of each term in the intersection of GO(A) with GO(B)divided by the sum of the IC of each term in their union.simGIC (A,B) =?t?{GO(A)?GO(B)} IC(t)?t?{GO(A)?GO(B)} IC(t)(3)As previously mentioned, in the [11] methodology, onlythe most common and specific function of a set is cho-sen as the most globally cohesive function. In this workit is also assumed that not all functional annotations inany given protein set (family) should characterize thatset. On the other hand, considering the frequent multi-functional nature of proteins, in this work, a set of anno-tation terms are selected in each protein set or familyas being its functional characteristic core. Therefore, thestrategy employed in this work to isolate the functionalcharacteristic cores in protein families was to resort toterm enrichment analysis. In particular, a Python imple-mentation of the ubiquitous term-for-term enrichmentapproach was developed. Sincemost of the study sets usedhere are relatively small, and with several terms having lowexpected frequencies (up to five expected observations)the Fisher exact test was used to determine enrichment.Hence, for each annotation term t in a given proteinset a 2x2 contingency table was generated according toTable 1 with N being the number of proteins in the set,M the number of proteins in all considered sets, nt thenumber of proteins annotated with term t in the set andmt the number of proteins annotated with term t in allconsidered sets (mt). The statistical evidence of enrich-ment was then postulated on the basis of the p-valuesobtained from the Fisher exact test being smaller than thechosen statistical significance (alpha). It should be notedthat on the term-for-term approach the graph nature ofGO will lead to a statistical dependency issue. That is,for a given term annotating a certain number of pro-teins, at least that same number of proteins or more willalso be annotated by the parental terms. Among the sev-eral strategies available to mitigate this issue, here, theTopology-based Elimination (Elim) strategy [20] was used.The strategy consists in targeting significant leaves inan annotation graph and iteratively subtracting the pro-teins annotated there from parent terms up to the rootTable 1 Fisher exact tests 2x2 contingency tableSet Backgroundannotated nt mt-ntnot Annotated N-nt (M-N)-(mt-nt)term. After all terms are processed new p-values arecomputed for each term. Thus, this mitigates the sta-tistical dependencies between nodes by downplaying thestatistical significance (and thus importance) of ancestornodes. This is a desired effect, since (for a similar level ofannotation quality) a more specific annotation is prefer-able to a general annotation. Therefore, the Elim methodfavours leaf terms found to be significant and at thesame time removes proteins annotated to significant childterms from the parent terms annotation counts, which inturn attenuates the childrens influence on the parentalterms. Additionally, it should be noted that the computedp-values for the GO terms under this strategy are condi-tioned on their children terms, and thus not independent.Therefore, direct application of the multiple testing the-ory is not possible. It is then preferable to interpret thereturned p-values as corrected or not affected by multipletesting [20].Coherence resilience assaysIn order to test our approach Polysaccaride Lyase (PL)families of the CAZy [21] database were used as a studycase. The protein (module) families in this databaseare organized into 5 different classes: Glycoside Hydro-lases (GH), GlycosylTransferases (GT), CarbohydrateEsterases (CE), Polysaccharide Lyases (PL) and Carbo-hydrate Binding-Modules (CBM). The CAZy databaseversion (c7-2011) that we used in our analysis has about138,000 distinct UniProt identifiers distributed throughthe families in these classes as shown in Table 2. The per-formed assessments were limited to using the UniProtidentifiers in those families because of their direct map-ping to GO term annotations. Thus, for the annotationmapping we have used the GOA [22] annotation files(version 2013-03). Within the CAZy database the PLclass is the one that is better characterized by the Gly-cobiology community, in part due to its more tractableTable 2 Number of protein UniProt identifiers (size) in each ofthe classes in the CAZy database (ver. c7-2011)SizeGH 70227GT 55461CBM 10907CE 8110PL 1766Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 6 of 11dimensions, as can be seen in Table 3. For this reasonwe also have elected to perform our assays using thisclass of families. We have run the coherence resilienceassays that we describe below only for families PL1 toP12, PL16, PL17 and PL22 because these are the onlyones that met the minimal size requirement for ourassaying.In order to perform our assays we subjected each ofthese families (sets) to a degeneration procedure as illus-trated by Fig. 2 where x% proteins in an original proteinset are replaced by random proteins. In our assay theseprotein replacements were obtained randomly from theremainder of the CAZy families. The degeneration pro-cedure was applied in discrete levels of 10% proteinreplacement (from 0% up to 100% protein replacement)to each of the sets. Hence, each original protein family(0% replacement) would gradually turn into a completerandom set (100% replacement). Consequently, for eachfamily the functional similarity is expected to degradeprogressively as the percentage of random replacement(noise) rises. Subsequently, we have used these gradualdegeneration sets to assay the behaviour of each of theAgreement, simUI [18], simGIC [4] and GS2 [7] metricsalong with two novel hybrid metrics, mUI and mGIC thatwe introduce further ahead. For each of the discrete levelsof degeneration (noise) one hundred iterations were runper family. During each iteration, both the original familyand the noise source were randomly sampled for the pro-teins to keep and the replacement proteins, respectively.The similarity was computed at the end of each itera-tion for each of the assayed metrics and then averagedfor the total one hundred iterations. For all the assayedmetrics (simUI, simGIC, mUI and mGIC), the global setresults were obtained by averaging all the term pairwiseresults within each protein set. The resulting averageTable 3 List of the protein families belonging to the PL class inthe CAZy database and their respective size (in number ofUniProt identifiers)Family Size Family SizePL1 491 PL12 80PL2 34 PL13 7PL3 229 PL14 38PL4 45 PL15 10PL5 37 PL16 22PL6 24 PL17 33PL7 82 PL18 5PL8 184 PL20 6PL9 148 PL21 9PL10 84 PL22 42PL11 84 unassigned 80scores are shown in Fig. 3 as plots of similarity (functionalcoherence) as a function of the percentage of randomlyreplaced proteins.Hybrid metricsFor this work two novel functional coherence metrics,mUI and mGIC were developed. They are based onthe combination of semantic similarity metrics simUIand simGIC and a term-for-term enrichment analysis asdescribed by the following algorithm:1. INIT annotationGraph andannotationGraph2. FOR each term IN annotationGraph3. EXECUTE enrichment analysis of term4. IF term enriched5. annotationGraph <- term6. ENDFOR7. mUI <- compute simUI ofannotationGraph8. mGIC <- compute simGIC ofannotationGraphThe annotation graph for a protein set (family) beingmeasured is generated (line 1). For each term in the anno-tation graph (line 2) enrichment analysis using a term-for-term (with Elim adjustment) strategy is performed aspreviously described. If a term is found to be statisti-cally enriched (line 4) it is added to a derived annotationgraph (line 5). When both annotation graphs are pro-cessed (line 6) the simUI and simGIC are applied to theshadow graph (annotationGraph) resulting in the valuesfor the mUI andmGICmetrics, respectively (line 7 and 8).Results and discussionFrom the analysis of Fig. 3 it can be seen, as expected, thatthe similarity reported by each metric generally decreasesas noise (in the form of random proteins) is increas-ingly added (replacing the original proteins) in each ofthe tested PL families. In this study, each of consideredmetrics is scaled on a [0, 1] theoretical range. The aim ofour protein family degeneration assays is to observe twomain aspects for each of the metrics, noise resilience andresolution. With noise resilience we check by how muchthe reported values can vary given the same amount ofnoise. As for resolution we register the difference betweenthe maximum and minimum values it can actually reportduring our assays.The Agreement metric is the least noise resilient met-ric, as can be seen by both the generally low values itreports and the steep declines after adding small amountsof noise to family sets with previously high agreement.Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 7 of 11Fig. 2 Protein set degeneration procedure. For each set (family) a chosen percentage of the set original proteins is replaced with proteins drawnrandomly from outside the setThis property is most evident in mono-functional fam-ilies like PL5, PL16 and PL17 and also PL12 where theintroduction of 10% random proteins produces a sharpdecline in the reported values. This occurs because thisnaive metric only equates the average of annotation termfrequencies in each protein family (or set). Thismetric waschosen and used as the overall baseline.The simUI and its derivative simGIC, as expected, havea similar behaviour because simGIC is a IC-weightedversion of simUI. Furthermore, in the obtained results(Additional file 1) it is noticeable that simGIC presentsa greater resolution than simUI (average range of 0.57against a range of 0.46, respectively, as can be com-puted from Table 4), a behaviour that was also previouslyreported by [17] in their assessment of semantic similar-ity metrics. In contrast, the GS2 metric has the smallestresolution (for the tested sets) of all the tested metricsshowing an average range of 0.18. In addition, to offeringa smaller range of values (and a thus lower resolution) itis important to notice that reported values for this metricfall within the 0.75-1.0 range of similarity. Given that it isexpected for protein (enzyme) families to have function-ally similar proteins it would also be expected (and opti-mal) that these families would display higher coherence.However, when the unadulterated families are consideredsome of them do not provide the necessary annotationssupporting such high global set functional coherence val-ues, especially when considering values produced fromthe 100% randomized sets.The mUI and mGIC (such as the metrics they arederived from) also display, as expected, similar behavioursto each other. Their results measure the enrichment con-tribution relative to the original semantic similarity met-rics. In fact, for most of the tested PL families and theirrespective degenerate sets the reported values are verysimilar. However, unlike the other tested metrics mUI andmGIC are resilient to noise (replacement with randomproteins). That is evident from the gradual curves in Fig. 3which in most families plateau until higher levels of ran-domization and typically only fall abruptly after additionof 90% random proteins. This resilience to noise is con-ferred by the term enrichment step which pre-selects onlythe subset of proteins that are annotated with the termsfound to be statistically significant by the enrichment pro-cedure. Thus, this is an important factor to consider whenanalysing the results provided by these two metrics. Asthey were engineered to capture local (subset) functionalcoherence, for a comprehensive evaluation they shouldonly be used in an analysis that also simultaneously con-siders the annotation coverage within the analysed set.This also explains the observed peaks at high noise lev-els in some of the families (PL2, PL6, PL9, PL11) wherea small number of terms annotates a small subset of pro-teins and thus creates a local similarity effect. That is, athigh levels of random protein replacement the originalfamilies are greatly degenerated because they lose the pro-teins that were characteristic for the identity of that familywhile, on the other hand, randomly gaining less relatedproteins. Hence, if a couple of random proteins beingintroduced happen to be very similar in terms of anno-tations and those terms are also found to be statisticallyenriched, then a new similarity core is introduced whichBastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 8 of 11Fig. 3 Plots of the average similarity as measured by six different metrics. For the first eight PL protein families (from the CAZy database) and theirderived sets. These sets were made by replacing the original proteins with increasing amounts (of 10% increments; 100 iterations) of randomproteins (taken from the CAZy database)Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 9 of 11Table 4 Difference between maximum and minimum valuesreported for each tested metric (Agreement, simUI, simGIC, mUI,mGIC, GS2) against each PL family and iterations of derivedrespective sets created by insertion of increasing amounts ofrandom proteins (from CAZy) into the original familiesMetrics PL1 PL2 PL3 PL4 PL5Agreement 0.122 0.391 0.260 0.368 0.874simUI 0.298 0.497 0.620 0.376 0.650simGIC 0.356 0.539 0.825 0.458 0.853mUI 0.139 0.214 0.671 0.353 0.801mGIC 0.147 0.216 0.672 0.343 0.802GS2 0.137 0.224 0.238 0.177 0.246Metrics PL6 PL7 PL8 PL9 PL10Agreement 0.405 0.201 0.180 0.058 0.178simUI 0.386 0.432 0.548 0.207 0.368simGIC 0.429 0.542 0.660 0.27 0 0.484mUI 0.469 0.501 0.329 0.368 0.564mGIC 0.474 0.505 0.285 0.372 0.559GS2 0.175 0.129 0.224 0.080 0.146Metrics PL11 PL12 PL16 PL17 PL22Agreement 0.229 0.771 0.831 0.869 0.400simUI 0.108 0.644 0.613 0.649 0.443simGIC 0.122 0.838 0.829 0.853 0.521mUI 0.378 0.744 0.903 0.831 0.494mGIC 0.373 0.741 0.905 0.831 0.501GS2 0.054 0.247 0.211 0.248 0.191results in the appearance of those peaks of high similar-ity. However, for this work this behaviour is advantageousbecause the underlying assumption is that each proteinfamily shares core annotations that define the group roleof that set of proteins. Thus, by using a term enrichmenttechnique the purpose is to target and select these coreannotation terms. The proteins annotated by these iden-tified core annotation terms can then, for instance, beused for annotation extension within that set as previ-ously proposed [23]. Thus, according to that proposal, foran hypothetical partially annotated protein set (with anexpected degree of functional relatedness) themUI/mGICmetrics can be used to identify the functional core ofthat set while reporting its functional similarity. If thatcore, reports a high similarity value and also providesenough statistical power (number of associated proteinsequences) it can be used to create, for instance, a HiddenMarkov Model profile model. Subsequently, that modelcan potentially be used as a classifier in order to extendannotations from the core to the sub-annotated sequencesin the original measured protein set.Defining a completeness state and quantitatively mea-suring it is a challenging task considering the complex-ity in generalizing rules needed to detect it. Instead weapproach it only qualitatively by analysing each differentprotein set, case-by-case by relying on domain knowl-edge (confirmed and expected functional associations)and then making empirical assertions about the state ofannotation completeness of each protein set. For that pur-pose we use GRYFUN [24], a web application that we havepreviously developed. This application allows for anno-tation visualization coupled with statistical assessment(term-by-term enrichment) and is used to produce anno-tation graphs like the one shown in Fig. 4. The graphportrayed in Fig. 4 subsumes all the GO terms (from themolecular function ontology branch) annotating a set ofPL10 family proteins. Unlike the typical GO graph wherethe edges point towards their parent terms, here the edgespoint towards their children and have widths proportionalto the number of proteins annotated to each successivechild term. The purpose is to convey the annotation flowand easily be able to identify annotation bottlenecks, orterms where annotation might have stopped despite theexpectation that more proteins in that set could have beenannotated to children terms of these bottlenecks.For the case of the PL10 family set portrayed in Fig. 4the bottleneck annotation is on the term lyase activity.Domain knowledge indicates that this term should anno-tate each protein in this family (e.g. the PL10 family is partof the Polysaccharide Lyases). However, this annotationterm is relatively generic and considering the proportionof proteins not annotated with children of this term (ascan be easily seen from the graph) it is fair to assumesubstantial annotation incompleteness. Additionally, con-sidering the plot in Fig. 3 that represents the degenerationof the PL10 set, it can be seen that the values for mUIand mGIC actually increase along with the degenerationof the set. As previously explained the enrichment processof the mUI/mGIC algorithm considers only a protein sub-set of the target set beingmeasured. Hence, it is importantto consider other metrics (for instance the parent metricssimUI/simGIC) in tandem with these novel metrics for aglobal assessment of functional coherence in a set. Nev-ertheless, these novel metrics allow the identification ofcore activities which can potentially be extended to moresequences within the original set.ConclusionsMeasuring the functional similarity between proteinsbased on their annotations is a non trivial task. Sev-eral metrics exist but due to characteristics both intrinsicto the nature of graphs and extrinsic natures related tothe process of annotation each measure can only capturecertain functional annotation aspects of proteins. Hence,when trying to measure the functional coherence of a setBastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 10 of 11Fig. 4 GRYFUN annotation graph. Annotation of GO molecular function ontology graph generated by the GRYFUN web application for a set ofproteins from the PL10 familyof proteins a single metric is too reductive. Therefore, itis valuable to be aware of how each employed similaritymetric works and what similarity aspects it can best cap-ture. Here we test the behaviour and resilience of somesimilarity metrics.Additionally, we propose a comprehensive approachat determining functional coherence in protein sets(families) based not only on metrics but also statistics(term enrichment) and visualization coupled with domainknowledge-based empirical assessments.Furthermore, we propose two novel metrics mUIand mGIC that combine two of the above mentionedapproaches, semantic similarity metrics and term enrich-ment. The goal is to capture protein subsets within fam-ilies (or other functionally related sets) that characterizethat family (or set), which can subsequently be used forannotation extension for potentially sub-annotated pro-teins within the same family (or set).The proposed approach is modular and can be inte-grated with other annotation methodologies mostly as apre-processing step. In the future, we will be implement-ing both mUI and mGIC (along with other) metrics intoour web application GRYFUN. This will more easily cap-ture the annotation functional cores in protein sets andpipeline them to a custom annotation extension modulebased on HMM profiles that we are currently developing.Additional fileAdditional file 1: Average similarity as measured by six different metricsfor each of the discrete levels of noise. (XLS 50 kb)AcknowledgementsThis work was supported by the Portuguese Fundação para a Ciência eTecnologia through a PhD research grant [PhD grant ref. SFRH/BD/48035/2008 to H.P.B] and three research centers Strategic Projects funding [projectUID/MAT/00006/2013 (CEAUL) to L.S. & UID/MULTI/04046/2013 (BioISI) toL.A.C. & UID/CEC/00408/2013 (LaSIGE) to H.P.B. and F.M.C.] and by theEuropean Commission [BiobankCloud project under the Seventh FrameworkProgramme grant #317871 to F.M.C.]. Furthermore we like to thank Faculdadede Ciências da Universidade de Lisboa for the tuition support for H.P.B.Authors contributionsHPB implemented all metrics and performed the evaluation, and theremainder authors supervised the work. All authors read and approved thefinal manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1LaSIGE, Faculdade de Ciências, Universidade de Lisboa, Lisboa, Portugal.2CEAUL, Departamento de Estatística e Investigação Operacional, Faculdadede Ciências, Universidade de Lisboa, 1749-016 Lisboa, Portugal. 3BioISI -Biosystems & Integrative Sciences Institute, Faculdade de Ciências,Universidade de Lisboa, 1749-016 Lisboa, Portugal.Received: 8 September 2015 Accepted: 17 May 2016Bastos et al. Journal of Biomedical Semantics  (2016) 7:41 Page 11 of 11Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 DOI 10.1186/s13326-016-0072-2SOFTWARE Open AccessPhenoImageShare: an image annotationand query infrastructureSolomon Adebayo1, Kenneth McLeod2, Ilinca Tudose3*, David Osumi-Sutherland3, Tony Burdett3,Richard Baldock1, Albert Burger2 and Helen Parkinson3AbstractBackground: High throughput imaging is now available to many groups and it is possible to generate a largequantity of high quality images quickly. Managing this data, consistently annotating it, or making it available to thecommunity are all challenges that come with these methods.Results: PhenoImageShare provides an ontology-enabled lightweight image data query, annotation service and asingle point of access backed by a Solr server for programmatic access to an integrated image collection enablingimproved community access. PhenoImageShare also provides an easy to use online image annotation tool withfunctionality to draw regions of interest on images and to annotate them with terms from an autosuggest-enabledontology-lookup widget. The provenance of each image, and annotation, is kept and links to original resources areprovided. The semantic and intuitive search interface is species and imaging technology neutral. PhenoImageSharenow provides access to annotation for over 100,000 images for 2 species.Conclusion: The PhenoImageShare platform provides underlying infrastructure for both programmatic access anduser-facing tools for biologists enabling the query and annotation of federated images. PhenoImageShare isaccessible online at http://www.phenoimageshare.org.Keywords: Image annotation, Genotype-phenotype associations, Semantic annotationIntroductionAs reference genomes and large-scale programs to gen-erate model organism mutants and knock-outs are com-pleted, there have been parallel and complementaryefforts from projects such as the International MousePhenotyping Consortium (IMPC) and the Asian MousePhenotyping Consortium (AMPC) to establish and codifyphenotype with genomic coverage [1]. Current pheno-typing efforts typically deliver a variety of images withannotations describing the phenotype on display. Theseare then stored in independent databases associated withthe primary data. These databases may be searched indi-vidually, albeit there is no mechanism for integration,cross-query or analysis, especially with respect to humanabnormality and disease phenotypes. We have developedPhenoImageShare (PhIS) to address this problem. PhIS*Correspondence: tudose@ebi.ac.uk3European Bioinformatics Institute (EMBL-EBI), European Molecular BiologyLaboratory, Wellcome Trust Genome Campus, CB10 1SD Hinxton, UKFull list of author information is available at the end of the articleis a cross-browser, cross-repository platform enablingsemantic discovery, phenotypic browsing and annota-tion of federated phenotypic images. PhIS provides acentralised repository that stores a limited set of meta-data including links to the originating resource, and theannotations generated through the use of PhIS. As such,the complete PhIS system is a federated resource thatincludes the central PhIS database and the repositories ofthe underlying image sources.Resources such as OMERO [2] allow users to store theirimages, but do not provide ontology-enabled tools. Fur-ther, images are often siloed by imaging methodology ordomain [3] and access to multiple different image repos-itories may require bespoke development against multi-ple modes of programmatic access which may evolve asresources change. PhIS achieves this level of integration byusing a lightweight annotation document structure, whichcan then be exposed through standard query engines suchas Solr [4]. PhIS is species and imaging technology neutraland currently provides access to 117,982 images federated© 2016 Adebayo et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 2 of 10from four different data resources with 53,000 regionsof interest (ROI) associated to anatomy or phenotypeontology term annotations. These can be accessed via theweb GUI or programmatically via web services. To datePhIS is populated with images from Drosophila and threedifferent mouse projects.Related workWhilst no existing service or tool is directly comparableto PhIS a number of products may be considered simi-lar. The Yale Image Finder (YIF) [5] offers an interfaceto query over 1.5 million images from open access jour-nals. A user has a few options to restrict or loosen thesearch to image description, whole article or abstract.NCI Visuals Online [6] provides access to about 3000images with the possibility to search the image descrip-tions making use of common text-search functionalitysuch as quotations for exact matches, term exclusion,multiple keywords, defaults to case insensitive search,stemming, et cetera. These are useful resources thataddress a different set of needs from PhIS. Ontology-backed search (i.e. semantic search) is not supported,although an advanced text search exists. YIF does notsupport the display of Regions Of Interest (ROI), onlineannotation, image submission or resource integration viaan API.The Semantic Enrichment of Biomedical Images (SEBI)[7] semantically enriches image meta-data from YIFand enables search over that meta-data. The SEBI plat-form is based upon a number of different modules.Collectively these modules facilitate automatic annota-tion (i.e., semantic enrichment) by using Semantic Auto-mated Discovery and Integration (SADI) [8] enabledservices to pull related information from other resources.When automatic annotation fails, there is provision forcrowd-based annotation. The generated data is storedwithin a triplestore called iCryus [9] that can be queriedthrough a SPARQL endpoint or navigated via a RDFbrowser. Both SEBI and iCyrus focus on DNA/proteinsequence images rather than the phenotypic imagesfound within PhIS. Another difference is the approachto annotation creation. SEBI/iCyrus take the meta-dataassociated with an image and extend it by using otherservices available on the Semantic Web. PhIS operates ata different level, helping to create and publish human-expert-generated annotations. A SEBI/iCyrus-like plat-form for phenotype images would be complementary toPhIS. An iCyrus-like phenotoype tool would pull imagedata from PhIS in same way that iCyrus pulls datafrom YIF.Another framework is proposed by Kurtz et al. [10], withsupport for annotation suggestion and semantic annota-tions, with focus on similarity metrics but with no link toan application.Wang et al. [11] have addressed the need for anontology-assisted image-annotation tool and have pro-duced software that supports RadLex [12] backed annota-tion of radiology images.While the project was successful,its scope is restricted to radiology annotations and is notaccessible online to external users. It does not aim to offerfurther integration with other resources through any API.A set of commercial desktop applications such as Osirix[13] and Amira [14] offer segmentation assistance andplain-text annotations, either integrated or through plu-gins. AISO [15] is a desktop tool that was developedfor plant image annotation and supports ontology use.AISO is restricted to the plant ontology, which it accessesthrough its own web service. OMERO offers annotationsupport both on the web client and the desktop applica-tion but no ontologies are integrated. The OMERO serveris not an online service but a server distribution so everygroup or institution needs to host its own server. Phe-noImageShare is a complementary resource that can beused in association with OMERO hosted images.There is a wide variety of public image portals [3]with some focusing on phenotype images: IMPC portal,EMAGE [16, 17], The Cancer Genome Atlas [18] to men-tion a few. All of these focus on species- or project-specificimages and image submission is not open to the outsideuser. IMPC and EMAGE support ontology annotation buthave limited or no support for region of interest (ROI) dis-play on the original image, however EMAGE does supportspatial mapping onto a standard atlas for the purposes ofspatial comparison and query.We have identified the need for a public and easy to useweb service that can federate cross-species, cross-projectimages, with open image submission and powerful seman-tic search. There is also a clear need for an online imageannotation tool with support of ontology terms and differ-ent ontologies. To the best of our knowledge such a tooldoes not exist.MethodsThe PhIS platform consists of three main software lay-ers: the User Interface (UI), the integration layer andthe backend services. The UI components provide anintuitive Graphical User Interface (GUI) to query cross-platform image meta-data. The GUI also provides a basicontology-enabled annotation service to allow image own-ers, and third parties to annotate images using ontologiesfor their own use and also for sharing images and anno-tations between consortia and collaborators. The inte-gration layer is the one which consolidates access to thedifferent backend services into one access point, used bythe GUI. The backend services provide API methods toquery and annotate the data as well as a data importmech-anism. The architecture described is also represented inFig. 1.Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 3 of 10Fig. 1 PhIS architecture. The different software components part of the PhIS platform. The arrows describe the collaboration of the components andthe direction of the function calls. For example the UI uses the methods provided by the IQS but the IQS alone cannot trigger any actions in the UI.The annotation submission API needs to be secured in order to keep the integrity of the database but the query API is not restrictedImage discovery infrastructureThe query API offers a simple REST-like interface toaccess the data in multiple ways. Approximate searchmethods are provided with the purpose of image discov-ery but also specific interfaces for example search by idare available. The API provides an endpoint for query-suggestion options on an input string. This was designedto follow the following order: 1) exact matches, 2) phrasesthat start with the given string as a word, 3) phrases thatstart with the given string as part of the first word, 4)exact match in a word in the phrase, other than at thestart of it, 5) matches where another word in the phrasestarts with the given string.To illustrate this through anexample, for the string eye, the suggestions will come inthe following order: eye (Case 1), eye hemorrhage (Case2), eyelids fail to open (Case 3), TS20 eye, TS21 eye,left eye, right eye, narrow eye opening, abnormal eyedevelopment (Case 4), abnormal eyelid aperture (Case5). We have implemented this after a series of formativesessions with users. We achieve this sorting by apply-ing different boost factors to Solr text fields tokenizedin different ways. Text matching is case insensitive in allcases.The database, XML schema (XSD) and Solr represen-tations share a common high-level schema such that theinformation storage is split three ways: image informa-tion, channel information and ROI/annotation informa-tion. This is represented in a simplified way in Fig. 2.Provenance information is an important part of theschema. PhIS currently implements a pragmatic sim-ple model, covering image/annotation creator, annota-tion editor, image and annotation creation/last edit date,repository or resource of origin or publication if one isprovided for the image.The API offers JSON responses, which is the stan-dard for modern APIs, is straight forward to consumeby the UI, has the advantage of human readability andis also the best fit for our Solr-backed infrastructure. Asthe API gains traction and users require other formats,new response types can be evaluated and provided. Theapproach used to create the datamodel for the Annotationand Image Markup project [20] has been applied to PhIS.Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 4 of 10Fig. 2 PhIS schema. The image entity is the core meta-data objectand stores generic, immutable meta-data such as imaging procedureand details, data source, credits and sample information, genotype orsample preparation techniques. The channel entity stores visualizationinformation, such as tags or labels and genomic information whenneeded, such as for expression images. The ROI entity holds both thecoordinates, within the given image, and the annotation valuesThe implementation is in Java and the query functionalityis primarily via Solr. Solr is built upon Lucene [19], whichis an open source indexing and search engine. Lucene-based technology powers a number of existing imagerepositories, e.g., OMERO and Yale Image Finder both useLucene to search for images. The authors have experienceof using Solr to build image repositories from their workon the IMPC portal. Solr offers fast, flexible, robust, opensource search solutions with scaling options such as Solrcloud. Through a varied array of features Solr enabled usto build a rich search and navigation experience with littleresources.Spatial annotationsMany phenotypes occur in a particular spatial loca-tion, which can be described using either an anatomy(ontology) term or by being mapped onto a biomedicalatlas [20]. Currently, PhIS only supports anatomy-baseddescriptions, with atlas-based descriptions targeted for afuture release. Anatomy-based descriptions are simpler touse, but less precise.Should a PhIS submission be missing a spatial descrip-tion, it may be possible to infer an appropriate anatomyterm from a supplied phenotype term. For example, giventhe phenotype term cataract the anatomy term eye canbe inferred. This inference relies upon the bridge ontolo-gies provided by the Monarch initiative [21]. Inferredspatial annotations are pre-computed and stored withinSolr allowing them to be queried by a user.User interfacePhIS delivers its functionality through a highly respon-sive and easy-to-use web interface built upon standardtechnologies such as Python, Django, JavaScript, Asyn-chronous JavaScript (AJAX), Bootstrap [22] and Flat-UI[23]. Portability, reusability and responsiveness are at thecore of GUI design considerations.Throughout the development of PhIS a series of for-mative evaluation sessions have been used to determineand then prioritise requirements. This included the cre-ation of a series of tasks against which the functionalityof the search capability can be tested. Additionally thesearch functionality has undergone a small scale sum-mative evaluation. Feedback from this has been inte-grated within the current beta release. Developmentof the annotation tool started after the search func-tionality, and so the annotation tool is still undergoingan iterative process of formative evaluation and devel-opment. A comprehensive summative evaluation thattests the complete workflow (search and annotation)will be undertaken when the annotation tool reachesmaturity.There are four distinct elements to the PhIS GUI, andthis section discusses each one in the order a user encoun-ters them in a typical workflow.Landing pageWhen arriving at www.phenoimageshare.org a user isgreeted by a visual summary of the data held withinthe PhIS repository that sits below a search box. Thevisual summary consists of three pie charts that collec-tively quantify and classify the types of images stored.The three dimensions visualised are imaging method,sample type (mutant vs. wild type) and image type(expression vs. phenotype). Clicking on a block withinone of the charts performs a query. For example, click-ing on the macroscopy block will display all the imagesthat have meta-data indicating they are macroscopyimages.Alternatively, the user can use the search box to queryPhIS. A query may consist of free-text, gene or allelesymbols, anatomical or phenotypic terms from standardontologies. An query-suggestion facility integrated withinthe search box provides the user with a drop-down listof terms predicted from the set of existing annotations.Search results are displayed in real-time on a dedicatedpage.Search interfaceThis page (see Fig. 3) displays a list of images that meet theusers query criteria, and provides the ability to filter thatlist or navigate through it. For each image in the list a briefsummary of the images meta-data is displayed alongsidea thumbnail.Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 5 of 10Fig. 3 Query interface with ontology-based search and faceting functionality. Search results for a phenotype (abnormal epidermis stratum basalemorphology) (a) is filtered by development stage (postnatal) (b) and confocal microscopy imaging method (c). Example query and help on usingfacets (d) are also providedTo reduce the images returned by a query the user canapply a range of predefined filters or a faceted search. Boththe filters and the search are facet-based and meta-datadriven. Nested checkboxes allow the user to reduce theimages displayed using filters including imaging method,stage, species and resource (i.e., the online resource fromwhich PhIS obtained the image). Faceted search is avail-able for anatomy, gene/allele and phenotype. If the usersearches for heart in the anatomy facet search box, onlythose images from the original query with an anatomyannotation featuring the term heart will be displayed.The filters and faceted search can be combined to deliverpowerful customised queries.To find an image of interest the user can either adjustthe filters/search or (s)he can undertake a completely newquery using the main search box (now in the menu bar).When the user identifies an image of interest in the searchresults, clicking on that image reveals more information inthe Image Interface page.Image interfaceIn the Image Interface (see Fig. 4) a single image is dis-played alongside a more comprehensive set of meta-datafor the image. Meta-data shown includes data capturedfrom the source (e.g., genotype, phenotype and prove-nance) and annotations generated by users of PhIS. Themeta-data contained within PhIS is a subset of the dataavailable at the originating resource, thus PhIS provides alink back to the originating resource enabling the user toinvestigate further.Image annotations can be exported in a variety of for-mats, with new formats to be added. The user can addtheir own annotation to this image using the PhIS annota-tion tool.Annotation interfaceThe annotation interface allows the users to semanticallyannotate an image. Annotations can apply to the wholeimage or to a region of interest within the image. ROIs areindicated by the user drawing a rectangle upon the image.This rectangle is treated as part of the annotation and isstored within PhIS for later reproduction. When creatingan annotation, the user has the ability to select a seriesof ontology terms obtained through the BioPortal widget[24]. All annotations submitted through the PhIS inter-face will appear on the website and become searchableinstantly.Currently the annotation tool only supports low resolu-tion images; however, it has been implemented with thegoal of displaying high resolution images via OpenSead-ragon [25]. OpenSeadragon is an extensible JavaScriptlibrary providing high quality zooming functionality thatwill enable PhIS to support high resolution images.Because it works well with OpenSeadragon, and requiredminimal extension, FabricJS [26] provides the function-ality for drawing ROIs on the images. Other possibleoptions (e.g., Annotorious [27] or AnnotatorJS [28]) wererejected because their support for OpenSeadragon wastoo immature at the time of testing.Figure 5 presents a screenshot of the Annotation Inter-face in use.Data and ontologiesTo make data available through PhenoImageShare batchsubmissions are generated in XML format by theresources wishing to submit data to PhIS. Our datareleases are versioned and the current one is listed inthe lower right corner of the page, which links to moredetails about each release. Old releases can be accessedAdebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 6 of 10Fig. 4 Detail view, showing meta-data, provenance and annotations associated with one of the images returned in the search shown in Fig. 2programmatically, through the API. Data sources providePhenoImageShare with up-to-date XML exports of theirdata, following the PhIS schema (XSD). Producing theXMLwill vary based on the amount of data exported whilethe processing on the PhIS server only takes a few min-utes. However, we do data releases so there might be a fewweeks lag between themoment the export is ready and themoment the data is published live. It is up to the data sub-mitter how often updated XML exports are submitted.Wethen perform XML and semantic validation, followed bydata enrichment before indexing. This includes additionof synonyms and ancestor relationships to the Solr indexallowing an improved semantic search. Some examples ofsemantic validation include checking if the term providedas an anatomy annotation comes from an anatomy ontol-ogy or checking if the label provided with the ontologyid matches. Ontology curators sometimes update the pre-ferred labels and databases have trouble keeping up withthat. As part of our import process we select the mostup to date label and index the old one as a synonym. Ifthe ontology term was deprecated and a replacement oneis suggested we use the replacement term. In order toFig. 5 Annotation interface in edit mode. Context menu (a) is available to user via right-click on selected terms from an ontology (b). User appendsselected ontology terms to the drawn region of interest (c)Adebayo et al. Journal of Biomedical Semantics  (2016) 7:35 Page 7 of 10achieve this semantic manipulation of the data we makeextensive use of the OWL API [29] and the Elk reasoner[30] to precompute relations that we then index in Solr.This approach offers more restricted semantic capabilitiesthan RDF/SPARQL, but it is faster to set up and covers ouruse-case needs, both in terms of semantic and text-basedsearch capabilities.DataThe current PhIS release contains data from four sources,presented with more details in Table 1. The TRACER database [31] contains imaging forembryos carrying a transgenic insertion generated bythe Sleeping Beauty transposon-based system. Images from the Wellcome Trust Sanger Institute,generated as part of KOMP2 [32] focus on singlegene knock-outs. Genotype, anatomy and phenotypeannotations are provided for these data. The EMAGE database provides embryo geneexpression images. Virtual Fly Brain (VFB) [33]. The VFB dataset offersan interesting use-case for cross species integration,but also adds value to our anatomy coverage with itsfocus on neuroanatomy and gene expression in theadult Drosophila melanogaster brain.OntologiesTo add semantic value (i.e. synonyms, hierarchical rela-Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 DOI 10.1186/s13326-016-0078-9RESEARCH Open AccessThe Proteasix OntologyMercedes Arguello Casteleiro1, Julie Klein2 and Robert Stevens1*AbstractBackground: The Proteasix Ontology (PxO) is an ontology that supports the Proteasix tool; an open-sourcepeptide-centric tool that can be used to predict automatically and in a large-scale fashion in silico the proteasesinvolved in the generation of proteolytic cleavage fragments (peptides)Methods: The PxO re-uses parts of the Protein Ontology, the three Gene Ontology sub-ontologies, the ChemicalEntities of Biological Interest Ontology, the Sequence Ontology and bespoke extensions to the PxO in support of aseries of roles: 1. To describe the known proteases and their target cleaveage sites. 2. To enable the description ofproteolytic cleaveage fragments as the outputs of observed and predicted proteolysis. 3. To use knowledge about thefunction, species and cellular location of a protease and protein substrate to support the prioritisation of proteases inobserved and predicted proteolysis.Results: The PxO is designed to describe the biological underpinnings of the generation of peptides. Thepeptide-centric PxO seeks to support the Proteasix tool by separating domain knowledge from the operationalknowledge used in protease prediction by Proteasix and to support the confirmation of its analyses and results.Availability: The Proteasix Ontology may be found at: http://bioportal.bioontology.org/ontologies/PXO. Thisontology is free and open for use by everyone.Keywords: Ontology, Proteasix, Protease prediction, Peptide, Cleaveage site, Open biomedical ontologiesBackgroundProteases are enzymes that catalyze peptide bond cleav-age and this activity can lead to the generation of proteincleavage fragments or peptides. Proteases have a widespectrum of specificity [1]. The human genome encodesover 550 different proteases, participating in many dif-ferent biological processes, including protein degradation,immunity response, regeneration or cell division and areinvolved in diseases such as cancer, inflammation andcardiovascular disease [2, 3].Body fluids (e.g. serum, urine, cerebrospinal fluid)contain thousands of protein fragments and disease-associated peptides. The proteolyticmechanisms that leadto the generation of these fragments may be associatedwith diseases, and are not well described in the literature.Further insight into the proteases implicated in peptidegeneration may help in understanding some diseases.*Correspondence: Robert.Stevens@manchester.ac.uk1School of Computer Science, University of Manchester, Oxford Road, M139PL Manchester, UKFull list of author information is available at the end of the articleThe particular function and specificity of each proteaseare defined by their binding to a characteristic aminoacid motif that forms a cleavage site in the protein tar-get [4]. Knowledge about proteases and their substratesand cleavage sites is scattered across publications anddatabases. These different resources do not permit cleav-age site information to be retrieved from peptide sequenceinput automatically and thus elucidating the proteasesimplied in peptide production is difficult.The Proteasix tool [5, 6], is an open-source peptide-centric tool that can be used to predict automatically theproteases involved in the generation of proteolytic cleav-age fragments (peptides). Proteasix is a tool that usesprotease/cleavage sites (CS) associations established byeither observations or predictions to suggest the proteasesimplicated in the generation of a peptide. Proteasix doesthis by using the N- and C-terminal sequences of pep-tides that are reconstructed using information from theUniProt knowledge base [7] to identify the possible pro-teases that were involved in their generation [5]. Obser-vations of protease/CS combinations were extracted from© 2016 Arguello Casteleiro et al.Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 2 of 7CutDB [8], UniProt and the literature. When a previousobservation has not been established, Proteasix calcu-lates the probability of protease/CS association by usingthe MEROPS [9] and BRENDA [10] databases. Pro-teases exhibit varying binding affinities for amino-acidsequences, ranging from strict restriction to one or fewcritical amino-acids in given positions, to generic bind-ing with little discrimination between different aminoacids [5].The predictions currently made by Proteasix are agnos-tic as to the taxon of the organism whence the peptidescome, the cellular location of the predicted proteases andthe proteins theymay cleave. Also, the function of the pro-teases, e.g., whether they are an endo- or exo-peptidaseis not taken into account. This is the kind of knowledgean ontology is able to provide. Thus the new version ofProteasix uses the Proteasix Ontology (PxO) to make thisknowledge available to its algorithm.We go on to describethe PxO and its role in Proteasix.Competency questions for the PxOThe PxO is written in theWebOntology Language (OWL)[11] using the Protégé [12] version 5.0.0 beta 17 editor.In creating PxO we wished to undertake as little de novoontology development as possible and to take advantageof the work already done in annotating gene products withthe Gene Ontology (GO) [13]. This implied a strategy ofre-using relevant Open biomedical Ontologies Consor-tium [14] (OBO) ontologies where possible, together withrelevant annotations. The choice of which of the OBO touse was driven by a set of competency questions (CQ)that the PxO should fulfil. Once chosen, relevant por-tions of the ontologies were taken and extended in a waythat accommodated the CQ, making appropriate commit-ments to the ontology used. The resulting PxO was thenevaluated against the CQ.To obtain the observed and predicted proteases respon-sible for the generation of peptides, the PxO needs toanswer the following CQ:1. What are the known protease and their target cleav-age sites (observed and/or predicted)?2. For a given peptide and protein from which it wasderived, what are the cleavage sites that led to its pro-duction and is it the product of observed or predictedproteolysis?3. What are the function, species and cellular locationfor both proteases and their substrate proteins?4. For a given protease, what are its cleavage site speci-ficity?5. Given an amino acid, what are its biochemical prop-erties?6. For a protease predicted to have generated a peptide,what are its function and the processes in which it isknown to participate?The Additional file 1 provides the ELK reasoner timesand shows the SPARQL SELECT queries for the CQand the execution times for the CQ using JENAARQ [15].Reuse of ontologies fromOBOTo enable these competencies to be answered the PxO re-uses parts of some of the OBO; PxO uses the OWL [11]versions. After downloading the OWL files, a selectionof class names (without deprecated classes); class expres-sions; class definitions; and annotation assertions wereextracted. Where only a portion of the source ontologywas required to support the CQ in PxO, we program-matically extracted a top-module [16]. A top-module isused as in the PxO only a restricted query supportinga CQ needs to be answered, rather than a query thatnecessitates all entailments from a signature to be pre-served. The following OBO or their parts were used inPxO:1. The Protein Ontology (PRO) [17]  Reuseof Protein(PR:000000001) and proteolyticcleavage product(PR:000018264) that are bothsubclasses of PROs amino acid chain (PR:000018263). In order to follow the PRO annotationguidelines [18], the relationships participatesin; located in; and has function weresubstituted with their Relationship Ontologyequivalents. The use of the PRO supports all theCQ.2. Relationship Ontology (RO) [19]  Where possi-ble, PxO uses object properties from RO. For PxO,this includes has_function, has_location,participates_in and only_in_taxon.3. the three Gene Ontology (GO) sub-ontologies [20] First, a class name extraction was performed based onthe three GO namespaces cellular component;molecularfunction; andbiological process.As an example of usage in the PxO, the GOclass peptidase activity (GO:0008233) wasused to define protease molecular function, whileproteolysis (GO:0006508) was used to describethe biological process of peptide production. Use ofthe GO supports CQ 1, 3 4 and 6.4. Chemical Entities of Biological Interest Ontology(ChEBI) [21]  Reuse of chemical entity(CHEBI: 24431) that has as subclass molecularentity(CHEBI: 23367). PROs amino acid chain(PR:000018263) and amino acid were madesubclasses of ChEBIs molecular entity(CHEBI:23367). The twenty amino acids are also taken fromChEBI. Some amino acids are interchangeable ata certain CS position, for they may have identicalbiochemical properties. This supports the answeringof CQ 5.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 3 of 75. Phenotypic Quality Ontology (PATO) [22]  Classesfrom PATO were reused to describe the prop-erties of the amino acids. Hence, classes such aselectric charge (PATO:0002193), polarity(PATO:0002182) as well as their superclasses likemolecular quality (PATO:0002182) andsubclasses such as negative charge (PATO:0002196) were extracted. The twenty amino acidsfrom ChEBI are classified taken PATOs descendantsfrom molecular quality and side chainstructure, which is outside of PATO. This helps toanswer CQ 5.6. the Sequence Ontology (SO) [23]  Cleavagesite regions and C- and N-terminus of polypep-tide sequences were described using polypeptideregion (SO:0000839) to describe Cleavage siteregion. Moreover, the key classes to link proteinsfrom Uniprot with gene names were described usinggene (SO:0000704) along with its subclass proteincoding gene (SO:0001217). Based on superking-dom and subclasses, i.e. upper-levels of the UniPro-tKB Taxonomy, the gene names are classified, andthereby, obtaining a hierarchy with three levels. Theseclasses were used to support the CQ 1, 2, and 4.7. The PRO proteins are organised based on taxonorganism, and therefore, new classes under the PROproteinclass were created according to the upper-levels of the UniProtKB Taxonomy [24]. These classeswere used to support CQ 3.8. GALEN ontology [25]  A medical ontology outsideof OBO, which can be downloaded from BioPortal[26]. Reuse of the class KnowledgeStatus and therelationship hasKnowledgeStatus to representobserved or predicted proteolysis. To describe thelevel of confidence associated with a predictedproteolysis, the relationship hasConfidenceLevelStatus the class ConfidenceLevelStatus were also extracted. these classes were usedto support CQ 1.PxO axioms and axiom patternsPeptide and protein: A peptide, also known asproteolytic cleavage productin PRO, isdescribed in the following way in PxO (all OWL frag-ments are represented using Manchester OWL Syntax[27]):Class :  p r o t e o l y t i c c l e a v a g e product SubClassOf : amino ac i d chain  , output of  some p r o t e o l y s i s , d e r i v e s from  some p ro t e i nKnowledge patterns are representations which capturerecurring structure within and across ontologies [28]. Andtherefore, knowledge patterns (patterns for short) canbe seen as generalisations where entities are replacedby variables [29]. The above pattern conteins two vari-ables ?Peptide and ?Protein. When the pattern is instan-ciated, the variables will be replaced with entities. Forexample, for peptide with ID 1023927 the variable?Protein will be replaced with the parent protein fromwhich the peptide is derived, which is PROs Collagenalpha-1(I) chain (PR:P02452). It should be notedthat a pattern (a.k.a. axiom pattern) does not neces-sarily coincide with the notion of ontology designpattern (see [29]). A pattern can also represent a setof OWL axioms. And thus an ontologys class expres-sions or definitions can be easily obtained instanciatingpatterns.The description of proteins in the PxO follows guide-lines for the PRO [18] and it uses the RO objectproperties has function to relate a protein toits GO molecular_function, has location torelate a protein to its location in a GO cellularcomponent and participates in to relate aprotein to the GO biological process inwhich it may participate. Proteins are thus described inthe PxO with the following axioms:Class : p r o t e i nSubClassOf : amino ac i d chain  , l o c a t e d in  some ce l lu l a r_component , p a r t i c i p a t e s in  some b i o l o g i c a l _ p r o c e s s , has func t ion  some molecu l a r _ func t i onThe aim in PxO is to describe protein types takenfrom Uniprot described by terms from the Gene Ontol-ogy according to PRO guidelines. Given the PRO proteinCollagen alpha-1(I) chain (PR:P02452), the fol-lowing two axioms are made by instanciating the abovepatternClass :  Co l l agen alpha ?1( I ) chain SubClassOf : l o c a t e d in  some  e x t r a c e l l u l a r reg ion  , l o c a t e d in  some  e x t r a c e l l u l a r space UniProtKB/Swiss-Prot [30] contain more than one hun-dred thousand protein records for metazoa (i.e. mul-ticellular animals) that were reviewed, and manuallyannotated. Some of these proteins have isoforms, i.e.alternatives to the canonical sequence. The PxO isreleased currently with the PxO metazoa that contains139 720 OWL protein classes (UniProtKB SwissProt andIsoform sequences), 4 591 OWL organism taxons, andwith 89 846 OWL gene classes. Each type of OWL Class(protein, gene, and organism taxon) is generated using itsown axiom pattern.Both proteins and peptides have N-terminus- and C-terminus regions. Thus, axioms were introduced to refinePROs amino acid chain:Arguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 4 of 7Class :  amino ac i d chain SubClassOf : mo lecu la r e n t i t y  ,h a s _pa r t some C?terminus reg ion  ,h a s _pa r t some N?terminus reg ion  ,h a s _pa r t some po l ypep t i d e_ r eg i on ,on l y_ in_ t axon some organismCleavage sites: On the one hand, a cleavage site (CS) ispart of a protein. On the other hand, a protein may haveone or more CS. Therefore, two patterns were created.PxO is released currently with 16 273 OWL CS classesfor known cleavage sites, which are associated with 5 084distinct protein classes.Protease: An equivalence axiom allows any proteinwith a GO annotation for peptidase activity(GO:0008233) or one of its children to be recognised as a pro-tease by Proteasix. The axioms for representing proteasesare as follows:Class : P r o t e a s eEquivalentTo :p r o t e i n and (  has func t ion some  p e p t i d a s e a c t i v i t y  )SubClassOf : i npu t of  some p r o t e o l y s i sIn the same vein, it is straight-forward to define aclass such as exopeptidaseby using the GO anno-tation for exopeptidase activity (GO:0008238).Definitions for endopeptidase, aminopeptidaseand carboxypeptidase are easily made by exploitingthe GOs catalytic activity hierarchy.Proteolysis: Taking the GOs proteolysis, it is feasi-ble to create additional axioms to describe the biologicalprocess that has input participants of a substrate protein,a protease and has output participants proteolyticcleavage fragmentClass : p r o t e o l y s i sSubClassOf : p r o t e i n metabo l i c process  , has input  some Protease , has output  some  p r o t e o l y t i cc l e a v a g e product  , has input  some ( p r o t e i n and( h a s_pa r t some  C leavage s i t e reg ion  ) )In the PxO there is a clear distinction between:a) observations of protease/CS combinations extractedfrom the literature (e.g. CutDB [8]); and b) predic-tion of cleavage based on proteases cleavage site speci-ficity from MEROPS [9] or exopeptidases cleavage siteannotation assertions with the catalytic activity fromBRENDA [10], which captures how likely it is for anamino acid to be present or absent in a certain posi-tion close to the CS. To represent this dichotomy,two classes were introduced: observed proteolysisand predicted proteolysis. To create their classdefinitions, the status of the proteolysis is indicated.Observed proteolysis is defined as:Class :  Observed p r o t e o l y s i s EquivalentTo :p r o t e o l y s i s and ( hasKnowledgeSta tussome  Observed s t a t u s  )In the PxO there are 20 229 observed proteolysis and 329 predicted proteolysis createdfrom two patterns.Annotation assertionsprovide the means toassociate aditional information with an entity, like anexact synonym (oboInOwl:hasExactSynonym), a databasecross reference (oboInOwl:hasDbXref ), or a definition(IAO:0000115). When a protein taken from the UniPro-tKB has a MEROPS specificity matrix, annotation asser-tion axioms are used in PxO to represent this data.However, the probability for a protease to cleave a proteinsubstrate is calculated outside of the PxO, although usingthe annotation properties and values in the PxO.PxO in useThere are two methods to find the protease classes in thePxO: 1) use an automated reasoner like ELK [31] to inferwhich proteins are proteases (see the Protease definedclass in the previous section); or 2) use the SPARQL 1.1query language [32] to create a SELECT query (Q0) thatretrieves the OWL protein classes with a GO assertionfor peptidase activity (GO:0008233) or any of itschildren. Likewise, using DL queries with ELK or SPARQLSELECT queries, the proteins that are endopeptidase,aminopeptidase or carboxypeptidase can beobtained.The essence of the Proteasix algorithmis givenbelow, which exploits the PxO ontology, and uses thecompetency questions CQ. The algorithm assumes thatprotease cleavage of substrate proteins is directed by shortamino acid motifs, from two to eight amino acids ofthe type (Pn . . . )P1 - P1(. . . Pn), with the scissile bondbetween P1 and P1 residues [5]. Residues towards theN-terminus of the substrate are on the non-prime sideand numbered as P1 P2 P3 P4 and so on; while residuestowards the C-terminus are on the prime side and num-bered as P1 P2 P3 P4 and so on [33].STEP 1: User input  For each peptide, the end-userprovides: a) the peptide identifier; b) the UniProt Acces-sion Number (AC) or identifier (ID) of the parent proteinfrom which the peptide is derived; c) the start amino acidposition with respect to the parent proteins sequence, i.e.P1 of the N-terminus CS; and d) the end amino acid posi-tion with respect to the parent proteins sequence, i.e. P1of the C-terminus CS.STEP 2: Reconstruct N- and C-terminus CS  EachOWL protein class created from the UniProtKB (Swiss-Prot/TrEMBL) has among others the following anno-tation properties: oboInOwl:id for AC; oboInOwl:hasAlternativeId for ID; and PxO:hasSequenceArguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 5 of 7where the amino acid sequence (sequence for short) of theprotein is stored. SPARQL query CQ2-2 in the Additionalfile 1 exemplifies how to obtain the amino acid sequencefor protein P02768 (PR:P02768). Outside of PxO, andusing the protein sequence, the peptide sequence for aninput peptide is extracted, and the N- and C-terminus arereconstructed, i.e. eight amino acids or fewer if close tothe beginning or end of the proteins sequence. The out-put of this step is the creation for each input peptide of anOWL peptide class along with an OWL N-terminus classand an OWL C-terminus class.STEP 3: Observed cleavage  Using the class expres-sion polypeptide region SubClassOf partof some amino acid chainOWL CS classesand OWL protein classes are linked, and therefore, thisclass expression represents that a CS is part of a pro-tein. An OWL CS class also has annotation propertiesfor storing the CS sequence and the P1 and P1 values.SPARQL query CQ2-1 in the Additional file 1 illustrateshow to retrieve for protein P02768 (PR:P02768) theobserved CS regions where the P1 value is 25. To makethe retrieval process more efficient, firstly, for eachpeptide, the sequence of its corresponding OWL N-terminus class and OWL C-terminus class are matchedagainst the OWL CS class sequence. If successful, amore detailed match is triggered. A successful outcomeof this step is an instantiation of the axiom pattern?peptide SubClassOf output of some(?proteolysis and (hasKnowledgeStatussome Observed status)).STEP 4: Predicted cleavage  For the OWL N-terminus and C-terminus classes with sequences thatremain unmatched after the previous step, a protein cleav-age prediction is attempted. SPARQL query CQ4 in theadditional file can be generalised by replacing the PROsprotease P08253 (PR:P08253) for a parameter ?C, andthus, the results of the query will be the set of proteasesfor which prediction can be undertaken by exploiting theMEROPS cleavage site specificity matrix. The probabilitycalculations are outside of PxO. Firstly, the probability ofcleavage is estimated from the proteases MEROPS speci-ficity matrix [9] using a log-likelihood. If the probabilityis above the 99th percentile of the population distributionof all possible sequences, then the sequence is taken asstatistically matched. A confidence level is then assignedto the matching, using levels from a simulation distribu-tion of thematching step. Secondly, for the sequences thatobtained a low/medium confidence level prediction or stillhave no prediction, BRENDA [10] exopeptidases catalyticinformation is used and a second prediction is attempted,assuming that after endopeptidase cleavage, and exopep-tidase cuts the free extremity (i.e. C- or N- terminusCS). A successful outcome of this step is an instantia-tion of the axiom pattern ?peptide SubClassOf outputof  some (?proteolysis and (hasKnowledgeStatus some Pre-dicted status)).Further validation of the observed and predicted prote-olysis (step 3 and 4) is accomplished by checking in PxOthat: a) If the source organism for the protease and thesubstrate are the same; and b) if both the protease and thesubstrate are co-located.Positive example: In the additional file, there isan SPARQL SELECT query (CQ3-2) that investi-gates whether the two above-mentioned conditions aremet for PROs substrate Serum albumin (PR:P02768)and PROs protease 72 kDa type IV collagenase(PR:P08253). The query indicates that both proteaseand substrate protein may come from the same taxonHomo sapiens (NCBI:9606) and may have the fol-lowing common co-locations: nucleus(GO:0005634);extracellular region(GO:0005576);extracellular space(GO:0005615). This is a pos-itive corroboration of co-location. Indeed, there is evi-dence that the cleavage is observed.Negative example: Reusing the SPARQL SELECTquery (CQ3-2) with different protease Neutrophilelastase(PR:P08246) and substrate proteinATP-binding cassette sub-family Amember 6(PR:Q8N139). The query indicates that bothprotease and substrate protein may come from thesame taxon Homo sapiens (NCBI:9606), howeverno common co-locations are found. This reinforces thelow confidence level status assigned to the predictionobtained.DiscussionThe first version of Proteasix was agnostic as to species,location and function of the proteases and their sub-strate proteins. The PxO allows knowledge of the domainto be added to the Proteasix algorithm. The PxO allowsProteasix to add semantics to its data such that the algo-rithm can check proteases for their function and bothprotease and substrate for species and location, as well asmaking the data reliably queriable. The aim here is two-fold: first, separating operational knowledge from domainknowledge; this will enable update and expansion of theknowledge component with relative ease. Second, the aimis to allow Proteasix and human users to check the valid-ity of Proteasix results. These results may not be improveddue to the use of PxO, but may be interpreted with moreconfidence.The bulk of the PxO has been developed through re-use of other ontologies and these were ontologies mainlyfrom the OBO Consortium. In doing so we have commit-ted to the ontological viewpoint of those ontologies. Wehave not substantially changed those ontologies, except toextend, as it makes interoperability with other OBO basedapplications harder, as well as update and maintenanceArguello Casteleiro et al. Journal of Biomedical Semantics  (2016) 7:33 Page 6 of 7more difficult. This commitment does, however, come ata potential cost. Like any commitment, making one com-mitment excludes others. The OBO, like most ontologies,are not without their controversies. The GO, in particular,has long had its critics both on ontological and logi-cal grounds [34, 35] There is debate about whether themolecular function in the GO is ontologically a functionor a finer grained process than theGOs biological process[34]. The GO has also been criticised for inconsistency inits modelling and lack of constraints that allow automatedreasoning to be applied more effectively [35, 36]. In thePxO we have taken these aspects into account with Pro-teasixs application needs and resources, and have decidedthat commiting to the OBO is an appropriate choice; thisdecision will be kept under review.There is much further work to be done in the PxO. Cur-rently, we only incorporate cannonical and isoform infor-mation from the UniProtKB, which also contains muchinformation about sequence variants. Inclusion of thismay improve the analysis done by Proteasix. The GeneOntology annotations used in describing proteins can beaccompanied with evidence codes [13], and therefore, tak-ing into account with what confidence annotations aremade may also improve the utility of the PxO in Pro-teasix. Another line of work is to map the cleavage tothe peptidase family instead of mapping the cleavage toan individual enzyme. A protein is typically representedas having many functions, in many locations and beinginvolved in many biological processes. At present whichfunctions, in which process and in which location is notrepresented. As this kind of representation emerges it willbe adopted in PxO and may contribute to the accuracy ofpredictions made in Proteasix.The PxO has its limitations in addition to those indi-cated by the future work. The PxO, like most ontologies,is limited by the state of knowledge in its domain, whichfor PxO is large. That confirmatory information on a pro-tease protein interaction is not found does not mean itcannot occur. The knowledge in the literature is muchgreater than that in ontological form. Nevertheless, if PxOcan help give confidence to Proteasixs predictions then itis a help.A further limitation comes in the ability to predictcleaveage sites in Proteasix. Proteases exhibit varyingbinding affinities for amino-acid sequences, ranging fromstrict restriction to one or few critical amino-acids ingiven positions, to generic binding with little discrim-ination between different amino acids. The MEROPSdatabase [9] lists such information. When available,MEROPS specificity weight matrices were added to PxO.The MEROPS specificity matrix shows how frequentlyeach amino acid occurred at each position in the cleavagesite. Matrices were further transformed into Probabil-ity Matrices, by dividing the number of occurrences foreach amino acid in each position with the total num-ber of observations. It has been acknowledged that to beable to study peptidase specificity and make predictionsabout where in a protein cleavage might occur, at least 40cleavages in substrates are required [33] and/or a mini-mal 10 times enrichment for one amino acid should beobserved in at least one position of the CS. Hence, theavailability of MEROPS peptidase specificity data is a hardlimitation of the current approach, which is a limitationinherent to experimental science. However, the predic-tions of Proteasix will improve as the body of evidenceincreases.This research work was done for the sysVASC project,and so the emphasis is on Metazoa, where the organismshuman, mouse, and rat are the main focus. Despite thelarge amount of work still to do in the PxO, the PxO nev-ertheless is a rich ontology supporting peptide analysisthat has been enabled by re-using the ontologies pro-duced by the OBO Consortium; the PxO has partitionedthese ontologies based on the task they need to supportand enriched them axiomatically on the same basis. As aresult Proteasix can better support the prediction of pro-teases implicated in the production of peptides and theconsequent elucidation of biological mechanisms.Additional fileAdditional file 1: PxO Metazoa ontology: Ontology metrics; ELK reasonertimes; and SPARQL queries execution times. (PDF 141 kb)AcknowledgementsThis work was supported by a grant from the European Union SeventhFramework Programme (FP7/2007-2013) for the sysVASC project under grantagreement number 603288 and by Pretreat H2020-MSCA-RISE-2015 (grantagreement number 690966). Original development of Proteasix wassupported by the grant ProteasiX FP7-PEOPLE-2011-IEF (300582), PretreatH2020-MSCA-RISE-2015 (690966) and the Fondation du Rein sous égide de laFondation pour la Recherche Médicale et ses partenaires, grant numberGENZYME 2014 FDR-SdN/FRM. The work reported here was also supported bythe EPSRC project:WhatIf: Answering "What if..." questions for OntologyAuthoring, EPSRC reference EP/J014176/1.Authors contributionsAll authors contributed to the development of the ontology and the writing ofthe paper. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1School of Computer Science, University of Manchester, Oxford Road, M13 9PLManchester, UK. 2Institut National de la Sante et de la Recherche Medicale(INSERM), U1048, 24105 Toulouse, France.Received: 14 December 2015 Accepted: 19 May 2016Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:31 DOI 10.1186/s13326-016-0082-0ERRATUM Open AccessErratum to: Towards exergaming commons:composing the exergame ontology forpublishing open game dataGiorgos Bamparopoulos1, Evdokimos Konstantinidis1, Charalampos Bratsas2 and Panagiotis D. Bamidis1*After publication of the original article [1] it wasbrought to our attention that an acknowledgement wasmissing from the article. The authors would thereforelike to add the following acknowledgement, and offertheir apologies that this was missed out in the originalpublication:This work was supported in part by the EuropeanUnions Seventh Framework Programme (Project USEFIL,GA 288532; http://www.usefil.eu), as well as the LLM Care(www.llmcare.gr) self-funded initiative that emerged as thebusiness exploitation of the Long Lasting Memories (LLMProject) (www.longlastingmemories.eu) originally fundedby the ICT-CIP-PSP Programme.Author details1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,Aristotle University of Thessaloniki, Thessaloniki, Greece. 2MathematicsDepartment, Aristotle University of Thessaloniki, Thessaloniki, Greece.Received: 24 May 2016 Accepted: 24 May 2016Huang et al. Journal of Biomedical Semantics  (2016) 7:25 DOI 10.1186/s13326-016-0064-2RESEARCH Open AccessOmniSearch: a semantic search systembased on the Ontology for MIcroRNA Target(OMIT) for microRNA-target gene interactiondataJingshan Huang1*, Fernando Gutierrez2, Harrison J. Strachan1, Dejing Dou2, Weili Huang3, Barry Smith4,Judith A. Blake5, Karen Eilbeck6, Darren A. Natale7, Yu Lin8, Bin Wu9, Nisansa de Silva2, Xiaowei Wang10,Zixing Liu11, Glen M. Borchert12, Ming Tan11 and Alan Ruttenberg13AbstractAs a special class of non-coding RNAs (ncRNAs), microRNAs (miRNAs) perform important roles in numerous biologicaland pathological processes. The realization of miRNA functions depends largely on how miRNAs regulate specifictarget genes. It is therefore critical to identify, analyze, and cross-reference miRNA-target interactions to better exploreand delineate miRNA functions. Semantic technologies can help in this regard. We previously developed a miRNAdomain-specific application ontology, Ontology for MIcroRNA Target (OMIT), whose goal was to serve as a foundationfor semantic annotation, data integration, and semantic search in the miRNA field. In this paper we describe ourcontinuing effort to develop the OMIT, and demonstrate its use within a semantic search system, OmniSearch,designed to facilitate knowledge capture of miRNA-target interaction data. Important changes in the current versionOMIT are summarized as: (1) following a modularized ontology design (with 2559 terms imported from the NCROontology); (2) encoding all 1884 human miRNAs (vs. 300 in previous versions); and (3) setting up a GitHub project sitealong with an issue tracker for more effective community collaboration on the ontology development. The OMITontology is free and open to all users, accessible at: http://purl.obolibrary.org/obo/omit.owl. The OmniSearch systemis also free and open to all users, accessible at: http://omnisearch.soc.southalabama.edu/index.php/Software.Keywords: microRNA, Non-coding RNA, Target gene, Biomedical ontology, Ontology development, Data annotation,Data integration, Semantic search, SPARQL queryIntroductionmicroRNAs (miRNAs) are a type of non-coding RNA(ncRNA) with important biological, biomedical, and clin-ical impact. Prior research [1, 2] indicates that miRNAsperform significant roles in both biological and patholog-ical processes, thus affecting the control and regulation ofvarious human diseases. miRNAs realize critical functionsvia binding to their respective target genes. The abilityto identify and analyze miRNA-target interactions in an*Correspondence: huang@southalabama.edu1School of Computing, University of South Alabama, Mobile, Alabama36688-0002, USAFull list of author information is available at the end of the articleeffective manner is thus a key step in the understandingand delineation of miRNA functions.The conventional method by which the users of data(e.g., biologists, bioinformaticians, and clinical investiga-tors) determine miRNA functions involves: Searching for biologically validated miRNA targets,for example, by querying the PubMed database [3];and Finding additional potential miRNA targets, forexample, by initiating inquiries on various predictiondatabases or websites such as miRDB [4], TargetScan[5], and miRanda [6].© 2016 Huang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 2 of 17Unfortunately, both steps currently require significantmanual effort because the relevant data sources areboth syntactically and semantically heterogeneous  thatis, the meaning of seemingly similar data from differ-ent sources may be quite different and thus open tomisinterpretation. It is therefore challenging for usersto identify and establish possible links among originaldata sources. As a result, conventional miRNA knowl-edge discovery and acquisition methodologies are time-consuming, labor-intensive, error-prone, and sensitive tolimitations in the prior knowledge of different end users.These barriers are exacerbated by the need to obtainadditional information for each and every miRNA tar-get (whether validated or putative) using existing datasources and analysis tools, including but not limited to:the DAVID Bioinformatics Resources (DAVID) [7], NCBIGene [8], the Medical Subject Headings (MeSH) Database[9], the HUGO Gene Nomenclature Committee (HGNC)Database [10], and NCBI Nucleotide [11].Emerging semantic technologies can help in address-ing the aforementioned challenges. The core of cur-rent semantic technologies include specifications such asthe resource description framework (RDF), RDF Schema(RDFS), and Web Ontology Language (OWL), all ofwhich are intended to provide a formal description ofclasses of entities of different types and of the relationsbetween them in such a way as to enable automatic rea-soning (inference). Semantic technologies can be appliedto miRNA knowledge acquisition by transforming dataobtained from heterogeneous miRNA-related databasesinto a common framework by utilizing a single format(such as RDF) and aligning the data through use of anno-tations from common, formally defined ontologies. Bymeans of this transformation we can use the SPARQLProtocol (SPARQL) [11] to query the enhanced data auto-matically.In previous research [1217], we investigated the con-struction of an application ontology for the miRNA field,named Ontology for MIcroRNA Target (OMIT), the firstontology to formally encode miRNA domain knowledge.By providing a standardized metadata model to establishmiRNA data connections among heterogeneous sources,the OMIT is able to fill two gaps: the lack of commondata elements and the lack of data exchange standards formiRNA research, especially with regard to miRNA-targetinteractions.We describe two major scientific contributions in thispaper: (1) recent improvements to the OMIT ontologyand (2) a semantic search system, which is built upon theontology and enables the capture of miRNA-target inter-action data in a way leading to more effective miRNAknowledge acquisition.The remainder of this paper is organized as follows.Related work Section summarizes state-of-the-artresearch in biomedical ontologies and semantic search,respectively. OMIT reconstruction Section reportsour efforts on reconstructing the OMIT ontology.OmniSearch: an OMIT-based semantic search systemSection describes technical details of OmniSearch,an OMIT-based semantic search system. Results anddiscussion Section reports our experimental results.Finally, Conclusions Section summarizes the majorpoints and presents ideas for future research.Related workRelated work in biomedical ontologiesThe use of ontologies to describe, define, and inte-grate biological entities has long been embraced by thebiological, biomedical, and clinical research commu-nities. Here we briefly describe some representativebio-ontologies included in both the Open Biologicaland Biomedical Ontologies (OBO) Library [18] and theNational Center for Biomedical Ontology (NCBO) Bio-Portal [19] that are pertinent to the development of thisproject.The Gene Ontology (GO) [20] is by far the mostsuccessful and widely used ontology for biologicaldata description. It consists of three independent sub-ontologies: biological processes, molecular functions, andcellular components, which describe these aspects of geneproducts: both protein and RNA. The GO has been widelyutilized to annotate gene products of model organisms.By the time of writing this paper, there were GO annota-tions for 36 organisms including Homo sapiens availablefor download.The Sequence Ontology (SO) [21] is an ontology to cap-ture genomic features and the relationships that obtainbetween them. This ontology contains the features neces-sary to annotate a genome with structural features such asgene models and also the terms necessary for the anno-tation of genomic variants. SO terms define the kinds ofand parts of ncRNA features, and these terms are usedto identify these features and their location in genomicsequence.The PRotein Ontology (PRO) [22] is a comprehensivedescription of the forms of protein, including isoforms,modifications, and the relationships between them. Pro-teins are functional entities in many processes eventuallyimpacted by the regulatory effect of ncRNAs (e.g., miRNAbindings). The PRO provides an ontological representa-tion of proteins with a particular focus on human proteinsand disease-related variants thereof.The RNA Ontology (RNAO) [23] is a candidate OBOfoundry reference ontology to catalogue the molecu-lar entities composing primary, secondary, and tertiarycomponents of RNA. The goal of this project is toenable integration and computation over diverse RNAdatasets.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 3 of 17Related work in semantic searchSemantic search is a research field that intends to improvethe access to contents by considering the semanticsbehind the search process [24]. In other words, semanticsearch goes beyond conventional, keyword-based searchby considering the contextual meaning of words, theintent of the user, and the nature of the search space.In general, semantic search requires the use of struc-tured knowledge, such as ontologies, in the modeling andinterpretation of queries. Ontologies can help improvethe search by query expansion. One main idea in manysemantic search systems (e.g., [2529]) is, the originalset of query keywords can be expanded by drawing onsynonyms and other relationships (e.g., subclass and part-hood) that are not part of the query. For example, inthe work by Chauhan et al. [29], the original query wasfirst expanded by considering synonyms, then terms withhigh semantic similarity were chosen from the ontologyto be integrated to the search query, and the seman-tic similarity used for the query expansion was com-puted by the distance among concepts in the ontology,the position in the hierarchy, and the number of upperclasses.Another way to implement semantic search is to useontologies to translate keyword-based search into formalsemantic queries. For example, Tran et al. [24] useda set of models (mental, user, system, and query) tocapture information, such as thought entities, languageprimitives, knowledge representation (KR) primitives,and query elements. These models were then com-bined with a set of assumptions to redefine originalqueries, filling the gap between terms with structuralinformation from an ontology. That is, each termwithin the query was considered a property of anotherterm.OMIT reconstructionModularized ontology designThe OMIT ontology consists of the following modules: omit.owl  Defines all OMIT-specific terms andrelations, for example, prediction_from_miRDB andgene_context_score_in_TargetScan. bfo.owl  Imports upper-level terms from the BasicFormal Ontology (BFO) [30], for example, genericallydependent continuant and material entity. ro-imports.owl  Imports common relations (sharedacross different ontologies) from the RelationOntology (RO) [31], for example, has participant andregulates. ncro.owl  Imports ncRNA-related terms andrelations from the Non-coding RNA Ontology(NCRO) [32], for example, miRNA_target_gene andmiRNA_gene_family. go-imports.owl  Imports gene product terms fromthe GO, for example, RNA binding and regulation ofbiological process. so-imports.owl  Imports sequence structuralfeature terms from the SO, for example,biological_region and insertion_site. obi-imports.owl  Imports life-science and clinicalinvestigation terms from the Ontology for BiomedicalInvestigations (OBI) [33], for example, cultured cellpopulation and organism. chebi-imports.owl  Imports molecular entity(especially small chemical compounds) terms fromthe Chemical Entities of Biological Interest Ontology(ChEBI) [34], for example, ribonucleic acid andribosomal RNA. iao-imports.owl  Imports information entity termsfrom the Information Artifact Ontology (IAO) [35],for example, information content entity. clo-imports.owl  Imports cell line-relevant termsfrom the Cell Line Ontology (CLO) [36], for example,cell line. pr-imports.owl  Imports protein-related entityterms from the PRO, for example, amino acid chainand protein. uberon-imports.owl  Imports cross-speciesanatomy terms from the Uberon multi-speciesanatomy ontology (UBERON) [37], for example,anatomical structure and organ. doid-imports.owl  Imports disease terms from theHuman Disease Ontology (DOID) [38], for example,disease of cellular proliferation and cancer.Note that:(1) Orthogonality among different ontologies is oneof the important practices proposed by the OBOFoundry Initiative, and has been widely accepted inthe bio-ontology community. As a result, to achievebetter orthogonality, it is a common practice to reusecontents defined in relevant, existing ontologies.(2) The OMIT ontology directly imported the NCROontology (a comprehensive ncRNA domainontology), which in turn, directly imported otherontologies in the above list. Therefore, the OMITontology itself includes two OWL files: omit.owland ncro.owl. All other OWL files,go-imports.owl and so-imports.owl for example,are shown as indirectly imported in Protégé.(3) Ontology concepts are referred to as classes inProtégé and terms in OBO Edit, respectively.Therefore, classes and terms are interchangeablyused throughout the whole paper.Table 1 lists a subset of important terms and relationsimported into the OMIT.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 4 of 17Table 1 A subset of imported terms and relationsImported term or relation Source ontology Original IDRO:part of Relation Ontology BFO_0000050RO:participates in Relation Ontology RO_0000056RO:has participant Relation Ontology RO_0000057BFO:entity Basic Formal Ontology BFO_0000001BFO:continuant Basic Formal Ontology BFO_0000002BFO:independent continuant Basic Formal Ontology BFO_0000004BFO:occurrent Basic Formal Ontology BFO_0000003BFO:material entity Basic Formal Ontology BFO_0000040CHEBI:molecular entity Chemical Entities of Biological Interest Ontology CHEBI_23367CHEBI:ribonucleic acid Chemical Entities of Biological Interest Ontology CHEBI_23367CHEBI:ribosomal RNA Chemical Entities of Biological Interest Ontology CHEBI_18111CHEBI:small nuclear RNA Chemical Entities of Biological Interest Ontology CHEBI_74035CHEBI:transfer RNA Chemical Entities of Biological Interest Ontology CHEBI_17843NCRO:human_miRNA Non-coding RNA Ontology NCRO_0000810NCRO:hsa-miR-125b-1-3p Non-coding RNA Ontology NCRO_0003283NCRO:hsa-miR-125b-2-3p Non-coding RNA Ontology NCRO_0003284NCRO:hsa-miR-125b-5p Non-coding RNA Ontology NCRO_0003282NCRO:miRNA_target_gene Non-coding RNA Ontology NCRO_0000025NCRO:miRNA_and_target_gene_binding Non-coding RNA Ontology NCRO_0000003NCRO:protein_miRNA_promoter_binding Non-coding RNA Ontology NCRO_0000011IAO:information content entity Information Artifact Ontology IAO_0000030IAO:measurement datum Information Artifact Ontology IAO_0000109 The format for the left column (Imported Term orRelation) is PREFIX:human-readable label, forexample, NCRO:miRNA_target_gene and RO:part of. The format for the right column (Original ID) isPREFIX_unique identifier, for example,NCRO_0000025 and BFO_0000001.Ontology core designThe core design of the OMIT ontology is shown in Fig. 1.Compared with earlier versions, the current version con-tains many important new terms and relations, and someof which are listed in Tables 2 and 3, respectively. Both terms and relations are represented in theformat of PREFIX:label in Fig. 1. For the purpose of better readability, labels ratherthan identifiers are used in Tables 2 and 3. Relations in Table 3 were either defined in orimported into the OMIT, which can be easilydistinguished from each other by different prefixesused in the first column.OmniSearch: an OMIT-based semantic searchsystemBased on the OMIT ontology, we developed a semanticsearch system:OmniSearch. First, the OmniSearch systemwill conduct semantic annotation on various sources thatwere originally heterogeneous in their semantics; follow-ing that, OMIT-annotated data will then be integratedinto a unified and consistent data layer in RDF; and finally,complex semantic queries will be performed to providemeaningful results and clues to system end users (e.g.,biologists, bioinformaticians, and clinical investigators).Data sources usedData sources used in the OmniSearch system includethree miRNA target prediction databases (miRDB, Tar-getScan, and miRanda), as well as PubMed, NCBI Gene,GO, RNA Central, DAVID, HGNC, and MeSH termdatabases. These sources contain both structured data(database instances) and unstructured data (free text), andare semantically heterogeneous among each other.Software architectureTheOmniSearch system consists of several softwaremod-ules: semantic annotation, data integration, and semanticsearch.Semantic data annotation is the process of taggingsource files with predefined ontological metadata likenames, entities, attributes, definitions, and descriptions.The annotation provides original data with extrametadataHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 5 of 17Fig. 1 The design of core terms and relations in the OMIT ontology (both terms and relations are represented in the format of PREFIX:label)Table 2 A subset of new OMIT termsOMIT new term Direct parent term Human-readable explanationcomputationally_asserted_evidence IAO:information content entity Evidence obtained from somecomputational methods.information_from_miRNA_ OMIT:computationally_asserted_evidence Records obtained from varioustarget_prediction_database miRNA target prediction databases.prediction_from_miRDB OMIT:information_from_miRNA_ Records specifically obtainedtarget_prediction_database from the miRDB database.prediction_from_TargetScan OMIT:information_from_miRNA_ Records specifically obtainedtarget_prediction_database from the TargetScan database.prediction_from_miRanda OMIT:information_from_miRNA_ Records specifically obtainedtarget_prediction_database from the miRanda database.target_score_in_miRDB IAO:measurement datum The score of some specificmiRNA-target binding predictionfrom the miRDB database.gene_context_score_in_TargetScan IAO:measurement datum The context score of some specificmiRNA-target binding predictionfrom the TargetScan database.mirSVR_score_in_miRanda IAO:measurement datum The mirSVR score of some specificmiRNA-target binding predictionfrom the miRanda database.information_from_NCBI_gene IAO:information content entity Records obtained from NCBI Geneaccording to gene IDs or gene symbols.information_from_NCBI_nucleotide IAO:information content entity Records obtained from NCBI Nucleotideaccording to GenBank Accession numbers.information_from_PubMed IAO:information content entity Records obtained from the PubMeddatabase according to PMIDs.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 6 of 17Table 3 A subset of new OMIT relationsNew relation Domain Range Human-readable explanationOMIT:miRNA_target_ NCRO:miRNA_and_ OMIT:computationally_ Specific miRNA-target bindingassumption_ target_gene_binding asserted_evidence prediction is based on somebased_on computationally asserted evidence.OMIT:is_quality_ IAO:measurement datum OMIT:computationally_ A piece of measurement datummeasurement_of asserted_evidence (e.g., the target score in miRDB)is a quality measurement ofcomputationally asserted evidence.OMIT:is_gene_ NCRO:miRNA_target_gene OMIT:target_protein A miRNA target genetemplate_of_protein serves as a templateof relevant protein.RO:has participant OMIT:prediction_from_miRDB SO:miRNA Each miRNA-target bindingprediction record has onemiRNA as a participant.RO:has participant OMIT:prediction_from_miRDB NCRO:miRNA_target_gene Each miRNA-target bindingprediction record has onetarget as a participant.RO:part of OMIT:target_score_in_miRDB OMIT:prediction_from_miRDB Each miRNA-target bindingprediction record frommiRDB contains one score.Each record from NCBIRO:part of OMIT:PubMed_summary_ OMIT:information_from_NCBI_gene Gene contains one orin_NCBI_gene more PubMed summaries.information formally defined in the OMIT ontology. Theoutput of semantic data annotation is a collection ofRDF triples (from both free text and database instances).These triples will be accumulated into a centralized RDFrepository: OmniStore.We used Python scripts to conduct automated seman-tic annotation and data integration. As an example, Fig. 2shows the flowchart of our programs to annotate miRDBdata. We explain below the detailed steps. One miRDBfile, the miRNA data file, contains two columns con-sisting of miRNA names and their associated interna-tionalized resource identifiers (IRIs). Another miRDB file,the gene data file, contains four columns consistingof miRNA names, gene IDs, gene symbols, and targetscores. Step One: As each miRNA name and its associatedIRI were read in from the miRNA data file, they wereplaced into a dictionary where the miRNA name isthe key and the IRI is the value. Step Two: All lines were read in from the gene datafile, and each line was converted into a total of fourRDF triples. (1) The first triple was generated torepresent a newly created instance of theprediction_from_miRDB class, namely, instance_i,and a new OMIT IRI was assigned to instance_i. (2)Next, the miRNA name read from the same line wasused to retrieve its corresponding IRI from thedictionary (generated in Step One). The second triplethen connected this retrieved IRI with instance_i. (3)Two more triples were generated to connectinstance_i with the corresponding gene ID and targetscore read in from the same line, respectively. Step Three: Finally, all generated RDF triples werewritten into a Turtle file.Note that:(1) Semantic annotation and data integrationSection exhibits some example triples resulted fromthe above-mentioned annotation process.(2) Mappings between database schemas andontological entities were defined in the OMITontology and can be reused or modified in the future,when needed.(3) Due to our automated annotation and integrationtechniques, only minimum effort will be required tointegrate a new resource in the future.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 7 of 17Fig. 2 Semantic annotation and data integration flowchart in the OmniSearch systemBecause all semantic tags are to be generated from theglobal metadata model defined in the OMIT ontology, theRDF triple repository will provide a unified view over orig-inal data sources at semantic level. Consequently, complexsemantic queries will be enabled. To implement semanticsearch, we made use of Apache HTTP server [39], PHP:Hypertext Preprocessor (PHP) server [40], and ApacheJena Fuseki server [41]. The overall software architec-ture is demonstrated in Fig. 3, with the following workingprotocol: Query parameters are sent from the clients browserto the Apache server through Ajax requests. SPARQL queries are dynamically generated by theApache server using these query parameters, whichare then sent to the Apache Jena Fuseki server. JSON objects, containing the requested information,are retrieved from the RDF triple store (installed onthe Apache Jena Fuseki server) after running thedynamically generated SPARQL queries. These JSON objects are returned to the Apacheserver, which are used to generate either (1) a list ofmiRNAs and/or MeSH terms or (2) the HTMLMarkup for the search result table. Finally, the Apache server sends the obtained data, oran error message if the search fails, back to theclients browser as a JSON object.User interface designThe OmniSearch is a Web-based search system that isfree and open to all users, accessible at: http://omnisearch.soc.southalabama.edu/index.php/Software. As shown inFig. 4, the main components of the graphic user inter-face (GUI) are: two search criteria boxes, a search resulttable, a pagination control, a set of result viewing filters,a result download tool, and DAVID analysis functionality.More discussion on our friendly user interface design canbe found in Search results and discussion Section.Results and discussionThe significantly refactored OMIT ontologyThe updated version of the OMIT ontology containsa total of 3169 terms and 46 relations (besides a totalof 5515 is_a relations). Note that out of 46 relationsHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 8 of 17Fig. 3 Semantic search architecture in the OmniSearch systemmentioned here, there are 5 data properties, and therest are object properties. Also note that these termsand relations include both OMIT-specific ones and thoseimported ones1.Compared with the previous versions [1217], impor-tant changes in the current version OMIT ontology aresummarized as follows. As discussed earlier in Modularized ontology designSection, we have followed amodularized ontologydesign in this new version, which will significantlyfurther facilitate the ontology maintenance andupdate. In particular, a total of 2559 terms in theupdated OMIT have been imported from the NCROontology [32]. Because the NCRO is a comprehensivedomain ontology in the ncRNA field, following theNCRO hierarchy will enhance the interoperabilitybetween the OMIT and future ontologies to bedeveloped in other ncRNA sub-domains. In the previous versions of OMIT, around 300human miRNAs were included. In the currentversion, all 1884 miRNAs appearing in humanshave been encoded, along with the information aboutthe gene family group of each and every miRNA.According to miRBase [42], there are a total of 320different gene family groups. This information can behighly valuable because the fact that two or moremiRNAs of interest indeed belong to the same genefamily group can provide biologists,bioinformaticians, and clinical investigators withcritical clues in constructing new hypothesis. In our previous investigations, we established adedicated project website [43], as well as entries inboth the OBO Library [44] and the NCBO BioPortal[45]. To further disseminate the ontology, and, togather feedback from community in a more effectivemanner, we have recently created a GitHub projectsite (https://github.com/OmniSearch/omit) for thisnew version OMIT ontology. We have alsoestablished a tracker [46] for an enhancedmechanism in handling the discussion amonggroups to further improve the ontology. Newconcepts, definitions, and their locations in theOMIT can now be proposed, debated, and approved(or rejected) by an open group of individuals throughthis tracker.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 9 of 17Fig. 4 GUI design in the OmniSearch systemSemantic annotation and data integrationExperimental setupThe OmniStore RDF repository is housed on a server withthe following configuration: Intel(R) Core(TM) i7-3632QM CPU @ 2.80 GHz 2.80 GHz; 32.00 GB memory; andWindows Server 8 Operating System.Semantic annotation and data integration resultsOmniStore contains a total of 6,136,514 RDF triples, andthe file size of OmniStore is 369 MB. All triples are rep-resented in RDF 1.1 Turtle: Terse RDF Triple Languageformat [47], for example:<http://purl.obolibrary.org/obo/OMIT_0015037>rdfs:subClassOf<http://purl.obolibrary.org/obo/NCRO_0000025> .<http://purl.obolibrary.org/obo/OMIT_0015037>rdfs:label"IRF4" .<http://purl.obolibrary.org/obo/OMIT_0995324>rdf:type<http://purl.obolibrary.org/obo/OMIT_0000020> .<http://purl.obolibrary.org/obo/OMIT_0995324><http://purl.obolibrary.org/obo/RO_0000057><http://purl.obolibrary.org/obo/OMIT_0015037> .<http://purl.obolibrary.org/obo/OMIT_0995324><http://purl.obolibrary.org/obo/RO_0000057><http://purl.obolibrary.org/obo/OMIT_0050688> .<http://purl.obolibrary.org/obo/OMIT_0995324><http://purl.obolibrary.org/obo/OMIT_0000108>100 .The semantics of the above six example triples is: IRF4(OMIT_0015037) is a subclass of the miRNA_target_geneclass (NCRO_0000025); one miRDB database record(OMIT_0995324), which is an instance of the pre-diction_from_miRDB class (OMIT_0000020), indicatesthat IRF4 is a predicted target of the miRNA hsa-miR-125b-5p (OMIT_0050688); and the prediction score(OMIT_0000108) is 100.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 10 of 17Semantic searchWe use one example in this section to demonstrate indetail how the OmniSearch system assists in end usersknowledge acquisition.Experimental setupSemantic search was conducted on a personal computer(PC) with the following configuration: Intel(R) Core(TM)i7-3632 QM CPU @ 2.50 GHz 2.50 GHz; 16.00 GB mem-ory; and Windows 10 64-bit Operating System.SPARQL query statementsThe SPARQL statements to generate the miRNA andMeSH term lists in the two search boxes are as fol-lows, where the PHP variable $type is used to determinewhether the client is requesting a miRNA or MeSH term,and the PHP variable $input contains either a partial orexact miRNA or MeSH term. Note that each line of thequery statement has a detailed explanation right above it(the line starting with a pound sign #).# prefix declarationsPREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#># result clauseSELECT ?label# query patternWHERE {# get IRI of either human_miRNA orMeSH_Term as parent?parent rdfs:label $type .# get all children of parent?child rdfs:subClassOf ?parent .# get label for each child?child rdfs:label ?label .# filter results to only include labelthat match the user inputFILTER REGEX(LCASE(?label), LCASE($input))}# order the result by labelORDER BY ?labelSuppose that the question of interest is: What is therole of hsa-miR-125b-5p in cancer drug resistance? TheSPARQL statements are as follows. Similarly, all querystatements have a detailed explanation.# prefix declarationsPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX obo: <http://purl.obolibrary.org/obo/># result clauseSELECT ?gene_symbol# group the gene ids together(GROUP_CONCAT(DISTINCT ?g_id;SEPARATOR=",") AS ?gene_id)# assign mdb_score to mirdb_score ifbound, otherwise assign 0(MAX(COALESCE((?mdb_score),?mdb_score,0)) AS ?mirdb_score)# assign ts_score to targetscan_scoreif bound, otherwise assign 0(MAX(COALESCE(?ts_score), ?ts_score, 0))AS ?targetscan_score)# assign absolute value of mrnd_scoreto miranda_score if bound, otherwiseassign 0(MAX(COALESCE(ABS(?mrnd_score), 0)) AS?miranda_score)# concatenate and group all pubmed idstogether, separated by a comma(GROUP_CONCAT(DISTINCT ?pmid;SEPARATOR=",") AS ?pubmed_ids)# query patternWHERE {# get microRNA IRI with label"hsa-miR-125b-5p"?mirna rdfs:label "hsa-miR-125b-5p" .# get prediction that has_human_miRNAmicroRNA IRI?prediction obo:OMIT_0000159 ?mirna .# get target where prediction has_miRNA_target_gene?prediction obo:OMIT_0000160 ?target .# get gene symbol label of target?target rdfs:label ?gene_symbol .# get target gene_id as g_id?target obo:OMIT_0000109 ?g_id .OPTIONAL {# get prediction of typeprediction_from_TargetScan?prediction rdf:type obo:OMIT_0000019 .# get prediction score as ts_score?prediction obo:OMIT_0000108?ts_score}.OPTIONAL {# get prediction of typeprediction_from_miRDB?prediction rdf:type obo:OMIT_0000020 .# get prediction score as mdb_score?prediction obo:OMIT_0000108 ?mdb_score}.Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 11 of 17OPTIONAL {# get prediction of typeprediction_from_miRanda?prediction rdf:type obo:OMIT_0000021 .# get prediction score as mrnd_score?prediction obo:OMIT_0000108 ?mrnd_score}.OPTIONAL {# get MeSH Term IRI for "drugresistance"?mesh_term rdfs:label "drugresistance" .### exact match #### get pubmed id associated withthe target gene?target obo:OMIT_0000151?pubmed_id .# get pubmed id associated withthe mesh term?mesh_term obo:OMIT_0000151?pubmed_id### narrower match #### get each successive child of meshterm#?child rdfs:subClassOf* ?mesh_term .# get pubmed id associated withthe target gene#?target obo:OMIT_0000151?pubmed_id .# get pubmed id associated withthe child mesh term#?child obo:OMIT_0000151?pubmed_id### broader match #### get each successive parent meshterm#?mesh_term rdfs:subClassOf*?parent .# restric parent to be a subclassof MeSH_Term#?parent rdfs:subClassOf obo:OMIT_0000110 .# get pubmed id associated withthe target gene#?target obo:OMIT_0000151?pubmed_id .# get pubmed id associated withthe parent mesh term#?parent obo:OMIT_0000151?pubmed_id}.}# group the results by gene symbolGROUP BY ?gene_symbol# order the results by mirdb score then bytargetscan scoreORDER BY DESC(?mirdb_score)DESC(?targetscan_score) DESC(?miranda_score)Search results and discussionCorresponding to the aforementioned question of inter-est, Fig. 5 demonstrates the search results from a queryon hsa-miR-125b-5p along with a MeSH-term filter drugresistance. The Candidate Targets column contains all targetspredicted by at least one target prediction database.The user can choose a prediction database and sortall targets by the scores, in descending order, fromthe selected database. The Predicted By column shows that each target ispredicted by which database(s), along with the Weblink(s) to these database(s). The Publications column links to all PubMedpublications that are relevant to the search andfiltering criteria. In this example, the criteria for anyline are: the predicted target on that line, the miRNAhsa-miR-125b-5p, and the MeSH-term filter drugresistance. The GO Annotations column connects to GOannotation results of each predicted target and themiRNA hsa-miR-125b-5p, respectively. Pathway analysis through DAVID can be performedon selected targets, either using the checkboxes to theleft of the table or clicking the Select All Targetscheckbox. Additionally, the user can select the desiredtool to perform such analysis, Gene FunctionalClassification, Functional Annotation Clustering,Functional Annotation Summary, and so forth. The whole result table can be downloaded in twodifferent formats (tab-delimited text or CSV format);the user is also able to download only the predictedtargets (selected ones or all).We examined the search results demonstrated in thisexample, and our observations are summarized below.1. Effective querying and accurate search results. Potential targets from all three miRNA targetprediction databases (miRDB, TargetScan, andHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 12 of 17Fig. 5 Search results for the question of What is the role of hsa-miR-125b-5p in cancer drug resistance?miRanda) were correctly retrieved. There were476 and 924 targets from miRDB andTargetScan, respectively; and there were 323common targets. Consequently, a total of 1077distinct targets were retrieved in the table whenthe Predicted by Any Database filter waschosen. Note that the miRanda database did notcontain prediction results for the miRNAhsa-miR-125b-5p; therefore, no resultsappeared in the table when the display filter wasset to Predicted by All Databases. In fact, thisobservation further verified the effectiveness ofthe OmniSearch system. Relevant papers according to the search criteriawere successfully retrieved. For example, twopublications (PMID: 2497002 and 22808086)were retrieved for the predicted target LIN28A,supporting the conclusion that Lin28Acontributes to cancer drug resistance; and threepublications (PMID: 21823019, 24643683, and19463775) were retrieved for the predictedtarget BAK1, supporting another conclusionthat BAK1 has an important role in cancer drugresponse and drug resistance. RNA Central annotations and GO annotationswere correctly obtained. In this example query,a total of five sequences regarding the miRNAhsa-miR-125b-5p were retrieved from RNACentral annotations, and GO annotations for allpredicted targets were retrieved as well. Forexample, a total of 117 GO annotations(GO_REF:0000038, GO_REF:0000033, and soforth) were retrieved regarding a potentialtarget, BAK1. Based on the above knowledge returned in theOmniSearch GUI, regarding the exampleHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 13 of 17question of What is the role ofhsa-miR-125b-5p in cancer drug resistance?end users obtained the following answer: It isreasonable to speculate that expression ofthe miRNA hsa-miR-125b-5p contributes tocancer drug resistance, possibly through itssuppression of expression for target genesBAK1 and/or LIN28A.Discussion:(1) miRDB, TargetScan, and miRanda databases havequite different meanings among each other in termsof their database entities. Due to the underlyingOMIT and the formally defined semantics in theontology, the OmniSearch system was able toeffectively integrate the prediction results from allthree databases. Note that conventional,database-oriented techniques can also implementsuch integration; however, inflexible, ad-hochard-coding will be required.(2) To retrieve a correct set of relevant papersrequires accessing numerous heterogeneous datasources such as NCBI Gene, PubMed, HGNC, andMeSH. Without the common data elements definedin the OMIT and the thereafter semantictechnologies including semantic annotation and dataintegration, it would have been extremely challengingto effectively integrate data from these sources, whichis the case in database-oriented search and querying.(3) As discussed earlier in OMITreconstruction Section, the OMIT is closelyconnected with the GO by importing a set of GOterms. Compared with data integration based ontraditional, relational databases, our approach hasfurther facilitated integrating data about GOannotations.2. More efficient querying process. One-stop visit rather than accessing differentdata sources separately, resulting in about 60 %of time saved for end users. DAVID analysis was performed in a moreefficient manner due to the target gene listautomatically generated by the system. resultingin about 50 % of time saved for end users. It was easier to compare different predictionresults among miRDB, TargetScan, andmiRanda databases, resulting in about 60 % oftime saved for end users. The above percentages of saved time werecalculated as follows: We asked theaforementioned domain experts to perform agiven set of queries using their conventionalmethods; next, they performed the same set ofqueries through the OmniSearch GUI; andfinally, the saved time for all domain expertswere averaged. Greater details on the systemtime and saved time for end users are containedin Table 4. Applying the MeSH-term filter resulted in amuch smaller number of relevant publicationsreturned. For example, 50 vs. 16 for the targetABCC5, 13 vs. 2 for the target DPH2, and 31 vs.3 for the target FOXQ1. More examples aredemonstrated in Table 5.Discussion:(1) The reduced time spent by users was due to bothdata integration and the more accurate semanticsdefined in the ontology.(2) In an non-ontology software system, to filteringon MeSH terms almost unavoidably results inhard-coding some ad-hoc searching rules in sourcecode. On the contrary, semantics-oriented systems,such as OmniSearch, can well handle this issue in amore efficient manner. By decoupling domainknowledge from source code, ontologies and softwareapplications can be developed independently, leadingto more flexible software development.(3) Based on the is_a relation, the OmniSearchsystem can perform logic reasoning over theontology concept hierarchy (that is, both broader andnarrower terms of the ontology term of interest),thus greatly improving the flexibility of search andquery capability. For example, after a MeSH term ischosen by users, they are able to search the exactMeSH term, or its broader terms (i.e., ancestorterms) and narrower terms (i.e., offspring terms)defined in the ontology. Such results would not havebeen obtained without semantic technologiesbecause systems based on relational databases arenot able to perform any logical reasoning. Of course,users can still manually perform numerous queriesand then obtain similar results as obtained from oursystem. However, such manual querying issignificantly more time-consuming andlabor-intensive, and more importantly, error-prone.(4) Cross-referencing among miRDB, TargetScan,and miRanda prediction results was made mucheasier because relevant database entities have alreadybeen formally defined in the OMIT. In other words,unambiguous semantics was accurately encoded withcommon data elements provided by the ontology,resulting in successful data sharing and exchangingamong heterogeneous data sources.(5) We asked the aforementioned domain experts toverify the accuracy of MeSH-term filtering. Becauseall returned publications contained thecorresponding MeSH term, the Precision measureHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 14 of 17Table 4 The system time and saved time for end usersQueryFirst search Second search System time User time Percentage of saved Percentage of saved Percentage of savedcriterion criterion (seconds) (seconds) time for end users time on DAVID analysis time on result comparison1 hsa-miR-1231 cell movement 2.51 10 62 % 55 % 61 %2 hsa-miR-1288-5p cell proliferation 2.89 9 61 % 51 % 62 %3 hsa-miR-143-3p mitosis 5.54 10 61 % 52 % 60 %4 hsa-miR-192-5p leukemic infiltration 2.24 8 53 % 53 % 59 %5 hsa-miR-216a-5p drug resistance, 4.09 11 65 % 55 % 62 %multiple6 hsa-miR-29c-3p recurrence 8.99 11 68 % 53 % 63 %7 hsa-miR-3155a dna cleavage 1.21 6 53 % 47 % 55 %8 hsa-miR-320b drug resistance 17.59 18 73 % 51 % 66 %9 hsa-miR-3622a-5p entosis 0.30 6 51 % 43 % 57 %10 hsa-miR-371b-5p mitochondrial 3.89 12 66 % 59 % 64 %dynamics11 hsa-miR-3934-5p dna methylation 0.93 8 61 % 45 % 59 %12 hsa-miR-4263 mutagenesis 1.65 6 52 % 46 % 56 %13 hsa-miR-4431 mitochondrial 0.17 6 53 % 47 % 55 %degradation14 hsa-miR-4505 cell transformation, 4.25 10 63 % 55 % 61 %neoplastic15 hsa-miR-4648 cell polarity 0.71 6 52 % 45 % 57 %16 hsa-miR-4700-3p neoplasm regression, 1.56 7 53 % 51 % 59 %spontaneous17 hsa-miR-4756-5p endocytosis 3.76 10 67 % 53 % 62 %18 hsa-miR-4802-3p drug resistance, 1.67 7 55 % 47 % 59 %microbial19 hsa-miR-501-3p insulin resistance 1.78 8 57 % 43 % 61 %20 hsa-miR-520a-3p ubiquitination 13.31 17 75 % 55 % 65 %Average   3.95 9.30 60.05 % 50.30 % 60.15 %was evaluated as 100 %. As for the Recall measure, ittook a much longer time to evaluate because weneeded to identify all publications that wereincorrectly filtered out by the system. For example,there were three (one, resp.) publications relevant toCSNK2A1 (DVL3, resp.) that should not have beenfiltered out. More such examples are demonstratedin Table 6. Overall, an average Recall of 73 % wasachieved, meaning that while a user is able to obtaindesired knowledge in a much more efficient manner(by reading significantly less publications, as shown inTable 5), the potential information lost is rather low.3. Friendly user interface. For both search boxes, a list of partiallymatching terms were presented in a drop-downbox as users typed in the box. Users were alsoallowed to not to type in anything, in which caseall terms will be presented. The Rows Per Page drop-down and paginationcontrol helped users to easily navigate among allpredicted targets. A set of display filters were designed to allowusers to conveniently and freely customize theirpreferred way to view retrieved results fromvarious facets. For example, results can besorted by the prediction score from any selectedprediction database; users can choose to viewonly results that have publication evidence, ordoes not have such evidence, or both; and soforth. Flexible download options were provided, andall downloaded documents had self-explanatory,meaningful file names that contain the searchHuang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 15 of 17Table 5 Reduced number of publications after applying theMeSH-term filter drug resistanceTarget gene Original number Number of papers Percentagesymbol of papers after MeSH filtering reducedABCC5 50 16 68 %DPH2 13 2 85 %FOXQ1 31 3 90 %CIAPIN1 43 4 91 %SLC38A9 12 1 92 %MCL1 452 31 93 %MKNK2 30 2 93 %BAG4 32 2 94 %ARID3B 18 1 94 %HSPB2 79 4 95 %THEMIS2 20 1 95 %BAK1 266 11 96 %SULT4A1 27 1 96 %FUT4 57 2 96 %GPC6 29 1 97 %DDX54 29 1 97 %MBD1 58 2 97 %PRDM1 118 4 97 %DTNB 30 1 97 %LIN28A 91 3 97 %SIRT7 33 1 97 %ZBTB7A 67 2 97 %NCOR2 240 7 97 %TTPA 35 1 97 %MAP3K10 35 1 97 %SGPL1 36 1 97 %MYO18A 36 1 97 %EIF4EBP1 217 6 97 %LIMK1 109 3 97 %TP53INP1 37 1 97 %CYTH1 39 1 97 %SLC7A1 41 1 98 %date, Query_Results_for_hsa-miR-125b-5p-2015-12-05.csv andTarget_List_for_hsa-miR-125b-5p-2015-12-05.txt for example.ConclusionsAs a special class of ncRNAs, miRNAs have been demon-strated to play important roles in various biological andpathological processes. Because miRNAs realize theirfunctions by regulating respective targets, it is critical toTable 6 An example set of publications correctly/incorrectlyfiltered by drug resistanceGene symbol Total number ofpublicationswithout applyingthe drug resistancefilterTotal numberof publicationsthat contain theMeSH term drugresistanceTotal number ofincorrectly filteredpublicationsIRF4 130 3 0ARID3B 18 1 0SGPL1 36 1 0ESRRA 131 3 0PAFAH1B1 129 1 0ETS1 287 5 0TTPA 35 1 0DVL3 60 1 1THEMIS2 20 1 0VTCN1 66 1 0WDR5 128 1 0ETV6 198 4 0TAZ 74 1 0IL6R 300 1 0DPH2 13 2 0BTG2 84 1 0CYP24A1 146 2 0LIN28A 91 3 0TRPS1 69 1 0CSNK2A1 619 5 3TP53INP1 37 1 0GPC6 29 1 0DICER1 291 3 0identify and analyze miRNA-target interaction data tobetter explore and delineate miRNA functions. Semantictechnologies and domain ontologies have been utilized toovercome limitations of conventional miRNA knowledgeacquisitionmethods. To this end, we followed the researchdirection identified in our previous investigations regard-ing the establishment of common data elements and dataexchange standards in the miRNA research. Specifically,our major scientific contributions in this paper are: We have significantly improved the OMIT ontologyby: (1) following a modularized ontology design; (2)encoding all 1884 human miRNAs; and (3) setting upa GitHub project site along with an issue tracker formore effective community collaboration on theontology development. The up-to-date ontology file isaccessible at: http://purl.obolibrary.org/obo/omit.owl. Based upon the OMIT, we built the OmniSearchsemantic search system, accessible at: http://Huang et al. Journal of Biomedical Semantics  (2016) 7:25 Page 16 of 17omnisearch.soc.southalabama.edu/index.php/Software. Our experimental results demonstratedpromising performance of OmniSearch.Consequently, more effective, more efficientmiRNA-related knowledge capture has beenachieved.Finally, some research directions are envisioned as fol-lows for our future work.(1) To investigate a new set of filters to perform a widerscope of ontology reasoning. For example, potential filterscan be developed according to different miRNA cate-gories such as: oncogenic or tumor-suppressive miRNAs;individual tissues and/or cell lines in which miRNAs areexpressed; and the gene family group to which miRNAsbelong.(2) To verify the consistency of contents retrievedfrom different data resources is another important futureresearch topic. It is not trivial to resolve conflicting factsamong different sources.(3) It would be terrific for users to have more flexibleoptions in further exploiting the semantics of the domain.Note that to construct more flexible queries will involvenatural language processing (NLP) techniques, which arebeyond the scope of this paper. Nevertheless, such aninteresting topic can be considered in the future.Endnote1There are 103 and 18 OMIT-specific terms andrelations, respectively.Competing interestsThe authors declare that they have no competing interests.Authors contributionsAll authors performed requirements analysis. HJS contributed to GUIdevelopment. JH, AR, YL, HJS, and FG contributed to ontology development,term definition, and annotation examples. All authors read and approved thefinal manuscript.AcknowledgementsFunding for Huang, J was provided in part by the National Cancer Institute(NCI) at the National Institutes of Health (NIH), under the Award NumberU01CA180982. Funding for Borchert, GM was provided in part by NaturalScience Foundation (NSF) CAREER grant 1350064 (GMB) awarded by Divisionof Molecular and Cellular Biosciences (with co-funding provided by the NSFEPSCoR program) and in part by the Abraham A. Mitchell Cancer ResearchFund. The views contained in this paper are solely the responsibility of theauthors and do not represent the official views, either expressed or implied, ofthe NIH, NSF, the U.S. Government, or the Abraham A. Mitchell CancerResearch Fund.Author details1School of Computing, University of South Alabama, Mobile, Alabama36688-0002, USA. 2Computer and Information Science Department, Universityof Oregon, Eugene, Oregon 97403-1202, USA. 3Miracle Query, Inc., Eugene,Oregon 97403-1202, USA. 4Department of Philosophy, University at Buffalo,Buffalo, New York 14260-4150, USA. 5Genome Informatics, The JacksonLaboratory, Bar Harbor, Maine 04609-1523, USA. 6Department of BiomedicalInformatics, University of Utah, Salt Lake City, Utah 84112-5775, USA.7Department of Biochemistry and Molecular and Cellular Biology, GeorgetownUniversity Medical Center, Washington D.C. 20007-1485, USA. 8Center forComputational Science, University of Miami, Miami, Florida 33146-2960, U.S.A.9Department of Microbiology and Immunology, First Affiliated Hospital,Kunming Medical University, Kunming, Yunnan 650032, China. 10Departmentof Radiation Oncology, Washington University School of Medicine, St. Louis,Missouri 63110-0001, USA. 11Mitchell Cancer Institute, University of SouthAlabama, Mobile, Alabama 36604-1405, USA. 12Department of Biology,University of South Alabama, Mobile, Alabama 36688-0002, USA. 13School ofDental Medicine, University at Buffalo, Buffalo, New York 14214-8006, USA.Received: 15 December 2015 Accepted: 12 April 2016Funk et al. Journal of Biomedical Semantics  (2016) 7:52 DOI 10.1186/s13326-016-0096-7RESEARCH Open AccessGene Ontology synonym generation ruleslead to increased performance in biomedicalconcept recognitionChristopher S. Funk1*, K. Bretonnel Cohen1, Lawrence E. Hunter1 and Karin M. Verspoor2,3AbstractBackground: Gene Ontology (GO) terms represent the standard for annotation and representation of molecularfunctions, biological processes and cellular compartments, but a large gap exists between the way concepts arerepresented in the ontology and how they are expressed in natural language text. The construction of highly specificGO terms is formulaic, consisting of parts and pieces from more simple terms.Results: We present two different types of manually generated rules to help capture the variation of how GO termscan appear in natural language text. The first set of rules takes into account the compositional nature of GO andrecursively decomposes the terms into their smallest constituent parts. The second set of rules generates derivationalvariations of these smaller terms and compositionally combines all generated variants to form the original term. Byapplying both types of rules, new synonyms are generated for two-thirds of all GO terms and an increase in F-measureperformance for recognition of GO on the CRAFT corpus from 0.498 to 0.636 is observed. Additionally, we evaluatedthe combination of both types of rules over one million full text documents from Elsevier; manual validation and erroranalysis show we are able to recognize GO concepts with reasonable accuracy (88 %) based on random sampling ofannotations.Conclusions: In this work we present a set of simple synonym generation rules that utilize the highly compositionaland formulaic nature of the Gene Ontology concepts. We illustrate how the generated synonyms aid in improvingrecognition of GO concepts on two different biomedical corpora. We discuss other applications of our rules for GOontology quality assurance, explore the issue of overgeneration, and provide examples of how similar methodologiescould be applied to other biomedical terminologies. Additionally, we provide all generated synonyms for use by thetext-mining community.Keywords: Biomedical concept recognition, Named entity recognition, Text-mining, Gene ontologyBackgroundThe Gene Ontology (GO) represents the standard bywhich we refer to functions and processes that genes/geneproducts participate in. Due to its importance in biologyand the exponential growth in the biomedical literatureover the past years, there has been much effort in utiliz-ing GO for text mining tasks [1, 2]. Performance on theserecognition tasks is lacking; it has been previously seen*Correspondence: christopher.funk@ucdenver.edu1Computational Bioscience, University of Colorado School of Medicine, AuroraCO 80045, USAFull list of author information is available at the end of the articlethat there is a large gap between the way concepts are rep-resented in the ontology and the many different ways theyare expressed in natural text [35].There are twomain applications of biomedical literaturemining where improved recognition of Gene Ontologycan improve downstream performance. 1) It is well knownthat manual curation can no longer keep up with theannotation of gene and protein function [6]. Automaticannotation is not our direct goal, but utilizing automaticmethods to highlight functions could provide input tocurators to help speed up manual curation. The moreaccurate automated methods become, the more useful© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 2 of 16their application becomes inmanual curation. 2) Themin-ing of GO concepts from large collections of biomedicalliterature has been shown to be useful for biomedical dis-covery, for example, pharmacogenomic gene prediction[7] and protein function prediction [8, 9]. Providing thesediscovery algorithms with not only cleaner, but more data,could increase the ability their accuracy of prediction andgeneralizability.Identification of gene ontology concepts in unstructuredtextThere are two main methods of identifying GO con-cepts within unstructured text, dictionary lookup andpattern/similarity based measures. Unfortunately, therehave been very few evaluations assessing the ability torecognize and normalize Gene Ontology concepts fromthe literature; this is mostly due to lack of gold-standardannotations.There are sub-tasks within the BioCreative I and IV[2, 10] community challenges that involve similar, butmore involved, tasks to GO term recognition  relat-ing relevant GO concepts given protein-document pairs.While the methods utilized for this specific tasks arebeyond the scope of this work, some systems utilize thesecorpora to evaluate their ability to identify GO conceptson unstructured text. Ruch et al. [11] implement pat-tern based matching on a 5 token window and a vectorspace indexing model. Their GO pattern based matchingreports highest average precision of 0.07 while their index-ing model reports highest precision at recall = 0 (0.15) onthe BioCreative I corpus. Gaudan et al. [12] utilize proxim-ity, specificity, and similarity to calculate the score of GOterm t appearing in zone z. They report average precisionand recall of 0.34 for the terms at rank 1 on the BioCreativeI corpus. A more recent system, GOCat [13], combinessemantic similarity and a machine learning based k-NNalgorithm to return the most similar k GO concepts insome text. On the Biocreative I corpus, GOCat reports0.56 precision at recall 0.20 (F-measure = 0.29). A pitfallof these types of algorithms is they do not identify theexact span of text that matched the GO concept. Theyonly specify that the concept could be present within thissentence or document.Dictionary based methods identify the exact span oftext that corresponds to the GO concept. Previous workevaluated concept recognition systems utilizing the Col-orado Richly Annotated Full Text Corpus (CRAFT). Funket al. [14] evaluated three prominent dictionary-based sys-tems (MetaMap, NCBO Annotator, and ConceptMapper)and found Cellular Component was able to be recog-nized the best (F-measure 0.77). The more complex termsfrom Biological Process (F-measure 0.42) and MolecularFunction (F-measure 0.14) were much more difficult torecognize in text. Campos et al. present a frameworkcalled Neji and compare it against Whatizit on theCRAFT corpus [15]; they find similar best performance(Cellular Component 0.70, Biological Process/MolecularFunction 0.35). Other work explored the impact of casesensitivity and information gain on concepts recogni-tion and report performance in the same range as whathas previously been published (Cellular Component 0.78,Biological Process/Molecular Function 0.40) [16]. Sinceall previously publishedmethods utilized dictionary basedsystems and report similar performance, there is a needfor more sophisticated methods of utilizing the informa-tion contained within the Gene Ontology. For furtherprogress to be made, the gap between concept representa-tion and their expression in literature needs to be reduced,which serves as major motivation for the work presentedin this manuscript.There have been efforts to increase the ability to recog-nize biomedical concepts through enumerating variabilityin terms through generation, rewriting, and suppressionrules. Tsuruoka et al. [17] generate spelling and punctu-ation variants based upon probabilistic generation ruleslearned from 84,000 MEDLINE abstracts. These typesof rules help to capture the surface variability withinconcepts, such as type I, Type I, type i, etc. Hettneet al. [18] implement rewriting and suppression rules forto reduce the variability in UMLS concepts. For identifica-tion of terms, they remove leading parentheses/bracketsand filter out some semantic types. Additionally, the sup-press certain terms that should not be matched on, i.e.only EC numbers or those that contain dosages. While therules presented here do not specifically utilize the meth-ods described above, the same underlying principles areincorporated.Compositionality of the gene ontologyThe structure of concepts from the Gene Ontology hasbeen noted by many to be compositional [1921]. A termsuch as GO:1900122 - positive regulation of receptorbinding contains another concept GO:0005102 - recep-tor binding; not only do the strings overlap, but the termsare also connected by relationships within the ontology.Ogren et al. explore more in detail terms as proper sub-string of other terms [19]. Additionally, previous workexamined the compositionality of the GO and employedfinite state automata (FSA) to represent sets of GO terms[20]. An abstracted FSA described in that work can beseen in Fig. 1. This example shows how terms can bedecomposed into smaller parts and how many differentterms share similar compositional structure. While usingregular expressions are useful for simple terms, there aremore complex concepts that require more sophisticateddecomposition.To facilitate generation of meaning (cross-product def-initions) and consistency within the ontology, a systemFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 3 of 16Fig. 1 Finite state automata representing activation, proliferation, and differentiation GO terms. An abstracted FSA adapted from a figure in Ogrenet al. [20] that shows how a particular term can be decomposed into its smaller components; where cell type can be any specific type of cellcalled Obol [22] was developed. This work involvedanalyzing the structure of terms through the creationof grammars to decompose and understand the for-mal language underlying the GO. An example grammardescribing the positive regulation of a molecular functionterm follows: process(P that positively_regulates(F)) ?[positive],regulation(P),[of ],molecular_function(F). Thesegrammars serve as templates for the decompositionalrules utilized in this work. Recently, GO has been movingaway from pre-computed term, towards post-computedon-the-fly creation of terms for annotations using cross-products [23]. Additionally, TermGenie [24] was devel-oped, using a pattern-based approach, to automaticallygenerate new terms and place them appropriately withinthe Gene Ontology. This work dealt with the analysis andgeneration of new terms for curation, but no work hasbeen focused on synonym generation.There has been previous work using the compositionalnature and common syntactic patterns within the GeneOntology itself to automatically generate lexical elemen-tary synonym sets [25]. This method generates a totalof 921 sets of synonyms with a majority being generatedfrom 13 terms; 80 % of the sets refer to orthographic{synthase, sythetase}, chemical products {gallate, gallicacid}, or Latin inflection {flagella, flagellum}. We believethis method is complementary to what we present here.In this work, we manually created these sets, along withmany more, through analysis of Gene Ontology annota-tions in unstructured text. Additionally we go beyond andincorporate derivational variants, i.e. flagella?flagellar,which have been shown to be very useful for capturingthe natural language text of concepts. We were currentlyunable to find them publicly available, but if we should, thesynonym sets by Hamon et al. could be seamlessly inte-grated within the synonym generation rules we presenthere.Other work takes advantage of the structure of the GeneOntology and relationships between GO terms to showthat these properties can aid in the creation of lexicalsemantic relationships for use in natural language pro-cessing applications [26]. Besides compositionality, previ-ous work tries to identify GO terms that express similarsemantics that use distinct linguistic conventions [27].They find, in general, that concepts from the Gene Ontol-ogy are very consistent in their representation (there aresome exceptions but these are quality issues that theconsortium would like to avoid or fix). The consistencyof term representation along with the underlying com-positional structure suggests the effective generation ofsynonyms for many terms using only a small number ofrules.Current synonyms are not sufficient for text-miningThe identification of Gene Ontology terms is more dif-ficult than many other types of named entities such asgenes, proteins, or species mainly due to the length [14]and complexity of the concepts. To help illustrate this, weexamined all variations of the GO term GO:0006900 -membrane budding within the CRAFT corpus. The entryfor this concept in the ontology file is seen below. Likemost other terms, the concept name appears as a nounand the entry contains a few synonyms (Table 1).There were eight varying expressions of membranebudding in all of CRAFT, five of which are containedTable 1 Example ontology entry for the concept membrane buddingid: GO:0006900name: membrane buddingnamespace: biological_processdef: "The evagination of a membrane resulting in formation of a vesicle."synonym: "membrane evagination" EXACTsynonym: "nonselective vesicle assembly" RELATEDsynonym: "vesicle biosynthesis" EXACTsynonym: "vesicle formation" EXACTis_a: GO:0016044 ! membrane organization and biosynthesisrelationship: part_of GO:0016192 ! vesicle-mediated transportFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 4 of 16within a single article about expression and localizationof Annexin A7 (PMID:12925238). In Table 2 we list theannotations along with sentential context. We find thatusing exact matching and context from the ontology file,the first two examples can be identified, but the otherscannot. This one example illustrates that a rather simpleterm can be expressed in natural language text in manydifferent ways, that convey identical semantic meaning.Objectives of this workWe hypothesize that due to the highly formalized andcompositional nature of the Gene Ontology [19], a smallnumber of generation rules can help to automatically gen-erate synonyms for current and novel GO concepts, dif-ferentiated from GO synonyms as generated synonyms.Additionally, we hypothesize that the variation capturedin these generated synonyms will allow for better recog-nition of Gene Ontology concepts from the biomedicalliterature. We are aware that our method might overgen-erate like Blaschke et al. [28], but we also hypothesize thatthose generated synonyms probably will not be found inthe biomedical literature, and therefore, will not hinderperformance.In this work, we present 18 manually created rulesto facilitate generation of synonyms from the entiretyof the Gene Ontology. We evaluate these automaticallygenerated synonyms both intrinsically, on a gold stan-dard corpus, and extrinsically, through manual valida-tion of annotations from a large literature collection.We show that these automatically generated synonymsincrease recognition of GO concepts over any publishedresults and illustrate the accuracy and impact the gen-erated synonyms have at a large scale. Additionally, weshow that the principles behind the rules generalizes tonovel GO concepts. It is the goal to generate and releasethese generated synonyms for the larger biomedical nat-ural language processing community. Currently, we donot suggest that all generated synonyms be consideredfor addition to GO, but filtering and classification meth-ods could be employed to suggest the most accurategenerated terms as synonyms. Not only does this workapply to the two tasks mentioned above, but it also addsthe ability to generate synonyms for newly created GOconcepts.Table 2 Examples of the membrane budding concept within asingle documentLipid rafts play a key role inmembrane budding. . .Having excluded a direct role in vesicle formation. . .. . . involvement of annexin A7 in budding of vesicles. . . Ca2+-mediated vesiculation process was not impairedRed blood cells which lack the ability to vesiculate cause. . .MethodsMethodological overviewThe main idea behind our method is made up of threedifferent steps:1. Recursively decompose each Gene Ontology term toits constituent terms2. Generate derivational variants for each of theseconstituent terms3. Recombine all forms of all constituent terms (theconstituent term itself, the generated derivationalvariants, and current synonyms of constituent termin the Gene Ontology) using differing syntactic andlexical rulesThis methodology is made possible due to the highlyformalized and compositional nature of the GeneOntology.Returning to the membrane budding example pre-sented above (Table 2), we illustrate the methodologybehind creation and application of our rules. By analyz-ing the different ways membrane budding is expressedin CRAFT, we find that a majority of the annotationsare phrased around the end product, the vesicle. To helprecognize these (currently) un-recognizable annotationsthere are two steps that should be done: 1) reorder wordsand change the syntax (budding of vesicles) and 2) gen-erate derivational variants of vesicle (vesiculation andvesiculate). We developed two classes of rules that inter-act seamlessly to generate these types of synonym varia-tion. The first we designate recursive syntactic and thesecond derivational variant, which are discussed imme-diately below. Each of our rules was manually createdthrough the analysis of the differences between conceptannotations within the gold standard CRAFT corpus andthe Gene Ontology itself, along with discussions with anontologist and biologist about how they most frequentlyexpress certain concepts. A more in-depth example ispresented within the description of the individual rules.Recursive syntactic rulesThe recursive syntactic rules perform step 1 & 3 outlinedin the Methodological overview section. The recursiverules, step 1, were developed through studying the gram-mars used in Obol [22], utilizing the dependency parse ofthe Gene Ontology terms from ClearNLP [29], and exam-ining common formalizations within Gene Ontology con-cepts. These represent semi-frozen expressions as anchorsto identify the constituent terms. The lexical and syntac-tic recombination rules, step 3, were derived by studyingthe transformations required to get from Gene Ontologyterm to the gold standard annotations that appear inCRAFT. Additionally, there were many discussions withbiologists on the variation in terminology in which theycould express the same concept.Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 5 of 16We identified 11 cases when terms can be broken downinto smaller composite terms; we acknowledge that thereare more, but choose to focus on the ones that affected themajority of concepts. Over 55 % (14,221 out of 25,471) ofthe Gene Ontology concepts can be decomposed using atleast one of these 11 different cases. Through our analy-sis we have developed an ordering for rule application, togenerate the most possible synonyms. The 11 cases, theorder, and examples applied are presented in Table 3; forfull enumeration and further explanation of all rules seeAdditional file 1.Derivational variant rulesOnce the original term is broken down to its constituentcomponents, step 1, through the recursive syntactic rulespresented above, we can apply derivational variant gen-eration rules, step 2. The goal of this step is to generatesynonyms that reflect the broader range of variabilitythat occurs in natural language text expression of GeneOntology concepts.We incorporate two open source toolsto generate the derivational variants, WordNet [30] andLexical Variant Generator [31]. There are a total of sevendifferent specific cases when we apply these derivationalgeneration rules (Table 4). These rules were developedby examining the transformations needed to create thetext spans annotated in the CRAFT gold standard fromthe information contained within the GO. For example,for single word terms we would generate both verb andadjective forms of the noun concept, if they exist, whichwould then both be incorporated compositionally withinthe more complex concepts. For additional explanationand full enumeration of the rules see Additional file 1.Example of rules appliedIn Fig. 2 we walk through all three steps of the syn-onym generation process with the concept GO:00507678- negative regulation of neurogenesis.1. It is decomposed into 2 constituent terms: 1)negative regulation of and 2) another GO concept GO:0022008 - neurogenesis. Since it cannot bedecomposed any further, we begin generatingsynonyms for both of these composite parts.2. Derivational variants for the term neurogenesis aregenerated utilizing the single word term rule. Thereare three different forms of neurogenesis, the termitself, the adjective form exists in WordNet [30] orcan be generated through LVG (lexical variantTable 3 Recursive syntactic rules order, constituent terms, and example generated synonymsOrder Rule GO term Constituent terms Generated synonyms1 via or involved intermsGO:0002679 - respiratory burstinvolved in defense responserespiratory burst, defenseresponsedefense response associated respiratoryburst2 regulation of terms GO:0030513 - positive regulationof BMP signaling pathwayBMP signaling pathway positive regulation of BMP receptorpathway, up-regulation of BMP receptorsignaling3 response to terms GO:0034263 - autophagy inresponse to ER overloadautophagy, ER overload ER overload responsible for autophagy,autophagy response to ER overload4 signaling terms GO:0035329 - hippo signaling hippo hippo signaling pathway, signaling ofhippo5 biosynthetic processtermsGO:0042095 - interferon-gammabiosynthetic processinterferon-gama interferon-gamma biosynthesis,production of interferon-gamma6 metabolic processtermsGO:0042120 - alginic acidmetabolic processalginic acid metabolism of alginic acid, alginic acidmetabolism7 catabolic processtermsGO:0042190 - vanillin catabolicprocessvanillin vanillin degradation, breakdown ofvanillin8 binding terms GO:0042314 - bacteriochlorophyllbindingbacteriochlorophyll binding of bacteriochlorophyll, bacteri-ochlorophyll bound9 transport terms GO:0042876 - aldarate transmem-brane transporter activityaldarate, transmembrane transportation of aldarate across themembrane, transporting aldaratetransmembrane10 differentiation terms GO:0043158 - heterocyst differenti-ationheterocyst heterocyst cell differentiation, differenti-ation into heterocyst11 activity terms GO:0043492 - ATPase activity, cou-pled to movement of substancesATPase, coupled to move-ment of substancesATPase, coupled to movement ofsubstances, coupled to movement ofsubstances activity of ATPaseWhile these examples show only one rule applied at once, each constituent term identified recursively goes through each rule in the order outlined to determine the mostbasic constituent terms, which will get derivational variations (discussed in next paragraph) and then combinatorially re-combined into generated synonyms of the originaltermFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 6 of 16Table 4 Individual derivational variant generation rulesOrder Rule Rule defined GO terms Example derivations1 Single word terms 1 {NN} ? {JJ} 1 GO:0043066 - negative regu-lation of apoptosis1 apoptotic down regulation2 {NN} ? {VB} 2 GO:0023040 - signaling viaionic flux2 signaled via ionic flux2 Double word terms 1 {NN_1 NN_2} ? {NN_1},{VB_2 NN_1}, {JJ_1 NN_2},{NN_1 JJ_2}1 GO:0048666 - neuron devel-opment1 neural development, neuroticdevelopment, neuronal develop-ment2 {JJ_1 NN_2} ? {JJ_1}, {JJ_1JJ_2}2 GO:0005576 - chromosomalregionchromosomal, chromosomeregion3 Triple word terms 1 {NN_1 NN_2 NN_3}? {NN_1NN_3}, {NN_3 NN_1}, {VB_3}1 GO:0052386 - cell wall thick-ening1 thickened wall, cellthickening, thickens cell wall4 cell part terms Introduce and re-order cell parttermsGO:0035452 - extrinsic com-ponent of plastid membraneperipheral to plastid membrane,extrinsic to plastid membrane5 sensoryperception termsIntroduce variants of the sense- sensory perception of {NN}GO:0050909 - sensory percep-tion of tastegustory, gustation6 transcription,X-dependenttermsIntroduce variants of transcrip-tionGO:0006410 - transcription,RNA-templatedRNA-dependent reverse tran-scription, RNA-dependentRT7 X strand annealingactivity termsIntroduce variants of anneal-ingGO:0033592 - RNA strandannealing activityRNA hybridization, hybridizeThe seven patterns that we generate derivational variants are presented along with examples of each. While these are presented individually, all derivational and recursivesyntactic (presented in Table 3) interact at each step. The examples provided are single GO terms, but any of the constituent terms produced through the above steps will gothrough all derivational rules, if possible. The bolded words in the GO Term and Synonyms generated column represent the impact of the rule. The Penn Treebankpart-of-speech (POS) tags are utilized below: NN = noun, VB = verb, JJ = adjective. All varying forms were converted to the basic POS tag, e.g. NNS = plural noun and wereconverted to NNgenerator) [31], and the current synonyms foundwithin the Gene Ontology.3. There are no derivational variants of negativeregulation of, but there are syntactic and lexicalsynonymous expressions enumerated in theregulation of terms rule. To generate synonyms ofthe original concept, the three forms ofneurogenesis are combinatorially combined withthe 12 different synonymous expressions of negativeregulation of to form 36 synonyms for the originalterm; the Gene Ontology currently only has 4synonyms for this concept.ConceptMapperConceptMapper (CM) is an open source highly con-figurable dictionary lookup tool created for identifyingnamed entities in text. CM is part of the Apache UIMASandbox [32] and is available at http://uima.apache.org/d/uima-addons-current/ConceptMapper. Version 2.3.1 wasused for these experiments.The first step in the CM pipeline is to convert the GOontologies to the required XML dictionary format. Thedocument text is then provided and tokenized. All tokenswithin a span, in this case a sentence, are looked upin the dictionary using a configurable lookup algorithm.The lookup algorithm has the ability to reorder words,insert gaps, ignore words, identify all or only longestmatch, etc. For each branch of GO we used the highestperforming parameter combination previously identified[14]. Additional file 2 provides a summation of the dif-ferent type of ConceptMapper parameters and shows theexact parameter combinations used for recognition ofeach sub-branch of the Gene Ontology.Concept recognition pipeline and baselinesThe baseline for GO recognition was established in pre-vious work [14] through parameter analysis of three dif-ferent concept recognition systems. The top performingsystem, ConceptMapper (CM), is used for the followingtest because it produced the highest F-measures on 7 outof 8 ontologies in the CRAFT corpus. CM takes an obo fileand converts it to an xml dictionary, which is used to rec-ognize concepts in free text. In analyzing the results thereare two different baselines that were provided. Both base-lines use the same ConceptMapper parameters settingsbut differ in the way the dictionary was created: B1, a CM dictionary containing only informationwithin the ontology obo file. B2, a CM dictionary that deletes the word activityfrom molecular function terms containing that word(for example, for term GO:0016787 - hydrolaseFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 7 of 16Fig. 2 Three steps of synonym generation applied. A single GO concept broken down into its composite parts (bolded and underlined), synonymsgenerated for each part (text underneath the part), then combination of all synonyms from all composite parts to form complete synonym of theoriginal conceptactivity a synonym of hydrolase is added). Thisaddresses a known property of molecular functionterms formalization that aims to separate of theprotein and the function of the protein.For the intrinsic evaluation pipeline on the CRAFT cor-pus, we use the version of GO used to annotate CRAFTfrom November 2007. We are aware of the great numberof changes made, but this was purposefully done to keepthe concepts available to the dictionary the same that wereavailable to the annotators when they marked up the goldstandard. To show that the rules created are able to gen-eralize and apply to the many new concepts added to theGene Ontology added since 2007, for the extrinsic evalu-ation on large collection we use an updated version of theGO from 9/25/2015.Evaluation corporaThere are two different corpora utilized in evaluation ofour generated synonyms.CRAFT corpusThe gold standard used is the Colorado Richly AnnotatedFull-Text (CRAFT) Corpus [33, 34] version 1.0 releasedOctober 19th, 2012. The full CRAFT corpus consists of 97completely manually annotated biomedical journal arti-cles, while the public release set, which consists of 67documents, was used for this evaluation. CRAFT includesover 100,000 concept annotations from eight differentbiomedical ontologies. Even though the collection is smallrelative to the size of PubMed, there is no other cor-pus that has text-level annotations of Gene Ontologyconcepts.Large literature collectionTo test generalization and for further analysis of theimpact our concept recognition can have, we utilizeda large collection of one million full-text articles fromElsevier. This is a collection of full-text documents froma wide variety of biomedical Elsevier journals that wasdelivered to the University of Colorado for internalanalysis.Evaluation of generated synonymsTo evaluate the synonyms given we use the same pipelinesdescribed in Funk et al. [14]. Synonyms are generatedby each method and then only those that are unique(both within the generated synonyms and GO itself )are inserted into a temporary obo file. The temporaryobo file is then used to create an xml dictionary usedby ConceptMapper [35] for concept recognition. TheCRAFT corpus is used as the gold standard and preci-sion, recall, and macro-averaged F-measure are reportedfor each branch of the GO.We provide counts of conceptsalong with changes from the evaluation on the large scalecorpus.Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 8 of 16Results and discussionApplication of gene ontology synonym rulesTo explore the impact that our rules had on the recogni-tion of concepts from the biomedical literature, we appliedour synonym generation rules to two different version ofthe Gene Ontology and compared the concepts identifiedbefore/after application on two different biomedical cor-pora. For evaluation on the CRAFT corpus, we appliedour rules to the CRAFT annotated version of GO con-taining 25,471 concepts; our 18 rules generated 291,031synonyms for 16,800 concepts (66 % of all concepts).Because the CRAFT version is from 2007, we appliedour methodology to a more recent version of GO fromSeptember 2015. On this recent version, our rules gener-ated ?1.5 million unique, but unconfirmed, synonyms for66 % of all GO concepts (27,610 out of 41,852). Only a fewrules, 18, can have wide applicability to a majority of theconcepts in the Gene Ontology due to the concepts beinghighly formalized and exhibiting a compositional nature.While our rules appear to overgenerate, the main focus ofthis work is to improve recognition of GO concepts fromthe biomedical literature; we expect overgeneration to notdecrease performance because a majority of the generatedsynonyms will not be seen in the biomedical literature.An easy method of reducing the overgeneration wouldbe to only include the generated synonyms that currentlyappear in all of MEDLINE or through an exact Googlesearch.Not only do the introduced rules generate newsynonyms, but are also able to recreate 67 % of allsynonyms (68,174 out of 101,615) from all conceptson the 2015 version. This illustrates the usefulnessof our presented methodology for not only synonymgeneration for ontology curation and enhance-ment. We now focus on how introducing variationthrough synonym generation aids in identificationof Gene Ontology concepts from the biomedicalliterature.Synonym evaluation on a gold standardThe overall results for all methods performance onCRAFT can be seen in Table 5 with more detailed anal-ysis of each method following. More details about howwe evaluated performance of each method can be seen inEvaluation of generated synonyms.Besides the rules presented, there are a number of man-ually curated external mappings from Gene Ontologyconcepts to other data sources such as UniProt [36], theBrenda database [37], andWikipedia [38]. To test the use-fulness of these mappings as sources of synonyms, weimputed synonyms for the Gene Ontology concept fromsynonyms of the linked concept in the respective datasource. Overall, we find that external ontological map-pings introduce significantly more errors than correctlyrecognized concepts and are not suggested to be useful,in their current form, as a whole, for concept recognition(methods and detailed analysis of each data source can beseen in Additional file 3).Overall, the best results are obtained by using bothsyntactic recursive and derivational rules; an increase inF-measure of 0.112 is seen (0.610 vs 0.498). This per-formance gain is the result of a large increase in recall(0.225) with only a modest decrease in precision (0.049).Examining the overall performance we find that all meth-ods perform better than B1, while all but the externalsynonyms perform better than B2. Overall, all genera-tion methods increase recall with a decrease in precision,which is to be expected when adding synonyms. We nowdiscuss the impact of synonyms generated through bothclasses of rules.Performance impact of generated synonymsThe Gene Ontology is broken down into three sub-ontologies, Cellular Component (CC), Biological Process(BP), and Molecular Function (MF). Terms from eachsub-ontology have differing biological meaning and tex-tual characteristics  some rules are more applicableto one sub-ontology than another, so we evaluate themseparately. We apply only the recursive syntactic rules(Steps 1 & 3, described in Methodological overview)to all concepts within the Gene Ontology and evalu-ate on the full-text CRAFT corpus using our dictionarybased lookup system ConceptMapper; performance canbe seen in Table 6. For Cellular Component, only afew new synonyms are generated, which is not surpris-ing, because concepts from this branch normally do notTable 5 Micro-averaged results for each synonym generation method on the CRAFT corpusMethod TP FP FN Precision Recall F-measureBaseline (B1) 10,778 6,280 18,669 0.632 0.366 0.464Baseline (B2) 12,217 7,367 17,230 0.624 0.415 0.498All external synonyms 12,747 11,682 16,704 0.522 0.433 0.473Recursive syntactic rules 12,411 7,587 17,036 0.621 0.422 0.502Recursive syntactic and derivational rules 18,611 10,507 10,836 0.639 0.632 0.636Bold highlighting indicates the method that produces the highest F-measureFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 9 of 16Table 6 Performance of manual Gene Ontology rules on the CRAFT corpusMethod Generated synonyms Affected terms TP FP FN P R FCellular Component (CC)Baseline (B1) X X 5,532 452 2822 0.925 0.662 0.772Baseline (B2) X X 5,532 452 2822 0.925 0.662 0.772Syntactic recursion rules 23 21 5,532 452 2,822 0.925 0.662 0.772Both rules 4,083 724 6,585 969 1,769 0.872 0.788 0.828Molecular Function (MF)Baseline (B1) X X 337 146 3,843 0.698 0.081 0.145Baseline (B2) X X 1,772 964 2,408 0.648 0.424 0.512Syntactic recursion rules 11,637 7,353 1,759 977 2,421 0.643 0.421 0.509Both rules 14,413 7,401 2,422 1,074 1,758 0.693 0.579 0.631Biological Process (BP)Baseline (B1) X X 4,909 5,682 12,004 0.464 0.290 0.357Baseline (B2) X X 4,913 5,951 12,000 0.452 0.291 0.354Syntactic recursion rules 182,617 6,847 5,120 6,158 11,793 0.454 0.303 0.363Both rules 272,535 8,675 9,604 8,464 7,309 0.532 0.568 0.549All Gene OntologyBaseline (B1) X X 10,778 6,280 18,669 0.632 0.366 0.464Baseline (B2) X X 12,217 7,367 17,230 0.624 0.415 0.498Syntactic recursion rules 194,277 14,221 12,411 7,588 17,036 0.621 0.422 0.502Both rules 291,031 16,800 18,611 10,507 10,836 0.640 0.632 0.636Bold highlighting indicates where the generated synonyms have a positive effect on the performanceappear compositional in nature. These new CC have noimpact when compared to the baselines.Eighty six percent (7,353 out of 8,543) of terms withinMolecular Function had at least one new synonym gen-erated by the recursive syntactic rules. Unexpectedly,performance on MF slightly decreases. The performanceon Biological Process slightly increases with the additionof recursive syntactic rules. BP sees the largest increasein the number of new synonyms generated, with over180,000 new synonyms for 46 % (6,847 out of 14,767) ofBP concepts. The syntactic recursive rules are most help-ful in generating Biological Process synonyms that matchinstances within CRAFT. For example, 74 more correctinstances of GO:0016055 - Wnt receptor signaling path-way, expressed in the gold standard as Wnt signalingand Wnt signaling pathway, are able to be identified.These are generated through the signaling terms rulewhich found that both the words receptor and path-way were uninformative.MF and BP share similarities in the kinds of errorsintroduced: a true positive (TP) in the baseline is con-verted to a false positive (FP) and false negative(s) (FN)because a longer term is identified through one of the gen-erated synonyms (one ConceptMapper parameter usedspecifies that only the longest match is returned). It is pos-sible that these are missing annotations within the goldstandard. For example, one of the generated synonyms forGO:0019838 - growth factor binding is binding growthfactor. In the corpus, bound growth factor is annotatedwith both GO:0005488 - binding and GO:0008083 -growth factor activity. With our generated synonymsadded to the dictionary, the same text span is only anno-tated with the more specific GO:0019838 - growth factorbinding which results in the removal of two true positivesand the introduction of one false positive, thus reducingoverall performance, but possibly increasing the accuracyof annotations. If this is a wide-spread issue, changing theparameters for our dictionary lookup will allow it to findall concepts, which would identify all three annotationsinstead of only the longest one.Overall, despite the decrease in performance of Molec-ular Function terms, the recursive syntactic rules slightlyimprove concept recognition of the Gene Ontology on theCRAFT corpus over baseline 2 (?200 more TPs and?200more FPs introduced). Because the CRAFT corpus con-tains only a small portion of the whole GO (1,108) andthese rules only account for reordering of tokens withinGO, we did not expect to see a large increase in conceptrecognition performance.When we apply both the recursive syntactic and deriva-tional rules (Steps 1, 2 & 3, described in MethodologicalOverview) to all concepts and evaluate on the full-textFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 10 of 16CRAFT corpus we see improvements for all branches(Table 6). (The derivational rules cannot be evaluated ontheir own due to an implementation dependency to therecursive syntactic rules. The derivational rules assumethat all concepts passed in will already be decomposedinto their smallest GO components. The real power comeswhen combining both rules because variation is beingintroduced in only parts of the longer GO concepts.) Eachbranch has different properties and when evaluated indi-vidually, we see an increase in F-measure for all. Thisincrease is due to a large gain in recall (up to 0.27). Forboth Biological Process and Molecular Function, preci-sion also increases, while precision slightly decreases forCellular Component. When performance is aggregatedover all branches of the Gene Ontology, an increase inF-measure of 0.14 (0.498 vs. 0.636) is seen; this comesfrom both an increase in recall (0.22) and precision (0.02).Our rules introduce ?291,000 generated synonyms whichcover 66 % (16,800 out of 25,471) of all terms within GO.Analysis of generated synonymsNow we explore which generated synonyms contributethe most to the increase in performance seen on thegold standard corpus. The top 5 concepts that impactthese performance numbers are presented in Table 7.For Cellular Component, the most helpful synonymgenerated immunoglobulin?antibody is seen manytimes within CRAFT and is contained within the dou-ble word rule. The other four are generated using thesingle word rule, specifically converting from the nounform from the ontology to the adjective form. Throughexamining Molecular Function terms, it became clearthat annealing was missing synonymous representationwithin the Gene Ontology; within the annealing rule weadd a synonym of hybridization. Two of the next mosthelpful synonyms are due to excluding low informationcontaining words and derivational variations. It shouldbe noted that within Molecular Function an even largerincrease in performance is seen between baseline 1 and2 (Table 6), which takes into account the many activityterms. These types of synonyms are also accounted forin our rules and are compositionally combined into otherterms. For Biological Process we observe that the mosthelpful synonyms are generated using the double word andsingle word derivational rules.We also find that generatingdifferent lexical forms of both single word concepts andwithin longer terms helps to introduce many true positiveannotations.From examining the top most helpful synonyms, weprovide evidence that the derivational synonyms improveperformance on a manually annotated corpus throughthe introduction of more linguistic variability, whichdecreases the gap between concepts in the ontology andtheir expression in natural language text. Overall, the topTable 7 The top 5 derivational synonyms that improve performance on the CRAFT corpusGO ID Term name TP FP FN Generated synonymsCellular ComponentGO:0019814 Immunoglobulin complex +548 +0 ?548 Antibody, antibodiesGO:0005634 Nucleus +218 +35 ?218 Nuclear, nucleatedGO:0005739 Mitochondrion +135 +0 ?135 MitochondrialGO:0031982 Vesicle +11 +3 ?11 VesicularGO:0005856 Cytoskeleton +15 +0 ?15 CytoskeletalMolecular FunctionGO:0000739 DNA strand annealing activity +327 +1 ?327 Hybridized, hybridization, annealing, annealedGO:0033592 RNA strand annealing activity +327 +1 ?327 Hybridized, hybridization, annealing, annealedGO:0031386 Protein tag +6 +79 ?6 TagGO:0005179 Hormone activity +1 +0 ?1 HormonalGO:0043495 Protein anchor +1 +10 ?1 AnchorBiological ProcessGO:0010467 Gene expression +2235 +361 ?2235 Expression, expressed, expressingGO:0007608 Sensory perception of smell +445 +1 ?445 OlfactoryGO:0008283 Cell proliferation +97 +71 ?97 Cellular proliferation, proliferativeGO:0007126 Meiosis +93 +2 ?93 Meiotic, meioticallyGO:0006915 Apoptosis +173 +2 ?173 ApoptoticThe GO terms that increase performance the most on CRAFT are along with the change () in number of true positives (TP), false positives (FP), and false negatives (FN) fromthe baseline B2 (activity removed baseline). The generated synonyms that result in this increase are shown under Generated synonymsFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 11 of 16generated synonyms that improve performance do nottake into accountmuch of the compositional nature of GOterms. We believe this is due to two aspects; 1) The anno-tation guidelines used to define what constitutes a correctmention of a GO concept in CRAFT [39] and 2) CRAFTis only a small representation of what is contained withinthe entire biomedical literature. This small representationis due to the paper content (only mouse papers resultingin functional annotation of at least one protein), small cor-pus size, and appearance of only a small subsection of theGene Ontology. To further evaluate the synonyms gener-ated by our rules without the aforementioned drawbacks,in the next section, we explore the impact our rules makeon a large collection of the biomedical literature.Evaluation of generated synonyms on a large full textcollectionWe evaluated the impact of synonyms generated by bothrecursive syntactic and derivational variant rules have onthe ability to recognize GO concepts within a large collec-tion of one million full text documents. Unlike the previ-ous evaluation, these documents do not have any manualannotation or markup of Gene Ontology concepts, so weare unable to calculate precision/recall/F-measure. How-ever, we can calculate descriptive statistics and performmanual evaluation of a random sample of the differencesin annotations produced when our rules are applied. Forthese we used a version of GO from September 2015.Applying our rules generates ?1.5 million new synonymsfor 66 % of all GO concepts (27,610 out of 41,852).Since one of the primary focuses of the Gene Ontologyis functional annotation of proteins, we imparted some ofthat knowledge into the large scale analysis by calculat-ing information content of each concept with respect tothe experimental UniProt GOA annotations [40]. We cal-culated the information content (IC) described in Resniket al. [41]. The IC scores range from 0-12.25; a lower scorecorresponds to a term that many proteins are annotatedwith and should appear many times in the literature whilea high scoring term is more specific and might have onlyone or two annotations in GOA. For example, a com-mon term such as GO:0005488 - binding has a score of0.80 while a more informative term GO:0086047 - mem-brane depolarization during Purkinje myocyte cell actionpotential has a score of 12.25. A score of undefinedcorresponds to a concept that is not currently annotatedto any protein with GOA. It is our hypothesis that themost informative terms (higher IC) would be more dif-ficult to identify in text and that our rules, describedabove, would help increase the frequency at which we canrecognize correct mentions of these highly informativeterms.Statistics for both the concepts recognized using theontology (baseline 2 presented above) and rules appliedalong with the differences broken down by informationcontent can be seen in Table 8. Utilizing only the informa-tion contained within the Gene Ontology, and accountingfor activity terms, ?97 million mentions of ?12,000unique GO concepts are identified. After generation ofsynonyms by both the recursive syntactic and derivationalTable 8 Statistics of annotations produced on the large literature collection by information contentBaseline B2 With generated synonyms Impact of synonymsIC # Terms # Annotations # Terms # Annotations New concepts New annotations ChangeUndefined 3,548 16,929,911 4,303 23,653,066 755 6,723,155 +39.7 %[0,1) 7 3,202,114 7 3,177,333 0 ?24, 781 ?0.1 %[1,2) 16 2,655,365 17 2,801,431 1 146,066 +0.1 %[2,3) 43 7,332,003 44 8,016,573 1 684,570 +0.1 %[3,4) 94 4,474,422 101 5,188,968 7 714,546 +0.2 %[4,5) 178 4,185,438 191 9,340,757 13 5,155,319 +123.8 %[5,6) 354 13,547,423 373 22,284,670 19 8,737,247 +64.4 %[6,7) 666 9,533,940 715 12,060,499 49 2,526,559 +26.3 %[7,8) 1,044 18,354,299 1,154 21,251,834 110 2,897,535 +16.8 %[8,9) 1,465 7,932,937 1,648 15,316,476 183 7,383,539 +92.4 %[9,10) 1,551 4,813,153 1,813 7,671,601 262 2,858,448 +58.3 %[10,11) 1,396 2,390,061 1,690 4,291,831 294 1,901,770 +79.1 %[11,12) 942 1,246,758 1,162 2,279,005 220 1,032,247 +83.3 %[12,13) 732 578,501 953 1,257,956 221 679,455 +117.2 %Total 12,036 97,176,325 14,171 138,592,000 2,135 41,415,675 +42.5 %Shows the number of unique terms and total number of annotations produced through baseline B2, both derivational and syntactic recursive rules applied, and the impactthe rules have overall. The change is percent change in total annotationsFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 12 of 16rules, ?138 million mentions of ?14,100 unique GOconcepts are identified. In summation, our rules aid in therecognition of?41million more mentions for all GO con-cepts (?42 % increase) along with the ability to recognize?2,000 unique GO concepts (?18 % increase) that are notpreviously identified using the ontology alone. There werea total of ?2.5 million mentions associated with the 2,135unique concepts that were only found when the synonymgeneration rules were applied. The other ?39 million newmentions are associated with the ?12,000 concepts bothdictionaries recognize.The biggest increase in number of annotations andconcepts identified can be seen in those concepts withundefined and higher information content (IC > 8). Thisshows that the our syntactic and derivational rules suc-cessfully introduce variation that allow the more specificand information containing concepts to be recognizedeither at all or more frequently. While we do not findmuch change in annotations produced on the lower infor-mation content concepts, we do see a negative change inannotations produced for some of the low informationcontaining concepts. This is due to our rules generatingsynonyms that can help to identify more specific con-cepts. For example, GO:0005215 - transporter activityis found ?75,000 fewer times after the addition of ourgenerated synonyms due to more specific transportersbeing identified. For instance, in the following sentence,the bold text corresponds to the concept recognized usingthe baseline, while the italicized concept is exactly gen-erated through the use of our rules: The present studywas aimed to evaluate whether intraperitoneal carnitine(CA), a transporter of fatty acyl-CoA into the mitochon-dria. . . . (PMID: 17239403). The usefulness of these rulesgoes beyond that of just improving our recognition of con-cepts from test as identification ofmore informational GOconcepts has been shown to increase performance on theprotein function prediction task [8, 9].Examining the overall numbers of concepts and men-tions recognized provides insights into how useful thesynonyms generated are for recognition of GO conceptsfrom the biomedical literature. Since most mentions iden-tified using only the ontology information were also foundwhen the rules were applied, this indicates that our rulesaid in identification of many new concepts along with newmentions of concepts, thus leading to an overall increasein recall. We saw in evaluation on CRAFT that both preci-sion and recall were increased; we explore throughmanualvalidation the accuracy of concepts identified utilizingthe generated synonyms on a large scale in the followingsection.Manual validation of gene ontologymentionsAlthough we found an improvement in performanceon the CRAFT corpus and on the larger corpus asignificant number of additional concepts and mentionswere identified through our synonym generation rules,we are hesitant to reach any further conclusions with-out some manual validation of the accuracy of thesegenerated synonyms. There are too many concepts andannotations produced to manually validate them all, sowe performed validation of a randomly distributed subsetof concepts and instances of those concepts within text.For cases where the validity of the term was unclear fromthe matched term text alone we went back to the origi-nal paper and viewed the annotation in sentential context.For a baseline of performance, we validated a randomsample of 1 % of baseline concepts (125 concepts with ?1,200 randomly sampled mentions) from each IC rangeand a random sample of 10 % of all new concepts (217terms with ?1,450 randomly sampled mentions) recog-nized through our rules; these results are presented inTable 9. We find that overall accuracy is very high (0.94)for the concepts recognized only utilizing the ontologyinformation. A majority of these text spans identifiedare exact, or very near, matches to the official onto-logical name or one current synonyms. The only vari-ation introduced is through a stemmer or lemmatizerused in the concept recognition pipeline (see Additionalfile 2 for more details). The annotations produced whenusing synonyms generated through our rules do not haveas high of accuracy (0.74) but still produce reasonableresults.Earlier, we hypothesized that overgeneration of syn-onyms would not hinder performance because synonymsthat contain incorrect syntactic format or those that arenot lexically sound, would not appear within the textwe are searching. While performing manual evaluationof annotations produced, we noted that a majority ofthe errors came from three scenarios: 1) naive stem-ming introducing incorrect concepts (60 %), 2) incorrectlevel of specificity due to information loss (25 %), and3) inclusion of incorrect punctuation (15 %). A detailederror analysis along with strategies to correct them ispresented in Additional file 4. Based upon these results,we do not believe that the 1.5 million new synonymsgenerated introduce many false positives from overgener-ation. While we see a decrease in accuracy in annotationsreturned from text when we include the synonyms gener-ated by our rules, we do not attribute the decrease entirelyto the synonyms themselves, as over half of the errors aredue to interaction of synonyms and the stemmer utilizedfor dictionary lookup (Additional file 4). An interestingobservation is that sometimes generating a phrase or syn-onym that initially appears incorrect can actually aid inrecognition. An example is the different adjective formsof protein; most would use the form proteinaceous,but another form is generated through Lexical VariantGenerator (LVG), protenic. This appears multiple timesFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 13 of 16Table 9 Results of manual inspection of random samples of annotationsBaseline B2 With rules OverallIC # Terms # Annotations Accuracy # Terms # Annotations Accuracy AccuracyUndefined 35 231 0.98 75 363 0.70 0.81[0,1) 1 15 0.20 0 0 0.00 0.20[1,2) 1 15 1.00 1 4 1.00 1.00[2,3) 1 15 1.00 1 4 1.00 1.00[3,4) 1 4 1.00 1 1 0.00 0.80[4,5) 2 30 0.60 2 24 0.88 0.72[5,6) 4 60 0.97 2 13 0.23 0.84[6,7) 7 79 0.99 5 41 0.49 0.82[7,8) 10 136 0.89 11 116 0.65 0.78[8,9) 15 197 0.98 19 163 0.83 0.91[9,10) 16 175 0.97 26 205 0.79 0.87[10,11) 14 119 0.83 30 217 0.80 0.81[11,12) 10 103 0.97 22 141 0.77 0.86[12,13) 8 93 0.98 22 156 0.72 0.82Total 125 1272 0.94 217 1448 0.74 0.83Accuracy, calculated via manual review of textual annotations for correctness, of random subsets of concepts recognized from the large literature collections. We sampled1 % of concepts, with up to 15 randomly sampled specific text spans per concept, from concepts identified using baseline B2. We sampled 10 % of concepts, with up to 15randomly sampled text spans per concept, from the new concepts recognized through the presented synonym generation rules. Overall accuracy is calculated by combiningannotations of the same IC from baseline and with our ruleswithin articles translated into English, for example, theconcept GO:0042735 - protein body is seen within thefollowing sentence The activity is exhibited through aprotenic body of NBCF. . .  (PMID: 1982217).The impact of supercomputing on concept recognitiontasksWe ran the our concept recognition pipeline over the largefull text collection on the Pando supercomputer locatedat the University of Colorado, Boulder campus. It has 60 64 core systems with 512GB each along with 4  48core systems with 1TB ram each, for a total of 4,032 com-pute nodes. We utilized a quarter of the machine andran our pipeline over 1,000 directories with 1,000 fulltext documents in each. We were able to produce GOannotations for all one million documents in around 10minutes. Granted, no components are particularly com-plicated. They consist of a sentence splitter, tokenizer,stemmer/lemmatizer, followed by dictionary lookup, butwe have performed similar tasks on a large memorymachine, with 32 cores and the complete task has taken34 weeks. Given that Pubmed consists of over 24 mil-lion publications, if it was possible to obtain all documentsand performance is linear to the number of documents,we could recognize GO concepts from the entirety of thebiomedical literature in around 4 hrs. More complex andtime consuming tasks, such as relation extraction, willtake longer but will still be on the order of days or weeksutilizing the power of a supercomputer, since these tasksare embarrassingly parallel.Generalization to other biomedical ontologiesThe synonym generation methodology presented here,of breaking down complex concepts into their mostconstituent parts, generating synonym for the parts,then recursively combining to form synonyms of theoriginal concept is one that can generalize to manyother ontologies or standardized terminologies. The GeneOntology contains very complex and lengthy wordedconcepts; the rules required to implement composi-tional synonyms in other ontologies might not needas many syntactic and derivational rules as we presenthere. Besides GO we can envision similar method-ologies easily applied to Human Phenotype Ontology(HPO), Chemical Entities of Biological Interest (ChEBI),SNOMED, and International Classification of Diseases 10(ICD10).One example, within the Human Phenotype Ontology(HPO) [42], there is a high level HPO term that cor-responds to phenotypic abnormality. There are justover 1,000 terms (?10 % of all HPO concepts) thatare descendants of phenotypic abnormality that canbe decomposed into: abnormality of [the] other con-cept (e.g. HP:0000818 - abnormality of endocrine sys-tem). Not only can we add syntactic rules to reorderwords, semantic synonyms of abnormality, such asFunk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 14 of 16malformation or deformity, can be added to expressthe concepts in similar ways. There are many otherconcepts that could benefit from recursively generatingsynonyms as the HPO appears to have compositionalcharacteristics as well. There could also be subsets ofrules depending on the context; recognizing conceptsin doctors notes or electronic medical record will beexpressed differently than those within the biomedicalliterature.ConclusionsIn this work, we present a set of simple language gen-eration rules to automatically generate synonyms forconcepts in the Gene Ontology. These rules take intoaccount the compositional nature of GO terms alongwith manually created syntactic and derivational vari-ants derived from discussions with biologists, ontolo-gists, and through analyzing Gene Ontology concepts asthey are expressed within the literature. The 18 hand-crafted rules automatically generate over ?1.5 millionnew synonyms for ?66 % of all concepts within the GeneOntology. We acknowledge the approach overgeneratessynonyms, but we find that many generated synonymsdo not appear within biomedical text, thus not hinderingperformance.We argue that current synonyms in structured ontolo-gies are insufficient for text-mining due to the vastdegree of variability of expression within natural lan-guage text; our methods do not propose to solve thisproblem, but make a step in the right direction. Thisclaim is supported through the examination of spe-cific examples of concept variation in biomedical textand an empirical evaluation of the overlap of cur-rent GO synonyms and their expression in the CRAFTcorpus.We evaluate our synonym generation rules both intrin-sically and extrinsically. Utilizing the CRAFT corpus forintrinsic evaluation, we evaluate three different sourcesof automatically generated synonyms 1) external ontologymappings, 2) recursive syntactic rules and 3) derivationalvariant rules. External mappings introduced too manyfalse positives and are currently not recommended for use.The recursive syntactic rules added ?194,000 new syn-onyms but did not significantly affect performance. Usinga combination of recursive syntactic rules and derivationalvariant rules ?300,000 new synonyms were generated,resulting in an increase in F-measure performance of 0.14,mostly due to greatly increased recall. This illustrates theimportance of derivational variants for capturing naturalexpression.Our rules were extrinsically evaluated on a large col-lection of one million full text documents. The rulesaid in the recognition of ?2,000 more unique conceptsand increase the frequency in which all concepts areidentified by 41 % over the baseline (Table 9), usingonly current information contained within the GeneOntology. Specifically, the synonyms generated aid inthe recognition of more complex and informative con-cepts. Manual validation of random samples concludeaccuracy is not as high as desirable (74 %). An erroranalysis produced concrete next steps to increase theaccuracy; simply removing one generation sub-rule, andfiltering mentions with unmatched punctuation, increasesaccuracy of a random sample of 217 newly recognizedconcepts (?1,450 mentions) to 83 %. Overall, man-ual analysis of 342 concepts (?2,700 mentions) leadsto an accuracy of 88 % (Additional file 4). We findthat our rules increase the ability to recognize con-cepts from the Gene Ontology within the biomedicalliterature.Even though we chose a specific dictionary based-system, ConceptMapper, to evaluate our rules, the gen-erated synonyms can also be useful for many otherapplications. Any other dictionary based system can sup-plement its dictionary with the generated synonyms.Additionally, any machine learning or statistical basedmethods will be able to utilize the synonyms we gener-ate to try to normalize the span of text identified as aspecific entity type to an ontological identifier; this willprovide a richer feature representation for target con-cepts. In addition, we provide examples of how theserules could generalize to other biomedical ontologies anddiscuss the impact of supercomputing on scaling thiswork.Not only have our rules proven to be helpful for recogni-tion of GO concepts, but there are also other applicationsseparate from the evaluated task. They could be used toidentify inconsistencies within the current Gene Ontol-ogy synonyms. Concepts that share similar patterns, i.e.regulation of X, should all contain synonyms that corre-spond to a certain syntactic pattern. While performingthis work we identified a few concepts that should con-tain synonyms but do not, illustrating the usefulness ofthe presented rules for ontology quality assurance as orig-inally outlined in Verspoor et al. [27]. Additionally, acertain conservative subset of our rules could easily beincorporated into TermGenie [24], a web application thatautomatically generates new ontology terms. Our ruleswould be of help to generate synonyms of the auto-matically generated concepts. It is our desire to submitthe good synonyms identified within the text to theGene Ontology Consortium for curation into the ontol-ogy. Additionally, there could possibly be a text miningsynonym category added or we can deposit them, for thetime being, within a larger application such as Freebase[43]. We would like other people to be able to use oursynonyms for text mining so we provide the full list asAdditional file 5.Funk et al. Journal of Biomedical Semantics  (2016) 7:52 Page 15 of 16Additional filesAdditional file 1: Full enumeration and explanation of each manuallycreated rule. (PDF 363 kb)Additional file 2: Outline of ConceptMapper parameters used for eachbranch of the Gene Ontology. (PDF 106 kb)Additional file 3: Analysis of using external ontological mappings assynonyms. (PDF 165 kb)Additional file 4: Detailed error analysis of manually reviewed conceptsfrom large scale evaluation. (PDF 108 kb)Additional file 5: The list of novel synonyms generated by this method.(TXT 129024 kb)AcknowledgementsThis work was funded by NIH grant 2T15LM009451 to LEH. The S10supercomputer resides on the University of Colorado Boulder campus andwas obtained through NIH grant 1S10OD012300-01.Availability of data andmaterialsThe list of novel unique automatically generated synonyms can be found inAdditional file 5.The CRAFT corpus is available at http://bionlp-corpora.sourceforge.net/CRAFT/.The large literature Elsevier collection is unavailable for public distribution. Weare aware of the difficulties associated with obtaining or even accessing full-text documents from subscription journal publishers in a machine readableformat, such as XML. This collection of articles was procured negotiating alicensing deal mediated through on campus librarians. A possible solution andlessons learned from that process is described in Fox et al. [44].Authors contributionsCSF developed and evaluated synonym generation rules. LEH, KBC, and LEHcontributed to the design of methods and offered supervision at every step.All authors read and approved the manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Computational Bioscience, University of Colorado School of Medicine, AuroraCO 80045, USA. 2Department of Computing and Information Systems,University of Melbourne, Parkville, Melbourne 3010, Australia. 3Health andBiomedical Informatics Centre, University of Melbourne, Parkville, Melbourne3010, Australia.Received: 10 April 2015 Accepted: 5 August 2016Slater et al. Journal of Biomedical Semantics  (2016) 7:49 DOI 10.1186/s13326-016-0090-0RESEARCH Open AccessUsing AberOWL for fast and scalablereasoning over BioPortal ontologiesLuke Slater1*, Georgios V. Gkoutos1,2, Paul N. Schofield3 and Robert Hoehndorf4AbstractBackground: Reasoning over biomedical ontologies using their OWL semantics has traditionally been a challengingtask due to the high theoretical complexity of OWL-based automated reasoning. As a consequence, ontologyrepositories, as well as most other tools utilizing ontologies, either provide access to ontologies without use ofautomated reasoning, or limit the number of ontologies for which automated reasoning-based access is provided.Methods: We apply the AberOWL infrastructure to provide automated reasoning-based access to all accessible andconsistent ontologies in BioPortal (368 ontologies). We perform an extensive performance evaluation to determinequery times, both for queries of different complexity and for queries that are performed in parallel over the ontologies.Results and conclusions: We demonstrate that, with the exception of a few ontologies, even complex and parallelqueries can now be answered in milliseconds, therefore allowing automated reasoning to be used on a large scale, torun in parallel, and with rapid response times.Keywords: Ontology, Reasoning, Scalable reasoning, Description logics, OWL, AberOWLBackgroundMajor ontology repositories such as BioPortal [1], Onto-Bee [2], or the Ontology Lookup Service [3], have existedfor a number of years, and currently contain several hun-dred ontologies. They enable ontology creators and main-tainers to publish their ontology releases and make themavailable to the wider community.Besides the hosting functionality that such reposito-ries offer, they usually also provide certain web-basedfeatures for browsing, comparing, visualising and process-ing ontologies. One particularly useful feature, currentlymissing from themajor ontology repositories, is the abilityto provide online access to reasoning services simultane-ously over many ontologies. Such a feature would enablethe use of semantics and deductive inference when pro-cessing data characterized by the ontologies these reposi-tories contain [4].For example, there is an increasing amount of RDF[5] data becoming available through public SPARQL [6]*Correspondence: lxs511@bham.ac.uk1College of Medical and Dental Sciences, Institute of Cancer and GenomicSciences, Centre for Computational Biology, University of Birmingham, B15 2TTBirmingham, United KingdomFull list of author information is available at the end of the articleendpoints [710], which utilise ontologies to annotateentities, and access to reasoning over ontologies will allowcombined queries over knowledge contained in ontologiesand the data accessible through the SPARQL endpoints[4].However, enabling automated reasoning over multipleontologies is a challenging task, since automated reason-ing can be highly complex and costly in terms of timeand memory consumption [11]. In particular, ontologiesformulated using the Web Ontology Language (OWL)[12] can utilize statements based on highly expressivedescription logics [13], and therefore queries that utilizeautomated reasoning cannot, in general, be guaranteed tofinish in a reasonable amount of time.Prior work on large-scale automated reasoning overbiomedical ontologies has often focused on the set ofontologies in Bioportal, as it is one of the largest collec-tions of ontologies freely available. To enable inferencesover this set of ontologies, modularization techniqueshave been applied [14] using the notion of locality-basedmodules, and demonstrated that, for most ontologies andapplications, relatively small modules can be extractedover which queries can be answered more efficiently.Other work has focused on predicting the performance ofreasoners when applied to the set of BioPortal ontologies© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Slater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 2 of 6[15], and have demonstrated that performance of particu-lar reasoners can reliably be predicted; at the same time,the authors have conducted an extensive evaluation ofthe average classification time for each ontology. Furtherapproaches apply RDFS reasoning [16] to provide lim-ited, yet fast, inference capabilities in answering queriesover Bioportals set of ontologies through a SPARQL inter-face [17, 18]. Alternatively, systems such as OntoQuery[19] provide web-based access to ontologies through auto-mated reasoning but limit the number of ontologies. Theperformance of OntoQuery has been found to be compa-rable to the performance of reasoning over ontologies intools such as Protege [19].The AberOWL [4] system is an ontology repositorywhich aims to allow access to multiple ontologies throughautomated reasoning, utilizing their OWL semantics.AberOWL mitigates the complexity challenges by usinga reasoner which supports only a subset of OWL (i.e.,the OWL EL profile [20]), ignoring ontology axioms andqueries that do not fall within this subset. This enablesthe provision of polynomial-time reasoning, which is suf-ficiently fast for many practical uses, even when appliedto large ontologies [21]. However, thus far, the AberOWLsoftware has only been applied to a few, manually selectedontologies, and therefore does not have a similar domaincoverage to other ontology repositories, nor does it caterfor reasoning over large sets of ontologies, such as theones provided by the BioPortal ontology dataset (Biopor-tal contains, as of 9 March 2015, 428 ontologies consistingof 6,668,991 classes).Here, we apply the AberOWL framework to reasonover the majority of the ontologies available in Biopor-tal. We evaluate the performance of querying ontologieswith AberOWL, utilizing 337 ontologies from BioPortal.We evaluate AberOWLs ability to perform different typesof queries as well as assess its scalability in performingqueries that are executed in parallel. We demonstrate thatthe AberOWL framework makes it possible to provide, atleast, light-weight description logic reasoning over mostof the freely accessible ontologies contained in BioPortal,with a relatively lowmemory footprint and high scalabilitywith respect to the number of queries executed in paral-lel, using only a single medium-sized server as hardwareto provide these services. Furthermore, we identify severalontologies for which the performance of reasoning-basedqueries is significantly worse than the majority of theother ontologies tested, and discuss potential explanationsand solutions.MethodsSelection of ontologiesWe selected all ontologies contained in BioPortal as can-didate ontologies, and attempted to download the currentversions of all the ontologies for which a download linkwas provided by BioPortal. A summary of the results ispresented in Table 1.Out of a total of 427 ontologies listed by Bioportal,only 368 could be directly downloaded and processed byAberOWL. Reasons for failure to load ontologies includethe absence of a download link for listed ontologies,ontologies that are only available in proprietary data for-mats (e.g., some of the ontologies and vocabularies areprovided in custom representation languages as part ofthe Unified Medical Language Systems [22]), or licenserestrictions which prevents their inclusion in public ontol-ogy repositories (e.g., SNOMED CT). Thirty nine ontolo-gies were not obtainable. Furthermore, 17 ontologies thatcould be downloaded were not parseable with the OWLAPI, indicating a problem in the file format used to dis-tribute the ontology. Three ontologies were inconsistentat the reasoning stage. Whilst several ontologies alsoincluded unobtainable ontologies as imports, we includedthese ontologies in our analysis, utilizing only the classesand axioms that were accessible. As AberOWL currentlyrelies on the use of labels to construct queries, we furtherremoved 31 ontologies that did not include any class labelsfrom our test set.Overall, we use a set of 337 ontologies in our experi-ments consisting of 3,466,912 classes and 6,997,872 logicalaxioms (of which 12,721 are axioms involving relations,i.e., RBox axioms). In comparison, BioPortal currently (9March 2015) includes a total of 6,668,991 classes.Use of the AberOWL reasoning infrastructureAberOWL [4] is an ontology repository and query servicebuilt on the OWLAPI [23] library, which allows accessto a number of ontologies through automated reason-ing. In particular, AberOWL allows users or softwareapplications to query a single ontology within AberOWLusing Manchester OWL Syntax [24], using the class andproperty labels as short-form identifiers for classes. Mul-tiple queries to single ontologies can be performed at thesame time, and AberOWL also provides functionality toperform a query on all ontologies within the repository.Table 1 Summary of Ontologies used in our testTotal 427Loadable 368Used 337Unobtainable 39Non-parseable 17Inconsistent 3No Labels 31The loadable ontologies are the ones obtained from BioPortal which could beparsed using the OWL API and which were found to be consistent when classifiedwith the ELK reasoner. We exclude 31 ontologies that do not contain any labelsfrom our analysisSlater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 3 of 6AberOWL exposes this functionality over the Internetthrough a JSON API as well as a web interface avail-able on http://aber-owl.net. To answer queries, AberOWLutilizes the ELK reasoner [25, 26], a highly optimizedreasoner that supports the OWL-EL profile. Ontologieswhich are not OWL-EL are processed by the reasonerby means of ignoring all non-EL axioms, although as of2013 50.7 % of ontologies in Bioportal were natively usingOWL-EL [27].We extended the AberOWL framework to obtain a listof ontologies from the Bioportal repository, periodicallychecking for new ontologies as well as for new versionsof existing ontologies. As a result, our testing version ofAberOWL maintains a mirror of the accessible ontolo-gies available in BioPortal. Furthermore, similarly to thefunctionality provided by BioPortal, a record of older ver-sions of ontologies is kept within AberOWL, so that, in thefuture, the semantic difference between ontology versionscan be computed.In addition, we expanded the AberOWL software tocount and provide statistics about: the ontologies which failed to load, with associatederror messages; axioms, axiom types, and number of classes perontology; and axioms, axiom types, and number of classes over allontologies contained within AberOWL.For each query of AberOWL, we also record the queryexecution time within AberOWL and pass this informa-tion back to the client along with the result-set of thequery. Thus, the figures presented here do not include thetime required to transmit the response.All information is available through AberOWLs JSONAPI http://aber-owl.net/help, and the source code is freelyavailable at https://github.com/bio-ontology-research-group/AberOWL.Experimental setupIn order to evaluate the performance of querying singleand multiple ontologies in AberOWL, queries of differentcomplexity were randomly generated and executed. Sincethe ELK reasoner utilises a cache for answering queriesthat have already been computed, each of the generatedqueries consisted of a new class expression. The follow-ing types of class expressions were used in the generatedqueries (for randomly generated class labels A, B, andrelation R): Primitive class: A Conjunctive query: A and B Existential query: R some A Conjunctive existential query: A and R some BThree hundred random queries for each of these typeswere generated for each ontology that was tested (1200queries in total per ontology). Each set of the 300 randomqueries generated, were subsequently split into three sets,each of which contained 100 class expressions. The ran-dom class expressions contained in the resulting sets werethen utilised to perform superclass (100 queries), equiv-alent (100 queries) and subclass (100 queries) queries,and the response time of the AberOWL framework wasrecorded for each of the queries.We further test the scalability of answering the queriesby performing the queries in parallel. For this purpose, weperform three separate tests, to query AberOWLwith onequery at once, 100 queries in parallel, and 1,000 queries inparallel.In our test, we record the response time of each query,based on the statistics provided by the AberOWL server;in particular, response time does not include networklatency. All tests are performed on a server with 128 GBmemory and two Intel Xeon E5-2680v2 10-core 2.8 GHzCPUs with hyper-threading activated (resulting in 40 vir-tual cores). The ELK reasoner underlying AberOWL ispermitted to use all available (i.e., all 40) cores to performclassification and respond to each query.Results and discussionOn average, when performing a single query overAberOWL, query results are returned in 10.8 ms (stan-dard deviation: 48.0 ms). The time required to answera query using AberOWL correlates linearly with thenumber of logical axioms in the ontologies (Pearson cor-relation, ? = 0.33), and also strongly correlates withthe number of queries performed in parallel (Pearsoncorrelation, ? = 0.82).Figure 1 shows the query times for the ontologiesbased on the type of query, and Fig. 2 shows the querytimes based on different number of queries run in par-allel. The maximum observed memory consumption forthe AberOWL server while performing these tests was66.1 GB.We observe several ontologies for which query timesare significantly higher than for the other ontologies. Themost prevalent outlier is theNCI Thesaurus [28] for whichaverage query time is 600 ms when performing a singlequery over AberOWL. Previous analysis of NCI The-saurus has identified axioms which heavily impact theperformance of classification for the ontology using mul-tiple description logic reasoners [29]. The same analysishas also shown that it is possible to significantly improvereasoning time by adding inferred axioms to the ontol-ogy. To test whether this would also allow us to improvereasoning time over the NCI Thesaurus in AberOWL andusing the ELK reasoner, we apply the Elvira modulariza-tion software [21], using the HermiT reasoner to classifySlater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 4 of 6abcdFig. 1 Query times as a function of the number of logical axioms inthe ontologies, separated by the type of queryabcFig. 2 Query times as function of the number of logical axioms in theontologies, separated by the number of queries executed in parallelSlater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 5 of 6the NCI Thesaurus to add all inferred axioms that fall intothe OWL-EL profile to the ontology, as opposed to ELKsapproach of ignoring non-EL axioms during classification.We then repeat our experiments. Figure 3 shows the dif-ferent reasoning times for NCI Thesaurus before and afterprocessing with Elvira. Query time reduces from 703 ms(standard deviation: 689 ms) before processing with Elvirato 51 ms (standard deviation: 42 ms) after processing withElvira, demonstrating that adding inferred axioms andremoving axioms that do not fall in the OWL-EL profilecan be used to improve query time.abFig. 3 Query times over the NCI ThesaurusAnother outlier with regard to average query time is theNatural Products Ontology (NATPRO, http://bioportal.bioontology.org/ontologies/NATPRO). However, as NAT-PRO is expressed in OWL-Full, it cannot be reliably clas-sified with a Description Logic reasoner, and thereforewe could not apply the same approach to improve theperformance of responding to queries.Future workThe performance of using automated reasoning for query-ing ontologies relies heavily on the type of reasoner used.We have used the ELK [25, 26] reasoner in our evalua-tion; however, it is possible to substitute ELK with anyother OWLAPI-compatible reasoner. In particular, novelreasoners such as Konklude [30], which outperform ELKin many tasks [31], may provide further improvements inperformance and scalability.We identified several ontologies as leading to per-formance problems, i.e., they are outliers during querytime testing. For these ontologies, including the NaturalProducts Ontology (NATPRO), and, to a lesser degree,the Drug Ontology (DRON) [32], similar culprit-findinganalysis methods may be applied as have previously beenapplied for the NCI Thesaurus [29]. These methods mayalso allow the ontology maintainers to identifying possi-ble modifications to their ontologies that would result inbetter reasoner performance.ConclusionWe have demonstrated that it is feasible to reason overmost of the ontologies available in BioPortal in real time,and that queries over these ontologies can be answeredquickly, in real-time, and using only standard server hard-ware. We further tested the performance of answeringqueries in parallel, and show that, for themajority of cases,even highly parallel access allows quick response timeswhich scale linearly with the number of queries.We have also identified a number of ontologies forwhich performance of automated reasoning, at least whenusing AberOWL and the ELK reasoner, is significantlyworse, which renders them particularly problematic forapplications that carry heavy parallel loads. At least forsome of these ontologies, pre-processing them using toolssuch as Elvira [21] can mitigate these problems.The ability to reason over a very large number of ontolo-gies, such as all the ontologies in BioPortal, opens up thepossibility to frequently use reasoning not only locallywhen making changes to a single ontology, but also mon-itor  in real time  the consequences that a changemay have on other ontologies, in particular on ontolo-gies that may import the ontology which is being changed.Using automated reasoning over all ontologies withina domain therefore has the potential to increase inter-operability between ontologies and associated data bySlater et al. Journal of Biomedical Semantics  (2016) 7:49 Page 6 of 6verifying mutual consistency and enabling queries acrossmultiple ontologies, and our results show that such asystem can now be implemented with available softwaretools and commonly used server hardware.AcknowledgementsThis paper is an extended version of a conference paper by the same name,presented at ICBO 2015.Authors contributionsRH and LS conceived of the study. LS implemented the software andperformed the experiments. All authors evaluated the results. RH, PS, GVGsupervised the research. All authors contributed to the manuscript. All authorsread and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1College of Medical and Dental Sciences, Institute of Cancer and GenomicSciences, Centre for Computational Biology, University of Birmingham, B15 2TTBirmingham, United Kingdom. 2Institute of Translational Medicine, UniversityHospitals Birmingham NHS Foundation Trust, B15 2TT Birmingham, UnitedKingdom. 3Department of Physiology, Development and Neuroscience,University of Cambridge, Downing Street, CB2 3EG England, United Kingdom.4Computational Bioscience Research Center, Computer, Electrical andMathematical Sciences & Engineering Division, King Abdullah University ofScience and Technology, 4700 KAUST, 23955-6900 Thuwal, Saudi Arabia.Received: 3 February 2016 Accepted: 8 July 2016SHORT REPORT Open AccessPublication of nuclear magnetic resonanceexperimental data with semantic webtechnology and the application thereof tobiomedical research of proteinsMasashi Yokochi1, Naohiro Kobayashi1, Eldon L. Ulrich2, Akira R. Kinjo1, Takeshi Iwata1, Yannis E. Ioannidis3,Miron Livny4, John L. Markley2, Haruki Nakamura1, Chojiro Kojima1 and Toshimichi Fujiwara1*AbstractBackground: The nuclear magnetic resonance (NMR) spectroscopic data for biological macromolecules archived atthe BioMagResBank (BMRB) provide a rich resource of biophysical information at atomic resolution. The NMR dataarchived in NMR-STAR ASCII format have been implemented in a relational database. However, it is still fairlydifficult for users to retrieve data from the NMR-STAR files or the relational database in association with data fromother biological databases.Findings: To enhance the interoperability of the BMRB database, we present a full conversion of BMRB entries totwo standard structured data formats, XML and RDF, as common open representations of the NMR-STAR data.Moreover, a SPARQL endpoint has been deployed. The described case study demonstrates that a simple query ofthe SPARQL endpoints of the BMRB, UniProt, and Online Mendelian Inheritance in Man (OMIM), can be used inNMR and structure-based analysis of proteins combined with information of single nucleotide polymorphisms(SNPs) and their phenotypes.Conclusions: We have developed BMRB/XML and BMRB/RDF and demonstrate their use in performing a federatedSPARQL query linking the BMRB to other databases through standard semantic web technologies. This will facilitatedata exchange across diverse information resources.Keywords: NMR, BMRB, Database, XML, RDFFindingsBackgroundThe BioMagResBank (BMRB; http://www.bmrb.wisc.edu/)is a worldwide data repository for experimental and de-rived data gathered from nuclear magnetic resonance(NMR) spectroscopic studies of biological molecules [1].For more than 15 years, the BMRB has used and devel-oped the NMR-STAR format based on the STAR formatspecifications [24] for data archiving and data exchange(see Fig. 1a). The NMR-STAR Dictionary, acting as ontol-ogy for NMR-STAR data, continues to be improved andexpanded to keep up with the needs of the biomolecularNMR community. The most important parameterarchived in the BMRB is assigned chemical shifts, whichcan be used directly to determine protein secondary struc-ture and to assist in the determination of their solutionstructures, to identify interactions of small molecules withtarget proteins for drug discovery, and to characterizeprotein-protein interactions.As of 2015, the BMRB archive contains more than10,000 entries, and ~800 new entries are being addedeach year. This growing biological data archive hasraised researchers interest in integrating the diversebiological information resources to form new hypothesesfor understanding biological functions and phenomena.The best approach for enhancing interoperability of the* Correspondence: tfjwr@protein.osaka-u.ac.jp1Institute for Protein Research, Osaka University, 3-2 Yamadaoka, Suita, Osaka565-0871, JapanFull list of author information is available at the end of the article© 2016 Yokochi et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Yokochi et al. Journal of Biomedical Semantics  (2016) 7:16 DOI 10.1186/s13326-016-0057-1Fig. 1 The sequential data conversion of a BMRB entry from NMR-STAR format. a Part of a BMRB entry in NMR-STAR format. Parts of the entryhave been converted to b XML format and c RDF format. d Schematic representation of linked external information resources, where shorter distancesfrom the BMRB represent closer relationships with the BMRBYokochi et al. Journal of Biomedical Semantics  (2016) 7:16 Page 2 of 4BMRB would be to convert the archive into standardweb formats, XML and RDF, using a data structure thatcorresponds closely to the NMR-STAR ontologydescribed by an XML schema and OWL.MethodsWe have extended the NMR-STAR Dictionary to accom-modate the derived data repositories on BMRB, such asLACS validation reports [5], structural annotations usingPACSY [6] and Protein Blocks [7], etc., followed by transla-tion of the dictionary to an XML schema [8] (BMRB/XMLSchema), using the PDBx/mmCIF Dictionary Suite de-veloped by the Research Collaboratory for StructuralBioinformatics (RCSB) Protein Data Bank (PDB)(http://sw-tools.rcsb.org/). To automate the XML con-version of BMRB entries, we have developed a softwaresuite (the BMRBxTool) that generates XML documentsand validates their format and data consistency accordingto the BMRB/XML schema. EXtensible Stylesheet Lan-guage (XSL) transformation [9] was applied to generateBMRB entries in RDF format from the corresponding XMLdocuments. Along with the RDF, we also provide itsontology written in RDF/RDFS/OWL syntax as BMRB/OWL [10, 11], which is a direct translation of the BMRB/XML schema. To bridge different data models between theXML tree and the RDF directed graph, we have introducedabstract OWL classes and RDF properties to the ontology[12]. For the data conversion from XML to RDF in accord-ance with the principles of Linked Data [13] and recom-mendations widely accepted by biological databasecommunity [14], we have developed a software suite calledBMRBoTool. We have tested as many as thirty SPARQLqueries to show how NMR experimental data can be re-trieved. In a case study to demonstrate a federated SPARQLquery, we performed a search for phenotypes annotatedwith information for SNPs from the human genome by in-tegrating three SPARQL endpoints: the BMRB, UniProt,and OMIM [see Additional file 1 for details].Results and discussionOn our portal site (http://bmrbpub.protein.osaka-u.ac.jp/,hereafter abbreviated as ~/), we have archived theBMRB/XML (~/archive/xml/), as shown in Fig. 1b. TheBMRB/RDF derived from the reduced version of theFig. 2 Federated search using multiple SPARQL endpoints of the BMRB, UniProt, and OMIM. a An example of a SPARQL query begins from theBMRB entry. b Results performed by the SPARQL query for BMRB entry 4280, showing the BMRB ID, mutation, OMIM ID, dbSNP ID, secondary structure,and SASA. c Ribbon models of NMR structure (PDB: 1QK9) of MECP2. The side-chains (space-filling model) for the mutation (E137 and A140) exposedto solvent are responsible for X-linked mental retardation [18]Yokochi et al. Journal of Biomedical Semantics  (2016) 7:16 Page 3 of 4BMRB/XML (lacking bulky information such as atomiccoordinates and NMR restraints) has also been archived(~/archive/rdf/) as shown in Fig. 1c. A bulk download ser-vice is available using the rsync protocol that helps usersmirror the latest data collections, which are updated peri-odically. A schematic RDF graph of linked databases is il-lustrated in Fig. 1d. Owing to the machine readability ofthe XML format, the BMRB/XML provides users with anexcellent starting point to develop new tools for use inbiochemistry and structural biology. The BMRB/RDF andassociated web services enable the integration of theBMRB archive with other biological databases, which mayfacilitate flexible data exchange and knowledge discovery.Furthermore, we provide a SPARQL endpoint for query-ing the BMRB/RDF (~/search/rdf) in the same way asBio2RDF [15] does. The RDF query language, SPARQL[16], realizes data exchange between databases in a con-cise syntax. The key feature of SPARQL is its capability ofjoining remote SPARQL endpoints in what is called a fed-erated SPARQL query [17]. We confirmed the feasibilityof such a query (see Fig. 2a), by demonstrating search andclassification of SNPs in an associated BMRB entry. In thecase study, we successfully collected residues in a BMRBentry whose sequences correspond to SNPs annotated byOMIM, as shown in Fig. 2b. The search results (Fig. 2c)were represented by structural parameters archived in theBMRB [see also Additional file 1 for methods applied andall results, chapter 3], allowing users to infer biologicalrelationships between the phenotype annotated SNPs andstructural features in human proteins (see Fig. 2c). The re-sults show that the BMRB/RDF offers a promising ap-proach for integrating biophysical information derivedfrom biological NMR spectroscopy with other bioinfor-matics resources of interest in biological and medicalscience research.Additional fileAdditional file 1: Further detail of the BMRB/XML, BMRB/RDF and webservices including SPARQL endpoint. Complete results of the SPARQLquery (Fig. 2a) and many other SPARQL query examples are alsoavailable. (PDF 6814 kb)AbbreviationsBMRB: BioMagResBank; MECP2: methyl-CpG-binding protein 2; NMR: nuclearmagnetic resonance; OMIM: Online Mendelian Inheritance in Man; OWL: webontology language; RDF: resource description framework; RDFS: RDF schema;SASA: solvent accessible surface area; SNP: single nucleotide polymorphism;SPARQL: SPARQL Protocol and RDF Query Language; XML: eXtensibleMarkup Language.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMY performed research, data analysis and wrote the paper with NK. TF wasprincipal investigator for this project, contributed to research design and feedbackon the manuscript. All authors read and approved the final version of manuscript.AcknowledgementsThis work was supported by National Bioscience Database Center (NBDC) ofJapan Science and Technology (JST); and the United States National Libraryof Medicine [LM05799] and National Institute of General Medical Sciences[GM109046].Author details1Institute for Protein Research, Osaka University, 3-2 Yamadaoka, Suita, Osaka565-0871, Japan. 2Department of Biochemistry, University of Wisconsin-Madison,Madison, WI 53706, USA. 3Department of Informatics & Telecommunications,University of Athens, Athens, Greece. 4Department of Computer Sciences,University of Wisconsin-Madison, Madison, WI 53706, USA.Received: 11 July 2015 Accepted: 18 March 2016RESEARCH Open AccessDiagnosis, misdiagnosis, lucky guess,hearsay, and more: an ontological analysisWilliam R. Hogan1* and Werner Ceusters2AbstractBackground: Disease and diagnosis have been the subject of much ontological inquiry. However, the insightsgained therein have not yet been well enough applied to the study, management, and improvement of dataquality in electronic health records (EHR) and administrative systems. Data in these systems suffer from workaroundsclinicians are forced to apply due to limitations in the current state-of-the art in system design which ignore the varioustypes of entities that diagnoses as information content entities can be and are about. This leads to difficulties indistinguishing amongst diagnostic assertions misdiagnosis from correct diagnosis, and the former from coincidentallycorrect statements about disease.Methods: We applied recent advances in the ontological understanding of the aboutness relation to the problemof diagnosis and disease as defined by the Ontology for General Medical Science. We created six scenarios thatwe analyzed using the method of Referent Tracking to identify all the entities and their relationships which mustbe present for each scenario to hold true. We discovered deficiencies in existing ontological definitions and proposedrevisions of them to account for the improved understanding that resulted from our analysis.Results: Our key result is that a diagnosis is an information content entity (ICE) whose concretization(s) aretypically about a configuration in which there exists a disease that inheres in an organism and instantiates acertain type (e.g., hypertension). Misdiagnoses are ICEs whose concretizations succeed in aboutness on thelevel of reference for individual entities and types (the organism and the disease), but fail in aboutness on thelevel of compound expression (i.e., there is no configuration that corresponds in total with what is asserted).Provenance of diagnoses as concretizations is critical to distinguishing them from lucky guesses, hearsay, andjustified layperson belief.Conclusions: Recent improvements in our understanding of aboutness significantly improved our understanding ofthe ontology of diagnosis and related information content entities, which in turn opens new perspectives for theimplementation of data capture methods in EHR and other systems to allow diagnostic assertions to be captured withless ambiguity.Keywords: Biomedical ontology, Referent tracking, Disease, Diagnosis, Information content entity, Representation,Ontological realismBackgroundAs administrative, clinical, and patient-reported data areincreasingly shared and reused, especially for patientcare [14] and research [1, 57], several issues withthese dataincluding diagnosis dataare of increasingconcern. The issue that appears to be of greatest concernis data error and the implications thereof for making deci-sions and conclusions based on them [813]. AlthoughShapiro et al., in a report for the Office of the NationalCoordinator for Health Information Technology, do notcite error as a concern for including patient-generatedhealth data into the electronic health record (EHR) [14],there are known errors with patient self reporting espe-cially in research [1522]. A second issue of concern isdata provenance [10, 23], i.e. information about who cre-ated the data, in what setting, how, when, for what* Correspondence: hoganwr@ufl.edu1University of Florida, 2004 Mowry Rd, P.O. Box 100219, Gainesville, FL32610-0219, USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 DOI 10.1186/s13326-016-0098-5purpose, and so on. For example, Johnson et al. noted thatthe provenance of symptom data was essential to usingthose data correctly to determine whether a colonoscopywas a screening vs. diagnostic procedure [23].Data error and data provenance are closely related.For example, Hersh et al. note that data recorded inbilling workflows for financial purposes are less accur-ate than clinical data [10]. Thus, timing, method, andpurpose of recording data at a minimumall aspects ofprovenanceare intertwined with accuracy. Further-more, a key result of the Johnson et al. study is thatResearchers who do not consider data provenance riskcompiling data that are systematically incomplete orincorrect [23].An ontological account of data error and data proven-ance can identify crucial distinctions. For example, thereare significant differences among (1) a measured weightthat is off because the scale was not properly tared, (2) arough weight of 70 kg entered in an emergency whenthe patient cannot be weighed, and (3) a weight meas-urement entered on the wrong patient. Detecting andaccounting for these differences and their causesespe-cially the aspects of provenance that influence themisnecessary to inform strategies to study, cope with, andimprove data error when using pre-existing EHR datafor research.Additionally, a recent review article on the methodsfor assessing quality of EHR data for clinical researchfound that: Most of the studies included in this reviewpresented assessment methodologies that were developedwith a minimal empirical or theoretical basis [24]. Itconcluded with a call for moving away from ad hoc ap-proaches to data quality assessment, to formal, validatedapproaches. Although error is only one aspect of dataquality (fitness for purpose and completeness are twoothers), a formal ontological understanding of data errorcould play a role in more formalized methods for dataquality assessment.In this work, we apply Smith and Ceusters recentontological account of incorrect information (i.e., error)[25] to diagnosis data in administrative systems, EHRs,and patient-reported information. Their account holdsthat a statement such as a diagnostic assertion can suc-ceed or fail in aboutness on at least two levels: (1) thelevel of denoting single entities and/or types (i.e., thelevel of reference) and (2) the level of veridical repre-sentation of a configuration of multiple entities and/ortypes (i.e., the level of compound expression).To succeed on the second level (compound expression),the information content entity (ICE) must be correct aboutall particulars, their relationships, and their instantiationsof types that it mentions. Failure on a single particular, re-lation, or instantiation causes the ICE to fail at the secondlevel while still potentially succeeding at the first level. Forexample, if Mrs. Jones has type 1 diabetes mellitus, thenthe sentence Mrs. Jones suffers from type 2 diabetes melli-tus fails in aboutness on the level of compound expressionbecause it misstates one thing: her disease does not instan-tiate type 2 diabetes mellitus. However, despite this failurethe sentence is nevertheless still about Mrs. Jones, abouther disease, and about type 2 diabetes mellitus on the levelof reference, because indeed it mentions those three en-tities. It is therefore, per Smith and Ceusters, an ICE that isabout something even though it is a misdiagnosis.Prior ontological work on the aboutness of clinicalstatements like diagnoses has been constrained by theview that an ICE is about nothing (or is perhaps noteven an ICE at all) if it fails on the level of compoundexpression. Martínez Costa and Schulz, for example, usethe universal quantifier when relating an informationentity to a clinical situation to avoid asserting the exist-ence of an entity the existence of which cannot be guaran-teed [26]. For an ICE such as suspected heart failure theywant to avoid the implication that there is some instanceof heart failure that it is about. Because they cannot guar-antee the existence of some heart failure, they use universalquantification to say if it is about anything, then it is aboutan instance of heart failure. Researchers working in areasother than diagnosis have encountered similar issues. Forexample, Hastings et al. note that chemical graphs and dia-grams are not always about types of molecules that exist[27]. They, too, used the workaround of replacing existen-tial quantification with universal quantification to avoidasserting that every chemical graph/diagram is about sometype of molecule that exists (level of compound expres-sion), while still allowing such graphs and diagrams to besubtypes of information content entity.In our own, previous ontological analysis of diagnosis,using the methodology of referent tracking, we identifiedwhat entities must exist or must have existed for a par-ticular diagnostic statement to hold true [28, 29]. A keyresult of this work is that a diagnosis is minimally aboutboth the patient and the type of disease that is assertedto exist. In addition, building on previous work on theOntology for General Medical Science (OGMS), thefoundations of which were laid down in Scheuermannet al. [30], we noted that for a diagnosis to exist (atleast in medicine and under the assumption that thediagnosis was made lege artis), there must also haveexisted a diagnostic process, a person who carried outthat process, and a clinical picture which was used asinput into that process.The hypothesis for the work described here was thatapplying Smith and Ceusters results to disease and diagno-sis, in combination with prior work on the ontology of dis-ease and diagnosis (including provenance of the latter),could address limitations encountered in previous onto-logical work on disease and diagnosis and improve ourHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 2 of 15representations of them in support of studying, copingwith, and reducing ambiguity in the generation of diagnos-tic statements and error in the interpretation thereof.MethodsTo test this hypothesis, we analyzed a set of scenariosthat we created and that involve correct and incorrectdiagnoses, lucky guesses, and justified layperson belief inthe existence of a disease of a certain type. The goal wasto explore whether, and if so how, a realism-based ac-count of information can deal successfully not only withdiagnostic statements asserting the ideal case of a cor-rect diagnosis, but also with deviations from the ideal.MaterialsIn our analysis we used as input (1) Smith and Ceusterswork on aboutness and their definitions of representation,mental quality, cognitive representation, and informationquality entity (Table 1), (2) definitions of disease, disorder,and diagnosis from the Ontology for General MedicalScience (Table 2), and (3) our prior work on analysis ofdiagnostic statements [27, 28].Smith and Ceusters stressed that the relation ofaboutness includes any portion of reality, rather thanbeing limited to just a single particular or a single uni-versal. A portion of reality (POR) can be a particular, auniversal, a relation, or a configuration. A configurationis a combination of particulars and/or universals andcertain relation(s) that hold among them.A representation, then, that is intended to be about aPOR but fails in its aboutness because it misrepresentsthat POR in some way, is misinformation. The sentenceBob Dylan was in the Beatles fails to represent notbecause Bob Dylan or the Beatles did not exist, butbecause such a configuration involving Bob Dylan andthe Beatles in the way as expressed, never existed. Thesentence fails in aboutness on the level of compoundexpression, but nevertheless is about Bob Dylan and theBeatles individually (on the level of reference) and thusis still an information content entity.Smith and Ceusters [25] deal more fully with the issueof what it means that a representation is intended to beabout some entity. Here, we highlight that it follows thedoctrine of the primacy of the intentional [31], whereour written and verbal expressions are to be understoodon the basis of the cognitive acts that generated them.That is, a sentence is about that to which its author wasdirecting his or her thoughts when she wrote it.In addition to Smith and Ceusters work, we also foundedour ontological analysis on the Ontology for GeneralMedical Science or OGMS [30]. This work distinguishesdisease, disorder, and diagnosis, and we used definitionsfrom OGMS as starting points for our analysis (Table 2).Note that in OGMS, a diagnosis refers to the existence of adisease of a given type. In clinical medicine, however, diag-noses also refer to (1) disease courses (e.g., acute hepatitisvs. chronic hepatitis), (2) disorders (e.g., fractures and tu-mors), and (3) the absence of any disease (i.e., a conclusionthat a person is healthy also is a diagnosis). It was not ourgoal to address this issue in this work, as it was not our goalto refine the OGMS definition of diagnosis.The scenariosAll the scenarios have in common a particular patient,Mr. Adam Jones, who suffers from type 2 diabetes mellitus.Thus in every scenario, there exists Mr. Jones, his disease,the type Type 2 diabetes mellitus, the configuration of thesethree entities (which includes the bearer of and instanceof relationships), and the placement in space and time ofthis configuration (Fig. 1).Table 1 Definitions based on Smith and Ceusters [25]Term DefinitionINFORMATION CONTENT ENTITY An ENTITY which is (1) GENERICALLY DEPENDENT on (2) some MATERIAL ENTITY and which is(3) concretized by a QUALITY (a) inhering in the MATERIAL ENTITY and (b) that is_about somePORTION OF REALITYINFORMATION QUALITY ENTITY A REPRESENTATION that is the concretization of some INFORMATION CONTENT ENTITYREPRESENTATION A QUALITY which is_about or is intended to be about a PORTION OF REALITYMENTAL QUALITY A QUALITY which specifically depends on an ANATOMICAL STRUCTURE in the cognitive systemof an organismCOGNITIVE REPRESENTATION A REPRESENTATION which is a MENTAL QUALITYRelation Explanationx is_about y x refers to or is cognitively directed towards y. Domain: representations; Range: portions of realityx concretizes y x is a QUALITY and y is a GENERICALLY DEPENDENT CONTINUANT (GDC) and for some MATERIAL ENTITY z,x specifically_depends_on z at t and y generically_depends_on z at t, and if y migrates from bearer z to anotherbearer w then a copy of x will be created in w.x is_conformant_to y =def. x is an INFORMATION QUALITY ENTITY and y is a COGNITIVE REPRESENTATION and there is someGDC g such that x concretizes g and y concretizes g.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 3 of 15Scenario 1: correct diagnosis by physician (ideal case)Dr. Anne Smith sees Mr. Jones in the office. She takes ahistory and physical, performs certain laboratory testing,and based on her analysis of the findings, correctly con-cludes that Mr. Jones has type 2 diabetes mellitus. Shesubsequently writes her diagnosis in the patients med-ical record.Scenario 2: subsequent correct diagnosis by physician usingfirst diagnosisA second doctor, Dr. John Brown, sees Mr. Jones inthe office at some later date. Mr. Jones has releasedhis records from Dr. Smith to Dr. Brown, who subse-quently sees Dr. Smiths diagnosis prior to seeing Mr.Jones. He uses that diagnosis plus his own findings toinfer a new clinical picture of Mr. Jones, which he subse-quently uses to make another correct diagnosis of Mr.Jones disease. He writes his diagnosis in Mr. Jones med-ical record.Scenario 3: incorrect diagnosis by physicianMr. Jones is traveling on vacation, when he falls ill.He sees Dr. Jane Miller who does not have any of hispast records available, and thus she is not aware ofthe previous diagnoses of Drs. Smith or Brown. Sheinfers a new clinical picture of Mr. Jones, and basedon it incorrectly concludes that Mr. Jones has type 1diabetes mellitus (as opposed to type 2). She recordsa diagnosis of type 1 diabetes mellitus in her medicalrecord for for Mr. Jones.Scenario #4: coincidentally correct conclusion by layperson(lucky guess)A friend of Mr. Jones is a seer. Mr. Jones asks his friendwhat is in his future. Having no prior knowledge of Mr.Jones medical conditions, the seer concludes based onMr. Jones horoscope and the position of the moon thathe has type 2 diabetes mellitus. He subsequently predictsthat Mr. Jones will be hospitalized for his diabetes andmiss his daughters wedding.Scenario #5: laypersons justifiable conclusionMr. Jones daughter, upon learning of her fathers type 2diabetes mellitus, adds this information into her letter toher brother, writing Dad has type 2 diabetes mellitus.Table 2 Key definitions from OGMS used in the analysisTerm DefinitionDISEASE A DISPOSITION (i) to undergo PATHOLOGICAL PROCESSes that (ii) exists in an ORGANISM because of one ormore DISORDERs in that ORGANISM.DISORDER A causally relatively isolated combination of physical components that is (a) clinically abnormal and (b) maximal,in the sense that it is not a part of some larger such combination.DIAGNOSIS A conclusion of an interpretive PROCESS that has as input a CLINICAL PICTURE of a given patient and as outputan assertion (diagnostic statement) to the effect that the patient has a DISEASE of such and such a type.DIAGNOSTIC PROCESS An interpretive PROCESS that has as input a CLINICAL PICTURE of a given patient and as output an assertion tothe effect that the patient has a DISEASE of a certain type.PATHOLOGICAL PROCESS A bodily PROCESS that is a manifestation of a DISORDER.PHENOTYPE A bodily feature or combination of bodily features of an organism determined by the interaction of the geneticmake-up of the organism and its environment.CLINICAL PHENOTYPE A clinically abnormal PHENOTYPE.CLINICAL PICTURE A representation of a CLINICAL PHENOTYPE that is inferred from the combination of laboratory, image and clinicalfindings about a given patient.CLINICAL FINDING A REPRESENTATION that is either the output of a clinical history taking or a physical examination or an imagefinding, or some combination thereof.MANIFESTATION OF DISEASE A QUALITY of a patient that is (a) a deviation from clinical normality that exists in virtue of the realization of adisease and (b) is observable.CLINICAL HISTORY TAKING An interview in which a clinician elicits a clinical history from a patient or from a third party who is authorizedto make health care decisions on behalf of the patient.CLINICAL HISTORY A series of statements representing health-relevant features of a patient.Fig. 1 The configuration of Mr. Jones, his disease, and type 2diabetes mellitusHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 4 of 15Scenario #6: correct diagnosis by computer-based expertsystemA medical student is seeing Mr. Jones in the clinic. Heperforms a history and physical, and types his findingsinto a diagnostic expert system. The diagnostic expertsystem infers based on these findings that Mr. Jones hastype 2 diabetes mellitus. The medical student writes thisdiagnosis in Mr. Jones medical record.The analysisOur analysis follows the method of Referent Tracking,which we have found to be a stringent test of ontologiesand their definitions [28]. This approach proceeds inthree main steps. First, we systematically identify all therelevant particulars that must exist for the scenario to betrue, regardless of whether the scenario explicitly men-tions them or only implies their existence. We assign eachparticular an instance unique identifier (IUI), of the formIUI-n, where n is any integer. Second, we identify foreach particular the type it instantiates and the temporalinterval during which it exists (and assign an identifier ofthe form tn to that interval). Lastly, we identify the rela-tionships that hold between the particulars as well as allrelevant relations particulars have to universals other thaninstantiation, including situations where a particular lacksa given relation to any instance of a certain type (for ex-ample, a statement that a patient has had no cough in thelast two weeks means that the patient does not stand inthe agent_of relation to any instance of the type Coughingevent, indexed temporally to the two-week interval) [32].This approach identifies problems in ontologies and theirdefinitions in two major ways. First, it identifies problemsthat occur when the scenario explicitly rules out the exist-ence of a particular whose existence is implied by an onto-logical definition (and vice versa). Second, it helps identifyexceptions to existing definitions and situations that shouldnot fall under a definition but are erroneously captured byit. Definitions in ontologies can subsequently be adjusted toavoid the errors so identified.Although our approach is to identify particulars im-plied by sentences in natural language, the ontologicalanalysis of language and the mechanism(s) by which itmakes implicit reference to certain entities is not thefocus of this work. Therefore, we convert a sentence likeMr. Jones has type 2 diabetes mellitus to ReferentTracking Tuples (e.g., as in Tables 3, 4, 5, 6 and 7) and itis these tuples in which inhere representations that arethe objects of our analysis.To simplify our analysis somewhat, we wrote scenariosunder which humans record diagnoses on paper. However,concretization of ICEs also occurs by pixels on monitors,binary switches in memory and processor chips, and mag-netic fields on hard disks. But a detailed account of theseconcretizations and transformations among them is notcentral to our analysis of what is a diagnosis. Our analysiscan be extended to these concretizations without modifica-tion of the method.Results and discussionIn each scenario, Mr. Jones (IUI-1) and his disease (IUI-2)exist, the latter inhering in the former (Table 3). Further-more, his disease is an instance of the type type 2 diabetesmellitus at any moment in time during which a diagnosisis formulated in any of the scenarios. Mr. Jones (IUI-1) ex-ists through a certain period of time (t1) of which we donot know the exact beginning or end. We use temporalidentifiers of the form tn to clearly distinguish such iden-tifiers from IUIs: where IUIs are always intended to be glo-bally and singularly unique, distinct temporal identifiersmay denote a unique period of time which is also denotedby another temporal identifier. We also assign an identifierto the time interval during which his disease (IUI-2) exists(t2). Diseases usually begin to exist after the organismdoes, but in the case of congenital genetic diseases, thetwo intervals might be coextensive. Also, we assume thatdisease IUI-2 existed at the time of diagnosing, but werecognize that diagnosing a disease thousands of yearsafter it existed is possible, such as in the case of archaeolo-gists recent diagnosis of Tutankhamuns malaria [33].Note that the configuration of organism, disease, anddisease type is anchored at a particular location in space-time, as is the diagnosis. But note also that the diagnosisadditionally has an implicit or explicit reference to thelocation of the configuration in spacetime. To be a correctdiagnosis, this reference must also be correct (it has torefer to some part, not necessarily the entirety of space-time, occupied by the configuration). Thus, for example,to say that Tutankhamun had malaria in 1000 C.E. ortoday is incorrect, as it would be to say that Mr. Jones hadtype 2 diabetes mellitus before his parents were born.Scenario 1: correct diagnosisIn this scenario, numerous PORs in addition to Mr. Jonesand his disease must exist and stand in certain relation-ships to each other (Tables 4, 5 and 6). Before Dr. Smith(IUI-3) writes (IUI-13) her diagnosis (IUI-8), there is acognitive representation (IUI-6) that is concretized insome anatomical part (IUI-5) of her cognitive system(IUI-4). Note that we follow Ceusters and Smith [34] inasserting that all anatomical entities in which cognitiverepresentations inhere are part of a persons cognitive sys-tem (that is, any entity used in cognition, including thebearing of cognitive representations, are necessarily withina persons cognitive system) at least during the temporalinterval that the cognitive representation exists. If, forexample, it would be the case that some white blood cellflowing through some brain capillary would through someof its molecules take part in the concretization of aHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 5 of 15cognitive representation, then that white blood cell wouldbe part of the cognitive system at least during the exist-ence of that concretization. It would not anymore be partof the cognitive system once it continues its journeythrough the body without participating in thought forma-tion. Additionally, Ceusters and Smith take the position(which we also follow) that the cognitive system is not ne-cessarily strictly limited to the brain or even to the entireneurological system of a person: the current state-of-the-art of neuroscience is yet searching for answers to ques-tions such as what is it in which cognitive representationsinhere? but until it reaches such answers, we remain inour representations agnostic.IUI-9 denotes the sentence Dr. Smith wrote, as it ex-ists on the particular piece of paper she used to write iton: The patient has type 2 diabetes mellitus. This writ-ten statement on paper (IUI-9) bears an informationquality entity (IQE, IUI-10) that concretizes her diagno-sis (IUI-8). The cognitive representation (IUI-6) and IQE(IUI-10) that concretize the diagnosis are both about theconfiguration (IUI-7) (the level of compound expres-sion), as well as about Mr. Jones, Mr. Jones disease, andthe universal Type 2 diabetes mellitus individually (thelevel of reference). The cognitive representation (IUI-6)and the diagnosis (IUI-8) are the output of Dr. Smithsdiagnostic process (IUI-11), which had as input Dr.Smiths clinical picture (IUI-12) of Mr. Jones. Becausethe cognitive representation and IQE concretize thesame ICE, the latter is conformant to the former (seeTable 1).A correct diagnosis is thus fundamentally an informationcontent entity that is concretized by a representation thatstands in an is_about relation to the configuration of an or-ganism, its disease, the relation of inherence between thedisease and the organism, a type that the disease instanti-ates, and the instantiation relation of the disease to thattype, all within a given portion of spacetime (Fig. 2). Fur-thermore, diagnoses are additionally differentiated fromTable 4 The entities in Scenario 1IUI Entity Existence period Type NotesIUI-3 Dr. Anne Smith t3 Human beingIUI-4 Cognitive system of IUI-3 t4IUI-5 An anatomical entity that ispart of IUI-4t5 Anatomical entity Which anatomical entity and its lifetime cannot be easilyspecified given current state of neuroscience.IUI-6 Quality that inheres in IUI-5and is about IUI-7t6 Cognitive representationIUI-7 The POR that is truth-makerfor IUI-8t7 Configuration Mr. Jones, his disease, their relationship, and diseases instantiationIUI-8 Dr. Smiths diagnosis t8 Diagnosis ICE concretized by IUI-6 and IUI-10IUI-9 That which is written down onpaper and forms the sentence.t9 Material entity I conclude therefore that Mr. Jones has type 2 diabetes mellitus.IUI-10 IQE that inheres in IUI-9. t10 Information quality entity The sentence began to exist as soon as ink was laid downon paper, but the IQE did not begin to exist until thesentence was finished.IUI-11 Dr. Smiths interpretive process occupies t11 Diagnostic process Dr. Smiths diagnostic process that led to her diagnosis IUI-8IUI-12 The clinical picture input into IUI-11 t12 Clinical picture Dr. Smiths clinical picture as ascertained prior to t6IUI-13 Dr. Smith writing her diagnosisin the noteoccupies t13 ProcessTable 3 Referent tracking tuples true in every scenarioIUI Entity Existence period Type NotesIUI-1 Mr. Adam Jones t1  the period duringwhich IUI-1 existsMaterial EntityIUI-2 IUI-1s disease t2 DispositionRelationships among particularsIUI-2 inheres in IUI-1 at t2IUI-2 instance of UUI-1 at t2 UUI-1 is a universal unique identifier that denotes type 2 diabetes mellitus.We assume that if something is at any time of its existence an instance oftype 2 DM, it is instance of type 2 DM at all times it exists.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 6 of 15other ICEs by the fact that they are generated by a diag-nostic process that has a clinical picture as input. Weexpand further on what constitutes a clinical picture inthe next scenario, Scenario 2, as well as revisit the diag-nostic process briefly in Scenario 4, although it was notour objective in this work to develop a fuller account ofthis process.Note that it is trivial to state that the particular diseaseinhering in the organism is an instance of entity or evendisease. Thus, there is an expectation that a diagnosis beTable 5 Additional temporal entities in Scenario 1Temporal identifier Description Notest14 The interval during which the anatomical entity (IUI-5)is part of the cognitive system (IUI-4)This interval is not easily specified given the current state ofneuroscience. It could be different than t3 and t4.t15 The interval during which the clinical picture (IUI-12) isused in the interpretive process (IUI-11)Could be shorter than t11t16 The point in time at which the cognitive representation(IUI-6) and diagnosis (IUI-8) begin to existt16 ends t11. Because the ICE does not exist until the cognitiverepresentationits first concretizationexists, this is also thepoint in time at which the diagnosis begins to exist.t17 The interval during which the cognitive representation(IUI-6) participates in the writing process (IUI-13)t18 The interval during which the diagnosis (IUI-8) participatesin the writing process (IUI-13)It is possible that the original cognitive representation (IUI-6) getscopied elsewhere in the brain for reasoning and thus that the ICEcontinues to participate after the initial cognitive representationt19 The interval during which that which is written on paper(IUI-10) begins to exist until it exists in fullThe writing process begins earlier than the time at which thesentence begins to exist: the author starts the process with gettinga pen and paper, any preparation necessary (clicking the pen), etc.Table 6 Relationships among particulars in Scenario 1IUI Relation IUI When relation holds in reality NotesIUI-4 part of IUI-3 at t4IUI-5 part of IUI-4 at t14 All anatomical components in which the cognitive representationinheres are part of the cognitive system. We do not assume thecognitive system is limited to the brain, as the state of neurosciencedoes not permit such an assumption.IUI-6 inheres in IUI-5 at t6IUI-6 is about IUI-7 at t6 The cognitive representation stands in aboutness to IUI-7 as long asit existsIUI-6 is about IUI-1 at t6 It is also about Mr. JonesIUI-6 is about IUI-2 at t6 And about Mr. Jones diseaseIUI-6 is about UUI-1 at t6 And about Type 2 diabetes mellitusIUI-6 concretizes IUI-8 at t6 It also concretizes the diagnosisIUI-10 inheres in IUI-9 at t9 The IQE inheres in the sentence on paperIUI-10 is about IUI-7 at t10 The IQE stands in aboutness to IUI-7IUI-10 is about IUI-1 at t10 It is also about Mr. JonesIUI-10 is about IUI-2 at t10 And about Mr. Jones diseaseIUI-10 is about UUI-1 at t10 And about Type 2 diabetes mellitusIUI-10 concretizes IUI-8 at t10IUI-10 is conformant to IUI-6 at t10 Is conformant to the cognitive representation as long as it existsIUI-3 agent in IUI-11 at t11IUI-12 input into IUI-11 at t15 Clinical picture input into IUI-11IUI-6 output of IUI-11 at t16 Cognitive representation output from IUI-11IUI-8 output of IUI-11 at t16 Both the diagnosis and its concretization are outputs of IUI-11IUI-8 input into IUI-13 at t17 The diagnosis is input into writingIUI-6 input into IUI-13 at t18 As is its cognitive representationIUI-10 output of IUI-13 at t19 The sentence is output of writingHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 7 of 15as precise (the most specific type) as possible and at aminimal level of granularity that is relevant to treat the pa-tient appropriately and to provide a reasonable prognosis.Scenario 2: second diagnosisThe second physician, Dr. Brown, makes a second diag-nosis at a later point in time, using the first diagnosisin addition to clinical and possibly other findings toinfer a new clinical picture of Mr. Jones. With the ex-ception of the configuration of Mr. Jones/his disease/type 2 diabetes mellitus (IUI-7), there is a one-to-onecorrespondence of PORs as in Scenario 1, numberedIUI-23 through IUI-33 (Additional file 1: Tables S1-S3).That is, there is no IUI-27 because the configuration is thesame POR across scenarios. Similarly, there is no IUI-21or IUI-22 because IUI-1 and IUI-2 already identify Mr.Jones and his disease, respectively, uniquely.In this scenario, Dr. Brown (IUI-23) makes a new diag-nosis (IUI-28), concretized both by his cognitive repre-sentation (IUI-26) in some part (IUI-25) of his cognitivesystem (IUI-24) and by the IQE (IUI-30) inhering in thesentence in his note (IUI-29). Dr. Smiths previous diag-nosis (IUI-8) can be viewed as either (view1) being inthe aggregate of things that Dr. Brown uses to infer hisclinical picture (IUI-32) that serves as input into hisdiagnostic process (IUI-31), or (view2) as somethingwhich serves as extra inputalongside his clinical pic-turefor the diagnostic process. The cognitive represen-tation and the IQE are about the configuration (IUI-7)as well as Mr. Jones (IUI-1), his disease (IUI-2), and type2 diabetes mellitus (UUI-1).The current definition of clinical picture in OGMS(see Table 2) seems to conflict with view1 about this sce-nario, because the definition seems to exclude using aTable 7 Relationships of representations to portions of reality in Scenario 3: Incorrect diagnosisRelationships among particulars NotesIUI-46 is about IUI-1 at t46 Dr. Jane Millers cognitive representation is about Mr. JonesIUI-46 is about IUI-2 at t46 And Mr. Jones diseaseIUI-46 is about UUI-2 at t46 And Type 1 diabetes mellitus (denoted by UUI-2)IUI-50 is about IUI-1 at t50 Likewise with the IQE inhering in the ink on paperIUI-50 is about IUI-2 at t50IUI-50 is about UUI-2 at t50IUI-46 is misrepresentation of IUI-7 at t46 But the cognitive representation is a misrepresentation of the configuration,i.e., it is intended to be about the configuration but fails on the level ofcompound expressionIUI-50 is misrepresentation of IUI-7 at t50 The same is true of the IQEFig. 2 Diagram of diagnostic process, its inputs, a correct diagnosis, its concretization, and the configuration that that the concretization is aboutHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 8 of 15past diagnosis to infer a clinical picture. Although thecurrent OGMS definition of clinical picture is inclusiveof clinical findings, diagnosis as currently defined is notan explicit subtype of clinical finding in OGMS. Further-more, it is common for clinicians to elicit a previousproviders past diagnosis from the patient or the patientscaregiver during an interview (for example, if Mr. Jonesin scenario #2 would have said: Dr. Smith says I havetype 2 diabetes mellitus). But the current OGMS defin-ition of clinical history (Table 2) conflicts with this possi-bility. It refers to health-relevant features of a patient, butfeatures as elucidated by OGMS include only qualities,processes, and physical components of the organismnotdispositions of which disease is a subtype. Therefore, arepresentation of a disease such as a diagnosis is currentlyexcluded from the OGMS definition of clinical history.We also note that the OGMS definition of clinical pic-ture is ambiguous in that it is not clear whether it requiresthat laboratory and image findings must always be used toinfer a clinical picture, or that they are the only entities thatcan be used. Regardless, it would be a mistake to do so,because diagnoses can and frequently are made from symp-tom findings alone. Laboratory and image findings are notnecessary components of a clinical picture in reality. Notethat a clinical picture can comprise findings of a singletype (laboratory alone, pathology image alone, radiologyimage alone, physical exam finding alone), or even a singlefinding instance (e.g. Reed-Sternberg cells for a diagnosis ofHodgkins lymphoma). All these issues are compounded bythe fact that the term clinical picture itself is not intuitive.Given that clinical history taking elicits past diagnosesroutinely in clinical medicine, we propose modifying thedefinition of clinical history to accommodate this reality(bolded sections represent changes to the definition):clinical history = def.  A series of statements representingone or more health-relevant features of a patient,possibly complemented by representations of diseasesand configurations.Note that the definition already allowsunder thebroader heading of featurerepresentations of disorders(kinds of physical component) and disease courses (kindsof process). Thus, the definition already accommodatesthese aspects of clinical histories. We also allow the state-ments to represent configurations, in line with Smith andCeusters [2]. These configurations might or might not in-clude various relevant types (for example, The patienthas not participated in any instance of vomiting in the lasttwo weeks.). Finally, note that by using the word repre-senting, the definition also accommodates per Smith andCeusters [2] that some statements might fail in aboutnessdespite their intention to be about such features. In otherwords, some statements in the clinical picture might bewrong: for example, a statement that the patient has a dis-ease or pain that she does not in fact have.To clarify that laboratory and imaging findings are notalways required inputs into the diagnostic process, andto capture realistic scenarios compatible with view2 (forexample, Dr. Brown reads Dr. Smiths note in the chart),we also propose a modified definition of clinical picture(changes in bold):clinical picture = def.  A representation of a clinicalphenotype that is inferred from a combination of, forexample, diagnoses and laboratory, image, and clinicalfindings about a given patient.These changes to the definitions of clinical history andclinical picture now properly capture situations wherepast diagnoses are elicited from the patient and/or hercaregiver during a clinical history taking: these diagnosesare now clinical findings in the clinical history that wasgenerated by the clinical history taking (see the definitionof clinical finding in Table 2).Scenario 3: MisdiagnosisThe third physician, Dr. Miller, misdiagnoses Mr. Jonestype 2 diabetes mellitus as type 1 diabetes mellitus (Fig. 3).Per Smith and Ceusters, because the misdiagnosis is stillabout Mr. Jones, his disease, the relationship betweenthem, and the type type 1 diabetes mellitus on the levelof reference, it is an information content entity. However,it fails to be about the configuration IUI-7 as a whole onthe level of compound expression.Again, in this scenario there exist PORs in one-to-onecorrespondence (except the configuration and its compo-nents) numbered IUI-43 through IUI-53 (Additional file 2:Tables S4-S6). Dr. Miller (IUI-43) writes (IUI-53) his mis-diagnosis (IUI-48) in Mr. Jones chart, and the IQE (IUI-50)inhering in the ink (IUI-49) is conformant to his cognitiverepresentation (IUI-46), and both are abouton the levelof referenceMr. Jones, his disease, and type 1 diabetesmellitus. But neither one is about the configuration (IUI-7).To capture the relation both (1) between the cognitive rep-resentation and the configuration and (2) between the IQEand the configuration, we define a new relation:is-misrepresentation-of: domain: representation, range:portion of reality.Def: x is-misrepresentation of y iif x is a representationand x is intended to be about y and it is not the casethat x is about y.Then we assert that the representations (IUI-46 andIUI-50) are misrepresentations of the configuration(Table 7 and Additional file 2: Table S6). Note that ourdefinition precludes the cognitive representation (IUI-46)and IQE (IUI-50) being about any configuration otherthan IUI-7, because they are not intended to be about, forexample, the configuration of the sun, earth, and moon ata particular date and time.Note that asserting the incorrect disease type is notthe only way to make a misdiagnosis. There are at leastHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 9 of 15six possibilities where a diagnosis fails to be about a con-figuration on the level of compound expression (Table 8).If a representation fails on the level of reference, it alsofails on the level of compound expressions, because aconfiguration cannot consist of that which does not exist.These six possibilities could also exist in combination, butif the 2nd, 3rd, and 4th possibilities are all present (for ex-ample, Ron Weasley has spattergroit), then there is not adiagnosis, or even any information content entity at all,because the representation is not about anything even onthe level of reference. Of course, if the organism itself doesnot exist, then there cannot be a clinical picture inferred,and thus it would not be a diagnosis or misdiagnosis,although it could still be an ICE if it is about a really-existingdisease type (for example, James Bond has influenza).Also, as medical knowledge evolves, the profession comesto understand that certain types of disease thought to existin fact do not. Thus past diagnoses of dropsy and con-sumption we now understand to be misdiagnoses.Despite searching the extensive literature on diagnosticerror, we could not find any studies that looked at whatpercentages of misdiagnoses fall into these categories. Weconjecture based on our past clinical expertise and experi-ence that asserting the incorrect disease type is the mostcommon mistake among those in Table 8, but confirm-ation or rejection of this conjecture requires study.Scenario 4: the lucky guessIn this scenario, a layperson (the seerIUI 63) correctlyconcluded coincidentally that Mr. Jones had type 2 diabetesFig. 3 Misdiagnosis of type of disease. The diagnosis is individually about the patient, the disease, and the incorrectly diagnosed disease type Y,but it is not about the configuration of patient, disease, and disease type XTable 8 Six possibilities for a diagnosis failing in aboutness on the level of compound expressionsProblem Where it fails first DescriptionNoninstantation, asserted type exists Level of compound expression Disease instantiates a different type than the stated type,but the stated type existsNoninstantation, asserted type does not exist Level of reference Disease instantiates a different type than stated, while thestated type of disease does not existDisease nonexistence Level of reference The disease instance does not existOrganism nonexistence Level of reference The organism instance does not exist. In this case, therecould not be a clinical picture properly inferred and thusit is not a misdiagnosis although it could still be an ICE.Disease non-inherence Level of compound expression The disease inheres in a different organism than the onestated. For example, the doctor mistakenly ascribesMr. Johnsons hypertension to his twin.Configuration is not located in that part ofspacetime where the diagnosis says it is located.Level of compound expression A diagnosis of type 2 diabetes mellitus 5 years ago is wrongbecause the patient didnt have the disease at that time,even though the patient has type 2 diabetes today. Also,a diagnosis that the patient has an upper respiratory tractinfection today when in reality the infection resolvedtwo weeks ago.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 10 of 15mellitus based on the position of the moon and Mr. Joneshoroscope (Additional file 3: Tables S7-S9). It would bewrong to say the seers reasoning (IUI-71) constituted adiagnostic process. To avoid coincidentally correct state-ments from qualifying as diagnoses, we additionally requireas input into the diagnostic process cognitive representa-tions of the disease type and the types instantiated by thesequalae, signs, symptoms, and any clinical, laboratory, orimaging findings or phenotypes of the instances of thisdisease type. Note that this is a minimal requirement:clinicians often additionally include in their diagnosticreasoning cognitive representations of other diseasetypes and associated PORs when considering alternativepossibilities for the disease type.This view is based on the extensive literature on clin-ical reasoning processes, especially diagnosis (for a re-view, see Norman [35]). This research has establishedthe use of representations, called knowledge structures,in the diagnostic process. The nature and form of theserepresentations evolves as clinical expertise develops[36], and we note that the differences in diagnostic pro-cesses that result could result in a typology of diagnosticprocesses in OGMS.Because the seer had no cognitive representations oftype 2 diabetes mellitus, let alone used them as inputinto his reasoning, his conclusion (IUI-68), althoughan ICE, is not a diagnosis. Similarly, if a physician makesa lucky guess based not on his cognitive representationsof the stated disease type but instead by flipping a coinor some such, that too would not be a diagnosis.To Table 3 we add an aggregate of cognitive represen-tations of disease types and associated entities as inputinto the diagnostic process (Table 9).We propose to redefine diagnostic process as follows:Diagnostic process = def. An interpretive PROCESSthat has as input (1) a CLINICAL PICTURE of a givenpatient AND (2) an aggregate of REPRESENTATIONsof at least one type of disease and at least one type ofphenotype whose instances are associated with instancesof that disease, and as output an assertion to the effectthat the patient has a DISEASE of a certain type.Scenario 5: laypersons justifiable conclusionMr. Jones daughter wrote a sentence in her letter to herbrother based on reading Dr. Smiths progress note say-ing that that her father has type 2 diabetes mellitus(Additional file 4: Tables S10-S12). Of course, the daugh-ter has not made a diagnosis. She is communicating toher brother what she believes to be the case.Had she merely written Dr. Smith says and then cop-ied Dr. Smiths sentence word for word into her letter,then her writing would concretize Dr. Smiths diagnosis(IUI-8). This is the case of hearsay (so-and-so said itwas the case that).As Smith and Ceusters showed, however, the same sen-tence written by two different people does not guaranteethey concretize the same ICE. ICEs are further differenti-ated by the provenance of their concretizations, includingwho created them and when, and to what POR they intendto be about. In their example, two people writing the sen-tence Barack Obama has never been President of the UnitedStatesone before and one after Obamas inauguration asPresidentgenerate two different ICEs. The one writtenafter fails on the level of compound expressions but not onthe level of reference, whereas the one written before suc-ceeds on both levels (it remains true that at the time whenthe sentence was written, he had never been President).We therefore distinguish between a human (1) merelycopying a representation, in which case the copy con-cretizes the same ICE as the original text and (2) creat-ing her own cognitive representation of the PORwhichinvolves forming a belief that the POR really existed asrepresentedand then subsequently creating an IQEthat is conformant to the cognitive representation. Inthe former case, a new ICE does not come into being. Itdoes not even require in the cognitive system of thecopier any representation of the POR that the originalrepresentation is about (as in the case of copying Germantext that one does not understand at all). In the latter case,by contrast, a new ICE does come into being.In Scenario 5, the daughter did not merely repeat Dr.Smiths diagnosis. She communicated to her brother herbelief about her fathers disease. She deliberately chose notTable 9 Additional tuples required to distinguish diagnosing from a lucky guessIUI Entity Lifetime Type NotesIUI-14 The aggregate of Dr. Smiths cognitiverepresentations of various disease typesand their associated types of phenotypesincluding type 2 diabetes mellitus that heused in the diagnostic processt20 Aggregate of cognitiverepresentationsRelationships among particularsIUI-14 input into IUI-11 at t21 t21 refers to the temporal interval duringwhich IUI-14 participated in the reasoningprocess. It could start at the same time ast11 or after t11, and end at the same timeas or before t11.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 11 of 15to merely convey Dr. Smiths diagnosis, but rather her be-lief that her father has type 2 diabetes mellitus. She heardthe opinion of an expert, in whom she had trust. Based on(1) her observations of her father, (2) Dr. Smiths diagno-sis, and (3) her trust in Dr. Smith, she reached the conclu-sion herself that her father suffers from type 2 diabetesmellitus. Because she did not begin with a clinical pictureand her own cognitive representations of type 2 diabetesmellitus, her conclusion is not a diagnosis.However, consider the scenario where she is given theclinical picture and has enough knowledge to arrive at aconclusion, which could be the case either if she were aphysician or somehow other acquired or were given thenecessary knowledge: it is analagous to Scenario #6,where she takes the place of the expert system (see ana-lysis of that scenario below). Thus, here in Scenario #5 itis important to note that she did not reason from a clin-ical picture to the diagnosis.In this scenario, therefore, the daughter has created anew ICE (IUI-88) that is not a diagnosis. She has con-cretized it in the sentence (IUI-89) in her letter.Scenario 6: diagnosis by non-humanThe diagnostic decision support system has made a diag-nosis (or misdiagnosis depending on whether it is correct),because it (1) takes as input a clinical picture and repre-sentations of the relevant disease type and one or moretypes of phenotypes with which it is associated; (2) partici-pates in a process of making a conclusion based on thisinput; and (3) outputs from this process a statement abouta configuration involving an organism, a disease, and adisease type.In this case, there are no cognitive representations. Intheir place are digital representations on hard drives,memory chips, and central processing units. If we as-sume the system generates a sentence and prints it onpaper, then we have an analagous IQE to the writtendiagnosis of the physician and ICE of the sister.Nothing in our proposed definitions conflicts with thisscenario. Replacing Dr. Smith and associated representa-tions and diagnostic process with various components ofthe computer and its digital representations as well asinferential process (which is an instance of diagnosticprocess) is straightforward.Returning briefly to a point made in Scenario #5, Mr.Jones daughter could follow the exact same algorithm(s)of the diagnostic expert system using the exact sameclinical picture as input, and she would arrive at (ormake) a diagnosis, in contrast to scenario #5 where herconclusion was an ICE but not a diagnosis.ConclusionsWe applied Smith and Ceusters results on aboutness[25] to diagnosis in order to develop an account ofdiagnosis, misdiagnosis, lucky guesses, hearsay, a layper-sons justified belief about disease configurations, and adiagnosis made by an expert system. Our key result isthat a correct diagnosis, as defined by OGMS, is about aconfiguration of an organism, its disease, and the typethe disease instantiates (level of compound expression)in a specified portion of spacetime. A misdiagnosis bycontrast is a misrepresentation of this configuration.Nevertheless, both diagnosis and misdiagnosis are stillaboutat the level of individual referencethe organismand (when they exist) a disease instance and a diseasetype. Also, they are both the output of a diagnosticprocess, which differentiates them from lucky guess andhearsay as well as the misinformation-based counterpartsto lucky guess and hearsay. We also carefully representedthe inputs and outputs of this process.We identified several subtypes of misdiagnosis (e.g.,wrong disease subtype, wrong patient, wrong temporalplacement) that have not been differentiated in theliterature on diagnostic error, to our knowledge. Studyingthe incidence and causes of these subtypes might advancethe study of diagnostic error and strategies to reduce it.Note that as we have defined it, misdiagnosis does notrefer to the diagnostic errors of absent diagnosis (failing todiagnose a disease at all, let alone incorrectly) and delayeddiagnosis. Lastly, we note that the current literature ondiagnostic error, per a 2016 Institute of Medicine report,does not lend itself to generating reliable estimates of inci-dence of diagnostic error per se, let alone any subtype ofsuch error [37].Although misdiagnoses involving non-existence of certainentities might at first seem to be of minor importance, wehighlight two cases where non-existence is relevant. First,in the case where the type of disease does not exist (con-sider past diagnoses of dropsy), it could well be that ourunderstanding of disease decades from now is much moreadvanced, and what we think are types of disease today infact are not. So just as with past diagnoses of dropsy, itcould be that todays diagnoses of schizophrenia are mis-diagnoses merely by referring to a type that does not exist.Second, in the case where the instance of disease does notexist, we consider two scenarios. The first scenario involvespast diagnoses of mental illness where neither the instancenor the type exists. For example, past diagnoses of runawayslaves as having drapetomania involved neither a reallyexisting instance nor a really existing type of disease. Thesecond scenario involves patients with hypochondria orwho are malingering. They feign a condition for which theunassuming practitioner mistakenly asserts the existence ofan instance and the instantiation of a type.Our results and typology of misdiagnosis could serveas the beginnings of a formal framework for studyingdiagnostic error as a component of data quality in EHRsand research data collections, in response to the call byHogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 12 of 15Weiskopf and Weng for more formal, generalizable, andvalidated methods for assessing data quality [24]. ApplyingCeusters detailed typology of mistakes in ontology (e.g.,asserting a type that does not exist) [38] and referenttracking systems (e.g., assigning an identifier but there isno corresponding particular that it identifies, assigningone identifier to two particulars, assigning two identifiersto one particular, etc.) [39] to diagnosis could build on ourwork here to build out such a framework. It remains fu-ture work to do so.The provenance of the ICE and its concretizations arecritical: lucky guesses, hearsay, and laypersons conclusionsabout disease (when not arrived at through a diagnosticprocess using a clinical picture and cognitive representa-tions of the associated type(s) of disease as input) do notconstitute diagnoses and therefore are different types ofICE than diagnoses. Provenance also includes which find-ings and other information constituted the clinical pictureused in the diagnostic process. Our analysis of the scenariosidentified past diagnoses as important input into the diag-nostic process, leading to proposed redefinitions of clinicalhistory, clinical picture, and diagnostic process for OGMS.Smith and Ceusters results on aboutness and our ex-tension of them here to diagnosis reduce the need forthe workarounds reported by Martínez Costa andSchulz [26] and Hastings et al. [27] It is perfectly legit-imate to relate suspected heart failure finding to con-gestive heart failure with an existential quantifier: if aninstance of this type is not about a really-existing con-figuration of patientdiseaseheart failure, it is still anICE that is individually about the patient, her condition,and the type heart failure on the level of reference. InOWL, we could assert:Suspected heart failure ICE - > ICE and (is aboutSOME Organism)Suspected heart failure ICE - > ICE and (is aboutSOME Condition)In more expressive formalisms including first-orderlogic, we could also assert that it is about the type heartfailure, where Type, Instance_of , and Is_about arepredicates in what follows, where the universal quantifi-cation applies to the ICE, not what it is about:Type(heart_failure)Type(suspected_heart_failure_ICE)?x (Instance_of(x, suspected_heart_failure_ICE) - >Is_about(x, heart_failure))Similarly, chemical graphs and diagrams are ICEs aboutindividual types of atoms such as carbon, oxygen, hydro-gen, and so on, even when they fail to be about any typeof configuration (e.g., molecule) of such atoms. However,because they are typically not about any instances, properexistential quantification in OWL is not possible. How-ever, we could relate in first-order logic the diagram ofoctaazacubane (a hypothetical molecule which would becomprised of eight nitrogen atoms arranged in a cubicstructure) to the nitrogen type of atom using existentialquantification (again where the universal quantification inwhat follows applies to the ICE and not what it is about):Type(nitrogen_atom)Type(octaazacubane _diagram)?x (Instance_of(x, octaazacubane_diagram) - >Is_about(x, nitrogen_atom))It is therefore not required to use universal quantificationover the range of things that an ICE is about, when relatingICEs to those entities they are about, to avoid failure ofaboutness on the level of compound expression. This resultis qualified by the constraints of representational formal-isms such as OWL that prevent directly asserting aboutnessto types. Schulz et al. describe workarounds in OWL toasserting aboutness to types, that may be of benefit in someuse cases [40].The use of universal quantification actually introducesproblems when we account for aboutness on the level ofindividual reference. For example, if we leave the suspectedheart failure finding of Martínez Costa and Schulz as beingonly about congestive heart failure, then it would result in acontradiction to say that it is about some organism. Like-wise for condition. So use of the universal quantifier pre-cludes aboutness on the level of individual reference, indirect conflict with the results of Smith and Ceusters onmisinformation.Although it was not the primary or even secondarygoal of the present work, other advantages of our ap-proach with respect to inference are easy to derive. First,in our approach with explicit representation of the diseasein addition to the diagnosis, we can infer all instances ofType 1 diabetes mellitus that have been misdiagnosed asType 2 diabetes mellitus at some point in time, in firstorder logic minimally and possibly in OWL with work-arounds. Generalizing slightly, we can query for all condi-tions that have been misdiagnosed as Type 2 diabetesmellitus. Using a typology of organisms, we can find in theveterinary domain all diagnoses and/or misdiagnoses of acertain type of disease in organisms of a certain type: forexample, misdiagnoses of foot and mouth disease in cattle.Having no ability to create an aboutness relation from amisdiagnosis, or more generally an incorrect clinical state-ment, to the organism it is about (due to the contradic-tions that will result as pointed out above) or even toanything in reality at all, the universal quantifier approachof Martínez Costa and Schulz would require substantialrevision to make these inferences.Hogan and Ceusters Journal of Biomedical Semantics  (2016) 7:54 Page 13 of 15In the realm of chemical diagrams, our approach en-ables one to query for all chemical diagrams that depictnitrogen atoms or certain chemical groups (e.g., hy-droxyl group and benzene rings), including the diagramsthat are not about any existing type of molecule. Theuniversal quantifier approach in Hastings et al., by con-trast, would require significant revision to return dia-grams that depict nitrogen, hydroxyl groups, benzenerings, and so on, but are not about any existing type ofmolecule. In depth exploration of the effects of our rep-resentation on inference remains future work, as it isnot our primary interest here.Our analysis also identified problems with, and sug-gested improvements to, the definitions of core termsfrom the Ontology for General Medical Science includ-ing diagnostic process and clinical picture. This resultis consistent with our past work, where we have foundthe method of referent tracking analysis to be a stringenttest of definitions in ontologies.This work is limited by the fact that we did not conductfurther ontological analysis of the diagnostic process be-yond OGMS and beyond what our scenarios required, asthis was not the purpose of the present work. We do notethat our requirement for including cognitive representa-tions of disease types as input into the diagnostic processis based on this literature, however. Engaging experts inthe study of clinical reasoning in future work to develop atypology of diagnostic processes has the potential to resultin a corresponding typology of diagnoses.Future work includes (1) an account of differential diag-nosis, where a clinician or expert system generates a list oflikely types of disease for further investigation to identifythe actual type the organisms disease instantiates; (2) pro-posing to the OGMS community to clarify the definitionsof clinical history, clinical picture, and diagnostic processas suggested here, and to expand the definition of diagno-sis to include disorders, disease courses, and absence ofdisease (i.e., healthy); (3) extending our analysis as re-ported here to this expanded definition of diagnosis; (4)conducting deeper ontological analysis of the diagnosticprocess, in coordination with experts in the study of clin-ical reasoning; and (5) more fully exploring the effects ofour representations on logical inference beyond somereadily evident advantages discussed here.Additional filesAdditional file 1: Table S1. Entities in Scenario 2: Second correctdiagnosis. Table S2. Additional temporal entities in Scenario 2: Secondcorrect diagnosis. Table S3. Relationships among particulars in Scenario 2:Second correct diagnosis. (DOCX 88 kb)Additional file 2: Table S4. Entities in Scenario 3: Incorrect diagnosis.Table S5. Additional temporal entities in Scenario 3: Incorrect diagnosis.Table S6. Relationships among particulars in Scenario 3: Incorrect diagnosis.(DOCX 88 kb)Additional file 3: Table S7. Entities in Scenario 4: Laypersons unjustifiedinference. Table S8. Additional temporal entities in Scenario 4: Laypersonsunjustified inference. Table S9. Relationships among particulars in Scenario4: Laypersons unjustified inference. (DOCX 95 kb)Additional file 4: Table S10. Entities in Scenario 5: Laypersons justifiedinference. Table S11. Additional temporal entities in Scenario 5: Laypersonsjustified inference. Table S12. Relationships among particulars in Scenario 5:Laypersons justified inference. (DOCX 92 kb)AbbreviationsBFO: Basic formal ontology; GDC: Generically dependent continuant;IAO: Information artifact ontology; ICE: Information content entity;IQE: Information quality entity; OGMS: Ontology for General Medical Science;POR: Portion of reality; RT: Referent tracking; RTT: Referent tracking tupleAcknowledgmentsThis work was suppoted in part by the NIH/NCATS Clinical and TranslationalScience Award to the University of Florida UL1TR001427.Authors contributionsThe authors contributed equally to the ontological analysis and developmentof results. Author WRH created the first version of the manuscript. Both authorshad full access to all materials and analysis and participated in revising themanuscript. Both authors approved the final version of the manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1University of Florida, 2004 Mowry Rd, P.O. Box 100219, Gainesville, FL32610-0219, USA. 2Department of Biomedical Informatics, University atBuffalo, 77 Goodell street, 5th floor, Buffalo, NY 14203, USA.Received: 1 October 2015 Accepted: 6 September 2016Han et al. Journal of Biomedical Semantics  (2016) 7:22 DOI 10.1186/s13326-016-0059-zRESEARCH Open AccessActive learning for ontological eventextraction incorporating named entityrecognition and unknown word handlingXu Han1, Jung-jae Kim2* and Chee Keong Kwoh1AbstractBackground: Biomedical text mining may target various kinds of valuable information embedded in the literature,but a critical obstacle to the extension of the mining targets is the cost of manual construction of labeled data, whichare required for state-of-the-art supervised learning systems. Active learning is to choose the most informativedocuments for the supervised learning in order to reduce the amount of required manual annotations. Previous worksof active learning, however, focused on the tasks of entity recognition and protein-protein interactions, but not onevent extraction tasks for multiple event types. They also did not consider the evidence of event participants, whichmight be a clue for the presence of events in unlabeled documents. Moreover, the confidence scores of eventsproduced by event extraction systems are not reliable for ranking documents in terms of informativity for supervisedlearning. We here propose a novel committee-based active learning method that supports multi-event extractiontasks and employs a new statistical method for informativity estimation instead of using the confidence scores fromevent extraction systems.Methods: Our method is based on a committee of two systems as follows: We first employ an event extractionsystem to filter potential false negatives among unlabeled documents, from which the system does not extract anyevent. We then develop a statistical method to rank the potential false negatives of unlabeled documents 1) by usinga language model that measures the probabilities of the expression of multiple events in documents and 2) by usinga named entity recognition system that locates the named entities that can be event arguments (e.g. proteins). Theproposed method further deals with unknown words in test data by using word similarity measures. We also applyour active learning method for the task of named entity recognition.Results and conclusion: We evaluate the proposed method against the BioNLP Shared Tasks datasets, and showthat our method can achieve better performance than such previous methods as entropy and Gibbs error basedmethods and a conventional committee-based method. We also show that the incorporation of named entityrecognition into the active learning for event extraction and the unknown word handling further improve the activelearning method. In addition, the adaptation of the active learning method into named entity recognition tasks alsoimproves the document selection for manual annotation of named entities.Keywords: Active learning, Biomedical natural language processing, Information extraction*Correspondence: jjkim@i2r.a-star.edu.sg2Data Analytics Department, Institute for Infocomm Research, 1 FusionopolisWay, 138632 Singapore, SingaporeFull list of author information is available at the end of the article© 2016 Han et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 InternationalLicense (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in anymedium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commonslicense, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 2 of 18BackgroundA common framework of information extraction systemsis supervised learning, which requires training data thatare annotated with information to be extracted. Suchtraining data are usually manually annotated, where theannotation process is time-consuming and expensive. Onthe other hand, in biomedical domain, recent researchefforts on information extraction are extending fromfocusing on a single event type such as protein-proteininteraction (PPI) [1] and gene regulation [2] to simulta-neously targeting more complicated, multiple biologicalevents defined in ontologies [3], which makes the man-ual annotation more difficult. There is thus the need ofreducing the amount of annotated data that are requiredfor training event extraction systems.Active learning is the research topic of choosing infor-mative documents for manual annotation such that thewould-be annotations on the documents may promotethe training of supervised learning systems more effec-tively than the other documents [4]. It has been studiedin many natural language processing applications, suchas word sense disambiguation [5], named entity recog-nition [68], speech summarization [9] and sentimentclassification. Its existing works can be roughly classifiedinto two approaches: uncertainty-based approach [10] andcommittee-based approach [11]. The uncertainty-basedapproach is to label the most uncertain samples by usingan uncertainty scheme such as entropy [12]. It has beenshown, however, that the uncertainty-based approachmayhave worse performance than random selection [1315].In the biomedical information extraction, theuncertainty-based approach of active learning has beenapplied to the task of extracting PPIs. For instance, [16]proposed an uncertainty sampling-based approach ofactive learning, and [17] proposed maximum uncertaintybased and density based sample selection strategies.While the extraction of PPI is concerned with a singleevent type of PPI, however, recent biomedical eventextraction tasks [18] involve multiple event types,even hundreds of event types in the case of the GeneRegulation Ontology (GRO) task of BioNLP-ST13 [19].The committee-based approach, based on a committeeof classifiers, selects the documents whose classificationshave the greatest disagreements among the classifiersand passes them to human experts for annotation. Thisapproach, however, has several issues in adaptation forevent extraction tasks. First, event extraction (e.g. PPIextraction, gene regulation identification) is different frommany other applications of active learning, which are inessence document classification tasks. Event extraction isto locate not only event keywords (e.g. bind, regulates),but also event participants (e.g. gene/protein, disease)within documents and to identify pre-defined relationsbetween them (e.g. subject-verb-object). Thus, even if theevent extraction systems produce confidence scores for itsresultant events, the confidence scores do not correspondto the probability of how likely a document expressesan event type: in other words, how likely a documentbelongs to an event type class, which should be the goalof classifiers of the committee-based approach for eventextraction. Second, previous classifiers for the committee-based approach may miss some details of events includingevent participants. For example, the keyword expressionmay mislead a classifier to predict that the document withthe keyword expresses gene expression event, althoughthe document does not contain any gene name.Our target tasks of event extraction for active learningin this paper are those introduced in BioNLP-ST13 [20],which involve multiple, complicated event types. Cur-rently, there is only one event extraction system availablefor all the tasks, called TEES [21], and we need an addi-tional classifier to follow the committee-based approach.We thus propose as an additional classifier a novelstatistical method for informativity estimation, which pre-dicts how likely a text expresses any event concept of agiven ontology. The method is based on a language modelfor co-occurrences between n-grams and event concepts.Furthermore, it independently estimates the presence ofevent participants in a text and the probabilities of out-of-vocabulary words and combines them with the predictionof event concept in the text. We collectively estimate theinformativity of a text for all the concepts in a givenontology, similarly to the uncertainty-based approachof [2224].We also present a revised committee-based approach ofactive learning for event extraction, which combines thestatistical method with the TEES system as follows: Sincethe confidence scores of the TEES system are not reli-able for active learning, we take TEES outputs as binary,that is, whether the system extracts any instance of a con-cept from a text or not. The disagreement between theTEES system and the statistical model is captured when,given a text (T) and an event concept (C), the TEES systemdoes not extract any instance of C in T, but the probabilis-tic model predicts a high probability of C in T. In otherwords, the TEES system is used for filtering potential falsepositives, and the probabilistic model for ranking them.We further adapt our active learning method and thestatistical method for event concept detection to namedentity recognition, including gene name recognition. Weshow that our method can improve active learning fornamed entity recognition as well, when tested against theBioCreative and CoNLL datasets.MethodsWe formalize the general workflow of active learning asfollows: At the start of round t, let U (t?1) be the pool ofunlabeled documents and let L(t?1) be the pool of labeledHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 3 of 18documents, where t starts from 1. In round t, we selectthe most informative document x(t) from U , manuallylabel it, and add it to L. If the label y(t) is assigned tothe document x(t) by the oracle, the labeled and unlabeleddocument sets are updated as follows:L(t) = L(t?1) ?{(x(t), y(t))} U (t) = U (t?1)\x(t)(1)Such process is iterated until a certain stopping criteriais met, such as when U = ? and after a pre-defined num-ber of rounds. It also can be done in a batch mode, wherea group of documents are selected at each round for themanual labeling.Active learning method for event extractionAs explained above, our active learning method followsthe committee-based approach. As the committee, weemploy two classifiers: A classifier based on an eventextraction system called TEES and a statistical classifierbased on language modeling (see the next section fordetails). The TEES [21] is a state-of-the-art biomedicalevent extraction system based on support vector machine,and was the only system that participated in all the tasks ofBioNLP-ST13, showing the best performance in many ofthe tasks [25]. The TEES system produces the confidencescore of each event it extracts. However, we do not usethe score for active learning because the confidence scoredoes not indicate the probability of the event in the doc-ument. We also assume that if the TEES system extractsan event (E) from a document (D), D is not informativefor E, because true positives are already not informativeand because the correction (i.e. labeling) of false positivesmight not be useful for training event extraction systemswhere event descriptions are scarce, and thus there are farmore negative examples than positive examples. In otherwords, the primary goal of our active learning method isto correct more false negatives, that is, to annotate thetrue events not extracted by the existing system. Figure 1depicts the workflow of the proposed method.Our method works iteratively as follows: In round t, wetrain the TEES system and the statistical classifier basedonL(t?1). Wemeasure the informativity of each unlabeleddocument among U (t?1) and choose the top documentsas feed for manual annotation. We measure the informa-tivity score of a document at the sentence level, that is, theaverage of the informativity scores of all the sentences inthe document, as illustrated in (2).x?Informativity = argmaxx I? t (x) I? t (x) =1||x||Sk?x?I(Sk)(2)Document (D) Concept/relation (C)Does TEES annotate C on D?D is informative for CUnlabeled corpus OntologyNo YesIs D likely to express C?Yes NoD is not informative for CFig. 1 Overview of proposed active learning method. The integrationof underlying system into active learning method. For eventextraction task, if the underlying event extraction system (TEES) canrecognize the concept (C) in the given document (D), the D is notconsidered as informative? t indicates the current models of the TEES system andthe statistical classifier at round t, but we will omit it forsimplicity.The informativity of a sentence (Sk) is measured for theevent concept set E , which contains all event defined ina given ontology, as expressed in (3). The informativityscore for event concept set is denoted as I(Sk , E). In fact,the BioNLP-ST13 tasks include not only events, but alsorelations. A key difference between events and relations isthat an event always involves an event keyword (e.g. reg-ulates for GeneRegulation), but a relation does not haveany keyword (e.g. partOf). For simplicity, wemention onlyevents in the paper, while ourmethod involves both eventsand relations in the same way.I(Sk) = I(Sk , E) (3)Informativity for event concept setThe informativity of a sentence for event concept set iscalculated as the sum of the informativity scores of thesentence for all the event as follows:I(Sk , E) =Ei?E?I(Sk ,Ei) (4)Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 4 of 18As explained earlier, we treat a sentence as non-informative for an event if the event extraction systemTEES can extract any instance of the event from thesentence. Otherwise, the informativity score is estimatedas the probability of the concept given the sentence asfollows:I(Sk ,Ei)={0 if Ei is recognized in Sk by the TEESmodel at round tp(Ei|Sk) otherwise(5)p(Ei|Sk) can be converted into (6) using the Bayestheorem.p(Ei|Sk) = p(Ei)p(Sk|Ei)p(Sk) (6)The P(Ei) is estimated using the maximum-likelihoodestimation (MLE) based on the statistics of event annota-tions in the training data.As for P(Sk|Ei), we score the correlation between thesentence Sk and the event Ei with a real value scoring func-tion Z (see below for details) and use the softmax functionto represent it as a probabilistic value, shown in (7).p(Sk |Ei) = ?(Z(Sk : Ei)) = 1?Ej?E exp(Z(Sk : Ej))exp(Z(Sk : Ei))(7)We use two types of units to approximately representthe sentence Sk : n-grams (NG) and predicate-argumentrelations (PAS) produced by the Enju parser [26]. A sen-tence is represented as a bag of elements of a unit, forexample, a bag of all n-grams or a bag of all predicate-argument relations from the sentence.A. Using N-gram feature for probability estimation Ifwe use the bag of n-gram model, the score Z(Sk : Ei)is measured using the average of the correlation scorebetween the n-gram (NG) contained in the sentence withthe event, expressed in (8), where len(Sk) is the normaliza-tion factor and is calculated as the word count of sentenceSk .Z(Sk : Ei) = 1len(Sk)NGj?Sk?p(NGj|Ei) (8)While the probability between the n-gram and eventp(NGj|Ei) is also calculated using a correlation scoreW (NGj,Ei) between the n-gram and the event, togetherwith the softmax function, shown in (9).p(NGj|Ei) = ?(W (NGj,Ei))= 1?NGl?U exp(W (NGl,Ei))exp(W (NGj,Ei))(9)The correlation scoreW (NGj,Ei) is calculated using oneof the following three methods: 1) Yates chi-square test,2) relative risk, and 3) odds ratio [27]. For the calculationof the three methods, a 2×2 table is constructed for eachpair of an N-gram and an event at the level of sentences,as shown in Table 1. For example, a indicates the numberof sentences that contain the N-gram NGj and express theevent Ei.Based on the 2×2 table, the three methods of Yateschi-square test, relative risk, and odds ratio calculate thecorrelation score for the pair as shown in the formulas(10), (11), and (12), respectively.W (NGj,Ei) = N(|ad ? bc| ? N/2)2NSNFNANB(10)W (NGj,Ei) = a/(a + b)c/(c + d) (11)W (NGj,Ei) = a/bc/d (12)B. Using predicate-argument relation for probabilityestimation Similarly for the bag of predicate-argumentrelation model, the score Z(Sk : Ei) is calculated with theaverage of the correlation scores between the event andthe predicate-argument relations from the sentence, as in(13).Z(Sk : Ei) = 1len(Sk)PASj?Sk?p(PASj|Ei) (13)Additional features of active learningWe introduce two additional features of our active learn-ing method: Incorporation of event participants and deal-ing with out-of-vocabulary words.Incorporation of event participantsThe absence of event participants should negatively affectthe prediction of events. To reflect this observation, weutilized a gene name recognition system, called Gimli [28],in order to recognize gene/protein names, since most ofthe BioNLP shared tasks involve genes and proteins (e.g.gene expression, gene regulation, phosphorylation). Weincorporate the results of the Gimli system into our activelearning method as follows:I(Sk) = I(Sk , E ,NG) + I(Sk ,N ) (14)Table 1 Numbers of sentences for the calculation of correlationscore between Ei and NGjExpress event Ei Not express event Ei TotalContain N-gram NGj a b NANot contain N-gram NGj c d NBTotal NS NF NHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 5 of 18I(Sk ,N ) = ? × T (15)T indicates the number of gene/protein names pre-dicted in a sentence Sk .In fact, the Gimli system can be replaced with othernamed entity recognition systems for tasks whose eventparticipants are other than gene/protein. Since the eventextraction tasks for evaluating our active learning method(i.e. BioNLP shared tasks) are mainly about gene/protein,we do not replace the Gimli system when evaluating theincorporation of event participants. When we apply ouractive learning method for the tasks of named entityrecognition (NER), however, we will evaluate it againsttwo NER systems (i.e. Gimli, Stanford NER system) (seefor details Sections Active learning method for NER taskin Page 8, Datasets and employed systems in Page 11,and Evaluation of active learning method for NER task inPage 19).Dealing with OOV issue with word similarityWhen we use the n-gram features, there is Out-of-Vocabulary (OOV) issue, such that some n-grams in thetest dataset may not appear in the training dataset. Totackle this issue, we adopt the word2vec system, which isan unsupervised method for representing each word as avector in a latent semantic model and for measuring wordsimilarity [29], as follows: Consider an n-gram NGout thatdoes not occur in the training dataset. We use word2vecto find the top-k n-grams NGin that are closest to NGout ,where the word similarity score between NGout and eachNGin is designated as Sim(NGout ,NGin). We then recal-culate the correlation scoring function W (NGout ,Ei) asshown in Formula (16). Note that since word2vec can onlyhandle unigrams, and also since unigrams show the bestperformance in our experiments of parameter optimiza-tion (see the next section), we only deal with unknownunigrams in this method. The word similarity scores aretrained a priori using the whole set of MEDLINE abstractsreleased in April 2014.WOOV (NGout ,Ei)=NGin?TrainingDataset?top?kW(NGin,Ei)× SimNGout ,NGin(16)We denote the n-gram-based informativity of sentencecalculated using the updated correlation scoring function(16) as I(Sk ,NGOOV ). For example, when the correlationscoring function in (9) is updated, the resultant informa-tivity in (4) is denoted as I(Sk , E ,NGOOV ).Linear combination of n-gram and predicate-structurerelation featuresWhile we choose either n-grams or predicate-argumentrelations as features, we also tested the linear combinationof the two feature sets for our active learning method, asfollows:I(Sk) = ? × I(Sk ,NGOOV ) + ? × I(Sk ,PAS) + ? × I(Sk ,N )= ? × I(Sk , E ,NGOOV ) + ? × I(Sk , E ,PAS) + ? × I(Sk ,N )(17)Table 2 illustrates the calculation of informativity scoresin pseudo codes.Active learning method for NER taskWe also adapt our active learning method to named entityrecognition (NER), considering the ontology concepts ofnamed entities (e.g. Gene, Disease) instead of events (e.g.PPI, gene regulation). Themethod for named entity recog-nition estimates informativity, or the likelihood of a textexpressing any named entities.Similar to Eq. (2), the informativity estimation in theNER task is expressed in (18).x?Infomativity = argmaxx I? t (x) I? t (x) =1||x||Sk?x?I(Sk)(18)? t indicates the current model of a given NER systemand the statistical classifier at round t, but we will omitit for simplicity. We evaluate our method with two NERsystems of Gimli for biomedical domain and StanfordTable 2 Proposed algorithm of active learning with TEESInput: labeled document pool L, unlabeled document pool U, batch size b// InitializationER0 = the set of events/relations annotated on LLearn a TEES modelM0 from ER0i = 0 // the index of the current round// Active Learning Loopwhile U is not empty:i += 1for each document Dij in U:Document informativity score I(Dij) = 0for each sentence Sk in Dij :ApplyMi?1 to Sk and collect the resultant events/relations set ERSkfor each event/relation er s.t. er /? ERsk :I(Dij) += informativity score I(Sk , er)I(Dij) = I(Dij) / sizeOf(Dij)Rank Dij in U based on I(Dij) and select the top b documents,designated as BRemove B from U, add B to L, and add the annotations on B to ERi?1,designated as ERiLearn a new modelMi from ERiHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 6 of 18NER system for general domain (see Section Results anddiscussion for details of evaluation), one system at a time.The informativity of a sentence for named entity set iscalculated as the sum of the informativity scores of thesentence for all the named entities as follows:I(Sk) = I(Sk ,N ) =Ni?N?I(Sk ,Ni) (19)Similar to the active learning method for event extrac-tion, we treat a sentence as non-informative for an namedentity if the NER system can recognize any instance of thenamed entity from the sentence. Otherwise, the informa-tivity score is estimated as the probability of the namedentity given the sentence as follows:I(Sk ,Ni)={0 ifNi is recognized in Sk by the NER system at round tp(Ni|Sk) otherwise(20)The probability p(Ni|Sk) is calculated as follows:p(Ni|Sk) = p(Ni)p(Sk|Ni)p(Sk) (21)Similarly to the estimation for event, p(Ni) is estimatedusing the maximum-likelihood estimation (MLE) basedon the statistics of named entities in the training data. Forthe calculation of p(Sk|Ni), we follow similar steps as in(7), using n-grams (i.e. Formula (8)), but not using PAS (i.e.Formula (13)).Comparison with related worksIn this section, we describe the previous methods of activelearning that we compare with our proposed methods forevent extraction in the evaluation experiments.A. Conventional committee-based method The com-mittee based active learning, based on a committee ofclassifiers, selects the documents whose classificationshave the greatest disagreements among the classifiers andpasses them to human experts for annotation, expressedas follows:x?Committee = argmaxx D? (Y |x) (22)D? (Y |x) is the disagreements among the classifiers fora document x under the model ? , and the Y is the wholelabel set. We use the summation of disagreement over thesentence Sk contained in the document x.D? (Y |x) =Sk?x?D(Y |Sk) (23)For each sentence, we measure the collective disagree-ment over the whole event concept set E defined in theontology by using the sum of all disagreement for allevent Ei.D(Y |Sk) =Ei?E?D(Ei|Sk) (24)The disagreement D(Ei|Sk) is calculated using the abso-lute value of the differences of the probability producedby the classifiers, named the aforementioned informativityestimation method and the TEES event extraction system.D(Ei|Sk) = |pInformativity(Ei|Sk) ? pTEES(Ei|Sk)| (25)The pTEES(Ei|Sk) is the probability estimated from theTEES system, and pInformativity(Ei|Sk) is from the infor-mativity estimation using statistical method, which is cal-culated in Eq. (6). Note that while p(Ei|Sk) in Eq. (5) isestimated using Eq. (6) only for the sentences from whichno Ei is recognized by the TEES, the same informativityprobability in Eq. (25) is estimated for all the sentences ofunlabeled documents.However, as the TEES is a support vector machine(SVM) based system and do not produce probabilistic out-put, we use the confidence the SVM classifier has in itsdecision for a event prediction as follows:pTEES(Ei|Sk) =?(C(Ei|Sk)) = 1?Ej?E exp(C(Ej|Sk))exp(C(Ei|Sk))(26)C(Ei|Sk) is the confidence for the classifier.The confidence is calculated using the difference-2 ofthe distance from the separating hyperplane, produced bythe SVM classifier. It is shown to have best performancein active learning [30, 31], and the calculation is expressedas follows:mmax = argmaxmdist(m, Sk)n = argmaxn=mmaxdist(n, Sk)C(Ei|Sk) = dist(mmax, Sk) ? dist(n, Sk)(27)The dist(m, Sk) is the distance of the predicted label min such sentence Sk .Similarly in adapting to the NER task, for each sen-tence, we measure the collective disagreement over thewhole named entity concept setN by using the sum of alldisagreement for all named entity Ni.D(Y |Sk) =Ni?N?D(Ni|Sk) (28)The disagreementD(Ni|Sk) is calculated using the abso-lute value of the differences of the probability producedby the classifiers, named the aforementioned informativityestimation method and the NER system.D(Ni|Sk) = |pInformativity(Ni|Sk) ? pNER(Ni|Sk)| (29)The pNER(Ni|Sk) is the marginal probability provided bythe Conditional Random Field (CRF) model from the NERHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 7 of 18system, and pInformativity(Ni|Sk) is from the informativityestimation using statistical method.B. Entropy based active learning method Entropy isthe most common measure for uncertainty, which indi-cates a variables average information content. The docu-ment selection of entropy-based methods is formalized asfollows:x?Entropy = argmaxx H? (Y |x) (30)The H? (Y |x) is the entropy of a document x under themodel ? and the Y is the whole label set. We use the sum-mation of entropy over the sentence Sk contained in thedocument x.H? (Y |x) =Sk?x?H(Y |Sk) (31)For each sentence Sk , we use the aforementioned bagof n-gram method, and estimate H(Y |Sk) as the averageentropy of each n-gram NGj in Sk , as follows:H(Y |Sk) = 1len(Sk)NGj?Sk?H(Y |NGj) (32)We estimate the collective entropy over the whole eventconcept set E defined in the ontology as the summation ofthe entropy for all event Ei.H(Y |NGj) =Ei?E?H(Ei|NGj) (33)H(Ei|NGj) is calculated by using the Weka package forthe calculation of entropy [32].C. Gibbs error based active learning method Gibbserror criterion is shown to be effective for active learning[33], which selects documents that maximize the Gibbserror, as follows:x?Gibbs = argmaxx HGibbs(?) (34)Similarly to the entropy-based method implementation,we calculate the collective Gibbs error as follows:HGibbs(?)=Sk?x?HGibbs(Y |Sk)=Sk?x? 1len(Sk)NGj?Sk? Ei?E?HGibbs(Ei|NGj)(35)For the calculation of HGibbs(Ei|NGj), we use the con-ditional probability of p(Ei|NGj), defined as follows [33],where p(Ei|NGj) is estimated using the proposed methodas shown in (9):HGibbs(Ei|NGj) = 1 ? p(Ei|NGj)2 (36)Results and discussionDatasets and employed systemsThe BioNLP shared tasks (BioNLP-ST) were organizedto track the progress of information extraction in thebiomedical text mining. In this paper, we used the datasetsof three tasks, namely GRO13 (Gene Regulation Ontol-ogy) [19], CG13 (Cancer Genetics) [34] and GE13 (GeniaEvent Extraction) [35]. Each corpus was manually anno-tated with an underlying ontology, whose number ofconcepts and hierarchy are different from each other. Acomparison between the datasets is given in Table 3. Notethat since the official test datasets for CG and GE tasks areinaccessible, we instead use parts of their training datasetsas the test datasets, and the statistics of the datasetsinclude only those accessible documents.Specifically, we employ the state-of-the-art StanfordNER [36] system for the CoNLL-2003 [37] dataset, and theGimli gene name recognition system [28] for the BioCre-ative II Gene Mention [38] dataset. Note that in BioCre-ative task, the named entities are naturally of one class, i.e.,the Gene/Protein name; while the CoNLL dataset involvesfour classes of named entities (i.e. Person, Organization,Location, Misc).Evaluation metrics for comparison of active learningmethodsTo compare the performance of the different strategiesof sample selection, we plot their performance in eachiteration. Since the difference between some plots is notobvious, however, we mainly use the evaluation metric ofdeficiency for comparison [39, 40], defined as follows:Defn(AL,REF) =?nt=1(accn(REF) ? acct(AL))?nt=1(accn(REF) ? acct(REF))(37)The acct(C) is the performance of the underlying clas-sifier C at tth round of learning iteration. AL is an activelearning method, and REF is a baseline method (see belowfor details). n refers to the total number of rounds (i.e. 10).A deficiency value smaller than 1.0 means that the activelearning method is superior to the baseline method, andin general, a smaller value indicates a better method.Table 3 Summary of task datasets used in the experimentsTask Corpus size (Dev/Train/Test) Document type No. event concepts No. relationsGRO13 300 (50/150/100) MEDLINE abstract 507 10CG13 400 (100/200/100) MEDLINE abstract 58 1GE13 20 (5/10/5) PubMed Central full text 13 20Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 8 of 18Parameter optimizationWe first take a parameter optimization step to determinethe most appropriate parameters for the aforementionedcalculation of informativity scores.Correlationmeasure and n-gram sizeAs mentioned above, we considered three correlationmeasures to estimate the correlation score between n-gram and event, including chi-square test, relative risk,and odds ratio.We also should determine the value of n forn-grams. To find the optimal solutions for the two tasks,we carried out a simulation of ontology concept predic-tion at the sentence level as follows: Given a sentence Siand Ni ontology concepts manually annotated on the sen-tence, we predict the top Ni ontology concepts in Si andcompare them with the Ni manually annotated concepts,measuring the overlap between the two concepts sets.We select the best combination of co-occurrence analysismethod and n-gram size for the rest of experiments in thispaper.Using 10-fold cross validation, the average predictionrate is calculated and reported in Table 4. Each columncorresponds to an n-gram size, and each row to one of thethree co-occurrence analysis methods used for the pre-diction. Note that when N=2 (i.e. bi-grams), it does notinclude unigrams for the calculation. N=1-2 indicates themixture of unigrams and bi-grams. This experiment iscarried out using the GRO13 dataset.As shown in Table 4, for all co-occurrence analysismethods, the accuracy mostly drops as the length of N-grams increases. This may happen due to the data sparse-ness problem for large N-grams. We choose to use chi-square test and unigrams for the following experimentsbased on the results.Parameter for the incorporation of event participantsThe parameter of ? in Eq. (15) is to determine the sig-nificance of effects of event participants on event con-cept prediction. We tested our active learning method inEq. (14) against the GRO13 dataset with the ? values setas 0.15, 0.25 and 0.35. We summarize the performanceresults in terms of deficiency in Table 5. We choose the? = 0.25 for the following experiments based on theresults.Table 4 Parameter optimization resultsCalculationmethodN-gramN = 1 N = 2 N = 3 N = 4 N = 5 N = 1 ? 2Chi-square 0.507 0.413 0.159 0.036 0.009 0.436Relative ratio 0.341 0.395 0.307 0.128 0.038 0.361Odds 0.420 0.395 0.274 0.117 0.035 0.407The averaged concept prediction accuracy is reported. The best accuracy ishighlighted in boldfaceTable 5 Parameter optimization resultsMethod GRO13RS_Average 1? = 0.15 0.716? = 0.25 0.706? = 0.35 0.713The deficiencies of active learning method using different factor against the GRO13are reported. The best deficiency is highlighted in boldface in this table and also inthe tables belowParameter for dealing with OOV issueIn dealing with the OOV issue, we choose top-k simi-lar words for an unknown word, as in Formula (16). Inorder to choose the optimal value for k, we use the linearcombination method in Eq. (17) with the other parame-ters ? = 0.1, ? = 0.1 and ? = 0.8, and test our activelearning method against the GRO13 dataset, as changingthe k value from 5 to 25. We summarize the deficiency ofthe active learning method using the different k values inTable 6. As the result, we choose k=25 for the remainingexperiments.Evaluation of active learning methods for event extractionActive learningmethods using informativity estimationIn the following evaluations, we show the learning curvesand deficiencies of the event extraction system TEESunder different sample selection strategies against thedataset of GRO13, CG13 and GE13 task. The activelearning methods use only the informativity estimation,but not the additional features such as incorporation ofevent participants and dealing with OOV issue, which willbe discussed in the next section.We compare the proposed active learning method withother sample selection strategies, including random selec-tion, and entropy-based [17], and Gibbs error [33] based,as well as a conventional committee based active learningmethods. We use the random selection as the baseline fordeficiency calculation. Each experiment has ten rounds,where in each round, 10 % of the original training dataare added for training the TEES system. The initial modelof the TEES system before the first round is trained onlyon the development dataset. Note that the test data ofTable 6 Parameter optimization resultsMethod GRO13RS_Average 1LC_(?=0.1,?=0.1,?=0.8), k = 5 0.611LC_(?=0.1,?=0.1,?=0.8),k = 10 0.600LC_(?=0.1,?=0.1,?=0.8),k = 15 0.617LC_(?=0.1,?=0.1,?=0.8),k = 20 0.628LC_(?=0.1,?=0.1,?=0.8),k = 25 0.563The deficiencies of active learning method using different factor against the GRO13Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 9 of 18Table 7 Deficiencies of sample selection methods for eventextraction against the GRO13, CG13 and GE13 datasetsMethod GRO13 CG13 GE13RS_Average 1 1 1AL(Entropy) 1.017 1.226 0.854AL(GibbsError) 1.039 0.993 0.850AL(ConventionalCommittee_PAS) 0.830 0.589 0.439AL(ConventionalCommittee_Unigram) 0.832 0.788 0.263AL(Informativity_PAS) 0.845 0.581 0.872AL(Informativity_Unigram) 0.760 0.768 0.139each dataset is fixed. The followings are considered for theselection of additional 10 % training data in each round: Random selection: We randomly split the trainingdata into 10 bins in advance, and during the trainingphase in each round, one bin is randomly chosen. Wereport the averaged performance of random selectionfor ten times (hereafter referred as RS_Average). Entropy-based active learning: We calculate theentropy of each document based on (30), sortdocuments by their entropy values and feed fromdocuments with top values to those with bottomvalues as training data. (designated as AL(Entropy)) Gibbs error based active learning: We calculate theGibbs error of each document based on (34), sortdocuments by their Gibbs error values and select thedocuments with top values as training data.(designated as AL(GibbsError)) Proposed active learning: We evaluate the methodusing either unigrams (Unigram) orpredicate-argument relations (PAS). The resultantmethod is referred as AL(Informativity_Unigram)and AL(Informativity_PAS), respectively. Conventional committee-based active learning: Weevaluate the committee based method based on (22),using the confidence score produced by TEES. Weestimate the informativity using either unigrams(Unigram) or predicate-argument relations (PAS)for the proposed statistical method. The resultantmethod is referred as AL(ConventionalCommittee_Unigram) and AL(ConventionalCommittee_PAS), respectively.We first apply those methods to the dataset of GRO13[19] and measure the performance change of the TEESsystem with the incremental feed of the training data. Wesummarize the deficiency for each method in Table 7.The proposed active learning methods and the con-ventional committee-based methods achieve deficiencyvalue of less than 1, while the entropy and Gibbs errormethod achieve a deficiency higher than 1, suggestingthat the entropy and Gibbs error methods do not per-form better than that of random selection. Particularly,the AL(Informativity_Unigram) method achieves the bestdeficiency of 0.760, while the corresponding conventionalcommittee based method achieves the performance of0.832 in AL(ConventionalCommittee_Unigram), whichis an 8.65 % improvement for the informativity basedmethod over that of conventional committee-basedmethod. However, when using the PAS model, the25272931333537394143450 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of PAS model in GRO13'RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_PAS)AL(Informativity_PAS)Fig. 2 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against GRO13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the GRO13 task dataset. The active learning method uses thepredicate-argument relation (PAS) modelHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 10 of 1825272931333537394143450 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of unigram model in GRO13' RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_Unigram)AL(Informativity_Unigram)Fig. 3 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against GRO13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the GRO13 task dataset. The active learning method uses theunigram modelAL(Informativity_PAS) achieves deficiency of 0.845,which is 1.78 % worse than that of the committee-basedmethod, whose deficiency is 0.830. In addition, whencomparing the performance of the methods using thePAS and unigram, we notice that using the unigram,the proposed informativity method shows an 10.1 %improvement over that using PAS model, yet this is notevident in the committee-based method. The results sug-gest that the proposed informativity method performsbest when using the unigram model in the GRO13dataset. We then plot the learning curves for each methodin Figs. 2 and 3. In Fig. 3, the AL(Informativity_Unigram)3237424752570 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of PAS model in CG13' RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_PAS)AL(Informativity_PAS)Fig. 4 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against CG13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the CG13 task dataset. The active learning method uses thepredicate-argument relation (PAS) modelHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 11 of 183237424752570 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of unigram model in CG13' RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_Unigram)AL(Informativity_Unigram)Fig. 5 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method,random selection against CG13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the CG13 task dataset. The active learning method uses theunigram modelmethod is consistently performing over the other meth-ods after 50 % of the documents are selected, which alsoexplains the results in the comparison of deficiency val-ues. In addition, in the comparison of average numberof instances per ontological concept provided in [41], theGRO13 dataset have 13 instances per concept, while suchvalue for GE13 dataset is 82. This also suggests that indatasets such as GRO13 whose document annotationmay not be abundant, the active learning method usingthe unigram may perform better than the PAS model.However, the experiment result in the GRO13 datasetindicates that the proposed informativity based activelearning method with unigram model can show betterperformance than the conventional committee-based, theentropy based and the Gibbs error based active learningmethods.We then carry out a similar experiment using the CG13dataset. We summarize the deficiency for each method40455055600 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of PAS model in GE13' RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_PAS)AL(Informativity_PAS)Fig. 6 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against GE13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the GE13 task dataset. The active learning method uses thepredicate-argument relation (PAS) modelHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 12 of 1840455055600 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure (%)Percentage of selected documentsLearning curve of TEES with active learning of unigram model in GE13' RS_averageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee_Unigram)AL(Informativity_Unigram)Fig. 7 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against GE13 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the GE13 task dataset. The active learning method uses theunigram modelin the Table 7. In this experiment, the Gibbs error basedapproach achieves the deficiency value of less than 1, whilethe deficiency for the entropy based method is 1.226.Comparing the PAS and unigram model, the deficiencyvalues for PAS model are generally better than those ofunigram model. For instance, in the committee-basedmethod, the percentage of deficiency difference is 25.3 %.Similarly in the proposed informativity method, there is a24.3 % change in the deficiency value. This may suggestthat the PAS model may be more suitable for the CG13dataset. In addition, while comparing the proposed infor-mativity method and committee-based method, the infor-mativity method achieves better deficiency value over thecommittee-based method. In terms of deficiency differ-ence, the improvements are 0.020 and 0.008, for PASand unigram feature, respectively, which is a less obviousimprovement for the informativity method. However, thisalso suggest that the PAS feature may be more sensitivethan that of unigram in the CG13 dataset. Note that oneof the specialties in CG13 dataset is that only a single2530354045500 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure(%)Percentage of selected documentsIntegration of prediction of named entities into active learningAL(Informativity_PAS + NE)AL(Informativity_PAS)AL(Informativity_Unigram + NE)AL(Informativity_Unigram)AL(ConventionalCommittee_PAS+ NE)AL(ConventionalCommittee_PAS)Fig. 8 Integration of named entity recognition into active learning with PAS and n-grams against GRO13 dataset. The learning curves for the TEESsystem under the proposed informativity method using predicate-argument relation (PAS) and unigram model, as well as the conventionalcommittee (ConventionalCommittee) based active learning method as the benchmark. In contrast, each method is integrated with the output fromthe named entity recognition result (NE)Han et al. Journal of Biomedical Semantics  (2016) 7:22 Page 13 of 18Table 8 Deficiencies of active learning methods with andwithout integrating the prediction of named entities (NE) againstGRO13 datasetMethod GRO13RS_Average 1AL(ConventionalCommittee_PAS) 0.830AL(ConventionalCommittee_PAS + NE) 0.693AL(Informativity_PAS) 0.845AL(Informativity_PAS + NE) 0.589AL(Informativity_Unigram) 0.760AL(Informativity_Unigram + NE) 0.706relation type of Equiv is defined. Equiv is a symmetricand transitive binary relation to identify entity mentionsas being equivalent in the sense of referring to the samereal-world entity [42]. Such relation is not evaluated inthe GRO13 or GE13 dataset. The better performanceof PAS model over unigram model may due to that thePAS model is more stable for identification of equiva-lent entity mentions than the unigram model. The learn-ing curves for the active learning method are plotted inFigs. 4 and 5.We extend the aforementioned active learning meth-ods to the GE13 dataset, and the Table 7 summarizethe deficiency of the methods. In Table 7, all methodsachieve deficiency values less than the random selection.The method of Gibbs error based approach achieve thedeficiency of 0.850, while the deficiency for the entropymethod is 0.854. The proposed active learning methodsusing the unigram shows a more obvious improvementthan that using PAS. For instance, in the committee-based method, there is an improvement of 40.1 % forthe unigram model over the PAS model. This may sug-gest that, against the GE13 dataset, the unigram featureis more suitable for proposed method than that of thePAS feature. We notice a more obvious improvement forthe unigram model in the informativity method. Partic-ularly, the best performing AL(Informativity_Unigram)achieve a deficiency value of 0.139.While the correspond-ing committee-based method achieve the deficiency of0.263 in AL(ConventionalCommittee_Unigram). We plotthe learning curves in Figs. 6 and 7. In the Fig. 7,the active learning method using unigram generallyshows obvious improvement over the baseline of ran-dom selection method, yet the active learning methodusing PAS show less significant improvement over thebaseline method. This may due to the fact that theontology defined in GE13 task is generally less com-plicated than that in GRO13 and CG13. In addition,the document annotation in the GE13 dataset may beabundant, as the average number of instances per onto-logical concept in GE13 dataset is 82, above six timesmore than that of GRO13 dataset [41]. Given the datasetwith less complicated ontological concepts and abun-dant training data of document annotation, the unigrammodel may show obvious improvement for active learningmethods.Active learningmethods using additional featuresIncorporation of event participants We evaluate theactive learning method that is incorporated with therecognition of gene/protein names for event extraction,as illustrated in Formula (14). We show the performance2530354045500 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measure(%)Percentage of selected documentsIntegration of word vector into active learningAL(Informativity_WordVec)AL(Informativity_Unigram)Fig. 9 Evaluation of incorporation of the word vector method into active learning with n-grams against GRO13 dataset. The word vector is appliedinto the active learning method to solve the out-of-vocabulary (OOV) issue that exists in the unigram model. For the unknown unigram, its score isreplaced by the top-25 most similar known unigramsHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 14 of 18Table 9 Deficiencies of using word vector to solve theOut-Of-Vocabulary(OOV) issue for the unigram modelMethod GRO13RS_Average 1AL(Informativity_Unigram) 0.790AL(Informativity_WordVec) 0.769of the TEES system, with active learning method thatis either with or without using the gene/protein names.Such experiment is carried out using the GRO13 dataset.The experiment results are plotted in Fig. 8 and wesummarize the deficiency values in the Table 8. Inthe Table 8, the incorporation of gene/protein namesshows positive effects towards the active learning methodfor event extraction, for both of bag of n-gram orPAS method. By using the gene/protein names, thedeficiency for the active learning method using PASis further improved from 0.845 to 0.589, which is a30.3 % improvement. Yet in the unigram model ofthe informativity method, the improvement is ratherless significant of 7.1 %, which may suggest that somenamed entities are already captured as n-grams, thusredundant.In addition, we notice similar improvement of the con-ventional committee-based method by incorporating theinformation of event participants into the part of sta-tistical informativity estimation, from 0.830 (i.e. Con-ventionalCommittee_PAS) to 0.693 (i.e. Conventional-Committee_PAS + NE), a 16.5 % improvement. How-ever, this improvement is significantly less than that forour proposed method, which may indicate that the con-fidence scores of the TEES used by the conventionalcommittee-based method hamper the effects of eventparticipants.Dealing with OOV issue with word similarity Then-gram model is based on the registered n-grams thatoccur in the training data, which has the issue of Out-of-Vocabulary (OOV) words. We solve this by usingthe word2vec toolkit to find top-k words that are clos-est to a given OOV word in the test data and to usetheir weights to estimate the weight of the OOV word.The results of evaluating the word vector incorporationagainst the GRO13 dataset are plotted in Fig. 9, and thedeficiency is summarized in Table 9. Note that the experi-ments about OOV word handling are carried out only forevents, excluding relations, observing that the relationsof the BioNLP-ST13 tasks are little affected by the OOVissue, since they are not associated with trigger words.By using the word similarity, the n-gram model methodis further improved, as the deficiency of n-gram modelgoes from 0.790 to 0.769, an improvement of 2.66 %.The rather less significant improvement may suggest thatsuch OOV issue is rather not prevalent in the GRO13dataset.Linear combination of n-gram and predicate-structurerelation featuresLastly, we linearly combine the proposed n-gram andpredicate-structure relation features for the active learn-ing, as expressed in Eq. (17), and to understand which ofthe active learning methods proposed in this paper aremore important towards the overall performance.We use four weight combinations of (?=0.8, ?=0.1,?=0.1), (?=0.1, ?=0.8, ?=0.1), and (?=0.1, ?=0.1, ?=0.8),as well as the equal distribution of weight (?=0.33, ?=0.33,?=0.33). The method of AL(Informativity_PAS + NE)Fig. 10 Evaluation of linear combination of active learning methods against GRO13 dataset. The active learning modules are assigned with differentweights and combined linearly. Different weight assignment strategies are comparedHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 15 of 18Table 10 Deficiencies of linear combination of active learningmethodsMethod GRO13RS_Average 1AL(Informativity_PAS + NE) 0.589LC_(?=0.33,?=0.33,?=0.33) 0.740LC_(?=0.8,?=0.1,?=0.1) 0.772LC_(?=0.1,?=0.8,?=0.1) 0.752LC_(?=0.1,?=0.1,?=0.8) 0.563LC_(?=0,?=0,?=1) 0.583is used as the benchmark, as it is the best perform-ing method in the previous experiments in the GRO13dataset. Note that the AL(Informativity_PAS + NE) cor-responds to the weight combination of (?=0, ?=1, ?=1).Additionally, we also use the benchmark of only using thenamed entity for the active learning, i.e the weight com-bination of (?=0, ?=0, ?=1), to check if simply using thetotal number of recognized named entities be sufficientfor the active learning method.The results of comparison are plotted in Fig. 10, andwe summarize the deficiency values in Table 10. Overall,the weight combination of (?=0.1, ?=0.1, ?=0.8) showsthe best performance (deficiency 0.563). Compared toPAS or unigram-based statistics, the incorporation ofevent participants has the most effect on the best per-formance. Note, however, that the model of using onlythe event participants, i.e., the weight combination of(?=0, ?=0, ?=1), achieves the deficiency of 0.583, higherthan the best deficiency, which indicates that the PASor n-gram based statistics are complementary to eventparticipants.Evaluation of active learning method for NER taskWe apply the active learning method into NER task asexpressed in Eq. (18), and follow the similar experimentdesign. Each sample selectionmethod starts with the sameheld-out labeled development dataset for model initializa-tion and a pool of unlabeled training dataset for selection.In each round, 10 % of the unlabeled documents in thetraining dataset are selected by different sample selec-tion strategies. For evaluation, we report the performanceof NER system trained with the selected training docu-ment in each round, against the same held-out test datasetfollowing the official evaluation procedure.The sample selection strategies are as follows: Random selection: We randomly split the trainingdataset into 10 bins in advance, one bin is randomlychosen in each round. Following 10-fold crossvalidation, we report the averaged performance ineach round. (hereafter referred to as RS_Average) Entropy-based active learning: The entropy ofdocuments are calculated, and select documents bytheir entropy values, from the top to bottom.(designated as AL(Entropy) ) Maximum Gibbs Error based active learning: Similarto the entropy-based method, but uses the Gibbserror, as introduced in [33]. (designated asAL(GibbsError) ) Proposed active learning method using informativityscoring only: Use the aforementioned system in0.640.690.740.790.840 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measurePercentage of selected documentsLearning Curve of Gimli with BioCreative dataset RS_AverageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee)AL(Informativity)Fig. 11 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against BioCreative dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error basedmethod (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposedinformativity method (Informativity), as well as the random selection (RS), when tested against the BioCreative task datasetHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 16 of 18Table 11 Deficiencies of sample selection methods against theBioCreative and CoNLL datasetsMethod BioCreative CoNLLRS_Average 1 1AL(Entropy) 1.171 0.737AL(GibbsError) 1.045 0.885AL(ConventionalCommittee) 0.684 0.763AL(Informativity) 0.514 0.575Eq. (18), and selects documents based on theirinformativity scores. (designated asAL(Informativity)) Conventional committee-based active learning: Weevaluate the committee based method based on (22),using the confidence score produced by NER system.The resultant method is referred as AL(ConventionalCommittee).We applied these methods to the BioCreative datasetand plotted the learning curve of Gimli in Fig. 11,and summarized their deficiency values in Table 11. InFig. 11, the proposed active learning method show steadyimprovement over the other methods in most rounds.Based on the deficiency comparison in Table 11, theproposed method achieved a deficiency value of 0.514,while the deficiency for the conventional committee basedmethod is 0.684.We carried out similar experiments with the CoNLLdataset, and the learning curves are plotted in Fig. 12,and the deficiencies are compared in Table 11. In Fig. 12,the proposed active learning method outperforms theother methods; and in terms of deficiency, the proposedmethod achieves 0.575 in the deficiency, a nearly 42 %improvement over the random selection. In contrast, thebenchmark of Entropy and Gibbs error based approachesalso are shows deficiency value of less than 1, yet theirimprovement over the random selection is nearly 26 %and 11 %. The deficiency for the conventional com-mittee based method is 0.763. The experiment resultsin the BioCreative and CoNLL datasets indicate thatthe proposed informativity based method can show bet-ter performance than the conventional committee-basedmethod, as well as the Entropy and Gibbs error basedmethods.ConclusionsIn this study, we proposed a novel active learning methodfor ontological event extraction, which is more complexthan the simple PPI extraction. Our method measuresthe collective informativity for unlabeled documents,in terms of the potential likelihood of biological eventsunrecognizable for the event extraction system. We eval-uated the proposed method against the BioNLP SharedTasks datasets, and showed that our method can achievebetter performance than other previous methods, includ-ing entropy and Gibbs error based methods and theconventional committee-based method. In addition, theincorporation of named entity recognition into the activelearning for event extraction and the unknown wordhandling further improved the active learning method.Finally, we adapted the active learningmethod into namedentity recognition tasks and showed that the method alsoimproved the document selection for manual annotationof named entities.0.780.790.80.810.820.830.840.850.860.870.880 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%F-measurePercentage of selected documentsLearning Curve of Stanford NER with CoNLL datasetRS_AverageAL(Entropy)AL(GibbsError)AL(ConventionalCommittee)AL(Informativity)Fig. 12 Comparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, andrandom selection against CoNLL dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error based method(Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativitymethod (Informativity), as well as the random selection (RS), when tested against the CoNLL task datasetHan et al. Journal of Biomedical Semantics  (2016) 7:22 Page 17 of 18Competing interestsThe authors declare that they have no competing interests.Authors contributionsXH conceived the study, designed and implemented the system, carried outthe evaluations and drafted the manuscript. JJK and CKK motivated the studyand revised the manuscript. All authors read and approved the finalmanuscript.AcknowledgementsThis research was partially supported by Ministry of Education, Singapore,grant (MOE2014-T2-2-023).Author details1School of Computer Engineering, Nanyang Technological University, 50Nanyang Avenue, 639798 Singapore, Singapore. 2Data Analytics Department,Institute for Infocomm Research, 1 Fusionopolis Way, 138632 Singapore,Singapore.Received: 21 February 2015 Accepted: 28 March 2016RESEARCH Open AccessInferring gene-to-phenotype and gene-to-disease relationships at Mouse GenomeInformatics: challenges and solutionsSusan M. Bello* , Janan T. Eppig and the MGI Software GroupAbstractBackground: Inferring gene-to-phenotype and gene-to-human disease model relationships from annotated mousephenotypes and disease associations is critical when researching gene function and identifying candidate diseasegenes. Filtering the various kinds of genotypes to determine which phenotypes are caused by a mutation in aparticular gene can be a laborious and time-consuming process.Methods: At Mouse Genome Informatics (MGI, www.informatics.jax.org), we have developed a gene annotationderivation algorithm that computes gene-to-phenotype and gene-to-disease annotations from our existingcorpus of annotations to genotypes. This algorithm differentiates between simple genotypes with causativemutations in a single gene and more complex genotypes where mutations in multiple genes may contributeto the phenotype. As part of the process, alleles functioning as tools (e.g., reporters, recombinases) are filtered out.Results: Using this algorithm derived gene-to-phenotype and gene-to-disease annotations were created for 16,000and 2100 mouse markers, respectively, starting from over 57,900 and 4800 genotypes with at least one phenotype anddisease annotation, respectively.Conclusions: Implementation of this algorithm provides consistent and accurate gene annotations across MGI andprovides a vital time-savings relative to manual annotation by curators.Keywords: Phenotype, Genotype, Disease, Mouse, AnnotationBackgroundGenetic mutations in mouse models have proven avaluable tool in investigating gene function and facili-tating research into human disease. The phenotypes as-sociated with these mutations in mice occur in thecontext of other defined or undefined mutations intheir genome. To determine if a phenotype is caused bya mutation in a specific gene, providing insight intogene function, the impact of each allele in the genotypeneeds to be evaluated. Doing this manually is a labori-ous and time-consuming process. Intensely researchedgenes may have dozens of alleles each with multiplegenotypes. The mouse gene Pax6 (MGI:97490) alonehas 53 mutant alleles present in some 150 mouse geno-types with phenotype annotations in Mouse GenomeInformatics (MGI, as of 12/29/2015). Only a fraction ofthese reported phenotypes are caused solely by the mu-tation(s) in Pax6.MGI (www.informatics.jax.org) provides gold-standardannotations to describe mouse models in the context ofboth the known alleles and strain backgrounds of themice [1]. In MGI, phenotype and disease annotations areascribed to a genetic representation (allele pairs andstrain background) of the mice that displayed the pheno-type. Sophisticated genetic engineering techniques haveallowed for the production of multi-genic models withspatiotemporal control of gene expression and the intro-duction of multi-color reporters. These increasinglycomplex models may include both causative mutationsand non-causative transgenic tools [2]. To relate pheno-type and disease annotations made to a genotype inMGI with the gene, genomic marker, or transgene con-taining the causative mutation, non-causative markers,such as transgenic tools (e.g., recombinases and* Correspondence: susan.bello@jax.orgMouse Genome Informatics, The Jackson Laboratory, Bar Harbor, ME 04609, USA© 2016 Bello and Eppig. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 DOI 10.1186/s13326-016-0054-4reporters), need to be computationally excluded fromconsideration. For example, mice carrying an inducibleknock-in of a mutant form of mouse Kcnj11 in theGt(ROSA)26Sor locus and a transgene expressing crerecombinase in pancreatic cells, Tg(Ins2-cre)23Herr(genotype MGI:4430413), are annotated to the Mam-malian Phenotype ontology (MP) [3] term decreasedinsulin secretion (MP:0003059) and are a model of per-manent neonatal diabetes mellitus (OMIM:606176) [4].The phenotype and disease annotations are correctly asso-ciated with Kcnj11. However, the annotations should notbe linked with the cre recombinase transgene orGt(ROSA)26Sor since neither directly causes the phe-notypes or disease displayed by the mice.MGI is implementing improvements throughout thedatabase to enhance the ability of users to evaluate thefunction of genes. As part of this, phenotype and diseaseassociations at the level of the gene are now being pre-sented (see below) in multiple locations in the MGI web-site. The gene-level associations give users an overviewof the phenotypes and diseases associated with a genethat can be challenging to decipher from detailed modelannotations. For both phenotypes and disease, creating agene-level annotation implies that mutations in this genecause the associated phenotype or disease. Therefore,the gene-level annotations may be useful to identify can-didate genes for specific phenotypes and/or diseases. Tocreate these gene-level associations, we have developedrules to algorithmically identify and computationallyseparate causative mutations from transgenic tools incomplex mouse genotypes.The first and simplest implementation of the rules ex-cluded all complex genotypes and removed recombinaseand wild-type alleles prior to inferring relationships. Theneed to separate causative mutations from transgenetools can best be illustrated by example. The complexgenotype Apoetm1Unc/ Apoetm1Unc Faslgld/Faslgld on an in-bred C57BL/6 strain genetic background (MGI:5514345)is annotated to the human disease Systemic Lupus Ery-thematosus, SLE (OMIM:152700) [5]. Inferring a causalrelationship between Apoe and/or Fasl and SLE may ormay not be correct, since it is unclear whether one orboth genes are responsible for the observed phenotype.For complex genotypes such as this one, the algorithmdoes not derive any gene annotations. Conversely,Smotm1Amc/Smotm2Amc Isl1tm1(cre)Sev/Isl1+ mice on a mixed129 strain genetic background (MGI:3689403) are anno-tated to the phenotype perinatal lethality (MP:0002081)[6]. The Isl1 recombinase allele is present to drive dele-tion of the loxP-flanked Smo allele in the cardiovascularsystem; thus, we do not want to associate the perinatallethality phenotype with Isl1. As we can clearly identifythe non-causative allele and distill this genotype toalleles associated to a single gene, we derive arelationship between the phenotype perinatal lethalityand the gene Smo.Other databases presenting phenotype and disease an-notations for model organisms also have to decide whenan annotation to a model can used to infer informationabout gene function. For example, the Zebrafish ModelOrganism Database (ZFIN, www.zfin.org, [7]) annotatesphenotypes to a fish line that includes the alleles, trans-genes and/or morpholinos used in an experimental co-hort. Each allele and morpholino has an assertedrelationship to a gene. Gene level annotations are theninferred for lines where only 1 asserted gene relationshipexists (Y. Bradford, personal communication). Gene levelannotations are not inferred for fish with more than oneasserted gene relationship or for fish expressing non-reporter transgenes. This is similar to the early stages ofthe MGI algorithm. A key difference between mouseand zebrafish models, for the purpose of inferring geneannotations, is the widespread use of knock-in mutationsin mouse where asserting the gene to allele relationshipis less straightforward.In contrast to the restrictive approach taken by ZFINand MGI, the Monarch Initiative (monarchinitiati-ve.org, [8]), which integrates data from both MGI andZFIN as well as many other sources, infers gene anno-tations for all genes in a model. Thus, in the exampleabove (Apoetm1Unc/ Apoetm1Unc Faslgld/Faslgld) gene an-notations would be inferred for both Apoe and Fasl(M. Brush, personal communication). This approachmaximizes the number of gene-to-phenotype annota-tions but means the user will need to evaluate the re-sults to remove false positive associations.In the current implementation, presented below, thealgorithm we have developed excludes additional trans-genic tools, accounts for the introduction of expressedgenes in alleles, and deals with multi-genic mutations.This approach increases the number of derived gene an-notations, while attempting to reduce both the numberof false positive and false negative annotations. Whilethe precise implementation would not be of use to otherdatabases the logic behind the algorithm should betransferable.Gene annotation derivation rulesRefinement of the derivation rules to eliminate add-itional types of transgenic tools has been an iterativeprocess. Various changes to the MGI database schemahave facilitated the identification and removal of manytypes of transgenic tools and non-causative marker asso-ciations. Throughout this process we have worked tominimize the number of false positive associations. Theoverall goal of these rules is to eliminate transgenic toolsalleles and then infer gene, multi-genic marker, or trans-gene relationships from genotypes with only a singleBello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 2 of 11remaining associated locus. Genotypes with multiple as-sociated loci are not used to infer gene relationships, with afew exceptions (see below). Recent re-implementation ofthese rules in a consistent manner across all MGI productshas improved the gene annotation data quality at the dis-play level and allowed us to make this data set available forexport.Details of the annotation derivation rulesIn the application of the derivation rules, genotypes areprocessed in a step-by-step fashion (see Fig. 1). First,the number of genetic loci associated with all alleles inthe genotype is determined (Fig. 1, box 1). Genetic lociinclude: genes within the mutation region, genesexpressed by the allele, transgene markers, and pheno-typic markers. For example, the alleles Apptm1Dbo,Tg(tetO-Notch4*)1Rwng, and Del(7Coro1a-Spn)1Dolm(MGI:2136847, MGI:4431198, MGI:5569506 respectively)are associated with one, two, and forty loci, respectively.The two loci associated with Tg(tetO-Notch4*)1Rwng arethe transgene itself and the expressed mouse gene, Notch4.The forty loci associated with Del(7Coro1a-Spn)1Dolminclude the deletion region itself (recorded in MGI asa single, unique genetic marker) and all thirty nineendogenous mouse genes overlapping the deletion region.Gene-to-phenotype and gene-to-disease annotations canthen be derived for the genes in nearly all genotypes witha single associated genetic locus (see docking sites belowfor the exception).For genotypes including more than one locus, such asthose described above, non-causative alleles are identi-fied and computationally excluded from consideration.Non-causative allele types in the algorithm include:transgenic transactivator alleles, transgenic reporter al-leles, knock-in and transgenic recombinase alleles, andwild-type alleles. Since many knock-in transactivator andreporter alleles may also be knock-out alleles that arecausative for a phenotype, only transgenic alleles of thesetypes are excluded. For recombinase alleles, curation inMGI distinguishes between conditional genotypes,where these alleles function as a recombinase, and non-conditional genotypes, where these alleles may becausative; therefore, both transgenic and knock-in re-combinase alleles may be eliminated when the genotypeis conditional. When the genotype is not conditional,recombinase alleles are retained. For a recombinase ortransactivator allele to be excluded, it must express onlya single gene. In cases where another gene is expressed,the allele is retained. For example the recombinase al-lele Tg(Tyr-cre/ERT2)1Lru (MGI:3617509) is excludedat this stage, so no derived annotation to the transgeneis computed as a result of this allele. But the alleleTg(Tyr-cre/ERT,-Hras1*,-Trap1a)10BJvde(MGI:4354013) is retained, as it expresses both Hras1and Trap1a in addition to cre. Additional rules de-scribed below address whether and how to derive anno-tations to those genes. Motifs (ERT2, ERT) designed toalter the expression of cre are not curated as expressedgenes and are therefore ignored by the algorithm.After excluding non-causative alleles, the number ofremaining loci is determined for each genotype. Gene-to-phenotype and gene-to-disease annotations are thenderived for genes and genomic markers in genotypeswith a single remaining locus. For genotypes with morethan one remaining locus, further processing is done toidentify additional cases where gene annotations can bederived. If the genotype is associated with a single multi-genic marker (e.g., Del(7Coro1a-Spn)1Dolm) and one ormore affected genes located in the region, then annota-tions are derived for the multi-genic marker and not forthe individual endogenous genes in the region (Fig. 1,box 4). Genotypes associated with more than one multi-genic mutation or with a multi-genic marker and anymarkers outside the mutation region are excluded andFig. 1 Flow chart for the application of gene annotation derivationrules. One gene*, annotations are derived only for certain cases ofgenotypes containing a single gene. See text for additional details.Transgene+, gene annotations are made to the transgene and anendogenous mouse geneBello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 3 of 11annotations are not derived for any of the genes or gen-omic markers involved.The number of inserted expressed genes is then con-sidered. Inserted expressed genes are genes that havebeen introduced into the mouse genome and the geneproduct is expressed in one or more tissues of themouse. Genotypes with multiple associated markers andno inserted expressed genes are eliminated. Genotypesassociated with multiple inserted expressed genes are as-sociated to the transgenic locus only, if there is a singletransgene associated with the genotype and no add-itional endogenous genes (Fig. 1, box 6). In this case, itis assumed that the transgene is expressing all of theinserted expressed genes and that the transgene as awhole, not the individual expressed genes, is causativefor the phenotypes or diseases annotated to the geno-type. For these genotypes, transgene-to-phenotype andtransgene-to-disease annotations are derived. Derivedannotations are not created for the inserted expressedgenes. Other genotypes having more than one insertedexpressed gene are excluded and no gene or transgeneannotations are derived.Genotypes associated with only a single insertedexpressed gene (Fig. 1, box 7) are divided into twotypes: those expressing a mouse gene and those ex-pressing a non-mouse gene. Genotypes associated withan expressed non-mouse gene are eliminated. No as-sumption is made that the phenotypes or diseases dis-played would also be produced if the orthologousmouse gene had been used instead. Gene-to-phenotypeand gene-to-disease annotations may be derived for atransgene and also an endogenous mouse gene in twocases: 1) if the genotype contains only a single transgenewhich carries a single inserted expressed mouse gene(Fig. 1, box 8); 2) if the transgene, inserted expressedmouse gene, and the single endogenous gene that is thesame as the inserted expressed mouse gene are associatedwith the genotype (Fig. 1, box 9). In both cases annota-tions are derived for both the endogenous mouse geneand the transgene (Fig. 1, transgene + ).Three genes (Gt(ROSA)26Sor, Col1a1, Hprt) are com-monly used, based on examination of alleles in MGI, asdocking sites in mouse to knock-in expressed genes,frequently under the control of a heterologous pro-moter sequence. For example, of the 63 alleles ofCol1a1 in MGI with the attribute inserted expressedsequence, 55 have a construct inserted in the untrans-lated region based on the molecular description in MGI(12/7/15). For genotypes associated with a docking siteand a single expressed mouse gene, gene-to-phenotypeand gene-to-disease annotations are derived for theexpressed gene and not for the docking site. There areno known phenotypes or diseases ascribed to mutationsin Gt(ROSA)26Sor (MGI:104735, [9]). Therefore, noderived annotations are created for Gt(ROSA)26Sor, evenwhen there are no associated expressed genes in MGI.MGI currently only annotates expressed genes with anortholog in mouse; therefore, not all Gt(ROSA)26Soralleles with an inserted expressed gene have an as-sociated expressed gene. For example the alleleGt(ROSA)26Sortm1(gp80,EGFP)Eces (MGI:5004724) expressesa gene from the Kaposi sarcoma herpes virus that doesnot have an ortholog in mouse. The phenotypes displayedby mice carrying this allele are the result of expression ofthe viral gene but as there is no display in MGI for anygene-to-phenotype annotations for a viral gene with nomouse ortholog, no derived annotations are created. In-sertions in Col1a1 (MGI:88467) and Hprt (MGI:96217)are typically made without altering normal endogenousgene function. For Col1a1 and Hprt alleles, annotationsare derived for the inserted expressed gene when one ispresent. If no expressed genes are present thenannotations are derived for the docking site gene itself(Fig. 1, box 10).The final case where gene annotations are derived iswhen the inserted expressed mouse gene is identical tothe endogenous gene (Fig. 1, box 11). No gene annota-tions are created for any remaining genotypes.Gene annotation derivation examplesTo illustrate the function of the derivation algorithm, fourexample genotypes have been overlayed on the flow chart(Fig. 2). For mice hemizygous for Tg(tetO-Notch4*)1Rwngand Tg(Tek-tTA)1Rwng (genotype MGI:5502689, Fig. 2a),the transactivator expressing transgene Tg(Tek-tTA)1Rwngis excluded from consideration. This leaves 2 remaininggenes, Tg(tetO-Notch4*)1Rwng and Notch4. As this leaves asingle transgene marker and a single expressed mousegene, gene level annotions are derived for both thetransgene and the expressed mouse gene. For mice homozy-gous for Prnptm1Cwe and Tg(Prnp*D177N*M128V)A21Rchi(genotype MGI:3836994, Fig. 2b) there are no non-causativealleles to remove. The single transgene in this case expressesthe same mouse gene that is mutated by the allelePrnptm1Cwe leaving the genotype associated with two genes,mouse Prnp and Tg(Prnp*D177N*M128V)A21Rchi. As thisfits the requirements for the transgene exception (Fig. 2, box9) annotations are derived for both the endogenous mousegene and the transgene. For mice heterozygous for thedeletion Del(7Coro1a-Spn)1Dolm and hemizygous forthe reporter transgene Tg(Drd2-EGFP)S118Gsat (genotypeMGI:5571091, Fig. 2c), the reporter transgene is excludedfrom consideration. As the deletion marker is associatedwith the 39 genes in the deletion region, this genotype fallsinto the Phenotypic mutation class for purposes of the algo-rithm. Gene annotations are derived for the deletion markerbut not for the 39 genes in the deletion region (Fig. 2c, box4). Mice heterozygous for Ewsr1tm2(FLI1*)Sblee and hemizygousBello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 4 of 11Fig. 2 (See legend on next page.)Bello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 5 of 11for Tg(CAG-cre/Esr1*)5Amc (genotype MGI:4429149,Fig. 2d) illustrate a case where gene annotations are notderived. While two non-causative alleles are removed bythe algorithm, the cre transgene and wild-type allele ofEwsr1, after processing is complete there are still twogenes associated with the genotype, Ewsr1 and FLI1. Asthe gene knocked into Ewsr1 is not a mouse gene thisgenotyope is excluded at box 7 in the flow chart. Even ifthe expressed gene had been a mouse gene this genotypewould have been excluded as the expressed gene is notthe same as the mutated endogenous gene.Output of the rulesOnce all genotypes with phenotype or disease annota-tions have been processed by the derivation rules the setof derived gene annotations are used throughout MGI,HMDC and MouseMine. As currently implemented, therules result in derived gene-to-phenotype and gene-to-disease annotations for over 16,000 and 2200 mousemarkers, respectively, starting from over 57,000 and4800 genotypes with at least one phenotype and diseaseannotation, respectively (as of 1/4/2016). Of the over57,000 genotypes processed, almost 40,000 contain onlymutations in a single marker (Table 1). Gene level anno-tations could be derived from these genotypes using thesimplest possible rule (only derive annotations whenthere is one marker associated with the genotype). Useof the derivation algorithm allows a further almost 8000genotypes to be processed and marker level annotationscreated. This represents an almost 14 % increase in thenumber of genotypes contributing phenotype annota-tions at the marker level. Of the approximately 18,000multiple marker genotypes, conditional genotypes andgenotypes involving alleles expressing inserted genes aretwo important subsets. Conditional genotypes areprimarily processed by removal of recombinase alleles.There are currently over 7000 genotypes where a recom-binase allele is removed (Table 2). The ability to includespecial and temporal specific phenotypes in the genelevel annotations enhances the overall picture of genefunction MGI provides to users. There are over 3700 al-leles (knock-in and transgenes) expressing at least oneinserted sequence involved in nearly 4800 genotypescurrently in MGI (as of 12/28/15). Over 2000 of these al-leles express a mouse gene and may therefore potentiallycontribute to gene level annotations. Incorporation of theseoverexpression and misexpression induced phenotypesimproves both the overall picture of gene function and therelation of mouse models of human disease to genes.There is a potential for the creation of false positiveand false negative annotations by the derivation algo-rithm. One possible source of false positive annotationsis the use of expressed gene relationships to identifywhen an allele is expressing a transcript that may alterthe phenotype. For example, the gene Col1a1 has 64 tar-geted alleles with the attribute inserted expressed se-quence of these 58 have an association to an expressedgene. Of the remaining 6 alleles, 5 are alleles where aninterfering RNA (RNAi) has been inserted into the gene.Determining how to represent the relationship betweenan RNAi expressing allele and the gene targeted by theRNAi is one of MGIs future projects. During the devel-opment of the algorithm the use of the insertedexpressed sequence attribute was still in developmentso the presence of an association to an expressed genewas used. We are reviewing the possibility of changingthe algorithm to use the presence of the insertedexpressed attribute instead of the presence of anexpressed gene association, as this would improve ourhandling of these cases.(See figure on previous page.)Fig. 2 Overlay of specific genotype examples on the flow chart of the gene annotation derivation rules. a Processing of a genotype that resultsin annotations to a transgene and endogenous mouse gene. b Processing of a genotype that fits the transgene exception rule, where thetransgene expresses a mouse gene and the same endogenous mouse gene is mutated in the mice. c Processing of a genotype with a reportertransgene and phenotypic mutation affecting multiple genes. d Processing of a conditional genotype where no gene annotations can be derivedTable 1 Number of genotype and gene annotations processedby the derivation algorithmGenotypes with MP and/or OMIM annotationsNumber of genotypes(percent of total, 1/4/2016)Number ofgenesTotal 57,920With Derived GeneAnnotations47,869 (82.6 %) 16,044One Marker Genotypes(Fig. 1, box 1)39,873 (68.9 %) 14,074Resolved Multiple MarkerGenotypes7996 (13.8 %) 3870 (1970novel markers)Table 2 Breakdown of resolved multiple marker genotypes.These numbers only include genotypes with MP or OMIMannotations that have more than 1 markerNon-causative alleles ingenotypesNumber of genotypesprocessed (as of 1/4/2016)NumberAllelesRecombinase allelesa 7,015 936Reporter transgenes 256 157Transactivator transgenes 282 84Wild-type alleles 5,371 1,577aOnly counting recombinase alleles in conditional genotypes which haveMP/OMIM annotationsBello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 6 of 11One possible source of false negative annotations isthe limitation of docking site alleles to only Col1a1,Hprt and Gt(ROSA)26Sor. For example, annotationsfrom the genotype MGI:5544092 could be associatedwith the mouse gene Edn2 if the marker for the inter-genic insertion site in the allele Igs1tm11(CAG-Bgeo,-Edn2)Natwas excluded from consideration. Instead of expandingthe list of markers used for docking sites, we are explor-ing implementation of a Docking Site attribute thatcould be applied to specific alleles. This would avoid theneed to modify the algorithm when new docking sitesare encountered but would require back annotation ofexisting alleles. Another source of false negative annota-tions is the use of reporter genes that are a mouse geneor with an ortholog in mouse. For example, there are 63knock-in alleles that use the mouse gene Tyr as a coatcolor reporter. Other than the pigmentation phenotype,phenotypes in these mice are the result of the mutatedendogenous locus and not due to the expression of Tyr.However, using the current algorithm gene annotationsare not derived for any of the annotated phenotypes.Correcting these would require modifying the algorithmto both ignore Tyr and teasing apart the phenotypes dueto the reporter from those due to the mutated endogen-ous locus.Impact of MGI improvementsThe development of these rules has relied heavily on theimplementation of other database improvements inMGI. For example, the introduction of allele attributesallowed a distinction to be made between reporter trans-genes that express only a reporter and transgenes thatexpress a reporter and some other gene. The attributeswere introduced as part of a restructuring of allele typesinto generation method and attributes. Attributes in-clude both changes to the endogenous gene function(null/knockout, hypomorph) and characteristics of theinserted sequence (reporter, recombinase). Some attri-butes may apply to either the endogenous gene or theinserted sequence (hypomorph, modified isoform). Anallele may have zero to many attributes but only onegeneration method. Certain attributes were then incor-porated into the rules. These attributes include: reporter,recombinase, transactivator, and inserted expressed se-quence. For example, exclusion of a reporter transgenerequires the allele to have the generation method trans-genic and the attribute reporter but not the attributeinserted expressed sequence. Therefore, the reportertransgene Tg(Cspg4-DsRed.T1)1Akik (MGI:3796063)that has only the attribute reporter is excluded as anon-causative allele. However, the reporter transgeneTg(CAG-Bmpr1a*,-lacZ)1Nobs (MGI:5473821) has mul-tiple attributes including reporter and inserted expressedsequence and is retained.The recent introduction of formalized data associa-tions between transgenic and knock-in alleles and thegenes expressed by these alleles has also been incorpo-rated into the rules. MGI now annotates alleles express-ing either a mouse gene or gene with a mouse orthologto the gene being expressed. Alleles expressing insertedgenes are then displayed on both the detail page for theendogenous locus where the insertion occurred and onthe detail page for the mouse gene or mouse ortholog ofthe inserted gene being expressed. The rules make useof these associations to avoid assigning phenotypes tothe endogenous gene in cases where an inserted expressedgene may be causative. They also allow annotations forphenotypes and diseases caused by transgenes expressinga mouse gene to be derived for the expressed mousegene. For example, phenotypes for the knock-in alleleCtnnb1tm1(Nfkbia)Rsu (MGI:3039783) may be the result ofloss of expression of Ctnnb1 or the expression of Nfkbiaand therefore no derived annotations are created. How-ever, phenotype and disease annotations for the transgeneTg(Prnp*D177N*M128V)A21Rchi (MGI:3836986) are as-sumed to be the result of the expression of the mousePrnp gene and derived annotations may be created forboth the transgene and the expressed mouse gene.Use of the derived annotations in MGIImplementation of the annotation derivation rules de-scribed here has improved both searching and display ofgene-to-phenotype and gene-to-disease annotations inMGI. Gene level annotations are used on multiple dis-plays and by multiple search tools in MGI. These dis-plays and tools provide users with different ways toaccess, group, and filter the data. Regardless of how theuser accesses the data, consistent results sets are nowreturned when searching for genes by a phenotype ordisease.One way a user may access the derived annotationsfor a gene or set of genes is using the Human-Mouse:Disease Connection (HMDC, www.diseasemodels.org,Fig. 3). In the HMDC, searches for mouse data are re-stricted to only the derived gene-to-phenotype andgene-to-disease annotations. In the results, users mayalso access the set of genotype annotations used to gen-erate the gene annotations, but multi-genic genotypesare excluded from the display. In MGI, the display of amouse gene on a disease detail page is based both onthe derived gene-to-disease annotations and on orthol-ogy relationships to known human disease genes. Agene that has both a derived gene-to-disease annotationand is orthologous to a known human disease gene is dis-played in the human and mouse section of the page.Those without an orthology relationship but with a de-rived annotation are shown in the mouse only section. Asimilar division is made on the all models page for aBello and Eppig Journal of Biomedical Semantics  (2016) 7:14 Page 7 of 11disease, with multi-genic models that have neither geneorthologs nor derived annotations shown in the additionalcomplex models section. The derived gene annotationsare also incorporated into the updated design of the MGIgene detail page. With this modification, users see a sum-mary graphic of the types of phenotypes caused by mu-tations in the gene (Fig. 4). On both the gene detailpage and in the HMDC, gene level annotations areshown at the MP system level. Users may click throughto see the detailed MP terms and associated allele pairs.This avoids the problem of displaying conflicting phe-notypes (i.e., increased vs decreased body weight) at thegene level. From both locations users can access detailsHuang et al. Journal of Biomedical Semantics  (2016) 7:24 DOI 10.1186/s13326-016-0066-0RESEARCH Open AccessThe Non-Coding RNA Ontology (NCRO): acomprehensive resource for the unification ofnon-coding RNA biologyJingshan Huang1*, Karen Eilbeck2, Barry Smith3, Judith A. Blake4, Dejing Dou5, Weili Huang6,Darren A. Natale7, Alan Ruttenberg8, Jun Huan9, Michael T. Zimmermann10, Guoqian Jiang10, Yu Lin11,Bin Wu12, Harrison J. Strachan1, Yongqun He13, Shaojie Zhang14, Xiaowei Wang15, Zixing Liu16,Glen M. Borchert17 and Ming Tan16AbstractIn recent years, sequencing technologies have enabled the identification of a wide range of non-coding RNAs(ncRNAs). Unfortunately, annotation and integration of ncRNA data has lagged behind their identification. Given thelarge quantity of information being obtained in this area, there emerges an urgent need to integrate what is beingdiscovered by a broad range of relevant communities. To this end, the Non-Coding RNA Ontology (NCRO) is beingdeveloped to provide a systematically structured and precisely defined controlled vocabulary for the domain ofncRNAs, thereby facilitating the discovery, curation, analysis, exchange, and reasoning of data about structures ofncRNAs, their molecular and cellular functions, and their impacts upon phenotypes. The goal of NCRO is to serve as acommon resource for annotations of diverse research in a way that will significantly enhance integrative andcomparative analysis of the myriad resources currently housed in disparate sources. It is our belief that the NCROontology can perform an important role in the comprehensive unification of ncRNA biology and, indeed, fill a criticalgap in both the Open Biological and Biomedical Ontologies (OBO) Library and the National Center for BiomedicalOntology (NCBO) BioPortal. Our initial focus is on the ontological representation of small regulatory ncRNAs, which wesee as the first step in providing a resource for the annotation of data about all forms of ncRNAs. The NCRO ontologyis free and open to all users, accessible at: http://purl.obolibrary.org/obo/ncro.owl.Keywords: Non-coding RNA, Biomedical ontology, Domain ontology, Reference ontology, Ontology development,Data annotationIntroductionIt is known that non-coding RNAs (ncRNAs), a specialclass of functional RNA molecules, will not be translatedinto proteins. The chemical identity and first guesses asto the role of RNA were discussed by Casperson andSchultz back in 1939, and the first RNA structure wasreported by Alexander Rich in 1956 [1]. Since then, manytypes of ncRNAs have been identified, including the nowwell-known transfer RNAs (tRNAs) and ribosomal RNAs*Correspondence: huang@southalabama.edu1School of Computing, University of South Alabama, Mobile, Alabama36688-0002, USAFull list of author information is available at the end of the article(rRNAs), in addition to the more recently discovered longnon-coding RNAs (lncRNAs), microRNAs (miRNAs), andso forth. Many ncRNAs perform important roles in therealization of a wide range of molecular functions as wellas in affecting many different biological and patholog-ical processes. As such, interest in ncRNA biology hasgrown throughout biomedicine, biomedical informatics,and clinical sciences. In addition, the fertile area of ncRNAresearch has been significantly enhanced in recent yearsby new sequencing technologies that have generated con-tinuously increasing quantities of available data. However,annotation and integration of data about ncRNAs, the© 2016 Huang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 2 of 12functions regulated by ncRNAs for example, has laggedbehind their identification, resulting in an urgent need foreffective methodologies to bring together discoveries con-tinuously deriving from different segments of the ncRNAresearch community.Emerging semantic technologies provide computationalmethodologies that promote more precise communica-tion among scientists, enable more effective informationretrieval and integration across diverse resources, andextend the power of computational technologies to per-form data exploration, inference, and mining [27]. Inparticular, the sorts of reasoning (inference) enabled bysemantic technologies are not available where we areconfined to traditional relational database systems or text-based search and query. By placing more emphasis on thesemantics (i.e., the intended meaning) of data, semantictechnologies and domain ontologies enable us to estab-lish more meaningful connections among original data,thereby helping to bridge gaps in our knowledge. More-over, semantic data connections are established in a highlyflexible manner that allows these connections to be muchmore easily extended  for example when new sorts ofentities are discovered  than is possible using moretraditional approaches.Among all successful efforts in applying semantic tech-nologies in the biomedical domain, the Open Biologicaland Biomedical Ontologies (OBO) Library [8] is of spe-cial importance in that it has served as an umbrellafor different ontologies shared across various biologi-cal, biomedical, and clinical domains. However, there hasuntil now existed in the OBO Library no comprehensiveontologies specifically designed for the ncRNA domain,although portions of the domain are catalogued in severalorthogonal ontologies. The National Center for Biomedi-cal Ontology (NCBO) BioPortal [9], a repository of biolog-ical and biomedical ontologies (short for bio-ontologies),is another effort in some ways parallel to the OBO Librarybut with a broader scope and lower hurdles for admis-sion. However the BioPortal, too, contains, no compre-hensive ncRNA ontologies. These observations indicatethat there is an important gap that needs to be filled hence the Non-Coding RNA Ontology (NCRO) project.As the first comprehensive, domain-specific ontology inthe ncRNA field, the NCRO ontology aims to supplya systematically structured, precisely defined controlledvocabulary for the ncRNA domain, consisting of a setof common, standardized terms and relations that willfacilitate the discovery, curation, analysis, exchange, andreasoning of data about the structures, functions, andmolecular, cellular, organismal, therapeutic, or biotechno-logical uses of ncRNAs. The NCRO ontology can serveas a resource for annotating and integrating ncRNA dataproduced by diverse communities, thereby significantlyenhancing integrative and comparative analysis of themyriad resources currently housed in disparate sources.We believe that the NCRO will help to address a vitalneed for the comprehensive unification of ncRNA biol-ogy. We aim to integrate genomic and sequence-basedannotation with gene expression regulation, secondaryand 3D structure information, protein interactions, andtheir inter-relationships. Our initial focus is on the onto-logical representation of small regulatory ncRNAs, whichwe see as the first step in providing a standardizedresource for (1) annotating data about all forms of ncR-NAs and (2) facilitating knowledge capture in the ncRNAdomain.The rest of this paper is organized as follows.Section Related work summarizes state-of-the-artresearch in ncRNAs and bio-ontologies; SectionOntology scope gives an overview of the scope coveredby the NCRO ontology; Section Ontology developmentintroduces NCRO development principles and proce-dure; Section NCRO terms, relations, and reasoningdescribes NCRO terms and relations, as well as ontologyreasoning; Section Examples in NCRO annotationspresents two examples to demonstrate how NCROannotations and ontology reasoning can be performed tofacilitate knowledge capture; finally, Section Conclusionsconcludes with future research directions.Related workRelated work in ncRNA researchPrior research, [1012] for example, has uncoverednumerous ncRNA genes, and recent advances in next gen-eration sequencing technology have resulted in an evengreater number and faster pace of discovery of ncRNAgenes. In fact, Nature has a whole site dedicated to keyapes in this area [13]. Given the relatively large propor-tion of the genome dedicated to ncRNA genes, significantpotential exists to explore ncRNAs that may have diversebiological roles.Abnormal expression of some ncRNAs is involvedin human disease. For example, alterations of gene-regulatory ncRNA expression are involved in thedevelopment, progression, and metastases of human can-cer [14]. When differentially expressed gene-regulatoryncRNAs play roles in altering target gene expression,further phenotypic effects can be realized. Differentialexpression of such ncRNAs in malignant versus nor-mal tissue can be exploited as a biomarker used fordiagnosis, prediction of patient outcome, or monitor-ing the effectiveness of cancer therapeutics. Therefore,these gene-regulatory ncRNAs are potential therapeu-tic targets for cancer therapy. In recent years seriousattempts have been made to effectively deliver ncRNAinto tumors in animal models. Some of the attempts havealready shown promising therapeutic efficacy [1517].Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 3 of 12In RNA interference therapy and drug development,a first-in-human trial has been conducted in cancerpatients who were administered with lipid nanoparticles(LNP) formulated siRNA targeting VEGF andKSP [18].Aberrant expression of ncRNAs has been associatedwith not only cancers but also numerous other dis-eases, including autism, hearing loss, Alzheimers disease,Prader-Willi Syndrome, diabetes, and psoriasis [1923].Tissue-specific miRNAs have been shown to be involvedin cardiovascular, muscular, and neurodegenerative dis-eases, and pharmaceutical companies are developing newtherapeutic molecules that alter the function or expres-sion of specific miRNAs for treating these and otherhuman diseases [24].Related work in bio-ontologiesThere are several pre-existing bio-ontologies that are rel-evant to the development of an ontology in the domain offunctional non-coding RNA. The RNAOntology (RNAO)[25] is a reference ontology created to catalogue themolecular entities composing primary, secondary, andtertiary components of RNA. The goal of the RNAOproject is to enable integration and analysis of diverseRNA datasets. The Gene Ontology (GO) [26] is by far themost successful and widely used bio-ontology, consistingof three independent sub-ontologies: biological processes,molecular functions, and cellular components. The GOhas been utilized to annotate both protein and RNAgene products across multiple organisms. The SequenceOntology (SO) [27] is an ontology that is designed to cap-ture genomic features and the relationships that obtainbetween them. This ontology contains the features nec-essary to annotate a genome sequence with structuralfeatures such as gene models and also the terms nec-essary for the annotation of the location and extent ofgenomic variants. The PRotein Ontology (PRO) [28] hasbeen developed with a particular focus on human pro-teins and disease-related variants thereof, providing anontological representation of proteins. As proteins areoften the functional entities in the processes impactedby the regulatory effect of ncRNAs, they are an impor-tant factor in the understanding of ncRNA. The Ontol-ogy for MIcroRNA Target (OMIT) [2931] is a miRNAdomain ontology that is being developed as part of theOmniSearch project. The purpose is to establish standardmetadata in miRNA domain for more effective identifica-tion of the roles of miRNAs in various human diseases.The ontology of Chemical Entities of Biological Interest(ChEBI) [32] provides the terminology and relationshipsto describe small molecules.There are also other bio-ontologies that are in use ina wider context that are also important for the descrip-tion of clinical impact of ncRNA. SNOMED CT [33] isa comprehensive, clinically oriented medical terminologysystem, and also a reference standard in the UnitedStates Meaningful Use program that promotes the useof certified electronic health record (EHR) technol-ogy to improve quality, safety, and efficiency, as wellas to reduce health disparities [34]. SNOMED CT isowned andmaintained by the International Health Termi-nology Standard Development Organization (IHTSDO).Anatomy description has been unified over multiplespecies with the Uberon anatomical Ontology [35]. Thisontology relates taxon-specific anatomies and is fullyintegrated with other bio-ontologies such as the GO.The Human Disease Ontology (DOID) [36] encapsu-lates the terminology of diseases and provides equivalentmappings to many related terminologies. The NCI The-saurus (NCIt) [37] is a reference biomedical ontologypublished by the National Cancer Institute (NCI) withterminology that includes clinical care, translational andbasic research, and public information and administrativeactivities.Additionally, there are ontologies that address thedomain of data collection and are pertinent to the under-standing of ncRNA. An ontology that covers the domainof translational research is the Ontology of BiomedicalInvestigations (OBI) [38], describing the foundational ter-minology needed to define experimental processes andinvestigation. Moreover, the Information Artifact Ontol-ogy (IAO) [39] arose as a branch of OBI, to define thefoundational entities of scientific information in the digi-tal domain.Note that all bio-ontologies described in this sectionexcept for SNOMED CT are included in both the OBOLibrary and NCBO BioPortal. SNOMED CT is includedonly in the BioPortal.Ontology scopeThe NCRO ontology will represent:1. All known subtypes of ncRNA molecules includingthose created in living organisms as well as thoseengineered or adapted for some purposes (aptamersfor example [40])  this aspect will utilize high-levelterms defined in both the SO and ChEBI, with morespecific terms defined in the NCRO;2. The structure involved in each ncRNA type,including sequence and conformation  this aspectwill utilize the RNAO;3. The functions, dispositions, and roles of ncRNAs, aswell as the processes in which these are realized1 this aspect will utilize, mostly, the GO, with gapsspecific to ncRNAs filled by the NCRO or otherontologies;4. Different clinical phenotypes associated withexpression of normal and/or abnormal ncRNAs Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 4 of 12this aspect will utilize the SNOMEDCT, NCIt, andHuman Disease Ontology (DOID); and finally,5. Various relations that are unique to ncRNAs andtheir different components.The initial focus of our work in building the ontol-ogy is on small regulatory ncRNAs. Nevertheless, wehave designed an overarching framework of high-levelterms for other ncRNAs, such as: circular RNA (circRNA),lncRNA, rRNA, small interfering RNA (siRNA), smallnuclear RNA (snRNA), and tRNA. These high-level terms,all of which are direct child terms of the term ncRNA,serve as placeholders: a more detailed hierarchy under-neath each term, along with relevant relations, will bedeveloped at a later project stage.Ontology developmentDevelopment principlesIn the development pipeline for the NCRO ontology,we have observed a set of practices proposed by theOBO Foundry Initiative [41, 42]. Above all, the ontol-ogy should be: freely available; expressed in a standardlanguage; documented for successive versions; orthogonalto existing ontologies; including natural language specifi-cations; developed collaboratively; and used by multipleresearchers.Compliance with established upper-level ontologiesAll NCRO terms descend from terms defined in the BasicFormal Ontology (BFO) v2.0 [43]. The BFO is a small,upper-level ontology that is designed for use in sup-porting information retrieval, analysis, and integration inscientific and other domains. Because the BFO is a well-established upper ontology adopted by all OBO ontolo-gies, our strategy to make the NCRO a BFO-compliantontology will set the stage for interoperability betweenthe NCRO ontology and other currently existing OBOontologies.As for relations, besides those defined in the NCRO,we have also used a set of well-defined relations in theRelation Ontology (RO) [44, 45], such as: part of, par-ticipates in, and precedes, all of which relate differenttypes defined in the BFO. Greater details of various rela-tions can be found in Section NCRO terms and relationsand Table 3.Strategy for orthogonalityOut of the set of OBO Foundry principles, orthogo-nality is of special importance in defining the noveltyof the NCRO ontology. Our strategy to abide by thisprinciple is that we have imported and reused extantterms wherever possible, focusing especially on termsfrom OBO ontologies, SO, GO, PRO, and ChEBI forexample. Such terms have been imported with theiroriginal identifier information using internationalizedresource identifiers (IRIs)/uniform resource identifiers(URIs). This strategy helps us to achieve the maximumpossible orthogonality. Table 1 demonstrates a subset ofimported terms. More details can be found in SectionNCRO terms, relations, and reasoning, where per-centages of imported terms from various existing bio-ontologies are calculated.The NCRO team and domain expertiseThe NCRO team members come from a wide variety ofcommunities, covering computer science, ontology engi-neering, wet-lab biological research, biomedical informat-ics, and clinical sciences. The wide scope of participantswill provide (1) the necessary expertise in ontology devel-opment and ontology-based reasoning and (2) the ncRNAdomain knowledge including expertise in ncRNA-relevantphenotype. It will also help to ensure (3) a diversity ofcommunities eager to adopt the NCRO ontology for usein representing and annotating ncRNA data.Dynamic ontology construction procedureThe NCRO development is from the top down (start-ing with more general terms), progressively utilizingthe ncRNA domain knowledge provided by the cellu-lar biologists and clinical investigators in the projectTable 1 A subset of terms imported into the NCRO ontologyImported term Source Ontology Original IDmiRNA Sequence Ontology SO:0000276ncRNA Sequence Ontology SO:0000655small_regulatory_ncRNA Sequence Ontology SO:0000370gene Sequence Ontology SO:0000704promoter Sequence Ontology SO:0000167binding Gene Ontology GO:0005488transcription, Gene Ontology GO:0006351DNA-templatedtranslation Gene Ontology GO:0006412metabolic_process Gene Ontology GO:0008152protein PRotein Ontology PR:000000001organism Ontology for OBI:0100026Biomedical Investigationscell Gene Ontology GO:0005623cell line Cell Line Ontology CLO:0000031molecular entity Chemical Entities of CHEBI:23367Biological Interest Ontologyorgan Uber Anatomy Ontology UBERON:0000062tissue Uber Anatomy Ontology UBERON:0000479disease Human Disease Ontology DOID:4Huang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 5 of 12team. Lower levels of the ontology were then furtherdeveloped on the basis of a thorough analysis of repre-sentative ncRNA-related databases (Table 2). Moreover,an iterative procedure, including a series of interviews,exchanges of documents, refinements, and related doc-umentations, is being followed to make the NCRO adynamic ontology. In addition to a dedicated projectwebsite [46], we have utilized GitHub [47] to furtherassist the management and version control of the ontol-ogy during both design and implementation, includingan established issue tracker [48] to facilitate discus-sion among the members of an open group of investi-gators, so that OBO Foundry principles can be betterfollowed.Naming conventionsEach NCRO term has a unique identifier consistingof a prefix and seven digit numerical string, as in:NCRO_0000001. On the other hand, each NCRO term isalso assigned a human-readable label. We have followeda set of OBO Foundry naming conventions [49] to designsuch labels. Specifically: Labels are written in lower cases except for commonlyaccepted acronyms such as RNA and ncRNA. Hypens are kept as is if they are commonly used in, oreasily understood by, the ncRNA community, as in:hsa-miR-125b.For greater readability, we italicize all relations through-out this paper, whether they are defined in the NCRO orimported from the RO and BFO.Ontology languages and development toolsWe have chosen both the Web Ontology Language(OWL) [50] and OBO formats to describe the ontol-ogy: both are widely accepted in OBO Foundry com-munity and the former is recommended by the WorldWide Web Consortium (W3C). A first version of theontology was authored in OBO-Edit [51] and trans-lated to OWL by the ROBOT tool [52]; then theOWL version has been subsequently edited. Moving for-ward, our focus will be placed on editing and releas-ing the OWL version to take advantage of OWL-specific features such as availability of ontology reasonersand triple stores, as well as enhanced annotationexpressivity.NCRO terms, relations, and reasoningNCRO terms and relationsThe current version NCRO (http://purl.obolibrary.org/obo/ncro.owl) is our first production release. Thereare a total of 3,078 terms and 27 relations (besidesa total of 5,394 is_a relations). Terms break down asfollows: 82.68% were defined in the NCRO ontologyitself, and the rest were imported from extant ontolo-gies: BFO (1.14%), GO (8.67%), SO (6.50%), PRO(0.10%), CHEBI (0.29%), OBI (0.13%), IAO (0.06%),DOID (0.13%), CLO (0.06%), and UBERON (0.16%).As for relations, many (55.56%) were imported fromthe RO, and the rest (51.03%) were defined in theNCRO.Orthogonality among different ontologies has beenwidely accepted in the bio-ontology community. Toachieve better orthogonality, it is a common practice toreuse contents defined in relevant, existing ontologies.This is our motivation to import terms and relationsfrom extant ontologies, as demonstrated above. On theother hand, it is not trivial to obtain 100% orthogo-nality, because ontologies are continuously being devel-oped for good reasons within specific domains and bydifferent groups. As a result, given the holistic natureof biology, along with the fact that different applica-tions most likely have adopted different developmentmethodologies and have focused on various emphases,Table 2 A list of ncRNA-related databasesDatabase name Brief introduction Web linkEnsembl ncRNA A database of ncRNA annotations. http://www.ensembl.org/info/genome/genebuild/ncrna.htmlGENCODE A database for annotation of gene features. http://www.gencodegenes.orglncRNAdb A reference database for functional lncRNAs. http://www.lncrnadb.orglncRNAtor A Web portal encompassing lncRNA data. http://lncrnator.ewha.ac.krmiRBase A database of miRNA sequences and annotation. http://www.mirbase.org/NDB A database of experimentally determined nucleic acids. http://ndbserver.rutgers.eduNONCODE A database of ncRNAs except for tRNAs and rRNAs. http://www.noncode.org/NRED An ncRNA expression database. http://nred.matticklab.com/cgi-bin/ncrnadb.plRfam A database of a collection of RNA families. http://rfam.xfam.orgRMDB Chemical Mapping Data of RNA Sequences. https://rmdb.stanford.eduHuang et al. Journal of Biomedical Semantics  (2016) 7:24 Page 6 of 12there will inevitably be some overlaps among ontolo-gies regarding their covered terms and/or relations. Forexample, the term analgesic_treatment defined in theNCRO2 is similar with the term analgesic treatmentdefined in the Malaria Ontology3. Such overlaps mayhave negative impacts on logical inferencing if ontol-ogy reasoning is performed across relevant ontologies.Whereas it is not realistic, if not impossible at all, toobtain pure (i.e., 100%) orthogonality, one effective wayRead et al. Journal of Biomedical Semantics  (2016) 7:30 DOI 10.1186/s13326-016-0071-3RESEARCH Open AccessThe BioHub Knowledge Base: Ontology andRepository for Sustainable BiosourcingWarren J. Read2, George Demetriou1, Goran Nenadic3, Noel Ruddock2, Robert Stevens1* and Jerry Winter2AbstractBackground: The motivation for the BioHub project is to create an Integrated Knowledge Management System(IKMS) that will enable chemists to source ingredients from bio-renewables, rather than from non-sustainable sourcessuch as fossil oil and its derivatives.Method: The BioHubKB is the data repository of the IKMS; it employs Semantic Web technologies, especially OWL, tohost data about chemical transformations, bio-renewable feedstocks, co-product streams and their chemicalcomponents. Access to this knowledge base is provided to other modules within the IKMS through a set of RESTfulweb services, driven by SPARQL queries to a Sesame back-end. The BioHubKB re-uses several bio-ontologies andbespoke extensions, primarily for chemical feedstocks and products, to form its knowledge organisation schema.Results: Parts of plants form feedstocks, while various processes generate co-product streams that contain certainchemicals. Both chemicals and transformations are associated with certain qualities, which the BioHubKB alsoattempts to capture. Of immediate commercial and industrial importance is to estimate the cost of particular sets ofchemical transformations (leading to candidate surfactants) performed in sequence, and these costs too are captured.Data are sourced from companies internal knowledge and document stores, and from the publicly availableliterature. Both text analytics and manual curation play their part in populating the ontology. We describe theprototype IKMS, the BioHubKB and the services that it supports for the IKMS.Availability: The BioHubKB can be found via http://biohub.cs.manchester.ac.uk/ontology/biohub-kb.owl.Keywords: Ontology, Repository, Environmental sustainability, ChemistryBackgroundThe aim of the BioHub project is to develop an Inte-grated Knowledge Management System (IKMS) that willenable chemists to source ingredients for chemical engi-neering processes from biorenewables rather than sourc-ing from non-renewable fossil feedstocks. An importantcomponent of the IKMS is the BioHub Knowledge Base(BioHubKB) which is an RDF store that uses an ontologywritten in the Web Ontology Language (OWL) [1] as aschema to organise knowledge about biorenewables; theircomponent chemicals will be used by the IKMS to seedthe exploration of possible chemical ingredients throughapplication of cheminformatics algorithms [2].*Correspondence: Robert.Stevens@manchester.ac.uk1School of Computer Science, University of Manchester, Oxford Road,M13 9PL Manchester, UKFull list of author information is available at the end of the articleOne class of compounds of particular interest thatdrives the BioHub project are surfactants, which formthe principal active ingredients in many personal careand household cleaning products. Thus the impetus ofthe BioHub project is to build an informatics infrastruc-ture to support the development of surfactants and otherchemicals, from sustainable agricultural feedstocks andco-product streams.The use of such agricultural streams as chemical feed-stocks is intended to obviate the need for sourcingchemicals from non-renewable fossil feedstocks, and toavoid the concomitant environmental costs associatedwith their extraction, refinement and use. The BioHubproject seeks to facilitate the move from fossil fuel tobiorenewable feedstocks. The process of exploring thesourcing of ingredients from biorenewables among theproject partners is currently ad hoc, relying on read-ing public literature and proprietory documentation on© 2016 Read et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 InternationalLicense (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in anymedium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commonslicense, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 2 of 7chemical analyses, which are sometimes decades old.Gathering such knowledge into a central resource shouldfacilitate the sourcing of novel ingredients. The projectis a collaboration between Unilever and several othercommercial and academic partner organisations. Thecommercial partners are motivated both to move to bio-renewables and to exploit more effectively their ownmaterials streams.The first use case in developing this prototype BioHubis to take co-product streams from the processing of sugarbeet and identify the ingredients and transformations thatcan be chained together to generate good candidate sur-factants. To make such a scenario work, the IKMS needsto enable chemists to describe the properties of a classof molecules they wish to derive from chemicals in bio-renewables. The IKMS will calculate a model of this classof chemicals by enumerating and selecting the possibilitieswith starting point chemicals [3] from knowledge in theBioHubKB about feedstocks and co-product streams andtheir component chemicals together with the chemicaltransformations in which they may participate.The IKMS process takes chemicals, their properties andthe transformations in which they may be involved, andsupplies those data to a chemical enumeration routine.The BioHubs hosted, web-based UI proposes sequencesof candidate processes leading to candidate molecules,with both cost and chemical property prediction. In thispaper we describe the BioHubs knowledgebase, the Bio-HubKB, and its role in the IKMS.The IKMS allows a chemist to specify the available co-product streams, the allowable transformations, and thespecific desired chemical properties, before the chemicalenumeration pipeline is run. A chemist might, for exam-ple, choose wool wax and rape oil as allowable sourcestreams; s/he might go on to specify ozonolysis and esterhydrolysis as allowable transformations, with surfactancyas the desired property, e.g. a range of allowable surfac-tancy values as measured by critical micelle concentration(CMC). The input for the enumeration pipeline comesfrom the BioHubKB.Figure 1 shows an overview of the IKMS. The compo-nents of the IKMS are:IKMS user interface: A web GUI that gives the chemistthe ability to specify the application-specific require-ment for a set of candidate molecules in suitablequantitative terms. Here the chemist may also selectthe allowable source streams of seed chemicals forthe enumeration pipeline, as well as restricting theset of allowable transformations if necessary.IKMS enumeration pipeline: A set of routines thatruns iteratively, which in each iteration computesthe allowable transformations and products for alltheoretically possible chemical reactions, based ona known set of substrates. These are then scoredaccording to user-defined design criteria, Pareto-ranked and a subset chosen as inputs to the nextiteration. In the first instance, the pipeline usesthe known molecular constituents of co-productstreams to seed the enumeration. Products of thesereactions form the substrates for the next generation,and so on.BioHubKB: An RDF store with a schema formedfrom ontologies describing feedstocks, co-productstreams, chemicals and transformations of thosechemicals. A set of Web services offers methods tothe enumeration pipeline to recover chemicals andthe transformations in which they may participate.Some example competencies [4] in the form of queriesthat the BioHubKB needs to support are:1. Return a list of chemicals originating from a specifiedco-product stream, or set of co-product streams.2. Return a list of chemicals that can undergo a singletransformation, or at least one transformation from asupplied list.3. Return a list of chemicals originating from among asupplied list of 1 or more co-product streams that arecapable of undergoing 1 or more transformationsamong a separate supplied list.4. Return the complete list of available chemicaltransformations.5. Return the complete list of available co-productstreams.6. Return a list of co-product streams that arethemselves a direct or downstream output of aspecified upstream co-product stream (e.g. woolwax).As well as being a repository for biorenewable feedstockdata, the BioHub also contains curated details of indus-trially available chemical transformations. The BioHubapplication will provide chemists with query access topossible chemical products of chains of such transforma-tions, starting from bio-sourced chemicals and thereafterusing successive rounds of products as possible substratesfor subsequent transformations. Queries specify desiredchemical properties (e.g. relating to surfactancy) thatare evaluated by one of several selectable computationalchemical models; the model is also used as a generationalfilter to prevent the combinatorial explosion of chemicalspecies.The BioHubKBThe BioHubKB is an application ontology [5], wherean ontology is created to fit a particular task model; inthis case the production of surfactants, that addresses thecompetencies and scenario outlined above.Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 3 of 7Fig. 1 The BioHub IKMSWhere practicalities of scope, content and cognitivecomplexity permit, we have re-used extant, communitybased ontologies made by the Open Biomedical Ontolo-gies consortium [6]. It re-uses (i) ChEBI [7] to describeits chemicals, and (ii) the Relations Ontology (RO) [8]for many of the relationships as well as some role hier-archy needed by the BioHubO. The Plant Ontology(PO) [9] is used to describe the parts of plants whencevarious feedstocks come. As well as plants, the IKMSwill use animal based feedstocks meaning the BioHubOwill need to be extended to animal species and thegeneric animal anatomy Uberon [10] wil be appropriatelyextended.The design of the BioHub ontology was based ondata sources that were made available to the devel-opers by domain experts. Such sources included (i)corporate reports about chemical experiments and indus-trial processes (British Sugar), and (ii) internal spread-sheets/databases about specific chemicals and theirpotential sources and transformations (Unilever). Thetransformations for extracting streams and chemicalsfrom sugar beet are well documented in the literature.Nevertheless, the developers paid particular attention tointernal data used to describe the derivation of certainchemicals from sugar-beet for the development of theontological structure of the sugar-beet representation. Inaddition, several key points in the ontology were discussedand clarified during consultation meetings with experts.Finally, external ontology resources, such as ChEBI, wereused primarily to populate the BioHub ontology. In pop-ulating the BioHubKB, chemicals in input data needed tobe mapped to chemicals in ChEBI and the representa-tion of chemical transformations; this was accomplishedusing SMILES strings [11] present in ChEBI, plus SMILESand SMIRKS strings [12] from our in-house developedrepresentations.The BioHubKB is authored in the Web Ontology Lan-guage (OWL) and classes etc. are denoted in this paperby the typeface Class name. Figure 2 shows the domaingeneral classes of the BioHubKB, indicating its scope.We have the class Substance that is a superclassfor distinct chemical entities and mixtures, slurries andsoups of chemical entities. We use ChEBI for distinctchemicals, but chemical engineering involves soups andslurries of mixtures of chemicals with various physicalproperties. The latter are covered by a class Melange,that is a superclass for continuous phase mixtures, plussoups/slurries which include components that co-exist indifferent phases. Formulation is a Melange of con-tinuous phase. Stream is a Melange and is a superclassof Feedstock, Product stream and Coproductstream.Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 4 of 7Fig. 2 The domain general classes covering the scope of the BioHub ontologyStream is a material stream that may be a completelyunprocessed agricultural feedstock, or the output of adownstream process or series of processes (such as sepa-ration) applied to such a feedstock or its derivatives.A Feedstock is an unprocessed, raw agriculturalStream such as Sugar beet feedstock (the freshlyharvested plants themselves). Sugar beet, once harvestedand entered into the processing is playing the roleof a feedstock; the Sugar beet feedstock is sourcedfrom Whole sugar beet (within the Plant Ontol-ogy). Feedstock may be a somewhat arbitrary roleassigned to some raw materials. A sugar beet feedstockis itself separated (by cutting) into the sugar beet rootand the sugar beet leaves, a co-product for furtherprocessing.A Coproduct stream is a stream of materialsobtained as a co-product of processing an enterprisesprincipal product(s). For example, Syrup is a Productstream derived from the Sugar beet feedstockand the Coproduct stream that is co-derived isSugar beet pulp; this goes on for further refine-ment and produces (an)other Product stream(s) andCoproduct stream(s) until distinct chemical entitiesare produced. Each stream has its components of interestdescribed (see below) down to chemical constituents andtheir proportions; for instance product stream Syrup hasa large proportion of sucrose. Figure 3 shows how sugarbeet and some of its co-product streams are representedas streams. The labels used for these streams are the onesused by the domain specialists with whom the BioHubOwas created.The BioHubKB classes below show sugar beet as a feed-stock and some derivations into a product such as syrupand co-product streams such as sugar beet pulp.Class: Sugar beet feedstockSubClassOf:Feedstock,has source type some Whole sugar beetClass: Sugar beet leavesSubClassOf:Coproduct stream,derivesFrom some Sugar beet feedstock,derivesFrom only (Sugar beet feedstock)Class: SyrupSubClassOf:Product stream,derivesFrom some Sugar beet feedstock,derivesFrom only (Sugar beet feedstock)Class: Beet pulpSubClassOf:Coproduct stream,derivesFrom some Sugar beet feedstock,derivesFrom only (Sugar beet feedstock)Class: 6-KestotrioseSubClassOf:Chemical entity,derivesFrom some Beet pulpA SourceType is the type of source (typicallyextracted from a type of animal, vegetable, or mineral)Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 5 of 7Fig. 3 The entities and relationships that represent the sugar beet feedstock and some of its co-product streams. The arrows joining the streamsrepresent Transformation processes such as Separation, Hydrolysis and Fermentationwhence a Substance derives. An OrganismalPart isan identifier assigned in the BioHubKB, to link a speciesID, and an anatomical entity ID from Uberon or thePlant Ontology. For instance, Whole sugar beet isan organismal part whence Sugar beet feedstockcomes (see above).Transformation is a chemical Process usedand/or made available industrially. For example, a trans-formation such as Separation will take Beet pulpand separate it into Cellulose, Hemicellulose andPectin (subclasses of Coproduct stream), as wellas Sugar mixtures (a Product stream). The classSeparation can be further specialised to more pre-cise separations, but the current representation is thatrequired by our domain experts and the application needs.Transformation also describes chemical processessuch as Acetylation. Among a transformations anno-tations are SMIRKS strings that capture the reactiontransforms. The SMIRKS annotations are used by theIKMSs enumerator to evaluate which molecules (eitherfrom the source streams or the products from a previousiteration of the enumerator itself ) constitute valid sub-strates for each transformation, based on their SMILESstrings.The class Participant expresses a ternary relationamong a Substance, a Transformation and a Role;There is always an Input, an Output, or both, in rela-tion to a Transformation. A MelangeComponentdescribes a part of a Melange, associated with aSubstance (i.e. either a ChemicalEntity or another,nested Melange) and a proportion, expressed as a simplepercentage. This enables BioHubKB to describe the yieldof transformations.Class: ParticipantSubClassOf:(input of some Transformationor output of some Transformation),has role some chemical role,is associated with some SubstanceA Role is a class imported from OBO RO, with variousre-used subclasses from the same, e.g. Buffer, Solventand Catalyst, in addition to other subclasses definedfor the BioHubKB. These are chemical Substrate,which is an input of some Transformation, andchemical Product, which is an output from someTransformation. RawMaterial acts as inputStream to some Process. OutputStream acts asoutput Stream from Process.A MeasurementProcess is a Process involvingthe quantitive measurement of some chemical propertysuch as may relate to its surfactancy. A Predictive-ModellingProcess is a process utilising a chemin-formatics model to predict the attributes of a particularmolecule. An Evaluation is a result generated bya Process, typically a MeasurementProcess orPredictiveModellingProcess.An Organisation is an entity typically having anownership relation to a Process or Substance.The class Quality is used to represent a quality asso-ciated with an Evaluation, e.g. surface interfacial ten-sion. QualityValue is a reifying class for associatingunit and number with a Quality, for an Evaluation.For a given transformation of a co-product stream a use-ful aspect for the IKMS is the percentage yield. We areinterested in yield as a factor of the overall cost calcula-tion associated with a Process, as applied to particularRead et al. Journal of Biomedical Semantics  (2016) 7:30 Page 6 of 7inputs and outputs. Yield is modelled as a Quality,with an associated QualityValue, which in turn hasan associated unit (percentage in this case) and unit-less number. Each QualityValue is associated with anEvaluation, which in the case of yield is the output of aMeasurementProcess (itself a subclass of Process).A user of the IKMS will describe the class of chemi-cals to be generated through selection of physicochem-ical properties of that class. To take a familiar example,soaps with potential for use in laundry applications area subclass of anionic surfactants which may be expectedto exhibit characteristic traits, reflected in characteris-tic ranges in qualities like surface interfacial tension andCMC, as well as typically being easily derived from cer-tain types of agricultural co-products. The modelling ofphysicochemical properties is difficult. The issues in doingso include the conditions in which a quality wasmeasured,the devices used to do the measurement and the units forthe measurement. There are several ontologies that couldbe used to capture some aspects of such measurements[13, 14], but pragmatic considerations led to a somewhatsimplistic axiom pattern.Actual feedstocks, products, co-products, etc. are rep-resented as classes below the BioHubO general domainontology to form the bulk of the BioHubKB. The con-tent in the BioHubKB comes from spreadsheets suppliedby our partners. We have a process to generate the Bio-HubKB using Tawny-OWL [15]. Spreadsheet worksheetsare exported into comma-separated-variables (CSV) files,then read by Java code (via the OpenCSV library);the data then populate a Tawny-OWL script skeletonwhose template format is defined by another Java library(FreeMarker). The populated template is then translatedinto OWL from Tawny-OWL.The BioHub ontology was developed as described andthen evaluated by project partners. We used a simplemechanism for evaluation whereby a diagram of thederivation of streams from feedstocks was presentedalong with a diagram of the gross structure of the ontol-ogy. These diagrams were associated with a simple survey[16]. The evaluation confirmed the overall representationin the ontology, but provided some useful extensions, inthe form of missing steps, to the derivation paths. Forexample, instead of Ferulic acid being derived fromBeet pulp, it was changed to reflect the true stepwisederivation of Ferulic acid, directly from Pectins.There were eight changes of this type. All proposedchanges were made to the ontology.Querying the BioHubKBThe BioHubKB is typically queried automatically (viaweb services) by other components of the IKMS. Oneexample is the IKMS user interface, which pre-populatesits selection drop-downs (e.g. lists of streams andtransformations) prior to any direct user interaction,based on the current content of the BioHubKB; anotheris the enumeration tool, which selects its initial feedmolecules from those listed in the BioHubKB as belong-ing to the user-selected streams, and amenable to a setof user-selected transformations. However, the user (i.e.the chemist) him- or herself will also have the option toissue direct queries to the BioHubKB, such as, Return alist of 3 chemicals found in the beet pulp stream, picked atrandom.DiscussionThe BioHubKB is a development of an application ontol-ogy that re-uses several OBO ontologies as well as beinga de novo ontology that satisfies a series of competencies.It is used by a cheminformatics enumeration pipeline thatgenerates a set of candidate chemicals based on a modelspecified by a user. If one of these candidate molecules istransformed from a chemical in the BioHubKB, then theBioHubKB contains the associated information about thefeedstock whence the chemical came, such as the organ-isation that supplies that feedstock, its cost and the setof transformations the chemical underwent to yield thepredicted chemical.The current IKMS and its BioHubKB is a prototypeto see if the approach is feasible. Hence there is muchpotential work to be done. The BioHubKB developmenthas so far concentrated on sugar beet as a biorenew-able; the other feedstocks are numerousoil seed rape,as well as many other agricultural feedstocks, both plantand animal, and their co-products. Further developmentis also needed in the descriptions of chemical proper-ties and their models used in the processing within theIKMS; incorporating some of the Ontology for BiomedicalInvestigations (OBI) [13] may help in this respect.The goal of the IKMS that the BioHubKB supports isto facilitate the use of bio-renewable feedstocks in thechemical manufacturing process. This is necessarily aknowledge-driven process, a job for which Semantic Webtechnologies appear to be suited. That the bio-ontologiescommunity has produced a range of ontologies that canbe slotted into an artefact such as the BioHubKB, despitethe scenario in which the IKMS is deployed not beingtraditional for the bio-ontology community, is a sign ofthe maturity and wider applicability of the communitysontologies. This supports a model of a collection of ref-erence ontologies that can be refactored and repurposedinto application ontologies in a broad range of settings.Ultimately, the BioHub has an auxiliary potential to actas a marketplace for sustainable production, enablingchemists from diverse backgrounds and organisationsto source bio-derived chemicals with multiple potentialapplications (including, but not limited to, surfactancy)from pre-existing industrial processing operations.Read et al. Journal of Biomedical Semantics  (2016) 7:30 Page 7 of 7Competing interestsThe authors declare that they have no competing interests.Authors contributionsAll authors contributed to the development of the ontology and the paper.The ontology effort was, however, led by WR. All authors read and approvedthe final manuscript.AcknowledgementsThe BioHub project is funded by awards from TSB (Innovate UK) grantsEP/L505808/1/TSB 101508 and TS/L001950/1/3TSB 101717. The partners inthese projects are: Unilever Research Port Sunlight, British Sugar Ltd, Croda,University of Liverpool, University of Manchester, University of Sheffield andCybula Ltd. We would like to thank Matthew White for his helpful commentson the sugar beet feedstock relationships. We thank Dr Mercedes ArgüelloCasteleiro for producing the ontology diagram. We thank the ChEBI team foradding chemicals to ChEBI in a most helpful manner.Author details1School of Computer Science, University of Manchester, Oxford Road,M13 9PL Manchester, UK. 2Unilever Research Port Sunlight, Bebington, CH624ZD Wirral, UK. 3Manchester Institute of Biotechnology, Princess St,Manchester M1 7DN, UK.Received: 3 December 2015 Accepted: 3 May 2016RESEARCH Open AccessNeuroRDF: semantic integration of highlycurated data to prioritize biomarkercandidates in Alzheimer's diseaseAnandhi Iyappan1,2, Shweta Bagewadi Kawalia1,2*, Tamara Raschka1,3, Martin Hofmann-Apitius1,2and Philipp Senger1AbstractBackground: Neurodegenerative diseases are incurable and debilitating indications with huge social andeconomic impact, where much is still to be learnt about the underlying molecular events. Mechanistic diseasemodels could offer a knowledge framework to help decipher the complex interactions that occur at molecularand cellular levels. This motivates the need for the development of an approach integrating highly curated andheterogeneous data into a disease model of different regulatory data layers. Although several disease modelsexist, they often do not consider the quality of underlying data. Moreover, even with the current advancements insemantic web technology, we still do not have cure for complex diseases like Alzheimers disease. One of the keyreasons accountable for this could be the increasing gap between generated data and the derived knowledge.Results: In this paper, we describe an approach, called as NeuroRDF, to develop an integrative framework for modelingcurated knowledge in the area of complex neurodegenerative diseases. The core of this strategy lies in the usage of wellcurated and context specific data for integration into one single semantic web-based framework, RDF. This increases theprobability of the derived knowledge to be novel and reliable in a specific disease context. This infrastructure integrateshighly curated data from databases (Bind, IntAct, etc.), literature (PubMed), and gene expression resources (such as GEOand ArrayExpress). We illustrate the effectiveness of our approach by asking real-world biomedical questions that linkthese resources to prioritize the plausible biomarker candidates. Among the 13 prioritized candidate genes, we identifiedMIF to be a potential emerging candidate due to its role as a pro-inflammatory cytokine. We additionally report on theeffort and challenges faced during generation of such an indication-specific knowledge base comprising of curated andquality-controlled data.Conclusion: Although many alternative approaches have been proposed and practiced for modeling diseases, thesemantic web technology is a flexible and well established solution for harmonized aggregation. The benefit of thiswork, to use high quality and context specific data, becomes apparent in speculating previously unattended biomarkercandidates around a well-known mechanism, further leveraged for experimental investigations.Keywords: RDF, Semantic web, Data integration, Data curation, Data harmonization, Disease modeling,Neurodegenerative diseases, Alzheimer's disease* Correspondence: shweta.bagewadi@scai.fraunhofer.deEqual contributors1Department of Bioinformatics, Fraunhofer Institute for Algorithms andScientific Computing (SCAI), Schloss Birlinghoven, 53754 Sankt Augustin,Germany2Bonn-Aachen International Center for Information Technology, RheinischeFriedrich-Wilhelms-Universität Bonn, 53113 Bonn, GermanyFull list of author information is available at the end of the article© 2016 Kawalia et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 DOI 10.1186/s13326-016-0079-8BackgroundAlzheimer's disease (AD), the most prominent neurode-generative disease (NDD), has become a global threat tothe aging society, affecting nearly 115 million people by2050 [1]. The imperfect understanding of the ADetiology has created a large gap in translating the pre-clinical findings into clinical trials dominantly observedin high drug attrition rates [2]. Early diagnosis and pre-ventive interventions could facilitate substantial reduc-tion in the number of affected cases to 9 million by 2050[3, 4]. Particularly, reliable biological markers of diseaseand disease progression could assist in early diagnosisand treatments catered to the patient [5]. In this direc-tion, considerable global research efforts have been dedi-cated to investigate molecular players underlying ADpathogenic events, contributing to an ever-growingwealth of disparate data. Refinement of this informationinto actionable knowledge representations requires agood interoperable and formalized framework, capableof inferring potential biomarkers across different facetsof the molecular physiology. Additionally, in silicodisease models that integrate complementary data fromvarious resources are capable of recapitulating keymechanisms for a given condition [68].Among others, most widely used data integrationstrategies include data warehousing (e. g., Pathway Com-mons [9]), data centralization (e. g., UniProt [10], IntAct[11]), and federated databases (e. g., BioMart [12]). Anexample of a data integration framework is tranSMART[13], which consists of a data warehouse covering vari-ous types of data and related data mining applicationsrequired for translational research and biomarkerdiscovery workflows. Such a harmonized aggregation ofheterogeneous data sources facilitates interpretation overa large knowledge space [14].However, one fundamental challenge with most ofthese integration approaches is to cope with the variabil-ity and heterogeneity in content, language, and formatsof incoming data from different source repositories.Moreover, regular updates of these data resources arenecessary to keep up with newly added information andto avoid incompleteness. The inaccessibility to the inte-grated data resources, due to altered database structureor change in the naming conventions is unavoidable[15]. Semantic web technologies have overcome theabove described challenges up to an extent by revolu-tionizing the lossless exchange of data and formalizingthe data format into a computable knowledge [16],calling it smart data" [17]. The capability of using richformal descriptions for data and its standardized map-ping allows complex querying in a more efficient waywithout information loss.Resource Description Framework (RDF) is the WorldWide Web Consortium (W3C) proposed standard forsemantic integration and modeling of data. RDF uses thesyntax of Extensible Markup Language (XML) and im-poses structural constraints to represent the meta-dataas a set of triples containing directed edges. One big ad-vantage lies in the usage of common namespaces acrossthe different data domains encoded as Unified ResourceIdentifiers (URIs). Initiatives such as Identifiers.org [18]provide persistent official identifiers in the biomedicaldomain, allowing sustained interlinking between distinctdata resources. This allows high levels of seamless inter-operability between data sources and the capability toaccess and map against additional related data unam-biguously, called data federation. On the contrary, largeefforts are still needed during an initial definition of theontologies to build the schema for data representation.Semantics in life sciencesThe idea of semantic web prevails in various domains,including life sciences. Recently, "The Monarch Initia-tive" [19] has taken the semantic route to enable reason-ing over genotype-phenotype equivalence within andacross species. They leverage on ontologies to link exter-nal curated data resources for generating new hypothesisand prioritizing candidates/variants based on the pheno-typic similarity. Stevens et al. [20] launched TAMBIS,multi-data application tool, which allows biologists to for-mulate complex molecular biology questions to databasessuch as Swiss-Prot [21], Enzyme [22], CATH [23], BLAST[24], and Prosite [25] through well-defined semantics.Among the early users of RDF, Lindemann et al. [26]applied it to centralize and flexibly access the heteroge-neous and varying quality of medical data obtained fromseveral clinical partners. The importance of semanticmining in the life science domain was brought to lime-light by the Bio2RDF project [27], which demonstratedthe possibility of querying life science knowledgebasesby linking public bioinformatics databases and providingpublic SPARQL endpoints. Subsequently, Linking OpenDrug Data (LODD) [16] demonstrated linking drug datainformation from DrugBank [28] and clinical trialsresources. Chem2Bio2RDF [29] demonstrates the poten-tial usage of the above two mentioned RDF repositoriesin the field of chemoinformatics.Observing the immense advantage of linked open data,several major publicly available life science databasessuch as UniProt, DisGeNet [30], Protein Data BankJapan (PDBj) [17], and EBI resources such as Gene Ex-pression Atlas [31], ChEMBL [32], BioModels [33],Reactome [34], and BioSamples [35], have made theirdata available in the form of RDF. Thus, the RDF plat-form has been increasingly adopted as a standard fordata exchange. Amidst prime users of RDF in elucidatingdisease pathophysiology, Shin et al. [36] demonstratedsystematic querying of linked experimental data toIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 2 of 15explore the effect of genes that are regulated by volatileorganic compounds in human blood. Qu et al. [6]showed semantic framework capability in drug re-purposing by proposing Tamoxifen, an FDA approveddrug for Breast Cancer, as a candidate drug for SystemicLupus Erythematosus. The above reported associationhas already been tested in mice by Sthoeger et al. [37],showing a leverage of semantic web in a real world sce-nario. Furthermore, Willighagen et al. [38] presented thelinkage of several RDF technologies in molecular che-moinformatics and proteochemometrics.To our knowledge, there has been very limited applica-tion of semantic web approaches to the research of neuro-degenerative diseases. Linked Brain Data (LBD) [39] is anupcoming initiative which focusses on understanding thebrain functionality by integrating resources such as gen-omic, proteomic, anatomical and biochemical resourceswith respect to neuroscience. Using such a multi-levelknowledgebase, they aim to understand the associationbetween cognitive functions and brain diseases. Lam et al.[40] made the first attempt to develop an e-Neurosciencedata integration framework, AlzPharm [41]. They ex-tracted AD-related drug information from BrainPharm[42] to be further integrated with manually inferredhypotheses from the scientific literature and published ar-ticles (SWAN [43]). They demonstrated the usage of sucha model by clustering AD drugs based on their moleculartargets and to filter publications (claims and hypotheses)specific to Donepezil effect on treatment of AD. AlthoughAlzPharm made use of manually inferred hypothesis, theylack the validation of their findings with experimental datasuch as gene expression and pathways.MotivationDespite the current advancements in semantic web tech-nology, we still do not have cure for complex diseases likeAD. One of the key reasons accountable for this could bethe increasing gap between generated data and the derivedknowledge. In order to increase the probability of thederived knowledge to be novel, data quality and datareliability is highly desirable. Moreover, the contextualspecificity of the data is of paramount importance.Compared to relational database management system(RDBMS) technologies, in RDF the relations have expli-cit meaning (expressiveness) in a given context and aredirectly accessible; allowing the user to extract meaning-ful knowledge from the data as opposed to an unstatedstructured data. In addition, RDF structures are moreadaptive and flexible, allowing fluidity in the data rela-tionships. This overcomes the fragility of RDBMS; whereif the underlying representation of the keys and flat tablechanges, the tentacled connections are lost. Moreover,triples from RDF can be transformed into RDBMS struc-ture and vice-versa. One another advantage of RDF is itsgraph representation that enables us to better explorerelations through network topological characteristicssuch as relatedness, network perturbation, centrality, in-fluence, etc. The usage of automated reasoners havelargely been beneficial to understand the semantics andto expand the associated relations [44].In this paper we propose NeuroRDF, an approach harnes-sing the potential of RDF as a framework for modeling neu-rodegenerative diseases to enable a close, biologicallysensitive integration of well curated, complementary, andmulti-faceted data. The fundamental principle of this strat-egy is to take advantage of semantics to develop a contextspecific, multi-layered in silico disease model, representedas a formalized, and computationally processable domainknowledge. A fine-grained analysis of the metadata fromvarious data resources empowers the user to ask more fo-cused questions around a hypothesized pathomechanisminvolving previously neglected or hidden candidates, furtherleveraged for experimental investigations. Considerable ef-forts have been invested to process and manually curatehuge amounts of data that is required to build such aknowledge base around a specific indication. This includesfor example the in-depth assessment of the respectivephenotype, the type of tissue used in an experiment, and in-formation around the donor of the tissue like gender, age,and possible comorbidities. Querying such a highly curatedand focused knowledgebase increases the chances of unrav-eling novel hypothesis, which could have been lost overtime or pave way to newly emerging knowledge.We used SPARQL to traverse each of these knowledgegraphs (derived from distinct resources) in an integrativemanner, allowing highly disease specific analysis of theunderlying data. Using this approach, we demonstrate anexample on how to prioritize novel candidates in ADmechanism.MethodsThe developed generic semantic web-based workflow in-tegrating heterogeneous data resources is outlined inFig. 1. This multi-layered model integrates data fromvarious public resources such as databases, literature,and gene expression information. The harmonization ofheterogeneous data to build RDF models was achievedby using several data/file parsers. The workflow alsoincludes a pre-processing step to monitor the quality ofeach incoming data type for specificity.Data collection and resourcesThis subsection depicts briefly the different data re-sources integrated into the NeuroRDF.Database-derived interactions for healthy brainA closer look into the healthy human brain interac-tions could improve identification of the dysregulatedIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 3 of 15mechanisms which further surges the plausibility ofidentifying AD drugs in clinical trails [45, 46]. How-ever, the mainstream AD research is biased towardsthe well known disrupted events such as APP, and taurather than recognizing their role in normal brainfunctions [47].Several publicly available databases provide protein-protein interactions (PPIs) and microRNA-target inter-actions (MTIs), which can be derived using multiplesources and methodologies. For instance, Human Pro-tein Reference Database [48], Molecular INTeractiondatabase [49], and miRTarBase [50] focus on experimentallyverified interactions that are manually mined from litera-ture by expert biologists. In addition to literature-derivedinformation, Biomolecular Interaction Network Database[51] centralizes interactions from high-throughput tech-nologies. Few other databases such as STRING [52], andmiRWalk [53] also provide predicted interactions.However, none of these databases mine interactionsspecific to a given context (for example AD pathologyor normal physiology).A lot of published healthy state PPIs are not directlymeasured in human cells but in artificial conditions suchas human cell lines, human genes transfected into yeastcells, etc., missing out on the biological plausibility inhumans and context specificity [54]. Hence, considerableeffort by Bossi and Lehner [55] was invested to verifythe tissue specificity of PPI interactions from 21 data-bases (including a few above mentioned) using humangene expression data. Furthermore, this additional actionto ensure validity of the interactions in normal state aidsimproved prediction of genes in disease state [56]. Inthat direction, our group has extracted a subset of theseexperimentally confirmed PPIs belonging to healthybrain physiology [57]. Currently, the healthy brain PPInetwork contains 7,192 genes and 45,001 PPIs.Extracting AD-specific interactions from literatureThe bridging factor between researchers and scientificaccomplishments are published as texts, warehoused inlarge repositories like PubMed [58]. These biomedicalarticles are the major information source of functionalfactors such as proteins, genes, microRNAs (miRNAs),etc. However, their functional descriptions are scatteredas unstructured text in literature [59]. Text-miningmethods could help us mine these articles and retrievethe associated relations/evidence for a given context.Since proteins are the chief players in almost all bio-logical processes and miRNAs have been established inthe last decade as important regulators of gene expres-sion, we focus our current research on MTIs and PPIs.In order to harvest AD-specific knowledge from theliterature, we used our in-house state-of-the-art namedentity recognition (NER) system ProMiner [60] and thesemantic search engine SCAIView [61]. Identification ofgenes/proteins and disease mentions was accomplishedusing dictionaries. The disease dictionary was built usingMeSH [62], MedDRA [63], and Allie [64] databases.Currently, it contains 4,729 concepts and 64,776 syno-nyms [65], which are normalized to MeSH names.Human Genes/Proteins dictionary [60] was compiledfrom three different resources: SwissProt, EntrezGene[66], and HGNC [67]. Currently, this dictionary consistsof 36,312 entries and 515,191 synonyms. All the identi-fied gene/protein names were normalized to HUGOgene symbols for maintaining homogeneity across allFig. 1 Overall workflow of NeuroRDF. The workflow illustrates the collection of data from various resources such as databases, and literature,followed by steps taken to pre-process and prune the collected data. These high-quality data are represented semantically as RDF models andstored in a triplestore. The stored knowledge can later be queried for biologically interesting questionsIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 4 of 15data resources and also for future comparisons andvisualizations.To identify MTIs from MEDLINE abstracts, we appliedour previously developed approach [65]. Here we ex-tracted novel miRNA mentions using a regular expression.These mentions were normalized to miRBase databaseidentifiers [68]. In addition, relation dictionary containingthe major classes of relationship terms between miRNAsand their target genes/proteins was also developed. A tri-occurrence based approach was used to extract the MTIs(co-occurring with a relation term) at the sentence level.Using the above-mentioned dictionaries, our grouppreviously harvested AD specific PPIs from MEDLINEabstracts and full text articles [69]. Here we used theinteraction terms compiled by Thomas et al. [70]. Astate-of-the-art machine learning based approach [71]was applied to retain true pairs of PPIs in a given sen-tence. Both approaches have been optimized for recall.Hence, the obtained relations have been manually fil-tered for false positives. After manual inspection, 339PPIs for 301 proteins and 99 MTIs for 36 microRNAsthat are specific to AD were obtained. Articles publishedin languages other than English could lead to increasedinformation content, however a dedicated approach toharvest them is needed. Moreover, separate parsers areneeded. Thus, for this work we extracted interactionsfrom the biomedical literature in English.AD gene expression dataA standard approach to test any generated hypothesis isto assess the gene expression of the involved candidatesbetween affected and healthy patients or in the absence ofhuman data we fall back to animal models or derived cellcultures [7275]. High-throughput technologies such asmicroarray, RNA-seq provide potential to measure geneexpression simultaneously for different experimental/bio-logical conditions. These studies are assembled in widelyadopted public archives: The NCBI Gene ExpressionOmnibus (GEO) [76] and ArrayExpress [77].For querying AD-specific gene expression data, we usedpreviously developed database, NeuroTransDB [78], whichcontains highly curated meta-data information for eligibleAD studies. It assembles studies from public resourcesnamely, GEO and ArrayExpress, using a keyword basedsearch approach. Among the 45 prioritized AD humanstudies, we filtered for microarray studies that measuregene expression in brain tissue extracted from both ADand healthy patients. In addition, availability of raw datawas a mandate for applying uniform pre-processing. Intotal, we obtained eight microarray studies to be integratedin NeuroRDF: GSE12685, GSE1297, GSE28146, GSE5281,E-MEXP-2280, GSE44768, GSE44770, and GSE44771.To assess the quality of the arrays we applied ArrayQuali-tyMetrics [79] package. The selected studies (independentof the platform type) were pre-processed using Bioconduc-tor (Version 3.0) packages in R [80], by applying similarmethods for maintaining consistency by reducing variance.All studies conducted on Affymetrix chips were normalizedby robust multi-array average method (rma) [81]. Similarly,package limma [82] was applied on Rosetta/Merck Human44 k 1.1 microarray chip. All the chips were normalized forbackground correction and quantile normalization. Thenormalized intensity values were log2-transformedand duplicate probes were averaged. To identify thedifferentially expressed genes between healthy andAlzheimers patients we used limma package by ap-plying Benjamini and Hochberg's method to controlfor false discovery rate (adjusted p-value ? 0.05).Data curationAlthough the current text-mining methods have startedto leverage expert curators to extract PPIs, MTIs, etc.from text, the extracted information are still prone tofalse positives [83]. Moreover, it is not straightforward touse these systems for retrieval of context-specific triplesdue to technological limitations [84]. Hence, the meticu-lousness of the identified triples to occur in a certain celltype, disease state, or events captured in AD-specificdocuments is not guaranteed. Thus, the need for manualverification is unavoidable, especially when consideringthe full text articles. The previously published test cor-pus used for evaluating the constructed AD PPI networkcontained AD-specific PPIs extracted by the machinelearning approach from 200 full text articles [69]. Man-ual inspection by the authors resulted in retaining PPIsfrom 38 articles that are truly specific to AD, thus dis-carding 81 % of the originally retrieved articles. Similarly,we retained only 68 abstracts from 250 articles (27 %)that were retrieved using our tri-occurrence based ap-proach for AD MTIs [65]. Thus, we can conclude thatonly about 2030 % of the (relation extraction based)extracted PPIs and MTIs are truly relevant to AD, point-ing out the need of manual curation.Similarly, in our recent publication [78], we havehighlighted the key issues related to retrieval and reusabil-ity of the datasets from public transcriptomics archives,such as GEO and ArrayExpress. We showed that a simplekeyword based search not necessarily asserts the specifi-city of the retrieved datasets to the queried disease ororganism. When manually inspected, we reported nearly20 % of these retrieved studies to be irrelevant for ADquery. In addition, basic metadata annotations such asage, gender, etc., which strongly contributes to the differ-ential estimates, were observed to be incomplete. Brazmaet al. [85] had earlier reported that not all the data submit-ted to GEO or ArrayExpress are MIAME compliant [86].We additionally noticed these missing annotations beingscattered as unstructured prose in database webpages,Iyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 5 of 15publications, supplementary material, figures, etc., leadingto a steep increase in the needed curation effort. Al-though the published research articles are rich in anno-tations, a large number of experiments have missingcitations [87], which have to be added manually. More-over, inconsistencies between the information stored inthe archives and in the associated publications werealso noted. On an average, about 30 min to 2 h of cur-ation effort was needed to retrieve pertinent informa-tion for a single dataset. The outcome of this workresulted in a highly curated metadata database, Neuro-TransDB, which is used in this work for extracting rele-vant AD gene expression studies.Generation of RDF modelsRDF data modelRDF allows the generation of models for processed datathat exchanges information on the Web [82]. The RDFdata model stores all the relationships between differententities as triples (subject-predicate-object). In RDFterminology, the subject, the predicate and the objectare known as resources and are represented by aunique Uniform Resource Identifiers (URIs)" in orderto support global data exchange. Literals are constantvalues such as numbers and strings mapped to the re-sources. Literals can only be used as objects but neveras subjects or predicates.RDF schemasWe constructed the RDF schemata by abiding the stand-ard RDF graph notation where an ellipse representsResource, an arrow for Property, and rectangle for Literal.In all the RDF schemas, we have maintained a commonresource representation for the Gene" namespace adaptedfrom Bio2RDF that maps to the NCBI gene database. Forthe namespaces with no available ontologies, we createdan internal namespace, called SCAI". Some of the proper-ties were described using URIs from Dubin Core MetadataElement [88].Four separate schemas (for each data resource) havebeen generated that are centered on genes for interoper-ability, associating each gene product to its official genesymbol. In the AD PPI schema (see Fig. 2), proteins andtheir interactions were represented using the UniprotCore Ontology [89]. Supporting literature evidence wereadapted to URIs from Bio2RDF namespaces. The articleresource was linked to its PubMed ID, sentence in whichthe interaction has been mentioned, and the associatedjournal. Experimental evidence that validates the giveninteraction (if any) were mapped to BioPax [90], MGED[91], ONTOAD [92], and SCAI namespaces. In the MTImodels (see Fig. 3), literature, genes, and proteins name-spaces were adapted similarly to the PPIs. To representthe miRNAs, we applied the Bio2RDF namespace thatlinks it to miRBase database [93].For the PPI schema encoding the healthy state, as seenin Fig. 4, we used the same ontologies as in case of ADPPI. Additional interaction evidence such as brainregion, reference database, experimental evidence, andliterature information were described using Core, BioPax,and Bio2RDF namespaces.The microarray schema has two branches that arelinked to the experiment: sample details and differentialexpression analysis. The majority of the resources andproperties are linked to URIs from EBI's Atlas (atlas) [94]and MGED [91] namespaces, cf. Fig. 5. Gene expressionexperiments could contain several samples that aremeasured in different conditions. A detailed description ofFig. 2 Schematic representation of the Diseased PPIs in RDF. The figure describes AD specific PPI interactions along with supporting evidencemined from literatureIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 6 of 15each sample is needed for accurate analysis. Thus, weassociated each sample to its meta-data annotations,namely age, gender, organism, organism part, platform,and phenotype. Organism under investigation is mappedto NCBI Taxonomy URIs [95]. The factor value of eachsample, i.e., the phenotype information, is described usingthe EFO ontology [96]. Each platform array is made up ofmultiple probes that may represent a gene. To be able toretain the expression values for individual probes, welinked the probe ID resource to platform. However, forbetter reasoning, quantitative values retrieved fromstatistical analysis are linked to genes and not to probes.The meta-analysis results, derived from limma [82], suchas differential expression value of a gene and its associatedp-value are all linked to the gene symbols.Construction, validation and storage of RDF modelsWe modeled all the triples (represented in the schemas)using the Apache Jena API [97]. Resources, and Proper-ties as Java classes were created from the ontologiesusing the corresponding in-built methods in the APIand with the help of Schemagen [98].In order to check for the correctness of our generatedRDF models, we made use of the online service RDFvalidator [99]. By using such a service, we verified themodels using their graph and triples representation.Fig. 4 Schematic representation of Healthy PPIs in RDF. The figure represents PPIs of healthy subjects extracted fromliterature and PPI specific databases. The schema also contains meta-information about these PPIsFig. 3 Schematic representation of MiRNA-target interactions in RDF. The figure encapsulates miRNA mentions along with their correspondinggene identifier from literatureIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 7 of 15Triple stores, such as Virtuoso [100], provides an op-portunity to store individual or integrated RDF modelsin one endpoint. Taking advantage of this, we stored allthe generated RDF models as individual graphs in a sin-gle Virtuoso instance. Using common URIs (e.g., Gene"identifier) as the connecting link between these models,it is possible to traverse through them integratively.Data mining and analysisIn RDF, all the stored triples are accessible using a com-mon query language, SPARQL Protocol and RDF QueryLanguage (SPARQL) [101]. We generated a Java librarywith embedded SPARQL queries to ask our endpointand the underlying networks biologically relevant ques-tions. Queries were generated from individual models,which were further integrated as nested queries totraverse different graphs. Each query uses the commonGene URI namespace (which is common across allmodels) to pass on the results used to the next nestedquery. One possibility to visualize the query results isthe SemScape Cytoscape [102], to represent the returnvalues as (sub-) graphs again.Results and discussionsNeuroRDF covers a wide range of curated AD relateddata resources, stored as four separate RDF models in asingle Virtuoso endpoint. It tries to address the mainconcepts (complementary) that contributes significantlyto unraveling AD pathology.Differentially expressed genesFor the eight selected microarray datasets, gene expres-sion analysis was performed between healthy anddiseased patients. Among these, GSE1297, GSE28146,and E-MEXP-2280 resulted in no differential genes foradjusted p-value cutoff 0.05. From the remaining studies,only genes that exhibited a log2 fold change of > 1.5were selected for analysis. In total, GSE5281 resulted in4,278 genes under p-value cutoff and 2 up-, and 48 down-regulated genes for the defined fold change cutoff. Simi-larly, GSE44770 provided 254 differentially expressedgenes, among which 16 up- and 11 down-regulated wereselected further. In case of GSE44771, we obtained 335differential genes that contain 11 up and 11 down-regulated genes that show > 1.5 log2 fold change. For both,GSE12685 and GSE44768, we obtained 1 and 51 genesunder the p-value cut-off. However, there were no genesthat had log2 fold change of >1.5. The list of all thedifferentially expressed genes that were selected for fur-ther analysis is provided in Additional file 1.RDF modelsTable 1 summarizes the content of the generated triplestore by providing some statistics of all integratednetworks. In total, there are 8353 unique triples in ADPPI, 1,204,194, 667 unique triples in Healthy PPI, and20,454 unique triples in gene expression RDF models(Additional file 2). The number of unique predicates(relations) for AD and healthy PPIs are 11, whereasfor MTI there are 5 and the gene expression modelFig. 5 Schematic representation of Gene Expression Data in RDF. This figure represents gene expression data obtained from public resourcessuch as GEO and ArrayExpressIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 8 of 15consists of 16. The number of entities present inthese models range from 300 to 78,852 (cf. Table 1).In case of the gene expression data, to avoid largetriples we excluded the gene expression values of in-dividual probes and included information only relatedto differential expression. Uploading and querying thesemodels was not computationally expensive due to lowerset of predicates and relatively small file size.Prioritization of AD candidatesTo illustrate the potential of NeuroRDF approach and todetermine novel AD candidates from the high qualityintegrated data, we exploit the underlying biologicalassociation between the different data resources andidentify the previously unknown information.Our prioritization criteria was based on the notionthat every data resource brings with it a piece of missingbiological information which is needed to understandthe mechanism of a certain candidate. We tried toassociate this distributed information by systematicallyaddressing the following questions:(1)Whether candidates in the diseased network tend tobe associated with normal physiology. If yes, whatare the common players that could help us in thedifferential estimates (called as causal candidates);(2)Which microRNAs regulate the selected causalcandidates that could give insights into their post-transcriptional dysregulation;(3)Have any of the selected causal candidates assessedfor their level of differential expression in anunbiased data source (e. g., gene expression data);(4)How strong is the influence of the neighboringgenes on the casual candidates. This is based on theassumption that strong candidates tend besurrounded by dysregulated genes and have aninfluence on the candidate itself;(5)Is there any functional relatedness between thecausal candidates and their neighbors;To answer these questions, we generated a set ofSPARQL queries. Figure 6 is an example SPARQL querysyntax used to obtain miRNAs that regulate the genes inthe AD networks. Similar querying has been applied tobuild a system of faceted searches for the above de-scribed questions. Firstly, we identified common genesbetween the healthy and AD PPI networks. This queryresulted in 230 intersecting genes. Looking into theMTIs, we found 13 of these genes to be regulated by atleast one microRNA (cf. Table 2). Among these 13genes, 9 were observed to be differentially expressed:APP, BACE1, ADAM10, IL1B, MAPK3, DLG4, LRP1PTGS2, and TGFB1. Except for APLP2, and IL6, all theother genes contained differentially expressed neighborseither in AD or in healthy PPIs. There were no miRNAsthat were common to these 13 genes.Sub-networks from the AD and healthy PPIs were ex-tracted to investigate the prioritized candidates (seeFigs. 7 and 8). As observed from Fig. 8, for healthy PPIsthere was one larger sub-network (containing APP,ADAM10, BACE1, MIF, MAPT, and LRP1) and asmaller one containing two genes (PTGS2, and IL1B).On the other hand, for diseased PPIs in Fig. 7, therewere two large sub-networks containing four (STAT4,JUN, MAPK3, and STMN2) and five genes (APP, LRP1,BACE1, DLG4, and TGFB1). The third sub-network wasmade up of two genes (MAPT, and TUBA4A). Amongthe prioritized candidates, APLP2 and IL6 had no com-mon links to other prioritized candidates. Thus, theywere discarded for further analysis.Relevance of prioritized AD candidatesThe remarkability of complementing wet lab researchusing the predictability and reproducibility of measuredoutcomes is one of the core reasons why researchers areTable 1 Statistics of generated RDF models stored in VirtuosoendpointModels No. of triples No. of entries No. ofpropertiesSize (mb)Alzheimers diseasePPI8353 19900 11 0.894Healthy State PPI 1204194 78852 11 99.102MTI 667 300 5 0.095Microarray 20454 9477 16 303.5Fig. 6 Example SPARQL query for information retrieval from NeuroRDF. SPARQL query as seen in the figure retrieves the miRNAs for a given geneIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 9 of 15more inclined to the field of bioinformatics. Therefore,in silico validation of predicted candidates for its rele-vancy is of utmost importance. In this direction, we pin-point the relevance of our prioritized candidates througha literature survey.AD established candidatesAlthough there are no FDA approved biomarkers for AD,researchers focus on some of the key candidates that arehypothesized to be involved in AD. In the current NDDresearch practice, APP has been established as the widelyused biomarker candidate. The classical pathological hall-mark of AD is formation of amyloid-beta aggregates (lead-ing to plaques) in brain. This is reported to be caused byfaulty proteolytic processing of APP that releases amyloid-beta [103]. Another hallmark of AD is tau pathology(MAPT gene), regulated by amyloid-beta. Hyperphosphor-ylation of tau causes accumulation of neurofibrillary tan-gles due to the disrupted functioning of axonal transport[5]. However, it is also interesting to note the paradigmshift in AD research due to recently failed drug trails thatfocused mostly around these hypotheses [2]. Never-theless, several neuroscientists still believe in the po-tential of APP and the tau hypothesis for elucidationof the underlying pathomechanism. As observed fromour generated sub-networks, our largest sub-networkwas established around the APP gene.When compared to APP, BACE1 has not been so fre-quently studied. However these genes often fall into the"most interesting gene zone" as far as AD is concernedsince it is involved in the formation of amyloid-beta.BACE1 is the major enzyme (beta secretase) involved inthe cleaving of APP at beta site and generating solubleamyloid-beta [104]. However, increased BACE1 activityhas been reported to be associated with amyloid-beta ag-gregation in AD patients [105]. Bu et al. have detailed outthe evidence that LRP1 is a receptor for APOE, a contrib-uting factor to AD [106]. Furthermore, in 1993, Strittmat-ter, Roses and colleagues [107] have identified APOE4 asthe major risk for late-onset AD. TGFB1 polymorphismhas been widely associated with an increased risk of late-onset AD. Deficiency in TGFB1 signaling leads to neuro-fibrillary tangle formation increasing the advancement ofmild cognitive impairment patients to AD, by increasingthe depressive symptoms [108]. DLG4 is a post-synapticscaffolding protein that interacts with postsynaptic recep-tors such as NMDA receptors for efficient postsynapticresponse [109]. However, its impairment has largely con-tributed to the synaptic degeneration in AD. Mutations inADAM10 gene have been associated to late-onset AD.ADAM10 enzyme has alpha-secretase activity to cleaveamyloid-beta, however BACE1 competes with ADAM10for cleavage. Thus, its decreased expression has beenimplicated in AD pathogenesis [110].Table 2 Prioritized AD candidate genesIntersected genesbetween healthyand AD PPIMiRNAs Differentially expressedneighborsNumber ofliterature articlesfor intersectedgenesHealthy PPI AD PPIAPP MIR101-1, ADAM10, TGFB1,MIR106A, MAPT, BACE1,MIR106B, MIF, LRP1MIR124-1, BACE1, 24550MIR137, LRP1MIR153-1,MIR181-C,MIR29A,MIR520C,MIR19-1BACE1 MIR107,MIR124-1, APP,MIR145, APP LRP1 1883MIR298,MIR29A,MIR29B1,MIR328,MIR9-1ADAM10 MIR451,MIR144,MIR1306, APP - 231MIR107,MIR103IL1B MIR146A,MIR155 PTGS2 - 1099MAPK3 MIR15A, - STMN2, 276MIR155 JUNMAPT MIR16-1, APP TUBA4A 3367MIR132APLP2 MIR153-1 - - 134DLG4 MIR485 - LRP1 151IL6 MIR27B - - 748JUN MIR144 - STAT4, 142MAPK3LRP1 MIR205 APP DLG4, 305APP,BACE1PTGS2 MIR146A IL1B - 474TGFB1 MIR155 - APP 276This table summarizes the literature based evidences of intersected genesbetween healthy and AD PPI and their corresponding miRNAs anddifferentially expressed genesIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 10 of 15AD emerging candidatesTo identify emerging knowledge in the context of AD, weperformed an individual gene analysis using SCAIView forpublications in PubMed. Here, we measured the co-occurrence of the causal genes (including its differentialneighbors) and AD over a period of last 10 years, see Fig. 9.Since the number of articles for the APP gene was rela-tively too high each year, we normalized the number of lit-erature evidence of other candidates using the APP gene'sarticle count for that year. Hence, the normalized rangefor the literature distribution is between 0 and 1, where 1is the highest number of articles for that year (the APPgene). Please refer to Additional file 3 for details of the lit-erature counts. Inspecting literature evidence, we foundthat all the prioritized causal candidates have been studiedin the context of AD. Moreover, among their differentiallyexpressed neighbors, STMN2 (8 articles), MAPK4 (1 art-icle), TUBA4A (2 articles), and MIF (15 articles) containedfewer articles related to AD. Among these genes, STMN2and MIF have been recently studied in the context of AD,whereas, MAPK4, STMN2, and TUBA4A were implicatedin AD nearly two decades before but failed to establish asrobust biomarker candidates.MIF's role in ADMacrophage Migration Inhibitory Factor (MIF) has forlong been known to participate in tumor proliferationdue to its pro-inflammatory cytokine functionality [111].In general, MIF acts as a key regulator of inflammatoryactivities such as innate and adaptive immunity [112].Apart from that, it is also known to play a significantrole as an anti-apoptotic factor of neutrophils as well asmacrophages [113].The MIF gene has been well studied in cancer andinflammation. However, recent studies are emergingaround a plausible role of MIF in neurodegenerative dis-eases, in particular AD. Moreover, Flex et al. [114] haveearlier reported that MIF polymorphisms are not linkedFig. 7 Extracted sub-networks from AD PPIs network. This figure symbolizes the diseased sub-graphs that were generated using prioritizedcandidates and their differentially expressed neighborsFig. 8 Extracted sub-networks from healthy PPIs network. This figuresymbolizes the healthy sub-graphs that were generated usingprioritized candidates and their differentially expressed neighborsIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 11 of 15to AD, but confirmed its complex immune and inflam-matory activities. Although, APP and tau have been as-sociated to play a key role in the pathophysiology of AD,many researchers strongly believe in the role of inflam-matory processes subsidizing to the pathology of AD.This stems from the fact that activated microglial cellsdischarge immunoregulatory cytokines which result invarious side-effects such as neuronal dysfunction and in-hibition of hippocampal neurogenesis [115]. MIF is onesuch pro-inflammatory cytokine which is known to bindwith amyloid-beta protein and enhance the plaque re-moval and neuronal debris from the brain during normalconditions [116]. Also, MIF has been identified to play arole in neuronal survival by inhibiting the activation ofERK-1/MAP kinases [117] (regulatory role in cell prolif-eration and glucocorticoid action) as well as its ability tosurpass the p53 mediated apoptosis [118]. Although, theprecise molecular function of MIF in the context of ADis unknown, it is known to play a role in inflammatoryprocesses around the plaque formation. MIF is alsohighly expressed in the neurons of rat hippocampus, oneof the primary regions to be affected by AD [117]. Bryanet al. [119] also report on the abnormal expression ofMIF in both microglia and in the hippocampal neuronsin human. This all makes MIF a plausible biomarker forinflammatory responses in AD.ConclusionNeuroRDF approach has been designed to identify newknowledge through semantic mining. The proposed inte-grative approach takes advantage of the RDF technologyto integrate well-curated data from various sourceswithin a specific indication area. From our perspective,it is necessary to focus on one indication or at least agroup of indications to build such a knowledge base forprecise modeling and analysis due to the high curationeffort one has to spend in order to reach the necessarydetails. We showed how to harmonize three major het-erogeneous resources (databases, gene expression data,and literature) used in the research area to generatehypotheses for underlying disease mechanisms. This ap-proach supports identification of novel insights withoutcompromising over quality. Furthermore, new data re-sources can be included without altering the overallframework. The usage of well-accepted ontologies pro-vides the advantage for further integration of external re-sources and databases (e.g., federated queries). Usingsuch an approach, we were able to prioritize MIF geneas an emerging candidate due to its role in inflammatoryprocesses implicated in AD pathogenesis.The advantage of using an RDF schema is that it ishighly supportive for data interoperability. Although thiswork represents the usage of the RDF schema specificfor AD, we have also extended the same to other diseasemodels such as Parkinson's and Epilepsy. However, thecurated data and the generated hypothesis for these twodiseases will be released in future under the terms of aNeuroallianz agreement [120]. Also, these resources areconstantly kept up-to-date as they are transferred tovarious upcoming projects such as AETIONOMY [121].Additional filesAdditional file 1: List of differentially expressed genes. This file containsthe list of differentially expressed genes (for each dataset used) that fallunder the adjusted p-value cutoff of 0.05. The differential expressionanalysis was performed using limma package in R statistical environment.The file is provided in an Excel format. (XLSX 68 kb)00.050.10.150.22005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016BACE1ADAM10IL1BMAPK3MAPTAPLP2DLG4IL6JUNLRP1PTGS2TGFB1MIFSTMN2TUBA4ASTAT4Normalized count of literature evidencesYearFig. 9 Statistics of the literature evidence for emerging candidate genes in the speculated sub-networksIyappan et al. Journal of Biomedical Semantics  (2016) 7:45 Page 12 of 15Additional file 2: The developed RDF models and the SPARQL queriesused are made available at: http://www.scai.fraunhofer.de/en/business-research-areas/bioinformatics/downloads/neurordf.html. (ZIP 178 kb)Additional file 3: Detailed count of literature evidences for prioritizedcandidates. This file contains the detailed count of number of evidencesavailable for each prioritized candidate for each year since 2005 incontext of Alzheimer's disease. These statistics were retrieved usingSCAIView knowledge discovery tool (as of 18 May, 2016). (XLSX 35 kb)AcknowledgementWe are grateful to Matthew Page, Translational Bioinformatics, UCB Pharma forproviding his valuable inputs during the design of the project and reviewingthe manuscript. We are thankful to Erfan Younesi and Ashutosh Malhotra forproviding the healthy state PPI and AD-PPI network respectively for this work.We also want to thank Christian Ebeling for his support in building the resourcesfor gene expression data. We would like to acknowledge the Semantic Miningin Biomedicine (SMBM2014) conference organizers, participants, andreviewers for inspiring discussions during the conference. The authors expressgratitude to the SMBM2014 conference organizers for providing an opportunityto submit to the Journal of Biomedical Semantics an extended version of theinitially published conference proceeding paper.FundingThis study was funded by a grant from the German Federal Ministry forEducation and Research (BMBF) within the BioPharma initiative "Neuroallianz".Authors contributionsAI, SBK, PS, and MHA conceived and designed the overall research strategyrequired for data integration. PS is the scientific supervisor to this work. SBKand AI are the main contributors to manuscript writing. TR contributed tothe analysis of gene expression data. PS, and MHA reviewed the content.All authors read and approved the final version of the manuscript.Competing interestsThe authors declare that they have no competing interests.DeclarationsThe underlying principles of this article have been previously published inProceedings of the 6th International Symposium on Semantic Mining inBiomedicine (SMBM2014), Aveiro, Portugal, 2014.Author details1Department of Bioinformatics, Fraunhofer Institute for Algorithms andScientific Computing (SCAI), Schloss Birlinghoven, 53754 Sankt Augustin,Germany. 2Bonn-Aachen International Center for Information Technology,Rheinische Friedrich-Wilhelms-Universität Bonn, 53113 Bonn, Germany.3University of Applied Sciences Koblenz, RheinAhrCampus,Joseph-Rovan-Allee 2, 53424 Remagen, Germany.Received: 1 March 2015 Accepted: 23 May 2016Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 DOI 10.1186/s13326-016-0070-4RESEARCH Open AccessFiltering large-scale event collectionsusing a combination of supervised andunsupervised learning for event triggerclassificationFarrokh Mehryary1,2*, Suwisa Kaewphan1,2,3, Kai Hakala1,2 and Filip Ginter1AbstractBackground: Biomedical event extraction is one of the key tasks in biomedical text mining, supporting variousapplications such as database curation and hypothesis generation. Several systems, some of which have been appliedat a large scale, have been introduced to solve this task.Past studies have shown that the identification of the phrases describing biological processes, also known as triggerdetection, is a crucial part of event extraction, and notable overall performance gains can be obtained by solelyfocusing on this sub-task. In this paper we propose a novel approach for filtering falsely identified triggers fromlarge-scale event databases, thus improving the quality of knowledge extraction.Methods: Our method relies on state-of-the-art word embeddings, event statistics gathered from the wholebiomedical literature, and both supervised and unsupervised machine learning techniques. We focus on EVEX, anevent database covering the whole PubMed and PubMed Central Open Access literature containing more than 40million extracted events. The top most frequent EVEX trigger words are hierarchically clustered, and the resultingcluster tree is pruned to identify words that can never act as triggers regardless of their context. For rarely occurringtrigger words we introduce a supervised approach trained on the combination of trigger word classification producedby the unsupervised clustering method and manual annotation.Results: The method is evaluated on the official test set of BioNLP Shared Task on Event Extraction. The evaluationshows that the method can be used to improve the performance of the state-of-the-art event extraction systems. Thissuccessful effort also translates into removing 1,338,075 of potentially incorrect events from EVEX, thus greatlyimproving the quality of the data. The method is not solely bound to the EVEX resource and can be thus used toimprove the quality of any event extraction system or database.Availability: The data and source code for this work are available at: http://bionlp-www.utu.fi/trigger-clustering/.Keywords: BioNLP, Event extraction, Trigger detection, Word embeddings*Correspondence: farmeh@utu.fi1Department of Information Technology, University of Turku, Turku, Finland2The University of Turku Graduate School (UTUGS), University of Turku, Turku,FinlandFull list of author information is available at the end of the article© 2016 Mehryary et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 2 of 13BackgroundThe overwhelming amount of biomedical literaturepublished annually makes it difficult for life scienceresearchers to acquire and maintain a broad view ofthe field, crossing the boundaries of organism-centeredresearch communities and gathering all of the informa-tion that would be relevant for their research. Modernnatural language processing (NLP) techniques strive toassist the researchers with scanning the available literatureand aggregating the information found within, automati-cally normalizing the variability of natural language state-ments. The applications of NLP in life sciences range fromautomated database curation and content visualization tohypothesis generation and offer intriguing challenges forboth NLP and life science communities [13].As a response to the need for advanced literature min-ing techniques for the biomedical domain, the BioNLP(Biomedical Natural Language Processing) communityof researches has emerged. The primary focus of themajority of research within the BioNLP community isto improve information retrieval (IR) and informationextraction (IE) in the domain.In this paper we focus on the task of event extraction, atask that has received much attention in BioNLP recently.Event extraction constitutes the identification of biolog-ical processes and interactions described in biomedicalliterature, and their representation as a set of recur-sive event structures. In its original form, introduced inthe 2009 BioNLP Shared Task on Event Extraction (ST)[4], the task focused on gene and protein interactions,such as RNA transcription, regulatory control and post-translational modifications. In subsequent Shared Tasks,while the overall setting remained unchanged, the taskhas been broadened to cover a large number of additionalbiological domains and event types [5, 6].More specifically, event extraction involves detectingmentions of the relevant named entities which are typi-cally genes and gene products (GGPs), the type of theirinteraction from a small vocabulary of possible types, thetrigger expression in the text which states the event, andthe roles of the participants in the event, e.g. regulatoror regulatee. One of the distinguishing features of eventsis that they can recursively act as participants of otherevents, forming recursive tree structures which preciselyencode the factual statements in the text, but are a chal-lenging extraction target. An example of an event is shownin Fig. 1.A number of event extraction systems have been intro-duced as the result of the series of BioNLP SharedTasks. Most of these systems focus solely on theimmediate textual context of the event candidates, butrecently approaches benefiting from bibliome-wide data,either through self-training or post-processing steps, havebeen introduced as well [79]. Unfortunately the recentadvancements in this field have been modest, reflectingthe complexity of the task. As an example, the best per-forming system in ST09, TEES (Turku Event ExtractionSystem) [10], has remained a state-of-the-art approach,winning also several categories in the later Shared Tasks,although the performance of the system has not increasedsubstantially during these years.Several event extraction systems have been applied ata large scale, extracting millions of events from massivetext corpora [11, 12]. These large corpora, typically thetotality of PubMed abstracts and PubMed Central full-text articles, contain a number of documents which arepartly or entirely out-of-domain for these systems, beingunlike the carefully selected data from narrow biologicaldomains on which the systems have been trained. Fac-ing documents from such previously unseen domains, thesystems often produce suboptimal output, making whatseems to a human like trivial mistakes. Tuning the per-formance of these systems in the general domain requiresfurther effort.Here we focus on EVEX [11], an event database cov-ering the whole PubMed and PubMed Central OpenAccess (PMC-OA) literature, produced using the afore-mentioned TEES system. Already a casual inspection ofEVEX reveals occasional occurrences of obviously incor-rect events especially in out-of-domain documents. Previ-ously, Van Landeghem et al. [13] have studied the outputof the event extraction systems on general domain datain further detail. Their analysis resulted in a set of rulesthat can be used to remove or correct erroneous events.Although applying this method produced only an increaseof 0.02pp in F-score when evaluated on the official SharedTask data, the consequences on large-scale resources suchas EVEX are significant: hundreds of thousands of falseevents can be excluded, thus greatly improving the qualityof the extracted data. This is because the official SharedTask test data does not contain the out-of-domain docu-ments found in the corpora used to build EVEX and manyof the error types made by the system will not be seen inthe test set output.Van Landeghem et al. point out that a large portionof the false event predictions originate from the triggerdetection phase, i.e. false positive identification of the tex-tual spans expressing the biological processes underlyingthe events. These, in turn, lead to the generation of falsepositive events by the system. Here it is important totake into consideration that the top-ranking event extrac-tion systems are based on machine learning and do notuniquely rely on a list of possible safe trigger words,which would result in an excessively low recall. Instead,any word can become a trigger word, which occasion-ally leads to wildly incorrect predictions. These, in turn,are easily spotted by the users of the event databases anddecrease the perceived credibility of the resources.Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 3 of 13Fig. 1 Visualization of a specific event occurrence. Genes and gene products (GGPs) are marked, as well as the trigger words that refer to specificevent types. Finally, arrows denote the roles of each argument in the event (e.g. Theme or Cause). (Adapted from [23])In this paper, we thus focus specifically on the eventtriggers in the EVEX event database, with the objectiveof automatically identifying and removing those that areobviously incorrect. To solve this problem, we introducea novel approach based on word embeddings, bibliome-wide statistics and both supervised and unsupervisedmachine learning techniques.Since our method relies on bibliome-wide statistics thatshould be gathered from a large-scale biomedical eventdatabase, it serves as a post-processing step in eventextraction pipeline to filter out incorrect events from thatdatabase, after the events are extracted.MethodIn this section, we first introduce the data that is used inthis study and then propose a 6-step method to achievethe aforementioned objectives.In the first five steps of our method, we focus on thetop most frequent trigger words which account for 97.1%of all events in EVEX. In steps 1, 2, 3 and 4 we per-form hierarchical clustering of these trigger words andbuild, analyze and prune the resulting binary tree to cat-egorize these triggers as correct/incorrect. In step 5, werefine these two sets using manual annotation. Finally instep 6, we build a predictive model based on support vec-tor machines (SVM) to classify the triggers as correct orincorrect.DataThis study is based on the EVEX resource [11] contain-ing 40,190,858 events of 24 different types such as binding,positive-regulation, negative-regulation, and phosphoryla-tion. These events are extracted using the TEES system[14] from 6,392,824 PubMed abstracts and 383,808 PMC-OA full-text articles that were published up to 2012 andwhich contain at least one gene/gene-product mention.The EVEX resource can be downloaded and browsedonline at www.evexdb.org.Trained on the ST-data sets, TEES extracts events basedon the recognition of an occurrence of a trigger word inthe underlying sentence. An event is thus representingthe link between the event trigger word and participat-ing argument GGPs. However, one textual span can actas a trigger for multiple events with varying arguments asillustrated in Fig. 2.In addition, a single unique trigger word, such asmodify,may have a number of occurrences in the data, acting as atrigger for many events. It is important to note that theseevents may be of different types. For instance the triggerword expression acts as a trigger for both gene-expressionand transcription events, depending on the context.Throughout this paper, we refer to the total number ofextracted events from a trigger as trigger frequency andto the actual occurrence count of the trigger in the corpusas trigger occurrence count. Clearly, trigger frequency isgreater or equal to trigger occurrence count since one trig-ger occurrence can be associated withmultiple events. Forexample, the frequency of expression is 3,909,759, whileits occurrence count is 2,736,782. It should be highlightedthat the aim of this study is to increase the precision ofextracted events, thus the focus is on the trigger frequency,i.e. the number of incorrect events that are finally removedfrom EVEX, when a particular trigger is identified asincorrect.In total, there are 137,146 unique event triggers (exclud-ing obviously incorrect trigger words that are purely num-bers and those which contain unicode special characters).Different trigger words have different frequency in thesystem ranging from 1 to 3,909,759.As expected, the vast majority of events in EVEX corre-spond to a small number of highly frequent trigger words,as shown in Table 1. For example, there are only 3,391 trig-ger words with frequency above 300 (i.e. corresponding toat least 300 event occurrences), but these words accountfor 97.1% of all events in EVEX. Consequently, when theaim is to increase the precision of the events in EVEX byrecognizing incorrect trigger words and eliminating them,the focus should be centered on highly frequent triggerwords instead of the rare ones. Accordingly, we decidedto concentrate on these 3,391 top most frequent triggerwords. Limiting ourselves to the top most frequent trig-ger words allows manual inspection of the hierarchicalclustering tree discussed in the following sections.Among the trigger words, we will target those whichare obviously incorrect, regardless of their context. Thesecould be for example, gene/protein/chemical names,author names or any other words such as hospital, univer-sity, research, diagram, box, clarify, investigate, visualiza-tion, knowledge, one or please. The main objective of thisstudy is thus to develop a method that can categorize theMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 4 of 13Fig. 2 Example sentence with multiple events sharing a single trigger. Two event occurrences extracted from the same trigger word recognizedtrigger words so as to eliminate the obviously incorrecttrigger words, thus increasing the precision of the eventextraction systems without impacting their recall.Another interesting aspect when studying the triggerwords is to build a general overview of all of the trig-ger words according to the 24 different event types andto study whether there exist groups/sub-groups of relatedtrigger words which would allow us to define subtypes ofthe 24 event types. Of specific interest will be studying thegroups/sub-groups before and after eliminating incorrecttrigger words.Hierarchical clustering of topmost frequent trigger wordsIn the first step, we induce a vector space representa-tion for the trigger words, and hierarchically cluster thetriggers based on this representation. Cosine similarity isused as the clustering metric with the Wards varianceminimization algorithm defining the distances betweennewly formed clusters. To build the vector space repre-sentations, we use the word2vec method of distributionalsemantics introduced by Mikolov et al. [15] and previ-ously applied in the biomedical domain by Pyysalo et al.[16]. The word2vec method comprises a simplified neu-ral network model with a linear projection layer and ahierarchical soft-max output prediction layer. The inputTable 1 Distribution of triggers and their associated eventpercentages in the EVEX databaseTrigger word frequency EVEX events coverage Number of trigger(at least) percentage words100 98.4 6339200 97.6 4263300 97.1 3391400 96.6 2880500 96.3 2538layer has the width of the vocabulary, while the projectionlayer has the desired dimensionality of the vector spacerepresentation. Upon training, the weight matrix betweenthe input and the projection layer constitutes the wordvector space embeddings. The network can be trainedin several different regimes, but in this work we use theskip-gram architecture, whereby the network is trained topredict nearby context words, given a single focus word atthe center of a sliding window context.We train the word2vec model on the lower-cased textsfrom the EVEX resource, i.e. all abstracts and full-text arti-cles in which at least one GGP was identified. All GGPmentions in the texts are replaced with the ggp place-holder and all numbers with the num placeholder todensify the text.An initial experiment in hierarchical clustering of thetop 100 most frequent trigger words revealed that on onehand many coarse/fine grained sub-clusters were formedin a way that each sub-cluster contained trigger wordswith biologically similar meaning. Many sub-clusterscould be clearly associated with a unique event type.On the other hand, many trigger words were clusteredtogether incorrectly, especially for the common positive-regulation and negative-regulation types (e.g. increase anddecrease) because they have a high similarity in the vectorspace representation.To address this issue, we add trigger/event type asso-ciation information as additional dimensions to the wordvectors, thereby affecting the clustering to more closelyconform to the event types. To obtain reliable event typedistribution for the trigger words, we use the BioNLPShared Task 2011 (ST11) training and development sets[5]. Out of the 1,447 unique trigger words in this data,995 are single-token trigger words and of these, 828 areactually among the top 3,391 most frequent EVEX trig-ger words. For these 828 triggers, we append a normalizedevent type distribution vector to their word2vec-basedMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 5 of 13vectors (the vectors for the remaining 2,563 triggersfor which a reliable event type information could notbe obtained are simply padded with 24 zeroes). Re-clustering with the modified vectors, we notice thatpositive-regulation and negative-regulation trigger wordsare no longer clustered together, obtainingmoremeaning-ful clusters with regard to the task at hand.Event type vectors of sub-clustersIn this step, event type distribution vectors for all nodes ofthe binary cluster tree are calculated. For each leaf of thetree (i.e., a trigger word), its corresponding trigger/eventtype vector is calculated based on the occurrence countsof its respective events in EVEX, and for each interme-diate node of the tree (i.e., a sub-cluster), its respectiveevent type vector is calculated by adding trigger/eventtype vectors of all triggers that belong to it.Using this information, it is possible to inspect howthe tree is organized and whether and how its differentbranches represent different event types. For example, bychecking which element in a sub-clusters event type vec-tor has the maximum value, we can tell what is the eventtype that this sub-cluster is mostly associated with andthe level of purity of that cluster. For example, while onesub-cluster can be 98% binding and is thus to a largeextent pure, another cluster can be 43% gene_expressionand cannot be assigned a single predominant type.Identifying possibly incorrect trigger wordsFocusing on 3,391 top most frequent trigger words, in thisstep we prepare a list of safe or supposedly correct triggerwords and regard the remaining triggers as possibly incor-rect. This is necessary for pruning the tree and finding thelist of incorrect trigger words in the next step.As stated in Section Hierarchical clustering of top mostfrequent trigger words, by analyzing the ST11 trainingand development sets, we obtain a list of 995 uniquesingle-token trigger words. Some of these triggers areoverlapping with EVEX triggers. However, our list con-tains many other trigger words that can not be directlyfound in the ST11 sets, but variations of them or vari-ations of their parts can. For instance, processing andco-regulation are in the EVEX-based list, while processedand regulation are in the ST11 sets.We therefore process BioNLP ST09 [4], ST11 [5], andST13 [6], training and development sets, to obtain a setof all single-token ST-trigger words. This trigger set, here-after ST-set, contains 1,092 trigger words. Then we per-form the following preprocessing steps on every triggerword in both EVEX and ST-set.1. Remove any punctuation or special characters fromthe beginning of the trigger word, retaining the restof the word as the trigger word. For example,-stimulated is transformed into stimulated.2. We split each trigger word based on occurrences ofthe following characters: {-, . , _ , /}. For example,co-express is divided into co and express, andsimilarly cross-reacts is divided into cross and reacts.3. For every trigger word, each of its split parts is savedif all of the following conditions are met:(1) The part is longer than one character.(2) The part is not a number.(3) The part is not in the following stop list: {32p,auto, beta, cis, co, cross, de, double, down,mono, non, out, poly, post, re, self, trans,under}. We obtained this list experimentallyby careful examination of the ST-set.4. Finally, we lemmatize all the trigger words, and all oftheir parts, using the BioLemmatizer tool [17] whichis specifically developed for the biomedical domain,and record all the produced lemmas for each triggerword.After the preprocessing, 977 EVEX trigger words thatcan directly be found in the ST-set are regarded as safe.The rest of the triggers are regarded as safe if their exactform, or one of their parts, or one of the lemmas of theirparts can be found in the ST-set, or ST-set words parts orpart lemmas. Otherwise, the trigger word is regarded aspossibly incorrect.Performing the aforementioned approach resulted inidentification of 506 trigger words which were added tothe list of safe triggers, totaling to a list of 1,483 safe triggerwords. The 1,908 remaining triggers are regarded as pos-sibly incorrect. Table 2 shows some example words fromEVEX triggers in our list that are matched against ST-settrigger words, parts, or lemmas.As discussed earlier, we do not save parts of the EVEXtrigger words if they belong to our stop list. The stoplist comprises the prefix parts obtained by splitting ST-settrigger words, which are not themselves ST-set triggerwords. For example, cross-link is a ST-set trigger word, butcross itself is not a stand-alone ST-set trigger, thereforeTable 2 Examples of matching EVEX trigger words againstShared Task exact trigger words or their correspondingparts/lemmasEVEX trigger word ST11-trigger word/Part/Lemmaco-transcribed transcribedcalcium-induced inducedco-immunoprecipitates immunoprecipitatedownregulating downregulaterecognise recognizepreceding precedeanalyzing analyseMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 6 of 13cross is included in the stop list. As a contrary example,up-regulation is a ST-set trigger word, however we did notinclude up in the stop list because up itself is a ST-set trig-ger word. We perform such approach because we do notwant any EVEX trigger word like re-X or cross-X (X can beany word) to be matched against ST-set words, parts andlemmas, just because it has a re or cross as a prefix.Pruning the treePruning the tree is done using the list of possibly incorrecttrigger words in four steps:1. If a trigger word exists in the list of possibly incorrecttrigger words, its corresponding leaf is marked asunsafe, otherwise it is marked as safe.2. If all of the children of an intermediate node aremarked as unsafe, this node (sub-cluster as a whole)is marked as unsafe as well, otherwise it is marked assafe.3. All of the descendants of the intermediate nodes thatare marked as unsafe, are deleted from the tree. Therespective trigger words of the deleted leaves aresubsequently added to the list of incorrect triggerwords.4. After tree pruning, the trigger words of all leaves thatremain in the tree, are marked as safe and regardedcorrect.After applying the aforementioned tree pruning algo-rithm, we obtain a set of correct and a set of incorrect topmost frequent EVEX trigger words.There is one important aspect in the pruning algorithm.Since the tree is binary, not all of the trigger words thatare in the list of possibly incorrect triggers were finallyregarded as incorrect trigger words, because if such a trig-ger word was clustered near a safe trigger word (i.e., had avery small cosine distance to a safe trigger word in the fea-ture space), it was not considered as an incorrect triggerword and remained in the tree. This helps us to identifymore correct trigger words.For example, co-localization which is an EVEX trig-ger word is also a Shared Task trigger word, so it hadbeen marked as safe in the matching step, howevercolocalization (another EVEX trigger word) originally hadbeen regarded as possibly incorrect, because our match-ing procedure could not have matched this trigger word(or its lemma) against any ST-set trigger word or part orlemma. However, because these two words are extremelysimilar in the vector space representation, they clusteredtogether in the binary cluster tree. Consequently, since anunsafe trigger was clustered with a safe trigger, that wholesub-cluster was regarded as safe and remained in the tree,so colocalization finally is regarded as a correct triggerword. To summarize, the tree pruning algorithm causesdeletions to be propagated to the upper level nodes of thetree only if all of the participating leaves are recognized asincorrect.After pruning, event type vectors for all intermediatenodes of the tree are recalculated so that we can comparethe tree before and after pruning.Refining correct and incorrect trigger setsThe output of the tree pruning step are the correct andincorrect trigger words sets, into which the top most fre-quent EVEX trigger words are assigned. As discussedin Section Results, our unsupervised method (steps 1-4)increases the precision and F-score of event extractionsystems, however it causes a comparatively small dropof recall. This means that some of the correct triggerwords are erroneously included in the incorrect triggerset, thus deleting their corresponding events from EVEXconsequently decreases the recall of that event extractionsystem. As our objective is to increase the precision with-out decreasing the recall, i.e. we try to avoid removingcorrect events from EVEX at any cost, we address the issueusing manual annotation to refine the results.Manual annotation of triggersA list of 3,391 trigger words was prepared by extract-ing the trigger words with frequency of at least 300 fromEVEX. As discussed in Section Identifying possibly incor-rect trigger words, 977 of EVEX topmost frequent triggersoverlap with the ST-set. We assume these triggers to becorrect and provided the 2,414 remaining trigger words toan annotator with prior experience in biomedical domainannotation.The annotator performed the manual annotation bydeciding for each trigger whether it is correct or incor-rect. On one hand, a trigger is correct if its occurrence canlead to the extraction of one or more of the 24 SharedTask event types, i.e. the given trigger word can repre-sent at least one of the ST event types in some context,although in another context they might still be invalid. Onthe other hand, an incorrect trigger cannot express SharedTask events in any context. The annotator was allowedto use any available resources, such as NCBI [18], GeneOntology [19] and KEGG [20] databases, to support theannotation.The annotation of the top most frequent EVEX triggersresulted in three categories: 2083 triggers were annotated as correct. 577 triggers were annotated as incorrect. 731 triggers were not annotated and remainedundecided.In a closer look at the annotation data, the mostcommon correct triggers are the words specificallyused in biomedical domains such as gene expression,regulation and transcription to state the events. TheMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 7 of 13incorrect triggers are mostly biomedical entities such asgenes, proteins and chemicals. While the majority of thetriggers (2660/3391, 78.44%) can be annotated, the anno-tator was unable to make a decision for 21.56% of thetriggers. Most of these undecided triggers are multiple-meaning words used in both biomedical and genericdomains such as conserved, deletion, and develop-ment. Thus it is possible to construct hypothetical sen-tences where these words are valid triggers, but the anno-tator was not able to find any evidence supporting theuse of these words as triggers from the existing literature.While going through all the sentences would be an idealsolution to resolve this issue, it is impossible in practicedue to the vast amount of the data.As this evaluation was conducted by a single annota-tor, we have not assessed the inter-annotator agreement(IAA) for this task. To our knowledge, the organizers ofthe BioNLP Shared Task have not reported the IAA forthe GE data set either. For the EPI data set (Epigenet-ics and Post-translational Modifications) the organizersreport agreement level of 82% measured in F-score [21].This evaluation, however, measures the annotation of thefull event structures and no direct conclusions can bemade for the trigger annotations.Aggregating unsupervisedmethod results withmanualannotation resultsIn this step, we aggregate the results from tree prun-ing (Section Pruning the tree) and manual annotation.We naturally prioritize the manual annotation, i.e., in theaggregated data a trigger remains correct or incorrect iflabeled as such in the manual annotation. The undecidedtriggers are assigned using the tree pruning method. As aresult, the final set is comprised of 2,242 correct triggersand 1,149 incorrect triggers.Classification of low-frequency event triggersIn the previous steps, the focus was on assigning a label fortop most frequent trigger words (those with frequency ofat least 300) which account for 97.1% of all EVEX events.However, this demanding manual annotation method cannot be applied to the huge number of triggers with lowerfrequency that exist in EVEX. To address this problem,we use support vector machines (SVM) to classify thelow-frequency triggers (i.e., triggers with frequency below300). As the training data, we use the aggregated triggerset from the previous section, assigning correct and incor-rect triggers as positive and negative examples, respec-tively. Our training set totals 3,391 training examples,consisting of 2,242 positive and 1,149 negative examples.We optimize the model using grid-search combined withcross-validation.Prior to building the model, we considered two impor-tant aspects which should be highlighted here. First, weprefer a conservative predictive model which tends tohave a very high positive recall, because if the classi-fier mispredicts a correct trigger as incorrect, all of itsrespective events mistakenly will be deleted from theoutput of event extraction systems which is very unde-sirable and that can also have a huge adverse effect onthe recall of events. Conversely, if the classifier mistak-enly predicts an incorrect trigger as correct, its respectiveevents will remain in the output of event extraction sys-tems, and in general we prefer to tolerate false eventsinstead of deleting correctly extracted events. Becauseof this reason, and because our training set is imbal-anced, we give weight 10 to the positive class and weight1 to the negative class during classifier training. Theseweights are set experimentally during the grid search andclassifier optimization. In addition, during optimization,instead of optimizing against F1-score we optimize againstF2-score, because F2-score weights recall higher thanprecision.Second, from the point of view of event extrac-tion systems, the respective events of the triggers aremore important than the trigger words themselves. Forinstance, misclassifying a correct trigger with frequencyof 200 will translate into removing 200 correct events,comparing it with the removal of a correct trigger with fre-quency of only 1. Consequently, we consider the precisionand recall of respective events (not the triggers) and adjustthe parameter optimization and training accordingly: During optimization, instead of optimizing againstthe F2-score of triggers, i.e., calculating F2-scorebased on the counts of true-positives (TP),true-negatives (TN), false-positives (FP) andfalse-negatives(FN), we optimize against F2-score oftrigger frequency, i.e., calculating F2-score based onthe sum of frequencies of TP, TN, FP and FN. We give a weight to each training example bycalculating the logarithm of its frequency.Thus the training examples with higher weights, i.e.higher event frequency, will be regarded moreimportant than lower weight examples, those withlower event frequency. In other words, classifier willbe penalized more on misclassifying the frequenttrigger words than lesser ones during training andk-fold cross-validations. As a result, the classifier istrained towards better performance on morefrequent triggers while we intentionally do not givethe trigger frequency as a feature to the classifier.Below is the set of features used by the classifier.1. word2vec-based vector for each trigger, which isexactly the same vector discussed inSection Hierarchical clustering of top most frequenttrigger words.Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 8 of 132. If the trigger word (or its lemma or its parts orlemmas of its parts) can be matched against ST-setwords/parts/lemmas (according toSection Identifying possibly incorrect trigger words)this feature is 1, otherwise it is zero.3. The value for this feature is calculated as following:feature_value = occurrence count of trigger / Xwhere X is the sum of occurrences of the word in allPubMed abstracts and PMC-OA full text articlespublished up to 2013, regardless of being recognizedas a trigger word in the underlying sentences or not.For many incorrect trigger words, this feature willhave a very low value. For example, for an incorrecttrigger word like hospital which is in EVEX (not intraining set), the value will be (928 / 1,266,408) =0.0007.4. For this feature, we first extract the set of single-tokentriggers with length of more than 6 characters in theST-set, introduced in Section Identifying possiblyincorrect trigger words. Then, for each trainingexample we calculate the feature value as following:feature_value = length(trigger word) / (Y + 1)where Y is the minimum edit distance (Levenshteindistance) of the trigger word to the words in thepreviously created set. For all training examples thatbelong to the ST-set, we assume Y to be zero.The longer the trigger word, and the smaller itsminimum edit distance, the higher will be the valueof this feature.This feature is beneficial for example in the case of amisspelled trigger (e.g., phosphoryalation instead ofphosphorylation), which is not recognized correctlyby our matching protocol discussed inSection Identifying possibly incorrect trigger words.5. Number of alphabetic characters divided by thelength of the trigger word.We perform a grid-search combined with 5-fold cross-validation to optimize the classifier and find the besthyper-parameters for the model (kernel type, C value, andthe gamma parameter for RBF-kernel) against the F2-score of trigger event frequency. Subsequently, we trainthe classifier using the best parameter values on all avail-able training examples.ResultsIn this section, we discuss the results in four parts. First,in Section Evaluation of event filtering, we evaluate theimpact of trigger pruning on event extraction systems.We then evaluate our predictive model and investigatethe effect of event filtering on the EVEX resource inSections Evaluation of low-frequency trigger classificationand Evaluation of event removal on the EVEX resource.Finally, in Section Tree organization before/after pruningwe examine the trigger cluster tree organization beforeand after the pruning.Evaluation of event filteringEvaluationmethodWe evaluate the impact of trigger pruning on event extrac-tion using the official test sets of the BioNLP ST11 andGENIA Event Extraction (GE) Shared Tasks (ST13). Asthe basis we consider the outputs of the TEES systementry [10, 14] in 2011 (3rd place) and in 2013 (2nd place)GE tasks and, for the 2013 Shared Task, also the winningEVEX entry [7]. We prune the outputs of these systemsby removing events whose trigger words are identified asincorrect using the aforementioned algorithm and eval-uate the resulting pruned set of events using the officialevaluation services of the respective Shared Task on theheld-out test sets. The results are shown in Table 3.It should be highlighted that naturally the magnitude ofthe F-score improvements is modest, as the top-rankingsystems are well optimized and major improvements havebeen difficult to achieve regardless of the approach. Notealso that a filtering approach such as the one proposed inthis paper cannot increase recall because it is unable toproduce new events. Our main focus thus is on improv-ing the precision while trying to retain the recall, aimingto increase the credibility of large-scale event extractionsystems in general.Evaluation of unsupervisedmethodIn this section, we investigate the effect of removingtriggers from event extraction systems using the set ofincorrect trigger words obtained from our unsupervisedmethod in Section Pruning the tree.As shown in Table 3, in all three instances (compar-ing our unsupervised method against the TEES systemspredictions on tasks 2011 and 2013, and the EVEX sys-tems predictions on task 2013), we see an improvementin both precision and F-score with a relatively small dropin recall. Especially for the ST13, the pruned TEES sys-tem (+0.23pp F-score over TEES) matches in performancewith the winning 2013 EVEX system. Since the EVEX sys-tem was also based on TEES, it is interesting to note thatwe have matched these improvements using a differentapproach. Finally, the pruned EVEX system (+0.18pp F-score over the EVEX entry) establishes a new top score onthe task.Evaluation ofmanual annotationmethodIn this section we investigate the effects on event extrac-tion if we rely ourmethod solely on themanual annotationresults. We remove events from those three aforemen-tioned event extraction system outputs, using the set oftrigger words that were annotated as incorrect by thehuman annotator.Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 9 of 13Table 3 Performance comparison of the different pruning approaches and the baseline methods (TEES/EVEX) on the official BioNLPShared Task GE data setsPredictions Precision Recall F1-scoreTEES-2011 (Shared Task 2011)Original TEES 61.76 48.78 54.51Pruned-TEES (Unsupervised Method) 62.39 48.75 54.74Pruned-TEES (Manual Annotation Method) 62.04 48.78 54.62Pruned-TEES (Aggregation Method) 62.26 48.78 54.70Pruned-TEES (Aggregation Method + SVM) 62.27 48.78 54.71TEES-2013 (Shared Task 2013)Original TEES 56.32 46.17 50.74Pruned-TEES (Unsupervised Method) 57.13 46.02 50.97Pruned-TEES (Manual Annotation Method) 56.63 46.17 50.87Pruned-TEES (Aggregation Method) 56.97 46.17 51.00Pruned-TEES (Aggregation Method + SVM) 57.01 46.17 51.02EVEX-2013 (Shared Task 2013)Original EVEX 58.03 45.44 50.97Pruned-EVEX (Unsupervised Method) 58.77 45.29 51.15Pruned-EVEX (Manual Annotation Method) 58.32 45.44 51.08Pruned-EVEX (Aggregation Method) 58.66 45.44 51.21Pruned-EVEX (Aggregation Method + SVM) 58.71 45.44 51.23As shown in Table 3, in all three instances (compar-ingmanual annotation method against the TEES systemspredictions on tasks 2011 and 2013, and the EVEX sys-tems predictions on task 2013), manual annotation retainsthe recall, which is obviously a better result than our unsu-pervised method. However in all three instances, its pre-cision and F-score is less than the precision and F-score ofour unsupervised method.The preserved recall suggest that our annotationstrongly agrees with the ST annotation guidelines. How-ever, the higher precision of the unsupervised pruningstrategy shows that some cases not clear for a humanannotator, can be classified with this method.This is exactly what we had anticipated. As preciseannotation was not possible for many trigger words,we have 731 undecided top most frequent triggers, andmany incorrect trigger words might actually be amongthem.To summarize, the manual annotation has produced analmost pure but incomplete set of incorrect trigger words.In comparison to original event extraction system perfor-mances, our manual annotation method does increase theprecision and F-score while retaining the recall, but itsprecision and F-score are not as high as our unsupervisedmethod.Evaluation of aggregationmethodAs shown in the previous sections, our unsupervisedmethod increases the precision and F-score, but slightlydrops the recall, whereas the manual annotation aloneretains recall with lesser increase in precision. In thissection, we investigate the effect of event filtering usingthe set of incorrect triggers obtained from the aggregationmethod discussed in Section Aggregating unsupervisedmethod results with manual annotation results.As shown in Table 3, in comparison with the TEESperformance on ST11 and ST13, and the EVEX perfor-mance on ST13, the aggregationmethod retains the recalland increases the precision and F-score. Interestingly, inall three cases, in comparison with manual annotationmethod it has a higher precision and F-score. Conse-quently, we conclude that our unsupervised method isindeed able to find incorrect trigger words elusive to thehuman annotator.If we compare the aggregation method performancewith our unsupervised method performance, we noticethat in all three instances, it does have a higher recalland in two cases also higher F-score. In one case theunsupervised method alone reaches the highest F-score.This might be due to trigger words that we have anno-tated as correct, but are used in wrong event types by theunderlying even extraction system, thus resulting in lowerprecision.As a conclusion, while all of our methods establishnew top scores on 2013 tasks, the aggregation method isthe best among them. It retains the recall, increases theprecision and has the best F-score in two cases out ofthree.Evaluation of low-frequency trigger classificationAs stated in Section Classification of low-frequency eventtriggers, we use all 3,391 top most EVEX frequent triggersMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 10 of 13to train the classifier and aim to apply it on those triggerswith frequency below 300.Similar to the previous evaluations, we first evaluatethe classifier performance against the Shared Task testsets as an end-to-end system together with the aggrega-tion method. For this aim, we apply the trained classifierto predict labels for EVEX triggers with frequency below300. This results in identification of 16,674 negative (sup-posed to be incorrect) triggers with total frequency of232,748 respective events in EVEX. The rest of the triggerswere predicted as correct. Then, we prune the output ofevent extraction systems using these recognized incorrecttriggers and incorrect triggers obtained by aggregationmethod.Results for this experiment are shown in Table 3. Com-parison of this method (Aggregation Method + SVMentries in the table) against our aggregation method, thepreviously best approach, shows slight increase in bothprecision and F-score in all three cases while retaining thesame recall. Thus, the classifier is able to recognize somepreviously undetected incorrect trigger words, giving usthe most complete set of incorrect trigger words.As this processing step focuses specifically on low-frequency (rare) triggers, unlikely to be found in thecarefully selected Shared Task data sets, the performanceimprovement is small, as anticipated. However, we expectthe outcome to be more significant in large-scale eventextraction and to show this we conduct another evaluationbased on the EVEX resource.In the second evaluation we form an evaluation set byrandomly selecting 700 words from the triggers with fre-quency less than 300 in EVEX and use the same manualannotation procedure discussed in Section Manual anno-tation of triggers to divide them into positive (correct) andnegative (incorrect) sets.The annotation resulted in 363 correct and 233 incor-rect triggers. For 104 triggers our annotator was unableto assign a label. Even though, in terms of our annotationprotocol, the triggers are divided into three independentclasses, for simplicity we exclude the 104 undecidable trig-ger words from our test set and use only the 596 remainingwords.The performance evaluation results against the test setare shown in three different tables. Table 4 shows the counts and respective eventfrequencies of true-positives, true-negatives,false-positives and false-negatives. Table 5 shows the performance in terms ofclassification of triggers. Precision, recall andF2-score in this table are calculated based on thecounts of the predicted triggers. Table 6 shows the performance in terms ofclassification of events. Precision, recall and F2-scoreTable 4 Trigger/event classification performance, measured onthe EVEX test set: The first column (Count) shows predictionresults based on the counts of trigger words (test set examples).The second column (Sum of frequency) shows the number ofrespective events of those triggers in the EVEX database. Forinstance, the first row (True-Positive) shows that the classifier hascorrectly predicted 352 test set trigger words to be correcttriggers, while these words account for 4,602 extracted events inthe EVEX resourceCountSum of frequency(Number of events)True-Positive 352 4602True-Negative 99 679False-Positive 134 850False-Negative 11 115Total 596 6246in this table are calculated based on the eventfrequencies of the predicted triggers (i.e., based onthe sum of frequencies of TP, TN, FP and FN).As mentioned in Section Classification of low-fre-quency event triggers, from the event extraction pointof view, the event frequencies are more important thanthe unique trigger words themselves. Thus, results listedin Table 6 are the most relevant for examining the per-formance of the classifier. As this is also the evaluationmetric the classifier hyper-parameters were optimizedagainst, the numbers in Table 6 are generally higher thanin Table 5.We can see that the classifier achieves recall of 0.98for the positive class, i.e. the correct triggers as shownin Table 6. This result suggests that we have succeededin our goal of preserving as much of the true events aspossible. Besides, the classifier also reaches recall of 0.44for the incorrect triggers, i.e. we are able to detect andexclude almost half of the events with false triggers in thisevaluation set.Table 5 Trigger classification performance on the EVEX resourcebased on trigger counts (test set examples). The predictionmeasures in this table are calculated based on the values in thefirst column of Table 4. This table shows how well the classifier isable to classify and distinguish between correct and incorrecttrigger words. The last column (Support) shows that there are363 correct and 233 incorrect trigger words in the test set, i.e, 596in totalPrecision Recall F2-score SupportNegative (incorrect) 0.90 0.42 0.48 233Positive (correct) 0.72 0.97 0.91 363Weighted averages, total 0.79 0.76 0.74 596Mehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 11 of 13Table 6 Classification performance on the EVEX resource basedon the respective event counts in the EVEX database. This tableshows how well the classifier will perform the prediction,preserving correct and eliminating incorrect respective eventsfrom the EVEX database. The prediction measures in this table arecalculated based on the values in the second column of Table 4.The last column (Support) shows that there are 1,529 incorrectand 4,717 correct corresponding events in the EVEX database(6,246 in total) which are extracted based on those 596 triggerwords in the test setPrecision Recall F2-score SupportNegative (incorrect) 0.86 0.44 0.49 1529Positive (correct) 0.84 0.98 0.95 4717Weighted averages, total 0.85 0.77 0.77 6246Evaluation of event removal on the EVEX resourceIn this section we investigate the impact of removingevents from the EVEX resource based on all trigger wordsrecognized as incorrect.Even though our manual annotation or aggregationmethods are able to preserve the recall when evaluatedagainst official predictions of Shared Task test sets, itis not guaranteed that the same performance will beachieved when applying them on a large-scale resourcesuch as EVEX. In fact there might be correct triggerswhich are not present in ST11 or ST13 test sets, but aremistakenly labeled as incorrect by the human annotator,our unsupervised method or the classifier. Consequently,in the evaluation against official Shared Task test sets,we do not delete these triggers and do not detect anydrop in recall. However based on our evaluation results,we are optimistic that most of the correct events will bepreserved if the method is applied on the EVEX resource.To investigate the impact of event removal on EVEX,for top most frequent triggers (accounting for 97.1% ofall EVEX events), we rely on our aggregation methodwhich had the best performance. The aggregation methodresulted in labeling 1,149 triggers as incorrect and theseaccount for 1,105,327 events in EVEX.For the rest of EVEX triggers (low frequency triggersaccounting for 2.9% of all EVEX events), we use theclassifier. However, the classifier could not be applied to48,960 triggers with 122,344 respective events (0.3% of allEVEX events). These words have less than 5 occurrencesin the corpus used for training the word2vec model, andthus do not have a corresponding vector representation,required by the classifier. Applying the classifier on therest of low frequency triggers (accounting for 2.6% of allEVEX events) resulted in identification of 16,674 incorrecttriggers with 232,748 events in EVEX.Consequently, in total we have been able to identify17,823 expected to be incorrect triggers in the wholeEVEX resource with 1,338,075 events which constitutes3.3% of all events in EVEX.Tree organization before/after pruningIn this section we address two questions. First, how theresulting binary cluster tree differs before and after thepruning, and second, whether we can define new eventsubtypes based on the organization of sub-clusters in dif-ferent branches of the tree. For these aims, we visualize thetree before and after pruning up to the depth of 9 usingthe Dendroscope software [22]. We label every intermedi-ate node of the tree with its mostly associated event typeand the level of purity of that sub-cluster (see Additionalfile 1 for diagrams of the tree).As expected, in both trees we notice that trigger wordsof same event types are clustered together, to someextent. By considering the length of the shortest pathin tree as a basic distant measure, we observe that sub-clusters of similar/related event types are closer in thetree, while sub-clusters of different event types are locatedfar. For instance, triggers for expressing different typesof post-translational modifications events (e.g., phos-phorylation, DNA-methylation, glycosylation, acety-lation) are clustered together, far from trigger wordsfor expressing positive/negative regulation or binding.Similarly, sub-clusters of gene-expression, transcrip-tion and localization trigger event types are close in thetree.We observe that before pruning the tree, sub-clustersare not pure. For example, many trigger words for pos-itive regulation events are often clustered together withthe ones for negative regulation events. By removing thesub-clusters of purely incorrect triggers, i.e. pruning, thesub-clusters in the middle levels of the tree become purerand are for the most part, associated with the same eventtype which signifies the possibility of identifying some ofthe event subtypes.We thus continue the analysis on the associated eventsin the sub-tree anticipating to recognize the patterns.However, to our surprise, there is no clear signal in thesub-clusters that would signify any of the subtypes. As aresult, we thus do not pursue further analysis on the trees.To conclude, our pruning algorithm yields a meaningfultree which can distinguish different event types into sub-clusters, however, the resulting clusters could not be usedto identify event subtypes.ConclusionsIn this paper, we propose a method which can be usedfor identification of incorrect trigger words and remov-ing incorrect events from the output of large-scale eventextraction systems.Our unsupervised method achieves a modest improve-ment over the winning system of the BioNLP 2013 SharedTask on GENIA event extraction and establishes a newtop score on the task. The aggregation of manual annota-tion results with our unsupervisedmethod results, furtherincreases the precision and F-score of the unsupervisedMehryary et al. Journal of Biomedical Semantics  (2016) 7:27 Page 12 of 13method. Besides, the unsupervised method decreases theoriginal recall when evaluated against official predictionsof Shared Task test sets, while our aggregation methodretains it.Because the highly demanding manual annotation is notpossible for all EVEX trigger words, we build a SVMclassi-fier for predicting incorrect triggers among low-frequencyEVEX triggers. While having 0.98 positive recall whichtranslates to preserving a huge proportion of correctevents, the classifier has 0.44 negative recall, meaning thatit is able to identify about half of the incorrect events.Combining the results of our aggregation method withincorrect trigger words identified by applying the classifieron all low frequency EVEX triggers, results in recog-nition of 17,823 expected to be incorrect triggers with1,338,075 respective events which constitutes about 3.3%of all events in EVEX resource.In this paper we have only discussed the identificationof the incorrect triggers and the outcome of removingthese triggers from a large-scale event resource. Althoughour evaluation shows only minimal drop in recall, bluntlyremoving the corresponding events might have unwantedeffects. As the EVEX resource ranks the events shownto the users based on a scoring system derived fromthe TEES classification confidence, we would thus as afuture work like to investigate how to incorporate thesenew findings in the ranking. This would let us, insteadof completely abolishing the likely incorrect events, onlyto decrease their scoring and conserve them for thoseuse cases that demand extremely high recall, but canovercome the noise in the data.Another direction is to investigate the different eventtypes in more detail. We hope this study will give us abetter insight of whether the method can be adapted toalso correctmistyped events, thus increasing the precisioneven further. For instance, it is possible that a detectedregulation trigger should in fact be classified as positive-regulation, a subtype of regulation, but the used triggerdetector has not been able to make this distinction. Byobserving how the given trigger word is located in thehierarchical cluster tree, these errors could be possiblycorrected.As the distributional semantics research is progress-ing towards better representations of phrases and largertext sections in addition to word-level embeddings, itmight be possible in the future to instead of judgingthe trigger words globally, to focus only on certain typesof contexts giving us the ability to make more precisedecisions.Availability of supporting dataThe source code and data sets supporting the resultsof this article are available at the Turku BioNLP groupwebsite at: http://bionlp-www.utu.fi/trigger-clustering/.Additional fileAdditional file 1: This tar file contains images of the binary cluster tree,before and after the pruning. The HowToInterpretTreeDiagrams.txt filedescribes how the diagrams should be interpreted. (TAR 1290 kb)Competing interestsThe authors declare that they have no competing interests.Authors contributionsThe method is designed, fine-tuned, and implemented by FM, with input fromall authors. SK did all manual annotations, while KH prepared an annotationframework. All authors equally contributed in analyzing the results anddrafting the manuscript. The work is carried out under the supervision of FG.All authors read and approved the final manuscript.AcknowledgmentsWe would like to thank Sofie Van Landeghem, Ghent University, for initiatingthe ideas for this project and her valuable suggestions. We also kindly thankJari Björne, University of Turku, for his help during the project.Author details1Department of Information Technology, University of Turku, Turku, Finland.2The University of Turku Graduate School (UTUGS), University of Turku, Turku,Finland. 3Turku Centre for Computer Science (TUCS), Turku, Finland.Received: 28 February 2015 Accepted: 1 May 2016RESEARCH Open AccessThe Apollo Structured Vocabulary: an OWL2ontology of phenomena in infectiousdisease epidemiology and populationbiology for use in epidemic simulationWilliam R. Hogan1*, Michael M. Wagner2, Mathias Brochhausen3, John Levander4, Shawn T. Brown5,Nicholas Millett6, Jay DePasse5 and Josh Hanna7AbstractBackground: We developed the Apollo Structured Vocabulary (Apollo-SV)an OWL2 ontology of phenomena ininfectious disease epidemiology and population biologyas part of a project whose goal is to increase the use ofepidemic simulators in public health practice. Apollo-SV defines a terminology for use in simulator configuration.Apollo-SV is the product of an ontological analysis of the domain of infectious disease epidemiology, with particularattention to the inputs and outputs of nine simulators.Results: Apollo-SV contains 802 classes for representing the inputs and outputs of simulators, of whichapproximately half are new and half are imported from existing ontologies. The most important Apollo-SV class forusers of simulators is infectious disease scenario, which is a representation of an ecosystem at simulator time zerothat has at least one infection process (a class) affecting at least one population (also a class). Other importantclasses represent ecosystem elements (e.g., households), ecosystem processes (e.g., infection acquisition andinfectious disease), censuses of ecosystem elements (e.g., censuses of populations), and infectious disease controlmeasures.In the larger project, which created an end-user application that can send the same infectious disease scenario tomultiple simulators, Apollo-SV serves as the controlled terminology and strongly influences the design of the messagesyntax used to represent an infectious disease scenario. As we added simulators for different pathogens (e.g., malariaand dengue), the core classes of Apollo-SV have remained stable, suggesting that our conceptualization of theinformation required by simulators is sound.Despite adhering to the OBO Foundry principle of orthogonality, we could not reuse Infectious Disease Ontologyclasses as the basis for infectious disease scenarios. We thus defined new classes in Apollo-SV for host, pathogen,infection, infectious disease, colonization, and infection acquisition. Unlike IDO, our ontological analysis extended toexisting mathematical models of key biological phenomena studied by infectious disease epidemiology andpopulation biology.(Continued on next page)* Correspondence: hoganwr@ufl.edu1University of Florida, P.O. Box 100219, 2004 Mowry Rd, Gainesville, FL32610-0219, USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 DOI 10.1186/s13326-016-0092-y(Continued from previous page)Conclusion: Our ontological analysis as expressed in Apollo-SV was instrumental in developing a simulator-independent representation of infectious disease scenarios that can be run on multiple epidemic simulators. Ourexperience suggests the importance of extending ontological analysis of a domain to include existing mathematicalmodels of the phenomena studied by the domain. Apollo-SV is freely available at: http://purl.obolibrary.org/obo/apollo_sv.owl.Keywords: Disease transmission model, Epidemic simulator, Epidemic simulation, Biomedical ontology, Infectiousdisease epidemiology, Population biology, InfectionAbbreviations: Apollo-SV, Apollo structured vocabulary; BFO, Basic formal ontology; DTM, Disease transmission model;EO, Epidemiology ontology; GO, Gene ontology; IDO, Infectious disease ontology; MIREOT, Minimum information toreference an external ontology term; OBO, Open biological and biomedical Ontologies; OGMS, Ontology for generalmedical science; OWL 2, Web ontology language version 2; OWL DL, OWL description logic; PURL, Permanent Uniformresource locator; SimPHO, Simulation modeling of population health ontology; UAL, Unique apollo label;XML, eXtensible markup language; XSD, XML schema documentBackgroundThe science and practice of infectious disease epidemi-ology, like climate science, is increasingly reliant oncomputational simulation [1], which is performed bysoftware applications known as epidemic simulators.The simulators require information about pathogens,host populations, rates of infection transmission, inter-ventions, and the disease outcomes of infections [2].Using this configuration informationwhich we referto as an infectious disease scenarioa simulatorsalgorithm computes the progression of one or moreinfections in one or more populations over time, underzero or more interventions. The result of this computa-tionthe output of the simulatoris information onwhich decision makers can base policy or decisionsabout disease control.The goal of our research for the past 4 years has beento increase the accessibility and ease of use of simulatorsto promote progress in the field of infectious disease epi-demiology [3]. A key focus has been reducing the timeand effort required to locate a simulator, access it,understand its characteristics, create an infectious dis-ease scenario to configure it, run it, and analyze its out-put. As an example of the effort required, Halloran et al.spent 6 months creating a comparative study of threesimulators [4]. Most of the effort was expended onrepresenting the same scenario in the different configur-ation representations and then converting results into acommon representation for comparisons. As an exampleof the syntatic and semantic differences among simula-tor configurations, to configure the FRED simulator ver-sion 2.0.1 [5] to simulate the closing of schools1 3 daysafter some event occurs (such as influenza incidencereaching a particular threshold) one would placeschool_closure_delay = 3 in its configuration file,whereas for FluTE version 1.15 [6] one would placeresponsedelay = 3 in its configuration file (unlikeFRED, this setting would also affect other interventionssuch as vaccination).To address this problem, we are developing a commonrepresentation for simulator configuration and output thatis capable of representing the configurations and outputof infectious disease simulators [3]. We use an XMLSchema Document (XSD) as our primary representationbecause the XSD language enabled us to represent theprobabilistic, mathematical, and other non-ontologicalknowledge required for and generated by simulation. Weinform the design of the XSD representation by formalontological analysis of the domain of infectious diseaseepidemiology, with particular attention to the inputs andoutputs of nine simulators. Our goal was for the XSD tohave the capability to represent the configuration and out-puts of not only these nine simulators, but also otherexisting and future simulators. We represent the results ofthis analysis in an OWL ontologycalled the ApolloStructured Vocabulary or Apollo-SV.Apollo-SV and XSD together can be understood as ahybrid approach to knowledge representation andreasoning as defined by Davis et al. in their seminalpaper on knowledge representation [7]. In particular,Apollo-SV (1) controls the terminology used in the XSD,(2) is a source of human-readable definitions of theterms for users of the XSD, and (3) serves as a record ofthe ontological commitments made by the developers ofthe XSD.Our hypothesis was that it is feasible to develop acommon representation for the configuration and out-put of simulators that are diverse both in their internalrepresentations and in the pathogens, modes of trans-mission, geography, and interventions that they model.We previously reported our initial versions of theXSD and Apollo-SV (versions 1.0), as well as ourcreation of a set of Web services to transmit a com-mon configuration to two simulators [3]. We useHogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 2 of 12configurations compliant with the XSD to invoke sim-ulators as part of these Web Services, but generatedthe OWL2 representationApollo-SVas our coreontology.In this paper, we describe new results from our subse-quent ontological analyses of additional simulators andour updated understanding of simulator configurationsthat we incorporated into Apollo-SV version 3.0.1.MethodsOur method for the development of the common repre-sentation was formal ontological analysis with rapidimplementation of the representation to configure simu-lators and feedback from the results of implementationinto further analysis.The next sections discuss our style of ontologydevelopment, the application in which the ontology isused, and the procedures and principles we followed inconstructing the OWL ontology, Apollo-SV.Gene Ontology Style of ontology developmentWe developed Apollo-SV using what we refer to as theGene Ontology style of ontology development andtestingor GO style for short. GO style is a method forontology development that emphasizes participation ofsubject matter experts and frequent and early feedback toontology developers generated from using the ontology insoftware applications. We adopted GO style because it wassuccessful for the Gene Ontology and because ourcommunity of developers and users was similar in manyrespects.A key strength of GO stylewhich the Gene OntologyConsortium cites as a factor in its successis that a com-munity of scientists, ontologists, artificial intelligenceexperts, and software developers all contribute in an egali-tarian fashion to the ontology and its applications [8]. Theteam developing Apollo-SV comprises experts in infec-tious disease epidemiology, simulator and other softwaredevelopment, disease surveillance, medicine, biomedicalinformatics, medical terminologies, ontological engineer-ing, artificial intelligence, and formal logic (the last one inthe list helps to ensure that OWL2 axioms that defineclasses are correct). All these individuals have beenactively engaged in development and review of Apollo-SV,and their feedback guides design decisions.A second strength of the GO style of ontology devel-opment is its emphasis on early use of the ontology inapplications, which identifies issues and generates rapidfeed back into ontology development [9]. We discuss theapplication of Apollo-SV in the next section. Additionalelements of the style, that have subsequently beenadopted by the Open Biological and Biomedical Ontol-ogies (OBO) Foundry as principles of ontology develop-ment, include creating textual definitions for each classand making the ontology publicly and freely available forcommunity use, review, and input [810]. We discusshow we implemented these additional elements of thestyle, as well as additional OBO Foundry principles, inthe section following application.The application in which the ontology is usedAs stated previously, Apollo-SV serves as the repositoryfor definitions and standard terminology for the ApolloXSD. The Apollo XSD in turn is used in a set of Webservices.The Web services, called the Apollo Web Services,allow a publicly available, Web-based, end-user applica-tion to access multiple epidemic simulators throughrequests to a single Broker service (Fig. 1). In Fig. 1, theSimple End User Application (SEUA) [11] creates aninfectious disease scenario for simulation, encoded in anXML document that conforms to the Apollo XSD syntax[12], which in turn uses terminology defined by Apollo-SV. The SEUA invokes the runSimulation() method ofthe Broker service with the XML-encoded infectious dis-ease scenario. The Broker service subsequently invokesthe Translator service, which translates the infectiousdisease scenario into the native terminology and syntaxof the requested simulator(s). The SEUA polls theBroker service for the current status of the simulatoruntil the status returned is COMPLETED. The SEUAthen invokes various visualization services on the simu-lator output to display epidemic curves and maps in theinterface.By standardizing the terminology in the Web services,Apollo-SV helps to ensure that the SEUA end user andthe simulators understand the XML-encoded infectiousdisease scenario to mean the same thing. Towards thatend, the SEUA displays the textual definitions of classesin Apollo-SV to help the end user specify her infectiousdisease scenario accurately and precisely.Beginning with the earliest development of Apollo-SV,exposing the terminology and definitions from Apollo-SVto subject matter experts, developers, and others in theSEUA was a significant source of critical feedback that ledto additional ontological analysis as well as refinements ofthe terminology and definitions.Procedures and principles of Apollo-SV constructionWe encode the results of our ontological analyses inOWL2. Our process proceeds concurrently with devel-opment of the Apollo XSD, and issues discovered inconstructing either the OWL or the XSD are fed backinto the analysis.We conducted a formal ontological analysis of sevenadditional simulatorstheir configuration files, outputfiles, documentation (including any user guides), andjournal and conference papers that either described orHogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 3 of 12used them. As part of this process, we reviewed termsthat we extracted from these sources with the developersof the simulators to identify relevant but missing terms,to discover synonymy among terms, and to detect andresolve ambiguity. Of the seven additional simulators,four are presently connected to the Apollo WebServices.We wrote a textual definition for every class that wecreate, in keeping with the GO style and OBO Foundryprinciples. We also created an elucidation annotation forclasses in Apollo-SV because formal ontological textualdefinitions are sometimes not accessible to domain ex-perts. The elucidation restates the definition in languagemore familiar to subject matter experts, while still refer-ring to the same type of entities as the definition.Also in accordance with the GO style of ontology de-velopment, we made Apollo-SV publicly available at[13], a permanent URL (PURL), to allow externalscientific review, comments, and requests for additionsas well as to encourage adoption of Apollo-SV. We en-sured that Apollo-SV is easily accessible for browsingand download at the Web-based Ontobee portal [14],analogous to Gene Ontology browsers (the GO itself isviewable on Ontobee). The issue tracker is located atthe Apollo GitHub site [15]. The PURL to the develop-ment version of Apollo-SV is at [16].Because the Gene Ontology has full membership sta-tus in the OBO Foundrya special status conferred onontologies that conform to the OBO Foundry principles,we also followed the principles of the OBO Foundry inaddition to openness and textual definitions [17, 18]. Perthose principles, we release it in a common format,OWL2 [19].We also adopted the Foundry principle of orthogonal-ity, which stipulates that ontology developers reuse pre-existing ontological representations into Apollo-SVwhen and where appropriate.We employed two methods for ontology reuse. Thefirst method is the OWL2 ontology-import mechanism.This method inserts into the target ontology all classesand object properties of the imported ontology. How-ever, bulk inclusion of large ontologies is often impracti-cal and can degrade the usability of the target ontology.Therefore, the second method we used is the MinimumInformation to Reference an External Ontology Term(MIREOT) methodology [20]. Using a MIREOT Protégéplugin that we developed [21], we import selected clas-ses, individuals, and properties from certain ontologiesinto Apollo-SV.We hypothesized that we would be able to reuse pre-existing ontologies or significant portions of them in de-veloping Apollo-SV. In particular, we anticipated reusingsubstantial portions of the Infectious Disease Ontology(IDO) [22]. IDO is an OBO ontology (but not a fullmember of the Foundry) that represents infections,infectious diseases, pathogens, and hosts from the per-spectives of infectious disease as a medical subspecialtyand infectious disease research.We adhered to OBO Foundry naming conventions[23]. We edited our terms to (1) avoid connectives (and,or), (2) prefer singular nouns, (3) avoid the use of nega-tions, and (4) avoid catch-all terms such as Unknown x.Fig. 1 The relationships of Apollo components and epidemic simulators. Apollo-SV defines the terminology used in Apollo XSD, which specifies themessage syntax for the Web services. The SEUA calls the Broker service to configure simulators (messages passed along blue arrows) and to accesssimulator output (messages passed along red arrows). The Translator service translates Apollo messages to/from native simulator input/output. Purpleovals represent Apollo standards; blue ovals represent Apollo-developed software that use the Apollo Web services; and red ovals represent entitiesinteracting with ApolloHogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 4 of 12To help link the OWL file to the XSD, we created aUnique Apollo Label (UAL) annotation for classes inApollo-SV. The UAL is the exact XSD type or elementname to which the class in Apollo-SV corresponds, for ex-ample, InfectiousDisease and BasicReproductionNumber.Although not required by OBO Foundry principles, weimported Basic Formal Ontology (BFO) version 1.1 [24]into Apollo-SV as its upper ontology as do many otherFoundry ontologies. The main reasons were (1) to main-tain the semantics of BFO-based ontologies and theircomponents that we reused and (2) to ensure that newclasses and their associated axioms in Apollo-SV did notintroduce inconsistencies to those semantics.We created description logic axioms according to thesyntax and semantics inherent in OWL2 for classes inApollo-SV (e.g., Figs. 2,3, 4 and 5). When possible, theseaxioms provide both necessary and sufficient criteria forclass membership. Many axioms, however, define onlynecessary criteria, most often because the descriptionlogic semantics of OWL2 were insufficiently expressiveto encode both the necessary and sufficient criteria ofthe class.ResultsApollo-SV version 3.0.1 comprises 868 classes, of which802 were required for describing simulator configurationand output. The remaining 66 classes are extraneousimported classes resulting from OWL2-based imports ofontologies in toto. Of the 802 classes, we created 397(49.5 %) new classes, of which 117 classes have necessaryand sufficient criteria. We imported 118 (14.7 %) classesvia the methodology of Minimum Information to Refer-ence and External Ontology Term or MIREOT (Table 1),and imported 287 (35.8 %) via OWL2-based import. Theontology comprises a total of 1180 logical axioms.High level classes in Apollo-SVThe most important Apollo-SV class for users of sim-ulators is infectious disease scenario, which representsan ecosystem at simulator time zero with at least oneinfection process (a class) affecting at least one popu-lation (also a class). The infectious disease scenarioincludes information about the infection process andits acquisition by a host organism (e.g., transmissionprobabilities and the durations of infectious and latentperiods). It can also include information aboutplanned or ongoing interventions to control infection(such as vaccination control measures). Representingecosystems, populations, and censuses thus expandedthe scope of Apollo-SV to population biology(Table 2). Including population biology subsequentlyinfluenced our definitions of key terms in infectiousdisease epidemiology.Fig. 2 Representation of the equivalent class axiom for infection in Apollo-SV. Boxes represent named classes, boxes with curved bases representanonymous classes, arrows represent object properties. In the boxes is the rdfs:label and the namespace of the source ontology, if different fromApollo-SV. Each arrow is labeled with the rdfs:label of the property it representsHogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 5 of 12Classes representing the infections, infection acquisi-tions, hosts, pathogens, and infectious diseases in anecosystem are foundational in Apollo-SV. The reason isthat the essential prediction of simulators is how manyinfections will occur given an infectious disease scenario.Nearly everything else that simulators predict are eventsthat revolve around infection. They either (1) occurdownstream of infection (such as disease outcomes in-cluding symptoms and death), (2) influence the proba-bilty of acquiring an infection (such as going to work orschool or being vaccinated), or (3) occur as part of an in-fectious disease control strategy to prevent infection ac-quisition (such as school closure or quarantine). Also,because one simulator that we analyzed predictscolonization of hosts by pathogens and the processes bywhich hosts acquire colonizations, it was also importantto represent colonization and how it differs from infec-tion (see below).Foundational classes where reuse of IDO was not possibleWe now describe a set of foundational classes we cre-ated in Apollo-SV after attempting unsuccessfully to re-use IDO classes and their definitions. We also discussthe reasons why these classes and definitions wereunworkable.InfectionApollo-SV defines infection as: A reproduction of apathogen organism of a particular biological taxon in atissue of a host organism from another taxon (Fig. 2).From the perspective of population biology, an infectionis merely a process by which one species reproduces,surviving from generation to generation, utilizing the re-sources of a host species. It is the normal biology of thepathogen species.Infection is distinguished from other types of pathogenreproduction in a hostnamely colonization (definedbelow)by violation of the integrity of tissue in the hostthrough tissue invasion. This tissue invasion mayoccurand subsequently endwithout causing anysymptoms or permanent ill effects on the host. Thus, in-fection does not equate to disease, and we carefully dis-tinguish between infection and infectious disease.Epidemic simulators represent infection as a processbecause infectious disease epidemiologists define infec-tion as a process. For example, [25, 26] define infectionas the invasion of a host organisms tissue by pathogens,the multiplication of those pathogens, and the reactionof the hosts tissue(s) to the pathogens and the toxinsthey produce. Further reinforcing the fact that infectionis a process is the fact that simulators represent periodsFig. 3 Representation of the equivalent class axiom for host in Apollo-SV. The graphical representation is analogous to Fig. 2Fig. 4 Representation of the equivalent class axiom for pathogen in Apollo-SV. The graphical representation is analogous to Fig. 2Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 6 of 12of (or ontologically speaking, occurrent parts of ) the in-fection: the latent period and the infectious period.Before we created a class for infection in Apollo-SV,we reviewed IDO for a class that represents the processof infection, whether labeled as infection or with someother term.We found that IDO defines infection as a physical thing,or material entity in the terminology of Basic FormalOntology (BFO). Specifically, it defines infection as: A partof an extended organism that itself has as part a popula-tion of one or more infectious agents and that is (1) clinic-ally abnormal in virtue of the presence of this infectiousagent population, or (2) has a disposition to bring clinicalabnormality to immunocompetent organisms of the sameSpecies [sic] as the host (the organism corresponding to theextended organism) through transmission of a member oroffspring of a member of the infectious agent population.Given that epidemic simulators and the relevant basicsciences on which they are founded recognize infection asa process, we needed to create a new class in Apollo-SVto represent it. The lack of a representation of the processof infection in IDO is surprising because IDOs definitionsof its classes host role and infectious agent role require aprocess to realize them. This process would presumablybe infection.ColonizationApollo-SV defines colonization as: A reproduction of apathogen of a particular biological taxon inside or onthe surface (e.g., skin, mucosal membrane) of a hostorganism of another taxon, without invasion of anytissues of the host. We required this class to representthe input of the Regional Healthcare Ecosystem Analyst[27] simulator, which models the spread of methicillin-resistant Staphylococcus aureus (MRSA). MRSA, as wellas methicillin-sensitive varieties of S. aureus, typicallycolonize the nasal mucosa and skin of humans, livingon these surfaces but not invading them. If a humanFig. 5 Representation of the equivalent class axiom for infectiousdisease in Apollo-SV. The graphical representation is analogousto Fig. 2Table 1 Re-use of classes and object properties frompre-existing ontologies in Apollo-SV via MIREOTOntology Classes ObjectPropertiesTotalUberon 7 1 8Ontology of Medically Related SocialEntities26 7 33Gene Ontology 13 0 13Ontology for General Medical Science 11 0 11Ontology of Biomedical Investigations 21 6 27Infectious Disease Ontology 3 7 10The Drug Ontology 1 0 1FlyBase Controlled Vocabulary 2 0 2Vaccine Ontology 4 0 4Drug-drug Interaction EvidenceOntology1 0 1Unit Ontology 5 0 5Phenotypic Quality Ontology 3 0 3Totals 97 21 118Table 2 Classes in Apollo-SV by domainDomain Classes in Apollo-SVInfectious diseaseepidemiologyInfection Infection acquisitionPathogen HostLatent period Infectious periodContaminated thing ContaminationacquisitionContaminationInfectious disease scenario Basic reproductionnumberTransmission coefficient TransmissionprobabilityDisease transmissionmodelInfectious diseasecontrol strategySusceptible population Exposed populationInfectious population Resistant populationPopulation biology Ecosystem Biotic ecosystemAbiotic ecosystem CommunityPopulation Population censusPopulation infection andimmunity censusAbiotic ecosystemcensusHogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 7 of 12host subsequently becomes immunocompromised orsuffers a breach of the integrity of these surfaces, thiscolonization may extend to infection. Colonization isan important epidemiological process because an indi-vidual may acquire colonization from another MRSAcolonized host.IDO defines colonization as An establishment oflocalization in host process in which an organism estab-lishes itself in a host. The latter part of the definition ismore general than the former (assuming that there areother types of establishment besides localization) andthus does not differentiate this IDO class from its parentin IDO. We did not consider it further.HostApollo-SV defines host as: An organism of a particu-lar biological taxon that is the site of reproduction ofan organism of a different taxon (Fig. 3). This defin-ition accomodates the host undergoing infection and/or colonization. We note that our use of site of inthis definition has a precise meaning as specified inthe Relation Ontology, where site of is a synonym forthe contains process relation, which relates an inde-pendent continuant and a process, in which theprocess takes place entirely within the independentcontinuant.We could not reuse IDOs definition of host, which is:An organism bearing a host role. To understand thisIDO definition, it is necessary to review two additionalIDO definitions:1. Host role: A role borne by an organism in virtue ofthe fact that its extended organism contains amaterial entity other than the organism.2. Extended organism: An object aggregate consisting ofan organism and all material entities located withinthe organism, overlapping the organism, or occupyingsites formed in part by the organism.Under these definitions, any organism that has an arti-ficial joint, a penny in its gut, or an arrow through itschest is a host. Classifying a person with a prostheticknee as a host is counterintuitive and not in keepingwith how host is defined in population biology or infec-tious disease epidemiology (or in clinical medicine). Fur-thermore, the definition is based on IDOs view ofinfection as a material entity and does not account forthe process of infection.PathogenApollo-SV defines pathogen as: An organism of a particularbiological taxon that is the bearer of a disposition that isrealized as its reproduction in the tissue of an organism of adifferent biological taxon (Fig. 4). Thus Apollo-SV defines apathogen as an organism that has the capability to repro-duce inside the tissue of a host organism of anotherbiological taxon. Note that this definition is inclusive oforganisms like MRSA involved in colonization: the organ-ism still has the potential to invade tissue and establishinfection and thus meets the definition.Once again, we had intended to reuse IDO. However,IDO defines pathogen as: A material entity with a patho-genic disposition. Again, this definition requires add-itional IDO definitions to clarify its meaning:1. Pathogenic disposition: A disposition to initiateprocesses that result in a disorder.2. Disorder: A material entity which is clinicallyabnormal and part of an extended organism.Disorders are the physical basis of disease.Thus, per IDO any material that causes injury is apathogen, including the endotoxin of Clostridium diffi-cile or an overdose of acetaminophen. This definition isnot how infectious disease epidemiology uses the termpathogen. IDO does have a class infectious agent as asubtype to pathogen that refers specifically to organismsthat can enter into a host and cause disease. The IDOdefinition of infectious agent, however, relies on IDOsdefinitions of infection and infectious disorder as materialentities. To be consistent with infection as a process, wecreated the above definition of pathogen in Apollo-SV.Infectious DiseaseApollo-SV defines infectious disease as: A disease thatinheres in a host and is realized as a disease course thatis causally preceded by an infection (Fig. 5). This meansthat the infection occurs first and creates abnormalitiesin the host that result in disease.This definition is compatible with the OBO Foundrydefinition of disease in the Ontology of General MedicalScience (OGMS) [28]. We thus were able to reuse theOGMS definition of disease, in keeping with the Foun-dry principle of orthogonality. Note that the diseaseinheres only in the host. From the pathogens perspec-tive, there is no clinical abnormality (which is a neces-sary condition to meet the definition of disease inOGMS) as infection is normal biology of pathogens.IDOs definition of infectious disease is incompatiblewith our definition of infection as process.Infection AcquisitionApollo-SV defines infection acquisition as: The biologicalprocess of a pathogen of a particular biological taxon en-tering (the tissues of the body of ) a susceptible host or-ganism of another taxon and reproducing using hostresources. A susceptible host can acquire an infectionfrom one of at least three routes:Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 8 of 121. From another host organism (of the same ordifferent species) that is infectious, which werepresent in Apollo-SV as the class Infectionacquisition from infectious host.2. From some object or its surface that is contaminatedwith the pathogen, which we represent in Apollo-SVas the class Infection acquisition from contaminatedthing.3. From self colonization with the pathogen, which werepresent in Apollo-SV as the class Infection acquisi-tion from self colonization.Note that we chose to define infection acquistion in-stead of transmission or transmission process. One rea-son was our insight that ontologically it is only thesecond, susceptible host that undergoes change duringthe process, and the term infection acquisition describesthis change better than the term transmission. Anotherreason is that we needed to represent the acquisition ofinfections from contaminated things and from self-colonization with a pathogen. In both cases, transmis-sion from host to host is indirect (mediated throughcontaminated surfaces and objects and through acquis-tion of colonization, respectively).As with other key terms, IDO lacked an adequate classand definition for the process of infection acquisition.IDO imports transmission process and its two definitionsfrom the Transmission Ontology:1. A process that is the means during which thepathogen is transmitted directly or indirectly from itsnatural reservoir, a susceptible host or source to anew host.2. Suggested definition: A process by which a pathogenpasses from one host organism to a second hostorganism of the same Species [sic].Beginning with the second definition (which for somereason the Transmission Ontology labels as a suggesteddefinition), it erroneously restricts transmission tooccur only between two hosts of the same species. It isthus not usable in infectious disease epidemiology orany other science that studies cross-species transmission,which frequently occurs in zoonoses and diseases likefoot and mouth disease.The first definition has two major problems. The firstproblem is circularity, defining transmission process interms of a pathogen being transmitted, with no defin-ition of transmitted. The definition also excludes infec-tion acquisitions from contaminated objects and selfcolonization and refers to the undefined terms naturalreservoir and source.The second problem is an ontological one. It attributesto one process the property of being the means by whichsomething else happens. For example, assume dropletspread of infection from one host to another by asneeze. This definition equates the sneeze with thetransmission process. That is, it says that only the sneezeexists, but it also has the property of having transmittedthe pathogen. However, equating the sneeze to thetransmission process is nonsensical because for example,droplets can remain airborne and infectious for hours.Thus the pathogen may not reach (or be transmitted to)another host until long after the sneeze is over. Thesneeze cannot therefore be the transmission process. Inreality, there are two distinct processes: the sneeze andthe subsequent acquisition of an infection by the secondhost.Testing Apollo-SV and its ontological commitments insoftwareWe created a capability to configure six simulators: usingthe SEUA, an end user creates an infectious disease sce-nario that conforms to the XSD and then submits it to thesimulators via Web services. The SEUA then retrieves theoutput of the simulators and displays it on maps andgraphs. This capability was the end product of iterative,concurrent development of Apollo-SV and the XSD ac-cording to our analysis of the simulators, which includedfeedback from implementation in the Web services andSEUA. In addition, the SEUA displays textual definitionsof Apollo-SV classes to the end user. Feedback on thesedefinitions was fed back into ontology development whichresulted in ontology changes including improved defini-tions. We are piloting a 7th simulator whose uniqueontological commitments are reflected in Apollo-SV andthe XSD, but are still undergoing refinement. The sixconfigurable simulators are (1) a compartmental modeldeveloped by authors MMW, NEM, and JDL (diseaseagnostic); (2) the FRED model developed by the Universityof Pittsburgh Public Health Dynamics Laboratory incollaboration with the Pittsburgh Supercomputing CenterPublic Health Applications group and the School ofComputer Science at Carnegie Mellon University (influ-enza A in humans); (3) the FluTE model developed by theUniversity of Washington and Fred Hutchinson CancerResearch Center in Seattle (influenza A in humans), (4) acompartmental model of anthrax developed by authorsMMW, NEM, and JDL, (5) the Computational ArthopodAgents (CLARA) dengue model developed by thePittsburgh Supercomputing Center Public Health Applica-tions group [29], and (6) an ebola model by Bellan et al.[30] These simulators are diverse in terms of underlyingmodel (compartment vs. agent-based), disease (influenza,anthrax, ebola, and dengue), transmission (vector and per-son to person), and geography, both in terms of granular-ity (tract vs. county vs. entire nation) and scale (from asingle state or nation to the entire globe).Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 9 of 12DiscussionWe developed and implemented a common representa-tion for simulator configuration and output and used itin an application that constructs and sends infectiousdisease scenarios to six different epidemic simulators.Our success in representing the inputs of a diverse sam-ple of simulators lends support to our hypothesis that acommon representation is feasible. Early usage of theontology and exposure of its definitions to subject mat-ter experts in software resulted in ontology improve-ments, most notably in the definitions of the coreclasses of Apollo-SV that we discussed here. This resultis consistent with those of other ontology developmentefforts.The ontological analysis we used to create the com-mon representation identified abstractions that spannedsimulators diverse in their core mathematical founda-tions (compartmental vs. agent based), pathogens, routesof transmission, geographical scope (single city orcounty vs. entire world), and interventions. The keyabstractions were that the input of a simulator was aninfectious disease scenario and that the scenario wasproperly understood as a representation of an ecosystemat a particular time, which corresponded to simulatortime zero. We note that there is nothing specific toinfectious disease in this conceptualization, whichsuggests that the ontology could be applied to simula-tion of other ecological phenomena.A novel aspect of our method was its focus on theontological analysis of epidemic simulators. This focusquickly brought into view the key biological phenomenabeing simulated and their fundamental nature. Addition-ally, simulatorsbeing mathematical modelsmakeexplicit ontological commitments about the core entitiesinvolved in infections and their acquisition, which led usto confront the issues involved in representing themfrom the outset. It is worth noting that simulators usedin epidemiology are often rigorously vetted through peerreview of simulator-based research, as well as peer re-view of the simulators themselves. A final advantage ofour focus on simulators is that they make a relativelysmall number of ontological commitments, whichallowed us to devote sufficient time to them, while stillbeing able to implement an application that continouslytested whether the evolving representation could config-ure an expanding set of simulators. We expect thatontological analysis of any domain for which mathemat-ical models exist would benefit from a focus on themodels. For example, for human physiology there is anextensive library of mathematical models that are thefocus of the Human Physiome project [31].Prior work on the use of ontologies for modeling andsimulation identified a distinction between so-calledreferential and methodological ontologies [32]. Theformer correspond with domain ontologies: a represen-tation of the phenomena simulated. The latter corres-pond with application ontologies: a representation ofsimulators, how they work, and parameters that specifytheir operations. Apollo-SV is both a domain (a.k.a. ref-erential) and an application (a.k.a. methodological)ontology in the field of infectious disease epidemiology.We were surprised that we were unable to reuse clas-ses from IDO for infection, pathogen, host, colonization,infectious disease, and transmission process. We conjec-ture that IDOs ontological analysis may have begun witha disease focus and worked from there to the nature ofinfection, whereas we began with a biological scienceperspective. Our focus differed fundamentally fromIDOs concentration on how the terms are used in clin-ical medicine. In particular, our focus led us to a require-ment to represent the process of infection, including keyparts of this process such as the infectious period, as op-posed to the steady-state, material-entity view of IDO.We note however that our definitions of infection,pathogen, host, and infectious disease do not conflict withhow these phenomena are understood by clinical medi-cine and thus could be reused without difficulty by ontol-ogies that support clinical applications. In fact, in the caseof zoonoses and infections that result from a prior processof colonization, our representations are a markedimprovement because our definition of infection acquisi-tion permits cross-species transmission and infectionsresulting from self colonization, whereas IDOs definitionof transmission process does not. Also, our definition ofhost and pathogen are more consistent with their usage byinfectious disease specialists.We also could not reuse other prior work on ontol-ogies that have overlap with Apollo-SV. This workincludes the Epidemiology Ontology (EO) [33] and theOntology for Simulation Modeling of Population Health(SimPHO) [34]. EOlike Apollo-SVstrives to meetFoundry principles [33]. However it, like IDO, also de-fines infection as a material entity. It erroneously definesinfection acquisition as occuring only in humans anddoes not axiomatize its classes. Okhmatovskaia et al. donot define for SimPHO [34] any of the terms in Table 1.Further comparison is not possible because SimPHO isnot publicly available for review/reuse.2Given that simulator configurations require represent-ing several kinds of knowledge including probabilisticand mathematical knowledge, it was not possible to usean OWL2 representation in the Web services to config-ure simulators. At present the application that createsinfectious disease scenarios does not invoke anydescription-logic reasoning supported by the axioms inApollo-SV. Nevertheless, we found it advantageous tocreate the OWL2 representation and reuse it at thelower level of information representation of XSD.Hogan et al. Journal of Biomedical Semantics  (2016) 7:50 Page 10 of 12However, in other work, our OWL2 representation (i.e.,Apollo-SV) supports reasoning in our ontology-basedcatalog of infectious disease epidemiology (OBC.ide),which is a catalog of datasets, publications, grey litera-ture, and simulators [35]. The OBC.ide search interfacemakes use of multiple OWL2 reasoning capabilitiesincluding the is a hierarchy, transitive roles such aspart of, and role chaining. Adaptation of Apollo-SV tothis purpose required no re-axiomatization of the classesdiscussed here.Our future plans include expanding Apollo-SV and theXSD to cover additional simulators and types of infor-mation used in infectious disease epidemiology.ConclusionsApollo-SV captures the output of our ontological analysisof the entities in reality represented by epidemic simulatorconfiguration and output. It also supplies the standardizedterminology used in epidemic simulator configuration andoutput, which also includes an XSD-based syntax anddatabase schema. We validated Apollo-SV through use ina simple end-user application that enables analysts to spe-cify an infectious disease scenario and submit it to one ormore of six simulators. Our analysis of biologically-grounded epidemic simulators and our process of testingthe ontology in software led to scientifically accurate defi-nitions that we have found to be reusable across diversesimulators to date. When available, mathematical modelsof natural phenomena like epidemics are potentially usefulstarting points for ontology development.Endnotes1Closing schools is one infectious disease control strat-egy that simulators study for the control of influenzaepidemics.2We are unable to find any remaining links to Sim-PHO, and past links while we were doing the work werebroken at the time.AcknowledgmentsThis work was supported by award R01GM101151 from the National Institutefor General Medical Sciences (NIGMS) and awards UL1 TR000064 andUL1TR001427 from the National Center for Advancing Translational Sciences(NCATS). This paper does not represent the views of NIGMS or NCATS. Thiswork used the Protégé resource, which is supported by grant GM10331601from NIGMS.Authors contributionsAuthors MMW, WRH, and MB conducted the ontological analysis. AuthorsWRH, MB, and JH created and curated the Web Ontology Languageimplementation of the analysis. Author MMW created and curated the XMLSchema Document (XSD) implementation of the analysis. Authors JL and NMbuilt the Apollo Web services based on the XSD and developed the SimpleEnd User Application (SEUA). Author NM incorporated Apollo-SV ontologydefinitions and elucidations into the SEUA. Authors STB and JD validated theontological analysis and implementations, provided feedback, and developedthe connection of the Web services to the FRED and CLARA simulators. Theyalso provided substantial insight into the FRED and CLARA simulators.Authors WRH, MMW, and MB developed the first manuscript draft. All otherauthors had significant input into the manuscript. All authors have reviewedand approved the final manuscript text.Competing interestsThe authors declare that they have no competing interests.Author details1University of Florida, P.O. Box 100219, 2004 Mowry Rd, Gainesville, FL32610-0219, USA. 2University of Pittsburgh, 5607 Baum Boulevard, Room 434,Pittsburgh, PA 15206, USA. 3University of Arkansas for Medical Sciences, 4301W. Markham St. Slot #782, Little Rock, AR 72205, USA. 4University ofPittsburgh, 5607 Baum Boulevard, Room 434G, Pittsburgh, PA 15206, USA.5Pittsburgh Supercomputing Center, 300 S. Craig St., Pittsburgh, PA 15213,USA. 6University of Pittsburgh, 5607 Baum Boulevard, Room 435 J, Pittsburgh,PA 15206, USA. 7University of Florida, P.O. Box 100212, Gainesville, FL32610-0212, USA.Received: 26 June 2015 Accepted: 10 August 2016SHORT REPORT Open AccessThematic issue of the Second combinedBio-ontologies and Phenotypes WorkshopKarin Verspoor1* , Anika Oellrich2, Nigel Collier3, Tudor Groza4,5, Philippe Rocca-Serra6, Larisa Soldatova7,Michel Dumontier8 and Nigam Shah8AbstractThis special issue covers selected papers from the 18th Bio-Ontologies Special Interest Group meeting and PhenotypeDay, which took place at the Intelligent Systems for Molecular Biology (ISMB) conference in Dublin in 2015. The paperspresented in this collection range from descriptions of software tools supporting ontology development andannotation of objects with ontology terms, to applications of text mining for structured relation extraction involvingdiseases and phenotypes, to detailed proposals for new ontologies and mapping of existing ontologies. Together, thepapers consider a range of representational issues in bio-ontology development, and demonstrate the applicability ofbio-ontologies to support biological and clinical knowledge-based decision making and analysis.The full set of papers in the Thematic Issue is available at http://www.biomedcentral.com/collections/sig.IntroductionThis special issue originates from the papers presentedin a 2-day meeting, combining the Bio-Ontologies SIG(Special Interest Group) meeting with a phenotype-focused Phenotype Day, held at the Intelligent Systemsfor Molecular Biology (ISMB) conference in Dublin,Ireland in 2015. This was the second combined event,following on from a successful event in 2014 [1]. Thepapers that feature in this special issue are a selection ofsubmissions to the meeting that were extended and sub-mitted for consideration by the journal. All papers weresubstantially revised from the original SIG meeting pa-pers, and underwent the standard journal peer reviewprocess.The Bio-Ontologies meeting, as in years past, invitedpresentation and discussion of research across a broadscope, encompassing the organization and disseminationof knowledge in biomedicine and any aspect of applica-tion of ontologies in life sciences research. There were14 submissions to the Bio-Ontologies SIG meeting, and12 were accepted to the workshop, including six shortpapers and six flash updates. Of these, four wereextended for this special issue.Phenotype Day brought together researchers from a largenumber of different domains to discuss aspects of represen-tation, integration and dissemination of phenotype dataacross domains and disciplines. There were 15 submissionsin total to Phenotype Day, with 11 accepted to the workshop(one full paper, six short papers, one position paper, threeposters). Of these, five appear in this special issue.Summary of selected papersIn correspondence with the broad selection of topics rep-resented at the Bio-Ontologies Special Interest Groupmeeting, the papers selected for this special issue alsocover this range of topics. The areas of interest includednot only the creation, further development and integrationof existing ontologies, but also their application in phe-nomics research. Application areas include the representa-tion and integration of model organism databases as wellas text mining of the scientific literature or clinicallyrelevant documents, such as electronic health records.Summary of papers from bio-ontologiesTwo papers in this special issue address issues core to theconstruction of ontologies for management of biologicalinformation. Vita et al. [2] directly propose an extensionto a previously-proposed Major Histocompatibility Com-plex (MHC) Restriction ontology called MaHCO [3], con-structed with the assistance of ontology design patterns,* Correspondence: karin.verspoor@unimelb.edu.au1Department of Computing and Information Systems, The University ofMelbourne, Melbourne, VIC, AustraliaFull list of author information is available at the end of the article© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Verspoor et al. Journal of Biomedical Semantics  (2016) 7:66 DOI 10.1186/s13326-016-0108-7while Jupp et al. [4] introduce a web-based tool to easeauthoring of ontologies with built-in support for enforcingprecisely such design patterns.Specifically, the ontology proposed by Vita et al. [2]aims to enable representation of MHC molecules in theImmune Epitope DataBase (IEDB), in terms of their rela-tion to immunological experiments. These moleculesplay an important role in the adaptive immune system,and because of their wide variation and broad relevance,pose a challenge to knowledge representation. Theenriched MHC ontology enables logical querying ofMHC molecules, in terms of a protein complex of twochains, and includes the details of their locus, haplotypeand/or serotype, as well as the haplotype of the host spe-cies. Finally, the experimental evidence for the MHC re-striction is also modelled. The authors have providedusers of the IEDB the capability to search complex rela-tionships among MHC genes and MHC restrictions, interms of standard ontology identifiers wherever possible.In their software article, Jupp et al. [4] introduce theWebulous application suite, including an add-on appli-cation for Google Sheets that allows population of ontol-ogy design templates with content, and demonstrate itwith a case study using the Experimental Factor Ontol-ogy (EFO). This software allows addition of ontologycontent in bulk, while ensuring consistency of that con-tent. It includes access to BioPortal services [5] thatallow users to search for existing ontology terms to fa-cilitate ontology integration and reuse. The templatesthemselves allow automatic creation of relations or as-sertions from data entered into a spreadsheet, using con-sistent transformation of the data to OWL axioms. Inshort, it supports large-scale ontology development withthe assistance of domain experts who may not them-selves be ontology experts.Webulous is used to create terms in EFO for the workdescribed by Sarntivijai et al. [6]. In the context of theCentre for Therapeutic Target Validation (CTTV), theyaim to represent disease-phenotype associations, withthe objective of linking rare and common diseases to en-able identification of potential therapeutic (drug) targets.A particular representational challenge tackled in this re-search is to capture phenotypes that are only sometimesassociated to a disease, to reflect that not all relevantphenotypes will be present in every presentation. This isdone through the use of a generic association modelOBAN (Open Biomedical AssociatioN) which allowsqualification of association with evidence and, eventu-ally, frequency. The authors describe the use of textmining of the literature to identify candidate disease-phenotype associations that are curated and transformedinto the OBAN model using EFO.Leung and Dumontier [7] similarly apply text miningin the context of disease associations, in their caseconsidering drug-disease associations as extracted fromdrug structured product labels. The identified associa-tions are compared to the clinical practice guidelines,with the finding that there is not a large overlap betweenthe disease indications for drugs in their structured la-bels, and the indications for those same drugs in clinicalpractice guidelines. The authors did find that using taxo-nomic relationships among drugs did improve the over-lap, but a substantial gap remained. The study raisesconcerns about the inconsistent evidence between thesedrug-related information sources and has implicationsfor clinical decision making in evidence-based practice.Summary of papers from Phenotype DayBello and colleagues report in Inferring Gene-to-Phenotype and Gene-to-Disease Relationships at MouseGenome Informatics: Challenges and Solutions [8] on analgorithm for the assignment of gene-phenotype andgene-disease association from the existing genotype-phenotype links contained in the Mouse Genome Inform-atics (MGI) database. The algorithm has been applied tothe existing wealth of data in this database to the effectthat 2100 mouse markers could be linked to human dis-ease and 16,000 mouse markers could be linked to pheno-types. The resulting gene-phenotype and gene-diseaseassociations are provided as part of the databases webpages and can be downloaded by interested parties.In Interoperability between phenotypes in researchand healthcare terminologies - Investigating partial map-pings between HPO and SNOMED CT [9], Dhombresand colleagues report about their investigations to deter-mine partial alignments between both the HumanPhenotype Ontology (HPO) and SNOMED CT usingmodifier terms and HPOs subsumption relations. Usingthe suggested approach, the authors identified partialmappings for 92% of the investigated HPO terms. 30%out of these 92% partial mappings correspond to equiva-lence statements, while the remaining 60% follow anext-best approach to allow for traversing between bothontologies.Mowery et al. in Extracting a Stroke Phenotype RiskFactor from Veteran Health Administration Clinical Re-ports: An Information Content Analysis conducted ex-periments to investigate the report of a stenosisphenotype in relation to stroke in radiology and text in-tegration utility notes [10]. These notes were gatheredfrom the Veteran Health Administration electronichealth records. The authors analyse sections and puretextual representations in both types of records usingpyConText. The results show that there are differencesin the performance of stenosis identification and thelocation of reporting for both types of note. Yet theauthors conclude that pyConText can still be used to fil-ter chart reports into significant and no/insignificantVerspoor et al. Journal of Biomedical Semantics  (2016) 7:66 Page 2 of 4stenosis findings for the data from the Veteran HealthAdministration, facilitating further studies on effective-ness of stroke prevention.Tudose et al. present in PhenoImageShare: An imageannotation and query infrastructure [11] a phenotypeannotation infrastructure for image data. Images areimported from four different resources, leveraging ontol-ogy annotations from the original repository. Further-more, images can be manually annotated using a varietyof ontologies, such as UBERON or the MammalianPhenotype Ontology (MP). The annotation service is in-dependent from species and image data. PhenoImage-Share holds to date ~118 k images (retrieved frommouse and fly databases) associated to anatomical orphenotype concepts (so called regions of interest). Thephenotype image data can be accessed either via a webinterface or an API.The manuscript Reporting phenotypes in mousemodels when considering body size as a potential con-founder by Oellrich and colleagues [12] investigates thechallenges surrounding confounding variables in experi-mental studies associating genotypes and phenotypes.The authors provide a case study based on the experi-mental results released by the International Mouse Phe-notyping Consortium (IMPC) and further discuss thelimitations of current ontological representation to re-port on confounding effects. The authors conclude thatfurther discussion is needed within the community toderive a community-approved representation and dis-semination of confounders in genotype-phenotype asso-ciation studies.ConclusionsThe storage, retrieval, and analysis of ever-growing bio-logical information is complicated by the complexity anddiversity of that information. To the extent thatconsistency in representation of this information can beachieved through the use of a common terminology andvalidated relationships, and that strategies for modelingrare, confounded, or highly contextual relationships canbe developed, it is possible to make progress on makingthat information findable and available for further inves-tigation. The papers in this thematic issue contribute tothose goals, both by addressing the foundational issuesof representational expressivity as well as consistency ofuse of bio-ontologies, and by demonstrating the applica-tion of such structured representations to support infer-ence from biological or biomedical data, on tasksranging from determination of stroke risk factors andpredictions of novel gene-disease associations. The pa-pers raise some important challenges yet add to the bodyof research that establishes the promise of improved bio-medical information access and analysis.AcknowledgementsWe thank the International Society for Computational Biology (ISCB) forhosting the Bio-Ontologies SIG and Phenotype Day at the 2015 IntelligentSystems for Molecular Biology (ISMB 2015) meeting in Dublin, Ireland.FundingNot applicable.Availability of data and materialsNot applicable.Authors contributionsNS, MD, PRS and LS were the organizers of the Bio-Ontologies 2015 SIG meeting atISMB 2015; NC, TG, AO, and KV were the organizers of the Phenotype Day at theBio-Ontologies 2015 SIG meeting at ISMB 2015. KV and AO were the Guest Editorsof the Thematic Issue and wrote the overview of the Thematic Issue papers. Allauthors read and approved the manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Author details1Department of Computing and Information Systems, The University ofMelbourne, Melbourne, VIC, Australia. 2MRC Social, Genetic & DevelopmentalPsychiatry Centre (SGDP), Kings College London, London SE5 8AF, UK. 3TheLanguage Technology Lab, Department of Theoretical and AppliedLinguistics, University of Cambridge, Cambridge, UK. 4Centre for ClinicalGenomics, Garvan Institute of Medical Research, Sydney, NSW, Australia. 5StVincents Clinical School, Faculty of Medicine, University of New South Wales,Sydney, Australia. 6University of Oxford e-Research Centre, 7 Keble Road, OX13QG Oxford, UK. 7Brunel University London, London, UK. 8Stanford University,Stanford, CA, USA.Received: 29 October 2016 Accepted: 18 November 2016RESEARCH Open AccessmiRiaD: A Text Mining Tool for DetectingAssociations of microRNAs with DiseasesSamir Gupta1*, Karen E. Ross2, Catalina O. Tudor1,2, Cathy H. Wu1,2, Carl J. Schmidt3 and K. Vijay-Shanker1AbstractBackground: MicroRNAs are increasingly being appreciated as critical players in human diseases, and questionsconcerning the role of microRNAs arise in many areas of biomedical research. There are several manually curateddatabases of microRNA-disease associations gathered from the biomedical literature; however, it is difficult forcurators of these databases to keep up with the explosion of publications in the microRNA-disease field. Moreover,automated literature mining tools that assist manual curation of microRNA-disease associations currently captureonly one microRNA property (expression) in the context of one disease (cancer). Thus, there is a clear need todevelop more sophisticated automated literature mining tools that capture a variety of microRNA properties andrelations in the context of multiple diseases to provide researchers with fast access to the most recent publishedinformation and to streamline and accelerate manual curation.Methods: We have developed miRiaD (microRNAs in association with Disease), a text-mining tool thatautomatically extracts associations between microRNAs and diseases from the literature. These associations areoften not directly linked, and the intermediate relations are often highly informative for the biomedical researcher.Thus, miRiaD extracts the miR-disease pairs together with an explanation for their association. We also developed aprocedure that assigns scores to sentences, marking their informativeness, based on the microRNA-disease relationobserved within the sentence.Results: miRiaD was applied to the entire Medline corpus, identifying 8301 PMIDs with miR-disease associations.These abstracts and the miR-disease associations are available for browsing at http://biotm.cis.udel.edu/miRiaD. Weevaluated the recall and precision of miRiaD with respect to information of high interest to public microRNA-disease database curators (expression and target gene associations), obtaining a recall of 88.4690.78. When weexpanded the evaluation to include sentences with a wide range of microRNA-disease information that may be ofinterest to biomedical researchers, miRiaD also performed very well with a F-score of 89.4. The informativenessranking of sentences was evaluated in terms of nDCG (0.977) and correlation metrics (0.678-0.727) when comparedto an annotators ranked list.Conclusions: miRiaD, a high performance system that can capture a wide variety of microRNA-disease relatedinformation, extends beyond the scope of existing microRNA-disease resources. It can be incorporated into manualcuration pipelines and serve as a resource for biomedical researchers interested in the role of microRNAs in disease.In our ongoing work we are developing an improved miRiaD web interface that will facilitate complex queriesabout microRNA-disease relationships, such as In what diseases does microRNA regulation of apoptosis play arole? or Is there overlap in the sets of genes targeted by microRNAs in different types of dementia?.Keywords: MicroRNA, Disease, Associations, Text-mining, Relation extraction, Natural language processing* Correspondence: sgupta@udel.edu1Department of Computer and Information Sciences, University of Delaware,Newark, DE 19711, USAFull list of author information is available at the end of the article© 2016 Gupta et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 DOI 10.1186/s13326-015-0044-yBackgroundMicroRNAs (miRs) are a class of small non-coding RNAsencoded in the genomes of animals, plants, and protozoa.In general, miRs negatively regulate gene expression bybase pairing with sequences in the 3-untranslated regionof mRNAs, which either inhibits their translation or trig-gers their cleavage. Thousands of miRs have been identi-fied in mammals, and they have been implicated in thecontrol of a wide range of biological processes [1].miRs are increasingly being appreciated as criticalplayers in human disease. The role of miRs in cancer isvery well established, with a wealth of studies demonstrat-ing the participation of miRs in multiple cancer-relatedprocesses in diverse tissue types [2]. miRs have also beenlinked to many other diseases, including cardiovasculardisease [3], diabetes [4], neurological disease [5] and liver[6] and intestinal [7] disorders.Although at a mechanistic level miRs influence diseasethrough their effects on the expression of their targetgenes, in the scientific literature miRs are associatedwith diseases through a variety of relationships. In somecases, miRs are directly associated with the disease itselfor with a feature or outcome of the disease, such as ag-gressiveness [8], invasiveness [9], or patient survival [10].In other cases, miRs are identified as biomarkers [11] ortherapeutic targets [12] for a disease. miRs can also belinked to biological processes that are, in turn, con-nected to the disease. This category includes miR-genetargeting events, as well as regulation of processes suchapoptosis [13], metastasis [14], or cholesterol transport[15] by miRs. Finally, in some cases, it is the state of themiR (e.g., over- or under-expression [16]) that is associ-ated with the disease.There are currently several high-quality databasesthat capture miR-disease associations and some of theabove relations, including miR2Disease [17], miRCan-cer [18] and the Human microRNA Disease Database(HMDD; [19]). These resources are literature basedand support searches for miR or disease of interest.miR2Disease and miRCancer provide information onmiR expression in disease, and miR2Disease addition-ally covers miR target genes. HMDD documents miRsthat are potential biomarkers and provides several ana-lysis tools, such as miR set enrichment analysis. miR2-Disease and HMDD are manually curated; thus theyare limited by the time-consuming nature of manualcuration and have difficulty keeping up with the explo-sion of publications in the miR-disease field. In 2014alone, using the PubMed query miRNA[TIAB] ORmicroRNA[TIAB] OR miR[TIAB], we obtained around5100 PubMed citations, which was a 120 % increasecompared to 2013 and a 160 % increase compared to2012. Additionally, we identified 19,402 abstracts (as ofFebruary 2015) that mention miRs and of these, weestimate 15,171 abstracts also mention disease terms(as detected by PubTator [20]).Automated literature mining tools could helpstreamline and accelerate the curation process as wellas provide researchers with fast access to the most re-cent published information; however, currently, suchtools are limited and have not been widely adopted.Most of the miR-related literature mining tools avail-able focus on extraction of miR-target gene relationswithout regard to disease, and rely on relatively simpletext mining techniques, such as co-occurrence of miRand disease in the same sentence or abstract. These in-clude miRSel [21] and the tools used by the miR-targetdatabases miRWalk [22], TarBase [23], and miRTarBase[24]. miRCancer [18], is one of the few resources thatuses a rule-based system to identify disease-relevantmiR information in literature, but is limited to detect-ing miR expression associations in cancer.In this work, we present miRiaD (microRNAs in asso-ciation with Disease), which automatically extracts fromthe biomedical literature associations between miRs anddiseases together with any intermediate relations thatbridge the association, thereby capturing myriad ways inwhich a miR can be associated with a disease. In generalmiRiaD connects a miR or an aspect of a miR (e.g., dif-ferential expression, methylated state) to a disease or adisease aspect (e.g., outcome or therapy) through somerelations (e.g., involvement, regulation, is-a). For example,miRiaD can extract a miRs involvement in the outcomeof a disease, or its role as a biomarker or therapeutic targetfor a disease. Additionally, miRiaD can extract the involve-ment of a miR in some cellular process that is highlyrelated to a disease, thus (indirectly) linking the miR withthe disease. These links between a miR and a diseasethrough cellular processes or target gene, which we referto as linking entity, are often implicit but highly inform-ative to researchers studying disease mechanisms. Ourprevious work, STEM [25], extracted relations betweentwo entities, namely a miR and process/function terms.miRiaD extends upon the previous work by allowing moretype of entities to be linked (e.g. disease with its outcome,miR with a disease outcome etc.).We have applied miRiaD to the entire set of Medline ab-stracts, and we provide a web interface through which theresults can be searched using PubMed-like queries. Detailsabout our miR-disease association extraction approach arepresented in the Methods section; screenshots and detailsabout the interface are provided in the Results and Discus-sion section. In conjunction with miRiaD, we developed aprocedure for ranking sentences containing miR and dis-ease mentions according to their informativeness, whichwe envision can be used in the future to guide how miR-iaD results will be presented to the user. The details of thisapproach are given in the Methods section.Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 2 of 15miRiaD was evaluated with two potential user commu-nitiesmiR-disease database curators and biomedical re-searchersin mind. To address the needs of curators, therecall and precision of miRiaD was evaluated with re-spect miR-target and miR expression information, whichare the two types of information curated by miR2Di-sease, the most comprehensive database for miR-diseaseassociations; this evaluations achieved recall results be-tween 88.4690.78 %. For biomedical researchers, whoare potentially interested in the full range of possibleconnections between miRs and disease, we evaluatedmiRiaD with respect to a variety of sentences in whichmiRs and diseases co-occur, resulting in F-scores of89.4 %. Finally, an evaluation of our informativenessranking system accomplished an nDCG of 0.9815, aswell as correlations of 0.6780.727, when compared toan annotators ranked list. Details about the experimen-tal setup and the evaluations are given in the Resultsand Discussion section.Methods: Approach and ImplementationIn developing the miRiaD system, we attempted to cap-ture the variety of ways in which connections between amiR and a disease are stated in text. Figure 1 schematic-ally depicts these relationships. First, both miRs and dis-eases are often associated with descriptive informationor properties, which we will collectively refer to as as-pects. Examples of miR aspects include expression leveland state (e.g., hyper-methylated or mutated); examplesof disease aspects include outcome/stage, biomarker, ortherapy. For convenience, we will refer to a miR or itsaspects as a miR entity (e.g. mir-9, overexpressed mir-9,hypermethylation of mir-9) and likewise refer to a dis-ease or its aspects as a disease entity (e.g. gastric cancer,biomarker for gastric cancer). In some sentences, a miRentity may be directly related to a disease entity. In othercases, a miR entity may regulate a target gene or be in-volved in a biological process that is in turn implicitlylinked to a disease entity. Even more complex associa-tions are possible; for example, a miR may regulate agene that is involved in a biological process that is ultim-ately relevant to a disease. These relationships may beexpressed in text using a variety of phrases and not allphrases are applicable to all types of relationships. Anassociation between a miR and a process or disease islikely to be described using relations such as involvedin or has a role in, whereas an association between amiR and biomarker is likely to be expressed using anis-a relation. A formal description of the patterns,the list of triggers and the types relations between thedifferent pairs is provided in Additional file 1.miRiaD identifies specific relations (i) between a miR andits aspect, (ii) between a disease and its aspect and (iii) be-tween a miR entity and a disease entity. For example, in thesentence Downregulation of mir-26a is associated withtumor metastasis in osteosarcoma., miRiaD will detectthe connection between mir-26a and its aspect, downregu-lation; between the disease osteosarcoma and its aspect,tumor metastasis; and between miR entity miR-26a down-regulation and the disease entity of tumor metastasis inosteosarcoma.miRiaD also detects multi-step connections where theconnection between the miR entity and disease entity ismediated through another entity or process. We call thisextra entity or process a linking entity. Consider the sen-tence, MicroRNA-9 promotes tumor metastasis viarepressing E-cadherin in esophageal squamous cell carcin-oma. Typically, as in this sentence, the miR entity regu-lates or is involved with the linking entity (E-Cadherin).This regulation in turn can be connected to the diseaseFig. 1 miR-disease associations extracted by miRiaD. miRs or their aspects (state or expression levels) can be directly associated with diseases ordisease aspects (outcome, stage, biomarker/therapy) through a variety of relations; association can also be bridged by a linking entity such as atarget gene or biological processGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 3 of 15entity (tumor metastasis of esophageal squamous cellcarcinoma). However, it is quite common for the connec-tion between the linking entity and the disease entity to beleft unstated with an understanding of the implicit con-nection requiring additional domain knowledge.miRiaD currently extracts information from Medlineabstracts. After the abstracts are retrieved the abstracttext and title are extracted. The text is split into individ-ual sentences using a tool developed in-house. miRiaDextracts the connection between a miR and disease,through the detection of the direct relations betweenmiR entities and disease entities, or through detection ofmultiple relations involving linking entities. miRiaD usesthe presence of certain lexico-syntactic dependencystructures in a sentence to detect these semantic rela-tions. Thus, the basic steps of the miRiaD system in-clude (i) Detecting miR/disease entities and linkingentities; (ii) preliminary syntactic processing; (iii) identi-fying syntactic dependencies between miRs and co-occurring terms; and (iv) assigning semantic relationsbetween miRs and co-occurring terms. These steps aredescribed below and also shown in Fig. 2. Finally, wealso describe a method to score sentences based on theirinformativeness in describing miR-disease connections.Detecting miR/disease entities and linking entitiesmiR entityA miR entity can be a miR in isolation or together with oneof its aspects (expression, mutation, methylation). AlthoughmiRs are mentioned in text in a variety of ways (e.g., miR-1,microRNA1, miRNA-1, let-1, etc.), they follow a well-established naming convention. miR mentions consists of aprefix (miR, MIR, miRNA, microRNA) followed by aunique identifying number, which is assigned based on se-quence similarity. This number may be followed by a suffixsuch as -a, -1, -3p or -5p, and/or a prefix that de-notes the species may be included. miRiaD detects suchmiR mentions by using simple regular expressions.miR aspects usually describe the abundance or prop-erties of a miR. We detect the former by searching fornoun phrases headed by trigger words such as level,expression and regulation as well as their variants.For the latter, we consider mutation terms such asmutation, variants or polymorphism as well asnominalized forms of common events such as methy-lation. Of course these terms are only candidates to bemiR aspects and are treated as such only after we de-tect their syntactic relation to a miR.Disease entityA disease entity can be disease in isolation or in combin-ation with one of its aspects (diagnostic, treatment, out-come). We detect disease mentions using Pubtator [20]database, which includes disease mentions tagged inMedline abstracts by DNorm [26]. The disease mentionsare normalized to Medic concept IDs. We process onlythose abstracts which have a disease mention and thusrecall for disease mention detection is important for themiRiaD system performance. The DNorm [26] systemreports a micro-averaged precision, recall and f-measureof 0.803, 0.763 and 0.762.We did some additional analysis for PubTator diseasedetection by randomly selecting 200 abstracts from themiR2Disease database and checking whether the diseaseannotated by the miR2Disease curators was detected byPubtator. A miR2Disease abstract can be annotated witha disease which is not mentioned in the abstract butmentioned in the full length article. Thus we selectedonly those miR2Disease abstracts where the annotateddisease either was mentioned in the title or the abstract.While checking if the annotated disease matched one ofthe disease mentions detected by Pubtator, we allowed forsynonym matches (breast cancer with breast carcinoma).Pubtator picked the exact name, a synonym or part of theannotated disease name in 100 % of the abstracts. How-ever in ten cases, it picked only part of the name, despiteFig. 2 miRiaD Pipeline. The steps of the miRiaD pipeline are illustrated with numbered grey blocks. External tools used throughout the pipelineare shown in bold and italic fontGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 4 of 15the fact that there was ample evidence of the fullname mention. For example, in a sentence that con-tained the phrase Oral Squamous Cell Carcinoma(OSCC) , PubTator only detected Squamous CellCarcinoma.Similar to miR aspect detection, we locate disease as-pects by looking for certain trigger words or and theirtextual variations. Commonly occurring disease aspectterms include disease stage or outcome terms/phraseslike clinical outcome, disease free survival (dfs), over-all survival (os), metastasis, sensitivity, prognosis,tumorgenicity, invasion, and progression. Diagnosis-related terms include biomarker, marker, predictor,profiler, prognostic, diagnosis, indicator, and theirtextual variations; and finally treatment-related triggers in-cluding therapeutic, treatment, target, therapy, andtheir textual variations. As with miR aspects, a candidatephrase is considered to be a disease aspect only after thesyntactic dependency (as described later) with a diseasemention is established. Several examples of miR entities(let-7i and low miR-335 levels) and disease entities(colorectal cancer metastasis, overall survival, and re-lapse-free survival) are highlighted in the followingsentence fragments: let-7i is associated with colorectalcancer metastasis and low miR-335 levels in EOCwere associated with shorter overall survival and relapse-free survivalLinking entityLinking entities are cellular processes or target genesthrough which the miRs association with the disease canbe explained. We use PubTator, which uses GenNorm[27] to identify gene mentions in a sentence. In order todetect cellular processes, we follow the method adoptedin eGIFT [28], which uses dictionary look-ups or mor-phological derivatives e.g., terms/phrases with -sion,?tion, ?sis, ?or, ?er, ?ment suffixes. Candidate linkingentity phrases are excluded if they are determined to bea miR/disease aspect phrase. Additionally syntactic de-pendency (as discussed later in this section) needs to beestablished for the candidate phrase to be considered asa linking phrase. In the sentence fragment mir-320adown-regulation mediates bladder carcinoma invasionby targeting itgb3 the linking entity is highlighted.Preliminary Syntactic ProcessingThe miRiaD approach attempts to identify a relation be-tween a miR entity and a disease entity or linking entityby first identifying syntactic dependencies betweenphrases of that sentence. Two steps facilitate the detec-tion of such syntactic dependencies: chunking and sim-plification. Note, to reduce overhead for the chunkingand simplification step, we filter out sentences that donot contain a miR or a disease mention. Chunking is thetask of identifying and grouping words in a sentence intoconstituents (noun groups, verb groups etc.) calledchunks. Sentences are tagged with part-of-speech(POS) tags using the Genia Tagger [29]. We furtherchunk the words based on syntactically related POS tagsto form noun phrases (NPs), verb groups (VGs) andprepositional phrases (PPs).After chunking, we use iSimp [30], which simplifies avariety of complex syntax structures in a sentence into arelatively small number of simple patterns, thus facilitatingthe identification of syntactic dependencies and relationextraction. iSimp [30] identifies syntactic constructs, suchas appositives, relative and reduced relative clauses, con-junctions, and parenthetical elements. These syntacticconstructs are used to form simple sentences from a com-plex sentence. For example, consider the followingsentence:We have profiled four miRNAs, miR-21, miR-210,miR-155, and miR-196a, all implicated in thedevelopment of pancreatic cancer with either provenor predicted target genes involved in critical cancer-associated cellular pathways. (PMID 19,723,895)We illustrate here how miRiaD identifies the relationbetween miR-21 and the disease entity pancreatic can-cer development. Automatically detecting a relationshipbetween miR-21 and pancreatic cancer in this sentencemight be difficult by just trying to match basic patternsand rules. Additional information of no immediate useoccurs between the two mentions, preventing the pat-terns from detecting a relationship. However, by usingsimplification constructs identified by iSimp, we cangenerate a simple sentence from the original sentencethat states the relationship in a straightforward way:Mir-21 is implicated in the development of pancreaticcancer. iSimp tags the following syntactic constructs:(i) A conjunction in the form of a list of elements(miR-21, miR-210, miR-155, and miR-196a),(ii) A conjunction (proven or predicted),(iii)The appositive construct involving the two nounphrases four miRNAs and miR-21, miR-210,miR-155, and miR-196a,(iv)The reduced relative clause all implicated in thedevelopment of pancreatic cancer with either provenor predicted target genes, which modifies the nounphrase four miRNAs(v)Another reduced relative clause involved in criticalcancer-associated cellular pathways, which modifiesthe noun phrase target genesUsing constructs (i) and (iii) we can replace fourmiRNAs by miR-21 in the construct (iv) and thusGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 5 of 15generate miR-21 is implicated in the development ofpancreatic cancer . Additional simplifications arealso generated for the remaining elements of the list(miR-210, miR-155, or miR-196a). Note that thechunking information is carried through when generat-ing simplified sentences from iSimp constructs.Identifying Syntactic DependenciesmiRiaD extends our previous system [25], which was builtto detect semantic relations, such involvement, regula-tion, state, attribute and association, between miRsand biological processes or functional aspects of miRs.. LikeSTEM, to capture the above semantic relations we detectthese semantic relations via the detection of the followinglexico-syntactic dependencies. 1) agent-theme of predicate(e.g., miR-9 regulates cell proliferation), and 2) nounmodification (e.g., miR-1 overexpression, metastasis ofgastric cancer). We detect these lexico-syntactic depend-encies after chunking and sentence simplification.Agent-theme of predicateWe are interested in extracting the relations of involve-ment and regulation where a miR entity is the agentand a process (for example) is the theme. These oftenhave a syntactic dependency counterpart (subject andobject of a verb). Consider the example sentence miR-9regulates cell proliferation. Here the regulation rela-tion is represented by the verb regulate and the agent(mir-9) and theme (cell proliferation) are the subject andobject of this verb. Typically the predicates are simpleverbs, and the arguments are noun phrases appearing tothe left and to the right of the predicate if the clause isin active voice. However, with the use of chunking, it ispossible to extract relationships from more complexsentences as well. Consider the sentence mir-9 isknown to directly regulate cell proliferation. Notice bychunking this sentence we will get two verb groups (isknown and to directly regulate). The predicate verb(regulate) is the head verb of the second of the twoconsecutive verb groups and thus the noun phrase be-fore this predicate is still miR-9. In the sentence ex-pression of mir-9 regulates proliferation of U87 cells.the agent and the theme are not base noun phrases, butare larger noun phrases that have attached prepositionalphrases, which are identified by chunking.The voice of the predicate verb is important in deter-mining the role of the arguments. For example, in thesentence apoptosis is regulated by miR-9, miR-9 isthe agent performing the action regulates and apop-tosis is the theme being regulated. This is the reasonwhy, rather than using syntactic notions of subject andobject, we refer to more thematic notions of agent andtheme of a predicate, abstracting away from word order.There are also cases in which the predicate isexpressed in a nominalized form. For example, considerthe fragment miR-9 regulation of cell proliferation,where the predicate is the nominalized form of the verbregulates. We extend our agent-theme extraction rulesto handle such cases by following the rules fornominalization from iXtractR [31].Predicates need not necessarily be single words. In thesentence miR-146a may play a role in cell prolifera-tion, the predicate spans multiple words. Phrases like,is crucial for, is important for, plays a role in aresome examples of multi-word predicates that we con-sider. We defer to the next part of this section for a dis-cussion of the words and phrases that constitute thepredicates we are interested in.In some sentences, an argument (agent/theme) canbe shared by two predicates. For example, consider thesentence miR-126 was able to inhibit laryngeal squa-mous cell carcinoma partly by suppressing Camsap1expression. Here, miR-126 is the agent for both in-hibit and suppressing predicates. Because it is awk-ward to mention the agent each time one of itspredicates is used, the agent is mentioned only once, inthe beginning, and omitted in the second case (i.e., forthe verb suppress). We call the cases in which the ar-gument is omitted and inferred from an earlier men-tion the null-argument agent-theme of predicate.Sentences with null-arguments have clauses separatedby prepositions to + a simple verb, via or through+ nominal form of a verb, or by + a verb ending ining. For example, Tumor suppressive miR-1 in-duces apoptosis through direct inhibition of SRSF9 inbladder cancer is a sentence containing a null-argument after through + nominal form of a verb.The null argument rules that identify the agent for thesecond predicate were taken from RLIMS-P [32] andiXtractR [31].Our rules are limited to some simple patterns corre-sponding to simple syntactic structures in order to beprecise. Sentence simplification allows us to handle morecomplex cases without making the patterns more in-volved. Consider the sentence breast cancer metastasissuppressor 1 up-regulates mir-146, which suppressesbreast cancer metastasis, which contains a relativeclause. Without simplification our rules would incor-rectly extract [which (agent), suppresses (predicate),breast cancer metastasis (theme)]. iSimp detects therelative clause and generates a simplified sentence mir-146 suppresses breast cancer metastasis, thus enablingus to extract the correct relation [miR-146 (agent), sup-presses (predicate), breast cancer (theme)]. As discussedin the pre-processing step the other simplification con-structs which iSimp handles include appositives, con-junctions, reduced relative clauses and parentheticals.Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 6 of 15Noun modificationWe detect state or attribute relations that are in anoun modification syntactic relationship. Consider thesubject noun phrase in the sentence mir-21 overexpres-sion is associated with glioblastoma. The entity miR-21acts a modifier of a noun, overexpression, which is anominalized form of a stative verb. In this case the headmodifier relation indicates the state of entity miR-9.Additionally the head modifier dependency can be statedwith a prepositional attachment (e.g., metastasis of gastriccancer). In this work, we only consider these two forms ofnoun modification syntactic dependency.Assigning Semantic RelationsHaving captured the syntactic dependency structures, welook at the predicate for the identification of the seman-tic relation between a miR entity and a disease entity ora linking entity. In our previous work we observed thatthe semantic relations that frequently connect a miR en-tity with a related term (disease entity or linking entity)fall into several categories, namely involvement, regu-lation, is-a, association found-in and state. Eachcategory encompasses a number of trigger words that arecommonly found in text. The predicate detected in thepattern used to detect the syntactic dependency can beused to assign the relation to the appropriate category. Forexample, if the predicate found in the sentence is is in-volved or is implicated, then the relation is categorizedas an involvement relation. As noted earlier, the verbgroup containing the predicate can contain additionalwords modifying the predicate (e.g., directly targets orwas able to inhibit). In these situations, the head of thefinal verb group is used when matching the trigger words(e.g., targets or inhibit). Example trigger words andsentences are given below for each of the four categories.Although trigger words are provided in present tense inthe examples below, the reader should assume all of theirtextual variations (tense and nominalized forms).Involvement relationsFor detection of the involvement relation, we expectthe use of the agent-predicate-theme dependency struc-ture. Further, we expect the miR entity and the diseaseentity or a linking entity to be picked as the agent andtheme respectively. A range of words or multi-word trig-gers can be used as the predicate including: is involved in,is implicated in, is required for, is needed for, play a rolein, is necessary/sufficient/crucial/etc. for, is dependent on,participates in, contributes to, influences, fosters, affects,allows, initiates, etc. The main verbs in these triggers canappear in different tense forms and the verb or noun canbe modified, as in the sentence miR-21 may play a criticalrole in chronic myelogenous leukemia.Regulation relationsRegulation relations are similar to involvement relationsexcept for the list of trigger words/phrases that can servein the predicate position. Based on our previous work,the regulation trigger words we use include: regulates,promotes, induces, elevates, targets, enhances, increases,decreases, raises, up/down-regulates, modulates, causes,results, interacts, blocks, mediates, etc. In this category,we also encounter cases in which the predicate isexpressed in a nominalized form. An example sentenceis restoration of mirna-143 (mir-143) regulates cox-2and inhibits cell proliferation of pancreatic cancer cells.Here the association between the miR-143 and the dis-ease pancreatic cancer is via two linking entities: gene(cox-2) and cellular process (cell proliferation).Association relationsTo find sentences where a miR entity is associated witha process, linking entity, a disease or a disease outcome,we consider sentences with agent-predicate-theme de-pendencies with the following trigger words or phrases:is associated with, correlated with, linked to, etc.For example, in the sentence reduced circulating mir-150levels are associated with poor survival in pulmonary ar-terial hypertension. a miR entity (expression of mir-150)is linked to a disease entity (poor survival in pulmonaryarterial hypertension).Is-a relationsRelations that link two entities via the is_a relation arenormally of agent-predicate-theme type. We expect themiR entity and the disease entity or linking entity to bepicked as the agent and theme respectively, with be astrigger. However we include a wider range of triggerwords or phrases such as:. is, are, acts as, functions as,serves as, etc. In the example sentence Plasma miR-601and miR-760 can potentially serve as promising non-invasive biomarkers for the early detection of colorectalcancer both miR-601 and miR-760 are in a is_a relationwith non_invasive biomarkers as indicated by the trig-ger phrase serve as. Note that sentence simplificationmakes it possible to link both miR to the theme non-in-vasive biomarkers.Found_inFor detection of the found_in relation, we expect theuse of the agent-predicate-theme dependency structure.Further, we expect the miR entity and the disease to bepicked as the agent and theme respectively. These rela-tions indicate that an aspect of the miR (expression,states like mutation, hyper-methylation) is found in thedisease. There are two classes of triggers used to detectsuch relations. First set of triggers include words ormulti-word triggers like: is found in, is detected in,Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 7 of 15is increased in etc. In these cases the miR entity will in-dicate the specific aspect of the miR (e.g. high level of mir-155 was found in gallbladder cancer). In other cases theaspect of the miR is inferred from the predicate trigger (e.g.miR-155 was overexpressed in gallbladder cancer). Thetriggers to detect such cases include: overexpressed in,highly expressed in, upregulated in, mutated in etc..State relationsThese relations are used to describe relations between amiR and its aspect or between disease and its aspect. Inthese sentences, the term describes the state in whichthe miR (or its promoter) is observed (e.g., overex-pressed, methylated) or an outcome, treatment, or diag-nostic property of the disease. Here we use the nounmodification syntactic dependency, where miR aspect ordisease aspect is the head noun and the miR or disease,respectively, is the modifier. For miR aspects we use trig-ger words such as methylation, expression, silencing, andknockdown while for disease aspects we use words/phrases such as level, biomarker, or disease free survival.An example sentence is Overexpression of the miR-200bis associated with hepatocellular carcinoma cell migrationthrough the epithelial to mesenchymal transition.Multiple predicate triggers In some sentences theremay be multiple predicate triggers appearing between anagent and a theme. For example in the sentence fragmentmiR-9 is involved in the regulation of apoptosis, thetwo triggers involved in and regulation connect themiR entity (mir-9) and the linking entity (apoptosis). Insuch cases we can expect that one of the triggers will besemantically more specific. Typically, we have found thatthe more specific predicate is of the type regulation andthe more general predicate is of the type is-a or involve-ment. Hence we modify the output to include only themore specific relation. For example we first extract thetuple [miR-9, is involved in, regulation of apoptosis] andresolve it to [miR-9, regulates, apoptosis], categorizing itas a regulation relation.Assigning scores to sentencesAs we have seen from the above sections, there aremany different ways in which a miR entity can be con-nected to a disease entity in a sentence through variouslinking entities and semantic relations. Biologists mightbe interested in sentences containing some categories ofrelation more than others, and might prefer sentencesmentioning certain types of semantic relations morethan others. We wanted to see if scoring sentences interms of linking entities, semantic relations, and otherfactors (such as hedging), might help rank the sentencesin a specific way that is preferred by biologists. Such ascoring system could potentially be used to prioritize themost relevant sentences when presenting miRiaD resultsto users.Our assumption is that biologists might want to seesentences mentioning a miR-disease relationship that isexplained via a target gene or a process. Equally import-ant might also be sentences describing the miR-diseaserelationship via a sequence of biological steps connectedby words such as contributes, results in, causes,supports and their textual variations, as well as theconjunction and. Less informative are sentences thatdescribe the miR-disease relationship in terms of expres-sion level of the miR. We assign a score between 1 and3 based on how informative the sentence is (highly in-formative, informative, somewhat informative), usingempiricallyderived rules.Sentences are assigned to the highly informative(3 points) category if they contain some explanationof the connection between the miR and the disease.We use two indicators for detecting the explana-tory component. The first is the detection of null argu-ment sentences. For example, in the sentence miR-137functions as a tumor suppressor by targeting CtBP1 toinhibit epithelial-mesenchymal transition and inducingapoptosis of melanoma cells the target gene aspect of themiR explains its functionality as a tumor suppressor. Thesecond indicator of the explanatory component is thepresence of at least two semantic relations, which forma sequence of events/process/outcome/diagnostic/treat-ment.. One such example is Treatment of gastric cellswith dihydroartemisinin (DHA) increased miR-15b andmiR-16 expression, caused a downregulation of Bcl-2,resulting in apoptosis of gastric cancer cells. Inaddition we also assign sentences to the highly inform-ative category if they contain target gene informationtogether with some other relation (e.g., UBASH3B is afunctional target of anti-invasive miR200a that is down-regulated in triple negative breast cancer).Sentences are assigned to the informative (2 points)category if they contain both diagnostic and treatmentdisease aspects, a linking entity regulation process, or atreatment aspect of a disease in the theme (e.g., miR is atherapeutic target for a disease). Some example sen-tences in this category include: miR-23b is epigeneti-cally down-regulated and restoration of miR-23b caneffectively suppress cell growth in glioma stem cells ormiR-139-5p is a potential biomarker for early diagnosisand prognosis and is a therapeutic target for esophagealsquamous cell carcinoma (ESCC).Sentences are assigned as somewhat informative(1 point) if they contain an altered expression rela-tion, an involvement relation, or a diagnostic aspect.Some examples include: High miR-199a expressionis associated with liver fibrosis or miR-125b couldbe an important prognostic indicator for colorectalGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 8 of 15cancer patients. All sentences that were notassigned highly informative or informative scores arealso considered to be somewhat informative.A sentence can be upgraded from somewhat inform-ative to informative or informative to highly in-formative if it contains an outcome aspect. A sentencecan also be downgraded if it contains a diagnostic/treat-ment aspect that was obtained through a co-occurrencerelation (i.e., not part of the relation itself but co-occurring in the sentence). Additionally, hedging or otherevidence of doubt will lower the score of the sentence byone point. The triggers used here for doubt were might,could, suggest, propose, and their lexical variations.Evaluation: Overview and Experimental SetupWe evaluated miRiaD with respect to the needs of two po-tential user communities: miR-disease database curatorsand biomedical researchers studying disease mechanisms.Study 1: Evaluation of miRiaD for assistance with manualcurationDatabase curators can benefit from automated text min-ing tools that make manual curation more efficient byquickly identifying relevant information in the literature.Because miR2Disease is the most comprehensive data-base aiming to manually curate miR-disease relation-ships, we focused our first study on the ability ofmiRiaD to retrieve the types of information annotatedby miR2Disease, namely miR expression and target in-formation in the context of disease. Initially, we testedthe recall of miRiaD using sentences from abstracts thatwere cited as evidence in miR2Disease entries. By gath-ering the evaluation set from the database itself, we areguaranteed to have sentences that are of interest tomiR2Disease curators; however, because all of the sen-tences are by definition positive, we cannot use them toassess precision. Therefore, we also evaluated the recalland precision miRiaD using a set of randomly selectedMedline abstracts not found in the miR2Disease database.These abstracts were disease-focused and mentioned amiR, but only a subset had target gene or miR expressioninformation, so we could test the ability of miRiaD to rejectpapers that are not relevant for curation by miR2Disease.We downloaded the entire mirRDisease database,which contained 3273 entries at the time (January 2015).Each entry in the downloaded file contained a miR, adisease, a title of an article, and the year in which thearticle was published. Because no PMIDs were providedwith the downloadable database, we looked up thePMID corresponding to each entry by matching the titleand the year of publication in the Medline 2015 corpus.We filtered out the entries for which the miR and thedisease mentions could not be found together in the ab-stract or the title of the article. This resulted in a total of486 entries in the miR2Disease database that we coulduse for our study.For these entries, we retrieved the title and the ab-stract of each referenced article from the Medline 2015corpus. We also obtained the descriptions accompanyingeach miR-disease-article entry from the miR2Disease on-line database. These descriptions consist of sentencestaken from the referenced article that support the miR-gene relation in the entry. Because we have so far onlyapplied miRiaD to abstracts, we used Perls StringSimi-larity Module, which is based on the algorithm describedin [33], followed by manual verification to detect entriesin which the description text was taken from the ab-stract. Of these entries, we randomly chose 100 to testthe recall of the miRiaD system.miR2Disease clearly mentions that their manual anno-tations include miRNA expression patterns in the dis-ease state, detection methods for miRNA expression,and experimentally verified miRNA target gene(s).Therefore, we manually reviewed each description, andmarked each sentence as containing expression and/ortarget information. Ninety descriptions were marked ascontaining expression information, and 58 descriptionswere marked as containing target information.Next, we selected randomly selected 100 additionalMedline abstracts not found in the miR2Disease database.The abstracts were selected to be focused on diseases (i.e.,a disease is mentioned in the title, the first or the last sen-tence of the abstract, or three or more times throughoutthe abstracts, as defined in the work by Tudor et al. [28]).One biologist annotator marked the abstracts in terms ofgenes targeted by the miR and miR expression informa-tion for the miR. There were 52 abstracts marked as con-taining expression information and 48 abstracts markedas containing target information.Study 2: Evaluation for general extraction of miR-diseaseassociationsFor our second study, we evaluated miRiaD with respectto the extraction of a wide range of relations that appearin text connecting a miR to a disease that may be ofinterest to biomedical researchers. As above, we con-ducted the study in two parts: first, we selected a setwhich is highly likely to contain such relevant relationbetween a miR entity and a disease or linking entity andassessed recall and precision. Next we assessed precisionand recall of relations for randomly selected sentencesfrom Medline abstracts. For the first set, we gatheredGene Reference into Function (GeneRIF) sentences fromthe EntrezGene entries for miRs. GeneRIFs are used toannotate EntrezGene entries and consist of short sen-tences or phrases with literature citations describing thefunction of the gene/miRNA [34]. These GeneRIF sen-tences may or may not be direct quotes from theGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 9 of 15abstract and may be rephrased by the annotator. Wechose these sentences because of their rich variety ofmiR-disease associations, which included relationshipsbetween miRs and miR aspects (expression) and diseasesand disease aspects (outcome, biomarker, therapy), aswell as linking entities (target, process). Importantly, forthis study, we were not testing the ability of miRiaD todetect appropriate sentences for EntrezGene annota-tions; we were simply using the EntrezGene sentences asa convenient source of the types of relations we designedmiRiaD to detect. The randomly chosen sentences men-tioned both a miR and a disease but were both positiveand negative for miR-disease associations. The annota-tors were asked to mark all the relations indicating amiR-disease association, such we can assess miRiaDsability to detect such relations.We downloaded the GeneRIF sentences from the Entrez-Gene database (ftp://ftp.ncbi.nih.gov/gene/GeneRIF). Thefile contained a total of 946,742 entries at the time of down-load (January 2015). Each entry in the downloaded file con-tains a taxonomy ID, a gene ID, a PMID list, the GeneRIFtext, and the timestamp of the last update. Using the geneID field, we extracted all the entries where the gene ID wasthat of a miR. Additionally, we looked in the GeneRIF textof these entries for the mention of at least one disease usingthe PubTator database [20] for the detection of diseasementions. This resulted in a set of 8476 GeneRIF entries.A set of 100 entries from all 8476 GeneRIF entries wasrandomly selected and presented to a second annotator.The annotator was asked to mark the GeneRIF sen-tences as relevant if they contained a relationship be-tween a miR and a disease mentioned within or notrelevant otherwise. Of the 100 sentences, 97 weremarked as relevant and 3 were marked as not relevant.In addition, the annotator marked all of the relations inthe sentence indicating an association between a miRand a disease, including miR and disease aspects andlinking entities. Because multiple miRs, diseases, andtypes of relationships could be found within the samesentence, the annotations yielded a total of 175 relations.We also randomly selected a second set of 100 sen-tences containing miR and disease mentions from Med-line abstracts. As discussed earlier, the reason forselecting an additional 100 sentences was to construct aless biased evaluation set that included some negativesentences. Ninety-two sentences were marked as rele-vant and contained a total of 159 relations; 8 sentenceswere marked as not relevant.Study 3: Evaluation of the miRiaD sentence informativenessrankingFinally, we evaluated our informativeness ranking approachusing a set of sentences containing miR and disease men-tions that were manually scored for informativeness by abiologist. We selected 100 random sentences from the 8476GeneRIF sentences that we obtained in the previous evalu-ation, plus 100 sentences randomly selected from Medlineabstracts. We asked an annotator to mark each sentencewith scores on a three-point Likert scale, depending onhow informative the sentence might be to a researcher in-terested in the disease, with a score of 1 indicating some-what informative, a score of 2 indicating informative,and a score of 3 indicating highly informative. The anno-tator marked 58 sentences as highly informative, 87 sen-tences as informative, and 55 sentences as somewhatinformative.Results and DiscussionStudy 1: Evaluation of miRiaD for assistance with manualcurationmiRiaD was applied to the sentences in 100 abstracts citedby miR2Disease and 100 randomly selected Medline ab-stracts (200 abstracts total) as described in Methods. Theresults of this evaluation are shown in Table 1. Forthe abstracts from miR2Disease, we obtained a recallof 90 % (81 true positives (TP) and 9 false negatives(FN)) for expression information and 89.6 % (52 TPand 6 FN) for target information. For the randomlyselected abstracts, we got very similar results: recallof 92.3 % for expression information and 83.3 % fortarget information. When we combined the two setsof abstracts, we obtained recalls of 90.78 % and88.46 % for expression and target information re-spectively. Using the randomly selected abstracts, wewere able to assess precision as well as recall. For ex-pression information we obtained precision of 92.3 %and f-score of 92.3 % (48 TP, 4 FN and 4 false posi-tives (FP)); for target information we obtained preci-sion of 97.5 % and f-score of 89.8 % for targetinformation (40 TP, 8 FN and 1 FP).Looking at the false negatives, we observed that mostof the errors were introduced by highly complex sen-tences, for which the iSimp tool could not generate use-ful simplified sentences. As a result, the syntactic andTable 1 Evaluation of miRiaD for assistance with manual curationTP FN TN FP Recall Precision F-scoremiR2Disease based setExpression 81 9 - - 90.0 - -Gene target 52 6 - - 89.6 - -Randomly chosen setExpression 48 4 46 4 92.3 92.3 92.3Gene target 40 8 52 1 83.3 97.5 89.8CombinedExpression 129 13 - - 90.78 - -Gene target 92 14 - - 88.46 - -Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 10 of 15lexical patterns could not be matched. An example is thefollowing sentence: Among 15 upregulated target genesof the miR-30 miRNA, four genes known to beexpressed and/or functional in podocytes were identi-fied, including receptor for advanced glycation end prod-uct, vimentin, heat-shock protein 20, and immediateearly response 3 (PMID 18776119). The four genes werenot identified as targets of miR-30 due to the inability tolink them to the four genes mention, and subsequentlyto the 15 upregulated target genes mention. Anotherclass of false negatives involves relations missed due to thepresence of anaphora (generally a pronoun referring to anentity). For example in the sentence fragment miR-181a, a small non-coding RNA believed to induce apop-tosis by repressing its target gene, BCL-2, the word itsrefers to miR-181a. We currently do not perform pro-noun resolution and hence miRiaD misses the target rela-tion with BCL-2. Looking at false positives, we observedmost of errors were introduced due incorrect identifica-tion of the simplification constructs. In the sentence theprotein inhibitor of activated STAT3 (PIAS3) was con-firmed as a direct miR-21 target, STAT3 was detected asthe parenthetical for PIAS3 and tagged as a target formiR-21 in addition to PIAS3.Study 2: Evaluation for general extraction of miR-diseaseassociationsFor the second study, 100 GeneRIF sentences fromEntrezGene entries for miRNA and 100 sentences withmiR and disease mentions randomly chosen from Med-line abstracts were processed by miRiaD (see Methods).The results of this evaluation are shown in Table 2. miR-iaD was able to identify relevant relations in 91 sen-tences from the GeneRIF set and 93 sentences from theMedline set. For calculation of TP, FN and FP wematched the relations annotated by the annotators withthe relations extracted by miRiaD. This yielded recall of84.0 %, precision of 94.8 %, and f-score of 89.1 % withrespect to GeneRIF sentences (147 TP, 28 FN and 8 FP)and recall of 84.2 %, precision of 96.4 %, and f-score of89.8 % with respect to the additional Medline sentences(134 TP, 25 FN and 5 TP). As in the previous evaluationwe do not see a significant difference in performancebetween the two evaluation sets. The combined f-scorefor all 200 sentences was 89.4 %.Looking at the false negatives in this evaluation re-vealed an additional type of error. We observed certainsyntactic dependencies between events that should becaptured, but which are not. For example, the followingsentence contains a temporal relation, which is triggeredby the word after: These data indicate for the firsttime a mechanism involving STAT1/2 upregulationunder the transcriptional control of INF-alpha signalingafter knockdown of miR-221/222 cluster in U251 gliomacells (PMID 20428775). The relationship [miR-221/222,negatively regulates, STAT1] could not be extractedbecause of the systems inability to detect the temporalrelation. As in or previous evaluation, we observed falsenegatives due to anaphora. For example, in the sentencefragment the tumor suppressor activity of miR-124could be partly due to its inhibitory effects on gliomastem-like traits and invasiveness, miRiaD misses theinhibitory relations between mir-124 and stem-liketraits and invasiveness.Most of false positives errors in sentences had an in-volvement relation between a miR and a disease inaddition to a more informative relation. For example inthe sentence this study further extends the biologicalrole of miR-92b in non-small cell lung cancer A549 cellsand for the first time identifies PTEN as a novel targetof miR-92b, miRiaD extracted an involvement relationbetween miR-92b and non-small cell lung cancer thatwas not marked as relevant by the annotator. Instead,the annotator only marked the relation between mir-92b and the target gene PTEN as relevant. This typeof errors will constitute the grounds for future work.Study 3: Evaluation of the miRiaD sentenceinformativeness rankingThis evaluation was conducted on 100 sentences ran-domly selected from GeneRIF set used for the previousevaluation 100 sentences randomly selected from Medlineabstracts, which were marked for their informativeness ona three-point scale by an annotator (see Methods). miRiaDranked 57 sentences as highly informative, 97 sentencesas informative, and 48 sentences as somewhat inform-ative. The comparisons between the annotators scoresand the miRiaD scores are shown in Table 3. Of the sen-tences ranked as highly informative by the system, mostof them (45/57) were also ranked as highly informativeby the annotator; the remaining 12 sentences were rankedas informative by the annotator. Looking at the diagonalof the table, we observe that the majority of the scores inall three categories were in agreement. The average scoredifference on the 3-point scale (absolute value) betweenannotator score and the miRiaD system score is 0.29.Various correlation measures were computed, where avalue of 1 is total positive correlation, 0 is no correlation,and ?1 is total negative correlation. First, we computedTable 2 Evaluation for general extraction of miR-diseaseassociationsTP FN TN FP Recall Precision F-scoreGeneRIF based Set 147 28 1 8 84.0 94.8 89.1Randomly chosen set 134 25 7 5 84.2 96.4 89.8Combined 281 53 8 13 84.1 95.5 89.4Gupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 11 of 15the Pearson product-moment correlation coefficient,which is widely used as a measure of the degree of lineardependence between two variables (0.727). Second, wecomputed the Kendall tau rank correlation coefficient,which measures the association between two measuredquantities by looking at the similarity of the orderings ofthe data when ranked by each of the quantities (0.678). Fi-nally, we computed the Spearmans rank correlation coef-ficient, which assesses how well the relationship betweentwo variables can be described using a monotonic func-tion, and got a value of 0.724, which indicates a verystrong positive relationship [35]. The p-values for thesecorrelation scores are very low, indicating that the null hy-pothesis of no correlation is extremely unlikely. To testagainst a stronger baseline than no correlation, we ran-domly reordered the annotated scores 10,000 times andcalculated the Pearson correlation coefficient. The meancorrelation was 0.544, and none of the random correlationvalues were above the reported correlation value 0.727.We also computed the normalized discounted cumula-tive gain (nDCG) [36, 37], which compares the orderingof a list by a system (e.g., miRiaD) with the perfect or-dering of that list by gold standard (e.g., our annotator).Because the sentences were given scores between 1 and3, and not actual ranks, multiple possible orderings arepossible when displaying the sentences marked with 3first, then the sentences marked with 2 second, andfinally the sentences marked with 1 at the bottom. Forthis reason, we considered 10,000 different possible or-derings. The mean of the nDCGs scores obtained thisway was 0.977, with the lowest being 0.950 and the me-dian being 0.978. The high nDCG values could be dueto the strong agreement between miRiaD and the anno-tator for the sentences marked as highly informative.Browsing the results onlineWe have developed a preliminary website for interactivequery of miRiaD miR-disease association extraction. Theinterface accepts PubMed-like queries as input, thussupporting queries like a miR name, or a disease name,or any biological concept. For example, a user interestedin miR-9 and breast cancer can submit a query suchas mir-9 AND breast cancer. To ensure that all of theabstracts passed to miRiaD contain a miR mention, we re-strict the user query by appending the AND operatorand miRNA keywords connected by the OR operator,i.e., query AND (miRNA[TIAB] OR microRNA[TIAB] ORmiR[TIAB]). The system then submits the query toPubMed, which returns all the PMIDs satisfying the query.miRiaD processes this list of PMIDs and displays the trip-lets < miR,Disease,Text Evidence/PMID>. The interface isavailable at the URL: http://biotm.cis.udel.edu/miRiaD.Figure 3ad provides screenshots of the interface.Figure 3a. shows the search form where the user sub-mits his/her query. Figure 3b. shows the triplet view inthe table after submitting the query mir-9 ANDbreast cancer. There are three columns in the tripletview, namely: the miR, Disease and Text Evidence.Note that the results also contains triplets for othermiR (mir-151, miR-200) and other diseases (hepatocel-lular carcinoma, Hodgkins lymphoma etc.). This is dueto the fact that miRiaD processses the entire abstractfor each PMID returned by the submitted query andsome abstracts may contain mentions of multiple miRand/or diseases. Results can be filtered to include onlythose where mir-9 is the miR and breast cancer is the dis-ease using the drop-down menus above each column asshown in Fig. 3c. Finally, clicking on the PMID in the TextEvidence column takes the user to the abstract (Fig. 3d.),where sentences indicating the miR-disease association areunderlined and the respective miRs and diseases arehighlighted in bold.Browsing the highlighted sentences in the 13 PMIDsshown in Fig. 3c reveals that miR-9 has been associatedwith a number of breast cancer phenotypes including me-tastasis, invasiveness, aggressiveness, cell motility, andpoor prognosis. miR-9 targeting of E-cadherin (CDH1)has been implicated in promotion of cell motility and in-vasiveness. Variations in miR-9 expression related to miR-9 promoter hypermethylation have also been observed inthe disease. As a consequence of these findings, miR-9 hasbeen suggested as both a potential biomarker and thera-peutic target. Interestingly, one study (PMID: 22761433)reported that miR-9 targeting of the mitochondrial en-zyme, MTHFD2, mediated a tumor suppressive effect inbreast cancer, and in contrast to other studies, found thatmiR-9 inhibited invasion. More careful consideration ofthe contextual information in these articles may help toresolve this apparent contradiction. This small example il-lustrates the wide variety of miRNA-disease informationthat can be easily obtained using miRiaD.ConclusionIn this paper, we have presented miRiaD, a relation ex-traction system that automatically extracts associationsbetween miRs and diseases from the literature. TheTable 3 Evaluation of the miRiaD sentence informativenessrankingAnnotated HighlyinformativeInformative SomewhatinformativeTotalSystemHighly informative 45 12 0 57Informative 11 64 20 97Somewhat informative 2 11 35 48Total 58 87 55 200Row values correspond to the frequency of scores assigned by miRiaD, whilethe columns denote the annotator score frequencyGupta et al. Journal of Biomedical Semantics  (2016) 7:9 Page 12 of 15miR-disease associations are sometimes indirect, withmiR and disease connected by a linking entity, such as atarget gene or cellular process. Similarly, miRs and dis-eases might be mentioned in terms of their aspects:(e.g., state or expression level for miRs or outcomes,diagnostics/treatments, or therapy information for dis-eases). Our method, which entails detecting syntacticdependencies and, subsequently, semantic relations be-tween miRs (or their aspects) and linking entities in thecontext of a disease or disease aspect, has demonstratedhigh performance (recall, precision, and F-scores) inidentifying information of interest to both database cu-rators and biomedical researchers studying the connec-tions between miRs and disease. Additionally, we haveshown how assigning scores to sentences containing amiR-disease association, based on syntactic dependencies,semantic relations, and linking entities, highly correlatesRESEARCH Open AccessUsing Semantic Web technologies for thegeneration of domain-specific templates tosupport clinical study metadata standardsGuoqian Jiang1*, Julie Evans2, Cory M. Endle1, Harold R. Solbrig1 and Christopher G. Chute3AbstractBackground: The Biomedical Research Integrated Domain Group (BRIDG) model is a formal domain analysis modelfor protocol-driven biomedical research, and serves as a semantic foundation for application and message developmentin the standards developing organizations (SDOs). The increasing sophistication and complexity of the BRIDG modelrequires new approaches to the management and utilization of the underlying semantics to harmonize domain-specificstandards. The objective of this study is to develop and evaluate a Semantic Web-based approach that integrates theBRIDG model with ISO 21090 data types to generate domain-specific templates to support clinical study metadatastandards development.Methods: We developed a template generation and visualization system based on an open source ResourceDescription Framework (RDF) store backend, a SmartGWT-based web user interface, and a mind map based tool forthe visualization of generated domain-specific templates. We also developed a RESTful Web Service informed by theClinical Information Modeling Initiative (CIMI) reference model for access to the generated domain-specific templates.Results: A preliminary usability study is performed and all reviewers (n = 3) had very positive responses for theevaluation questions in terms of the usability and the capability of meeting the system requirements (with theaverage score of 4.6).Conclusions: Semantic Web technologies provide a scalable infrastructure and have great potential to enablecomputable semantic interoperability of models in the intersection of health care and clinical research.Keywords: BRIDG, RDF, CIMI, Doman analysis model, Clinical study meta-data standards, Detailed clinical model,Semantic Web technologiesIntroductionThe Biomedical Research Integrated Domain Group(BRIDG) model is a formal domain analysis model forprotocol-driven biomedical research, and serves as thesemantic foundation for application and message devel-opment in the standards developing organizations(SDOs) [1, 2]. The increasing sophistication and com-plexity of the BRIDG model requires new approaches tothe management and utilization of the underlying se-mantics to harmonize domain-specific standards.A typical use case for the BRIDG model comes fromthe Clinical Data Interchange Standards Consortium(CDISC) [3]. CDISC initiated the Shared Health AndClinical Research Electronic Library (SHARE) project tobuild a global, accessible electronic library, which en-ables standardized data element definitions and richermetadata to improve biomedical research and its linkwith healthcare [4]. In it, CDISC envisioned integrateddomain-specific templates built from the classes and at-tributes from the BRIDG model and ISO 21090 datatypes as a foundation for the definition of research con-cepts in the therapeutic target areas.The CDISC SHARE approach to domain-specific tem-plates has much in common with an international col-laboration effort initiated by the Clinical Information* Correspondence: jiang.guoqian@mayo.edu1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,Rochester, MN 55905, USAFull list of author information is available at the end of the article© 2016 Jiang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jiang et al. Journal of Biomedical Semantics  (2016) 7:10 DOI 10.1186/s13326-016-0053-5Modeling Initiative (CIMI) [5], an international collab-oration that is dedicated to providing a common formatfor detailed specifications for the representation ofhealth information content so that semantically inter-operable information may be created and shared inhealth records, messages and documents [6]. While thedomain-specific templates defined in CDISC SHARE arefocused on clinical research and CIMI is more focused onelectronic health records (EHR) and secondary use ofEHR data, we see the semantic interoperability of the twomodels as critical for predictable exchange of meaning be-tween two or more systems in the area of health care andclinical research. We also believe that the emergingSemantic Web technologies based on World Wide WebConsortium (W3C) standards can provide much of the in-frastructure and tools needed to accomplish this goal.The W3C standards include the Resource DescriptionFramework (RDF) and the Web Ontology Language(OWL) [7, 8], which provide a scalable framework forsemantic data integration, harmonization and sharing.These technologies are beginning to appear in both clin-ical research and health care workspaces and have beenleveraged in several notable projects, including the UKCancerGrid [9], the US caBIG [10] and the NationalCenter of Biomedical Ontologies (NCBO) [11]. The Se-mantic Web Health Care and Life Sciences (HCLS) Inter-est Group has been formed under the auspices of theW3C to develop, advocate for and support the use of theSemantic Web technologies across the domains of healthcare, life sciences, clinical research and translational medi-cine [12]. In some of our previous studies, we exploredthe use of OWL to represent clinical study metadatamodels such as HL7 Detailed Clinical Models (DCMs)[13] and the ISO/IEC 11179 model [14], and investigateda Semantic Web representation of the Clinical ElementModel (CEM) for secondary use of the EHR data [15, 16].The objective of this study is to develop and evaluate aSemantic Web-based approach that integrates theBRIDG model with ISO 21090 data types to generatedomain-specific templates to support clinical studymetadata standards development. The main purpose ofthe tools developed in this study is to support SDOssuch as CDISC to create information models that canenable data exchange between clinical care systems(e.g., in a CIMI model) and clinical trial systems (e.g.,in a BRIDG model). In it we developed a template gen-eration and visualization system based on an opensource Resource Description Framework (RDF) storebackend, a SmartGWT-based web user interface, and amind map based tool for the visualization of gener-ated domain-specific templates. We also created aRESTful Web Service informed by the Clinical Informa-tion Modeling Initiative (CIMI) reference model for ac-cess to the generated domain-specific templates. Apreliminary usability study is performed to evaluate thesystem in terms of the ease of use and the capability formeeting the requirements using a selected use case.BackgroundBRIDG modelIn 2004, CDISC initiated the Biomedical Research Inte-grated Domain Group (BRIDG) in collaboration withHL7 and National Cancer Institute (NCI). The collabor-ation effort developed a domain analysis model that is ashared view of the dynamic and static semantics for thedomain of protocol-driven research and its associatedregulatory artifacts [1]. The BRIDG model was basedon the HL7 Development Framework. Multiple repre-sentations of the model were introduced in the BRIDG3.0 release, including the canonical Unified ModelingLanguage (UML)based representation, a HL7 Refer-ence Information Model (RIM)-based representationand a ontological representation in a single OWL file.Figure 1 shows BRIDG multiple-perspective representa-tions in UML, HL7 RIM and OWL.CDISC standards developmentThe mission of CDISC is to develop and support global,platform-independent data standards that enable infor-mation system interoperability to improve medical re-search and related areas of healthcare [17]. Over thepast decade, CDISC has fulfilled its mission by publish-ing and supporting a suite of standards that enable theelectronic interchange of data throughout the lifecycle ofa clinical research study [18].Specifically, CDISC has developed standards for useacross the various points in the research study lifecycle: Planning: Protocol Representation Model Version 1,which includes Study Design, Eligibility Criteria andClinical Trial Registration Data Collection:o Clinical Data Acquisition StandardsHarmonization (CDASH) for the collection ofdata through case report formso Operational Data Model (ODM) for thecollection of operational data through electronicdata exchangeo Laboratory Model (LAB) for the collection ofclinical laboratory data through electronic dataexchange Data Tabulationso Study Data Tabulation Model (SDTM) forsubmission of human subject data to regulatoryagencieso Standard for the Exchange of Nonclinical Data(SEND) for submission of non-human subject datato regulatory agenciesJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 2 of 10 Statistical Analysis: Analysis Data Model (ADaM)for submission of statistical analysis data toregulatory agencies.Clinical information modeling initiativeThe Clinical Information Modeling Initiatives (CIMI)was officially launched in July, 2011 with more than 23participating organizations. The initiative was establishedto improve the interoperability of healthcare informa-tion systems through shared implementable clinical in-formation models [5]. The principles of the CIMIinclude 1) CIMI specifications will be freely available toall. 2) CIMI is committed to making these specificationsavailable in a number of formats. 3) CIMI is committedto transparency in its work and product. The goals ofthe CIMI include: 1) shared repository of detailed clin-ical information models; 2) a single formalism; 3) a com-mon set of base data types; 4) formal bindings of themodels to standard coded terminologies; and 5) reposi-tory is open and models are free for use at no cost. As ofMay 7, 2013, CIMI is finalizing its reference model spe-cification that consists of a core reference model, a datavalue type model and a party model.Semantic Web technologiesThe World Wide Web Consortium (W3C) is the maininternational standards organization for the World WideWeb [7]. Its goal is to develop interoperable technolo-gies and tools as well as specifications and guidelines torealize the full potential of the Web. The W3C toolsand specifications that we used in this study includethe Resource Description Framework (RDF) [8], RDFSchema (RDFS) [19], the Web Ontology Language(OWL), OWL 2 [20], the Simple Knowledge OrganizationSystem (SKOS) [17], the SPARQL Protocol and RDFQuery Language (SPARQL) [21], and the SPARQL Infer-ence Notation (SPIN) [22], which is a W3C MemberSubmission that can be used to represent SPARQLrules and constraints on Semantic Web models.MethodsSystem requirementsThe system requirements for this study were based on aCDISC SHARE project, in which building domain-specific templates based on BRIDG model is an essentialprocess for clinical study metadata standards develop-ment. These requirements include: Selection from multiple BRIDG classes. Forexample, describing a measurement on a subject(such as vital signs like body temperatures) mayinclude the BRIDG classes Defined Observation,Defined Observation Result, Performed Observation,Performed Observation Result and Reference Result. Selection of specific attributes from each selectedBRIDG class. The attributes include the inheritedattributes from its parent classes. For example whenselecting attributes based on a BRIDG class Person,the inherited attributes (e.g., name, birthDate, etc.)from its parent class Biologic Entity shall be availablefor the selection. Specification of the subcomponents of the data typefor a specific attribute of a BRIDG class. BRIDGattributes are associated with ISO 21090 data types,each of which has multiple components with itsown data type, which may also be a complex. Usingthe BRIDG class Person as an example, the attributeeducationLevelCode has the data type CD. CD, inturn has a set of components including code,displayName, codeSystem, codeSystemName,Fig. 1 BRIDG multiple-perspective representations in UML, HL7 RIM and OWLJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 3 of 10codeSystemVersion, valueSet, etc. Each of whichcomponents has their own data type. Selection of attributes from the BRIDG classes thatlink to a selected BRIDG class through potentialassociation relationships. For example, through theassociation be reported by, the class PerformedObservation links to a set of BRIDG classesincluding Subject, Healthcare Provider, Laboratory,Device, etc. The attributes from associated classesare available for building a domain-specific template. Provide a standard representation of generatedtemplates, which is scalable for supportingdownstream development and harmonization ofclinical study metadata standards.System architectureFigure 2 shows the system architecture. The systemcomprises the following modules: 1) a normalizationpipeline module; 2) a backend module that uses a RDFstore; 3) a frontend module that includes a BRIDGmodel browser, a template generation mechanism and amind map viewer for generated templates.ImplementationMaterialsBRIDG model in OWL In the release of the BRIDGversion 3.2, an ontological perspective, i.e., OWL repre-sentation of BRIDG semantics is developed for theBRIDG model. For this release, the scope of the OWLcontents is limited to the information found in theBRIDG UML model. In this study, we used the OWLrendering of the BRIDG model that is publicly availablefrom the release package of the BRIDG 3.2 [1].HL7 V3 data types in OWL The HL7 OWL project haspublished an initial draft of the Core HL7 V3 in OWL.The publicly available draft was released on January2013 and can be downloaded from the HL7 OWL pro-ject web site [23]. In this study, we use the HL7 OWLrendering of HL7 V3 data types in place of the ISO21090 equivalents.Backend implementationWe started with the 4store, an open source RDF storedeveloped at Garlik [24]. We then loaded the RDF imageBRIDG model and HL7 V3 data types in OWL into twoseparate graphs. We also established a SPARQL end-point that provides standard query services against theRDF store backend.To make all of the inherited attributes and associationsexplicit for each BRIDG class, we used Jena ARQ API-based script [25] that recursively retrieved the attributesand associations from parent classes of each BRIDGclass and materialized them explicitly using two BRIDGpredicates: bridg:attributeProperty and bridg:association-Property. We also used a template, spl:Attribute, fromthe SPARQL Inference Notation (SPIN) to model themetadata of each attribute and association, including thecardinality and a predicate bridg:isInherited indicatingwhether the target attribute or association is inherited ornot. Figure 3 shows an example of the flattened repre-sentation for an association and an attribute of theBRIDG class Person. Following this, we combined thenamespaces used for the HL7 V3 data types and theOWL renderings of the BRIDG models.Frontend implementationBuilding a BRIDG model browser and a templategeneration mechanism We developed a BRIDG modelbrowser as a web application based on the SmartGWTAPI [26]. SmartGWT is a Google Web Toolkit (GWT)-based framework that allows users to utilize its compre-hensive widget library for user interface development.The browser displays a hierarchical tree of BRIDG classes(see Fig. 4). For each class, the browser displays ametadata structure comprising Children, Attributes andAssociations, which streamlined those metadata associ-ated with each class. We defined a set of SPARQL queriesto retrieve the children, attributes and associations foreach class. Figure 5 shows a SPARQL query to retrieve allattributes associated with the BRIDG class Person.If a BRIDG class has children, they will be displayedunder the folder Children. The Attributes folder displaysall inherited and non-inherited attributes and their datatypes. Separate icons are used to differentiate which at-tributes are local vs. inherited. The sub-components aredisplayed for complex data types. As an example, theupper right corner of Fig. 4 shows the sub-componentsof the data type CD for the attribute maritalStatusCode.Fig. 2 A diagram illustrating the system architectureJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 4 of 10Fig. 3 An example of flattened representation for an association and an attribute of the BRIDG class Person using a SPIN templateFig. 4 A customized BRIDG model browser with a metadata structure for each class. In the left hand panel, a hierarchical tree of BRIDG classes isdisplayed. In the right upper part, it displays nested sub-components and their selection for the data type (i.e., CD) of an attribute Person.maritalStatusCode.In the right lower part, it displays the associations of the class PersonJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 5 of 10Data type sub-components can be expanded to displayinterior data types.The Associations folder shows inherited and non-inherited associations with icons representing their in-heritance status. The associated class will be displayedand it can be expanded to show its corresponding struc-ture. The lower right hand of Fig. 4 shows the expansionof the Associations folder for the class Person.We also developed a template generation mechanismby allowing selection of specific elements in the BRIDGmodel browser. A target template can be constructedfrom the attributes (including data type components)from one or more BRIDG classes. Based on the systemrequirements, a set of rules is applied when users maketheir selections. The upper right hand part of Fig. 4shows the user selecting the data type ST data type ofthe CD.displayName component with the full path ofthe selected attribute used as the attribute name:Person.maritalStatusCode.CD.displayName.ST.A generated template with a set of selected attributes(including data type components) can be rendered as amind map. We use the Freemind browser [27] to dis-play a target mind map.A CIMI reference model-based Semantic Web repre-sentation of generated domain templates We createda mapping between CDISC standard objects and CIMIreference model elements. In it a domain-specific tem-plate corresponds to the CIMI element ENTRY (the lo-gical root of a single clinical statement within a clinicalsession) and the component BRIDG classes and BRIDGattributes correspond to the CIMI element CLUSTER (aset of ELEMENTs) and ELEMENT (a type of data ITEM,which does not itself contain ITEMs) respectively. Usingthis mapping, we were able to create a CIMI-complaintSemantic Web representation for generated BRIDGdomain-specific templates. Figure 6 shows an exampleof a CIMI-compliant Semantic Web representation for adomain-specific template generated from the BRIDGclass AdverseEventSeriousness. As illustrated, we usedthe elements from the CIMI reference model, such ascimi:ENTRY, cimi:CLUSTER, cimi:ELEMENT, and cimi:-CLUSTER.item. We also used the SPIN template spl:at-tribute to attach the metadata of each selected attributeincluding the cardinality.We then developed the RESTful Web Service thatprovides programmatic and browser access to the CIMIreference model-based representations of the domain-specific templates. As an example, the CIMI referencemodel-based representation for the AdverseEventSer-iousness domain in Turtle format is shown in Fig. 6.Results and discussionSystem evaluationWe performed a preliminary evaluation on the system interms of the usability and the capability of meeting thesystem requirements as described in the Section 3. Forthe evaluation design, we created a use case test scriptthat describes the use case of generating a templateMeasurement on a Subject. The target of the use caseis to develop a template that covers 5 BRIDG classes, 20BRIDG attributes and 5 BRIDG associations. We re-cruited three reviewers: one reviewer (JE, a co-author)from CDISC SHARE team who has extensive expertiseon BRIDG model and clinical study metadata standarddevelopment, and two other reviewers who are biomed-ical informatics researchers. We arranged a teleconfer-ence meeting and introduced the background of theproject and demonstrated the basic features and usagesof our frontend widgets to them. We made the web ap-plication accessible to the three reviewers who followedthe test script to build a template for the target use case.Each reviewer worked individually to complete the testcase. After they completed, the three reviewers are askedto answer the evaluation questions in a 1-5 scale, inwhich 1 stands for Strongly disagree, 2 for disagree, 3Fig. 5 A SPARQL query to retrieve all attributes associated with the BRIDG class PersonJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 6 of 10for neutral, 4 for agree and 5 for Strongly agree.The preliminary results indicated that all three reviewerssuccessfully created the template as described in the testscript. All reviewers had very positive responses for theevaluation questions in terms of the usability and thecapability of meeting the system requirements (with theaverage score of 4.6). The reviewers also provided free-text feedback on the system. Some of comments include1) the suggestion to add a search button for users wholook for a particular class and attribute; 2) the suggestionthat the icon used for the folder Children could be mis-leading and confusing; 3) the issues for displayingFreemind map in different browsers; 4) the suggestion ofallowing multiple ways to de-select an attribute; 5) thesuggestion of allowing to reload the generated templatefor modification; 6) the suggestion of allowing to con-strain the data type of ANY in a specific data type.DiscussionIn this study, we designed, developed and evaluated aBRIDG-based domain-specific template generation andvisualization system for supporting clinical study meta-data standards development. We consider that the sys-tem and approach developed in this study are significantFig. 6 A CIMI-compliant Semantic Web representation in the Turtle format for a domain-specific template generated from theclass AdverseEventSeriousnessJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 7 of 10in both domain specific perspective and technicalperspective.Domain specific significanceThe system requirements were derived directly from areal-world CDISC SHARE project [4], which demon-strated that a scalable mechanism for access and modu-lar use of the BRIDG model elements is essential forsupporting metadata standards development. With theincreasing complexity of the BRIDG model, the BRIDGdevelopment team has made efforts to deal with thescalability issue. One example is the six subdomainviews, Adverse Event, Common, Protocol Representa-tion, Regulatory, Statistical Analysis, and Study Conduct,which help domain experts to navigate subsets of thedomain semantics. In addition, multiple representationsas described in the Background section are used to meetthe requirements from different use cases. In this study,we focused on the domain-specific template generationuse case and developed a customized BRIDG browserthat enables the standards developer to interact with theBRIDG model elements. Specifically, we streamlined themetadata for each BRIDG class using a metadata struc-ture of Children, Attributes and Associations. The pre-liminary evaluation demonstrated the positive results interms of the ease of use and the capability to meet thesystem requirements. In addition, the generated domain-specific templates can be rendered in a Mind Map view,which has been widely used in the standards develop-ment community. Furthermore, we developed a Seman-tic Web representation informed by CIMI referencemodel for the generated domain-specific templates, pro-viding a modular representation for a specific domainexposed as a standard RESTful service. This will enablesemantic harmonization with other CIMI-compliantmodels, potentially developed from different contexts.Technical significanceSemantic Web technologies played a critical role in thesystem design and development. First, the RDF datamodel and the triple store technology enabled data inte-gration of the BRIDG model and ISO 21090 data typemodel. All BRIDG attributes have defined data typesbased on ISO 21090. For those complex data types, theyhave multiple components. Some of the components ofa complex data type are required for a domain-specifictemplate. For example, the CD data type has the compo-nents valueSet and valueSetVersion that can be used forthe valueset binding. Utilizing the Semantic Web OWL/RDF version of the two models, we were able to seam-lessly link the data type defined for each BRIDG attri-bute with their components defined in the ISO 21090data type model. Note that we unified the namespacesused for the data types in the two models for the inte-gration purpose.Second, the subsumption property, rdfs:subClassOf,asserted in the OWL/RDF version of the BRIDG modelprovides an elegant way to compute and retrieve theinherited attributes and associations from parent classesfor a BRIDG class. The BRIDG model is authored in theUML, in which a child class should inherit all assertedattributes/associations from their parent classes, just asin object-oriented model. Being able to browse and se-lect the inherited attributes/associations is one of keysystem requirements for domain-specific template gen-eration. As part of the normalization pipeline, we re-trieved and materialized all inherited attributes/associations for each BRIDG class, which streamlinedthe metadata of each BRIDG class and made the attri-bute selection straightforward to users.Third, a SPARQL endpoint was established to providestandard SPARQL query services for accessing the con-tent of the BRIDG model elements. We defined a set ofSPARQL queries to extract the metadata for eachBRIDG class. We found that the normalization pipelineas we implemented it was very helpful to simplify thequery building. For example, as we materialized theinherited attributes and associations for each BRIDGclass, building the SPARQL queries for retrieving thiskind of metadata was simplified. In addition, theSPARQL endpoint based on 4store implementation sup-ports SPARQL 1.1 update features, which enables thestorage and update of generated domain-specific tem-plates with their provenance information and providespotential for future authoring application development.Fourth, a CIMI-compliant Semantic Web representa-tion was developed for representing the generateddomain-specific templates and the elements from theCIMI reference model were used. As we mentionedabove, the CIMI is finalizing its reference model. A Se-mantic Web representation of the CIMI reference modeland its compliant clinical information models is one ofkey tasks envisioned by the CIMI community. We con-sider that our current efforts in this study would provideuseful experiences and test cases for the CIMI commu-nity. In addition, we used a SPIN template to representthe metadata of an attribute in a domain-specific tem-plate. The SPIN framework is designed to represent theSPARQL rules and constraints in Semantic Web models.SPARQL rules are a collection of RDF vocabulary thatbuilds on the W3C SPARQL standard to let us definenew functions, stored procedures, constraint checking,and inference rules for Semantic Web models. The rulesare all stored using object-oriented conventions and theRDF and SPARQL standards. We expect that the SPINframework will provide a natural way to represent theconstraints and rules in a CIMI-compliant model andJiang et al. Journal of Biomedical Semantics  (2016) 7:10 Page 8 of 10enable an automatic mechanism for model validationand consistency checking.Limitations and future studyThere are several limitations in the study. First, a morerigorous evaluation from a panel of domain experts frombroader communities would be helpful in the future.The system will be iteratively enhanced based on thefeedback from the evaluators. For example, the searchfunctionality would be helpful to allow users to find atarget class/attribute more quickly. Second, the systemevaluation was limited to the ease of use and the fulfill-ment of those basic requirements. We have not evalu-ated the system in terms of the CIMI conformance forgenerated domain-specific templates. We are activelyworking with the CDISC SHARE and CIMI communi-ties to review the current prototype representation. Oneof main tasks is to develop the mappings between theISO 21090 data types used in the BRIDG model and thedata type defined in the CIMI reference model.ConclusionsIn summary, we developed and evaluated a SemanticWeb based approach that integrates the model ele-ments from both BRIDG model and ISO 21090 modeland enables a domain-specific template generation mech-anism for supporting clinical study metadata standards de-velopment. The source code of the application areavailable from the project GitHub website at https://github.com/caCDE-QA/bridgmodel. We demonstratedthat Semantic Web technologies provide a scalable infra-structure and have great potential to enable computablesemantic interoperability of models in the intersection ofhealth care and clinical research.Availability of supporting dataThe data set(s) supporting the results of this articleis(are) included within the article (and its additionalfile(s)).AbbreviationsBRIDG: The Biomedical Research Integrated Domain Group; SDO: Standardsdeveloping organizations; RDF: Resource Description Framework; CIMI: TheClinical Information Modeling Initiative; CDISC: The Clinical Data InterchangeStandards Consortium; SHARE: The Shared Health And Clinical ResearchElectronic Library; EHR: Electronic health records; W3C: World Wide WebConsortium; OWL: The Web Ontology Language; NCBO: National Center forBiomedical Ontology; HCLS: The Semantic Web Health Care and LifeSciences; DCMs: Detailed Clinical Models; CEM: Clinical Element Model;NCI: National Cancer Institute; UML: Unified Modeling Language; RIM: ReferenceInformation Model; CDASH: Clinical Data Acquisition Standards Harmonization;ODM: Operational Data Model; LAB: Laboratory Model; SDTM: Study DataTabulation Model; SEND: Standard for the Exchange of Nonclinical Data;ADaM: Analysis Data Model; SKOS: The Simple Knowledge Organization System;SPIN: The SPARQL Inference Notation; GWT: Google Web Toolkit.Competing interestsThe authors declare that they have no competing interests.Authors contributionsConceived and designed the study: GJ, HRS. Developed the system: CME, GJ.Designed and conducted the system evaluation: JE, GJ. Wrote the paper: GJ,HRS, JE, CGC. All authors read and approved the final manuscript.AcknowledgmentsThe authors thank Dr. Chunhua Weng from Columbia University and Dr. CuiTao from Mayo Clinic who participated in the evaluation. The authors alsothank the technical support from Mr. Craig Stancl from Mayo Clinic. Thestudy is supported in part by the SHARP Area 4: Secondary Use of EHR Data(90TR000201).Author details1Department of Health Sciences Research, Mayo Clinic, 200 First St SW,Rochester, MN 55905, USA. 2Clinical Data Interchange Standards Consortium(CDISC), Austin, TX, USA. 3Johns Hopkins University, Baltimore, MD, USA.Received: 30 May 2014 Accepted: 2 December 2015Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 DOI 10.1186/s13326-016-0080-2RESEARCH Open AccessCOMODI: an ontology to characterisedifferences in versions of computationalmodels in biologyMartin Scharm1*, Dagmar Waltemath1, Pedro Mendes2,3 and Olaf Wolkenhauer1,4AbstractBackground: Open model repositories provide ready-to-reuse computational models of biological systems. Modelswithin those repositories evolve over time, leading to different model versions. Taken together, the underlyingchanges reflect a models provenance and thus can give valuable insights into the studied biology. Currently,however, changes cannot be semantically interpreted. To improve this situation, we developed an ontology of termsdescribing changes in models. The ontology can be used by scientists and within software to characterise modelupdates at the level of single changes. When studying or reusing a model, these annotations help with determiningthe relevance of a change in a given context.Methods: Wemanually studied changes in selected models from BioModels and the Physiome Model Repository.Using the BiVeS tool for difference detection, we then performed an automatic analysis of changes in all modelspublished in these repositories. The resulting set of concepts led us to define candidate terms for the ontology. In afinal step, we aggregated and classified these terms and built the first version of the ontology.Results: We present COMODI, an ontology needed because COmputational MOdels DIffer. It empowers users andsoftware to describe changes in a model on the semantic level. COMODI also enables software to implementuser-specific filter options for the display of model changes. Finally, COMODI is a step towards predicting how achange in a model influences the simulation results.Conclusion: COMODI, coupled with our algorithm for difference detection, ensures the transparency of a modelsevolution, and it enhances the traceability of updates and error corrections. COMODI is encoded in OWL. It is openlyavailable at http://comodi.sems.uni-rostock.de/.Keywords: Ontology, Modelling, Difference detection, SBML, CellML, Version controlBackgroundModelling plays an important role in the life sciences.The multi-disciplinary approach requires scientists toreuse other work. This task is greatly supported by com-mon languages to describe model-based results [1]. Com-putational models of biological systems are frequentlydescribed in XML standard formats such as the SystemsBiology Markup Language (SBML, [2]) or CellML [3].These formats have several advantages: Models can be*Correspondence: martin.scharm@uni-rostock.deEqual contributors1Department of Systems Biology and Bioinformatics, University of Rostock,Rostock, GermanyFull list of author information is available at the end of the articlesimulated, analysed, and visualised using different soft-ware tools; models encoded in standard formats mayoutlive the tool used to create the model; model exchangebecomes feasible; and models can more easily be shared,published, and reused. SBML and CellML focus on encod-ing the biological network, the mathematics, and thedynamics of the system. This information enables thetechnical reuse of model code. However, sustainablemodel reuse requires a basic understanding of (i) thebiological background, (ii) the modelled system, and (iii)possible parametrisations under different conditions. Forthis purpose, terms from ontologies and controlled vocab-ularies can be linked to the model, adding a semanticlayer.© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 2 of 8Bio-ontologies are formal representations for areas ofknowledge [4]. They clarify the intended semantics ofthe data, which makes the data more accessible, sharable,and interoperable [5]. An ontology is a tool to providemeaning to data, the information of which can then besubjected to algorithmic processing [6, 7]. For example,the Gene Ontology [6] provides additional information onthe genomic level, the NCBI Taxonomy [8] provides infor-mation about the nomenclature of species, and UniProt[9] provides information about proteins. We believe thata similar approach should be taken for the semanticdescription of differences between versions of a model.Using the semantic layer to describe changes in a modelallows for storing meaning together with possible implica-tions of these changes. Changes can then be filtered andanalysed automatically.Models evolve over time. Both before and after pub-lication, regular changes lead to different versions of amodel [10]. For example, modellers test different hypothe-ses, maintainers of databases initially curate the model,and other scientists later on correct or extend it. Specif-ically, semantic annotations are added, parameter valuesare updated, errors are corrected, models are adopted tochanges in the underlying format, etc. On average, amodelchanges 4.87 times during the first five years after publish-ing in an open repository1. It is important to track thesechanges for a number of reasons. Changes in parametrisa-tions or on the underlying network may lead to a situationwhere the original results are not reproducible anymore.Furthermore, all contributions to the model code shouldbe correctly attributed.With respect to simulation results,change records can help to predict modifications in thesimulation outcome. Finally, a good communication ofmodel changes increases the trust of scientists wishing toreuse a model for their own purposes.In our previous work we developed BiVeS, an algorithmto identify and communicate the changes between twoversions of a model [11]. BiVeS encodes changes in anXML file and, thus, tools can visualise and post-processidentified changes. Small changes might be easy to graspwithout the aid of a principled annotation scheme. How-ever, as the list of changes increases it becomes harder tograsp their relevance. To address this problem, we presentan ontology to annotate the changes identified with theBiVeS algorithm. The ontology is sufficiently generic thatit can be used to annotate the differences between com-putational models, including those encoded in SBML andCellML.ResultsWe developed the COMODI ontology, because COm-putational MOdels DIffer. COMODI provides terms thatdescribe changes in models. These terms can be linkedto single differences between two versions of a model,such as typo corrections in a parameter name. Basedon the resulting set of annotations, differences can becharacterised and classified.Design considerations and identification of termsCOMODI was developed based on a study of changesin versions of SBML and CellML models. The modelswere retrieved from the respective model repositories.More specifically, we started our investigation by man-ually analysing a predefined set of cell cycle models2from BioModels [12]. We subsequently extended this setwith randomly chosen models from both, BioModels andthe Physiome Model Repository [13]. The single steps ofdevelopment are summarised in Fig. 1 and explained inthe following:1. Using the BiVeS algorithm we identified thedifferences between all subsequent versions of eachmodel and exported the deltas in XML-encoded files.A delta is a set of operations on entities (nodes orattributes, respectively) necessary to transform onedocument into another [11].2. Each found difference was manually translated into ahuman-readable description and recorded in a wikisoftware to share and discuss it with collaborators. Intotal, we investigated more than 10000 differences.3. Afterwards, we manually analysed the verbosedescriptions of changes to understand their effectson the model and to derive hypotheses andexplanations for a change. For example, the changeof an entity name from Gulcose to Glucose renamesa species and can be considered as the Correctionof a Typo that effects an EntityName.4. We then grouped the changes into several logicalclusters, according to the derived hypotheses andexplanations of a group of changes. These clustersare based on our own experiences and on feedbackfrom domain experts. The knowledge we gained ledto candidate terms for the ontology. We used thehuman-readable description as a basis for the termdefinitions.5. In a last step, we designed a first version of theontology from the obtained clusters. The ontologywas afterwards extended with concepts stemmingfrom standard formats (SBML and CellMLterminology, e.g. ParameterSetup) and from theXML domain (e.g. EntityIdentifier). Thus,COMODI contains a whole subtree that specificallyfocuses on XML encodings.We quickly identified technically driven properties ofchanges. For example, it is easy to determine the type ofa change as BiVeS already distinguishes between inser-tions, deletions, updates, and moves of entities in XMLScharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 3 of 8Fig. 1 Development process of COMODI. The development process involved five steps with several iterations. First, we used BiVeS to generate thedifferences between all subsequent model versions. Second, we converted the formal description of more than 10000 differences intohuman-readable descriptions. Third, we manually studied these descriptions and derived hypotheses and explanations for them. Fourth, wegrouped the human-readable descriptions into sets of concepts and derived candidate terms for the ontology. Fifth, we aggregated and classifiedthese terms and implemented the first version of the ontology in Protégédocuments. Moreover, it is always possible to specify theXML entity that is subject to a change. It was, however,more difficult to identify terms describing the reason,intention, or target of a change. The absence of appro-priate terms led us to derive new terms from our humanreadable description of changes. The initial set of termswas then shaped in discussions with other researchers.Throughout the development of COMODI we soughtfeedback from experts in the fields, e.g., through per-sonal communication or poster presentations at meetings.Finally, we implemented the derived ontology in the WebOntology Language (OWL) [14] using Protégé [15].Ontology organisation and contentCOMODI is organised into four branches around thecentral concept Change: XmlEntity, Intention,Reason, Target (cf. Fig. 2). As a running example, weuse the change of a parameter in an imaginary SBMLmodel. We assume that the parameter changed from 0.5to 0.8 in the new version of the SBML model.The subtree rooted by the Change class can be used tospecify the type of a change in more detail. Model enti-ties may be inserted (Insertion), deleted (Deletion),updated (Update), or moved (Move). In our example,the modification of the parameter value from 0.5 to 0.8corresponds to an update (Update) of an attribute value.Many models are encoded in XML documents. Inthese cases, a change is always applied to a certainXmlEntity. We distinguish between an XmlNode, anXmlAttribute, or an XmlText element. The updateof the parameter value in our example is applied to anXmlAttribute.Intention and Reason both indicate the purposeof a change. On the one hand, the Intention spec-ifies the aim of a change, particularly with respectto consequences in the future. In our example, theintention of modifying the parameter value is aCorrection. On the other hand, a Reason specificallyfocuses on the cause of a change. In our example, aMismatchWithPublication caused an update of theparameter value.Most prominent is the Target branch. It containsterms to specify possible targets of a change. COMODIbasically distinguishes between five layers in a modeldocument, that can be subject to a change:1. The ModelEncoding corresponds to the formalencoding of the model document. Terms of thisbranch can, for example, be used to describe anupdate of the underlying SBML specification.2. The ModelAnnotation branch corresponds tothe semantic layer of a model document. Terms ofthis branch can, for example, be used to capturechanges in the annotations.3. The ModelDefinition refers to the actualbiological system, for example a reaction network.Terms of this branch can, for example, be used tospecify the parts of a model that are affected by achange.4. The ModelSetup branch can be used to describechanges in the simulation environment. Terms ofScharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 4 of 8Fig. 2 Structure of the COMODI ontology. Differences between computational models can be annotated with the Change term. Using theproperties appliesTo, hasIntention, hasReason, and affects, the differences can be linked to the terms of the four major branches ofCOMODI: XmlEntity, Intention, Reason and Target. All arrows between terms within these five branches indicate an is-a relation, unlesslabelled otherwisethis branch can, for example, be used to describechanges in parameter values.5. The ModelBehaviour links to the TEDDYontology [16]. Thus, it is possible to capture changesin the dynamics of the system. Such changes may, forexample, affect the stability characteristics.Finally, different changes might be linked to each otherif they have mutual dependencies. For example, the dele-tion of a biological reaction triggers the deletion of itskinetic law. Similarly, the deletion of an XML node (e.g.an SBML species) triggers the deletion of all its attributes(e.g. the species initial concentration). Those changes canbe linked using the wasTriggeredBy relationship toexpress relations between changes.COMODI version 2016-03-11 contains a hierarchy of65 classes and includes five object properties. The objectproperties can be used to establish relationships betweenmembers of the Change class and members of the fourmain branches of the ontology. We list and explain theseproperties in Table 1.AvailabilityThe COMODI ontology is licensed under the termsof the Creative Commons Attribution-ShareAlike 4.0International License3. The OWL encoding of the lat-est version may be downloaded from http://purl.uni-rostock.de/comodi/comodi.owl. Additionally, users mayalso browse the ontology at http://purl.uni-rostock.de/comodi/. COMODI is also available at http://bioportal.bioontology.org/ontologies/COMODI throughBioPortal [17].ApplicationsThe COMODI ontology is specifically designed for theannotation of differences between versions of a compu-tational model in the life sciences. In the following weshow the usefulness of COMODI for annotating changes,Scharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 5 of 8Table 1 List of object properties defined in COMODIName Description Domain Rangeaffects Provides information about the parts in a model that were affected by a change. Change TargetappliesTo Stores information about the entity type in an XML document that was changed. Change XmlEntityhasIntention Links a change to an intention that was to be achieved by the corresponding change. Change IntentionhasReason Links a change to a reason that made this change necessary. Change ReasonwasTriggeredBy Represents dependencies among changes: A change might trigger further changes. Change Changepredicting the effects of changes on the simulation result,and filtering versions of a model for specific differences.Annotation of changesSBMLmodels typically use parameters to define the kinet-ics of a process. The corresponding entity in the SBMLdocument looks as follows:<parameter name="Km1" value="23.24"units="molesperlitre" />Here the value of the parameter Km1 is 23.24molesperlitre.Updating the parameter value to23.42 molesperlitreresults in an update of the corresponding XML entity. Thenew version of the model contains the following piece ofSBML code:<parameter name="Km1" value="23.42"units="molesperlitre" />BiVeS identifies the difference as an update of theparamter value. The XML-encoded serialisation providesthe new and the old value of Km1:<update><attribute id="1"oldPath="/sbml[1]/../parameter[1]"name="value" newValue="23.42"oldValue="23.24" [...] /></update>Using COMODI the detected update can now be anno-tated. Some information can directly be inferred and thusbe annotated automatically with BiVeS. For example, weknow that the above is an update and can link it to theXML entity XmlAttribute. BiVeS is even able to rec-ognize that this change corresponds to a change of theParameterSetup. The combination of several state-ments using terms of the different branches allows usersto be very specific. COMODI offers terms describingthe reason and the intention of a change. Following theexample from the previous section, the annotation of theparameter update might look like4:#1 a comodi:Update ;comodi:appliesTo comodi:XmlAttribute ;comodi:affects comodi:ParameterSetup ;comodi:hasIntention comodi:Correction ;comodi:hasReasoncomodi:MismatchWithPublication.The full example is included in Additional file 1 andexplained in Additional file 2.Prediction of the possible effects of a changeThe modification of the ParameterSetup also affectsthe ModelSetup (cmp. ontology terms in Fig. 2) andthus potentially influences the simulation results. Simi-larly, modifications of a FunctionDefinition or theKineticsDefinition can influence the simulationoutcome. Finally, changes in the network structure (e. g.,modification of the ReactionNetworkDefinitionby transforming a reactant into a modifier) will not onlyaffect the simulation outcome, but in addition the visualrepresentation of the network. For this subset of changes,modellers should be notified of any modification.Another case are changes that affect theModelEncoding. For example, models are regularlyupdated to remain compliant with new versions of formatspecifications. These changes are especially relevant forsoftware tools dealing with model code. As not all toolsfeature the full set of SBML constructs [18], for example,the upgrade of a model may require the use of anothersoftware tool. Thus, changes that result from modifica-tions of the format specification can be of indirect interestfor modellers. They may not affect the modelled systembut the tools that are needed to interpret and simulate it.However, other changes may not be as relevant. It canbe helpful to hide them and thereby help users focus onimportant changes. For example, the reading and subse-quent writing of amodel file using different software tools,such as COPASI [19] or CellDesigner [20], often resultsin a re-shuffling of elements within the document. How-ever, the sequence of certain elements might not matterto the encoded model. In SBML for example, the order ofparameters defined in the listOfParameters is irrel-evant for the encoded system, as SBML does not give anysemantic meaning to element orders [21]. Thus, changesthat only affect the order of elements, can be neglected.Even if BiVeS reports them in its XML serialisation, thesechanges can be discarded if annotated with the corre-sponding COMODI terms. For other types of changes,the decision whether to neglect a change or not dependson the user. A new identifier scheme for the semanticannotations, for example, is relevant to curators and tooldevelopers, while it is probably irrelevant for the majorityScharm et al. Journal of Biomedical Semantics  (2016) 7:46 Page 6 of 8of modellers. However, modellers who based their modelanalysis, comparison, or visualisation on semantic anno-tations need to be notified about this type of change. Here,COMODI terms need to be evaluated based on the usersBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 DOI 10.1186/s13326-016-0089-6RESEARCH Open AccessTowards natural language questiongeneration for the validation of ontologiesand mappingsAsma Ben Abacha1*, Julio Cesar Dos Reis2, Yassine Mrabet1, Cédric Pruski1 and Marcos Da Silveira1AbstractBackground: The increasing number of open-access ontologies and their key role in several applications such asdecision-support systems highlight the importance of their validation. Human expertise is crucial for the validation ofontologies from a domain point-of-view. However, the growing number of ontologies and their fast evolution overtime make manual validation challenging.Methods: We propose a novel semi-automatic approach based on the generation of natural language (NL)questions to support the validation of ontologies and their evolution. The proposed approach includes the automaticgeneration, factorization and ordering of NL questions from medical ontologies. The final validation and correction isperformed by submitting these questions to domain experts and automatically analyzing their feedback. We alsopropose a second approach for the validation of mappings impacted by ontology changes. The method exploits thecontext of the changes to propose correction alternatives presented as Multiple Choice Questions.Results: This research provides a question optimization strategy to maximize the validation of ontology entities witha reduced number of questions. We evaluate our approach for the validation of three medical ontologies. We alsoevaluate the feasibility and efficiency of our mappings validation approach in the context of ontology evolution.These experiments are performed with different versions of SNOMED-CT and ICD9.Conclusions: The obtained experimental results suggest the feasibility and adequacy of our approach to support thevalidation of interconnected and evolving ontologies. Results also suggest that taking into account RDFS and OWLentailment helps reducing the number of questions and validation time. The application of our approach to validatemapping evolution also shows the difficulty of adapting mapping evolution over time and highlights the importanceof semi-automatic validation.Keywords: Ontology validation, Mapping validation, Question generation, Knowledge managementIntroductionAn ontology can be defined as a formal, explicit specifi-cation of a shared conceptualization [1] that can play akey role in many different applications [2]. In the med-ical domain, ontologies are becoming popular to repre-sent clinical knowledge. Several ontologies became a defacto standard in the domain (e.g., SNOMED CT1, NCI2).As multiple ontologies can describe the same domain,*Correspondence: asma.benabacha@gmail.com1Luxembourg Institute of Science and Technology (LIST), Esch-sur-Alzette,LuxembourgFull list of author information is available at the end of the articlesemantic mappings are often defined to link ontology ele-ments that refer to the same real-world entity but belongto different domain-related ontologies [3]. These linksplay a key role for systems interoperability tasks as theyallow them to reconcile data annotated using differentontologies [4].New ontologies and mappings are frequently publishedand updated on the Web. For instance, Bioportal3 is abiomedical ontology repository where more than 350 dif-ferent ontologies are published and maintained. CISMeF4is another example of how biomedical ontologies can beused to retrieve relevant information on the Web. Thesuccessful application of ontologies brings new challenges© 2016 Ben Abacha et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 2 of 15related to their construction, maintenance and validation.Natural Language Processing (NLP) techniques have beenextensively used to retrieve and extract information fromtextual sources in order to automatically identify con-cepts, instances, and relations used in a specific domain.This led to the rapid growth of biomedical ontologies, andconsequently, to an increasing need of content validation.Erroneous facts can be included in ontologies becauseof several factors; including automated concept and rela-tion extraction methods, disagreements between differenthuman actors involved in ontology design, and ontol-ogy evolution. The ontology validation process can targetdifferent aspects depending on the requirements of vali-dation [5]: e.g., human understanding, logical consistency,modeling issues, ontology language specification, real-world representation and semantic applications (a sum-mary of specific techniques for ontology evaluation can befound in [6]).While a wide range of approaches tackled logical con-sistency, few approaches considered the validation of theconceptualization itself from a domain point of view. Inthis article, we propose a semi-automatic approach for theconceptual validation of ontologies and their mappings.By conceptual validation we refer to the assessment ofwhether a given fact is true or false with regards to real-world knowledge: i.e., is the real-world model compliantwith the formal model (ontology)? [7].We propose a semi-automatic approach to conceptual validation based on theautomatic generation of natural language questions andthe processing of experts answers to these questions. Wepropose an optimization method to reduce the numberof questions required to validate a given ontology usingRDFS and OWL entailment [8].Our second goal is to validate mapping relationsbetween different ontologies (e.g., equivalentClass, sub-ClassOf, equivalentProperty, subPropertyOf). We partic-ularly focus on the context of evolving ontologies andthe validation of automatically-generated mapping adap-tations.We propose an adapted semi-automatic validationapproach based on the automatic generation of naturallanguage questions. When the expert invalidates a givenmapping, Multiple-Choice Question (MCQ) are automat-ically generated to propose correction alternatives fromthe ontology itself [9].Our contributions can be summarized as follows: We propose a semi-automatic approach to simplifythe human intervention during the validation of anexisting ontology. The proposed techniqueautomatically generates well-formulated questionsfrom the target ontology using pattern-basedmethods. The introduced patterns are instantiatedwith ontology labels to generate Boolean questions,which are submitted to domain experts. The answersreturned by the experts are used to prune a subset ofthe remaining questions using inverse RDFSentailment. To this end, the initial question sets areranked according to their impact on the remainingquestions following RDFS entailment rules. Thisapproach can also be applied to subsets of ontologiescorresponding to their evolution (i.e., new andmodified facts). We extend our approach to address the problem ofmapping validation. We propose a novel approach tosupport human experts to validate recommendedmodifications in mappings affected by ontologyevolution. We investigate techniques to generateMCQ that allow suggesting new decisions in themapping modification process. The proposedapproach analyses both the old and the updatedcontext of concepts to propose alternative choices ifthe initial adapted mappings are invalidated by theexperts. We experimentally assess our approaches conductingevaluations using real-world biomedical ontologiesand mappings established between them. Wemeasure different aspects to observe the quality andeffectiveness of the questions and the definedapproaches. Our results show innovative findingsregarding the way that questions are generated andtheir relevance for the validation of ontologies andmappings.We present and discuss research work related to ques-tion generation and the validation of ontologies and map-pings in Section Background. In Section Methods wepresent our approaches for the validation of ontologies(Section Ontology validation method) and their map-pings (Section Mapping validation method). Our exper-iments and results are detailed and discussed in SectionsExperimental evaluation and Discussion and Futurework.BackgroundIn this section, we review existing techniques relatedto ontology evaluation (e.g., ontology verbalization) andmethods related to the validation of ontologies andmappings. We also present the original aspects of ourapproach with regard to related works.Ontology evaluation criteriaInitial approaches tackling the quality of ontology con-tents emphasized on statistical aspects such as the num-ber of classes, the number of properties or the numberof leaf classes [10]. While these numbers reveal someinformation about the complexity of an ontology, theydo not cover other aspects of validation as discussed inSection Introduction.Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 3 of 15In their work in the PERTOMED project, Baneyx andCharlet [11] introduced several relevant criteria for theevaluation of ontology quality at various moments of itslife-time (i.e., construction, evolution and maintenance)with a particular focus on the biomedical domain andits specifics. Some of the criteria deal with the struc-tural and logical aspects of the ontology, while otherstackle the conceptualization of the represented domain.Among those related to the conceptualization, they dis-cuss the ontological commitment as essential. They advo-cate that when designing an ontology, a minimal numberof hypotheses must be assumed to represent the domain.The authors also stress the usability of the ontology as wellas its ability to fulfill the set of requirements it has beendesigned for.Stvilia [12] defines a model with twelve different crite-ria to evaluate the quality of an ontology. He also con-siders statistical criteria which exploit explicitly-definedontological elements, including: the number of classes orproperties, subjective aspects like semantics and struc-tural consistency. He also considers volatility as a newcriterion to evaluate the duration for which an ontologyremains valid by measuring the period of time elapsedbetween two successive updates. Therefore he takes intoaccount, to a certain extent, the evolution of the con-sidered ontology. As noted by Baneyx and Charlet [11],this work also considers the usability of an ontology bycounting the number of applications that are using it.Djedidi and Aufaure [13] proposed an approach toassess the quality of an OWL ontology at evolutiontime. They proposed a set of quality criteria dealing withcomplexity, cohesion (e.g., average number of connectedcomponents), conceptualization (e.g., average number ofobject properties per classes), abstraction (e.g., maximumnumber of classes between the root and the leaves of theontology), completeness and comprehension (i.e., numberof annotated classes or individuals). Nevertheless, the pro-posed approach is clearly dependent on the OWL model,since the implementation of the metrics relies on OWLprimitives.Sabou and Fernandez [6] introduced two other dimen-sions to consider when evaluating ontologies. The firstone consists in finding relevant criteria for the selectionof an existing ontology, instead of creating a new ontol-ogy from scratch. This makes sense because of the largenumber of available ontologies through theWeb. The sec-ond criterion deals with the modularity of an ontology.They proposed to evaluate modules that require combina-tion for a given purpose, or for a particular application, todecide the relevance of an ontology.More recently, Rico et al. [14] presented the OntoQual-itas framework. In this work, no new type of criterionaddressing the quality of an ontology has been introduced,but the metrics to calculate them have been improved andrefined. The authors also provided a concrete case studyto assess the framework.Question generation and verbalization of ontology contentSeveral efforts have addressed the automatic generation ofNL questions5. Most of them focused on the generation ofquestions from text (text-to-question task) [15]. The ques-tion generation process can rely on manual patterns [16]and/or on statistical techniques [17].AUTOQUEST [15] stands for one of the first questiongeneration systems proposed to assess text understand-ing. More recently, Heilman [17] proposed to generateWH questions from text, relying on hand-crafted trans-formation patterns and a statistical ranking model. For arelated e-learning task, Liu et al. [16] proposedG-Asks, anautomatic question generation system. To support learn-ing through writing, G-Asks generates specific questionsusing manual patterns that are associated with differentquestion types. Mitkov and Ha [18] tackled the generationof multi-choice questions from instructional documents.The main proposed process employs NLP techniques fordomain term extraction and shallow parsing. Their workalso defined hand-crafted transformational rules to gener-ate the questions from declarative sentences with minimalmodification to the original words.The current volume of dense ontologies requires noveltechniques to guarantee the semantic precision of thecontents. Papasalouros et al. [19] suggested an automaticapproach to generate MCQs from ontologies that remainsindependent from linguistic resources and domain-specific constraints. They proposed ontology-level strate-gies according to the basic RDFS types and primitives(e.g., classes, properties and subClassOf axioms). Theapproach defined these strategies to produce distractorsin the scope of computer-assisted assessment. However,they do not explore advanced NL generation methods.For example, the same stem, Choose the correct sentence"is used in all questions. Similarly, Cubric and Tosic [20]examined the same type of ontology-related strategy for e-assessment, by defining a generic questions ontology andlinking it to domain ontologies.The MoKI systems designed by Pammer [21] is, to thebest of our knowledge, the only work that addresses theproblem of ontology validation by means of question-answering techniques. The proposal is to generate ques-tions from the content of the ontology and submit them todomain experts in order to get their feedback, leading tothe validation or modification of the underlying ontology.However, the question generation process does not inte-grate the fact that domain experts are rarely familiar withformal ICT languages. The questions generated by MoKIlook very similar to description logic formulas hardlyunderstandable by experts. Hence, the outcome of theproposed system still require a substantial interventionBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 4 of 15of ICT experts to support domain experts through thevalidation process.In another application, Teitsma et al. [22] presented anontology-based generation approach for situation deter-mination. A traffic accident ontology and two databaseson accidents were used to: (i) generate questions frominfons (sets of field values describing the situation; suchas weather or injuries) and (ii) get validation answersfrom human observers whowatched video scenes describ-ing the target situations. They used rule-based logicaloptimization techniques (e.g., if the observers validatedrain for weather the system does not ask if the roadis wet). However, this work does not provide detailson the NL level of the generation process, which seemsadhoc with respect to the concepts of the selected domainontologies.Ontology validationAs discussed above, a significant amount of work tack-led the study and proposition of a set of metrics dealingwith the quality of ontologies. These metrics are based onmeasurable properties such as the number of classes, indi-viduals and properties. While it can be argued that suchmetrics can give a relevant evaluation of the quality ofan ontology, they cannot be used to validate the ontologycontent, i.e., an ontology with a good quality can containmore erroneous facts that an ontology with an estimatedlower quality.Gangemi et al. introduced a model for evaluating andvalidating ontologies [23]. Based on a meta-ontologycalled O2 and semiotics, the authors propose to evalu-ate ontologies by considering structural, functional andusability-profiling measures. The validation is then com-plemented with an ontology called oQual aiming at pro-viding necessary criteria to select an ontology to answer aparticular need.Köhler et al. provided a rule-based methodology (i.e.,a set of conventions) to establish well-defined labels forGene Ontology (GO) concepts [24]. The rules aim at avoid-ing circular definitions (i.e., term of the label that are alsoin the definition of the considered concept) and obscurelanguage (i.e., labels of concepts must be understood bynon expert persons). Although interesting, this work canhardly be applied to other ontologies because GO labelsare very domain specific. They are not only built on lin-guistic aspects, but they use a lot of alphanumeric symbolsto denote gene and proteins.A similar argument is used by Verspoor et al. [25]. Theauthors proposed amethodology to classify medical termsthat denote the same meaning but use different linguis-tic conventions in order to standardize them. Such kindof approaches addresses an important aspect which isthe choice of the terminology to describe ontological ele-ments. However, elements that are implicitly defined (e.g.,concepts that are logically defined by inference) are notstandardized.Dimitrova et al. designed the ROO tool for supportingdomain experts designing OWL ontologies [26]. This pro-vides a controlled language interface and offers systematicguidance throughout the whole ontology constructionprocess with an aim of optimizing the quality of the result-ing ontology. However, nowadays ontologies are ratherbuilt automatically from the textual content of relevantdocuments or data [27, 28], or rather slightly modifiedby virtue of knowledge evolution. Accordingly domainexperts are mostly involved in the validation phase andless and less from the beginning of the ontology life cycle.In their work [29], vor der Bruck and Stenzhorndescribed a method to validate ontologies using anautomatic theorem prover and MultiNet axioms. Tothis end, the authors focused on the logical structureand ignored the conceptualization part. Therefore, theirmethod requires formal ontologies expressed in logic-based languages, which is not always the case in thebiomedical domain. The system implementing the pro-posed algorithm, is accompanied with a user friendlysoftware interface to speed up the fixing of the detectederroneous axioms facilitating users intervention.More recently, Poveda-villalón et al. [5] proposed theOOPS! system. This consists of detecting pre-definedanomalies or bad practices in ontologies to enhance theirquality. However, the real-world representation dimen-sion is neglected in this approach, which refers to howaccurately the ontology represents the domain intendedfor modelling. This is left to the discretion of domainexperts.Other families of approaches have been proposedaddressing the validation of the domain-conceptualizationside. Some of them emphasize on user interface devel-opment to better present large amount of (structured)data without overwhelming users [30] to support thevalidation effort. Other research work promotes the useof NLP techniques to better involve domain experts inthe ontology validation process [21].Mapping validationSimilar to ontologies, previous studies have revealedthe real difficulties and importance of validating map-pings and involving human experts in the process [31].Mapping revision refers to a method aiming to iden-tify and repair invalid mappings that can be exploredfor mapping validation. Existing techniques may detectthe invalid mappings at ontology evolution time. Meilickeet al. [32] proposed an automatic mapping debuggingbetween expressive ontologies eliminating inconsisten-cies, caused by erroneous mappings, through logicaldiagnostic reasoning. Mapping revision still demandsBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 5 of 15logically expressive ontologies. This motivates furtherresearch on alternative methods to validate the adap-tation of mappings. Similarly, Serpeloni et al. [33]proposed a semi-automatic process to validate map-pings through graph algorithms that select instances forverification.In order to take human aspects into account, recentstudies have examined the interactive aspect to validatemappings. Some approaches tackled the design of interac-tive tools to support the ontology mapping process withrelevant visualizations [34]. Other studies proposed ontol-ogy alignment and validation based on users in commu-nity [35] and crowdsourcing [36]. However, these users arenot necessarily experts in the (sub-)domain representedby the ontology.PositioningAlthough several studies addressed recently the tasks ofontology validation, mapping validation and question gen-eration, little attention has been given to the problem ofvalidating ontology contents (including semantic align-ments) from the perspective of domain conceptualization.In this context, our first contribution is a semi-automaticapproach to reduce and simplify the human interven-tions required to validate ontology contents andmappingsfrom a domain point of view. Our approach is basedon the generation of boolean questions from the ontol-ogy. Expert answers to these questions are then processedautomatically to validate and correct the ontology. Wealso use the expert feedback incrementally to prune asubset of the remaining questions using inverse RDFSentailment.On the other hand, to the best of our knowledge, thereare no studies that tackle particularly the possibility ofvalidating ontology mappings via automatic question gen-eration. We propose a mapping validation approach thattackles the special characteristics of modifications inmap-pings over time, taking the involvement of users intoaccount. More precisely, our second contribution aimsto support human experts during the mapping validationprocess.MethodsWe briefly present the preliminary definitions neededto describe our approach. We first introduce the for-mal notions of ontology and mapping before definingthe problem of conceptual validation and mapping val-idation (Section Definitions and problem statement).In the second part of this section we present ouroptimisation approach for the generation of booleanquestions to validate ontologies (Section Ontologyvalidation method) and our method to search forcorrection alternatives for invalid mappings (SectionMapping validation method).Definitions and problem statementAn ontology O = (Concepts,Relationships,Attributes)consists of a set of Concepts interrelated by directed Rela-tionships. We define a set of concepts of an ontology Oxat time t as Concepts(Otx) = {Ct1,Ct2, ...,Ctn}. Each conceptC ? Concepts has a unique identifier and is associatedwith a set of attributes Attributes(C) = {a1, a2, ..., ap}(e.g., label, definition, synonym, etc.). A relationship r ?Relationships interconnects two concepts and has a spe-cific type, e.g., is_a or part_of .The context of a concept (CT(Ci)) in the ontologystands for the union of the sets of super concepts (sup(Ci)),sub concepts (sub(Ci)) and sibling concepts of Ci (sib(Ci)),as following:CT(Ci) = sup(Ci) ? sub(Ci) ? sib(Ci) (1)wheresup(Ci) = {Ck |Ck ? Concepts(O),Ci  Ck ? Ci = Ck}sub(Ci) = {Ck |Ck ? Concepts(O),Ck  Ci ? Ci = Ck}sib(Ci) ={Ck |Ck ?Concepts(O),?a, bs.t.Ca? sup(Ck)andCa? sup(Ci)}where Ci  Ck means that Ck subsumes Ci.An ontology mapping MtOA,OB , established at time t,interlinks a set of given concepts Ca and Cb from twodifferent ontologiesOA/OB by so-called correspondences:MtOA,OB={(Cta,Ctb, semTypetab, conf t , statust)|Ca ? Concepts(OA),Cb ? Concepts(OB), confidence ?[ 0, 1] ,semanticType ? {?,?,?,?},status?{"valid","invalid","inactive","handled","to?verify"}}A correspondence corCA,CB = (CA,CB, confidence,semanticType) links two concepts CA ? Concepts(OA)and CB ? Concepts(OB). The confidence value representsthe semantic similarity between CA and CB (indicatingthe confidence of their relation [3]). The higher the value,the more related are both concepts. The semanticTypein corCA,CB refers to the semantic relation connecting CAand CB. We consider the following types of semantic rela-tions: unmappable [?], equivalent [?], narrow-to-broad[?], broad-to-narrow [?] and overlapped [?].The conceptual validation problem consists of defin-ing a method to get feedback from domain experts on thecorrectness of ontology facts (or mappings) and interprettheir answers to modify the ontology/mapping accord-ingly. We propose to examine question generation tech-niques to cope with the conceptual validation problem.Two issues have to be investigated in particular: How to formulate Natural Language (NL) questionsthat would lead to expert answers that are bothrelevant and computer-interpretable? (clarityproblem) How to avoid overwhelming human experts withunnecessary questions? (optimisation problem)Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 6 of 15We define the mapping validation problem as follows:starting from an ontology OA at time t, noted OtA, andanother different ontology OB at time t, noted OtB, a setof correspondences exist between them MOtA,OtB . In par-ticular, the investigated problem consists in validating thecorrespondences if the ontology OtA evolves to a new ver-sion Ot?A at time t?. We divide the problem by consideringthe evolution of only one ontology at a time (i.e., the targetontology remains unchanged).In the following, we define our methods for ontol-ogy validation (Section Ontology validation method)and mapping validation (Section Mapping validationmethod).Ontology validation methodApproach overviewWe propose a semi-automatic approach based on ques-tion generation to validate ontologies. Figure 1 describesour ontology validation system, called SAVANT. The firststep consists of automatically generating a list of booleanquestions from the ontology under validation.These questions are submitted to domain experts whoprovide an agreement decision (Yes/No) and a textualfeedback. The next step consists on interpreting expertfeedback to validate or modify the ontology. The noveltyof our approach relies on the fact that manual interven-tions are performed only by Health Professionals (HPs),who will lead the ontology validation process. ICT expertsare required only when the error cannot be solved auto-matically. This increases the quality of exchanges betweenactors and reduce errors and time consumption.We explore the proposed approach to (i) validate ontolo-gies constructed automatically from medical texts (e.g.,clinical guidelines) and also (ii) to re-validate ontologies(constructed manually or automatically), since medicalknowledge evolves quickly over time.We focus on validating the following types of ontologystatements: A rdfs:subClassOf B (class A is a subclass of B) P rdfs:subPropertyOf Q (property P is a sub-propertyof Q) P rdfs:domain D (D is the domain class for property P) P rdfs:range R (R is the range class for property P) I rdf:type A (I is an individual of class A) I P J (the property P links the individuals I and J)The proposed approach uses manually constructed pat-terns for each kind of ontology element as described in thefollowing section.Pattern-basedmethod for boolean question generationWe start from the hypothesis that all the elements of amedical ontology must be validated. This involves validat-ing concepts (e.g., Substance), relations between concepts(e.g., administrated for), concept instances (e.g., activatedcharcoal is an instance of Manufactured Material), rela-tions between concept instances (e.g., chest X-ray can beordered for Chronic cough) or between concept instancesand literals (e.g., give oral activated charcoal 50 g indi-cates the dose of the substance to be administrated 50 g).These ontology elements provide the main keywords ofthe question patterns through the labels of concepts, rela-tions and instances.We constructed manually question patterns associatedto each type of ontological element (5 different elementsin our preliminary experimentations). A question patternFig. 1 Proposed approach to ontology validation based on the automatic generation of boolean questionsBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 7 of 15consists of a regular textual expression with the appro-priate gaps [37]. For instance, the pattern Is DOSE ofDRUG well suited for PATIENTS having DISEASE? isa textual pattern with 4 gaps: DOSE, DRUG, PATIENTSand DISEASE. This question pattern aims to validate adrug dose administrated to a patient having a particulardisease. The singular or plural form of the verb in theexpression is determined using the Stanford parser6. Sin-gular is used by default if the detection is not possible, afrequent case that occurs because of the heterogeneity ofontology labels.Table 1 presents examples of boolean-question patterns.Question optimization strategyAt this level, our main objective is to investigate atechnique to build relevant questions from formalizedknowledge in order to validate the maximum number ofassertions with the minimum number of questions.We propose an optimization strategy relying on theRDFS logical rules to rank the questions according to theelements that imply the more changes in the ontology.For instance, if we have the following data: hasSuitedAntiobioticsType rdf:subPropertyOfhasTreatment Antibiotics rdfs:subClassOf Treatment hasSuitedAntiobioticsType rdfs:range Antibioticsand the expert invalidates Antibiotics rdfs:subClassOfTreatment", than the property hasSuitedAntiobioticsTypecannot be declared as a sub-property of hasTreatmentbecause the hasSuitedAntibioticType relation has not acommon range with the property hasTreatment, whichleads to a formal error regarding the RDFS entailmentrules.We consider all RDFS entailment rules7. Table 2presents some inversed forms of these rules to show theimpact of invalidating each one of the target ontologystatements.This technique enables ranking questions in a mannerthat allows to delete some of the remaining questions ifone of the RDFS entailment rules apply. This leads to thefollowing validation order:1. A rdfs:subClassOf B2. P rdfs:domain D and P rdfs:range R3. P rdfs:subPropertyOf Q4. I rdf:type A5. I P JAnswer analysis and ontology updateThe second step of our approach refers to the exploitationof expert feedback to validate or modify the target ontol-ogy. The ontology under validation might contain con-cepts, individuals and relations defined between conceptsor individuals.Feedback consists of two main parts: (i) an assertionon the correctness of the target knowledge and (ii) a freetextual explanation if provided8. In the scope of this arti-cle, we take into account ontologies that are formally-valid(with no inconsistencies) and emphasize the validation ofdomain conceptualization.In this context, Yes answers have no impact on theontology. The ontology is modified on the No answersprovided by the domain experts. Invalidating an ontol-ogy element implies different impacts according to theelement type.We use the same RDFS entailment rules to update theontology. The ontology item invalidated by the expert andthe inferred invalidations are deleted from the ontology,as well as the questions associated to them.Mapping validation methodApproach overviewWe consider as input a set of adapted correspondencesMOt?A ,OtB . Adaptation here refers to the automatic mappingadaptation that occurs after the evolution of the sourceontology OtA. We consider that the old mappings are cor-rect and we want to validate only the new ones. Similarlyto the ontology validation method, our approach to val-idate mappings relies on the generation of NL questionsfrom the new mappings. Figure 2 describes our proposedapproach for mapping validation. Figure 3 presents ourmethod for question generation through a state transitiondiagram.Table 3 presents examples of correspondences betweenSNOMED-CT and ICD9. Figure 4 shows examplesof more or less ambiguous correspondences retrievedbetween the biomedical ontologies SNOMED-CT andICD9. This selection provides concrete examples of theissues related to the heterogeneity and broadness of someTable 1 Examples of boolean-questions patterns used for ontology validationQuestion pattern Example of instanceDoes a(n) CLASS have a PROPERTY Does an effect have a measurement method?Does a treatment have an administration scheme?Is CLASS a type of CLASS? Is statistical evidence a type of evidence?Is SUB-PROP of a CLASS a PROP of the same CLASS? Is primary treatment of a disease a treatment of the same disease?Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 8 of 15Table 2 Examples of ontology update rules with respect to invalidated elements used for ontology validationNOT A rdfs:subClassOf B ? NOT A rdfs:subClassOf C s.t. C rdfs:subClassOf BNOT P rdfs:domain A ? NOT P rdf:subPropertyOf Q s.t. Q rdfs:domain ANOT I rdf:type A ? NOT <I, P, J> s.t. P rdfs:domain ANOT <J, P, I> s.t. P rdfs:range Aconcept definitions. Dealing with this problem requires todefine flexible answer types to ensure a relevant interac-tion with the human validators.STEP 1: Boolean questionsIn the first step, our method translates the proposedadapted correspondences into a NL question using tex-tual patterns associated to each relation type. Let X be thesource concept label and Y be the target concept label, themain patterns are as follows:1. (Is|Are) X <equivalent to> Y?2. (Is|Are) X <more specific meaning than> Y?3. (Is|Are) X <less specific than> Y?4. Do(es) X <partially correspond to> Y?5. X <cannot be matched with> Y?These patterns are instantiated with the involvedconcepts of a correspondence. We present three instanti-ation examples from our dataset in the following:1. Are intestinal diseases equivalent to vascular disorderof intestines?2. Does the Trousseau sign partially correspond toill-defined and unknown causes of morbidity andmortality?3. Is the Eisenmenger Complex more specific thanother congenital malformations of cardiac septa?STEP 2: multiple choice questionsIn the second and main step, negative answers triggermultiple choice questions (MCQs) that are submitted tothe expert in order to detect alternative correct mappingsbetween Ct?a (the source concept) and the concept Ct?b inFig. 2 Proposed approach to validate mapping adaptation based on the automatic generation of boolean questions for new mappings andmultiple choice questions for invalid mappingsBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 9 of 15Fig. 3 The question generation process as a state-transition diagramtarget ontology Ot?B . MCQ consists of (i) a problem knownas the stem and (ii) a list of suggested alternatives. In ourapproach, we have three categories of stems/questions (cf.Fig. 3):1. Revision of Ca. This suggests revising the sourceconcept by candidate proposals from the new sourceontology Ot?A. This category preserves the semanticmapping-relation between the source concept Ct?aand the target concept Ct?b , and proposes candidateconcepts from Ot?A that are semantically close to theinitial source concept Cta. In this MCQ category, wepropose stems of the form:What concept <semanticType> <targetconcept>? corresponds to the revision of thecandidate source concept, where< semanticType > refers to the type of mappingrelation semanticTypet?ab andTable 3 Examples of correspondences between SNOMED-CT and ICD9CMConcept source label semanticType Concept target labelIntestinal diseases equivalent to [?] Vascular disorders of intestineNail-Patella syndrome more specific than [?] Congenital malformation syndromespredominantly involving limbsRespiratory tract infections less specific than [?] Acute upper respiratory infection, unspecifiedAbnormality of gastric inhibitorypeptide secretion (disorder) partially correspond to [?] BladderBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 10 of 15Fig. 4 Examples of ambiguous correspondences between SNOMED-CT and ICD9CM<target concept> consists of the label of the targetconcept Ctb. For instance, an instantiation of this stempattern is: What concept is more specific than otherrestrictive cardiomyopathy? The alternatives for thisstem stand for the top n most semantically-closeconcepts to the initial source concept (e.g.,Cardiomyopath, Restrictive). Section Selection ofalternative concepts and mapping relations presentsthe selection of alternative candidate concepts.2. Revision of mapping relation (MR). This categoryof questions proposes revising the type of mappingrelation. More precisely, in case of a negative answerin the previous MCQ option, our method preservesthe initial concept candidate Ct?a and modifies thesemanticType of the adapted correspondence,selecting another alternative mapping relation (cf.Section Selection of alternative concepts andmapping relations). We propose stems of the formChoose the correct mapping relation alternative.The proposed alternatives are declarative sentencesderived from the question patterns.3. Revision of both. In case of a negative answer in theprevious option, our method revises both candidateproposals and semantic relation types, aggregatingboth option 1 and 2 in a single multiple choicequestion. We formulate stems of the form Choosethe correct source concept and relation typecorresponding to the revision of both the candidatesource concept and semanticType of mapping. Wepresent alternatives for the question generation in 3columns format, where the first column consists ofthe list of selected source concept alternatives, thesecond column presents the list of new suggestedtypes of semantic relations and the third columncontains the target concept.We present two instantiation examples from our datasetin the following:1. Is Other spontaneous pneumothorax more specificthan closed pneumothorax?Alternative source concepts: Iatrogenic pneumothorax Secondary spontaneous pneumothorax2. Is Gastroparesis more specific than DiabeticGastroparesis associated 2 diabetes mellitus?Alternative source concepts: Acute dilatation of stomach Dyspepsia and other specified disorders offunction of stomachSelection of alternative concepts andmapping relationsThe defined approach based on MCQ (cf. Section STEP2: multiple choice questions) requires selecting candi-date concepts and different semanticType relations assuggested alternatives in question generation to supportmapping validation over time. For this purpose, we pro-pose an algorithm to select similar concepts to the originalsource concept.Selection of alternative conceptsIn the scope of source concept revision, we generate alter-natives in MCQs by using candidate concepts from thecontext of the initial source concept in the ontology. Weaim at combining the answers from these questions topropose re-adapting correspondences if necessary. Forexample, if a given correspondence between source con-cept Cta and target concept Ctb is adapted, such that aconcept Ct?k ? Concepts(Ot?A) replaces the original conceptCta, this generates an adapted correspondence at time t?between Ct?k and Cj?b ? Concepts(Ot?B).Therefore, we retrieve from the ontological context CT(cf. Eq. 1) a set of other concepts which differs fromCt?k , Candidates = {(Ct?ai , simi)i ? [ 1..n] }, where Ct?i ?CT(Ct?a ).Algorithm 1 presents the designed procedure to retrievethe candidate concepts from the context, given a sourceconcept Cta of a mapping. The algorithm sorts thebest top n candidate concepts from CT(Ct?a ) using asimilarity measure. We use the the bigram similarityBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 11 of 15measure following the observations of [38] on its suit-ability for ontology matching tasks. For two given labels,Bigram similarity is computed as the euclidean dis-tance, using all possible bigrams from both labels asdimensions. In our approach, we compute the similaritybetween pairs of comparable attributes that are selectedbeforehand as a parameter (e.g., the name and syn-onym attributes). We denote the similarity function assimAtt(ati .value, at?j .value) between two attribute valuesati .value and at?j .value.Algorithm 1: Find candidate concepts in contextRequire: Cta ? Concepts(OtA);CT(Ct?a ) ? Ot?A;Ct?k ?Concepts(Ot?A); n ? N;Attributes(Cti )forallCti ?Concepts(OtA)Ensure:Candidates = {(c1, sim1), (c2, sim2), ..., (cn, simn)}Candidates ? ?;maxSim ? 0;for all atp ? Attributes(Cta) dofor all Ct?i ? CT(Ct?a ) doif Ct?i = Ct?k thenfor all at?i ? Attributes(Ct?i ) dos ? simAtt(atp.value, at?i .value);ifmaxSim < s thenmaxSim ? s;end ifend forCandidates ?Candidates ? {(Ct?i ,maxSim)};maxSim ? 0;end ifend forend forreturn Candidates ? sort(Candidates, n); {Select topn concepts}Given all attributes of the original source concept, thealgorithm retrieves all concepts in context CT at timet?. For all retrieved concepts different from the conceptCt?k , to which the adapted mapping is associated, the algo-rithm selects their attributes and calculates the similaritybetween the attribute values (between attributes of thesource concept and attributes of concepts inCT). For eachcandidate concept, the algorithm keeps the maximal sim-ilarity value calculated among the attributes. Finally, thealgorithm sorts the top n retrieved candidate conceptsaccording to the calculated similarity. We use these can-didates as alternative answers in our MCQ approach, sothey play a central role for the automatic generation of thequestions.Selection of alternativemapping relationsRevising the semantic relation semanticType in ourquestion generation method demands retrieving alter-natives for the second category of proposed MCQ (cf.Section STEP 2: multiple choice questions). To this end,we recover a set of semantic relations Relalternatives ={(semanticTypei)i ? [ 1..n] } where semanticTypei ? {?,?,?,?} such that semanticTypei = semanticTypet?ab.We use the Relalternatives to formulate the question in therevision ofMR category. The alternative relations are pro-posed from the most precise one to the more general one(i.e., ?, ?, ?, then ?).Experimental evaluationWe selected a set of ontologies and mappings anddesigned a series of experiments to evaluate the pro-posed methods. In this article, we considered medicalontologies and mappings in English language, but ourapproaches can be applied to other languages as well.We present the obtained results in Sections Exper-iments on ontology validation and Experiments onmapping validation and discuss our findings in SectionDiscussion and Future work.Experiments on ontology validationMaterialsWe tested our ontology validation approach on threedifferent medical ontologies that cover different aspectsof the medical domains (Treatment-Disease vs. mentalhealth) and constructed using different methods: Caries Ontology (CO). CO was developed manuallyby a dentistry expert in our company. Disease-Treatment Ontology (DTO). Weconstructed an OWL translation of the ontologyproposed by Khoo et al. [39]. Mental Diseases Ontology (MDO). This ontology ispublicly available.Results of ontology validationFor the first step of the experiment, Table 4 presentsthe number of questions with respect to the num-ber of classes, properties and instances of eachontology (DTO, MDO and CO) without questionoptimization.The number of generated questions depends on theontology size and shows the importance of question rank-ing and optimization. The results indicate that the opti-mization method works better in case of ontologies withmany instances. For the CO ontology, this strategy helpsminimizing the number of submitted questions from 290to 283 questions with only four NO answers. For theMDOontology, our method allows asking 239 questions insteadof 243 with only two NO answers. In case of ontologiesBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 12 of 15Table 4 The number of ontology elements (OE) and the number of generated questions for different medical ontologies withoutoptimizationOntology Number of classes Number of properties Number of instances Total number of OE Number of questionsDTO 49 148 0 197 165MDO 149 76 18 243 243CO 26 266 13 305 290with more NO answers (i.e. more invalid elements), thenumber of deleted questions will increase.For the DTO ontology, the concepts have no instancesand all facts were evaluated as correct by the expert, con-sequently the initial number of questions was conserved.The ontologies used in these experiments were con-structed manually and semi-automatically. More experi-ments should be conducted on automatically constructedontologies when available in order to evaluate more accu-rately the benefits of question optimization.In the case of ontologies with few invalid elements (fewNO answers), other methods should be used to optimizethe presentation and reduce the time needed to answerthe questions. For example, the following presentationmethods can be studied: (i) question factorization accord-ing to an ontology element (concept, relation or individ-ual) and (ii) logical chaining (A hasRelation1With B, BhasRelation2With C, etc.). Such organization can be effec-tive in helpingmedical experts understand and answer thequestions more quickly.Experiments onmapping validationMaterials and experimental procedureWe evaluate the NL quality of the questions generatedautomatically to validate mappings. We use two biomed-ical ontologies SNOMED-CT9 (SCT) and ICD-9-CM10(ICD9) including different versions of official mappingsestablished between them.We aim to investigate to which degree it is possibleto generate NL sentences that can adequately describemappings. For this purpose, we evaluate the generatedquestions according to three standard measures in NLgeneration: grammaticality, fluency and meaning preser-vation. Since our approach aims to facilitate human inter-vention in mapping adaptation, we assume that it isrelevant to assess the NL quality of the automatically-generated questions.We presented the generated questions to three differ-ent human assessors who were asked to associate a scorevalue between 1 and 10 for each dimension and each ques-tion. Assessors were ontology experts and familiar withthe biomedical domain. We evaluated the approach forthe validation of 20 randomly-selected adapted mappingsgenerated from the evolution of mappings between SCTand ICD9.We measure the Inter-Assessor Agreement (IAA) forgrammaticality, fluency and meaning preservation. IAAcorresponds to the average ? measure defined in [40].The ? measure indicates how much the assessors agree-ment is above the probability of an agreement by chance,and it is commonly used in computational linguistics. Inorder to have relevant measures, we define 3 score inter-vals for grammaticality, fluency andmeaning preservationwhich are: [0..3], [4..6], [7..10]. We use these intervalsas categories in the calculation of the ? measure, whichcorresponds to:? = P(a) ? P(c)1 ? P(c) (2)where P(a) refers to the observed inter-assessor agree-ment and P(c) is the probability of a chance agreement. ?values range from -1 to 1 (cf. Table 5 for results).Results of mapping validationTable 5 presents the obtained results for the 20 Booleanquestions that are generated for the 20 targeted mappings.They present the measures of IAA for grammaticality,fluency and meaning preservation.Our second focus is to evaluate the usefulness of eachquestion type. To this end, we count (i) the number ofreturned answer-types for the set of 20 questions accord-ing to the different question types and (ii) the number ofvalidation/invalidation according to the observed adaptedmappings (examining the evolution of the two officialTable 5 Quality of the NL generated questions for mapping validation and average ? Inter-Assessor AgreementGrammaticality Fluency MeaningMin. Max. Avg. Min. Max. Avg. Min. Max. Avg.Assessor 1 0.4 1 0.775 0.4 1 0.81 0.4 1 0.915Assessor 2 0.4 1 0.745 0.4 1 0.77 0.4 1 0.86Assessor 3 0.6 0.9 0.735 0.6 0.9 0.745 0.6 0.9 0.77Average ? 0.28 0.48 0.45Ben Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 13 of 15releases mappings in our dataset) (cf. Table 6). In this sec-ond independent evaluation, we asked the assessors tofind a common agreement on the semantic correctness ofthe correspondences.The overall assessment of the generated initial BooleanQuestions (cf. Section STEP 1: Boolean questions) indi-cates good values for grammaticality and fluency becausethe attained average values are satisfactorily high regard-ing the used metric. The most important criterion forthe mapping validation, which is meaning preservation,had the best score by the assessors. The ? Inter-AssessorAgreement is also relatively high (? is not negative), whichprovides a positive test on the reliability of the assessorsratings. In short-term perspectives, further tests will bemade with a correlation-based approach to have a moreprecise view on the levels of agreement.Table 6 shows the percentage of different answer typesreturned during the validation of the adapted mappings.Results indicate that 80 % of the initial adapted mappingswere validated or led to the validation of another newmapping (i.e., re-adaptation) (cf. Final output validationrow in table 6), discovered during the validation process.On the test set of 20 mapping adaptations, only one ques-tion was rated as ambiguous, due to an incomplete con-cept label. The low percentage of Yes answers for the initialBoolean questions indicates that the automatic selectionof mapping adaptations using the approach described in[31] were insufficient for this dataset. The 3rd type ofquestion, revising only the mapping relation, allowed tovalidate 40 % of the mappings. Revising the source con-cepts alone fails to validate more mappings, but togetherwith the revision of the mapping relation it allowed to val-idate 40 % of the mappings (cf. see 4th type of questionsrevision of both in Table 6).Discussion and Future workThe experiments on ontology validation showed the needto add other specific types of questions and answer types.In some observed cases an answer can be YES but fora specific kind of patients (e.g. Infant) or also NO for aspecific kind of patient, or under a specific condition. Inour experiments the experts answered NO for such cases.Table 6 Answer types according to question typesAnswer typeYes answers No answers Ambiguoussource/targetQuestion typeBoolean question 0 % 95 % 5 %Revision of cs 0 % 95 % 5 %Revision ofMR 40 % 55 % 5 %Revision of both 40 % 55 % 5 %Final output validation 80 % 15 % 5 %Therefore, it would be interesting to give to the expert thepossibility to specify a contextual element or an additionalcondition to their YES/NO answers. A possible solutioncan be to integrate factual questions as possible questiontype, which will also contribute to enrich the ontologyduring the validation process.On the other hand, even if our optimization methodmay allow significantly reducing the number of questions,it is still challenging to validate very large ontologies withnatural language questions. In this context, advanced con-tent selection techniques such as summarization can playan important role. As suggested by Sure et al. [41], a sum-mary of an ontology might include a couple of top levels inthe ontologys class hierarchy, and also the ontologys hubconcepts (i.e. concepts with the largest number of links).To validate huge ontologies, we are consideringapproaches based on summarization. A possible solu-tion can be by using the Key Concepts Extraction (KCE)algorithm which automatically extracts the most repre-sentative classes of an ontology. More particularly, sum-marization can be adapted to the validation task by takinginto account several features such as the number of ques-tions needed to validate a given extract or summary andthe number of key concepts.On the level of Mapping Validation, the conducted liter-ature survey indicated that the generation of NL questionsfor mapping validation was not investigated. Our proposaloriginally evaluated the quality of generated NL questionsto help human experts judge the quality of correspon-dences under evolution. The proposed method uses thecontext of the source concept to select similar concepts asalternatives in case of invalidated mappings.In the conducted experiments on mapping validation,the analysis of quality-deficient Boolean questions pro-duced by the NL generation system highlighted to twomain error causes: The heterogeneity and length of the literal attributesthat led to some inadequacies with the conceivedpatterns, e.g., Are other eye disorders more specificthan family history degenerative disorder of macula? Errors in the concepts attributes (mainly labels), e.g. Is other more specific than mechanicalcomplication of suprapubic catheter?.These observations show the importance of evaluatingthe linguistic quality of the ontology literals beforehand.This issue is particularly discussed in [42], where a meta-model is proposed to link ontology elements to relevantlexical entries. In the scope of our approach on ques-tion generation, an enhancement could be to (i) haveseveral patterns that paraphrase the same mapping rela-tion and (ii) syntactically parse the generated question todetect more trivial errors and choose alternative patternsBen Abacha et al. Journal of Biomedical Semantics  (2016) 7:48 Page 14 of 15if needed. According to our experiments, multiple choicequestions with 5 correction alternatives for each invalidmapping proved to be efficient to locate the correct map-ping relation and/or mapping target.ConclusionsThe methods for automatic ontology construction havehighlighted the problems of validation of ontologies andmappings. Research efforts and interests have grownmoreand more, but mostly at a formal level. In this article, weaddressed the problem of ontology validation includingmappings from a conceptual and a semantic point of view.We proposed novel semi-automatic approaches basedon the generation of questions and answers interpreta-tion to facilitate the communication with domain experts.The defined methods generates natural language ques-tions from medical ontologies and mappings and uses theanswers from the expert to update/correct them.Our approach implementing automatic methods mightguide domain experts in the validation process. Relying ondomain experts to monitor the validation can lead to var-ious benefits, ensuring reliable communication betweenheterogeneous systems.We evaluated the proposed methods based on severalreal datasets from different releases of biomedical ontolo-gies and their associated mappings. The achieved resultsunderscored the feasibility of our general approach for thevalidation of ontologies and mappings, and its relevantrole in the completion of the their adequate evolution overtime.Endnotes1http://ihtsdo.org/snomed-ct/2http://ncit.nci.nih.gov/3http://bioportal.bioontology.org/4http://www.chu-rouen.fr/cismef/5An international workshop is also held on questiongeneration since 2008.6http://nlp.stanford.edu/software/lex-parser.shtml7http://www.w3.org/TR/rdf-mt/#RDFSRules8Free textual explanation is considered for future per-spectives and is not used in this work.9www.nlm.nih.gov/research/umls/licensedcontent/snomedctarchive.html10www.cdc.gov/nchs/icd/icd9cm.htmAcknowledgementsWe thank the SMBM 2014 program committee members for their feedbackand for the opportunity to extend our original submission [9] in the scope ofthis special issue. We would also like to thank our anonymous reviewers fortheir insightful comments that helped to improve our paper. This researchwork was funded in part by the the São Paulo Research Foundation (FAPESP)granted to JDR (Grant #2014/14890-0). The opinions expressed in this work donot necessarily reflect those of the agency that funded one of the authors.Authors contributionsABA, CP and MDS designed the proposed approach for ontology validationusing natural language questions. CP constructed the OWL translation of theDisease-Treatment ontology. ABA designed the approach for questionoptimization, implemented and evaluated the question generation system forontology validation and wrote the first manuscript. All authors contributed tothe manuscript. ABA and JDR designed the proposed approach for mappingvalidation. ABA implemented the system for mapping validation usingboolean questions for new mappings and multiple choice questions withsuggested alternatives for invalid mappings. JDS implemented the generationof candidate concepts from the context of the initial source concept in theontology. ABA, JDR and YM evaluated the mapping validation system. Allauthors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Luxembourg Institute of Science and Technology (LIST), Esch-sur-Alzette,Luxembourg. 2Institute of Computing, University of Campinas, Campinas,Brazil.Received: 5 March 2015 Accepted: 22 June 2016Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 DOI 10.1186/s13326-016-0095-8RESEARCH Open AccessInformationandorganization inpublic healthinstitutes: an ontology-based modeling of theentities in the reception-analysis-report phasesGiandomenico Pozza1* , Stefano Borgo2, Alessandro Oltramari3, Laura Contalbrigo1and Stefano Marangon1AbstractBackground: Ontologies are widely used both in the life sciences and in the management of public and privatecompanies. Typically, the different offices in an organization develop their own models and related ontologies tocapture specific tasks and goals. Although there might be an overall coordination, the use of distinct ontologies canjeopardize the integration of data across the organization since data sharing and reusability are sensitive to modelingchoices.Results: The paper provides a study of the entities that are typically found at the reception, analysis and reportphases in public institutes in the life science domain. Ontological considerations and techniques are introduced andtheir implementation exemplified by studying the Istituto Zooprofilattico Sperimentale delle Venezie (IZSVe), a publicveterinarian institute with different geographical locations and several laboratories. Different modeling issues arediscussed like the identification and characterization of the main entities in these phases; the classification of the(types of) data; the clarification of the contexts and the roles of the involved entities. The study is based on afoundational ontology and shows how it can be extended to a comprehensive and coherent framework comprisingthe different institutes roles, processes and data. In particular, it shows how to use notions lying at the borderlinebetween ontology and applications, like that of knowledge object. The paper aims to help the modeler to understandthe core viewpoint of the organization and to improve data transparency.Conclusions: The study shows that the entities at play can be analyzed within a single ontological perspectiveallowing us to isolate a single ontological framework for the whole organization. This facilitates the development ofcoherent representations of the entities and related data, and fosters the use of integrated software for datamanagement and reasoning across the company.Keywords: Ontology, DOLCE, Data transparency, Data integration, Knowledge objectsAbbreviations: A, Responsible agent; ASO, Agentive social object; BPMN, Business process model and notation; D,Information entity; DOLCE, Descriptive ontology for linguistic and cognitive engineering; GIS, Geographic informationsystem; GUI, Graphical user interface; IZSVe, Istituto Zooprofilattico Sperimentale delle Venezie; ISO/TC, InternationalOrganization for Standardization/Technical Committee; LARU, Reception and public relation laboratory; LIMS,Laboratory information management system; LOINC, Logical observation identifiers names and codes; M, Materialentity; MDA, Model driven architecture; NPED, Non-physical endurant; KO, Knowledge object; SNOMED CT,Systematized nomenclature of medicine - clinical terms; UML, Unified modeling language*Correspondence: gpozza@izsvenezie.itEqual contributors1Istituto Zooprifilattico Sperimentale delle Venezie, Viale dellUniversitá, 10,35020 Legnaro (PD), ItalyFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 2 of 15BackgroundOntologyAn ontology is the part of the information system thatexplicitly commits it to a certain conceptualization of theworld [1, 2].When described in terms of lexical semantics,ontologies take the simple form of dictionaries or thesauri;when described in terms of axioms in a logical language,we talk about formal ontologies; if logical constraints areencoded in a computational language, formal ontologiesturn into computational ontologies. Finally, an ontologythat concentrates on general and domain-independentcategories is called foundational.The applied ontology approach has had a huge impact inall branches of information science. The techniques devel-oped in the last twenty years are now exploited by compa-nies around the world in particular in areas like intelligentinterfaces [3], data access [4] and warehouse [5], semanticweb standard [6] and medicine [7, 8].In the life sciences, ontological techniques are appliedtowards a variety of goals [9] among which the studyand organization of areas like genomics [10], anatomy[11, 12], plant anatomical and morphological struc-tures [13], phenotype annotation [14], with importantresults also in knowledge modeling, organization, integra-tion and exploitation [8, 15, 16]. Notwithstanding theseachievements, the application of ontological techniques isstill problematic [17].Our aim in this paper is to show how ontology canbe applied to understand the organization and the activ-ity of large institutes woking in the life sciences. We usea public veterinary organization located in north-easternItaly, namely the Istituto Zooprofilattico Sperimentaledelle Venezie [18], to clarify our analysis and exemplifythe application of ontological techniques.The Istituto Zooprofilattico Sperimentale delle VenezieThe Istituto Zooprofilattico Sperimentale delle Venezie(IZSVe) is an Italian public veterinary institute deputed toconduct prevention, control, research and services in thefields of animal health and food safety. The IZSVe belongsto the Italian National Health Service, is part of a nationalnetwork that consists of nine other public veterinary insti-tutes, employsmore than 600 people (veterinarians, biolo-gists, chemists, technicians and administration staff ), andhas eleven geographical locations with groups and labo-ratories devoted to areas like animal welfare, diagnosticservices, food risk communication, geographic informa-tion systems (GIS), international cooperation, training,veterinary biobank. The IZSVe carries out routine tests indisciplines like diagnostics, virology, parasitology, micro-biology, molecular biology and chemistry. Its researchactivities concentrate on the animal health and food safetyfields aiming to develop new diagnostic techniques as wellas vaccines and vaccination procedures.Limits of standards and software toolsThere are many standards centered in aspects relevant tothe reception, analysis and report phases of a public insti-tute like IZSVe: the Logical Observation Identifiers Namesand Codes (LOINC) [19], the Systematized Nomencla-ture Of Medicine Clinical Terms (SNOMED CT) [20],the ISO/TC 212 Clinical laboratory testing and in vitrodiagnostic test systems and others [21]. Also, there areseveral languages and conceptual modeling techniquesthat the knowledge engineer can use to develop informa-tion systems, e.g., the Business Process Modeling Nota-tion (BPMN) [22] or the Unified Modeling Language(UML) [23] and its related standards like the ModelDriven Architecture (MDA) [24]. However, all these stan-dards and modeling systems focus on some elements, e.g.LOINC, or aspects, e.g. BPMN, of the complex scenar-ios in public institutes like the IZSVe. Even the UMLlanguage, which is perhaps the most broadly applicable,assumes that the modeler adopts a viewpoint, i.e., hasan understanding of the scenario. How to reliably under-stand the domain and, more specifically, the scenarioof interest is a goal explicitly addressed by ontologicalanalysis.Regarding the analyses carried out in laboratories,typically a Laboratory Information Management System(LIMS) is exploited to manage and coordinate the infor-mation on the tests and the related set of materials andprocedures. Nonetheless, the management of a large setof laboratories and tests is quite complicated and in manycases the LIMS is applied to the subset of data that arehomogeneous across laboratories and activities. This isthe situation at the IZSVe, which offers about 950 types oftests and runs almost 1.7 million tests per year. It has beenrecognized that having a partial management via the LIMSreduces the possibilities to coordinate, monitor and ana-lyze the institutes activities and the performance. On theother hand, the adoption of different LIMS allows to takeinto account the specificity of each laboratory. However,fine tuning each LIMS to a specific case jeopardize thepossibility of an integrated data and process managementsystem.These problems arise from a substantial lack of a uni-fying understanding of the institutions scenarios and ofthe role that the different elements (objects, tools, data,personnel) play in its activities. We study this issue look-ing at the IZSVes use case. The goal is to develop a globalview for a centralized management system and verify thecontribution of ontological techniques in understandingcomplex situations.The ontological approach is also promising in deal-ing with the information systems evolution. The IZSVeadopted a state-of-the-art LIMS ten years ago and, sincethen, the system have gone through several updates,revealing a certain lack of flexibility. It is recognizedPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 3 of 15today that the LIMS is unable to evolve with and adaptto the companys needs. It is unclear whether the prob-lem lies in the technical and implementation aspects ofthis LIMS or in the initial analysis of the institute. Mostlikely, it is a combination of both. In any case, to alignthe LIMS and the institutes activities, a deep under-standing of the institute seems necessary, including itsreasons to exist and its strategies. This information isabout what the institute does and why, and should notbe confused with how the institute does it today or atany other period of time. These are inherently ontologicaldistinctions.Ontological analysisSome of the most relevant issues posited by the scenariowe study are: (a) to identify and characterize the elementsin the institutions activities; (b) to classify the types ofdata that are needed and to identify where they are used;and (c) to make explicit the context and the role of theinvolved entities.To study the IZSVe scenario we adopt an approachbased on a foundational and formal ontology. The use of afoundational ontology ensures that the principles are notconstrained by the specific domain we work with which,in turn, leads to an understanding of the institute inde-pendent from contingent settings (like the organization ofthe institute at some point in time or the set of tests itmakes available). This feature makes the resulting modelmore flexible. For example, the introduction or modifi-cation of other laboratory methodologies and techniques,which requires important changes in traditional rule-based systems, is managed in an ontological model viaextensions, e.g. by adding (or dropping) classes and theirdescriptions. Furthermore, the use of a logic languagewith formal semantics allows to check the consistencyof the system well beyond approaches like BPMN andUML.In the rest of the paper we use ontological analysis anda foundational ontology to study the reception, analysis,and report phases in public institutes in the life sciencedomain. The goal is to show how to understand the sce-nario, how to isolate and distinguish the relevant entities,to indicate their relationships and which roles they play.The entities we discuss, like specimens, reports, sampleseals, laboratory tools, data sets, administrative stuff andso on, will be classified according to their ontologicaltypes.One important result of this work is the developmentof a single framework where it is possible integratingall the discussed entities, data and roles. Working witha single model helps also to evaluate the coverage ofthe domain, to check the coherence across elements andphases, and to facilitate maintenance. Finally, as pointedout earlier, by using a foundational ontology we expectthat the model we obtain will remain valid across time,provided the essential constraints remain unchanged (e.g.,in the scenario, the law defining the IZSVes institutionalgoals).MethodsIn this part we first introduce the ontological analysisapproach and give some indication on how its applica-tion can be evaluated. Then, we use aspects of the IZSVescenario to introduce the rationale, the basic structureand some categories of the DOLCE foundational ontology.Later, in the next section, we will use ontological analysisto discuss typical modeling issues taken from our sce-nario, and will use the results of this analysis to expand theDOLCE ontology to an ontology for public institutes in thelife sciences.Ontological analysis and its assessmentGenerally speaking, it is important to distinguish howthings occur in the activities and what are the expectationswe have about them. Ontological analysis helps to makesubtle, yet crucial, distinctions. For example, ontologicalconsiderations lead to separate the seal of a specimen con-tainer as a signal of integrity (a role) from the seal as anartifact (an object). Once we have identified the entitiesin a scenario, we need to organize them in a hierarchyand to establish their relationships (e.g. it is the object-seal that plays the role-signal of integrity, and stops whenbroken), and to ensure that the hierarchy leaves space forfuture extensions. After all, a finer analysis may lead tointroduce new entities, and one may want to expand thisvery hierarchy on aspects not yet considered like new pro-cedures, safety regulations, responsibilities or workloadmanagement.Usually, a model presents a perspective. It can presentthe scenario from the perspective of the service user,from that of the overall organization, or from that of asample to be analyzed. Since all the key elements thatcharacterize these different tasks and views could be orga-nized within an ontological framework, we aim to showthat it is indeed possible to construct such a generalframework. This framework provides a conceptual sys-tem that comprises the different perspectives and, thus, issuitable for tasks as different as data management, qual-ity assessment and responsibility tracking. Our work isbased on techniques that have been developed from the90s to guide the development of robust ontologies, seee.g. [2531].Being a conceptual tool, ontological analysis is not suit-able for quantitative evaluation. However, there are differ-ent qualitative parameters to assess its results, e.g. [32].We will apply them to evaluate the results we obtain instudying our guiding scenario (see Sections Devising theontology and Framework Evaluation).Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 4 of 15The IZSVe scenarioAll the IZSVe laboratories activities related to samplereceiving, delivering and transferring are coordinated bya centralized delivery service, the Reception and PublicRelation Laboratory (LARU), located at the headquar-ters. All the information on the samples and the analyt-ical processes (like tracking the samples distribution andmanaging the analysis procedures) are registered in thededicated LIMS, called IZILAB. IZILAB manages also dataunnecessary for the tests, e.g., information about involvedparties (sample deliverer, sample owner, sample collector,etc.), reason of investigation, submission form number,breeding or food processing plant where the sample wascollected. Data are collected from different sources fromhuman operators to automatic access to, e.g, the farmregistry database.The rest of this section presents a typical IZSVe sce-nario. To keep the presentation simple, the descriptionfocuses on some interesting parts of the overall sys-tem. Later we will refer to this scenario as our guidingexample.A qualified technician, personally or via a delivery ser-vice, delivers a sample  e.g., a pathological specimen fromalive or dead animals or food for human consumption to a IZSVe reception point requesting to test the speci-men on some characteristics, e.g. microbiological safety.The reception unit personnel collects the sample, the sub-mission form with the required analysis and performssome preliminary check like the presence and integrityof sample seals; the samples storage temperature (whenneeded); the match between the declared number of sam-ples (or units) and the delivered items; the presence ofthe requested analysis in the list of services provided byIZSVe.The reception unit may perform more specific checksto verify, for instance, that: the needed information isreported in the submission form; the form lists any spe-cial management constraint (e.g. the analysis might berequested at a fixed date and time for the participationof external observers); the sample has been correctly col-lected and preserved for the requested analysis type (e.g.storage temperature; timing of the analysis).After the checks, the sample is registered in the IZILABand unique identification labels are physically attached tothe sample container and to the analysis submission form.A receipt with the identification number is released to thedeliverer. From this point on, the IZSVe is fully responsi-ble of the sample management and all the data needed forthe IZSVe internal procedures are made accessible to theoperators via the IZILAB.Next, the sample is stored in a ward (storage room,cooler, freezer) to be distributed to the laboratories. Theregistration data, called batch, is added to the IZILABsbatch-list of the ward (a kind of loading/unloadingregister). The sample is then collected by laboratory per-sonnel or delivered via the IZSVe service. At the lab-oratory, the administrative and technical staff make afinal assessment on the suitability of the sample for therequested analysis: the documents and the compliance ofthe specimen to the test requirements are verified. Then,the seals are broken and direct inspection of the samplecontent can be done. The laboratory personnel completethe data via a dedicated interface in IZILAB: the batchcode ensures that data are added to the right record aswell as the consistency of the sample tracking informa-tion. Some fields are shown in the IZILAB labs receptionGUI - Graphical User Interface -(Fig. 1). The numberof external acceptance and the date of external accep-tance are needed to coordinate further tests, if any, runby other IZSVe Laboratories. The flag delivery chargeactivates the fields for delivery charges. The flag identi-fication of the payer indicates the party charged for theprocedure costs. The rules for test assignment providesinformation on the acquisition of digital data while thefirst/sequence is needed when there are several samplesand/or several analyses are done on the same specimen.Once the data are uploaded in IZILAB, a working paper(lab sheet) is prepared where the lab technicians reportthe steps of the analysis process. At the end of the anal-ysis, the administrative assistant uploads into IZILAB therough data and the Laboratory Head checks whether fur-ther tests are needed, e.g. for verification. If s/he decidesfor further tests in a different laboratory, the process(transferring and analysis) is repeated as before but thebatch-list (loading/unloading) is now linked to a wait-ing acceptance list devoted to samples moving from labto lab. Once the analyses are completed and all dataare collected in IZILAB, the report is produced by theadministration and the Laboratory Head digitally signs it.The file is then made accessible (in full or limited form)through the web to the parties that have the right toaccess it.Devising the ontologyResearch in applied ontology has devised a series of logic-based relations and categories that furnish the startingpoint for entity analysis and classification. The idea is toexplicitly list all the types of entities that are taken to exist(or at least to be of relevance) in the application domainand to classify each specific item as belonging to one sin-gle category. Further constraints help to enforce the rightuse of the hierarchy.In this paper we adopt the foundational ontologyDOLCE [33] with some extensions relative to the cat-egories of roles and descriptions as presented in [34].DOLCE is a foundational and formal ontology developedfrom cognitive and linguistic considerations and with par-ticular emphasis on social reality (Fig. 2). Our choice ofPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 5 of 15Fig. 1 GUI at the labs reception. Graphical user interface (GUI) translated, original in ItalianDOLCE relies on a few observations: DOLCEs underly-ing principles and construction techniques have been welldescribed [33] and there is evidence that this ontology ispreferred even by non trained users [15], the ontology isavailable in different formalisms [35], it is stable and sev-eral extensions are available, e.g. social roles [36], artifactsand products [37] and mental states [38]. Furthermore,the ontology has been verified in terms of ontological andlogical soundness [39, 40].Here we describe the parts of the ontology structure,and some of the basic relations, that are needed to under-stand the material in this paper. The presentation isminimal, the user can find a complete introduction in[33, 34]. DOLCE is based on the basic distinction betweenobjects, events and qualities. Other important categoriesare those of descriptions and roles. Objects (DOLCE cate-gory: Endurant) are entities that are mainly identified bytheir relationship with space while they persist in time.People, samples, laboratory equipments as well as the cav-ity of a specimen container are classified as objects inDOLCE. Events (DOLCE category: Perdurant) form a dif-ferent category; these are things that necessarily happenin time like the (process of ) delivering a sample or runninga laboratory test. The category Quality gathers individualproperties, i.e. properties associated to a specific objector a specific event, and that serve to qualify them: eachsample has its own specific weight and temperature, eachinstance of a laboratory test has its own duration etc.Qualities can be simple like weight, temperature and dura-tion; or complex like price, frequency and speed. The cat-egory Description collects information entities, like pro-cedure specifications, and the category Social Conceptsentities like the Italian and the European legal notions oforganization. We anticipate that members of the last twocategories are generally seen as temporal objects (they liveand change in time) but for practical reasons they are heretreated as atemporal entities. Thus, here we ignore thatthe society, the laws and the concepts evolve in time. Thischoice will become clear in Section Roles and players inthe IZSVe scenario. Descriptions and social concepts aredistinguished in the ontology from their physical supportsand realizations: the EU legislation on the control of thenotifiable disease of livestock (e.g. Foot and Mouth Dis-ease) is ontologically a concept, thus a different kind ofentity than the document where it is written (which is aphysical object), and the application of the regulation isstill another kind of entity, namely an event. This event isthe instantiation of a procedure, which is a description. Afurther class, Role, collects properties that identify a tem-poral status of an entity usually dependent on some socialPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 6 of 15Particular(PT)Abstract(AB)Endurant(ED)Non-physicalendurant (NPED)Perdurant(PD)Eventive(EV)Stative(STV)Quality(Q)Physicalquality (PQ)Temporalquality (TQ)....Description(DS)....Socialconcept(SC)Non agentiveph. obj.(NAPO)Agentive ph. obj.(APO)Agentivesocial obj.(ASO)Non agentive social obj.(NASO)....Role(RL)Legal Concept (LC)........Accomplishment(ACC)Achievement(ACH)State(ST)Process(PRO)Fig. 2 Category hierarchy of the DOLCE ontology. DOLCE fragment, from [33], with an extension of the social object category (gray boxes). Arrowsrepresent ISA relationships and dotted arrows chains of ISAcontexts [36, 41, 42]. For example, an individual agent mayplay the role of IZSVes Laboratory Technician in someperiod of time. Similarly, a physical object can play the roleof a Sample or of a Seal within a IZSVe activity (which pro-vides the context). A fragment of the DOLCE categories,with the mentioned extensions, is presented in Fig. 2.Our goal in the rest of this paper is to expand theDOLCE framework with new categories tuned to the mod-eling issues elicited from scenarios like that of IZSVe.This activity is known as domain adaptation [43] andaims to cover the notions characterizing the applicationdomain.The relations among the categories must also be fixed.Structural relationships, like subsumption (aka ISA),instantiation and parthood [29] are already part of theDOLCE language. Subsumption is the subclass relation: forinstance, the category of Agentive social object (ASO) issubsumed by the category of Non-physical object (NPED),Fig. 2. This means that any ASO entity, e.g. a company, isalso an NPED entity. Similarly, the IZSVe category Sub-analysis is subsumed by the IZSVe category Analysis: anyIZSVe sub-analysis is also a IZSVe analysis. The instanti-ation relation applies to a class and an individual, it statesthat the individual is an instance of the class. As an exam-ple, the object identified by code PD/A123 is an instanceof the category Sample. Note that, being this latter cate-gory a subclass of Endurant, the object identified by codePD/A123 is also an instance of the category Endurant.This result is a consequence of the formal interactionbetween the instantiation and the subsumption relations.Finally, the parthood relation is used to state that an entityis part of another entity, e.g. report X of laboratory A canbe part of report Y of laboratory B (say, when the firstreports about a related analysis on the same specimen).The example needs clarification: report X, in the senseof a description (a collection of data, thus an informationentity), is part of report Y , also understood as a descrip-tion. On the other hand, report X in the sense of a pieceof paper (or an electronic file) is part of report Y pro-vided now we understand even Y in the sense of a pieceof paper (or an electronic file). The ontology tells us thatno other combination of these senses holds. The use ofa formal language, as in DOLCE, helps to keep these twoparthood statements apart and to correctly relate themthe right meaning of the terms. Similarly, parthood canbe used to state that an instantiation (a specific event) ofprocedure A (a type of event) is obtained by the (mere-ological) sum of instances of procedures B1, B2 etc. Werefer the reader to [33] and [29] for further details on theserelations.Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 7 of 15Results and discussionIn this part we exemplify the use of ontological analy-sis via the IZSVe scenario that leads to the classificationin Table 1, introduce other relevant notions and thenevaluate the results. We pay particular attention on theselection and description of the static elements and inparticular on roles and dependencies across entities. Thediscussion of events and their interrelationships aims tohighlight the variety of distinctions that can be made.A comprehensive analysis of events from the ontologicalviewpoint is out of the scope of this paper.Categories and participants: a look at the IZSVe scenario Endurant. The objects are grouped via newsubcategories of the DOLCE category Endurant. Weintroduce categories of documents like the analysisresult form and the analysis result (an analysis resultform is a document suitable to list the data obtainedfrom an analysis procedure, the analysis result is thedocument filled out with the data), the analysisrequest form and the analysis request, the registrationform and the test report. Other objects are person,organization, agent, laboratory tool, laboratorymaterial, container, building etc. Perdurant. The new types of event are subcategoriesof the DOLCE category Perdurant. We focus inparticular on perdurants identifying activities like:running a laboratory test; disposing, storing,transferring and delivering a sample; preparing,signing and delivering a test report; requesting ananalysis; breaking a seal etc. Among these, activitieslike running a laboratory test and disposing a sample,have a clear ending point (these activities are calledaccomplishment in DOLCE). Others, like storing asample, do not and are called states. Quality. Qualities in the scenario are divided into avariety of subcategories. Beside the usual qualities likeweight, shape, duration, speed etc. we model alsoaccuracy, reproducibility and repeatability of the tests,price and priority as individual or relational qualities. Description. The category Description is asubcategory of Non-agentive social object andcollects social entities that are neither agentive normaterial. These are information objects that serve asclassifiers for other entities. Typical examples aredescriptions of a tool or a specimen (descriptions ofendurants, for instance the data associated to aspecimen), descriptions of specific processes oractions (descriptions of perdurants, for instance adescription of how specimen X was delivered tolaboratory Y ) and descriptions of methods(descriptions of rules and other constraints, forTable 1 IZSVe elements in our guiding exampleCooler Analysis report Batch-listLaboratory equipment Freezer IZILABIZSVe Laboratory report Laboratory roomReceipt Sample Sample reception pointSample label Seal Submission formStorage room Working sheetWaiting the acceptance list issue Awarding of the batch number Booking of additional testsChecking the rough data Checking the sample Colleting the sampleFilling the submission form Delivering samples to the laboratory Delivering the sample to the LARUIssuing the receipt Editing the batch list Editing the reportSigning the test report Breaking the seals Recording the dataRegistrating the sample Requesting the analysisStorage temperature Accuracy of a test Cost of a testPriority of a test Duration of a test Integrity of sealSample temperature Repeatability of a test Reproducibility of a testNumber of aliquots of a sample Number of steps in a procedureWorking sheet content Analytical method description LARU procedure descriptionLaboratory procedure description Laboratory result for a sample European (national etc.) proceduresSample Owner Administrative Assistant DelivererRequirer Frontdesk Operator Report ReceiverHead of Laboratory Laboratory Technician Qualified TechnicianPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 8 of 15instance the description of the procedure forspecimen delivery). Social Concept. This is a subcategory of Non-agentivesocial object closely related to the previous (seeFig. 2). It is populated with information objects thatacquire social relevance. Here we find the content ofofficial regulations and laws, or the data produced byan official test (these data, legally binding or not, givealways a description in the sense of the previouscategory). Other descriptions with a binding socialvalue, like the official description of a test procedure,are in this category. Two subcategories of SocialConcept are particularly relevant to our work: theRole and the Legal Concept categories. Role. We focus on two types of roles: agent roles,played by agentive entities only (typically humans ororganizations), and functional roles played genericallyby non-agentive physical objects. Among the agentroles there are laboratory technician and laboratoryhead, sample deliverer and analysis payer. Among thefunctional roles, we have specimen, reagent, seal andreference material (note the distinction between thematerial, typically an amount of matter or an artifact,and its role in a procedure). Legal Concept. This category is here introducedfollowing studies in the legal domain [44]. It collectsEuropean, national, regional and internal laws andregulations intended as sets of normativespecifications, that is, not mere descriptions norgeneric social concepts. Abstract. The category of abstract entities does notseem relevant in this context. We mention it just toremind the reader that it serves to classify entities likenumbers and quality spaces [33].These are the most relevant categories in our specificscenario. More specialized categories are discussed laterin this section. Although the overall scenario is reallyrich, these categories exemplify all the issues we foundand suffice to clarify the key modeling choices in thisscenario.The notion of knowledge objectDue to historical, legacy and business factors, organi-zations may adopt special perspectives which are notontologically justified [45]. For instance, the IZSVe con-siders a type of object that is actually a mix of threeDOLCE ontological types: a material entity (e.g. a sample),an information entity (the sample information in IZILAB)and an agent role (e.g. the personnel in charge of the sam-ple). This mix defines an official sample which is a crucialelement in the institutes legal activities. This situationis fairly common across companies although the charac-teristics of these entities may change considerably. Sincethese entities are important for the organization and arenot ontologically consistent (an ontological entity cannotbe member of disjoint categories), to model them we usea specific methodology [46, 47] allowing to introduce anew type of objects called knowledge objects. The exten-sion is not necessary for the organizations information(the ontology suffices for this goal) but helps to include inthe model these special perspectives of the organization.Let us see how knowledge objects can be modeled bystudying the IZSVe case. Given the above description, anIZSVe knowledge object KO comprises a material entityM, an information entity D and an agent role A. Then,the object KO is the triple (M,D,A) such that at time t,the element D of KO collects the information that IZSVehas at that time t on M and the element A is an IZSVerole responsible of the status of and the changes in Mand D at that time. Typically, the M is a physical objector a quantity of matter officially delivered to IZSVe foranalysis. The entity D is created when at the time of theM reception an IZSVe operator creates the data recordabout M in the IZILAB. Thus, D is the information objectrelative to M stored in the IZILAB database and regu-larly updated during the IZSVes procedures on M. A isthe person responsible of the service or laboratory that ismanagingM.Since knowledge objects are not part of the ontology,they should be seen as auxiliary elements of the informa-tion system. They evolve in agreement with their ontolog-ical components (M, D and A). A sketch of the changesthat a knowledge object KO undergoes from its status atthe reception time t, given by (M,D,A), to its status at thetime t? in which the laboratory report is written, indicatedby (M?,D?,A?), is shown in Fig. 3. The figure highlightsthree temporal points t, t? and t??, and three phases (timeis oriented top-down): specimen check, specimen analysisand report writing. The solid arrow on the left is markedby events relative to changes in the material componentM, the solid arrow in the center is marked by events rel-ative to the information component D, and the one onthe right to the agentive component A. KO, as a whole,has also its own specific properties. We say that a KO =(M,D,A) is completewheneverD contains all informationrelative toM that are of value for the IZSVes activities andinstitutional goals. Furthermore, the procedure to managethe KO is said to preserve correctness if, given that the datainD relative to some property ofM at a time t is true, thenat any later time t? D contains only true data relatively tothat property ofM.Recall that, due to the introduction of knowledgeobjects, the model we are building is not purely ontolog-ical. The reason is that these knowledge objects, beinga combination of elements from disjoint categories, relyon a combination of properties that is incompatible withthe assumptions of the ontology. In the ontology M, DPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 9 of 15AGENT ROLE(A)INFORMATION(D)SPECIMEN(M)knowledge object K corresponding to (M',D',A') at lab report timeknowledge object K corresponding to (M,D,A)  at lab reception timeTIMEtanalysisprocedurestarts pre-analysis data updatespecimen check endsM       D       A  M'      D'        A'  t'  specimen is acted upon actions and data are reportedspecimencheckspecimenanalysisreportwriting t''  analysis procedure endsadministrativelab technicianlab technicianFig. 3 Tracing the changes in IZSVes knowledge objects. Possible changes that a knowledge object undergoes from (M,D, A) to (M? ,D? , A?) withinthe IZSVe scenarioand A are separated elements interconnected via depen-dency relations (D contains information about M, A isresponsible for D and M etc.). The corresponding knowl-edge object KO is ontologically understood as a set ofcross-categorical constraints. If there is a need to includeknowledge objects within the ontology itself, one canapply the reification technique presented in, e.g, [30].Processes in the IZSVe scenarioThe categories previously discussed allow to talk about theorganizations activities, the flow of information in the dif-ferent phases and the participating roles. Ontology can beextended to model finer information.For instance, take the flowchart (ontologically this isa description) of the IZSVe laboratory analysis phase(Fig. 4), indicating the roles of the lab technician and thelab head. The figure takes the viewpoint of the sample;the graph lists the set of actions following the temporalorder and some causality constraint.This graphical representation is weak from the onto-logical perspective since it hides distinctions that can beimportant for, say, management tasks. Ontology analysistells us that a generic event is a bundle of more spe-cific events: the stable properties (these identify a statein DOLCE); the dynamic regularities (a process in theDOLCEs terminology); the evolving conditions that leadto complete the activity (an accomplishment in DOLCE);the key transition moments (an achievement in DOLCE).Thus, ontologically an activity like the data entry intoLIMS (see Fig. 4) comprises several types of events thatwe might want to distinguish, for instance: the continu-ous relationship between the hardware, the software, thework location, the data and the operator (state); the evolv-ing input/output interactions between the operator, thedatabase, the program and the hardware (process); thesteps that lead to the update of the information storedin the database (accomplishment); and the event of theinstant in which the new data is physically recorded in thedatabase (achievement).Roles and players in the IZSVe scenarioThe role category is characterized by dynamicity (one canplay a role just temporarily), anti-rigidity (playing or nota role does not change the entitys ontological status) andrelational dependency (a role depends on external defini-tions, its context) [33]. Other views exist: [48] separatesthe role hierarchy from the category hierarchy so that a(specific) student depends on a (specific) person but isitself not a person; [49] sees roles as realizable dependententities that fully exist only while they are played; and [42]takes the relationship role-context as primary.Notwithstanding the differences, to model the socialreality we need roles. Agent and actors are differentthings: an agent is an entity that can act, an actor isan agent whose acts acquire a social value. There arealso roles played by non-agentive entities: a quantity ofPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 10 of 15Fig. 4 Partial flowchart of the IZSVes scenario - Laboratory. Laboratory technician and head views onlybiological material can play the role of a specimen (Fig. 5).On the other hand, not all the distinctions are equally rel-evant. For instance, do we need to distinguish betweena specimen container (the artifact) and that very objectwhen used as a container in a laboratory (the role)? Onto-logically, these things are kept apart and, yet, in the mod-eling context the distinction is negligible. Thus, we suggestto have the two notions in the ontology but organize theinformation system in such a way that it uses only one(typically, the role category). Unfortunately, we lack moreprecise guidelines on this issue (see also Section Func-tional roles).Agent rolesWe separate roles that act for the company, in our scenariothese are called IZSVe internal roles, from the others, herecalled IZSVe external roles (non-IZSVe roles, for short),see Fig. 5.The internal roles are components of the companyand are played by its personnel: all these roles mustbe played by individual agents and are thus markedby I in Fig. 5. The scenario involves six internalroles, namely, Health Director, Head of Laboratory, Headof the Reception Service, Laboratory Technician, Front-desk Operator and Administrative Assistant. The rela-tionship of supervision holding among roles is quitestandard in todays social organizations and we donot discuss it further (see Supervision hierarchy inFig. 6).The non-IZSVe roles are mentioned in the companysprocedures but not structured within its organization,namely: Sample Owner, Submitter, Payer, Report Receiver,Deliverer and Requirer (Fig. 5). We find that another role,here calledQualified Technician, needs to be added to thelist although it is never introduced in the IZSVe scenario.Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 11 of 15RoleFunctionroleReagentroleSealroleReferencematerial roleSpecimenroleSampleOwnerSubmitter (I) PayerReportReceiverDeliverer RequirerTecnician (I)FrontdeskOperator (I)Head of the Reception Service (I)AdministrativeAssistant (I)LaboratoryTechnician (I)HealthDirector (I)Head of theLaboratory (I)AgentroleIZSVe Agent rolenon-IZSVe Agent role............Fig. 5 Some roles in the IZSVe scenario. Major roles of the scenario in Section Methods, I indicates individual rolesA Qualified Technician is a role that can be played onlyby individuals with a specific degree, e.g. a veterinary title,and registered in a dedicated public repository. This roleis needed to justify the social power of the Submitter role.Also, the Submitter and Qualified Technician are the onlynon-IZSVe roles that must be performed by an individ-ual agent. In all remaining cases the player can be anindividual agent or an organization. Four of these roles,namely Submitter, Report Receiver, Payer and Deliverer,are the interfaces between IZSVe and the external socialsystem: the Submitter makes the official analysis requestand thus triggers the IZSVe activity; the Report Receiver isthe receiver of the service report, the Payer is needed forthe economic sustainability of the service, and the Deliv-erer (which physically delivers the sample to the IZSVereception site) takes care of the physical interactions. Itis interesting to note that the Sample Owner role, theonly non-IZSVe role remaining, is not a figure that has adirect relationship with the IZSVe. This is also seen by thefact that its involvement is not motivated in the scenario.These specific cases are due to the institutional goals ofthe IZSVe: to collect data on the regional territory andto anticipate possible problematic situations. When thereis a suspect of potential food contamination or presenceFig. 6 Constraints on agent roles in the IZSVe scenario. The supervision relation among the internal roles is standardPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 12 of 15of a disease, knowledge of the Sample Owner allows fastreactions by the authorities.Differently from the internal roles, data on non-IZSVeroles might be missing from IZILAB. This lack of infor-mation is a factual issue, not a modeling problem. TheIZILAB system requires explicit knowledge only of someroles: Submitter, Report Receiver and Payer. For exam-ple, the Deliverer role is marked optional in the IZILABGUI. Since the analysis request form is delivered togetherwith the sample, the information about the Delivererwas considered marginal. This, however, shows that theIZILAB designer did not have an integrated view of thegoals/duties of the different roles. As of today, nothingprevents the (player of the role) Sample Owner to playthe Deliverer role as well. But the two roles may haveconflicting interests: the Submitter trusts the Deliverer tocorrectly manage the sample delivery so that it can be cor-rectly tested. Yet, the Sample Owner could be interestedin altering the sample (e.g. not following storing require-ments) to prevent the possibility to test it correctly. Ouranalysis suggested to better model the roles interrela-tions so to prevent these cases, possibly enriching existingguidelines.Our analysis shows that all IZSVe external roles aremutually independent with two exceptions. (1) The Sub-mitter role must also play the Qualified Technician role,as we have already discussed. (2) The Report Receiver rolemust be played by the player of either the Requirer or theSubmitter.Functional rolesThis is the other subcategory of roles we deal with inFig. 5. Here it is important to understand the artifactualand the contextual status of objects. A laboratory tool isan artifact manufactured to realize some functionality, aspecimen is a quantity of (natural or artificial) materialselected as representative of some substance or object.A general approach for artifactual entities and their rolesis presented in [42] where one can model a lab containerwhen used as such or when used as, say, a pencil case.Indeed, the lab container artifact may play a role for whichit was not produced. Unfortunately, this approach is basedon the notion of context which is hard to model [50].The study of functional roles in the IZSVe scenario leadsto many subtle distinctions, which may be sensitive to thegranular level of the description [51]. We model the arti-factual status of the entities via the notion of ontologicalartifact [37] and technical artifact [52]. (Alternative onto-logical views, e.g. [53], could be easily adopted.) Entitieslike equipment and laboratory material, are always playingthe intended role in this model since the IZSVe organiza-tion and its scenario are quite rigid on this. Note howeverthat where different levels of granularity are needed, oneshould not simplify the model in this way. For instance,we classify specimens, seals and reagents as roles (Fig. 5)but treat laboratory tools and containers as endurants, seeSection Categories and participants: a look at the IZSVescenario.Of course, a specimen is an artifact in the sense of [37]since it has been intentionally selected and it has the(intentionally attributed) capacity to provide informationabout the whole material from which it is extracted.However, in the IZSVe procedures specimens may havedifferent status. This happens in particular when thedependencies between the components of the corre-sponding knowledge object (section The notion of know-ledge object) are broken, e.g. when a wrong or incorrectprocedure is applied or the responsibility chain is broken.In these cases, the specimen looses its official or legalstatus. To make room for this change, we include both thespecimen as an artifact and the specimen as a role. Sim-ilarly, when one entity enters in the scenario as a seal, itdoes so to guarantee the integrity of the container. Oncethe seal is broken, the seal looses its role and thus changesits status, it is still a seal from the artifactual perspectivebut it is not sealing anymore. Similar arguments apply tothe modeling of reference material and of reagent (beforeand after their use). In contrast, entities like a registra-tion form, a test report and a laboratory tool (in the senseof non-consumable tool like a microscope) maintain theirstatus throughout the IZSVe activities independently onwhat happens. Note that this leaves out from the modelevents that destroy the objects functionalities (beyondmalfunctioning).Framework EvaluationThe evaluation of an ontology developed for applica-tion scenarios is still largely debated in the literature[32, 5456]. Among the different criteria listed in [32],our work aims to: (a) reach an agreement about meaningsof terms in a vocabulary, (b) provide a uniform view tofacilitate data integration across distributed sources, and(c) develop a formal model that allows automatic verifica-tion of its own consistency and accuracy. According to theanalysis in [32], the use of foundational ontology increasescoherence and interoperability; the provision of unam-biguous and formal documentation increases coherenceand clarity; the provision of machine-readable documen-tation allows for automated data processing, automatedknowledge- and data-integration, semantic integration;consistency verification helps to detect modeling errorsand increase data coherence. Since we focus on mod-eling methodologies and ontological analysis in existingcomplex scenario, we will concentrate on the conceptualanalysis, reusability and consistency criteria. The modelis in the design phase and has not been implemented.User feedback is limited and restricted to people thathave been involved in the scenario analysis or interviewedPozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 13 of 15to describe the IZSVe phases. Unfortunately, these dataare limited and not suitable for evaluation methods likestatistical analysis.Following todays practice [32], we listed five parame-ters to evaluate the result of the application of ontologicalanalysis, namely: generality, openess, flexibility, coherenceand consistency.1. Generality. As discussed in Section Devising theontology, we started from an existing ontology,DOLCE, which has been deployed in domains asdifferent as engineering, finance, fishery and medicalimage analysis. This previous experience indicatesthat DOLCE is comprehensive, conceptually soundand not focused on any particular perspective of thescenario. Also, other independent evaluations, e.g.[39], established that DOLCE is comprehensive andsuited for mesoscopic entities, that is, commonsenseentities cast by human reasoning and language, andthat are at the center of our social environment.In extending the ontology to cover the IZSVedomain we have followed the DOLCE principles andconstruction methodology. This approach ensurestwo things: no new restriction is added to the system,and the different areas (data management, laboratoryconduct, responsibility hierarchy, resourceadministration etc.) are or can be included in theontology. Since the new categories have an auxiliaryrole and do not form partitions, our extensioninherits the generality of DOLCE and preserves it.2. Openness. The proposed extension of DOLCEmodels domain notions like specimen, laboratorytool and method description. This is obtained byintroducing specialized categories and by populatingthem without introducing new partitions or crosscategorial constraints on DOLCE itself. It follows thatfurther categories can be added at each taxonomicallevel of our extension. For instance, a new categorycollecting the roles related to some other process(e.g., contract management) can be added to theIZSVe roles without having to revise the ontologicalsystem already developed. This design choice ensuresthat the system is open to revisions and furtherextensions.3. Flexibility. Flexibility is obtained by balancingontological assumptions and formal constraints. Ourextension adds a new set of domain-dependentproperties to characterize the new categories. Insome cases, e.g. for knowledge objects (Section Thenotion of knowledge object), we departed from theDOLCE perspective and applied a methodology tomodel entities not classified by the ontology. Ourapproach ensures that this new kind of information ismanaged in the ontology as a set of requirements ordependencies. This choice allowed us to mediatebetween the strict ontological sieve and the morepermissive attitude one has in applications. Finally,the constraints introduced via knowledge objects arelocal in the sense that they apply to only thoseentities that are connected via the notion ofknowledge object. As such, these constraints do notlimit the ontology itself.4. Coherence. A foundational ontology like DOLCEprovides a unifying view within which one canidentify and classify every entity in the domain ofstudy. This allowed us to develop a classification ofthe entities and roles in the scenario withoutcommitting to a specific perspective, and toreconstruct the perspective of, say, a lab technicianby looking at its role definition, its associated goalsand the activities it performs. The fact that theseviews are obtained by extracting information within asingle formal ontology ensures that these views arecoherent (they logically co-exist and do notcontradict each other) and aligned in the sense thatthey relate to the same set of integrated events.5. Consistency. As observed in Section Devising theontology, consistency is ensured at development timeby applying ontological analysis to identify theneeded categories and by modeling them in theformal language of DOLCE. At run time conceptualconsistency is preserved because of the clear criteriafor classifying the entities in the ontology.Technically, logical consistency is achieved by thecomputable versions of the ontology, like that in thecomputational language OWL [6]. There aresoftware environments for managing OWLontologies, such as Protègè [57]. OWL also supportsefficient reasoning so that ontology consistency atrun time can be ensured by state-of-the-artautomatic inference engines, such as Pellet (freelyavailable for most of the ontology tools [58] andRacer (highly customizable proprietary system) [59].Novelties, Impact and Limitations of the studyIn this study we showed how to use ontological analysisto develop an ontological model for public institutes inthe life sciences. Differently from the literature, we fol-lowed a principled top-down approach by starting from afoundational ontology and expanding it via an ontologicaldiscussions of the domain. Typically, the opposite is done:one starts from a domain model and aligns it to a foun-dational ontology. This standard strategy may improvethe interoperability of the existing models but does notincrease our understanding of the domain nor introducesmore flexibility. Instead, we obtained a model rich of newdistinctions and that can be used to reason from differentperspectives.Pozza et al. Journal of Biomedical Semantics  (2016) 7:51 Page 14 of 15Among the advantages of our ontological model, werecall that it has a rich role hierarchy useful to highlightconflicting goals and other dependencies, distinguishesphysical objects from their descriptions and their socialstatus, allows multiple views on single processes andmakes space for modeling hybrid elements as we showedwith knowledge objects.It is too early to talk about the implementation of thisontological model in the IZSVe information system. Thenumber of required changes is considerable also becauseontological models lead to important changes from thedata management viewpoint. This is a relevant limitationof our work since actual capacities and advantages can beestablished only when the system is practically exploited.At the moment, the gained deep understanding of thedomain is the most important result we can report about.Future workThe work in this paper concentrated mainly on the studyof objects and roles in the management of samples andrelated data from the reception to the release of the anal-ysis report. This part of the model needs to be betterconnected to the analysis of the processes which is stillongoing.In the future we need to evaluate which parts of thenew model are more complex to implement and disrup-tive with respect to the existing information system. Also,the changes suggested by the new model will likely triggernew requirements and services, which should be evalu-ated beforehand. Finally, we need to understand how tomodularize the model in order to reduce developmentconcerns and to optimize software implementation.ConclusionsIn this paper we presented elements of a wide-range analy-sis of processes, data and roles in a large public institute inthe life sciences. We showed how to perform an ontologi-cal analysis of the domain, what distinctions it highlights,and how to model them in an ontology. This principledapproach led us to define a series of notions that togethercover a large variety of data, procedures and objects; theseare indicative of the complexity of real-life organizationsand of the capacity of ontological models approaches tomodel them.The result of this work is an ontological framework,technically an extension of DOLCE, which is tuned to theIZSVe scenario.Finally, we have also exemplified the use of flexible con-ceptual techniques, via the notion of knowledge object,which help to reconstruct the organizations perspectivewithin the ontological viewpoint.AcknowledgementsThis paper contributes to the IZSVE RC 19/08 project and to the IZSVe RC16/09 project, funded by the Italian Ministero del Lavoro, della Salute e dellePolitiche sociali.FundingItalian Ministero del Lavoro, della Salute e delle Politiche sociali: Projects IZSVeRC 19/08 and IZSVe RC 16/09.Availability of data andmaterialsNot applicable.Authors contributionsTopic and scenario: GP and SM. Performed analysis: SB, GP and LC. Developedthe ontology: SB and AO. Validated the results: SB, GP, LC, SM. Wrote the paper:SB, GP, AO, SM, LC. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Author details1Istituto Zooprifilattico Sperimentale delle Venezie, Viale dellUniversitá, 10,35020 Legnaro (PD), Italy. 2Laboratory for Applied Ontology (LOA), ISTC CNR,Via alla Cascata 56/C - Povo, 38100 Trento, Italy. 3Bosch Research andTechnology Center, 2555 Smallman Street, 15222 Pittsburgh, Pennsylvania,USA.Received: 9 August 2016 Accepted: 24 August 2016Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 DOI 10.1186/s13326-016-0055-3SOFTWARE Open AccessWebulous and the Webulous GoogleAdd-On - a web service and application forontology building from templatesSimon Jupp* , Tony Burdett, Danielle Welter, Sirarat Sarntivijai, Helen Parkinson and James MaloneAbstractBackground: Authoring bio-ontologies is a task that has traditionally been undertaken by skilled experts trained inunderstanding complex languages such as the Web Ontology Language (OWL), in tools designed for such experts. Asrequests for new terms are made, the need for expert ontologists represents a bottleneck in the developmentprocess. Furthermore, the ability to rigorously enforce ontology design patterns in large, collaboratively developedontologies is difficult with existing ontology authoring software.Description: We present Webulous, an application suite for supporting ontology creation by design patterns.Webulous provides infrastructure to specify templates for populating ontology design patterns that get transformedinto OWL assertions in a target ontology. Webulous provides programmatic access to the template server and a clientapplication has been developed for Google Sheets that allows templates to be loaded, populated and resubmitted tothe Webulous server for processing.Conclusions: The development and delivery of ontologies to the community requires software support that goesbeyond the ontology editor. Building ontologies by design patterns and providing simple mechanisms for theaddition of new content helps reduce the overall cost and effort required to develop an ontology. The Webuloussystem provides support for this process and is used as part of the development of several ontologies at the EuropeanBioinformatics Institute.Keywords: OWL, Ontology, Spreadsheet, Webulous, Google AppIntroductionLike most data resources, ontologies are rarely com-plete, and healthy ontologies are continually growing andimproving, as the state of knowledge progresses [1, 2].Typically, authoring ontologies is a task performed bytrained experts, familiar with ontology development prac-tices and the complexities of languages such as the WebOntology Language (OWL). This presents a major bot-tleneck to the ontology development process as the timeand availability of trained experts is limited and ontol-ogy development is hard to fund [3]. Tools are now beingdeveloped to simplify the addition of content to ontologiesthat are based on populating ontology design patterns viadata entry templates.*Correspondence: jupp@ebi.ac.ukEuropean Bioinformatics Institute (EMBL-EBI),European Molecular BiologyLaboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge, UKOntology design patterns (ODPs) are commonly usedin ontology development in guiding the ontology devel-oper in the modeling of knowledge [4, 5]. They also helpin enforcing consistency and best practice in ontologydesign whilst reducing arbitrary class descriptions withinan ontology that can lead to both errors and ontologiesthat are difficult to maintain. Whilst ODPs can providea sound methodological framework, ontology expertiseis still required to establish and apply modeling pat-terns for real-world entities from a particular domain ofinterest [6].Several tools have been previously developed to sup-port building OWL ontologies from design pattern tem-plates [1, 7, 8]. The main aim of these tools is to providea simple interface for populating a design pattern that© 2016 Jupp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 InternationalLicense (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in anymedium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commonslicense, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 Page 2 of 8shields the users from the underlying OWL vocabulary.These systems help to enforce rigour and adherence toa design pattern and allow new content to be addedin bulk in a reproducible manner. Although these toolshelp in enforcing consistency of ontology development, inorder to truly mediate content contributions from non-ontology experts, tools that use a familiar paradigm todomain experts are required. Such tools should enablenon-ontologists to contribute whilst also tackling theissues of translating input into OWL ontologies.In this paper we describe the Webulous framework thatprovides software for the management of ontology designpatterns and ontology building templates. Webulous isbuilt around a client/server architecture, where the serverhosts a number of ontology building templates that canbe served to any number of client applications. Data sub-mitted to the server is translated into OWL assertionsaccording to design patterns expressed in the OntologyPre-Processing Language (OPPL) [9]. We have developeda client application for Webulous using the Google SheetsAdd-On framework that allows design pattern templatesto be loaded into Google Sheets and submitted back to aWebulous server for processing. The Webulous client isaimed at domain experts adding new content to ontolo-gies and is demonstrated as a term submission tool for theExperimental Factor Ontology [10].ResultsWebulous provides a public service for the creation andmanagement of ontology design templates. A Webulousserver can host a number of ontology building templatesthat use OPPL statements to translate input data intoOWL axioms. A Webulous template specifies a series offields for the input data, and fields can can be restrictedto only allow values from a list of ontology terms. TheWebulous API can be used by client side applications toautomatically build the user interface for a given tem-plate. Once a user populates a template with data this issubmitted back to a Webulous server where the patternsare instantiated to create new OWL statements ready forimport into the target ontology.Google Sheets Add-onProviding Webulous as a service means that a range ofclient-side applications can be developed for populating atemplate. We built a Google Sheets Add-On that supportsloadingWebulous templates from a server and submittingpopulated templates back to the server for processing. Wechose Google Sheets for their convenient document man-agement and sharing functionality and for the familiarityof the spreadsheet format for users.When a Webulous template is loaded via the GoogleAdd-On, each template input field represents a column inthe sheet. Columns can be restricted to a set of allowedontology terms by using term labels to create data vali-dation. This data validation provides the user with con-venient term autocomplete when entering data into a celland will alert the user when an invalid term has beenentered. Data submitted from Google Sheets is associatedwith the users Google account so the server can notifyboth the user and template admin via e-mail once thetemplate has been processed.TheWebulous Google Sheets Add-On (Fig. 1) has addi-tional functionality by allowing users to connect directlyto BioPortal services [11]. The Webulous Add-on pro-vides a side bar for searching BioPortal for ontology termsand creating custom ontology-based data validations. Thesidebar allows users to create a validation, which consistsof a restricted set of term labels, and provides a conve-nient way to create further validations using subclasses ofany particular term.Application of WebulousThe Experimental Factor Ontology contains descriptionsof experimental variables ranging from diseases, celltypes, cell lines, anatomy, assays, chemical compoundsand phenotypes. It is developed as an application ontol-ogy that integrates and bridges several external referenceontologies (such as ChEBI and the Gene Ontology). EFOenriches these existing ontologies by including additionalaxioms that connect terms like diseases to tissues andanatomical systems; cell lines to cell types, diseases andtissue; and link common and rare diseases through asso-ciated anatomical parts and phenotypes. EFO is used toannotate resources spanning multiple omics including;transcriptomics data in ArrayExpress [12] and ExpressionAtlas [13], genomics data in the NHGRI-EBI GWAS Cat-alog [14], proteomics data in PRIDE [15] and cell linedata in Encode [16]. EFO is also used by the Centre forTherapeutic Target Validation (CTTV)1 as their core dataannotation resource.One of the appealing features of EFO is that manyof the design patterns are well established and appliedconsistently across large portions of the ontology. Thisuse of design patterns makes EFO nicely amenable tothe generation of new content using templates. Priorto the work presented here most cell lines have beenadded to EFO for the ENCODE project using Excel-based spreadsheets that were processed with Populous[17]. As more resources adopt EFO there is an increas-ing pressure on the editors to add new content, muchof which remains in a spreadsheet-based format onsubmission.A dedicated Webulous instance is now running at theEBI to serve templates for adding new content to EFO2.This instance currently contains six EFO templates sum-marised in Table 1. Four of these templates are foradding new terms to EFO that include new cell lines,Jupp et al. Journal of Biomedical Semantics  (2016) 7:17 Page 3 of 8Fig. 1Webulous Google Add-on. Screenshot of the Webulous Google Sheets Add-on showing a ontology-based data validationdiseases, assays or measurement terms. There is a ded-icated template for adding synonyms to existing termsand a more general template for adding other types ofRESEARCH Open AccessDermO; an ontology for the description ofdermatologic diseaseHannah M. Fisher1, Robert Hoehndorf2, Bruno S. Bazelato3, Soheil S. Dadras4, Lloyd E. King Jr.5,Georgios V. Gkoutos3,6,7, John P. Sundberg8 and Paul N. Schofield1,8*AbstractBackground: There have been repeated initiatives to produce standard nosologies and terminologies forcutaneous disease, some dedicated to the domain and some part of bigger terminologies such as ICD-10. Recently,formally structured terminologies, ontologies, have been widely developed in many areas of biomedical research.Primarily, these address the aim of providing comprehensive working terminologies for domains of knowledge, butbecause of the knowledge contained in the relationships between terms they can also be used computationally formany purposes.Results: We have developed an ontology of cutaneous disease, constructed manually by domain experts. Withmore than 3000 terms, DermO represents the most comprehensive formal dermatological disease terminologyavailable. The disease entities are categorized in 20 upper level terms, which use a variety of features such asanatomical location, heritability, affected cell or tissue type, or etiology, as the features for classification, in line withprofessional practice and nosology in dermatology. Available in OBO flatfile and OWL 2 formats, it is integratedsemantically with other ontologies and terminologies describing diseases and phenotypes. We demonstrate theapplication of DermO to text mining the biomedical literature and in the creation of a network describing thephenotypic relationships between cutaneous diseases.Conclusions: DermO is an ontology with broad coverage of the domain of dermatologic disease and wedemonstrate here its utility for text mining and investigation of phenotypic relationships between dermatologicdisorders. We envision that in the future it may be applied to the creation and mining of electronic health records,clinical training and basic research, as it supports automated inference and reasoning, and for the broaderintegration of skin disease information with that from other domains.Keywords: Dermatology, Ontology, Disease, DermatopathologyAbbreviations: DermO, dermatology ontology; DO, human disease ontology; GWAS, genome-wide associationstudies; HPO, human phenotype ontology; ICD-9, 10, 11, international classification of disease versions 9, 10, and 11;MeSH, medical subject headings; MPO, mammalian phenotype ontology; NOS, not otherwise specified; OBO, openbiomedical ontologies; OMIM, online mendelian inheritance of man; OWL, web ontology language; PheWAS,phenome-wide association studies; PNS, Paul N. Schofield; UMLS, unified medical language system.* Correspondence: pns12@cam.ac.uk1Dept. of Physiology, Development and Neuroscience, University ofCambridge, Downing Street, Cambridge CB2 3EG, UK8The Jackson Laboratory, 600, Main Street, Bar Harbor Maine ME 04609-1500,USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Fisher et al. Journal of Biomedical Semantics  (2016) 7:38 DOI 10.1186/s13326-016-0085-xBackgroundEstimation of the impact of skin disease on populationmorbidity and mortality has always been complicated bythe nature of the terminologies used for recording dis-ease incidence, prevalence and cause of death. Howevermeasured, it remains a highly significant social, eco-nomic and clinical burden. In the USA, collective preva-lence estimates for skin disease are greater than those ofobesity, hypertension and cancer [1]. There have beenrepeated initiatives to generate structured terminologiesof sufficient granularity to accurately capture skin dis-ease diagnoses, but the available tools, such as ICD-10,remain blunt instruments in the face of the pressingneed for precision phenotyping, and are unsuitable formany types of computation-based research. We have,with the close involvement of domain experts in derma-tology, pathology, and genetics, consequently created anew ontology, DermO, for cutaneous disease.There have been several previous initiatives to deviselexicons or structured comprehensive terminologies forthe description of cutaneous disorders, for example theDermatologischer Diagnosenkatalog [2]. The most recentis the DermLex ontology [3, 4], created under the auspicesof the American Academy of Dermatology, with the pur-pose of providing a definitive nomenclature for clinicaldermatology [5]. This terminology was merged with theBritish Association of Dermatologists BAD Index and ismapped to International Classification of Disease (ICD9-CM) codes. However, maintenance of DermLex was dis-continued in 2009. This lexicon covered sporadic andinherited cutaneous disorders and consisted of 6104 termsincluding a nosology, classic signs, therapeutic procedures,and anatomical distributions. To date DermLex has beenthe most comprehensive tool for capturing dermatologicaldisease information. The Human Disease Ontology (DO)[6] also contains an integumentary branch of 234 terms,with skin and adnexal diseases classified as skin, hair, andnail disease. Likewise, the Human Phenotype Ontology(HPO) [7] contains a branch of skin and adnexal pheno-types with terms focused on phenotypic manifestations ofcutaneous disorders. Skin diseases are largely out of thescope of HPO, which primarily focuses on phenotypesand does not aim to cover the breadth of cutaneous condi-tions we envisage. Similarly, the DOs coverage andorganization of the integumentary branch is limited andmay not entirely capture the breadth and diversity of cuta-neous conditions required for the purposes of patientstratification, population analysis, cross-species compari-sons, database query expansion and automated reasoning.The current revision of ICD, ICD-11, will contain themost radical revision of dermatology terms since 1948and so far around 2000 terms have been assigned to thedermatology chapter (XII) with 20 major subdivisions[8]. ICD-11 is not currently finalized (June 2016) anduptake is expected to be gradual (for example the USAhas only recently transitioned to ICD-10 for electronichealth records as of 1.10.15) [9].The other main resource for dermatological diseaseterms is SNOMED-CT. Whilst SNOMED containsmore than 2000 terms related to cutaneous disease thecoverage and structure of the terminology does notlend itself to detailed clinical description and having arelatively flat hierarchy does not permit the relation-ships between diseases to be used computationally ina useful way. (Discussed in [4]). An additional issueremains the licensing of SNOMED-CT by the Inter-national Health Terminology Standards DevelopmentOrganization (IHTSDO) that precludes its free use insome countries.The main applications for DermO are data captureand informatic analyses that only recently have becomefeasible using electronic health records, and semanticintegration of disease information between species andacross domains of knowledge; these analyses require asufficiently rich structure, granularity, and coveragefrom the available terminologies. We have therefore cre-ated a new ontology from the scientific literature with theclose involvement of domain experts in dermatology,pathology, and genetics, while explicitly maintaining inter-operability with established ontologies and vocabularies inthe biomedical domain, DermO is organized in a mannerintuitive to dermatologists and dermatopathologists. Theframework adopted for the development of DermO wasbased on that of the definitive clinical dermatology text,Dermatology by Bolognia et al. [10]. The specific aim is toinclude all of the currently accepted primary and second-ary skin diseases, including those caused by systemic dis-orders, external insult, and the genodermatoses. DermOwas developed with the intention of creating a tool applic-able to patient care, clinical training and basic research, aswell as to support automated inference and reasoning. Itcan be used for patient stratification, genotype/phenotypestudies, and for the broader integration of skin dis-ease information with that from other domains, suchas model organism phenotypes and pharmacogenomicsfor translational science. DermO is freely available onhttps://github.com/dermatology-ontology/dermatology.MethodsOntology constructionDermO was constructed by domain experts using theframework of the most recent definitive text on Derma-tology edited by Bolognia et al. [10]. The approach takenwas to produce a classification familiar to dermatologists,as the envisaged uses of DermO include both patient diag-nostic annotations by clinicians and mining of electronichealth records. The formalization of patient informationprovides a data resource that can be expanded to integrateFisher et al. Journal of Biomedical Semantics  (2016) 7:38 Page 2 of 9historical and newly generated information from bothhuman and mouse dermatology, and genetic studies.The structure adopted for DermO is familiar both fordiagnostic support and patient data recording purposes.Because of the inclusion of systemic and inherited dis-eases, DermO also includes diseases that would normallybe found in other branches of a disease ontology, suchas Mendelian monogenic syndromes, and for completenesswe therefore use a degree of polyhierarchy and includegenetic diseases (the genodermatoses) as well as systemicdiseases. For the same reasons the ICD-11 topic advisorygroup adopted a similar approach [11].The ontology was manually constructed using OBO-Edit[12] and the OWL2 version prepared using Protégé [13].Consistency was verified using the HermiT reasoner [14],which detected no inconsistencies and no unsatisfiableclasses, mainly due to the absence of disjointness axiomsin the ontology. The main ontology is available in both theOBO Flatfile format [15] and the Web Ontology Language(OWL) [16]. DermO is housed in a Github repository andis made available via Bioportal (permanent URL: http://purl.bioontology.org/ontology/DERMO) [17], Aber-OWL(permanent URL: http://aber-owl.net/ontology/DERMO)and on the projects website https://github.com/dermatol-ogy-ontology/dermatology.Content, relations and mapping to other ontologiesDermO was constructed as a simple ontology with limitedpolyhierarchy, and currently contains 3,425 classes (Fig. 1,Table 1). These are currently mapped to concepts in othermajor terminologies and provided with synonyms (Table 2).Synonyms were sourced from Bolognia et al. [10],DermnetNZ [18] and from expert curator knowledge.Class labels and synonyms were initially lexically mappedto concepts in UMLS [19], ICD-10, MPO [20], HPO [7],DO [6], OMIM (Online Mendelian Inheritance in Man)[21], SNOMED-CT [22], Medical Subject Headings(MeSH) and DermLex [4]. Only exact mappings were in-Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 DOI 10.1186/s13326-016-0107-8RESEARCH Open AccessThe flora phenotype ontology (FLOPO):tool for integrating morphological traits andphenotypes of vascular plantsRobert Hoehndorf1,2* , Mona Alshahrani1,2, Georgios V. Gkoutos3,4,5, George Gosline6, Quentin Groom7,Thomas Hamann8, Jens Kattge10,11, Sylvia Mota de Oliveira8, Marco Schmidt9, Soraya Sierra8,Erik Smets8, Rutger A. Vos8 and Claus Weiland9AbstractBackground: The systematic analysis of a large number of comparable plant trait data can support investigationsinto phylogenetics and ecological adaptation, with broad applications in evolutionary biology, agriculture,conservation, and the functioning of ecosystems. Floras, i.e., books collecting the information on all known plantspecies found within a region, are a potentially rich source of such plant trait data. Floras describe plant traits with afocus on morphology and other traits relevant for species identification in addition to other characteristics of plantspecies, such as ecological affinities, distribution, economic value, health applications, traditional uses, and so on.However, a key limitation in systematically analyzing information in Floras is the lack of a standardized vocabulary forthe described traits as well as the difficulties in extracting structured information from free text.Results: We have developed the Flora Phenotype Ontology (FLOPO), an ontology for describing traits of plantspecies found in Floras. We used the Plant Ontology (PO) and the Phenotype And Trait Ontology (PATO) to extractentity-quality relationships from digitized taxon descriptions in Floras, and used a formal ontological approach basedon phenotype description patterns and automated reasoning to generate the FLOPO. The resulting ontology consistsof 25,407 classes and is based on the PO and PATO. The classified ontology closely follows the structure of PlantOntology in that the primary axis of classification is the observed plant anatomical structure, and more specific traitsare then classified based on parthood and subclass relations between anatomical structures as well as subclassrelations between phenotypic qualities.Conclusions: The FLOPO is primarily intended as a framework based on which plant traits can be integratedcomputationally across all species and higher taxa of flowering plants. Importantly, it is not intended to replaceestablished vocabularies or ontologies, but rather serve as an overarching framework based on which differentapplication- and domain-specific ontologies, thesauri and vocabularies of phenotypes observed in flowering plantscan be integrated.Keywords: Phenotype, Biodiversity, Flora, Botany, Morphological traits*Correspondence: robert.hoehndorf@kaust.edu.sa1Computational Bioscience Research Center (CBRC), King Abdullah Universityof Science and Technology, 4700 KAUST, 239556900 Thuwal, Kingdom ofSaudi Arabia2Computer, Electrical and Mathematical Sciences & Engineering Division(CEMSE), King Abdullah University of Science and Technology, 4700 KAUST,239556900 Thuwal, Kingdom of Saudi ArabiaFull list of author information is available at the end of the article© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 2 of 11BackgroundFor hundreds of years, information on plant species foundacross the world has been collected in Floras, taxonomicmonographs and annotations to collection material. Flo-ras are books collecting the information on all knownplant species found within a region. They describe planttraits with a focus onmorphology and other traits relevantfor species identification in addition to other characteris-tics of plant species, such as ecological affinities, distribu-tion, economic value, health applications, traditional uses,and so on. Floras not only allow identification of plantsfound within a region, but also provide a large knowledgebase of the phenotypic diversity found within ecosys-tems. The systematic analysis of such large-scale trait datacan support investigations into phylogenetics and ecolog-ical adaptation, with broad applications in evolutionarybiology, conservation, and the functioning of ecosystems.Moreover, the provision of trait data enables integratedknowledge discovery for agriculture (i.e. plant breeding)and phytomedicine. In particular many medicinal plantsare not as comprehensively characterized as food cropsor model plant systems. A comprehensive overview ofthe Floras available at the global level is given by [1]. Akey limitation in systematically analyzing information inFloras is the lack of a standardized vocabulary for thedescribed traits as well as the difficulties in extractingstructured information from free text.To facilitate integration and analysis of the informationcontained in Floras, we have developed the Flora Pheno-type Ontology (FLOPO), an ontology for describing traitsof plant species found in Floras. Ontologies provide for-mal, machine-readable definitions of the vocabulary usedwithin a knowledge domain [2, 3]. The FLOPO builds onexisting ontologies for morphological structures and phe-notypic qualities, in particular the Plant Ontology (PO)[4] and the Phenotype And Trait Ontology (PATO) [5].We have used these ontologies to extract entity-qualityrelationships from digitized taxon descriptions in Floras,and used a formal ontological approach based on pheno-type description patterns [6] and automated reasoning togenerate the FLOPO. Phenotype description patterns areformal statements in the Web Ontology Language (OWL)[7] that express the content of a phenotype description,i.e., the features of an organism when it has a particularphenotype.The FLOPO allows integration of qualitative trait datafrom different sources, including text-based descriptionsof phenotypes, such as those found in Floras and mono-graphs, image-based representations of plant traits such asthose found in photos and specimen scans (e.g., informa-tion stored in herbaria), as well as information about traitsand phenotypes in trait databases such as TRY [8] or theEncyclopedia of Lifes TraitBank [9]. Through its links toestablished ontologies, it can also be used to link this datato data sources from other domains, such as genomics,macroecology or systems biology.In our initial use case, our aims were to (1) identifythe traits associated with taxa in Floras, (2) represent thetraits in a semantic form amenable to computational anal-ysis, (3) link the traits to standard vocabularies of plantmorphology used in related areas of biological research,(4) and demonstrate that these traits can subsequentlybe integrated and compared with traits recorded in otherdatabases. The FLOPO is freely available under a CC-0license at http://purl.obolibrary.org/obo/flopo.owl.MethodsData sourcesBuilding upon a collaborative prototype developed at the2014 Biodiversity Data Enrichment Hackathon [10], anevent similar to the popular BioHackathon series [11],we used several Floras (Flora Malesiana [12], Flore duGabon [13, 14], Flore dAfrique Centrale [15], Flore duCongo Belge et du Ruanda-Urundi [16], and a collectionof Kews African Floras available at http://www.kew.org/science-conservation/research-data/science-directory/projects/e-floras, including the Flora Zambesiaca, Floraof Tropical East Africa, Flora of West Tropical Africa,Flora of Tropical Africa, Flora Capensis and the UsefulPlants of West Tropical Africa). The Floras were availablein digitized form, with most Floras written in English, andthree in French (Flore dAfrique Centrale, Flore du CongoBelge et du Ruanda-Urundi, and Flore du Gabon).We assembled a vocabulary of plant morphological enti-ties, attributes and attribute values. The terms for thisvocabulary were taken from ontologies that are widelyused in biological research: PO [17] for plant morpho-logical entities, and PATO [5] for attributes and attributevalues. Each ontology provides one or more English termsassociated with one kind of plant entity (i.e., the labelsand synonyms of classes in the ontologies). To identifythe French terms associated with these entities, we used adictionary provided by the Missouri Botanical Garden athttp://www.mobot.org/mobot/glossary/ that was used bythe project partners in the context of the FlorML project[18]. As result of this step, we obtained two dictionariescomprised of French and English terms for plant morpho-logical entities, and attributes and attribute values.Text processingFloras are available in different formats, including thestructured XML-based format FlorML [18] as well asfree text in taxonomic databases. In each Flora, we iden-tified taxon names and identifiers together with com-plete (textual) taxon descriptions. We then processedthe text using natural language processing (NLP) toolsprovided by the Apache Lucene [19] standard analyzer(basic stemming, stopword removal), applied a sentenceHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 3 of 11identification method to tokenize the text into sentences(using the OpenNLP toolkit https://opennlp.apache.org/)and stored the resulting sentences together with theirtaxon names and identifiers in a fulltext index using theApache Lucene framework.We then applied the same stemming and stopwordremoval steps on the labels and synonyms of the ontol-ogy classes, and used Lucene to query the full text indexfor taxa descriptions in which sentences contain both alabel or synonym of a quality (from PATO) and a label orsynonym from a morphological entity (from PO). Whenquerying French Floras, we first performed a dictionary-based translation of the labels, then applied the same pre-processing as applied to the textual taxa descriptions andperformed the same query. Finally, we used the Stanfordparser [20] to identify whether the quality term stands inan attributive relationship to the entity term.As a result, we identified Entity-Quality pairs [5] inwhich entity-terms refer to plant morphological entities(from the Plant Ontology), and quality-terms to attributesor attribute values (from PATO). For example, from a sen-tence The flowers are red we identify the entity-qualitypair (Flower, Red), where Flower is taken from the PlantOntology (PO:0009046), and Red is taken from PATO(PATO:0000322). More complex relationships, such asconnectivity between two morphological structures, areexpressed as ternary relations in PATO (requiring thetwo connected entities and an additional instance of arelational quality as arguments), and we ignore them inour analysis; instead, we introduced placeholders whichstate that each structure is related to something withoutproviding information on the second entity.To filter the results, we used lexical parsing to determinewhether the sentence expresses an attributive relation-ship between the quality and the entity we identified. Forexample, in the sentence The flowers are red with yel-low stamens., an attributive relationship exists betweenFlower and Red as well as Yellow and Stamen.As a result of this text processing pipeline, we obtained aset of 502,693 PATO-based entity-quality descriptions oftraits found in the Floras we analyzed. The entity-qualitybased descriptions consist of 20,584 distinct combinationsof morphological structures from PO and qualities fromPATO, using 287 distinct plant morphological structuresand 545 distinct qualities, and are associated with 26,104taxa.Ontology generation and automated reasoningTo generate the FLOPO, we use the extracted informa-tion in phenotype definition patterns [6], i.e., OWL axiompatterns for defining classes of phenotypes. We mainlygenerate three types of classes which we fully define inOWL: first, we create grouping classes representing thephenotypes of a plant structure or any of its parts (e.g.,flower phenotype); second, we create classes for traits (orcharacters) of plant structures (e.g., flower color); finally,we create classes for the values of traits (or characterstates) of plant structures (e.g., flower red).Using OWL, we generate the following classes andaxioms for each entity-quality pair (E,Q): E phenotype EquivalentTo: has-partsome ((part-of some E) andhas-quality some quality) E Q EquivalentTo: has-part some(E and has-quality some Q) If Q is in the values subset of PATO, we identifythe most specific superclass T of Q that is in PATOsattribute subset, and generate the axiom E TEquivalentTo: has-part some (E andhas-quality some T).For example, for the entity quality pair (flower, red), wegenerate flower phenotype EquivalentTo:has-part some ((part-of some flower)and has-quality some quality) flower red EquivalentTo: has-partsome (flower and has-quality somered) flower color EquivalentTo:has-part some (flower andhas-quality some color)The intuition behind our axiom patterns is that theyalways define a phenotype with respect to what must betrue for a whole organism if the phenotype is present. Forthis purpose, we prefix every axiom with a has-partsome restriction.The use of this prefix pattern allows combining sim-ple phenotypes (expressed through a single entity-qualitypair) into complex phenotypes (requiring combinationsof entity-quality pairs) through a simple intersection; forexample, to describe the complex phenotype of havingboth red flowers and yellow stamens, the flower red andstamen yellow phenotypes would be intersected to formthe complex phenotype of a whole organism having twoparts, flowers that are red and anthers that are yellow.Without such a prefix, phenotypes could not easily becombined in such a way since flower and anthers aredisjoint morphological entities, and red and yellow aredisjoint qualities.The parthood relation is used in another pattern togroup traits by plant morphological structure as well asall parts of that morphological structure. In particular,the part-of relation (the inverse of the has-partrelation) is both reflexive and transitive, and thereforesubclasses of part-of some X include X as well as allHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 4 of 11classes with instances that necessarily are a part of someX. For example, subclasses of part-of some flowerinclude, among others, flower, petal, and androecium, andusing the parthood relation in the definition of the phe-notype classes will lead to petal phenotype, androeciumphenotype, etc., to become subclasses of flower phenotype.We do not use this pattern on the level of traits such asflower color as the traits of the parts will be different fromthe trait of the flower (e.g., the flower may be red while itsstamens are yellow).To distinguish between traits and their values, we usethe distinction between attributes and attribute values inPATO in which classes are tagged through an annotationproperty either as attribute or value.When using theOBOFlatfile Format [21], these distinctions are expressed as theattribute slim and value slim of PATO.Following the generation of the axioms for FLOPObased on the axiom patterns, we added one further axiomto the resulting ontology to remove impossible combina-tions of entity and quality, in particular those in which amorphological structure in PO is asserted to have a qualitythat can only be the quality of processes:has-part some (owl:Thing and has-qualitysome process quality) SubClassOf:owl:NothingThe ontology was generated using a Groovy script basedon the OWL API [22] and the Elk reasoner [23]. Sourcecode for processing Floras and generating the ontologyis freely available (under a BSD-style license) at https://github.com/flora-phenotype-ontology/flopoontology.ResultsData-driven generation of the Flora Phenotype OntologyFor creating the Flora Phenotype Ontology we used as pri-mary use case the traits and phenotypes described in theFloras listed in the Methods section. Figure 1 provides anoverview of our workflow. Using the Entity-Quality pairsextracted from the Floras, we developed a data-drivenapproach to generate a prototype of an ontology thatwould likely be capable of characterizing a large numberof the traits observed in our study. In each Entity-Qualitypair, the entity term directly maps to a morphologi-cal entity (in the Plant Ontology), and the quality termmaps to an attribute or value (in the PATO ontology).We aimed to exploit the background knowledge in theseontologies together with an automated reasoner to gener-ate an ontology in which each class characterizes a traitand is associated with at least one taxon in one of theFloras we processed. Specifically, we aimed to exploit theinformation about parthood relations between morpho-logical structures and biological processes, and the sub-class relations between qualities, morphological parts andphysiological processes to generate the ontology [6]. Weused a pattern-based approach in which we create axiompatterns that combine information obtained through ourNLP-based approach with information in the referencedontologies. Through the axiom patterns, we achieve: structural organization based on anatomicalparthood (e.g., a petal phenotype should become asubclass of the flower phenotype based on petalbeing a part of flower), separation of types of trait for each morphologicalstructure (e.g., a flower color should be separate fromthe flower shape), but both should be more closelyrelated to each other than to root color as both areflower traits, separation and structural organization of attributesand values (e..g, flower red should become a subclassof flower color), and semantic interoperability with existing ontologies inthe plant domain, including the Plant Ontology andTrait Ontology.Flora Phenotype OntologyThe Flora Phenotype Ontology (FLOPO), available athttps://purl.obolibrary.org/obo/flopo.owl, is the result ofclassifying the axioms generated from our text-miningpipeline together with the PATO and PO ontologies. Clas-sification of an ontology is a reasoning task in whichthe axioms within the ontology are used to determinethe most specific sub- and super-class for each classin the ontology. As all generated axioms are in theOWL EL profile [24], we used the Elk reasoner [23]to perform the classification. The resulting ontology,the FLOPO, consists of 25,407 classes (24,076 classesunique to the FLOPO, in addition to the classes inPO and PATO). Each class is assigned a unique IRIin the namespace http://purl.obolibrary.org/obo/FLOPO_followed by a unique numerical identifier. For example,the class flower red has the identifier FLOPO:0007599when using FLOPO: to refer to the FLOPO namespace,i.e., FLOPO:0007599 will refer to the IRI http://purl.obolibrary.org/obo/FLOPO_0007599.The classified ontology closely follows the structure ofPO in that the primary axis of the classification shows theobserved plant anatomical structure, while more specifictraits are classified based on parthood and subclass rela-tions between anatomical structures as well as subclassrelations within PATO. Figure 2 shows the upper level ofthe FLOPO.As most classes in the FLOPO are fully defined usingaxioms in OWL, it can be queried using either the labelsof a class, the identifier of a class, or semantically usingthe axioms that are used to define the class. The lat-ter kind of query is particularly useful when queryingHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 5 of 11Fig. 1 Overview over the main workflow in generating the FLOPO. After generation of the FLOPO, it can then be used to access plant trait andphenotype data in different databasesfor classes that are not currently contained within theFLOPO. For example, the FLOPO does not currentlycontain a class for the flower being deep pink. Neverthe-less, a semantic query using the entity-quality pair flowerand deep pink and the axiom patterns we would usewithin the FLOPO (has-part some (flower andhas-quality some deep pink), it is possible toquery for the equivalent or direct superclasses of thatdescription which will return flower pink as the clos-est matching class.Following the automatic generation of the FLOPO, wehave also begun to manually add classes to FLOPO basedon user requests and our own use cases. While we aimto fully define all classes in FLOPO, some classes can-not be defined without also extending other ontologiessuch as PO or PATO. FLOPO currently contains 198man-ually created classes of which more than 50 % are fullydefined while the others are restricted by subclass axiomsalone.Evaluation: coverage of traits in Floras and plant databasesTo test the coverage of traits in FLOPO, we manuallyannotated taxon descriptions from Floras and evaluatedthe correctness and coverage of traits in FLOPO. Correct-ness refers to the creation of nonsensical classes generatedby the automated analysis, while coverage (i.e., recall)refers to the number of characters in plant descriptionsthat have a corresponding FLOPO class.We have not performed a quantitative assessment ofhow many of the classes in FLOPO do not make sensebut did a qualitative analysis instead. Classes such asxylem vessel member tomentose, peduncle female, andlower glume subacute are obviously artifacts of the auto-mated generation and constitute a significant number ofclasses in FLOPO. We distinguish two main sources ofthese artifacts: The parsing of the descriptions failed to correctlyassociate entities with attributes. Parsing ofFig. 2 An overview of the top-level structure of the FLOPOHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 6 of 11descriptions is difficult, and entities and qualities inthe same sentences may incorrectly be identified asentity-quality pairs. Some labels of qualities in PATO can be used inanother context to refer to completely differentqualities. For example, acute in PATO is a quality ofprocesses, used to characterize for example diseasessuch as acute malaria, while the term acute in aplant description usually refers to an angle. Thesequalities are then propagated up the hierarchy (due toinheritance in PATO) and yield further non-sensicalclasses (such as leaf intensity).In response to our evaluation, we have manually depre-cated several classes and added an axiom to prevent theuse of any process qualities in FLOPO. Currently, 564classes in FLOPO have been deprecated for this reason.While nonsensical classes clutter the ontology with use-less classes, they do not prevent the use of the remainingclasses in standardizing the description of traits.To evaluate coverage of FLOPO, we have performed arigorous application of the ontology to eight plant descrip-tions from several Floras and within a number of taxo-nomic groups. The detailed results can be found in theAdditional files 1 and 2. We identified between 40 and 85characters for each taxon, and the coverage of charactersin FLOPO ranged from 48 to 70 %. Simple characters suchas stem diameter are well represented in FLOPO. Morecomplex characters, however, are often lacking, althoughsome complex characters such as petiole margin undulate(often a useful character for identification) are present.The largest number of missing classes in FLOPO are dueto qualities missing in PATO. Examples of these includecaulescent, chartaceous, and axillary (full list provided asAdditional file 3). While truncate is present in PATO, wedid not match truncated in our text processing method.Furthermore, some PO classes were also missed due tomissing labels or synonyms and our use of exact matchingin text processing. For example, ovule was not matchedbecause it corresponds to the class plant ovule in POwhich has no synonym ovule. Any plant organ class miss-ing in PO leads to an absence of FLOPO classes for thatorgan. Additionally, some combinations of PO and PATOare not identified, sometimes due to the lack of compar-ative classes (or synonyms) in PATO such as unequal orlonger than.To further evaluate the coverage of FLOPO, we haveused independent trait data from African Plants  a photoguide [25] database, an expert-based tool using trait datafor identification purposes. The trait life form and quan-titative traits such as the number of petals, that do notfit with the entity-quality terms in FLOPO, have beenexcluded beforehand. Out of 80,887 taxon-trait combina-tions, 44,200 (55 %) could be matched to FLOPO classes.Out of 88 traits that were used in the African Plantsdatabase, 31 were already present in FLOPO and 57 weremanually created in FLOPO following this evaluation.The link to genetics: integrating wild-type andmodelorganism phenotypesPhenotypes are not only collected in a natural context,but also in the context of model organisms [26]. In manycases, model organism databases collect abnormal phe-notypes [26]. These differ from phenotypes observed ina biodiversity context in the fact that they represent dif-ferences to a control group. For example, while a flowerred phenotype in a biodiversity context states that themembers of a particular species, or an individual sam-ple of that species, have red flowers, it may indicate in amodel organism context that, based on some experimen-tal conditions such as a gene knockout or environmentalalteration, the flowers of the organism are red under theexperimental conditions while the control group has dif-ferently colored flowers. These experiments can provideuseful information on functional genetics by revealingthe phenotypic effects associated with particular genesor revealing the mechanisms underlying environmentaladaptation [26, 27].While the FLOPO is primarily focused on describingthe traits and phenotypes in wild-type plants, its classescan also be used to characterize divergent phenotypesas, for example, observed in functional genetics experi-ments. To test this assumption we used a dataset of formalphenotype descriptions recorded in mutant models ofArabidopsis thaliana, maize, barrel medic, rice, soybean,and tomato [28]. Out of 5,186 phenotype statements con-tained in the dataset that involve a plant anatomical entity,315 directly match one of the classes in the FLOPO, whilethe others have superclasses in the FLOPO. The low num-ber of directly matching classes may be a consequence ofthe different way in which the phenotypes are recorded;in a model organism context, phenotype descriptionsinclude statements such as whole plant increased sizeor seed inviable, which are not recorded, or meaningful,without an explicit group to which phenotypes are com-pared. Nevertheless, these results show that FLOPO canbe used to combine plant phenotype data from differentdatabases and domains.DiscussionInteroperability with plant trait vocabulariesThe FLOPO is primarily intended as a framework basedon which plant traits can be integrated computationallyacross all species and higher taxa of flowering plants.Importantly, while FLOPO can be used for annotationdirectly, it is not intended to replace established vocab-ularies or ontologies, but rather serve as an overarchingframework based on which different application- andHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 7 of 11domain-specific ontologies, thesauri and vocabularies ofphenotypes observed in flowering plants can be inte-grated. Using the axiom patterns we defined and usedto generate the FLOPO, any ontology-based pheno-type description using the entity/quality method canbe directly integrated with the FLOPO, and appropri-ate equivalent classes, sub- and super-classes can beidentified using automated reasoning (either using anautomated reasoner directly or querying through pub-lic repositories such as AberOWL [29] which providereasoning services for ontologies, including the FLOPO).Additional terminological resources, such as the PlantTrait Thesaurus [30], the Crop Ontology [31], the PlantTrait Ontology [32], as well as general and application-specific plant-related thesauri, can be integrated andsemantically enriched through mappings to the FLOPO.These mappings can either be established manually bydomain experts or, in some cases, automatically throughmapping of labels.Multi-modal data sourcesWe have primarily used a large corpus of plant taxa inFloras as a source for the FLOPO. However, an increas-ing number of automated methods is being developedto detect traits, phenotypes and species from multi-modal information sources including photographs [33],herbarium sheets [34, 35], microscopy images [36], orschematic drawings. The FLOPO can also be utilized tointegrate data obtained from different sources and anal-ysis approaches. To achieve this goal, analysis methodsthat detect morphological traits and phenotypes in plantswould either output FLOPO classes directly, or the outputof these methods would be mapped to FLOPO classes.As different data sources and analytic approaches havedifferent error rates and levels of confidence, data sourcesthat integrate multi-modal information should providedifferent kinds of evidence and additional information,at least the data source (e.g., the collection of which itis a part), the type of data (e.g., whether it is textualdata, or photographs), the protocol that was applied toobtain the data, the data extraction method (e.g., imageanalysis, text mining), and the environmental conditionsunder which the phenotype has been observed. Differ-ent ontologies and checklists have been developed tocapture these aspects of scientific data collection. Forexample, the Provenance Ontology (PROV-O) [37] canbe used to specify the data source and authoring infor-mation. The Biological Collections Ontology (BCO) [38]can be used to specify the plant specimens mentioned inthe species treatments and thereby link to geography andspecies concepts. The Plant Experimental Assay Ontology(PEAO) (https://bitbucket.org/PlantExpAssay/ontology/raw/v0.1/PlantExperimentalAssayOntology.owl) can beused to specify the assays that were used to process boththe original plants of which phenotypes were recordedand the protocols used to collect the data. The EDAMontology [39] can be used to specify how the data wasextracted, e.g., whether FLOPO classes were assignedmanually or automatically, and if the latter, whichmethodswere used to extract the information. A crucial compo-nent in any description of observed phenotypes is thecombination of environmental conditions under whichthe phenotypes have been observed, and several ontolo-gies have been established for this purpose. In particular,the Environment Ontology (EnvO) [40] covers environ-ments in which organisms are found and can also providerelevant classes applicable to plant biodiversity. We havealso attempted to annotate the Floras in our study withclasses from EnvO. However, in contrast to plant mor-phology and phenotypes, in which we can filter lexicalmatches by the syntactic relations between the term refer-ring to a morphological entity and the term referringto a quality, we find that environmental conditions aremore difficult to identify precisely using purely lexicalapproaches. Especially in Floras, environmental descrip-tions may be context-specific and require prior knowl-edge of the area. Future research will include develop-ing and applying dedicated environmental named entityrecognition approaches [41], as well as using additionalplant-specific ontologies such as the Plant EnvironmentOntology [4] to precisely identify and characterize envi-ronmental conditions.Automatic generation of phenotype ontologies andcomparisonThe initial draft of FLOPO was generated from literatureusing a pattern-based approach in order to maintain abalance between trait descriptions that are actually usedto characterize plants and the totality of all descriptionsthat are possible when using the PO and PATO ontolo-gies. The axiom patterns we use in FLOPO are motivatedprimarily by the aim to generate an ontology in whichthe basic underlying taxonomy follows the distinctionsmade in classifyingmorphological structures in plants andare comprehensible to domain experts using the ontol-ogy. However, the axioms we use to define traits andphenotypes are distinctly different from the axioms usedin other phenotype ontologies [42], including the widelyused Mammalian Phenotype Ontology [43] and HumanPhenotype Ontology [44]. The classes we generate are alsonot explicitly declared to be subclasses of quality (fromPATO), as in some other ontologies and applications[42, 45]; while we do not perform an explicit analysisregarding the ontological state of our classes, the intentionis that our axioms provide a description of a whole organ-ism and what must be true of it when having a particularphenotype. These can either be considered as qualitiesof a whole organism (and therefore a subclass of PATOsHoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 8 of 11quality class), or equivalently as subclasses of the wholeorganism (a material entity) [6].The pattern-based approach we use is inspired by recentsuggestions to go beyond the quality-centric approach ofdefining phenotypes, and instead explicitly characterizethe configurations of the whole organism that has the phe-notype, including the parts the organism has or lacks,the processes it participates in or not, the functions ishas, and the qualities is has or lacks [6, 46, 47]. Theseapproaches have the advantage of explicitly being able toutilize knowledge from anatomy or physiology ontologies[6, 46], and have successfully been applied to integratea large number of phenotype ontologies [28, 48]. How-ever, a difference in axiom patterns to other phenotypeontologies may increase the effort required in integrat-ing these ontologies with FLOPO. Should it be requiredto treat the classes in FLOPO as subclasses of quality,all our axiom patterns can further be prefixed withinheres-in some in order to make them subclassesof quality. These changes can be applied automaticallywithout changing any of the inferences we describe [6], orincreasing the expressiveness of the language required toexpress the axioms (i.e., OWL 2 EL).Continuing development of the FLOPO and its annotationsWe used largely an automated and data-driven processto generate the FLOPO. As a consequence, the gener-ated FLOPO contains several artifacts that are a con-sequence of the text matching process. In particular, itcontains traits that are not relevant or measured, suchas bark surface area, and may lack traits that are dif-ficult to identify through a lexical approach. Therefore,after our largely automatic approach, we have alreadystarted to manually improve both correctness and cov-erage of FLOPO, and we aim to continue the devel-opment of the FLOPO further with involvement ofdomain experts. For this purpose, we provide an issuetracker (at https://github.com/flora-phenotype-ontology/flopoontology/issues) in which FLOPO users can requestchanges, ask for new classes to be added, and activelycontribute to the further development of the FLOPO.One instance of a further manual evaluation of theFLOPO by domain experts is an ongoing study at Natu-ralis (involving TH, SMO and RV) to extract homologizedtraits and their values, i.e., characters and characterstates in the context of evolutionary comparative anal-ysis, for the economically valuable tropical plant familyPiperaceae. In this study, automatically extracted entitiesand respective qualities are scrutinized by botanists, andtheir fidelity to the entity-quality context in the sourceevaluated. This longer-term study will help to furtherimprove FLOPO.We also aim to develop semantic annotations of taxawith the FLOPO. Currently, we are using a custom textprocessing pipeline to extract entity-quality pairs withthe primary aim of building a comprehensive ontology.However, there is an extensive body of research on ana-lyzing traits and phenotype found in text; in particu-lar the CharaParser [49, 50] has achieved high accuracyin extracting formalized character statements from Flo-ras, and we intend to evaluate its use in the future.We plan to apply similar methods and make FLOPO-based annotations of taxa available using Semantic Webtechnologies, and link the taxa to their correspond-ing International Plant Names Index (IPNI) [51] iden-tifiers to enable interoperability with databases of planttraits and phenotypes. IPNI provides a service for URNs(LSID), which we are currently evaluating among otherservices like Identifiers.org (URL based) [52] to pub-lish the taxon annotations as Linked Open Data (seeFig. 1).ConclusionsWe have developed the Flora Phenotype Ontology(FLOPO), an ontology of plant traits and phenotypesfound in Floras and monographs. The FLOPO is an ongo-ing, community-driven project, and is intended both fordata annotation and as a framework based on whichplant traits can be integrated computationally across allspecies and higher taxa of flowering plants.The FLOPOis being used for annotation of traits, in particularwithin the African Plants Database [25], and in ongoingprojects for the annotation and integration of plant traitdata.Additional filesAdditional file 1: Annotation of Floras 1. The file contains the manualannotation of Salacia erecta, Cucumeropsis mannii, Oxalis, and Basella alba.The file includes identified phenotypes and the mapping to FLOPO. It alsohighlights missing classes in PATO or PO. (XLSX 24 kb)Additional file 2: Annotation of Floras 2. The file contains the manualannotation of Salacia erecta, Andropogon chinensis, Oxalis, and Anisopappuschinensis. The file includes identified phenotypes and the mapping toFLOPO. It also highlights missing classes in PATO or PO. (XLSX 36 kb)Additional file 3: Missing PATO classes. A list of missing classes in thePATO ontology identified by our study. (TXT 4 kb)Additional file 4: Flora descriptions. The original descriptions of Salaciaerecta, Andropogon chinensis, Oxalis, and Anisopappus chinensis based onwhich the manual annotation was performed. (PDF 24 kb)AbbreviationsBCO: Biological collections ontology; EnvO: Environment ontology; FLOPO:Flora phenotype ontology; IPNI: International plant names index; LSID: Lifescience identifier; NLP: Natural language processing; OBO: Open biologicaland biomedical ontologies; OWL: Web ontology language; PATO: Phenotypeand trait ontology; PEAO: Plant experimental assay ontology; PO: Plantontology; PROV-O: provenance ontology; URN: Uniform resource nameAcknowledgementsThe initial draft of the Flora Phenotype Ontology was created at the 2014Biodiversity Data Enrichment Hackathon (Leiden, the Netherlands).Hoehndorf et al. Journal of Biomedical Semantics  (2016) 7:65 Page 9 of 11FundingThe initial draft of the Flora Phenotype Ontology was created at the 2014Biodiversity Data Enrichment Hackathon (Leiden, the Netherlands), which wassponsored by the the pro-iBiosphere project (Grant Agreement number312848), funded by the European Commission under the 7th FrameworkProgramme. Funding for GVG was provided by the National ScienceFoundation (Grant Number: IOS-1340112), the BBSRC national capability inplant phenotyping (Grant Number: BB/J004464/1) and the FP7 European PlantPhenotyping Network (Grant Agreement No. 284443). Funding for MS and CWwas provided by the Deutsche Forschungsgemeinschaft (DFG) under grantno. HI 1538/2-2 (GFBio). RH and MA were supported by funding from the KingAbdullah University of Science and Technology.Availability of data andmaterialsAll data and materials are available from https://github.com/flora-phenotype-ontology/flopoontology. The FLOPO is available from http://purl.obolibrary.org/obo/flopo.owl. Evaluation results are available as Additional file 1 andAdditional file 2.Authors contributionGG, QG, TH, SS, RV, CW, RH implemented the first version of the FLOPO at theLeiden Hackathon; MA, RH, MS, CW edited, critically revised and updated theFLOPO; GG, QG performed the manual evaluation; GVG, GG, QG, TH, JK, SMO,MS, SS, RV, ES evaluated applications of FLOPO to biological data and helpedto revise FLOPO; all authors contributed to writing the manuscript. All authorsread and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethical approval and consent to participateNot applicable.Author details1Computational Bioscience Research Center (CBRC), King Abdullah Universityof Science and Technology, 4700 KAUST, 239556900 Thuwal, Kingdom ofSaudi Arabia. 2Computer, Electrical and Mathematical Sciences & EngineeringDivision (CEMSE), King Abdullah University of Science and Technology, 4700KAUST, 239556900 Thuwal, Kingdom of Saudi Arabia. 3College of Medicaland Dental Sciences, Institute of Cancer and Genomic Sciences, Centre forComputational Biology, University of Birmingham, B15 2TT Birmingham,United Kingdom. 4Institute of Translational Medicine, University HospitalsBirmingham, NHS Foundation Trust, B15 2TT Birmingham, United Kingdom.5Institute of Biological, Environmental and Rural Sciences, AberystwythUniversity, SY23 2AX Aberystwyth, United Kingdom. 6Royal Botanical Gardens,Kew, Richmond, TW9 3AB Surrey, United Kingdom. 7Botanic Garden Meise,Nieuwelaan 38, 1860 Meise, Belgium. 8Naturalis Biodiversity Center, P.O. Box9517, 2300 RA Leiden, The Netherlands. 9Senckenberg Biodiversity andClimate Research Centre (BiK-F), Senckenberganlage 25, 60325 Frankfurt amMain, Germany. 10Max Planck Institute for Biogeochemistry, Hans Knoell Str.10, 07745 Jena, Germany. 11German Centre for Integrative BiodiversityResearch (iDiv) Halle-Jena-Leipzig, Deutscher Platz 5e, 04103 Leipzig, Germany.Received: 2 December 2015 Accepted: 1 November 2016RESEARCH Open AccessExtracting drug-enzyme relation fromliterature as evidence for drug druginteractionYaoyun Zhang1, Heng-Yi Wu2, Jingcheng Du1, Jun Xu1, Jingqi Wang1, Cui Tao1, Lang Li2 and Hua Xu1*AbstractBackground: Information about drugdrug interactions (DDIs) is crucial for computational applications such aspharmacovigilance and drug repurposing. However, existing sources of DDIs have the problems of low coverage,low accuracy and low agreement. One common type of DDIs is related to the mechanism of drug metabolism: aDDI relation may be caused by different interactions (e.g., substrate, inhibit) between drugs and enzymes in thedrug metabolism process. Thus, information from drug enzyme interactions (DEIs) serves as important supportiveevidence for DDIs. Further, potential DDIs present implicitly could be detected by inference and reasoning basedon DEIs.Methods: In this article, we propose a hybrid approach to combining machine learning algorithm with triggerwords and syntactic patterns, for DEI relation extraction from biomedical literature. The extracted DEI relationsare used for reasoning to infer potential DDI relations, based on a defined drug-enzyme ontology incorporatingbiological knowledge.Results: Evaluation results demonstrate that the performance of DEI relation extraction is promising, with an F-measureof 84.97 % on the in vivo dataset and 65.58 % on the in vitro dataset. Further, the inferred DDIs achieved a precision of83.19 % on the in vivo dataset and 70.94 % on the in vitro dataset, respectively. A further examination showed that theoverlaps between our inferred DDIs and those present in DrugBank were 42.02 % on the in vivo dataset and 19.23 % onthe in vitro dataset, respectively.Conclusions: This paper proposed an effective approach to extract DEI relations from biomedical literature. PotentialDDIs not present in existing knowledge bases were then inferred based on the extracted DEIs, demonstratingthe capability of the proposed approach to detect DDIs with scientific evidence for pharmacovigilance anddrug repurposing applications.Keywords: Drug-enzyme interaction, Pharmacokinetic drug-drug interactions, Semantic graph kernel,Ontology-based inference, Relation extraction, Literature miningBackgroundDrugdrug interaction (DDI) is a situation when onedrug alters the effect of another drug in a clinicallymeaningful way [1]. It has been demonstrated as oneof the major causes of adverse drug reactions and athreat to public health [24]. Existing resources ofDDIs include expert-curated knowledge bases such asDiDB (http://www.druginteractioninfo.org/), DrugBank(http:// www.drugbank.ca/), and pharmacy clinical sup-port systems [5]. Significant efforts have been invested toincorporate DDIs into various data sources. However,existing sources suffer from the problems of low coverage[6], low accuracy [7] and low agreement [8].Under such circumstance, scientific evidence revealingthe mechanism behind the drug interactions are neces-sary to provide support for reliable DDI information [9].One common type of DDIs is related to the mechanismof drug metabolism. For example, suppose drug A is a* Correspondence: hua.xu@uth.tmc.edu1School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USAFull list of author information is available at the end of the article© 2016 Zhang et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 DOI 10.1186/s13326-016-0052-6substrate of enzyme E, i.e., enzyme E is responsible forthe metabolism of drug A. If the enzyme is inhibited orinduced by drug B, the metabolism process of the drugA may be affected. Thus, the bioavailability of drug Acould be different than expected, potentially causingadverse effect [10]. Therefore, drug-enzyme interactions(DEIs) serve as one type of important supportive evi-dence for DDIs. Besides, DDIs not explicitly stated intext may be detected by linking and reasoning over DEIspublished in different scientific articles.Since newly reported DEIs are rapidly accumulating inthe huge archive of scientific literature [11], text miningtechniques are needed to automatically extract DEIs assupportive scientific evidence for DDIs [6]. One pilotwork in this direction is [10], which tried to extract therelations between drugs and enzymes based on proper-ties of drug metabolism; potential DDIs were thendetected by inference and reasoning. In [10], sentencesin PubMed were stored as parse trees in a database, andSQL queries consisting of keywords and simple syntacticand semantic constraints were used to extract DEIs.SemRep [12], a widely used tool to extract relations frombiomedical literature, also uses rule-based methods toextract DEI relations.One problem with current DEI extraction methods isthat their performance tend to be poor [10], given thatsentences in scientific literature tend to be long and havecomplex structure. Hence, more data-driven, statisticalmethods such as machine learning algorithms are neces-sary to automatically improve the performance. Further-more, no biological knowledge of concept hierarchies isinvolved in the inference process for DDIs currently. Forexample, if the drug Delavirdine is an inhibitor ofCYP3A [13], it could be an inhibitor of all enzymes inthe subfamily of CYP3A, such as CYP3A4. PotentialDDIs between Delavirdine and drugs that are substratesof CYP3A4 could then be inferred. In this way, moreimplicit potential DDIs may be identified.In this article, we propose a hybrid approach toextracting DEI relations. First, related drug enzyme pairsare extracted from sentences using the all-path graphkernel based machine-learning algorithm [14]. SpecificDEI relation types are then assigned according to triggerwords and syntactic patterns. After that, variations ofdrug and enzyme names are normalized to remove re-dundant relations. In the last step, inference rules arebuilt based on the drug-enzyme ontology and biologicalknowledge about mechanisms of drug metabolism andinteraction. Using these inference rules, the extractedDEI relations are then used for reasoning and inferringpotential DDI relations.Our approach differs from existing approaches intwo ways. First, we propose a hybrid method to im-prove the performance of DEI relation extraction.Second, we establish an ontology-based inference process,incorporating hierarchical relations between enzymes.Our evaluation results using the DEI corpus [15] demon-strates that our proposed approach outperforms SemRepsignificantly. Moreover, implicit DDI relations are inferredwith supportive evidence from DEIs, which may contrib-ute to existing DDI knowledge bases such as DrugBank.MethodsTwo DEI datasets, consisting of in vivo studies and invitro studies, were used in this study. Our methodinvolves three steps. First, related drug-enzyme pairswere extracted using an all-path graph kernel basedmachine-learning model. Different relation types werethen assigned based on the trigger words and syntacticpatterns. Second, variations of drug and enzyme nameswere normalized to remove redundant relations. In thelast step, inference rules were built on the basis ofdrug-enzyme ontology and biological knowledge aboutmechanisms of drug metabolism and interaction.Using these inference rules, the extracted DEI relationswere used for reasoning about potential DDI relations.DatasetsThe corpus of DEI relations built by Wu, Karnik et al.[15] was employed in this study. The DEI relations weremanually curated using 428 related abstracts from Med-Line [15]. Related abstracts were retrieved from MedLineusing the keywords of probe substrate/inhibitor/inducersfor specific metabolism enzymes in queries. The abstractsfor annotation were randomly selected from the searchresults. The abstracts in this corpus were categorized intotwo datasets for in vivo studies and in vitro studies,respectively, in order to accommodate the differencesfound between them the two study types. Two examplesentences with DEI relations from the in vivo and in vitrostudies are listed in Table 1.All the drug enzyme pairs that co-occur in one sen-tence were considered as candidate DEI pairs. Theinteraction relations between drug pairs were labeledas DEI (positive) or NDEI (negative). Table 2 showsthe statistics from the two datasets.Table 1 Example sentences with drug enzyme relations fromliteraturePMID Study type Sentence with drug enzyme interaction10223773 in vivo Rifampin (INN, rifampicin) is a potent inducer ofCYP3A4 and some other CYP enzymes.11353758 in vitro Rifalazil-32-hydroxylation in microsomes wascompletely inhibited by CYP3A4-specific inhibitors(fluconazole, ketoconazole, miconazole,troleandomycin) and drugs metabolized byCYP3A4 such as cyclosporin A and clarithromycin,indicating that the enzyme responsible for therifalazil-32-hydroxylation is CYP3A4.Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 2 of 8Relation extractionOur relation extraction method consisted of threesteps. First, we represented sentences with dependency-based syntactic structures. Second, all-path graph ker-nels describing the syntactic connections within thesentences were generated from those representations. ASupport Vector Machine (SVM) classifier was trainedbased on the graph kernels to generate a predictivemodel and to identify if the candidate drug-enzyme pairwas related. In the last step, trigger words and syntacticpatterns of different mechanisms of metabolism, i.e.,substrate, inhibitor, inducer, were used for specificDEI relation assignment.Sentence representationSentences with candidate DEI pairs were represented bythe dependency syntactic structure. For generalization,specific drug/enzyme names in a candidate DEI pairwere replaced with Drug/Enzyme in a preprocessingstep. For example, CYP2C9 and sildenafil in S1 werereplaced with Enzyme1 and Drug1.Enzyme1Drug1S1: CYP2C9 exhibited substantial sildenafil N-demethylase activity.Dependency graph of a sentence was constructedbased on its syntactic parse structure. It was a directedgraph that included two types of vertices: a word vertexcontaining its lemma and part-of-speech tags (POS), anda dependency vertex containing the dependency relationbetween words. In addition, both types of vertices con-tained their positions, which differentiated them fromother vertices. Figure 1(a) illustrates the dependencygraph of S1. Since the words connecting the candidateentities in a syntactic representation are particularlylikely to carry information regarding their relationship[16], the labels of the vertexes on the shortest undirectedpaths connecting drug and enzyme were differenti-ated from the labels outside the paths using a special tagIP. Further, the edges were assigned weights; all edgeson the shortest paths received a weight of 0.9 and otheredges received a weight of 0.3 as in [14]. Thus, the short-est path is emphasized while also considering the otherwords outside the path as potentially relevant.All-path graph kernelA graph kernel calculates the similarity between twoinput graphs by comparing the relations between com-mon vertices. The weights of the relations are calculatedusing all possible paths between each pair of vertices.Our method follows the all-paths graph kernel proposedby Airola et al. [14]. The kernel represented the targetpair using graph matrices based on two sub-graphs. Thefirst sub-graph represented the structure of a sentenceusing the dependency graph; the second sub-graphTable 2 Statistics of drug enzyme relation datasetsDataset Abstract Sentence Relation pair True pairin vivo Train 174 2114 1287 326Test 44 546 364 110in vitro Train 168 1894 4337 1360Test 42 475 1262 348Fig. 1 Illustration of the all-path graph representation. The candidate interaction pair is marked as Enzyme1 and Drug1. The shortest path betweenthe enzyme and the drug is shown in bold. In the dependency based sub-graph (a), all nodes in the shortest path are specialized using a post-tag (IP).In the linear order subgraph (b), possible tags are (B)efore, (M)iddle, and (A)fterZhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 3 of 8represented the word sequence in the sentence, and eachof its word vertices contained its lemma, its relative pos-ition to the target pair and its POS; all edges received aweight of 0.9 as in [14] (please see Fig. 1(b)).Assuming that V represents the set of vertices in thegraph, calculation of the similarity between two graphsused two types of matrices: edge adjacent matrix A andlabel matrix L. The graph is represented with the adja-cent matrix A ? R|V| × |V| whose rows and columns wereindexed by the vertices, and [A]i,j contains the weight ofthe edge connecting vi ? V and vj ? V if such an edgeexists, and 0 otherwise. In addition, the labels were pre-sented as a label allocation matrix L ? R|I| × |V|, so thatLi,j = 1 if the j-th vertex had the i-th label, and Li,j = 0otherwise. Using the Neumann Series, a graph matrix Gis calculated as:G ¼ LTX?n¼1AnL ¼ LT I?Að Þ?1?I L ð1ÞThis matrix sums up the weights of all the paths be-tween any pair of vertices, where each entry representsthe strength of the relation between a pair of vertices.Given two instances of graph matrices G? and G?, thegraph kernel K(G',G' ') is defined as follows:K G 0; ;G00 ¼X Lj ji¼1X Lj jj¼1G0ijG00ij 0 ð2ÞRelation type assignmentAfter recognizing the related drug-enzyme pairs, the rulesgenerated from trigger words and common syntactic pat-terns of various mechanisms of drug metabolism wereused to assign specific relations, i.e., isSubstrateOf, isIn-hibitorOf and isInducerOf. Some rules of each relationare illustrated in Table 3. For example, the sentence Themetabolism of MDZ, which is specifically metabolized byCYP3A4 in humans matches the pattern of Drug me-tabolized by Enzyme, from which the relation that MDZis a substrate of CYP3A4 could be identified. The sourcecode for relation assignment rules can be accessed follow-ing the link https://sbmi.uth.edu/ccb/resources/dei.htm.Concept normalizationIn the DEI datasets employed in this study, the drugnames were recognized using DrugBank and regular ex-pressions of various drug metabolites; enzyme nameswere recognized using regular expressions of variousforms of enzymes [15]. Many variations of drugs and en-zymes were annotated in the dataset. For example,CBZ is an abbreviation of the drug Carbamazepine.Both P4503A4 and 3A4 were mentions of the en-zyme CYP3A4. Hence, drug names and enzyme nameswere first normalized to reduce relation redundancybefore the reasoning step. Drug names were normal-ized to concepts in Unified Medical Language System(UMLS) [17] using MetaMap [18]. Enzyme nameswere normalized to CYP450 enzymes, as defined in thehuman cytochrome P450 allele nomenclature database,http://www.cypalleles.ki.se/. The number of extractedDEIs were reduced accordingly.Knowledge representation and reasoningDrug-enzyme ontology definitionTo incorporate the knowledge of drug metabolism withthe extracted DEI relations from biological literature, wecreated a DEI ontology. There are two classes in DEIontology: Drug and Enzyme. Each extracted drug orenzyme was considered an individual of Drug or Enzymerespectively. Further, biological knowledge of mecha-nisms in drug metabolism were represented by objectproperties between Drug and Enzyme in the ontology.As shown in Table 4, five object properties were definedbetween Drug and Enzyme. We implemented the DEIontology in OWL 2 (Web Ontology Language) [19].OWL 2 uses description logic to represent formal se-mantics for semantic inference. OWL API (ApplicationProgramming Interface) was used for the creation andmanipulation of the DEI Ontology [20].Drug enzyme ontology based inferenceAfter the ontology was populated, we defined propertychain rules to infer new DDI. The following are threerules that we defined to infer DDI:Table 3 Trigger words and syntactic patterns of different DEIrelation typesRelation Trigger words & syntactic patternsisSubstrateOf Drug  mediated/catalyzed/metabolized by EnzymeEnzyme responsible for/contribute to Drug metabolismMetabolismDrug(Enzyme)isInhibitorOf Drug  an inhibitor of EnzymeEnzyme inhibitor(Drug)Enzyme inhibit Drug activityisInducerOf Drug inducedEnzymeDrug  as a potent inducerof EnzymeTable 4 Logic facts definition for drug drug interactioninferenceDrug enzyme relationisSubstrateOf (d, e) Drug d is metabolized by enzyme eisInhibitorOf (d, e) Drug d inhibits the activity of enzyme eisInducerOf (d, e) Drug d induces the activity of enzyme eEnzyme enzyme relationisAncestorOf (e1, e2) Enzyme e1 is an ancestor of enzyme e2 inthe enzyme familyDrug drug relationDDI(d1, d2) Drug d1 and drug d2 have an interactionZhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 4 of 8Rule 1: isSubstrateOf (d1, e) and isInhibitorOf (d2, e)- >DDI (d1, d2)Rule 2: isSubstrateOf (d1, e) and isInducerOf (d2, e) - >DDI (d1, d2)Rule 3: isSubstrateOf (d1, e1) and isAncestorOf (e1, e2)- > isSubstrateOf (d1, e2)Rule 1 and Rule 2 encode the knowledge that if a givendrug d1 is a substrate of enzyme e, and drug d2 is aninhibitor/inducer of enzyme e, then drug d1 and d2 havea potential interaction. Rule 3 defines that the isSubstra-teOf relation can be inherited by a descendant enzymefrom its ancestors. Similar rules of inheritance werethen defined for the other drug-enzyme relations basedon the enzyme hierarchical relations. The reasonerHermiT was employed for DDI relation inference,which could check consistency of ontologies, computethe classification hierarchy, and explain inferences(Horrocks, et al., 2012). The ontology can be downloadedfrom https://sbmi.uth.edu/ontology/files/DEIOntology.owl.ExperimentsMachine learning (ML) algorithmSVM algorithms are the dominant ML methods (Segura-Bedmar et al., 2013) among the existing DDI systems.This study used the sparse version of RLS, also knownas the least squares SVM, to learn the DEI predictionmodel based on the all-path graph kernel [14].Experimental setupPOS-tags and dependency trees of the datasets weregenerated by Stanford parser [21]. We used the standardevaluation measures (Precision, Recall and F- measure)to evaluate the performance. We evaluated the perform-ance of our system on each test dataset after training onthe corresponding training dataset. Because our datasetswere imbalanced with much more NDEI relations thenDEI relations, the same candidate drug-enzyme pairpresent in multiple instances may be classified as DEIin one instance and as NDEI in another. In this case,we treated this candidate DEI pair as a true DEI pair toenhance the precision. Hence, the performance evalu-ation of relation extraction was carried out at the entity-level instead of the sentence level.The following systematic analyses were conductedbased on the experiments implemented in our study:(1)Comparison of DEI relation extraction performancebetween the all-path graph kernel based model(GraphKernel) with the model of java simple relationextraction (JSRE) [22]. JSRE is another state-of-the-art relation extraction model. It has demonstratedcomparable performance with the all-path graphkernel based model in protein-protein interactionrelation extraction [14, 23]. Different kernel optionsand parameters provided by JSRE were examined by10-fold cross validation on the training datasets. Theoptimal performance of JSRE was used for comparisonin our study, which was achieved by employingthe shallow linguistic context kernel with defaultparameters. Further comparison was made withthe existing knowledge base SemMedDB of literaturerelations, which was built using the SemRep system[12]. To select relations between drugs and genesfrom SemMedDB, PMIDs were used as one of thequery constraints, to ensure that the selected relationswere within the same publications as the test datasets.(2)Comparison of generated DDI relations withDrugBank: for each drug, we looked into theoverlap between the generated DDI relations withthe DrugBank. Specfically, novel DDI relationsgenerated in our study were examined by checkingtheir supportive evidence.Results and discussionPerformance of drug-enzyme relation extractionTable 5 illustrates the performance of DEI relation extrac-tion. As can be seen, JSRE obtained higher recall on bothdatasets as compared to the GraphKernel (in vivo: 83.67 %vs. 85.30 %; in vitro: 57.96 % vs. 71.20 %), while its precisiondropped significantly (in vivo: 86.32 % vs. 72.70 %; in vitro:75.51 % vs. 61.90 %). Overall, GraphKernel outperformedJSRE on the in vivo dataset (F1: 84.97 % vs. 78.50 %), with aslightly lower F1 on the in vitro dataset (F1: 65.58 % vs.66.20 %). The DEI relations extracted by SemRep are ofonly two types (INTERACTS_WITH and INHIBITS),most of which were of the INTERACTS_WITH type(105/165). Therefore, only the performance with referenceto recognition of related drug-enzyme pairs was comparedbetween our method and SemRep. As shown in Table 5,GraphKernel outperformed SemRep significantly (in vivo:84.97 % vs. 30.53 %; in vitro: 65.58 % vs. 15.32 %). Besides,the performance of the in vivo study dataset was muchhigher than that of the in vitro study dataset. Specifically,in the in vitro study dataset, the recall was much lower ascompared to the in vivo study dataset (GraphKernel:84.97 % vs. 65.58 %; SemRep: 30.53 % vs. 15.32 %).Table 5 Drug enzyme relation extraction performanceDataset Method P R F1in vivo GraphKernel 86.32 % 83.67 % 84.97 %JSRE 72.70 % 85.30 % 78.50 %SemRep 60.60 % 20.41 % 30.53 %in vitro GraphKernel 75.51 % 57.96 % 65.58 %JSRE 61.90 % 71.20 % 66.20 %SemRep 55.73 % 8.88 % 15.32 %Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 5 of 8Table 6 illustrates the performance of our system interms of drug-enzyme relation assignment. After drugand enzyme normalizations, 30 isSubstrateOf, 29 isInhi-bitorOf and 7 isInducerOf relations were identified in thein vivo dataset totally; 62 isSubstrateOf, 67 isInhibitorOfand 5 isInducerOf relations were identified in the in vitrodataset. As can be seen, the performance for the isSub-strateOf relation was relatively higher among the threerelations in both datasets (in vivo: 87.48 %; in vitro:72.79 %). The performance in the in vitro dataset ismuch lower than that in the in vivo dataset, since manyof DEI pairs were already lost in the first stage of recog-nizing related drug-enzyme pairs (Table 5). The ex-tracted relations were used to populate the DEI ontologydefined in Section 2.4.1. Totally, the current ontologycontains 104 individuals in Drug, 16 individuals in En-zyme, and 213 triples for drug metabolism, including 81isSubstrateOf triples, 96 isInhibitorOf triples, 12 isIndu-cerOf triples, and 24 isAncestorOf triples.Performance of drug-drug interaction inferenceEvaluation results of inferred DDIs are listed in Table 7.Totally, 181 DDIs were inferred from the in vivo dataset,and 376 DDIs were inferred from the in vitro dataset, re-spectively. For comparison, only relations between drugspresent in DrugBank were examined during evaluation.Totally, 31 drugs and 40 drugs in the in vivo and in vitrodatasets, were present in DrugBank respectively. For thedrugs present both in our corpus and DrugBank, totally119 DDIs were inferred from the in vivo dataset, ofwhich 69 DDIs were not included in DrugBank; 234DDIs were inferred from the in vivo dataset, of which189DDIs were not included in DrugBank. As illustrated inTable 7, the overlap between inferred DDIs in this studyand DrugBank was low (in vivo: 42.02 %; in vitro:19.23 %). However, by manually checking the supportiveevidences, i.e., the underlying DEI relations for thoseDDIs, it was verified that the inferred DDIs achieved aprecision of 83.19 % for the in vivo dataset and 70.94 %for the in vitro dataset, respectively.DiscussionDEIs are important supportive evidence for DDIs. Thisstudy applied a hybrid approach for DEI relation extractionfrom biomedical literature. Reasoning was then conductedon the extracted DEIs to infer potential DDI relations, byincorporating biological knowledge into drug-enzymeontology. Evaluation results demonstrated the effectivenessof our approach: potential DDIs were inferred with reliableprecisions (in vivo: 80.30 %; in vitro: 72.09 %), indicating itscapability to detect DDIs with scientific evidence.The model of GraphKernel obtained much higher pre-cision and lower recall than JSRE (Table 5). This demon-strated that GraphKernel and JSRE have advantages ofdifferent aspects on the DEI datasets. One potentialexplanation could be the essential kernel difference be-tween these two models. JSRE only relies on shallowlinguistic features of text, such as tokens, POS and lem-mas, while GraphKernel combines shallow linguisticfeatures with more complex structural syntactic features.Thus, the constraints of JSRE were relatively relaxedon the text in comparison with GraphKernel, leadingto the high recall of JSRE and the higher precision ofGraphKernel. Overall, GraphKernel outperformed JSREsignificantly on the in vivo dataset (F1: 84.97 % vs.78.50 %), with a slightly lower F1 on the in vitro dataset(F1: 65.58 % vs. 66.20 %). This indicates that there is roomfor further improvement in the relation extraction fromthe in vitro dataset.As shown in Table 5, our approach outperformed Sem-Rep significantly in terms of DEI relation extraction. Onepossible reason could be that SemRep is a general infor-mation extraction tool for biomedical literature, which isnot focused on the DEI relation. On the other hand, ourmodel was trained on the datasets dedicated to DEI rela-tions. Another possible reason is that instead of usingrule-based methods as in SemRep, our study applied stat-istical machine-learning model first to recognize relateddrug-enzyme pairs to remove false positive DEI relationpairs and to improve the performance. As an illustration,in the sentence the possibility of in vivo drug interactionof azelastine and other drugs that are mainly metabolizedby CYP2D6, the candidate relation pair of azelastine andCYP2D6 matches the pattern of the isSubstrateOf rela-tion. However, it is a false positive relation and isremoved in the first step by the statistical model.Although for the drugs present both in our corpus andDrugBank, only 42.02 % of inferred DDIs from the invivo dataset and 19.23 % from the in vitro dataset arecovered by DrugBank, manual examination demon-strated that our approach could find potential DDI rela-tions with supportive evidence. For example, from theTable 6 Drug enzyme relation assignment performanceDataset Relation P R F1in vivo isSubstrateOf 89.34 % 85.71 % 87.48 %isInhibitorOf 83.33 % 77.42 % 80.27 %isInducerOf 71.43 % 57.14 % 63.49 %in vitro isSubstrateOf 80.15 % 66.67 % 72.79 %isInhibitorOf 73.88 % 55.37 % 63.30 %isInducerOf 69.74 % 40.00 % 50.84 %Table 7 Performance of drug drug relation inferenceDataset P R F1 DrugBankOverlapin vivo 83.19 % 52.84 % 64.63 % 42.02 %in vitro 70.94 % 42.11 % 52.85 % 19.23 %Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 6 of 8sentence  and is probably caused by inhibition ofCYP3A4 -mediated voriconazole metabolism (PMID:16890574), we identified that the drug voriconazole is asubstrate of CYP3A4; meanwhile, from the sentence oxcarbazepine (OXCZ) are well-known inducers of drugmetabolism via CYP3A4 (PMID: 17346248), we identi-fied the relation that the drug oxcarbazepine is an in-ducer of CYP3A4. One potential interaction betweenvoriconazole and oxcarbazepine could then be inferred,which is not listed in DrugBank. More examples ofinferred DDIs as well as their supportive evidence fromliterature are listed in Table 8.Despite the fact that our proposed method of DEI rela-tion extraction achieved a F1 of 84.97 % on the in vivodataset, the F1 of 65.58 % obtained on the in vitro data-set is still low. Based on our empirical observation, themajor reason for the performance difference betweenthese two datasets lied in the essential difference of theirlinguistic structures, which originated from the differ-ence between the in vivo and in vitro studies. In vivostudies focus on evaluating the effect of an investiga-tional drug on other drugs, by checking the changes ofpharmacokinetic parameters. Different from in vivostudies, in vitro studies can qualitatively provide themechanisms of a potential DDI based on the observationof enzyme kinetics parameters. Thus, sentences in the invitro dataset contained more drug enzyme interactions;whereas they were also much complex than those in thein vivo dataset, with more multiple clauses, long con-junctive structures and rare patterns. When we lookedinto the errors of DEI relation extraction, especially inthe in vitro dataset, we found that the major causes offalse negative instances include conjunctive structures ofdrugs/enzymes (e.g., Studies using the CYP3A4 inhibi-tors ketoconazole, troleandomycin, and erythromycin),and the rare patterns uncovered by the statistical model(e.g. Induction of CYP2C9 would explain the increasedsystemic elimination of glipizide). On the other hand,the major causes of false positive instances include theinability to catch the context information differentiatingbetween positive and negative relations (e.g., the wordconfirm indicates the uncertainty of the DEI relationin the sentence  to confirm that fluvoxamine inhibitsCYP2C19), and wrong predictions between drugs andenzymes across multiple clauses, as in the sentenceGreater inhibition was produced by the less selectiveCYP3A inhibitors parathion, quinidine, and ketocona-zole; CYP1A inhibitors were ineffective..The above problems should be addressed in the futureto further improve the DEI relation extraction perform-ance. Specifically, additional advanced methods tailored tothe in vitro dataset should be explored, including auto-matic pattern recognition methods to identify conjunctivestructures of drugs/enzymes, multiple clauses split beforefeature extraction, keyword expansion to indicate theuncertainty (e.g., to determine and was examined).One limitation of our current work is the size of theannotated corpus. For practical usage, we plan to applyour system to all the related articles in PubMed to ob-tain a more comprehensive list of DEIs and potentialDDIs. Besides, further improvements of our system mayneed to be conducted after evaluation on a larger DEIcorpus. In addition to narrative literature text describingDEIs, tables of DEIs with details of interactions in thepublished full text articles are another valuable resourceto obtain such information that we plan to incorporate.Extracting DEIs from tables is more straightforward andpotentially have more accurate results as compared tothe text. However, in comparison to accessing titles andabstracts of articles through MedLine, one problem oftables is that the automatic access to full text is limited.Actually, these two resources could be complementaryto each other for mining DEIs from biomedical litera-ture. In our future work, methods of mining tables fromDEI related articles would be explored. Another draw-back of our current approach for DDI relation inferenceis that the information of specific conditions requiredfor the occurrence of DEIs and DDIs, such as dosages ofTable 8 Examples of inferred drug drug interactions and supportive evidence from literatureDrugs with interaction Enzyme EvidenceCarbamazepine/oxcarbazepinequinidineCYP3A4 We performed a study in healthy volunteers to investigate the relative inductive effect of CBZ and OXCZon CYP3A4 activity using the metabolism of quinidine as a biomarker reactionWe confirm a clinicallysignificant inductive effect of both OXCZ and CBZ. (PMID: 17346248)Lidocaine fluvoxamine CYP1A2 Lidocaine is metabolized by cytochrome P450 3A4 (CYP3A4) and CYP1A2 enzymesWe conclude thatinhibition of CYP1A2 by fluvoxamine considerably reduces the presystemic metabolism of oral lidocaine(PMID: 16918719)Quinidine itraconazole CYP3A4 Quinidine is eliminated mainly by CYP3A4-mediated metabolism Itraconazole increases plasma concentrationsof oral quinidine, probably by inhibiting the CYP3A4 isozyme during the first-pass and elimination phasesof quinidine. (PMID: 9390107)Propofol orphenadrine CYP2B6 Involvement of human liver cytochrome P4502B6 in the metabolism of propofol orphenadrine, a CYP2B6inhibitor, reduced the rate constant of propofol by liver microsomes by 38 % (P < 0.05) (PMID: 11298076)Rifalazil fluconazole CYP3A4 Rifalazil-32-hydroxylation in microsomes was completely inhibited by CYP3A4-specific inhibitors (fluconazole,)  indicating that the enzyme responsible for the rifalazil-32-hydroxylation is CYP3A4. (PMID: 10923859)Zhang et al. Journal of Biomedical Semantics  (2016) 7:11 Page 7 of 8drugs, was not considered. Information of such condi-tions is also very critical for supportive evidence for DDIrelations, which should be taken into consideration inthe next step.ConclusionOur study proposes a hybrid approach of combiningmachine-learning algorithm with rule-based patternsto extract DEIs from biomedical literature, from whichpotential DDI relations can be inferred by reasoning.Evaluation results demonstrate that the performanceof DEI relation extraction outperformed SemRep sig-nificantly, with a F-measure of 84.97 % on the in vivodataset and 65.58 % on the in vitro dataset. Moreover,potential DDIs not present in DrugBank were alsoinferred, indicating that this proposed approach couldbe used to detect DDIs supported by scientific evidence ofdrug metabolism and interaction.Competing interestsThe authors declare that they have no competing interests.Authors contributionsYZ, HW, LL and HX were responsible for the overall design, development,and evaluation of this study. LL, HW and YZ developed the annotationguidelines and annotated the data set used for this study. JD, JX and JWworked with YZ on the algorithm development. YZ and HX did the bulkof the writing, JD and CT also contributed to writing and editing of thismanuscript. All authors reviewed the manuscript critically for scientificcontent, and all authors gave final approval of the manuscript for publication.AcknowledgementsThis work was supported by Cancer Prevention & Research Institute of Texas[R1307]; GM10448301, and LM011945.Author details1School of Biomedical Informatics, University of Texas Health Science Centerat Houston, Houston, TX, USA. 2School of Medicine, Indiana University,Indianapolis, IN, USA.Received: 15 November 2015 Accepted: 11 February 2016Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 DOI 10.1186/s13326-016-0049-1RESEARCH Open AccessModularising ontology and designinginference patterns to personalise healthcondition assessment: the case of obesityAleksandra Sojic*, Walter Terkaj, Giorgia Contini and Marco SaccoAbstractBackground: The public health initiatives for obesity prevention are increasingly exploiting the advantages of smarttechnologies that can register various kinds of data related to physical, physiological, and behavioural conditions.Since individual features and habits vary among people, the design of appropriate intervention strategies formotivating changes in behavioural patterns towards a healthy lifestyle requires the interpretation and integration ofcollected information, while considering individual profiles in a personalised manner. The ontology-basedmodelling is recognised as a promising approach in facing the interoperability and integration of heterogeneousinformation related to characterisation of personal profiles.Results: The presented ontology captures individual profiles across several obesity-related knowledge-domainsstructured into dedicated modules in order to support inference about health condition, physical features,behavioural habits associated with a person, and relevant changes over time. The modularisation strategy isdesigned to facilitate ontology development, maintenance, and reuse. The domain-specific modules formalised inthe Web Ontology Language (OWL) integrate the domain-specific sets of rules formalised in the Semantic WebRule Language (SWRL). The inference rules follow a modelling pattern designed to support personalised assessmentof health condition as age- and gender-specific. The test cases exemplify a personalised assessment of theobesity-related health conditions for the population of teenagers.Conclusion: The paper addresses several issues concerning the modelling of normative concepts related to obesityand depicts how the public health concern impacts classification of teenagers according to their phenotypes. Themodelling choices regarding the ontology-structure are explained in the context of the modelling goal to integratemultiple knowledge-domains and support reasoning about the individual changes over time. The presentedmodularisation pattern enhances reusability of the domain-specific modules across various health care domains.Keywords: Obesity, Ontology modularisation, Personalised inference, Physical constitution, Physical activity,Nutritional habits, Healthy lifestyle, Person, TeenagerBackgroundOverweight and obesity are estimated to result in thedeaths of about 320 000 people in western Europe everyyear [1]. The prevalence of obesity among children andadolescents motivated public health organisations topromote a healthy lifestyle by specifically engaging chil-dren [1, 2] and adolescents [1, 3, 4]. The initial activitytowards the engagement of individuals consists in the* Correspondence: aleksandra.sojic@itia.cnr.itInstitute of Industrial Technologies and Automation (ITIA), National ResearchCouncil (CNR), Via Bassini 15, Milan, Italy© 2016 Sojic et al. Open Access This article isInternational License (http://creativecommonsreproduction in any medium, provided you gthe Creative Commons license, and indicate if(http://creativecommons.org/publicdomain/zedesign of a scientifically informed strategy, i.e. the de-velopment of a model that defines the key features asso-ciated with obesity. Capturing this knowledge is acomplex task since it often requires understanding inter-twined relations between various phenotypic parametersand socio-behavioural aspects of lifestyle [5]. Availabilityof technological devices that can register data associatedwith physical constitution, physiology, behavioural habitsrelated to physical activity, and nutrition enables theacquisition of more specific insight into physical char-acteristics of individual people and their behaviouralpatterns [6]. The interpretation and understanding ofdistributed under the terms of the Creative Commons Attribution 4.0.org/licenses/by/4.0/), which permits unrestricted use, distribution, andive appropriate credit to the original author(s) and the source, provide a link tochanges were made. The Creative Commons Public Domain Dedication waiverro/1.0/) applies to the data made available in this article, unless otherwise stated.Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 2 of 17the acquired data involves multiple domains of know-ledge and the analysis of heterogeneous information.The relevant data need to be collected, organised, andintegrated in order to provide a feedback that is ap-propriate to a specific personal profile.The task of representing personal profiles in a modelthat integrates diverse kinds of data provided by varioussources motivates the employment of Semantic Web tech-nologies [7]. In particular, ontologies are recognised as aconvenient approach to deal with complex and heteroge-neous information across various domains [810], enab-ling data interoperability [11, 12] and also knowledgegeneration via reasoning [8, 9]. Unlike some alternativemodelling approaches (i.e. relational databases), ontologymodels incorporate semantics, formalising and explicatingshared understanding of a domain that can be easier toreuse across various applications (for the comparison ofontologies and relational databases see [1316]).Several studies report the use of ontology and seman-tic technologies to target obesity (e.g. [2, 17, 18]). Scalaet al. [18] present an e-Knowledge platform, based on aWeb Ontology Language (OWL) [19] ontology andSemantic Web Rule Language (SWRL) [20] rules,classifying individuals according to the obesity leveland certain medical conditions (Sarcopenia, Hypertension,Dyslipidemia, Diabetes, Insulin resistance, Metabolicsyndrome). Arash et al. [17] and Addy et al. [2] presentthe preliminary stage of an ontology designed tosupport a knowledge-based infrastructure, promotinghealthy eating habits and lifestyles. In particular, Addyet al. [2] aim to support the ontology-based decisionmaking across multi-stakeholder partnerships (MSPs)of the Quebec community involved in the manage-ment of childhood obesity.The relevant literature addressing the issues related toobesity mostly refers to specific scenarios, focusing ei-ther on adults with certain diseases [18] or on childrenwithin a local community context [2, 17].Since the public health concerns related to obesity [1]include various scenarios (e.g. diverse social, geopoliticaland age groups, etc.), a generic model that would coverdiverse knowledge-domains and application-contexts re-lated to obesity would be beneficial as it could exploitthe full potential of ontology-based modelling that goesbeyond single application (see e.g. [11, 21]).In order to face the need to model complex and di-verse aspects of obesity-related knowledge, this paperin particular deals with: Formalisation of generic knowledge related to obesity. Specialisation of the generic model into a teenagertailored model. Modularisation to support integration of generic andspecific knowledge. Changes of personal features over time. Automatic inference of personal health status that isrelevant for obesity assessment and prevention.The possibility of having both a general and a specificmodel is achieved by adopting a modular design strat-egy. The core ontology module specifies certain genericclasses that are applicable to any human being and ageneric characterisation of individual health conditions.The domain-specific ontology modules (applicable toany person) provide obesity-related classifications. Thecore module as well as the domain-specific ontologymodules are formalised in OWL (see the following sec-tions). The modules specifying sets of rules are modelledin SWRL and explicitly provide reference values thatsupport inference and classification of personal profilesfor the population of teenagers. In particular, theontology design is driven by the need to track changes inhealth condition over time (i.e. the issue that was notaddressed by [17, 18]).Thus, the ontology model presented in this paperaddresses the problem that was only partially addressed inthe models previously described in the literature (i.e.[2, 17, 18]) as it faces modelling of personal profileson a generic level to support specific inference withina comprehensive ontology model of the obesity-relatedknowledge. The developed ontology formalises infor-mation about obesity-related human features, enables rea-soning, and enables information flow and interoperabilitybetween the technological tools and platforms employedto monitor the changes of health status, behaviour, andnutritional habits of humans in general, and adolescentsin particular.The development of this ontology was initiated withinthe European research project named PEGASO [22]whose main goal is the enhancement of self-awarenessand motivation of adolescents towards a healthy lifestyle[3, 23, 24]. Like some other initiatives (e.g. [2, 17]), theproject is driven by the public health concerns aiming atthe decrease of obesity-related risks to health [3, 25]. Thetarget population is represented by the future adultswhose behavioural habits at an early age can significantlyimpact their health status on a life-long horizon [23]. Theproject includes several research initiatives and interven-tional strategies such as the development of serious games[3, 26] that should promote a healthy lifestyle, the designof a life companion [24], the use of wearable gadgetsequipped with sensors to monitor health status [3, 24], thedesign of mobile applications such as an e-diary used torecord dietary habits, etc. [3, 24].In the following sections we first outline the theoreticaland practical context that is relevant in explaining and jus-tifying the decisions taken during the ontology designphase. We discuss the modular structure of the ontologySojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 3 of 17as related to the methodological approach that considersontology-design from two perspectives:(1) task dependent modelling that faces a particularapplication scenario and (2) extrapolation of generalmodelling patterns that can be used in a broad contextthat goes beyond a single application task. We specific-ally describe an ontology module that captures the phys-ical domain and classifies health conditions based on theassessment of body constitution. In the second part ofthe paper we present the inference patterns that are usedin the current version of the ontology. In order to exem-plify the employment of reasoning patterns, we providethe case of reasoning over a personal assessment ofhealth condition by combining OWL and SWRL rules.The concluding remarks outline some advantages of themodular structure and inference patterns, discussing thepotential of their reuse in other application scenarios.MethodsThe aim of this section is to explain and justify the rep-resentational choices employed in the ontology design.The initial step in the ontology development includes amulti-disciplinary analysis that considers a person as adynamic agent who is constantly changing in theirinteraction with their environment. Thus, the method-ology for the ontology design includes a detailed specifi-cation of the modelling domain(s), goal(s), and contextof current scientific knowledge, i.e. theories used to de-fine the key concepts relevant for capturing obesity-related knowledge in a comprehensive manner. Sincethe domain problem covers several fields of knowledgethat are related to the problem of obesity, the specificfields are identified as distinct sub-domains of know-ledge. The identified fields are later used to structureknowledge into the dedicated ontology modules, each ofwhich can exist independently as the modules capturefield-specific aspects of human features that are relevantfor the modelling task and are also applicable to a widescope of related scenarios. The methodology is in linewith the tradition that considers ontology as an engineer-ing artifact that is useful to model some aspects of theworld. In other words, we accept the position that inArtificial Intelligence Systems, what exists is what canbe represented ([27], p. 908909).A preliminary study of a cross-disciplinary approachThe preliminary analysis of domain-knowledge, as pre-sented below, is relevant for (1) the specification of theontology goal and scope, (2) the methodology for theontology design, and (3) the justification of the represen-tational choices regarding the study of obesity and itsprevention. The impact of cross-disciplinary studieson the ontology design are considered in the contextof background knowledge and theories that theontology needs to capture formally. The explication ofthe design-rationales aims at reducing opacity of thedeveloped ontology and increasing its re-usability (c.f.[28], p. 222).While dealing with the problem of obesity, its character-isation and prevention, it is important to consider severalfactors such as physical (in)activity, physiological (dys)-function, (un)healthy eating habits, social and psycho-logical problems [3]. In some cases, one of these aspectscan be more decisive than the others causing overweightor obesity, whereas in other cases the overweight-condition (or a related disease) is the result of a combin-ation of several factors. In order to identify and, poten-tially, modify the most relevant factor(s) or a specific habitof an individual that increases the likelihood of developingan overweight-condition, it would be optimal to considerones current state from the perspective of a comprehen-sive model that captures the features of a human being asa whole [3]. Such a model can be understood as an ab-stract representation that aims at integrating the cross-disciplinary knowledge of humans in a broad context.Lafortuna et al. [5], Guarneri et al. [3], Caon et al. [23]and Carrino et al. [24] carried out multidisciplinary studiesto address the issue of obesity and its prevention via em-ployment of smart devices and persuasive technologies[23]. The studies considered intertwined relationships be-tween human individuals and their environment. In par-ticular, the results of the studies on physical, physiological,and behavioural aspects of human phenotypes provided acomprehensive model, i.e. the so-called Virtual IndividualModel (VIM) [5, 23] that is meant to be a theoreticalframework to deal with obesity prevention. The VIMidentifies the key components that influence thehealth status of a person with reference to overweightand obesity, focusing on adolescents in particular. Sincethe VIM captures obesity-related knowledge by commonrepresentational means, e.g. natural language definitions,tables and graphs (readable to competent human experts),the presented information was not specified in a formallanguage. In other words, the VIM lacks a formal seman-tics and explicitness that would disambiguate its termino-logical and ontological assumptions in order to structurethe concepts and relations in a comprehensive andmachine-readable form.An elaboration of the contents of the VIM led to theidentifications of the key targets of the ontology-model:(1) capture health conditions of individual teenagers; (2)detect personal obesity-related risk factors; and (3) opti-mise the information structure in order to provide a per-sonalised feedback that can motivate behavioural changestowards a healthy lifestyle.After the identification of the ontology goal, the nextstep in the ontology design was to partition the modellingdomain by specifying the most relevant fields of knowledgeSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 4 of 17that can be formalised as independent sub-domain on-tologies, i.e. the modules that can be later integrated intothe final ontology model (see the following section).The identification of knowledge sub-domains startedwith the analysis of the VIM that characterises humanindividual on three levels:(i) the physical-physiological level (see Physical Statusin [23], p.1812),(ii) the nutritional level (see Dietary Habits in [23],p.1813), and(iii) the psycho-social level (see Psychological Status in[23], p.1812).Each of these three levels of the VIM addressed the prob-lem of obesity and its prevention from diverse disciplinaryperspectives, thus allowing the domain specialists tocontribute with their expertise to a comprehensive view onthe problem. However, since we aim at characterising theknowledge sub-domains as distinct, coherent and comple-mentary segments of knowledge, the characterisation of thelevels (i-iii) is insufficient as it lacks a clear demarcationcriterion necessary for the development of independentontology modules. For instance, level (ii) includes the char-acterisation of dietary habits and as such it partly intersectswith level (iii), which also (from another perspective, i.e.psychological) aims at targeting behavioural habits, e.g.fruit and vegetable intake. On the other hand, level (i) dealswith both physical and physiological aspects that areclosely related, but nonetheless (ontologically) distinct. Inaddition, the physical description in (i), besides body con-stitution, also includes the characterisation of habits suchas physical activity that actually represents a behaviour. Assuch, the characterisation of physical activity captures fea-tures that are distinct from those used in the description ofsome physical parameters related to body constitution.For the requirements of an ontologically clean andcoherent model that follows proper classification cri-teria [11, 29, 30], a more specific distinction of themodelling-domain and its sub-domains is required.The task at hand is to specify sub-domains of knowledgein a way that can support a sustainable ontology de-velopment, thus following a coherent modularisationapproach. Accordingly, we define and combine thetopic-centred and discipline-oriented demarcation cri-teria, where by discipline we consider any field ofstudy that is covered by the current educational system(see e.g. ISCED: International Standard Classification ofEducation [31]). On the other hand, the topic-centred cri-terion considers not merely a topic of study addressed bysome discipline, but the features of the objects targeted bysome study are also taken into account according to theOntoClean methodological perspective [29]. Thus, the cri-terion distinguishes on a meta-level kinds of objects thatare targeted by the study (i.e. meta-topic). The identifica-tion of a meta-topic can be illustrated by the previous ex-ample of physical activity and physical features of humansthat can be the topics of study addressed by psychologists,nutritionists, general practitioner, and so on. As a selectedtopic of study might target ontologically distinct objects ofinterest, we used a meta-topic characterisation to discrim-inate between static and dynamic parameters, featureschangeable over time vs. rigid features, etc. For instance,while a living being will necessarily have weight andheight, their values will change over time. Likewise, thedate of birth can be considered as a rigid parameter boundfor a person, while the age of an individual changes overtime. Also the characterisation of physical activity mightbe considered as a topic that includes the description ofphysical features, but from the meta-topic point of viewthe description of the activity captures behaviour and notsome static physical features. The description of physicalfeatures might complement the description of physical ac-tivity, but the two concepts have different meanings asthey capture diverse aspects of the physical reality. Thus,the demarcation of the topic of interest was performed ac-cording to an onto-sensitive approach that was usedjointly with the disciplinary criterion to define the ontol-ogy modules, as described in the following section.Ontology modularisationOntology modularisation is recognised as an importanttopic especially regarding the implementation, mainten-ance, and reuse of ontology [3234]. Despite the factthat modularisation plays a significant role in ontologyengineering, there is no universally accepted methodo-logical approach to modularisation [35, 36]. Some ap-proaches focus on logical criteria (see e.g. [37]),whereas others address the issue of modularisationfrom a broader perspective (see e.g. [35, 36]) arguingthat the choice of a modularisation technique andmethodological approach actually depends on the par-ticular requirements defined by the modelling goal andthe application scenario.Even so, the opportunity to modularise an ontologyalready in its early developmental stages provides nu-merous advantages related to its evolution, maintenanceand reuse.Concerning the most appropriate modularisation strat-egy for the task at hand as well as for potential futureapplications [28, 32, 38, 39], several criteria were com-bined to define the modules, as outlined in Fig. 1.First, the linearisation methodology for robust modu-lar implementation of ontologies [32] is taken into ac-count by adopting the following design criteria: the ontology modules are identified and separatedfrom the whole;Fig. 1 The Multidimensional Modularisation Methodology (MMM): the modularisation dimensions identified to support the ontological coherenceon the theoretical level and sustainability on the application level (i.e. independent development, evolution, and validation)Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 5 of 17 ontology maintenance is enhanced by enablingindependent work on single modules; modules can evolve independently and new modulescan be added with minimal side effects; the differences between different domain-specificcategories are represented explicitly, thus enablingboth human understanding and formal machineinference.Furthermore, the validation criterion ([35], p.69) and thedomain coverage criterion ([35], p.74) are taken intoaccount in order to enable the independent validation ofdefined modules by different experts (See DisciplinaryPerspective criterion in Fig. 1). Besides the fact thatmultiple fields of knowledge must be captured andvalidated independently, the intended formalisation intwo modelling languages (i.e. OWL and SWRL) requiresthe language-specific validation [20] that motivates theseparation of the segments formalised in OWL and thesegments formalised in SWRL (see Formal Specificationcriterion in Fig. 1).Fig. 2 Ontology modularisation in the case of the Obesity domain: Applyinidentify specific ontology modulesFinally, the main ontology structure is designed accord-ing to the Multidimensional Modularisation Methodology(MMM) (Fig. 1) that identifies the following criteria:(a) the criterion of disciplinary perspectives; (b) themeta-topic coherence view, i.e. the criterion used to define(b1) the specific Topic that should be captured within anontology module in an onto-sensitive and coherent man-ner, thus narrowing down the scope of a disciplinary per-spective and domain coverage; and (b2) features that canbe either dynamic or static (see the previous section). Themeta-topic view is also used to specify the scope of (c) theintegrative-view criterion that is used to identify commonconcepts shared across-domains, thus supporting the in-ter-module integration (e.g. intersecting the Cross-domaincriterion and Human Health Condition topic  the ex-ample that will be discussed in the Results section).Figure 2 illustrates the application of MMM to theObesity domain, where criterion (a) was crucial indefining domain-specific modules (see O1-O5 withthe segments T1-T5 and R1-R5), criterion (c) was particu-larly relevant in defining cross-domain modules (see T6 ing The Multidimensional Modularisation Methodology (MMM) toSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 6 of 17Fig. 2), whereas criterion (b) had an important role in thecharacterisation of all the modules, providing an onto-logically coherent structure. In addition, the validationcriterion focused on Formal Specification, jointly with (b),is used to distinguish the ontology segments modelled inOWL (T1-T6) from the rule segments modelled in SWRL(R1-R5) (see Figs. 1 and 2). The resulting ontology consistsof the following modules:(O1) PhysicalStatus that captures physical features ofthe human body.(O2) PhysicalActivity that captures the physicalbehaviour and habits.(O3) PhysiologicalStatus that captures certainphysiological parameters.(O4) NutritionalBehaviour, capturing nutritional habitsand behaviour.(O5) ApplicationContext that specifies the contextualinformation relevant for potential applicationscenarios, e.g. geographical location.(O6) Common module captures cross-domaininformation to support the interoperabilityacross the modules (O1-O5).Figure 3 presents the modular architecture designed tosupport independent development, validation, use, and evo-lution of the domain-specific ontology modules. The Com-mon module (O6), formalised in OWL, consists of a TBoxthat provides terminological contents and the most genericspecification of classes and relations that are common forall other modules (O1-O5). On the other hand, thedomain-specific ontology modules (O1-O5) are composedof a TBox component formalised in OWL (see T1-T6) andan RBox component containing only domain-specificSWRL rules (see R1-R5) specified as extension of the corre-sponding TBox modules. In particular, the following RBoxFig. 3 The modular architecture supporting independent ontologydevelopment, validation, use, and evolution. The sub-domain ontologies(O1-O5) are composed of domain-specific TBox and RBox modules.Module O6 consists of a TBox that is common to all other modulesmodules are used for personalised inference and classifica-tion of individuals within the target population of teenagers:(R1) PhysicalStatus Rules - used for personalisedassessment of health conditions as based on theobesity classification.(R2) PhysicalActivity Behaviour Rules - used for theassessment related to behavioural habits, e.g.sedentariness.(R3) PhysiologicalStatus Rules - related to theassessment of health conditions based onphysiological parameters, e.g. metabolism rate.(R4) NutritionalBehaviour Rules - used for the assessmentof nutritional characterisation of individuals as basedon the food and drink intake, e.g. breakfast skipper.(R5) ApplicationContext Rules - used for the context-dependent assessment to characterise conditionsthat vary across socio-cultural contexts, e.g.modifying the assessment based on the informationabout geographical location.These RBoxes define the rules based on current know-ledge of the relationships between the captured parametersand the reference values acquired by the team of experts,the World Health Organisation (WHO) reference tables forthe population of adolescents [40], and the most recent lit-erature in the domain of interest that is provided in theontology annotation. On the other hand, the TBoxes aremodelled to be more stable and population independent.By keeping TBox and RBox separate, the ontology valid-ation, maintenance, reusability, and evolution are enhanced.For example, the rules in the RBox are defined according tothe current state of knowledge that defines cut-off valuesused in classifying a teenager as obese, over-weight etc. Incase the state of knowledge changes (or a target populationchanges), any change in cut-off values specified in the ruleswill not impact the ontology as a whole and modificationscan be made only within the rules that contain the up-dated values. In addition, separating RBoxes specified inSWRL from the TBoxes specified in OWL enables inde-pendent and the language-specific validation [20] of theOWL and SWRL ontology segments.Results and discussionThis section focuses on the ontology content. While pre-senting the modelling patterns, we describe the Commonand PhysicalStatus ontology modules, specifying the mostrelevant body features as related to measurements and toseveral other classes of health conditions that are used todefine the obesity-related status and potential risk factors.Capturing normative concepts: assessment of obesity as ahealth conditionIn general terms, a description of a person (e.g. a teen-ager) via some structural, functional, and behaviouralSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 7 of 17characteristics is actually capturing aspects consideredto be relevant to describe his/her phenotype (that mightbe a teenager-specific phenotype). The main focus is onthe phenotypic features describing the class to which aperson belongs as determined by the characterisation ofhis/her physical and behavioural features [41]. Thus, weconsider that a persons phenotype belongs to the classobese based on his/her characteristics, description ofwhich (despite of individual variations) fits to the de-scription of an obese phenotype that is typical for everyperson of a certain gender and age range. We define typ-ical features of an obese phenotype in terms of a conven-tional agreement at the current stage of knowledge. Thereference system that we use to characterise the physicalfeatures of an obese phenotype is provided by the WorldHealth Organisation [25] and it includes the age- andgender-specific ranges of values, e.g. body mass index ofteenager (see [40]). Moreover, we treat the description ofbody constitution as a specific characterisation of pheno-type that is associated with health condition.HealthCondition is conceptualised as the mostgeneral class capable of capturing diverse physical andbehavioural features that describe health-related features ofPerson. Accordingly, the most general classes of theontology, i.e. the class HealthCondition and the classPerson, are defined together with the most relevant objectand data properties in the Common module (Fig. 4), i.e. thecross-domain TBox composed of the classes and propertiesthat are common to all the domain-specific ontologymodules. While subclasses of Person, i.e. Male andFemale, are defined in the Common module, thesubclasses of HealthCondition are specified in thededicated domain-specific modules (T1-T5 in Fig. 3).Since the characterisation of a phenotype as obese isbased on the assessment of ones physical constitution(weight, height, body mass index, considered in the contextof age and gender), the obesity classification is capturedwithin the PhysicalStatus module which definesfurther the subclasses of HealthCondition as thehierarchy of PhysicalConstitutionConditionFig. 4 Common module classes. Linking of Person and HealthConditintegration of domain-specific modules used for the gender-specific assess(see Fig. 6). Likewise, other domain-specific modules definesubclasses of HealthCondition (Fig. 5) in an onto-sensitive manner. Thus, PhysicalActivityConditionand its subclasses are defined in PhysicalActivityBehaviourmodule, NutritionalCondition classes in Nutritional-Habits module, PhysiologicalCondition classes inPhysiologicalStatus module. The class HealthConditionof the Common module enables the integration of thesub-domain modules via the subsumption relationship(c.f. Figs. 3 and 5). On the other hand, each of the domain-specific modules can be used independently as they consistof domain-specific hierarchies that describe target domainsin a comprehensive and exhaustive manner (see e.g. Fig. 6).Besides the hierarchies, each of the modules specifies dataproperties that define how some obesity-related health con-dition is assessed. In this way, each module explicates for-mally evidence for the assessment of some health conditionas based on the current scientific knowledge.Consider, for instance, the PhysicalStatus module asthe example on which we illustrate formalisation of theevidence-based assessment of health condition. Figure 6presents the hierarchy of the relevant health conditions,specifying the physical constitution that considers adi-posity, body fat distribution, body mass, and centralobesity. Each of the conditions is associated with aspecific classification and linked to the referencevalues that characterise physical features relative togender and age [40]. These classifications are distinctas they are using diverse criteria to describe a condi-tion of body constitution.For instance, the criterion of body mass (provided asbody mass index [40]) in one of the classifications is usedto distinguish people as belonging to one of the followinggroups: obese, underweight, overweight or normal weight[25]. According to the classification that considers fatdistribution, a person may be classified either as androidor as gynoid. Figure 6 depicts the hierarchy of healthcondition subclasses based on the description of bodyconstitution via diverse classificatory criteria. Numbers(1-5) in Fig. 6 associated with the classes stand for theion via the object property isInHealthCondition enables thement of health conditionFig. 5 Extending HealthCondition with the key subclasses. The subclasses of HealthCondition are captured in the domain-specific ontologymodules (O1-O4); the heterogeneous knowledge-domains relevant for the obesity assessment and prevention are integrated via the subsumption relationshipSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 8 of 17classificatory criteria used in PhysicalStatus module tocharacterise the associated condition via the followingdata properties:In other words, central obesity can be assessed by provid-ing information on either (2) or (3) or (4); gynoid andFig. 6 Classifying the obesity-related conditions within the PhysicalStatus mrelevant for the obesity assessment distinguish the classes according to4) Waist To Height Ratio; and 5) Body Fat Massandroid status is assessed based on (2); obesity status isassessed as one of the body constitution classes as based on(1); adiposity status is assessed based on (5).In addition, the classification is annotated with thereference sources and relevant scientific literature pro-viding evidence for the classificatory choices. Finally, theclassification (Fig. 6) captures various types of obesitysub-classifications that are grouped into one (i.e. Physi-calStatus module) because all of them satisfy thecommon criterion of describing phenotype via character-isation of physical constitution.odule. The criteria used to specify the conditions of physical constitution1) Body Mass Index; 2) Waist To Hip Ratio; 3) Waist Circumference;Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 9 of 17Regarding the above mentioned specialisation of themodel to characterise health conditions specifically for thepopulation of teenagers, the inference rules, together withthe reference values, are defined within the domain-specificsets of SWRL rules. The following subsections present howthe ontology is used in practice to personalise, and auto-matically asses, an obesity-related health condition. Thepersonalised inference is achieved by combining OWL-TBoxes and SWRL-RBoxes that specify inference rules,particularly considering the population of teenagers.Combining OWL and SWRL to personalise obesityassessmentSWRL is an expressive DL-based rule language thatallows specification of rules expressed in terms of OWLconcepts while enhancing the deductive reasoningcapabilities [20, 42]. A SWRL rule is structured as a con-ditional, consisting of an antecedent (i.e. body), and aconsequent (i.e. head), as illustrated below with the ex-amples of SWRL rules (see Ax1 - Ax10). SWRL supportsonly the conjunctive form, and it does not support ne-gated atoms or disjunction [20]. The predicate symbolsof a SWRL atom within a rule can include OWL classes,properties or data types. The SWRL arguments can alsobe OWL individuals, data values or variables. In order toface the undecidability that might accompany the highexpressivity of SWRL, we follow the recommendationfor the use of DL-safe SWRL rules (see [42], p. 113).In order to personalise inference about someones healthcondition, the classification of individuals is performed bycombining the defined SWRL rules with the OWL declara-tions that formalise the domain-specific yet generic classifi-cations of health condition and the facts asserted aboutinstances of classes Person and HealthCondition.While the domain-specific modules (see T1-T5 inFig. 3) provide the hierarchies that are utilised in the in-stantiation of health conditions, the Common module(O1) provides the classes used in the instantiation ofbasic personal information, including the link betweenthe person and their health condition. Precisely, theCommon module (O1) specifies Person and Health-Condition as two key OWL classes. These two classesare linked by a restriction involving the object propertyFig. 7 A fragment of the model designed to infer automatically personal hisInHealthCondition (Fig. 7), so that an instanceof Person can be associated with one or more healthconditions. Specification of physical and behavioural fea-tures to characterise directly health condition and onlyindirectly a person (via the relationship isInHealth-Condition) enables the assignment of diverse healthconditions to a person, thus capturing diverse physicaland behavioural features of an individual while trackingthe evolution of the captured health conditions andassociated features over time (the pattern is extrapo-lated from an analogous approach [43]). In order toassess a specific condition it is necessary to declarethe key properties describing a person (see P1) andthe time of the condition assessment (H1) that arespecified in OWLs Manchester syntax [44] as thefollowing restrictions:The inference patterns are modelled further as therules that classify personal health conditions as belong-ing to some of the HealthCondition subclasses (seeFig. 6). In other words, the rules identify which class anassessed condition actually belongs to (based on (P1),(H1), and data that characterise the assessed condition,e.g. BMI specified as 1 in (C1)). The reasoning over theclasses and inference of a certain condition attributed to aspecific person, are performed by means of SWRL rulesand Pellet reasoner [45] that makes use of the assertedealth conditionSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 10 of 17facts about (1) physical (structural) and functional (meta-bolic, etc.) features, (2) gender, and (3) the age of a person.The asserted functional and structural features as wellas age are directly associated with a health condition.For example the body mass index (BMI) defined via thedata property isCharacterizedByBodyMassIndex(see 1 in (C1) and Fig. 6) characterises the health condi-tion of a person that is assessed at specific age (seeisAssessedAtAge in (H1)). In particular, BMI is usedas the criterion to classify people as being in a healthcondition that belongs to one of the BodyMassCondi-tion subclasses (Fig. 6). However, BMI is not sufficientto classify a person as being in ObeseCondition orin OverweightCondition. Associating one of thePhysicalConstitutionCondition subclasses witha person requires assertion of the facts that define age andgender of that person [25, 40].The gender is defined by instantiating a Person asbelonging to one of its subclasses, i.e. classes Maleand Female within the Common module (Fig. 7).The age of a person at the time when the healthcondition is evaluated is crucial information because thereference values for the assessment are particularly vari-able in adolescence when body grows and changes [40]. Inorder to capture this variability that can impact on the as-sessment, the classes Person and HealthConditionare characterised via the object properties and restrictionsspecified in (P1) and (H1), thus enabling the age-specificassessment of health condition.Having the data related to the date of birth and time ofassessment, we can apply a rule modelled in SWRL [20] inorder to get an age-value associated with a personal condi-tion assessment, so that all the needed elaborations can beperformed by a reasoning tool without needing to inter-face with other applications. The rule is specified asThe age calculation rule (Ax0) utilises the SWRL built-insdefined for various numeric types [20]. Such a software-independent age calculation facilitates testing, as shown inthe examples that employ rules to infer specific health con-dition (see Section on the instantiation, Fig. 9).The following axioms (Ax1 - Ax10) exemplify the setsof SWRL rules that are defined according to the domain-specific criteria used to asses some health condition asbased on age and gender. The axioms labelled with the oddsubscripts are specifying inference-rules for the malepopulation, while the even-subscript axioms define therules to classify health conditions associated with female in-dividuals. The examples are just a fragment of the rule setsformalised within the PhysicalStatus RBox (see R1).Generally speaking, whenever the conditions specified inthe antecedent hold, then the conditions specified in theconsequent must also hold [20]. The listed rules are struc-tured to specify in antecedent (body) a variable p that canhave as its extension some of the instances asserted asmembers of the class Person (specified in OWL) withinthe Common module. The variable h should have asits extension members of the class HealthCondition.Age is represented by the variable age (calculated in aseparate rule, Ax0), while gender is specified as a predicate(either Male or Female) associated with p. For instance,Axiom 10 can be interpreted in natural language as aconditional declaring that for any female person of agebetween 13 and 17, who is also in health condition thatis characterised by waist to hip ratio greater or equal0.85, we can infer that the asserted health condition ofthat person is AndroidCondition. In this way therules lead to new knowledge, thus expanding theKnowledge Base with new information that classifieshealth conditions associated with persons based on theinformation describing particular phenotype.Sojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 11 of 17The following subsection presents how the above-introduced classes are instantiated in practice and explainsthe reasoning steps that exploit specific information at thetime of assessment, date of birth, specific characterisationof physical features that are all together used to infer apersonal health condition.The ontology instantiation and testingThe ontology validation was performed on the test casesdesigned to capture diverse profiles of teenagers by instan-tiating classes Person and HealthCondition andassigning data values to the instances in a realistic manneracross the domain- specific modules. This section illus-trates the employment of the ontology in the reasoningover instances by means of an example of the obesity as-sessment that provides an explanation of the above-presented design patterns on the application level.Figure 8 is an extension of Fig. 7 that exemplifies theinstantiation of the classes Person and HealthCondi-tion. In particular, we will focus on the instance represent-ing a boy, named Tom, born in October 2000. By declaringexplicitly Toms birth data, body mass index (BMI) valueand the date of BMI assessment, the ontology enrichedwith the rules (Ax0 and Ax5) automatically infers that thecondition TomCondition1 associated with Tom is anObeseCondition. This fact presents the new informa-tion inferred via ontology which ABox previously containedonly the facts about Toms birth data, gender, and the dataused to characterise his health condition (i.e. BMI).Since the goal of the ontology is to capture changes ofphenotypes associated with a person over time, considerthe scenario in which Tom is assigned two assessmentsof his health condition at two different time points:TomCondition1 (assessed in November 2014), andTomCondition2 (assessed in November 2015) (seeFig. 9). Since the reference value for the assessment of anobese condition changes with age (see the age dependentreference values in Ax1 - Ax10), given that Toms agechanges while his body mass index stays unchanged, onlyTomCondition1 is inferred to be ObeseCondition, whileTomCondition2 is classified as OverWeightCondition.Fig. 8 Depicting the reasoning over instances, e.g. the dashed bright arrowTomCondition1 as an ObeseConditionThe following specification captures this in OWLsManchester syntax [44].The facts resulting from the reasoning can be saved intothe ontology, thus actually enriching the Knowledge Base.(orange) stands for an inferred relationship, automatically classifyingFig. 9 A fragment of the ontology with the example of inference to the personal health conditions at different time points; TomCondition1 isclassified as ObeseCondition while TomCondition2 is OverWeightConditionSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 12 of 17The inferred facts can be safely added to the KnowledgeBase because the addition of new instances of the assessedhealth conditions will not invalidate the previous in-ferences, thanks to the adopted modelling pattern.Moreover, the analysed ontology provides the explanationthat Toms condition TomCondition2 is assessed asOverWeightCondition in November 2015 based onBMI. On the other hand, the ontology explains that TomsObeseCondition is assessed in November 2014 basedon the BMI that characterises TomCondition1.The same reasoning can be performed with persons ofdifferent ages and genders. The domain-specific RBoxesspecify the inference rules with the reference values rele-vant for assessing the obesity-related categories of a per-sons health condition (characterised by measure of waistcircumference, waist to hip ratio etc. as illustrated e.g. inFig. 6) by defining a total of 76 SWRL rules specific for thepopulation of teenagers. Inference to health conditionbased on information about body mass assessment speci-fies 56 rules; the assessment based on body fat distribu-tion consists of 4 rules; the inference on the presence ofcentral obesity is performed via employment of 8 rules,and adiposity condition assessment also specifies 8 rules.Besides the 76 rules used to classify obesity-related behav-ioural, physical, and physiological conditions, several rulesare used to calculate derived parameters, such as bodymass index, age, etc. As a comparison, the work by Scalaet al. [18] contains approx. 40 rules, covering only a frac-tion of the obesity types.The ontology testing is performed by instantiation ofthe classes that capture physical, physiological, and be-havioural features of individual people (see Figs. 3, 5,and 6) and then running the Pellet reasoner [45] toproperly classify the asserted health conditions associ-ated with the instances of persons. Pellet supports infer-ence over the DL-Safe rules and reports on possibleerrors and misuses of SWRL. The ontology editorProtégé [46] was employed in the creation of TBoxesand RBoxes.The ABoxes are managed as separate modules via theOntoGUI tool [47] that, inter alia, supports fast instanti-ation of ontology Tbox (see Fig. 10). The modules (i.e.libraries generated by OntoGUI) can be used afterwardsas inputs for other ontology based applications. Thecontrol panel of OntoGUI enables the creation andloading of ontology modules from a repository (eitherfile-based or triple stores) and provides access toseveral functional modules (see [47]), includingIndividual Manager (Fig. 10) that is a general purposetool for the exploration, generation and char-acterisation of OWL individuals. The user interface ofthe Individual Manager is dynamically reconfiguredwhenever an OWL class is selected. The dynamic reconfig-uration is enhanced with the OntoGUIs capacity to extractFig. 10 The OntoGui Individual Manager tool instantiates classes Person and HealthCondition, storing the facts about them into dedicated librariesSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 13 of 17information related to the following axioms defined in theTbox ontology: Equivalent classes, both defined as single classes orunion of classes; Subclasses, both defined as single classes or union ofclasses; Restrictions of any degree if they involve universalquantifier, existential quantifier, or cardinalityconstraints.Figure 10 presents the case of instantiation of asubclass of the class Person and assertion of the keyfacts about the instance via the OntoGUI IndividualManager. The facts are automatically stored in thededicated ABoxes.Integrating the privacy concerns and modelling tasksThe above-presented ontology can capture physicaland behavioural features of individuals related to theirhealth condition that include sensitive information. Ac-cordingly, the Knowledge Base (KB) [48, 49] architecturewas designed specifically to support management of per-sonal information by means of the ABox modularisation(Figs. 11, 12 and 13). Besides the TBox and RBox modules(Fig. 3), the KB consists of the ABox modules dedicated tothe assertion of facts about instances that represent indi-vidual people and their health status. More specifically,the ABox is partitioned into: HealthCondition ABox containing instances thatrepresent the target health conditions and factsabout them, i.e. criteria used to characterise theassessed condition; Person ABox containing instances that representpersons and basic facts about them, i.e. gender anddate of birth; The Integration ABox that imports Person ABox andHealthCondition ABox and contains assertions ofthe links between the facts stored within theseparate ABoxes.Accordingly, access to the stored data can be managedseparately on each of the three levels. Any assertion ofthe facts about instances requires a concurrent access toboth ABoxes that will only jointly associate some healthstatus with appropriate instances representing persons.The partition of ABoxes is motivated by ethical concernsand it aims to support privacy of the health-related dataalso on the modelling level.Figure 13 depicts the complete KB architecture, includingthe relationships between the ontology modules: RBoxesFig. 11 The modularisation pattern that enables (re)use of a singledomain-specific ontology module (e.g. characterising body constitution);the ABox partitioning supports protection of personal dataSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 14 of 17import the dedicated TBoxes that in turn provide termin-ology for the reasoning rules formalised in RBoxes. ABoxeson the other hand import directly RBoxes and indirectlydomain-specific TBoxes and Common TBox, thus enablingthe merging of sub-domain ontologies into the integratedand populated ontology. Alternatively, the modularstructure enables independent development, employment,re-use, and evolution of the domain-specific segments.For example, Fig. 11 presents only one fragment of theontology presented in this paper (O1), i.e. the ontologyFig. 12 The modularisation pattern that enables (re)use of twodomain-specific ontology modules, O1 and O2, integrating them withthe partitioned ABox that supports protection of personal datathat captures obesity-related knowledge focused on phys-ical constitution (see Fig. 6). If the modelling task changesto include the information related to physical activity andintegrate it with the model that represents physicalconstitution, the relevant modules will be imported andintegrated accordingly (see Fig. 12).Figures 11, 12 and 13 together exemplify a variety ofthe modes to (re)use the ontology-modules, whereparticular modules are employed according to thedemands of the modelling task that might be focusedon data collection, data retrieval, and/or reasoningover some of the obesity-related domains such aspersonal body constitution, physical and nutritionalbehaviour, etc.Nonetheless, independently of the modelling task, themodule-integration patterns conserve the links between the Common module (O6) and Person ABox; between the domain-specific ontology modules(O1-O5) and the HealthCondition ABox.In other words, the domain-specific ontology modules(O1-O5) are used to classify individual health conditions,while the assertions of the facts about the conditions arestored in the corresponding HealthCondition ABox. Theintegrated ABox brings together personal informationand generic obesity-related knowledge (captured asthe OWL classes (T1-T5) and SWRL reasoning rues(R1-R5)) as it stores both asserted and inferred facts,including the links that hold between instances of theclasses Person and HealthCondition.ConclusionsThis paper described the ontology that captures severalobesity-related knowledge-domains: Physical StatusDomain, Physical Activity Behaviour-Domain, PhysiologicalStatus Domain, and Nutritional Habits Domain. Theontology is designed to support flexible use and reuseof captured information, the interoperability betweentechnological devices, the integration of collected informa-tion, and automated inference about personal status overtime. The modular structure is adopted in order to enableindependent development, maintenance, evolution, andvalidation, as well as the integration of diverse domain-specific modules. The modular design enables the use andcombination of the modules according to the needs of aparticular task, while each of the modules can be used sep-arately from others and the ontology can be extended withnew modules that can be added at a later stage.Besides the validation and domain-coverage criteria, themodularisation methodology included the criterion thatdistinguishes disciplinary perspectives, the meta-topic cri-terion, and the integrative-view criterion. In particular, theCommon ontology module was developed to support theFig. 13 The modularisation architecture that integrates multiple domain-ontologies and supports protection of personal information via theABox partitioningSojic et al. Journal of Biomedical Semantics  (2016) 7:12 Page 15 of 17integration and interoperability between the domain-specific modules as well as tracking the evolution of per-sonal health condition over time. The combination of twoformal languages motivated partition of the modules intoTBoxes specified in OWL and RBoxes specified in SWRL,while the ethical concerns motivated partition of the ABoxinto the segments that store separately facts about personsand those about health conditions.In particular, the paper illustrated how health conditionsare associated with the physical constitution (i.e. obesity-related) classes, and are then employed to infer automatic-ally personal health status as age- and gender-dependent.The forthcoming task is to perform the ontology test-ing within a Semantic repository that will be developedand populated with real data (i.e. instances representingadolescents and their phenotypic features) acquiredthrough the pilot studies of the PEGASO project ([23]p. 19). In terms of databases integration and interoperability,the activities will include mappings between theontology and several task-oriented databases developed tostore the data acquired from wearable devices, nutrition-related questionnaires etc. The research related to theexploitation of a Semantic repository (see e.g. [50]) willhave to deal further with the compatibility between theontology and available technological solutions (e.g.Stardog [51]) that add certain modelling constraintsin terms of supported OWL2-profiles [52].Regarding the interoperability and integration with otherontologies, future work will examine possible links andalignments [32, 53] with the relevant phenotype ontologies[54, 55], the reference terminologies and ontologies[5662], as well as the foundational ontologies [6366].Berges et al. Journal of Biomedical Semantics  (2016) 7:60 DOI 10.1186/s13326-016-0104-yRESEARCH Open AccessTrhOnt: building an ontology to assistrehabilitation processesIdoia Berges*, David Antón, Jesús Bermúdez, Alfredo Goñi and Arantza IllarramendiAbstractBackground: One of the current research efforts in the area of biomedicine is the representation of knowledge in astructured way so that reasoning can be performed on it. More precisely, in the field of physiotherapy, informationsuch as the physiotherapy record of a patient or treatment protocols for specific disorders must be adequatelymodeled, because they play a relevant role in the management of the evolutionary recovery process of a patient. Inthis scenario, we introduce TRHONT, an application ontology that can assist physiotherapists in the management ofthe patients evolution via reasoning supported by semantic technology.Methods: The ontology was developed following the NeOn Methodology. It integrates knowledge from ontological(e.g. FMA ontology) and non-ontological resources (e.g. a database of movements, exercises and treatment protocols)as well as additional physiotherapy-related knowledge.Results: We demonstrate how the ontology fulfills the purpose of providing a reference model for the representationof the physiotherapy-related information that is needed for the whole physiotherapy treatment of patients, since theystep for the first time into the physiotherapists office, until they are discharged. More specifically, we present theresults for each of the intended uses of the ontology listed in the document that specifies its requirements, and showhow TRHONT can answer the competency questions defined within that document. Moreover, we detail the mainsteps of the process followed to build the TRHONT ontology in order to facilitate its reproducibility in a similar context.Finally, we show an evaluation of the ontology from different perspectives.Conclusions: TRHONT has achieved the purpose of allowing for a reasoning process that changes over timeaccording to the patients state and performance.Keywords: Ontologies, Knowledge representation, Clinical decision support systems in physiotherapyBackgroundWhenever a patient is treated in a physiotherapy unitsome amount of information is generated, which includesthe clinical data relevant to the current situation of thepatient, as well as information regarding their personalhabits and family history. This information composes thephysiotherapy record of a patient and must be adequatelymodeled in order to be efficiently consulted. Moreover,it is important to recognize achievements of goals inorder to manage the evolutionary recovery process ofthe patient. For that reason, specific protocols must bedefined for specific disorders and customization of exer-cises is usually needed depending on the circumstances*Correspondence: idoia.berges@ehu.eusUniversity of the Basque Country, UPV/EHU, Paseo Manuel de Lardizabal, 1,20018 Donostia-San Sebastián, Spainof each patient. Thus, knowledge about state and contextof patients, disorders, phases of protocols, goals, achieve-ments, and recommended or contraindicated exercisesdepending on the patients state must be properly rep-resented to assist the design of the treatments and tosupport some decisions during their execution.Undoubtedly, information technologies are playing arelevant role in the research and improvement of thehealthcare domain [1]. Proof of this fact is the plethora ofworks that have been published in this area. Since the pur-pose of this paper is to present an ontology for physiother-apy, we will restrict the review of the related literature tothree kinds of works: (1) works which address the develop-ment of ontologies for different areas related to medicineother than physiotherapy; (2) works that focus on the field© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 2 of 21of physiotherapy but use technologies other than ontolo-gies; (3) works where ontologies for physiotherapy arepresented.Many solutions in the field of medicine [25] tendtowards the use of semantic technologies such as ontolo-gies, which can play a relevant role in transforminginformation into knowledge that facilitates the work ofthe physicians. Thus, a great effort has been made onthe development of ontologies that cover medical knowl-edge [6]. One example is the Foundational Model ofAnatomy (FMA) [7], whose current release contains over100,000 classes and properties for the OWL represen-tation of the phenotypic structure of the human body.Ontologies have also been used for Clinical Decision Sup-port (CDS) [8] in health-related fields. In [9] an ontologyfor cardiac intensive care units is introduced, which cap-tures the patients vital parameters and provides expertswith a recommendation regarding the treatment to beadministered. In [10] an ontology-based pervasive health-care solution is presented with the purpose of deliveringe-health services in self care homes, such as recommen-dations about daily physical activities, changes of roomtemperature, etc. due to the residents conditions. Theontology developed in [11] uses both recent informationtaken at the point of care and past patient data stored inelectronic health records to provide support to three clin-ical applications: triage of pediatric abdominal pain, triageof pediatric scrotal pain and postoperative managementof radical prostatectomy. In [12] ontologies are used inthe development of a preoperative assessment system torecommend preoperative tests to patients while in [13] alung cancer ontology for categorizing patients and pro-ducing guideline-based treatment recommendations isdescribed.Considering the field of physiotherapy, software forphysiotherapy and rehabilitationmanaging has been avail-able, on the one hand, as commercial products for someyears [14, 15]. These systems represent the transitionfrom a paper-based storage to standardized electronicrecords, but while they have been specially focused ondata recording and administrative purposes, they do notuse technologies such as CDS that would allow them todeepen in the use of the data gathered for assisting phys-iotherapists in diagnosis, treatment definition and patientmonitoring. On the other hand, there exist proposals suchas Gross et al. [16] that use machine learning techniquesto develop a CDS tool for selecting appropriate rehabili-tation interventions for injured workers; Hawamdeh et al.[17] that use resilient backpropagation artificial neuralnetwork algorithm to accurately predict the rehabilita-tion protocols prescribed by the physicians for patientswith knee osteoarthritis; and finally, Farmer [18], whichpresents a CDS system based on a Bayesian belief networkfor musculoskeletal disorders of the shoulder.However, to the best of our knowledge, only fewphysiotherapy-related works address problems from thepoint of view of semantic technologies. Button et al. [19]present TRAK, an ontology that models information forthe rehabilitation of knee conditions. It aims to standard-ize knee rehabilitation terminology and integrate it withother relevant knowledge sources. Although we acknowl-edge the usefulness of TRAK, we feel that it does nottake advantage of all the capabilities that semantic tech-nologies offer, especially with regard to reasoning, whichwould require greater detail in the definition of concepts.In [20] Dogmus et al. introduce REHABROBO-ONTO,an ontology that represents information about rehabilita-tion robots. This ontology helps in the process of selectingthe right rehabilitation robots for a particular patient ora physical therapy, by means of a web interface. How-ever, this solution does not integrate the description of thepatient report and thus it is just a query tool.In this paper we present TRHONT (TelerehabilitationOntology), an ontology whose goal is to assist physiother-apists in the following daily tasks: Recording and searching information about the itemsthat compose the physiotherapy record of a patient :providing a means to represent information regardingage, symptoms, personal and family history, recoverygoals, results of explorations, etc. in a structured wayallows for reasoning about that information. Noticethat it also facilitates interoperability with ElectronicHealth Record (EHR) data recorded in otherinstitutions which also make use of ontologies [21]. Defining treatment protocols for a specific disorder,by selecting the exercises that can be performed ineach phase of the protocol : a treatment protocol iscomposed usually of different phases that a patientmust go through until their recovery is completed.Each of the phases contains exercises whose difficultylevel is in line with one that the phase requires.Among others, the representation of protocols,phases, and exercises in an ontology allows for areasoning-based selection of suitable exercises foreach phase. Moreover, definitions of new protocols,phases or exercises can be proven consistent bymeans of reasoning. Identifying in which phase of a treatment protocol apatient is at some specific moment: resoning plays arelevant role in the decision making process related tothe evolution of the patient. Thanks to the ontologicaldescription of the state of the patient, and moreprecisely of the results that they have achieved whenperforming the exercises, the patient can be classifiedin one phase of the treatment protocol. Howevernotice that this classification is not final: it will evolvealongside the evolution of the patient in the therapy.Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 3 of 21 Identifying which exercises are most suitable for apatient at some specific moment: given all theinformation that is known about a patient (theircurrent state, their personal history, their age, etc.)the ontology provides a means to identify which ofthe exercises are recommended or contraindicatedfor them at that specific moment. As a result, themost suitable exercises for the patient can bedetected and suggested.The contribution presented in this paper focuses onthe rehabilitation of the glenohumeral joint. Neverthe-less, the general nature of our method makes it repro-ducible to model any other body structure deservingrehabilitation.MethodsIn order to achieve the goal described in the previoussection we implemented an ontology following the NeOnmethodology [22]. The NeOn Methodology frameworkpresents a set of scenarios for building ontologies andontology networks. These scenarios are decomposed intoseveral processes or activities, and can be combined inflexible ways to achieve the expected goal.In our case three of the scenarios proposed by NeOn(scenarios 1, 2 and 4, see Fig. 1) have been combinedto obtain the current version of the ontology, namedTRHONT, which contains over 2400 classes and proper-ties to represent: The physiotherapy record of a patient. Movements, exercises and treatment protocols: Anontology module named KIRESONT (KinectRehabilitation System Ontology) was developed. Thismodule is imported by TRHONT. A description of a selected part of the human body:We focused on the glenohumeral joint and the bodyparts that are related to it. An ontology modulenamed GLENONT (Glenohumeral Ontology) wasdeveloped. This module is also imported by TRHONT. Other relevant information for the physiotherapeuticdomain.Fig. 1 Scenarios of NEON used in the development of TRHONT. Scenarios for building ontologies and ontology networks that were used in thedevelopment of TRHONT. Adapted from [22]Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 4 of 21A detailed account of how each of those scenarios wasapplied is presented next.Scenario 1: from specification to implementationThis scenario is composed of the five core activities tobe performed in the development of any ontology: ontol-ogy requirements specification, scheduling, conceptual-ization, formalization and implementation. Ontology requirements specification: It produces asoutput the Ontology Requirements SpecificationDocument (ORSD), where information such as thepurpose, the scope and the intended uses of theontology is described (Table 1). Special attentionmust be paid to the definition of groups ofcompetency questions, which are the set of questionsthat the ontology must be able to answer. In our case,competency questions related with physiotherapyrecords, body parts and treatment protocols weredefined, as well as some general-purpose competencyquestions that either fall in more than one of thosecategories or do not fall in any of them.Once the ORSD was generated, we performed asearch for candidate knowledge resources. Thesearch was performed following the activities definedin Scenarios 2 and 4 of NeOn, which will beexplained later. The outcome of these activities werethe KIRESONT and GLENONT ontology modules. Scheduling: The selected ontology network life cyclewas the Six-Phase Waterfall, described in [22],because apart from the initiation, design,implementation and maintenance phases that4-phase cycles usually include, it integrates a reusephase and a re-engineering phase. Conceptualization and Formalization: Both activitieswere performed together to obtain a formal model ofthe ontology, where all the classes and properties thatare needed to answer the competency questions weredescribed by means of a Description Logic [23] (seeResults section). Implementation: The formal model wasimplemented in the ontology language OWL 2 DL[24] using Protégé 5.0.0 [25].Scenario 2: reusing and re-engineering non-ontologicalresourcesThis scenario was used to select non-ontologicalresources that represent information related to jointmovements, rehabilitation exercises and treatment pro-tocols for disorders of the shoulder, and convert thatinformation into one ontology. Two processes were car-ried out: reuse and re-engineering. The reuse processcomprises three activities: Search non-ontological resources: Among others, adocument about exercises and treatment protocolsfor rehabilitation after shoulder dislocation from theNational Health Service (NHS) was found [26].Moreover, a database of shoulder movements andexercises from a Kinect-based telerehabilitationsystem [27] was considered, as well as a set oftreatment protocols for several shoulder-relateddisorders provided by expert physiotherapists. Werestrict the description of the remaining activities tothese resources. Assess the set of candidate non-ontological resources:We performed the assessment keeping in mind theintended uses of the target ontology (Table 2). In thecase of resources that contain movements the qualityof their description was assessed (i.e. does themovement indicate the initial and final position?Does it indicate the ROM?). In the case of resourcesthat contain exercises, the quality of the descriptionand the easiness to identify single movements withinthose exercises was evaluated. Finally, concerningresources that contain treatment protocols, we tookinto account the number of disorders that wereconsidered, as well as the existence of phases in thoseprotocols and conditions to classify patients in phases. Select the most appropriate non-ontologicalresources: We selected the database of theKinect-based telerehabilitation system as a resourcefor movements and exercises, due to the richness oftheir descriptions, which provide great informationfor our reasoning purposes. Moreover, we selectedthe pool of treatment protocols provided by expertphysiotherapists since it covers a wide range ofdisorders with definition of phases and theirconditions (Fig. 2).After the reuse process, the re-engineering process wascarried out to obtain an ontology from the gathered infor-mation. Three activities were performed: Non-ontological resource reverse engineering: theresources were analyzed to identify their underlyingcomponents. In the case of movements, their name,type (flexion, extension, internal/external rotation,(horizontal) abduction, (horizontal) adduction),range of motion, plane (frontal, sagittal, transverse),initial/final posture, execution and affected bodylocation were identified. It was also detected that insome cases a single movement is composed of morethan one submovement that take placesimultaneously but with different values for the {type,ROM, location} triplet. In the case of exercises theirname and sequence of movements were considered.As for treatment protocols, their name, relatedBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 5 of 21Table 1 Excerpt of the Ontology Requirements Specification Document defined for our ontology1. PurposeThe purpose of the TrhOnt ontology is to provide a reference model for the representation of the physiotherapy-related informationthat is needed for the whole physiotherapy treatment of a patient, since they step for the first time into the physiotherapists office,until they are discharged.2. ScopeThe ontology will focus on physiotherapy issues related to the glenohumeral joint.3. Implementation languageThe ontology has to be implemented in a formalism that allows classification of classes and realization between instances and classes.4. Intended Users User 1: Physiotherapists5. Intended uses Use 1: To record and search information about the items that compose the physiotherapyrecord of a patient. Use 2: To help the process of defining general treatment protocols for a specific disorder, byselecting the exercises that must be performed in each phase of the protocol. Use 3 To help the process of identifying in which phase of a treatment protocol a patient is atsome specific moment. Use 4: To identify which exercises are most suitable for a patient at some specific momentgiven all the information that it is known about him.6. Ontology requirements(6.a) Non-functional requirements (not applicable)(6.b) Functional requirements: Groups of competency questions CQG1: Physiotherapy record-related competency questions:?CQ1.1: What is the patients age??CQ1.2: Which health issue does the patient report??CQ1.3: Which are the patients recovery goals??CQ1.4: How much pain does the patient report on the Visual Analogue Scale (VAS)??CQ1.5: Which results are obtained from the exploration of the joint movement of thepatient??CQ1.6: What is the physiotherapy diagnostics of the patient??CQ1.7: Which is the family and personal past history of the patient?? . . . CQG2: Body-related competency questions:?CQ2.1: Which are the body parts that compose a more general body part??CQ2.2: Which is the laterality of a specific body part?? . . . CQG3: Treatment protocol-related competency questions:?CQ3.1: Which is the type of a movement??CQ3.2: Which body part does a movement refer to??CQ3.3: Which range of movement does a movement cover??CQ3.4: Which movements compose an exercise??CQ3.5: Which exercises compose a phase of a treatment protocol??CQ3.6: Which are the conditions that an exercise must fulfill to be a candidate exercise fora phase of a treatment protocol??. . .Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 6 of 21Table 1 Excerpt of the Ontology Requirements Specification Document defined for our ontology (Continued) CQG4: General competency questions:?CQ4.1: Which are the conditions that a patient must fulfill in order to be in a phase of atreatment protocol??CQ4.2: Which phase is a patient in??CQ4.3: Which exercises are recommended for a patient at some specific moment??CQ4.4: Which exercises are contraindicated for a patient at some specific moment??CQ4.5: Which exercises do patients usually perform badly?? . . .7. Pre-glossary of termsPatient, goal, joint, movement, exercise,. . .disorder, sequence of phases (which are made up of acollection of exercises), conditions of the phases,number of repetitions of each exercise and number oftimes the whole phase must be repeated in the samesession were identified. Non-ontological resource transformation: Aconceptual model relating the underlyingcomponents identified in the previous activity wasgenerated. Ontology forward engineering: A formal modelexpressed in a Description Logic was generated fromthe conceptual model and later implemented inOWL 2 DL using Protégé (see Results section). Theresulting ontology module, KIRESONT, was theoutcome of this scenario.Scenario 4: reusing and re-engineering ontologicalresourcesThis scenario was used to select ontological resources thatrepresent the glenohumeral joint and related body parts.As in the previous scenario, reuse and re-engineering wereperformed. More specifically, four activities were carriedout in the reuse process: Ontology search: The search for an ontology thatcovered only the glenohumeral joint and its relatedbody parts was unsuccessful, so we expanded thesearch to ontologies that cover the whole humanbody. Two candidate ontologies were selected:OpenGALEN [28] and FMA [7]. Ontology assessment: The assessment wasperformed taking into account five criteria: Coverage,Understandability effort, Integration effort, Reuseeconomic cost and Reliability. We restricted theassessment to the Human Anatomy extension ofOpenGALEN. In the case of FMA, version 4.0 wasassessed (Table 3). Ontology comparison: Both ontologies cover thedomain of the glenohumeral joint to an appropriateextent. Moreover, we think that the hierarchy andnomenclature used in FMA are much clearer thanthose in OpenGALEN, which reduces the expectedman-hours of work and thus the reuse economic cost.Since an implementation of both ontologies in OWLexists, both of them are suitable for OWL reasoners.However FMA includes unsatisfiable classes [29, 30],as opposed to OpenGALEN, although the literaturehas proved that fully satisfiable modules can beobtained from it [31]. Both ontologies are consideredreliable since they were developed by reputableinstitutions and have been used in multiple projectsthroughout the years [3235]. Ontology selection: Given the need of involving aphysiotherapist for pruning the ontology, we optedfor selecting the FMA due to its clarity, alwayskeeping in mind that we would need to check theTable 2 Summarized assessment of candidate non-ontological resourcesNHS document Database Kinect-based system General treatment protocolsMovements: Quality of description   Exercises: Quality of description   Exercises: Easiness to identify movements X  Protocols: Number of disorders 1  10Protocols: Phases   Protocols: Transition conditions X  A tick () indicates that the resource fulfils the requirement, an X that the resource does not fulfill it, and a hyphen () that the requirement does not apply to that resourceBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 7 of 21Fig. 2 Example of movement and treatment protocol. Example of movement and excerpt of treatment protocol from the selected non-ontologicalresourcessatisfiability of the glenohumeral joint module onceextracted.After the reuse process, the re-engineering process wascarried out to obtain the glenohumeral joint module.More precisely, two activities were performed: Ontology re-specification: The scope of the FMAontology (with over 104,000 classes and 170properties) was modified to consider just theglenohumeral joint and its related classes. Ontology re-conceptualization: We pruned theFMA ontology with the help of a module extractor[36, 37] and a physiotherapist to obtain theglenohumeral joint module, used to represent theconcepts about rehabilitation processes of shoulderpathologies. The outcome was the GLENONTontology module (with 2054 classes and 23properties). The module extractor works selectingconcepts that are logically connected to a list ofconcepts passed as an argument. This way weBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 8 of 21Table 3 Summarized assessment of candidate ontological resourcesRequirements OpenGALEN FMACoverage It must cover at least the glenohumeral joint andits related body parts at great detail Understandability effort Pruning supported by a physiotherapist will beneeded to obtain a module about the gleno-humeral joint. Thus the structure of the ontologyin ontology development tools such as Protégémust be easy to understandToo many classes defined atthe top level, it makes it difficultto understand the actual hier-archy. Many classes have verylong names, which are difficultto read.Integration effort It should be easy to integrate the candidateontology with the ontology being developed.Moreover its implementation must adapt to thereasoner being used, and be logically satisfiable.In our case it is sufficient if the glenohumeral jointmodule is satisfiable. It includes unsatisfiable classes,but it is known that satisfiablemodules can be obtained fromit [19, 31]Reuse economic cost It refers to the cost of accessing and using theontology, including licensing costs.30 man-hours. No licensingfees.20 man-hours. No licensingfees.Reliability The candidate ontology should come fromreliable sources obtained a module of classes and propertiescomposed of elements connected between them.In our case we performed an upper hierarchyextraction using GlenoHumeral Joint" as the onlyargument for the extraction process. A conceptselected this way will always be connected withsome other hierarchically or by a property.Then we performed a clean-up process to removethose concepts that were clearly not related withupper limbs (e.g. toe, ankle, pelvis). After that, weapplied another round of the module extractor toremove orphan" terms that might be left after theremoval. Finally, this new module was presentedto a physiotherapist that checked it manually, andvalidated its content removing those terms thatwere considered inadequate for the representationof upper limb pathologies in rehabilitation. Thismodule proved to be free of unsatisfiable classes.We also incorporated UMLS (Unifided MedicalLanguage System [38]) codes for those FMAclasses that had an equivalent class in UMLS. Thisresulted in 272 classes from GLENONT for whichalignment axioms appeared in the UMLSrepository, allowing interoperability with othersources that use this terminology.ResultsWe developed a new application ontology, namedTRHONT, which imports both KIRESONT andGLENONTontology modules, and contains other physiotherapy-related information that will be presented next. Theresulting ontology covers the four intended uses men-tioned in the ORSD (see Scenario 1 in Methodssection),which are related to the competency questions listed inthat same document.Results for intended use 1In this intended use the ontology is regarded as a means torecord and search information about the items that com-pose the physiotherapy record of a patient. It must be ableto answer the competency questions in groups CQG1 andCQG2 (see Table 1).The core class is PhysiotherapyRecord. EachPatient is related to their physiotherapy record(s),which is composed of a set of answers.Patient  ?hasRecord.PhysiotherapyRecordPhysiotherapyRecord  ?hasAnswer.AnswerFor each of the competency questions of CQG1 a repre-sentation of its answer was defined within the physiother-apy record. For example class CA1.4 is used to representthe answer to CQ1.4: How much pain does the patientreport on the Visual Analogue Scale (VAS)?, and includesthe necessary properties (hasVASvalue) to store thepatients response as well as restrictions in its type and/orvalue (double[? 0.0,? 10.0]). When needed, otherclasses related to the terms in the competency questionswere defined to representmore complicated concepts (e.g.MovementExploration).Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 9 of 21CA1.1? Answer  ?hasAge.integer[?0]CA1.4?Answer  ?hasVASvalue.double[?0.0,?10.0]CA1.5? ?hasMovementExploration.MovementExplorationMovementExploration ?hasMovementType.MovementType?hasLocation.Joint  ?hasROMvalue.double  ?hasPain.booleanMovementType? Flexion unionsq Extension unionsq ExtRotationunionsqIntRotation unionsq Abduction unionsq AdductionunionsqHorizAbduction unionsq HorizAdductionCA1.7? ?hasPastHistory.FamilyOrPersonalPastHistoryItemFamilyOrPersonalPastHistoryItem? PathologicalCondition  ?hasPatient.(Self unionsq Relative)  ?hasIntensity.Intensity  ?hasTimespan.TimespanDislocationOfLeftGlenohumeralJoint PathologicalConditionRecorded answers about a specific patient are repre-sented as individuals of classes in the ontology. Hence, theinformation about patient with ID patient2015 seen inFig. 3 is transformed, among others, into the following setof triples:Competency questions in CQG2 can be answered bymeans of the GLENONT part of the ontology that wascreated in Scenario 4. For example, one relevant prop-erty in that ontology is constitutional_part, usedto describe meronymy relationships between body parts.In Fig. 4 we show a snapshot of the class Glenohumer-alJoint. It illustrates the description of this joint and howthe relations with other body parts and anatomical struc-tures are represented (e.g. constitutional_part,constitutional_part_of, nerve_supply).Results for intended use 2In the second intended use the ontology is seen as a meansto help the process of defining general treatment proto-cols for a specific disorder. It should help in the selectionof the exercises that must be performed in each of thephases of the protocol and it must be able to answerthe competency questions in group CQG3 (see Table 1).These requirements are covered by the definitions of theKIRESONT ontology module, which was created fromnon-ontological resources in Scenario 2.Representation ofmovements, exercises and treatmentprotocolsA Movement is represented by its initial and final pos-tures, and is composed of one or more Submovementsthat take place simultaneously within that movement.Simultaneity is needed for movements that occur inmore than one anatomical plane (e.g. diagonals) or whichrequire the movement of two joints at the same time (e.g.both right and left glenohumeral joints).?patient2015 rdf:type Patient??patient2015 hasRecord record2015??record2015 rdf:type PhysiotherapyRecord??record2015 hasAnswer ca1.4??ca1.4 rdf:type CA1.4??ca1.4 hasVASvalue 0.0??record2015 hasAnswer ca1.5??ca1.5 rdf:type CA1.5??ca1.5 hasMovementExploration movexp1??movexp1 rdf:type MovementExploration??movexp1 hasMovementType flexion??movexp1 hasLocation leftGlenoJoint2015??leftGlenoJoint2015 rdf:type GlenohumeralJoint??movexp1 hasROMvalue 80??movexp1 hasPain false??record2015 hasAnswer ca1.7??ca1.7 rdf:type CA1.7??ca1.7 hasPastHistory phi1??phi1 rdf:type DislocationOfLeftGlenohumeralJoint??phi1 hasPatient self?Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 10 of 21Fig. 3 Patient record. Excerpt of the patient record of patient patient2015Movement ? ?hasComponent.SubmovementFor each Submovement its Joint, MovementTypeand ROM are represented, which for example can be usedto answer competency questions CQ3.1 to CQ3.3Submovement  ?hasLocation.Joint ?hasMovementType.MovementType  ?hasROMmin.integer  ?hasROMmax.integerMov2.1.5d and Mov2.2.1z are examples of classesof movements with one and more submovements respec-tively.Mov2.1.5d? Movement  ?hasInitialPosture.value{Arms on the sides}?hasFinalPosture.value{Arm remainsseparated...}?hasComponent.(Submovement  ?hasLocation.GlenohumeralJoint  ?hasMovementType.Abduction  ?hasROMmin.value{0}?hasROMmax.value{90})Mov2.1.5d ?hasName.value{Abduction of theshoulder at 90 degrees}Mov2.2.1z? Movement  ?hasInitialPosture.Fig. 4 Glenohumeral joint. Glenohumeral Joint class description in ProtégéBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 11 of 21value{The initial posture for...}?hasFinalPosture.value{Arm flexedand adducted...}  ?hasComponent.(Submovement  ?hasLocation.GlenohumeralJoint  ?hasMovementType.Flexion  ?hasROMmin.value{0}?hasROMmax.value{180}) ?hasComponent.(Submovement  ?hasLocation.GlenohumeralJoint  ?hasMovementType.Adduction  ?hasROMmin.value{0}?hasROMmax.value{50})  ?hasComponent.(Submovement  ?hasLocation.GlenohumeralJoint  ?hasMovementType.ExtRotation  ?hasROMmin.value{0}?hasROMmax.value{90})Mov2.2.1z ?hasName.value{Diagonal of flexion,adduction and external rotation}An Exercise is represented as a sequence of move-ments. Thus, every exercise must have an initial move-ment, which can be followed by another movement, andso on (This serves to answer CQ3.4). For example, in thecase of Exer2.1.5d, this exercise is composed of twomovements. The initial movement belongs to the classMov2.1.5d, while the second one belongs to the classMov2.1.5d_inv.Exercise ? ?hasMovement.MovementExer2.1.5d ? Exercise  ?hasMovement.(Mov2.1.5d  ?hasMovNum.value{1})  ?hasMovement.(Mov2.1.5d_inv  ?hasMovNum.value{2})  =2 hasMovement.MovementA treatment protocol is represented as a sequenceof phases. Each phase contains a sequence of exer-cises to be performed during that phase, as well asthe conditions that indicate when a patient is in thatphase. Actually, those conditions are the key for theconceptualization. They specify the Range Of Motion(ROM) that patients may achieve and the pain they mayreport during the performance of the exercises. Next,the representation of the treatment protocol for lim-ited flexion of the glenohumeral joint shown in Fig. 2 ispresented:TreatmentProtFlexGlenoJ? TreatmentProtocol  ?hasPhase.(Phase1FlexGlenoJ  ?hasPhaseNum.value{1})  ?hasPhase.(Phase2FlexGlenoJ  ?hasPhaseNum.value{2})  ...?hasPhase.(Phase5FlexGlenoJ?hasPhaseNum.value{5})Phase2FlexGlenoJ? Phase?hasExercise.(Exer2.1.1a ?hasExerNum.value{1})  ?hasExercise.(Exer2.1.1b?hasExerNum.value{2})  ...?hasExercise.(Exer2.1.6a  ?hasExerNum.value{15})  ?hasSeries.value{4}?hasConditions.Cond2FlexGlenoJCond2FlexGlenoJ? ?ROMFlex.double[<90.0]  ?ROMExt.double[<25.0]  ?ROMAbdu.double[<90.0]  ?ROMAddu.double[<27.0]?ROMIntRot.double[<45.0]  ?ROMExtRot.double[<55.0]  ?hasVASvalue.double[<3.0]It should be noticed that the set of classes ofmovements,exercises and protocols in KIRESONT can be extended byphysiotherapists. Currently we are developing a graphi-cal tool for this purpose (see Context of use of TrhOnt inDiscussion section).Selection of the exercises to be performed during a phaseWhenever a physiotherapist creates a general treatmentprotocol, they can rely on ontology-based reasoning toselect the exercises for each phase. Once the num-ber of phases of the protocol has been defined along-side the patient assessment conditions of each phase,new class descriptions capturing the notion of candi-date exercises for each phase are automatically gener-ated and included in the ontology. For example, classCandExe2FlexGlenoJ describes the candidate exer-cises for phase 2 of the protocol for patients withlimited flexion of the glenohumeral joint. A candidateexercise for this phase must be composed of at leastone movement that is allowed in this phase, and moreimportantly, all its movements must also be allowed inthis phase.Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 12 of 21CandExe2FlexGlenoJ? Exercise  ?hasMovement.Phase2FlexGlenoJAllowedMov?hasMovement.Phase2FlexGlenoJAllowedMovPhase2FlexGlenoJAllowedMov? Movement  (MovAbduGJLessEqual90unionsqMovAdduGJLessEqual27unionsqMovExtGJLessEqual25unionsqMovExtRotGJLessEqual55unionsqMovFlexGJLessEqual90unionsqMovIntRotGJLessEqual45)A movement is allowed in a phase if it complieswith the conditions of the phase. As can be seen inthe definition of CandExe2FlexGlenoJ, the move-ments allowed in phase 2 of the protocol for the lim-ited flexion of the glenohumeral join must belong to theclasses MovAbduGJLessEqual90, MovAdduGJLessEqual27, MovExtGJLessEqual25, MovExtRotGJLessEqual55, MovFlexGJLessEqual90 or MovIntRotGJLessEqual45. For example, MovFlexGJLessEqual90 represents those movements of flexion of theglenohumeral joint with a ROM lower or equal to 90°.MovFlexGJLessEqual90? Movement  ?hasComponent.(Submovement ?hasLocation.GlenohumeralJoint?hasMovementType.Flexion  ?hasROMmax.double[?90.0])Specific movements (e.g. Mov2.1.5d, Mov2.2.1z)are properly classified as subclasses of these sort of classdescriptions (e.g. Mov2.1.5d is classified as subclassof MovAbduGJLessEqual90), and moreover, exercisesget classified as subclasses of the corresponding candi-date classes (e.g CandExe2FlexGlenoJ), depending onthe movements they include. More precisely, any exerciseclass that only contains movements that are subclassesof Phase2FlexGlenoJAllowedMov is classified as asubclass of CandExe2FlexGlenoJ, and will be pre-sented to the physiotherapist on demand of exercisesfor phase 2 of the selected protocol. This happens, forinstance, with Exer2.1.5d.If they decide to select that exercise class for the proto-col definition, the following new axiom is created:Exer2.1.5d  Exe2FlexGlenoJNow, Exer2.1.5d will not only be a subclass ofthe class for representing candidate exercises for phase2 (CandExe2FlexGlenoJ), but also a subclass ofthe class for representing proper exercises for phase 2(Exe2FlexGlenoJ).Classes for representing candidate exercises for otherphases are defined likewise:CandExe3FlexGlenoJ? Exercise  ?hasMovement.Phase3FlexGlenoJAllowedMov?hasMovement.Phase3FlexGlenoJAllowedMovPhase3FlexGlenoJAllowedMov? Movement (MovAbduGJLessEqual144unionsqMovAdduGJLessEqual36unionsqMovExtGJLessEqual40unionsqMovExtRotGJLessEqual88unionsqMovFlexGJLessEqual144unionsqMovIntRotGJLessEqual72unionsqMovHorAbduGJLessEqual32unionsqMovHorAdduGJLessEqual112)Notice thatone of the classes (CandExe3FlexGlenoJ)subsumes the other (CandExe2FlexGlenoJ), meaningthat all the exercises classified in CandExe2FlexGlenoJare also members of CandExe3FlexGlenoJ. Thisis considered conceptually correct by physiotherapists,because at any point they should be able to select milderexercises, in order, for example, to warm the joint up.Results for intended use 3The third intended use gives response to some of thecompetency questions defined in CQG4 (see Table 1).The ontology is used as an artifact to help the processof identifying in which phase of a treatment protocola patient is at some specific moment. This is done bytaking into account the results of the movement capa-bility explorations of the patient at that time. Analo-gously to the previous intended use 2, the classificationis guided by the conditions specified in the phases of theprotocols. In this case, conditions regarding the ROMand the pain are considered. Then, one ontology classis automatically created for each phase of each proto-col based on the associated conditions. For example,the definitions of the classes Patient2FlexGlenoJand Patient3FlexGlenoJ that can be seen next rep-resent those patients who are respectively in phase 2and 3 of the protocol to treat the limited flexion ofthe shoulder.Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 13 of 21Patient2FlexGlenoJ? Patient  ?hasRecord.(PhysiotherapyRecord  ?hasAnswer.(CA1.4  ?hasVASvalue.double[<3.0])?hasAnswer.(CA1.5?hasMovementExploration.(MovExploFlexGJLessThan90unionsqMovExploExtGJLessThan25unionsqMovExploAbduGJLessThan90unionsqMovExploAdduGJLessThan27unionsqMovExploIntRotGJLessThan45unionsqMovExploExtRotGJLessThan55)))Patient3FlexGlenoJ? Patient  ?hasRecord.(PhysiotherapyRecord  ?hasAnswer.(CA1.5  ?hasMovementExploration.((MovExploFlexGJBetween90And143unionsqMovExploExtGJBetween25And39unionsqMovExploAbduGJBetween90And143unionsqMovExploAdduGJBetween27And35unionsqMovExploIntRotGJBetween45And71unionsqMovExploExtRotGJBetween55And87unionsqMovExploHorAbduGJLessThan32unionsqMovExploHorAdduGJLessThan112)?hasPain.value{false})))Definitions of the classes with the prefix MovExplo*refer to one type of movement exploration asses-sed in a patient. For instance the definition ofMovExploFlexGJLessThan90 describes an explo-ration of the flexion of the shoulder where the ROMachieved by the patient is below 90°.MovExploFlexGJLessThan90? MovementExploration  ?hasLocation.GlenohumeralJoint  ?hasMovementType.Flexion  ?hasROMmax.double[<90.0]The other explorations are defined likewise. Thus,whenever a patient presents a movement exploration thatsatisfies the definition of any of the MovExplo* classes inPatient2FlexGlenoJ and reports a VAS value lowerthan 3.0, the patient will be classified as belonging to theclass Patient2FlexGlenoJ.For instance, considering the set of the triples aboutpatient patient2015 presented in Results for intendeduse 1 section, patient patient2015 would be clas-sified as a Patient2FlexGlenoJ, because they havereported a VAS value of 0.0 (<3.0) and there exists in theircurrent physiotherapy record a movement exploration offlexion of the glenohumeral joint where they achieveda ROM of 80° (which satisfies conditions of the classMovExploFlexGJLessThan90). Notice that the clas-sification of the patient evolves alongside their evolutionin the therapy: if after being in phase 2 and performingthe exercises recommended for that phase the aforemen-tioned ROM increases to 100° and the patient reports nopain when performing those exercises, some triple asser-tions are deleted and some others added. As a result, thepatient would no longer be classified as a patient of phase2, but as a patient of phase 3 (see previous definition forPatient3FlexGlenoJ).Results for intended use 4In the last intended use, the ontology is regarded as ameans to identify which exercises are most suitable for apatient at some specific moment given all the informationthat is known about them. Three cases are considered: Recommended exercises due to the physical state ofthe patient: This is done by taking into account theresults of the movement explorations of the patient atthat time. For example, if the movement explorationsof patient2015 indicate that they are in phase 2(intended use 3) then they have as recommendedexercises those for the patients in phase 2 (that groupof patients is represented by classPatient2FlexGlenoJ). Then, the followingaxiom represents that knowledge:Patient2FlexGlenoJ  ?recommended.Exer2FlexGlenoJNotice that the exercises of a certain phase wereinferred as shown in the intended use 2. Recommended/Contraindicated exercises due togeneral physiotherapy knowledge: Some domainspecific axioms have been added to the ontology torepresent general physiotherapy knowledge such asPeople with a personal past history of dislocation ofglenohumeral joint should not perform exercises thatcontain abduction movements with a ROM greaterthan 80° (e.g. class axioms for the left glenohumeraljoint are shown in the following).PatientPastDislocationLeftGlenoJ? Patient  ?hasRecord.(PhysiotherapyRecord?hasAnswer.(CA1.7?hasPastHistory.(DislocationOfLeftGlenoJ ?hasPatient.Self)))PatientPastDislocationLeftGlenoJ ?contraindicated.ExerAbduLeftGlenoJGreaterThan80Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 14 of 21 Recommended/Contraindicated exercises for aspecific patient: The physiotherapist can specify atany time that an exercise is recommended/contraindicated for a specific patient. For examplepatient2015 should not perform exercises thatcontain extension movements.Patient2015 ? {patient2015}Patient2015  ?contraindicated.ExerExtensionExerExtension ? Exercise  ?hasMovement.MovExtensionMovExtension ? Movement  ?hasComponent.(Submovement?hasMovementType.Extension)Object properties recommended and contraindicatedhave been created to represent this knowledge.The most suitable exercises for a patient p will berepresented by the named classes Xp such that Xp ?RecommendedFor(p) but Xp ? ContraindicatedFor(p)whereRecommendedFor(p) = {Z| namedClass(Z) ??Y ?C(namedClass(Y ) ?namedClass(C) ? Z  Y ?p?C ? C  ?recommended.Y )}ContraindicatedFor(p) = {Z| namedClass(Z) ??Y ?C(namedClass(Y ) ?namedClass(C) ? Z  Y ?p?C ?C?contraindicated.Y )}EvaluationIn this section we present a threefold evaluation of ourontology. First, we show the results of checking our ontol-ogy using the OntOlogy Pitfall Scanner OOPS! [39] inorder to diagnose potential design errors. Then, an eval-uation of the ontology using criteria related to ontologyquality is presented. Finally, a list of several ontologymetrics regarding its size and composition is shown.Detection of potential pitfallsOOPS! evaluates an ontology against a catalogue of 41potential pitfalls classified in three levels (critical, impor-tant, minor). We performed an evaluation of our ontol-ogy and corrected the reported pitfalls. As a result, weobtained the fixed current version of the ontology. How-ever we feel the need to introduce here some of the pitfallsthat were related to the GLENONT module, because thesepiftalls are also present in FMA ontology. Tables 4 and 5respectively present the critical and important pitfalls ini-tially reported by OOPS!. For each pitfall we indicate itscode, description, where in the ontology it appears, thereason why the pitfall is flagged and other useful informa-tion that is needed to understand it, its implications andhow we corrected it.Ontology qualityNext we evaluate our ontology against several quality cri-teria described in [40], which are presented as part of acommon framework for aspects of ontology evaluation.Accuracy: The definitions and descriptions in the ontol-ogy agree with the experts knowledge about thedomain. The GLENONT ontology module wasobtained from the well-known FMA ontology. TheKIRESONT module and the information regardingpatients were developed using actual physiotherapyrecords and recovery protocols, and with the supportof physiotherapists.Adaptability: We have opted for implementing theontology in several modules that are related to eachother via import clauses. The file GlenOnt.owl1contains the GLENONT ontology module. TheKIRESONT ontology module has been divided intotwo files: KiReSOntFM.owl2, which contains genericclasses and properties for describing movements,exercises and protocols, and KiReSOnt.owl3, whichcontains the descriptions of specific movements,exercises and protocols (e.g. Mov2.1.5d). Finally,the main file TrhOnt.owl4, incorporates the patientrecord, general axioms about physiotherapy and therelations to the other files. This choice enhancesextensibility and reusability, and makes the ontologybe easily adaptable to several contexts. Moreover, weprovide a merged file5 with all the resources.Clarity: All the terms in the ontology have beengiven a non-ambiguous label or description usingrdfs:label or rdfs:comment, so that theontology communicates effectively the intendedmeaning of those terms. For example, class CA1.4has been described as Answer to the question Howmuch pain does the patient report on the Visual Ana-logue Scale (VAS)?, while class Mov2.1.5e hasbeen described as Movement of abduction of theshoulder at 90 degrees.Completeness: This feature measures whether the ontol-ogy can answer all the questions that it should be ableto answer, that is, how well the ontology representsthe domain it models. Those questions were speci-fied in the ORSD and it has been checked carefullythat all of them can be answered.Computational efficiency: It must be admitted that theGLENONT module as a whole is still too big forsome of our purposes. Current DL reasoners are notable to handle it in what we consider reasonableBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 15 of 21Table 4 Critical pitfallsCode P28: Defining wrong symmetric relationshipsDescription A relationship is defined as symmetric when the relationship is notnecessarily symmetric.Appears in SymmetricProperty(continuous_with)Reason the domain of continuous_with is different from the range ofcontinuous_with (Material Anatomical Entity vs.Physical Anatomical Entity).Other useful information subClassOf(Material Anatomical Entity,Physical Anatomical Entity)Implications Let Material Anatomical Entity(x), PhysicalAnatomical Entity(y), continuous_with(x,y). Due toSymmetricProperty(continuous_with), the reasoner infersthat Physical Anatomical Entity(x) and MaterialAnatomical Entity(y)Correction Change the domain of continuous_with to MaterialAnatomical Entity.Code P05: Defining wrong inverse relationshipsDescription Two relationships are defined as inverse relationships when they are notnecessarily inverse.Appears in inverseOf(continuous_with,continuous_with)Reason the domain of continuous_with is different from the range ofcontinuous_with (Material Anatomical Entity vs.Physical Anatomical Entity).Implications Let Material Anatomical Entity(x), PhysicalAnatomical Entity(y), continuous_with(x,y). Dueto inverseOf(continuous_with,continuous_with), thereasoner infers that Physical Anatomical Entity(x) andMaterial Anatomical Entity(y)Correction This pitfall corrects itself as a result of correcting pitfall P25 (see Table 5)time6. However, we must distinguish two uses ofGLENONT: when the physiotherapist is defining newmovements, exercises or protocols they would havethe whole GLENONT module at their disposal, sothat they can choose from awide range of body parts,because in this case the purpose of GLENONT isannotation and not reasoning. Thus, computationalefficiency will not be an issue in this case. Oncethe definitions have been made, the handful of theclasses of GLENONT that are used within them canbe used as seeds in the module extractor in orderto obtain on the spot a lighter module to be usedin those moments where reasoning is necessary (e.gwhen asking for the exercises that are recommendedfor a patient). We are currently working on a toolthat given a set of treatment protocols, automaticallyreduces the number of terms in the GLENONT mod-ule by using the module extractor and the terms usedto describe those protocols, in such a way that nosemantic loss is involved.Conciseness: Since the development of the ontology wasmade with the help of physiotherapists, we asumethat the ontology does not contain irrelevant termswith regard to the domain that is being covered.Moveover, checking our ontology with OOPS! hasdiscarded the presence of redundant terms (see pit-fall P30 in Table 5).Consistency: Reasoning was performed on the ontologyusing Fact++. No inconsistencies were found.Ontology metricsIn Table 6 a summary of ontology metrics obtained fromthe Protégé development framework can be found. Thesemetrics are related to the size of the ontology and itscomponents.DiscussionTRHONT is an application ontology that can assist physio-therapists in themanagement of the patients evolution viareasoning supported by semantic technology. We can findin the literature many ontologies (e.g. [4244]) that havebeen built with the purpose of supporting a precise andcomprehensive semantic annotation of resources. How-ever, TRHONT goes an step further and it also provides aframework where the reasoning process takes a relevantrole.TRHONT contains terms from the well-known FMAontology, that covers the whole anatomical structure ofBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 16 of 21Table 5 Important pitfallsCode P11: Missing domain or range in propertiesDescription Object and/or datatype properties without domain or range (or none of them) areincluded in the ontology.Appears in For example: http://purl.org/sig/ont/fma/partReason All the cases refer to meronimy relations that can be applied to any of the classes of theontology.Other usefulinformationsubClassOf(Physical Anatomical Entity, MaterialAnatomical Entity)Implications None.Correction We chose not to change anything, since it is not an error per se, just an implication of thecurrent domain.Code P25: Defining a relationship as inverse to itself.Description A relationship is defined as inverse of itself.Appears in inverseOf(continuous_with, continuous_with),inverseOf(articulates_with, articulates_with)Reason This relationship could have been defined as owl:SymmetricProperty instead.Correction Remove both inverseOf axioms. SymmetricProperty(continuous_with)and SymmetricProperty(articulates_with) already existed in the ontology.Code P26: Defining inverse relationships for a symmetric one.Description A symmetric object property is defined as inverse of another object property.Appears in inverseOf(continuous_with, continuous_with),SymmetricProperty(continuous_with), inverseOf(articulates_with, articulates_with), SymmetricProperty(articulates_with)Correction This pitfall corrects itself as a result of correcting pitfall P25.Code P24: Using recursive definitions.Description An ontology element is used in its own definition.Appears in continuous_with, articulates_with,Frontal part of headOther usefulinformationFrontal part of head ? attributed_part.(?(related_part.Frontal part of head)  (1 partition.{Partition}))Correction The problems concerning continuous_with and articulates_with correctthemselves as a result of correcting pitfall P25. Moreover, we feel that the aforementionedaxiom involving Frontal part of head is correct, so we chose not to change it.Code P34: Untyped class.Description An ontology element is used as a class without having been explicitly declared as suchusing the primitives owl:Class or rdfs:Class.Appears in Anatomical entityCorrection owl:Class(Anatomical entity) added to the ontology.Code P30: Equivalent classes not explicitly declared.Description Missing the definition of equivalent classes (owl:equivalentClass) in case ofduplicated concepts.Appears in Cheek vs. Face, Ear vs. Pinna, Mouth vs. Lip, Limb vs. ArmReason The names of both classes appear in a common synset (set of synonyms) in WordNet [45].Correction None. We checked each of the suggestions by looking up in WordNet the synsets whereeach pair appears. For example, Cheek and Face appear in a synset with terms such asBoldness, Nerve and Brass, refering to impudent aggressiveness, not the body part, which isthe meaning intended in the ontology. Same applies to the other pairs. Thus, we did notchange anything in the ontologyBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 17 of 21Table 6 Ontology metricsMetricsAxiom 28181Logical axiom count 6161Class count 2351Object property count 65Data property count 35Individual count 134DL expressivity ALCROIQ(D)Class axiomsSubClassOf 4982EquivalentClasses 216DisjointClasses 617GCI count 0Hidden GCI count 199Object property axiomsSubObjectPropertyOf 7EquivalentObjectProperties 0InverseObjectProperties 5DisjointObjectProperties 0FunctionalObjectProperty 0InverseFunctionalObjectProperty 0TransitiveObjectProperty 0SymmetricObjectProperty 2AsymmetricObjectProperty 0ReflexiveObjectProperties 0IrreflexiveObjectProperty 0ObjectPropertyDomain 49ObjectPropertyRange 52SubPropertyChainOf 3Data property axiomsSubDataPropertyOf 0EquivalentDataProperties 0DisjointDataProperties 0FunctionalDataProperty 9DataPropertyDomain 29DataPropertyRange 35Individual axiomsClassAssertion 143ObjectPropertyAssertion 10DataPropertyAssertion 2NegativeObjectPropertyAssertion 0NegativeDataPropertyAssertion 0SameIndividual 0DifferentIndividuals 0Annotation axiomsAnnotationAssertion 19402AnnotationPropertyDomain 0AnnotationPropertyRangeOf 0the human body, as well as terms that describe the phys-iotherapy records of patients, and also movements ofbody parts, exercises for physiotherapy and treatmentprotocols. The selection of terms was made with thesupport of physiotherapists, and their descriptions turnTRHONT into an actionable tool for physioterapists intheir daily work. Moreover, UMLS codes have been alsoincorporated to some terms imported from the FMAontology, favouring in this way interoperability with othersources that use this terminology. TRHONT is still inactive development, and we expect it to grow consider-ably, in the area of physiotherapy; however, in the currentstate it contains enough terms for supporting open contri-butions from professionals desiring to populate the ontol-ogy with more therapy elements. That is to say, TRHONTis ready to be used.Next, some of the decisions made during the develop-ment of TRHONT are presented. Moreover, the scope andcontext of use of the ontology are also discussed.Decisions in the development of TRHONTDuring the development process, different alternativeshave been considered, and we present in the followingsome of the choices we made:Representation of the physiotherapy recordThe core class in the representation of the physiother-apy record of a patient is PhysiotherapyRecord. Thisclass is related to Answer, whose subclasses (e.g CA1.1)represent the answers to competency questions aboutthe physiotherapy record (see Results for intended use1 section). This representation facilitates the extensibilityof the model, since the addition of new competency ques-tions related to the physiotherapy record will not interferewith the current ones.Storage of patient informationAs we indicated in Results for intended use 1 section,recorded answers about a specific patient are representedas individuals of classes in the ontology. However, havingall the information about all the patients always stored inthe assertional box of the ontology would take an unnec-essary toll in the efficiency of any EHR system that usesthe ontology. Thus, we envisage a use of the ontologywhere the patient information and the terminology partof the ontology are kept separate. Whenever a physiother-apist is treating a patient, only the information of thatpatient is loaded onto the ontology, and then dischargedwhen the interaction is over. Moreover, specific classessuch as Patient2015 (see Results for intended use4 section) are also temporary. They will be created whenreasoning about the recommended/contraindicated exer-cises for a patient and deleted once the reasoning processis over.Berges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 18 of 21Representation ofmovements, exercises and treatmentprotocolsIn order to select the right descriptions of those core termsin the physioterapy treatments, we collaborated closelywith physiotherapists. The idea was to get descriptionsthat mimic their conception of the elements related tothe treatments. Therefore, a movement is representedby its initial and final postures, and is composed ofone or more submovements that take place simultane-ously within that movement. An exercise is representedas a sequence of movements, and a treatment proto-col is represented as a sequence of phases. Each phasecontains a sequence of exercises to be performed dur-ing that phase, as well as the conditions that indicatewhen a patient is in that phase. The sequential nature ofthe descriptions allows checking proper concatenation ofthem.Scope of TRHONTAlthough in this paper we have focused on the rehabilita-tion of the glenohumeral joint, the exposedmethod can beeasily reproduced in order to cover other body structuressubject to rehabilitation. The same module extractor thatwas used as a basis in the creation of GLENONT can beused to perform hierarchical extractions of concepts usingother body structures as argument for the extraction pro-cess. Other parts of TRHONT (e.g. the patient record) willnot be affected by a change in the selected body structure.The scaffolding of the phases, exercises and movementscan be reused. Specific information to that body structurewill have to be added (e.g. if instead of the glenohumeraljoint the ankle is selected, then specific exercises for therecovery of the ankle will have to be defined).Context of use of TRHONTTRHONT is a nuclear part of a telerehabilitation sys-tem called KiReS, a Kinect-based system which covers,on the one hand, the needs of physiotherapists in theprocess of creating, designing, managing and assigningphysiotherapy treatment protocols, as well as evaluatingthe performance of patients in those protocols; and, onthe other hand, the needs of the patients, by providingthem an intuitive and encouraging interface for perform-ing exercises, which also gives useful feedback to enhancethe rehabilitation process.Most technical details related to the creation of classesand the behavior of TRHONT are encapsulated in KiReS.Its interface handles aspects concerning, among others,the creation of exercises and the design of protocols. Inorder to be able to design adequate protocols for thepatients, first of all, exercises must be created in KiReS.Postures and movements, basic components of the exer-cises, are recorded by being performed in front of Kinect.Later, they are combined to create exercises. The inter-face provides step by step assistance in this process.Next, we present two snapshots that show respectively theappearance of the interface for recording movements andcreating exercises.In Fig. 5 the interface for recording new movements isshown. The definition of a movement requires, at least,to assign a name that identifies the movement and toselect the initial and final postures that the movement willFig. 5Movement recording. Interface for recording new movements in KiReSBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 19 of 21have. Then, the physiotherapist can visualize in the inter-face two avatars that show these postures and proceed torecord the transition between the postures that best mim-ics the optimal execution of the movement. After reachingthe final posture, a recording player tool is available andthe physiotherapists can replay the movement and decidewhether to store it or not.Moreover, exercises are created by a combination ofone or more movements. The only restriction when com-bining movements is that the final posture of a move-ment must match the initial posture of the next one. Theinterface for creating exercises (see Fig. 6) allows physio-therapists to define the composition of exercises. In theleft side of the interface the name and description of theexercise can be filled. The right side of the interface isdivided into three areas. In the top area physiotherapistscan restrict the search for movements by indicating cer-tain conditions about them, such as the type of movement,the specific joint or the ROM. The search is performedover the list of movements stored in the system and bymeans of DL Queries that are transparent to the users.The area in the middle shows the results of the search(e.g. Mov2.1.5a, Mov2.1.5b, ...). Finally, the last areashows the movements selected by physiotherapists (e.g.Mov2.1.5d). When selecting a movement the systemchecks whether the final posture of the previous move-mentmatches the initial of the new one. If posturesmatch,then the movement is added to the exercise. Once this isdone the exercise will be stored in the system and will beavailable to be added to a treatment protocol.Currently we are developing a new functionality forKiReS that will allow physiotherapists to create theirown treatment protocols for their patients, guided by theontology, using the exercises that have been stored withthe aforementioned interfaces.Movements, exercises and treatment protocols createdin this way are represented internally as classes of theTRHONT ontology. This representation is generated auto-matically at the same time that the physiotherapists createthem with KiReS.Furthermore, once treatment protocols are assigned topatients, those patients are monitored at the same timethey are performing the exercises for each phase of treat-ment (see Fig. 7. More technical details in [27]). Allcaptured data are recorded in the KiReS database andasserted as facts in the TRHONT ontology. Due to a rea-soning process (see Results for intended use 3 section),the physiotherapists can see if the patients have overcomeeach phase of the recommended treatment and thereforedecide whether to end the rehabilitation process or toassign new exercises to the patients.ConclusionsSemantic technologies have been widely used in severalmedical fields in order to facilitate the work of physicians.In this paper we have presented an ontology for phys-iotherapists from two different perspectives. On the onehand, from the point of view of its creation, by showinghow it has been created by integrating information fromdifferent resources: pre-existing ontologies, databases ofmovements, exercises and treatment protocols, expertsknowledge, patient records, etc. On the other hand, fromthe perspective of its usage and the relevant informa-tion that it provides for the physiotherapists via reasoningFig. 6 Creation of exercises. Interface for creating new exercises in KiReSBerges et al. Journal of Biomedical Semantics  (2016) 7:60 Page 20 of 21Fig. 7 Execution of exercises. Inferface for patients when executing exercises in KiReSprocesses. This information includes recommended exer-cises depending on the physical state of the patient, thepatients past history, etc. That is, information that canimprove rehabilitation processes.In summary, TRHONT conceptualizes medical knowl-edge (in order to deal with aspects related to physiother-apy), process knowledge (in order to describe treatmentprotocols), and instrumental knowledge (in order todescribe treatment elements such as exercises). It has beendesigned to assist physiotherapists in several daily taskssuch as recording and searching information in the phys-iotherapy record of a patient, defining treatment protocolsby selecting suitable exercises for each phase of a proto-col, determining the current state of a patient and showingtheir evolution, by identifying which phase of a protocolthe patient is in, and detecting which exercises are mostsuitable for a patient at some specific moment taking intoaccount all the information that it is known about them.TRHONT has been developed in such a way that it can beeasily extended, e.g. by adding new competency questionsto the physiotherapy record of the patient, or by addingnew exercises or treatment protocols. Moreover, it is alsoreusable, since it has been implemented in several mod-ules that could be used for other purposes. The work hasbeen completed with a threefold evaluation of the ontol-ogy centered on piftfall detection, quality assessment andontology metrics.Endnotes1Available from http://bdi.si.ehu.es/bdi/ontologies/GlenOnt.2Available from http://bdi.si.ehu.es/bdi/ontologies/KiReSOntFM.3Available from http://bdi.si.ehu.es/bdi/ontologies/KiReSOnt.4Available from http://bdi.si.ehu.es/bdi/ontologies/TrhOnt.5Available from http://bdi.si.ehu.es/bdi/ontologies/MergedTrhOnt.6More precisely, it took the FaCT++ reasoner [41] about1.5 min to classify TRHONT in an Intel(R) Core(TM) i7-4610M CPU @ 3.00GHz with 8GB of RAM when theGLENONT module was used as a whole.AcknowledgementsAuthors thank Dr. Jon Torres and Dr. Jesús Seco for their help with thephysiotherapy-related aspects. Authors thank Dr. María Poveda-Villalón for herhelp with OOPS!. This work was supported by the Spanish Ministry ofEconomy and Competitiveness [grant number FEDER/TIN2013-46238-C4-1-R]and by the Basque Country Government [grant number IT797-13].Authors contributionsAll authors participated in the definition of the process and in the discussionof relevant aspects. DA and IB perfomed the analysis of ontological andnon-ontological resources. The extraction of the GLENONT ontology modulewas made by DA and AG. IB and JB designed the ontology. IB was in charge ofthe implementation of the ontology. DA, IB, JB and AI wrote the manuscript.All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 28 June 2016 Accepted: 20 September 2016Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 DOI 10.1186/s13326-016-0105-xRESEARCH Open AccessDione: An OWL representation ofICD-10-CM for classifying patients diseasesMaría del Mar Roldán-García1,2*, María Jesús García-Godoy1,2 and José F. Aldana-Montes1,2AbstractBackground: Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) has been designed as standardclinical terminology for annotating Electronic Health Records (EHRs). EHRs textual information is used to classifypatients diseases into an International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM)category (usually by an expert). Improving the accuracy of classification is the main purpose of using ontologies andOWL representations at the core of classification systems. In the last few years some ontologies and OWLrepresentations for representing ICD-10-CM categories have been developed. However, they were not designed to bethe basis for an automatic classification tool nor do they model ICD-10-CM inclusion terms as Web OntologyLanguage (OWL) axioms, which enables automatic classification. In this context we have developed Dione, an OWLrepresentation of ICD-10-CM.Results: Dione is the first OWL representation of ICD-10-CM, which is logically consistent, whose axioms define theICD-10-CM inclusion terms by means of a methodology based on SNOMED CT/ICD-10-CM mappings. The ICD-10-CMexclusions are handled with these mappings. Dione currently contains 391,669 classes, 391,720 entity annotationaxioms and 11,795 owl:equivalentClass axioms which have been constructed using 104,646 relationships extractedfrom the SNOMED CT/ICD-10-CM and BioPortal mappings included in Dione using the owl:intersectionOf and theowl:someValuesFrom statements. The resulting OWL representation has been classified and its consistency testedwith the ELK reasoner. We have also taken three clinical records from the Virgen de la Victoria Hospital (Málaga, Spain)which have been manually annotated using SNOMED CT. These annotations have been included as instances to beclassified by the reasoner. The classified instances show that Dione could be a promising ICD-10-CM OWLrepresentation to support the classification of patients diseases.Conclusions: Dione is a first step towards the automatic classification of patients diseases by using SNOMED CTannotations embedded in Electronic Health Records (EHRs). The purpose of Dione is to standardise and formalise amedical terminology, thereby enabling new kinds of tools and new sets of functionalities to be developed. This in turnassists health specialists by providing classified information from EHRs and enables the automatic annotation ofpatients diseases with ICD-10-CM codes.Keywords: ICD-10-CM, SNOMED CT, Ontologies, Automatic classification*Correspondence: mmar@lcc.uma.es1Ada Byron Research Center, University of Malaga, Ampliación del Campus deTeatinos, Málaga, Spain2IBIMA Instituto de Investigación Biomédica de Málaga, University of Malaga,Málaga, Spain© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 2 of 16IntroductionThe International Classification of Diseases, 10th Revi-sion (ICD-10) [1] is a standard diagnostic tool for healthmanagement, epidemiology and clinical purposes. ICD-10 comprises Chapters I to XXII which cover diseases,a variety of signs and symptoms, abnormal findings,complaints, social circumstances and external causes ofinjuries and diseases. ICD-10-CM corresponds to thetenth version, clinical modifications, which is the currentICD version. This medical classification standard, main-tained and published by the World Health Organisation(WHO) is used to classify diseases and health problemsthat have been recorded on death certificates and in otherrecords. The accuracy of this classification is a very impor-tant issue because it is used, for example, to set capitationrates and allocate resources to medical centers. It is alsoused by medical and health services researchers to deter-mine the case fatality and morbidity rates. Furthermore,ICD-10-CM has beenmandatory in the U.S. and thereforein regular and routine use since October 1, 2015.The concept of ontologies has been widely used innumerous real-word applications domains from HealthCare and Life Science to Finance and Government. Themajority of current ontologies are expressed in the well-known Web Ontology Language (OWL) [2]. SemanticReasoners such as Pellet [3], ELK [4], KAON2 [5] andRacerPro [6] are all widely used to develop ontology-basedautomatic classification systems. Improving the accuracyof classification is the main purpose of using ontologiesand OWL representations as the basis for a classifica-tion system. The literature review refers to several OWLontologies for representing ICD-10-CM categories thathave been developed. However, they were never intendedto be the basis for an automatic classification tool nor dothey model ICD-10-CM inclusion terms as OWL axioms,which enables this type of automatic classification.Systematized Nomenclature of Medicine - ClinicalTerms (SNOMED CT) [7] is a broad clinical terminol-ogy which covers a wide range of disciplines, clinicalspecialties and requirements. One of the main aims ofSNOMED CT is for it to be used as the standard termi-nology in Electronic Health Records (EHRs) systems. Theuse of SNOMED CT enables providers and EHR to use acommon language. Therefore, EHRs are annotated usingseveral SNOMEDCT concepts and relationships betweenSNOMED CT concepts, which codify patients informa-tion such as previous diseases, affected part of the body,and symptoms. Figure 1 shows an excerpt from an EHRwhere SNOMED CT codes are embedded.SNOMED CT/ICD-10-CM alignments (mappings)have been established and described by Unified MedicalLanguage System (UMLS) [8]. These mappings have a car-dinality of many to many. This means that one SNOMEDCT concept can be mapped with many ICD-10-CM targetcategories and vice versa. When a SNOMED CT conceptis not mapped to an ICD-10-CM category it means thatit is not classifiable or is awaiting editorial review. One ormore SNOMED CT source concepts can also be mappedwith the same ICD-10-CM target category. In additionto these mappings provided by UMLS, BioPortals usershave also defined SNOMED CT/ICD-10-CM mappings[9]. In this case, the Bioportals mappings have the samecardinality as the mappings provided by UMLS.The current situation is that EHRs are annotated usingSNOMED CT concepts and textual information fromthese records is then used to classify the patients dis-eases into an ICD-10-CM category (usually by an expert).Interestingly, SNOMED CT/ICD-10-CM mappings canbe exploited to define the ICD-10-CM inclusion termsbased on the SNOMED standard definition of patientsmedical evidence, affected part of the body and symp-toms and their relationships, connecting the standardannotations in EHRs with an ICD-10-CM category.Themain hypothesis of the work presented here is: (H1)It is possible to code, as OWL axioms, the ICD-10-CMinclusion terms obtained from SNOMED CT/ICD-10-CM mappings and use these OWL axioms to buildan OWL representation of the ICD-10-CM diseases.As a result (H2) we obtain a useful OWL represen-tation, which can be used as the basis for a seman-tic classification system. This system will then use theset of SNOMED CT concepts and relationships betweenSNOMED CT concepts, taken from EHRs, as input toautomatically classify patients diseases into an ICD-10-CM category. The objective of using this OWL rep-resentation is to improve the accuracy of the manualannotation. The accuracy of the classification is partic-ularly relevant at a time when diagnoses codes, such asthe ICD-10-CM codes, can significantly affect the totalfunding that a hospital may receive for patients admitted[10]. For example, in the United States, diagnosis-relatedgroups (DRGs) based on ICD codes are the basis forhospital reimbursement for acute-care stays of Medicarebeneficiaries [11]. Another fact is that health servicesresearchers use the ICD codes to study risk-adjusted,cross-sectional, and temporal variations in access tocare, quality of care, costs of care, and effectiveness ofcare [12].This has motivated us to develop an OWL represen-tation to help find an automated approach to classifypatients diseases in a medical context. Inclusion termsfor each ICD-10-CM category are formalised as OWLaxioms by exploiting SNOMED CT/ICD-10-CM map-pings. These mappings allow SNOMED CT concepts andrelationships to be used to define the inclusion terms.The resulting OWL representation called Dione1, whichincludes the ICD-10-CMChapters I to XIV, is as completeas the available SNOMED CT/ICD-10-CM mappingsRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 3 of 16Fig. 1 SNOMED CT codes embedded in an Electronic Medical Record [44]. EHRs are annotated using SNOMED CT concepts (in parentheses). Forexample, the SNOMED CT concepts 248152002 and 267036007 correspond to Female and Shortness of Breath, respectivelyallow. The exclusions proposed by ICD-10-CMare alreadyhandled by the mappings.The main contributions of this paper are summarised asfollows: We have defined an algorithm which, starting froman ICD-10-CM category code, is able to obtain itscorresponding SNOMED CT concept and all itsrelationships. The relationships obtained are considered to be theinclusion terms for the ICD-10 category. Therefore,we have also defined an algorithm for translatingthese relationships to OWL axioms. To test H1 we have developed Dione, an OWLrepresentation of ICD-10-CM, specifically designedto classified patients diseases by exploiting OWLs(Description Logic) reasoning capabilities. Dionerepresents the ICD-10-CM categories as classes andthe inclusion terms as OWL axioms related to theclass by means of the owl:equivalentClass statement.Classes representing ICD-10-CM categories as wellas classes representing SNOMED CT concepts areorganised as several hierarchies. To test H2, Diones consistency has been checkedand information from real clinical records has beenclassified to show Diones applicability through threeclinical use cases from the Virgen de la VictoriaHospital (Málaga, Spain).BackgroundThere have been some attempts made to construct OWLmodels using biomedical classifications like SNOMEDCTand ICD-10. For ICD, the work developed by [13] was thefirst attempt to model the ICD-9 ontology, an older ver-sion of the current ICD-10. In [14], the authors proposedthe first formal representation of the ICD-10 based onthree logical layers of the GALEN Core Reference Model(CRM) terminology system [15]. They used a descriptionlogic-like language called GRAIL [16] which allows classesto be inferred with the semantics of role propagation andlinks a more detailed description of a diagnosis to a moreabstract class. The ICD-10 ontology presented in [14]contains only ICD-10 categories and their definitions. Thehierarchical relationship of the ICD-10 is not representedand the ICD-10 category definitions are limited to threeconcepts defined by a multi-axial conceptual system thatincludes the anatomy, the morphology and the etiology.However, it has to be said that the methodology adoptedby the authors to formalise the ICD-10 has some limi-tations: first, only two ICD-10 chapters are represented;second, not all the ICD terms are represented usingGALEN and finally, the ontology was not loaded intoan OWL reasoner and therefore, the formal consistencywas neither checked nor classified. Given these problems,the authors presented a DOLCE-based formal represen-tation [17]. DOLCE is a descriptive upper-level ontol-ogy designed for ontology cleaning and interoperability.In this formal representation of the ICD-10, anatomi-cal entities were taken from the Foundational Model ofAnatomy (FMA) [18], morphological abnormalities andprocedures were taken from SNOMED CT, the organ-isms used were from the biological taxonomy and thechemical objects were taken from the International Unionof Pure and Applied Chemistry nomenclature (IUPAC).Despite these improvements over the previous versionof the GALEN-based ICD-10 representation, some prob-lems have yet to be solved. For example, not elsewhereclassified diseases are modeled as logical exclusions ofelsewhere classified ICD categories from the appropriateparent concepts. This solution does not provide any infor-mation for a system which aims to automatically classifya patients disease d. The doctor should assert or the sys-tem should infer that d is an instance of the negationof a class. Due to the OWA (Open World Assumption)semantics of OWL, if d is not an instance of class C, thereasoner cannot infer that d is an instance of ¬C. Further-more, the ontology has not been checked or classified by areasoner.The last approach to represent ICD-10 in OWL wasdeveloped in [19]. In this study, an ontology was cre-ated based on two super-classes, the icd10:Entry andthe icd10:Modifier which contain ICD-10 codes fromRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 4 of 16the WHO and the German Institute for Medical Docu-mentation and Information (German: Deutsches Institutfür Medizinische Dokumentation und Information) [20],respectively. The general structure of the ICD-10 ontol-ogy includes Chapters I to XXI; classes are representedby an URL which consists of a name space and the ICD-10 code name, and their relations are established withowl:subClassOf axioms. The ICD-10 exclusions are han-dled with the owl:disjointWith axiom. This approach doesnot provide information as to in which class the dis-eases should be classified. If the same disease is classifiedin both classes, the reasoner infers that the ontologyis inconsistent, but is unable to distinguish the correctclass. Furthermore, to solve the problem of exclusionsthat are shared with multiple exclusions, the authorsproposed the inclusion of icd10:hasExcludes that linksto a icd10:ICDdescription (with a rdf:type and rdf:labelpredicates) which has an icd10:concernsClass property.As icd10:concernClass can involve other ICD-10 cate-gories, the ontology requires an OWL-full expressivity.The inclusions are modelled in the same way as theexclusions. The OWL-full properties presented in theontology invalidate it for use by reasoners and thus, itis not possible to check the ontologys consistency orclassify it.According to the literature review, there have beenseveral attempts to model ICD-10 in OWL. However,these studies have various weaknesses which can be sum-marised as follows: 1) some difficulties in correctly han-dling inclusions and exclusions in an OWL representationof ICD-10; 2) the lack of a validation process using anOWL reasoner to check the consistency of the ontology.This step is very important in ontology development andtesting [21] because an ontology can be used by OWLreasoners without human supervision. If an ontology isinconsistent, the reasoning may lead to erroneous con-clusions; 3) no application of these OWL representationsto real clinical use cases to show how they can sup-port a clinician in decision making and 4) the reviewedwork uses ICD-9 and/or ICD-10. In this paper, we haveworked with ICD-10-CM. Although the intention is toreplace ICD-9-CMwith ICD-10-CM, it has been reportedin [22] that for reasons such as the complexity of ICD-10-CM and the costs of migrating from one system to theother would explain why the ICD-9-CM version is stillin use [22]. The Center for Disease Control and Preven-tion (CDC) encourages the use of the ICD-10-CM versionbecause of the improvements that it has over ICD-9-CMand ICD-10 [23]. These improvements include the addi-tion of information relevant to ambulatory and managedcare encounters; expanded injury codes; the creation ofcombination diagnosis and symptom codes to reduce thenumber of codes needed to fully describe a condition; theaddition of sixth and seventh characters; incorporation ofcommon fourth and fifth digit subclassifications; lateralityand greater specificity in code assignment [23].MethodsFormalising the ICD-10-CM categories in OWLFor the construction of Dione, we focused on three basicissues that are critical when modelling an ontology oran OWL representation, and are specified in the Ontol-ogy 101 development process methodology [24]. First, aselection of the concepts used to cover the objectives tobe accomplished in the health domain; second, the organ-isation of all concepts in a hierarchy and third, a semanticformalisation of these concepts using a knowledge rep-resentation language such as the description logic (DL)formalism.In order to select the terms for each concept, an XMLfile containing the ICD-10-CM categories in the Englishversion was downloaded from the Centers for DiseaseControl and Prevention (CDC) website that stores allthe ICD versions [25]. ICD-10-CM consists of chap-ters that are sub-divided into homogeneous blocks ofthree-character categories (a capital letter and two ara-bic numerals). These categories are sub-divided by meansof four-character categories (a capital letter and threearabic numerals) and these are further divided into five-character categories. The file with the ICD-10-CM cate-gories was parsed to output a tree with parent and childnodes (Additional file 1). The upper-level and lower-levelnodes of the tree generated from the XML file corre-spond to the ICD-10-CM upper and lower levels, whichinvolve blocks of three-, four- and five-character cate-gories. For the semantic formalisation, the hierarchy treefrom the output file, which includes Chapters I to XIV, wasencoded in OWL using the OWL API library [26]. Usingthe information from ICD-10-CM about blocks and theircategories, the OWL hierarchy was modelled establishingthe Diseases category as super-class because Chapters Ito XIV, are related to diseases. All Dione classes are iden-tified by an URI, which consists of a namespace and aspecial term which corresponds to the name of each ICD-10-CM category code. The Diseases category includesthe following ICD-10-CM categories: A00-B99 (Certaininfectious and parasitic diseases), C00-D49 (Neoplasms),D50-D89 (Diseases of the blood and blood-forming organsand certain disorders involving the immune mechanism),E00-E89 (Endocrine, nutritional and metabolic diseases),F01-F99 (Mental, Behavioural and Neurodevelopmentaldisorders), G00-G99 (Diseases of the nervous system),H00-H59 (Diseases of the eye and adnexa), H60-H95 (Dis-eases of the ear and mastoid process), I00-I99 (Diseasesof the circulatory system), J00-J99 (Diseases of the respi-ratory system), K00-K95 (Diseases of the digestive system),L00-L99 (Diseases of the skin and subcutaneous tissue),M00-M99 (Diseases of the musculoskeletal system andRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 5 of 16connective tissue) and N00-N99 (Diseases of the genitouri-nary system).In this hierarchy, theDiseases class includes the ICD-10-CM disease classification, the classes inside each diseasecategory are related to their upper-level category usingthe is-a relationship; these relationships are formalisedin Dione with the owl:subClassOf axiom. Therefore, abranch of the disease ICD-10-CM hierarchy was created(Additional file 2). Figure 2 shows part of the Diseaseshierarchy, specifically the A00-B99 (Certain infectious andparasitic diseases) branch and its subclasses. For example,A00-A09 (Intestinal infectious diseases) has as subclassesA00.0 (Cholera due to Vibrio cholerae 01, biovar cholerae),A00.1 (Cholera due to Vibrio cholerae 01, biovar eltor)and A00.9 (Cholera, unspecified). An English label withthe name of each category was included for each classof the ICD-10-CM disease classification hierarchy, usingthe rdf:label statement. Once the Dione hierarchy hadbeen constructed, the next step consisted in including theowl:equivalentClass axioms for each Dione class to modelthe ICD-10-CM inclusion terms. Therefore, in order tocomplete the semantic formalisation of the concepts ofthe OWL hierarchy, the SNOMED CT/ICD-10-CMmap-pings were used to construct the owl:equivalentClassaxioms for each class. Where an SNOMED CT/ICD-10-CMmapping was not available, we completed Dione withthe inferred mappings provided by the BioPortal website.These steps are fully explained in the following sections.Inclusion of the OWL axiomsThe files related to SNOMED CT were downloaded fromthe National Institutes of Health (NIH) webpage [27].The SNOMED CT concepts, the descriptions, the is-a relationships and the other relationships that repre-sent other concept associations were stored in an Oracle11g Database. The mapping files between ICD-10-CMand SNOMED CT (also known as the map) weredownloaded from UMLS [8]. The SNOMED CT/ICD-10-CM mappings were also extracted and stored in thedatabase. To generate the inclusion terms to be includedas owl:equivalentClass axioms for each OWL class ofthe Dione hierarchy, an automatic process was imple-mented (Additional file 3) based on the SNOMED CTrelationships that provide information about ICD-10-CMinclusion terms (see Table 2), namely, the SNOMED CTrelationships that we found in EHRs which providedinformation for classifying patients diseases. Therefore,relationships representing historical attributes such asmaybe a (ambiguous concept), was a (erroneous con-cept), moved to (moved to elsewhere concept), andother relationships with qualifier values such as Episod-icities etc. have not been considered. In total, there are12 SNOMED CT relationships that are used to define theICD-10-CM categories. For example, the SNOMED CTFig. 2 Part of the A00_B99 category hierarchy included in the Diseasescategory displayed on the Protegé software interface. A00_B99 is asubclass of the main class Diseases. According to the ICD-10-CMstructure, A00_A09 is a subclass of A00_B99 and A00_0, A00_1 andA00_9 are subclasses of A00_A09. According to the semantics ofOWL, if x is an instance of A00_9, it is also an instance of A00_B99 andtherefore, it is also an instance of A00_A09Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 6 of 16causative-agent relationship, which identifies the directcausative agent of a disease which can be an organism,substance or physical force, that is represented by thecaused-by-agent Dione object property. Another exampleis the associated-morphology relationship, which speci-fies the changes that are seen at the cellular level or intissues, caused by a disease, that is represented by has-Associated-Morphology Dione object property. Table 1shows materialised examples of such relationships.Once the Dione properties had been identified, therelationships between two SNOMED CT concepts (oneof which maps to the ICD-10-CM code included inDione ICD-10-CM hierarchy) were extracted and mod-elled by means of the owl:equivalentClass restriction ofDione classes and owl:intersectionOf statement in order tomodel the ICD-10-CM inclusion terms. The ICD-10-CMcategory was defined with the following properties: theSNOMED CT relationship (the object property in Dione),the owl:someValuesFrom restriction and the SNOMEDCT concept. In order to illustrate this, a good use caseis the I10 (Essential primary hypertension) ICD-10-CMcategory:I10 (Essential primary hypertension) is mapped to thesetwo SNOMED CT concepts: Hypertensive episode (disorder) (62275004) Complication of systemic hypertensive disorder(disorder) (449759005)According to the ICD-10-CM guidelines, the I10 cat-egory (Essential primary hypertension) has the inclusionterms High Blood pressure and Hypertension, whichcould be considered as a set of symptoms from a givenpatient. The SNOMED CT structure indicates that theSNOMED CT concept Hypertensive episode (disorder)(62275004) is related to the concept Finding of increasedblood pressure (finding) (24184005) by the relation-ship Has definitional manifestation (363705008). In thesame way, the SNOMED-CT concept Complication ofsystemic hypertensive disorder (disorder) (449759005)is related to the concept Hypertensive disorder, sys-temic arterial disorder (38341003) by the relationshipAssociated with (47429007). The SNOMED CT con-cepts Finding of increased blood pressure (finding)(24184005) and Hypertensive disorder, systemic arterialdisorder (38341003) are equal to High Blood pressureand Hypertension, respectively. These two inclusionterms have been modelled in Dione, by means of theowl:someValuesFrom statement (?), as follows:? hasDefinitionalManifestation.241840052? associatedWith.38341003.To associate the inclusion terms in a conjunctive form,the owl:intersectionOf () statement was used. Finally, theconjunction was associated with the class I10 throughan owl:equivalentClass (?) axiom. Therefore, the defini-tion of the I10 class including the inclusion terms is thefollowing (Fig. 3 also shows it graphically):I10 ? ? hasDefinitionalManifestation.24184005 ? associatedWith.38341003Figure 4 describes how the inclusion terms of I10 (Essen-tial primary hypertension) have been modelled in Dione.It is also worth noting that some SNOMED CT relation-ship names to define the Dione properties were changedto avoid any ambiguity in Dione. For example, the Hasdefinitional manifestation relationship was renamed ashasDefinitionalManifestation object property to avoidgaps between words as displayed in the first and secondcolumns of Table 2.The rest of the relationships of the SNOMED CT con-cepts mapped with I10 (Essential primary hypertension)were also included as OWL axioms to complete the defi-nition of the class (see Fig. 5).Dione is composed of several branches: 1) the ICD-10-CM disease hierarchy, which comprises Chapters Iand XIV, including the concepts that are the domainof the relationships included and 2) the SNOMED CTimported concept hierarchies (and their annotations inrdf:label), which comprise those SNOMED CT conceptstaken from the relationships whose ranges include theseconcepts. These imported concept hierarchies were cre-ated in OWL using the is-a relationship, as described inPhase I (Additional file 4). Like the previous example ofhow the inclusions are modelled for I10 class, the I10 hasassociatedWith, hasDefinitionalManifestation and affectsproperties identified in the owl:equivalentClass restric-tion. These properties have a range that corresponds to aSNOMED CT concept. For example, affects and hasDef-initionalManifestation have SNOMED CT concepts suchas Systemic circulatory system structure (body struc-ture) and Finding of increased blood pressure (finding)Table 1 SNOMED relationships examples. Examples of SNOMED CT concept related to SNOMED CT through causative-agent andassociated-morphology relationshipsSNOMED CT concept SNOMED CT relationship SNOMED CT conceptCholera-non-01 group vibrio, disorder causative-agent Vibrio cholerae, non-O1, an organismBullous pyoderma, disorder associated-morphology Chronic superficial ulcer, a morphologic abnormalityBold data are SNOMED relationshipRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 7 of 16Fig. 3 OWL definition of the Class I10. ICD-10-CM category I10 inclusion terms are modelled as owl:someValuesFrom statement (?) and theirintersection are defined as equivalent of the class I10 by means of the owl:equivalentClass () axiom. The Description Logic syntax of OWL is usedas range, respectively. These concepts are included in theDione SNOMED CT imported hierarchy as shown inFig. 6.Completion with BioPortal mappingsOnce Dione had been developed, we have determinedthe number of classes defined with the relationshipsfrom the SNOMED CT/ICD-10-CM mappings pro-vided by UMLS (Additional file 5). Those classes thatdid not involve either an inherited or non-inheritedaxiom (in the case of a superclass of the ICD-10-CM disease branch of Dione) were defined using theSNOMED CT/ICD-10-CM mappings from the BioPortalwebsite [9]. To do this, the new mappings between theSNOMED CT concepts and the ICD-10-CM categorieswere extracted from the BioPortal API [28] and inferredto avoid duplicate SNOMED CT/ICD-10-CM mappingsfrom NIH. Following the methodology described in theprevious subsections, the new mappings were stored inthe database. The new OWL statements for the inclu-sion terms of the ICD-10-CM categories without anyaxioms were generated and included in Dione. Thus,we obtained the most complete Dione version possi-ble with the resources available, given that some classesFig. 4 Description of the process to model I10 inclusion terms in Dione. From the SNOMED CT/ICD-10-CM mappings and by exploiting theSNOMED CT relationships the ICD-10-CM inclusion terms are modelled. This figure shows there exists a SNOMED CT/ICD-10-CMmapping for eachICD-10-CM inclusion termRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 8 of 16Table 2 SNOMED CT relationships that were used to define the Dione classesDione object property SNOMED CT relationship Description Number of uses in DioneAffects Finding site The part of the body affected by a condition 10,812After-Of After Represents a sequence of events where a clinicalfinding occurs after another491Associated-With Associated with Represents a clinically relevant association betweenconcepts without either asserting or excluding acausal or sequential relationship between the two469Caused-By-Agent Causative agent Identifies the direct causative agent of a disease (e.g.,an organism)1,701Due-To Due to Relates a clinical finding directly to a cause such asanother clinical finding or a procedure504Has-Associated-Finding Associated finding Links concepts in the situation with explicit contexthierarchy to their related clinical finding66Has-Associated-Morphology Associated morphology Specifies the morphologic changes seen at thetissue or cellular level that are characteristic featuresof a disease7,634Has-Definitional-Manifestation Has definitional manifestation Links disorders to the manifestations (observations)that define them818Has-Occurrence Occurrence Refers to a specific period of life during which acondition first presents520Has-Pathological-Process Pathological process Provides information about the underlyingpathological process for a disorder, but only when theresults of that process are not structural andcannot be represented by the associatedmorphologyrelationship1,867Interprets Interprets Refers to the entity being evaluated or interpreted,when an evaluation, interpretation or judgment isintrinsic to the meaning of a concept425IsPartOf Part of Represents a sequence of events where a clinicalfinding occurs after another clinical finding or aprocedure11The first and second columns include the selected SNOMED CT relationships with their Dione and original names. The third column is a description obtained from theSNOMED CT User guide [45] and the fourth column includes the number of times that these relationships were used in Dionewere not defined (statistics are presented in the Resultssection).Completing the definition of Dione classesAs we have mentioned, we could not find SNOMEDCT/ICD-10-CM mappings for all ICD-10-CM categories.This means that we could not include OWL statementsthat model inclusion terms, for all Dione classes. In somecases where we did find a mapping, the SNOMED CTconcept which the ICD-10-CM category was mapped todid not have relationships that could be translated intoOWL statements.This incompleteness of Dione means that a patients dis-ease can be classified into ICD-10-CM categories belong-ing to different chapters. In order to partially solve thisproblem and taking into account that the main objec-tive of Dione is to classify patients diseases using theSNOMED CT annotations embedded in EHRs, we haveimplemented a first process to include the axioms defin-ing a class in the definition of its subclasses (Additionalfile 6). Then we have completed a second process toinclude the owl:someValuesFrom statement from thedefinition of sibling subclasses in the definition of theirFig. 5 Complete definition of the class I10. In addition to the SNOMED CT/ICD-10-CM mappings representing ICD-10-CM inclusion terms we modelthe rest of mappings as owl:someValuesFrom statement (?) in order to complete the definition of the class. ? affects.113257007 models that I10affects 113257007 (Structure of cardiovascular system), ? affects.51840005 models that I10 affects 51840005 (Systemic circulatory system structure),? associatedWith.38341003 models that I10 is associated with 38341003 (Hypertensive disorder, systemic arterial) and ?hasDefinitionalManifestation.24184005 models that I10 has definitional manifestation 24184005 (Finding of increased blood pressure)Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 9 of 16Fig. 6 Structure of the ICD-10-CM disease hierarchy and the owl:equivalenClass axioms identified and included. The range of the properties in Dionecorresponds to the SNOMED CT concepts imported in the SNOMED CT hierarchysuperclass (Additional file 7). Before this process, Dionedefined:K58 (Irritable bowel syndrome) ? ?affects.113276009 (Intestinal structure)E73_0 (Congenital lactase deficiency) ? ?affects.113276009 (Intestinal structure)  ?hasOccurrence.255399007 (Congenital)Therefore, if a patients disease was classified in E73_0(Congenital lactase deficiency), it was also classified inK58 (Irritable bowel syndrome), because E73_0 was asubclass of k58. The problem here was that we found amapping but we could not find more OWL statementsto define K58 from SNOMED CT relationships. How-ever, we did observe that K58 has two subclasses (K58_0and K58_9) and both subclasses share ? affects.71854001(Colon structure) in their definitions. Therefore, we wereable to add this OWL statement to K58 and now K58 isdefined as:K58 (Irritable bowel syndrome) ??affects.113276009 (Intestinal structure) ?affects.71854001 (Colon structure)With this new definition E73_0 is not a subclass of K58.Dione consistency and classificationFor Dione classification, we used the ELK reasoner withthe OWL API (Additional file 8). After some attemptsto apply Dione classification with reasoner systems suchas Fact++ [29], Hermit [30], Pellet [3], TrOWL [31],RacerPro [6] and CEL [32], it was found that the ELKreasoner [4] was the only reasoner able to classify Dionewhile simultaneously checking that Dione was consistent.Fact++, Pellet, RacerPro and CEL failed due to an out-of-memory error (heap space set to 12 GB). TrOWL andHermit failed due to a timeout after 48 h. The experimentswere performed on a PC Intel(R) Core (TM) i7-2600 CPUwith 3.39 GHz and 16 GB of RAM and took 2781 s.Results and discussionLevel of completion of DioneDione has been built based on the ICD-10-CM terms(2014 release) provided by the CDC [25] and SNOMEDCT terms from UMLS (March 2013 release). Dione con-tains 391,669 classes, 391,720 entity annotation axiomsand 19,797 owl:equivalentClass axioms which were con-structed with 104,646 relationships extracted from theSNOMED CT/ICD-10-CM and Bioportal mappings andincluded in Dione using the owl:intersectionOf and theowl:someValuesFrom constructs.The current version of Dione has 21,616 classesfor modelling ICD-10-CM categories. After using theSNOMED CT/ICD-10-CM mappings from UMLSmetathesaurus, the percentage of classes with axioms was93 %. So, to provide a more complete version of Dione,we extracted a set of ICD-10-CM/SNOMED CT map-pings from BioPortal. These BioPortal mappings includeone-to-one mappings and one-to-many mappings. Anexample of the second case is the ICD-10-CM categoryA93.8 (Other specified arthropod-borne viral fevers)which is mapped to three SNOMED CT concepts. ThisRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 10 of 16concept points to the SNOMED CT concept [X] Otherspecified arthropod-borne viral fevers, Piry virus dis-ease and [X] Other specified viral hemorrhagic fevers.The relationships from these SNOMED CT concepts forboth types of mappings were extracted and included inDione to define the ICD-10-CM categories. Therefore,with the new mappings from BioPortal included in Dioneas OWL statement for defining ICD-10-CM categories,the percentage of Dione classes with axioms is 93,3 %(with an average of 4,8 axioms per class of the diseasehierarchy, affects being the most used Dione objectproperty as shown in Table 2). Dione currently contains391,669 classes; 21,616 classes (5,5 %) taken from theICD-10-CM hierarchy and 370,053 classes (94,5 %) takenfrom the SNOMED CT imported hierarchies. As we havementioned in the Methods section, this version of Dioneis still incomplete, and we could still obtain incorrectinferences from the reasoner, for example, when we donthave a definition for a specific class. In addition, in somecases we could not find an OWL statement to distinguishbetween subclasses. Therefore, some classes are inferredto be equivalent to their descendants. Figure 7 presentsan extract of Dione which shows how Dione looks whenclasses have complete definitions. In this example, A00(Cholera) has two subclasses A00_0 (Cholera due toVibrio cholerae 01, biovar cholerae) and A00_1 (Choleradue to Vibrio cholerae 01, biovar eltor). The definition ofA00 is included in the definition of its subclasses becausethey are both Cholera. However, the definition of A00_0and A00_1 includes a new owl:someValuesOf statementto distinguish between the different types of Cholera. IfFig. 7 Dione classes and subclasses modeling Cholera. This is anextract of Dione which shows how Dione looks when classes havecomplete definitions. The definition of A00_0 and A00_1 includes anOWL statement to distinguish between the different types of Cholerawe are to solve all the incorrect inferences we will haveto find new SNOMED CT/ICD-10-CM mappings as wellas align Dione with other medical terminologies and/orontologies so as to complete the definition of the classes.It may also be possible to obtain information from newmappings established and reviewed by the scientificcommunity (UMLS and Bioportal) or from an expert (i.e.doctor) to complete the definition of some classes.Validation of Dione axiomsThe Dione axioms have been included using theSNOMED CT/ICD-10-CM mappings, which has beenconstructed by a collaborative community of trained ter-minology specialists (closely following the methodologyof SNOMED CT to ICD-10 Crossmap project). Thesefinal mappings are published only if they have been estab-lished as identical by a group of experts and pass a finalreview. In the case that SNOMED CT/ICD-10-CM map-pings are not available for certain ICD-10-CM category,BioPortal provides SNOMED CT/ICD-10-CM mappingsthat have been previously inferred by the BioPortal algo-rithm and/or included (and validated) by the BioPortaluser community [33, 34]. The SNOMED CT/ICD-10-CM mappings that are equal to the UMLS mappingshave been removed to avoid duplicate ICD-10-CMinclusions.As the percentage of Dione classes with axioms is 93,3 %,it is worth noting that we could have manually completedthe mappings to generate the axioms for defining thoseclasses which do not have any axiom, either inherited fromparent classes or defined. However, we prefer to releasethe first version of Dione using only those mappings thatare available and widely accepted by the scientific com-munity. We have called this current version Dione V0.933.As new mappings are created, new axioms will be used tocomplete the current version of Dione.Applicability of Dione in clinical use casesAs Dione is logically consistent, we have used it togetherwith the ELK reasoner to classify clinical records. Clini-cal record information is codified by means of the Dioneobject property assertions which use SNOMED CT con-cepts. The objective of these use cases is to show howDione can assist health specialists by providing ICD-10-CM classified information. We have chosen three clinicalrecords from the Virgen de la Victoria Hospital (Málaga,Spain).The first clinical record from the Hematology depart-ment describes the case of a 61-year-old man, heavysmoker, who presented with obesity, no fever, severehypoventilation, arrhythmia and signs of hypersensibility.The results of the blood test show a low level of plateletsand the electrocardiogram (ECG) determines an auric-ular fibrillation. The hematologic and main diagnosesRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 11 of 16are thrombocytopenia and atrial fibrillation, respectively.According to ICD-10-CM guidelines, the hematologicdiagnosis code D69.6 corresponds to Thrombocytope-nia, unspecified which was diagnosed by the health spe-cialist. The ICD-10-CM category that corresponds to themain diagnosis is I48. For the hematologic diagnosis per-formed by the reasoner, instances with object propertyassertions such as interprets Platelet count, hasDefin-tionalManifestation Platelet count below reference range(finding) and hasPathologicalProcess Hypersensitivityprocess (qualifier value) were created and included inDione (Fig. 8). The ELK reasoner classified the informa-tion in the Dione D69_6 class, which corresponds to thecorrect ICD-10-CM category. In the case of themain diag-nosis, we have included object property assertions suchas affects Atrial structure (body structure), affects Car-diac conducting system structure (body structure) andStructure of cardiovascular system (body structure). Allthis information was classified in the I48 class, whichcorresponds to the ICD-10-CM category of Atrial fibril-lation and flutter. The list of inferred classes providedby the reasoner also contains other diseases that involveatrial dysfunction because the lack of definitions to dis-tinguish between classes in the same ICD-10-CM chapterand few ones belonging to a different chapter, becausethe lack of class definitions (see Level of completion ofDione sub-section). This first use case is an exampleof how information from two complementary diagnosesof a given patient can be included in Dione and reasonedby ELK.The second clinical record from the department of Gen-eral Clinical Surgery and Digestive Apparatus describes a53-year-old man who presented with pain, no fever andswelling of the perianal area that is identified by the clin-ician as an abscess. The blood test revealed a high levelof leukocytes. The main diagnosis was an Ischiorectalabscess that corresponds to the K61.3 class. The objectproperty assertions to be included in Dione and classi-fied by the ELK reasoner are: affects Anorectal struc-ture (body structure), hasAssociatedMorphology Abscess(morphologic abnormality) and hasPathologicalProcessInfectious process (qualifier value) (Fig. 9). The rea-soner classified this information in the ICD-10-CM K61class, which corresponds to the Abscess of anal and rec-tal regions. This class includes a further five classes: K61.0(Anal abscess), K61.1 (Rectal abscess), K61.2 (Anorectalabscess), K61.3 (Ischiorectal abscess) and K61.4 (Intras-phincteric abscess). Our objective is to provide the healthspecialist with an as accurate as possible ICD-10-CMcode taken into account the information obtained fromthe EHR. This use case is an example of how addi-tional annotations by the health specialist on the patientsclinical record can lead to a more exact diagnosis withthe proposed approach. For example, if the clinician hadannotated Ischiorectal fossa structure on the clinicalrecord, an additional individual with the object propertyassertions affects some Ischiorectal fossa structure (bodystructure) would have been included in Dione and there-fore, the information would have been classified in a morespecific ICD-10-CM class, like K61.3.Fig. 8 Representation of the object property assertions for the first use case. Blue and black arrows represent the object property expressions thatrelate individual Disease to the rest of individuals that correspond to SNOMED CT codes which have been manually annotated from thehematologic and main diagnosesRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 12 of 16Fig. 9 Representation of the object property assertions for the second use case. The black arrows represent the object property expressions thatrelate individual Disease to the rest of individuals that correspond to SNOMED-CT codes. The SNOMED CT codes have been manually annotatedfrom the medical reportThe third clinical record from the department of Inter-nal Medicine and Nephrology describes the case of a77-year-old woman who underwent surgery to removerenal carcinoma. The patient did not undergo chemother-apy treatment and received the usual treatment withBisoprolol and acetylsalicylic acid. For the next fewmonths following the surgery, she lost weight. The clin-icians exploration through an abdominal computerisedtomography revealed several neoplasms in lung, liver,in the retroperitoneal space and also some retroperi-toneal adenopathies. The diagnosis was multiple neo-plasms that had spread from the primary renal cancer.The diagnosis involves several ICD-10-CM categories,which are included in the class C78 (Secondary malig-nant neoplasm of respiratory and digestive organs) suchas C78.0 (Secondarymalignant neoplasm of lung), C78.3(Secondary malignant neoplasm of other and unspeci-fied respiratory organs) and C78.80 (Secondary malig-nant neoplasm of unspecified digestive organ). Theobject property assertions that have been defined arehasAssociatedMorphology Neoplasm metastasic (mor-phologic abnormality), affects Structure of retroperi-toneal lymph node (body structure), Lung structure(body structure), Liver structure (body structure), Peri-toneum (serous membrane) structure (body structure),Abdominal lymph node structure (body structure) andRetroperitoneal structure (body structure) (Fig. 10). Thedisease instance was classified in the following ICD-10-CM codes: C78, C78.30 (Secondary malignant neoplasmof unspecified respiratory organ) and C78.80. This thirduse case describes a second diagnosis of a patient whohad originally suffered from renal cell carcinoma. Thediagnosis by the health specialist involves several ICD-10-CM categories and the information extracted from theclinical record includes information from different bodystructures. Therefore, according to the results obtained inthis use case and the others, Dione has been able to pro-vide a list of classified instances to help doctors classifyclinical information from medical records.Advantages of formalising the ICD-10-CM categories inOWLTwo possible approaches can be applied to formally definethe ICD-10-CMdisease terms: 1) build anOWL represen-tation to be reasoned by the OWL reasoners, as proposedin this paper and 2) establish mappings between ICD-10-CM with other terminologies and ontologies and thereby,apply indirect reasoning [35]. This second approach hasbeen adopted by UMLS with the SNOMED CT to ICD-10-CM map. This map has been applied in the I-MAGICalgorithm [36], a Java program in which these mappingsare applied using the mapping rules. The BioPortal web-site has also adopted the same methodology and providesmappings between ICD-10-CM terms and other ontolo-gies. However, it is has to be noted that this secondapproach has some drawbacks:1. There are two types of SNOMED CT/ICD-10-CMmappings: one-to-one and one-to-many mappings.This means that not every SNOMED CT conceptcan be mapped to only one ICD-10-CM categorieswith an identical meaning. Rather it can be mappedto more than one ICD-10-CM categories with severalmeanings.2. This approach does not allow direct reasoning basedon hierarchy relationship of ICD-10-CM categoriesestablished by owl:subClassOf axiom, the axiomsincluded in owl:equivalentClass axioms to define theICD-10-CM categories and the type of objectproperties that are established in the OWL model.Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 13 of 16Fig. 10 Representation of the object property assertions for the third use case. The black arrows represent the object property expressions thatrelate individual Disease to the rest of individuals that correspond to SNOMED CT codes. The SNOMED CT codes have been manually annotatedfrom the medical reportFor these reasons, we have built an OWL hierarchy withICD-10-CM categories and used the SNOMED CT/ICD-10-CM mappings to model the ICD-10-CM inclusionterms. According to the applicability of Dione to real clin-ical use cases demonstrated in the Results section, thisapproach provides users with a direct OWL reasoningover a set of instances from one or more actual problems(Electronic Health Records) proposed by the physician toinfer new relationships and provide a new approach to theclassification problem.Comparison with other OWL ICDmodelsThe representation from a clinical terminology to anOWL model can cause semantic inconsistencies. Accord-ing to the reviewed literature (Background section), thereis a lack of consistency checking in the proposed ICD-10-CM formal representations and therefore, ABox and TBoxclassifications3 have not been done. In this approach,Dione has been validated by the ELK reasoner, which wasfound to be the only reasoner able to classify it, aftertesting all reasoners that have been successful in clas-sifying large and widely-used real-world ontologies likeSNOMED CT [37]. The ELK reasoner returns that Dioneis consistent and performs TBox and ABox classifications.In order to carry out the ABox classification of Dione, a setof instances from clinical use cases taken from the Virgende la Victoria Hospital (Málaga, Spain) has been includedin Dione and classified to ICD-10-CM categories as is fullyexplained in the Results section.In the Background section, we highlighted some limita-tions of existing work in the literature. In this section, wediscuss the formal representation in OWL of ICD-10-CMproposed by [19] given that it is an ICD-10-CM represen-tation that has improved upon other approaches. In thisapproach, the authors created an ICD-10-CM hierarchywith owl:subClassOf handling the ICD-10-CM exclusionswith owl:disjointWith axioms. We consider that such anapproach is limited given that only using owl:disjointWithcould result in a lack of information, because if the samediseases is classified in two disjoint classes, the reasonerinfers that the ontology is inconsistent, being unable todistinguish the correct class. Furthermore, the exclusionsthat are shared withmultiple exclusions are modelled withOWL-Full, making it impossible to validate and classifythe ontology with a reasoner. The inclusion terms aremodelled in the same way as the exclusions. In our case,the semantic Disease hierarchy of Dione is constructedwith owl:subClassOf axioms (using the ICD-10-CM con-cepts which have not been used in other approaches in theliterature) and the inclusion terms are modelled from theinformation extracted from SNOMED CT/ICD-10-CMmappings. The approach adopted solves the problem ofintegrating features fromOWL-Full. As mentioned, in thecase of dealing with the exclusions, the mappings includean exhaustive mapping of the low-level descendants ofthose SNOMEDCT concepts that could lead to a differentICD-10-CM category given ICD-10-CM exclusions andother rules.ConclusionsThis paper has presented the implementation process ofDione, an OWL representation of ICD-10-CM, whichuses SNOMED CT/ICD-10-CM mappings to formalisethe ICD-10-CM diseases categories and their inclusionRoldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 14 of 16terms. Themain hypothesis guiding us is: (H1) It is possi-ble to code, as OWL axioms, the ICD-10-CM inclusionterms obtained from SNOMED CT/ICD-10-CM map-pings and use these OWL axioms to build an OWLrepresentation of the ICD-10-CM diseases. Therefore,we have used an automatic process to build a hierarchytree with ICD-10-CM disease categories and their axiomsusing the owl:equivalentClass axiom. The main objectiveof our approach has been to build a model that can beused by a reasoner. Therefore, we have also shown thatDione is consistent and a TBox classification has beencarried out by the ELK reasoner. It is worth noting thatthe automatisation of the ICD-10-CM disease categoriesis important given that new mappings are continuouslybeing added to complete Dione, whose initial version isreleased with this paper. In its current version, we havenot been able to find validated SNOMED CT/ICD-10-CM mappings for each ICD-10-CM category and so didnot have correct results in several cases. Therefore, ourfirst objective is to complete the definition of all classes.We plan to add more mappings from other ontologieswhich will relate Dione with more biological informationfrom different areas that involve personalised medicine.BioPortal provides ontologies mapped to ICD-10-CM cat-egories such as the National Drug Data File (with 2,857ICD-10-CM mappings) [38], OMIM (with 3,921 ICD-10-CM mappings) [39], the Human Phenotype Ontology(with 1,370 ICD-10-CM mappings) [40] and the Regu-lation of Transcription Ontology (with 61 ICD-10-CMmappings) [41, 42]. Using the approach presented in thispaper, the mappings can be extracted from BioPortal andstored in a database. The axioms to be included in Dioneusing owl:equivalentClass can be defined with an affectobject property or also with the axioms that are definedin the ICD-10-CM mapped class of the target ontology. Itmay also be possible to obtain information from an expert(i.e. doctor) to complete the definition of some classes.As a secondary hypothesis, we (H2) obtain a usefulOWL representation which can be used as the basisfor a semantic classification system. This enables a setof SNOMED CT concepts and their relationships, takenfrom EHRs, as input to automatically classify patients dis-eases into an ICD-10-CM category. This has been testedby including object property assertions in Dione fromthe Virgen de la Victoria Hospitals (Málaga, Spain) clin-ical records which have been classified into ICD-10-CMcategories showing the applicability of the OWL repre-sentation. After completing Dione, we plan to measurethe accuracy of the classification and to study new waysto improve it. As the development of Dione is ongoing,further work will include looking at how Dione reason-ing can assist experts in providing classified informationfor ICD-10-CM disease categories from actual use casesof patient records. Dione should be supported in thefuture by a semi-automatic EHR annotation tool, proba-bly based on text mining and natural language processingtechniques. Our intention is to use an end-user interfacewhere the information processed can be displayed in sucha way as to make it easily understandable to specialistsin the field. This application could provide functionali-ties which allow users to make specific OWL queries suchas: Find a disease which affects a body structure (find-ing site) like Systematic circulatory system structure (bodystructure) AND hasDefinitionManifestation Finding ofincrease blood pressure (finding). These kinds of queriescan be used to retrieve an ICD-10-CM disease categorythat has the same definition in Dione and also, to automat-ically generate a list of possible ICD-10-CM categories inwhich the instances from real clinical records can be clas-sified. Finally, we will study howDione could be combinedwith other techniques such as SWRL (Semantic Web RuleLanguage) rules [43] and probabilistic databases in orderto develop a diagnostic assistance tool.Endnotes1Dione is available at http://www.khaos.uma.es/dione2According to the semantics of OWL, this representsan anonymous class. It has an object property hasDef-initionalManifestation. At least one value for hasDefi-nitionalManifestation must be an instance of 24184005Finding of increased blood pressure (finding).3 TBox and ABox are known as terminological andassertion components, respectively. TBox statementsdescribe the set of Dione classes and properties and ABoxstatements are used to describe the instances associatedwith those classes and properties.Additional filesAdditional file 1: Creating the ICD-10-CM categories tree. PDF filecontaining the algorithm for parsing the XML-file containing the ICD-10-CMcategories to output a tree with parent and child nodes. (PDF 82 kb)Additional file 2: Representing ICD-10-CM categories in OWL. PDF filecontaining the algorithm for creating the ICD-10-CM representation inOWL. (PDF 73 kb)Additional file 3: Including OWL axioms in Dione. PDF file containing thealgorithm for obtaining axioms from SNOMED CT/ICD-10-CM mappingsand for adding these axioms to Dione. (PDF 73 kb)Additional file 4: Imported hierarchies. PDF file containing the algorithmfor creating SNOMED CT imported hierarchies and for including thesehierarchies in Dione. (PDF 73 kb)Additional file 5: Dione level of completeness. PDF file containing thealgorithm for recursively counting the number of classes defined usingrelationships from the SNOMED CT/ICD-10-CM mappings. (PDF 72 kb)Additional file 6: Classification of Dione. PDF file containing thealgorithm for including the axioms defining a class in the definition of itssubclasses. (PDF 68 kb)Roldán-García et al. Journal of Biomedical Semantics  (2016) 7:62 Page 15 of 16Additional file 7: Classification of Dione. PDF file containing the algorithmfor including the owl:someValuesFrom statement from the definition ofsibling subclasses in the definition of their superclass. (PDF 68 kb)Additional file 8: Classification of Dione. PDF file containing thealgorithm for classifying Dione using the ELK reasoner through the OWLAPI. (PDF 72 kb)AbbreviationsCDC: Center for disease control and prevention; CRM: Core reference model;DRGs: Diagnosis-related groups; EHR: Electronic health record; FMA:Foundational model of anatomy; ICD-10: International classification of diseases,tenth revision; ICD-10-CM: International classification of diseases, tenthrevision, clinical modification; IUPAC: International union of pure and appliedchemistry; NIH: National institutes of health OWL: Web ontology language;SNOMED CT: Systematized nomenclature of medicine - clinical terms; UMLS:Unified medical language system; WHO: World health organisationAcknowledgementsThe authors would like to thank the Admissions and Medical DocumentationService at the Virgen de la Victoria Hospital (Málaga, Spain) for providing uswith the necessary clinical reports to demonstrate the viability of Dione.FundingThis work was partially funded by grants TIN2014-58304-R (Ministerio deEconomía y Competitividad), P11-TIC-7529 and P12-TIC-1519 (Plan Andaluz deInvestigación, Desarrollo e Innovación).Authors contributionsMMRG designed the methodology for creating the Dione inclusion terms asOWL axioms from the SNOMED CT/ICD-10-CM mappings, implemented therelational database containing the SNOMED CT concepts and relationshipsand designed the algorithms for implementing Dione. MJGG contributed tothe design of the algorithms, implemented the algorithms and the OWLrepresentation and validated Dione. MMRG and MJGG designed andimplemented the use cases for testing the applicability of Dione and wrotethe manuscript. Finally, JFAM is the director of the research group. He devisedand supervised the work and collaborated in writing the manuscript. Allauthors discussed, read and approved both Dione and the manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 1 June 2016 Accepted: 21 September 2016RESEARCH Open AccessThe Ontology of Biological and ClinicalStatistics (OBCS) for standardized andreproducible statistical analysisJie Zheng1*, Marcelline R. Harris2, Anna Maria Masci3, Yu Lin4, Alfred Hero5, Barry Smith6 and Yongqun He4*AbstractBackground: Statistics play a critical role in biological and clinical research. However, most reports of scientificresults in the published literature make it difficult for the reader to reproduce the statistical analyses performed inachieving those results because they provide inadequate documentation of the statistical tests and algorithmsapplied. The Ontology of Biological and Clinical Statistics (OBCS) is put forward here as a step towards solving thisproblem.Results: The terms in OBCS including data collection, data transformation in statistics, data visualization,statistical data analysis, and drawing a conclusion based on data, cover the major types of statistical processesused in basic biological research and clinical outcome studies. OBCS is aligned with the Basic Formal Ontology(BFO) and extends the Ontology of Biomedical Investigations (OBI), an OBO (Open Biological and BiomedicalOntologies) Foundry ontology supported by over 20 research communities. Currently, OBCS comprehends 878terms, representing 20 BFO classes, 403 OBI classes, 229 OBCS specific classes, and 122 classes imported from tenother OBO ontologies.We discuss two examples illustrating how the ontology is being applied. In the first (biological) use case, wedescribe how OBCS was applied to represent the high throughput microarray data analysis of immunologicaltranscriptional profiles in human subjects vaccinated with an influenza vaccine. In the second (clinical outcomes)use case, we applied OBCS to represent the processing of electronic health care data to determine the associationsbetween hospital staffing levels and patient mortality. Our case studies were designed to show how OBCS can beused for the consistent representation of statistical analysis pipelines under two different research paradigms. Otherongoing projects using OBCS for statistical data processing are also discussed.The OBCS source code and documentation are available at: https://github.com/obcs/obcs.Conclusions: The Ontology of Biological and Clinical Statistics (OBCS) is a community-based open source ontologyin the domain of biological and clinical statistics. OBCS is a timely ontology that represents statistics-related termsand their relations in a rigorous fashion, facilitates standard data analysis and integration, and supports reproduciblebiological and clinical research.Keywords: OBCS, Biological statistics, Clinical outcomes statistics, Standardization, Statistical analysis, Dataintegration* Correspondence: jiezheng@upenn.edu; yongqunh@med.umich.edu1Department of Genetics, University of Pennsylvania Perelman School ofMedicine, Philadelphia, PA 19104, USA4Department of Microbiology and Immunology, Unit for Laboratory AnimalMedicine, University of Michigan Medical School, Ann Arbor, MI 48109, USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 DOI 10.1186/s13326-016-0100-2BackgroundThe movement to advance reproducibility of research ad-vocates the use of open data, standard operating proce-dures, and reproducibility of methods used in bothcomputation [1] and statistics [2]. To support suchreproducibility it is imperative that standard metadata for-mats be used to describe how results were generated. Con-sider, for example, the case of ImmPort (the ImmunologyDatabase and Analysis Portal; https://immport.niaid.nih.-gov/), which is the worlds largest repository of public-domain de-identified clinical trial data related to immun-ology [3, 4]. All data derived from clinical trials funded bythe Division of Allergy, Immunology and Transplantation(DAIT) of the National Institute of Allergy and InfectiousDiseases are required to be published on the ImmPort por-tal. In addition, the ImmPort portal includes data obtainedfrom the work of the Human Immunology Project Consor-tium (HIPC, www.immuneprofiling.org/) as well as rele-vant data from a number of external sources such as theGates Foundation. ImmPort currently contains data setsfrom 211 studies with 34,801 subjects and 1054 experi-ments, including complete clinical and mechanistic studydata all of which are publicly available for download in adeidentified form. To facilitate data import and processing,ImmPort has created templates for data representation anddocumented standard operating procedures for workingwith imported data. To facilitate discovery and usability ofthese data to external users, and also to address the goal ofreproducibility, ImmPort is seeking wherever possible todraw on publicly available ontologies such as the CellOntology (CL) [5] and the Protein Ontology (PRO) [6] assources for the terms used in these templates.If the information-driven research documented in re-sources like ImmPort is to be reproducible, however, thenthe research results contained in the ImmPort and similarrepositories must also be annotated using ontology termswhich specify the protocols and methods used in datageneration and analysis. The OBI has been created to thisend, and the OBCS follows in the footsteps of OBI by pro-viding terms and formal definitions representing the stat-istical methods used in biological and clinical research.OBCS will thereby not merely allow researchers to repro-duce statistical analyses in order to validate the conclu-sions reached by study authors but also allow newpossibilities for discovery of and for comparison betweenstudies. We believe that it will also serve as impetus forthe creation of new sorts of software tools supportingmore advanced use of statistics in complex analysis andmeta-analysis of large and heterogeneous data sets.Ontologies are human- and computer-interpretablerepresentations of the types of entities existing in spe-cific scientific domains and of the relations betweenthese types. Since the creation of the first version of theGene Ontology (GO) in 1998 [7], many influentialontology resources have been created, most recently fol-lowing the principles of the Open Biological and Bio-medical Ontologies (OBO) Foundry [8]. Ontologies builtin accordance with OBO Foundry principles are de-signed to allow not only consistent classification, com-parison and integration across heterogeneous datasets,but also automatic reasoning with the data annotatedwith their terms. This is achieved in part through theadoption of Basic Formal Ontology (BFO) [9] as a com-mon top-level ontology, and also through employmentof a common set of logically defined relations. OBI is aprominent example of ontology that aligns with BFO.OBI provides a set of logically defined terms covering abroad range of investigation processes, experimentalconditions, types of equipment and documentation, anddata analysis methods performed on data generated fromthe experiments [10, 11]. OBO ontologies were initiallysuccessful in the representation of model organism re-search data and have subsequently been influential inhealth-related data standardization and processing [12].Ontobee, the default OBO ontology linked server, incor-porates over 100 OBO ontologies and provides the facilityto track how ontology terms are reused in multiple ontol-ogies [13].OBCS has its origin in a 2010 study of the ANOVA(ANalysis Of VAriance) meta-analysis of vaccine protec-tion assays [14], which led to the addition into OBI ofstatistical terms related to ANOVA and survival rateanalysis. Later, an OBI statistics branch was generated toidentify and fill gaps in OBIs representation of statistics[15]. The OBCS resulted directly from these efforts.OBCS extends OBI, and the two ontologies share over-lapping development groups.MethodsOBCS developmentOBCS is a community-based ontology of statistical toolsand methods used in biological and clinical investigationsthat follows OBO Foundry principles in providing the pos-sibility for enhanced representation and analysis of datagenerated through complex statistical procedures.OBCS is expressed using the W3C standard Web Onto-logy Language (OWL2) (http://www.w3.org/TR/owl-guide/).The meta-data schema of OBCS is implemented usingOWL annotation properties defined in the InformationArtifact Ontology (IAO, http://purl.obolibrary.org/obo/iao),which is widely used by OBO Foundry and other ontologies(https://github.com/information-artifact-ontology/IAO/wiki/OntologyMetadata). In what follows, we use singlequotes to represent terms from ontologies (includingOBCS), and italics to represent object properties (alsoknown as relations) such as is_a and part_of. The Pro-tégé OWL editor (http://protege.stanford.edu/) was usedfor ontology editing and the Hermit reasoner (http://Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 2 of 13hermit-reasoner.com/) for consistency checking and infer-encing. OBCS-specific terms were generated with IDs usingthe prefix OBCS_ followed by auto-generated seven-digitnumbers.OBCS is released under Creative Commons Attribution(CC BY) 3.0 License, and the OBCS source code is freelyavailable at: https://github.com/obcs/obcs and also on theOntobee [13] (http://www.ontobee.org/ontology/OBCS) andNCBO BioPortal (http://purl.bioontology.org/ontology/OBCS) websites. The summary information of ontologyterms in OBCS based on term types and resources can befound here: http://www.ontobee.org/ontostat/OBCS.The RDF triples for the OBCS ontology have been savedin the He group Triple store [13], which allows easy re-trieval of related OBCS contents using Semantic WebSPARQL technology. OBCS can be queried from the Onto-bees SPARQL query endpoint (http://www.ontobee.org/sparql) [13].A combination of top-down and bottom-up methods wasused in OBCS development. The top-down approach was ini-tiated by extending OBCS from the latest version of OBI(http://purl.obolibrary.org/obo/obi/2015-12-07/obi.owl) usingthe BFO 2.0 classes-only version (http://purl.obolibrary.org/obo/bfo/2014-05-03/classes-only.owl) and the middle tierontology IAO (http://purl.obolibrary.org/obo/iao/2015-02-23/iao.owl) [11]. OBCS also reuses ontological models (as de-scribed in the Results section) developed in OBI and IAO.The remaining parts of the ontology were then populated onthe basis of a survey of statistics workflows, which led to theidentification of a number of statistics-related terms not in-cluded in other ontologies. These terms were supplementedthrough the prototype statistics data analysis workflow pro-posed in [14] and through the study of specific use cases de-scribed below.This bottom-up strategy was combined with a top-downapproach involving definition terms through downwardmigration from higher-level ontology classes. For example,the Robust Multi-Array Average (RMA) normalization is acommonly used microarray data normalization method[16] and the corresponding term robust multi-array aver-age normalization (OBCS_0000140) was generated as asubclass of OBI normalization data transformation. Popu-lating the ontology in this way provided a simple strategyfor creating definitions of the terms introduced using themethod of specific difference [17] since the genus (and itsdefinition) were inherited from OBI it was necessary forpurposes of OBCS to supply only the differentia.Reusing existing ontology resourcesOBCS imports the subset of OBI consisting of allstatistics-related terms and associated parent terms usingthe Ontodog tool [18]. The Excel input data used for thispurpose is available at: https://github.com/obcs/obcs/raw/master/docs/OBI_statisitics_subset.xlsx. To ensure validreasoning, Ontodog was set to import all the terms usedin the expression of the ontological axioms relating toeach imported OBI term. For example, when Ontodogfetched the OBI term log-log curve fitting, the axiom:log-log curve fitting: achieves planned objectivesome curve fitting objective was also retrieved,together with the terms achieves planned objectiveand curve fitting objective.To eliminate redundancy and ensure orthogonality,terms already defined in OBO Foundry ontologies werereused in accordance with the Minimum Information toReference an External Ontology Term (MIREOT) guide-line [19]. OntoFox, a software program implementingand extending MIREOT [20], was used to extract indi-vidual terms from external ontologies using this strategy.Driving use cases for OBCS developmentThe first of two use cases driving OBCS development con-cerns a study of the systems biology of influenza vaccin-ation described in [21], relating to a transcription profilingby array experiment which has as its objective the identifi-cation of the gene expression profiles in human study sub-jects after influenza vaccine administration. Human bloodspecimens were used in this study, so that both biologicaland clinical domains were involved. The second use caseconcerns the study of clinical outcomes of nursing servicesdata with the aim of investigating statistical associationsbetween variable levels of nurse staffing and inpatient mor-tality [22]. Using observational data collected from a clin-ical setting, a Cox proportional hazards model estimationwas conducted to draw the conclusion that understaffedshifts were significantly associated with increased inpatientmortality [22]. Transcription profiling by array experi-ment and Cox proportional hazards model estimationare among the OBCS terms deriving from these use cases.ResultsOBCS overview and high level hierarchy structureThe latest release of OBCS contains a total of 878 terms,including 780 classes, 42 object properties, 25 annota-tion properties, and 6 datatype properties. Among these780 classes, 229 classes are unique to OBCS, 403 wereimported from OBI. The remaining terms were importedfrom various OBO ontologies, such as BFO (20 classes),IAO (51 classes), the Statistics Ontology (STATO)(http://stato-ontology.org/) (36 classes), the PhenotypicQuality Ontology (PATO) (10 classes) [23] (Table 1).Figure 1 illustrates the top-level hierarchical structureand some key ontology terms of OBCS, showing termsfrom both the continuant and occurrent branches ofBFO [9]. The continuant branch represents entities (e.g.,material entity) which endure through time; theZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 3 of 13occurrent branch represents entities such as processwhich occur in time. Many key continuant terms inOBCS are classified under IAOs information contententity, including: data item, probability distribution, aswell as directive information entities such as protocolsand algorithms (Fig. 1). OBCS includes 178 subclassesunder the branch of IAO data item (IAO_0000027).Major occurrent terms in OBCS relate to differenttypes of planned process, including: data collection, dataoperation, data visualization, and drawing a conclusionbased on data. The term data operation (OBI_0200000;a new alternative label for the imported OBI term datatransformation) is defined as A planned process thatproduces output data from input data. Data operationsatisfies two logical axioms:has_specified_input only/some data itemhas_specified_output only/some data itemTwo important child terms of data operation are: datatransformation in statistics and statistical data analysis(Fig. 1). A data transformation in statistics converts adata set to another data set by applying a deterministicmathematic function to each data item in the input dataset. For example, the OBI term logarithmic transform-ation (OBI_0200094) represents the kind of process thattransforms input data to output data by applying a loga-rithm function with a given base. In this case, the datatransformation process concretizes realize the mathemat-ical function. A key subclass under data transformation instatistics is normalization data transformation, which in-cludes various normalization processes such as robustmulti-array average normalization (RMA) [16].General design pattern used in the OBCS representationof statistical studiesOBCS is designed to represent all aspects of a statisticalstudy. A general statistical study workflow is representedTable 1 Summary of ontology terms in OBCS as of June 4, 2016Ontology Names Classes Object properties Datatype properties Annotation properties Instance TotalOBCS 229 2 0 1 1 233OBI (Ontology for Biomedical Investigations) 403 9 2 3 4 421IAO (Information Artifact Ontology) 51 8 4 13 18 94STATO (Statistics Ontology) 36 0 0 0 0 36BFO (Basic Formal Ontology) 20 6 0 2 0 28PATO (Phenotypic Quality Ontology) 10 0 0 0 0 10RO (Relation Ontology) 0 17 0 1 1 19Other ontologiesa 31 0 0 4 1 42Total 780 42 6 25 25 878athe name and statistics of other ontologies used in OBCS can be found on the Ontobee website: http://www.ontobee.org/ontostat/OBCSFig. 1 The top level OBCS hierarchical structure and key ontology terms. The terms shown in boxes with the prefix OBCS: in bold font areOBCS-specific terms, and the other terms are imported from existing ontologies including BFO, IAO and OBIZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 4 of 13in Fig. 2, which shows five major processes including: datacollection, data transformation in statistics, statistical dataanalysis, data visualization, and drawing a conclusionbased on data. These are all subtypes of OBI plannedprocess (Fig. 1) which comprehends two major subtypesof data item (Fig. 3), namely: measurement datum andderived data from statistical analysis. The latter is furtherdivided into: derived data from descriptive statistical ana-lysis (e.g., median and mode) and derived data from in-ferential statistical analysis (e.g., p-value).OBCS defines many different types of data directlyunder data item, and then provides logical axioms thatFig. 2 Semantic representation of statistical data analysis studies using OBCS. The boxes highlighted in red represent key planned processes inOBCS, and the terms in black boxes represent different information content entitiesFig. 3 Illustration of selected OBCS terms under data item and statistical data analysis and their hierarchies. a Illustration of part of the assertedhierarchical structure for the OBCS branch of data item. Note that there is no subclass under derived data from statistical analysis. b The inferredhierarchical structure after using the reasoner HermiT 1.3.8. After the reasoning, the derived data from statistical analysis has two direct subclassesderived data from descriptive statistical analysis and derived data from inferential statistical analysis. c Illustration of a part of the statistical dataanalysis branch in OBCS. Note that many OBCS terms under these branches are not shown, and these selected terms are used for demonstration.These screenshots came from the OBCS display using the Protégé OWL editorZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 5 of 13can be used to infer a data type under a particular classbased on a statistical data analysis (Fig. 3). For example,derived data from inferential statistical analysis is de-fined by an equivalence class axiom as:data item and (is_specified_output_of someinferential statistical data analysis)Given this equivalence class, every data item sub-class (e.g., specificity) having the axiom of is_specified_out-put_of some inferential statistical data analysis will beinferred to be a subclass of derived data from inferentialstatistical analysis.Next we will introduce how OBCS represents each ofthe five major processes. Figure 4 represents the majorparts of the OBCS data collection branch, which compre-hends 12 subclasses corresponding to different approachesto data collection, such as from experiment, literature, ob-servation, by online extraction, or by sampling. Online ex-traction can be performed from online databases orthrough a web crawler. Sampling can be achieved throughsurvey or censoring. Data collection may face difficultiesof different sorts. For example, data fields may be incom-mensurable, data may be missing, incompatibilities mayarise due to different types of study design, and we mayhave only partial information concerning data provenance,and so forth. To address these factors OBCS includesterms such as generation of missing data, which repre-sents a planned process that generates possible values ofmissing data. OBCS also includes the term processing in-compatible data that represents a data transformationprocess that attempts to transforms incompatible datainto data that are compatible. Different methods can beused to support these processes.After data are collected, the data often need to be reor-ganized or processed by different data transformation pro-cesses that transform the data into a format suitable forstatistical data analysis. A typical method is to transforma list of data by associating data items with probabilityvalues yielding one or other type of probability distribu-tion, for example, normal (or called Gaussian) distribu-tion (Fig. 5). Such a distribution follows a specificprobability density function, a term that is also includedin OBCS. Normalization data transformation is a com-monly used data transformation in statistics that adjustsvalues measured on different scales to a notionally com-mon scale and makes variables comparable to each other.OBCS defines 34 different types of normalizationmethods. Other data processing methods applied beforestatistical data analysis  for example permutation, sort-ing, or data partitioning  are also represented in OBCS.Figure 1 represents are two types of statistical dataanalysis methods called descriptive and inferential, re-spectively. A descriptive statistical data analysis quanti-tatively summarizes a feature of a collection of data. Itincludes the application of many statistical operationsthat summarize a data set in terms of its arithmeticmean, standard deviation, empirical probability distribu-tion, and so on. An inferential statistical data analysis,in contrast, infers properties of a collection of datathrough analysis of data. An inferential statistical ana-lysis includes testing hypotheses and deriving estimates.These methods can be performed on multiple data setsand thereby generate a p-value, R-squared value, likeli-hood ratio, or other quantitative confidence value(Fig. 3c). In total, OBCS now includes 12 types of de-scriptive statistical analysis methods and 92 types of in-ferential statistical analysis methods.Fig. 4 Illustration of logical axioms and subclasses of data collection in OBCS. This is a screenshot of Protégé OWL editor display of OBCS. Asshown here, this term is defined by an equivalent axiom, four subclass axioms, and one inherited subclass axiom. There are 12 subclasses underdata collectionZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 6 of 13OBCS defines a statistical variable, including independentand dependent variable, as a directive information entitythat is about a data item and can only be realized in a stat-istical analysis. Without a statistical analysis, a statisticalvariable does not exist. For example, a clinical study mayhave collected data about age, sex, weight, and whether dia-betes. Age data item is about age quality. A statistician canspecify that an independent variable (for instance an age-independent variable) is about an age data item and adependent variable is about whether a given individual hasdiabetes, and then test whether age significantly affects thelikelihood of the occurrence of diabetes in a human popula-tion. The age data item is about the age quality of a humansubject. A scalar measurement datum includes two parts:data value and data unit. If a human subject is 20 years old,the age data item can be represented as:((has value value 20) and (has measurement unitlabel some year)) and is about some age.The age data item will vary from subject to subject,which is the reason why we can get an age data set in astudy. For example, if a clinical study includes three hu-man subjects whose ages are 20, 40, and 50 years, thenthe three age data items form an age data set.One or multiple data sets can be visualized as an image orgraph by performing a process of data visualization (Fig. 2).Two logical axioms are defined for data visualization:has_specified_input only/some data itemhas_specified_output only/some (graph or image)Currently, OBCS includes four subclasses of datavisualization: clustered data visualization, gene listvisualization, classified data visualization, and back-ground corrected data visualization. To support datavisualization, OBCS imports 25 terms from the graphbranch in the STATO ontology.Based on the results of a statistic data analysis, we candraw a conclusion based on data (Fig. 2). The descrip-tive statistical analysis results (such as median andmode) describe the features of a data set. The result ofan inferential statistical data analysis, such as a p-value(a type of quantitative confidence value), is used to helpus to draw a conclusion either accepting or rejecting ahypothesis. One important quantitative confidencevalue is R-squared value, which is often used for analyz-ing prediction problems. The class draw a conclusionbased on data also includes subclasses corresponding todifferent sorts of biological feature conclusions, such ascomparative phenotypic assessment and assigning geneproperty based on phenotypic assessment.Next we focus on the OBCS modeling of the two usecases introduced in the Methods section above.OBCS statistical representation of a systems vaccinologyuse caseThe first use case, which is of a sort typically found in highthroughput biomarker analysis, is a study selected fromthe field of systems vaccinology [21]. Each of the twentyeight enrolled human subjects was vaccinated once withFluarix, a trivalent inactivated influenza vaccine (TIV). Atdays 0 (baseline), 3 and 7 post vaccination, blood speci-mens were collected from human subjects and from theseFig. 5 OBCS modeling of statistical terms related to normal distribution. The normal (or Gaussian) distribution is a continuous probabilitydistribution of a numerical data set that follows the normal distribution probability density function. The formula of the density function is at thebottom of the figure and included in OBCS as a mathematical formula annotation. A normal distribution transformation is able to transform adata set to normally distributed data setZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 7 of 13samples peripheral blood mononuclear cells (PBMCs)were prepared. RNA extracts were then prepared fromthe PBMCs and used in a transcription profiling assayexamining expression of a large number of genes usingAffymetrix microarray technology. The basic investigationprocedure can be presented using OBI. For the statisticalsteps however the additional resources of OBCS areneeded. To illustrate how OBCS is used for annotation ortagging of statistics workflows we single out one humansubject (subject X) (Fig. 6). The initial statistical data ana-lysis step is data collection from experiment. Such a datacollection process has specified output the raw gene ex-pression data at different time points post vaccination.After the raw data collection, the gene expression resultsfor individual genes such as TNFRSF17 (tumor necrosisfactor receptor superfamily, member 17) at days 0 (base-line) and 7 post vaccination were normalized using theRobust Multi-array Average (RMA) method [16]. TheRMA (OBCS_0000140) statistical method has the follow-ing two asserted axioms (Fig. 2b):RMA: has_part some background correction datatransformationRMA: has_part some quantile transformationThe above RMA data normalization process ensuresthat all Affymetrix microarray data from the study canbe analyzed. Normalized gene expression values for anygiven gene (for example TNFRSF17) across all subjectsat different time points can then be used in a range ofstatistical tests, including ANOVA, signal-2-noise ana-lysis (S2N) and significance analysis of microarrays(SAM). The p-values for these tests are then reported,the p-value of <0.05 obtained for all 3 tests indicatingthat a null hypothesis that the two groups (baseline Day0 vs Day 7) have the equal means of gene expression in-tensities can be rejected. From this we can draw a con-clusion that the gene in question is significantlyregulated in study subjects consequent to influenza vac-cination (Fig. 6).Note that for simplicity some important experimentalfactors (such as sex, age, and microarray format) andstatistical results (for example fold change) are not rep-resented in the Figure. These factors can be representedusing OBO Foundry-related ontologies such as PATOand the Experimental Factor Ontology (EFO) [24]. Rep-resentation of these factors can be incorporated in astatistical analysis using the OBCS approach. The Figureprovides, we believe, a good illustration of how OBI andOBCS are used to annotate the sorts of biostatisticsstudies commonly found in high throughput molecularassay analysis and biomarker analysis.For users to explore and better understand how OBCScan help biomedical data annotation and analysis, weprovided an example data set and supplementalFig. 6 Ontological representation of an influenza microarray study. In this example, each of 28 human subjects was vaccinated once with aninfluenza vaccine [21]. At day 0, 3, and 7 post vaccination, human blood samples were extracted, peripheral blood mononuclear cell (PBMCs)were isolated from the blood samples, and RNAs were prepared from PBMCs. An Affymetrix microarray experiment was then conducted usingthe RNA samples. After RMA data normalization, the gene expression data from different groups (separated on the basis of time) were used forthree types of statistical tests (ANOVA, S2N and SAM). All the boxes represent instances, labelled by the class names of these instances. All therelations are italicizedZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 8 of 13materials available for downloading at https://github.-com/obcs/obcs/wiki/OBCS-example. The example con-tains an Ontodog-generated OBCS subset including allthe terms, relations, and axioms introduced in the Fig. 6use case. We created instances of the OBCS subset clas-ses to present transcriptional expressions of 3 genes(TNFRST17, MAPK1, and CNOT2) in the PMBCs col-lected from three individuals at day 0, 3, and 7 after anadministration of TIV. This example shows how dataitems (e.g., gene expression intensities) can be groupedtogether to form a data set (e.g., the set of expression in-tensities of the genes in a microarray chip for one sam-ple), and how data sets themselves can be groupedtogether to form a data matrix, which is data set ofhigher order (for example a set of gene expression dataobtained from applying multiple microarray chips tomultiple samples). As shown in Fig. 6, planned pro-cesses link the data sets collected from experimentsto the conclusion based on data drawn from statis-tical data analysis.OBCS representation of a clinical outcomes research use caseThe second use case is focused on medical informat-ics analysis in clinical research. In a study of clinicaloutcomes of nursing services [22], data were obtainedfrom electronic medical records (EMR) and trans-formed to data with standard measurement units.This study examined the effect of variable levels ofnurse staffing on inpatient mortality. The statisticalanalysis of these variables is represented ontologicallyin OBCS (Fig. 7). On a shift-by-shift basis, the uniton which each patient was located was identified andunit characteristics and staffing data for the shift weremerged with outcomes-relevant patient data. Thisprocess resulted in 3,227,457 separate records, withinformation for each patient relating to each shiftduring the period in which they were hospitalized.The records included measures of patient-level andunit-level characteristics, nurse staffing, other shift-specific measures, and patient mortality. In this study,the initial statistical data analysis step is data collec-tion from an observation, rather than from an experi-ment. Covariates were constructed to adjust forfactors not controlled in the study and yet still affect-ing the dependent variable, which is in this case mor-tality. Such a process has specified output thedifference between targeted and actual staffing levelsat different time points during hospital stay. The rela-tive risk of increased mortality was then estimated usingCox proportional hazards models. Reported statistical re-sults included p-value, mean, standard deviation, confi-dence interval, and standardized mortality ratio. Theobtained p-value of <0.05 indicates that a null hypothesisof equal average of mortality ratios between those withand without exposure to understaffed shifts was rejected.Therefore, we can draw a conclusion that understaffedshifts were significantly associated with increased mortal-ity (Fig. 7).Fig. 7 Ontological representation of a clinical study. This use case study analyzed the effects of variable levels of nurse staffing on inpatientmortality [22]. The data was obtained from eMedical records and then transformed. Cox proportional hazards model estimation (a type of survivalanalysis) was performed to identify the effect of hospital unit shift rates (independent variable) on the patient mortality rate (dependent variable).A p-value of <0.01 was used to draw a conclusion of the statistical significance between these two variablesZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 9 of 13OBCS data queryOBCS can be queried using SPARQL, a Resource De-scription Framework (RDF) query language for retrievingontology data stored in the RDF format [25]. Figure 8shows how a simple SPARQL query can be used to iden-tify the number of methods under the OBCS class stat-istical data analysis (OBCS_0000001). As shown inFigs. 1 and 3, there are two types of statistical data ana-lysis: inferential and descriptive. The SPARQL query canrecursively search all the layers of the two branches andidentify the total number of subclasses in each branch.DiscussionThere is a critical need to standardize data representa-tion, including standardization and formal representa-tion of statistical methods. In this paper, we haveintroduced Ontology of Biological and Clinical Statistics(OBCS), focusing on the introduction of high level sta-tistics terms in OBCS and on how OBCS can be used incombination with OBI for the ontological representationof statistics-related biological and clinical data process-ing. OBCS provides a timely source of statistical termsand semantics in various areas of biological and clinicalstatistics. OBCS provides a consensus-based rigorouslycurated representation of the steps involved in statisticspipelines in different domains in biological and clinicalfields, thereby supporting reproducibility of research.The current OBCS development team is composed ofresearchers from a number of complementary back-grounds. Jie Zheng (PhD) is an experienced ontology de-veloper and biomedical researcher. Marcelline Harris(PhD, RN) is a domain expert in clinical statistics. AlfredHero (PhD, with appointment in the Department of Sta-tistics at the University of Michigan) is a domain expertin statistics and biostatistics. Anna Maria Masci (PhD) isan immunologist who is well trained in ontology. Dr YuLin (MD, PhD) is experienced in both clinical and bio-medical informatics and ontology development. BarrySmith (PhD) is a co-creator of the BFO ontology and ofthe OBO Foundry. Yongqun He (DVM, PhD) is anontology developer and a domain expert in vaccine andimmunology research as well as computer science.OBCS and OBI are closely related. OBCS extends OBIby focusing on data collection, normalization and statisticalanalysis performed on data. Some core terms in OBCS aretaken from OBI but the term coverage in OBCS includesterms relating not only to data generated through experi-ments but also to data from other resources such as surveystudies, text mining, clinical observations, online databases,and so on. While many statistical data analysis methodsand related terms, including ANOVA and p-value, areused quite generally, there are also statistical methods andterms that are applied only in certain specific domains.The RMA [16] and GSEA (gene set enrichment analysis)[26] methods, for example, are used only in relation to bio-logical data. Since OBCS originated from and inherits itscoverage domain from OBI, it places its emphasis on bio-medical statistics. However, OBCSs coverage goes beyondthat of OBI, since it contains terms relating to statisticalmethods commonly used in clinical fields. The major pur-pose of OBCS is to provide a standardized representationof statistical processes, methods and algorithms acrossboth the biological and clinical science. OBCS can serve asan integrative metadata platform to support statistical datarepresentation, analysis, and data integration and facilitatestatistical validation, reproducibility, and discovery of pub-lished research involving statistical analysis.Several ontologies have been developed that containterms related to statistics [2729]. Above all, the STATO(http://stato-ontology.org/) was recently announced.While OBCS is focused on biological and clinical statis-tics, STATO aims to cover a broader scope and toFig. 8 SPARQL query of the number of statistic data analysis methods in OBCS. This SPARQL query was performed using the Ontobee SPARQLquery website (http://www.ontobee.org/sparql/). This query found 108 statistical data analysis methods in OBCS. These terms are all under theOBCS term statistical data analysis (OBCS_0000001)Zheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 10 of 13include also natural science domains outside the life sci-ences. OBCS and STATO thus focus on different aspectsof statistics. After STATO was officially released, wereused STATO terms in OBCS wherever applicable. Wehave imported many terms with a focus especially onthe graph branch of STATO. To deal with the residualsets of terms common to both OBCS and STATO, weadded the STATO-OBCS mapping information usingthe oboInOwl:database_cross_reference annotationproperty in OBCS. STATO terms contain many logicalaxioms, which can be used for generating queries suchas those listed at http://stato-ontology.org/. Such imple-mentations are very helpful will be incorporated withinOBCS wherever possible. On the other hand, the majordriving use case for OBCS is to serve annotation of bio-medical data in a way that promotes reproducibility  andeventual automation  of statistical data analyses for thewhole sets of biomedical experimental data [30]. As a re-sult, OBCS focuses more than STATO on providing de-tailed data analysis pipelines, as illustrated in Figs. 2, 5, 6,and 7, to support systematic statistical data analyses. Weare collaborating with STATO to achieve a consensus div-ision of development effort in the future.The OntoDM ontology [27], developed with a focuson data mining, also has some statistical components, asdoes the Hypothesis and Law Ontology (HELO), whichfocuses on representing probabilistic scientific know-ledge and hypothesis evaluation [28]. In addition, theOntology of Clinical Research (OCRe) incorporatessome statistics terms used in clinical studies [29]. How-ever, OCRe, in contrast to all the aforementioned ontol-ogies, does not provide definitions for its terms, and it isnot aligned with BFO or OBI. After careful comparisonand examination, we found that none of these ontologiesfocuses on the sort of comprehensive representations ofbiological and clinical statistics that meets our needs informally representing the statistical tools and methodsused in data collection, organization, analysis, presenta-tion and interpretation. OBCS is the first ontology thatsystematically represents the five major processes in stat-istical studies (Fig. 2), and lays out general design pat-terns for representing statistical distributions (e.g.,normal distribution as shown in Fig. 5) and relatedterms. OBCS also includes many statistics terms unavail-able in other ontologies.Our two use cases lie at opposite ends of the trans-lational science continuum from T0 (basic biomedicalresearch) to T4 (translation to population). These usecases demonstrate the usage of OBCS in basic andtranslational biomedical research. We are developingOBCS applications addressing other points on thiscontinuum, including T1 (basic to clinical translation),T2 (demonstrating efficacy), T3 (translation to prac-tice) [31, 32].In addition to the two use cases detailed in this manu-script, OBCS is currently being used for the standardizedrepresentation and annotation of genomics data analysis inthe Beta Cell Genomics Database (http://www.betacell.org/gbco/). The OBCS design pattern and strategies are consist-ent with previous research of using the combination of OBI[10] and the Vaccine Ontology [33, 34] to support the stat-istical meta-analysis of variables contributing to the effectsof protective immunity [14, 35]. OBCS is being tested inthe work of the ImmPort project team for standard datarepresentation and statistical data analysis, and it is beingevaluated also for its ability to support statistical softwaretools such as RImmPort [4] and Python for PopulationGenomics (PyPop; http://www.pypop.org/) developed topromote reproducible and automated analysis of biologicaland clinical data. Since OBI, VO, PATO, and otherOBO ontologies provide many terms to represent fea-tures of biological and clinical studies, OBCS can usethe corresponding terms when representing the corre-sponding experimental variables (such as vaccine,gender, age, mortality) in a statistical analysis.In the future, OBCS will be further developed to in-clude new statistical methods and inference procedures,support data integration, and be applied to more re-sources. The OBCS development is driven primarily byuse cases. Our current use cases have been focused onvaccinology, immunology, flow cytometry, microarray,and clinical nursing scenarios. Many more statistics-specific questions and cases will still be generated andstudied in these areas. In addition to standard statisticaldata representation and integration, OBCS will be usefulalso in supporting more consistent extraction of data,thereby allowing new kinds of search (for example: forall data derived using a specific type of analysis or a spe-cific type of variable). Many statistical methods havebeen implemented in different software programs suchas many statistical programs available in BioConductor[36] and RImmPort [4]. We plan to relate OBCS statis-tical analysis methods to corresponding software pro-grams by reusing the Software Ontology (SWO) terms[37]. Such linkage between OBCS statistical methodsand software programs can support standard statisticalmethod representation and software integration, repro-ducible data analysis, and interoperable communicationsbetween different software programs. We believe thatbiomedical and clinical databases and software programstargeting big data analysis will benefit considerably fromthe standardized definitions and logical representationsof statistics terms and relations [30] of the sort whichOBCS provides.ConclusionThe Ontology of Biological and Clinical Statistics(OBCS) is a community-based open source ontology inZheng et al. Journal of Biomedical Semantics  (2016) 7:53 Page 11 of 13the domain of biological and clinical statistics. We pre-sented the rationale, history, scope, contents, top levelhierarchy, and a general design pattern of OBCS. Sec-ond, we provided detailed accounts of the main branchesof OBCS, including data item, statistical data analysis,and data collection. Third, the OBCS approach to stat-istical terms related to normal distribution is presented,and it is shown how this approach can be generalized toother statistical distributions. Fourth, two OBCS usecase are studied and presented, demonstrating howOBCS can be applied to the ontological representationof real statistical studies. Lastly, a SPARQL query ex-ample (Fig. 8) is provided to demonstrate how to quicklyquery OBCS information stored in an RDF triple store.Overall, we believe that OBCS is a timely ontology ableto represent statistics-related terms and their relations ina rigorous fashion, facilitate standard data analysis andintegration, and support reproducible biological andclinical research.AcknowledgmentsWe thank Drs. Jennifer Fostel, Bjoern Peters, Alan Ruttenberg, Larisa N. Soldatova,Christian J. Stoeckert Jr., and the OBI Consortium for their valuable discussions andfeedback.FundingThis work was supported by NIH grants R01AI081062 (YH and YL) and5R01GM93132 (JZ), NIAID contracts HHSN272201200028C and NIGMS andNCATS grants R01GM080646 and 1UL1TR001412 (BS) and NIH U01CA157703and DOD W81XWH-15-1-0467 (AMM), and ARO grant W911NF-15-1-0479(AH) and DARPA grant W911NF-15-1-0161 (AH). The article-processing chargewas paid by a discretionary fund from Dr. Robert Dysko, the director of theUnit for Laboratory Animal Medicine (ULAM) in the University of Michigan.Authors contributionsJZ and YH are primary developers of OBCS, generated initial use case modelsand all figures, and prepared the first draft of the manuscript. MRH addedand annotated many terms associated with clinical statistics and supportedthe clinical use case modeling. AMM supported the modeling and analysis ofthe influenza microarray use case. YL participated in many discussions andprovided input in ontology development. AH provided many comments andsuggestions as a domain expert in statistics and biostatistics. BS providedvaluable suggestions on ontological matters including alignment with BFO.YH also initiated the OBIstat project that was a prototype of OBCS. Allauthors edited and approved the manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Department of Genetics, University of Pennsylvania Perelman School ofMedicine, Philadelphia, PA 19104, USA. 2Division of Systems Leadership andEffectiveness Science, University of Michigan School of Nursing, Ann Arbor,MI 48109, USA. 3Department of Biostatistics and Bioinformatics, Duke MedicalCenter, Duke University, Durham, NC 27710, USA. 4Department ofMicrobiology and Immunology, Unit for Laboratory Animal Medicine,University of Michigan Medical School, Ann Arbor, MI 48109, USA.5Department of Electrical Engineering and Computer Science, Department ofBiomedical Engineering, and Department of Statistics, Michigan Institute ofData Science, University of Michigan, Ann Arbor, MI 48109, USA.6Department of Philosophy and National Center for Ontological Research,University at Buffalo, Buffalo, NY 14203, USA.Received: 7 January 2016 Accepted: 6 September 2016DATABASE Open AccessThe cellular microscopy phenotypeontologySimon Jupp1*, James Malone1, Tony Burdett1, Jean-Karim Heriche2, Eleanor Williams3, Jan Ellenberg2,Helen Parkinson1 and Gabriella Rustici1AbstractBackground: Phenotypic data derived from high content screening is currently annotated using free-text, thuspreventing the integration of independent datasets, including those generated in different biological domains, suchas cell lines, mouse and human tissues.Description: We present the Cellular Microscopy Phenotype Ontology (CMPO), a species neutral ontology fordescribing phenotypic observations relating to the whole cell, cellular components, cellular processes and cellpopulations. CMPO is compatible with related ontology efforts, allowing for future cross-species integration ofphenotypic data. CMPO was developed following a curator-driven approach where phenotype data wereannotated by expert biologists following the Entity-Quality (EQ) pattern. These EQs were subsequently transformedinto new CMPO terms following an established post composition process.Conclusion: CMPO is currently being utilized to annotate phenotypes associated with high content screeningdatasets stored in several image repositories including the Image Data Repository (IDR), MitoSys project databaseand the Cellular Phenotype Database to facilitate data browsing and discoverability.Keywords: Ontology, OWL, Cellular phenotype, ImagingIntroductionRecent advances in imaging techniques make the studyof complex biological systems feasible, particularly at thecellular level, complementing existing omics ap-proaches, most notably genomics and proteomics, by re-solving and quantifying spatio-temporal processes withsingle cell resolution [1]. High content screening (HCS)is an imaging based multi-parametric approach that al-lows the study of living cells. HCS is used in biologicalresearch and drug profiling, to identify substances, suchas small molecules or RNA interference (RNAi) re-agents, which can alter the phenotype of a cell. It canalso be used to look at the effect of knocking out genescompletely, or to determine protein localization bymodifying genes to produce tagged proteins that can bevisualized. Phenotypes may include morphologicalchanges of a whole cell, or any of its cellular compo-nents, as well as alteration of cellular processes.Correlative analysis of cellular phenotypes, specific toindividual genes, with morphological imaging data fromdiseased tissue specimens (both human and mouse tis-sues) allow us to link phenotypic data to associatedimage annotations and metadata, leading to a powerfulpredictor of disease biomarkers as well as drug targets.For example, when a certain cellular phenotype, like mi-totic delay or multi-nucleated cells, observed in cellsafter gene knockdown experiments, is also observed incells of a cancer tissue, this could give us an indicationof which gene(s) might be involved in the aetiology ofthe disease, in that specific tissue. Knowledge of thefunctional implications of somatic tumor mutations canthus be used to design more targeted drug therapies.Data derived from live cell imaging is typically associ-ated with rich metadata, including genetic information,and can be more easily interpreted and linked to under-lying molecular mechanisms. As we move to higher or-ganisms, such as mouse and human, the degree ofmetadata available decreases (e.g. no genetic information* Correspondence: jupp@ebi.ac.uk1European Bioinformatics Institute (EMBL-EBI), European Molecular BiologyLaboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UKFull list of author information is available at the end of the article© 2016 Jupp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 DOI 10.1186/s13326-016-0074-0is available for diseased human tissues), alongside thefeasibility of assays that can be carried out in such or-ganisms (e.g. genetic engineering is only possible in celllines and mouse models). Taking this into consideration,it becomes evident that integrating imaging datasetsfrom different biological domains could greatly advanceour understanding of the molecular mechanisms under-lying specific diseases.Due to its late arrival on the omics scene, the im-aging field has not yet achieved the same degree ofstandardization that other high-throughput approacheshave already reached [1], thus hampering integration ofimage data with current biological knowledge. Standardsare needed for describing, formatting, archiving and ex-changing image data and associated metadata, includingsuitable nomenclatures and a minimal set of informationfor describing an imaging experiment. This is crucial toenable the establishment of databases and public reposi-tories for image data and allow for the integration of in-dependent datasets.The use of ontologies to annotate data in the life sci-ences is now well established and provides a means forthe semantic integration of independent datasets. Des-pite the availability of several species-specific ontologiesfor describing cellular phenotypes (e.g. the Fission YeastPhenotype Ontology), there isnt an appropriate infra-structure in place to support the large-scale annotationand integration of phenotypes across species and differ-ent biological domains.As part of the BioMedBridges project,1 efforts are un-derway to integrate biological imaging datasets providedby emerging biomedical sciences research infrastruc-tures, including Euro-BioImaging,2 for the provision ofcellular image data; Infrafrontier,3 for mouse tissueimage data, and BBMRI/EATRIS,4 for human tissueimage data. Such infrastructures are generating a wealthof imaging data that can only be made interoperablethrough consistent annotation with appropriateontologies.There has been much work published on the develop-ment of cross-species phenotype ontologies and theirbenefits [2]. To date ontologies describing phenotypesexist for a host of species including mammalian pheno-types (MP; [3]), Ascomycetes (APO; [4]), S. pombe(FYPO; [5]) and C. elegans (WPO; [6]). There are alsowell established ontology design patterns for modelingphenotypes in a species and domain independent man-ner that utilise the Phenotype and Trait Ontology(PATO) [7]. These phenotypic descriptions are basedaround the Entity-Quality model (EQ) that refers to de-scribing a phenotype in terms of an Entity (E), from oneof many given reference ontologies, such as Gene Ontol-ogy (GO, [8]) and an associated Quality (Q), from PATO[9]. For example, a large nucleus phenotype could beexpressed in EQ using the entity term nucleus[GO:0005634] from GOs cellular component and thequality term increased size [PATO:0000586] fromPATO. This model has been adopted by a range ofmodel organism databases for the annotation of variousphenotypes ranging from disease, anatomical and cellu-lar phenotypes [10].Ontology languages, such as the Web Ontology Lan-guage (OWL), allow us to express logical definitions forclasses that describe class membership based on quanti-fied relationships to other classes. The Basic FormalOntology (BFO) defines the inheres in [BFO:0000023]relationship that can be used to capture the relationshipbetween qualities, which in BFO are specificallydependent continuants, and the bearer of those qualities,which are typically independent continuants. For ex-ample, in order to logically define a large nucleusphenotype we say that the quality of increased sizeinheres in the bearer, which in this case would be thenucleus. We can express this relationship logically inOWL using existential quantification to assert that theclass of all large nucleus phenotype is equivalent to theclass of things that have an increased size quality thatinheres in a nucleus. We could go on to further de-scribe another class of phenotypes, such as a more gen-eral nuclear size phenotype and by virtue of the factthat increased size is a subclass of a more general sizequality, use an OWL reasoner to automatically classifylarge nucleus phenotype as a subclass of nucleus sizephenotype. Highly scalable reasoners, such as ELK [11],have made it practical for ontology engineers to fully ex-ploit this expressivity when working with large ontol-ogies. In the case of building phenotype ontologies, itmeans we can now build logical class definitions for alarge number of phenotypes following the EQ pattern,and let the reasoner do the work to classify those pheno-types and infer equivalence across different phenotypeontologies.A previous effort to develop a species neutral cellu-lar phenotype ontology (CPO) was undertaken byHoehndorf et al. [12]. The CPO was automaticallygenerated and includes logical class definitions com-posed from GO and PATO terms. Whilst in principlethis is a reasonable approach, in practice the resultingontology was difficult to work with and did not pro-vide a good vocabulary for data annotation. The sizeof the ontology coupled with limitations in standardontology authoring software made it impractical toextend and maintain this ontology whilst keeping insync with GO and PATO via the automatic gener-ation process. The size and automatic label creationstrategy also made it difficult for the biocurators tofind terms for annotating data. It would have been aconsiderable amount of effort to manually clean theJupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 2 of 8CPO to make it fit for purpose as a general annota-tion vocabulary for imaging datasets.Our approach was therefore to build CMPO from theavailable data, using a post-composition approach wherephenotypes were manually annotated with ontologyterms that were later used to compose new stablephenotype terms in the ontology. These new terms wereannotated with appropriate meta-data, such as synonymsand definitions that reflect how the terms are used inthe data and literature.ResultsAs of release 1.9 CMPO contains 361 phenotype terms.CMPO provides a root class called cellular phenotypewhich is further divided into five major sub-types,namely; cell process phenotype, cellular componentphenotype, molecular component phenotype, single cellphenotype and cell population phenotype. (Fig. 1). Eachof these categories represents a different level of granu-larity for which we see phenotype descriptions in thedata. Every effort is made to ensure that each CMPOterm has an equivalence axiom that describes the termusing an OWL class expression. We strive to avoidasserting subclass axioms between named phenotypeclasses and instead use a reasoner to infer classificationusing logically defined classes.Cell process phenotypeThe cell process phenotypes aim to capture phenotypicdescriptions at the level of cellular processes. Using theManchester OWL syntax (MOS) notation5 we can ex-press a CMPO cell process phenotype as being logicallyequivalent to the anonymous OWL class has_part some(process quality and (inheres_in some biological_pro-cess)), where the process quality comes from PATO andthe biological_process term is from GO. In some cases,such as the CMPO mitotic process phenotype, wewould like to capture all phenotypes that inhere eitherthe GO mitotic cell cycle or part of the GO mitotic cellcycle. Whilst OWL provides the vocabulary for union(OR) operators in OWL class descriptions, this wouldtake CMPO outside of the OWL-EL6 sublanguage. Inorder to keep CMPO within EL and have the ability tocompute desirable subclass relations, we used two separ-ate equivalence class axioms e.g. mitotic process pheno-type is equivalent to has_part some (process qualityand (inheres_in some (part_of some mitotic cell cycle)))and equivalent to has_part some (process quality and(inheres_in some mitotic cell cycle)).There are also cases where phenotype descriptions at-tempt to capture the absence of a process e.g. absence ofmitotic chromosome decondensation phenotype. WhilstPATO contains a quality called lacking processual parts,it would be incorrect to assert that absence of mitoticchromosome decondensation is a quality that inheres inthe mitotic chromosome decondensation process itself.To deal with such cases we make use of the BFO ontol-ogy specifically depends on at all times [BFO:0000070](also referred to as s depends on or towards) relation,that can be used to relate a relational quality or dispos-ition to a relevant entity. For absence of mitotic chromo-some decondensation phenotype we describe it as alacking processual parts quality that inheres in the cellcycle as a whole, where the lacking processual partsquality specifically depends on the mitotic chromosomedecondensation phenotype. The fully qualified equivalentclass description for this phenotype is has_part some(lacking processual parts and (towards some mitoticchromosome decondensation) and (inheres_in some cellcycle)). A similar pattern is used throughout CMPO todeal with cases of phenotypes where the phenotype de-scribes the absence of a particular entity.HCS data often includes phenotypes relating to pro-tein localisation in the cell. CMPO aims to describeFig. 1 Visualisation of the top-level terms in the CMPO phenotypeontology showing cell process, single cell, cellular component andmolecular component phenotypesJupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 3 of 8protein localisation phenotypes in terms of the proteinlocalisation process that is occurring along with detailsof the protein complex being transported and in somecases the target and end location of the protein. Usingthe transports or maintain localization of  and has tar-get end location object properties from the OBO Rela-tion Ontology we describe a complex phenotype asequivalent to has_part some (occurrence quality and(inheres_in some (protein localization and (transportsor maintains localization of  some polypeptide) and (hastarget end location some cellular_component)))). TheGO provides good coverage of protein localisation pro-cesses that CMPO has utilised to develop a branch ofprotein localisation phenotypes relating to various cellu-lar components and the CMPO design pattern is con-sistent with the pattern used in the OWL edition of GO.Cellular component phenotypesAll cellular component phenotypes are logically de-scribed as any quality (non processual quality) thatinheres in any cellular component from GO e.g. inMOS notation has_part some (quality and (inheres_insome cellular_component)). Typically these observa-tions relate to the morphology or position of a par-ticular component in a cell. In order to drive all thenecessary inference to infer subclasses of a generalterm such as nuclear phenotype we describe theseterms using three equivalence class axioms to capturequalities of the nucleus, nuclear parts, and any qual-ities that11 are towards the nucleus.Molecular component phenotypesThe molecular component phenotype branch describesphenotypes at the level of molecules in the cell. All mo-lecular component phenotypes are logically equivalent tohas_part some (quality and (inheres_in some molecularentity)) where the molecular entity is a bio-moleculefrom the ChEBI ontology [13]. To date this branch ofthe CMPO only contains phenotypes terms relating tothe shape of DNA molecules within the cell.Single cell phenotypesSingle cell phenotypes in CMPO describe phenotypesthat are observed at the level of the whole cell. Singlecell phenotypes are described as logically equivalentto has_part some (quality and (inheres_in somecell invitro)) where cell in vitro is imported from the CellOntology [14]. The single cell phenotypes are furtherclassified in terms of cellular component number,whole cell morphology, cell movement, cell nucleationand cell viability.Cell populationCMPO describes a cell population phenotype as a collec-tion of qualities that inhere in a population of cells. Wedistinguish between qualities of the population as awhole and qualities of individual cells within the popula-tion using the Relation Ontology bearer of  relationship.For example, CMPO describes a fewer mitotic meta-phase cells phenotype as a has fewer part of type qualitythat inheres in a population that bears a mitotic meta-phase phenotype. In MOS we can define fewer mitoticmetaphase cells as equivalent to cell population thathas_part some (has fewer parts of type and (bearer of some mitotic metaphase phenotype)).CMPO annotation propertiesCMPO follows many standard conventions from the OBOfoundry for ontology term metadata. Every CMPO termmust have an rdfs:label and definition using the Informa-tion Artifact Ontology (IAO) definition [IAO:0000115]predicate. In cases of phenotype terms that could betraced back to a source publication or dataset, we usedthe definition source [IAO:0000119] predicate from IAOto link the term to the publication. The standard set ofOBO synonym properties are also used to capture exact,broad and narrow synonyms for a term. The sourceCMPO OWL file imports the full GO and PATO ontologyto support development of the ontology and to drive theinference. Finally we define a CMPO slim so that we caneasily extract a simplified version of CMPO for a releaseof the ontology that exclude all the PATO and GO terms.CMPO availabilityThe CMPO homepage (http://www.ebi.ac.uk/cmpo) pro-vides access to the ontology and issue tracker for sub-mitting new term requests. The source ontology forCMPO is hosted on GitHub7 and it is also available fromthe NCBO BioPortal [15] and the EMBL-EBIs OntologyLookup Service (OLS).8Applications of CMPOIn the context of the BioMedBridges project, we want todemonstrate the power of interoperability of large-scaleimage data sets from different biological scales to enabledrug target and biomarker discovery for human diseases,focusing on cancer as an example.CMPO is being used to annotate mitotic phenotypesobserved in live human cells, as well as cellular pheno-types from tissue microarrays of diseased tissues fromboth human patients and mouse models. Analysingphenotypic correlations between cellular and tissue datasets, and linking imaging data with molecular data, in-cluding the cancer genome sequence and expressiondata, will allow for in silico validation of the predictionsand prioritization of biomarkers for validation in clinicalJupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 4 of 8research. In particular, we focus on genes with a functionin controlling cell cycle and cell division, as well as inva-sive behaviour, for which comprehensive molecular andcellular datasets are available.CMPO is currently being utilized to annotate pheno-types associated with HCS datasets stored in the ImageData Repository (IDR), a next generation repository cur-rently being developed to: (i) provide easy access to ref-erence image data linked to peer-reviewed publicationsand support browsing, search and visualization of imagedata and metadata; (ii) facilitate the establishment andadoption of data standards to enable interoperability ofimage data; (iii) link such data to other biomoleculardata resources (e.g. genomics databases, structural data-bases and functional annotation) and (iv) build a compu-tational resource to support the re-analysis of imagedata and the development of new computational tools.IDR is built upon established, actively developed opensource platforms and applications, including theOMERO software for visualization, management andanalysis of biological microscope images [16]. TheOMERO API is currently being extended to explicitlysupport ontological annotations and access CMPOthrough OLS to look up of additional information andsubsumption queries [17]. Since CMPO has been appliedto annotate phenotypes associated with IDR data (Fig. 2),50 new phenotype terms have been added to the ontol-ogy. CMPO has also been integrated into the MitoSysproject database9 and the Cellular Phenotype Database[18] to facilitate data browsing and discoverability. Workis in progress to add a functionality for ontology basedbrowsing in CellCognition [19].MethodEleven imaging datasets were initially sourced to collecta set of candidate phenotypic descriptions for manualontology annotation [2030]. Our approach was to an-notate the phenotypes with terms from GO and PATOto generate EQ based annotations that would be laterpost-composed to form new CMPO terms. We devel-oped a simple Web application called Phenotator for thedata providers to submit and annotate their phenotypeswith an EQ. The Phenotator is built using services fromthe NCBO BioPortal [15] to generate simple drop downmenus and autocomplete search functionality to guidethe users in generating EQs with appropriate terms(Fig. 1). Phenotator has a feature to export the collectedEQ annotations as an OWL file containing new termsthat are logically defined according to the SUBQ pat-tern,10 which can be expressed in Manchester OWL syn-tax as (has_part some (<Quality> and inheres_in some<Entity>)). One hundred twenty-seven phenotypeFig. 2 Screenshot of the Image Data Repository showing image meta-data that include phenotype annotation to CMPO term decreasedduration of mitotic prophase [CMPO:0000329]Jupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 5 of 8descriptions from the original 11 datasets were enteredinto Phenotator, together with 41 phenotypes collectedfrom cell migration assays (Z. Kam, personal communi-cation) and 193 phenotypes from the GenomeRNAidatabase [31]. The domain experts entered EQ based de-scriptions for a total of 201 phenotypes.The EQs were exported from Phenotator as anOWL file and loaded into the Protege OWL ontologyeditor. The generated OWL file imported the fullGene Ontology and PATO and the ELK reasoner wasused to compute an automatic classification of thepost-composed EQ terms. The biological curators andontology experts were able to use this classification toboth verify the collected EQs and inform the organ-isation of the upper level of the ontology so that theterms were classified into useful categories. After sev-eral iterations of this process, the post-composedterms were assigned permanent CMPO identifiers andrelevant metadata for each term was collected inpreparation for the initial release.CMPO accepts new terms requests via the CMPOwebsite and also accepts more structured term requestsvia the Webulous application. Webulous provides a ser-vice for specifying ontology term creation templates.These templates can be loaded into tools such as GoogleSheets using the Webulous Google Sheets Add-on,11 sothat users can submit batch requests of new terms toCMPO. The CMPO Webulous templates have been usedby the Image Data Repository (IDR) curators as a mech-anism for adding new terms to CMPO for both cellularprocess and cellular protein localisation phenotypes.CMPO releases are managed using a continuous inte-gration server and the OBO ontology release tool(Oort).12 CMPO is released as four files: a single OWLfile that contains all axioms and the full GO and PATOimport; a single Mireoted13 version of CMPO with onlyrelevant GO and PATO terms, and two simple versionsthat only contain CMPO terms that are available inOWL or OBO format. All files are made public via theCMPO website and the CMPO GitHub repository.14DiscussionCMPO follows established best-practices from the OpenBiomedical Ontology community and can provide a wayto bridge low-level cellular phenotype data across spe-cies. Merging CMPO with other post-composed pheno-type ontologies, such as FYPO, and classifying thesetogether using a reasoner shows that equivalent termscan be inferred. Some manual intervention is required toharmonise the URIs used for some of the relationshipsand many terms dont merge as expected because theOWL version of FYPO doesnt use the SUBQ patternused in CMPO. Best practices for the translations of EQannotations into OWL statements are still emerging andinconsistent use of common OBO relationships and lackof shared design patterns suggest that there is still somework to be done to integrate the various cellular pheno-type ontologies.Most cellular phenotype ontologies contain terms fordescribing features such as cell size, shape and morph-ology that are often observations that can be consideredsubjective or are only valid in the context of a particularassay. For instance, nuclei are not bright unto them-selves, but we have data where the phenotype has beenrecorded as bright nuclei in response to a particulartreatment. CMPO currently includes terms such asbright nuclear body and increased cell size, however,these terms are unlikely to have a shared meaning acrossindependent datasets. We believe having these terms inthe ontology is important as they represent the vocabu-lary of the domain, but their use without additional con-text may be of less value for data integration. Ontologiesfor describing types of microscopy assays already existand should be used in combination with ontologies likeCMPO in order to provide a meaningful annotation,however, best practices and tooling to support this kindof structured data annotations are still lacking.Despite the generality of the ontology building meth-odology applied, several challenges remain, including thelack of common design patterns that curators could con-sistently use when creating new terms, in the pre-composition phase. The need for common design pat-terns can be illustrated with an example from CMPO forthe creation of an increased cytoplasmic actin pheno-type term. This term was initially problematic to anno-tate with a basic EQ because no term for cytosolic actinexisted in GO. The curators initially used a close ap-proximation which was EQ(actin filament, present ingreater number in organism), but the fact that the actinis localised to the cytosol is lost in the EQ. To increasethe expressivity of the annotation in Phenotator a thirdcolumn was added to capture additional modifiers to theEQ resulting in annotations emerging like EQE2 (actinfilament, localised, cytosol). There are other ways thatone might consider describing this phenotype such asEQE2 (cytosol, has extra parts of type, actin filament).Guidelines and tooling that help with guiding the cura-tors to create a good EQ annotation are thereforeneeded to resolve ambiguities and develop a consistentstrategy for creating new ontology terms.Pattern-based tooling to rapidly generate new termsare emerging and these could nicely complement exist-ing tooling that are primarily aimed at annotating phe-notypes with EQs such as Phenotator and Phenote.Phenotator and Phenote do little to guide the annotatorto make a correct EQ annotation and the translation ofthese annotations to OWL typically only allows for abasic SUBQ pattern. Tools like TermGenie [32] andJupp et al. Journal of Biomedical Semantics  (2016) 7:28 Page 6 of 8Webulous offer greater flexibility for post composingterms, as they are not restricted to EQ alone and canuse more expressive design patterns for the translationof input data into OWL.ConclusionCMPO is a species neutral ontology for describing cellu-lar phenotypes that has been established according tothe best practices from the Open Biomedical Ontologycommunity. This allows CMPO to be developed inde-pendently from other phenotype ontologies, but to alsoremain interoperable via inference derived from the useof logical class descriptions. This interoperability willallow future integration of data annotated with species-specific vocabularies with imaging data annotated withCMPO.We are committed to the continued development ofCMPO and the use of CMPO in tools such as CellCog-nition and resources such as the IDR, Mitosys and Mito-check. We are developing better tools to supportbuilding ontologies from design patterns that allow us toengage the imaging user community in the future devel-opment of CMPO. Beyond the benefits in browsing andsearching phenotypic data, CMPO also enables new dataanalysis. For example, by replacing free-text annotations,CMPO makes automatic evaluation of phenotypic simi-larity possible and allows systematic exploration of thelinks between gene function and loss of function pheno-types across experiments thus facilitating the conversionof phenotypic annotations to functional annotations.Additional work to harmonise the various cellular phe-notypes ontologies with CMPO will provide new possi-bilities for integration and analysis of this kind of dataacross species.Endnotes1http://www.biomedbridges.eu/2http://www.eurobioimaging.eu/3https://www.infrafrontier.eu/4http://bbmri-eric.eu/5http://www.w3.org/TR/owl2-manchester-syntax/6http://www.w3.org/TR/owl2-profiles/#OWL_2_EL7https://github.com/EBISPOT/CMPO8http://www.ebi.ac.uk/ols9http://www.mitosys.org/10https://github.com/obophenotype/upheno/blob/mas-ter/docs/OWLAxiomatization.md11https://chrome.google.com/webstore12https://github.com/owlcollab/owltools13http://obi-ontology.org/page/MIREOT14https://github.com/EBISPOT/CMPOCompeting interestsThe authors declare that they have no competing interests.Authors contributionsCMPO was conceived by SJ, JK, JM and GR. SJ, GR and JM designed theontology and were responsible for much of its content. The Phenotator andWebulous software was developed by SJ and TB. Many of the phenotypeswere annotated with ontology terms by GR, EW and JK. All authors read andapproved the final manuscript.AcknowledgementsThe authors would like to acknowledge the following for their contributionsto CMPO; Johan Lundin, Zvi Kam, Bran Herpers, Beate Neumann, ThomasWalter, Claudia Lukas, Frauke Neff, Jennifer L. Rohn,. Funding for CMPOdevelopment was provided by EU-FP7-BioMedBridges project, grant agree-ment no. 284209. A subset of the phenotypic data used to build CMPO wasprovided by the EU-FP7-Systems Microscopy NoE, grant agreement no.258068. IDR development is supported by the BBSRC for bioscience big datainfrastructure, award BB/M018423/1.Author details1European Bioinformatics Institute (EMBL-EBI), European Molecular BiologyLaboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UK. 2European Molecular Biology Laboratory, Meyerhofstrasse 1, 69117Heidelberg, Germany. 3Centre for Gene Regulation and Expression, Universityof Dundee, Dundee DD1 5EH, UK.Received: 16 December 2015 Accepted: 10 May 2016Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 DOI 10.1186/s13326-016-0083-zRESEARCH Open AccessA corpus of potentially contradictoryresearch claims from cardiovascular researchabstractsAbdulaziz Alamri* and Mark StevensonAbstractBackground: Research literature in biomedicine and related fields contains a huge number of claims, such as theeffectiveness of treatments. These claims are not always consistent and may even contradict each other. Being able toidentify contradictory claims is important for those who rely on the biomedical literature. Automated methods toidentify and resolve them are required to cope with the amount of information available. However, research in thisarea has been hampered by a lack of suitable resources. We describe a methodology to develop a corpus whichaddresses this gap by providing examples of potentially contradictory claims and demonstrate how it can be appliedto identify these claims from Medline abstracts related to the topic of cardiovascular disease.Methods: A set of systematic reviews concerned with four topics in cardiovascular disease were identified fromMedline and analysed to determine whether the abstracts they reviewed contained contradictory research claims. Foreach review, annotators were asked to analyse these abstracts to identify claims within them that answered thequestion addressed in the review. The annotators were also asked to indicate how the claim related to that questionand the type of the claim.Results: A total of 259 abstracts associated with 24 systematic reviews were used to form the corpus. Agreementbetween the annotators was high, suggesting that the information they provided is reliable.Conclusions: The paper describes a methodology for constructing a corpus containing contradictory researchclaims from the biomedical literature. The corpus is made available to enable further research into this area andsupport the development of automated approaches to contradiction identification.Keywords: Contradictory claims, Natural language processingBackgroundThe research literature in medicine is vast and increas-ing rapidly. These papers contain a massive amount ofinformation, including claims about the research ques-tion being addressed. However, papers may not come tothe same conclusion about a particular research questionand claims in different papers may even contradict oneanother. Contradictory claims make it difficult to under-stand the current state of knowledge about a researchquestion. Systematic reviews aim to avoid this problem byevaluating and assessing the evidence related to a particu-lar research question, including contradictory claims, and*Correspondence: adalamri1@sheffield.ac.uk1Department of Computer Science,The University of Sheffield, Sheffield, UKpresenting it in a summarised format. However, these arenot available for all research questions and are also limitedby the evidence that was available when the review waswritten.Tools that support the automatic identification of con-tradictory claims would be useful for those that rely onbiomedical literature. They could be used to highlightresearch claims that are contradicted by other researchfindings, assist in the creation of systematic reviews [1]and literature surveillance systems [2]. They would also beuseful for automatic textmining applications which gener-ally accept claimsmade within research literature as primaface correct. Despite this there has been little explorationinto this problem. The work that has been carried out [3]focused on descriptions of molecular events in a corpus© 2016 Alamri and Stevenson. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 2 of 9mainly generated from the events in the BioNLP09 corpus[4], and was restricted to a single indicator of contradic-tion, the use of negation. Outside the biomedical domainthere have only been a few attempts to study the problemof contradiction identification independently of the moregeneral problem of textual inference [57].One of the reasons for this limited progress is a lack ofsuitable resources that can be used to develop and testapproaches. Developing these resources is not straight-forward given the volume of research that has beenpublished and the difficulty of identifying contradictoryclaims within them. This paper presents an approachto developing a corpus containing examples of poten-tially contradictory research claims which are identifiedbymaking use of information found in systematic reviews.The corpus is designed to include a wider range of topicsfrom the biomedical literature and wider range of linguis-tic phenomena that can be used to indicate contradiction(e.g. negation, use of antonyms and adjective polarity)than previous work.Reviewing the published biomedical literature to iden-tify the best available information related to biomedicalquestions is standard practice [2]. However, the litera-ture may contain findings that contradict one anotherand investigators have shown a tendency to reproduce thefindings of original research with contradictory claims.Consequently, editors and publishers attracted by theseresults tend to publish them faster than those with less sig-nificant findings, dubbed the Proteus phenomenon [8]. Agood example of contradictions between research claimsthat have appeared in the biomedical research literatureconcerns the relation between aspirin and heart attackprevention.Aspirin has been widely used as a pain killer andan effective drug for preventing blood clots. A conflicton its benefits started when doctors began prescrib-ing a daily dosage to protect heart attack victims fromfurther attacks. At that time there was no biomedicalresearch to prove that this was effective. An attemptto investigate [9] found that aspirin was significantlybeneficial in preventing heart attacks. However, a sub-sequent trial was less confident about that because itfound little difference between the fatality rate of peoplewho never used, seldom used or often used aspirin [10].Another study [11] was compatible with that result as itfailed to show the preventative role of aspirin on heartattacks. The first team, who found a significant benefitof aspirin on the heart, conducted another experiment[12] and reported a positive results that supported theirfirst claim. The contradictions between aspirin researchclaims lasted 20 years, until researchers finally concludedthat aspirin reduces the risk of non-fatal heart attacks,but its effects on other problems such as stroke are stillunclear [13].Other research topics such as the effectiveness of mam-mographies for discovering breast cancer or whether theDalkon Shield caused pelvic infections are also rich withcontradictory claims [13].MethodsThis section discusses some of the key concepts used inthis work, beginning with the definition of contradictionand followed by the types of claim.Defining contradictionContradiction has been defined as the existence of twoor more incompatible propositions that describe the samefact [14]. In another words, two fragments of text, T1and T2, are contradictory when they assert informationabout the same fact that cannot both be true at the sametime. The problem of contradiction has previously beenexplored within work on textual entailment [5, 7, 15]where a common approach is to consider T1 and T2 to becontradictory when one of them entails the negation ofthe other. De Marneffe et al. [6] used a looser definitionintended to be less restrictive: two fragments of text arecontradictory when they are extremely unlikely to be trueat the same time.There has been some previous exploration of the prob-lem of identifying contradictions in biomedical docu-ments [3]. Contradiction was defined as two texts thatdescribe events sharing certain attributes (e.g. theme,cause and anatomical location) but with different polarity.That work was restricted to statements about a very spe-cific type of information (chemical interactions) and oneway of expressing contradiction (negation).This work also focusses on biomedical documents butuses a less restrictive definition of contradiction. Twotexts, T1 and T2, are said to contradict when, for a givenfact F , information inferred about F from T1 is unlikely tobe true at the same time as information about F inferredfrom T2.This definition of contradiction is based on inferencesfrom statements being unlikely to be true at the same timerather than being logically inconsistent. This approachavoids the definition of contradiction being overly restric-tive and has been used by previous work [6]. Researchfindings in scientific documents are often expressed cau-tiously, e.g. using hedges [16], reducing the chances ofstatements being logically inconsistent with one another.Nevertheless, researchers are often interested in obtainingas much information as possible about a research ques-tion of interest and are likely to be interested in statementswhich are unlikely to be simultaneously true.The language used in biomedical documents tends toinvolve complex sentence structures with multiple factsdescribed within the same sentence and it is there-fore important to consider contradiction relative to aAlamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 3 of 9particular research question or fact. For example, sen-tences (2) and (4) in Table 1 would be considered contra-dictory in relation to some facts but not others. Sentence(2) states that fish intake does not prevent heart failurewithout providing information about the types of fish orpopulation groups the assertion applies to. Sentence (4)asserts that fish intake does prevent heart failure for aparticular population group (older adults) and types offish (tuna, broiled or baked). The sentences would notbe considered contradictory relative to the fact consump-tion of fried fish prevents heart failure, since both suggestthat it does not. However, they would be considered con-tradictory if the fact being considered was eating tunaprevent heart attack in older adults since sentence (4)suggests that it does while sentence (2) suggests that itdoes not.It is possible that contextual information may affectwhether pairs of statements are considered contradic-tions (e.g. there would be no contradictions between sen-tences (2) and (4) if sentence (2) only applied to teenagersand fried fish). However, they are considered in isolationand do not take account of their context. This approachensures that the problem does not become intractableand is common with other work on contradictiondetection [6].Claim definition and typesThe identification of claims, and the contradictionsbetween them, is made more complex by the range ofdifferent types of claim that can occur in biomedical liter-ature and various typologies have been proposed.We nowdefine research claim and discuss the types used in thecorpus.A research claim can be defined as the summary ofthe main points presented in a research argument; thesepoints can either introduce new knowledge to readers orupdate their knowledge on a topic [17]. The claim con-tains themost important piece of information that authorswant to communicate. It represents the research findingsor outcomes. In biomedical literature claims tend to sum-marize the authors findings and occur at the end of thestudy [17].Blake [18] identified five types of claims: explicit,implicit, correlations, observations and comparisons. Thistypology was formulated based on the availability of cer-tain information (facets): two concepts, a change andthe basis of the claim. Although the typology provides aframework of how a biomedical claim can automaticallybe analysed, it was not clear how a judgmental claim suchas effectiveness of a drug or a technique can be analyzed,for example, sentence (6) in Table 2.We use another framework [17], which has beenconstructed from a general perspective rather than specif-ically for the biomedical domain. The framework con-sists of four types of claim: factual, recommendation,evaluative and causal. Causal and evaluative claims are themost relevant types for our corpus. Factual claims tendto be generally accepted information and these claims aremost commonly found in background sections rather thanthe conclusion. Recommendation claims often providerecommendations of courses of action supported by themain research claim rather than providing any new infor-mation. We describe causal and evaluative claims usingexamples from the biomedical literature.Evaluative claims occur when an author expresses ajudgment about the value of a biomedical concept (e.g.drug, procedure, equipment, gene, protein). This type ofclaim is often used as an interpretation of evidence pre-sented in the research. It is usually expressed by eithermaking a statement about the properties of a conceptTable 1 Claims extracted from the abstracts described in Table 3Claim PMID Value Type1 In this large, population-based sample of African-American and white adults, whole-grain intake wasassociated with lower HF risk, whereas intake of eggsand high-fat dairy were associated with greater HF riskafter adjustment for several confounders18954578 YS CAUS2 Our findings do not support a major role for fish intakein the prevention of heart failure19789394 NO CAUS3 Moderate consumption of fatty fish (1-2 servings perweek) and marine omega-3 fatty acids were associatedwith a lower rate of first HF hospitalization or death inthis population20332801 YS CAUS4 Among older adults, consumption of tuna or otherbroiled or baked fish, but not fried fish, is associatedwith lower incidence of CHF15963403 YS CAUS5 Increased baked/broiled fish intake may lower HF risk,whereas increased fried fish intake may increase HF riskin postmenopausal women21610249 YS CAUSAlamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 4 of 9Table 2 Claims typology examplesClaim PMID Type6 Combined clopidogrel and aspirin overcome single drug resistances, are safe forbleeding22942294 Judgemental7 Aspirin plus clopidogrel is more effective in venous graft patency than aspirin alone inthe short term after CABG, but further, long-term study is needed21050973 Comparative8 Although a bedtime dose of doxazosin can significantly lower the blood pressure, itcan also increase left ventricular diameter, thus increasing the risk of congestive heartfailure.18551024 Excitatory9 Routine use of postoperative aspirin after coronary artery bypass grafting (CABG)reduces graft failure and cardiovascular events21146675 Inhibitory10 In the Spanish Mediterranean area, the presence of antigens B-15 and DQ3 would beassociated with advanced DCM10198739 Neutral(judgment), e.g. sentence (6), or comparing the conceptwith another, e.g. sentence (7).Causal claims suggest a relationship between two con-cepts and assert that one concept influences the other.Hashimoto et al. [19] described three types of influences:excitatory, inhibitory and neutral. Excitatory influenceindicates a direct activation or enhancement, e.g sentence(8) shows the doxazosin had an excitatory influence onleft ventricular diameter. An inhibitory influence is theopposite of excitatory and indicates direct deactivation orsuppression. For example, sentence (9) is a casual claimswhich asserts that Routine use of postoperative aspirin hasan inhibitory effect on graft failure and cardiovascularevents. The final type of causal claim, neutral, is nei-ther excitatory nor inhibitory. For example, sentence (10)asserts a relationship between presence of antigens B-15and DQ3 and advanced DCM (Dilated Cardiomyopathy)but doesnt explicitly state whether it is excitatory orinhibitory.Corpus construction stagesCorpus data collectionThe corpus was created using research abstracts of studiesconsidered in systematic reviews related to cardiovasculardiseases. Cardiovascular diseases have been reported asa major contributor to world mortality and their causesare commonly explored in research papers [20]. Given thevolume of research published on the topic we expect tofind some contradictory findings.Four types of cardiovascular disease were chosen: Car-diomyopathy, Coronary artery, Hypertensive and Heartfailure. The Pubmed search engine was used to retrievesystematic reviews associated with these types. For exam-ple, the query Cardiomyopathy[title] ANDmeta-analysis[title] was used to search forsystematic reviews discussing cardiomyopathy disease,and the same procedure were applied on the other types.Themodifier [title]was used to ensure that the searchkeywords occurred within the title of the article.Systematic reviews were used since they gather find-ings from multiple studies related to a defined researchquestion and summarise their results using statisticalmeta-analysis. Results of the meta-analysis are often pre-sented using a diagram called a forest plot [21] whichrepresents the findings of a set of studies. In each sys-tematic review, the forest plot diagram was examined todetermine whether it suggested contradictions betweenthe studies included. If any potential contradictions wereidentified then all the studies included within the sys-tematic review were included in the corpus. For exam-ple, Fig. 1 shows a forest plot in which a single study,(Comstock & Webster, 1969), favours the placebo andconsequently this study may contain a claim regardingthe effectiveness of the treatment which contradicts thosemade in the other studies. Although, the difference maynot be significant and may not even be reflected in theabstract description, the forest plot diagram is still a goodindication that the details discussed in this review maycontain contradictory claims.Table 3 shows a list of abstracts titles; title (11) refers tothe systematic review that was collected from MEDLINE;and titles (12-16) are the studies used in that review. Thesestudies were included as candidate datasets since thediagram of the systematic review showed disagreementbetween at least one of theirs findings.Question formulationBiomedical literature contains claims with complex struc-ture, often expressing multiple facts in the same sentence.This may confuse the annotators when annotating con-tradictory claims in the dataset. To avoid this issue andensure that annotations correspond with the definition ofcontradiction being used, a common question shared byclaims needs to be identified to ensure that the annotatorsfocus on the same fact when identifying contradictions.As an attempt to achieve that goal, an annotator withan advanced degree in medicine was asked to use thetitles of each systematic review and the studies abstractsAlamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 5 of 9Fig. 1 The diagram represents the outcome of studies exploring the effectiveness of a vaccine (BCG) for preventing tuberculosis. Studies that favourthe vaccine are shown on the left side of the vertical column while those that favour the placebo are shown on the right side. This diagram showsone study that favoured the placebo (Comstock & Webster, 1969) and two for which no statistically significant difference between the vaccine andplacebo could be identified, (TPT Madras, 1980) and (Comstock et al.,1976). The vaccine was favoured in all other studies. The dataset was retrievedfrom metafor [25], an R package for conducting meta-analysesincluded, as information to formulate a suitable questionfor the group of studies in that review. The annotatorwas asked to formulate closed questions (i.e. ones thatcould be answered as either yes or no) written in sim-ple present tense. He was also asked to ensure that thequestions were compiled with the PICO standard [22] toinclude information about the patient problem or popu-lation (P), Intervention (I), comparison (C) and outcomes(O). For example, the question In patients with chronicheart disease (P), does bone marrow stem cell transplanta-tion or injection (I), compared to none (C), improve cardiacfunction (O)?.This approach will enable the annotators to measurethe assertion values of claims with respect to the ques-tion. Thus, when two claims provides different assertionvalue or conclusion to a question, they are consideredpotentially contradictory.Corpus annotationThe final stage of corpus construction was to identifyand annotate the claims in each abstract. Two annotatorswere recruited. Each annotator had native-level Englishfluency, an advanced degree in a field related to medicineand was employed in a medical role (one in an academicdepartment and another in a hospital). Both were familiarwith biomedical research literature and evidence-basedmedical research. The annotators were asked to carry outthree tasks: choose a claim, annotate the claim with anassertion value (YS/NO) with respect to the question for-mulated for the review group, and annotate the claimtype (CAUS/EVAL) according the claim types describedearlier.The first task required examining each study abstractand, considering the question that had been formulatedfrom the systematic review, identifying the claim sentenceTable 3 A systematic review title and the titles of its associated studiesTitle PMID11 Review Fish consumption and incidence of heart failure a meta-analysis of prospectivecohort studies2348980612 Study Incident heart failure is associated with lower whole-grain intake and greaterhigh-fat dairy and egg intake in the Atherosclerosis Risk in Communities (ARIC)study1895457813 Study Intake of very long chain n-3 fatty acids from fish and the incidence of heartfailure: the Rotterdam Study1978939414 Study Fatty fish, marine omega-3 fatty acids and incidence of heart failure 2033280115 Study Fish intake and risk of incident heart failure 1596340316 Study Fish intake and the risk of incident heart failure: the Womens Health Initiative 21610249Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 6 of 9within each abstract that could answer that question. Ifthe abstract contained multiple candidate claim sentencesthen annotators were asked to select one that correspondsto the overall abstract finding (as represented in the for-est diagram) and best describes the contribution of theresearch relative to the question. After the claim had beenidentified annotators were asked to mark it as either YS(to indicate the claim was an affirmative answer to thequestion) or NO (to indicate that it was not). Finally, theannotators were asked to identify the type of the claim (i.e.causal or evaluative). After each annotation phase the twoannotators met to resolve disagreements and decided onthe final annotation.Results and discussionExamination of forest plot diagrams lead to the identifi-cation of 40 suitable systematic reviews and a questionwas formulated for each. A total of 397 studies werementioned in these reviews. These studies were retrievedand annotators asked to identify a claim in each, decidewhether this claim agreed with the question that had beenformulated and determine the claim type. 19 of the studieswere excluded since the annotators were unable to iden-tify a claim that provided a clear answer to the question.No contradictions were identified for 16 of the system-atic reviews (i.e. the annotators judged all of the claims inthe studies associated with the review as either agreeingor disagreeing with the question). These reviews and thestudies associated with them were also excluded.The final corpus consists of 259 studies used withinthe 24 systematic reviews that were not excluded. Table 4shows the number of studies associated with each sys-tematic review and their distribution across the assertionvalues (YS and NO). The questions formulated for eachsystematic review are shown in Table 5. The corpus isformatted in XML as shown in Fig. 2.The annotators were asked to complete three tasks:identify a single claim within each abstract, determinewhether that claim agreed with the research question ornot, and annotate the claim type (CAUS/EVAL).Inter-annotation agreement for the claim identificationtask was 92%. The main reason for disagreement wascases where there were multiple claims in the same studyabstract that potentially answer the question formulatedfor the systematic review. For example, Table 6 showstwo sentences, (17) and (18), extracted from an abstractthat potentially answer the question In women with pre-eclampsia, is polymorphism in angiotensin gene associatedwith pre-eclampsia?. In such cases the annotators wereasked to prefer sentences in the conclusion sections of theabstracts.Agreement for the second task, determining whetherthe claim agreed with the question or not, was veryhigh (97%). The disagreements that did occur arose fromTable 4 Claims classes and type distribution among the groupsAssertion TypeTopic Review-PMID #Abstracts YS NO CAU EVACardiomyopathy22498326 4 3 1 2 223623290 9 7 2 3 621556773 15 12 3 12 3Coronary artery24035160 5 3 2 2 324135644 20 13 7 13 724036021 4 2 2 4 024212980 20 12 8 14 624039708 18 11 7 17 124172075 7 2 5 0 724090581 8 4 4 2 624040766 5 4 1 5 0Heart failure23489806 4 3 1 4 023181122 5 4 1 5 023962886 15 13 2 14 124163234 29 22 7 13 1624165432 6 4 2 2 423219304 10 6 4 5 521521728 11 7 4 6 5Hypertensive23602289 17 14 3 10 722795718 14 13 1 9 523435582 6 4 2 5 122854636 5 3 2 2 323223091 7 6 1 6 122086840 15 7 8 10 5TOTAL 259 180 79 165 94claims that did not provide a conclusive answer to the rel-evant question. This problem was generally avoided byformulating a question for each systematic review but insome cases multiple inferences can be derived from thesame claim. For example, the question In the elderly, is n-3 fatty acid from fish intake associated with reduction inrisk of developing heart failure? asked about the associa-tion of n-3 fatty acid from fish and the risk of developingheart failure but did not specify the type of fish. Table 7shows multiple inferences derived from claims (4) and (5)in Table 1 (inferences (4a) and (4b) from claim (4) andinferences (5a) and (5b) from claim (5)). These inferencesdiffer in their agreement with the question. In such situa-tions annotators were asked to choose the inference that isthe best fit for the question. In this case, inference (4a) wasused for claim (4) since it is more general than the alter-native (4b). Similarly, inference (5a) was used for claim (5)rather than (5b) since the second referred to a restrictedpopulation (postmenopausal women).Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 7 of 9Table 5 A list of the 24 questions formulated for the final corpusReview-PMID Question22498326 In patients with HCM, does using imaging technique, compared to conventional techniques, serve as a predictor foradverse prognosis?23623290 In patients with chronic heart disease, does Bone marrow Stem cell transplantation or injection, compared to none,improve cardiac function?21556773 In patients with dilated cardiomyopathy, are HLA genes associated with development of Dilated Cardiomyopathy?24040766 In Han Chinese population, is SNP T-778C of apolipoprotein M associated with risk of developing Diabetes or stroke?24212980 In patients undergoing coronary bypass surgery, does Aspirin usage, compared to no aspirin, cause bleeding?24035160 In patients undergoing choronary artery bypass, does the combination of aspirin and clopidogrel, compared to aspirinalone, prevent graft occlusion or improve patency?24172075 In patients undergoing coronary by pass surgery, is Off-pump, compared to conventional on pump coronary arterybypass grafting, more beneficial?24135644 In patients with choronary artery disease, is mutation or polymorphisms in endothelial nitric oxide synthase geneassociated with CAD or MI or ACS development?24036021 In patients with atherosclerotic plaque or myocardial infaction, does ?463G or ?463A polymorphism in MPO geneinfluence MI or CAD development?24039708 In patients with coronary artery disease (CAD), is C242T polymorphism of P22(PHOX) gene associated in developmentof CAD?24090581 In patients with coronary artery diseases, does combining CABD and CEA, compared with CABG or CEA alone, reducemorbidity?24165432 In elderly patients with CHF, does physical exercise or cardiac rehabilitation, compared to no exercise, improve cardiacfunction?23962886 In patients with heart failure, do statin drugs treatment, compared to non statin drug, treatment improve cardiacfunction or prevent cardiac morbidity?23219304 In patients with renal or cardiovascular disease, does treatment with ACE inhibitors, compared with placebo, improverenal function or protect against cardiovascular incidents respectively?23181122 In the elderies, is n-3 fatty acid from fish intake associated with reduction in risk of developing heart failure?23489806 In the elderlies, does omega 3 acid from fatty fish intake, comparedwith no consumption, reduce the risk of developingheart failure?24163234 In patients with CHF, does care giving or teleguidiance-telecare, compared to usual care, reduce morbidity?21521728 In patients with advanced diabetes, does treatment with antihypertensives, compared with placebo, improve renalfunction or protect againct cardiovascular incidents?22854636 In patients with hypertension, does revascularisation, compared with medical therapy, improve blood pressure?22795718 In patients with hypertension, does treatment with ACE inhibitors, compared to placebo, reduce risk of cardiovascularevent or improve blood pressure?23602289 In patients with hypertesion or hypercholesterolemia, does statin drugs, compared to placebo, reduce blood pressureor lipid levels?23435582 In women with pre-eclampsia, does treatment with L Arginine, compared to placebo, reduce blood pressure or pre-eclampsia?22086840 In women with pre-eclampsia, is Polymorphism in angiotensin gene associated with pre-eclampsia?23223091 In women with pre-eclampsia, is mutation in renin-angiotensin gene associated with pre-eclampsia?Lower agreement (86%) was obtained for the final task,annotation of claim type. The main cause of disagree-ment were claims that could potentially be simultane-ously interpreted as causal or evaluative. For example,the claim These results suggest that HLA-DR4 antigenmay be a genetic marker for susceptibility to dilated car-diomyopathy can be considered a causal claim since itdescribes an association relation between the two con-cepts HLA-DR4 antigen and dilated cardiomyopathy. Butit can also be evaluative since the author is evaluating theeffectiveness of that gene as a genetic marker. Annotatorswere reminded that evaluative claims should express ajudgement, which is not the case here and it was conse-quently annotated as a causal claim.The high inter-annotator agreement figures indicatethat the annotation tasks are well-defined and that theannotations are reliable and form a sound basis for futurestudies. Although agreement for the claim type identifica-tion task is lower than the others, the informationmay stillbe useful for further exploration.Alamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 8 of 9Fig. 2 Examples of formatted claimsAutomatic identification of contradictory claims is a dif-ficult problem and a number of challenges were identifiedduring the construction of our corpus. Claims tend toappear at the end of abstracts and consequently authorsoften use shorter forms such as acronyms, for exam-ple Our observations indicate a significant relationshipbetween p22phox C242T and PARP-1 Val762Ala poly-morphisms, CAD and its severity, but not with occur-rence of MI in T2DM individuals with significant coro-nary stenoses. This complicates the process of identifyingclaims, particularly since acronyms are often ambiguousin biomedical text [23, 24].Identifying connections between statements is alsocomplicated by authors use of alternative terms. Forexample, statin, atorvastatin and rosuvastatin were allused to refer to drugs that lowers cholesterol levels instudies included in the corpus.The corpus developed in this research could be used as aresource for researchers to explore the problems of identi-fying, analysing and resolving contradictory claims madein the biomedical literature. For example, it could be usedto build a machine learning system that discriminatesbetween claims that agree or disagree with a query, wherecontradiction occurs between them when they providedifferent answers to the same query. Moreover, the con-struction methodology described in this paper could beapplied to construct other corpora containing potentiallycontradictory claims.ConclusionsThe contradictory claims found in biomedical literaturepresent a challenge to evidence-based evaluation intothe effectiveness of approaches. Automatic identification,analysis and resolution of these claims would be useful forthose that rely on this literature.This paper described the development of a corpus con-taining contradictory claims found within Medline. Sys-tematic reviews were used to identify studies that containcontradictory statements regarding particular researchquestions. Claims within the studies were identified andannotated. Analysis shows that the agreement betweenannotators is reliable, suggesting that the information inthe corpus will be useful for those who wish to explore thisproblem. The corpus construction methodology could beapplied to other topics in the biomedical domain.The corpus can be accessed via: http://staffwww.dcs.shef.ac.uk/people/M.Stevenson/resources/bio_contradictions/.Table 6 Potential answers to a formulated question from the same abstractSentence PMID Value Type17 The frequency of T allele of angiotensinogen T174Mgene was slightly increased, but not significantly, inpreeclampsia (0.11) than in controls (0.07)15082899 YS CAUS18 In conclusion, a molecular variant of ACE, but notangiotensinogen, gene is associatedwith preeclampsiain Korean women15082899 YS CAUSAlamri and Stevenson Journal of Biomedical Semantics  (2016) 7:36 Page 9 of 9Table 7 Multiple inferences derived from two claimsInference PMID Value4a consumption of tuna or other broiled or baked fish is associated with lower incidence of CHF 15963403 YS4b fried fish is not associated with lower incidence of CHF 15963403 NO5a Increased baked/broiled fish intake may lower HF risk 21610249 YS5b Increased fried fish intake may increase HF risk in postmenopausal women 21610249 NOAcknowledgementsThe authors thank Adams Aminat and Yomi Yusuf for annotating the corpusand the anonymous reviewers for their feedback.FundingMS was funded by the Engineering and Physical Sciences Research Council(EP/J008427/1).Authors contributionsAA carried out the research, data analysis and wrote the initial draft of thepaper. Both authors revised the paper and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 23 May 2015 Accepted: 26 May 2016Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 DOI 10.1186/s13326-016-0102-0DATABASE Open AccessA drug target slim: using gene ontologyand gene ontology annotations to navigateprotein-ligand target space in ChEMBLPrudence Mutowo1* , A. Patrícia Bento1, Nathan Dedman1, Anna Gaulton1, Anne Hersey1, Jane Lomax2and John P. Overington1AbstractBackground: The process of discovering new drugs is a lengthy, time-consuming and expensive process.Modern day drug discovery relies heavily on the rapid identification of novel targets, usually proteins thatcan be modulated by small molecule drugs to cure or minimise the effects of a disease. Of the 20,000proteins currently reported as comprising the human proteome, just under a quarter of these can potentiallybe modulated by known small molecules Storing information in curated, actively maintained drug discoverydatabases can help researchers access current drug discovery information quickly. However with the increasein the amount of data generated from both experimental and in silico efforts, databases can become very largevery quickly and information retrieval from them can become a challenge. The development of database toolsthat facilitate rapid information retrieval is important to keep up with the growth of databases.Description: We have developed a Gene Ontology-based navigation tool (Gene Ontology Tree) to help usersretrieve biological information to single protein targets in the ChEMBL drug discovery database. 99 % of singleprotein targets in ChEMBL have at least one GO annotation associated with them. There are 12,500 GO termsassociated to 6200 protein targets in the ChEMBL database resulting in a total of 140,000 annotations. The slimwe have created, the ChEMBL protein target slim allows broad categorisation of the biology of 90 % of theprotein targets using just 300 high level, informative GO terms.We used the GO slim method of assigning fewer higher level GO groupings to numerous very specific lower levelterms derived from the GOA to describe a set of GO terms relevant to proteins in ChEMBL. We then used the slimcreated to provide a web based tool that allows a quick and easy navigation of protein target space. Terms from theGO are used to capture information on protein molecular function, biological process and subcellular localisations.The ChEMBL database also provides compound information for small molecules that have been tested for theireffects on these protein targets. The ChEMBL protein target slim provides a means of firstly describing the biologyof protein drug targets and secondly allows users to easily establish a connection between biological and chemicalinformation regarding drugs and drug targets in ChEMBL.The ChEMBL protein target slim is available as a browsable Gene Ontology Tree on the ChEMBL site underthe browse targets tab (https://www.ebi.ac.uk/chembl/target/browser). A ChEMBL protein target slim OBO filecontaining the GO slim terms pertinent to ChEMBL is available from the GOC website (http://geneontology.org/page/go-slim-and-subset-guide).(Continued on next page)* Correspondence: prudence@ebi.ac.uk1European Molecular Biology Laboratory, European Bioinformatics Institute(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UKFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 2 of 7(Continued from previous page)Conclusions: We have created a protein target navigation tool based on the ChEMBL protein target slim.The ChEMBL protein target slim provides a way of browsing protein targets in ChEMBL using high levelGO terms that describe the molecular functions, processes and subcellular localisations of protein drug targetsin drug discovery. The tool also allows user to establish a link between ontological groupings representingprotein target biology to relevant compound information in ChEMBL. We have demonstrated by the use ofa simple example how the ChEMBL protein target slim can be used to link biological processes with druginformation based on the information in the ChEMBL database. The tool has potential to aid in areas ofdrug discovery such as drug repurposing studies or drug-disease-protein pathways.Keywords: Ontologies, Bioinformatics, Drug discovery, Database, Biology, ProteinBackgroundThe use of small molecules in alleviating symptoms in adisease state is generally evaluated against a protein tar-get [1]. The human proteome is reported to have around20,000 proteins [2] with literature sources reporting a greatvariation in the number of proteins deemed to be druggable[3, 4]. The biology of the druggable proteome (alternativelythe druggable genome) is usually described in terms of dis-tinct and well-studied protein families. Protein family classi-fication can be used together with protein-centric bio-ontologies to better understand the characteristics of a drugtarget of interest.ChEMBL is a manually curated, freely available resourcecontaining bioactive ligands with drug-like properties aswell as quantitative bioassay results and the biologicaltargets of these molecules [5]. Biological targets reportedin ChEMBL assays include nucleic acids, cell-lines, tissues,subcellular-fractions, whole organisms and proteins. Pro-tein targets are the largest portion of targets in ChEMBL.This target group is further divided into single proteintargets, protein complexes, protein families, and protein-protein interactions. Compound information to ChEMBLprotein targets is obtained by manual curation of selectedpublished medicinal chemistry literature and data depo-sitions from data sharing partnerships.There has been a steady increase of data in ChEMBLover time. There has been a fourfold increase in thetotal numbers of assays from the first ChEMBL release(ChEMBL1) to the current release (ChEMBL21) whichhas 1.2 million assays. The number of single proteintargets doubled with the first release having 3222 singleprotein targets to just over 6000 in the current release.Standardising protein information in ChEMBL is useful tofacilitate ease of protein target retrieval. Single proteintargets in the database are cross-referenced to a variety ofprotein property descriptors including Gene Ontology(GO) based annotations. GO is a set of concepts, struc-tured as a graph or tree, that provide a controlled andconcise way of capturing the processes, molecular func-tions and subcellular localisations of gene products in thiscase proteins [6]. An annotation is an evidence-based as-sertion created to capture biological information about aprotein [7]. GO annotations to protein targets in ChEMBLare obtained from the GO consortium database [8]. Theseannotations provide useful insight into the biology ofproteins in drug discovery. GO annotations vary in theirinformation content depending on the specificity of theterm used in annotation. Some annotations contain veryspecific and fine-grained information about a proteinwhile others contain broad, high level information. Com-paring, grouping or searching through protein targetsannotated at different levels of GO information contentcan be time consuming and challenging. GO slims areoften used to allow comparison of protein informationcaptured at different levels of the GO.A GO slim is a high-level subset of the GO created bycollapsing specific terms and mapping them to theirhigher level parent terms using the parentchild hierar-chies inherent in the GO. GO slimming allows for a repre-sentation of biological information by using high levelterms that provide a broad overview of the biology [8].GO slims are typically generated for specific organism orparticular areas of scientific interest and have been usedto aid visualisation, exploration and summarization ofGO functional data [9, 10]. We have created a ChEMBLprotein target slim to allow users to easily access thebiological information to targets with GO annotation.Construction and contentCreating the ChEMBL protein target slimWe created the ChEMBL target slim by retrieving all rele-vant annotations to single protein targets in ChEMBLusing the QuickGO tool [11]. QuickGO is a web browserfor GO terms and annotations. GO terms in QuickGO areidentified by an alpha numeric identifier, a term definitionand relationships established between a specific term andother terms in GO. Annotations retrieved from QuickGOare obtained from the Gene Ontology consortium GOdatabase and are created by consortium members. Weretrieved all annotations to the protein set across allTable 1 Proteins mapped to GO slim terms per speciesSpecies Proteins targets mapped to slimHomo sapiens 3254Rattus norvegicus 899Mus musculus 828Bos taurus 194Sus scrofa 98Escherichia coli K-12 74Oryctolagus cuniculus 74Mycobacterium tuberculosis 73Saccharomyces cerevisiae S288c 70Staphylococcus aureus 50Mutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 3 of 7evidence codes. The output was downloaded as a GeneAssociation File (GAF) which contains protein accessionsand GO term information.We used the generic GO term mapper tool [12] toidentify an initial high level set of GO terms represen-ting the annotation information for our protein targetset. The GO term mapper tool uses the map2slim algo-rithm in count mode to identify high level term parentterms to terms in the annotation set. The slim termssuggested by the algorithm are grouped according to thenumber of proteins whose initial annotation has beenmapped to a higher level GO term. GO terms with ahigh number of proteins mapped are incorporated in theslim while terms with no proteins are removed. Weselected the Generic GO slim (version 1.2) as the refe-rence slim for the selection of slim terms. This slim isnot species specific.From the output the term mapping we manuallyinspected and customised the slim to the ChEMBL pro-tein target set as follows:GO term refinementWe identified fine grained annotations that could bemapped to higher level terms. One of the considerationsmade in this exercise was the information content of thehigher level terms. An example being proteins annotatedto granular protein binding terms like GO:0017124 SH3domain binding were not mapped up to the higher levelparent GO:0005515 protein binding due to loss of infor-mation content.GO term selectionWe assessed the number of accessions not mapped toany term in the initial reference generic GO slim with aview to customising the slim terms to reflect the biologyof the protein annotations in the set. We manuallyadded terms to the GO slim to address this.We removed terms from the generic GO slim that didnot have any annotations in the ChEMBL protein set.This addition, removal and term refinement was donein several rounds of term-to-accession-mapped inspec-tion until we obtained a set of slim terms providing agood coverage (in this case 90 %) of the protein targetsin ChEMBL.ResultsThe resultant ChEMBL protein target slim generated con-tains a total of 300 high level GO terms representing thebiology of 5600 protein targets in ChEMBL in the threeareas of GO. In total, proteins from 532 different speciesare mapped to terms in the GO slim. The top ten species(in terms of number of proteins) in the current ChEMBLrelease are shown in Table 1.UtilityBased on these slim term categories, we created a GO-based navigation tool which is available on the ChEMBLwebsite. This tool termed the Gene Ontology tree canbe found by clicking on the Browse Targets tab on theChEMBL home page and selecting the radio button nextto the tree name.The two key functionalities of the GO tree are:a. Protein target browsing by GO categoriesThe ChEMBL protein target slim in the form of anavigational GO tree also allows users to establishwhich processes or functions proteins are involvedin by selecting the process and function nodes. Thecellular component node provides a quick overviewof the subcellular locations of protein targets as wellas a link out to small molecules interaction withtargets in a selected localisation (Fig. 1, Panel 1a).The numbers affiliated with each GO category onthe tree allow a rapid assessment of which areas ofbiology have high proteins giving an indication oftarget prioritisation in drug discovery endeavours.b. Searching for proteins and related compoundinformationThe tree has a search functionality that allows usersto search the database for protein information usinga specific biological key word or phrase to retrieveall proteins targets annotated to that term as wellas the compounds and bioactivities to the selectedsubset. Figure 1 shows how to use the tree to searchfor all proteins involved in response to toxicsubstance. By using a key phrase toxic substance inthe search box, the GO slim allows retrieval of allproteins annotated to GO:0009636 response to toxicsubstance (Fig. 1, Panel 1a). The tree shows 432protein targets annotated to this term as well asshowing the more specific child terms ofGO:0009636 which are GO:0046677 response toantibiotic. Right clicking on the GO term groupingFig. 1 Searching the ChEMBL database using the GO tree to retrieve all proteins involved in response to toxic substance and their relatedcompound and bioactivity information. Panel a shows the biological process node of the GO tree with a 'toxic substance' keyword search. Panelb shows the search output of the list of proteins annotated with the 'toxic substance' GO termMutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 4 of 7information leads the user to a page containing allthe protein targets annotated to that term as wellas the compounds tested against them and thebioactivities reported for the assays (Fig. 1, panel1b). A link exits to the QuickGO webpage to viewthe definition of the GO term of interest.Case study- using Gene Ontology and drug ATCinformation to further establish links betweenbiological and chemical space in ChEMBLAnother useful application of the biological groupingscreated by the GO slim is the ability to provide insightinto the biology of a group of proteins that are targets ofdrugs used for specific indications. The ChEMBL database contains information to small molecules importantin drug discovery. Small molecule to protein informationlinks are primarily established by considering the bio-assay that the two entities are reported together in. Inaddition, for FDA approved drugs, targets responsiblefor their efficacy (mechanism of action) are assignedmanually. These high level drug classification categorieswere combined with the higher level GO classificationcategories for their targets to show the mechanism ofaction information displayed in Fig. 3.The database also uses World Health Organisation ana-tomical therapeutic (ATC code) classifiers that describethe mechanism of action of a drug to group drugs inhigher level categories. For examples drugs used againstparasitic infections are grouped as anti-protozoals with anWHO ATC code of P01.Figure 3 is a simple example of selecting a drug clas-sification of interest from the ATC classification. Weretrieved all drugs from ChEMBL that are used as Anti-neoplastic and Immunomodulating agents (WHO ATCclassification L [13]), and the curated mechanism ofaction information and readily retrieving the biology ofthe targets of these drugs by using the GO slim catego-ries that describe their assigned efficacy targets.The targets of all therapeutic drugs in release 20 ofChEMBL consists of 1179 individual proteins. Of thisnumber, 196 proteins are targets of drugs in the ATC LMutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 5 of 7class [13]. We used the ChEMBL slim to navigate thebiological processes that these proteins are involved in.Figure 2 shows a venn diagram [14] of the 5 GO biologicalprocess categories for these proteins as cell death, cellmotility, cell morphogenesis, cell proliferation and celldeath. The number of drugs that modulate the proteins ineach of the groupings are shown in the venn diagram sets.It is immediately apparent that 24 drugs have targets rep-resented in all 5 categories of biological grouping. Usingthe drug mechanism of action information in ChEMBLwe probed this set of 24 drugs on mechanism and the twomain mechanism of actions shown are protein kinaseinhibitor action and growth factor receptor inhibition(Fig. 3).Considering higher level drug and target classificationcan give broad insight into the biology for protein tar-gets of drugs in the same classification.DiscussionThe existence of large numbers of protein annotationsin the GO database provides a useful resource for com-putational querying of protein sets annotated in thisway. The fact that not all GO annotations are made tothe same level of term specificity can make it proble-matic to query protein sets annotated with this way. GOslims are a useful of compressing annotation informationto obtain a broad but informative overview of proteinbiology. Grouping proteins into biological categoriesusing a GO slim approach comes with the caveat thatits is possible for a single protein to being representedin more than one category due to the multi-functionalFig. 2 Number of drugs used as Antineoplastic and Immunomodulating Agenerated using the ChEMBL slimnature of certain proteins. Similarly some proteins thathave yet to be annotated or protein whose annotations areyet to be deposited in a database may not feature in suchgrouping systems until such a time when the annotation iscreated and incorporated in the database. The same canbe said of drug information. Some drugs have been knownto have protein targets from more than one biologicalgrouping due to the drug having multiple targets and/ordifferent mechanisms of action. However with the bio-logical information captured using GO to protein targetsin ChEMBL, the GO slim still provides a quick and usefulway of navigating protein target space and related smallmolecule information.ConclusionWe have created a protein target navigational tool usingthe ChEMBL protein target slim specifically designedfor browsing drug discovery protein targets. This toolprovides a rapid way of searching for biological informa-tion to proteins in a large database. The tool also allowsfor a rapid overview of the biology of protein target space.Besides providing information on the biological processand molecular functions of protein targets the navigationtree also readily provides an overview of protein targetsubcellular localisation.The slim is freely available for use and is updated regu-larly to reflect changes in both the GO and ChEMBLprotein target space. We anticipate the slim will be a use-ful tool for other researchers and tool developers wishingto display, explore and summarize GO data in the area ofdrug discovery.gents (ATC Class L) targeting proteins in 5 biological process categoriesFig. 3 Mechanism of action for drugs at intersection of protein GO categoriesMutowo et al. Journal of Biomedical Semantics  (2016) 7:59 Page 6 of 7Availability and requirementsThe ChEMBL drug target slim is freely available from theChEMBL website https://www.ebi.ac.uk/chembl/target/browser [15]. The GO terms slim terms used for the slimclasses are available from the Gene Ontology Consortiumtogether with the other GO slims [16]. The ChEMBL datais made available on a Creative Commons Attribution-Share Alike 3.0 Unported License.AbbreviationsGAF: Gene association file; GO: Gene ontology; WHO ATC: World HealthOrganisation Anatomical Therapeutic Chemical ClassificationAcknowledgementsWe would like to thank the GO consortium for agreeing to host the slim ontheir website. Special thanks go to David Osumi-Sutherland, Paola Roncaglia,and Chris Mungall for specialist ontology advice and assisting with theformatting of the ontology files.FundingThis work was supported by the EMBL member states and Wellcome TrustStrategic award [WT086151/Z/08/Z].Authors contributionsPM created the ChEMBL GO slim, carried out protein target analysis anddrafted the manuscript. APB integrated the WHO ATC classification for drugsand informed the data analysis for the drugs. AG integrated the biology andchemistry concept of the slim and created tables in the database to housethe slim terms and generated scripts for their updates and continuedmaintenance as well as helping to draft the manuscript. AH conductedchemistry analysis on the representation of the compound data and howthis is best represented in the context of the slim and helped to draft themanuscript. JL provided input into the ontology terms and in the interativeselection of appropriate terms for the slim. ND created the ChEMBL targetslim visual on the ChEMBL website. JPO conceived the idea of a method oftarget navigation incorporating Gene Ontology and the design of theconcept regarding the user needs and database capabilities and helped todraft the manuscript. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1European Molecular Biology Laboratory, European Bioinformatics Institute(EMBL-EBI), Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UK. 2Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus,Hinxton, Cambridge CB10 1SA, UK.Received: 2 December 2015 Accepted: 16 September 2016RESEARCH Open AccessTowards exergaming commons: composingthe exergame ontology for publishing opengame dataGiorgos Bamparopoulos1, Evdokimos Konstantinidis1, Charalampos Bratsas2 and Panagiotis D. Bamidis1*AbstractBackground: It has been shown that exergames have multiple benefits for physical, mental and cognitive health.Only recently, however, researchers have started considering them as health monitoring tools, through collectionand analysis of game metrics data. In light of this and initiatives like the Quantified Self, there is an emerging needto open the data produced by health games and their associated metrics in order for them to be evaluated by theresearch community in an attempt to quantify their potential health, cognitive and physiological benefits.Methods: We have developed an ontology that describes exergames using the Web Ontology Language (OWL); itis available at http://purl.org/net/exergame/ns#. After an investigation of key components of exergames, relevantontologies were incorporated, while necessary classes and properties were defined to model these components. AJavaScript framework was also developed in order to apply the ontology to online exergames. Finally, a SPARQLEndpoint is provided to enable open data access to potential clients through the web.Results: Exergame components include details for players, game sessions, as well as, data produced during thesegame-playing sessions. The description of the game includes elements such as goals, game controllers andpresentation hardware used; what is more, concepts from already existing ontologies are reused/repurposed. Gamesessions include information related to the player, the date and venue where the game was played, as well as, theresults/scores that were produced/achieved. These games are subsequently played by 14 users in multiple gamesessions and the results derived from these sessions are published in a triplestore as open data.Conclusions: We model concepts related to exergames by providing a standardized structure for reference andcomparison. This is the first work that publishes data from actual exergame sessions on the web, facilitating theintegration and analysis of the data, while allowing open data access through the web in an effort to enable theconcept of Open Trials for Active and Healthy Ageing.Keywords: Serious games, Exergames, Ontology, Linked open data, Exergame commons, Active and healthyageing, Open clinical trialsIntroductionContinuous monitoring through self-tracking tools suchas serious games may result in better health assessments.These games produce large amounts of data, however, ifthey are published in a non-standardized way, the capacity to exploit data richness, by combing andintegrating patient-related datasets will be destroyed,which may weaken its assessment capabilities. Moreover,large volumes of data are nowadays collected from appli-cations that monitor posture and body movements. Thedifficulty of assessing physical activity and/or sedentarybehavior can be addressed by describing the data in astandardized format. A recent study has presented theontology of physical activity (OPA) in order to provide aformal description of physical activity [1]. The ontologyof physical exercise (OPE) was developed towards asimilar purpose, in an attempt to describe the physicalexercise in terms of functional movements, muscles* Correspondence: bamidis@med.auth.gr1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,Aristotle University of Thessaloniki, Thessaloniki, GreeceFull list of author information is available at the end of the article© 2016 Bamparopoulos et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 DOI 10.1186/s13326-016-0046-4involved, as well as, hardware and monitoring devicesthat are used [2].In recent years, there has been an increased interest bythe game industry in game controllers that enable userinteraction through body movements such as Wii BalanceBoard and Microsoft Kinect. These technological advanceshave led to the development of the so called exergames[3], a special kind of serious games, which combinegaming and exercise resulting in maintenance and im-provement of physical status, focusing on large musclegroups. Serious games in general have an impact onusers such as a change in knowledge, behavior, physicalstate, cognitive function as well as health and mentalwell-being [4]. Moreover, they may form a part of thehealth care system in the foreseeable future, as they en-able focusing on the patient needs through personalizedinterventions demanded by recent focus on active andhealthy ageing [4]. Konstantinidis et al. [5] presented anexergaming platform which was used in the Long LastingMemories project [68]. In the latter papers, it was im-plied that the perceived, by the elderly users, usefulness ofthe physical exercise through exergames contributes totheir motivation and their subsequent adherence to theexercise protocol. In accordance with other studies, it wasalso reported that exergames can positively impact manyhealth areas, such as balance, gait [9], motion control [10],quality of life [5], mood and sociability [11, 12], self-esteem and reduced risk for depression [13]. Regular exer-cise at home could help to prevent diseases aggravated bya sedentary life imposed by conditions such as cardiovas-cular diseases and stroke [14]. Furthermore, health bene-fits from exercises could contribute to the reduction ofhealth care costs for insurance companies and the publichealth system as well [15].Only recently some studies started investigating andpresenting exergames as health monitoring tools,through collection and analysis of the data produced bytheir metrics [16]. Such metrics are quantitative mea-sures of the characteristics of game objects, used torecord a players performance and behavior. Theyrange from simple measures such as rating and totalgame time to more complex ones, which are calculatedby combining various variables/features [4]. More spe-cifically, metrics of balance games have been studied inthe context of fall prediction [17, 18], while other stud-ies have found significant correlations between re-sponse time and the risk of falling [19]. Besides this,measurements such as grip strength, heart rate, weightand speed, measured during exergaming sessions, havebeen proposed as potential indexes of frailty [20]. Theuse of game metrics as monitoring and assessment tools isfollowed by low cost development, time saving [16] andreal time recording and users performance visualizationof [15, 16]. Finally, game metrics are considered as reliabledeterminants towards exploitation for personalized gamedifficulty adjustment [21, 22].Moreover, the unobtrusive monitoring attitude ofgames contributes to elimination of user anxiety whichis apparent in conventional clinical cognitive assessment,as users do not realize the fact that they are being tested;they are, therefore, absorbed by the game [23]. Accord-ing to literature, assessments performed using gamemetrics and algorithms could be equally or even moreeffective and reliable than those performed in clinicalsettings [15, 20]. Furthermore, the role of the seriousgames in-game metrics in early detection of cognitive orphysical decline symptoms has recently gained the inter-est of researchers [2426].A point that deserves attention, however, is the fact thatthe lack of experience in using new technologies can causehigh mental and emotional stress as well as cognitive over-load. Moreover, it is difficult to correlate user interactivityin a game with game metrics as well as with clinical out-comes, since any two users playing the same game may in-deed have pretty different experiences [4].However, understanding and further exploiting gamedata derived from game metrics can be based on stan-dards and knowledge sharing, among people involved inthe design and development of exergames, as well as, itsuser deployment [15].The majority of existing games are designed for rec-reational purposes aimed at a typical healthy person,making it particularly challenging for elderly users [5].Therefore, the development of exergames that targetspecific populations [27, 28] such as people with de-mentia or cognitive decline and memory or vision im-pairments is of paramount importance [15, 29]. Inlight of this, some researchers have developed user-tailored games by combining game data with physicalexercise protocols [30] (planned, structured and repeti-tive motion in order to maintain or improve physicalfitness and body capacity [31]) or even just physical ac-tivity (body movement produced by the contraction ofskeletal muscles).Following this concept, a noteworthy number of ontol-ogies covering different domains of this type of seriousgames have emerged over the last years. Ontologies de-scribing games and gameplay achievements [32], gamedevelopment [33], multiplayer entities [34], graphics[35], physical activity [1] and physical exercise [2] arepublicly available. However, the fact that exergames havejust started being considered as therapeutic interven-tions [15, 36], along with the immature exploitation oftheir associated performance metrics, calls for an exer-game ontology combining all the relevant ontologies andfurther developing and describing recently introducedconcepts. Needless to say that the number of papers pre-senting game metrics as a tool of cognitive and physicalBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 2 of 15assessment in correlation with clinical assessment testsis rather limited. The current piece of work presents thefirst ontology that describes exergames and puts em-phasis on detailed performance metrics. Built on top ofothers, after a scrutiny of the exergame literature, theproposed ontology focuses on the semantic descriptionof games and game sessions and combines the conceptsof game and exercise. More specifically, the ontology in-troduces non-gaming concepts such as physical exercise,muscles involved and necessary equipment for games.This study suggests a unified model for concepts relatedto exergames. To that extent, it introduces a well-structured model for exergames description and opengame metrics data publishing, thereby incorporatinggame-related concepts such as game components, ses-sions and results. Consequently, it aims to facilitatemonitoring and understanding of health-related situa-tions though the establishment of common vocabulariesand data exchange standards through exergames. Theproposed ontology was adopted and integrated in anexergaming platform which was then piloted by fourteen(14) elderly users who participated in a number of gamesessions. The acquired game results were automaticallyconverted to RDF triples and published on the web asopen data, accessible through a SPARQL Endpoint.The remainder of this paper is structured as follows.The background section enumerates the existing ontol-ogies in the domain of games as well as some exergam-ing paradigms that already highlight the value of theirin-game metrics and performance indicators. In themethods section the development tools and processesare presented. The results section presents details of keyconcepts composing the exergame ontology. An exer-gaming platform realizing and utilizing the proposedontology is demonstrated in the same section along withinformation on accessing the RDF triples, derived fromthe actual game sessions, through a SPARQL endpoint.At the end of the paper light is shed on the importanceof this work towards contributing to healthcare therebyputting emphasis on monitoring and assessing theplayers status. Finally, the research limitations and chal-lenges of this work are discussed along with further en-visaged work.BackgroundSimilarly to the need for describing diseases in standardizedways by using suitable vocabularies such as InternationalClassification of Diseases (ICD) [37] and SystematizedNomenclature of Medicine (SNOMED) [38], there seemsto be an emerging need to allow standardized descriptionsof user interactions with exergames. An early effort alongthese lines was the Game Ontology Project (GOP) [32],which was one of the first attempts toward the semanticdescription of games; it provides a structured model for thegame and establishes relationships among its elements.GOP consists of five main components, namely, the userinterface, rules, goals, entities and entity manipulations. Asfar as game metrics are concerned, GOP includes metricssuch as score, time and success level. The Game ContentModel (GCM) is another ontology developed for seriousgames aiming at game development facilitation by non-specialists in the context of role-playing and simulationgames [33]. GCM creators have spotted that the GOP de-scribed game concepts from the end user perspective anddid not include notions that describe the game environ-ment, its structure and its events. Thus, Chan and Yuen[39], based on the GOP, developed the Digital Game Ontol-ogy (DGO) which expressed events and concepts related togames, such as users actions and production details, whilethey used the Semantic Web Rule Language (SWRL) tomodel game rules. They also highlighted the need for astandard format for data recorded by the game since thatcould contribute to the understanding of user-game inter-action. Moreover, Mepham and Gardner [34] developed anontology for online multiplayer games; this ontology de-scribed the common elements of games such as playersand game objects. Roman et al. [40] suggested an ontologyfor role-playing games, including elements such as charac-ters, story and resources. Furthermore, an ontology consist-ing of three main components comprised of graphics,configuration entities and multimedia has been presentedin the domain of mobile video games [35].Semantic web technologies can transform games fromisolated standalone programs to distributed and inter-connected systems enabling the combination of userdata with other datasets to produce useful knowledge[34]. Moreover, the use of ontologies and SWRL rulesmodelling could be deemed as an artificial intelligencemethodology [40]. Furthermore, game ontologies pro-vide a structured vocabulary that facilitates modellingand representation of design specifications as well asmaintenance and development of games [41]. In con-temporary projects with large groups from all over theworld, knowledge management, which can be further fa-cilitated by game and other semantics web technologies[42, 43], could be vital in game development.According to recent literature, there are no availablemethods established for evaluating the data generated bygame metrics [44]. Recently, exergames were attributeda role in unobtrusive health monitoring. However, theevaluation of game algorithms, such as those calculatingexpenditure and heart rate through game controllers orassessing physical and cognitive status, requires re-searchers to compare the data generated by these gameswith measurements from calibrated and certified devices(calorimeters, heart rate monitors, etc) or clinically validassessment tests. For instance, studies have shown thatcalculation of energy expenditure through games hasBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 3 of 15moderate correlation with corresponding measurementsfrom external devices [16], while game metrics have mod-erate correlation with cognitive assessment tests [32].However, in a typical scenario nowadays, game dataare usually the very property of gaming companiesthereby resulting in inability to verify any potential clinicaloutcomes of serious games. If only exergaming metricswere proved reliable and valid, exergames could replaceexternal devices or even ideally account as additionalmeasurement methods and information providers [16]. Itis, therefore, imperative that there is a need to open thiskind of data. One could appreciate that the incorporationof common models, specifications and data interoperabil-ity standards through a commons approach may result ina more effective collaboration among researchers and as-sist in the design and development of such games as wellas knowledge sharing. Exergame commons, comprisingshared knowledge, standards and tools, along with otherinitiatives such as open data commons [45] which may fa-cilitate publishing of game metrics data, would promotecollaborative and community-driven research therebymaximizing the return on investment [46].MethodsOntology editing tools and resourcesThe ontology was implemented using standard semanticweb technologies namely the Resource DescriptionFramework (RDF), RDF Schema (RDFS) and WebOntology Language (OWL), while some parts of theontology were developed using Protégé Desktop 5.0beta ontology editor [47]. Protégé is an open-sourceplatform that provides tools to develop ontologies andknowledge-based applications.Ontology development processDuring the design and development process, wefollowed the steps outlined in the work of Noy &McGuinness [48]. Firstly, the field of exergames as wellas the potentials uses of the ontology were carefully ex-amined. The author team has been designing and devel-oping exergames for more than seven years so far. Suchgames have been tested in large-scale pilots with over200 participants, facilitating a deep understanding ofplayer-game interactions [5, 36]. Extended analysis ofdata has been conducted deriving different game met-rics, but also revealing their likely correlations with clin-ical assessment tools. These findings led them also tosuggest some exergaming design guidelines for stealthassessment of cognitive and physical status [49]. Inaddition, the authors have already investigated the con-tribution of the exergame results to decision supportsystems [50] and the use of new, accessible to seniorstechnologies for the development of exergames [51]. Inorder to obtain a deep understanding of all aspects andconcrete entities comprising exergames themselves, aswell as, exergame sessions, the author team has orga-nized meetings, where multidisciplinary researcherssuch as exergame developers, psychologists and physicaltrainers/sport scientists, have attempted to collabora-tively define key exergame elements to be recorded.Then, a detailed search in literature as well as in reposi-tories of biomedical ontologies such as BioPortal [52],Ontobee [53] and AberOWL [54] was performed inorder to find relevant ontologies. Various relevant on-tologies were identified both in research papers as wellas in biomedical repositories. The most suitable oneswere incorporated in the proposed exergame ontology:i) FOAF: this ontology is used for the description ofplayers since it is one of the most popular ontologies forthe representation of people profiles; ii) OPE: this ontol-ogy was found at then BioPortal repository and is usedfor the representation of physical exercises. The exer-game ontology utilizes OPE for the semantic descriptionof the exercises involved in games; iii) NCI Thesaurus(NCIt) for the description of muscles involved in exer-cises: this ontology was found at the BioPortal reposi-tory and is used for the description of muscles that arenot included in OPE; iv) Quantity, Unit, Dimensionand Type collection of ontologies (QUDT) for definingthe units of measuring of game metrics; such metricsare used in measuring of physiological variables likeheart rate and blood pressure, while others focus onphysical dimensions such as time, degrees, etc. To be ofutility when analyzing the performance of the persondoing the exercise, these metrics have both value andunit of measurement. QUDT was selected for the ex-pression of the measurements; v) vCard for informationabout the site where a game session is carried out andvi) SKOS for the development of a vocabulary of com-mon terms for exergames. Afterwards, a list of game-related concepts was developed and the class hierarchywas defined. Since there are many ways to develop anontology and model the concepts related to a particulardomain, we have defined a straightforward class hier-archy, aiming to provide an easy-to-use model whichcan be effortlessly integrated to exergames. Subse-quently, the properties that represent relationships be-tween classes were identified and the propertyrestrictions were defined. In this step, difficulties en-countered linked with game metrics representation, sinceit was needed to model an n-array relationship. For thisreason, an auxiliary class was developed to associate gameresults, metrics and their values. Moreover, in order to letend-users have several options when it comes to propertyranges, restrictions were kept to a minimum. Finally, in-stances of classes were developed, which facilitate the con-nection of different data sets by referencing to sameconcepts. The development of the ontology was aBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 4 of 15repetitive process where the multidisciplinary team con-tributed to each stage.AvailabilityThe developed exergame ontology is available through theOCLC Persistent uniform resource locator (PURL) http://purl.org/net/exergame/ns#. This Uniform Resource Iden-tifier (URI) is dereferenceable, serving both RDF docu-ment and ontology documentation in HTML format,depending on the request. The documentation is gener-ated automatically using the Live OWL DocumentationEnvironment (LODE) [55]. In oder to facilitate the usageof the ontology, apart from the RDF/XML format of theontology, this methodology enables end-users to retrieve adescription of the resource through their browser at thesame URI, in line with repositories of biomedical ontol-ogies such as Ontobee and BioPortal providing an inter-face for all ontology terms.ResultsThe purpose of this study is to investigate and provide astructured description of the key concepts related to exer-games such as components of games, game sessions andgame results, composing the exergame ontology. Since thenature of the data produced by different games werehighly diverse and heterogeneous, special attention wasgiven to the design of the structure of game results to-wards a universal model for all exergames. All classes andproperties of the exergame ontology are depicted in Fig. 1.Konstantinidis et al. [5], proposed an architecturescheme of exergames, by defining the fundamentalbuilding blocks of an exergame. Aligning with theirparadigm, the exergame ontology incorporates classesthat refer to most of the layers and components of thatarchitecture (Fig. 2). In this study, the intention was tocover exergames from a user monitoring point of view,thereby focusing on game sessions, players and game re-sults. At a later stage, further game-related componentsFig. 1 Exergame ontology overview. A graphical representation of the ontology. All classes and properties that do not have a prefix belong toexergame ontologyBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 5 of 15would be incorporated in the ontology, such as the gameengines used for its development as well as event man-agement details.In the sections below, each concept of the developedontology is presented separately.GameFor the description of a game as an entity we use con-cepts from GOP and GCM including goals, game con-trollers and presentation hardware. However, taking intoaccount the particularities of exergames, the ontologieswere extended to cover new concepts such as physicalexercise and auxiliary equipment (e.g. weights and fit-ness springs). For each game, an instance of class exer-game:Game is created and then all sessions refer to thisgame by its URI.GoalsThis section includes game goals, exercise goals andgame metrics. Game goals refer to the objectives andconditions that must be fulfilled in order for the playerto succeed in the game. They could be either high levelobjectives such as victory or success at some level, orlower level objectives such as avoiding an obstacle. Aseach game incorporates a physical exercise, one of themain objectives is the proper execution of this exercisewhich is referred to as an exercise goal. Game metricsare shared among the games and thus instances of theclass exergame:GoalMetric, that could be referenced byall games, should be developed. A list of instances whichwas developed in this study is available at http://pur-l.org/net/exergame/metric#. Moreover, each game metricshould have a unit of measurement which is an instanceof the class exergame:Unit, an equivalent class of qud-t:Unit. Some instances of the qudt:Unit can be found atthe QUDT Units Ontology [56].PlayerRefers to the user playing the game and it is connectedwith game sessions and their results. The FOAF ontologyis used to describe each player through the foaf:Personclass.InterfaceInterface is the shared boundary between user and com-puter and refers to how the player interacts with the gameas well as how the game is presented to the player. It con-sists of the presentation hardware and the game control-lers. The former represents the device on which the gameis presented (e.g. tablet, computer, smart phone, videogame console, etc.). The latter stands for the deviceswhich provide input to a game in order to control an ob-ject or a character in it (e.g. keyboard, joystick, steeringwheel, etc.). The game controllers include also contem-porary controllers which detect motion and users pos-tures and gestures (e.g. Kinect, Wii Balance Board andWii Remote) as well as other recently introduced deviceswhich record brain activity such as NeuroSky MindWaveFig. 2 Exergame architecture. An illustration of exergames components along with their semantic description. Dark blocks refer to concepts thatcan be modeled using the exergame ontology. In each of these blocks, the classes being used to model these concepts are included insidebrackets. The remaining (light) blocks will be incorporated to the ontology in a following version.Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 6 of 15and Emotiv. Instances of the class exergame:GameCon-troller were developed for most modern game control-lers and are available at http://purl.org/net/exergame/controller#. These instances were connected throughthe owl:sameAs property with the corresponding re-sources of DBpedia, which is the core of linked data.Due to the small size of game controllers alongside thelow rate of new emerged controllers, the links betweenthese instances and DBpedia were done manually, sincethe use of automatic tools was not worth the effort. Ifthe number of instances increases dramatically, employ-ing automatic and semi-automatic semantic linkage andintegration frameworks such as SILK [57] will be ofparamount importance.Physical exerciseEach exercise is described using the OPE ontology in-cluding the muscles involved, the equipment used dur-ing the exercise as well as possible health benefits, likeimproved fitness and clinical outcomes. Physical exer-cises like biceps and leg extension are represented asinstances of the class exergame:Exercise.Game sessionGame session refers to the period in which a player isplaying a game. It begins when the game starts and endseither with the end of the game or when for some rea-son the game stops. The session, apart from the startand end time, is associated with concepts like the player,the game itself, the site and the data produced by gamemetrics.Game resultsA game result includes all data derived from game metricsin one session of a game. Each game may have more thanone metrics and more than one iterations (referring to thecorresponding physical exercise iteration). Each game ses-sion is associated with an instance of the class exerga-me:Result and this instance is linked with a set of gamemetrics along with their values and units of measurement.In RDF and OWL, a property is a binary relation used tolink two instances or an instance to a literal value. For thisreason, in order to define an n-array relationship [58] thatincludes an instance of the class exergame:Result, in-stances of the classes exergame:GoalMetric and exerga-me:Unit as well as an RDF literal which represents themetrics value, the auxiliary intermediate class exergame:-MetricRelation is used, which represents this relationship.In case that a game consists of one metric and iteration,the sessions result includes an instance of exergame:Me-tricRelation which is linked to a metric, a unit of measure-ment and a value. In the event of more iterations, theproperty exergame:metricValue links to a sequence ofmetric values using rdf:Seq.SiteThe site refers to the venue where a game is conducted in-cluding the address and the type of it (e.g. Ecologicallyvalid Active and Healthy Ageing Living Lab area - MedicalPhysics Laboratory of the Aristotle University of Thessalo-niki). Venue details may include street address, email ad-dress, telephone numbers, coordinates etc. For thesedetails, many well-known ontologies could be used suchas vCard, but the range of the property exergame:siteAd-dress was left open thereby allowing for any other form.For example, a vCard instance can be used as its valueand besides address it can provide site coordinates inthe form of geographical coordinates (geo URI) usingvCard:hasgeo property. As far as site category is con-cerned, instances of the class exergame:SiteType weredeveloped, which represent specific categories such ashome, research laboratory and senior center (http://pur-l.org/net/exergame/sitetype#).Vocabulary for common conceptsTo date, there is no common vocabulary for terms re-lated to exergames that uses semantic web technologies.Therefore, in order to support the reuse of exergameconcepts, the exergame ontology incorporates the exer-game:concept property which links instances to the con-cept they represent. Moreover, a vocabulary of suchterms was developed using the SKOS ontology; this isnow available at http://purl.org/net/exergame/concept#.For example, in the case that an exergame includes atime-related metric such as reaction time, the instancethat represents this metric could connect with the con-cept of time, which is an instance of skos:Concept. Thisvocabulary is intended to be scalable and form an inte-gral part of exergame commons, since it can be viewedas a more general dictionary, just to cover for those re-searchers for which the already defined concepts do notfit their own needs. As an illustration, when an instanceof a class (e.g. an instance that represents a game metricwhich calculates the distance covered by the player) con-cerns a concrete concept which is not included in theaforementioned vocabulary, it can be extended by devel-oping an instance of skos:Concept. Therefore, the exist-ence of a rich vocabulary is on the exergamecommunitys best interest since it facilitates the linkageof two distinct instances that refer to a common con-cept. As a result, game results derived from distinctexergames can be analyzed together or compared toeach other, based on their common characteristics.ApplicationsThis study provides opportunities to capitalize on openexergame data for active and healthy ageing. Publishinggame results as open data will enable the exergame com-munity to test these data, validate their algorithms,Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 7 of 15combine game results with other datasets and producenew knowledge. The task of adapting the ontology toan exergame is pretty simple, regardless the technologyused; In fact, each game should be semantically de-scribed only once. Regarding game sessions, all infor-mation, including game results, can effortlessly besemantically enriched and stored to a triplestore aswell. On the other hand, in most games the results arestored in a database. Hence, already stored game resultscan be converted to RDF utilizing a language for ex-pressing mappings from relational databases to RDFdatasets, such as R2RML [59]. As far as privacy is con-cerned, any information about patients that might re-veal their identity should not be stored. For this reason,a plain unique identifier that corresponds to a particu-lar patient may be attached to game sessions data.Integration in HTML5 exergamesExergame ontology was integrated in HTML5 games onwebFitForAll (wFFA) web platform (Konstantinidis EI,Bamparopoulos G, Bamidis PD: Moving Real ExergamingEngines on the Web: The webFitForAll case study in an ac-tive and healthy ageing living lab environment, submitted.).wFFA is an exergaming platform developed to support e-health applications incorporating various game controllerssuch as Wii Balanceboard, Wii Remote, Microsoft Kinectand NeuroSky MindWave through the transparent Con-troller Application Communication (CAC) Framework[60]. wFFA contributes to fitness maintenance and well-being of elderly people through specifically designedgames/interfaces with simple graphics, which belong tothree categories. In the first category, there are gamesthat incorporate physical exercises such as stretching,resistance and weight lifting, aiming at power increaseof the upper and lower limbs [5]. These games com-prise a predetermined number of iterations and includemetrics such as success, reaction time and goal time.Moreover, there are games such as Apple, Hiking,Fishing and Golf which are related to physical activ-ities in conjunction with light cognitive tasks, such aswalking, balance and reaction. The metrics of thesegames are total time and players score, which is calcu-lated differently for each game (number of apples in thebasket, number of steps). Finally, the last categoryincludes interfaces which facilitate the health measure-ments collection, including but not limited to weightand blood pressure. These measures are treated in thesame way as game metrics.Integration architectureThe wFFA is based on a JavaScript library which is re-sponsible for several tasks such as coordination of gamesand exercise protocols and consists of three main com-ponents: the initializing component, the processingcomponent and the output component. For the purposesof this study, specific extensions were developed for eachcomponent in order to support the semantic descriptionof players, games and sessions as well as to enablegames data storage to a triplestore. One of the maintasks of the initializing component is the game control-lers management and the exercises difficulty adjustment.In this study, an extension was developed that initializesgame metrics in order to record players behavior.The processing component accounts for the properfunction of the game and consists of a set of subcompo-nents, being executed during a game session, includingcommon game tasks such as feedback control. We ex-tended this component in order to enable data collectionfrom game metrics resulting in a detailed record of usersactions. Furthermore a preprocessing component was de-veloped to gather all information about the game sessionby utilizing the users unique identifier to retrieve the URIof the instance of class exergame:Player that representsthem, which was stored automatically when the users ac-count was created. Any information that could potentiallydisclose the user identity is omitted. As far as the site isconcerned, an HTML form was developed extending theusers settings to include information about the site (a setof predefined site categories are provided as options).Once this form is completed, site details are added to thesession. Finally, the start date and time are added to thesession, which is almost ready to be saved.The output component is activated at the end of eachgame and is responsible for storing the result of a gamesession in a relational MySQL database (through a RestAPI). In this study, a subcomponent was developed thatprocesses the data from game metrics, describes them se-mantically according to the ontology and adds them in thegame session along with the end date and time. As long asall the information about the game session is converted toRDF triples, it is uploaded in a Sesame triplestore usingAjax. All triples concerning games, exercises, game sessionsand patients are stored in a single graph in the triplestore.Description of game SideRaisesIn the SideRaises game, players start by holding handweights straight down at their sides. Then they have toraise both arms to the side at shoulder height and afterthey hold this position for a few seconds, they lowertheir hands and continue with the next iteration. De-pending on their physical condition, users can use differ-ent weights. The game consists of the following metrics:success, reaction time and goal time and it utilizes aKinect as a game controller and a smart TV as a presen-tation hardware. The corresponding exercise involvesthe deltoid, the infraspinatus, teres minor and major andthe latissimus dorsi muscles. For this example, the fol-lowing prefixes are used:Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 8 of 15@prefix : <http://www.fitforall.gr/resources#>.@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>.@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.@prefix exergame: <http://purl.com/net/exergame/ns#.@prefix exergame-metric: <http://purl.org/net/exergame/metric#>.@prefix exergame-gamecontroller: <http://purl.org/net/exergame/controller#http://purl.org/net/exergame/controller#>.@prefix ope: <http://www.semanticweb.org/ontologies/2013/2/OPE.owl#>.@prefix nci: <http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#>.The exercise and muscles involved are described first.In the case that a muscle is not involved in OPE, it is de-fined as a subclass of nci:Muscle.Thereafter, the :SideraisesGame is defined alongside itsmetrics, game controller and presentation hardware:Piloting with users in a living labA number of elderly volunteers were visiting a speciallydesigned environment in the laboratory of Medical Physicsat Aristotle University of Thessaloniki [60] daily and wereplaying a set of exergames. This environment, facilitatingalso the trials of USEFIL project [6165], had two sepa-rated areas, a furnished one with sofa, table and chairs toresemble a small living room and an area with a sink thatresembles a bathroom. This configuration was designed tocreate an ecologically valid experimentation environment.ParticipantsFourteen elderly people (3 males and 11 females, averageage 73.4 years old) who had no previous experience withtechnology participated in this study. On average, eachelderly person visited the specially designed environmentfor 6 days in total and played/interacted with 7 differentgames each time. The total time of all game sessions forall users was about 35 h.Game dataThe game metrics data which was produced duringsessions of the aforementioned games were convertedto RDF by the game framework and were uploaded ona triplestore as open data. In order to save the resultsof game sessions, there are about 30,000 records inthe triplestore corresponding to 35 h of gaming. Alldata are accessible from a SPARQL endpoint that isavailable at http://www.fitforall.gr/sparql, where quer-ies can be made using the GET or POST method. InTable 1, data from a game session of SideRaisesgame are depicted. For instance, the SPARQL queryfor getting all values from the metric that measuresgoal time (exergame-metric:GoalTime) during the 3rditeration of SideRaises game, along with the corre-sponding player and date of game session, is the fol-lowing:DiscussionIn this study, an ontology for exergames was developed byfusing concepts from gaming and physical exercise into asingle structure. As existing gaming ontologies targetmainly at the game design and development including con-cepts such as game scenario and game objects [32, 33, 39],they do not provide a comprehensive model that describesthe data produced in a game session. As a consequence, astandard structure that facilitates publishing of game dataon the web was developed. This was achieved by modellingBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 9 of 15the concept of the game session, which in combinationwith the game, the player and the game results, formed auniversal model for the semantic description of the playingprocess. This structure could then be effortlessly integratedinto other existing ontologies.Another neat contribution of this work though, is thatof opening the game data emerging from the aforemen-tioned game sessions. For the first time, such data weredescribed semantically in a machine-readable formatusing W3C standards and some instances are linked torelated resources from DBpedia. Complying with thehighest level in the 5-star open data scheme introducedby Tim Berners-Lee [66], this dataset is, to the best ofthe authors knowledge, the worlds first available opendataset of exergames metrics described semantically bymeans of Linked Open Data (LOD). Furthermore, open-ing game data in an RDF format is in researchers, care-givers and patients best interests, as game analytics anddata visualization techniques can be subsequently ap-plied, enabling them to monitor patients performanceand retrieve visual signs of early detection and deterior-ation. Further exploitation of this dataset, along withother semantically annotated monitoring sources of theproject (gait analysis, cognitive assessment tests, etc.),will open new avenues in Active and Healthy Ageing[67] as well as health monitoring per se as it will providenew insights of the exergames role as assessment tools.This is in line with the emerging trend of knowledge ex-traction from multimodal sources, towards knowledgeunderstanding by combining information with differentgranularity ranging from muscles and exercices to cogni-tive functionality and health status [68].Contribution to healthcareThis work aims at semantically enriching exergamingdata, thereby enabling semantic processing in the fieldof exergames. Semantic description of game resultsalongside open access enables the biomedical semanticscommunity to develop automatic processing, analysisand visualization tools for these data. Such tools aremuch meaningful in the biomedical domain since fur-ther research and study of the produced information willhopefully contribute to a better understanding of any pa-tients health state and condition. Abiding to the holisticdefinition of health by WHO [69] as well as currentehealth emphasis on personalized, citizen driven elec-tronic health record systems (EHRs) [70, 71], it wouldbe reasonable to envision expansion and exploitation ofherein presented achievements within future EHR sys-tems. This may be merely attempted by incorporating oreven better semantically linking a persons EHR withexergaming behavior and performance data. The advan-tages of such an EHR approach promoting joint exploit-ation of heterogeneous information sources [72], wouldTable 1 RDF triples produced during a session of SideRaises gameSubject Predicate Objecta_:SideRaisesGameSession rdf:type exergame:GameSession_:SideRaisesGameSession exergame:result _: SideRaisesGameResult_:SideRaisesGameSession exergame:site <http://www.fitforall.gr/resources/MedicalPhysicsLaboratoryAuth>_:SideRaisesGameSession exergame:startDateTime 2014-09-22T18:09:39 02:00_:SideRaisesGameSession exergame:endDateTime 2014-09-22T18:09:39 02:00_:SideRaisesGameSession exergame:player <http://www.fitforall.gr/resources/player162>_:SideRaisesGameSession exergame:game <http://www.fitforall.gr/resources/SideRaisesGame>_:SideRaisesGameResult rdf:type exergame:Result_:SideRaisesGameResult exergame:metricRelation _:MetricRelation1_:SideRaisesGameResult exergame:metricRelation _:MetricRelation2_:SideRaisesGameResult exergame:metricRelation _:MetricRelation3_:MetricRelation1 exergame:metricName <http://purl.com/net/exergame/metric# Success>_:MetricRelation1 exergame:metricValue _:node197jv5qonx686_:node197jv5qonx686 rdf:type rdf:Seq_:node197jv5qonx686 rdf:_1 TRUE_:node197jv5qonx686 rdf:_2 TRUE_:node197jv5qonx686 rdf:_3 FALSE_:node197jv5qonx686 rdf:_4 TRUE_:node197jv5qonx686 rdf:_5 FALSE_:node197jv5qonx686 rdf:_6 TRUEaFor brevity, _:genid-6569e0b8e8774a22b73a515a98c0f963- is abbreviated as _:Bamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 10 of 15be twofold. First, the EHR information would give a bet-ter understanding of the exergaming information and itsvalue in the context of the general health. Secondly, theEHR information, decoded to exergaming aspects, willlead to automatic, design user-tailored exercise proto-cols, taking into account any possible conditions andlimitations.The need for evaluation of exergamesSeveral studies highlight the importance of exergames asmonitoring and evaluation tools which is achievedthrough the collection and analysis of data produced bytheir metrics [9, 10, 16] and the gaming environment[73]. However, the absence of large volumes of researchthat evaluates these data hinders the full integration ofexergames in healthcare systems [44]. This work en-hances the prospects for further research on evaluatingexergames and opens up new ways for open linked gamedata, thereby facilitating their analysis, verification andunderstanding by the scientific community.The need for comparison of games metrics and algorithmsto external monitoring devicesAs mentioned previously in this document, some seriousgames integrate interfaces performing various measure-ments through game controllers such as energy expend-iture. Hence, there is a need to compare their resultswith data from corresponding measurements from exter-nal devices as well as self-monitoring devices in terms ofquantified self. To this end, several studies indicate thatthese measurements should be published as open dataenabling researchers to compare them and verify theirreliability [16]. In the exergame ontology, these measure-ments are treated as they were data produced by gamemetrics and described using the same format, laying thebasis for publication of such data.Real-time data analysisIn the context of collecting data in home environments,there is an increasing interest in automated analysis. Dataproduced in such environments can be analyzed in realtime, reducing the costs of health services [74]. Healthprofessionals can provide advice to patients based on deci-sion support systems that consume these data automatic-ally, thereby facilitating personalized healthcare [75, 76].Decentralized informationMoreover, the need to store data in a central database isreduced since patients can play games at their homeswhich results in the development of a decentralizedknowledge base. For instance, information concerninggames, exercises and game results could be stored in dif-ferent locations and recovered through queries overmultiple SPARQL Endpoints. Linking patient data frommultiple sources may result in a better understanding ofhealth-related problems and even in acquisition of newknowledge. The latter can only be achieved though,under the condition that the data will be open. By open-ing access to medical databases we can save valuabletime facilitating early detection of diseases [77]. For in-stance, datasets from open public health initiative [78],in which federal health agencies publish health-relateddata along with population data can be linked with dataproduced by exergames. Moreover, they can be com-bined with or even extend patient personal health re-cords derived from well-known personal health recordsystems such as Microsoft HealthVault, Dossia andWorld Medical Card [79].Unobtrusive monitoringNowadays, a noteworthy number of research efforts is car-ried out on early detection of cognitive decline utilizingsensors at home [8082], paying extra attention in pre-serving a sense of unobtrusiveness. Serious games havejust been considered as potential unobtrusive monitoringsources [30]. Therefore, ontologies like the one proposedin this paper will further facilitate the evolution of the ser-ious games into considerable sources of information.ChallengesBig dataThe advent of embedding processors to everyday de-vices, converting them to smart objects, opens the roadtowards ubiquitous computing [83]. Quantified-self, in-cluding data about diet, mood, physical activity, sleepquality etc., is leveraged from such technological ad-vances, as they facilitate acquisition of daily living data.Smart sensors, self-reporting via mobile applications andother devices are employed to the quantified-selfrealization [84], resulting in large amounts of data. Bigdata has been receiving an increased attention in bio-medical and healthcare applications, paving researchfrom hypothesis-driven to data-driven. Therefore, thereis an increasing demand in new analysis techniques de-velopment, following the size, variety and complexity ofthese data hindering information retrieval [85, 86]. Bigdata are either formed as structured data, such as thoseproduced by games, or unstructured information. Theunstructured data constitute more than 80 percent ofthe available data [86]. Considering the ever increasinguse of serious games, the data produced is expected toscale exponentially. Taking into account the rapid in-crease in the volume of these data, standards for the ex-change and linkage should be developed by healthprofessionals, so as to facilitate data interoperability andintegration on different platforms [75]. Furthermore,special attention should be paid to the storage anddistribution limitations as well as the development ofBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 11 of 15techniques, tools and infrastructures for data processingand analysis [74].PrivacyDespite the large number of positive statements about theneed for transparency on information about clinical trialsover many years [87, 88], as far as privacy is concerned, dis-closing users personal data should be carefully handled[15]. In alignment with the patient commons, it must beensured that any information system used in storage wouldprotect them from unauthorized access, since such data aresensitive while ownership issues are not always clear [74].In the realm of data openness, special attention should bepaid when publishing game results on the web to ensurethe protection of privacy [15]. In this study, informationabout game sessions was published obscuring any in-formation that may reveal whose data are included ineach session. Additionally, the protocol followed by thepilots of this work has been approved by the BioethicsCommittee of the Medical School of the AristotleUniversity of Thessaloniki (No.93/26-6-2014). However,continuous identification of users to different sources is arisk that increases with the volume of data [89]. Inaddition, apart from the raw data, origin, type and acquisi-tion method should be part of the metadata schema. Onthe other hand, sharing data that is incomplete and notdescribed properly may have an adverse effect, therebyleading to incorrect assumptions [90].Weaknesses and limitationsAlthough the power of data openness and sharing is ac-knowledged, there are some concerns in the scientificcommunity about publishing data on the web. More spe-cifically, many researchers are reluctant to open theirdata as they are afraid that others will not follow thesame approach or that they could not be able to controldata access [90]. In recent years, there have been effortscontributing to this direction by reinforcing data accessand exchange policies. In 2010, U.S. Presidents Councilof Advisors on Science and Technology (PCAST) issueda report [91] about the potential of health informationtechnology to improve healthcare, underlining the needfor the adaptation of a universal exchange language forhealthcare information. Other studies state that RDF canbe served as a Universal Healthcare Exchange Language,since it is format independent and all existing vocabular-ies such as Health Level 7 (HL7) and Clinical DocumentArchitecture (CDA) can be mapped to RDF [92].With regard to the exergame ontology, a limitation isthat this study focused mainly on the high-level conceptsdescription such as game controllers, goals, game metricsand presentation hardware, while existing ontologies offera more detailed description of a game itself (e.g. game sce-nario and game entities). Moreover, the ontology includesonly structured exercises, which is going to be addressedin the authors future work by incorporating physical ac-tivity as well. Furthermore, possible weaknesses of userssuch as vision and movement problems as well as mentaldisorders are not part of the players description. For in-stance, the results produced during a game session mayvary between a healthy user and a user with mobility prob-lems. The authors intend to study corresponding ontol-ogies and integrate them in the exergame ontology.Finally, this work included a small number of real pa-tients and exercises focusing on establishing the basiscorrectly and studying the feasibility of the employed in-frastructure. Performance and scalability evaluation witha large number of seniors, participating in a completeprotocol (in terms of exercises) will take place within anewly funded research project (www.uncap.eu) wherethis infrastructure will be further exploited.Future workLinking game results with other datasetsThe next decade will be the beginning of integrated gam-ing, when game data and social network data will belinked with other datasets [4] such as electronic health re-cords [44]. An attempt to this direction was made byAetna, a US insurance company that developed an openplatform facilitating data integration from various sourcessuch as medical records [84]. Game data can be combinedwith data produced by smart phones in the context of util-izing sensors such as accelerometers and GPS towardsproducing large amounts of location specific data con-cerning physical activity and physical exercise [86]. Apartfrom fitness-related data, information from other sourcescan also be combined with games such as data about diet,sleep quality and allergies. The need for such data integra-tion has led to the emergence of Data as a Service (DaaS)platforms such as linked life data that provides access todata from various biomedical databases through a singleSPARQL Endpoint [93]. With the aid of these technolo-gies, anonymous data can be effortlessly extracted and uti-lized by pharmaceutical and insurance companies as wellas agencies on health related information to improve drugdesign and health protocols altogether. Towards this dir-ection, we aim to semantically describe all exergames thatare part of the web FitForall platform. Additionally andapart from the instances developed in this work represent-ing contemporary game controllers that are already linkedto DBpedia, other concrete entities will be linked to exist-ing datasets as well. More specifically, game metrics, exer-cises involved in games as well as ailments for whichexercises might be employed as treatments or preventativemeasures will be linked to DBpedia and relevant datasetswhich utilize biomedical ontologies such as SNOMEDand ICD. Moreover, when the amount of RDF triplesrepresenting game sessions and results grows, the numberBamparopoulos et al. Journal of Biomedical Semantics  (2016) 7:4 Page 12 of 15of links with other LOD datasets will be increased as welland as a result, exergame-related datasets could then be-come part of the LOD cloud.Game analytics and semantic processing toolsApart from linking game results with other datasets, thereis an increasing interest in game analytics in the field ofgame research. Players today have a wide range of ages,backgrounds, intellectual abilities and motivations. Theneed for understanding players behavior and experienceduring the game has led to integration of game analyticsinto design and development process [94]. Data fromgame metrics and players behavior are collected and ana-lyzed throughout the game, revealing valuable informationthat range from details about the players profile to design-related information such as software bugs and designproblems [95]. Game analytics constitute an importantfield of business intelligence for the game industry, whilethe combination of game data with other information setssuch as market reports and quality assurance systems facil-itates knowledge management, marketing and decisionmaking. The semantic description of games, facilitatessearching and retrieval of their data and enables the devel-opment of knowledge extraction and data mining toolsthrough automatic processes and semantic web crawlers,opening the way for serious game and exergame analytics.Personalized exercise protocolsToday, the rapidly increasing gaming population, regard-less of age and background, has led to the maximizationof the need for customized games [27]. In the context ofphysical activity and exercise, recommendation systemsfor exergames may be developed, utilizing automatedreasoning. In this manner, personalized exercise proto-cols can be designed, taking into account several factorssuch as game sessions data, biological factors as well aspossible disabilities.ConclusionsIn this work, a unified model for the semantic representa-tion of exergames is proposed, while actual data from gamesessions are published as open data on the web. Aligningwith a commons approach, this study aims to be the outsetof an exergame commons initiative, which will establishshared standards across the researchers and facilitate exer-game research. Last but not least, this exergame commonsand open exergame data initiative is one of the very first ef-forts to enable the concept of Open Trials (an initiative thatwill aggregate information from a wide variety of existingsources in order to provide a comprehensive picture of thedata and documents related to all trials of medicines andother treatments around the world) for health monitoring,in general, as well as, Active and Healthy Ageing morespecifically.AbbreviationsCAC: Controller application communication; CDA: Clinical documentarchitecture; DGO: Digital game ontology; EHR: Electronic health record;FOAF: Friend of a friend; GCM: Game content model; GOP: Game ontologyproject; HL7: Health level 7; ICD: International classification of diseases;LOD: Linked open data; LODE: Live OWL documentation environment;MMSE: Mini-mental state examination; OPA: Ontology of physical activity;OPE: Ontology of physical exercise; OWL: Web ontology language;PCAST: Presidents Council of Advisors on Science and Technology;PURL: Persistent uniform; resource locator; RDF: Resource description framework;RDFS: Resource description framework schema; SNOMED: Systematizednomenclature of medicine; SWRL: Semantic web rule language; URI: Uniformresource identifier; wFFA: Web fitforall.Competing interestsThe authors declare that they have no competing interests.Authors contributionsGB contributed to the ontology conceptualization, design, developmentand integration to HTML5 games, was involved in the data collection aswell as the literature review and had primary responsibility for writing themanuscript. EK contributed to the ontology conceptualization and design,was involved in the data collection, literature review and writing themanuscript. CB contributed to the ontology conceptualization. PDBcontributed to the ontology conceptualization, directed the design as wellas the literature review and had primary responsibility for writing andreviewing the manuscript. He is the guarantor of this manuscript. All authorsread and approved the final manuscript.Author details1Medical Physics Laboratory, Medical School, Faculty of Health Sciences,Aristotle University of Thessaloniki, Thessaloniki, Greece. 2MathematicsDepartment, Aristotle University of Thessaloniki, Thessaloniki, Greece.Received: 2 July 2015 Accepted: 25 January 2016RESEARCH Open AccessNormalizing acronyms and abbreviations toaid patient understanding of clinical texts:ShARe/CLEF eHealth Challenge 2013, Task 2Danielle L. Mowery1*, Brett R. South1, Lee Christensen1, Jianwei Leng1, Laura-Maria Peltonen2, Sanna Salanterä2,Hanna Suominen3, David Martinez4,5, Sumithra Velupillai6, Noémie Elhadad7, Guergana Savova8,Sameer Pradhan8 and Wendy W. Chapman1AbstractBackground: The ShARe/CLEF eHealth challenge lab aims to stimulate development of natural languageprocessing and information retrieval technologies to aid patients in understanding their clinical reports. In clinicaltext, acronyms and abbreviations, also referenced as short forms, can be difficult for patients to understand. For oneof three shared tasks in 2013 (Task 2), we generated a reference standard of clinical short forms normalized to theUnified Medical Language System. This reference standard can be used to improve patient understanding bylinking to web sources with lay descriptions of annotated short forms or by substituting short forms with a moresimplified, lay term.Methods: In this study, we evaluate 1) accuracy of participating systems normalizing short forms compared to amajority sense baseline approach, 2) performance of participants systems for short forms with variable majoritysense distributions, and 3) report the accuracy of participating systems normalizing shared normalized conceptsbetween the test set and the Consumer Health Vocabulary, a vocabulary of lay medical terms.Results: The best systems submitted by the five participating teams performed with accuracies ranging from 43 to72 %. A majority sense baseline approach achieved the second best performance. The performance of participatingsystems for normalizing short forms with two or more senses with low ambiguity (majority sense greater than80 %) ranged from 52 to 78 % accuracy, with two or more senses with moderate ambiguity (majority sensebetween 50 and 80 %) ranged from 23 to 57 % accuracy, and with two or more senses with high ambiguity(majority sense less than 50 %) ranged from 2 to 45 % accuracy. With respect to the ShARe test set, 69 % of shortform annotations contained common concept unique identifiers with the Consumer Health Vocabulary. For these2594 possible annotations, the performance of participating systems ranged from 50 to 75 % accuracy.Conclusion: Short form normalization continues to be a challenging problem. Short form normalization systemsperform with moderate to reasonable accuracies. The Consumer Health Vocabulary could enrich its knowledge basewith missed concept unique identifiers from the ShARe test set to further support patient understanding ofunfamiliar medical terms.Keywords: Natural language processing, Acronyms, Abbreviations, Consumer health information, Unified MedicalLanguage System* Correspondence: danielle.mowery@utah.edu1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USAFull list of author information is available at the end of the article© 2016 Mowery et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 DOI 10.1186/s13326-016-0084-yBackgroundInternational healthcare policies aim to improve patientsaccess to their clinical record and involvement in theirhealthcare delivery, e.g. in the United States [1], inAustralia [2], and in Finland [2]. These policies have mo-tivated healthcare organizations to adopt patient-centered approaches e.g., the United States Open Notesproject [3], some resulting in modest benefits and min-imal risks [48].Patient access to easy-to-understand, simple text inclinical reports is also stipulated in several countries bylaw. For instance, regulations in the United States [9]and European Union [10] state that patients should haveaccess to their clinical information upon request [11].United Kingdom guidelines describe best practices forpatient access [12]. Laws and statutes in Sweden [13]and Finland [14] state that clinical notes must be explicitand comprehensive, including only well known, acceptedconcepts and abbreviations.Automated tools for text simplification can help clini-cians comply with regulations and improve informationreadability for patients. For instance, statistical approachescan identify, reduce, and disambiguate unfamiliar con-cepts. Specifically, unsupervised methods and statisticalassociations can automatically learn unfamiliar terms,identify potential semantic equivalents, and present layterms or definitions [1517]. Text simplification architec-tures can analyze, transform, and regenerate sentences forpatients e.g., simplifying Wall Street Journal sentences forAphasia patients [18]. In the biomedical domain, one textsimplification tool reduces the semantic complexity ofsentences conveying health content in biomedical articlesby substituting unfamiliar medical concepts with syno-nyms or related terms, and the syntactic complexity bydividing longer sentences into shorter constructions [19].In the clinical domain, a prototype translator reducesthe semantic complexity of clinical texts by replacingabbreviations and other terms with consumer-friendlyterms from the Consumer Health Vocabulary and ex-planatory phrases [20].Making annotated corpora available to the natural lan-guage processing community through shared tasks can fur-ther stimulate development of technologies in this researcharea [21]. Like the Message Understanding Conference(MUC) [22], Text REtrieval Conference (TREC) [23, 24],Genome Information Acquisition (GENIA) [25, 26], andInformatics for Integrating Biology and the Bedside (i2b2)challenges [2731], the 2013 Shared Annotated Resources/Conference and Labs of the Evaluation Forum (ShARe/CLEF) eHealth Challenge evaluated participant naturallanguage processing systems against a manually-generatedreference standard [32]. The 2013 ShARe/CLEF eHealthChallenge took initial steps toward facilitating patientunderstanding of clinical reports by identifying andnormalizing mentions of diseases and disorders to astandardized vocabulary (Task 1) [33], by normalizingacronyms and abbreviations (Task 2) [34], and by re-trieving documents from health and medicine websitesfor addressing patient-centric questions about diseasesand disorders documented in clinical notes (Task 3)[35]. This paper describes studies related to Task 2.We review acronym and abbreviation recognition inthe context of text normalization. We are motivated bythe need for creating an annotated corpus of acronymsand abbreviations to encourage the development of nat-ural language processing tools that improve patient un-derstanding and readability of clinical texts.Text processing for acronyms and abbreviationsConceptually disambiguating the meaning of a word orphrase from clinical text often involves mapping to astandardized vocabulary [36]. For example, natural lan-guage processing tools that normalize words and phrasesto Unified Medical Language System (UMLS) conceptunique identifiers (CUIs) include IndexFinder [37],KnowledgeMap [38], MetaMap [39], Medical LanguageExtraction and Encoding System (MedLEE) [40] andclinical Text Analysis and Knowledge Extraction System(cTAKES) [41]. Acronyms and abbreviations are short-ened words used to represent one or more concepts[42]. Acronyms are formed from the first letters ofwords in a meaningful phrase (BP = Blood Pressure) andcan be pronounced as words (CABG = Coronary ArteryBypass Graft, pronounced cabbage) or letter-by-letter(TIA = Transient Ischemic Attack, pronounced T-I-A).Abbreviations are shortened derivations of a word orphrase (myocard infarc =myocardial infarction) and aregenerally pronounced as their expanded forms (myocardinfarc, pronounced myocardial infarction). We will referto acronyms and abbreviations throughout the manu-script as short forms for brevity and to convey a mixtureof both acronyms and abbreviations, including their lex-ical variants from the clinical text.Accurate short form detection methods may handlevarious linguistic characteristics and phenomena associ-ated with short form usage in text. Short forms are doc-umented using different orthographic constructionsincluding varied letter case (CAD vs cad). Punctuationcan be applied to acronyms to represent one concept(b.i.d. means twice a day) or list many related concepts(m/r/g represents three heart sound concepts - mur-murs, rubs, gallops). Syntactically, short forms may beconveyed using both singular and plural forms (TIA/TIAs) as well as possessives (Pts). Syntactically, a shortform can conceptually represent different long forms ofthe same concept and semantically, short forms may bepolysemous, having different, but related word senses(LV can stand for an adjectival phrase like leftMowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 2 of 13ventricular or a noun phrase left ventricle). Short formsmay be homonymous, having different, unrelated wordsenses within and across report genres (LV can standfor both left ventricle and low volume in an echocardio-gram report, but would more likely stand for lumbarvertebrae in a radiology report). In fact, short form am-biguity can lead to unintended medical errors; therefore,many short form are banned from clinical documentusage by the Joint Commission, in Do not use List: Ab-breviations, Acronyms, and Symbols [43]. Short formscan occur with misspellings (myocrad infrac should bespelled myocard infarc) and can be concatenated withother words (70 year oldM = 70 year old M). We de-veloped an annotation schema and guidelines for humanannotators that addressed the annotation of such exam-ples from clinical text (described under MethodsAnnotation Schema).Text normalizationIn general, text normalization, which may include shortform (mention-level) boundary detection, word sensedisambiguation, and named entity and event recognition,can be an important processing step for some clinical in-formation extraction approaches. For example, in orderto extract a disease and disorder mention and link it toinformation to help a patients understanding of the un-familiar medical concept e.g., such as Abdominal tender-ness from ABD: Tender, a natural language processingsystem would need to 1) detect ABD as a short form, 2)disambiguate ABD as Abdomen not Aged, Blind andDisabled, 3) normalize ABD to a concept in a controlledvocabulary (e.g., C0562238: Examination of the Abdo-men), 4) post-coordinate ABD with the adjacent findingtenderness (e.g., C0234233: Tenderness) to define anevent (e.g. C0232498: Abdominal tenderness), and finally5) link it to a web-based information resource like MedlinePlus. For the purposes of our assessment, we have focusedon short form disambiguation (2) and normalization (3).Acronym and abbreviation detection and normalizationEarly and ongoing work on aspects of short form detectionand normalization focused on developing resources in thebiomedical literature domain, in particular, MEDLINE ab-stracts. A common and reasonable, baseline approach todetecting and normalizing biomedical short forms isexploiting short form  long form patterns [4446]. Thismethod is advantageous because most short forms demon-strate no or low ambiguity and can be mapped to the mostfrequent sense usage. Furthermore, few short forms dem-onstrate moderate to high ambiguity due to polysemousand/or homonymous usage.Researchers have also developed more sophisticated,high performing biomedical short form disambiguationmodules by training supervised models and evaluatingagainst MEDLINE corpora, e.g., Medstract Gold Stand-ard Evaluation corpus (support vector machines:98 % F1-measure [47], logistic regression: 81 % F1-measure [48]) and semi-supervised (AbbRE): 91 % F1-measure [49]). Further resources  databases andtools  for disambiguating biomedical short forms in-clude Acronym Resolving General Heuristic (ARGH),Stanford Biomedical Abbreviation Database, AcroMed,and Simple and Robust Abbreviation Dictionary (SaRAD)[48, 50]. However, few resources exist for short form rec-ognition from clinical texts.Indeed, a comparison study of state-of-the-art clinicaltext normalization tools suggests that clinical shortforms detection and normalization is still in its earlystages [51]. This study determines that clinical shortforms normalization tools generally demonstrate low tomoderate performance - clinical Text Analysis andKnowledge Extraction System (F1-measure: 21 %), Meta-Map (F1-measure: 35 %), and Medical Language Extrac-tion and Encoding System (F1-measure: 71 %) [51].Natural language processing systems can perform withlow normalization scores due to multiple senses for ashort form. The study suggests that the reason that theMedical Language Extraction and Encoding System out-performs MetaMap and clinical Text Analysis andKnowledge Extraction System for disambiguating am-biguous short form is due to its highly integrated clinicalsense inventories [51]. Natural language processing re-searchers have successfully produced sense inventoriesand automated disambiguation modules using rule-based and machine learning-based approaches [52, 53].For instance, a short form sense inventory was generatedusing regular expressions and morphological heuristicsfrom 352,267 clinical notes and the most frequent shortforms were manually mapped to three vocabularies Stedmans Medical Abbreviations, Acronyms & Symbols,the Unified Medical Language System, and AnotherDatabase of Abbreviations in Medline (ADAM) [52].Such sense inventories were developed using featuresgenerated from the Internet, Medline, and Mayo clinicalnotes to train decision tree and maximum entropy clas-sifiers for eight short forms [53]. Both decision tree(94 %) and maximum entropy (96 %) classifiers demon-strated more accurate short form classification than amajority sense baseline (71 %). Disambiguation modulesfocus on ambiguous word-senses of clinical short forms[54]. One disambiguation module uses a support vectormachine trained with 125 samples that achieved high ac-curacy (over 90 %) for the 50 most frequent short formswith varied senses from a dataset of 604,944 clinicalnotes. In addition to support vector machines, decisiontrees and naïve bayes classifiers are able to disambiguateshort forms with high accuracy (exceeding 90 %) usingpart-of-speech, unigram, and bigram features [55]. Semi-Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 3 of 13supervised (Specialist Lexicon Release of Abbreviationsand Acronyms (LRABR) with multi-class support vectormachine) and unsupervised (hierarchical clustering) ap-proaches have also demonstrated moderate to excellentdisambiguation performance [56]. Although rule-basedand machine learning-based approaches can disambigu-ate short forms with multiple senses from a subset ofdata, more work can be done addressing a larger subsetof short forms and report types. To enable further pro-gress in this area, we have developed a corpus annotatedwith clinical short forms linked to normalized values.With recent patient-centered initiatives, the focus ofthe 2013 ShARe/CLEF eHealth Challenge was to facilitatedevelopment of natural language processing applicationsthat could be used to help patients understand the contentof a clinical report, and Task 2 focused on normalizationof short forms. We describe the performance of the par-ticipating systems at automatically normalizing clinicalshort forms to the Unified Medical Language Systemcompared to a majority sense baseline, evaluate the per-formance of participating systems according to shortform terms with variable majority sense distributions,and assess each participating systems performance forconcepts shared between the ShARe test corpus and avocabulary containing simplified health terms. Thestudy extends the overview of all three tasks [32] andorganizers working notes on Task 2 [34, 57] by focus-ing on Task 2 in significantly greater depth, focusing on1) the difficulty of handling multiple short form senses,and 2) the utility of each participating system with re-spect to a vocabulary containing simplified health termsfor potentially supporting patient understanding ofclinical text.MethodsIn this section, we describe the short form schema, data-set, shared task, sense categorization, and short formcoverage using the Consumer Health Vocabulary.Annotation schemaWe developed our annotation schema and guidelinesusing a top-down and bottom-up methodology. We ap-plied top-down knowledge of text normalization bystarting with an annotation approach focusing on clin-ical short forms described in [51, 58]. We added rulesbased on guidelines from Task 1 developed for diseaseand disorder annotation and refined these rules throughfeedback provided by a panel of four natural languageprocessing experts (WWC, SP, NE, and GS) to developan initial schema and guidelines. Annotation by two bio-medical informatics students (DLM and BRS) on ten re-ports provided a bottom-up approach to validate theserules and clarify instructions through examples in theguidelines. For example, we applied a top-down rulederived from the Task 1 guidelines to exclude modifyinginformation like negation, history, and change in theconcept description (e.g., no eff ). After annotating tenreports, we refined this rule with a bottom-up approachto include anatomic locations, sidedness, and structureswithin the short form span boundaries (e.g., bilat pleureff ) based on the data. We included sections, diseasesand disorders, signs and symptoms, diagnoses, proce-dures, devices, gender, healthcare unit names. Weexcluded medications, lab results, measurement units,non-medical short forms, severities, and salutations. An-notators were also provided Task 1 disease and disorderannotations to help annotate short forms and interpretthe annotation rules. For instance, annotators were pro-vided the Task 1 annotation C0232498: Abdominal ten-derness for the finding ABD: Tender. and encouragedto use this knowledge to assign ABD as Abdomen ra-ther than Aged, Blind and Disabled. Similar to Task 1,annotators were instructed to assign the label CUI-lessto a short form span when no appropriate concept de-scription existed in the vocabulary. For Task 2, annota-tors mapped short form spans to the Unified MedicalLanguage System. The final schema contained inclusionand exclusion rules for 1) identifying the character spansof short form terms in the corpus (boundary detection)and 2) normalizing short forms to CUIs from the Uni-fied Medical Language System 2012 using an applica-tion program interface call within an annotation tool(extensible Human Oracle Suite of Tools - eHOST).The final guidelines can be viewed in detail on theShARe website [59].DatasetFor this IRB-approved study, we leveraged the ShARecorpus, a subset of de-identified discharge summary,electrocardiogram, echocardiogram, and radiology re-ports from about 30,000 ICU patients provided by theMultiparameter Intelligent Monitoring in Intensive Care(MIMIC) II database [60]. As part of ShARe/CLEFeHealth Challenge Task 1 [59], 298 clinical reports weresplit into training (n = 199 reports) and test (n = 99reports) sets and annotated for disease and disordermentions and their Systematized Nomenclature OfMEDicine Clinical Terms (SNOMED CT) codes by twoprofessional medical coders. We maintained these splitsand provided the Task 1 corpus to Task 2 annotators toannotate clinical short forms along with their normal-ized values. We achieved high inter-annotator agree-ment of 91 % for the test dataset between annotationsthat were reviewed and adjudicated by a biomedicalinformaticist and a respiratory therapist. We furthercharacterize the corpus development and inter-annotatoragreement in [3234].Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 4 of 13ShARe/CLEF eHealth challenge shared task 2 - participatingteamsThe annotated ShARe corpus was released as part of the2013 ShARe/CLEF eHealth Evaluation Challenge [61].Two training sets were provided containing short formspans and CUIs. Participants were instructed to developa natural language processing normalization system topredict the CUI for each provided short form span in thetest dataset. In summary, five teams  UTHealthCCB[62], LIMSI [63], TeamHealthLanguageLABS [64], THCIB[65], and WVU [66] - submitted systems for Task 2. Fourteams approached this task using machine learning-basedmethods: three teams built conditional random field clas-sifiers [63, 64, 66] and one team used support vector ma-chines [62]. The teams used a variety of features includinglexical, morphological, and structural features from theUnified Medical Language System, Systematized Nomen-clature Of MEDicine Clinical Terms, clinical Text Ana-lysis and Knowledge Extraction System, and gazetteers.One team built a rule-based system combining clinicalText Analysis and Knowledge Extraction System and rulesdeveloped from the training data [65]. Specifically, the fiveparticipating teams developed the following short formnormalization solutions:? UTHealthCCB [62] applied one of four differentsense tagging methods based on short formcharacteristics of frequency (high or low) andambiguity (present or not): 1) a trained support vectormachine mapped high frequency and ambiguous shortforms, 2) a majority sense method mapped highfrequency and unambiguous short forms, 3) a vectorspace model mapped all low frequency short forms,and 4) a Unified Medical Language SystemTerminology Services Application ProgrammingInterface mapped any unseen short forms.? LIMSI [63] applied clinical Text Analysis andKnowledge Extraction System and MetaMap to extractfeatures - lexical and morphological (unigrams, shortform terms, and token characteristics), syntactic(unigrams and bigrams part of speech), document(report and section types), semantic (MetaMapsemantic type and CUI), and Wikipedia (semanticcategory) features. Many features included a contextwindow of 1-3 tokens. These features were used totrain a linear-chain conditional random field classifierusing Wapiti.? TeamHealthLanguageLABS [64] trained a linear-chain conditional random field classifier using context(bigram window), lexical (Lexicon Management Systemterms), grammatical (lemma, part of speech andchunk), ring-fence (complex and compound shortforms) and Systematized Nomenclature Of MEDicineClinical Terms (terms, concept id and category)features to identify short forms. A sequence of gazet-teers applied the optimal CUI mapping based on pos-sible expansions, usage frequency, and token contexts.? THCIB [65] developed a rule-based system combin-ing clinical Text Analysis and Knowledge ExtractionSystem with custom short form and full name diction-aries developed from the training set as well as theSTANDS4 online medical dictionary.? WVU [66] trained a linear-chain conditional randomfield algorithm from the Factorie toolkit using a dic-tionary of short forms generated from the training data,Unified Medical Language System data sets, and gen-eral websites.System evaluation metricsWe compared each participating system predictionsagainst the short form annotations in the test set usingaccuracy defined as the count of correct short formsdivided by the total count of the reference standardshort form annotations [67]. A system short form wascorrect if the assigned CUI matched the reference stand-ard CUI. Participating teams were allowed to submit twosystems each.Majority sense baselineFrom the training data, we developed a majority sensebaseline classifier, as this approach has been successfulin other biomedical short form studies [4446]. Basedon the training data annotations, for which each shortform annotation contains the short form span offset,term, and Systematized Nomenclature Of MEDicineClinical Terms CUI, we generated a majority sense dic-tionary using frequency counts for each CUI associatedwith a unique short form term (converted to lower-case). The dictionary was structured as a list sorted firstby CUI frequency and the most frequent CUI value wasselected. For example, the short form term ca contains2 unique CUI labels representing C0006826: MalignantNeoplasms: 5 or C0443758: Carbohydrate antigen: 1. Ifwe observed ca as the short form term in the test set,we selected the most frequent CUI value for caC0006826 as the normalization value (based on the fre-quency of the training set annotations); otherwise theshort form term was assigned CUI-less. If the CUI wereequally probable, we randomly selected the CUI to beused in the sense dictionary. For example, for the shortform term lle we randomly selected C0239340 fromthe following CUI list: [C0230416: Left lower extremity:1, C0239340: Edema of lower limbs: 1]. We comparedthe majority sense baseline and participant system accur-acy scores for statistical significant differences using ran-dom shuffling [68].Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 5 of 13Sense prediction evaluationWe report the proportion of annotations from the testset for which a short form term has one unique senseversus two or more senses (CUI normalization values orCUI-less). Applying a discretization method [55], we re-port the majority sense distributions annotated for eachof the top ten most frequent short form terms contain-ing two or more senses and variable distributions acrossvalue sets. Furthermore, we assessed each participatingteams system performance according to the sense dis-tribution categories below, which were defined tocharacterize the ambiguity of short form terms in thetest dataset [55]:? no ambiguity: short form terms with 1 unique sense? low ambiguity: short form terms with > = 2 senses,majority sense >80 %? moderate ambiguity: short form terms with > =2senses, majority sense 5080 %? high ambiguity: short form terms with > = 2 senses,majority sense <50 %Consumer health vocabulary coverageWe evaluated the coverage of short form concepts andannotations from the ShARe corpus against a vocabularyof simplified, consumer-friendly terms, the ConsumerHealth Vocabulary [69] developed by Zeng and col-leagues [66]. The Consumer Health Vocabulary provideslay terms for clinical concepts and contains a mappingto Unified Medical Language System preferred terms foreach Consumer Health Vocabulary term. We queriedeach Unified Medical Language System CUI against theConsumer Health Vocabulary concept terms flat filefrom the Consumer Health Vocabulary website [70] todetermine how frequently the preferred term was thesame both in the Unified Medical Language System andthe Consumer Health Vocabulary, and how frequentlythey differed.For the test set, we report the prevalence of uniqueshort form CUIs in the ShARe corpus and ConsumerHealth Vocabulary. We report the proportion of theConsumer Health Vocabulary concepts that provide adifferent preferred name than the preferred name in theUnified Medical Language System as mapped in theConsumer Health Vocabulary resource. For example, thepatient-friendly term CT scan may be preferred over theclinical-friendly preferred term X-Ray Computed Tom-ography. From the test set, we also evaluated the cover-age of short forms found in each vocabulary using recall,with true positives (TP) defined as a ShARe short formoccurring in the vocabulary and false negatives (FN) de-fined as a ShARe short form not occurring in the vo-cabulary. Of annotations represented by CUIs shared byboth the test set and the Consumer Health Vocabulary,we evaluated how well each participants system com-pleted the normalization task using accuracy, with a TPdefined as an short form correctly normalized to ashared ShARe/Consumer Health Vocabulary CUI and aFN defined as an short form missed or incorrectlynormalized to a shared ShARe/Consumer HealthVocabulary CUI.ResultsWe characterized the ShARe short form corpus, assessedparticipants systems, reported majority sense distribu-tions for the most prevalent terms, assessed participantssystems for each majority sense distribution category,evaluated the coverage of short form concepts againstthe Consumer Health Vocabulary, and evaluated howwell each participants system could normalize shortforms with shared CUIs between the test set and theConsumer Health Vocabulary.Test corpusOn the test set of 99 clinical texts, we observed 3774short form annotations, 603 unique terms, and 707unique normalization values (CUIs and CUI-less). Sixpercent (221/3774) of short form annotations wereassigned CUI-less.ShARe/CLEF eHealth challenge shared task 2 - systemperformancesResults for the participating systems and the majoritysense baseline for normalizing short forms in the test setare summarized in Table 1. Although there were only3774 observations in the test set, a total of 4892 uniqueannotations were submitted among participating teams.As a result of creating end-to-end systems (i.e. also pre-dicting short form spans), several teams were missing an-notations  from 163 (LIMSI.1) to 1415 (TeamWVU.1).UTHealthCCB had the highest accuracy (71.9). We com-pared the performance of the majority sense baselineagainst the performance of the top-performing system,UTHealthCCB.B.1. The majority sense baseline achievedan accuracy of 69.6. about 3 percentage points lower thanthe UTHealthCCB.B.1 system. However, the majoritysense baseline outperformed the second ranked systemfrom the same team, UTHealthCCB.B.2.Sense prediction evaluationWe observed that 603 unique terms from a total of 2095(55 %) short form annotations in the test data have noambiguity (1 unique sense); 135 unique terms from 1679(45 %) annotations have two or more normalizationvalues (CUI or CUI-less). Of the short forms with twoor more normalization values, 47 unique terms, from971 (26 %) annotations, have low ambiguity (equal orgreater than 80 % majority sense); 80 unique terms, fromMowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 6 of 13641 (17 %) annotations, have moderate ambiguity (50 to80 % majority sense); and 8 unique terms from 67 (2 %)annotations, have high ambiguity (less than 50 % major-ity sense). In Table 2, we enumerate the top ten mostfrequent short form terms and their majority sensedistributions for cases when two or more senses areobserved according to ambiguity classes.In terms of overall system performance,UTHealthCCB.B.1 achieved the highest accuracy acrossall sense categories (Fig. 1). The performance of partici-pating systems for normalizing short form terms with noambiguity compared to low ambiguity short form termsranged from a slight increase in accuracy of 1.17 to 1.19points. The performance of participating systems fornormalizing low ambiguity short form terms comparedto high ambiguity short form terms ranged from a de-crease in accuracy of 31.4 to 55.9 points.Consumer health vocabulary coverageThe Consumer Health Vocabulary consists of 158,519terms and 57,819 unique CUIs. The ShARe/CLEF shortform test set consists of 860 unique terms and 706unique Unified Medical Language System CUIs. We ob-served 66 % (466/707) of unique CUIs from the ShARetest set in the Consumer Health Vocabulary. Of theshared CUIs, 54 % (250/466) had a Consumer HealthVocabulary preferred term. For instance, C0027051:Myocardial Infarction occurs with a Consumer HealthVocabulary preferred name heart attack. We determinedthat 52 % (129/250) of the shared CUIs have a Con-sumer Health Vocabulary preferred term (patient-friendly name) that differed from the Unified MedicalLanguage System preferred term (clinically-friendlyname). For instance, C0013516: Echocardiography has aConsumer Health Vocabulary preferred term of heartultrasound and Unified Medical Language System pre-ferred name of echocardiography. Two thousand fivehundred ninety four of the 3774 (69 %) annotations con-tained common CUIs between the Consumer Health Vo-cabulary and the ShARe test set. For these possibleannotations, UTHealthCCB had the highest accuracy(75.0), followed by the majority sense baseline (73.2),and THCIB.B.1 (73.1) (Table 3).DiscussionWe characterized the ShARe short form corpus, assessedparticipants systems, reported majority sense distribu-tions for the most prevalent terms, assessed participantssystems for each majority sense distribution category,evaluated the coverage of short form concepts againstthe Consumer Health Vocabulary, and assessed how welleach participants system could normalize short formswith shared CUIs between the ShARe test set and theConsumer Health Vocabulary.Test corpusWe estimated that around 81 % of the short form anno-tations represent terms with none or low ambiguity (ei-ther one unique sense or two senses with a majoritysense over 80 %); in contrast, about 19 % of the shortform annotations are moderately to highly ambiguous(two senses with a majority sense between 50 and 80 %,or two senses with a majority sense less than 50 %). Forexample, the term trach had two senses with a majoritysense less than 80 %, requiring word sense disambigu-ation. For instance, in now s/p trach, trach representsa Therapeutic or Preventative Procedure - C0040590:Tracheostomy Procedure. In Assess for trach place-ment, trach represents a Medical Device - C0184159:Tracheostomy Tube. In the case of these polysemous(different, but related) senses, predicting C0040590:Tracheostomy Procedure instead of C0184159: Trache-ostomy Tube may not necessarily result in a misunder-standing of the text by a patient due to level of sharedconcept similarity. In the case of the following hom-onymous (different and unrelated) sense example, PTcan represent C0949766: Physical therapy or C0030705:Patients. In such a case, it would be more important fora system to accurately select the correct sense for patientunderstanding of clinical text due to the lack of conceptsimilarity.Table 1 aParticipant system performances from [32, 34] compared against a majority sense baseline performanceShort form normalization system Unique predictions by the system Annotations comparable with reference standard AccuracyaUTHealthCCB.B.1 3,774 3,774 71.9*Majority Sense Baseline 3,774 3,774 69.6aUTHealthCCB.B.2 3,774 3,774 68.3aLIMSI.1 3,896 3,611 66.4aTHCIB.B.1 3,774 3,774 65.7*aTeamHealthLanguageLABS 2,987 2,633 46.7*aWVU.1 3,068 2,359 42.6*Indicates that the difference in accuracy is statistically significant with the system immediately below (p < 0.01)Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 7 of 13Table 2 Top 10 most frequent lexical variants with two or more senses according to distribution typeShort form term Total count Senses according to concept unique identifiers Distribution of sensesLow ambiguitypt 137 C0030705: Patients 89 %C0949766: Physical therapy procedure 4 %C0086835: Structure of the posterior tibial artery 4 %3 more senses 8 %ct 82 C0040405: X-Ray computed tomography 95 %C1274037: Cardiothoracic surgery 2 %C0008034: Thoracic drain 2 %1 more sense 1 %m 62 C0024554: Male gender 81 %C0018808: Heart murmur 16 %C0026591: Mother 2 %1 more sense 2 %ekg 41 C0013798: Electrocardiogram 98 %C1623258: Electrocardiographic procedure 2 %f 37 C0015780: Female 92 %C0015967: Fever 5 %CUI-less 3 %cath 33 C0007430: Catheterization 97 %C0085590: Catheter 3 %lad 33 C0226032: Anterior descending branch of left coronary artery 85 %C0497156: Lymphadenopathy 15 %pcp 31 C0033131: Primary care physicians 84 %C0032305: Pneumonia, Pneumocystis carinii 16 %cad 31 C1956346: Coronary artery disease 97 %C0010068: Coronary heart disease 3 %abd 29 C0562238: Examination of abdomen 90 %C0000726: Abdominal 10 %Moderate ambiguitybp 53 C1271104: Blood pressure finding 68 %C0005823: Blood pressure 32 %r 43 C0205090: Right 58 %C0232267: Pericardial rub 23 %C0035508: Rhonchi 11 %2 more senses 9 %hr 40 C0577802: Finding of heart rate 68 %C0018810: Heart rate 33 %neuro 34 C0027853: Neurologic examination 79 %C0205494: Neurologic (qualifier value) 6 %C0221571: Nervous system problem 6 %3 more senses 9 %pod 28 CUI-less 79 %C0032790: Postoperative period 21 %ra 26 C2709070: On room air 62 %Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 8 of 13ShARe/CLEF eHealth challenge shared task 2  systemperformancesWe evaluated participants system performance for nor-malizing acronyms/abbreviations to Unified MedicalLanguage System CUIs on the test set (Table 1).Compared to the majority sense baseline results, onlythe highest performing system by UTHealthCCB.1showed improvement. Our majority sense baseline ap-proach results (~70 % accuracy) are comparative to pre-viously reported clinical majority sense baseline resultsTable 2 Top 10 most frequent lexical variants with two or more senses according to distribution type (Continued)C0225844: Right sided atrium 35 %C0456165: Right atrial pressure 4 %bs 26 C0232693: Bowel sounds 77 %C0035234: Respiratory Sounds 23 %pa 19 C1996865: Postero-anterior 53 %C0034052: Pulmonary artery structure 37 %C0428642: Pulmonary artery pressure 11 %rrr 18 C0232185: Cardiac rhythm AND/OR rate finding 67 %C0232188: Normal heart right 28 %C0513693: Monitor rate, rhythm, depth, and effort of respirations 6 %mr 18 C0026266: Mitral valve insufficiency 78 %C0024485: Magnetic resonance imaging 22 %High ambiguityc 25 C0010520: Cyanosis of skin 32 %C0149651: Clubbing 32 %C0205064: Cervical 24 %2 more senses 12 %trach 9 C0040590: Tracheostomy procedure 33 %C0184159: Tracheostomy tube 33 %C0040591: Tracheotomy procedure 11 %2 more senses 22 %meds 9 C0013227: Pharmaceutical preparations 44 %C0025118: Medicine 33 %C0033081: Drug prescriptions 22 %cont 8 C0549178: Continuous 38 %CUI-less 38 %C0584669: Recommendation to continue with treatment 13 %1 more sense 13 %v 6 C0042963: Vomiting 33 %C0348013: Venous 33 %C2228490: Examination of trigeminal nerve 33 %d/c 3 C0030685: Patient discharge 33 %C1444662: Discontinued 33 %C1548175: On discharge 33 %pos 3 C0205531: Oral route 33 %C0518037: Oral food intake 33 %C1446409: Positive 33 %cvp 3 C0199666: Measurement of central venous pressure 33 %C0428640: Central venous pressure 33 %C1321771: Central venous pressure finding 33 %Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 9 of 13(71 % accuracy) [53]. On the training set, THCIB reports20 % of the short forms from a sentence input could notbe mapped to CUIs using clinical Text Analysis andKnowledge Extraction System. We believe this demon-strates that out-of-the-box text normalization systemswill perform moderately for normalizing short forms.Many participants incorporated clinical Text Analysisand Knowledge Extraction System pre-processing, con-ditional random field, and custom dictionaries fromtraining data and online resources to develop theirsystems.Systems with post-processing, sense disambiguation,and machine learning trained with natural language pro-cessing features can outperform a baseline short formnormalization system. The system by UTHealthCCBused a hybrid approach incorporating rule-based andmachine learning techniques and achieved an accuracyof 72 % which suggests short form normalization con-tinues to be a challenging natural language processingresearch problem. Some teams developed an end-to-endsystem including short form boundary detection andnormalization. This reason accounts for some variationin the number of predictions by participating systems.Sense prediction evaluationWe observed that of most short form terms with no orlow ambiguity, over 80 % could be normalized with rea-sonable accuracies by participants systems. In contrast,short form terms with moderate or high ambiguity couldbe normalized with low to modest accuracy by partici-pants systems (Fig. 1). This trend was consistent for allparticipating systems and approaches. This finding is notsurprising, as we would expect some reduction in per-formance due to ambiguity.Consumer health vocabulary coverageOf the 3774 ShARe/CLEF short form annotations, weobserved most (94 %) short form annotations map to aCUI in the Unified Medical Language System i.e., onlyabout 6 % of short form annotations were CUI-less,demonstrating excellent short form coverage. Over half(66 %) of the unique Unified Medical Language SystemCUIs in the test corpus also occurred in the ConsumerHealth Vocabulary implying that a substantial portion ofshort form concepts (34 %) could be considered foraddition to the Consumer Health Vocabulary. About52 % of the shared CUIs had a Consumer Health Vo-cabulary patient-friendly preferred name that differedfrom the Unified Medical Language System. In thesecases, a patient-friendly alternative may be offered to apatient to improve understanding of clinical text. In con-trast, some shared CUIs (48 %) have a Consumer HealthVocabulary preferred term that matched the UnifiedMedical Language System preferred term. In these casesa patient-friendly alternative may not be necessary. InFig. 1 Accuracies of participating systems and Majority Sense Baseline for each majority sense distribution categoryTable 3 Accuracy of normalizing short forms with conceptunique identifiers shared between the ShARe test set and theConsumer Health VocabularyShort form normalization system AccuracyUTHealthCCB.B.1 75.0Majority Sense Baseline 73.2THCIB.B.1 73.1UTHealthCCB.B.2 70.4LIMSI.1 69.6TeamHealthLanguageLABS 50.9WVU.1 50.1Mowery et al. Journal of Biomedical Semantics  (2016) 7:43 Page 10 of 13future work, we plan to identify patient-friendly pre-ferred terms for short form CUIs from the test corpusthat did not occur in the Consumer Health Vocabularyand propose them for inclusion.In terms of normalizing short forms with commonCUIs between the Consumer Health Vocabulary and theShARe test set, participant systems demonstrated mod-erate to reasonable accuracy suggesting promising re-sults for supporting patient understanding of clinicaltext by replacing these concepts with a more lay term orlinking these terms to web resources.ConclusionWe completed the 2013 ShARe/CLEF eHealth Challengewith the focus on creating resources that could be lever-aged to develop technologies to aid patients understand-ing of his or her electronic medical record. For Task 2,we developed a reference standard for short formnormalization with high inter-annotator agreement, add-ing an additional meta-data layer to the openly availableShARe corpus [48]. The natural language processingcommunity demonstrated that a short form normalizercould be created with reasonably high accuracy; how-ever, more work needs to be done to resolve short formswith moderate to high ambiguity. We demonstrated thatmore concepts could be added to the Consumer HealthVocabulary to support patient understanding of shortforms used in clinical reports.AbbreviationsADAM, Another Database of Abbreviations in Medline; ARGH, AcronymResolving General Heuristic; CLEF, Conference & Labs of the EvaluationForum; cTAKES, clinical Text Analysis and Knowledge Extraction System; CUI,Concept Unique Identifier; eHOST, extensible Human Oracle Suite of Tools;GENIA, Genome Information Acquisition; i2b2, Informatics for IntegratingBiology and the Bedside; MedLEE, Medical Language Extraction andEncoding System; MIMIC, Multiparameter Intelligent Monitoring in IntensiveCare II database; SNOMED CT, Systematized Nomenclature Of MEDicineClinical Terms; MUC, Message Understanding Conference; SaRAD, Simple andRobust Abbreviation Dictionary; ShARe, Shared Annotated Resources; TREC,Text REtrieval ConferenceAcknowledgmentsWe extend our gratitude to our funding sources, natural languageprocessing experts, and annotators for their invaluable contributions. Wethank Ken Pierce for working with us to complete task 2 data requests fromPhysionet.org and Qing Zeng for making the Consumer Health Vocabularyavailable to the community. We appreciate the useful feedback andsuggestions from our anonymous reviewers. This work was partially fundedby NICTA, which was supported by the Australian Government through theDepartment of Communications and the Australian Research Councilthrough the ICT Center of Excellence Program, the CLEF Initiative, EuropeanScience Foundation (ESF) project ELIAS, Khresmoi project, funded by theEuropean Union Seventh Framework Programme (FP7/2007-2013) undergrant agreement no 257528, ShARe project funded by the US NationalInstitutes of Health (R01GM090187), US Department of Veterans Affairs (VA)Consortium for Healthcare Informatics Research (CHIR), US Office of theNational Coordinator of Healthcare Technology, Strategic Health IT AdvancedResearch Projects (SHARP) 90TR0002, Vårdal Foundation (Sweden), Academyof Finland (140323), and National Library of Medicine 5T15LM007059.Authors contributionsWWC, BRS, DLM, SV, HS, NE, SP, and GS defined the task, DLM, WWC, BRS ledthe overall task, BRS, DLM, L-MP, and SS led the annotation effort, HS, SV,and SS co-chaired the lab, BRS and JL developed the annotation infrastruc-ture, DLM, BRS, LC, and DM processed and distributed the corpus, and DLM,DM and WWC led result evaluations. DLM, SV, BRS, L-MP, and WWC wrotethe manuscript then all coauthors reviewed and significantly extended thefinal version. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Availability of supporting dataFollowing submission of human subjects training and a data use agreement,the corpus and annotations can be downloaded from Physionet.org. Thedata access protocol can be found on the 2013 ShARe/CLEF eHealthChallenge website: https://sites.google.com/site/shareclefehealth/data underObtaining Datasets (Tasks 1 and 2).Author details1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USA. 2Nursing Science, University of Turku, and Turku University Hospital,Turku, Finland. 3Data61, CSIRO, The Australian National University, Universityof Canberra, and University of Turku, Locked Bag 8001, Canberra 2601, ACT,Australia. 4MedWhat.com, San Francisco, CA, USA. 5University of Melbourne,Parkville, VIC, Australia. 6Department of Computer and Systems Sciences(DSV), Stockholm University, Stockholm, Sweden. 7Department of BiomedicalInformatics, Columbia University, New York, NY, USA. 8Boston ChildrensHospital, Harvard Medical School, Boston, MA, USA.Received: 28 August 2014 Accepted: 1 June 2016RESEARCH Open AccessExtending gene ontology in the context ofextracellular RNA and vesiclecommunicationKei-Hoi Cheung1,2,21*, Shivakumar Keerthikumar3,21, Paola Roncaglia4,22, Sai Lakshmi Subramanian5,21,Matthew E. Roth5,21, Monisha Samuel3, Sushma Anand3, Lahiru Gangoda3, Stephen Gould6,21,23,Roger Alexander7,21, David Galas7,21, Mark B. Gerstein8,9,10,21, Andrew F. Hill3,24, Robert R. Kitchen8,21, Jan Lötvall11,24,Tushar Patel12,21, Dena C. Procaccini13,21, Peter Quesenberry14,21,24, Joel Rozowsky8,10,21, Robert L. Raffai15,21,Aleksandra Shypitsyna4,22, Andrew I. Su16,21, Clotilde Théry17,24, Kasey Vickers18,21, Marca H.M. Wauben19,24,Suresh Mathivanan3,21,24, Aleksandar Milosavljevic5,21 and Louise C. Laurent20,21*AbstractBackground: To address the lack of standard terminology to describe extracellular RNA (exRNA) data/metadata, we havelaunched an inter-community effort to extend the Gene Ontology (GO) with subcellular structure concepts relevant tothe exRNA domain. By extending GO in this manner, the exRNA data/metadata will be more easily annotated andqueried because it will be based on a shared set of terms and relationships relevant to extracellular research.Methods: By following a consensus-building process, we have worked with several academic societies/consortia,including ERCC, ISEV, and ASEMV, to identify and approve a set of exRNA and extracellular vesicle-related terms andrelationships that have been incorporated into GO. In addition, we have initiated an ongoing process of extractions ofgene product annotations associated with these terms from Vesiclepedia and ExoCarta, conversion of the extractedannotations to Gene Association File (GAF) format for batch submission to GO, and curation of the submitted annotationsby the GO Consortium. As a use case, we have incorporated some of the GO terms into annotations of samples from theexRNA Atlas and implemented a faceted search interface based on such annotations.Results: We have added 7 new terms and modified 9 existing terms (along with their synonyms and relationships) toGO. Additionally, 18,695 unique coding gene products (mRNAs and proteins) and 963 unique non-coding gene products(ncRNAs) which are associated with the terms: extracellular vesicle, extracellular exosome, apoptotic body, andmicrovesicle were extracted from ExoCarta and Vesiclepedia. These annotations are currently being processed forsubmission to GO.Conclusions: As an inter-community effort, we have made a substantial update to GO in the exRNA context. We havealso demonstrated the utility of some of the new GO terms for sample annotation and metadata search.Keywords: Ontology, Extracellular RNA, Extracellular vesicle, Metadata, Faceted search, Atlas* Correspondence: kei.cheung@yale.edu; `llaurent@ucsd.edu1Department of Emergency Medicine, Yale Center for Medical Informatics,Yale University School of Medicine, New Haven, CT, USA20Department of Reproductive Medicine, University of California, San Diego,La Jolla, CA, USAFull list of author information is available at the end of the article© 2016 Cheung et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Cheung et al. Journal of Biomedical Semantics  (2016) 7:19 DOI 10.1186/s13326-016-0061-5BackgroundExtracellular RNAs (exRNAs) are broadly defined asRNAs that are present in the acellular portions of bio-fluids, such as body fluids (blood, cerebrospinal fluid,bile, lymph, vitreous humour, amniotic fluid, ascites,pleural, pericardial and peritoneal fluids, etc.), secretions(saliva, urine, sweat, tears, milk, seminal fluid, etc.), andcell and tissue culture supernatants. RNA sequencinganalyses of exRNAs demonstrate that they represent al-most the entire range of cellular RNA species, includingrRNAs, tRNAs, mRNAs, miRNAs, piRNAs, lncRNAs,and circular RNAs ([15]). However, the profiles of cel-lular and exRNAs are not identical, as some cellularRNAs appear to be highly enriched in the exRNA frac-tion, while others appear to be significantly underrepre-sented, and still others lie between these extremes ofenrichment or exclusion.Among the many reasons cells might release exRNAs,perhaps the most intriguing possibility is that exRNAsmight contribute to intercellular communication. Exportof exRNAs may also be used to eliminate undesiredRNAs from the originating cell. Finally, some exRNAsmight be generated in a nonspecific manner, either byliving cells (e.g. by a bulk flow process) or as a conse-quence of cell death (reviewed in [6]).ExRNAs appear to be universally associated with carriervehicles, likely due to the rapid degradation of unpro-tected RNAs in biofluids ([4, 710]). The biochemicalproperties of these carriers are likely to be the primary de-terminant of the types and specific identities of exRNAsthat are secreted from cells, as well as their stability in theextracellular milieu. The first exRNA carriers identifiedwere RNA viruses, which carry not only viral RNAs, butalso varying levels of host-encoded RNAs. For example,retroviruses typically carry two host tRNAs and sub-stoichiometric levels of host mRNAs from the cell inaddition to two copies of the viral RNA genome and keyviral proteins ([1116]). In fact, retrovirally infected cellsproduce more empty virus-like particles (VLPs) than in-fectious virions. These VLPs have failed to encapsulate theviral RNA genome, but can carry as much as 10 kb ofhost-encoded RNA ([17]). More recently, it has been dis-covered that virally uninfected cells also release RNAs intothe extracellular space, and that these exRNAs are associ-ated with extracellular vesicles (EVs), lipoproteins (LPPs,most commonly HDLs ([10, 18]), LDLs ([18, 19])), andribonucleoprotein particles (RNPs, most commonlyAgo2-containing RNPs ([9, 20]). While the biogenicmechanisms underlying the release of exRNA-containingEVs, LPPs, and RNPs are still being investigated, it is clearthat they are not generated by a mechanism that is com-mon to all of them.Evidence for EVs and exRNA was first provided morethan seventy-five years ago by Albert Claudes observationthat uninfected chick and mammalian cells release RNA-containing vesicles ([21]). However, these non-viral EVsremained largely uninvestigated for decades. EVs re-entered the literature in the late 1960s with descriptionsof calcifying matrix vesicles released by chondrocytes([22]) and vesicular dust released by platelets ([23]).These and other such vesicles are now commonly referredto as exosomes, a term coined by Trams et al. in 1981([24]) to refer to secreted vesicles that may serve aphysiologic function, including both small vesicles of~100 nm in diameter, and larger vesicles of ~600 nmdiameter or greater.This first definition of the term exosome has beensubsequently overlooked at least twice, first in 1987 byinvestigators studying the vesicular secretion of thetransferrin receptor, who adopted a more restrictive def-inition of the term, conflating it with a delayed mode ofvesicle secretion in which the vesicles bud at endosomemembranes to create a multivesicular body (MVB),followed later by MVB fusion with the plasma mem-brane to release the vesicles into the extracellular space([25]). In 1997, investigators studying an RNA-degradingprotein complex adopted the term exosome, this timefor an entirely unrelated intracellular biochemical entity([26]). Not surprisingly, other investigators have come todifferent conclusions about which definition holds scien-tific precedent, resulting in variable use of the term exo-some in different laboratories. EV-related nomenclaturesare further complicated by the common use of additionalterms for secreted vesicles that are variably associated withdifferent biophysical properties or biogenesis pathways, aswells as terms based on the cell type or tissue of origin.The former include ectosomes (which refer to EVs thatare produced by budding from the plasma membrane([27, 28]) and microvesicles (which are frequently oper-ationally defined as EVs that pellet at moderate centrifuga-tion speeds (~10,00020,000 xg) [29]), while the latterinclude prostasomes ([30]), epididymosomes ([31]),immunosomes ([32, 33]), oncosomes ([34]), and plateletdust ([23]). There are even some vesicle names that referto observed biological activities (e.g. tolerosomes ([35])and calcifying matrix vesicles ([22]).The International Society for Extracellular Vesicles(ISEV) has previously attempted to clarify the nomencla-ture in this field. Its primary achievements have been to(1) introduce the term extracellular vesicle (EV) as ageneral term intended to encompass all secreted vesicles,and encourage its broad acceptance, and (2) encouragethe use of broad-definition terms until a more compre-hensive understanding of the biogenesis and molecularcompositions of different types of vesicles is developed.Given the inconsistent vesicle nomenclatures prior tothese ISEV efforts, these were major advances. However,some inconsistencies still persist. For example, someCheung et al. Journal of Biomedical Semantics  (2016) 7:19 Page 2 of 9investigators use the term microvesicle for larger EVs(>200 nm diameter) and exosome for smaller EVs(~30200 nm diameter), while other investigators rejectthese size-based definitions and adopt a set of biogenicdefinitions in which microvesicle describes vesicles thatbud from the plasma membrane while exosome de-scribes vesicles that bud into endosomes and are se-creted only later upon MVB fusion with the plasmamembrane.Therefore, investigators are currently forced to make achoice between these competing definitions, the first be-ing practical but mechanistically barren, while the latterbeing impractical but mechanistically appealing. Unfor-tunately, there is as yet no unambiguous way to distin-guish between vesicles that bud from the plasmamembrane versus those that bud at the endosome mem-brane based on either biophysical properties or molecu-lar content. This lack of standard nomenclatures createsi) a problem for individual researchers to annotate/sharetheir data in an unambiguous manner and ii) a barrier toproductive interactions with the broader research com-munity, most prominently the biomedical, genomics,and computational biology communities. To addresssuch issues, we initiated our metadata standard effortswithin the Metadata Working Group (MWG) of theExtracellular RNA Communication Consortium (ERCC)funded by the National Institutes of Health (NIH). Aspart of these efforts, MWG matched metadata terms toexisting biomedical ontologies and identified theexRNA-relevant terms that were absent from major on-tologies such as the Gene Ontology (GO).Use of ontologies in metadata annotationAs described in [36], the MWG of the ERCC has devel-oped the data and metadata standards for annotatingexRNA profiling data for submission to the Data Man-agement and Resource Repository (DMRR) of the ERCC.Particularly, a process has been established to submit ex-periment data to DMRR along with metadata in stand-ard machine-readable formats (using Linked Datatechnologies). The standards cover metadata about do-nors, biosamples, experiments, studies, and analysissteps. Such metadata enable targeted selection of sam-ples of interest (e.g. specific health condition of thedonor, biofluid or cell/tissue type, library preparationmethod, and sequencing assay) for integrative analyses.The metadata also helps organize the data for efficientinteractive as well as programmatic access (e.g. RESTApplication Programming Interfaces (APIs)).The MWG identified existing biomedical ontologiesaccessible through the NCBO BioPortal [37] as a sourceof commonly accepted terms for annotating exRNAdatasets. Such ontology-based metadata annotation doesnot only allow semantic retrieval/query of data in ERCCdata sources, it also allows DMRR data to be integratedwith other data sources with metadata annotated usingthe same ontologies. For ontologies to be useful for bio-logical applications, it is critical that the relevant ontol-ogies contain meaningful and broadly accepted terms, aswell as ensuring that the relationships between the in-cluded terms be both accurate and accepted by the per-tinent scientific community. For an ontology such asGO which is frequently used for functional enrichmentanalysis of genomic datasets, it is also important thatterms be associated with specific gene products (codingand non-coding RNAs and proteins) in an empiricallysupported manner.The Gene Ontology (GO) Consortium (GOC; http://www.geneontology.org) is a community-based bioinfor-matics effort. This Consortium develops, maintains andextends two interconnected resources: the Gene Ontol-ogy itself, and a database of GO annotations that associ-ate specific gene products with concepts in the GeneOntology. As of March 2016, there are >42,000 GOterms for describing concepts relevant to gene productfunction in a species-independent manner, providing notonly comprehensive coverage of biological concepts butalso community-wide agreement on how those shouldbe used to describe gene functions across all organisms.The GO is organized into three aspects [38]: these aregraph structures comprised of classes for molecularfunctions, the biological processes these contribute to,the cellular locations where these occur (cellular compo-nents), and the relationships connecting these classes. AGO annotation describes the association between aAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 DOI 10.1186/s13326-016-0093-xRESEARCH Open AccessExpansion of medical vocabularies usingdistributional semantics on Japanese patientblogsMagnus Ahltorp1, Maria Skeppstedt2*, Shiho Kitajima3, Aron Henriksson4, Rafal Rzepka3 and Kenji Araki3AbstractBackground: Research on medical vocabulary expansion from large corpora has primarily been conducted usingtext written in English or similar languages, due to a limited availability of large biomedical corpora in most languages.Medical vocabularies are, however, essential also for text mining from corpora written in other languages than Englishand belonging to a variety of medical genres. The aim of this study was therefore to evaluate medical vocabularyexpansion using a corpus very different from those previously used, in terms of grammar and orthographics, as well asin terms of text genre. This was carried out by applying a method based on distributional semantics to the task ofextracting medical vocabulary terms from a large corpus of Japanese patient blogs.Methods: Distributional properties of terms were modelled with random indexing, followed by agglomerativehierarchical clustering of 3×100 seed terms from existing vocabularies, belonging to three semantic categories:Medical Finding, Pharmaceutical Drug and Body Part. By automatically extracting unknown terms close to thecentroids of the created clusters, candidates for new terms to include in the vocabulary were suggested. The methodwas evaluated for its ability to retrieve the remaining n terms in existing medical vocabularies.Results: Removing case particles and using a context window size of 1 + 1 was a successful strategy for MedicalFinding and Pharmaceutical Drug, while retaining case particles and using a window size of 8 + 8 was better for BodyPart. For a 10n long candidate list, the use of different cluster sizes affected the result for Pharmaceutical Drug, whilethe effect was only marginal for the other two categories. For a list of top n candidates for Body Part, however, clusterswith a size of up to two terms were slightly more useful than larger clusters. For Pharmaceutical Drug, the bestsettings resulted in a recall of 25 % for a candidate list of top n terms and a recall of 68 % for top 10n. For a candidatelist of top 10n candidates, the second best results were obtained for Medical Finding: a recall of 58 %, compared to46 % for Body Part. Only taking the top n candidates into account, however, resulted in a recall of 23 % for Body Part,compared to 16 % for Medical Finding.Conclusions: Different settings for corpus pre-processing, window sizes and cluster sizes were suitable for differentsemantic categories and for different lengths of candidate lists, showing the need to adapt parameters, not only tothe language and text genre used, but also to the semantic category for which the vocabulary is to be expanded. Theresults show, however, that the investigated choices for pre-processing and parameter settings were successful, andthat a Japanese blog corpus, which in many ways differs from those used in previous studies, can be a useful resourcefor medical vocabulary expansion.Keywords: Japanese language processing, Medical vocabulary expansion, Distributional semantics, Randomindexing, Agglomerative hierarchical clustering*Correspondence: maria@gavagai.se2Department of Computer Science, Linnaeus University/Gavagai,Växjö/Stockholm, SwedenFull list of author information is available at the end of the article© 2016 Ahltorp et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 2 of 18IntroductionThe ability to recognise named entities in text, and thento map these to concepts in medical ontologies, is keyfor most systems that rely on medical language pro-cessing, such as information extraction systems for syn-dromic surveillance [1], automatic detection of adversedrug events [24] and co-morbidity analyses [5]. As aresult, much research has been conducted, and many sys-tems developed, with the aim of improving these buildingblocks of medical information processing systems.There are a number of systems for performing mappingto specific vocabulary concepts, for instance MetaMap[6], IndexFinder [7], MedLEE [8] and SAPHIRE [9]  allof these map entities to concepts in the Unified MedicalLanguage System (UMLS) [10]. Although mapping sys-tems typically employ techniques for handling abbrevia-tions, misspellings, inflections and word order differences[11, 12], the availability of a comprehensive vocabularyis perhaps the most important prerequisite for perform-ing high-quality concept mapping. Extensive vocabulariesare also essential for named entity recognition approachesthat rely on vocabulary mapping [13, 14] as well as formedical text simplification [15]. Vocabularies have, more-over, also been shown to be useful for generating featuresto be used when training machine learning models torecognise named entities [16].There are a number of extensive medical vocabular-ies, many of which are included in the UMLS Metathe-saurus [10]. These vocabularies are, however, not availablein all languages; in languages for which they are avail-able, they are often less extensive than resources forEnglish. Although also less extensive vocabularies havebeen shown useful for medical text mining [17], limita-tions in the vocabularies used can lead to decreased per-formance. Previous studies on applying Swedish UMLSresources (which are less extensive than resources forEnglish) for detecting entities in Swedish clinical textshowed, for instance, a recall of 55 % for detecting Dis-orders, 33 % for detecting Findings, 80 % for detect-ing Body Parts [18], and a recall of 74 % for detectingPharmaceutical Drugs [19]. Controlled medical vocab-ularies are, moreover, typically focused on terms fromthe professional medical language, despite the importantapplications of text mining from social media, such assyndromic surveillance [20, 21] or detection of adversedrug reactions [22]. The fact that terms used by laymen todescribe medical concepts often differ from those used byhealth professionals [15, 23] render the controlled medicalvocabularies less useful for text mining from social media[22, 24] as well as for applications such as medical textsimplification [25].To improve the performance of concept mapping,entity recognition and medical text simplification, exist-ing vocabularies thus need to be expanded and adaptedto the text domain in which such systems are to beemployed. To manually expand and adapt vocabulariesto variations in language between different text genresand over time is, however, expensive and time-consuming.Methods that can support this process are therefore valu-able, for instance methods for semi-automatic vocabularyexpansion.Many previously explored methods for medical vocab-ulary expansion [2628] rely on terms and abbreviationsbeing explicitly defined or classified in the text. There are,however, many medical text genres in which the termsused are only very rarely explained to the reader, e.g.,the narrative text of health records and the above men-tioned social media texts. For such genres, other strategiesfor vocabulary expansion are required. One strategy is touse existing vocabularies as a starting point, and searchfor additional terms that occur in similar contexts as theexisting vocabulary terms. This can be motivated by thedistributional hypothesis, which states that words whichoccur in similar contexts often have similar meanings [29].For most languages, there is a limited availability of largemedical/biomedical corpora that can be used for research.As a result, research on medical vocabulary expansion,using distributional semantics methods developed forlarge corpora, e.g., random indexing or word2vec, has typ-ically focused on English vocabularies [3034], and onvocabularies for relatively similar languages, e.g., Swedish[3537]. Less work has been carried out on vocabularyexpansion using languages not related to English, or onmedical text genres with laymen authors.Outside of the medical domain, the variability of stud-ied languages is larger, and distributional semantics hasbeen applied to e.g., Japanese text, in studies using con-text in the form of noun-verb and noun-noun dependen-cies [38, 39]. Research on the adaption of distributionalsemantics to Japanese corpora, using context informa-tion in the form of several neighbouring words, has,however, not always been successful [40]. This indicatesthat, although distributional semantics methods are oftenclaimed to be language independent, there might be lan-guages for which adaptions of standard configurations andpre-processing methods are required. One difficulty asso-ciated with Japanese emerges from the fact that whitespace is normally not used ([41], p. 17). The standardapproach for segmentation used in distributional seman-tics, in which the white-space segmented word forms thebasic semantic unit (with special handling of punctuationmarks and sometimes also of abbreviations), can thereforenot be employed. Grammatical differences between lan-guages could pose another challenge and might entail thatthe pre-processing choices that are most optimal for, e.g.,English are not necessarily suitable for, e.g., Japanese.In this study, we aim to investigate the possibility ofexpandingmedical vocabularies by applying distributionalAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 3 of 18semantics on a medical corpus that differs from corporaused in previous studies. We use a corpus that differs interms of language as well as text genre, namely a corpusof Japanese patient blog texts. We focus on three seman-tic categories that are highly relevant for  just to name afew the aforementioned tasks of syndromic surveillanceand detection of adverse drug events: the semantic cat-egories Medical Finding, Pharmaceutical Drug and BodyPart. We will compare different pre-processing strate-gies and parameter settings with the aim of investigatingwhether standard pre-processing and parameter settingsneed to be adapted to (1) the Japanese language, (2) thetext genre, or (3) to the semantic category.BackgroundThere are a number of methods designed to support thecreation or expansion of vocabularies that, more specif-ically, categorise words into semantic categories [42] oridentify hyponyms of selected terms [43]. With the exis-tence of semantic categories, (semi-)automatic vocabularyexpansion can be seen as a word classification task, inwhich it should be determined whether an unknown wordbelongs to a certain semantic category or not. When cat-egories have not been defined, however, word clusteringcan instead be considered as a means to discover poten-tial semantic categories [44]. As there is extensive workon the definition of semantic categories within the med-ical domain, e.g., within UMLS [10], the first approachis typically taken for medical vocabulary expansion, i.e.,classification of unknown words into pre-defined medicalcategories.The material for creating or expanding a vocabularycould be an existing vocabulary, for instance when iden-tifying synonym candidates by searching for similar termdescriptions in a lexicon [45], or when translating a vocab-ulary from one language into another [46]. An alternativeto using information from existing vocabularies is to usecorpora for extracting term candidates for inclusion in avocabulary. A frequently used approach is to find text pat-terns in which terms used are explained to the reader.From the text pattern term1 also known as term2 [47],for instance, it could be deduced that term1 and term2 aresynonyms, while the text pattern term1 such as term2could be used for extracting terms of a given semantic cat-egory [42]. These patterns can be either manually craftedor automatically extracted from the corpus. A relatedapproach is to, instead of using explicit language patterns,rely on frequent co-occurrences in a large corpus between,e.g., terms and their hypernyms [48]. Another type ofpattern-based vocabulary extraction from corpora is theconstruction of patterns for terms consisting of wordswith certain syntactic or semantic relations, e.g., automat-ically constructed patterns for extracting specific typesof noun-verb pairs [49]. These corpora-based methodshave been applied in the biomedical domain for synonymextraction [27, 47, 50] and for extracting terms of a cer-tain semantic category [28, 48]. In the latter study, termsbelonging to the semantic category Disease were automat-ically extracted by first extracting all sentences containingthe word disease and thereafter using a known set ofdisease terms to train an SVM classifier to detect termsbelonging to this semantic category. A precision of 38 %and a recall of 45 %were achieved when applying the auto-matically constructed vocabulary for vocabulary-basednamed entity recognition of diseases.As mentioned in the introduction, it is also possible toextract terms on the basis of the contexts in which theytypically occur [44]. This approach is more suitable tosome types of corpora, e.g., blog posts or health recordnarratives, since it does not depend on hyper/hyponomybeing explicitly stated in the text. Instead, it exploits thefact that words that frequently occur in similar contextsare likely to have similar meanings. Representation mod-els for such term co-occurrence can, for instance, beprobabilistic, such as in Brown clustering [51], or spatialmodels [52], in which term co-occurrences are given ageometric representation in the form of a vector space.Here, semantic similarity is based on geometric proxim-ity. Zhang and Elhadad [32] have explored the approachof using distributional semantics for vocabulary expansionin the biomedical domain [32]. They constructed signa-ture vectors for noun phrase chunks in a corpus basedon the words included in the chunks, the surroundingcontext words (two words to the left and right, respec-tively, of the noun phrase chunk), as well on their inversedocument frequency. As seed words, they used the termsof the relevant semantic categories available in UMLS.Cosine similarity was then measured between the nounphrase chunks in the corpus and the average of the seedterms signature vectors. All noun phrase chunks with asimilarity to the average signature vectors above a cer-tain threshold were considered as candidates for newterms belonging to the investigated semantic categories.For detecting named entities of the categories MedicalProblem, precision scores of between 27 % and 28 % andrecall scores of between 31 % and 34 % were achievedwhen applying their method on the three different clinicalsub-corpora within the 2010 i2b2/VA challenge.Most research on the expansion of medical vocabulariesusing distributional semantics in large corpora has beenperformed on English, but similar research using smallercorpora has been carried out on a larger variety of lan-guages. From a French 85000 token coronary diseasescorpus, for instance, unknown tokens with dependencyrelationships similar to known medical vocabulary termshave been automatically extracted [53, 54]. The same cor-pus has been used for evaluating techniques for buildingword space models specifically adapted to small corporaAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 4 of 18by generalising and normalising distributional contexts[55]. The techniques were evaluated for the extent towhich the neighbours in the semantic word space hada semantic relation in existing medical resources, e.g.,relations of synonymy and co-hyponymy.Random indexing is another spatial model of distribu-tional semantics that can be used to build a semanticspace. This method was originally proposed by Kanervaet al. [56] to deal with the performance problems, interms of computational cost, that were associated withthe commonly used models of distributional semanticsat the time (latent semantic analysis/indexing). Due toits computational efficiency, random indexing remains apopular method when building distributional semanticmodels from very large corpora, e.g., large web corpora[57] orMedline abstracts [58]. There are, however, a num-ber of other methods available, e.g., word2vec [59] andGloVe [60] that are also often used for creating spatialdistributional semantics models from large corpora.The efficiency of random indexing is achieved by cir-cumventing the need to perform dimensionality reductionon the original term-by-contextmatrix (where the contextcan be defined as, e.g., a document or the surround-ing words), which is an important component of manyother models of distributional semantics [57]. Instead, asemantic space with a smaller, predefined dimensionalityis created from the beginning. This is achieved by firstassigning to each context feature (e.g., each unique term),a sparse, random vector of the required dimensionality.These vectors, called index vectors, are generated by ran-domly distributing a small set of non-zero (+1 and ?1)values, with the remaining elements set to 0. If the dimen-sionality is sufficiently large in relation to the number ofcontext features, the index vectors will, with a high prob-ability, be nearly orthogonal to each other. The index vec-tors are only used for building the semantic space, which,instead, is composed of semantic vectors. The semanticvector of a term, of the same dimensionality as the indexvectors, is obtained by adding up all the index vectors ofthe terms with which it co-occurs in a predefined context typically a symmetric window of surrounding words.The similarity between terms can, e.g., be expressed by thecosine similarity between their vectors, i.e., the cosine ofthe angle (? ) between the semantic vectors u and v. Thisis computed as follows ([61], pp. 127134):cos(?) = u · v?u??v? =n?i=1ui · vi?n?i=1(ui)2 ·?n?i=1(vi)2With random indexing and a window-based contextdefinition, it is possible to create different types of seman-tic vectors. If the index vectors of the surrounding wordsare added to the target terms semantic vector as-is, theresulting semantic vectors are known as context vectors.This, however, entirely ignores word order within the con-text window. There is also a version of random indexing,sometimes called random permutation, in which this istaken into account. It does so by rotating the elements inthe index vector one step to the left or right, dependingon if the context term appears to the left or right of thetarget term. When the permuting of index vectors is per-formed in this manner, the semantic vectors are denoteddirection vectors. Random permutation spaces with direc-tion vectors have been shown to better detect synonymythan random indexing spaces with context vectors [62].There is a previous study, in which a random index-ing space constructed from Swedish medical journal textwas used for expanding a Swedish medical vocabularylist, consisting of MeSH terms denoting Medical Find-ing and Pharmaceutical Drug [36]. Using a set of 91 seedterms for each of the two semantic categories, it waspossible to extract 53 % of the 90 expected Medical Find-ings and 88 % of the 90 expected Pharmaceutical Drugsamong the top 1000 retrieved terms. The manual eval-uation of precision showed results of 80 % for top 50and 68 % for top 100 for Medical Finding and 64 % fortop 50 and 47 % for top 100 for Pharmaceutical Drug. Inthat study, as well as in the previously mentioned studyin which distributional semantics was used for expandingmedical vocabularies [32], terms belonging to semanticcategories given by currently available vocabularies (e.g.,the categories Medical Finding and the PharamceuticalDrug) were treated as belonging to one distributionallysimilar category of terms. In both studies, the criterionfor ranking unknown words as potential candidates was,therefore, based on similarity to all of (or to the averageof) the seed terms from one of the categories in the exist-ing vocabularies. This is not, however, necessarily a goodstrategy since there might be a number of distributionalsub-clusters within each semantic category of the exist-ing vocabularies. If such sub-clusters are positioned atlarge distances from each other in the semantic space, thismight have the effect that words that are not part of thesesub-clusters, but close to two or more clusters, will incor-rectly receive a higher ranking than words that are close tothe centroids of the sub-clusters. This was shown to be thecase in a study using distributional semantics for expand-ing a Swedish vocabulary of cue terms for uncertaintyand negation [63]. The strategy of first clustering the seedterms used into more distributionally similar subsets andthereafter using similarity to the centroids of these subsetsas the criterion for ranking unknown words outperformedthe strategy of treating the seed terms used as one singledistributionally similar category of terms.We will take the possibility into account that this mightbe the case also for the three semantic categories thatAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 5 of 18we investigate here, i.e., Medical Finding, Pharmaceuti-cal Drug and Body Part, and therefore study the effectof dividing seed terms of these categories into smaller,more distributionally similar, subsets before using themfor vocabulary expansion.MaterialsTwo types of materials were used: a blog corpus andexisting Japanese vocabularies.CorpusThe corpus used is a Japanese blog corpus from theTOBYO site, which collects blogs written by patientsand/or their relatives [17]. After a first normalising stepdescribed below, the corpus contained 270 million char-acters and after pre-processing, also described below, itcontained 50 million semantic units (2.5 million unique).VocabulariesSince the semantic spaces were evaluated for their abil-ity to expand vocabularies with terms belonging to thethree semantic categories Medical Finding, Pharmaceu-tical Drug and Body Part, Japanese terms belonging tothese categories were gathered from existing vocabularies.These were then used both as seed terms and as evaluationdata.For Medical Finding, the following terms were used:MeSH terms classified under the nodes Diseases (C)and Mental disorders (F03) [64], all MedDRA/J termsexcept those classified as investigations, social circum-stances and surgical and medical procedures [65, 66], aswell as terms from the Byomei diagnosis list [67]. ForPharmaceutical Drug, the following terms were used:MeSH terms classified under the node Chemicals andDrugs (D) and pharmaceutical brand names availableat the TOBYO site [68]. For Body Parts, the followingterms were used: MeSH terms under the node Anatomy(A) except those under the sub-nodes Plant Structures(A18), Fungal Structures (A19), Bacterial Structures (A20)and Viral Structures (A21), as well as terms from a lan-guage education web page listing body parts in Japanese[69]. The Japanese translations of MeSH and Med-DRA were obtained from the U.S. National Library ofMedicine.1Vocabulary terms occurring more than 50 times inthe segmented corpus as a semantic unit in the contextof a sentence were included in the set of terms used.Terms in existing vocabularies that were segmented intoseveral semantic units were therefore excluded, as wereinfrequent terms, due to the weak statistical foundationfor their context vectors. The number of terms in eachsemantic category, before and after the frequency filtering,is shown in Table 1.Table 1 Vocabulary sizeMedical finding Pharmaceutical drug Body part(# semantic (# semantic (# semanticunits) units) units)All terms in usedvocabularies77350 27912 2960More than 50occurrences inthe segmentedcorpus as asemantic unit inthe context of atleast one othersemantic unit753 276 214MethodsIn addition to not being able to employ the standardsegmentation approach of white space tokenisation, weidentified a number of grammatical differences betweenJapanese and languages similar to English that might berelevant when constructing distributional semantics mod-els. A morphologic normalisation in the form of a totallemmatisation is sometimes performed on corpora usedfor distributional semantics. Japanese is, however, highlyagglutinative ([70], p. 297), with the possibility to add sev-eral suffixes to verbs and to one of the two Japaneseadjective types, the verbal adjectives ([41], p. 45). The suf-fixes are, for instance, used for expressing negation ([41],p. 54), desire ([41], p. 111) or level of politeness/formality([41], pp. 8183). Full lemmatisation could therefore resultin severe loss of information. In addition, distributionalsemantics studies on Germanic languages have shownthat employing a small context window of co-occurringwords (typically 12 preceding and following words) ismost suitable when building models for word similarity[62]. This is not necessarily the case for languages withanother sentence structure, such as Japanese. The basicword order of Japanese (SOV) is different from the wordorder of English, and it is also relatively free since the func-tion of a word (e.g. whether it is a topic, subject or object)is indicated by case particles ([41], pp. 35-38). Therefore,another context window size might be more appropri-ate for Japanese. Also the stop word filtering, often usedfor e.g., English vocabulary extraction [62], might have tobe adapted to Japanese, possibly retaining the frequentlyoccurring case particles.Previously performed experimentsWe have previously performed preliminary experimentsusing random indexing for extracting Medical Find-ings, Pharmaceutical Drugs and Body Parts from aJapanese blog corpus [71]. In these experiments, wecompared three different corpus pre-processing versionsto investigate the identified grammatical differences. Inthe first pre-processing version, the corpus was fullyAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 6 of 18lemmatised and stop word filtering was performed byremoving all semantic units except verbs, adjectives andnouns/pronouns. In the second version, parts of the infor-mation contained in the suffixes, which potentially has alarge impact on the semantics of the surrounding seman-tic units, were retained. This included polarity (negationor affirmation), grammatical mood and voice, while e.g.,formality level and tense were excluded. In the third ver-sion, case particles were also retained to study if thiscould compensate for the relatively free word order ofJapanese. We also experimented with different contextwindow sizes, constructing distributional semantic spaceswith four different window sizes for each pre-processingversion, using a context window of 1 + 1, 2 + 2, 4 + 4 and8 + 8 surrounding semantic units.The results from these initial experiments showed thatthe optimal window size and pre-processing techniquewas dependent on which semantic category was tar-geted. For extracting Medical Findings and Pharmaceuti-cal Drugs, the two versions in which case particles wereremoved outperformed the version in which they wereretained, while a context window size of 1 + 1 was opti-mal. For Body Parts, on the other hand, better resultswere obtained when case particles were retained. Varia-tion in context window size had hardly any effect on theresults for this semantic category when case particles wereretained, but marginally better recall was obtained with awindow size of 8 + 8.Aim of the performed experimentsThe previously performed experiments revealed substan-tial differences in the optimal pre-processing choices forthe investigated semantic categories: removing case parti-cles and using the smallest context window was the mostsuitable for Medical Finding and Pharmaceutical Drug,whereas retaining case particles and using the largest con-text window was the most suitable for Body Part. In theprevious study, however, a very simple method for lever-aging the position of the seed terms in the semantic spacewas used [36], in which a summed similarity to a semanticcategory for every term in the random indexing space wascalculated. The calculation was carried out by summingthe cosine of the angle (?u¯,s¯) between the context vector ofthe semantic unit (u¯) and the context vector of each term(s¯) in the set of seed terms (S) of the semantic category inquestion.summedsimilarity(u¯) =?s¯? Scos(?u¯,s¯)It was thus not taken into consideration that each oneof the three investigated semantic categories might inreality consist of a number of smaller distributionally sim-ilar sub-clusters, and that a more successful strategy forexpanding a vocabulary might be to use proximity to thesesub-clusters rather than proximity to all seed terms.In addition, in the previously performed experiments,only a small vocabulary resource was used for evaluat-ing the large semantic category Medical Finding, and theevaluation was only performed for two randomly selectedsets of seed terms, which might make the results diffi-cult to generalise for seed terms of the three investigatedcategories in general.Given the results and the limitations of the preliminaryexperiment, two hypotheses were posed: Dividing the seed terms into sub-clusters and usingsimilarity to their centroids as the ranking criterion isa more successful strategy than using the method ofsummed similarity to all seed terms. The appropriate choice of corpus pre-processingtechniques and context window size depends on thesemantic category of interest.Performed experimentsTo investigate the posed hypotheses, an experiment wascarried out in the following four steps: 1) Pre-processingof the corpus for random indexing; 2) construction ofa semantic space using random indexing; 3) hierarchicalclustering of vectors in the semantic space that corre-spond to seed words and production of ranked lists ofterms according to their proximity to centroids of the con-structed clusters (one list per maximum cluster size); 4)automatic evaluation of recall of the terms in the producedlists against a reference standard.Pre-processing of the corpus for random indexingJapanese is written using three sets of characters: kanji,the logographic characters borrowed from Chinese writ-ing, are used for lexical morphemes; hiragana, one ofthe two syllabic character sets, are used for grammaticalmorphemes, both grammatical morphemes as individualwords and as inflections; and katakana, the other syllabiccharacter set, is used for loan words of non-Chinese origin([72], pp. 184192). There are, however, also some lexicalmorphemes for which there is no kanji and that, therefore,are written using hiragana, as well as morphemes that arecommonly written using hiragana, despite a possibility touse a kanji character. The use of logographic charactersoften makes the morphological boundaries in compoundsmore evident than what is the case in a phonetic writ-ing system. This has been used in previous studies, inwhich the morphological compounds of medical terms inlanguages with phonetic writing systems have been deter-mined by mapping them to their corresponding Japaneseterm [73]. In contrast tomany other writing systems, how-ever, word boundaries are not marked by white space inJapanese ([72], pp. 184192). This has the effect that whiteAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 7 of 18space tokenisation cannot be applied, which is a standardtokenisation method used in many previous distributionalsemantics studies.The absence of white space was addressed by apply-ing a number of steps for creating a segmented versionof the corpus. A basic pre-processing was first carriedout by removing smileys and sentences solely contain-ing Latin characters, as well as normalising the hiraganaand katakana characters by transforming half-width formsinto the corresponding full-width form. Thereafter, thesegmented version of the corpus was built in the followingtwo steps: (a) applying the dependency parser CaboCha tothe corpus [74] and (b) applying the semantic role labellerASA [75] to the parsed corpus.Two different versions of the tokenised corpus werethen created to investigate the two different pre-processing strategies that had been the most successfulin the previously performed experiments. In the first ver-sion, only semantic units classified by CaboCha as eithera verb (not including helper verbs or copula), an adjec-tive (including verbal adjectives, adjectival nouns andadverbial derivations of adjectives) or a noun (includ-ing pronouns) were retained; all other semantic unitswere removed. In addition, all verbs and verbal adjectiveswere fully lemmatised (as nouns, pronouns and adjecti-val nouns are not inflected in Japanese, they cannot belemmatised). In the second version of the corpus, case par-ticles were retained, in addition to verbs, adjectives andnouns/pronouns. Verb and verbal adjective inflectionsindicating polarity (negation/affirmation), grammati-cal mood (subjunctive/imperative/optative/interrogative/indicative) and voice (passive/causative/potential/active)were also retained in this version of the corpus.Apart from constructing semantic units based on theconstituent and morpheme boundary information givenby CaboCha, ASA also provides the inflection type infor-mation used in the pre-processing. The output of seman-tic role labels, which is also given by ASA was, however,not used.Construction of a semantic space using random indexingFor the corpus version in which case particles wereremoved, a random indexing space with a window size of1 + 1 was created, and for the version with retained caseparticles, a random indexing space with a window size of8 + 8 was created. The choices were based on the win-dow sizes that were the most successful for the differentpre-processing techniques in the previously performedexperiments [71].The parameter settings of the constructed spaces weregenerally based on standard settings for random indexing[62], i.e., using random permutation with 2000 dimen-sional direction vectors, with 10 non-zero elements in theindex vectors. All semantic units in the context windowsused were given equal weight.In initial tests using the blog corpus for constructingrandom indexing spaces, index vectors corresponding tosentence boundaries were added to the semantic vec-tor of a semantic unit whenever it occurred near thebeginning or end of a sentence  a setting which haspreviously been used for, e.g., Swedish medical jour-nal text [36]. This, however, led to semantic spaces inwhich a majority of the context vectors were positionedwithin close proximity to each other in the constructedspace, probably since blog texts to a larger extent than,e.g., medical scientific text, consist of sentences withvery few semantic units. These index vectors indicatingsentence beginning and sentence ending were thereforeremoved, leading to a better distribution of the contextvectors.Clustering of seed termsThe general approach for generating a list of candidateterms for possible inclusion in the vocabulary was to usea set of seed terms for ranking all words in the corpus notin the seed term set (the unknown terms) according totheir proximity to the seed terms in the semantic space.For each one of the three investigated semantic categories,the following was carried out.Agglomerative, hierarchical clustering ([76], p. 700) wasapplied to the direction vectors of the seed terms. Thiswas carried out by first assigning each vector its owncluster. The pairwise distances between clusters werethereafter calculated, and the two clusters with the largestcosine similarity between their respective centroids werethen merged into a new cluster. This hierarchical cluster-ing process was iteratively repeated until all the vectorsof all seed terms of the semantic category formed a singlecluster.The cluster sizes that were most appropriate for thetask were determined by successively moving upwards inthe tree of the created clusters. First, clusters consistingof a maximum of one term were used for creating theranked list of unknown terms (i.e., the case in which eachseed term was treated as its own cluster). Then, clustersconsisting of a maximum of two terms were used andthe maximum cluster size was then successively increaseduntil the root of the tree was reached. In each step in thisprocess, a ranked list of candidate terms was created byordering the unknown terms according to cosine similarlyto their most closely located cluster centroid. That is, asimilarity score was computed for each unknown term inthe corpus (u¯) by calculating the cosine of the angle (?u¯,c¯)between the unknown term and each of the centroids (c¯)in the set of constructed centroids (C) and returning thelargest cosine similarity score. The unknown terms wereAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 8 of 18subsequently ordered according to decreasing similarityscore.clustersimilarity(u¯) = maxc¯?Ccos(?u¯,c¯)As a final step, in order to enable comparison to themethod from the previously performed experiment, thesummed similarity method was also used for ranking theunknown terms.EvaluationThe existing vocabulary terms in each semantic categorywere divided into two sets: one set of 100 seed termsand one set of evaluation terms, comprising the remain-ing terms (n terms). A situation was thus simulated, inwhich there is 100 terms in an existing vocabulary for eachsemantic category, while the remaining n terms representnew terms that ought to be added to the vocabulary. Recallfor retrieving the terms in the evaluation set was mea-sured for the top n, 2n. . . 10n candidates in the constructedlists. This simulates e.g., a medical terminologist manuallyscanning the top n, 2n. . . 10n candidate terms in search ofnew terms to add to a medical vocabulary.To make the results less dependent on which termswere selected for the set of seed terms and which wereused in the reference standard, a bootstrap resampling[77] approach was taken, in which the experiment wasrepeated 500 times, each time with a new random selec-tion of seed terms. The final results were obtained byaveraging the recall values obtained for each resampling.A very crude baseline was also calculated in order to givean idea of how well the method performs in comparisonwith a list of randomly extracted semantic units from thecorpus. When listing all semantic units that occur morethan 50 times in the corpus in a random order, on averageevery xth item in the list would be a semantic unit from thereference standard. The top t terms in the list would thuson average contain t?(1/x) terms from the reference stan-dard. There are a total of 43050 semantic units that occurmore than 50 times in the corpus, and if n is the numberof semantic units in the reference standard for a given cat-egory, then x is 43050/n. Thereby, t?(1/x) is t?(n/43050),and the top n semantic units in this list would therefore onaverage contain n ? (n/43050) of the semantic units in thereference standard, the top 2n terms 2n ? (n/43050), andso on.In addition to the automatic calculation of recall againstthe reference standard, an analysis was performed of fac-tors influencing whether a reference standard term wasoften or seldom included as a highly ranked candidate.A manual analysis was also carried out to get an under-standing of what terms were suggested as highly-rankedcandidates, apart from those included in the referencestandard. This manual analysis was carried out for a listof unique terms from the top 100 candidates in eachfold, when using the best settings for each of the threecategories.ResultsThe results, presented in Fig. 1, provide a good basis foraccepting the second hypothesis, i.e., that a larger win-dow size and retained case particles and inflections is amore suitable setting for retrieving term candidates for thesemantic category Body Part, while a small window sizeand the removal of case particles and inflections is moresuitable for the semantic categories Pharmaceutical Drugand Medical Finding. The best average recall values for awindow size of 1 + 1 and a window size of 8 + 8 are alsoshown in Table 2 for top n candidates and in Table 3 fortop 10n candidates. A 95 % confidence interval is given bythe 2.5 %- and 97.5 %-percentiles of the 500 recall valuesobtained by bootstrap resampling [77].The largest difference between the two settings wasobserved for the category Pharmaceutical Drug: thesemantic space with a window size of 1 + 1 resulted in amaximum recall average of 25 % for top n and a maximumrecall average of 68 % for top 10n, while the correspond-ing scores for the semantic space with a context window of8+ 8 were 11 % and 32 %, respectively. Likewise, for Med-ical Finding, better results were achieved with the 1 + 1space at each of the ten measurement points; however, thedifference is smaller than for Pharmaceutical Drug, withmaximum average recall values of 16 % for top n and 58 %for top 10n for the 1 + 1 space versus 12 % and 45 % forthe 8 + 8 space. For Body Part, the reverse results wereobserved, i.e., that better results were achieved with the8+8 space at each measurement point, with 23 % recall attop n and 46 % recall at top 10n versus 16 % recall and 37 %recall with the 1+1 space. At the top 10n level for windowsize 1+1 for Pharmaceutical Drug and especially for BodyPart, the results had a larger variance than for the otherresults shown in Tables 2 and 3, indicating that the resultsfor these categories and settings were more dependent onwhat terms were used as seed terms.When comparing the results of the three categories, itcan also be concluded that, for a candidate list of 10ncandidates, the evaluated method is most successful forthe category Pharmaceutical Drug, followed by MedicalFinding, for which the maximum recall average is 10 per-centage points lower, while it is yet another 13 percentagepoints lower for Body Part. For the top n candidates, onthe other hand, the best results were achieved for Phar-maceutical Drug and Body Part, with slightly lower resultsfor Medical Finding.With respect to the first hypothesis, i.e., that a cluster-ing approach would be more successful than the simplesummed similarity method, the results are less evident,also when focusing on the context window size and pre-processing strategy that was the most successful for eachAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 9 of 18Fig. 1 Recall for retrieving semantic units belonging to the three investigated semantic categoriesTable 2 Best results for top n. For window sizes 1 + 1 and 8 + 8%Window size 1 + 1 Best strategy Average 2.5 % percentile 97.5 % percentile VarianceMedical Finding summed similarity 16.3% 13.9 % 19.2 % 0.0002Pharmaceutical Drug summed similarity 24.8% 20.1 % 29.4 % 0.0006Body Part cluster level 83 16.2 % 7.5 % 21.9 % 0.0011Window size 8 + 8 Best strategy Average 2.5 % percentile 97.5 % percentile VarianceMedical Finding cluster level 12 12.1 % 9.1 % 14.9 % 0.0002Pharmaceutical Drug cluster level 1 11.1 % 7.3 % 14.7 % 0.0004Body Part luster level 2 23.1% 18.4 % 28.1 % 0.0006The best results are shown in bold faceAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 10 of 18Table 3 Best results for top 10n. For window sizes 1 + 1 and 8 + 8Window size 1 + 1 Best strategy Average 2.5 % percentile 97.5 % percentile VarianceMedical Finding cluster level 100 58.1% 55.3 % 60.8 % 0.0002Pharmaceutical Drug cluster level 14 67.9% 56.2 % 77.4 % 0.0029Body Part cluster level 73 36.6 % 20.6 % 46.5 % 0.0036Window size 8 + 8 Best strategy Average 2.5 % percentile 97.5 % percentile VarianceMedical Finding cluster level 20 44.9 % 40.5 % 49.7 % 0.0006Pharmaceutical Drug cluster level 2 32.2 % 26.6 % 37.9 % 0.0010Body Part cluster level 83 45.7% 38.6 % 51.8 % 0.0011The best results are shown in bold facecategory. To use the centroid of all seed terms was moresuccessful for retrieving Body Part terms than to use thesummed similarity method. There was, however, a verysmall difference between different cluster levels for thissemantic category, except for the top n and 2n candidates,for which it was slightly more successful to use proximityto centroids of seed term clusters of size 2 as the rank-ing criterion. Likewise, for the category Medical Finding,there were no large differences between different clusterlevels, but minimally better results were achieved wheneither using the centroid of all seed terms or the summedsimilarity method, depending on how many candidateterms were taken into account. The category Pharmaceu-tical Drug was the only category for which the resultsvaried for different cluster levels for a longer candidatelist. Using the centroids of clusters with a maximum sizeof 14 gave, on average, the best results with a candidatelist of top 10n and 9n. No clear conclusions can, how-ever, be drawn for this category either, since either thesummed similarity method or using the centroid of allseed terms wasmore successful whenmeasuring the recallfor a shorter candidate list. Especially for top 2n and top3n, using proximity to the centroid of all terms outper-formed the use of proximity to clusters up to a size of 90seed terms.It could also be observed that the curves have differentforms depending on the two explored window sizes/pre-processing choices. For the 8+ 8 space, all curves are veryflat, while for the 1 + 1 space, the use of different clusterlevels has a bigger effect on the results.Analysis of retrieved entitiesTo investigate patterns for which terms were and were notretrieved by the evaluated methods, statistics of the pro-portion of times a term was retrieved when it appeared inthe reference standard used were gathered. The best set-tings for each of the three studied categories were used,i.e., the setting that resulted in the best recall for amajorityof the ten points of measurement. The results, visualisedin Fig. 2, show that the distribution of retrieved termsamong the top 10n candidate terms is highly skewed for allthree investigated entity categories. Regardless of whichset of seed terms is used, a large proportion of the termsare found in more than 95 % of the cases, while anotherlarge proportion is found in less than 5 % of the cases.For these two distinct groups of terms those that werefound in less than 5 % of the cases and those that werefound in more than 95 % of the cases  the frequencyof the terms in the TOBYO corpus was investigated andis visualised in Fig. 3. For the category Medical Finding,and even more clearly for Body Part, terms that occurrelatively infrequently in the corpus are overrepresentedamong those that were retrieved in fewer than 5 % ofthe cases. For Body Part, these infrequently occurringterms were more typical for the specialised language ofmedical professionals than the more frequently occurringFig. 2 This illustrates how often a term is found when used asreference standard term. The first stack shows the number of termsthat are correctly retrieved between 0 % and 5 % of the times they areused in the reference standard, the second stack shows the numberof terms retrieved between 5 % and 10 % of the times, and so on. Thestatistics are shown for top 10n candidate terms (using cluster level100 and fully lemmatised and stop word filtered corpus for MedicalFinding and Pharmaceutical Drug and cluster level 34 with the corpusretaining more information for Body Part)Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 11 of 18Fig. 3 This illustrates the frequency of the terms in the TOBYO corpus for two opposite groups of terms used as evaluation data; those terms thatwere found in less than 5 % of the cases they were used as a reference standard term and those that were used in more than 95 % of the cases theywere used as a reference standard termterms. Terms such as tongue , waist/hips , fingerand throat , which are likely to occur naturallyin laymen text, were often retrieved. Many more tech-nical terms, such as cranial nerve , pancreaticduct and nasal cavity , were, on the otherhand, never retrieved. For the category Medical Finding,however, no such evident difference between laymen lan-guage and professional language could be found betweenfrequently and infrequently retrieved terms. The visuali-sation shows that increasing the frequency cut-off (whichwas 50 occurrences in the corpus) by another 10 or 20occurrences would have resulted in a higher recall for theevaluation method applied, especially for Body Part, forwhich no infrequent terms were found.For the category Pharmaceutical Drug (which containedterms under the MeSH node Chemicals and Drugs), therewas also a trend of infrequently occurring terms beingoverrepresented among those retrieved in less than 5 %of the cases. The trend was, however, not as evidentas for the other two categories. A brief inspection indi-cated that terms denoting chemicals often referred to innon-medical domains were more frequent among termsthat were rarely found than among those that were oftenfound. A manual classification of the terms used for eval-uating the category Pharmaceutical Drug was thereforeperformed, in which the termswere classified according tothe three groups: a) Terms often used for denoting a typicalpharmaceutical drug, b) Terms often used when referring toAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 12 of 18concepts not related to pharmaceutical drugs and c) Termsoften used for pharmaceutical drugs as well as for conceptsnot related to pharmaceutical drugs. The classification,which was performed by an annotator without knowledgeof which terms were often or rarely retrieved, showed thatamong terms found in more than 95 % of the cases, 91 %belonged to the class Terms often used for denoting a typ-ical pharmaceutical drug, while the same figure for termsfound in less than 5 % of the cases was 34 %.Examples of terms that were rarely found are dia-monds , vehicular emission and tablesalt , while cough suppressants , diclofenacand analgesic drugs are examples ofterms often found. As the patient blogs are not solelyfocused on medical issues, it is very likely that there aremany terms under the MeSH node Chemicals and Drugsthat occur frequently in the corpus, without referring toconcepts related to pharmaceutical drugs.Analysis of highly ranked candidate terms not included inthe reference standardsFor producing lists of highly ranked terms not includedin the reference standards used, the following was carriedout for each one of the three semantic categories: The top100 terms in the candidate list (produced using the bestsettings according to the criteria described above) weregathered from each one of the 500 folds, resulting in alist of 50000 items. The list was then reduced to includeonly one unique occurrence of each term (as most termswere on top 100 lists produced from many of the folds,most terms occurred several times in the gathered list).In addition, terms included in existing vocabularies wereremoved from the list.This resulted in a list of 313 terms for Medical Finding(397 when including those found in existing vocabularies),94 terms for Pharmaceutical Drug (143 including termsfrom vocabularies) and 407 terms for Body Part (485including terms from vocabularies). The very large reduc-tion of the lists when only keeping unique terms, showsthe consistency of candidate terms generated between dif-ferent folds. The lists of unique terms were then manuallyanalysed by searching for categories of terms.Around a fifth of the candidate terms for Medical Find-ing could be classified as belonging to this category.Among them were terms describing states of mind, mostof them negative, e.g., depression , anxiety/fear ,lack of sleep and and worry/pain ;compound terms, e.g., blood vessel+pain=vascular painand tear+eyes=teary eyes ; terms consisting oforthographic variants of those found in vocabularies, e.g.,breast cancer and ; English and German loanwords, e.g., panic , trauma and complex. Some of these might be possible to includein a medical vocabulary of more formal terms, whileothers would be typical to a resource of laymen terms. Atype of expressions that might be considered too infor-mal for the professional language are the double-formonomatopoetic words that were found among the newMedical Finding terms, e.g, dizzy , completelyexhausted/weak and , worn-out/shabby. Despite their informal nature, they might, how-ever, still be useful, for instance when mining for descrip-tions of patient reactions to drugs, and there are examplesof such terms in the existing vocabularies used in thestudy, e.g., the term feel sick/irritated .It is difficult to draw the exact line for when a describedstate of mind should be considered a Medical Finding;as a result, an additional 4 % of the candidate termswere not considered Medical Findings, but more generaldescriptions of state of mind, e.g., loneliness ,cowardice , as well as hypernyms to terms describingstates of mind, e.g., feeling/mode and feeling/emotion. Among the candidates not categorised as MedicalFindings, there were also about the same amount of termsdescribing some kind of level or change of state, e.g.,change of mood , change and , half priceand development , of which somemight be usedfor describing that there is a medically relevant change inthe patient.Around a tenth of the candidates described generalphenomena, most of them phenomena that at least insome contexts could be described as negative, e.g., badhabit/peculiarity , uproar/disturbance ,trouble and problem/question . Althoughsome of these terms are likely to be semantically closeto Medical Findings due to frequently occurring in sim-ilar negative contexts, there were also terms that typ-ically occur in the context of descriptions of Medi-cal Findings, e.g., the term custom/habit  that,for instance, is a component in the expression life-styledisease  and the term condition , forinstance used in an expression such as be in a good stateof health . These kinds of terms might betoo general to include in a vocabulary of professionallanguage, but might be the expressions used by patientswhen describing their health and therefore an importantresource for medical text mining from laymen texts.The candidate terms for the category PharmaceuticalDrug could easily be divided into two main groups. Thefirst group (53 %) were terms denoting pharmaceuticals;of which 28 % were trade names, e.g., Tamifluand an abbreviated form of Elental ; 50 % werespecifications of types of pharmaceuticals, e.g., painkiller, antipyretic , vitamin pills andsleeping pills and ; and finally 22 % wereorthographic and other versions of words for pharmaceu-ticals, e.g., honorific form , written with hiragana andkatakana and , a misspelling/pre-processingAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 13 of 18error , as well as versions of their physical forms, e.g.,pill and capsule .The second group of Pharmaceutical Drug candidateterms (46 %) were drinks, e.g., coffee ,milkand tea , including three drink-related words gulp-ing/gulp down and and straw .Apart from the two groups pharmaceuticals and drinks,there was only one additional term, endoscope .The verb used in Japanese for taking a medicine is thesame word used for drinking , which explains thelarge group of drinks in the candidate list.The candidate terms for Body Part had a larger seman-tic diversity than those for Pharmaceutical Drug. Around17 % of the terms denoted body parts/structures/fluids,e.g., gums , cartilage , digestive organsand blood. Of these terms 10 % were specifications ofbody parts, e.g., right shoulder and left arm ;23 % were orthographic variants or honorific forms, e.g.,head written in katakana and stomach written inhiragana .The largest group (20 %) among the candidate termsfor Body Part were, however, terms describing persons,many of them family members or persons associated withhealth care, e.g., baby , twins , physician, patient in polite form , nurse andJapanese person . Three other evident groups wereMedical Findings (7 %), computer related terms (4 %) andanimals (3 %), e.g., wound, , infection , personalcomputer ,monitor , fish and dog .There were also terms semantically related to body parts,e.g., those denoting things that are physically close to orcan be worn on the body, such as pillow , wristwatch, trousers or smile/laughter , as well asthings that have a physical form similar to body parts, e.g.,container/vessel , balloon and pump .DiscussionWe have experimented with different pre-processing andparameter settings for expanding a vocabulary using dis-tributional semantics on Japanese text, more specificallyJapanese patient blog text and the three semantic cate-gories Medical Finding, Pharmaceutical Drug and BodyPart. As mentioned above, previous research on medi-cal vocabulary expansion has mainly been conducted onEnglish or similar languages, and also often on text gen-res in which the vocabulary used is sometimes defined orexplained to the reader, which is not normally the case inthe blog genre.Quality of the results compared to those of previous studiesResults obtained in previous studies are difficult to com-pare directly to those obtained here, as results are heav-ily dependent on, e.g., the evaluation strategies used. Inprevious approaches, in which automatically expandedvocabularies have been used for named entity recogni-tion, the category Disease was recognised with a preci-sion of 38 % and a recall of 45 % [28] and the categoryMedical Problem with precisions values between 27 %and 28 % and recall values between 31 % and 34 %[32]. Although these studies take a more indirect andapplication-oriented approach to evaluation than the onetaken here, the results indicate that fully automatic gen-eration of medical vocabularies is a difficult task, also forEnglish, for which vocabulary expansion from corpora hasbeen studied more thoroughly.For the vocabulary extraction from the French coronarydiseases corpus, Diseases/Diagnoses had a precision of17 %, while Chemicals, Drugs and Biological Products hada precision of 38 % for the 24 and 8 new terms, respec-tively, that were assigned those semantic categories [54].Some of the results obtained here for vocabulary expan-sion could therefore be described asmore than acceptable;for instance, being able to expand a hypothetical seedvocabulary of 100 terms with at least 119 new Phar-maceutical Drugs by manually scanning a list of 1760candidate terms (i.e., top 10n candidate), and at least 203new Medical Findings by scanning through a list of 1959candidates (i.e., top 3n candidates). Moreover, for the cat-egory Pharmaceutical Drug, scanning through the list of1760 candidates resulted in 68 % of the terms in a currentvocabulary being retrieved, which is a relatively high cov-erage for a relatively low-cost effort. For Medical Finding,a maximum recall of 58 % was achieved for the top 10ncandidates. Although this would require the more labo-rious task of scanning through 6530 candidates, it couldbe motivated by the fact that the Medical Finding cat-egory contains many vocabulary terms (as can be seenwhen comparing the categories in Table 1). Scanning a listof 1140 (top 10n) candidates for the category Body Partresulted in at least 52 new Body Part terms being addedto the vocabulary, which comprises 46 % of the expectedterms, i.e., a lower result than for the other two categories.It should be noted that the number of new terms thatwould be retrieved at different lengths of the candidatelist are described as at least x number of terms, sincethe candidate lists also contain instances of terms that arenot yet included in available vocabularies, as was shownby the manual analysis of the top 100 candidate terms.In addition, the recall values would also be higher if alarger cut-off value for term frequency would be used, aslow-frequency terms were overrepresented among termsnot found. Therefore, the explored methods have an evenlarger potential than indicated by the recall scores.The results previously obtained for extracting Med-ical Findings and Pharmaceutical Drugs from Swedishmedical journal text [36] are slightly more comparable,as a seed term set of approximately the same size wasused, and since the results were measured for a numberAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 14 of 18of candidates close to 10n. When directly comparingthe recall results, it can be concluded that higher recallwas achieved for extracting Pharmaceutical Drugs fromSwedish text (88 % compared to a top 10n recall of 68 %achieved here), while we here achieved a slightly higherrecall for the category Medical Finding (58 % comparedto the previously achieved recall of 53 %). Although, aspreviously mentioned, the results cannot be directly com-pared, they show that there are no dramatic differencesstemming from the fact that we here expanded a vocab-ulary using distributional semantics on a Japanese blogcorpus instead of using a corpus of a more formal textgenre written in a Germanic language.Adaptions to Japanese text and to the blog genreThe quality of the results show that the general strate-gies for pre-processing the corpus were successful, i.e.,using CaboCha and ASA, as well as removing all functionwords/all function words but case particles.Vocabulary terms had to occur in the corpus as inde-pendent semantic units more than 50 times to be usedas seed/evaluation terms, and it is likely that some ofthe many terms that were excluded from vocabulary listsused were excluded because they were multi-word termsand, thereby, had been segmented into multiple seman-tic units by CaboCha/ASA. This is, however, not specificto the use of CaboCha/ASA for creating semantic units,but is an even larger problem when creating seman-tic spaces built on corpora pre-processed with white-space tokenisation. Therefore, many of the vocabularyterms that were tokenised as independent semantic unitsby CaboCha/ASA, and also suggested as candidates forvocabulary expansion, would not have been suggested inthe similar study conducted in Swedish [36], as they wouldhave been divided into two different semantic units by thewhite-space tokeniser. (For instance diabetes mellitus type2, Japanese: 2 , Swedish: Typ 2-diabetes). The dif-ference is even larger between CaboCha/ASA tokenisedJapanese and white-space tokenised English, as compoundwords are less frequent in English than in Swedish. There-fore, e.g., skin diseases (Japanese: , Swedish: hud-sjukdomar) would be correctly tokenised for this purposein Swedish and in CaboCha/ASA-tokenised Japanese, butnot in English. This is typically dealt with by creating n-grams [30], or by compositional distributional semantics[78].The removal of smileys and of the index vectors indi-cating sentence beginning and end were the only specificadaptions in the pre-processing and parameter settingsthat were performed for the blog text genre.Which vocab-ulary terms were used for the evaluation were, however,indirectly governed by the choice of text genre, as someof the terms in medical vocabularies are more frequent inthe language used by patients. For the semantic categoryBody Part, terms occurring rarely in the corpus were veryoverrepresented among those that were seldom retrieved.These terms were also termsmore typical for the languageused by medical professionals than for the language usedby patients, which functions as a reminder that vocabu-lary expansion from patient blogs can, and should, onlyaim at expandingmedical vocabularies with terms that areincluded in the language used by patients.Differences between the three studied semantic categoriesApart from the more general conclusion that the pre-processing used was successful for Japanese text and forthe genre of patient blogs, the most important conclu-sion that can be drawn from the conducted experimentsis that different settings for expanding a vocabulary mightbe suitable for different semantic categories. The mostsuitable settings might also depend on the number of can-didate terms that, e.g., a medical terminologist is willing toscan through. To construct sub-clusters of seed terms had,for instance, no (or very marginal) effect when extract-ing Medical Findings, or when using the best settings forextracting Body Parts from 10n candidate terms. Whenonly looking at the n or 2n top candidates for Body Part,on the other hand, to use proximity to centroids of clusterswith amaximum size of two seed terms in each cluster wasmost successful. To use proximity to centroids of clusterswith a maximum size of 14 seed terms or to the centroidof all 100 seed terms, were successful ranking strategieswhen extracting Pharmaceutical Drugs from a term listof 10n candidates, while there was a dip in performancewhen using the centroid of clusters with a maximum sizeof 90 terms.In addition, retaining case particles/inflections andusing a large context window size was most successfulwhen extracting Body Parts, while removing case parti-cles/inflections and using a small context window wasbetter for Medical Finding and Pharmaceutical Drug. Thereason why different parameter settings are optimal fordifferent semantic categories could be that Medical Find-ing and Pharmaceutical Drug are possible to distinguishfrom other semantic categories given neighbouring lexi-cal words, such as the verb take a medicine/drinkfor pharmaceuticals, while adding additional informationjust adds extra noise. Terms from the category Body Parts,on the other hand, seem to occur in lexical contexts sim-ilar to those of terms belonging to other large semanticcategories, e.g., terms describing persons and animals.Retaining more grammatical information, as well as usinga larger context window, could, however, be a strategy thatbetter distinguishes Body Parts from these other semanticcategories.In short, parameter settings need to be adapted toeach semantic category for which the vocabulary is to beexpanded. In a realistic setting, the aim would be to findAhltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 15 of 18themaximumnumber of new relevant terms of a semanticcategory given the currently available terms, rather than,as in this study, evaluate the suitability of a method forexpanding a vocabulary, independently of which terms arealready included in the vocabulary. All available vocabu-lary terms belonging to the semantic category of interestwould in such a case be used as seed terms, and the mostsuitable parameter settings given this particular seed setshould then instead be determined, e.g., by leave-one-outcross validation.Implications and future workThere are a number of English medical corpora in whichentities of the three semantic categories explored in thisstudy have been manually annotated [7981]. In some ofthese studies, the laborious annotation effort has beenfacilitated by automatic pre-annotation built on the exten-sive medical vocabulary resources that are available forEnglish [82]. For a language (or genre) with less exten-sive vocabulary resources, on the other hand, vocabulary-based pre-annotation might not be as useful. The resultsof our study show, however, that distributional seman-tics can be leveraged for semi-automatically extending anexisting, small vocabulary. This might enable high-qualityvocabulary-based pre-annotation also for a language withlimited vocabulary resources. We believe that scanningthrough a list of candidate terms and determining whichof these belong to a certain semantic category is fasterthan scanning text for these entities, especially since theresults of our study show the potential for creating listswith a higher density of the relevant term candidates thanwould be the case when scanning through text. The resultsshow that these methods, previously applied on Englishand languages similar to English, can also be success-fully applied to text written in a very different language.Since the methods work for both language types, despitethe existence of important orthographic and grammati-cal differences, we believe that there is a high potentialfor these methods to work well also on several other lan-guages. We therefore hope that the implication of ourstudy will be that medical annotation projects on corporafrom languages with less extensive vocabulary resourceswill include an initial step in which medical terminologiesare semi-automatically expanded by methods similar tothose explored here. This would enable vocabulary-basedpre-annotation also for such annotation projects.Our future plans include the employment of this strat-egy for annotating the three investigated semantic cate-gories in a subset of the TOBYO patient blog corpus. Weaim to use all available vocabulary terms as seed terms,and determine what settings are most suitable for thislarger seed term set by employing leave-one-out cross-validation. By applying distributional semantics on patientblogs, we aim to expand existing vocabularies with moreterms that are typical to the language used by patients.2To gather lists of terms belonging to certain semanticcategories is sufficient for named entity recognition, butvocabulary lists are not enough to be able to perform con-cept mapping of detected entities. Future work, therefore,also includes strategies for positioning the gathered termswithin the hierarchy of a vocabulary, either as a synonymto an existing vocabulary concept or as a new indepen-dent concept that is to be positioned as a hyponym to oneof the existing vocabulary concepts. Distributional seman-tics could be applied also for this task, as has been shownin previous research [37].Another future plan could be to automate the pro-cess of optimising the parameter settings for differentsemantic categories. Alternatively, it might be useful toexplore if there are methods whose performance is morerobust to different choices for pre-processing and param-eter settings, e.g., ensembles of semantic spaces, whereinthe constituent semantic spaces are built with differentparameter settings. Semantic space ensembles have beenshown often to lead to better predictive performance thanthe use of any of the constituent semantic spaces on arange of tasks, includingmedical synonym extraction [83].Semantic space ensembles also make it possible to com-bine different types of corpora in an effective manner[37]; in future work, it would be interesting to combine ablog corpus with corpora from other genres, for instancebiomedical and clinical corpora but also other corpora inwhich layman terminology is likely to be used. Finally, amanual curation of the seed term set, before using it forexpanding the vocabulary, might be worth exploring, e.g.,by removing seed words from existing vocabularies thatare atypical for the semantic category in question.ConclusionsWe have studied different pre-processing strategies andparameter settings for expanding a medical vocabularywith terms belonging to the three semantic categoriesMedical Finding, Pharmaceutical Drug and Body Part. Ascenario was simulated, in which vocabulary lists of 100terms of each semantic category would be available, and inwhich a medical terminologist, for instance, would man-ually scan through a list of candidate terms in search fornew terms to add to the vocabulary lists of the investigatedsemantic categories. The candidate lists were produced byapplying random indexing on a Japanese patient blog cor-pus, and recall against existing Japanese medical vocabu-laries was measured for a scenario in which the medicalterminologist would scan through the top n to the top10n terms in the generated candidate lists, where n is thenumber of terms in existing vocabularies of the categoryin question, i.e., the number of terms that the evaluatedsystem should aim to find.Ahltorp et al. Journal of Biomedical Semantics  (2016) 7:58 Page 16 of 18It could be concluded that different settings for expand-ing a vocabulary were suitable for different semantic cat-egories. Retaining case particles in the pre-processing ofthe corpus and using a large context window size wasmostsuccessful for expanding the list of Body Part terms, whileremoving case particles and using a small context windowwas better for the categories Medical Finding and Phar-maceutical Drug. To use proximity to centroids of clusterswith a maximum size of 14 seed terms or to the centroidof all 100 seed terms were slightly better ranking strategieswhen extracting Pharmaceutical Drugs from a term list of10n candidates than to use the centroid of clusters witha maximum size of 90 terms. For the other two investi-gated categories, however, different cluster sizes only hada very marginal effect on the results for the best pre-processing settings, when looking at recall for a list of thetop 10n candidates. The most suitable settings, however,also depended on the number of candidate terms that thesimulated medical terminologist would be willing to scanthrough. For instance, when comparing recall for the topn candidates for Body Part, proximity to centroids of clus-ters with amaximum size of two seed terms in each clusterwas slightly more successful than using the centroid oflarger clusters.The best pre-processing, context window size and clus-tering settings resulted in the best average recall valuesfor Pharmaceutical Drug, for which a recall of 25 % wasachieved for top n candidates and a recall of 68 % for top10n. For a candidate list of top 10n candidates, the sec-ond best category was Medical Finding, for which a recallof 16 % was achieved for top n and a recall of 58 % fortop 10n candidates. When only taking the top n candi-dates into account, however, results for Body Part werebetter than for Medical Finding with a recall of 23 %. Forthe top 10n candidates, on the other hand, the strategywas least successful for Body Part, with a recall of 46 %. Amedical terminologist would thereby, for instance, be ableto expand the hypothetical, small vocabulary with at least203 new Medical Findings and 119 new PharmaceuticalDrugs by scanning through 1700-2000 candidate terms.These results demonstrate that the pre-processing andparameter settings applied were successful. They alsoshow the potential in using large corpora for semi-automatic medical vocabulary expansion, not only thosecomprising formal biomedical texts written in English orsimilar languages  which have been used in previousstudies  but also texts that differ in style and language,such as the Japanese patient blog texts that have been usedhere.We hope that the results of this study will inspirean increased use of semi-automatic medical vocabularyexpansion methods, for medical vocabularies in a largerrange of languages, as well as for vocabularies that areadapted to a wider variety of medical text genres. This, inturn, might widen the types of texts to which importantmedical text mining applications can be applied; appli-cations such as syndromic surveillance or detection ofadverse drug reactions.Endnotes1MeSH: https://www.nlm.nih.gov/research/umls/sourcereleasedocs/2008AB/MSHJPN/mrsab.html MedDRA/J:https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/MDRJPN/sourcerepresentation.html.2The lists of analysed terms (top 100 terms for each ofthe 500 folds), that were not included in current vocab-ularies can be found at: http://people.dsv.su.se/~mariask/resources/japanese_vocabulary/.AcknowledgementsThe authors would like to thank the main funder, Scandinavia-Japan SasakawaFoundation, for supporting this work. We would also like to thank the SwedishFoundation for Strategic Research, as well as the anonymous reviewers.Authors contributionsMA designed and implemented the functionality for carrying out corpuspre-processing, construction of random indexing spaces and agglomerativeclustering, while MS designed and implemented the functionality for carryingout the evaluation. SK was responsible for the manual analysis of the results,and, together with MS and RR, selected and gathered vocabulary to use forevaluation. AH provided input on how to build the random indexing spacesand RR on how to apply the pre-processing tools. KA provided input on theoverall design of the study, including choice of materials. The manuscript waswritten jointly by MA, MS and AH, with comments and translations from theother authors. All authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.DeclarationsPublication of this study was funded by a grant from the Scandinavia-JapanSasakawa Foundation. AHs contribution to the study was supported by theproject High-Performance Data Mining for Drug Effect Detection at StockholmUniversity, funded by Swedish Foundation for Strategic Research under grantIIS11-0053.Author details1Stockholm, Sweden. 2Department of Computer Science, LinnaeusUniversity/Gavagai, Växjö/Stockholm, Sweden. 3Graduate School ofInformation Science and Technology, Hokkaido University, Sapporo, Japan.4Department of Computer and Systems Sciences (DSV), Stockholm University,Stockholm, Sweden.Received: 1 March 2015 Accepted: 15 August 2016RESEARCH Open AccessInteroperability between phenotypesin research and healthcareterminologiesInvestigating partialmappings between HPO and SNOMED CTFerdinand Dhombres and Olivier Bodenreider*AbstractBackground: Identifying partial mappings between two terminologies is of special importance when oneterminology is finer-grained than the other, as is the case for the Human Phenotype Ontology (HPO), mainly usedfor research purposes, and SNOMED CT, mainly used in healthcare.Objectives: To investigate and contrast lexical and logical approaches to deriving partial mappings between HPOand SNOMED CT.Methods: 1) Lexical approachWe identify modifiers in HPO terms and attempt to map demodified terms toSNOMED CT through UMLS; 2) Logical approachWe leverage subsumption relations in HPO to infer partialmappings to SNOMED CT; 3) ComparisonWe analyze the specific contribution of each approach andevaluate the quality of the partial mappings through manual review.Results: There are 7358 HPO concepts with no complete mapping to SNOMED CT. We identified partialmappings lexically for 33 % of them and logically for 82 %. We identified partial mappings both lexically andlogically for 27 %. The clinical relevance of the partial mappings (for a cohort selection use case) is 49 % forlexical mappings and 67 % for logical mappings.Conclusions: Through complete and partial mappings, 92 % of the 10,454 HPO concepts can be mappedto SNOMED CT (30 % complete and 62 % partial). Equivalence mappings between HPO and SNOMED CTallow for interoperability between data described using these two systems. However, due to differences infocus and granularity, equivalence is only possible for 30 % of HPO classes. In the remaining cases, partialmappings provide a next-best approach for traversing between the two systems. Both lexical and logicalmapping techniques produce mappings that cannot be generated by the other technique, suggestingthat the two techniques are complementary to each other. Finally, this work demonstrates interestingproperties (both lexical and logical) of HPO and SNOMED CT and illustrates some limitations of mappingthrough UMLS.Keywords: Partial mapping, Human phenotype, Ontology, Standard terminologies, Interoperability* Correspondence: olivier@nlm.nih.govNational Library of Medicine, National Institutes of Health, Bethesda, MD, USA© 2016 Dhombres and Bodenreider. Open Access This article is distributed under the terms of the Creative CommonsAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a linkto the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedicationwaiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwisestated.Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 DOI 10.1186/s13326-016-0047-3IntroductionIn parallel to the deep sequencing effort enabled by NextGeneration Sequencing technologies, a need for deepphenotyping has emerged [1]. Clinical phenotypes canbe recorded in reference to multiple terminologies, in-cluding the Human Phenotype Ontology (HPO), mainlyused for research purposes, and the StandardizedNomenclature of Medicine Clinical Terms (SNOMEDCT), mainly used in healthcare. The interoperability ofphenotypes between datasets (including electronic healthrecord data) annotated with different terminologies iscritical to translational research [2] and rests on theinteroperability between the corresponding terminolo-gies. For example, electronic health record (EHR) datacoded with SNOMED CT are increasingly used as aresource for cohort selection (e.g., for selecting patientsexhibiting a specific phenotype defined in reference toHPO). In this case, a mapping between SNOMED CTand HPO is key to bridging between datasets annotatedto different terminologies.The interoperability between HPO and SNOMED CTcan be addressed in several complementary ways, throughcomplete or partial mappings. Moreover, these two typesof mappings can be obtained lexically (through the lexicalproperties of phenotype names) or logically (through thelogical definitions and the hierarchical arrangement ofphenotype concepts).Complete lexical mappings identify exact and normal-ized matches between existing (pre-coordinated) termsin HPO and SNOMED CT and denote equivalent rela-tions between the corresponding concepts. In previouswork, we showed that only 30 % of HPO concepts couldmap to pre-coordinated SNOMED CT concepts [3]. Forexample, Multicystic dysplastic kidney [HP:0000003]maps to Multicystic renal dysplasia [SCTID:204962002](through synonymy).Complete logical mappings. Since both HPO andSNOMED CT are developed using description logics, it isbe possible to compare the logical definitions of pheno-type concepts between the two terminologies. However,given the differences in modeling choices in HPO andSNOMED CT, few matches would be expected. Instead,in previous work, we analyzed the logical definitions ofexisting phenotype concepts in SNOMED CT and createdpatterns (post-coordinated expressions) from these defi-nitions that could be applied to HPO phenotypes not rep-resented in SNOMED CT as pre-coordinated concepts.Through this approach, 1617 additional mappings couldbe identified between HPO and SNOMED CT [4]. Forexample, Aplastic clavicle [HP:0006660] would be equiva-lent to the following post-coordinated expression inSNOMED CT: Disease and (Role group some ((Associatedmorphology some Hypoplasia) and (Occurrence someCongenital) and (Finding site some Clavicle))).Partial lexical mappings identify matches similar tocomplete lexical mappings, but allow some words of theHPO terms to be omitted in the mapping to SNOMEDCT. Such mappings denote subsumption (subclass)relations between the more specific HPO concept andthe more general SNOMED CT concept mapped to.For example, Bilateral renal atrophy [HP:0012586]maps to the more general concept Atrophy of kidney[SCTID:197659005] (ignoring the modifier bilateral).Leveraging the compositional features of HPO terms formapping purposes had already been suggested by [5].Partial logical mappings identify a subclass relationbetween one fine-grained HPO concept and a more generalSNOMED CT concept, when an ancestor of the sourceHPO concept is equivalent to some SNOMED CT concept.For example, the concept Oral cleft [HP:0000202] is in sub-class relation to Abnormality of the mouth [HP:0000153] inHPO, and Abnormality of the mouth is equivalent to theSNOMED CT concept Congenital anomaly of mouth(disorder) [SCTID:128334002] through a complete lexicalmapping. Therefore, a partial logical mapping (denoting asubClassOf relationship) can be inferred between Oral cleft[HP:0000202] and Congenital anomaly of mouth (disorder)[SCTID:128334002].The objective of this paper is to investigate andcontrast lexical (based on lexico-syntactic propertiesof clinical phenotype terms) and logical (based onsubsumption relations between phenotype concepts)approaches to deriving partial mappings between HPOand SNOMED CT.BackgroundIn this section, we introduce the resources used in thisinvestigation (HPO, SNOMED CT and the UMLS). Webriefly review related work on partial mappings andpresent the specific contribution of our work.ResourcesHPO. The Human Phenotype Ontology (HPO) is anontology of phenotypic abnormalities developed collab-oratively and used for the annotation of databases such asOMIM (Online Mendelian inheritance in Man) andOrphanet (knowledge base about rare diseases) [6]. Theversion of HPO used in this investigation is the (stable)OWL version downloaded on January 21, 2015 (build#1337) from the HPO website (http://www.human-phenotype-ontology.org/). It contains 10,589 classes(concepts) and 16,807 names (terms) for phenotypes,including 6218 exact synonyms in addition to onepreferred term for each class.SNOMED CT is developed by the International HealthTerminology Standard Development Organization(IHTSDO) [7]. It is the worlds largest clinical termin-ology and provides broad coverage of clinical medicine,Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 2 of 13including diseases and phenotypes. SNOMED CT in-cludes pre-coordinated concepts (with their terms) andsupports post-coordination, i.e., the principled creationof expressions (logical definitions) for new concepts. TheU.S. edition of SNOMED CT dated March 2015 used inthis work includes about 300,000 active concepts, ofwhich 103,748 correspond to clinical findings.UMLS. The Unified Medical Language System (UMLS)is a terminology integration system developed by the U.S.National Library of Medicine [8]. The UMLS Metathe-saurus integrates many standard biomedical terminolo-gies, including SNOMED CT. Although the version ofUMLS available at the time of this investigation does notyet integrate HPO, it is expected to provide a reasonablecoverage of phenotypes through its source vocabularies.In the UMLS Metathesaurus, synonymous terms fromvarious sources are assigned the same concept uniqueidentifier, creating a mapping among these source vocabu-laries. Terminology services provided by the UMLSsupport the lexical mapping of terms to UMLS concepts.We used the 2015AA version of the UMLS.Related workOntology matchingThe general framework of this investigation is that ofontology matching. More specifically, we investigatedifferent mappings techniques between the classes oftwo medical ontologies. Considering the matching tech-niques classification of Euzenat et al. [9], our approachfalls under schema matching approaches, as it only relieson schema-level information. (Concepts in biomedicalterminologies and ontologies represent classes, while thecorresponding instances are found in EHR systems). Sev-eral techniques have been developed for schema matchingand these approaches can be combined [10, 11]. Mostrelevant to our work are matching techniques that lever-age the structural (i.e., the subsumption hierarchy of anontology) and the lexical (i.e., the terms used as labels forthe classes of an ontology) characteristics of the ontol-ogies [12]. Establishing equivalence mappings is themost common approach to making two ontologiesinteroperable. However, partial mappings can advanta-geously extend interoperability when one ontology isfiner-grained than the other [13].Most ontology matching techniques have been de-veloped for and applied to broad, ambiguous domains(e.g., the Semantic Web as a whole) and may not be asefficient when applied to specialized, less ambiguousdomains, such as biomedicine. For example, when theontologies to be matched cover different domains (e.g.,DBpedia), bootstrapping the mappings with unsuper-vised filters to delimit the target domain can improvethe quality of the resulting mappings [14]. However,while the improvement was significant for particularlyambiguous datasets, the domain filter did not improve(and could even decrease) the mapping quality forextremely specialized and unambiguous datasets, such asthe subdomain Pathological Function in the UMLS[14]. Along the same lines, the BLOOMS system is aninteresting solution for Linked Open Data (LOD) schemaalignment, but has not been evaluated on LOD datasetsfrom the life sciences domain [15].In the next paragraphs, we review some relevantrelated work conducted in the in the medical domain onpartial lexical mappings and partial logical mappings.Partial lexical mappingsParticularly relevant to this investigation where weattempt to find partial lexical mappings for HPOconcepts in SNOMED CT by removing some of modi-fiers that specialize phenotype terms in HPO is workdone on the compositional aspects of biomedical terms.Terminologies, such as the Gene Ontology, have beenshown to be highly compositional [16, 17] in that someof their more complex terms are derived from simplerterms by addition of modifiers. Moreover, it has beenreported that the compositional structure of GeneOntology terms impacts its usage [18] and can supportautomatic ontology extension [19]. Similarly, the com-positional structure of SNOMED terms has beenexploited for assessing the consistency of its hierarchicalstructure [20]. Recent work based on the compositional-ity of phenotype terms investigated skeletal abnormal-ities [21] and clinical phenotypes across species [22].However in the latter study, the Entity-Quality decom-position strategy yielded better results on the Mamma-lian Phenotype Ontology than on HPO. Also of interestis the work involving partial mappings by Mili?i? et al.[23] in the context of mapping the rare diseases of theOrphanet terminology to the UMLS. Partial lexical map-pings leveraging increasingly aggressive normalization ofOrphanet terms were used to rank candidate mappingsfor comprehensive expert curation.Partial logical mappingsWe are not using supervised machine learning approachesin order to discover new partial mappings, as was done in[13]. Instead, we use existing equivalence relations be-tween HPO and SNOMED CT and subsumption relationsasserted in HPO to infer partial logical mappings. Theresulting partial mappings denote a subclass relationbetween a fine-grained HPO concept and a more generalSNOMED CT concept. A similar approach was used in adifferent domain to map adverse drug events (ADEs)between SNOMED CT and MedDRA. In this investiga-tion, the fine-grained concepts in SNOMED CT weremapped to more general concepts in MedDRA throughpartial logical mappings [24].Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 3 of 13Specific contributionThe specific contribution of this work is not to proposenew mapping techniques. Rather, we leverage existingtechniques to extend the mapping of clinical pheno-types from HPO to SNOMED CT. More specifically,we leverage the lexico-syntactic properties of HPOterms and the logical structure of HPO to derive partialmappings. Moreover, we contrast the contribution oflexical and logical approaches to the development ofpartial mappings.MethodsOur investigation of partial mapping can be summa-rized as follows. We extracted phenotype concepts(along with their terms) from HPO and SNOMEDCT. We identified complete lexical mappings betweenthe two resources. We leveraged the lexico-syntacticproperties of phenotype terms to derived partial lexicalmappings, and the subsumption hierarchy of phenotypeconcepts to derive partial logical mappings. Finally, weanalyzed the specific contribution of each approach andevaluated the quality of the partial mappings throughmanual review.Extracting phenotypes termsFrom HPO, we selected the concept Phenotypic abnormal-ity [HP:0000118] and all its descendants with their corre-sponding terms (preferred terms and synonyms). In orderto restrict SNOMED CT to phenotypes and disorders, weselected the concept Clinical Findings [SCTID:404684003]and all its descendants, along with their terms (referred toas descriptions in SNOMED CT).Identifying complete lexical mappingsAlthough the focus of this investigation is on partialmappings, we rely on complete lexical mappings (denot-ing equivalence relations) for two reasons. Partial map-pings are primarily useful for those concepts for whichno complete mapping exists, and the complete lexicalmappings are key to identifying partial logical mappings.To identify equivalent mappings between HPO andSNOMED CT concepts, we mapped each originalphenotype term (preferred term or synonym) fromHPO to the clinical findings of SNOMED CT lexicallythrough UMLS synonymy, as previously described in[3]. For example, the HPO concept Abnormality of themouth [HP:0000153] has a complete lexical mappingto the SNOMED CT concept Congenital anomaly ofmouth (disorder) [SCTID:128334002], as indicated bythe UMLS Concept Mouth Abnormalities [C0026633]in which Abnormality of the mouth and Congenitalanomaly of mouth (disorder) are synonyms. (Theissue of congenitality will be addressed in the Dis-cussion section.)Deriving partial lexical mappingsTo derive partial lexical mappings, we identified mo-difiers in phenotype terms (through lexico-syntacticanalysis), and we performed increasingly aggressivedemodification of HPO terms until the demodified HPOterms could be mapped to SNOMED CT (Fig. 1).Identifying modifiers through lexico-syntactic analysisIn order to identify modifiers in HPO terms (preferredterms and synonyms), we performed a lexico-syntacticanalysis (shallow parsing) of these terms using theminimal commitment parser available as part ofnatural language processing tool SemRep [25]. Forexample, the HPO term Bilateral renal atrophy[HP:0012586] is analyzed as two adjectival modifiers,Bilateral and renal, followed by the head nounatrophy. Its lexico-syntactic profile would therefore berecorded as [MOD-MOD-HEAD].More specifically, we focused on terms with a[MOD]*[HEAD] profile (i.e., one or more adjectival ornoun modifiers followed by a head noun). We also con-sidered terms containing one prepositional attachment,in which we treated each element of the prepositionalphrase as a modifier (of the main head noun) for thepurpose of this analysis. Complex terms with multipleprepositional attachments were ignored, because theiranalysis requires more sophisticated parsing techniques.Demodifying phenotype termsSince our intuition is that modifiers in specialized HPOterms prevent mapping to the more general terms found inSNOMED CT, we attempted to remove the modifiers iden-tified in HPO terms through lexico-syntactic analysis andto map the demodified terms to SNOMED CT through theUMLS, thereby creating a partial lexical mapping of theoriginal HPO term to SNOMED CT. In practice, we itera-tively removed all combinations of modifiers from anoriginal HPO term (preferred term or synonym), in increas-ing order of aggressiveness, i.e., first removing one modifierat the time, then, two modifiers, etc. until only the headnoun remained. For example, after removing the modifierbilateral from the HPO term Bilateral renal atrophy[HP:0012586], the demodified term renal atrophy mappedto SNOMED CT through the UMLS. Note that from thisterm, where the head noun atrophy is modified by bilateraland renal, we generated the following three demodifiedterms. By removing one modifier (level-1), we obtainedbilateral atrophy and renal atrophy. After removing bothmodifiers (level-2), we generated atrophy. As an exampleof term with a prepositional attachment, Congenital ab-sence of uvula [HP:0010292] has for lexico-syntactic profile[MOD HEAD][PREP HEAD]. Except for the head noun ofthe main noun phrase (absence), all the other lexical itemsare treated as modifiers (congenital, of, and uvula).Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 4 of 13Mapping demodified terms through UMLSWe attempted a complete lexical mapping of thedemodified HPO terms to SNOMED CT through theUMLS, as was done for the original HPO terms in[3]. Note that the complete mapping of a demodifiedterm corresponds to the partial mapping of theoriginal term prior to demodification. In order toselect the closest mappings, we only recorded themapping for the less demodified term(s). For ex-ample, there is no complete mapping to SNOMEDCT for Bilateral renal atrophy [HP:0012586], but alevel-1 partial mapping is found to Atrophy ofkidney [SCTID:197659005] after removing one modi-fier, bilateral.Deriving partial logical mappingsTo derive partial logical mappings, we mapped HPOconcepts to equivalent SNOMED CT concepts and weinferred partial logical mappings from the subsumptionrelations of HPO (Fig. 2).Most HPO concepts have no complete lexical map-ping (i.e., no equivalence relation) to SNOMED CT.For these concepts, we attempted a partial logicalmapping. In practice, when an equivalent mapping toSNOMED CT was found among the ancestors of agiven HPO concept, we inferred a partial logical map-ping between this HPO concept and the SNOMEDCT concept(s) equivalent to its ancestor. More specif-ically, if several ancestors of the HPO concepts haveequivalence relations to SNOMED CT, we only recordas partial logical mappings those ancestors that arethe closest to the source HPO concept.For example, the HPO concept Oral cleft [HP:0000202]has no complete lexical mapping in SNOMED CT. Thisconcept is a subclass of Abnormality of the mouth[HP:0000153], which has an equivalent relation tothe concept Congenital anomaly of mouth (disorder)[128334002] in SNOMED CT. Therefore, a partiallogical mapping denoting a subclass relation is inferredbetween Oral cleft [HP:0000202] and Congenital anomalyof mouth (disorder) [128334002]. This logical mapping isdeemed level-1 because it is based on an equivalentmapping of a direct ancestor (i.e., parent concept). In thecase of Short upper lip [HP:0000188], the resulting partiallogical mapping was deemed level-3 because its closestancestor achieving a complete mapping was threelevels above the source HPO concept (Short upper lip[HP:0000188] is a subclass of Abnormality of upperlip [HP:0000177], which is a subclass of Abnormalityof the lip [HP:0000159], which is a subclass of Abnor-mality of the mouth [HP:0000153]).EvaluationQuantitative evaluationWe quantified the number of complete lexical mappingsand the number of partial mappings (lexical partialmappings and logical partial mappings) between HPOconcepts and SNOMED CT concepts. The analysis wasstratified by level of demodification for the partial lexicalmappings and by level of subsumption for the partiallogical mappings. Then we analyzed the overlap betweenpartial lexical and logical mappings, as well as thecombined coverage of HPO concepts provided by bothtypes of partial mappings.SNOMED CTClinical findingtermsLexico-syntactic profiles for HPODemodified HPO termsSemRepHPOPhenotypetermsPartial lexical mappingthrough UMLSComplete lexical mappingthrough UMLSremove modifiersFig. 1 Identifying partial lexical mappings between HPO and SNOMED CTDhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 5 of 13Qualitative evaluationWe evaluated the quality of the partial mappings bymanual review of a random subset of 10 % of the partiallexical mappings. Additionally, we evaluated a sample ofthe partial logical mappings consisting of 25 mappings perlevel in the subsumption hierarchy. One of the authors(FD), a physician, tagged the partial mappings as onto-logically valid if they were consistent with a subclass rela-tion. For example, the mapping of Bilateral renal atrophy[HP:0012586] to Atrophy of kidney [SCTID:197659005] isontologically valid. In contrast, the mapping of Abnor-mality of the paranasal sinuses [HP:0000245] to Con-genital malformation (disorder) [SCTID:276654001] isnot ontologically valid, because some subclasses ofAbnormality of the paranasal sinuses (e.g., Sinusitis[HP:0000246]) are obviously not necessarily of con-genital origin. (We will come back to this issue in theDiscussion section).Additionally, ontologically valid mappings were evalu-ated for clinical relevance from the perspective of cohortselection. In practice, the mappings were tagged asclinically relevant if they were clinically useful forbuilding a cohort of patients exhibiting a particularphenotype, i.e., for selecting medical records describingthe clinical phenotypes of such patients. For example,the mapping of Bilateral renal atrophy [HP:0012586] toAtrophy of kidney [SCTID:197659005] is deemed clinic-ally useful, because it would be relatively easy to selectpatients with Bilateral renal atrophy from patients withAtrophy of kidney. In contrast, the mapping of Abnormalrespiratory motile cilium morphology [HP:0005938] toMorphologic finding [SCTID:72724002] is not deemedclinically useful, because few patient records annotatedwith Morphologic finding would actually correspond tocases of Abnormal respiratory motile cilium morphology.In other words, this metric of clinical relevance attemptsto assess whether the partial mappings are closeenough for a specific use case, here cohort selection.ResultsIn this section, we present the results for each step ofour approach to establishing partial lexical and logicalmappings. We also provide an extended example toillustrate our mapping approach.Extracting phenotypes termsFrom HPO, we selected 10,454 concepts specificallyrepresenting phenotypic abnormalities (10,454 preferredterms and 6158 synonyms). From SNOMED CT, weselected 103,748 concepts for clinical findings (103,748fully specified names and 167,491 synonyms).Identifying complete lexical mappingsOf the 10,454 phenotype concepts in HPO, we identifieda complete lexical mapping to clinical findings inSNOMED CT for (at least one term of the) 3096 HPOconcepts (30 %). This proportion is consistent with ourprior findings ([3]). We used the remaining 7358concepts (10,631 terms) for identifying partial mappingslexically and logically.Deriving partial lexical mappingsIdentifying modifiers through lexico-syntactic analysisThe lexico-syntactic analysis of the 10,631 HPO termsproduced 494 distinct lexico-syntactic profiles, the mostfrequent of which being [MOD-HEAD] (23 %). The listof the 10 most frequent lexico-syntactic profiles(accounting for 65 % of the HPO terms) is shown inTable 1. A total of 6959 HPO terms had lexico-syntacticprofiles amenable to demodification, corresponding to35 distinct lexico-syntactic profiles. Of note, 218 HPOterms consisting of a single head noun ([HEAD]), were ofcourse not amenable to demodification. The remaining3454 HPO terms are complex terms and were not con-sidered for demodification.A total of 2864 distinct modifiers extracted from theseHPO terms were associated with 1838 distinct headHPOPhenotypeconceptHPOPhenotypeancestor conceptPartial logical mappingComplete lexical mappingthrough UMLSSNOMED CTClinical findingconceptSubclassofrelation in HPOFig. 2 Identifying partial logical mappings between HPO and SNOMED CTDhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 6 of 13nouns. The number of modifiers per term ranged from 1to 8 (median = 2). The most frequent head nouns wereabnormality, hypoplasia, epiphyses, ossification, atrophy,phalanx, aplasia, phalanges, EEG and sclerosis. Exclud-ing prepositions, the most frequent modifiers wereabnormal, increased, absent, hypoplastic and decreased.Demodifying phenotype termsThe demodification process resulted in the creation of23,936 demodified terms from the 6959 original terms.Mapping demodified terms through UMLSOf the 7358 HPO concepts with no complete mapping toSNOMED CT, we identified a partial lexical mapping for(at least one term of the) 2464 HPO concepts (33 %). Amajority of the partial mappings occurred at level 1 (i.e.,after removing a single modifier). An analysis of the lowestlevel at which the mapping occurred is presented in Fig. 3.Among the modifiers, metabolism, progressive, recurrent,generalized, abnormal, bilateral, morphology, distal, unilat-eral, epiphysis and congenital are the most frequentlyremoved when a mapping was found. The most frequentprofiles involved in these mappings were [MOD-HEAD](e.g., Fasciculiform cataract [HP:0010926]), [MOD-MOD-HEAD] (e.g., Bilateral renal atrophy [HP:0012586]),[HEAD][PREP-DET-HEAD] (e.g., Osteosclerosis of the clav-icle [HP:0100923]), and [HEAD][PREP-MOD-HEAD] (e.g.,Abnormality of glutamine metabolism [HP:0010903]).Deriving partial logical mappingsOf the 7358 HPO concepts with no complete map-ping to SNOMED CT, we inferred a partial logicalmapping for 6009 HPO concepts (82 %). The partiallogical mappings were distributed across 10 levels ofsubsumption. The first level represented 2106 (35 %)of the partial logical mappings, and the first 4 levelsrepresented 5197 (86 %) of all the partial logicalmappings (Fig. 4).EvaluationQuantitative evaluationOf the 10,454 phenotype concepts in HPO, we identi-fied complete mappings for 3096 (30 %), partial lex-ical mappings for 2464 (24 %), and partial logicalmappings for 6009 (57 %). As shown in Fig. 5, weidentified partial mappings, lexical or logical, for 6474HPO concepts (62 %).Qualitative evaluationIn our randomly selected evaluation subset of 247 partiallexical mappings, 62 % were ontologically valid and 49 %were both ontologically valid and clinically relevant. Asshown in Table 2, the quality of these mappings is higherfor the first level of demodification.Of the 125 logical mappings randomly selected amongconcepts with no lexical partial mappings, 71 % wereontologically valid and 67 % were both ontologicallyvalid and clinically relevant. As shown in Table 3, thequality of the mappings is relatively consistent across thefirst 4 levels of logical mappings.Extended exampleTo illustrate the main steps of our partial mappingapproach, we consider the HPO concept Recurrent bron-chitis [HP:0002837], for which there is no completelexical mapping to SNOMED CT.Partial lexical mappingThe lexico-syntactic profile of this term is [MOD-HEAD],in which the head noun bronchitis is modified by theadjective Recurrent. We demodified this term by removingits sole modifier, Recurrent, resulting in the bare headnoun, bronchitis. According to the UMLS, bronchitis isequivalent to three SNOMED CT concepts, Bronchitis(disorder) [SCTID:32398004], Acute bronchitis (dis-order) [SCTID:10509002], and Acute tracheobronchitis(disorder) [SCTID:35301006]. Therefore, we identifiedTable 1 Most frequent lexico-syntactic profiles of the 10,631 HPO terms not involved in a complete lexical mappingLexico-syntactic profile Terms (%) Examples of HPO terms[MODHEAD] 2478 (23 %) Oral cleft, Aplastic clavicles, Abnormal philtrum[MODMODHEAD] 1811 (17 %) Asymmetric limb shortening, Multicystic kidney dysplasia[HEAD] [PREPDETHEAD] 536 (5 %) Abnormality of the philtrum, Polydactyly of the foot[MODMODMODHEAD] 478 (4 %) Small proximal femoral epiphyses, Increased cup disc ratio[HEAD] [PREPMODHEAD] 386 (4 %) Delay in motor development, Abnormality of renal excretion[MODHEAD] [PREPHEAD] 321 (3 %) Hypertensive disorder of pregnancy, Coronal cleft of vertebrae[HEAD] [PREPHEAD] 259 (2 %) Abnormality of upper lip, Tremor at rest, Tetralogy of Fallot[HEAD] 218 (2 %) Gastroschisis, Polydactyly, Pre-eclampsia[HEAD] [PREPDETMODHEAD] 209 (2 %) Abnormality of the paralabial region, Fragmentation of the metacarpal epiphyses[MODHEAD] [PREPDETHEAD] 202 (2 %) Downturned corners of the mouth, IgA deposition in the glomerulustop 10 6898 (65 %)Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 7 of 13Fig. 4 Complete and partial logical mappings between HPO and SNOMED CTFig. 3 Complete and partial lexical mappings between HPO and SNOMED CTDhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 8 of 13a level-1 partial lexical mapping for Recurrent bron-chitis [HP:0002837] to three target concepts inSNOMED CT.Partial logical mappingThe concept Recurrent bronchitis [HP:0002837] hasthree direct ancestors in the subsumption hierarchy ofHPO, Abnormality of the bronchi [HP:0002109], Bron-chitis [HP:0012387] and Recurrent upper respiratorytract infections [HP:0002788]. According to the UMLS,the concept Abnormality of the bronchi [HP:0002109]has no equivalent in SNOMED CT. The conceptBronchitis [HP:0012387] is equivalent to the same threeconcepts identified as a mapping for the demodifiedterm bronchitis. Finally, the concept Recurrent upperrespiratory tract infections [HP:0002788] is equivalent totwo SNOMED CT concepts: Upper respiratory infection(disorder) [SCTID:54150009] and Recurrent upper re-spiratory tract infection (disorder) [SCTID:195708003].Therefore, we inferred a partial logical mapping for Re-current bronchitis [HP:0002837] to five target SNOMEDCT concepts, three from Bronchitis [HP:0012387] andtwo from Recurrent upper respiratory tract infections[HP:0002788]. Of note, since a partial mapping wasfound through a direct ancestor of Recurrent bronchitis[HP:0002837], we did not explore its more distantancestors.OverallA partial mapping to SNOMED CT can be derived forthe HPO concept Recurrent bronchitis [HP:0002837]both lexically and logically, at the first level (of demodifi-cation or subsumption) in both cases. Moreover, all thetarget concepts from the lexical mapping were also iden-tified by the logical mapping, which also identified twoadditional target concepts.DiscussionEnhanced mapping of phenotype concepts between HPOand SNOMED CTIn addition to the 30 % of HPO concepts that can bemapped to SNOMED CT through complete lexical map-ping (through UMLS), we assessed that 62 % of all HPOconcepts have a partial lexical or logical mapping toSNOMED CT, bringing to 92 % the proportion of HPOconcepts mapped to SNOMED CT with an equivalent orsubclass relation (Fig. 5). Partial mapping techniques signifi-cantly increase the rate of mapping for phenotype conceptsbetween HPO and SNOMED CT, which confirms our intu-ition that HPO concepts tend to be more specialized thanphenotype concepts in SNOMED CT, where they can oftenbe mapped to more general phenotype concepts.Relative contribution of the partial lexical and logicalmapping approachesOverallUnsurprisingly, the partial logical mapping approach isfar more productive that the partial lexical mappingapproach. More specifically, of the 7358 HPO conceptswith no complete mapping to SNOMED CT, the propor-tion of partial mappings obtained is 82 % for the logicalapproach vs. 33 % for the lexical approach.By levelLexical and logical mappings also differ in the level atwhich the mapping occurs. A majority of the partialTable 2 Qualitative evaluation of the partial lexical mappingsontologically valid mappings clinically relevant mappingslevel yes no total (in proportion of the ontologically valid mappings)1 130 68 % 60 32 % 25 103 54 %2 18 40 % 27 60 % 25 14 31 %3+ 5 42 % 7 58 % 25 4 33 %all 153 62 % 94 38 % 125 121 49 %Table 3 Qualitative evaluation of the partial logical mappings, with no lexical mappingontologically valid mappings clinically relevant mappingslevel yes no total (in proportion of the ontologically valid mappings)1 22 88 % 3 12 % 25 20 80 %2 19 76 % 6 24 % 25 17 68 %3 15 60 % 10 40 % 25 15 60 %4 18 72 % 7 28 % 25 17 68 %5+ 15 60 % 10 40 % 25 15 60 %all 89 71 % 36 29 % 125 84 67 %Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 9 of 13lexical mappings (95 %) occur after removing one ortwo modifiers (Fig. 3), while the partial logical map-pings are distributed across a larger number of levelsof subsumption (Fig. 4), with only 54 % of the map-pings occurring over the first two levels. Althoughthe levels for the lexical approach (i.e., number ofmodifiers removed) and for the logical approach (i.e.,number of edges in the concept hierarchy) cannot bedirectly compared, this difference indicates that thelexical mappings are generally closer in meaning tothe source HPO concept compared to the logicalmappings.Overlap between partial lexical and logical mappingsThe overlap between the lexical and logical approachesto partial mapping is limited. As shown in Fig. 5, of the6474 HPO concepts for which a partial mapping toSNOMED CT was identified, 1999 (31 %) were commonto both approaches. In other words, the lexical approachonly generated 456 mappings (7 %) that could not bederived logically.For example, Severe periodontitis [HP:0000166] maps toPeriodontitis (disorder) [SCTID:41565005] both lexically (atlevel 1) and logically (also at level 1). In contrast, VitaminB8 deficiency [HP:0100506] maps to Vitamin deficiency(disorder) [SCTID:85670002] only through lexical mapping,and Small face [HP:0000274] maps to Dysmorphic facies(finding) [SCTID:248200007] only through logical mapping.Of note, the overlapping partial mappings identifiedthrough lexical and logical approaches for a given sourceHPO concept are not always the same. For example,Median cleft lip [HP:0000161] maps to Cleft lip (disorder)[SCTID:80281008] lexically (at level 1) and to Congenitalanomaly of mouth (disorder) [SCTID:128334002] logically(at level 3). As suggested by its closest proximity, thelexical mapping is more meaningful. One strategy forselecting between lexical and logical mappings for a givenHPO concept when the mappings are different would beto give precedence to the mapping with the lowest level. Adetailed comparison of the levels at which the map-pings occur between the lexical and logical approachesis presented in Table 4.Fig. 5 Partial logical mappings between HPO and SNOMED CTDhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 10 of 13Qualitative aspectsAs mentioned earlier, the quality of the partial logicalmappings tends to be higher than that of the partiallexical mappings (71 % vs. 62 % for ontological validityand 67 % vs. 49 % for clinical relevance).Failure analysisWe investigated some of the cases where no partialmappings could be found and present the main reasonsfor failure.Lexical partial mappingsReasons for failure to derive a partial lexical mappinginclude terms with a head noun outside the domain ofdisorders, complex lexico-syntactic patterns not proc-essed in this investigation, and complex lexical itemsidentified as HEAD. Head noun outside the domain of disorders. Forexample, the HPO concept Hypoplastic sacrum[HP:0004590] is demodified to sacrum, for whichcannot find a mapping to phenotypes inSNOMED CT, because sacrum is an anatomicalentity. (In previous work, we have addressed thisissue through the creation of post-coordinatedexpression [4].) Complex lexico-syntactic patterns. For example,Complete duplication of the proximal phalanx ofthe 5th toe [HP:0100415] has for lexico-syntacticpattern [MOD-HEAD][PREP-DET-MOD-HEAD][PREP-DET-MOD-HEAD]. We ignored nounphrases with multiple prepositional attachmentsfrom our processing and were therefore unableto identify a partial lexical mapping for thisconcept. Complex lexical items identified as HEAD. Forexample, Pyruvate dehydrogenase complex deficiency[HP:0002928] is a complex lexical item, whichprevents it from being demodified.Logical partial mappingsThe main reasons for failure to derive a partial logicalmapping is that none of the ancestors of the HPOsource concept have an equivalent mapping toSNOMED CT through the UMLS. For example, none ofthe 10 ancestors of the HPO concept Absent sternal ossi-fication [HP:0006628] has an equivalence to SNOMEDCT. The limitations of the UMLS as a source of eq-uivalence mappings between HPO and SNOMED CTdirectly impact our partial logical mapping approach,albeit in a relatively small way, since a partial logicalmapping can be derived for 82 % of the HPO concepts(for which there is no equivalent mapping).Impact of implicit congenitality on the quality of thepartial mappingsCongenitality tends to be expressed explicitly in SNOMEDCT concepts, while it is often implicit in HPO concepts.For example, the HPO concept Renal hypoplasia[HP:0000089] is equivalent to Congenital hypoplasiaof kidney (disorder) [SCTID:32659003] in SNOMEDCT according to the UMLS. Here, congenitality is impliedin HPO, because hypoplasia is always a congenital condi-tion. In other cases, however, an HPO concept withoutmention of congenitality is mapped to a SNOMED CTconcept with explicit mention of congenitality through theUMLS. For example, according to the UMLS, Abnormal-ity of the mouth [HP:0000153] is equivalent to Congenitalanomaly of mouth (disorder) [SCTID:128334002], whichis not always true since not all mouth conditionsoccur congenitally. The conflation between congenitaland non-congenital (or not-always-congenital) entitieswithin the same UMLS concept can lead to incorrectpartial mappings.Partial lexical mappingsAs mentioned earlier, the mapping of Abnormality of theparanasal sinuses [HP:0000245] to Congenital malforma-tion (disorder) [SCTID:276654001] is inaccurate, becauseTable 4 Comparison of the level of the partial mappings in the lexical and logical approachesPartial logical mapping No logicallevel 1 level 2 level 3 level 4 level 5 level? 6 mapping totallevel 1 1041 314 135 77 36 29 263 1895Partial level 2 137 55 58 23 9 3 163 448lexical level 3 13 20 8 17 5 0 34 97mappings level 4 4 6 2 3 0 0 5 20level 5 0 0 1 1 0 0 0 2level? 6 0 0 0 2 0 0 0 2No lexical mapping 911 729 809 831 582 148 884 4894total 2106 1124 1013 954 632 180 1349 7358Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 11 of 13Sinusitis [HP:0000246], a subclass of Abnormality of theparanasal sinuses, is not necessarily of congenital origin.The problem here is the equivalence provided by theUMLS between anomaly and Congenital malformation(disorder) through the UMLS concept Congenital Abnor-mality [UMLS:C0000768].Partial logical mappingsThe mapping of Abnormal calcification of the carpalbones [HP:0009164] to Congenital anomaly of the hand(disorder) [SCTID:34111000] is inaccurate, because somecalcifications can be acquired. The problem here is theequivalence provided by the UMLS between Abnormalityof the hand, an ancestor of Abnormal calcification of thecarpal bones, and Congenital anomaly of the hand(disorder) [SCTID:34111000] through the UMLS conceptCongenital Hand Deformities [UMLS:C0018566].ImpactThe mapping of HPO concepts without mention ofcongenitality to SNOMED CT concepts with mention ofcongenitality is the main raison for creating partiallogical mappings that are not ontologically valid. Sincemany HPO terms are demodified to the head nounAbnormality (mapped to Congenital malformation), thisissue also has a profound impact on the quality of thepartial lexical mappings. Furthermore, we estimated thatthe partial mappings would gain in clinical relevance(+11 % for partial lexical mappings and +2 % forpartial logical mappings) if the issue of congenitalitywas addressed. This issue is of particular importanceat a time when HPO intends to represent phe-notypes not only for genetic diseases, but also forcommon diseases [26].Limitations and future workOne of the limitations of this work is that the mappingswere investigated from the perspective of the source(HPO) rather than the target (SNOMED CT). Morespecifically, we report results in terms of proportion ofthe HPO concepts mapped to SNOMED CT withoutinvestigating the SNOMED CT concepts mapped to orthe mappings themselves (i.e., the HPO-SNOMED CTconcept pairs). Investigating the perspective of the targetwas beyond the scope of this work, but should be theobject of future research.Our partial lexical mapping approach only considers alimited number of lexico-syntactic profiles for the gener-ation of demodified terms. Moreover, some of the lexicalitems characterized as HEAD by our shallow parseractually correspond to complex items, some of whichcould be amenable to demodification (e.g., corticalcataract from the HPO concept Posterior cortical cata-ract [HP:0010924] is identified as a single lexical item,but could be decomposed into the modifier cortical andthe head noun cataract). However, further refinement ofthe lexical processes is unlikely to dramatically increasethe performance of the partial lexical mapping approach.The equivalence between HPO and SNOMED CTconcepts derived through the UMLS is a key componentof our partial logical approach. While SNOMED CT isfully integrated in the UMLS, HPO was not at the timeof this investigation and we had to rely on the lexicaltools provided by the UMLS to derive this mapping.HPO is now integrated in the UMLS (as of version2015AB) and this curated mapping is likely to providebetter equivalences between HPO and SNOMED CTconcepts, which will be highly beneficial to our partiallogical mapping approach.ConclusionsThrough complete and partial mappings, 92 % of the10,454 HPO concepts can be mapped to SNOMEDCT (30 % complete and 62 % partial). Equivalencemappings between HPO and SNOMED CT allow forinteroperability between data described using thesetwo systems. However, due to differences in focusand granularity, equivalence is only possible for 30 %of HPO classes. In the remaining cases, partial map-pings provide a next-best approach for traversingbetween the two systems. Both lexical and logicalmapping techniques produce mappings that cannotbe generated by the other technique, suggested thatthe two techniques are complementary to each other. Theclinical relevance of the partial mappings (for a cohortselection use case) is 49 % for lexical mappings and 67 %for logical mappings. Finally, this work demonstratesinteresting properties (both lexical and logical) of HPOand SNOMED CT and illustrates some limitations ofmapping through UMLS.AbbreviationsHPO: Human Phenotype Ontology; UMLS: Unified Medical Language System;EHR: Electronic health records; LOD: Linked open data.Competing interestsThe authors declare that they have no competing interests.Authors contributionsThe two authors conceived and designed the study, performed theexperiments and analyzed and interpreted the results. All authorscontributed to the redaction of the manuscript. All authors read andapproved the final manuscript.AcknowledgmentsThis work was supported in part by the Intramural Research Program of theNIH, National Library of Medicine, the French Gynecology and ObstetricsAssociation (Collège National des Gynécologues et Obstétriciens Français), andthe Philippe Foundation.Received: 4 November 2015 Accepted: 2 February 2016Dhombres and Bodenreider Journal of Biomedical Semantics  (2016) 7:3 Page 12 of 13RESEARCH Open AccessThe environment ontology in 2016:bridging domains with increased scope,semantic density, and interoperationPier Luigi Buttigieg1*, Evangelos Pafilis2, Suzanna E. Lewis3, Mark P. Schildhauer4, Ramona L. Walls5and Christopher J. Mungall3AbstractBackground: The Environment Ontology (ENVO; http://www.environmentontology.org/), first described in 2013, is aresource and research target for the semantically controlled description of environmental entities. The ontology'sinitial aim was the representation of the biomes, environmental features, and environmental materials pertinent togenomic and microbiome-related investigations. However, the need for environmental semantics is common to amultitude of fields, and ENVO's use has steadily grown since its initial description. We have thus expanded,enhanced, and generalised the ontology to support its increasingly diverse applications.Methods: We have updated our development suite to promote expressivity, consistency, and speed: we nowdevelop ENVO in the Web Ontology Language (OWL) and employ templating methods to accelerate class creation.We have also taken steps to better align ENVO with the Open Biological and Biomedical Ontologies (OBO) Foundryprinciples and interoperate with existing OBO ontologies. Further, we applied text-mining approaches to extracthabitat information from the Encyclopedia of Life and automatically create experimental habitat classes within ENVO.Results: Relative to its state in 2013, ENVO's content, scope, and implementation have been enhanced and muchof its existing content revised for improved semantic representation. ENVO now offers representations of habitats,environmental processes, anthropogenic environments, and entities relevant to environmental health initiativesand the global Sustainable Development Agenda for 2030. Several branches of ENVO have been used to incubateand seed new ontologies in previously unrepresented domains such as food and agronomy. The current releaseversion of the ontology, in OWL format, is available at http://purl.obolibrary.org/obo/envo.owl.Conclusions: ENVO has been shaped into an ontology which bridges multiple domains including biomedicine,natural and anthropogenic ecology, omics, and socioeconomic development. Through continued interactionswith our users and partners, particularly those performing data archiving and sythesis, we anticipate that ENVOsgrowth will accelerate in 2017. As always, we invite further contributions and collaboration to advance thesemantic representation of the environment, ranging from geographic features and environmental materials,across habitats and ecosystems, to everyday objects in household settings.Keywords: Environmental semantics, Habitat, Ecosystem, Ontology, Anthropogenic environment, Indoorenvironment, Sustainable development* Correspondence: pier.buttigieg@awi.de1Alfred Wegener Institut, Helmholtz Zentrum für Polar- undMeeresforschung, Am Handelshafen 12, 27570 Bremerhaven, GermanyFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 DOI 10.1186/s13326-016-0097-6BackgroundAn environment includes the natural or anthropogenicsystems which can surround a living or non-living entity.This broad definition encompasses an enormous diver-sity of entities and scales, thus presenting numerouschallenges for constructing ontologies and standards.Previously, we described the Environment Ontology(ENVO; [1]), a community-driven project which repre-sents environmental entities including biomes, environ-mental features, and environmental materials. At thattime, our focus was primarily on representing the envi-ronments associated with metagenomic samples: ourgoal was to provide a vocabulary with which to charac-terise sequenced environmental samples, together withan ontological structure to facilitate search, advancedquerying, and inference in support of the aims of theGenomics Standards Consortium (GSC; [2]). This pre-vious version of the ontology contained a variety ofclasses for describing a sample along three primaryaxes: the biome or ecosystem within which an entity ofinterest (usually an organism or community) is embed-ded; the environmental features that are in the vicinityof and have a strong causal influence on the entity; andthe environmental material that is the substance sur-rounding or partially surrounding the entity. Althoughthe use case is primarily microbial, the approach canencompass larger organisms  for example, a killerwhale in a neritic epipelagic zone biome, present in anecosystem defined by a marine subtidal rocky reef, andsurrounded by coastal water. We also described thedynamic nature of the ontology, and the process forcommunity extension of the ontology.New challengesIn the time since our initial publication, we have orientedENVOs development to a suite of emerging challengesextending our original and core case of describing samplesof environmental and biomedical importance (e.g. [35]).On the one hand, sequencing projects are targeting evermore diverse environments such as city transit systems[6] and also phenomena such as soil compaction in for-est ecosystems [7]. This has driven new requests fromadopters such as MG-RAST [8] and the iMicrobe pro-ject (http://imicrobe.us/) which has annotated some2813 environmental metagenomic samples with ENVOterms (see http://data.imicrobe.us/ and [9]). On the otherhand, we have encountered a number of entirely new usecases in areas such as ecology and biodiversity science.Both of these fronts have, at times, required the expan-sion of existing branches in the ontology and, at others,required the creation of either entirely new branches,or the refactoring of existing branches. This increase inscope also presented challenges and opportunities interms of how the ontology should be interwoven withother ontologies in the OBO Foundry and Library(http://obofoundry.org/) [10].In this update, we describe how we have extendedand in some cases broken apart ENVO to meet theabove challenges. We also describe how these effortshave connected ENVO to a broader movement tofurther extend OBO-aligned semantics into the realmof ecology and biodiversity science [1113], centred onco-development with ecologically themed ontologiessuch as the Population and Community Ontology(PCO) and Bio-collections Ontology (BCO) [14]. Theseefforts have been catalysed by several workshops andmeetings e.g. [4] which have greatly supported ENVOin contending with entities such as habitats, environ-mental processes, and environmental dispositions whileorienting its content to address issues of globalimportance.Expanding usage and coordinationAlong with its scope, the use of ENVO is also growingand supporting data annotation, searching of datasets,and the mobilisation of sample data. For example, thejournal Scientific Data (Nature Publishing Group; ISSN2052?4463) now uses ENVO classes to annotate its DataDescriptor articles [15], allowing articles to be browsed withfaceted interfaces (http://scientificdata.isa-explorer.org), andPANGAEA, a data publisher for Earth and environmen-tal science, is continuing to use the ontology to enrichits metadata and data archives (http://www.pangaea.de).Parallel efforts such as those convened by the GlobalBiodiversity Information Facility (GBIF) have moved toenhance the widely used Darwin Core (DwC; http://rs.tdwg.org/dwc/; [16]) glossary by using ENVO in habi-tat descriptions [17]. Other users have begun to exploreENVOs potential in data analysis [18] and in contrib-uting to semantically aware biodiversity informatics(e.g. [19, 20]). Further, synthesis centres such as theNational Centre for Ecological Analysis and Synthesis(NCEAS; Santa Barbara, USA; http://nceas.ucsb.edu/) andthe Centre de synthèse et danalyse sur la biodiversité(CESAB; Aix-en-Provence, France; http://cesab.org/) haveengaged with us to explore further possibilities for usageand provide advice on coordination and community needslinked to projects such as the Data Observation Networkfor Earth (DataONE; www.dataone.org). Indeed, it is thediverse needs of these communities, as well as those ofmore recent partners (see Results and Discussion), whichhave compelled ENVO to develop with generalisabilityand versatility in mind, as is appropriate for a domainor reference ontology. Representations of microscaleenvironments co-exist and interoperate with those ofplanetary-scale systems and are being further harmo-nised as the ontology grows in scope.Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 2 of 12An overview of this updateBelow, we describe the updates made to improveENVOs ability to maintain coherence while meeting theneeds of its diversifying user base and implementationpartners. Our Methods section describes key technicalupdates while our Results focus on content-level changes.The first section of our results describes ENVOs in-creased expressivity, acquired through transitioning to amore powerful development language. The second sectiondescribes the addition of processes to ENVOs content,which has widened ENVOs range of application andenriched the relationships between its classes. Building onits updated expressivity, the third section describes howENVO distinguishes between environments and habitatsand how thousands of habitats linked to species de-scriptions have been represented using text-mining ap-proaches. Departing from the natural setting, the fourthand fifth sections describes the increased efforts madein representing anthropogenic or anthropised environ-ments and how these changes relate to the monitoringof policy objectives and global development. Finally, wecomment on how ENVO intends to handle its rapidlygrowing scope while maintaining expert-guided repre-sentations. From a wider perspective, we believe these up-dates represent multi-stakeholder convergence on the goalof integrating data through environmental contextualisa-tion across the biosphere.Technical noteAs a technical note, the reader is advised that OBO Libraryontologies are assigned unique acronyms or initialisations,such as BFO or ENVO, that serve as shorthand identifiersfor that ontology. In the following text, ontology classes(or, synonymously, terms), are written in italics and aretaken from ENVO unless otherwise marked throughthe provision of an appropriate ontology prefix, as inPATO:laminar. The unique shorthand fragment of eachterms Permanent Uniform Resource Locator (PURL), e.g.ENVO_00002297 for environmental feature, will beincluded on first mention of any class, in which casethe redundant namespace prefix shall be omitted. FullPURLs are of the form: http://purl.obolibrary.org/obo/ENVO_00002297, and are resolved to OWL as well asto human-readable web pages via OntoBee [21].MethodsThe development of ENVO is now conducted usingProtégé (http://protege.stanford.edu), rather than OBOEdit [22], allowing more expressivity through the WebOntology Language (OWL). For global interoperability, wepreferentially use relations from the Relations Ontology(RO; [23]) and the Basic Formal Ontology (BFO; [24]) toconnect these classes. Additional relations are present, butwill be incorporated into RO pending an open discussionand vetting process. The ontology is still released in bothOBO and OWL formats and a number of custom exportshave been made upon request (e.g. flat, character delimitedformats suitable for import into relational databases, table-oriented analysis software, or network visualisation andanalysis solutions). We continue to maintain obsoletedterms and link them to their replacements (whereavailable) in a machine readable way to support auto-mated updating of user implementations.As with most other OBO Library ontologies, ENVOs re-pository has been moved to its own GitHub organization(https://github.com/EnvironmentOntology). This changedoes not affect downstream users who consume theontology using standard permanent URLs; however, itdoes provide a better mechanism for stakeholders tobecome involved with the development of the ontologythrough, for example, an improved issue tracker [25].Further, it allows easier reference to previous versionsof the ontology for backwards compatibility.OWLTools (https://github.com/owlcollab/owltools) andROBOT [26] (https://github.com/ontodev/robot/) arecurrently being used for release management, and for theimport of classes from other OBO Foundry and Libraryontologies in alignment with the Minimum Informationto Reference an External Ontology Term (MIREOT; [27])guidelines. These import procedures are primarily used toexpress environments that are dependent on entitiesdefined outside of ENVO. For example, environments de-fined by anatomical entities and chemical entities areexpressed using classes from ontologies such as the UberAnatomy Ontology (UBERON; [28]) and the ChemicalEntities of Biological Interest Ontology (CHEBI; [29])to prevent duplicating existing, well-developed seman-tics relevant to terms such as xylene contaminatedsoil [ENVO_00002146] and axilla skin environment[ENVO_08000001].We have created a TermGenie instance (http://envo.termgenie.org/) [30] that allows for web-based additionof new terms that conform to a pre-defined template, orfollowing a free-form pattern. We are also documentingour design patterns (ODPs) using the emerging deadsimple owl design patterns standard (https://github.com/dosumis/dead_simple_owl_design_patterns) and are usingthese patterns to generate small portions of the ontology.Further, we have begun to use the results of text-miningapproaches, noted in [1], discussed below, and docu-mented by Pafilis et al. [31], to automatically generateexperimental classes which, upon curation, can be inte-grated into the core ontology.Results and discussionENVO now includes some 2159 classes primarily repre-senting biomes, geographic features, and environmentalmaterials, along with 18,791 axioms (logical statements)Buttigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 3 of 12defining, interconnecting, and interrelating them. Thiscontrasts with 1644 classes and 14,542 axioms presentwhen ENVOs original description was published. Thegrowth of the ontology was primarily driven by the needsof the omics community using the Minimal Informationabout any (x) Sequence (MIxS; [32]) checklist and itsextensions such as MIxS for the Built Environment(MIxS-BE; [33]). These needs were communicated throughindividual requests for new classes and requests coordi-nated through, for example, curation efforts of organisa-tions such as the European Nucleotide Archive (ENA)(e.g. [34]). More currently, the bulk of the changes toENVOs content have been motivated by the ontologysgrowing adoption and engagement with new user com-munities as well as the need to integrate their varyingapproaches to describing environments.Increases in semantic density and expressivityAs we are now developing ENVO using the expressivityof OWL (see Methods), we have increased the variety anddensity of linkages between many of ENVOs classes aswell as the detail in their logical definitions. This increasedsemantic density offers more flexibility when using theontology for querying, inference, and semantically en-hanced analysis. To illustrate the increased expressivity,an oasis [ENVO_01001304] (Fig. 1) is represented as asubclass of vegetated area [ENVO_01001305] which has,as a part, some spring [ENVO_00000027] and is partiallysurrounded by a portion of either rock [ENVO_00001995],sand [ENVO_01000017], or soil [ENVO_00001998]which, itself, is arid [ENVO_01000230]. This repre-sentation has several facets which involve type hier-archy (i.e. class and subclass relationships), parthood,and adjacency, and which define key properties ofone or more of the classes involved. Practically, usersand machine agents can now identify an oasis (and anydata that has been associated with that class) by any oneof these routes such as querying for a vegetated area thatis surrounded by arid environmental materials or whichhas a spring as a necessary part.The increased axiomatisation described above has alsoimproved our ability to represent semantically problematicclasses such as hydrographic feature [ENVO_00000012]and marine pelagic feature [ENVO_01000044]. The is-sues with these somewhat artificial or convenience group-ings are discussed in [1]; in brief, their membership isdictated more through convention than physical or forma-tive similarities, often adding ambiguity and confoundingsearch and inference. For example, one is correct inasserting that a lighthouse, a lake, and a coral reef arehydrographic features due to the nautical conventionsof hydrography; however, these entities are substantiallydifferent from one another and much better distributedin hierarchies true to their physical attributes and/orthe processes of their formation. With ENVOs greatersemantic flexibility, the varied criteria for including aclass in one of these convenience groupings can be moreprecisely defined and classes which satisfy these criteriacan be interlinked through automated inference: the actionof reasoning software which can use logical statements toinfer relationships and hierarchies which were notasserted by a human. For example, any class which hasbeen asserted to be adjacent to some water body or par-tially surrounded by some water will be inferred to be asubclass of hydrographic feature. Similarly, marine pela-gic feature would be populated by any entity which hasbeen asserted to be part of  some marine water body or'composed primarily of' some sea water. Similarly, manysubclasses of environmental material [ENVO_00010483]are now placed in inferred hierarchies using various sub-classes of quality [PATO_0000001] such as quality of asolid [PATO_0001546], quality of a gas [PATO_0001547],and quality of a liquid [PATO_0001548]. Such assertionsprovide a way to construct and populate classes likeFig. 1 Illustrative example of ENVOs improved semantic expression with OWL axioms. An oasis is a vegetated area which has, as a part, a springand is surrounded by an arid portion of soil, rock, or sandButtigieg et al. Journal of Biomedical Semantics  (2016) 7:57 Page 4 of 12solid or liquid through inference, avoiding assertedmultiple inheritance while simultaneously preserving clearrepresentations based on multiple criteria.As illustrated above, the flexibility that comes with in-creased axiomatisation is an important step in support-ing multiple, varying classifications of environmentalentities in an integrated fashion. We will leverage thesecapacities to disentangle the semantics of environmentalentities across user groups which use different defini-tions for syntactically similar terms. The hundreds of of-ficial and operational definitions of forest [35], whichcan influence critical decisions in conservation and sus-tainable land use [36, 37], will be one of our first targetsin this process. We anticipate that ENVO will host mul-tiple classes representing the different entities typicallygathered under one label, using synonym lists and cross-Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 DOI 10.1186/s13326-016-0106-9RESEARCH Open AccessTNM-O: ontology support for staging ofmalignant tumoursMartin Boeker1* , Fábio França1,2, Peter Bronsert3 and Stefan Schulz4AbstractBackground: Objectives of this work are to (1) present an ontological framework for the TNM classification system,(2) exemplify this framework by an ontology for colon and rectum tumours, and (3) evaluate this ontology byassigning TNM classes to real world pathology data.Methods: The TNM ontology uses the Foundational Model of Anatomy for anatomical entities and BioTopLite 2 as adomain top-level ontology. General rules for the TNM classification system and the specific TNM classification forcolorectal tumours were axiomatised in description logic. Case-based information was collected from tumourdocumentation practice in the Comprehensive Cancer Centre of a large university hospital. Based on the ontology, amodule was developed that classifies pathology data.Results: TNM was represented as an information artefact, which consists of single representational units. Correspondingto every representational unit, tumours and tumour aggregates were defined. Tumour aggregates consist of theprimary tumour and, if existing, of infiltrated regional lymph nodes and distant metastases. TNM codes depend on thelocation and certain qualities of the primary tumour (T), the infiltrated regional lymph nodes (N) and the existence ofdistant metastases (M). Tumour data from clinical and pathological documentation were successfully classified withthe ontology.Conclusion: A first version of the TNM Ontology represents the TNM system for the description of the anatomicalextent of malignant tumours. The present work demonstrates its representational power and completeness as well asits applicability for classification of instance data.Keywords: TNM classification, Tumour classification, Tumour staging, Anatomical extent, TNM ontology,Description logicBackgroundClinical and pathological staging of malignant tumours isone of the most important procedures in the diagnosisof cancer for prognosis assessment and treatment plan-ning. The staging procedure compiles several clinical andpathological parameters such as the location and the sizeof the primary tumour, the location and the number ofthe infiltrated regional lymph nodes, and the existence ofdistant metastases.A prerequisite for an evidence-based cancer treat-ment is a correct and unambiguous cancer diagnosis.*Correspondence: martin.boeker@uniklinik-freiburg.de1Institute for Medical Biometry and Statistics, Medical Center  University ofFreiburg, Faculty of Medicine, Stefan-Meier-Str. 26, 79104 Freiburg i. Br.,GermanyFull list of author information is available at the end of the articleInterdisciplinary expert groups, e.g. from clinicalmedicine, imaging, and pathology, have been working inclose cooperation to establish criteria for precise tumourdiagnoses [1]. One of the most challenging tasks inclinical oncology is to correctly classify and code clinicalfindings, using a multitude of available coding systems.By far, the most important coding system for tumourstaging is the Tumour-Node-Metastasis (TNM) classifica-tion [2] formalignant tumours, published by the Union forInternational Cancer Control (UICC)1. Besides a growingnumber of reliable biomarkers, TNM classification andstaging are the most important information for the ther-apy planning for patients with colorectal cancer [35] andother solid tumours (e.g. cancer of the head and neck[6] or breast tumours [7]), except cancers of the central© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 2 of 11nervous system. In addition, the TNM classificationsystem is important in cancer research for a cor-rect description and classification of the anatomicalextent of a given tumour. This is not only rele-vant for cancer epidemiology but also in fundamen-tal tumour research (e.g. the dataset descriptions forresearchers of the Surveillance, Epidemiology, and EndResults Program (SEER) of the National Cancer Insti-tute2 and predefined results using TNM stratifieddata3).The TNM coding procedure requires advanced skills,encompassing both experience in tumour documentationand in-depth domain knowledge. The criteria for classifi-cation of the different primary tumour locations differ tothe same extent as the underlying diseases. As a conse-quence, even expert coders and physicians for one organsystemmight encounter difficulties in the correct applica-tion or interpretation of TNM in a different organ system.Several combinations of tumour findings are difficult toencode due to ambiguous or overlapping criteria (non-disjoint definitions) or non-exhaustive definitions, whichoften result in cases where no TNM code or more thanone TNM code is applicable to a given tumour state. Avariety of problems with TNM coding has been describedfor different tumour locations. Main issues that arise inthe practice of TNM coding derive from overly com-plex definitions of the underlying medical situation, whichthen result in interpretation problems even for experts[810]. The required in-depth knowledge of the domain,together with specific competences needed for TNM cod-ing, result in poor coding completeness and quality, espe-cially with the clinical staging in outpatients [11, 12].Given the importance of TNM staging for the individualpatient, deviation rates of about 20% for clinical codingand 10% for pathological coding can be interpreted asvery high [13].The complexity of TNM is mainly due to the develop-ment of the TNM classification as an evolutionary pro-cess [14], which has been constantly incorporating hugeamount of new scientific insights in tumour prognosis andthe dependency of therapeutic effects on tumour stage.Controlled by medical experts, TNMs underlying struc-ture has become more and more complex over the years.Experts in different fields of oncology have demanded achange in TNM maintenance, to address the increasingcomplexity, the detachment from clinical practice, and theresources needed for documentation [15, 16]. Therefore,standardisation of tumour classification and staging is anurgent requirement for improvement of tumour docu-mentation in primary documentation, clinical studies andcancer registries [11, 1720].Despite its importance and formal precision, to theknowledge of the authors, no formal representationof the complete TNM is available so far. Formal,i.e. computable representations would have several advan-tages over TNMs current publication as a textbook.An initial attempt to represent staging of lung tumoursand glioma tumours was not continued [21, 22]. Morerecently, a description logics based (DL) approach waspresented [23].One of the major requirements a formal representa-tion of TNM could satisfy is the automatic classifica-tion of instance data obtained from clinical databasesor mined from textual reports [2426]. Consecutively,instance data classification could inform higher order pro-cesses such as clinical documentation systems. Instancedata on pathological or clinical conditions are collectedduring routine health care processes in pathology or otherclinical information systems. Users could be supportedby automatic encoding of instance data to TNM in realtime or in spatially and temporally disseminated settings(e.g. in tumour documentation). For intelligent documen-tation systems in clinical oncology and pathology, a TNMontology could be deployed as part of the knowledgebase supporting the coding of tumour-related findingsand the interpretation of TNM codes. In such systems aTNM ontology could enable automated reasoning basedin description logics, which would timely detect logicalinconsistencies and complexity related coding problemsin databases and textual reports. In integrated clinicaldecisions support systems (DSS) TNM could be deployedto inform users about guideline-conformant treatment[27]. A further advantage of a formal approach would bethe enhanced support for development and refinement ofTNM.With a taxonomic backbone and axiomatic descrip-tions, the current complex natural language descriptionscould be converted into computable structures. Thiswould help decompose the descriptions into all theirdefining criteria, which in turn could facilitate the detec-tion of coding errors, inconsistencies, and ambiguities indefinitions [28, 29].Description logics is the method of choice for a for-malization of TNM [30]. Advanced retrieval and query-ing tools would be additional benefits that come witha logical representation following principles of AppliedOntology [31]. For these use cases, a formalised TNMversion could constitute a unified source on which a vari-ety of clinical documentation and analysis tools couldbe based. In addition, such a resource could be mappedto other DL-based clinical ontologies, especially toSNOMED CT.With this work, we propose to close the gap of a miss-ing formal representation by outlining and prototypingthe TNM ontology (TNM-O). Following up on initialattempts in the breast cancer domain [32], the objectivesof this work are (1) to present an ontological frameworkfor the TNM classification system, (2) to implement aTNM ontology, describing colon and rectum tumoursBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 3 of 11based on this framework, and (3) to evaluate this ontologyusing a tool for classifying pathology data.The TNM classificationThe canonical description of the TNM classificationbased on the anatomic extent of disease (EOD) is pub-lished by the UICC and the AJCC [2, 33]. The UICCpublished the first edition of the TNM coding systemin 1968. Since then, the system has undergone sev-eral revisions, with the 7th edition published in 2009.The AJCC has recently announced the release of the8th edition of the TNM classification for the beginningof 20174. The part of the new version for lung can-cer is already in use with its important changes satis-fying urgent medical requirements [34]. The objectivesof the TNM coding system are six-fold. It supportstreatment planning, prediction of outcomes (prognosis),evaluation of treatment results, exchange of informationbetween different participants in health care processes,continuing research in malignant diseases, and cancercontrol [2, 14].The core TNM classification uses three descriptors: T(tumour), N (metastasis in regional lymph nodes), and M(distant metastasis). The extent of the disease is indicatedby integer values resp. character modifiers: TX (Tumourcannot be assessed), T0 (No evidence of primary tumour),T1-4 (increasing size or local extent), Tis (Carcinoma insitu); NX (Regional lymph nodes cannot be assessed), N0(No regional lymph node metastasis), N1-3 (Increasinginvolvement of regional lymph nodes); M0 (No distantmetastasis), M1 (Distant metastasis). For some entitiesfurther subdivisions of the categories are possible indi-cated by lower case characters (e.g. N2a and N2b).The specific medical denotation for the differentdescriptors is dependent on the localisation of the tumour,designated by the ICD-O localisation code5. It is notpossible to list all single regions addressed by the TNMclassification here (for a current list see [2]). However, theTNM classification is not available for all body regionsor systemic malignancies (e.g. C70-C72 Tumours of theCentral Nervous System, C33 Trachea, C42, and C77Tumours of haematopoietic and lymphoid tissues). Formost of these malignancies the anatomical extent is eithernot determinable (systemic malignancies e.g. leukaemia)or the tumours have no metastasis (e.g. CNS tumours).The World Health Organisation (WHO) has publishedthe 3rd edition of International Classification of Diseasesfor Oncology (ICD-O) in 2003. As an extension of theInternational Classification of Diseases (ICD-10) [35] fortumour diseases, the ICD-O is a dual classification systemfor the tumour morphology and the tumour localisation[36]. ICD-O is widely used in clinical medicine, tumourdocumentation, and research to encode tumour morphol-ogy and tumour localisation.With an additional modifier, the TNM classificationis divided into the pre-treatment clinical (indicated ascTNM) and post-surgical pathological (pTNM) classifica-tion. pTNM codes can only be assigned to the disease afterpathological assessment following surgery and is the mostimportant diagnostic item for following (adjuvant) radio-or chemotherapy or their combination. The results fromthe clinical assessment have to be accurately discernedfrom the pathological assessment due to their differentmeanings and evidence levels.Besides the already complex semantics of the mainnumeric TNM codes, a series of additional symbols exists,which might have largely different meanings in the differ-ent tumour locations. Prefixes, suffixes, and certainty fac-tors increase the confusion, e.g. for carcinoma in situ thesuffix is has to be used (Tis). As TNMallows putting anX wherever the information about the clinical or patho-logical situation is incomplete or inaccurate, incompletecode assignments become widespread (e.g. MX for nostatement on metastases possible). In this work only theclasses with the descriptors T, N, andMwith themodifiersc and p are represented (for a full list see Table 1).Table 1 TNM classification descriptors and additional modifiersDescriptor Values MeaningT 0-4, is, X Extent of the primary tumourN 0-3, X Extent of metastasis in regionallymph nodesM 0-1 Existence of distant metastasisPrefix to T, N, M p, c Clinical (pre-therapeutical) orpathological (post-surgicalassessment)Suffix to pNn (mi) Micrometastasis (< 0.2 cm)Suffix to pNn (sn) Sentinel lymph node metastasisSuffix to pN0 or pM0 (i+), (mol+) Isolated tumour cells, positivefindingsG X, 1-4 Histopathological gradingSuffix to T (m) Multiple primary tumours at asingle sidePrefix to c/ p y Assessment during multimodaltherapyPrefix to c/ p r Recurrent tumourPrefix to c/ p a Assessment during autopsyL X, 0-1 Lymphatic invasionV X, 0-2 Venous invasionPn X, 0-1 Perineural invasionC 1-5 Validity of the assessment, canfollow each of T, N, MR X, 0-2 Residual tumourDepending on the organ of the primary tumour, T, N, and M values can be furthersubdivided into levels a-c, e.g. N1a-c, N2a-c, and M1a-b in colorectal tumoursBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 4 of 11pTNM codes are grouped into stages which are basedon the prognosis of the patients. Stages are designated bythe roman numerals I-IV and further subdivided into sub-stages described by capital letters A-C. TNM staging hasbeen subject to frequent changes during the history ofthe TNM classification, according to scientific and medi-cal progress [34]. The mapping of the TNM classificationfor colon and rectum tumours to stages for version 7 isprovided in [2, 4].MethodsTNM-O, the TNM ontology presented here, uses theFoundational Model of Anatomy [37] for anatomical enti-ties, together with BioTopLite 2 (BTL2) as a domaintop-level ontology [38, 39]. Tailored for the biomedicaldomain and based on description logics [30], BTL2 pro-vides upper-level types both for general categories likeMaterial object, Process, Information object, Quality etc.,as well as constraints on all of them, using a set of sixteencanonical relations, partly derived from the OBO Rela-tion Ontology (RO) [40]. They constrain each category bymeans of a set of general class axioms. BTL2 also containsother axioms such as relationship chains, existential andvalue restrictions. Thus, the building of domain ontolo-gies under BTL2 heavily constrains the freedom of theontology engineer, which is fully intended as it guaran-tees a higher predictability of the outcomes of the domainontology production under BTL2.The design of BTL2 is top-level agnostic and has beeninfluenced both by the Basic Formal Ontology (BFOand BFO2) and the Descriptive Ontology for Linguis-tic and Social Engineering (DOLCE) which is discussedin more detail in [39]. BTL2 is especially appropriateas domain top-level for TNM-O because it provides alean, yet exhaustive ontological framework for the repre-sentation of clinical documentation artefacts. Moreover,it is fully axiomatised using RO (see above) so that itis interoperable with other ontologies in the biomedicaldomain.The development of TNM-O is an ongoing process.For this study, colorectal cancer was chosen as use casefor several reasons. It is the third most common cancerworldwide and accounts for 9% of all cancer incidence[41, 42], affecting more than one million humans in 2002.Treatment of cancer patients and research on causes ofcancer are main goals of worldwide cancer control pro-grams6. In prior work, the TNM classification for breasttumours (ICD-O C50) had been formally represented[32]. The selection of breast and colorectal tumours wasmotivated both by their paramount medical importanceand their complexity in TNM, where both follow non-trivial medical classification principles, especially for thecN and pN classifications. Demonstrating the appropri-ateness and feasibility of TNM-O for these two tumourlocations provides a good support for the general applica-bility of the approach.The general rules of the TNM classification and the spe-cific TNM classification for tumours of the colon and therectum (ICD-O topography chapters C18  C21, for ICD-O morphology codes see Table 2) were represented asdescribed [2, 43].A classifying tool for individuals (instances) derivedfrom pathology reports was developed employing theOWL API (version 4.0.1)7 and the HermIT DL reasoner(version 1.3.8)8. It classifies breast tumour and colorectaltumour data based on the corresponding TNM ontolo-gies. It reads either tabular input data from files orprocesses data from manual entry via a graphical userinterface.The objective of TNM-O is not to re-design an exist-ing tumour classification into a new system. At the cur-rent level of development, TNM-O is the result of anontological analysis of what has been developed by themedical community over a long period, followed by itstranslation into a formal language, incorporating onto-logical principles, in order to improve the development,maintenance, and application of the TNM classificationsystem.In the following two sections, we describe (1) the TNMclassification in detail as foundation of what has to be rep-resented by TNM-O, (2) how the TNM classification arte-facts are represented by information artefacts of TNM-O,(3) how these information artefacts are related to theactual tumour entities, and (4) how the patho-anatomicalreality of tumour disease is constructed in terms of whatis required for the TNM classification.Design of the TNM-OThe relation between the artefacts of the TNMclassification and the actual tumour diseases is denota-tional: the T code denotes the extent (size, infiltration)of the primary tumour, the N code the extent of regionalTable 2 ICD-O 3 morphology codes for tumours of the colonand the rectumType ICO-O 3 morphologyAdenocarcinoma 8140/3Mucinous adenocarcinoma 8480/3Signet-ring cell carcinoma 8490/3Small cell carcinoma 8041/3Squamous cell carcinoma 8070/3Adenosquamous carcinoma 8560/3Medullary carcinoma 8510/3Undifferentiated carcinoma 8020/3Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 5 of 11lymph node metastases, and the M code the existence ofdistant metastases. For TNM-O, we adopted an approachwhich is compliant with the Information Artefact Ontol-ogy from the OBO Foundry and recently publishedwork on the aboutness relation [44, 45]. In TNM-O,coding artefacts of the TNM classification i.e. theclasses of the classification are represented by subclassesof btl2:InformationObject as RepresentationalArtefact.Information reported on individual patients, e.g. as TNM-codes in patient records are thus individuals of theseclasses. Individuals from subclasses of InformationObjectare related by btl2:represents to individuals of classesabout the current disease state (AnatomicalStructure).The inverse relation is btl2:isRepresentedBy connectsmaterial or processual entities with the respectiveTNM-artefact.As the TNM classification is compositional, the individ-ual classes of the three descriptors can be independentlycombined to a joint code. Classes are only dependent onthe location of the primary tumour and additional mod-ifiers c or p: e.g. cN1 for colon cancer has a differentmeaning than cN1 for breast cancer, and cT1 has a differ-ent meaning than pT1 for all locations where these codesare available). This characteristic is conserved in TNM-O. The class RepresentationalUnit is a superclass of organspecific classes separated in a clinical and a pathologicalbranch.For representing anatomical structure, TNM-O usescontent from the Foundational Model of Anatomy,restricted to cancer-related anatomy as referred to bythe TNM classification. All primary tumours individualsand metastases are then related to individuals anatomi-cal entities by the relation btl2:locatedIn, thus providingthem with an exact topography and extent. The extent ofprimary tumours cannot only be described by their local-isation (i.e. occupying space or infiltrating through layersof an organ) but can be further characterised by qualities,e.g. tumour size or infiltration patterns. These qualitiesare dependent on the localisation of the primary tumourand can substantially differ between them.What makes a lymph node a regional lymph nodedepends on its proximity to a primary organ. An axillarylymph node is a regional lymph node of the breast glandbut not of the colon. For all relevant organs, these regionallymph node groups are to be defined. Moreover, the for-malisation of infiltrated regional lymph nodes depends onthe aggregate of a localised primary tumour together withsome metastasis in a regional lymph node of that organ inwhich the primary tumour is located. Thus, an infiltratedaxillary lymph node is a regional lymph node metastasisfor a breast tumour, but certainly not for a colon cancer.Distant metastases are, by definition, those located in atumour aggregate that is not a regional lymph node of theprimary tumour.Classification of pathology dataWe computationally classified data describing the extentof 291 colorectal cancer specimens into TNM, docu-mented at the Institute of Surgical Pathology, MedicalCenter  University of Freiburg using a pathology infor-mation system. This data were re-coded as RDF-OWLinstance data and classified into classes of TNM-O byan application based on the OWL API using an OWLclassifier9. Automatic classification was solely based onaxioms defined in the colorectal TNM-O version 7 (TNM-O_colon_7.owl). The complete set of criteria is shown inTable 3.For comparison of the ontology-based TNM classi-fication with a manual expert TNM classification, thedata were manually classified by a pathologist into TNMversion 7.ResultsTNM-O is designed as a modular system of independentontologies under BTL2. For every organ or organ systembased module of the TNM classification system, TNM-OTable 3 Criteria of TNM version 7 for colorectal cancers. All TNM codes can be inferred from this criteria. The exact wording of thetextual definitions of the TNM in version 7 is diverging. Exact count of infiltrated organs in distant metastasis is omittedCriterion btl2 superclass ValuePrimary tumour extension MaterialObject Epithelium, Submucosa, Lamina propria, Subserosa, Adventitia, VisceralPeritoneumPrimary tumour growth pattern Quality Infiltrative, ConfinedPrimary tumour epistemology Quality NoAssessment, NoEvidenceRegional LN number Quality Cardinality1, Cardinality2or3, Cardinality4to6, Cardinality7orMoreRegional LN epistemology Quality NoAssessment, NoEvidenceDistant Mx location MaterialObject PeritoneumDistant Mx/no. of organs Quality Cardinality1, Cardinality2orMoreDistant Mx epistemology Quality NoEvidencehttp://cancerstaging.blogspot.de/2005/02/colon-and-rectum.htmlBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 6 of 11provides a set of specific ontologies. The TNM connect-ing ontology serves as a hub to import BTL2 as well asthe organ and organ system specific TNM ontologies (seeTable 4). With the modular architecture only those mod-ules are included that are needed by a tumour-specificapplication.The hub TNM Ontology for all tumours can bedownloaded from http://purl.org/tnmo/TNM-O.owl. Theontologies for breast tumours and colorectal tumoursare named according to Table 4 and can be downloadedfrom the same site. They need to be loaded in the hubontology.Without inclusion of BTL2, the TNM hub ontologyhas the description logic expressivity of ALC (for ashort introduction to the DL nomenclature see [46]section Description Logic Nomenclature). It consists of79 axioms, 38 logical axioms, and 39 classes. It includes35 subClassOf and one EquivalentTo axioms. Most ofthe classes are proxy classes to BTL2. Inclusion of BTL2changes the DL expressivity to SRI.The TNM ontology for colorectal tumours has thedescription logic expressivity of ALC. For TNM version7.0 (version 6.0 in brackets), it consists of 366 (357)axioms, 198 (199) logical axioms, and 161 (149) classes. Itincludes 123 (160) subClassOf, 57 (18) EquivalentTo and18 (18) DisjointClasses axioms.Representational units in the TNM-OntologyThe representation of the TNM system is decomposedinto the representational units T, N, and M, togetherwith the location of the primary tumour. Thus, for everyexisting code Tn, Nn, and Mn in combination with a spe-cific organ there exists one TNM-O:RepresentationalUnitwhich is an btl2:InformationObject. E.g. every TNMcode for colorectal cancer is represented by a separateclass. Axioms using the relation btl2:isRepresentedByintroduce possible TNM values for subclasses of Primary-Tumour or TumourAggregate. This is done by connectingTable 4 Modular structure of TNM-O. Codes in clinicaldocumentation and cancer registries follow TNM versions,because the meaning of codes and stages may change betweenversions. The modular structure is designed to include versionsfor every available TNM encoded entity (tumour location) so thatthe intended meaning is preserved according to the versionused for codingName DescriptionBTL2 Upper domain level ontologyTNM-O TNM-O central connecting ontologyTNM-O_breast_7 TNM-O for breast cancer (TNM version 7) in: [32]TNM-O_colorectal_6 TNM-O for colorectal cancer (TNM version 6)TNM-O_colorectal_7 TNM-O for colorectal cancer (TNM version 7)these values via the universal quantifier ONLY (rolerestriction). In all of these cases, the clause or (not Repre-sentationalUnitInTNMClassification) allows other valuesthat are not TNM representational units. In the remainingtext, the namespace of the TNM ontology is suppressedfor clarity:TumourOfColonAndRectumWith7OrMoreMetastaticRegional-LymphNodes subClassOfTumourAggregate andbtl2:isRepresentedBy only(ColonRectumTNM_pN2b or ColonRectumTNM_N2bor (not RepresentationalUnitInTNMClassification))Representation of the primary tumourThe primary tumour is represented as PrimaryTumour, asubclass of MalignantAnatomicalStructure. The tumour char-acteristics relevant for the representational unit T of theTNM classification system are represented as location andqualities of PrimaryTumour. For colorectal tumours, theexact localization of the tumour in the gut wall, the qual-ity of the tumour confinement with respect to neighbour-ing organs (confined or invasive), the quality of the assess-ment (no assessment, no evidence or carcinoma in situ), areimportant:InvasiveTumourOfSubmucosaOfColonAndRectumEquivalentTo ColonAndRectumTumour and(btl2:isBearerOf some (Confinement and(btl2:projectsOnto some Invasive))) and(btl2:isIncludedIn someSubmucosaOfLargeIntestine)The specific tumour defined as subclass of PrimaryTumourabove is directly related to the corresponding representationalunit as introduced in the section above.InvasiveTumourOfSubmucosaOfColonAndRectumsubClassOfbtl2:isRepresentedBy some(ColonRectumTNM_T1 orColonRectumTNM_pT1) andbtl2:isRepresentedBy only(ColonRectumTNM_T1 orColonRectumTNM_pT1 or(not RepresentationalUnitInTNMClassification))Representation of regional lymph nodesThe most complex part of the TNM classification of many pri-mary tumour locations is the interpretation of the axisN, whichdescribes the extent of infiltration of regional lymph nodes bythe primary tumour. The anatomy of lymph nodes draining thecolon and rectum was modelled according to clinical anatom-ical conventions. Metastatic regional lymph nodes can exactlybe located by the exact subclass of infiltrated regional lymphnode:MetastaticLymphNodeOfColonAndRectumTumourEquivalentTo LymphNode and(btl2:hasPart someMetastasisOfColonAndRectumTumour)Boeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 7 of 11MetastaticRegionalLymphNodeOfColonAndRectumTumourEquivalentToMetastaticLymphNodeOfColonAndRectumTumour andColonAndRectumRegionalLymphNodeTo define regional lymph node metastases of colorectalcancers, the aggregate of primary tumour and infiltratedlymph nodes around the colon and rectum (TumourAggre-gate) has to be considered as one (composite) entity. Therepresentational unit N of the TNM classification of col-orectal cancers depends on the count of metastatic regionallymph nodes and the presence of subserosal tumour depositswithout regional lymph node metastases. The count ofmetastatic lymph nodes is represented by subclasses ofCardinalityValueRegion:TumourOfColonAndRectumWith2or3MetastaticRegional-LymphNodes EquivalentToTumourOfColonAndRectumWith1to3MetastaticRegional-LymphNodes and(btl2:isBearerOf some(Cardinality and(btl2:projectsOnto someCardinality2or3) and(btl2:projectsOnto onlyCardinality2or3)))Representation of distant metastasesFor the representational unitM of the TNM classification sys-tem the existence and number of distant metastases are eval-uated. The definition of distant metastases excludes regionallymph nodes as their localisation:DistantMetastasisOfColonAndRectumTumour EquivalentToMetastasisOfColonAndRectumTumour and(not (btl2:isIncludedIn someColonAndRectumRegionalLymphNode))TumourOfColonAndRectumWithDistantMetastasisEquivalentToTumourOfColonAndRectumAggregate and(btl2:hasPart someDistantMetastasisOfColonAndRectumTumour)TumourOfMammaryGlandWithDistantMetastasissubClassOf(btl2:isRepresentedBy only(MammaryGlandTNM_M1 orMammaryGlandTNM_pM1 or(not RepresentationalUnitInTNMClassification))Classification of pathology dataAll instance data of 291 samples of colorectal cancercould be classified into classes of TNM-O on colorectalcancer. A posteriori comparison of the automatic classifi-cation results with a manual TNM coding based on thesame findings from the pathology database by an experi-enced pathologist showed 100% agreement. Table 5 shows15 exemplary tabular instance data rows and the cor-responding manual and automatic classification results.Figures 1 and 2 shows an example of an RDF-OWLinstance which corresponds with rows 6 and 8 of Table 5.For clarity, the RDF example focuses on TNM N, otherdetails on tumour invasion and distant metastasis wereleft out. All automatic classification results are based onTable 5 TNM relevant tabular data, manual expert TNM classification (subscript P), and ontology-based automatic TNM classification(subscript O)Invasion of rLN tp rLN TD/ Sat. dMT ip dMT TP NP MP TO NO MOSubserosa 31 0 no 0 no pT3 pN0 M0 pT3 pN0 M0Muscular layer 13 0 no 0 no pT2 pN0 M0 pT2 pN0 M0Subserosa 19 0 no 0 no pT3 pN0 M0 pT3 pN0 M0Submucosa 18 0 no 0 no pT1 pN0 M0 pT1 pN0 M0Muscular layer 11 0 no 0 no pT2 pN0 M0 pT2 pN0 M0Visc. peritoneum 19 2 no 0 no pT4a pN1b M0 pT4a pN1b M0Subserosa 20 0 yes 0 no pT3 pN1c M0 pT3 pN1c M0Subserosa 14 2 no 0 no pT3 pN1b M0 pT3 pN1b M0Muscular layer 14 0 no 0 no pT2 pN0 M0 pT2 pN0 M0Subserosa 24 4 no 0 no pT3 pN2a M0 pT3 pN2a M0Other 16 6 no 0 no pT4b pN2a M0 pT4b pN2a M0Subserosa 17 0 no 0 no pT3 pN0 M0 pT3 pN0 M0Visc. peritoneum 40 29 no 0 no pT4a pN2b M0 pT4a pN2b M0Subserosa 15 0 no 0 no pT3 pN0 M0 pT3 pN0 M0Visc. peritoneum 24 15 no 1 no pT4a pN2b M1a pT4a pN2b M1arLN: Number of regional lymph nodes inspected; tp rLN: Number of tumour-positive regional lymph nodes, TD/ Sat.: Tumour deposits/ satellites; MT: Number of distantmetastases; ip MT: Intra-peritoneal metastasesBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 8 of 11Fig. 1 N1b representational unit of TNM-O for colorectal tumours. Graph of the patho-anatomical structures represented by an N1b representationalunit of the TNM-O for colorectal tumours version 7 (TNM-O_colorectal_7.owl). T and M representational units are unspecifiedFig. 2 RDF-OWL instance of a tumour aggregate and corresponding OWL classes. Graph of an RDF instance of a tumour aggregate as created fromtabular data according to TNM-O for colorectal tumours version 7 (TNM-O_colorectal_7.owl). RDF instances data are depicted with a purplediamond. RDF instance for T and M classification are omitted. Instances of this type are classified as TNM N1bBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 9 of 11TNM-O, TNM-O_colorectal_7 and RDF-OWL instancedata.DiscussionTNM is a globally accepted system to describe theanatomical extent of malignant tumours [2, 14]. AlthoughTNM is of high importance for tumour staging, to theknowledge of the authors, there exists no comprehensiveformal representation of TNM so far. With this work,the authors provide a first version of a TNM ontology(TNM-O) and a prototypical implementation of TNM forcolorectal cancers. Further, this work shows that TNM-Oclassifies instance data.Over time, TNM has developed into a coding system,which had to accommodate both the pragmatics of codingand representational accuracy. The literature on ambi-guities and difficulties of TNM in practice is abundant.The discussion of TNM for breast tumours illustratesthe dilemma of its maintainers [8, 47, 48]. They had toaccount for the rapid progression of scientific knowledgeon tumours and to keep it usable at the same time: newversions of TNM are already outdated when comparedwith new scientific insights. On the other hand, TNM hasbecome increasingly complex, with a negative impact onits usability by both expert and non-expert documentationstaff and physicians.Encoding clinical conditions using TNM as well as theselection of the right treatment according to TNM codesis daily routine in oncology. In order to assist in thesedifficult and time consuming decision processes, sev-eral systems have been proposed, usually based on textextraction from pathology reports and machine learn-ing algorithms [2426]. The accuracy of these approacheswas relatively low [24]. Here, we present an ontology,which classifies instance data with 100% accuracy inan experimental setting based on structured data. Wehypothesise that DL based classification using TNM-Ocould also improve the results from automated informa-tion extraction from unstructured data as done in theabove mentioned approaches. Such systems could also bemade available in intelligent documentation systems inthe form of embedded decision support systems, whichcould help to choose the right codes for a clinical condi-tion and/ or the right guideline compliant treatment fora given code (describing a clinical condition). Further-more, we think that with an ontology the curation of theTNM itself could be improved. Based on a taxonomicand axiomatic description, the detection of coding errors,inconsistencies, and ambiguities in definitions could befacilitated [28, 29]. A formal description logic basedaxiomatisation allows the use of specific reasoning toolsto check for inconsistencies during the ontology engi-neering process, which would indicate conflicting axioms.Redundancies or wrong hierarchical dependencies isdetected by checking the inferred class hierarchy after DLclassification.This study is limited as far as we provide here afirst version of the TNM Ontology (TNM-O), limitedto mammary gland [32] and colorectal tumours. Asthese two tumour entities are the most complex andbest represented ones in TNM, the current versionis already sufficiently complete and stable to be usedas a blueprint for TNM-O extensions to other organsystems.Due to the nature of the domain and the rich top-levelontology employed, the computational resources neededto classify the ontology are considerable. In order toalleviate performance issues, TNM-O will be provided asmodules for different organ systems. Thus, the users canimport only the modules of interest into their applicationcontext.Future research should evaluate the presented prototypeontology (i) by implementing further tumour locations,and (ii) by systematic application in clinical classificationand retrieval scenarios. We will provide the formalizationof TNM for other primary tumour locations in a modu-lar way, so that users can select which part of the TNM-Othey would like to use. In this way, we hope to reduce thecomputational resources already needed to a minimum.ConclusionWe presented a first version of an ontology (TNM-O) thatrepresents the TNM tumour classification system. Thepresent work demonstrates its representational power andcompleteness as well as its applicability for classificationof instance data. This work provides a foundation for anexhaustive TNM ontology.Endnotes1 http://www.uicc.org2 http://seer.cancer.gov/seerstat/databases/ssf/3 http://seer.cancer.gov/csr/1975_2013/sections.html4 https://cancerstaging.org/About/news/Pages/8th-Edition-Publication-Date-Announced.aspx5 http://codes.iarc.fr/usingicdo.php6 http://www.who.int/cancer/modules/en/7 http://owlapi.sourceforge.net/8 http://hermit-reasoner.com/9 http://owlapi.sourceforge.net/AcknowledgementsThe article processing charge was funded by the German ResearchFoundation (DFG) and the Albert Ludwigs University Freiburg in the fundingprogramme Open Access Publishing.Authors contributionsMB and SS designed the structure of TNM-O. FF implemented TNM-O forcolorectal cancer, developed the module structure of TNM-O and curatedTNM-O for breast cancer. PB and MB designed the classification study onBoeker et al. Journal of Biomedical Semantics  (2016) 7:64 Page 10 of 11pathology data for which PB provided the pathology dataset and evaluatedthe classification results. The manuscript was primarily drafted by MB and SS,and edited and approved for publication by all authors.Competing interestsThe authors declare that they have no competing interests.Author details1Institute for Medical Biometry and Statistics, Medical Center  University ofFreiburg, Faculty of Medicine, Stefan-Meier-Str. 26, 79104 Freiburg i. Br.,Germany. 2Department of Informatics, University of Minho, Campus deGualtar, 4710-057 Braga, Portugal. 3Tumorbank Comprehensive Cancer CenterFreiburg and Center for Surgical Pathology, Medical Center  University ofFreiburg, Faculty of Medicine, Breisacher Straße 115a, 79106 Freiburg i. Br.,Germany. 4Institute of Medical Computer Sciences, Statistics andDocumentation, Medical University of Graz, Auenbruggerplatz 2, 8036 Graz,Austria.Received: 2 February 2016 Accepted: 25 October 2016Baek and Park Journal of Biomedical Semantics  (2016) 7:55 DOI 10.1186/s13326-016-0094-9RESEARCH Open AccessMaking adjustments to event annotationsfor improved biological event extractionSeung-Cheol Baek1,2* and Jong C. Park1AbstractBackground: Current state-of-the-art approaches to biological event extraction train statistical models in asupervised manner on corpora annotated with event triggers and event-argument relations. Inspecting such corpora,we observe that there is ambiguity in the span of event triggers (e.g., transcriptional activity vs. transcriptional),leading to inconsistencies across event trigger annotations. Such inconsistencies make it quite likely that similarphrases are annotated with different spans of event triggers, suggesting the possibility that a statistical learningalgorithm misses an opportunity for generalizing from such event triggers.Methods: We anticipate that adjustments to the span of event triggers to reduce these inconsistencies wouldmeaningfully improve the present performance of event extraction systems. In this study, we look into this possibilitywith the corpora provided by the 2009 BioNLP shared task as a proof of concept. We propose an InformedExpectation-Maximization (EM) algorithm, which trains models using the EM algorithm with a posterior regularizationtechnique, which consults the gold-standard event trigger annotations in a form of constraints. We further proposefour constraints on the possible event trigger annotations to be explored by the EM algorithm.Results: The algorithm is shown to outperform the state-of-the-art algorithm on the development corpus in astatistically significant manner and on the test corpus by a narrow margin.Conclusions: The analysis of the annotations generated by the algorithm shows that there are various types ofambiguity in event annotations, even though they could be small in number.BackgroundCurrent state-of-the-art approaches to biological eventextraction train statistical models in a supervised learn-ing manner on annotated corpora, where event triggers, orthe expressions indicative of events, and event-argumentrelations, or relations between events and their partici-pant, are annotated (e.g., [1, 2]). The readers are referredto [3] if the tasks are not familiar. Inspecting such corpora,we observed some cases where there is residual ambi-guity in the span of event triggers (e.g., transcriptionalactivity vs. transcriptional). Because of the ambiguity,these gold-standard corpora would manifest inconsisten-cies across the span of event triggers. That is, there wouldbe similar phrases where the span of their counterparts ofevent triggers is differently annotated, and as a result, such*Correspondence: scbaek@nlp.kaist.ac.kr1Department of Computer Science, KAIST, 291 Daehak-ro, Daejeon, Republicof Korea2Agency for Defense Development, Daejeon, Republic of Koreaevent triggers are syntactically characterized in a differ-ent way, suggesting a possibility that a statistical learningalgorithm is hard to generalize from such event triggersthat are similar, but differently annotated in a training cor-pus. We anticipate that adjustments to event annotationsto reduce such inconsistencies would lead to a meaning-fully improved performance of even the state-of-the-artevent extraction systems. In this study, we look into thispossibility with the corpora provided by the 2009 BioNLPshared task [3]. We note that this paper reports an exten-sion of our previous work [4] with detailed discussions andmore experimental results.For example, consider sentence (1) from the trainingcorpora, where the annotated event triggers are set inbold-face.© 2016 Baek and Park. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 2 of 12(1) ... express either decreased or increased numbersof VDR. (PMID:9783909)The phrases decreased and increased numbers areannotated as event triggers of Negative and Positive Reg-ulation events, respectively, that take a Gene Expressionevent with the event trigger express. These annota-tions are justifiable with respect to the meaning of thesephrases, but there are alternatives, including one wherethe phrase increased becomes the trigger of the PositiveRegulation event. Despite the semantic similarity betweenthese two events, their event-argument relations to theGene Expression event are syntactically different (Fig. 1),in that the event trigger decreased is the adjectival mod-ifier (AMOD) of the direct object (DOBJ) of the phraseexpress, while the event trigger increased numbers isthe direct object (DOBJ) of the phrase express. However,if these event triggers are slightly adjusted, for exampleby dropping the word numbers from the event trig-ger increased numbers, these event triggers and event-argument relations will come to have similar to share thesimilarity also in syntactic characteristics with respect tophrasal categories and shortest dependency paths. Theinconsistencies would provide a valuable opportunity forimproving the performance of event extraction, but thecurrent state-of-the-art approaches have not seriouslyaddressed them yet.Note that one may still find that sentence (1) indicatesa Regulation event, not these Positive and Negative Reg-ulation events, but we can leave the identification of theRegulation event to an inference engine that would bedeployed after event extraction systems, since the exactnature of an event can be inferred from the disjunctionof these Positive and Negative Regulation events indicatedby the syntactic construction either A or B.The only reported effort would be to normalize multi-word event triggers into single-word event triggers withthe help of the Head-Word rule, or a rule of taking thesyntactic head word of an event trigger (e.g., [2, 5]), eventhough the rule often makes an apparently bad choice, asin the example above, where it picks out the constituentword numbers from the event trigger increased num-bers, which does not have any meaning relevant to Pos-itive Regulation events, and furthermore, is inconsistentwith the event trigger decreased.In this paper, as a proof-of-concept study, weexamine the benefits of reducing inconsistenciesacross event annotations as follows. First, we use theExpectation-Maximization (EM) algorithm with Viterbiapproximation, where latent variables encode events.Our experimental results show that the unmodified EMalgorithm is defeated by our baseline algorithm, whichis a learning algorithm that successfully trained state-of-the-art event extraction systems [2], in part becauseFig. 1 Dependency Graphs of Example Sentences. The graphs are basic Stanford dependency analyses by the Charniak-Johnson parser with aself-trained biomedical parsing model. In (1) and (4), dashed arrows indicate inferred dependency relations based on conjunctions. In (3), dashedarrows indicate that corresponding dependency relations are naturally expected dependency relations, but are missed in the analysis generated bythe parserBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 3 of 12the EM algorithm adjusts the models to extract similarbut unintended events. To overcome this problem, weuse a posterior regulation technique of consulting thegold-standard annotations in the form of constraints.We come up with four constraints on the possible eventannotations to be explored by the EM algorithm. Theresulting algorithm, to be called the informed EM algo-rithm, turns out to outperform our baseline algorithmon the development corpus in a statistically significantmanner (p?value =9.59 E-12) and on the test corpus bya narrow margin (51.6 % vs. 51.3 %). Thus, we found itbeneficial to make proper adjustments to event triggerannotations. An analysis of the annotations generatedby the algorithm shows that there are various types ofambiguity in event annotations including ambiguity inthe span of event triggers, even though the algorithmfinds only a small number of such cases.To the best of our knowledge, this would be the firststudy where adjustments to the gold-standard annotationsare made, even though there are NLP studies on a similaruse of posterior regularization techniques including theone by Okita and colleagues [6], where partial annotationsof alignment links are incorporated as prior knowledgeinto the word alignment process.The rest of this paper is organized as follows. ThisBackground section ends with the following short subsec-tion Biological Event Extraction Task, which defines thetask of biological event extraction. The Methods sectiondevelops our statistical models and learning algorithms.The Results section presents and analyzes experimentalresults. The Conclusion section presents possible futureresearch directions and concludes this paper.Biological event extraction taskAs a case study, we addressed the event extraction taskas defined in the 2009 BioNLP shared task 1 [3], whichwas later renamed as GENIA Event Task 1 and extendedto cover full papers in the 2011 BioNLP shared task [7],where biological events are used to refer to the changesof a state of one or more biological macromolecules.The task is to extract structured information on eventsfrom sentences in the biological literature, which con-sists of their event type and participants encoded witha controlled vocabulary that consists of nine event typeterms (e.g., Gene Expression) and two role type terms (i.e.,THEME and CAUSE).The nine event types are divided into three groupsaccording to their participants. The first group is plainprotein-taking events that must take a single protein asTHEME (e.g., Gene Expression). The second one ismulti-ple protein-taking events, or events that take one or moreproteins as THEME (e.g., Binding events). The third oneis event-taking events that must take a single protein andevent as THEME and may take a single protein and eventas CAUSE (e.g., Positive Regulation and Negative Reg-ulation). The events of the first group may be viewedas binary relations between event triggers and proteinmentions, but those of the last two groups are differ-ent from binary relations, in that multiple protein-takingevents take more than one argument and event-takingevents allow nested event structures. Thus, the extrac-tion of events poses challenges other than those of theextraction of binary relations, which have been exten-sively studied in the biomedical information extractioncommunity.MethodsFollowing Björne and colleagues [5], we viewed the eventextraction task as constructing directed graphs, whereevent triggers and event-argument relations are encodedwith labeled nodes and edges, respectively. We con-structed these directed graphs with the help of variousresources including syntactic analyses. In this section, wefirst describe these resources used in our event extractionsystem and then develop graph representations, statisticalmodels and learning algorithms, in this order.ResourcesWe used lexical and syntactic analyses to encode tokensand the relation between tokens into statistical mod-els. As for lexical analyses, we used the baseforms andpart-of-speech (POS) tags of the tokens included in theanalyses by the Enju parser, which are available in theofficial website of BioNLP shared tasks (http://weaver.nlplab.org/~bionlp-st/BioNLP-ST/downloads/support-downloads.html). As for syntactic analyses, we use basicStanford dependency analyses by the Enju parser with theGENIA model [8] together with those by the Charniak-Johnson parser [9] with a self-trained biomedical parsingmodel [10], since the Enju parser fails to generate analysesfor a few sentences. These syntactic analyses are alsoavailable in the official website of BioNLP shared tasks.As for protein mentions, we used their gold-standardannotations available on the official website of BioNLPshared tasks, which were given to the participants inthe BioNLP shared tasks. The annotations contain multi-word protein mentions. Since most of them correspond tosyntactic units (i.e., single words and phrases), we can eas-ily combine tokens in multi-word protein mentions intosingle tokens and redirect their dependency relations.Following Miwa and colleagues [1] and Kilicoglu andBergler [11], we developed an event trigger lexicon for eachevent type for the purpose of identifying apparently incor-rect candidates for event triggers as follows. Constituentwords within annotated event triggers in the training cor-pus are scanned one by one. Each scanned constituentword is put into the lexicon that corresponds to thetype of events anchored at the event trigger. When theBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 4 of 12constituent word contains hyphens, it is split by hyphensand the resulting components of the word are put intothe lexicon together with the original constituent word.In a similar manner, we also constructed the stemmedversion of each event trigger lexicon using PorterStemmer.The automatically constructed lexicons would containa number of entries not helpful for checking if a tokenis part of an event trigger. To identify and remove suchentries, we computed the reliability score Rw,e of eachentry w in the lexicon for each event type e, as defined byKilicoglu and Berger [11]:Rw,e = Cw,eCw (1)where Cw,e is the number of times the entry w appearswithin the event trigger of events of type e and Cw is thenumber of times the entry w appears. Finally, we removedentries with reliability scores below 1 %.After this removal, these lexicons still have either a partor the whole of 98 % of the annotated occurrences of eventtriggers in the training corpus, and can be used to identifycandidate pairs of words w and event types e indicatingthat w might be part of an event trigger for event type e,where around 11 % of them are actually part of annotatedevent triggers.Graph representationsLet us consider how to encode multi-word event trig-gers. We came up with the following four possible formsof multi-word event triggers and manually searched thetraining corpus for cases corresponding to each possibil-ity with the help of syntactic analyses by the Charniak-Johnson parser [9] with a self-trained biomedical parsingmodel [10], as shown in Fig. 1.The first is that some event triggers are inherently multi-word expressions, as exemplified in (2), where wordswithin the bold-faced event trigger negative regulatoryof a Negative Regulation Event fully describe the nature ofthe event only together each other:(2) ... contains a novel negative regulatory element ...(PMID:10359895)Second, some words in multi-word event triggers areadjacent to one another, but have no dependency rela-tions among them, suggesting that at least the first and lastwords of each event trigger should be marked. Returningto sentence (2), the two words negative and regulatoryare adjacent to each other and have no dependency rela-tions between them in the generated dependency graph.The third is that some words within multi-word eventtriggers are not consecutive to one another but havedependency relations among them, suggesting that depen-dency relations combining words within event triggersshould be encoded. As an example, consider sentence (3),where the bold-faced word expression is annotated asthe trigger of Transcription and Gene Expression events,which produce the mRNAs and proteins of the geneE-selectin, respectively.(3) ...mRNA and surface expression of E-selectin. ...(PMID:10202027)Our intuition is that the word expression in combi-nation with the word mRNA describes the nature ofTranscription events more fully than the word expressionalone, but only that the words and and surface appearin-between. That is, the words mRNA and expression insentence (3) are not consecutive, but have a dependencyrelation NN between them.Fourth, some words in multi-word event triggers mightnot be consecutive to one another and might not havedependency relations among them either. The effort tomanually find such a case was not successful but we founda similar case. In sentence (4), the words positive andregulatory indicate together the presence of Positive Reg-ulation events (not annotated on the training corpus), butthese two words are not consecutive to each other and arenot linked to each other through dependency relations inthe generated dependency graph where these two wordshave the dependency relation AMOD to elements.(4) ... several positive and negative regulatory elements.... (PMID:1429562)Since these four different types of multi-word event trig-gers would make it complicated to represent the span ofevent triggers in the graphs, and since our focus here isnot on exactly identifying the span of event triggers, wemark only single words within event triggers and encodethe context of these marked words into statistical modelsto exploit other words within the span of event triggers insensing the presence of the event triggers including them.For example, we may mark the word regulatory as theanchor word of the event trigger negative regulatory insentence (2) and encode its contextual features includingthe fact that the word regulatory is adjacent to the wordnegative.One natural candidate for words to be marked is theconstituent words of an event trigger that we can useto encode syntactic relations between the event triggerand other words since we need them anyway, but thisdecision did not help to uniquely determine which wordshould be marked. Another conceivable decision, to bepursued in this article, is that a marked word can be usedin describing as many syntactic relations between eventtriggers and participants as possible so that it is possi-ble to easily find regularities in these syntactic relationsonly from a small number of instances. Henceforth, weBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 5 of 12will call such words meeting these decisions and beingmarked as anchor words. Of course, the choice of anchorwords would be dependent on the way for describingsyntactic relations between words and the training cor-pus, but there are predictable characteristics of anchorwords.First, when an event trigger corresponds to a phrase(e.g., the first and third observations above), the naturalchoice for the anchor word of the event trigger would bethe head word of the phrase, since the dependency pathsbetween the head word and words outside the phrasedo not have other constituent words in the event trig-ger so that the located dependency paths can be used fordifferent event triggers with the same head words. As aresult, in sentence (3), expression is preferable to mRNA.Second, when an event trigger does not correspond toa phrase (e.g., the second and fourth observations above),the natural choice for the anchor word of the event trig-ger would be the word frequently occurring in variousevent triggers for the same event type. Since seven PositiveRegulation event triggers contain positive in the train-ing corpus but only one Positive Regulation event trigger(positive regulatory role) contains regulatory, positiveis preferable to regulatory in sentence (4).Now let us define the desirable output labels of tokens inthe training corpus for trigger identification. All the wordsexcept for anchor words will be given the label negative.Anchor words will be labeled with more than one eventtype, since some event triggers indicating two differenttypes of events share an anchor word as shown in sentence(3), where the word express would be preferable anchorwords for Transcription and Gene Expression events.When turning to the label of edges, a question ariseswhether edges can be labeled with more than one roletype, that is, whether an event takes a protein or anotherevent both as THEME and CAUSE. To answer this ques-tion, we constructed graphs for sentences in the trainingcorpus of 800 annotated abstracts with the Head-Wordrule. There are only six edges labeled with more thanone role (of around 8,200 edges labeled with one or moreroles), suggesting that they are likely to be annotationnoise. As a result, we allow edges to be labeled with atmost one role type.The issues we have discussed so far are also relevant torelations, but there are issues specific to events, includ-ing the one that graphs with cycles and loops may leadto an infinite number of event-taking events with distinctevent participants. As an example, consider Fig. 2, wherethe word in bold-face is the annotated event trigger ofa Gene Expression event and a Positive Regulation eventthat takes the Gene Expression event as THEME. It wouldbe straightforward to derive these Gene Expression andPositive Regulation events from the graph. The problem isthat there is no principled way to rule out another PositiveFig. 2 Event Graph with a LoopRegulation event with the derived Positive Regulationevent as THEME.One way is to disallow graphs with cycles and loops.Constructing graphs for the training sentences, we coulddiscard graphs with cycles and remove loops with someexceptions, since some of the loops would be justifiable.Upon analyzing such loops, however, we came up witha possible explanation for their presence, which is thatthe annotators might have failed to find the appropriatetype for some events in sentences in the limited controlledvocabulary and would have attempted to use the com-bination of more than one component event to presentthe event (merged events). In Fig. 2, Gene Expression andPositive Regulation events with the event trigger overex-pressed exemplify such merged events. Most of the otherloops would be due to words hyphenating protein men-tions and event triggers (e.g., IFNgamma-induced). Weidentified the pairs of Gene Expression and Positive Reg-ulation events making loops and then replace them withsingle merged events.Finally, we point out two differences between our graphrepresentation and the widely used one proposed byBjörne and colleagues [5]. One is that their represen-tation allows only predefined labels of combined eventtypes (e.g., Gene Expression/Positive Regulation), but thatour representation allows any possible labels of combinedevent types. Another is that they do not use mergedevents, while we evaluate the consequences of these dif-ferences, as shown in the Results and Discussion section.Statistical modelGiven a sentence x = (x1 . . . xn), we constructed graphrepresentations of events by finding the most reliableassignment of labels in a complete directed graph with thewords as nodes and removing edges labeled as irrelevantfrom the graph. We measured the reliability of assign-ments of labels in terms of output scores of a modifiedversion of a state-of-the-art model proposed by Riedel andMcCallum [2], since their model does not allow wordswithmore than one event type. They proposed threemod-els ranging from the simplest one, Model 1, to the mostcomplex one, Model 3. Model 3 was ranked the secondin the GENIA Event subtask of the 2011 BioNLP sharedtask and its variant was ranked the first [12]. However,Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 6 of 12we developed our model from Model 1 for convenienceof experiments, since Model 3 was reported to be muchslower than Model 1 in training and predicting.Given an assignment L, our model M first checks if theassignment L satisfies the following two conditions. Oneis that identified anchor words (i.e., constituent words ofevent triggers) have at least one edge labeled with THEMEstarting from them. Another is that all edges labeled with arole type come from identified anchor words. If the assign-ment satisfies the conditions, our model assigns scoresMi,e(Li,e|x) to all the pairs of an event type e and a wordxi (i.e., vertices) and scores Mi,j(Li,j|x) to all the pairs ofwords xi and xj (i.e., edges) and take the sum of thesescores as the scoreM(L) of the assignment L as follows:M(L) =?(i,e)Mi,e(Li,e|x) +?i,jMi,j(Li,j|x) (2)where Li,e takes on a value of either positive or nega-tive, while Li,j takes on a value of either THEME, CAUSEor negative. Now the extraction of events can be viewedas finding the assignment with the highest score. To findthe optimal assignment for a given sentence, we use amodified version of the dynamic programming algorithmproposed by Riedel and McCallum [2]. One may supposethat valid assignments should satisfy other constraints,such as the one that the edge labeled with role types goesto either anchor words or protein mentions. However,such constraints make it hard to efficiently find the opti-mal assignment of graphs. For this reason, the system firstfinds the optimal assignment without such constraints,and if the resulting assignment does not contain anycycles, we attempt to refine the assignment so that it satis-fies all the constraints. For example, the label negative isreassigned to all the incoming edges of a word other thanthe identified anchor words and protein mentions. Whenthe resulting assignment has a cycle, it does not generateany events for the input sentence.We scored pairs of a word xi and an event type e using aweight vector we as follows:Mi,e(positive|x) = we · (xi), (3a)Mi,e(negative|x) = ?we · (xi), (3b)where (xi) is the feature vector of words xi. To definethe feature vectors of words, we used their lexical and lin-ear/syntactic contextual information. Lexical informationabout words is encoded with their surface form, base-form, POS tag and the reliability scores of the entriesderivable from them in each event trigger lexicon. Thereliability scores are encoded as real-valued features.The linear contextual features of a word (e.g., the worddecreased in sentence (1)) include center-marked n-grams of words centered around the word (n = 2-4)and made out of pairs of baseforms and POS tags (e.g.,decrease:VBN) and a special symbol PROTEIN for pro-tein mentions. For example, the center-marked trigrameither:CC [decrease:VBN] or:CC is used as a feature forthe word decreased in sentence (1). They also includethe distance from the word to proteins (e.g., Protein-Distance:5 for the word decreased and the protein VDRin sentence (1)) and the distance from the word to poten-tial anchor words within the sentences relative to them(e.g., Trigger-Distance:2 for the word decreased and theword increased in sentence (1)). The distances of proteinmentions are encoded as binary features (i.e., taking either0 or 1), but features for the distances of potential anchorwords take on the maximal reliability score of the corre-sponding entries in the lexicons. As syntactic contextualfeatures, we encoded their syntactic governors and mod-ifiers (e.g., number:NNS-MOD(amod)-decrease:VBNand numbers:NNS-GOV(dobj)-express:VBP for theword numbers in sentence (1)). Note that these contex-tual features are intended to exploit words other thananchor words in sensing the event triggers including them.We also scored the label L of a word pair (xi, xj) using aweight vector wL as follows:Mi,j(L|x) = wL · (xi, xj), (4)where (xi, xj) is the feature vector of a word pair (xi, xj).To define the feature vector of a word pair (xi, xj), weused the following features based on the features usedin [1]. Our feature vector consists of the lexical andlinear/syntactic contextual features of each of them asdefined above, the length of the shortest paths betweenthem and various representations of substructures ofpaths between them as defined below. First, from a short-est path, we generated the token sequence of the pairsof baseforms and POS tags (e.g., decrease:VBN num-ber:NNS express:VBP for the word decreased and theword express in sentence (1), the dependency sequence ofpairs of the types and directions of dependency relations(e.g., MOD(amod) GOV(dobj) for the word decreasedand the word express in sentence (1), and the token-dependency sequence of the pairs of baseforms and POStags and pairs of the types and directions of depen-dency relations (e.g., decrease:VBN MOD(amod) num-ber:NNS GOV(dobj) express:VBP the word decreasedand the word express in sentence (1). From the result-ing sequences, we generated n-grams of these sequences(n = 2-4).For efficiency, we assign the label negative to thosewords that do not contain any entry in our event trig-ger lexicons. We also assigned the label negative to edgesother than those edges whose starting word contains anyentry in the lexicons and whose ending word either refersto a protein or contains an entry in the lexicons forevents that take proteins. Since about 98 % of the anno-tated occurrences in the training corpus contain an entryBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 7 of 12in the lexicons, this does not incur a large performancepenalty but greatly reduces the size and complexity of theproblem.Learning algorithmsAs a baseline algorithm, we used the online prediction-based Passive-Aggressive algorithm [13] with the costfunction of penalizing false negative event triggers andedges 3.8 times more heavily than false positive ones asin [2], since the algorithm with this setting successfullytrained Model 1 in [2], the one similar to our statisticalmodel. The pseudo-code of this algorithm is shown inFig. 3. It begins with an initial model with all weights setto 0 (line 1). It takes several passes over the training cor-pus D = ((x1, y1), ..., (xN , yN )), where xi and yi are the i-thsentence and the gold-standard graphs that are automat-ically derived from the gold-standard event annotationsusing the Head-Word rule, respectively (line 2). Given asentence xj, it constructs a graph y (line 6) with the helpof the current interimmodelmt,i?1. When it makes a mis-take (i.e., the predicted graph y is not the same as thegold-standard graph yj), we constructed the next interimmodel mt,i, whose output score of yj goes beyond that ofyj by at least the cost c incurred by the mistake, by makinga few modifications to the current interim model mt,i?1to keep the knowledge learned so far as much as possible.We take a total of 20 passes over the training corpus, sav-ing the average Mt of the interim models weight vectorsafter each pass (line 15), since the average Mt of interimweight vectors is less likely to over-fit to the training cor-pus than the individual interim weight vectors as shownby Collins [14]. Here, the total number of passes, that is,20, was arbitrarily chosen, but it turns out that the numberis sufficiently big for learning a statistical model.Fig. 3 Baseline algorithmWith a modified version of the baseline algorithm as theM step, we developed the Informed EM algorithm, or theEM algorithm with a posterior regularization techniqueas shown in Fig. 4, where sentences x and event annota-tions z are observed and assignments y of labels to wordsand word pairs are missing. Since it would be intractableto enumerate all the possible assignments producing thegold-standard event annotations z, we use the Viterbiapproximation to the EM algorithm under the unreason-able assumption that the most probable assignment hasa remarkably higher probability than the second probableassignment. This case may also have the counterpart ofthe Inside-Outside algorithm, or the efficient implemen-tation of the EM algorithm widely used in learning PCFGsin an unsupervised manner, but we leave the design ofsuch an algorithm for future research. To incorporatethe gold-standard annotations into the EM algorithm, weFig. 4 Informed EM algorithmsBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 8 of 12impose constraints on possible assignments, which arederived from the gold-standard annotations.Now we describe the pseudo-code of this algorithm asshown in Fig. 4. We constructed the adjusted annota-tion set D?, where the adjusted graphs yi are initially theircorresponding gold-standard graphs (line 1). It takes sev-eral rounds (line 6), but behaves like the conventionalEM algorithm of alternatively applying the E and M stepsafter the first five rounds (line 7). Here, the number ofrounds for initialization, that is, five, was arbitrarily cho-sen. Since the EM algorithm may converge models intolocal optima, we need to take care of initial models withwhich the EM algorithm begins. During the first fiverounds, we trained the model by applying only the Mstep in a supervised learning manner similar to that ofthe baseline algorithm, since the resulting model wouldbe closer to the true model, if it exists, than randomlyconstructed models. In the E step, it predicts a graph yfor a sentence xi with the current interim model Mt (line8). It sets the adjusted graph yi to the prediction y if theprediction y is not matched with the current adjustedgraph yi and satisfies predefined constraints (lines 10 and11). To enforce models to predict anchor words otherthan the head words of the annotated event triggers,we modify the cost function to penalize errors for sen-tences with updated graphs 10 times more severely thanfor the others as in domain adaptation studies (e.g., [15])(lines 24-26).We came up with the following constraints. One is thebasic constraint that the adjusted graph should encodethe same event types and argument types as the gold-standard graphs. For example, if a Positive Regulationevent with a Gene Expression event as THEME appearsin the gold-standard annotations, this constraint requiresthat one or more Positive Regulation and Gene Expres-sion events appear in the adjusted graphs and that thePositive Regulation events should take a Gene Expres-sion event, but does not take care of their event trig-gers. Another is the confidence constraint such that thepercentage difference in output scores between can-didate graphs y for next adjusted graphs and currentadjusted graphs yj should be equal to or greater thanthe confidence constraint constant ?. To reflect the gold-standard annotations more faithfully, we come up withthe non-overlapping constraint (NOC for short) that twoevent triggers with the same event type in gold-standardgraphs cannot be mapped into a single word in theircorresponding adjusted graphs. For example, considersentence (5).(5) The c-jun mRNA, which is constitutively expressedin human peripheral-blood monocytes at relativelyhigh levels, was also slightly augmented ...(PMID:1313226)The words high levels and augmented indicate thepresence of two distinct Positive Regulation events withthe same Gene Expression event with the event triggerexpressed as THEME. Since they indicate the presenceof events of the same type with the same participants,those assignments where only one of the words is labeledwith Positive Regulation violate the non-overlappingconstraint, but not the first two constraints.Since there might be cases when the event triggers ofmore than one event of the same type can be mergedwithout any problems but when the non-overlapping con-straint prohibits any merging, we came up with a morerelaxed constraint, or the distance constraint that the dis-tance between event triggers in candidate graphs y fornext adjusted graphs and event triggers with the sameevent type in current adjusted graphs yj (e.g., the dis-tance between levels and augmented is four) should beequal to or less than the distance constraint constant ? .In sentence (5), those graphs without any one of the eventtriggers of these two Positive Regulation events would alsoviolate the distance constraint with ? ? 3.Results and DiscussionWe used the baseline and informed EM algorithms totrain our statistical models and evaluated the models onthe development corpus with respect to standard evalua-tion metrics, such as recall, precision and F-score.Evaluation of proposed graph representationsTo measure the consequence of the substitution of singlemerged events for Positive Regulation and Gene Expres-sion events sharing single words, we reconstructed train-ing event annotations by converting the gold-standardannotations into graphs and the resulting graphs intoevent annotations, since events that cannot be encoded ingraphs cannot survive in the reconstruction process. Thesubstitution may decrease the number of events removedafter the reconstruction process but may increase thenumber of incorrect events generated by the graph-to-event conversion algorithm after the reconstructionprocess, leading to changes of the F1-score of the recon-struction event annotations on the original event anno-tations. We measured the F1-score of the reconstructedevent annotations twice, one with the substitution andanother without the substitution. We found that the sub-stitution leads to an increase in the F1-score of thereconstructed event annotations by 1.13 % points, and inparticular for Positive Regulation events by 3.14 % points,though the F1-score for Gene Expression events is slightlydecreased by 0.06 % points.We also measured the consequences of allowing wordswith more than one event type. We used the baselinealgorithm to train multi-label and single-label statisticalBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 9 of 12models. We found that the multi-label models outper-form the single-label models most of the time as shownin Fig. 5. To evaluate the statistical significance of thesuperiority of the multi-label models over the single-labelmodels, we carried out the one-tailed paired Students t-test for the pairs of the two points with the same x value.The reason for the use of the one-tailed test, but notthe two-tailed test, is that only one direction (multi-labelmodels scores > single-labeled models scores) is consid-ered to be against the null hypothesis that the multi-labelmodels are not superior to the single-labeled models.According to the test, the superiority of the multi-labelmodels over the single-label models is shown statisti-cally significant with a p-value of 0.0013. After the firstfive rounds, the more rounds we took to train modelsthe lower performance the resulting models showed. Thiswould be because the models trained by taking manyrounds are likely to over-fit to the training corpus. Wesuggest to stop the learning process of models whenthe models performance on a held-out corpus starts todecrease.Table 1 shows the summary of the performance of themodels of each type. Since the Informed EM algorithmapplies the E step after the first five rounds, to be fair withthe Informed EM algorithm, we calculate averages andsample standard deviations of the F-scores of the modelstrained by taking more than five passes.The single-label models are in fact our implementa-tion of Model 1 of Riedel and McCallum [2], whichwas reported to have the F1-score of 56.2 % for thedevelopment corpus, and the best has a similar F-score of 55.1 %, where the difference may be dueto implementation details regarding the feature vectorconstruction.Table 1 Performance of multi-label and single-label statisticalmodels. These models are trained using the baseline algorithmSingle-label (R/P/F) Multi-label (R/P/F)BEST 46.8/67.0/55.1 47.3/67.7/55.7AVG. 46.2/66.6/54.6 46.6/67.1/55.0(STD.) (0.36/0.41/0.32) (0.23/0.21/0.30)Evaluation of the informed EM algorithmTo examine the effect of the posterior regulation, we firstuse the Informed algorithm without any constraints (thepure EM algorithm) to train models. It is again unsur-prising that the more rounds we took to train models thelower performance the resulting models showed as shownin Fig. 6. As a result, the best one is the model it took sixpasses to train, which shows a recall of 47.12 %, a precisionof 67.04 % and an F-score of 55.34 %.At the first E step, more than a thousand of adjustedgraphs were updated and at subsequent E steps, fewerthan half a hundred graphs were updated, suggesting thatthe models are converging (the total number of sentencesis about seven thousands) and the pure EM algorithmwould have trained models to predict similar but unin-tended graphs.We evaluated our Informed EM algorithm with variousconstraint sets, all of which include the basic constraint,as shown in Tables 2 and 3. The comparison betweenTable 1 on the one hand and Tables 2 and 3 on theother shows that most models outperformmodels trainedby the baseline algorithm in terms of both the best andaveraged F-scores. To assess the statistical significance oftheir superiority over the models trained with the base-line algorithm, we calculate p-values with respect to theFig. 5 Comparison Between Multi-Label and Single-Label Statistical Models. Each point (x, y) indicates that a model trained by taking x rounds hasan F-score of y. These models are trained using the baseline algorithmBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 10 of 12Fig. 6 Comparison Between the Baseline and Pure EM Algorithm. Each point (x, y) indicates that a model trained by taking x rounds has an F-scoreof yone-tailed paired Students t-test for the pairs of modelstrained by taking the same number of rounds, as shown inTable 4.We analyzed the effect of the choice of constraints onthe performance of models. The high confidence con-straint constant ? reduces the number of updates in theadjusted graphs, making the resulting models similar tomodels trained by the baseline algorithm as shown inTable 5. The distance constraint (? = 2) reduces the num-ber of updates in the adjusted annotation set and for mosttimes increases the best F-scores but not the averaged F-scores. The non-overlapping constraint also reduces thenumber of updates but not always increases the best andaveraged F-scores. Note that, even though our best modelis the model we trained with the non-overlapping con-straint, the best combination of constraints would be withTable 2 Best performance of informed EM models? = ? = 2 (R/P/F) ? = 100 (R/P/F)Without NOC0.1 48.0/68.2/56.3 47.6/68.3/56.10.2 47.6/68.6/56.2 47.4/68.5/56.00.3 47.7/68.8/56.3 47.3/67.5/55.70.4 47.1/67.8/55.6 47.6/67.7/55.9With NOC0.1 47.3/68.9/56.1 47.5/68.1/55.90.2 47.3/68.0/55.8 47.5/69.3/56.40.3 48.1/68.9/56.7 47.2/68.1/55.80.4 46.8/68.9/55.8 47.3/67.7/55.7The best figures are set in bold-facethe ? value of 0.3 and the ? value of 2 and without thenon-overlapping constraint as indicated in Table 4.Finally, we chose the best baseline model (a multi-labeled model) and best proposed model (?=0.3, ?=0.2,no use of non-overlapping constraint) in terms of the per-formance on the development corpus and evaluated themTable 3 Average performance of informed EM models? = ? = 2 (R/P/F) ? = 100 (R/P/F)Without NOC0.1 47.9/66.8/55.8 47.3/67.7/55.7(0.27/0.56/0.31) (0.22/0.30/0.23)0.2 47.1/68.0/55.7 47.1/68.1/55.7(0.35/0.86/0.42) (0.22/0.21/0.16)0.3 47.4/67.9/55.8 47.3/66.8/55.4(0.18/0.39/0.23) (0.13/0.22/0.13)0.4 46.7/67.5/55.2 47.0/67.7/55.5(0.38/0.52/0.21) (0.35/0.23/0.30)With NOC0.1 46.9/68.0/55.5 47.1/67.6/55.5(0.23/0.39/0.26) (0.15/0.23/0.16)0.2 47.1/67.6/55.5 47.2/68.3/55.8(0.22/0.29/0.20) (0.22/0.65/0.35)0.3 47.6/68.0/56.0 47.0/67.1/55.3(0.38/0.45/0.40) (0.27/0.36/0.29)0.4 46.5/68.4/55.4 47.1/67.6/55.5(0.33/0.72/0.42) (0.24/0.39/0.22)The best figures are set in bold-face and the sample standard deviations arebracketedBaek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 11 of 12Table 4 p-values for informed EM models? = ? = 2 (w.o/w NOC) ? = 100 (w.o/w NOC)0.1 3.32E-09/1.86E-04 1.03E-06/4.47E-060.2 9.98E-07/1.21E-08 3.58E-09/1.05E-080.3 9.59E-12/3.93E-09 4.38E-06/2.95E-030.4 4.37E-02/1.19E-04 2.50E-08/6.70E-07The best figures are set in bold-faceon the BioNLP09 test corpus. We found that the bestbaseline model has a recall of 42.2 % and a precision of65.5 % and F-score of 51.3 %, while the best proposedmodel has a recall of 42.2 % and a precision of 66.4 % andF-score of 51.6 %, suggesting that the proposed modelsslightly outperform the best baseline model.Analysis of adjusted graphsWe observed updates of shifting themark of anchor wordsfrom empty words into content words (e.g., activityvs. -binding in the noun phrase DNA-binding activity(PMID:9115366)) and from words distant from the partic-ipants of the anchored events into words closer to them(e.g., simulates vs. activation in the phrase simulates theactivation of (PMID:8557975)). There were also updatesof labeling more than one words as the event trigger ofan identical event (e.g., results and increases in a phrasestarting with results in increases of (PMID:2121746)).Unexpectedly, we found that sets of edges were updatedmore often than the position of anchor words. Someedges were copied and redirected (e.g., copies of all edgescoming from results are attached to increase in the pre-ceding example), leading to new events whose presencemakes sense, where some of them have correspondingexisting ones and some of them are completely new. Forexample, a Regulation event of granulocyte-macrophagecolony-stimulating factor was created on sentence (6) withan annotated Regulation event of the Expression event ofthe protein.(6) Regulation of granulocyte-macrophage colony-stimulating factor and E-selectin expression inendothelial cells by cyclosporin A and the T-celltranscription factor NFAT. (PMID:7545467)Table 5 Updated graphs for informed EM models? = ? = 2 (w.o/w NOC) ? = 100 (w.o/w NOC)0.1 72/47 98/500.2 34/18 46/310.3 16/11 25/150.4 9/8 9/5Some edges not used in deriving events from the graphsare removed, leading to the removal of events that seem tobe inferred. For example, sentence (7) below has an anno-tated Positive Regulation event of H2 receptors, which wasremoved by an update. The rationale behind this annota-tion is that a sensible way of usingH2 receptors to increasecAMP and c-fos expression is to activate H2 receptors.(7) Histamine transiently increased cAMP and c-fosexpression throughH2 receptors. (PMID:9187264)Of course, there are inexplicable updates. Table 6 showsthe distribution of types of 16 updates occurring in learn-ing the best proposed model (?=0.3, ?=0.2, no use ofnon-overlapping constraint). It shows that there are vari-ous types of ambiguity, even though the algorithm finds asmall number of each type of cases.ConclusionIn this study, we looked into the possibility that adjust-ments to the annotated span of event triggers to reduceinconsistencies across them lead to an improved per-formance of event extraction systems. In order to makeadjustments automatically in favor of statistical models,we developed an Informed EM algorithm, or the EMalgorithm with a posterior regularization technique thatexploits the gold-standard event trigger annotations inthe form of constraints. The algorithm (the best F-score=56.7 %) is shown to outperform our baseline algo-rithm (the best F-score=55.7 %) on the BioNLP09development corpus in a statistically significant manner(p-value=9.59E-12), indicating that proper adjustmentsof event trigger annotations would be beneficial. Ouralgorithm (F-score=51.6 %) is also shown to slightly out-perform our baseline algorithm (F-score=51.3 %) on theBioNLP09 test corpus. The annotations generated by thealgorithm indicate that there are various types of ambigu-ity in event annotations including ambiguity in the spanof event triggers, even though the algorithm finds only asmall number of such cases.However, there are still remaining issues. First, we usethe Viterbi approximation to the EM algorithm, whosesoundness is not well grounded. We anticipate that thereTable 6 Distribution of types of 16 updatesDescription CountAdding events similar to existing ones 7Adding missing but reasonable events 4Shifting the mark of anchor words 2Removing duplicated and inferred events 2Wrongly adding an incorrect event 1Total 16Baek and Park Journal of Biomedical Semantics  (2016) 7:55 Page 12 of 12would be a counterpart of the Inside-Outside algorithm,an efficient implementation of the EM algorithm usedin learning PCFGs. Second, we fixed the parameters ?and ? as confidence and distance constraint constants,respectively, during training models. However, Smith andEisner [16] show that it would be beneficial for the EMalgorithm guided by prior knowledge to soften the con-straints, as model parameters are converging. We antici-pate that such update scheduling would also be beneficialfor the informed EM algorithm. Third, we applied thisapproach only to the 2009 BioNLP shared task. Since thisapproach is not specific to this task, there is a possibil-ity of applying this approach successfully to similar tasks,such as Infectious Disease (ID) and Epigenetic and Post-translational Modification (EPI) tasks defined in the 2011Bio-NLP shared task. We plan to address these issues inthe future.AcknowledgementsThis work was supported by the National Research Foundation of Korea(NRF)grant fundedby the KoreaGovernment (NSIP)(No. NRF-2014R1A2A1A11052310).Competing interestBoth authors declare that they have no competing interests.Authors contributionsSCB implemented the project and wrote a draft, while JCP designed theproject and revised the manuscript. Both authors read and approved the finalmanuscript.Received: 2 June 2013 Accepted: 15 August 2016RESEARCH Open AccessOverlap in drug-disease associationsbetween clinical practice guidelines anddrug structured product label indicationsTiffany I. Leung1* and Michel Dumontier2AbstractBackground: Clinical practice guidelines (CPGs) recommend pharmacologic treatments for clinical conditions, anddrug structured product labels (SPLs) summarize approved treatment indications. Both resources are intended topromote evidence-based medical practices and guide clinicians prescribing decisions. However, it is unclear howwell CPG recommendations about pharmacologic therapies match SPL indications for recommended drugs. In thisstudy, we perform text mining of CPG summaries to examine drug-disease associations in CPG recommendationsand in SPL treatment indications for 15 common chronic conditions.Methods: We constructed an initial text corpus of guideline summaries from the National Guideline Clearinghouse(NGC) from a set of manually selected ICD-9 codes for each of the 15 conditions. We obtained 377 relevant guidelinesummaries and their Major Recommendations section, which excludes guidelines for pediatric patients, pregnant orbreastfeeding women, or for medical diagnoses not meeting inclusion criteria. A vocabulary of drug terms was derivedfrom five medical taxonomies. We used named entity recognition, in combination with dictionary-based andontology-based methods, to identify drug term occurrences in the text corpus and construct drug-diseaseassociations. The ATC (Anatomical Therapeutic Chemical Classification) was utilized to perform drug name anddrug class matching to construct the drug-disease associations from CPGs. We then obtained drug-diseaseassociations from SPLs using conditions mentioned in their Indications section in SIDER. The primary outcomeswere the frequency of drug-disease associations in CPGs and SPLs, and the frequency of overlap between thetwo sets of drug-disease associations, with and without using taxonomic information from ATC.Results: Without taxonomic information, we identified 1444 drug-disease associations across CPGs and SPLs for15 common chronic conditions. Of these, 195 drug-disease associations overlapped between CPGs and SPLs,917 associations occurred in CPGs only and 332 associations occurred in SPLs only. With taxonomic information,859 unique drug-disease associations were identified, of which 152 of these drug-disease associations overlappedbetween CPGs and SPLs, 541 associations occurred in CPGs only, and 166 associations occurred in SPLs only.Conclusions: Our results suggest that CPG-recommended pharmacologic therapies and SPL indications do notoverlap frequently when identifying drug-disease associations using named entity recognition, although incorporatingtaxonomic relationships between drug names and drug classes into the approach improves the overlap. This hasimportant implications in practice because conflicting or inconsistent evidence may complicate clinical decisionmaking and implementation or measurement of best practices.Keywords: Clinical practice guidelines, Drug labeling, Drug therapy, Information storage and retrieval, Chronic disease* Correspondence: tileung@stanford.edu1Division of General Medical Disciplines, Stanford University, Stanford, CA, USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 DOI 10.1186/s13326-016-0081-1BackgroundClinical practice guidelines provide recommendationsintended to optimize patient care that are informed bya systematic review of evidence and an assessment ofthe benefits and harms of alternative care options [1].Increasingly, guideline developing organizations are ex-pected to produce guidelines based upon a systematicreview of evidence relevant to the scope of the guide-line; for example, guidelines typically are limited inscope to a single condition, and possibly even to a subdo-main of that disease, e.g. screening, prevention, or treat-ment. High-quality CPGs constitute one of the highestlevels of application of evidence-based medicine, based oncomprehensive searches and appraisal of the literature, in-cluding systematic reviews if available [2]. The U.S. Foodand Drug Administration (FDA) provides drug structuredproduct labels (SPLs) for every approved drug. SPLs in-clude structured information such as drug indications,contraindications, and adverse effects. Such labeling isbased on data from clinical trials, and evidence about drugeffectiveness for specific indications or conditions may beprovided. Both CPGs and SPLs are each produced usingdifferent and rigorous methodologies, but with commonintents of promoting evidence-based medical practicesand guiding clinician prescribing decisions. As systematicreviews which form the evidence base for CPG recom-mendations depend upon well-designed clinical trialsand studies of drugs clinical effectiveness, and SPLs areproduced using clinical trials on drug effectiveness, itfollows that the evidence base for CPG recommenda-tions and SPL indications should support similar pre-scribing practices.Text mining of biomedical texts is increasingly per-formed to extract associations from otherwise machine-inaccessible text. Electronic health record documents, suchas clinical notes and discharge summaries, published scien-tific literature, and SPLs are all well-established corpora fortext and natural language processing. Dictionary-basednamed entity recognition (NER) systems and machinelearning approaches have been applied to identify entities,including drugs and diseases, in such texts, [37] however,to our knowledge, text mining of clinical practice guide-lines for these entities had not been done until recently. Ina previous study, we applied dictionary-based NER as atext mining method to identify disease co-mentions forcommon comorbid chronic conditions in chronic diseaseclinical practice guidelines [8]. We focused on 15 commonchronic conditions, including obesity, and 14 of the 15most prevalent chronic conditions among Medicare benefi-ciaries: hypertension, diabetes mellitus, hyperlipidemia,stroke, asthma, atrial fibrillation, Alzheimers dementia andsenile dementias, osteoporosis, chronic obstructive pul-monary disease, depression, chronic kidney disease, heartfailure, arthritis, and ischemic heart disease [9]. In thatstudy, ontologies from Stanford Universitys National Cen-ter for Biomedical Ontologies were compiled into a com-prehensive dictionary of disease concepts for the NER task.Initial evaluation yielded reasonable precision and recallwith this approach. While annotating biomedical or clinicaltext is not a novel concept, the current study uniquely ex-amines clinical practice guidelines, a text corpus not previ-ously studied with this approach. Additionally, the currentstudy aims to demonstrate proof-of-concept of evaluatingdrug-disease associations in clinical practice guidelines.In previous unpublished work, we constructed a diction-ary of drug concepts to use in the NER task of miningpharmacologic treatment recommendations in CPGs. Drugconcepts were utilized as a flat list without utilizing avail-able taxonomic information available [10]. We focused onthe same 15 chronic conditions to examine how well CPGrecommendations about pharmacologic treatment optionsfor the chronic conditions match with SPLs that includeone of these conditions as a treatment indication. Wefound that our recall was low because we did not accountfor drug classes in the drug-disease associations. Wereasoned that differences in the language and structureof CPGs and SPLs may contribute to differences in identi-fied drug-disease associations in CPGs and SPLs. For ex-ample, a CPG on heart failure may recommend using anangiotensin-converting enzyme inhibitor, a drug class ra-ther than a specific drug. However, a SPL for a specificdrug, such as lisinopril, would specify heart failure as an in-dication. In this case, angiotensin-converting enzyme inhibi-tor-heart failure in a CPG drug-disease association shouldalso match a similar drug-disease association in SPLs, suchas lisinopril-heart failure. Simple term recognition methodshave an advantage of scaling well to larger datasets with lit-tle to no impact on accuracy, compared to advanced nat-ural language processing methods [11]. Given CPG text hasnot previously been mined before, it was reasonable toapply NER as the initial method for CPG text processing,although the approach has been applied to clinical notes,biomedical literature, and SPLS previously [37].In this work, we use terminologies with structured hier-archies to improve our approach. We utilized the parent-child relationships from the taxonomic structure of a drugclassification to find class-based matches for ontologicallyrelated terms. To accomplish this, we selected one ontol-ogy that had the highest precision when drug names weremanually reviewed in a subset of guideline recommenda-tions in the text corpus. We hypothesize that drug-diseaseassociations in CPG recommendations should overlapwith drug-disease associations in SPL treatment indica-tions when drug classes and drug names are matched usingtaxonomic relationships. This would suggest that FDA-approved indications for drugs and guideline-recommendedpharmacologic therapies for certain conditions reinforcesimilar evidence-based drug prescribing.Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 2 of 10MethodsFirst, we constructed a text corpus containing guidelinesummaries relevant to the 15 chronic conditions of inter-est. Then, we created a comprehensive vocabulary ofterms for the chronic conditions and performed namedentity recognition to identify drug names and drug classesin the text corpus. Next, we performed an evaluation ofthe method of constructing CPG drug-disease associa-tions. Finally, we compared the overlap between the twosets of drug-disease associations for each chronic condi-tion (Fig. 1). All files used and produced during this studywill be available for download at https://github.com/tileung/DrugsInCPGs.To construct drug-disease associations from the text ofCPG recommendations and SPL treatment indications,we apply text mining methods using an expanded set ofdrug and disease names from multiple terminologicalresources with taxonomic structure, such as NDF-RT(National Drug Formulary  Reference Terminology) andMESH (Medical Subject Headings), and a data source ondrugs, SIDER. A drug-disease association in a CPG is de-fined as the occurrence of a drug name mention at leastone time in a guidelines recommendations. A drug-dis-ease association in a SPL is defined as the occurrence of achronic condition mention at least one time within theIndications section of a SPL.Data sourcesWe used data and resources from multiple publiclyavailable data sources: (1) guideline summaries from theNational Guideline Clearinghouse, (2) drug product labeland indication data from SIDER, (3) chronic disease datadefinitions from the Medicare Chronic Conditions DataWarehouse, and (4) disease and drug ontologies from theNational Center for Biomedical Ontology and ABER-OwlRepository [12].National guideline clearinghouseThe National Guideline Clearinghouse (NGC), first devel-oped in 1997, identifies published CPGs that meet inclu-sion criteria and summarizes their highlights across 54guideline attributes, such as Guideline Title, Major Rec-ommendations, and Target Population [13, 14]. For eachguideline, the Major Recommendations section includessummarized key recommendations as indexed by the Na-tional Guideline Clearinghouse. Each guideline summaryis also tagged with Unified Medical Language System(UMLS) Metathesaurus concepts, identifying major areasof clinical medicine or health care addressed in the guide-line [15]. The NGC then indexes the guideline summarieson a publicly accessible website for retrieval in multipleformats, including XML and HTML. In June 2014, theNGC implemented a new set of inclusion criteria forguidelines included in the NGC repository [1]. As ofSeptember 2015, the NGC featured more than 2400guideline summaries. NGC guideline summaries, in com-bination with a comprehensive drug vocabulary con-structed in this study, were the source of drug-diseaseassociations in CPGs.SIDERSIDER is a publicly available resource that interprets andextracts information from text and tables from FDA-approved drugs SPLs, identifying side effects and medicalconditions in the SPLs using UMLS concepts [15]. EachSPL contains a structured section on Indications, whichspecifies diseases or clinical conditions for which the drugis FDA-approved for use. SIDER 2 was the source of drug-disease associations in SPLs in this study.Medicare chronic conditions data warehouseThe Centers for Medicare and Medicaid Services providesa research database, the Chronic Conditions Data Ware-house (CCW), of Medicare beneficiaries chronic diseasecare. Chronic conditions are defined by ICD-9 codes inthe CCW data dictionary available since 2010 [16].BioPortalThe National Center for Biomedical Ontology (NCBO)[17], based at Stanford University, provides online toolsfor accessing and integrating ontological resources, in-cluding BioPortal, a repository of biomedical ontologies.Fig. 1 Pipeline for generating and comparing drug-disease associations in clinical practice guidelines and structured product labelsLeung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 3 of 10BioPortal contained more than 460 biomedical ontologiesas of September 2015. ATC (Anatomical TherapeuticChemical Classification) was included and obtained fromBioportal because this ontology contains high-level drugclasses as well as related drug formulations and ingredi-ents. For similar reasons, NDF-RT was also included, andwas obtained directly from the National Library of Medi-cine. In NDF-RT, certain parent classes and their childrenwere included, specifically, Chemical/Ingredient, ExternalPharmacologic Class, VA Product, Mechanism of Action,and Therapeutic Categories.Aber-OWL repositoryAber-OWL is a framework that consists of an ontology re-pository, as well as web services that enable ontology-basedsemantic access to biomedical knowledge [12]. Specifically,additional ontologies and their semantic knowledge wereobtained from Aber-OWL, including MESH (MedicalSubject Headings), NCIT (National Cancer InstituteThesaurus), and CHEBI (Chemical Entities of BiologicalInterest Ontology), in order to further expand the drugvocabulary. Only subsets of these ontologies were re-trieved. For instance, we restricted the set of MESH termsto subclasses of organic chemicals, chemical actions anduses, pharmaceutical preparations and polycyclic com-pounds. For NCIT, we restricted to drug, food, chemicalor biomedical material. Finally, for CHEBI we restrictedthe classes to those under role and organic molecule.Guideline recommendations text corpusA corpus of guideline summaries was obtained from thewebsite of the National Guideline Clearinghouse. Previ-ously, guideline summaries were obtained from the NGCwebsite in XML format [8], however, an updated text cor-pus of guideline summaries was constructed in September2015 because the NGC updated inclusion criteria forguideline summaries. We obtained 445 ICD-9 codes fromMedicare CCW data dictionary to identify 14 of the com-mon chronic conditions of interest [9], and added threeadditional ICD-9 codes for the 15th condition, obesity.The 448 ICD-9 codes representing concepts for the 15chronic conditions were then mapped to UMLS conceptunique identifiers (CUIs). Using the NGC RSS feed, avail-able in XML format, a total of 2472 guideline documentswere identified. The mapped CUIs for the 15 chronic con-ditions were used to identify UMLS concepts tagged toeach guideline summary and identified relevant guidelinesummaries for retrieval and build the text corpus. Initially,505 relevant guideline documents were identified. Manualreview of the retrieved guideline documents revealed thatthree were expert commentaries, a different type of NGCsummary, and these were excluded. Guideline summarieswere also excluded from the text corpus if the target pa-tient population for the guideline was exclusively pediatricpatients, pregnant or breastfeeding women, or for a med-ical diagnosis that was not among the 15 common chronicconditions. Additionally, 17 guideline summaries were ex-cluded because they were not available in XML formatfrom the NGC website. After exclusion criteria, 377 NGCrelevant guideline summaries remained for inclusion(Fig. 2). We extracted the Major Recommendations sec-tion from each guideline summary in order to build thetext corpus because this section would be the most likelyof all sections in the summaries to contain pharmacologicrecommendations.Text mining for drug namesWe constructed a comprehensive drug vocabulary of97,079 drug names from five ontologies. We performednamed entity recognition of these drug names in each ofthe 377 guideline summaries and identified 1986 uniquedrug names in the text. We compared the drug-diseaseassociations in CPGs with the drug-disease associationsin SPLs. We also examined the overlap between the twosets of drug-disease associations for each chronic condi-tion. To evaluate the approach, a subset of five heart fail-ure guideline summaries were manually annotated withdrug names and drug classes to build a reference standard,as there is no existing set of annotated CPGs to performthis evaluation. Additionally, CPGs may have representa-tions of drug names or drug classes that may not appearin alternative clinical or biomedical texts. The approachwas evaluated for each of the five ontologies in order toidentify one ontology with the highest precision. Theselected ontology was then applied to map drug namesand drug classes using the existing taxonomic relation-ships and generate the final set of drug-disease associa-tions in CPGs.ResultsWe identified 1986 unique drug names in the corpus of377 clinical practice guideline summaries from the Na-tional Guideline Clearinghouse. We found 1109 uniquedrug-disease associations in CPGs. We identified 533SPLs with an indication for one of the 15 chronic condi-tions. We obtained 449 unique drug-disease associationsfrom the SPLs.EvaluationTo evaluate the approach of identifying drug names inCPG text, one annotator with medical expertise (TL)manually annotated drug names and drug classes in fiveguideline summaries for heart failure to construct a ref-erence standard using TextAE, a text annotation client[18]. To guide manual annotation, occurrences of drugnames and drug classes that can be prescribed were an-notated. For example, in one guideline summary, Forpatients with systolic dysfunction (EF <40 %) who haveLeung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 4 of 10no contraindications: Angiotensin-converting enzyme(ACE) inhibitors for all patientsDigoxin only for pa-tients who remain symptomatic despite diuretics, ACEinhibitors and beta blockers or for those in atrial fibrilla-tion needing rate control, [19] the concepts Angioten-sin-converting enzyme (ACE) inhibitors and ACEinhibitors were annotated drug classes and consideredsynonymous, digoxin was a drug name, and diureticsand beta blockers were drug class names. From theguidelines annotated manually, a total of 178 annota-tions of drug-disease associations were obtained. Wecompared the manual annotations in the referencestandard with the annotations collected from applyingthe constructed drug vocabulary from each of the fiveterminologies: ATC, CHEBI, MESH, NCIT, and NDF-RT. Precision, recall, and F-measure were calculated foreach terminology in order to identify the most appropri-ate terminology for the NER task performed on the clin-ical practice guideline corpus. For this task, a highprecision is desirable, where identified drug names andclasses using one of the terminologies is more predictiveof a true positive identification of a drug name or classin the text. Of the five terminologies, ATC yielded thehighest precision of 0.75 and recall of 0.47 (Table 1). Forthis reason, the taxonomic relationships in ATC wereused to perform drug class and drug name matching toproduce drug-disease associations in CPGs.Drug name and drug class matchingWe utilized ATC (Anatomical Therapeutic ChemicalClassification), a commonly used drug classification pro-duced by the World Health Organization [20], to identifymatches of different drug names to their parent classes.For drugs included in the set of drug-disease associationsin SPLs, ATC codes were applied to the drug names ifavailable by using the World Health Organizations indexof ATC codes. The most specific, or descendent, drugname was used to construct drug-disease associations inCPGs. For example, eplerenone (C03DA04) - ischemicheart disease was a drug-disease association identified inboth SPLs and CPGs; however, aldosterone antagonists(C03DA) - ischemic heart disease was a drug-disease asso-ciation in CPGs only and not in SPLs. In this case, thesewere considered overlapping, or identical, drug-diseaseassociations in CPGs and SPLs because eplerenone is adescendent of the class of aldosterone antagonists. Inanother example, pneumococcal vaccines (J07AL) - heartfailure was a drug-disease association in CPGs, and noadditional descendants of the class of pneumococcalvaccines were identified. In this case, this was included asa drug-disease association in CPGs only. Additionally,drug names that were descendants of the same parentclass were considered similar. For example, eplerenone(C03DA04) - heart failure and spironolactone (C03DA01)- heart failure are similar because both descendants of theclass of aldosterone antagonists (C03DA).In previous work [10], without matching drug namesand drug classes, there was minimal overlap betweenFig. 2 Inclusion diagram for guideline summaries from the National Guideline ClearinghouseTable 1 Evaluation metrics for each drug terminology identifyingdrug names and drug classes in guideline summariesPrecision Recall F-measureATC 0.75 0.47 0.58MESH 0.5 0.02 0.04NCIT 0.31 0.32 0.32NDF-RT 0.25 0.1 0.14CHEBI 0.25 0.02 0.04Leung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 5 of 10drug-disease associations in CPGs and SPLs for all thechronic conditions (Fig. 3). Without taxonomic informa-tion, we identified 1444 drug-disease associations acrossCPGs and SPLs for 15 common chronic conditions. Ofthese, 195 drug-disease associations overlapped betweenCPGs and SPLs, 917 associations occurred in CPGs onlyand 332 associations occurred in SPLs only. After matchingusing taxonomic information, 859 unique drug-disease as-sociations were identified across CPGs and SPLs. Of these,152 of these drug-disease associations overlapped betweenCPGs and SPLs. This means that CPGs mentioned 541drug-disease associations that were not also mentioned inSPLs across all conditions; conversely, SPLs mentioned 166drug-disease associations that were not also mentioned inCPGs across all conditions. The frequency of drug-diseaseassociations in CPGs, SPLs, or both varies depending onwhich chronic disease guidelines are of interest (Fig. 4).DiscussionOur results suggest that guideline-recommended pharma-cologic therapies and drug product label indications arereasonably well-matched when taxonomic relationships be-tween drug names and drug classes are incorporated intothe text mining approach. Our approach, using both drugnames and drug classes, produced superior results overour previous work in which taxonomic relationships werenot incorporated into the text mining approach, whichresulted in a larger number of mismatches betweenguideline-recommended pharmacologic therapies and drugproduct label indications. Overall, the current study dem-onstrated proof-of-concept that NER, in combination withtaxonomic information, can be helpful in identifying drug-disease associations in clinical practice guidelines. It is pos-sible that existing NER and natural language processingsystems could be similarly applied out-of-the-box tothe text corpus [11]. In clinical practice, knowledge ofwhether there is consistent and clear medical evidencein both CPGs and SPLs to support certain prescribingpractices is informative in the medical-decision makingprocess and personalization of care to the individual pa-tient. Additionally, areas of consistency in the medical evi-dence in CPGs and SPLs is supportive in the applicationof such knowledge into best prescribing practices thatcould be incorporated into clinical information and deci-sion support systems, which is the highest level of applica-tion of evidence-based medicine [2].Overlapping drug-disease associationsIn the approach described here, 541 drug-disease associa-tions were identified in both CPGs and SPLs. An increasedoverlap between two sources was obtained when informa-tion about parent-child relationships between drug classesand drug names was used into the text mining approach.Additionally, the text corpus in this work reflects the mostrecent NGC guideline summaries as of September 2015,and exclusion criteria were applied to ensure that relevantchronic condition guidelines were included in the study.As of June 2014, the NGC implemented a new set of inclu-sion criteria to ensure that accepted guidelines provideadequate documentation of their process for systematic re-view of literature as the basis for the recommendations.This resulted in the retirement of existing guidelines in theNGC repository that no longer met the required criteria.Exclusion criteria appropriately ensured that the includedguidelines summaries were applicable to the 15 selectedhighly prevalent chronic conditions in the Medicare popu-lation. As a result of these updates, a corpus of 377 guide-line summaries was included.Fig. 3 Overlap between drug-disease associations between the two sets of drug-disease associations for each chronic condition, without usingtaxonomic information on drug names and drug classesLeung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 6 of 10CPG drug-disease associations not in SPLsOur results demonstrate that CPGs contain drug-diseaseassociations that are not also identified in SPLs. One pos-sible explanation is that the natural language of CPGs andSPLs inherently differ, where CPGs may recommend adrug class for a particular condition rather than a specificdrug. Additionally, NGC guideline summaries may origin-ate from guidelines produced by professional societies ororganizations worldwide, contributing to CPG recommen-dations for similar drugs of the same drug class. We pri-marily addressed this limitation by utilizing hierarchicalrelationships to better match drug-disease associations.However, there remain non-overlapping drug-disease as-sociations. Another possible explanation may be that someCPGs may recommend off-label drug prescribing, whichwould by definition not be found in SPLs. For example,one CPG recommendation for diabetic neuropathy man-agement states to offer a trial of duloxetine, gabapentinor pregabalin if a trial of tricyclic drug does not provide ef-fective pain relief [21]. However, gabapentin does nothave a FDA-approved indication for use in diabetic per-ipheral neuropathy, while duloxetine and pregabalin dohave such an indication in their SPLs. Further, it is pos-sible that CPGs may consider utilizing the best evidenceavailable from less robust studies in making recommenda-tions, even though they are intended to be based onsystematic reviews of best evidence, which may not beavailable. In such cases, CPGs typically also convey thestrength of evidence for the recommendation. This maymean that CPGs might weakly recommend a certain drugin the treatment of a chronic condition. In contrast, sucha process does not exist in the production of SPLs and aSPL for a FDA-approved drug would not suggest prescrib-ing a drug without data on its safety and efficacy fromclinical trials. Weak recommendations in CPGs and off-label indications of drugs in CPGs may present opportun-ities for post-marketing surveillance of the drug for thesuggested prescribing practices or may be opportunitiesfor further study towards drug repurposing.SPL drug-disease associations not in CPGsOur results suggest that there are indications from SPLsthat are not mentioned in CPGs, even though hierarch-ical information improved the overlap. One possible ex-planation is that accurate identification of drug-diseaseassociations in SPLs is necessary. Manual validation ofthe drug-disease associations in SPLs identified fromSIDER would ensure that there is in fact an FDA-approvedindication for one of the chronic conditions in each SPL.Another possible explanation is that there may be a delayin integrating the evidence from FDA-approved treatmentindications into CPG recommendations. Guideline devel-opment can be a prolonged and labor-intensive process,during which new evidence may become available before aguidelines finalization and approval. This would requirefurther investigation, and may also present important op-portunities to streamline the process of implementingmedical evidence on the efficacy of newly approved drugsinto CPG updates and best practices in clinical medicine.LimitationsOur approach is not without limitations. First, a primaryobjective of this study was to demonstrate proof-of-conceptthat NER, in combination with taxonomic information, canbe helpful in identifying drug-disease associations in clinicalpractice guidelines. Out-of-the-box NER and natural lan-guage processing systems may perform as well as or betterthan the current approach. Additionally, the evaluation ofFig. 4 Overlap between drug-disease associations between the two sets of drug-disease associations for each chronic condition, using taxonomicinformation on drug names and drug classesLeung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 7 of 10the CPG text mining method could be more robust including a larger manually annotated set of guidelinesummaries, for a variety of chronic conditions and by atleast two annotators. Annotating a larger set of guidelinesfor the evaluation, rather than focusing solely, on heart fail-ure, facilitate a more robust evaluation. Second, utilizingexisting annotated clinical or biomedical corpora [4, 22]would allow evaluation of the approach against existingNER tools.Additional limitations include the guideline repositoryused to construct the text corpus. Because we utilizedthe NGC-formulated Major Recommendation section ofguideline summaries, these may not contain all the drugmentions found in the full-text documents as publishedby the original developers. However, if recommendationswere made on pharmacologic treatments, then thesewould likely be identified in the NGC guideline summar-ies. Accessing and text mining corresponding full text arti-cles will properly assess whether significant differencesexist. Additionally, the National Guideline Clearinghouseis the largest available guideline repository that also has awell-indexed, structured, and selective set of guidelines forinclusion. While the current approach was designed tofacilitate the process of examining a large corpus of guide-lines, after NGC inclusion as well as application of inclu-sion criteria for this investigation, a relatively small set ofguideline summaries remained. Further improvements ofthe text mining approach may be necessary to ensure ac-curate information retrieval from the small set of guidelinesummaries.Examining drug names and drug classes occurring intext may not be adequate alone. Co-reference resolution,in which a named entity may reference the entity ofinterest, is a common challenge in text mining and nat-ural language processing tasks and also may impact thecurrent findings. Additionally, we do not extract a pre-cise relationship between a drug mention in a diseaseCPG. Relation extraction of the context of drug name orclass occurrence in the text may better inform the drug-disease association identified [23]. Here, we performedan initial assessment of drug-disease associations inCPGs, and additional methods may better disambiguatethe meaning and context of each drug mention in thetext. Initial manual examination of a random sample of20 drug-disease associations yielded seven types of indi-cation relationships (true positives), five other types ofrelationships (false positives), and two drug-disease associ-ation misclassifications. These findings can inform futurework improving the text mining approach. Expanding thereview to 30 drug-disease associations yielded a higher fre-quency of all of the indication relationships but did notchange the numbers of types of relationships identified onthe initial review. Of the 30 drug-disease associations,there were two drug misclassifications where the contextof a drug mention in CPG text was not as a prescribabledrug (stroke-oxygen and diabetes mellitus-glucose). Of theremaining 28 drug-disease associations and their sourceguideline summaries, nine overlapped with drug-diseaseassociations in SPLs. Of the drug-disease associationsidentified, there were 27 occurrences of seven types of in-dication relationships, including having an indication: (1)for only the primary disease, (2) for a patient characteristicpresent with the primary disease, (3) in a specific clinicalsetting, (4) in combination with another drug for the pri-mary disease, (5) as alternative or non-first-line therapy,(6) for prevention of a comorbid condition, and, mostfrequently, (7) for the primary disease when another co-morbid condition was also present. These relationshipsrepresent true positives of drug-disease associations inCPGs. Additional relationships included: drug causes thedisease as an adverse effect, drug has a contraindicationfor the primary disease, drug necessitates additional moni-toring requirements in the setting of the disease, and norecommendation can be made about the indication of adrug for a disease. These relationships represent false pos-itives of drug-disease associations. The risk of false posi-tives may be mitigated by additional text processingdepending on the relationship extracted, for example, forcontraindications, examining the Contraindications sec-tion of a SPL may be a useful task. Among the remaining20 drug-disease associations in CPGs that did not overlapwith SPLs, four of the indication relationships were repre-sented (for only the primary disease, in combination withanother drug for the primary disease, for prevention of acomorbid condition, and for the primary disease when an-other comorbid condition was also present). In somecases, the indication was off-label for the primary disease,for example, one guideline on atrial fibrillation states,Where oral anticoagulants are unavailable, cliniciansmight offer a combination of aspirin and clopidogrel [24].In this case, clopidogrel has FDA-approved indicationsonly for acute coronary syndrome and recent myocardialinfarction, stroke or established peripheral arterial disease[25]. Detailed and thorough manual review of a larger setof drug-disease associations would provide additionalinsight about the types of relationships between drugs anddiseases in CPGs, and also would be informative in improv-ing the text mining approach. Although labor-intensive toperform a detailed review manually, this would be an im-portant contribution to this field, as clinical practice guide-lines have only recently been used as a corpus for textmining.Finally, SIDER is a database of curated drug-side effectpairs as the primary database, but may include false posi-tives when examined for drug-disease associations in theindications sections of SPLs. At the time of revised sub-mission of this manuscript for publication, a newer ver-sion of SIDER was published that determines its accuracyLeung and Dumontier Journal of Biomedical Semantics  (2016) 7:37 Page 8 of 10against other resources and estimates adverse event mis-classification [26]. Utilizing the latest dataset of SPLs fromSIDER 4 may facilitate more accurate identification ofdrug-disease associations from SPLs for future work.ConclusionsOur work offers a first look at the overlap in CPG andSPL content with respect to drug-disease associations.Our results suggest that CPG-recommended pharmaco-logic therapies and SPL indications do not overlap fre-quently when identifying drug-disease associations usingnamed entity recognition, although incorporating taxo-nomic relationships between drug names and drug classesinto the approach improves the overlap. Mismatches be-tween guideline-recommended pharmacologic therapyand FDA-approved drug indications may have a numberof implications, including presenting practical challengesin evidence-based clinical practice, such as adding com-plexity to clinical decision making and implementation ormeasurement of best practices. Further evaluation and im-provement of our methods may be necessary, includingexamining the relationship between a chronic conditionand a drug in a guideline or drug label. Additionally, man-ual annotation of a larger reference standard or use ofexisting annotated biomedical or clinical corpora will berelevant to evaluate our approach and how well it per-forms for each chronic condition. Finally, a detailed man-ual review of areas where drug-disease associations do notoverlap between CPGs and SPLs would be informative,potentially guiding opportunities for further investigationabout areas of uncertainty in drug prescribing and theirindications. This study is a first step towards further un-derstanding of CPGs and SPLs as congruent resources forevidence-based clinical practice.AcknowledgementsWe thank the Bio-Ontologies Special Interest Group for their recognition of thisproject with an Outstanding Presentation Prize at the groups workshop at the23rd Annual International Conference on Intelligent Systems for Molecular Biology.We thank our anonymous reviewers for their thorough and insightful commentsthat improved the quality of the manuscript.Availability of data and materialsAll files used and produced during this study will be available for downloadat https://github.com/tileung/DrugsInCPGs.Authors contributionsTL carried out text corpus construction and design of the study, performedprogramming for text mining drug-disease associations in guidelines, evaluatedthe method, and drafted the manuscript. MD carried out vocabulary construction,generated queries to obtain drug-disease associations from structured productlabels, and helped draft the manuscript. Both authors read and approved the finalmanuscript.Authors informationTL is a Clinical Assistant Professor and practicing general internist inStanford Primary Care and MD is an Associate Professor of Medicine inStanford Department of Medicine.Competing interestsThe authors declare that they have no competing interests.Author details1Division of General Medical Disciplines, Stanford University, Stanford, CA, USA.2Stanford Center for Biomedical Informatics Research, Stanford University,Stanford, CA, USA.Received: 1 November 2015 Accepted: 24 May 2016Conway et al. Journal of Biomedical Semantics  (2016) 7:5 DOI 10.1186/s13326-015-0043-zSOFTWARE Open AccessDeveloping a web-based SKOS editorMike Conway1*, Artem Khojoyan2, Fariba Fana3, William Scuba1, Melissa Castine1,Danielle Mowery1, Wendy Chapman1 and Simon Jupp4AbstractBackground: The Simple Knowledge Organization System (SKOS) was introduced to the wider research communityby a 2005 World Wide Web Consortium (W3C) working draft, and further developed and refined in a 2009 W3Crecommendation. Since then, SKOS has become the de facto standard for representing and sharing thesauri, lexicons,vocabularies, taxonomies, and classification schemes. In this paper, we describe the development of a web-based,free, open-source SKOS editor built for the development, curation, and management of small to medium-sizedlexicons for health-related Natural Language Processing (NLP).Results: The web-based SKOS editor allows users to create, curate, version, manage, and visualise SKOS resources.We tested the system against five widely-used, publicly-available SKOS vocabularies of various sizes and found thatthe editor is suitable for the development and management of small to medium-size lexicons. Qualitative testing hasfocussed on using the editor to develop lexical resources to drive NLP applications in two domains. First, developing alexicon to support an Electronic Health Record-based NLP system for the automatic identification of pneumoniasymptoms. Second, creating a taxonomy of lexical cues associated with Diagnostic and Statistical Manual of MentalDisorders (DSM-5) diagnoses with the goal of facilitating the automatic identification of symptoms associated withdepression from short, informal texts.Conclusions: The SKOS editor we have developed is  to the best of our knowledge the first free, open-source,web-based, SKOS editor capable of creating, curating, versioning, managing, and visualising SKOS lexicons.BackgroundThe Simple Knowledge Organization System (SKOS)standard was introduced to the wider community by a2005 World Wide Web Consortium (W3C) working draft[1], and further developed and refined in a 2009W3C rec-ommendation [2, 3]1. Since then, SKOS has become the defacto standard for representing thesauri, lexicons, vocab-ularies, taxonomies, and classification schemes, both asa useful data format in its own right, and as a meansfor sharing resources on the semantic web. In this paper,we describe the development of a web-based, free, open-source SKOS editor suitable for the creation and curationof knowledge organization systems in general, and health-related lexicons designed to support clinical NaturalLanguage Processing (NLP) in particular.SKOS is a flexible standard designed to represent andencode a wide number of different types of knowledge*Correspondence: mike.conway@utah.edu1Department of Biomedical Informatics, University of Utah, 421 Wakara Way,Salt Lake City, UT 84108, United StatesFull list of author information is available at the end of the articleorganization systems, including vocabularies, thesauri,and classification systems. The standard is widely usedby governments [4] (e.g. United Kingdom Public SectorVocabularies, French National Library Subject Headings,United States Library of Congress Subject Headings),scientific bodies (e.g. International Virtual ObservatoryAlliance Astronomy Vocabulary,NASA vocabularies,The-saurus for the Social Sciences), and non-governmentalorganisations (e.g. Wikipedia categories, UNESCO The-saurus, General Multilingual Environmental Thesaurus).In contrast to its sibling World Wide Web Consortiumsemantic web standard, the Web Ontology Language(OWL), SKOS follows the principle of minimal ontolog-ical commitment [3]. That is, SKOS concepts and rela-tions are lightly specified, using thesaurus-style relationslike broader rather than logically formalised relationscommonly used in OWL (e.g. IS_A).SKOS models consist of concept schemes which serveas containers for concepts. Concepts can be related© 2016 Conway et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 2 of 9together in various ways to create a hierarchical struc-ture. The most important of these semantic relationsare: skos:broader can be read as has broaderconcept. For instance, the relation PHOTOPHOBIAskos:broader VISIONPROBLEM, asserts thatPHOTOPHOBIA has broader conceptVISIONPROBLEM. skos:narrower which can be read as hasnarrower concept. For instance, the relationVISIONPROBLEMskos:narrower PHOTOPHOBIA, asserts thatVISIONPROBLEM has narrower conceptPHOTOPHOBIA. skos:related can be read as is related to. Forinstance, the relation PHOTOPHOBIAskos:relatedDIPLOPIA, asserts that PHOTOPHOBIA is related toDIPLOPIA.Each SKOS concept can be associated with several typesof lexical labels: skos:prefLabel (preferred label ) provides amechanism to link a preferred label to a concept. TheprefLabel is the primary means of referring to aconcept. Only one prefLabel per language should beassigned to each concept. For example, the SKOSconcept FEVER could have the skos:prefLabelfever@en (note that @en refers to Englishlanguage). skos:altLabel (alternative label ) provides amechanism to specify synonyms or near-synonymsfor a given concept. For example, the concept FEVERcould have the skos:altLabel febrile@en.This relation is especially useful for specifyingsynonymous terms necessary for NLP. skos:hiddenLabel (hidden label ) provides amechanism to specify non-standard synonymousterms (e.g. misspellings, typographical errors). Forexample, the concept FEVER could have theskos:hiddenLabel feber@en. Hidden labelsare particularly useful for encoding commonmisspellings necessary for NLP systems.In addition to the semantic relations and lexical labelsdescribed above, SKOS also provides facilities to add addi-tional metadata to concepts and map SKOS concepts toexternal vocabularies.Given its lightweight semantics, SKOS is particularlysuitable as a basis for the development and sharing ofvocabularies to support NLP tasks. A key part of the work-flow in developing some NLP systems  in particularNLP systems designed to process health-related text  isthe development of custom lexicons, including commonabbreviations, synonyms (including slang terms), andtruncations [58].Since its inception in 2005, significant effort has beenexpended on the development of software tools for theSKOS standard, in particular in editing and viewing SKOSvocabularies. Whilst OWL editors, such as Protégé2, canbe used to create and edit SKOS, they require a userto understand SKOS in terms of OWL; an unnecessaryoverhead for a user simply interested in creating SKOS.Furthermore, a major requirement for a SKOS editingtool is the ability to visualise and navigate SKOS con-cept scheme broader/narrower hierarchies, a functional-ity that is unlikely be supported by generic OWL andRDF (Resource Description Framework) tools. Notableexamples of SKOS aware tools include a SKOS Appli-cation Programming Interface (API) and editing module[9] for Protégé 43 (the Protégé SKOS Editor), PoolParty,an online SKOS editing and manipulation tool [10], andSKOS functionality built into the TopBraid ComposerRDF editing platform [11]4, all of which facilitate thecreation, development, and utilisation of SKOS vocab-ularies. However, to the best of our knowledge, untilnow no free, open-source, web-based SKOS editor hasbeen available to the research community (note that Pool-Party, although web-based, is a commercial product). Inthis paper, we present a web-based SKOS editing toolthat is suitable for developing and modifying the health-related lexicons necessary for large-scale informationextraction from clinical notes and other health-relatedtext, yet is also general purpose enough for any small-to-medium-sized SKOS vocabulary development or curationproject.ImplementationA key advantage of using a web-based editor, is that itcan be used anywhere, on any machine, without com-plex user installation. Given that our target users areclinicians, public health workers, and domain experts i.e. those with little or no experience of semantic weblanguages  rather than informatics professionals, easeof use is an important requirement. We took the deci-sion to simplify the editors user interface as much aspossible, hiding some of the general OWL/RDF func-tionality available in tools like Protégé and TopBraidComposer.Considerable effort was expended on designing the userinterface (a screenshot of the system is shown in Fig. 1showing a SKOS thesaurus designed to drive a NLP sys-tem for the automatic identification of biosurveillance-relevant symptoms from Electronic Health Records(EHRs) [12]). After some experimentation, we adoptedan interface that consists of three panes, from left toright:Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 3 of 9Fig. 1 Screenshot of the system interface showing a biosurveillance lexical resource, with a new concept pop-up CONCEPT PANE: An editable taxonomic hierarchy ofSKOS concepts representing skos:broader andskos:narrower relations, which the user can clickon to expand and collapse the tree RELATIONS PANE: An editable list of relationsbetween concepts, particularly the skos:related,skos:broader, and skos:narrower relations LINGUISTICS PANE: An editable list of lexical itemsrelated to each SKOS concept (e.g.skos:prefLabel, skos:altLabel,skos:hiddenLabel)We identified six core functionalities necessary for theeditor, partially based on the requirements identifiedby [9]: Create, edit, and delete SKOS entities Assert SKOS relationships between SKOS concepts(e.g. broader/narrower) Assert and edit skos:prefLabel,skos:altLabel, and skos:hiddenLabel dataproperties Visualise broader and narrower relationships in abrowsable hierarchical tree Support for SKOS documentation properties Provide alternative renderings (e.g. multilingualprefLabels) within the editorAdditionally, our editor provides versioning, and a Wiz-ard tool to expedite the SKOS concept hierarchy creationprocess.In building our web-based SKOS Editor, we relied heav-ily on existing OWL, SKOS and RDF tooling, in particular,the SKOS API [9] (developed by author Jupp) and theOWL API [13]. The system is a Liferay portlet applicationthat uses a standard Model-View-Controller architectureimplemented using the following technologies: Business (Model) Layer: Java SKOS API and OWLAPI Presentation (View) Layer: JavaScript/JSP/JQueryLibraries provides a rich web 2.0 user interfaceconnected to the middle layer via AJAX calls Controller/Middle Layer: The Liferay Portletapplication using the JSR 286 Portlet frameworkconnects the presentation layer to the SKOS API, aswell as providing user management, authorisation,and authentication.Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 4 of 9AMySQL database is used to save files and file versions,as well as user specific settings. The application is a SinglePage Application, with all server/client communicationbased on Ajax calls using a JQuery library (client-side) andLiferay portlet (server-side).A screenshot of the system interface is shown in Fig. 1and a diagram representing the system architecture isshown in Fig. 2.Results and discussionThe web-based SKOS editor allows a user to upload aSKOS file from their local machine for editing, load aSKOS file from a URL, create a SKOS file ab initio,and download an edited SKOS file to a local machine.Furthermore, the editor supports versioning of SKOSfiles, and provides a GUI-based Wizard to expeditethe creation of concept hierarchies. The Wizard allowsa user to input a plain-text tab indented concept hier-archy, a functionality that has been shown in our qual-itative user testing to expedite the hierarchy creationprocess (see Fig. 3 and Additional file 1). The tool takesits inspiration from the Protégé SKOS editor developedby author Jupp, and supports core SKOS functional-ities. In the Concept Pane, SKOS concept schemesand concepts can be created and manipulated with ahierarchical tree structure. The Relation Pane showshierarchical relations defined in the concept pane, andallows these relations to be modified, including the addi-tion of non-hierarchical relations between concepts. TheLinguistics Pane allows lexical information  prefLa-bels, hiddenLabels, altLabels  to be associated with eachconcept.While there have been attempts at developing bestpractices for SKOS thesauri development (e.g. [3, 14])considerable heterogeneity exists between different SKOSresources [15]. We built an editor that is designed to han-dle even those SKOS resources that do not adhere tosuggested best practice (e.g. the thesauri has more thanone prefLabel for a specified language, or a SKOS conceptexists outside a Concept Scheme).Loading and editing sample SKOS vocabulariesIn order to demonstrate and test the capacities of theSKOS editor, we tested the performance of the editor inexecuting some key editing functions. To test the editor,we used an Apple MacBook with 16GB of memory andthe Firefox web browser (version 32).We chose five widelyused SKOS resources:Fig. 2 Flowchart describing system functionalityConway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 5 of 9Fig. 3 Concept creation Wizard designed to expedite the creation of SKOS concept hierarchies STW for Economics Thesaurus is used for indexingeconomics research papers [16] New York Times (NYT) Subject Descriptions is usedto index NYT news stories [17] United Kingdom Archive Thesaurus is a generalpurpose subject heading thesaurus developed by theUK government [18] Australian Curriculum Thesaurus, a resourcedeveloped by the Australian government formanaging educational resources [19] The UNESCO United Nations Educational,Scientific and Cultural Organization  Thesaurusprovides general subject terms across the fields ofeducation, culture, natural science, social and humansciences, communication, and information [20]Table 1 shows the capabilities of the editor in edit-ing large thesauri, where it can be seen that the 5.1 MBUNESCO Thesaurus took six seconds to load into thetool. However, larger thesauri  e.g. the UK Archive The-saurus at 9.4 MB  do not load quickly due to limitationswithin the Liferay web framework. The tool is primarilydesigned for developing relatively small, linguistically-oriented vocabularies. In addition to testing whetherTable 1 General functioning evaluationTHESAURUS SIZE #CONCEPTS LOADING TIME HIERARCHY1 PREFLABEL2 SAVE3STW Thesaurus 15MB 6584 40 sec Y Y YNYT Subj Descriptions 1.4 MB 499 3 sec Y Y YUK Archive Thesaurus 9.4 MB 13,976 120 sec Y Y YAus. Curr. Framework 312 KB 170 0.5 sec Y Y YUNESCO Thesaurus 5.1 MB 44408 6 sec Y Y Y1Is the SKOS broader/narrower hierarchy rendered correctly? [Y or N]2Are the prefLabels rendered correctly? [Y or N]3Can the file be successfully edited, saved, then reopened? [Y or N]Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 6 of 9various existing SKOS vocabularies could be loaded intothe tool and rendered correctly, for each of the SKOS the-sauri evaluated, we tested basic editing functionality (e.g.whether a new concept could be created and inserted intothe existing thesauri, whether concepts could be deleted).The results of this evaluation are shown in Table 2. Notethat even very large vocabularies (e.g. STW Thesaurus)could be edited successfully using the tool.Qualitative evaluationOur qualitative evaluation of the SKOS editor centredon two use cases. For the first use case, an experiencedknowledge engineer (author Castine) used the SKOS edi-tor to build a lexical resource to drive an EHR-orientedNLP algorithm based on the Centers for Disease Controlpneumonia definition (see Fig. 4 for a screenshot of theresulting SKOS resource). The pneumonia resource tooka total of 40 min to build using the Web SKOS Editor, asopposed to the Protégé SKOS Editor Plug-in, which took45 min. Note that the knowledge engineer did not usethe Wizard concept creation functionality, a tool whichwe believe is likely to expedite the concept hierarchycreation process substantially. For the second use case,an experienced NLP researcher (author Mowery) usedthe tool to develop a resource designed to map lexicalcues to Diagnostic and Statistical Manual of Mental Dis-orders (DSM-5) diagnoses with the goal of facilitatingthe automatic identification of symptoms associated withdepression from short, informal texts [21] (see Fig. 5 fora screenshot of the SKOS resource creation process). Thedepression resource took less than one hour to create,and it was reported that the Wizard greatly expedited theconcept creation process. However, several enhancementswere suggested, including the development of an auto-save feature, and the ability to configure default values forlanguage labels (for example, default to English  @enlabels).LimitationsWhile the SKOS editor is suitable for building andcurating special purpose SKOS vocabularies to runbespoke clinical NLP systems, it does have several limita-tions: It is not suitable for editing very large SKOSvocabularies As the tool is built around the SKOS API [9], somelanguage features outside core SKOS [1] are notsupported (e.g. skos:closeMatch,skos:relatedMatch).Future directionsOur long-term goal is to integrate the SKOS editor as alexicon development and management module within acomprehensive platform for developing clinical NLP algo-rithms. As part of this long term goal  and informed bythe comments and suggestions of our early users  weplan three major system enhancements: In the medium term, we plan to add multi-userfunctionality and collaborative editing to the system. We plan to include the ability to search othervocabularies  in particular the UMLS (UnifiedMedical Language System) [22]  from within theeditor interface in order to expedite the synonymidentification process. We plan to extend the current documentation andtutorial materialConclusionsThe SKOS editor we have developed is  to the best ofour knowledge  the first free, open-source, online, SKOSeditor capable of creating, curating, versioning, and man-aging SKOS vocabularies. The editor is free to use5 andthe source code is available under an Apache Version 2.0License.Availability and requirementsProject name:Web-based SKOS editorProject home page: An instantiation of the tool isavailable at http://blulab2.chpc.utah.edu:8080/web/guest/skos. Source code is released under an open-source licenseTable 2 Editing functioning evaluationTHESAURUS NEW CONCEPT1 DELETE NEW CONCEPT2 ADD/EDIT PREFLABEL3 ADD TOPCONCEPT4STW Thesaurus Y Y Y YNYT Subj Descriptions Y Y Y YUK Archive Thesaurus Y Y Y YAus. Curr. Framework Y Y Y YUNESCO Thesaurus Y Y Y Y1Can a new SKOS concept be created (Y/N)?2Can a SKOS concept be deleted (Y/N)?3Can a SKOS PrefLabel be added and edited (Y/N)?4Can a SKOS Concept be added and edited (Y/N)?Conway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 7 of 9Fig. 4 Pneumonia lexical resource based on Centers for Disease Control definitionFig. 5 Building a depression lexicon  entering a preferred labelConway et al. Journal of Biomedical Semantics  (2016) 7:5 Page 8 of 9and can be found at the University of Utahs BiomedicalLanguage Understanding Lab GitHub page https://github.com/Blulab-UtahOperating system:Multi-platform, browser-basedProgramming languages: Java, JavaScriptOther requirements: No other requirementsLicense: Apache 2.0 LicenseAnyrestrictionstouse by non-academics:No restrictionsEndnotes1Note that additional SKOS tutorial material isDATABASE Open AccessThe Cell Ontology 2016: enhanced content,modularization, and ontologyinteroperabilityAlexander D. Diehl1*, Terrence F. Meehan2, Yvonne M. Bradford3, Matthew H. Brush4, Wasila M. Dahdul5,6,David S. Dougall7, Yongqun He8, David Osumi-Sutherland2, Alan Ruttenberg9, Sirarat Sarntivijai2, Ceri E. Van Slyke3,Nicole A. Vasilevsky4, Melissa A. Haendel4, Judith A. Blake10 and Christopher J. Mungall11AbstractBackground: The Cell Ontology (CL) is an OBO Foundry candidate ontology covering the domain of canonical,natural biological cell types. Since its inception in 2005, the CL has undergone multiple rounds of revision andexpansion, most notably in its representation of hematopoietic cells. For in vivo cells, the CL focuses on vertebratesbut provides general classes that can be used for other metazoans, which can be subtyped in species-specificontologies.Construction and content: Recent work on the CL has focused on extending the representation of various celltypes, and developing new modules in the CL itself, and in related ontologies in coordination with the CL. Forexample, the Kidney and Urinary Pathway Ontology was used as a template to populate the CL with additional celltypes. In addition, subtypes of the class cell in vitro have received improved definitions and labels to provide formodularity with the representation of cells in the Cell Line Ontology and Reagent Ontology. Recent changes in theontology development methodology for CL include a switch from OBO to OWL for the primary encoding of theontology, and an increasing reliance on logical definitions for improved reasoning.Utility and discussion: The CL is now mandated as a metadata standard for large functional genomics andtranscriptomics projects, and is used extensively for annotation, querying, and analyses of cell type specific data insequencing consortia such as FANTOM5 and ENCODE, as well as for the NIAID ImmPort database and the CellImage Library. The CL is also a vital component used in the modular construction of other biomedicalontologiesfor example, the Gene Ontology and the cross-species anatomy ontology, Uberon, use CL to supportthe consistent representation of cell types across different levels of anatomical granularity, such as tissues andorgans.Conclusions: The ongoing improvements to the CL make it a valuable resource to both the OBO Foundrycommunity and the wider scientific community, and we continue to experience increased interest in the CL bothamong developers and within the user community.* Correspondence: addiehl@buffalo.edu1Department of Neurology, University at Buffalo School of Medicine andBiomedical Sciences, Buffalo, NY 14203, USAFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 DOI 10.1186/s13326-016-0088-7BackgroundThe Cell Ontology (CL) was initially developed in 2004with the goal of representing knowledge about in vivoand in vitro cell types [1]. Cells are a fundamental unitof biology, and most other entities in biology have directrelationships to identifiable cell types, for example par-ticular proteins being produced by unique cell types, tis-sues and organs containing specific combinations of celltypes, or biological processes being dependent on par-ticular cell types. Cells therefore are an obvious set ofentities to represent ontologically, and provide a usefulpole for organizing and driving data acquisition and ana-lysis in biology.The content in the CL is populated via gradual and enmasse class additions, most notably through several roundsof improvements to representation of hematopoietic cellsin the ontology [24]. Originally, the CL was designed toinclude cell types from all major model organisms includ-ing both plants and animals [1]. However, as a result ofcommunity interest and severe resource limitations, con-tinuing development of the CL currently focuses primarilyon vertebrate cell types. The CL provides general classesthat can be used for other metazoans (muscle cell, neuron),and the ontology can be extended in species-specificontologies.The CL is built according to the principles establishedby the OBO Foundry [5] and is the designated candidateontology for metazoan cell types within the Foundry.The domain and content of CL is intended to be orthog-onal to other Foundry ontologies to allow for the con-struction of compositional classes via logical definitions,as exemplified by the Gene Ontology (GO) [3, 68].Work on the CL over the past several years has re-sulted in many improvements in the ontologys structureand content. As described below, cooperation among anumber of working groups has resulted in a modular ap-proach to improving the CL, and continued enhance-ment of logical definitions in the CL have increased itsintegration and interoperability with other ontologies aswell as enhancing its utility for data analysis.Construction and contentEditorial management of the CLThe CL is maintained primarily by a small group of editors(ADD, YB, MH, DOS, CVS, NV, CJM), working in con-junction with interested parties from the ontology commu-nity. The editors use biweekly teleconferences to discusssignificant issues related to CL ontology development.Because the CL has not been directly funded in recentyears, most efforts are contributed as part of other pro-jects and reflect the cooperative efforts of ontology de-velopers and users based in different communities,such as the Gene Ontology Consortium [8, 9], the Im-munology Database and Analysis Portal (ImmPort)[10], the Human Immunology Project Consortium(HIPC) [11], the Phenoscape project [12, 13], the MonarchInitiative [14], and model organism databases such as theZebrafish Model Organism database (ZFIN) [15] andMouse Genome Informatics (MGI) [16]. Consequentlyterm creation occurs at an uneven pace, based on requestsand editor availability. Over the past few years, we havereceived approximately 35 term requests per month.Most requests are accommodated in 13 months. TheCL is released on an ad hoc basis, with new releases 56times per year.We welcome involvement of the community on par-ticular domain specific developments, as has been donewith kidney cell types (see below) and with immune celltypes through our continuing collaboration with HIPC.Collaboration with the larger biological and biomedicalcommunity occurs both through our issue tracker andthrough direct contacts with any of the editors.Cell types in CLAs of June 2016, The CL contains approximately 2,200classes, compared with 1534 at the time of our last report[3].The relative distribution of number of cell types amongcategories remains relatively constant, with one of the mostwell-represented being the hematopoietic cell branch, asdescribed in [24], currently totaling 575 classes. Althoughthe size of this branch has remained relatively constant, thecontent is continually refined and improved. For example,many of the original hematopoietic cell definitions are be-ing reviewed and generalized to be applicable beyondmouse and human.One area of expansion has been kidney cell subtypes,resulting from collaboration with the Kidney and UrinaryPathway Ontology (KUPO) project [17] as well as the GeneOntology [18]. This has resulted in the addition of 125 newclasses to represent kidney cell subtypes.Over 400 cell types were added by generalizinghuman-specific classes from the Foundation Model ofAnatomy (FMA) [19, 20]many of these were compos-itional classes that we enhanced by adding both textualdefinitions and logical definitions connecting to Uberon.An example is epithelial cell of thyroid gland(CL:0002257, FMA:0002257), logically defined as endo-epithelial cell and (part of some thymus) [20].New and revised skeletal cell typesWork on the Vertebrate Skeletal Anatomy Ontology(VSAO), a unified ontology for the representation ofskeletal cells, tissues, biological processes, organs, andsubdivisions of the skeletal system [21], resulted in mod-ifications to 13 existing cell types in the CL to ensurethat the classes applied across vertebrates, and theaddition of 18 new cell types. New relationships betweencell types and skeletal tissues were also added, inDiehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 2 of 10addition to developmental relationships between skeletalcell types. These improvements enable broader querieson skeletal diversity across different biological scales.Improvements in the representation of skeletal tissues,organs, and subdivisions of the skeletal system havesince been incorporated from VSAO into the Uberonmulti-species anatomy ontology [22], and the logical def-initions of associated cells to refer to the Uberon classes.Extending the CL to encompass vertebrate diversityAn ongoing challenge in developing the CL is to in-crease the number and granularity of cell types repre-sented for well-studied species such as mouse andhuman, while providing high level classes needed for therepresentation of cell types in non-mammalian verte-brates. To ensure that CL classes are applicable to non-mammalian vertebrates two courses of action have beennecessary: 1) add non-mammalian classes to the CL; 2)ensure that general cell type definitions do not uninten-tionally exclude certain organisms. Examples of non-mammalian cell types that have been recently added tothe CL include the pigmented cells iridoblast(CL:0005001) and xanthoblast (CL:0005002) [23], andthe Kolmer-Agduhr neuron (CL:0005007) [24]. Ensur-ing that classes are applicable across species is a multifa-ceted problem and includes optimizing of cell typedefinitions, as well as (ideally) crafting class hierarchiesthat incorporate non-mammalian cell types frominception. Cell type definitions can unintentionally ex-clude non-mammalian vertebrates by including mamma-lian specific anatomical structures or by includingspecies-specific proteins in the logical definition. At thesame time, highly specified cell types for particular taxaare needed to enable querying of complex data using theCL. By adding less specific intermediate classes with in-clusive definitions, such as multi-ciliated epithelial cell(CL:0005012), the CL can be used by a wide variety ofmodel organism databases and evolutionary biologistsfor data annotation, while serving the needs of sophisti-cated bioinformatics analyses focused on cell types ofmedical interest.Improved delineation of content and coordination withother ontologiesThe primary focus of CL is to describe in vivo cell types[3], and while the priority of CL curators has been on invivo cell types over the past few years, the ontology doesin fact include a branch for in vitro cells. In order toclarify the representation of the domain of all cell types,representatives of the CL, Cell Line Ontology (CLO)[25], Reagent Ontology (ReO) [26], the Gene Ontology[9], and Ontology for Biomedical Investigations (OBI)[27], have agreed that the root class cell (CL:0000000)in CL should be regarded as the root of all cell type clas-ses in OBO Foundry ontologies (Fig. 1), and is equiva-lent to the GO class cell (GO:0005623). As a result,cellmortal cell line cellimmortal cell line cellcell line cell primary cultured cellcultured cellexperimentally modified cell in vitroexperimentally modified cell cell in vitro native cell(inferred)Fig. 1 High level cell types in CL and related ontologies. The hierarchy of high-level cell types is shown. CL nodes: green, ReO: blue, CLO: orangeDiehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 3 of 10changes were made to the upper level classes, to allowfor a modular approach that represents in vivo and exvivo cells types more accurately. Two of the children ofthe root class cell are cell in vitro (CL:0001034), andnative cell (CL:0000003) (which was formerly known ascell in vivo). The definition for native cell reads asfollows,A cell that is found in a natural setting, whichincludes multicellular organism cells 'in vivo' (i.e. partof an organism), and unicellular organisms 'inenvironment' (i.e. part of a natural environment).This definition reflects the fact that while cells ofmulticellular organisms are naturally considered in vivoin their native state, single celled organisms often inhabitenvironments that are not part of another organism, andthus are not in vivo in that sense. The naturally occur-ring in vivo cell types of multicellular organisms aretherefore properly considered subtypes of native cell.Another agreed upon change in CL is that the classescell line cell, immortal cell line cell, and mortal cell linecell were deprecated (i.e., made obsolete) in CL and re-placed with equivalent classes from CLO (see discussionbelow and Sarntivijai et al. [25] for additional details). AsCLO specifically represents cell line cells, it seemed ap-propriate for CLO to contain its own root class andhigh-level cell type classes, and for the CLO developersto assume editorial control for these classes. Whereneeded, these three CLO classes were imported into CLusing the MIREOT method [28, 29] to support existingannotations to these classes, and users of these classes,primarily MGI [16], were informed well in advance ofthese changes.Similarly, ReO [26] contains the class experimentallymodified cell (Fig. 1) and a variety of related classessuch as genetically modified cell and experimentallymodified multicellular organism cell in vivo. These celltype classes most commonly denote reagents of sometype and fall outside of the domain of the CL proper,and clearly are within the domain of ReO.Plant cell types and insect cell types are now han-dled independently of the CL as separate modules.The Plant Ontology (PO) has recently undergone newdevelopments and the PO team has taken responsibil-ity for curation of all plant cell type classes [30]. Con-sequently, all plant cell type classes in CL have beenmade obsolete. These plant cell types classes in the CLwere already duplicates of existing PO classes, andwere thus redundant and confusing to users. PO celltype classes may be imported into an extended versionof CL as an OWL import in the future, retaining theirPO IDs. [31]. A similar process is already used to cre-ate a pan-metazoan version of CL as part of theUberon release process [32]; this will be extended toinclude Viridiplantae.While the CL continues to represent a number of highlevel insect cell types, the Drosophila Anatomy Ontology(FBbt) contains cell types for many insect cell types notrepresented in CL, particularly insect neurons [3335].Similarly, the Zebrafish Anatomy Ontology (ZFA) alsocontains neuron types not represented in CL [36]. Goingforward, the general approach is that non-mammalianspecies-specific cell types will be represented as is_achildren of the appropriate CL parent in the species-specific anatomy ontology when such an ontology exists.The CL will continue to maintain general cell types forrepresentation of non-mammalian cells where no separ-ate resource or ontology exists and will remain the prin-cipal ontology for the representation of mammalian celltypes.As described above, the root class cell (CL:0000000)in CL is declared to be logically equivalent to the GOclass cell (GO:0005623), within the Cell Ontology.While this arrangement mostly works for practical useof the CL, a long class proposal has been to deprecatecell (CL:0000000) and simply make the GO class cell(GO:0005623) to be the root of the Cell Ontology. How-ever, there are still some minor differences in the waythe two classes are defined, and questions about whetherthe Gene Ontology with its orientation to describingnormal or physiological biology should provide the CLroot node cell, whose subtypes include tumor cell types,cell line cell types, and other experimentally modifiedcell types. This issue awaits additional discussion withthe Gene Ontology Consortium and other interestedparties.Natural Language and Logical Definitions in CLThe proportion of classes with natural language defi-nitions has remained relatively constant, with a cover-age of 82 % in both 2011 and the present. We still aimto boost this proportion to have 100 % coverage. Thelast five years have seen general improvements in lo-gical axiomatizationin 2011 we reported the numberof classes with defining equivalence axioms (logicaldefinitions) to be 340, this number has increased to1534, added through both manual and automatedmethods [3, 20].The set of ontologies imported into the CL to providelogical definitions remains constant, and consists ofUberon [22, 37], Protein Ontology (PRO) [38], GO [9],the Chemical Entities of Biological Interest (ChEBI)ontology [39], and the Phenotypic And Trait Ontology(PATO) [40]. Some classes make use of a variety of clas-ses in the same axiom, such as T-helper 1 cell, which in-cludes a mix of relations to both PRO classes and GOclasses (Fig. 2).Diehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 4 of 10Improvements to nervous system cell typesIn order to improve the representation of neurons and re-lated cell types, we adopted the relations and methods ori-ginally developed from the Drosophila Anatomy Ontology[34, 35]. These include synapsed_to and has_synaptic_ter-minal_in, used to capture connectivity of neurons to eachother and larger anatomical structures. We aim to coalescewith other neuron-specific vocabularies and ontologies, inparticularly those that were part of the Neuroscience Infor-mation Framework (NIF) Standard suite of ontologies [41].The analogous task has already been performed for neuronparts [42], and the gross neuroanatomical structure subsetof NIFSTD has been incorporated into Uberon. As an ini-tial task, we have aligned the contents of NIF-Cell with theCL by matching up identical or similar classes in the twohierarchies to identify gaps in both ontologies and differ-ences in the ontologies structures. We will then definestandard patterns for neuronal cell types, and import miss-ing neuron cell types from NIF. In order to synchronizewith the corresponding Neurolex wiki system, we have de-veloped an approach for translating the Neurolex semanticwiki into OWL [43].Recent improvements in CL development methodologyThe CL was originally developed using the OBO-Formatand the OBO-Edit ontology editor [1, 44], without anyautomated quality control, release pipeline or automatedprocedures for building the ontology. We previously re-ported on improvements to this methodology, specific-ally leveraging the OWL2 ontology language [45] andassociated tooling such as OWL reasoners, and the Pro-tégé 5.x editor [20, 46].We have made further changes and improvements tothe ontology engineering framework we use. Previously,the editors version (source code) for the ontology wasin OBO-Format, necessitating a conversion to OWL stepprior to reasoning and debugging in Protégé. We havesince switched the editors version to be in OWL, simpli-fying the procedure for working with the OWL stack oftools (note that we still produce editions in OBO-Format along with every release, as many bioinformaticstools still rely on this format). This switch also gives usgreater flexibility for expressing concepts using thericher constructs available in the OWL language.We have also implemented a TermGenie [47] instance,available at cl.termgenie.org. This provides a wider com-munity of users a web frontend for instant provisioningof new classes, either conforming to pre-defined tem-plates (i.e. design patterns), or templateless free-form ad-ditions. Currently the only design pattern implementedis a simple part-whole template for the addition of clas-ses like epithelial cell of forearm. One of the main usersFig. 2 Logical definition for T-helper 1 cell (CL:0000545). The logical definition for the cell type T-helper 1 cell as presented by the Protégéontology editor. The logical definition uses imported classes the Protein Ontology (T-box transcription factor TBX21, PR:000001835; C-X-Cchemokine receptor type 3, PR:000001207; and C-C chemokine receptor type 6, PR:000001202) and the Gene Ontology (interferon-gammaproduction, GO:0032609). Note some anonymous ancestor classes are not shown due to space considerationsDiehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 5 of 10of the TermGenie instance has been the curators of theENCODE project (see below).We make use of the Jenkins Continuous Integration sys-tem, as developed and implemented by the Gene OntologyConsortium, for quality control and validation [8, 48]. Thissystem alerts the editorial team if changes are made thatinadvertently introduce logical, terminological, or struc-tural errors into the ontology (for example, a cell that is lo-cated in two disconnected locations, or two cell classesthat share the same name). We are in the process ofswitching to Travis-CI as this provides more direct integra-tion with the GitHub system, where we manage the ontol-ogy. This system is also used to generate releases, creatinga package of ontology files in OWL2 and OBO formatsthat are pre-reasoned and in some cases simplified for leg-acy use for systems that do not support logical definitions(See Table 1 for listing of available CL files).In the time since we last published on CL, we have mi-grated the source repository we use to manage theontology on two occasions. We originally migrated fromSourceForge to GoogleCode; some time later, Google an-nounced the retirement of GoogleCode, so we thenfollowed many other ontologies and migrated to GitHub,where the source now resides [49]. Note however thatmost users of the CL do not interact with GitHub dir-ectly, and retrieve the ontology from the URLs providedin Table 1. Class requests and other inquiries for theontology developers should be made through the CLissue tracker [50]. We have deprecated the older issuetrackers on SourceForge and GoogleCode, and we mi-grated the tickets on these systems to GitHub.While this migration process caused some disruption,this is compensated by efficiencies afforded by theGitHub systemfor example, the ability to link edits onthe ontology to tickets. The GitHub release mechanismalso works well for ontology releases. One feature wehope to deploy this year is the ability to move to aGitHub-flow style of development, allowing external edi-tors the ability to make pull requests on the ontology,with complete validation being performed by Travis.Utility and discussionUse of CL classes in development of other ontologiesCells are central to understanding biology from the mo-lecular to the organismal level, and the CL is increasinglyuseful as a tool for representing and organizing cell typesand data related to cell types in a variety of projects. As thedesignated ontology for the representation of cells in theOBO Foundry, the CL is used in a number of ontologiesfor the development of compositional classes via logicaldefinitions. Gene Ontology developers have long employedthe principle of cross-product class development, inwhich two classes from different ontologies are combinedto make a more expressive pre-composed (or compos-itional) ontology class [68]. The class neuron differenti-ation (GO:0030182), for instance has the logical axiom'cell differentiation' and (results in acquisition of features ofsome neuron), where neuron is a CL class. As GO devel-opers continue to implement logical definitions for cross-product classes, they have increasingly needed new celltypes in CL for use in these logical definitions. In order tofacilitate this process, GO ontology developers have beentrained in CL ontology editing as well and are now makingdirect contributions to the CL. An extended version of theGO that includes a subset of the CL together with linkingaxioms is available [51].Development of the Cell Line Ontology (CLO) has ref-erenced CL cell types and the hierarchy of the CL [25].In CLO, all cell line cells are under the CLO class cellline cell, which is a child of CL cultured cell. Initially,CLO listed over 30,000 cell line cells immediately underthe parent class cell line cell. To better identify the rela-tions among different cell line cells, CLO generatedmany intermediate cell line cell classes (e.g., immortalepidermal cell line cell and immortal keratinocyte cellline cell) based on a basic relation design that a CLOcell line cell is derived from a CL cell, for example,CLO immortal epidermal cell line cell derives_fromsome epidermal cell, and CLO immortal keratinocytecell line cell derives_from some keratinocyte. In CL, theclass epidermal cell is a parent of keratinocyte. Basedon this CL hierarchical definition, CLO automatically in-cludes a logical definition that immortal epidermal cellline cell is a parent of immortal keratinocyte cell linecell. In total over 130 CL classes were imported to CLOwith the hierarchy of the CL informing CLO structure.These newly generated CL-matched CLO classes werethen used as parent classes for the over 30,000 cell linecells in CLO to layout an improved hierarchy of the cellline cells [25].Interactions between different ontologies in the scopeof biological cells can become complicated as we imple-ment a thorough and precise representation of know-ledge in this domain. As described above, CLOs cellline cell is a subclass of CLs experimentally modifiedTable 1 CL Ontology FilesFile pre-reasonedwith externalontology classesPURLcl-edit.owl no yes N/Acl.owl yes yes http://purl.obolibrary.org/obo/cl.owlcl.obo yes yes http://purl.obolibrary.org/obo/cl.obocl-basic.owl yes no http://purl.obolibrary.org/obo/cl-basic.owlcl-basic.obo yes no http://purl.obolibrary.org/obo/cl-basic.oboDiehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 6 of 10cell in vitro where experimentally modified cell in vitrois inferred as a subclass of ReOs experimentally modi-fied cell. The correct relationships among these relatedclasses are only seen when all have been loaded intoProtégé and a reasoner has been run. This degree ofinterrelatedness and complexity is becoming more com-mon in bio-ontology practice, and demonstrates theneeds for effective communication within the commu-nity. Being the center of interactions in situation likethis, the CL has acted as the facilitating moderator ofthis kind of communication.CL development allows for modular development ofspecies-specific extensions. These extensions enable thecreation of very granular cell types defined in ways thatare unique to a particular species or limited to a subsetof species. However, many cell types can be genericallydefined across species, and the CL provides the appro-priate OBO ontology for their representation. In orderto allow for comparison and integration of cell type spe-cific data between species, species-specific cell typesshould always be subtypes of generic CL cell types.While development of modular extensions to CL is en-couraged, the well-developed hierarchy of classes in theCL provides a valuable resource for data annotatorsworking in species who do not have time or resources todevelop CL extensions.As examples of this methodology, developers ofspecies-specific anatomy ontologies such as the Zebra-fish Anatomical Ontology (ZFA) [36] and the XenopusAnatomy Ontology (XAO) [52] have extended the CL byincorporating species-specific cell classes as is_a chil-dren of CL classes in their ontologies. This strategy al-lows ontologists to make species-specific classes that areis_a children of the appropriate CL class for use in dataannotation at model organism databases. The integrationof the CL with the species-specific ontologies also allowsthe CL classes to be used in phenotype and expressionannotations at ZFIN [15] and expression annotations atXenbase [53].As ontologies such as the Infectious Disease Ontology(IDO) [54] or the Neurological Disease Ontology [55]are developed, CL classes are being used to represent in-formation such as viral tropism or neurons affected inParkinsons disease. As with the GO, there is communi-cation between developers of related biomedical ontol-ogies that contribute to the development of both. TheCL is also a component of the Experimental FactorOntology (EFO), used to provide descriptions of experi-mental variables in databases at the European Bioinfor-matics Institute [56].The CL is also being used far more extensively in theGO, in particular the GO has added a way to provideadditional cellular context to gene associations using amechanism called annotation extensions [57]. Thesecross-ontology linkages are used by a number of modelorganism databases in GO annotation and visible inAmiGOfor example, the page for neuron includesGO annotations for neuronal parts [58].Use of the CL as Metadata in ENCODE and FANTOM5ProjectsTwo major projects studying gene expression have uti-lized the CL as part of their data analysis pipelines. TheEncyclopedia of DNA Elements (ENCODE) Consortium,which is funded and organized by National Human Gen-ome Research Institute (NHGRI), aims to discover anddefine the functional elements encoded in the humangenome [59]. ENCODE investigators are utilizing a pri-oritized set of various cell types to complete annotationsabout genes and their RNA transcripts and transcrip-tional regulatory regions and have developed data stan-dards that utilizes the CL, among other ontologies, todescribe the metadata for cell types used and experimen-tal assays [60, 61].The value of the CL for data integration and analyseswas adeptly demonstrated in a recent series of notablepapers from the FANTOM5 Consortium, which reliedin part on the CL for large-scale data analyses of tran-scriptional start sites [62], enhancers [63], and waves oftranscription in differentiating cell types [64]. The FAN-TOM5 Consortium utilized the CL as a component ofthe FANTOM Sample Ontology, in combination withUberon, the Disease Ontology and the EFO to identifycellular, tissue, disease sources and experimental modifi-cations for the samples used in transcriptional analyses[65]. By relying on the ontological hierarchy provided bythe CL, the FANTOM5 Consortium was able to classifytranscription patterns associated with individual celltypes, groupings of related cell types, and cell lineagesduring differentiation [62, 64].Use of the CL in other non-ontology projectsThe CL is being used as metadata in a variety of non-ontology projects, such as The Cell: An Image Library[66], CELLPEDIA [67], Phenoscape [13], LINCS [68],the Human Immunology Project Consortium (HIPC)[11], and ImmPort [10]. The HIPC and ImmPort pro-jects are National Institute of Allergy and InfectiousDiseases (NIAID) sponsored programs to collect andorganize data from immunology experiments performedby NIAID supported investigators in order to facilitatesecondary usage [10]. In support of these projects, theCL is being used both as a controlled vocabulary of celltypes for use as metadata, and as part of an analyticalpipeline for analyzing high-dimensional flow cytometryand mass cytometry data (e.g. CyTOF) [69] submitted tothe ImmPort data repository. Developers of the CL havealready incorporated novel B cell types discovered viaDiehl et al. Journal of Biomedical Semantics  (2016) 7:44 Page 7 of 10high-dimensional flow cytometry [70], such as IgG-posi-tive double negative memory B cell (CL:0002103) andIgD-negative CD38-positive IgG memory B cell(CL:0002107). The CyTOF method is yielding informa-tion about even more granular cell types [71]. In orderto facilitate the analysis of data generated in high-dimensional flow cytometry or CyTOF, the flowCL soft-ware package matches cell populations identified viaautomated gating algorithms against existing cell typesin the CL based on their combinations of markers, orimmunophenotypes [72, 73].ConclusionThrough cooperative efforts between the Cell Ontologyeditors and various stakeholders, ongoing developmentof the CL has ensured that it continues to be a valuableresource for users and developers of related ontologies.Use of the CL by a broad range of third party efforts, in-cluding the high visibility ENCODE and FANTOM5projects, as a source of metadata and for data integrationand analysis shows the value of the CL to the wider sci-entific community. As big data collection and analysiscontinues to grow in importance as a source of bio-logical discovery, we expect the CL will be of key utilityin organizing and understanding these data. We invitecommunity feedback and participation to continue theimprovements to the CL.Availability and requirementsLike all OBO library ontologies, the CL is available froma standard purl [http://purl.obolibrary.org/obo/cl.owl].The main URL for the project[http://cellontology.org/] and links to various browsersare available from the main OBO Library page for CL[http://obofoundry.org/ontology/cl.html].AcknowledgementsWe would kindly thank Barry Smith, Lindsay Cowell, Anna Maria Masci,Richard Scheuermann, Jose Mejino, David Hill, Terry Hayamizu, MorganHightshoe, Wade Valleau, Jane Lomax, Paola Roncaglia, Tanya Berardini,Heiko Dietze, Maryann Martone, Stephan Larson, Gordon Shepherd, JylBoline, Mihail Bota, Giorgio Ascoli, Paul Katz, Robert Burgess, Patrick Ray,Jonathan Bona, Paula Mabee, Laurel Cooper, Ramona Walls, Pankaj Jaiswal,Darren Natale, Cathy Wu, Cecilia Arighi, Alistair Forrest, Hideya Kawaji, HelenParkison, Simon Jupp, Robert Stevens, Ryan Brinkmann, Melanie Courtot,Raphael Gottardo, Cliburn Chan, Jie Zheng, Shai Shen-Orr, and YannickPouliot, for discussions about and contributions to the Cell Ontology project.ADD, TFM, CJM, and JAB were supported by NHGRI grants HG002273-09Zand HG002273 for portions of this work. CJMs work was supported by theDirector, Office of Science, Office of Basic Energy Sciences, of the U.S.Department of Energy under Contract No. DE-AC02-05CH11231. ADD andAR are supported by NIGMS grant 2R01GM080646-06 and NIAID contractHHSN272201200028C for portions of this work. YMB and CVS are supportedby NIH HG002659 for portions of this work. MAH, MHB, and NAV are supportedfor portion of this work by 1R24OD011883 from the NIH Office of the Director.WD is supported by NSF grants DBI-0641025, DBI-1062404, and DBI-1062542, andby the National Evolutionary Synthesis Center under NSF EF-0423641 and NSFEF-0905606 for portions of this work. YH was supported by NIH 1R01AI081062.Any opinions, findings, and conclusions or recommendations expressed in thismaterial are those of the author(s) and do not necessarily reflect the views of theNational Science Foundation or the National Institutes of Health. We gratefullyacknowledge the support of the International Neuroinformatics CoordinatingFacility for portions of this work.Authors' contributionsAll authors have contributed to CL ontology development throughdiscussions of key issues and/or contributions of ontology classes. CJMdeveloped the CL production system. The CL project is managed by ADD,AR, MAH, JAB, and CJM. ADD wrote the manuscript with contributions fromYMB, WMD, YH, CVS, DOS, MAH, NV, SS, TFM, and CJM. All authors read andapproved the manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Department of Neurology, University at Buffalo School of Medicine andBiomedical Sciences, Buffalo, NY 14203, USA. 2European Molecular BiologyLaboratory, European Bioinformatics Institute, Hinxton, Cambridge CB10 1SD,UK. 3ZFIN, the Zebrafish Model Organism Database, 5291 University ofOregon, Eugene, OR 97403, USA. 4Ontology Development Group, Library,Oregon Health and Science University, Portland, Oregon 97239, USA.5Department of Biology, University of South Dakota, Vermillion, SD 57069,USA. 6National Evolutionary Synthesis Center, Durham, NC 27705, USA.7Southwestern Medical Center, University of Texas, Dallas, TX 75235, USA.8Unit for Laboratory Animal Medicine, University of Michigan Medical School,Ann Arbor, MI 48109, USA. 9Oral Diagnostics Sciences, University at BuffaloSchool of Dental Medicine, Buffalo, NY 14210, USA. 10The Jackson Laboratory,Bar Harbor, ME 04609, USA. 11Genomics Division, Lawrence Berkeley NationalLaboratory, Berkeley, CA 94720, USA.Received: 15 April 2016 Accepted: 23 June 2016Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 DOI 10.1186/s13326-016-0087-8RESEARCH Open AccessThe ontology of medically related socialentities: recent developmentsAmanda Hicks1*, Josh Hanna1, Daniel Welch1, Mathias Brochhausen2 and William R. Hogan1AbstractBackground: The Ontology of Medically Related Social Entities (OMRSE) was initially developed in 2011 to provide aframework for modeling demographic data in Resource Description Framework/Web Ontology Language. It is builtupon the Basic Formal Ontology and conforms to Open Biomedical Ontologies Foundrys best practices.Description: We report recent development of OMRSE which includes representations of organizations, roles,facilities, demographic data, enrollment in insurance plans, and data about socio-economic indicators.Conclusions: OMRSEs coverage has been expanding in recent years to include a wide variety of classes and hasbeen useful in several biomedical applications.Keywords: Ontology, OMRSE, Social entities, Social roles, OrganizationsBackgroundThe Ontology of Medically Related Social Entities(OMRSE) [1] is a realist representation of medicallyrelated social entities. We initially developed OMRSE tocover demographics data and common roles of peoplein healthcare encounters for reuse in the context of theOBO Foundry [1]. We created a framework for defininggender roles, legal roles, healthcare provider roles, health-care organization roles, and patient roles inWebOntologyLanguage (OWL), one of the accepted languages for theOBO Foundry and a standard for the Semantic Web. Wehave since developed this ontology by adding more spe-cific classes and creating frameworks for additional topicsto facilitate uses arising out of projects related to epi-demic modeling, the organizational structure of traumasystems, and common health care data models. OMRSEis a middle level ontology in the sense described in [2].It is designed to bridge the gap between the upper ontol-ogy, Basic Formal Ontology (BFO), and more specificdomain ontologies as well as provide classes for reuse inapplication ontologies. While we acknowledge that the*Correspondence: aehicks@ufl.edu1Department of Health Outcomes and Policy, University of Florida, Gainesville,FL; USAFull list of author information is available at the end of the articledemarcation between middle and domain level ontolo-gies is not crisp [2], OMRSE contains mid-level classessuch as employee role, smoker role, and party to a mar-riage contract that span multiple sub-domains and can bereused in both more specific domain ontologies as well asapplication ontologies.ApplicationsOMRSE classes are reused in several application ontolo-gies. It is available to the wider biomedical communitythrough OntoBee [3] and NCBO Bioportal [4].The Apollo [5] and MIDAS projects reuse OMRSEclasses in Apollo-SV to produce synthetic ecosystems foragent based epidemic modeling. The CAFÉ Project reusesOMRSE classes in the Ontology of Organizational Struc-tures of Trauma centers and Trauma systems (OOSTT)(http://purl.obolibrary.org/obo/oostt.owl). OOSTT is anOWL representation of organizational structures (organi-zations, committees, roles, etc.) specific to trauma centersor trauma systems. It is used to compare the organiza-tional structure of trauma centers and trauma systems.OMRSE classes are also being used to create a seman-tic representation of the PCORnet Common Data Model(CDM).© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 2 of 4Construction and contentIn keeping with the OBO Foundry principles [1], OMRSEreuses classes from other ontologies including the BasicFormal Ontology (BFO) [6], NCBI Taxonomy [7], theInformation Artifact Ontology [8], and the DocumentActs Ontology (D-acts) [9]. We searched OntoBee orBioportal for existing classes in BFO-based ontologiesbefore adding new ones to OMRSE. We participate in dis-cussion about BFO and other OBO ontologies to ensurecompatibility with other OBO/BFO ontologies. The addi-tions that we report in this paper have been developedwith investigators on these projects to ensure accurateand useful semantics of new classes. The recent devel-opments in OMRSE can be classified into eight contentareas described in the subsections below. In addition tointroducing new content, we have also updated OMRSEto be compatible with the Basic Formal Ontology 2.0(BFO2).Organizations and the roles they createSome roles, such as employee and student roles, exist onlyin relation to organizations. To represent the relation-ships between these roles and organizations we leveragedthe work in D-Acts, an OWL ontology built accord-ing to the OBO Foundry principles and using BFO. D-acts is based on works by Reinach [10] and Smith [11]explaining how social acts create new entities. The ontol-ogy represents social acts (such as signing a contractor enrolling as a student), the socio-legal entities thosesocial acts create (such as rights and obligations or a stu-dent role), and the object properties relating these kindsof entities [10].Demographic dataThe major developments for representing demographicdata consist in modeling social identities (race and ethnic-ity) and marital status. Social identities differ from otherdemographic data since the referent of identities is onto-logically unclear. For considerations of space and clarity,we reserve a discussion of social identities for a futuremanuscript. At present details can be found in [12] andhttps://www.youtube.com/watch?v=-pcQUNWtnVk.Marital statusMany coding schemes have several values for maritalstatus (HL7 has nine), but we model marital status asbinary. In clinical settings marital status is recorded todocument whether a patient has a spouse who can makedecisions on his or her behalf; either they have a spouseto make decisions on their behalf or they do not. We cur-rently have no use case for knowing a person has neverbeen married nor for capturing what process resultedin the end of a marriage (i.e., death of the spouse or adivorce).Typology of trauma patientsWe worked with a team of trauma experts who identifiedand reviewed definitions for trauma patients for the TheCAFÉ project (1R01 GM111324). Current classes includeinjured patient role, burn patient role, and trauma patientrole. A typology of burn patients was also defined. Weplan to add these classes once there are suitable classes fortypes of burns (e.g., thermal vs. chemical burns) in anotherOBO ontology.Health care facilitiesWe developed a typology of twelve types of health carefacilities that are referred to in the PCORnet CDMsdischarge status field in the encounters Table [13] (e.g.,hospital facility, urgent care facility, nursing home facil-ity). We distinguish the types of facilities based on theirfunctionsHealth care provider rolesWehave distinguished health care provider roles along thelines of what kind of entity can bear that role. Accord-ingly, we have health care provider organization role andsubclasses that inhere in organizations. These subclassesare, hospital role, integrated delivery network, and physi-cian practice. We also have health care provider role as asubclass of human health care role. These include nurserole, physiatrist role, physician role, and US physicianrole.Smoking statusesOMRSE captures smoking status using smoker roles.Smoker role is defined as a role that inheres in an humanbeing and is realized by habitually smoking tobacco prod-ucts. The subclasses heavy smoker role and light smokerrole are defined in terms of number of cigarettes habitu-ally smoked per day. Further distinctions based on smokeexposure and source can be added as applications requirethem.Enrollment in an insurance planModeling enrollment in an insurance plan requires mod-eling three different types of entities: (1) insurance poli-cies, (2) the roles involved in an insurance policy, and (3)enrollment dates.Insurance policiesInsurance policies come into existence through docu-ment acts. In technical terms, they are the specifiedoutput of document acts. The document acts involvetwo parties (1) a group of persons (the insured par-ties) and (2) the organization that issues the plan. Theorganization and the primary insured persons on thepolicy are parties to a legal agreement (an insurancepolicy).Hicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 3 of 4RolesThere are two types of roles introduced to model insur-ance policies: those that inhere in the insurance companyand those that inhere in the insured. More details abouthow these roles are modeled are available at http://ncor.buffalo.edu/2016/Hicks.pptx.U.S. Census households and housing unitsHouseholds and housing units are pivotal for representingU.S. Census data and epidemic modeling. OMRSErepresents the distinction between households, which arecollections of people, and housing units and asserts thathousing units are individuated by their residence func-tions.Socio-economic dataAlthough we do not directly represent socio-economicstatus, with the exception of employee role and insuranceenrollment information, we do represent data that areabout socio-economic status. These terms are included tofacilitate the annotation of data sets that contain infor-mation about employment status, care plans, income, andother socio-economic indicators.BFO 2.0 ConversionThe only modification to OMRSE that was required tocomplete this conversion was to import version 2015-10-07 of the Relation Ontology [14].Utility and discussionValidationWe have generated competency questions to validatethe following representations: (1) employee roles, stu-dent roles, and household data, and (2) health careorganizations and the typology of patients. Compe-tency questions and queries for (1) are freely availableat www.github.com/ufbmi/socid and http://tinyurl.com/syneco-queries respectively. Competency questions havebeen developed for OOST, but OOST is still under devel-opment and has not yet been validated. Table 1 has samplecompetency questions.LimitationsThe typologies that we mention here are not exhaustive.For instance, the typologies of health care provider rolesand patient roles are relatively sparse compared to themyriad of roles that a provider or patient might bear. Thisis the result of our use case driven approach. We maintainan active mailing list and issue tracker to manage requeststo the ontology.Stating that an individual person is single, i.e. lacks aparty to a marriage contract role, in OWL is challeng-ing but also necessary to support modeling demographicdata in a manner that is compliant with OBO FoundryTable 1 Sample competency questionsTopic Ontology QuestionDemographics PCORowl Who has identified as Asianaccording to both OMB andPCORnet CDM guidelines?Marital status/households MIDAS Who are the marriedhouseholders according to theU.S. Census?Roles and organization CAFÉ How many anesthesiologistsdoes institution x have on staff?principles. Two approaches are (1) using negative objectproperty axioms and (2) defining a class single person asa person who is not the bearer of a party to a marriagecontract role. The former is often not supported by com-mon reasoners and the latter would lead to a proliferationof absence classes for every case where some individ-ual lacks a relationship to some class of entities. Theanalogous problem is still outstanding for non-smoker,for example. A detailed description of this problem goesbeyond the scope of reporting updates, but we will addressthis in future work and are currently investigating the prosand cons of these approaches, as well as attempting tocome up with additional approaches.ConclusionOMRSE is an ontology designated for representing med-ically related social entities in a manner that is consistentwith BFO and OBO Foundry ontologies. Its coverage hasbeen expanding in recent years to include a wide vari-ety of classes.. and has been useful in several biomedicalapplications.Availability and requirementsOMRSE is free and open to all users (https://github.com/ufbmi/OMRSE). There is a Google Group for dis-cussing the project at http://groups.google.com/group/omrse-discuss.AcknowledgmentsPart of the research reported in this publication was supported by the NationalInstitute of General Medical Sciences of the National Institutes of Health underaward number 1R01GM111324. This work was also supported in part by theNIH/NCATS Clinical and Translational Science Awards to the University ofFlorida UL1 TR000064 and the University of Arkansas for Medical Sciences UL1TR000039 and the James and Esther King Foundation Biomedical ResearchProgram 4KB16. The authors would like to thank the CAFÉ Domain Expertpanel, namely Jane Ball, Stephen M. Bowman, Robert T. Maxson, RosemaryNabaweesi, Rohit Pradhan, Nels D. Sanddal, Mihael E. Tudoreanu, and Robert J.Winchell.Authors contributionsAH manages OMRSE. She contributed to each of the developments discussedin this paper. She drafted the main sections of this manuscript. DWcontributed to the development of OMRSE, in particular to the classes relatedto U.S. Census data, households, and housing units. JH contributed to theHicks et al. Journal of Biomedical Semantics  (2016) 7:47 Page 4 of 4representations of race and ethnicity, marital status, smoking status, insurancepolicies, U.S. Census households and housing units, and socio-economic data.MB contributed to the classes related to the CAFÉ project. WH was the originalcreator of OMRSE in support of demographics applications, and hassubsequently participated in its development around synthetic ecosystemsand common data model support including insurance enrollment. All authorsread and reviewed the manuscript.Competing interestsThe authors have no competing interests to declare.Author details1Department of Health Outcomes and Policy, University of Florida, Gainesville,FL; USA. 2Department of Biomedical Informatics, University of Arkansas forMedical Science, Little Rock, AR; USA.Received: 3 December 2015 Accepted: 21 June 2016RESEARCH Open AccessExtracting a stroke phenotype risk factorfrom Veteran Health Administration clinicalreports: an information content analysisDanielle L. Mowery1,2*, Brian E. Chapman1,2, Mike Conway1, Brett R. South1,2, Erin Madden3, Salomeh Keyhani3and Wendy W. Chapman1,2AbstractBackground: In the United States, 795,000 people suffer strokes each year; 1015 % of these strokes can beattributed to stenosis caused by plaque in the carotid artery, a major stroke phenotype risk factor. Studiescomparing treatments for the management of asymptomatic carotid stenosis are challenging for at least tworeasons: 1) administrative billing codes (i.e., Current Procedural Terminology (CPT) codes) that identify carotidimages do not denote which neurovascular arteries are affected and 2) the majority of the image reports arenegative for carotid stenosis. Studies that rely on manual chart abstraction can be labor-intensive, expensive, andtime-consuming. Natural Language Processing (NLP) can expedite the process of manual chart abstraction byautomatically filtering reports with no/insignificant carotid stenosis findings and flagging reports with significantcarotid stenosis findings; thus, potentially reducing effort, costs, and time.Methods: In this pilot study, we conducted an information content analysis of carotid stenosis mentions in termsof their report location (Sections), report formats (structures) and linguistic descriptions (expressions) from VeteranHealth Administration free-text reports. We assessed an NLP algorithm, pyConTexts, ability to discern reports withsignificant carotid stenosis findings from reports with no/insignificant carotid stenosis findings given these threedocument composition factors for two report types: radiology (RAD) and text integration utility (TIU) notes.Results: We observed that most carotid mentions are recorded in prose using categorical expressions, within theFindings and Impression sections for RAD reports and within neither of these designated sections for TIU notes. ForRAD reports, pyConText performed with high sensitivity (88 %), specificity (84 %), and negative predictive value(95 %) and reasonable positive predictive value (70 %). For TIU notes, pyConText performed with high specificity(87 %) and negative predictive value (92 %), reasonable sensitivity (73 %), and moderate positive predictive value(58 %). pyConText performed with the highest sensitivity processing the full report rather than the Findings orImpressions independently.Conclusion: We conclude that pyConText can reduce chart review efforts by filtering reports with no/insignificantcarotid stenosis findings and flagging reports with significant carotid stenosis findings from the Veteran HealthAdministration electronic health record, and hence has utility for expediting a comparative effectiveness study oftreatment strategies for stroke prevention.Keywords: Natural language processing, Stroke, Phenotype, Information extraction* Correspondence: danielle.mowery@utah.edu1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USA2IDEAS Center, Veteran Affair Health Care System, Salt Lake City, UT, USAFull list of author information is available at the end of the article© 2016 Mowery et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 DOI 10.1186/s13326-016-0065-1BackgroundIn biomedicine, we define a disease or mutant phenotypeexperienced by an individual as observations caused byinteractions between the environment and his/her gen-ome that differ from the expected, normal wild type.Over the last several years, the biomedical communityhas begun to leverage informatics and electronic healthrecord (EHR) data to define and identify phenotypes forgenetic analyses using genome-wide (GWAS) andphenotype-wide (PheWAS) association studies [1, 2]. Forinstance, PheKB is a knowledgebase that contains phe-notypes defined using EHR data and subsequently vali-dated within one or more institutions. This catalogue ofphenotypes was primarily generated by the ElectronicMedical Records and Genomics (eMERGE) network, aUnited States (US) National Human Genome ResearchInstitute-funded consortium, but is also supplemented bythe informatics community at large (https://phekb.org/phenotypes) [35]. Similarly, the Strategic Health IT Re-search Program for secondary use of EHRs (SHARPn),funded by the US Office of the National Coordinator forHealth Information Technology, aims to transform het-erogeneous EHR data from various sites into a standard-ized form to support high-throughput phenotyping [6].Phenotyping with electronic health record dataSeveral phenotypes have been the foci of informaticsstudies including cancer, diabetes, heart failure, rheuma-toid arthritis, drug side effects, cataract, pneumonia,asthma, peripheral artery disease, and hypertension [7].EHRs provide a groundbreaking opportunity to defineand identify these complex phenotypes leveraging dataelements from the longitudinal patient record. Specific-ally, patient phenotypes are often inferred from bothstructured EHR data elements (e.g., administrative bill-ing codes, vital signs, medications, laboratory valuesfrom data fields including dropdown lists and check-boxes) and unstructured EHR data elements (e.g., symp-toms, signs, histories, and diagnoses within clinical notesincluding progress notes and discharge summaries).These heterogeneous data elements are then mapped tological representations used to classify a patient into oneor more phenotypes [8]. Outstanding challenges remainfor next-generation phenotyping of EHR data includingthe need for approaches that address data complexity,inaccuracy, coverage, and biases [9].Natural language processingTraditionally, International Classification of Disease(ICD-9) billing codes have been leveraged to identifyphenotype risk factors with variable results. Inaccurateperformance can result from poor granularity withincode descriptions and documentation of risk factors inpatient clinical texts [10, 11]. Natural languageprocessing (NLP) may improve risk factor detection byidentifying missed risk factor mentions (improving sensi-tivity) and filtering spurious risk factor mentions (improv-ing positive predictive value) from these clinical texts.However, extracting risk factors associated with pheno-types from clinical texts can be challenging due to theusage of variable lexical expressions (e.g., occlusion, re-duced arterial diameters), ambiguous abbreviations (PADcan stand for peripheral artery disease or pain and dis-tress), spelling errors (diabetes misspelled as dia-beetes), and telegraphic constructions (e.g., PHx: HTNmeans past history of hypertension) within clinical texts.Furthermore, multiple mentions of the same risk factorcan be recorded within and across reports. This informa-tion might be integrated with structured data elements re-quiring logic to classify a patient with a phenotype. Thesuccess of an algorithm is often defined by performancemetrics of sensitivity (or recall), positive predictive value(or precision), negative predictive value, and specificity bycomparing the predicted phenotype from the system/algo-rithm against the coded phenotype from a domain expert[12].Extracting stroke risk factors using natural languageprocessingNLP has been applied and, at times, integrated withstructured data to successfully identify several stroke riskfactors such as peripheral artery disease [5, 13], diabetes[4, 14], heart failure [15], and hypertension [16] as partof large, coordinated research projects. Specifically,Savova et al. extended the Clinical Text Analysis andKnowledge Extraction System to extract and classifypositive, negative, probable, and unknown mentions ofperipheral artery disease (PAD) [13]. Kullo et al. thenleveraged this system to encode casecontrol status,comorbidities, and cardiovascular risk factors from theEHR for a GWAS study of PAD cases and controls forthe eMERGE project [5]. Wilke et al. applied theFreePharma system to extract medication histories andcombine them with diagnoses and laboratory resultsto identify a diabetes mellitus cohort as part of theMarshfield Clinic Personalized Medicine Research Project(PMRP) [14]. Kho et al. extracted diagnoses, medications,and laboratory results leveraging NLP to encode variablesfrom unstructured fields for various sites to identifytype 2 diabetes cases and controls for a multi-institutionalGWAS study also as part of the eMERGE project [4].Garvin et al. extracted left ventricular ejection fraction asan indicator for heart failure using the Unstructured Infor-mation Management Architecture (UIMA) as part of aTranslational Use Case Project and quality improve-ment project within the Veteran Affairs (VA) Consor-tium for Healthcare Informatics Research (CHIR) [15].Finally, Thompson et al. translated the nine algorithmsMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 2 of 12for phenotypes including hypertension developed fromthe eMERGE project into the Quality Data Model(QDM) to support EHR-based quality measures [16].Although NLP has addressed many stroke-associatedrisk factors for genotype-phenotype and other studies,few studies have leveraged NLP to identify these riskfactors specifically for stroke prevention research. Fur-thermore, to our knowledge, no NLP study has targetedsignificant carotid stenosis - a known risk factor forstroke. Our long-term goal is to develop a comprehen-sive stroke phenotyping framework that extracts predic-tors of stroke subtypes e.g., ischemic or hemorrhagic aswell as their precise endotypes e.g., ischemic stroke endo-types of cardiac embolism, large artery atherosclerosis, orlacunar infarction, other uncommon causes, from theEHR powered by NLP. Our short-term goal is to developan NLP algorithm for a National Institute of Health(NIH)-sponsored comparative effectiveness study ofischemic stroke prevention treatments that automatic-ally filters carotid reports for patients exhibiting no/insignificant carotid stenosis of the internal or com-mon carotid arteries from chart review. In this pilotstudy, we completed a qualitative and quantitativestudy of where and how mentions of carotid stenosisfindings occur in radiology reports and how thisaffects an NLP algorithms performance.MethodsIn this Institute Review Board (IRB or Ethics committee)and Veteran Affairs (VA) approved pilot study, we aimedto conduct an information content analysis of a majorpredictor of stroke, significant stenosis of the internal orcommon carotid arteries, for a sample of free-text re-ports from the Veteran Health Administration. Our goalis to automatically distinguish reports denoting one ormore sides of significant stenosis (defined as greaterthan 50 %, moderate, or severe stenosis) from reportsdenoting no/insignificant stenosis (defined as negated,ruled out, mild, less than 50 % stenosis) from both ofthe internal or common carotid arteries. In this study,we conducted an information content analysis of carotidstenosis findings with respect to three aspects of docu-ment composition - location (Sections), format (struc-tures), and descriptions (expressions). We assessed theperformance of pyConText, an NLP algorithm, at auto-matically extracting and encoding stenosis findings giventhese three document constituents.DatasetWe selected all reports from the VA EHR for patientswith an administratively documented carotid image pro-cedure code (CPT code) restricted to those within ?1 to+9 days of the procedure code date and that contained acarotid term (carot, ica, lica, rica, or cca). In ourprevious study, we leveraged 418 randomly sampled VAradiology reports for developing our NLP algorithm,pyConText, to identify mention-level stenosis findings[17]. We extended this previous study by randomlyselecting a new set of reports to classify document-levelstenosis based on identified mention-level carotid stenosisfindings. This dataset consists of 598 radiology reports(RAD: mainly ultrasound reports) and 598 text integrationutility notes (TIU: mainly progress notes, carotid duplexexams, and carotid triplex exams) (see Fig. 1). Becausemuch of our algorithm development was completedduring our previous study [17, 18] and the prevalence ofstenosis positive reports is low, we chose a larger testingset for each report type. We also chose to maintainthe natural distribution to give us a better sense ofwhether pyConText could correctly retain stenosispositive reports (high sensitivity) and to extrapolatethe potential chart review savings from filteringFig. 1 Sample texts by report type. Each text contains fictional, but realistic informationMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 3 of 12stenosis negative reports (high negative predictivevalue). The dataset was randomly split into two sets:200 development reports (100 RAD and 100 TIUnotes) for algorithm knowledge base development[18] and 996 testing reports (498 RAD and 498 TIUnotes) for information content analysis and algorithmevaluation. For the information content analysis, threeresearch associates (domain experts) each independ-ently and manually annotated the dataset for Sections,structures, and expressions as well as classified thereport at the document-level as stenosis positive (ifthe report contained one or more mention of signifi-cant carotid stenosis) or stenosis negative (if the re-port contained only mentions of no/insignificantcarotid stenosis). For the algorithm evaluation, theRAD reports were extracted from the VA EHR as twoseparate parts, Findings and Impressions. For the TIUreports, we parsed the Findings and Impressionsusing regular expressions written as a python script.We assessed pyConTexts performance when providedthe Findings only, Impressions only, and the fullreport.Information content assessmentWe aimed to characterize mentions of carotid sten-osis findings according to Sections, structures, andexpression types. Each report could have zero, one,or more relevant carotid stenosis findings recordedwith zero, one, or more Sections, structures, andexpression types.SectionsRAD and TIU reports can be structured usingcanonical sections e.g., Indication, Findings, andImpression sections. We evaluated information con-tent in the Findings (including Comments) versusImpressions (including Interpretations and Conclusions)sections [19].StructuresVA notes can be generated using narrative or boilerplatetemplates in which the contents are saved as unstruc-tured or semi-structured texts, respectively. For example,findings may be present in a variety of structures includ-ing: prose, lists, tables, headings, and other (Table 1). Weevaluated information content according to these struc-ture types [20].ExpressionsWe have identified three types of expressions describingcarotid stenosis findings: category, range, or exact. Wecharacterized the information content according to theseexpression types [21] (Table 2).pyConText algorithmpyConText is a regular expression-based and rule-basedsystem that extends the NegEx [22] and ConText [23]algorithms. NLP developers can train pyConText toidentify critical findings and their contexts by definingregular expressions for these targeted findings and theirdesired modifiers within its knowledge base, respectively[24]. These modifiers can be used to filter spuriousfinding mentions that would otherwise generate falsepositives if generating a cohort based on simple keywordsearch. For example, a negation modifier can reducefalse positives by filtering denied findings e.g., nocarotid stenosis. Furthermore, a severity modifier mayreduce false positives by filtering insignificant findingse.g., slight carotid stenosis. In a previous study, pyCon-Text identified pulmonary embolism from computedtomography pulmonary angiograms by filtering spuriousmentions using modifiers of certainty, temporality, andquality with high sensitivity (98 %) and positive predict-ive value (83 %). The pyConText pipeline is composedof three main parts: named entity recognition, assertiondetection, and document-level classification.Named entity recognition and assertion detectionSpecifically, we adapted pyConTexts knowledge base offindings and modifiers to filter no/insignificant carotidstenosis findings using regular expressions. These ex-pressions contain lexical variants including synonyms,acronyms, abbreviations, and quantifications commonlydocumented in clinical text to represent carotid stenosisfindings, semantic modifiers of severity, neurovascularanatomy, and sidedness, and linguistic modifiers of exist-ence, temporality, and exam [25]. In Fig. 2, we providethe schema representing findings and each modifier aswell as the possible normalized values. We representthese mentions and their normalized values using thefollowing syntax: finding/modifier(lexical variant: nor-malized value). For example, in Fig. 3, Moderate plaqueTable 1 Structure types with example sentencesExample sentenceProse 3045 % stenosis in the right ICA.List 1. Both ICAs are occluded.Table 95 % RICA 50 % LICA 75 % LECAHeading Right: ICA: stenosis >70 %.Other Any structures not listed aboveTable 2 Expression types with example sentencesExample sentenceCategory severe stenosisRange stenosis ranging from 40 to 70 %Exact 60 % stenosisMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 4 of 12in the right ICA is encoded as finding(plaque: carotiddisease), severity(Moderate: critical value), neurovascularanatomy(ICA: internal carotid artery), sidedness(right:right), and existence(default: definite existence) using theknowledge base. pyConText leverages these normalizedmodifier values to determine whether a mention of acarotid finding(carotid disease) in the neurovascularanatomy(internal carotid artery, common carotid artery,carotid bulb or carotid bifurcation) represents no signifi-cant stenosis (stenosis with existence: definite negatedexistence), insignificant stenosis (stenosis with severity:non-critical value e.g., values less than 50 % stenosis), orsignificant stenosis (stenosis with severity: critical valuese.g., values equal or greater than 50 % stenosis).Document classificationFor document-level classification, if either side or bothsides of the internal or common carotid artery are deter-mined to have significant stenosis, pyConText classifiesthe reports as stenosis positive; otherwise, it classifies itas stenosis negative. For RAD report example 1, in Fig. 3,the report would be classified as stenosis positivebecause two mentions of significant stenosis in the rightinternal carotid artery were identified. Figure 4 depictsRAD report example 1 fully processed by pyConText.pyConText evaluationpyConText applies a simple processing approach ofsegmenting and tokenizing sentences to process reports.The algorithm does not make use of Sections andstructures. Therefore, we quantified how frequentlycomplex document composition - Sections, structures,and expressions - are utilized to report carotid stenosisfindings to gauge whether document decomposition pro-cessing such as section or structure tagging is needed toaccurately extract findings. We evaluated the frequencyof errors by Sections, structures, and expressions bycomparing the predicted report classifications by pyCon-Text to those generated by our domain experts.Specifically, we defined a true positive when a report iscorrectly classified by pyConText as stenosis positiveand a true negative when a report is correctly classifiedby pyConText as stenosis negative. In contrast, wedefined a false positive when a report is spuriouslyclassified by pyConText as stenosis positive and a falsenegative when a report is spuriously classified by pyCon-Text as stenosis negative [12]. We assessed pyConTextsperformance by each Section and the full report usingstandard performance metrics of sensitivity, positivepredictive value (PPV), specificity, and negative predictivevalue (NPV) as follows:1. sensitivity ¼ true positivetrue positiveþfalse negative2. positive predictive value ¼ true positivetrue positiveþfalse positive3. specificity ¼ true negativetrue negativeþfalse positive4. negative predictive value ¼ true negativetrue negativeþfalse negativeResultsOur testing set was comprised of 498 radiology reports(RAD) ultrasounds and 498 TIU notes. At thedocument-level, for RAD reports, 353 (71 %) werestenosis negative and 145 (29 %) were stenosis positive;Fig. 2 Schema representing findings as well as semantic and linguistic modifiers and their possible normalized value setsMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 5 of 12for TIU reports, 396 (80 %) were stenosis negative and102 (20 %) were stenosis positive. The RAD training setdistribution of 68 % stenosis negative and 32 % stenosispositive was comparable to the RAD testing set distribu-tion. The TIU training set distribution of 87 % stenosisnegative and 13 % stenosis positive reports differedslightly from the RAD testing set distribution.Information content assessmentOf the 498 RAD reports, we observed most carotidmentions occur within the Impressions (488), are re-corded using prose (706), and are expressed as categor-ical expressions (713). Carotid mentions occurred oftenwithin both Findings and Impressions (359) (Table 3).In contrast, of the 498 TIU reports, we observed thatmost carotid mentions did not occur in either the Find-ings or Impressions (286). However, similarly to RADreports, carotid mentions were recorded using prose(294), and were expressed as categorical expressions(344) (Table 3).Fig. 4 The resulting RAD report example 1 processed by pyConTextfrom Fig. 3Fig. 3 Illustration of pyConTexts pipeline encoding a sentence and classifying the document from Fig. 1 RAD report example 1. Some modifierse.g., temporality and exam are not displayed for brevity. Blue mentions indicate templated mentions classified as no/insignificant stenosis; redmentions indicate templated mentions classified as significant stenosisMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 6 of 12For RAD reports, within Findings, most carotid men-tions were recorded as prose (306) followed by headings(66); within Impressions, most carotid mentions wererecorded as prose (352) followed by lists (127) (Table 4).In contrast, for TIU reports, within Findings, mostcarotid mentions were recorded as headings (43)followed by tables (33); as Impressions, most carotidmentions were recorded as prose (88) followed by headings(48) (Table 4).For RAD reports, of the carotid mentions reportedwithin both Finding and Impression (n = 359 reports;379 paired mentions), there was repetition of structuretypes between sections (239 paired mentions, 63 %)(diagonals in Table 5). In cases where a different struc-ture was used between sections (140 paired mentions,37 %), the most frequent cases were Finding: prose/Im-pression: list, and Finding: heading/Impression: prose(discordants in Table 5). For TIU reports, of the carotidmentions reported within both Finding and Impression(n = 67 reports; 53 paired mentions), there was repetitionof structure types between sections (22 paired mentions,41 %) (diagonals in Table 5). In cases where a differentstructure was used between sections (31 paired mentions,59 %), the most frequent cases were Finding: table/Im-pression: prose followed by Finding: heading/Impression:list and Finding: heading/Impression: heading (discordantsin Table 5).For RAD reports, both Findings and Impressions,most carotid mentions were expressed as category (330and 381, respectively) followed by range (73 and 178,respectively) (Table 6). We observed similar trends forTIU reports: category (73 and 116, respectively)followed by range (59 and 110, respectively) (Table 6).For RAD reports, of the carotid mentions reportedwithin both Findings and Impressions (n = 359 reports;526 paired mentions), there was repetition of expressiontypes between sections (345 paired mentions, 66 %)(diagonals in Table 7). In the cases where a differentexpression type was used between sections (181 pairedTable 3 According to report type, overall frequency of atleast one carotid mention within sections, types of structuresfor all carotid mentions, and types of expressions for allcarotid mentionsInformation type Information subtype Report typesRAD TIUSectionsFindings Total 368 106Impressions Total 488 173Findings Only 9 39Impressions Only 129 106Both 359 67Neither/Not Applicable 1 286StructuresProse 706 294List 256 76Table 0 36Heading 46 152Other 2 6ExpressionsCategory 713 344Range 254 314Exact 48 19Findings Total = Findings only + Both; Impressions Total = Impressionsonly + Both. Neither = report has Findings and Impressions, but does not containcarotid mentions; Not Applicable = report does not have Findingsand ImpressionsTable 4 Structure type usage according to sections and reporttypeProse List Table Heading OtherRADFindings 306 3 0 66 3Impressions 352 127 0 22 0TIUFindings 25 6 33 43 0Impressions 88 21 13 48 0Table 5 Structure type usage between Findings (rows) andImpressions (columns) for repetitive mentions by report typeProse List Table Heading OtherRADProse 233 (61 %) 73 (19 %) 0 (0 %) 1 (<1 %) 0 (0 %)List 1 (<1 %) 1 (<1 %) 0 (0 %) 0 (0 %) 0 (0 %)Table 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)Heading 35 (9 %) 27 (7 %) 0 (0 %) 5 (1 %) 0 (0 %)Other 2 (<1 %) 1 (<1 %) 0 (0 %) 0 (0 %) 0 (0 %)TIUProse 12 (23 %) 4 (7 %) 0 (0 %) 3 (6 %) 0 (0 %)List 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)Table 15 (28 %) 0 (0 %) 1 (2 %) 0 (0 %) 0 (0 %)Heading 0 (0 %) 9 (17 %) 0 (0 %) 9 (17 %) 0 (0 %)Other 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %) 0 (0 %)Table 6 Expression type usage by sections and report typeCategory Range ExactRADFindings 330 73 25Impressions 381 178 23TIUFindings 73 59 8Impressions 116 110 5Mowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 7 of 12mentions, 34 %), the most frequent cases were Finding:category/Impression: range and Finding: range/Impres-sion: category (discordants in Table 7). For TIU reports,of the carotid finding mentions reported within bothFindings and Impressions (n = 67 reports; 105 pairedmentions), there was repetition of expression typesbetween sections (45 paired mentions, 43 %) (diagonalsin Table 7). Similar to RAD reports, in the cases where adifferent expression type was used between sections (60paired mentions, 57 %), the most frequent cases wereFinding: category/Impression: range and Finding:range/Impression: category (discordants in Table 7).pyConText evaluationFor RAD reports, pyConText achieved the highest posi-tive predictive value (80 %) and specificity (93 %) whenprovided Impressions only (Table 8). However, thealgorithm performed with lower sensitivity (74 %) andnegative predictive value (90 %) compared to perform-ance when provided the full report performing withhigher sensitivity (88 %) and negative predictive value(95 %). For TIU reports, we observed a similar trend.pyConText achieved the highest positive predictive value(76 %) and specificity (98 %) when provided Impressionsonly, but higher sensitivity (73 %) and negative predict-ive value (92 %) when provided the full report (Table 8).For RAD reports, given the full report (including Find-ings and Impressions), pyConText generated 128 trueand 56 false positive, and 297 true and 17 false negatives.The 73 reports were misclassified due to non-mutuallyexclusive errors of 96 prose, 42 list, 0 table, 12 headings,and 0 other. These non-mutually exclusive errors werethe result of missed cues or erroneous scoping for 91category, 50 range, and 16 exact expressions. In termsof locality of errors, 53 mentions were in both sectiontypes, 1 mention was in Findings only, 19 mentions werein Impressions only, and 0 mentions were in neithersection. For TIU reports, given the full report (includingFindings and Impressions), pyConText generated 74 trueand 53 false positive, and 343 true and 28 false negatives.The 81 reports were misclassified due to non-mutuallyexclusive errors of 58 prose, 10 list, 8 table, 50 headings,and 0 others. These non-mutually exclusive errors werethe result of missed cues or erroneous scoping for 74category, 85 range, and 2 exact expressions. In terms oflocality of errors, 14 mentions were in both sections, fivementions were in Findings only, 21 mentions were in Im-pressions only, and 41 mentions were in neither section.DiscussionWe conducted a pilot study evaluating informationcontent of internal or common carotid finding mentionsin terms of Section, structure, and expression usage. Wealso assessed pyConTexts performance given thesethree factors.Information content assessmentFor RAD reports, most carotid mentions occurred inboth Impressions and Findings with a substantial por-tion occurring in both sections. Overall mentions wererecorded mainly as prose structure using category ex-pressions. When carotid mentions were reported inFindings and Impressions, they were most often encodedin prose. For these cases, pyConTexts simple text pro-cessing can accurately extract most of these mentions.In many cases, carotid mentions are repeated betweenFinding and Impressions, mainly as prose. In the case ofdiscordant structure usage, this redundancy can be aprocessing advantage. Specifically, one of the mostfrequent cases was Finding: heading/Impression: prose.Therefore, if given the full report, pyConText can stillcorrectly extract carotid mentions from the Impressionswhen it incorrectly extracts mentions from the Findingsdue to more complex structures like headings. Mostmentions were found in Impressions composed mainlyusing expressions of category. In cases of repetitivedescriptions between Findings and Impressions, mostare Finding: category/Impression: category and men-tions with discordant structure usage were Finding: cat-egory/Impression: range. These observations suggestTable 7 Expression type usage between Findings (rows) andImpressions (columns) for repetitive mentions by report typeCategory Range ExactRADCategory 278 (53 %) 108 (20 %) 14 (3 %)Range 35 (7 %) 53 (10 %) 2 (<1 %)Exact 16 (3 %) 6 (1 %) 14 (3 %)TIUCategory 30 (29 %) 23 (22 %) 1 (<1 %)Range 26 (25 %) 13 (12 %) 3 (3 %)Exact 3 (3 %) 4 (4 %) 2 (2 %)Table 8 pyConText performance according to report typeSensitivity PPV Specificity NPVRADFindings 57 67 88 83Impressions 74 80 93 90Full report 88 70 84 95TIUFindings 60 55 88 89Impressions 19 76 98 82Full report 73 58 87 92For each metric and report type, the highest metric value is boldedMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 8 of 12that most severity descriptions can be extracted lever-aging lexical, qualitative (e.g., severe) regular expres-sions rather than quantitative (e.g., 7099 %) regularexpressions.For TIU reports, in contrast to RAD reports, mostcarotid mentions occurred in neither Findings norImpressions, suggesting localized processing of reportsfor extracting carotid mentions would be suboptimal.In the few cases when carotid mentions were reportedin Findings, they were most often headings followedby table structures. Similar to RAD reports, carotidmentions were reported in Impressions using prose,but also using headings, suggesting that complex docu-ment processing could be useful. Additionally, mostmentions were found in Impressions composed mainlyusing expressions of category and exhibited the similardistributions of repetitive expression descriptions betweenFindings and Impressions.For both RAD and TIU reports, we observed severalmentions with two or more expressions or structures.For example, 55 % moderate ICA stenosis containstwo expressions: exact (55 %) and category (moderate).pyConText evaluationWe aimed to optimize the number of flagged positivecases for review (high sensitivity), while minimizing theloss of positive cases due to filtering (high negativepredictive value); therefore, we conclude that pyConTextperformed best with the full report rather than with onlythe Finding or Impression sections. We hypothesize thatproviding pyConText with the full report resulted in thehighest sensitivity because carotid mentions occurredwith variable prevalence within Findings and Impres-sions (RAD) or within neither section type (TIU).Error analysisA detailed error analysis of pyConTexts outputs revealedseveral areas of improvement to reduce false positives andnegatives. For each error described, we provide anexample and potential solution to boost performancewithin pyConTexts processing pipeline.Error 1: For both RAD and TIU reports, some falsepositives were due to missing category or range expres-sions for semantic modifiers. For instance, in Example 1,although we had small as a non-critical value for sever-ity and moderate as a critical value for severity, we didnot have small to moderate in our knowledge base dueto mixing of quality (small) and quantity (moderate)descriptors. In these cases, our domain experts used thelower bound (small) to classify the severity value andassert the carotid mention as insignificant stenosis.However, pyConText did not recognize this as a rangeexpression and the upper bound (moderate) wasincorrectly used to classify the severity value and assertthe finding as significant stenosis.Example 1. small to moderate amount of calcifiedplague in the left carotid bulb.Potential solution 1: To improve assertion detection,we can add missed cues and expand upon existing regu-lar expressions for the severity modifier. We could alsoadd a rule that classifies ranges by the lowest bound fora severity value range by selecting the non-critical valueover the critical value.Error 2: In some cases, false positives were due tomissing lexical variants for linguistic modifiers. InExample 2, we did not have a regular expression forfails to demonstrate for existence: definite negatedexistence; therefore, the algorithm classified the findingas significant stenosis.Example 2. examination of carotid arteries fails todemonstrate significant stenosis.Potential solution 2: To improve assertion detection,again, we can add missed cues and expand upon existingregular expressions to identify linguistic modifiers fromthe text.Error 3: Sometimes, the expressions were correct, butspuriously attributed to flow velocities that were notused to assert stenosis findings as in Example 3.Example 3. diameter reduction.. cca with velocity of82.Potential solution 3: To improve assertion detectionand scope, we could have created another modifiervelocity to correctly scope the severity modifier and filterthis mention from classification.Error 4: Our results suggest that we achieved lowerperformance for TIU reports than RAD reports due tomore frequent usage of complex document structuressuch headings and tables rather than less complexdocument structures of prose and lists. In Example 4,ICA was correctly attributed to Left 40 % stenosis,but not associated to Right 30 % stenosis.Example 4. ICA: Left 40 % stenosis. Right 30 %stenosis.Potential solution 4: To improve assertion detectionand scope, we could boost pyConTexts performance byintegrating outputs from a section tagger to identifymentions of neurovascular anatomy from headings/sub-headings and associate them to all subsequent sentenceswithin that section with relevant findings.Error 5: In few examples, the algorithm generated afalse negative due to its failure to identify co-referredfindings of plaque. For Example 5, we observed twoconsecutive, long sentences. The first sentence containsa finding and neurovascular anatomy, but the secondsentence contains its severity modifier. To link theseverity in the second sentence to the finding and itsneurovascular anatomy in the first sentence, we wouldMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 9 of 12need to resolve that the finding plaque in the secondsentence co-refers to the finding plaque in the firstsentence and merge their templates.Example 5. ..calcified plaque in the left ica data are consistent with between 50 and 80 % stenosisby plaque.Potential solution 5: To improve named entity rec-ognition and assertion detection, we could handle co-reference, by identifying co-referring expressions andeither merging or resolving conflicting values for eachfinding template.Error 6: Not all failures resulted in a document mis-classification. In Example 6, the finding is not given, butimplied by the checkbox and associated modifiers ofsidedness, neurovascular anatomy, and severity sopyConText did not extract a stenosis finding. However,if this statement represented a significant stenosis mention,a false negative would have resulted.Example 6. Left ICA [x]: 015 %.Potential solution 6: To improve named entity recogni-tion and assertion detection, we could integrate outputsfrom document decomposition software [26] that readilyidentifies checkbox and question/answer constructsbased on characters within the text. We could leveragethese patterns to predict when and how these constructsshould be used to extract assertions and correctly asserttheir scope when a finding is not explicitly mentioned.Error 7: Similarly, although pyConText did not classifya finding mention in one sentence due to a missingmodifier, it was able to identify and extract a findingmention from another sentence to correctly classify thereport. In Example 7, pyConText does not find a neuro-vascular anatomy modifier for the second sentence, so itignores it, but correctly classifies the report by correctlyextracting information from the first sentence.Example 7. Right ICA occluded 1) occlusion onthe right.Potential solution 7: To improve document classifica-tion, we could classify sentences without a neurovascularanatomy modifier, but this strategy would have caused asignificant increase in the number of false positives whenthe mention represents an irrelevant neurovascular anat-omy such as the external carotid artery, increasing thenumber of reports for chart review by abstractors.Error 8: Finally, false positives could be attributed to alack of topical context. In Example 8, the sentence doesnot contain an actual finding, but rather guidelines forclassifying mentions as significant stenosis.Example 8. definitions: 7099 % = significant stenosisPotential solution 8: To improve document classifica-tion, we could exclude extracted findings and assertionsdetected from all sentences that occur in the context ofknown guidelines e.g., documented NASCET legends byfiltering these mention with a semantic modifierguidelines and regular expressions with guideline-associated keywords like definitions, legend orNASCET.Although many of these solutions could prove useful,they may add significantly to pyConTexts processingtime and complexity. For this study, it was only neces-sary to identify about 6,000 Veterans for cohort inclu-sion; therefore, we applied the system to the greater setof patient records based on these results. Because ourgoal is to retain as many stenosis positive cases as pos-sible while filtering as many stenosis negative cases aspossible, we provided pyConText the full report ratherthan only processing Impressions. To date, we haveencoded over 150,000 RAD and 200,000 TIU reports.Given these results, we estimate that we have reduced thechart review task for study abstractors to about 85,000(~25 %) of the possible reports. The manual review of thisfiltered set was completed in 4 months by three ab-stractors rather than 12 months without the NLPfiltering.LimitationsOur study has a notable limitation. We only addressreports from the VA EHR; therefore, pyConTextsperformance may or may not generalize to reports fromother institutions. However, if the reports containsimilar Sections, structures, and expressions, we wouldexpect similar results. We will evaluate pyConTextsgeneralizability on University of Utah Healthcare Systemreports for both genotype-phenotype association and strokerisk assessment studies in the near future.Future workAlthough for this study, we developed a sensitive NLP algo-rithm to identify high risk patients for stroke to support acomparative effectiveness review study, we plan to extendour algorithm to extract additional stroke risk factors forprecise stroke subtype phenotyping e.g., ischemic andhemorrhagic stroke subtypes and endotypes e.g., ischemicstroke endotypes of cardiac embolism, large artery athero-sclerosis, and lacunar infarction, other uncommon causesfor genotype-phenotype association studies. We are activelygenerating a pipeline with our knowledge base authoringsystem, Knowledge Author, to leverage existing vocabular-ies such as the Unified Medical Language System (UMLS)[27] and Radiology Lexicon (RadLex) as well as ontologiessuch as our Modifier Ontology to encode these stroke riskfactors in a more streamlined manner [28, 29].ConclusionsWe conclude that an information content analysis canprovide important insights for algorithm development andevaluation including understanding information redun-dancy and challenges when processing clinical texts toMowery et al. Journal of Biomedical Semantics  (2016) 7:26 Page 10 of 12identify stroke risk factors. Our study demonstrates that,in spite of these challenges, a simple NLP algorithm, canbe leveraged to reduce chart review efforts by filteringreports with no/insignificant carotid stenosis findings andflagging reports with significant carotid stenosis findingsfrom Veteran Health Administration clinical reportsto support a comparative effectiveness study of strokeprevention strategies.Availability of the supporting dataThe supporting annotated dataset contains protectedhealth information and is stored in the Veteran AffairsInformatics and Computing Infrastructure (VINCI). It isnot available to researchers outside of the Department ofVeteran Affairs. However, pyConText is available throughhttps://github.com/chapmanbe/pyConTextNLP. Additionalstudy information and collaborative development forpyConText can be found at http://toolfinder.chpc.utah.edu/content/pycontext.AbbreviationsCPT: current procedural terminology; RAD: radiology; TIU: text integrationutility; EHR: electronic health records; GWAS: genome-wide associationstudies; PheWAS: phenotype-wide association studies; ML: machine learning;NLP: natural language processing; eMERGE: electronic medical records andgenomics; SHARPn: Strategic Health IT Research Program; PAD: peripheralartery disease; IRB: Institute Review Board; VA: veteran affairs;CHIR: consortium for healthcare informatics research; PPV: positive predictivevalue; NPV: negative predictive value; UMLS: unified medical languagesystem; RadLex: radiology lexicon; VINCI: veteran affairs informatics andcomputing infrastructure; PMRP: personalized medicine research project;UIMA: unstructured information management architecture; QDM: quality datamodel; NIH: National Institute of Health.Competing interestsWe have no competing interests.Authors contributionsDM and BS designed the evaluation. EM and SK defined the stroke risk factordefinition, queried the dataset from the Veteran Affairs electronic medicalrecord, and facilitated the annotation of the dataset. BEC and WWC designedthe original pyConText algorithm and knowledge base. DM extended theseknowledge bases and adapted the pyConText algorithm. DM and MC wrotethe original manuscript and provided this manuscript for editing and feedbackto all coauthors. All authors read and approved the final manuscript.AcknowledgmentsThis work was partially funded by VA HSR&D Stroke QUERI RRP 12-185, NHLBI1R01HL114563-01A1, NLM R01 LM010964, & NIGMS R01GM090187. We thankAlexandra Woodbridge, Ann Abraham, Rosa Ahn, and Susan Saba for theirexcellent annotation efforts. We would also like to thank the Phenotype Dayworkshop organizing and program committee for the opportunity to extendour original submission [18] for this special issue and our anonymousreviewers for their insightful feedback.Author details1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT,USA. 2IDEAS Center, Veteran Affair Health Care System, Salt Lake City, UT,USA. 3San Francisco Veteran Affair Health Care System, San Francisco, CA,USA.Received: 21 October 2015 Accepted: 19 April 2016Bolleman et al. Journal of Biomedical Semantics  (2016) 7:39 DOI 10.1186/s13326-016-0067-zRESEARCH Open AccessFALDO: a semantic standard fordescribing the location of nucleotide andprotein feature annotationJerven T. Bolleman1*, Christopher J. Mungall2, Francesco Strozzi3, Joachim Baran4, Michel Dumontier5,Raoul J. P. Bonnal6, Robert Buels7, Robert Hoehndorf8, Takatomo Fujisawa9, Toshiaki Katayama10and Peter J. A. Cock11AbstractBackground: Nucleotide and protein sequence feature annotations are essential to understand biology on thegenomic, transcriptomic, and proteomic level. Using Semantic Web technologies to query biological annotations,there was no standard that described this potentially complex location information as subject-predicate-object triples.Description: We have developed an ontology, the Feature Annotation Location Description Ontology (FALDO), todescribe the positions of annotated features on linear and circular sequences. FALDO can be used to describenucleotide features in sequence records, protein annotations, and glycan binding sites, among other features incoordinate systems of the aforementioned omics areas. Using the same data format to represent sequence positionsthat are independent of file formats allows us to integrate sequence data from multiple sources and data types. Thegenome browser JBrowse is used to demonstrate accessing multiple SPARQL endpoints to display genomic featureannotations, as well as protein annotations from UniProt mapped to genomic locations.Conclusions: Our ontology allows users to uniformly describe  and potentially merge  sequence annotations frommultiple sources. Data sources using FALDO can prospectively be retrieved using federalised SPARQL queries againstpublic SPARQL endpoints and/or local private triple stores.Keywords: SPARQL, RDF, Semantic Web, Standardisation, Sequence ontology, Annotation, Data integration,Sequence featureBackgroundDescribing regions of biological sequences is a vital partof genome and protein sequence annotation, and in areasbeyond this such as describing modifications relatedto DNA methylation or glycosylation of proteins. Suchregions range from one amino acid (e.g. phosphorylationsites in singalling cascades) to multi megabase contigsmapped to a complete genome. Such annotation has beendiscussed in biological literature since at least 1949 [1] andrecorded in biological databases since the first issue of theAtlas of Protein Sequence and Structure [2] in 1965.*Correspondence: jerven.bolleman@sib.swiss1Swiss-Prot group, SIB Swiss Institute of Bioinformatics, Centre MedicalUniversitaire, 1 rue Michel, Servet, 1211 Geneva 4, SwitzerlandFull list of author information is available at the end of the articleThere are many different conventions for storinggenomic data and its annotations in plain text flat file for-mats such as Generic Feature Format version 3 (GFF3),Genome Variation Format (GVF) [3], Gene Transfer For-mat (GTF) and Variant Call Format (VCF), and morestructured domain specific formats such as those fromINSDC (International Nucleotide Sequence DatabaseCollaboration) or UniProt, but none are flexible enoughto discuss all aspects of genetics or proteomics. Fur-thermore, the fundamental designs of these formats areinconsistent, for example both zero-based and one-basedcounting standards exist, a regular source of off-by-oneprogramming errors, which experienced bioinformati-cians learn to look out for.© 2016 Bolleman et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Bolleman et al. Journal of Biomedical Semantics  (2016) 7:39 Page 2 of 12Although non-trivial, file format interconversion is acommon background task in current script-centric bioin-formatics pipelines, often essential for combining toolssupporting different formats or format variants. As aresult of this common need, file format parsing is a partic-ular strength of community developed open source bioin-formatics libraries like BioPerl [4], Biopython [5], BioRuby[6] and BioJava [7]. While using such shared libraries canreduce the programmer time spent dealing with differ-ent file formats, adopting Semantic Web technologies haseven greater potential to simplify data integration tasks.As part of the Integrated Database Project (http://lifesciencedb.mext.go.jp/en/) and the Core Technol-ogy Development Program (http://biosciencedbc.jp/en/33-en/programs/236-programs) to integrate life sciencedatabases in Japan, the National Bioscience Database Cen-ter (NBDC) and the Database Center for Life Science(DBCLS) have hosted an annual BioHackathon seriesof meetings bringing together biological database teams,open source programmers, and domain experts in Seman-tic Web and Linked Data [811]. At these meetings itwas recognised that failure to standardise how to describepositions and regions on biological sequences would bean obstacle to the adoption of federalised SPARQL Pro-tocol and RDF Query Language (SPARQL) queries whichhave the potential to enable cross-database queries andanalyses. Discussion and prototyping with representa-tives from major sequence databases such as UniProt[12], DDBJ (DNA Data Bank of Japan) [13] (part of theINSDC partnership with the National Center for Biotech-nology Information (NCBI)-GenBank [14] and EuropeanMolecular Biology Laboratory (EMBL)-Bank [15]), and anumber of glycomics databases (BCSDB [16], GlycomeDB[17], GLYCOSCIENCES.de [18], JCGGDB, RINGS [19]and UniCarbKB [20]) and assorted open source devel-opers during these meetings led to the development ofthe Feature Annotation Location Description Ontology(FALDO).FALDO has been designed to be general enough todescribe the position of annotations on nucleotide andprotein sequences using the various levels of locationcomplexity used in major databases such as INSDC(DDBJ, NCBI-GenBank and EMBL-Bank) and UniProt,their associated file formats, and other generic annotationfile formats such as Browser Extensible Data (BED), GTFand GFF3. It includes compound locations, which are thecombination of several regions (such as the join locationstring in INSDC), as well as ambiguous positions. It allowsus to accurately describe ambiguous positions today insuch a way that future more precise knowledge does notintroduce logical conflicts, which potentially could only beresolved by intervention of an expert in the field.FALDO is suited to accurately describe the position ofa feature on multiple sequences. This is expected to bemost useful when lifting annotation from one draft assem-bly version to another. For example, a gene can start at aposition for a given species genome assembly, while theconceptually same gene can start at another position inprevious/following genome assemblies for the species inquestion.FALDO has a deliberately narrow scope which does notaddress general annotation issues about the meaning ofor evidence for a location, rather FALDO is intended beused in combination with other relevant ontologies suchas the Sequence Ontology (SO) [21] or database-specificontologies. That is, it is used only to describe the lociof features, not to describe the features themselves. AFALDO position relative to a sequence record is compa-rable to a coordinate position on a map: it makes no claimabout how that sequence record or map is related to thereal world.ImplementationFALDO is a small web ontology language version 2(OWL2) ontology with 16 classes, 11 of these deal withthe concept of a position on a sequence (Fig. 1). Theinstances of the faldo : ExactPosition representpositions that are accurately determined in respect to areference sequence. There are two convenience subclassesof faldo : ExactPosition to represent positions onthe N and C-terminal of a amino acid sequence. Threeof those classes are used to describe accurately what weknow of a position that is not precisely determined. Fourclasses are used to describe the concept of a position on astrand of DNA, e.g. positive, negative and on both strands.All ten of these classes are sub classes of the genericfaldo : Position super-class. The eleventh class is theconcept of a region i.e. something with a end and startposition. The remaining 3 classes are used to groupregions which are biologically related but for which noexact semantics are available e.g. some legacy data sourcescannot be mapped cleanly without expert intervention.In contrast to other representations, FALDO has noexplicit way to say that it is not known on which stranda position is, because this explicit statement unknownstrand position can introduce contradictions when merg-ing different data sets. For example, some positions couldend up being contradictorily typed both as forward-stranded as well as being located on an unknown strandposition.Thereare3moreclasses (faldo: CollectionOfRegionsand its subclasses) that are only there for backwardscompatibility with INSDC join features with uncer-tain semantics. i.e. those join regions where a con-version program can only state that there are someregions and that the order that they are declared inthe INSDC record might have biological significance.However, here the INSDC record needs intelligentBolleman et al. Journal of Biomedical Semantics  (2016) 7:39 Page 3 of 12Fig. 1 The classes and object properties used in FALDOinspection before the data can be cleanly converted to adata model with rich semantics.FALDO defines a single datatype property,faldo : position, that is used to provide a one-based integer offset from the start of a referencesequence. This property, when used together withthe faldo : reference property, links the con-cept of a faldo : Position to an instance of abiological sequence. Note that these terms are case-sensitive: faldo : position is a property, andfaldo : Position is a concept.For compatibility with a wide range of data, FALDOmakes very few assumptions about the representa-tion of the reference sequence, and can be used todescribe positions on both single- and double-strandedsequences. When both strands of a double-strandedsequence are represented by a single entity (recommendedover each strand being represented separately), integerfaldo : position properties are counted from the 5end of whichever strand is considered the forwardstrand.A key part of the FALDOmodel is the separation of fea-ture and where a feature is found in a sequence record. Forthis we use the faldo : location object property. Thisproperty is used to distinguish between a conceptual geneas an unit of inheritance and the corresponding repre-sentation of the DNA sequence region encoding the geneas stored in a database.As in the INSDC data model and the associated Gen-Bank ASN.1 notation, each location in FALDO has anidentifier for the sequence it is found on [22]. This meansthat the position information is complete without furtherRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 DOI 10.1186/s13326-016-0109-6RESEARCH Open AccesstESA: a distributional measure forcalculating semantic relatednessMaciej Rybinski and José Francisco Aldana-Montes*AbstractBackground: Semantic relatedness is a measure that quantifies the strength of a semantic link between twoconcepts. Often, it can be efficiently approximated with methods that operate on words, which represent theseconcepts. Approximating semantic relatedness between texts and concepts represented by these texts is animportant part of many text and knowledge processing tasks of crucial importance in the ever growing domain ofbiomedical informatics. The problem of most state-of-the-art methods for calculating semantic relatedness is theirdependence on highly specialized, structured knowledge resources, which makes these methods poorly adaptablefor many usage scenarios. On the other hand, the domain knowledge in the Life Sciences has become more and moreaccessible, but mostly in its unstructured form - as texts in large document collections, which makes its use morechallenging for automated processing. In this paper we present tESA, an extension to a well known Explicit SemanticRelatedness (ESA) method.Results: In our extension we use two separate sets of vectors, corresponding to different sections of the articles fromthe underlying corpus of documents, as opposed to the original method, which only uses a single vector space. Wepresent an evaluation of Life Sciences domain-focused applicability of both tESA and domain-adapted ExplicitSemantic Analysis. The methods are tested against a set of standard benchmarks established for the evaluation ofbiomedical semantic relatedness quality. Our experiments show that the propsed method achieves resultscomparable with or superior to the current state-of-the-art methods. Additionally, a comparative discussion of theresults obtained with tESA and ESA is presented, together with a study of the adaptability of the methods to differentcorpora and their performance with different input parameters.Conclusions: Our findings suggest that combined use of the semantics from different sections (i.e. extending theoriginal ESA methodology with the use of title vectors) of the documents of scientific corpora may be used toenhance the performance of a distributional semantic relatedness measures, which can be observed in the largestreference datasets. We also present the impact of the proposed extension on the size of distributional representations.Keywords: Bioinformatics, Semantic relatedness, Semantic similarity, Distributional linguistics, Knowledge extraction,Explicit semantic analysis, Biomedical semanticsBackgroundIntroductionA rapid growth in scientific publishing has been observedin recent years. Thanks to online resources, the accessto this literature seems easier and quicker than ever, butoften the sheer volume of potentially relevant articlesmakes it extremely difficult for the end user. However,working with these large text collections may actually*Correspondence: jfam@lcc.uma.esDepartamento LCC, University of Malaga, Campus Teatinos, 29010 Malaga,Spainresult in the development of methods for automaticsemantic processing and annotation that could greatlyimprove intelligent data access. This paper focuses onthe problem of calculating distributional semantic relat-edness based on a large document corpus by leveragingthe semantics from different sections of the corpus ele-ments (i.e. by making an explicit use of the semantics oftitles of scientific papers). Semantic relatedness is a met-ric that can be assigned to a pair of labels in order torepresent the strength of the relationship of the conceptsdescribed by those labels. The automated calculation of© The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 2 of 14the metric is the building block for numerous semanticallyenhanced data processing techniques such as: word sensedisambiguation [1] (used for matching word contexts tothe best word senses), text summarization [2] (used forevaluating cohesion of the lexical chains) and informationretrieval [3] (incorporated in the query-document rankingmethod). Similar applications of relatedness and similar-ity (which is a narrower concept) metrics within the scopeof Life Sciences include entityentity relationship extrac-tion [4, 5], semantic search [6] and redundancy detectionin clinical records [7]. An overview of applying semanticsimilarity to the problem of comparing gene products isdiscussed in [8]. In [9] the authors discuss the applicationof a relatedness measure as an approximation of semanticsimilarity in the biomedical domain.The methods for calculating semantic relatedness canbe roughly divided into two main groups: those that relyentirely on a specialized and structured knowledge-richresource (e.g. [1012]), and distributional measures thatrely on implicit statistical features of a large documentcollection (e.g. [13, 14]). With the increased popularity ofusing Wikipedia as a Knowledge Base (KB) for seman-tic relatedness estimation this division has become muchless clear, as Wikipedia combines the features of bothworlds. It does implicate a structure, as it comprises a setof topic-oriented and categorized entries, which are alsointerconnected with hyperlinks. It can also be treated asa large collection of documents, as it contains over 2Marticles with at least 150 words each.In this paper we focus on corpus-based distributionalmethods for calculating semantic relatedness and wepresent a new measure, which can be applied in thebiomedical domain without having to rely on specializedknowledge rich resources. In our approach, which is anextension of a well-established state-of-the-art method,we superimpose the semantics of different sections ofdocuments (i.e. wemake additional use of the titles of sci-entific articles). We demonstrate that our method slightlyoutperforms other state-of-the-art approaches while rely-ing on the very limited structure of the documents withinthe corpus (only abstracts and titles from the Medlinecorpus [15] are used in the best performance setting).Related workThere is a significant body of work devoted to biomedi-cal ontology-independent (to a certain degree) relatednessmeasures that rely on context vectors, i.e. the immediateneighborhoods of the phrases/words throughout a docu-ment corpus, e.g. [16]. In the method presented in [16],context vectors are created using a sliding window tech-nique, that is, scanning through contexts of a certain sizethroughout the entire corpus of documents in order tofind words that co-occur with certain terms or phrases ofinterest. In order for this technique to be employed, theauthors use a predefined set of these terms/phrases, i.e.SNOMED CT. SNOMEDCT is the largest medical vocab-ulary collection, with over 400K systematically organizedconcepts with their lexical representations and additionalinformation. In the method presented in [16], the distri-butional representations are created for each SNOMEDCT concept by adding word vectors of tokens relevant torespective concepts. Despite the fact that the approachuses additional resources (SNOMED, Mayo Clinic The-saurus), the relatedness calculation depends on the corpusco-occurence distribution, without referring explicitly tothe ontological structure of SNOMED. Both in [17], andmore recently in [9], a similar approach has been usedwith a different set of resources. Themain feature that setsthe method presented in our paper apart is that it does notneed pre-existing concept descriptions (such as those ofSNOMED CT) in order to produce the relatedness score.As mentioned briefly, there is a large group of methodsthat use Wikipedia as a knowledge resource/documentcollection, some examples include [1820]. Most of thesemeasures exploit Wikipedia-specific features such as linksor categories. Nonetheless, Wikipedia as a resource (atleast currently) is too general in nature for many Life Sci-ences applications. Therefore, from our perspective, themethods that treat the data more like a generic documentcollection seem more appealing, the most notable exam-ple being Explicit Semantic Analysis (ESA) [21]. In ESA,the input texts are represented by a vector, in which eachelement corresponds to aWikipedia article. Values of eachof the elements are determined by the importance of theinput text to the contents of each article, i.e. i-th elementof the vector for a word or a phrase will be determined bythe importance of the word within the i-thWikipedia arti-cle (formal description of the method is provided furtheron in this paper). The relatedness between the inputs iscalculated as the cosine similarity between those vectors.Numerous extensions of ESA have been proposed,many of which combine the original approach with theWikipedia-specific features, through concept-to-conceptfeature/similarity matrices, e.g. [2224]. Some of thoseextensions, e.g. NESA [25] (Non - Orthogonal ESA), alsoprovide variants that are generic enough to be used withany document collection. The aim of NESA is to lever-age inter-document similarity in the calculations. In ourmeasure the input is modeled in a way similar to ESA,but we propose an extension so as to capture the fea-ture based similarity between sets of documents. Howeverour method is much more resource efficient than NESA,which facilitates handling a large corpus of documents.In the biomedical domain there have also been sev-eral attempts to use Wikipedia based methods, recentexamples include [26] and [27]. The former presents anapplication of the ESA methodology to a KB extractedautomatically from MedLine Plus corpus in the contextRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 3 of 14of semantic relatedness. The latter uses ESA inspiredmethodology with yet another KB in the context of docu-ment classification.As we have previously argued [28], results compara-ble to those of state-of-the-art methods can be obtainedby approximating the context vectors with the vectorsextracted from the relatively small sample of best-fitdocuments from a moderately sized PMC open subsetcorpus [29]. We now expand on these conclusions incombination with an ESA inspired approach to achievebetter results, coverage and independence from the spe-cific parameters of the algorithm, which was one of thedrawbacks in our previous approach. The new methodtakes advantage of a larger document collection (Med-line), but performs well with only the abstracts and titlesavailable.Within the NLP community, so called word embeddingmethods have received much attention. In these tech-niques words or phrases from the original corpus aremapped to low dimensional vectors through languagemodelling and/or feature learning. One of the most widelydiscussed representative of this group, word2vec [30] is agroup of methods that use neural networks for unsuper-vised training of a model that either predicts a contextgiven a word, or predicts the word given a context. Appli-cation of word2vec in biomedical settings is presented ina recent study [31].There is also a significant body of work related to KB-based semantic relatedness measures which use highlyspecialized resources, described in a detailed overview in[32] and [33]. KB-based methods are useful wherever anadequate domain knowledge model can be used to com-pute semantic relatedness. In [34] the authors showcasethe performance of a wide spectrum of ontology basedInformation Content (IC) methods, which use SNOMEDCT as a knowledge resource. The IC measures use anontological structure (positions of concepts in the ontol-ogy, distance between them, number of sub-concepts,etc.) to compute a semantic score between a pair of con-cepts. Our method, although dependent on a specificcorpus, does not rely on high level KB representations ofthe domain, which makes it more flexible and easier toadapt to non-standard use cases.ContributionsHere we present Title vector Explicit Semantic Analysis(tESA), a novel approach for approximating word-basedsemantic relatedness, which uses a document corpusas its only source of background knowledge. The tESAmethod itself is an extension of ESA, based on usingtwo sets of vectors corresponding to different sections ofthe documents of the corpus. Together with the exper-iments detailing its performance, tESA is our primarycontribution.Additionally, we present a parallel evaluation of theoriginal ESA methodology in the same settings (corporaand reference standards). To the best of our knowl-edge it is the first time that the ESA implementationhas been evaluated in such detail within the biomedicaldomain.In the Methods section we present a detailed descrip-tion of ESA, tESA and the experimental evaluation. Wealso highlight the distinguishing design features of tESAby comparing it to other corpus-based methods. Then, inthe Results and discussion section we present the resultsobtained through the evaluation, compare them to otherstate-of-the-art methods and discuss some of the impli-cations. In the final Conclusions section, apart from pre-senting the final remarks, we also outline possible lines offuture work.MethodsIn this section, we firstly explain the basic conceptsthat will help clarify the design of the tESA method.We then provide a short description of the origi-nal ESA method and then we introduce the tESAmethod, while outlining the main differences between thetwo.Basic notionsThe black-box view of a semantic relatedness approxima-tion system is fairly simple - the system takes two inputtexts (also referred to as inputs) and returns a relatednessapproximation (score). The inputs can be texts of vari-able length, typically single words or short phrases areconsidered.The actual processing involves the inputs and a col-lection of documents - referred to as the corpus. Weuse a term document to denote a semistructured textualresource that forms part of this collection, i.e. a documentcan be formed by a number of sections; here, we focus ona simplified case of documents consisting either of titlesand abstracts or titles and the fulltext body (depending ontheir availability in various document collections includedin the evaluation).As mentioned, our method is based on a distribu-tional vector representation of input texts. As is com-mon in many distributional linguistics algorithms, weuse certain variations of the tf-idf (term frequency,inverse document frequency) weighting scheme as theunderlying vector model for text representation. So,at the most basic level, prior to relatedness calcu-lations, any texts (inputs, document abstracts, titles)are modeled as tf-idf weighted vectors. Term fre-quency is the number of times a given term appearswithin the scope of a certain text (i.e. certain sectionof a document), while inverse document frequency isdefined in the context of a specific document collection:Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 4 of 14idf(t,D, f ) = log N|df ? D : t ? df | , (1)where D denotes a certain corpus of documents, Ndenotes size of the corpus, t denotes the term and da document, f denotes a section of documents fromthe corpus and df a text of the section f of a docu-ment d. Those elements lead us to the formula for tf-idf :tfidf(t, df ,D) = tf(t, df ) × idf(t,D, f ) (2)The equation presents a basic implementation of tf-idfweighting, whereas within our approach we use slightlydifferent variants. For modelling abstracts the in-builtLucene [35] scoring function is used. It uses a documentlength normalization factor, a square root norm for thetf factor and a square norm for the idf factor. For titleswe assume tf equals 1 whenever a term appears withinthe title and zero otherwise. Nonetheless, the basic idea isthat within a vector for a single document higher weightsare assigned to terms that either appear more often withinthe document or are less common throughout the entirecorpus. When creating the vector representation of textusing the tf-idf scheme, vectors are assembled by placinga weight corresponding to each of the documents termsat the position corresponding to the term, so the dimen-sionality of the model is given by the number of uniquewords present in the section of the documents throughoutthe collection. Therefore the vector space is of a very highdimension, while the actual vectors are normally sparse.It is worth noting, that, given a corpus and a specificsection of its documents, the vector representation canbe created for any text, regardless of whether the textbelongs to the corpus or not. This representation willobviously differ depending on the choice of the corpusand the section. This notion is typically used in vector-based information retrieval (IR), where most relevantdocuments are found for an input query and a field or acombination of fields of an index, where fields correspondto sections and index to the corpus. Commonly, to decidewhether a document fits the query, one can compare thevector representing the query with the vector represent-ing the section of a document. We use cosine similarity asthe basic tool for pairwise vector comparison. This appliesto word-based tf-idf vectors and extends to other typesof vectors, as explained further on in this section. For apair of n element vectors A and B the cosine similarity isdefined as follows:cosine(A,B) =n?i=1AiBi?n?i=1A2i?n?i=1B2i(3)Text preprocessingWe use standard Lucene mechanisms for pre-processingof texts prior to the tf-idf vectors computations. Textsare transformed to lowercase and stopwords (words thatoccur very commonly, but provide little or no semanticinformation, e.g. the, of, at, a, etc.) are eliminated. Num-bers are also eliminated and non-alphanumeric characters(e.g. -) are normalized. In case of the titles, we also disre-gard words that appear in less than 3 different documentsof the respective corpora.ESAThese basic notions lead us to the more complex oneof a doc vector (also referred to as concept vector in theoriginal ESA paper [21]), which is the central buildingblock of ESA. In the ESA method the doc vectors are usedto provide a distributional representation of the inputs.The relatedness is then approximated for a pair of inputsby comparing their doc vectors. Cosine similarity is usedto obtain the numeric result of this comparison. By adoc vector of an input q we mean a vector in which thevalue of an i-th element is calculated as a cosine similar-ity between: (a) the tf-idf vector representing the inputq w.r.t. the IDF values calculated for the abstracts of thecorpus; (b) tf-idf weighted vector representing an abstractof an i-th document of the corpus 1. It is worth not-ing that the dimensionality of the doc vector is givenby the size of the corpus. Ttf-idf vector qabstract repre-sents an input q w.r.t. the statistics (i.e. IDF) derivedfrom the abstract section of the corpus documents. Wecan define the doc vector qD as a vector of weights wi,q,wherewi,q = cosine(abstracti, qabstract)(4)where abstracti denotes the tf-idf vector of the abstractfor the i-th document from the N document corpus. Inthe original method a corpus of Wikipedia articles isused, along with their text contents. In this paper, apartfrom the original Wikipedia-based implementation, wealso present experiments with domain-focused corpora.In practical implementations it is enough to consider aset of M highest scores within the vector, as the tail ofN-M values are either zeroes or have little impact on fur-ther processing. As such, ESA methodology can also beexplained in information retrieval terms, with the inputtreated as a query and the results represented with a docvector of non-zero values at M most significant elements.Those values, in a most basic tf-idf weighted vector spacemodel representation, are given with the formula for wi,q.This intuitive explanation of ESA might clarify the step-by-step processing of tESA, presented further on in thissection.Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 5 of 14tESAIt can be observed, that a corpus with documents thathave more than one section can be used to establish morethan one independent vector space, i.e. a corpus with doc-uments that consist of titles and abstracts can be usedto create a vector space of titles and a vector space ofabstracts. Creation of a doc vector involves the vectorspace of abstracts to determine the weights/elements atpositions corresponding to certain documents. Nonethe-less, the doc vector itself is expressed in a yet anotherspace of dimensions (of documents, rather than words).The main idea behind tESA is to create a similar vectorexpressed in a different vector space, i.e. one with notablyfewer dimensions - a vector space of document titles. ThetESA vector is a doc vector transformed through a mul-tiplication by the column matrix of tf-idf vectors of titles(which means term-document matrix of title-associatedtf-idf weights). The matrix represents the vector spacemodel of the document titles. By tf-idf vectors of titleswe refer to word-based tf-idf representations of individualtitles of documents, while a tESA vector is a distributionalrepresentation of an input text, much like a doc vectorin ESA. C denotes the column matrix of tf-idf vectors oftitles; Cji, which denotes the element of j-th row and i-thcolumn of C (which therefore corresponds to the title ofthe i-th document and j-th term of the title vector space),is given by (see Eq. 2):Cji = tfidf (kj, dtitle(i),D), (5)where dtitle(i) denotes the text of the title of the i-th doc-ument and D denotes the corpus of documents and kjdenotes the j-th term of the title vector space.Given the matrix C defined above, let qT denote a tESAvector of input q, while qD denotes the doc vector of inputq. The tESA vector qT is defined as follows:qT = CqD (6)This means, that using the Eq. (4) the j-th element of qT ,qT j, corresponding to a j-th row of the matrix C (an thusto the j-th term of the title vector space), is given by:tT j =N?i=1cosine(abstracti, qabstract)× Cji, (7)where abstracti denotes a tf-idf vector of the abstract ofthe i-th document and qabstract denotes a tf-idf represen-tation of the input q in the vector space of documentabstracts. A j-th element of the tESA vector is thereforedefined as a weighted sum of tf-idf weights of the j-th term(of the titles vector space) over the corpus of the docu-ment titles. This sum is weighted with the input-abstractcosine similarities from the doc vector.As mentioned, in our implementation for the title vec-tor space, we assume that tf (kj, titlej) = 1 if term kj ispresent in the j-th title, otherwise the value of the tf term is0. Additionally, to reduce the computations, in our imple-mentation we calculate the tESA vector from a doc vectortruncated at M of its most significant elements, as: (a) thetail values have little impact on the final results; (b) mostcommonly the doc vector will have fewer thanMnon-zerovalues anyway (which is discussed in the next section ofthis paper).As displayed in Fig. 1, the processing of our method canbe divided into three main steps:I Finding doc vectors of both inputs, truncated at Mhighest-value elementsII Calculation of the tESA vectors for each of theinputs (see Eq. 5).III Using the tESA vectors to compute therelatedness approximation as the cosine similaritybetween the tESA vectors.Under information retrieval terminology, we use theinput text as a query for the abstract/fulltext based vec-tor space model. Results of this query (scores for each ofthe individual documents, M values at the most) are rep-resented by the doc vectors. In ESA we would use the docvectors as the final representations of the inputs, mean-while in tESA we perform an additional calculation. Inother words, we transform the doc vectors to tESA vectorsusing the title vector space of the corpus and the formulaof Eq. 6. Therefore, the resulting vector will have non-zeroweights at positions corresponding to the vocabulary oftitles of the documents in which the input terms appearwithin the abstracts. Additionally, we promotemeaningfulterms from the titles (through IDF), especially in the con-text of documents, in abstracts of which the input termsplay a prominent role (modeled with the doc vector ele-ments, here used as a prior). We expect this additionalcomputational effort to provide an improvement on twolevels: (a) an improvement in the quality of the results and(b) using smaller representation vectors to model inputs.When it comes to improving the quality of the results,our expectations are based on the fact, that statisticallyit is likely that sets of titles of similar/related documentswill share some part of the vocabulary. Our approachadds another level of intrinsic similarity between doc-ument sets, i.e. the input terms are related not only ifthey appear in the same abstracts, but also if the setsof abstracts they appear in share common features (titlevocabulary). Our expectation of smaller representationscan be derived directly from two assumptions. Firstly, thedimensionality of the vector space of titles is much smallerwhen compared to the dimensionality of the vectors usedRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 6 of 14Fig. 1 Overview. Overview of the methods componentsin ESA (e.g. in the case of Medline the difference isof 300K compared to 14M). Secondly, using very shorttf-idf word vectors to represent titles (the vectors aretruncated to represent only the top-idf  vocabulary), com-bined with the expectation that some title vocabulary willoverlap between documents, should result in representa-tion vectors with fewer non-zero elements than the docvectors. Both hypotheses, (a) and (b) are evaluated in theexperiments.Design differences: tESA vs other distributional approachesOn a conceptual level the processing in our method issimilar to ESA, except that in ESA the relatedness approx-imation is calculated directly as the cosine similarity of thedoc vectors. The direct application of the ESA approachwill also be discussed. As mentioned, the tESA vectorswere designed to take advantage of inter-document sim-ilarity, by expressing the doc vector in the title vectorspace, in which the documents, or more importantlygroups of documents, may share common features. XESAand NESA also benefit from the use of inter-documentsimilarity but in an explicit manner, through the use ofthe document-to-document similarity matrix. The NESAapproach uses an N × N sized dense document similaritymatrix, which requires costly preprocessing and signif-icant resources for runtime processing. The authors ofXESA also contemplate the use of a truncated similaritymatrix.ESA and tESA provide a flexibility and efficiency advan-tage over approaches such as those presented in [16]and [17] and their extensions. Specifically, they use cor-pus statistics instead of relying on contex window wordcounts, which means that the new distributonal represen-tations can be created without having to actually scanthrough all the documents that contain the input terms,so the cost of creating the representation vectors is muchlower.Word embeddings (i.e. word2vec) have the advantageof using dense representation vectors of relatively lowdimension (typically around 200), which makes thosemethods computationally appealing. However, the use ofmachine learning to pre-train the model hinders the flex-ibility of those methods to a certain degree. For example,switching from unigram to bigram inputs would requireeither re-training of the entire model or using some kindof composition strategy involving unigram vectors (addi-tion, multiplication), while ESA and similar methods canbe adapted relatively easily or need no adapting at all,depending on the actual implementation.tESA can also be presented as an extension of themethod presented in [28]. The previous approach uses amuch smaller M to limit the number of relevant docu-ments even further. Furthermore, it does not distinguishthe importances of those documents, i.e. the represen-tation vector was created simply by adding the M mostimportant tf-idf truncated vectors of fulltext documents(not their titles). The extensions that differentiate tESAfrom the original method at the design level can there-fore be summarized as follows: increased size of M, useof a vector transformation (see Eq. (5)) and use of titlevectors instead of fulltext/abstract vectors. These changesmight seem minor, but they actually represent an impor-tant change of focus, from an attempt to capture thesample of most relevant vocabulary to represent an input,to modeling the distribution an input generates over atitle vocabulary of a corpus.ExperimentsThe tESA method was designed to work with the Med-line baseline corpus, which provides us with over 14MRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 7 of 14abstracts with corresponding titles. In addition, the meth-ods were tested with different document collections,which included PMC Open Access (PMC OA) andWikipedia articles. A summary of the corpora used in theexperiments is presented in Table 1.The reference datasets used in the experiments were:mayo101 [36], mayo29c, mayo29ph [16], umnsrsRelate,umnsrsSim [37]. Each of the datasets represents a separateexperiment, in which a group of annotators rated pairsof concepts for semantic relatedness (mayo101, mayo29c,mayo29ph, umnsrsRelate) or similarity (umnsrsSim). Thedatasets contain a list of pairs with a single consen-sus score. The consensus score available in the referencedatasets was achieved by calculating an average score overmultiple annotators. It is important to note that mayo29cand mayo29ph are high-agreement sets, rated by medicalcoders and physicians respectively. The mayo101 datasetconsists of 101 concept pairs rated by a group of profes-sional medical coders from Mayo Clinic. The remainingtwo datasets, i.e. umnsrsRelate and umnsrsSim, containclinical concept pairs rated for similarity/relatedness by agroup of medical residents. The latter two also include astandard deviation calculated for each pair of the labels,which can be used to approximate an inter-annotatoragreement on each of the average scores. We use thisfeature to demonstrate the performance of the methodsunder discussion on high-agreement subsets of these twodatasets. The size and other features of the referencedatasets are summarized in Table 2.In the experimental evaluation of an automated mea-sure, the pairs of labels from the reference dataset aretreated as inputs. Inmost cases each input is a single word,although there are two-word inputs as well. For a list ofpairs of inputs a list of relatedness scores is generated bythe system. This list is then compared to the list of averagescores generated by human annotators. The performanceof the methods in approximating human judgement wasmeasured as the Spearmans rank correlation coefficient,as the problem can be seen as one of ordering the con-cept pairs within each dataset by their relatedness, i.e.both the consensus score and the approximation systemrank the pairs within each reference dataset from themost related to the least related (by assigning scores).The performance has been measured for our imple-mentation of ESA and tESA and is evaluated againstother state-of-the-art methods, which, to the best of ourknowledge, represent the best results reported in theliterature.Additionally, due to the nature of the methods, eachpairing of a dataset and corpus may be associated with acertain recall value, which provides information on howappropriate the corpus is for the benchmark. Recall inour setting is defined as a ratio of the number of inputswith a representation to the total number of distinctitems from a given dataset. It therefore gives the per-centage of inputs that are present in each of the corpora,which means that they can be assigned a distributionalrepresentation.Our experiments involved three methods: ESA, tESA,and the method presented in [28]. Each of the methodswas evaluated with a combination of three different cor-pora. Additionally, we also compared them to the bestresults reported in the literature. NESA and XESA werenot present in the evaluation, largely due to the high com-putational cost involved in creating an N × N similaritymatrix for a corpus as large as Medline. Furthermore, ourearly experiments with a truncated similarity matrix actu-ally caused an important performance drop compared tothe original ESA setup with the same domain-focused cor-pus, which might indicate a high corpus sensitivity of themethod and is is briefly discussed in the following section.As stated, the quality of the methods is measured as arank correlation with the reference scores produced byhuman annotators. In order to compare the performanceof two methods we effectively compare the correlationsthey produce w.r.t. a specific reference sample of limitedsize. To provide a full perspective on our results, we eval-uate the statistical significance of correlation comparisonsusing a methodology presented in [38]. Specifically weconstruct a 0,95 confidence level confidence intervals (CI)Table 1 Presentation of the general characteristics of the corpora used in the experimentsMEDLINE PMC OA WikipediaSize 14073912 1024890 3807314Type Scientific Scientific EncyclopedicDocuments Abstacts and titles Mostly fulltext +abstracts +titles Fulltext +titlesSnapshot date Autumn 2015 September 2015 December 2015Token count [M] 2531,14; 264,84 3684,89; 15,8 2434,55; 11,13Unique token count [M] 3,85; 1,24 35,57; 0,48 12,53; 0,98Token counts and unique token counts are expressed in millions. These statistics are collected for raw texts (before preprocessing) and raw corpora (e.g. there might be anuneven number of titles and abstracts in Medline). For each corpus and count type we provide two metrics - of the documents textual contents (abstract or full articles) andtitles. The statistics are included to highlight the compositional differences between the corporaRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 8 of 14Table 2 Presentation of the general characteristics of the datasets used in the experiments; number of pairs and distinct itemsdescribe the size of the datasets; the focus of the dataset column contains the information on the type of relationship captured in thereference resultsDataset No of pairs Distinct items Reference Focus of the dataset Annotators Scale ICC(2,1)umnsrsSim 566 375 [37] Similarity Residents 0 - 1600 0.47umnsrsRelate 587 397 [37] Relatedness Residents 0 - 1600 0.5mayo101 101 191 [36] Relatedness Medical coders 1 - 10 0.5mayo29c 29 56 [16] Relatedness Medical coders 1 - 10 0.78mayo29ph 29 56 [16] Relatedness Physicians 1 - 10 0.68The ICC (2,1) presents interclass corelation coefficient, which provides an objective measure of inter-annotator agreement; the issues of inter-annotator reliability are coveredin more detail in the corresponding reference papersfor dependent overlapping correlations (as for a pair ofmethods, both of them produce their correlation againstthe same reference dataset). This test allows us to refute,under the assumed confidence level, the null hypothesisof the two correlations being equal. As our main goal is toevaluate tESA, we test the statistical significance of tESAcorrelations vs those of other methods. We used [39] as apractical guide to implement the statistical test.Results and discussionTable 3 shows the scores obtained with ESA, tESA, andthe method presented in [28], with different corpora, foreach of the reference datasets. The table also featuresthe best reported score for each of the datasets. Theresults for tESA and ESA were obtained for M=10000, soeach doc vector has non-zero values at, at most, 10000positions (corresponding to the highest scoring docu-ments). This value of the M parameter has been selectedas a possibly small value for optimal performance of allsetups/methods included in the evaluation - Fig. 2 showshow the results depend on the values of M for ESAand tESA with different corpora on the umnsrsRelatedataset.Figure 3 presents the correlation coefficient obtainedby the methods set up with the Medline corpus in thefunction of inter-annotator agreement for the umnsrsRe-late dataset. For each run the dataset had a standarddeviation threshold decreased in order to exclude thelow agreement portions of the datasets. The data pre-sented in Fig. 3 indicates that both ESA and tESA providemore accurate results for the sets that were more agreedupon by the human annotators. Although this seems intu-itive, the improvement of the ranking in the function ofinter-annotator agreement indicates that the method doesprovide a decent approximation of human judgment par-ticularly w.r.t. the difficulties in reaching a correct scorefor the same pairs of inputs which seemed problematicfor human annotators. In the case of a similar experimentTable 3 Overview of the results for different experimental settings - corpus and benchmark pairs; ESA and tESA runs with M=10000and DS (the method described in [28]) runs with M=200 and cutoff at 0,02 (robust parameters, that can be expected to provide decentresults in different experimental settings)Corpus Method umnsrsRelate umnsrsSim mayo101 mayo29ph mayo29cESA 0.608 0.621 0.546 0.835 0.734Medline tESA 0.649 0.639 0.549 0.783 0.687DS 0.46 0.438 0.511 0.483 0.493ESA 0.588 0.597 0.543 0.855 0.75PMC tESA 0.595 0.607 0.484 0.796 0.7DS 0.574 0.626 0.504 0.738 0.673ESA 0.501 0.5 0.548 0.822 0.722Wiki tESA 0.484 0.484 0.502 0.801 0.755DS 0.444 0.463 0.413 0.627 0.597Best reported (citation) 0.54 [28] 0.58 [28] 0.6 [28] 0.84 [16] 0.9 [34]The table row for best reference results has been compiled with results reported in the domain literature for the respective datasets, regardless of the type of method used toachieve those results. Best reported results for umnsrsRelate, umnsrsSim and mayo101 were attained with specific parameter combinations in our experiments (presented in[28]), whereas for the two smaller datasets the best results were previously obtained with knowledge-rich methods (distributional and IC-based respectively for mayo29phand mayo 29c). Updated best results are highlighted with bold fontRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 9 of 14Fig. 2 Performance changes for different M (cutoff limit for a maximum number of documents considered in the distributional representation). Thefigure shows the correlation with human judgement of ESA and tESA with different corpora in the function of M; the values were obtained forumnsrsRelate datasetperformed on the umnsrsSim dataset, see Fig. 4, the linkbetween the IAA and the quality of the results does notseem to be evident for tESA (which begins to show adecrease in performance at some point), while for ESAthe performance decreases initially and begins to improveat a certain point. Considering that there is little evi-dence (only two experiments) it is difficult to reach adefinite conclusion. There is a possibility. that the resultspresented in Fig. 4 are due to the fact that the umn-srsSim dataset is focused on semantic similarity, which isa narrower concept than semantic relatedness.As shown in Table 4, all corpora provide similar recallvalues, with the highest values for Medline and lowest forWikipedia. In other words, the datasets contain informa-tion on a similar percentage of inputs, so the differences inperformance of the methods set up with different datasetswill be related to the quality/precision of the informationcoverage rather than to its range.Table 5 shows the results of the statistical signifi-cance testing for pairs of experimental runs. We showwhich correlation differences from Table 3 are statis-tically significant w.r.t. a 0.95 confidence interval. TheFig. 3 Performance in the function of increased inter-annotator agreement - umnsrsRelate. The figure shows the correlation with humanjudgement of ESA and tESA in the function of decreasing threshold for standard deviation, which is used to model the inter-annotator agreement,calculated for the umnsrsRelate reference datasetRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 10 of 14Fig. 4 Performance in the function of increased inter-annotator agreement - umnsrsSim. The figure shows the correlation with human judgementof ESA and tESA in the function of decreasing threshold for standard deviation, which is used to model the inter-annotator agreement, calculatedfor the umnsrsSim reference datasettable lists CIs which indicate statistical significance of thecomparisons, i.e. only CIs that do not include zero arepresented.A quick glance at Table 3 reveals that both methods,i.e. tESA and ESA, surpass the existing methods on thetwo larger datasets, with the improvement beingmore evi-dent in the case of tESA and the umnsrsRelate dataset(which is also evident in Table 5). This gain is less evi-dent for the smaller datasets, nonetheless the ESAmethodpaired with the PMC OA corpus provides a result whichis better than the previously known best score. Addition-ally, the mayo29 datasets contain a very small data sampleand mayo101 is only of moderate size, so it seems reason-ably safe to assume that they are somewhat less reliableor at least more prone to incidental variations (which alsoshows in Table 5). Nonetheless, the scores achieved onmayo29 benchmarks seem to be comparable with severalwell established KB-based relatedness measures (refer tothe evaluation presented in [34]).Table 4 Recall for different dataset-corpus pairs. Recall ismeasured as a ratio of unique items (single input labels)represented by non-zero vectors to the total number of uniqueitems in their respective datasets. As mayo29ph and mayo29ccontain the same set of item pairs, the recall is identical for bothdatasetsDataset Medline PMC WikiumnsrsRelate 0.985 0.977 0.95umnsrsSim 0.989 0.981 0.963mayo101 0.957 0.951 0.929mayo29 0.982 0.982 0.982Also, tESA and ESA are only outscored by the previousmethod for a specific combination of runtime param-eters for a specific dataset. They do however seem todisplay more robustness, both in terms of parameterand corpus variations, i.e. they outperform the originalmethod method presented in [28] on sub-optimal (con-sensus) settings used in Table 3. Furthermore, data pre-sented in Fig. 2 suggest that both ESA and tESA performconsistently through a range forM values, so little corpusspecific optimization for M is necessary (for the samplesbetween 10K-40K, at 5K interval, range for neither of themethods exceeded 0,005). Obviously the value of M is stillcorpus dependent to some extent, i.e. it is best to avoidcutting off the significant portions of the vectors. Thedata presented in Fig. 2 suggests that setting the value ofM well above the average vector length works well, whilekeeping the size of long-tailed vectors (which representvery common tokens) under the limit. TheM value of 10Kwas chosen for the main experiments, as it does not seemto hinder the performance of any of the method-corpuscombinations.Table 6 shows the mean number of non-zero vectorelements throughout the reference datasets for ESA andtESA set-up with each of the corpora. Although tESA doesrequire more processing to obtain a vector representa-tion of an input (the method does the same as ESA, andthen more, i.e. the computation of tESA vectors usingthe C matrix), the data shows that one can reasonablyexpect tESA vectors to have fewer non-zero values, whichis especially evident in the case of the optimal Medline-based configuration. Additionally, tESA vectors are alsoless dimensional, as the titles contain fewer unique tokens(see Table 1) than the total number of documents ineach of the corpora considered in our evaluation. TheseRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 11 of 14Table 5 Statistical tests (confidence intervals) for differencesbetween correlations reported in Table 3tESA config Other method Dataset CI ComparisonMedline DS (Medline) mayo29ph (0.09; 0.59) +Medline ESA (Medline) umnsrsRel (0.003; 0.09) +Medline ESA (PMC) umnsrsRel (0.025; 0.099) +Medline ESA (Wiki) umnsrsRel (0.097; 0.2) +Medline DS (Medline) umnsrsRel (0.13; 0.25) +Medline DS (PMC) umnsrsRel (0.026; 0.12) +Medline DS (Wiki) umnsrsRel (0.15; 0.26) +Medline tESA (PMC) umnsrsRel (0.02; 0.09) +Medline tESA (Wiki) umnsrsRel (0.11; 0.22) +Medline ESA (PMC) umnsrsSim (0.004; 0.08) +Medline ESA (Wiki) umnsrsSim (0.09; 0.19) +Medline DS (Medline) umnsrsSim (0.14; 0.26) +Medline DS (Wiki) umnsrsSim (0.11; 0.24) +Medline tESA (Wiki) umnsrsSim (0.1; 0.21) +PMC DS (Medline) mayo29ph (0.1; 0.61) +PMC ESA (Wiki) umnsrsRel (0.04; 0.15) +PMC DS (Medline) umnsrsRel (0.07; 0.2) +PMC DS (Wiki) umnsrsRel (0.096; 0.21) +PMC tESA (Wiki) umnsrsRel (0.06; 0.16) +PMC ESA (Wiki) umnsrsSim (0.056; 0.16) +PMC DS (Medline) umnsrsSim (0.1; 0.24) +PMC DS (Wiki) umnsrsSim (0.09; 0.2) +PMC tESA (Wiki) umnsrsSim (0.07; 0.18) +Wiki DS (Medline) mayo29c (0.04; 0.55) +Wiki DS (Medline) mayo29ph (0.11; 0.62) +Wiki DS (Wiki) mayo29ph (0.01; 0.41) +Wiki ESA (Medline) umnsrsRel (-0.18; -0.07) -Wiki ESA (PMC) umnsrsRel (-0.15; -0.05) -Wiki DS (PMC) umnsrsRel (-0.16; -0.025) -Wiki ESA (Medline) umnsrsSim (-0.19; -0.086) -Wiki ESA (PMC) umnsrsSim (-0.16; -0.06) -Wiki DS (PMC) umnsrsSim (-0.21; -0.07) -The CIs were constructed for pairs of correlations involving at least one tESA setup.The table provides all the information necessary to track the CI back to Table 3, i.e.the corpus of the tESA method, the method (and corpus) to which the tESA resultsare being compared and the reference dataset. We also provide the CI itself,additionally indicating if the result is positive or negativefeatures account for an advantage of tESA over ESA, espe-cially in scenarios where the costly part of the methodcan be delegated to a one time pre-processing effort. Inother words, once the distributional representations havebeen computed, tESA is faster than ESA with two outof three corpora. Most importantly, it is more efficientin handling the representations extracted from Medline,Table 6 Average vector lengthMedline PMC WikitESA 3222,7 3547,4 535,8ESA 4579,4 3391,9 751The table shows an average of non-zero elements in tESA and ESA vectors,calculated throughout reference datasets for each of the corporawhich is the largest of the corpora and also provides thebest-performance setting.From the perspective of the corpus choice, it canbe argued that ESA-related methods rely on domain-adequacy of the entire corpus (thus the slight drop inperformance for Wikipedia), but could also benefit froma larger document collection (increase in performancefor Medline over PMC), all of which is consistent withthe conclusions drawn in [40]. On the other hand, themethod presented in [28] apparently depends more on thequality of individual documents, i.e. PMCs full researchpapers return better results than Wikipedia articles andWikipedia articles still give better results than abstracts inthe Medline collection. This can be explained by the factthat the ESA-related methods, with high enough values ofM, rely on the distribution of words throughout the col-lection. Whereas, the method presented in [28] relies onthe presence of a small sample of documents fromwhich adecent representation of the input can be retrieved. Bear-ing this in mind, one should note that the quality of eachmethod is closely related to a combination of its intendeduse and available document collection.The ESA methodology paired with the Wikipedia cor-pus is essentially an implementation of the original ESAwith a cutoff, so it provides an important baseline forother methods to be compared against. This baselinescore is surpassed by ESA combined with domain spe-cific corpora (Medline/PMC) on all benchmarks with theexception of mayo101, where the difference is statisticallyinsignificant. tESA provides significantly better resultsthan the original ESA baseline for the two larger datasets.It also provides a better result for themayo101 dataset, butthe gain is statistically insignificant.When comparing the performances of ESA and tESA,tESA seems to provide better results (at least for the mostrelevant benchmarks) when the methods use domain-oriented collections. One possible explanation is that thetitles of scientific articles are simply more descriptivethan those of Wikipedia. At the same time, the Wikipediatitles are usually short and contain discriminative tokens(almost like identifiers), and those tokens are sometimesaccompanied by a broad categorical description (e.g.Medicine) intended for human disambiguation, which inthe presented settings may increase noise. We believe thatfine tuning the extraction method for title representa-tion could improve tESA even to the point of achievingRybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 12 of 14results more comparable with ESA with both methodsusing Wikipedia as the document corpus. Nonethelessusing a document collection with more descriptive titlesseems to be a safer choice when it comes to improvingperformance.The results obtained both with tESA and ESA (espe-cially with the Medline corpus) seem ecouraging giventhe results presented recently in [31]. Both tESA and ESAseem to achieve better results when evaluated against thetwo largest benchmarks than all the methods discussed inthe study, while performing at least comparably to the bestones on the smaller reference datasets, although a deeperstatistical analysis would be needed to provide more per-spective. It is worth noting however, that both tESA andESA operate on much larger structures (vectors) thansome of the methods presented in the cited evaluation(e.g. word2vec-trained word embedding), which meansthat ESA-based approaches might be less appropriate forlarge scale tasks.The approach used in tESA is similar to that used inthe NESA methodology in the sense that it is aimed atleveraging the inter-document similarity. In NESA thisis achieved by the explicit usage of a similarity matrixfor all the documents, while in tESA it is done throughthe creation of the representation vectors as described inthe Methods section. In other words, NESA and XESAcontemplate leveraging the actual document-documentsimilarity, while in tESAwe assume that sets of documentsmight share common vocabulary features. The advantageof tESA is that it can be directly applied to larger corpora,as it needs a representation vector per word or document(depending on the actual implementation) and the tar-get vector space is relatively small, while NESA requiresstoring a dense similarity matrix of an N × N size. In[22], the use of a truncated matrix is contemplated, how-ever our initial experiments with the truncated cosinesimilarity matrix have shown decreased performance andincreased processing and preprocessing times when com-pared to tESA and ESA, which might point to an issuewith the adaptability of the approach to domain-specificcorpora and the specificity of the concepts within the eval-uation datasets (especially when we compare it with thelength and coverage of biomedical journal papers). As thetask of adapting the similarity based ESA extensions is anindependent research problem (which might be or not befeasible), it has been left to be considered in our futurework, as outlined below.Obviously, the tESA model is limited in terms ofrepresenting the inter-document similarity (as it doesnot reflect the similarity of actual document-documentpairs), it does however seem to benefit from the intrin-sic characteristics of the titles of the scientific papers.Nonetheless, our impression is that relatedness methodscould be further enhanced by experimenting with themapping and the target representation space. The goalof further work should therefore be to provide a bet-ter similarity modelling within the target representationspace. We believe that this could be achieved by: (A) anintelligent approach towards extracting more informativerepresentations from full texts/abstracts, (B) using NESA-like distribution based representations obtained for titles.With respect to (A) it has to be noted that prelimi-nary experiments with the parameters of the approachpresented in [28] (increasing the query size, decreasingthe cutoff threshold) did not provide satisfactory results,probably due to the amount of noise introduced in therepresentations, therefore research thread (A) will centeron finding a representation extraction method that maxi-mizes information content, while reducing noise. The lineof research related to (B) will focus on providing represen-tations that do not lead to dimensionality problems andcan be adapted to the biomedical domain, and comparingtheir performance with the NESA-like approaches.ConclusionsIn this paper we have presented a new, robust methodfor computing lexical semantic relatedness for biomedi-cal use - tESA. The approach uses a vector space of titlesof scientific articles combined with ESA principles. Wehave also provided a side-by-side comparison of tESA andESA, the latter method having not been evaluated as thor-oughly in similar experimental settings. Both methodswere reviewed with direct benchmarks, i.e. their abil-ity to approximate human judgement was assessed. Thealgorithms outperfomed other state-of-the-art methodsin the largest-to-date datasets used to evaluate biomedicalsemantic relatedness and similarity, with the original tESAmethod gaining a slight advantage.Also, we have demonstrated that tESA uses smallerand more dense vectors than ESA, so it might be a bet-ter fit in cases where vector computation cost (which ishigher in tESA) is less important than the cost of onlinecomputations.The results obtained with both tESA and ESA seem tobe on par with the other state-of-the-art methods, a recentstudy [31] being a good point of reference.The results obtained in our evaluation seem to indicatethat the performance of the method can be optimizedby choosing a correct background corpus, i.e. a domainoriented corpus of documents will provide a qualityimprovement in assessing domain-oriented relatedness.The baseline score of the original ESA has been sur-passed by bothmethods on the two largest (and thus morestatistically significant) reference datasets.We believe that the approach and detailed evaluationthat we have presentedmay be a good fit wherever seman-tic relatedness approximation is a necessity, especiallywithin subdomains that lack a detailed KB domain model,Rybinski and Aldana-Montes Journal of Biomedical Semantics  (2016) 7:67 Page 13 of 14but are well covered in the scientific literature. Guidelinesto tuning and applicability of the discussed methods havealso been presented here. Finally, two interesting lines forfuture research have been outlined, both of which we hopeto pursue in the near future.Endnote1 The method actually uses either abstracts or full arti-cles, depending on the features of the actual corpus, asexplained further on.AbbreviationsESA: Explicit semantic analysis; KB: Knowledge base; NESA: Non-orthogonalexplicit semantic analysis; PMC: PubMed Central, also refers to PubMed CentralOpen Access document corpus; tESA: Title vector explicit semantic analysis; Tf- idf: Term frequency inverse document frequencyAcknowledgementsNot applicable.FundingWork presented in this paper was partially supported by grantsTIN2014-58304-R (Ministerio de Ciencia e Innovación), P11-TIC-7529 andP12-TIC-1519 (Plan Andaluz de Investigación, Desarrollo e Innovación) and EUFP7-KBBE-289126 (the EU 7th Framework Programme, BIOLEDGE).Publication costs for this article were funded by grants TIN2014-58304-R(Ministerio de Ciencia e Innovación) and P11-TIC-7529 and P12-TIC-1519 (PlanAndaluz de Investigación, Desarrollo e Innovación).Availability of data andmaterialsThe reference datasets used in this study are available at: http://rxinformatics.umn.edu/SemanticRelatednessResources.html. The Medline corpus isavailable (on request) at: http://www.nlm.nih.gov/bsd/pmresources.html. TheWikipedia data is available at: https://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki. The PMC OA corpus is available at: http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/. The snapshots of the datasets used in this study, as wellas the data supporting our findings are available from the correspondingauthor on reasonable request.Authors contributionsBoth authors contributed to the design of the method and experiments. MRwas responsible for the implementation, performing the experiments andwriting of the manuscript. Both authors have read and approved the finalmanuscript.Competing interestsThe authors declare that they have no competing interests.Consent for publicationNot applicable.Ethics approval and consent to participateNot applicable.Received: 26 February 2016 Accepted: 13 November 2016RESEARCH Open AccessRepresenting vision and blindnessPatrick L. Ray1, Alexander P. Cox1, Mark Jensen1, Travis Allen1, William Duncan2 and Alexander D. Diehl2,3*AbstractBackground: There have been relatively few attempts to represent vision or blindness ontologically. This isunsurprising as the related phenomena of sight and blindness are difficult to represent ontologically for a variety ofreasons. Blindness has escaped ontological capture at least in part because: blindness or the employment of theterm blindness seems to vary from context to context, blindness can present in a myriad of types and degrees,and there is no precedent for representing complex phenomena such as blindness.Methods: We explore current attempts to represent vision or blindness, and show how these attempts fail atrepresenting subtypes of blindness (viz., color blindness, flash blindness, and inattentional blindness). We examinethe results found through a review of current attempts and identify where they have failed.Results: By analyzing our test cases of different types of blindness along with the strengths and weaknesses ofprevious attempts, we have identified the general features of blindness and vision. We propose an ontologicalsolution to represent vision and blindness, which capitalizes on resources afforded to one who utilizes the BasicFormal Ontology as an upper-level ontology.Conclusions: The solution we propose here involves specifying the trigger conditions of a disposition as well asthe processes that realize that disposition. Once these are specified we can characterize vision as a function that isrealized by certain (in this case) biological processes under a range of triggering conditions. When the range ofconditions under which the processes can be realized are reduced beyond a certain threshold, we are able to saythat blindness is present. We characterize vision as a function that is realized as a seeing process and blindness as areduction in the conditions under which the sight function is realized. This solution is desirable because it leveragescurrent features of a major upper-level ontology, accurately captures the phenomenon of blindness, and can beimplemented in many domain-specific ontologies.BackgroundThe human visual system is a complex, organized collec-tion of specialized cells and structures that functiontogether to encode and represent one type of stimulus(light). Light enters the system through the eye, a com-plex organ that includes structural, optical, and muscularcomponents, which operate together to ensure an intactfocused image reaches the retina. In the retina, two typesof photoreceptor cells encode visual information in theform of light by converting it to a neuronal signal. Anetwork of synapses connect the photoreceptor cells viabipolar neurons, ganglion cells, horizontal cells, andamacrine cells to the optic nerve, which connects to thebrain proper. Many of the fibers of the optic tractinnervate at the lateral geniculate nucleus in the poster-ior thalamus while others project to the superior collicu-lus. The lateral geniculate nucleus sends projection cellaxons to the visual cortex. The superior colliculus, inaddition to signals from the retina, receives indirectsignals from the visual cortex. The visual cortex iscomposed of layers that are responsible for differenttasks. Most of the input from the lateral geniculatenucleus occurs in layer IV of the primary visual cortex.Cells in layer IV are connected in patterns that workto integrate visual data from the lateral geniculatenucleus into a cognitive representation of objects inthe visual field [1].Because the human visual system involves manyanatomical structures and complex functions, it can failin a multitude of ways. Such failures can occur in any ofthe various structures that make up the system, and* Correspondence: addiehl@buffalo.edu2New York State Center of Excellence in Bioinformatics and Life Sciences,University at Buffalo, Buffalo, NY, USA3Department of Neurology, Jacobs School of Medicine and BiomedicalSciences, University at Buffalo, Buffalo, NY, USAFull list of author information is available at the end of the article© 2016 Ray et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Ray et al. Journal of Biomedical Semantics  (2016) 7:15 DOI 10.1186/s13326-016-0058-0often result in visual impairment or blindness. Blindnesshas been studied from multiple perspectives, whether asthe result of accident or disease. Most of the associatedanatomical entities are well described, and many of theaffected processes involved in visual perception are wellunderstood; yet the representation of the fundamentalnature of blindness from an ontological perspectiveremains incomplete.In its most basic form, blindness is the impairment ofvisual function below a certain threshold. Where thisthreshold is set varies from context to context, andstandards for blindness vary across international andinstitutional borders. The World Health Organizationcharacterizes blindness as visual acuity of less than 20/500 or a visual field of less than 10 degrees. In theUnited Kingdom, the Certificate of Visual Impairmentcharacterizes blindness as visual acuity of less than 20/400. In the United States, the American Medical Associ-ation characterizes blindness as visual acuity of less than20/200 or a visual field of less than 20 degrees. Therehave also been recent calls by the International Councilof Ophthalmology to define blindness and visual impair-ment according to their own standards, at least part ofwhich involve visual substitution skills employed bypersons [2]. This expansion to include visual substitutionskills in the account of blindness is important because itdemonstrates that visual acuity only represents onedimension of blindness. Other types of visual impairmentsfall outside the scope of visual acuitysuch as the abilityor inability to differentiate colors.The relationships between blindness, visual impair-ment, and visual acuity can be difficult to describe with-out resort to fiat definitions, such as those describedabove, which offer little insight into the underlyingnature of the phenomena. We start with the premisethat blindness is a specific type of visual impairment andthat blindness results from a failure in the visual process.Visual acuity is the specific ability an individual has withrespect to vision (how well or poorly an individual isable to visually process stimulior perhaps the sharp-ness of the representations of external stimuli by anindividual). Many of the problems associated with repre-senting blindness result from a conflation of closelyassociated concepts about the vision process and itsabsence. By identifying the entities in reality, the compo-nents of the visual system and their associated functionsand processes, we may therefore sort out what blindnessis, and what blindness results from.Current efforts to define terms related to blindness,while systematic, are rather problematic. The NationalHealth Interview Service (NHIS) offers a definition of'an individual who suffers from vision loss' as those whoreport they have trouble seeing [3]. The problem withsuch a definition is that self-assessment is a reliably poorcriterion for sensory perception. People are oftenmistaken about their abilities and skills regarding theirperception. An older NHIS survey, the National HealthInterview Survey on Disability (NHIS-D) implemented asharp cut-off for those who are legally blind, a visualacuity of 20/200 or less. The American CommunitySurvey (ACS) provides a definition of those with'difficulty seeing' as individuals who self-report eitherblindness or serious difficulty seeing even when wearingeyeglasses [3]. Along with the aforementioned problemwith self-assessment we now have an additional problemof qualification with corrective lenses. (Notice that thisdefinition is too vague in that even a person with ex-ceptional vision could have serious difficulty seeingwhen wearing glasses with a strong prescription; wesurely would not want to label such a person ashaving difficulty seeing).Governmental organizations implement their ownstandards for blindness. The Department of Labor'sBureau of Labor Statistics employs a Current PopulationSurvey as a means for identifying those who have seriousvision loss or blindness. The standard, once again, is oneof self-assessment. People with vision loss were identi-fied by the CPS if they reported that they or someone intheir household is blind or has serious difficulty seeingwhen wearing eyeglasses [3]. One of the problems withsuch wide-ranging definitions or criteria of blindness isthat there seems to be no one standard. As a result it isvery difficult to determine precisely how many peoplesuffer from blindness or vision loss.From a diagnostic perspective, the National Eye Insti-tute (NEI) defines blindness as the best-correctedvisual acuity of 6/60 or worse (=20/200) in the betterseeing eye [4]. The American Optometric Associationfollows the protocol of the World Health Organizationin defining total blindness as no light perception andnear total blindness as a best-corrected visual acuityof less than 20/1000 [5]. The standard United Stateslegal definition of blindness follows the criterionendorsed by the NEI, as a best-corrected visual acuityof 20/200 or worse.The problems that arise for a definition of blindnessdate to at least the beginning of the 20th century. N.Bishop Harman writes of a proposed definition of blind-ness, By no possible phrasing can we present a suffi-ciently simple definition that will embrace the possiblevariations in these several factors that make up sight,and state that such and such a variation shall be accountedblindness [6]. Other, more recent assessments of the feasi-bility of defining blindness have been met with similarreluctance but somewhat more optimism. The fact is,however, that the briefest analysis of any dictionarydefinition will reveal the word blind to be neitherstraightforward nor respectable [7].Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 2 of 12The aforementioned problems notwithstanding, thereare still difficulties surrounding blindness as a subject ofinvestigation. If we understand blindness as the impair-ment of visual functioning beyond a certain thresholdand accept that there are many different ways in whichvisual functioning can be impaired, then there are manydifferent ways in which one may be blind. Furthermore,there are many different mechanisms that will causeblindness. Moreover, if we focus on the different types ofvisual impairment with respect to the features of theworld that are not effectively represented in the visualprocess, blindness can be used to describe these differ-ent types of failures of representation along the lines offeatures of the world (e.g. color blindness). This compli-cates our task because it shows that not only are theremany different ways one can become blind, but differentways in which one may be blind. The nature of blindnessis more complicated than it appears at first glance. Notonly are there complications with respect to contextual-ized thresholds for determining blindness, but also diffi-culty in examining the natures of the differing types ofvisual impairment that lead to blindness.There are two obvious and immediate challenges forrepresenting and defining visual impairment and blind-ness. First, different groups use different standards ofmeasurement. Second, different standards of classifica-tion can be used in conjunction with a single standardof measurement. These challenges make it difficult toshare, reuse, and compare data on blindness and visionrelated disorders. In addition, there are more complexproblems that arise in representing blindness in formalontology. This paper explores the difficulties that arisein representing blindness ontologically and proposes anovel solution to representing visual impairment andblindness in a formal manner. While we believe thatthe definition of blindness (and its related terms) willnot come easily, our goal in this paper is to provide anontological representation of blindness through acareful examination of the nature of blindness andrelated phenomena.Current ontological representations of vision andblindnessThe Gene Ontology defines visual perception as Theseries of events required for an organism to receive avisual stimulus, convert it to a molecular signal, andrecognize and characterize the signal. Visual stimuli aredetected in the form of photons and are processed toform an image [8]. Thus, visual perception, or seeing, isa relational process between an agent and the stimulusitself. The process of seeing is representational insofar asthe agent represents the stimulus in some manner (we leavethe source of this stimulus and the nature of this represen-tation to further examination). We may characterize thediminishment or cessation of this relational process asblindness or a loss of vision.Currently there are relatively few ontologies thatcontain the term blindness and fewer still that offer awell-formed definition of blindness. There are norealism-based ontologies that represent the phenomenasurrounding blindness in a manner that reflects itscomplexity. Results of previous attempts to characterizeblindness using current ontologies are listed in Table 1.Of the attempts to represent blindness in biomedicalontologies, it is a popular strategy to classify blindnessas a phenotype [9].Biomedical ontologies that seek to represent pheno-types typically rely on Entity-Quality (EQ) methodology[10]. The EQ methodology leverages the existing struc-ture of ontologies to generate a schema where the sub-ject of the phenotype (the entity) is described by thephenotype that inheres in that entity (the quality). Thisapproach is advantageous in that it provides a computa-tional resource for researchers working with phenotypicqualities. This allows researchers to leverage reasonerswhere they were not leveraged before. In addition, theEQ method allows that [phenotypes be] recordedusing multiple ontologies in a highly expressive andfinely detailed manner while maintaining correct logicand computability [10]. This is a very powerful andinnovative method used within the biomedical ontol-ogy community.The EQ methodology works well for phenotypes andqualities that inhere in entities. For example, if wewanted to say, some human being has red eyes, we couldaccomplish this via EQ methodology by leveraging termsfrom the Uber Anatomy Ontology (UBERON) and thePhenotype and Trait Ontology (PATO). We would use theterms red from PATO and eye from UBERON and applythe EQ methodology to yield EQ =UBERON:eye +PATO:red [10].While this solution has its advantages, its shortcomingsoutweigh its advantages for the subject of blindness. Themain problem with applying the EQ method to blindnessis that it is unclear whether blindness is a phenotype. TheOntology for General Medical Science (OGMS) providesthe following definition for phenotype:A (combination of) quality(ies) of an organismdetermined by the interaction of its genetic make-upand environment that differentiates specific instances ofa species from other instances of the same species [11].If blindness is a phenotype and phenotypes are qualities(or combinations thereof), then blindness is a quality (orcombination thereof). If blindness is a quality (or combin-ation thereof), then blindness is a specifically dependentcontinuant that needs no further process in order to beRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 3 of 12realized. But this consequence is incorrectblindness isthe inability of the vision function to be realized. Blindnessis not a quality of a visual system or of an organism thatthe visual system is part of. Blindness is a failure in therealization of the visual function.Another option to consider is an alternative definitionof phenotype. One standard definition for phenotypeis that a phenotype is the outcome of a given genotypein a particular environment [12]. If it is the case thatblindness is a phenotype and that a phenotype is theoutcome of a given genotype in a particular environ-ment, then blindness is the outcome of a given genotypein a particular environment. Of course this poses noproblems when we are discussing instances of blindnesswith a genetic basis. However, some instances ofblindness do not have a genetic basis, but rather arethe result of acute trauma or some other non-geneticphenomenon. Since there are cases of blindness thathave non-genetic bases, it is clear that blindness is nota phenotype in all cases according to this definition ofphenotype. Since blindness is not a phenotype in allcases (if any), we should look for an alternativeaccount of blindness.One of the general problems with the treatment ofblindness as a phenotype (quality) is that it is inconsist-ent with the proposed definitions provided in theseaccounts. If blindness is a lack of sight (or vision) andsight (or vision) is a realizable entity, then blindnessshould be the lack of a process or the lack of somerealizable entity. If blindness is a phenotype and there-fore a type of quality, then there is a quality that is a lackof a realizable entity. But this is impossible. Therecannot be an entity that is both a lack of a realizableentity and a quality at the same time. The lesson here isnot that blindness does not exist but rather thatblindness is something other than a phenotype.If blindness is not a phenotype, what other ontologicalsolutions are there? One of the most likely candidatesolutions involves using the Human Disease Ontology(DO). The Human Disease Ontology currently does notprovide a definition of 'blindness', but we could proposea plausible candidate on their behalf following theircharacterization of color blindness as an inability ordecreased ability to detect light stimulus. 'Color blind-ness' is defined in DO as: a blindness that is character-ized by the inability or decreased ability to see color, orperceive color differences, under normal lighting condi-tions [13]. (Table 1) Moving from this definition of aspecific type of blindness to a general definition of blind-ness should produce the result that blindness is theinability or decreased ability to see or perceive, undernormal lighting conditions.While this is a generally attractive view, it does notstand up to careful examination. In the first place, DOTable 1 Previous attempts to define and represent blindnessOntology Term Definition Parent ClassGene Ontology (GO) Visual perception The series of events required for an organism to receivea visual stimulus, convert it to a molecular signal, andrecognize and characterize the signal. Visual stimuli aredetected in the form of photons and are processedto form an image.Sensory perceptionof light stimulusGO Detection of visible light The series of events in which a visible light stimulus isreceived by a cell and converted into a molecular signal.A visible light stimulus is electromagnetic radiation thatcan be perceived visually by an organism; for organismslacking a visual system, this can be defined as light witha wavelength within the range 380 to 780 nm.Detection oflight stimulusGO Detection of light stimulusinvolved in visual perceptionThe series of events involved in visual perception inwhich a light stimulus is received and converted intoa molecular signal.Visual perceptionGO Determination ofsensory modalityThe determination of the type or quality of a sensation.Sensory modalities include touch, thermal sensation,visual sensation, auditory sensation and pain.Sensory processingMammalian Phenotype (MP) Blindness Loss of the sense of sight. Abnormal visionMP Abnormal vision Inability or decreased ability to see. Abnormal eye physiologyMP Decreased visual acuity Loss of visual acuity or ability to distinguish small details Abnormal visual acuityHuman Disease Ontology (DOID) Blindness N/A Retinal diseaseDOID Color blindness A blindness that is characterized by the inabilityor decreased ability to see color, or perceive colordifferences, under normal lighting conditions.BlindnessHuman Phenotype (HP) Blindness Blindness is the condition of lacking visual perceptiondue to physiological or neurological factors.Visual ImpairmentRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 4 of 12categorizes blindness as a disease. Blindness is not a dis-ease though it may result from a particular disease.Moreover, it is not a type of retinal disease as DOcurrently represents it. There are many diseases thatmay result in blindness and many diseases can compli-cate the sightedness of individuals, but blindness is notitself a disease. Furthermore, blindness need not resultfrom a disease. It may instead be caused by a singleevent, as is the case with acute trauma or flash blind-ness. Blindness also need not be limited to problems inthe retina. Cortical blindness is a type of blindness thatdoes not involve any malfunction with the retina. Asdetailed in the last section, certain types of blindness arenot limited to just one mechanism of realization, or torealization in only one location.These problems notwithstanding, the more pressingconcern with this solution is that there does not seem tobe any indication of what an inability or decreased abilitywould be. If abilities are dispositions or functions, thenthey are realizable entities. The concern is plainaccord-ing to BFO a realizable entity cannot lack. Realizableentities cannot present in degrees, as their existence is anall-or-nothing affair. If blindness is an inability to detectlight, then all cases of blindness will be a complete inabil-ity to detect light stimulus, which fails to capture the casesof blindness that are not the complete inability to detectlight stimulus. If blindness is a decreased ability to detectlight, then it cannot be represented as a decreased func-tion or disposition in BFO. But, if sight is a function andblindness is the lack of sight, then an account of blindnessas an inability cannot be given either. Hence, we believethat this type of account is confused.Moving away from an account based on the DO repre-sentation, another route for capturing blindness is to main-tain that blindness is a disorder, where 'disorder' is definedby OGMS as [a] material entity that is clinically abnormaland part of an extended organism [11]. The problem withthis approach is that it is unclear that blindness is a materialentity. If one thinks that blindness is the absence of thesight function, then it does not seem that blindness is a ma-terial entity (absences of functions are not material entities).Further, one cannot point to a material entity and identify itas blindness because blindness is not spatially extended andspatial extension is a hallmark of material entities. For thesereasons, blindness cannot be a disorder per OGMS.The realist approach to blindnessThere are several lessons to be learned from this concisereview of the representations of blindness in biomedicalontologies. First, it is rather difficult to characterize anentity via a lack or absence, which seems to be the casewith blindness (the lack of sight) [14]. Consider theparadigm case of an ontological absence involvingmaterial entities: a hole. There does not seem to beanything to which one can attribute characteristics. Inthe Basic Formal Ontology (BFO), holes are continuantentities susceptible to characteristic ascription like ma-terial entities, but are essentially non-material. This indi-cates at least a prima facie problem with characterizingentities via a lack. BFO is an upper-level ontology that isrealist, fallibilist, perspectivalist, and adequatist [15]. Thecore of BFO lies in its distinction betweencontinuants and occurrents, which tracks the differ-ence between the two different types of fundamentalentities in the world. BFO is currently used as theupper-level ontology for many biomedical ontologiesand is the backbone of the Open Biological andBiomedical Ontologies collaborative [16].A second problem arises when we consider attemptsto capture the existence conditions of a hole. If a hole isdefined by a lack (where the lack is both necessary andsufficient), then there are all sorts of things that wouldqualify as a hole. For example, if one were to characterizea hole as a lack of matter surrounded by matter, then theinterior of a room would count as a hole, as would theinside of a bag. A strategy has been implemented inanatomy for representing lacking a part; however, it iscontentious whether it will translate well for things thatare not material entities, such as processes or functions 1[17]. The reason that such a strategy will not work wellfor non-material entities is that the lacks_part relationdoes not apply to functions because functions do not haveparts. 2 Processes, on the other hand, do have parts butthe parts are temporal parts, not material entities (pro-cesses can include material entities as participants, butthese are not themselves parts of processes).Blindness does not seem to yield a precise definitionor even clearly differentiated conditions under which itis present or absent, apart from the fiat standardsdiscussed previously. Blindness often presents graduallywhich is commonly due to degeneration of the eye orapparatuses associated with vision. As a result, manycases of blindness are progressive and it is exceedinglydifficult to determine at which point blindness has comeinto existence. In addition to these complications, thereis controversy over the threshold for blindness. Hence, itis common for publications regarding blindness tospecify which definition of blindness they employ [18].Even given these complications regarding blindness,we contend that it is useful to give a univocal account ofthe phenomenon for purposes of ontological deve-lopment. Such an account should capture all or a vastmajority of the cases of blindness and the variousclassifications of blindness found in the literature.Thus, the account should remain general and flexibleenough to capture a wide range of characterizations yetit should also be rigid enough to remain informativeand insightful.Ray et al. Journal of Biomedical Semantics  (2016) 7:15 Page 5 of 12There are many types of blindness. For example, thereis color blindness and change blindness. Blindness canalso be defined relative to a context. An individual mightbe legally blind but still be able to detect some lightstimulus. Similarly, one might be visually impaired tothe extent that they are prohibited from flying a jetaircraft but not from driving. Hence, we might say thatsomeone is blind according to [x] where [x] is somestandard of evaluation for sightedness. In this sense,blindness comes in degrees. The extent to which some-one has a lack of vision will be graded. If we think ofseeing as a relational process between an agent who isrepresenting and the thing represented and assess theaccuracy of such representations on a scale of 1 to 0(complete representational veracity to no representa-tional veracity), the ability of the person to see will besomewhere on the continuum from 0 to 1the closer to0 ones representation of the stimulus is, the more blindthat individual is.Lending to the confusion surrounding the status ofblindness (and vision) is the method used for assessingvisual acuity. Typically, visual acuity is expressed as a re-lationship between two valuesthe distance a subjectstands from an optical chart and the distance at which anormal subject would stand from the chart to discernthe same visual detail. Putting aside the problemsassociated with this particular type of visual acuityassessment, we have discussed above how this canlead to confusion regarding what conditions are indi-cative of blindness [18].Given the above considerations, one might concludethat there is not a single coherent ontological categorythat corresponds to what blindness is as an entity.Instead, blindness could be an amalgam of loosely relatedentities or something that is not ontologically well-formed. We find this conclusion unsatisfactory. It is usefulfor clinicians and researchers to have a coherent theory ofblindness that encompasses the range of conditionscommonly understood to be forms of blindness. Wesimultaneously realize that blindness seems to be charac-terized as relative or context-sensitive (the term itselfmight be context-sensitive or the phenomenon might becontext-sensitive or both). We favor the view that theterm blindness denotes a single phenomenon reflectingsevere visual impairment relative to a particular context ofevaluation. Thus, blindness denotes an ontologically well-formed category.According to BFO, a function is a type of disposition.A disposition is a realizable entity whose realizationoccurs in virtue of the bearers physical constitution andbecause the bearer is in some special physical circum-stances. A function is a disposition that exists as theproduct of natural selection or intentional design on thepart of the agent. Because functions are dispositions theyare realizable entities. They are realized as processes thatare sometimes called functionings. Functions are regardedas non-accidental in BFO meaning that they come intobeing because of natural selection or intentional design[19]. All of the functions a given entity possesses areintimately tied to the type of entity under examin-ation, whether the entity is biological or artifactual.Functions are internally-grounded realizable entities.An internally-grounded realizable entity is a realizableentity that is a reflection of the (in-built or acquired) phys-ical make-up of the material entity in which it inheres[19]. Changing the physical structure of an entity thatbears a given function might alter the realization of thefunction in question.A reason to believe that sight is a disposition is that itis realized by processes grounded in a material entity.This is a hallmark of a disposition (of which functionsare a special type) as described above. We have anotherreason to think that sight is a disposition: the fact thatif sight ceases to exist, then the bearer is physicallychanged. Although the entities still have the dispos-ition to see, they are blind because they can no longerrealize that disposition due to some change in theirphysical constitution.Furthermore, we believe that sight is a function of avisual system (or at least of visual systems of entitieswith higher-order cognitive functions). One reason isthat development of the ability to see is the result of anevolutionary process. The physical makeup of particularorganisms have changed and adapted over time to selectfor the existence of the sight function. For non-biologicalentities possessing sight, if any, the sight that they possessis not accidental, but rather a product of intentionaldesign on the part of the creator. For these reasons,we contend that sight is a function.It is important to note here that it is not the case thatfor all instances of structural change in the bearer of afunction that the function ceases to be realizable. Inother words, it is not the case that every physical changein a bearer alters the function in question. It is also thecase that some bearers of functions can undergo changesthat render them unable to realize a function while thefunction persists in its bearer. For example, one of thefunctions of the human heart is to pump blood. Physicalchanges to the heart may cause the heart to be unable toperform its functiona large tear in a ventricle wallmight be such a physical change. Other physical changesmight not cause the loss of the ability to perform a func-tiona slightly enlarged section of muscle might besuch a physical change. But in all cases where a functionceases to exist or comes into being there must be acorresponding physical change. To use an artifactualexample, a radiator has the function to disperse heat.Certain physical changes to a radiator will render theRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 6 of 12radiator unable to realize its function, such as thepresence of corrosion on the cooling fins or a crackedregulator valve. But in this case we would not say thatthe radiator has lost its function, rather we would saythat its function cannot be realized. If there were a suffi-ciently significant physical modification of the right kindto an entity, only then would we contend that thefunction ceases to exist. This is an important pointthat will play a significant role later in our discussionof blindness.We can characterize the specific type of function (sight)by identifying and describing its defining features.Employing such a strategy, we provide an account of sightas the function to receive photons and interpret them asvisual information. Similarly, we can characterize seeing asthe process by which photons contacting the retina arecoded into an action potential and interpreted as visualinformation. Having given an account of vision as therealization of the sight function, it is then natural to iden-tify the processes by which the sight function is realized asvision. There are many other functions involved in therealization of the sight function as described above. Weare omitting detailed discussion of these functions for easeof exposition.If we are correct in our claim that sight is a function,and that blindness is related to the realization of thisfunction, then it seems that we can develop our under-standing of the phenomena of blindness through thecharacterization of sight as a function. Because blind-ness is a wide-ranging phenomenon (i.e., there aremany types of blindness), it would behoove us toexplore the range of blindness types in an attempt tofully capture blindness.Color blindnessColor perception is possible for humans because we(typically) have three different types of cone cell photo-receptors that detect photons within correspondingranges of wavelength. Photons within the range of aparticular type of cone cell photoreceptor cause anaction potential in the corresponding cone cell and theaction potential is sent along eventually to the primaryvisual cortex. Color is coded by the differentiation of thefiring of these types of cone photoreceptor cells.Color blindness is a condition wherein an individual hasan inability to distinguish between two or more colors. Insome cases photons of differing color spectrum wave-length are represented or interpreted as the same whenthey are distinct. In other cases, an individual cannotreport a difference between two or more wavelengths ofphotons [20]. The inability to distinguish between two ormore types of light is not limited to just one cone type[21]. Complicating this picture somewhat is that there aremany mechanisms identified as causes of color blindnessand that these mechanisms are not localized to oneanatomical region. Some color blindness is due to an indi-vidual lacking cone cells or a certain type of cone cell.Other times the cause is cortical [22]. For example, thecomplete lack of the ability to distinguish colors, achroma-topsia, can have its underlying causal mechanismlocated within the retina (congenital achromatopsia)or the visual cortex (cerebral achromatopsia). Thuscolor blindness is similar to other types of blindnessin that its causes and the mechanisms associated withit are diverse and complex.Flash blindnessFlash blindness is a type of blindness that results fromsudden exposure to bright lightsuch as the bright flashthat results from the flash bulbs on older cameras or thedetonation of an atomic weapon. The sudden influx oflight oversaturates the photopigments of the retina andthe individual becomes unable to convert photons to aneural signal [23]. Flash blindness is commonly tempor-ary, where the subject regains their full ability to seewithin a few minutes. There are some extreme cases,however, where flash blindness results in permanentvision loss [24].Inattentional blindnessInattentional blindness can be described by an individ-uals lack of perception of a salient object or feature intheir visual field due to lack of attention [25]. Investiga-tion into the mechanism for this phenomenon hasrevealed both a nonconscious and conscious component.The nonconscious mechanism includes contour inte-gration in the early visual cortex while consciousintegration includes involvement from the lateraloccipital cortex and is mediated more strongly byfocused attention [26].Proposed solutionDrawing on the lessons from the previous sections, wepropose a solution to the problem that blindness posesfor ontology development. Because sight is a functionand blindness is seemingly the non-realization of thatfunction, we set forth an account of blindness whereblindness is a reduction of the trigger conditions underwhich the sight function is realized. To understand this,we need to understand how dispositions are related totriggering conditions. A disposition is a causal propertythat is linked to a realization, i.e., to a specific behaviorwhich the individual that bears the disposition will showunder certain circumstances or as a response to a certainstimulus (trigger) [27].This solution is able to deal with the cases outlinedabove. Color blindness is a reduction in the (color)trigger conditions under which a vision function isRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 7 of 12realized. Although different types of color blindness willinvolve different types of reduction of conditions, theyare unified as a single phenomenon by the fact that theyall involve the reduction of the number of particularlight wavelengths that result in differentiated visualrepresentation. For flash blindness, we say that there is atemporary (or possibly permanent) reduction in theconditions under which the vision function is realized,whatever the mechanism realizing the function ofsightedness may be. It may be that cases of temporaryflash blindness are impairments of functions in thefollowing manner: they are grounded in a physicalchange that results in a malfunction but not a (per-manent) loss of a function. Because the function isrealized by a rather complicated functioning in bothcases, the type of blindness can range over differenttypes of failure in functioning so long as the reductionof conditions is similar.The case of inattentional blindness is interesting andproblematic. It is unclear whether the inattentionalblindness is a type of blindness or the use of the termblindness is metaphorical. If it is not the case that inat-tentional blindness is a type of blindness, then we neednot worry about its formalization. If, however, intatten-tional blindness is a type of blindness, then we proposethe following formalization: inattentional blindness is aninstance of a non-realized disposition (the vision func-tion is non-realized). The disposition will not be realizedin one of two ways: nonconsciously by some mechanismof contour integration in the early visual cortex orconsciously by failure of function by the lateral occipitalcortex. This is why inattentional blindness is much likemore traditional types of blindness even though itsmechanism and presentation differs from these trad-itional types of blindness. One could also classify typesof blindness by the types of failure in sight functionings,if one so chose.Having provided an informal account of what consti-tutes blindness we now provide a formalization of thesolution. Because our account relies heavily on triggers,dispositions, and processes realizing those dispositions,any formalization will have to invoke all of these at aminimum. We can characterize triggers, dispositions, andprocesses realizing dispositions in the following man-ner. Dispositions exist outside of their realizationsthedisposition of fragility inheres in a vase whether or notit there is a process of shattering to realize that dis-position. Some dispositions exist without any realiza-tionsthat is to say, there are some dispositions thatwill never be realized because the triggering conditionsare not met. Realizations are processual entities andhave as participants the material entities. Dispositionsand qualities inhere in material entities. For dispositions,then, we can identify the following features.Dispositions, triggers, and background conditionsThe relationship between triggering conditions, disposi-tions, and realizations described above is over-simplified.Keep in mind that the dispositional account givencharacterizes dispositions as non-probabilistic and theirassociated phenomena as straightforwardly accessible. Inreality, many dispositions are most likely probabilisticand the precise nature of their associated phenomena(triggering conditions, realization processes) is currentlyunknown. This should not deter one from seeking togive an account of dispositions, triggers, and realizationprocesses, however. One of the more attractive accountsproffered is from Rohl and Jansen [27]. According tothis account, there is a primitive (undefined) relation(has_triggerR) that holds between the triggering processand the realization process. The relationship between thedisposition and the triggering process is defined in termsof that primitive relationship as follows:3d has triggerDt iff : there exists some rdhas realization r and r has triggerRtð ÞRoughly, this says that some particular disposition hasa particular trigger just in case there is some realizationprocess such that the particular disposition has thatrealization process and that realization process has sometrigger. We would then say that the particular dispos-ition in question has that particular trigger. This is aninstance-level relationship. An example of such a rela-tionship would be the fragility of a vase. The particulardisposition a vase has for shattering is realized by someinstance of a realization process (a shattering process),which is triggered by some particular triggering event,e.g., a dropping process. The relationship between theparticular realization process that realizes the dispositionand the process that triggers or precedes the realizationprocess is irreducible.Once there is a relationship between the instances of dis-positions and triggers, it is rather straightforward to extendthis to types of dispositions. Following Rohl and Jansen, thisrelationship can be captured in the following manner:Dhas triggerDT¼DEF? ð instance ofD ? ?y has triggerDy ? y instance of Tð ÞÞThis definition is at the level of types. It specifies the rela-tionship between disposition types and triggering processtypes. In a similar manner the relationship betweenrealization types and triggering types can be captured:R has triggerRT¼DEFfor ?  ð instance ofR ? ?yy instance ofT  has triggerRyð ÞÞThis relationship holds between realization processtypes and triggering process types. Since dispositionsinvolve triggering processes, realization processes, andRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 8 of 12dispositions themselves, it is essential to capture therelationships between these three entities. The aboverelationship specifications do just that. To see how thisworks in our case of vision, consider the followingrelation for the vision of an individual:Agent xs disposition for vision has_triggerD particularlighting conditions iff: there exists some neural processesof agent x (agent xs disposition to see has_realizationneural processes of agent x and neural processes ofagent x has_triggerR particular lighting conditions).The relationships would then exist at the universal level:Vision Disposition has_triggerDLighting Conditions(range of light wavelengths) = DEF ?x (x instance_ofVision Disposition ? ?y (x has_triggerD particular lightingconditions ? particular lighting conditions instance_ofLighting Conditions (range of light wavelengths)).Photon to Neural Signal Transduction has_triggerR-Lighting Conditions (range of light wavelengths) = DEF for?x (x instance_of Photon to Neural Signal Transduc-tion ? ?y (y instance_of Lighting Conditions (range oflight wavelengths) & x has_triggerR y)).4The above specifications may be complex but have anumber of advantages. First, they capture the processesthat underlie dispositions and the dispositions them-selves, which allows for more accurate modeling.Second, this approach accounts for all of the processes(or at least all of the relevant processes) involved indispositions. Approaches that do not account for all ofthe relevant processes involved in dispositions will beincomplete. Third, it leverages BFO as an upper-levelontology and inherits all of the benefits therein. Namely,using BFO provides a realist framework for modelingbiomedical entities of interest and provides pre-theoreticalinteroperability with other biomedical ontologies that alsoemploy BFO as an upper-level ontology.Possible objectionsOne possible objection to this account is that sinceblindness is a reduction in the range of conditions underwhich a disposition is realized, blindness is extrinsic tothe agent that is blind. So, according to our account,turning off the lights is sufficient to make someoneblind. The thrust of this objection being that the reduc-tion of triggering conditions is extrinsic to the agent, butblindness is a phenomenon that is intrinsic to an agent.Therefore our account is incorrectWe think that this objection is confused. The confu-sion here is one between the presence of the conditionsand the range of conditions. To take an example, waterhas a disposition to freeze at or under a certaintemperature at a given level of atmospheric pressure(which just happens to be a range of conditions underwhich the freezing process is realized). The temperatureconstitutes a triggering condition for the freezingdisposition. Lowering the temperature brings the dispos-ition closer to realization due to the change in the exter-nal environmental conditions, but it does not mean thatwater freezes at a lower temperature. Nor does thischange the disposition that water has to freeze. It is notthat the range of conditions that changed, rather thatthe conditions themselves changed. The conditionsunder which a disposition is realized are external to anentity, much like the range, but these two things areindependent of one another. Conditions themselves canchange without the range of conditions changing, as isthe case with reductions in triggering conditions. In thisway we can see that turning off the lights does notconstitute a reduction in the range of conditions forwhich a sight function is realized. Rather, it only con-stitutes a reduction in the actual lighting conditionsin that situation.There is an important point to be gleaned from thisobjection, however, in regard to a concern over thedistinction between intrinsic and extrinsic propertiesand whether blindness is intrinsic or extrinsic. We haveno reason to suppose that blindness is either intrinsic orextrinsic, at least not pre-theoretically. The reduction inthe range of conditions under which the vision functionis realized is due to a change in the physical basis of thebearer of the function. This, we believe, accounts for theintuition that blindness is an intrinsic property. Indeedour account of blindness is compatible with this obser-vation, so those who believe that blindness is intrinsicshould not be hostile to our view. We leave furtherdiscussion of intrinsic/extrinsic for a later and moredetailed paper.A second objection runs along the following lines: thesolution offered fails to adequately distinguish a failureof seeing (or sight) from the inability to see. In the caseof inattentional blindness one is failing to see somethingbut yet they have the ability to see that thing. This is animportant distinction: in fact it is the very differencebetween being blind and being able to see perfectly well.It is only when one lacks the ability to detect visualstimuli that they are blind. The account provided con-founds these two phenomena and is thus incorrect.We think this objection raises a very important pointthat we have alluded to earlier. We agree that there is adistinction to be made between having the ability to seeand failing to notice a feature of the world versus nothaving the ability to see. The former typically would notbe considered a case of blindness but the latter surelywould. As we have stated before, the case of inattentionalblindness is interesting and problematic. We have coveredthe possibility that inattentional blindness is a case ofblindness and given an account of how that should beformalized. If the envisioned case driving the objection isone where the use of blindness is metaphorical, then weRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 9 of 12need not and should not explain inattentional blindness asa type of blindness. If, on the other hand, inattentionalblindness is not a metaphorical blindness (it is an instanceof a non-realized visual disposition), then it will be formal-ized in the manner other types of blindness are formalizedaccording to this schema. That is, the disposition will notbe realized in one of two ways: nonconsciously by somemechanism of contour integration in the early visualcortex or consciously by a failure of function by the lateraloccipital cortex.In the case where one fails to see not through aninability to realize a disposition, but rather a simplemistake of perception (the objection at hand), this wouldfirmly fall into a case of metaphorical blindness. If it isthe case that all instances of inattentional blindness aresuch failures to see not because of an inability to realizea disposition, but rather by a mistake of perception, theninattentional blindness is a metaphorical blindness. Ifnot, then we have a formalization at the ready.A third objection can be described in the followingway: One way to explain blindness is that there are manysub-processes that participate in the more complexprocess of vision. When one of these sub-processes doesnot occur, then the vision process does not occur. Whatis happening in the vision case is complicated, at least inpart because there are fine points regarding processes,dispositions (functions), and their respective sub-processesand sub-dispositions (sub-functions). It is the case thatmany of the large, easily observable dispositions seem to benot single dispositions but rather many dispositions work-ing together. It is more accurate to say that the realizationof many dispositions is a complex of processes, some ofwhich may themselves constitute realizations of disposi-tions that, in a way, seem to be sub-dispositions. Indeedwhen we consider complex systems like the human visualsystem it seems that this more complex picture is closer tothe truth than the simple explanation proffered by thecanonical dispositional picture. For example, the humanvisual system contains many components and those com-ponents each have distinct functionsthe function of theretina is to convert photons to action potentials and thefunction of the optic nerve is to transmit the action poten-tial to the lateral geniculate nucleus and the function of theprimary visual cortex is to integrate the action potentialsand organize that information in such a way that it is intel-ligible for a human being, etc. Each of these processes couldfail to be realized due to a failure of some mechanism, andthose are the causally responsible entities in the caseof blindness or impaired vision. When we think ofthese smaller processes and the material entities theyare dependent upon, we see that color blindness is atemporary or permanent loss of one of the disposi-tions of one or more structures of the visual system bytheir physical alteration. This view is simpler becausewe can explain the disposition of an organism byappealing to their parts and the realization of sub-pro-cesses, which is more accurate than simply appealing tolarger-scale dispositions.We find the approach described in the previous para-graph problematic for a few reasons. First, the nature ofdispositions in BFO is such that the account thisresponse posits is insufficiently robust. As we havediscussed, dispositions are such that they cannot bewithin the domain or range of the lacks_part relation-ship. If there is a loss of a disposition or function, wehave to determine how to represent a loss of a dispos-ition, which is not something we can do simply byleveraging existing relationships.Second, one of the problems for such an account isthe nature of functions, which are a special type ofdisposition in BFO. It is the case that functions can belost under certain circumstances, i.e., under circum-stances of extreme physical, structural change of thebearer of the function. However, it is not the case thatany physical change in a bearer of a function such thatthe function ceases will entail the loss of the function.This is the case in BFO as well as cases of vision underexamination in this paper.For example, a door has a function to open. It is essen-tial to the proper functioning of the door that its func-tion is only realized under certain conditions. If the dooropens too easily (under a range of conditions that is toowide, say such that it opens under the pressure of thewind), we would still contend that the door bears thefunction to open, but that it is malfunctioning (due to afaulty latch or some such). Likewise, a door can be diffi-cult to open while retaining its function to open, alsodue to some structural alteration. But we would still saythat the door has the function to open (and close) andthat it is malfunctioning. For each of these examples thedoor retains its function but the function is realizedunder a smaller (or larger) range of conditions (i.e., ittakes more or less force to open the door). The range ofconditions under which a function is realized is reducedbecause of an alteration in physical structure of thebearer of the function. The function of the entity is notaltered, however; its functioning is. In this way a difficultdoor is similar to some cases of blindness, in that bothof the entities bearing the function retain that functioneven though the function is realized under a broader ornarrower range of conditions.Now, it could be appropriate to frame such an eventin terms of the various sub-processes or processes rea-lizing the function (what are called functionings) notoccurring, and this is indeed what is happening. Butsuch a framing would fail to accurately capture all of asingle type of phenomenon, in our case blindness.Further, what unites all cases of blindness is not thatRay et al. Journal of Biomedical Semantics  (2016) 7:15 Page 10 of 12some functioning or another is not occurring, but thatwe have as a result a reduction of the range of condi-tions under which a disposition is realized. Any suchreduction (under a certain threshold) is a case of blind-ness, no matter what sub-functioning is unrealized. Ofcourse we may find further subtypes of blindness existand these may be demarcated according to the variousunderlying physical mechanisms that reduce the rangein conditions under which the vision function is realized,but this is consistent with our view. In fact it is our view.In each of the cases discussed, the difference is themechanism underlying the reduction in the range oftriggering conditions for the disposition of vision.DiscussionGiven the problems associated with the currently exist-ing accounts of blindness, we thus propose a new defin-ition of blindness: blindness is a reduction of the triggerconditions under which the sight function is realized. Ifvisual impairment admits of degrees, then it cannot be afunction or disposition. Blindness cannot be a lack of adisposition or function because many things lack thefunction yet should not be classified as blind. The bestoption available for defining blindness is to say that sightis the function to receive photons and interpret them asvisual information and then proceed to define blindness asa reduction of the conditions under which the dispositionis realized.One of the consequences of this reasoning is thatmany of the terms in various phenotype ontologiesmistakenly refer to entities that are dispositions ratherthan qualities. Our goal in this paper is not to demon-strate that many terms in phenotype ontologies aremistakenly characterized as denoting qualities ratherthan dispositions, but merely that blindness is not aphenotype. We think that, as is consistent with BFO,dispositions are not phenotypes and phenotypes are notdispositions. It would be a mistake to confuse pheno-types and dispositions. This does not entail that thereare no terms in various phenotype ontologies thatdenote dispositions, only that representing a dispositionas a phenotype is incorrect from the perspective of BFO.While there may have been efforts to annotate datausing 'blindness' in phenotype ontologies in the past, wefeel these efforts are mistaken.Our approach has certain advantages. First, it accountsfor the graded nature of blindness. The slow and some-times gradual onset of blindness raises special problemsfor ontology construction as it admits of degrees andseemingly vague boundaries. Second, it classifies sight asan internally-grounded realizable entity, which makesuse of the framework provided by an upper-level ontol-ogy such as BFO. Third, it is ontologically innocent inthat there are no new entities to countenance in anyupper-level ontology. The entities referenced by oursolution are already present in BFO so there is no needto introduce new entities.ConclusionsThe motivation of this project is to provide a simple yetflexible ontological account of blindness. Since blind-ness can result from a variety of diseases, the construc-tion of ontologies that incorporate both blindness andthe diseases that cause blindness, either directly or indir-ectly, is of importance to the biomedical community.But this is not a purely classificatory exercisetheemployment of conditions under which a disposition (orfunction) is realized is a novel application of a tool thathas been available for ontological developers for sometime. It is the opinion of these authors that this type ofusage could yield further fruitful results.Endnotes1Functions in BFO are a special type of disposition.2Functions do not have parts but the entities that havefunctions do have parts.3Note that the relationship has_trigger is two-place dueto the limitations of formal ontology languages like OWL,which is only able to handle two-place relationships.4We leave bound variables uninstantiated for the reader.Competing interestsThe authors declare that they have no competing interests.Authors contributionsPLR composed, edited, and revised the manuscript. APC, MJ, WD, TA, andADD provided comments and revised the manuscript. ADD directed thework. All authors read and approved the final manuscript.AcknowledgementsThe authors would like to thank Isaac Berger for insightful comments on themanuscript as well as participants of the 5th International Conference onBiomedical Ontologies (Houston, TX, October 2014) for discussion on apresentation of an earlier version of this work.Author details1Department of Philosophy, University at Buffalo, Buffalo, NY, USA. 2New YorkState Center of Excellence in Bioinformatics and Life Sciences, University atBuffalo, Buffalo, NY, USA. 3Department of Neurology, Jacobs School ofMedicine and Biomedical Sciences, University at Buffalo, Buffalo, NY, USA.Received: 26 March 2015 Accepted: 22 March 2016López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 DOI 10.1186/s13326-016-0101-1RESEARCH Open AccessCan SNOMED CT be squeezed withoutlosing its shape?Pablo López-García* and Stefan SchulzAbstractBackground: In biomedical applications where the size and complexity of SNOMED CT become problematic, usinga smaller subset that can act as a reasonable substitute is usually preferred. In a special class of use caseslikeontology-based quality assurance, or when performing scaling experiments for real-time performanceit is essentialthat modules show a similar shape than SNOMED CT in terms of concept distribution per sub-hierarchy. Exactly howto extract such balanced modules remains unclear, as most previous work on ontology modularization has focusedon other problems. In this study, we investigate to what extent extracting balanced modules that preserve the originalshape of SNOMED CT is possible, by presenting and evaluating an iterative algorithm.Methods: We used a graph-traversal modularization approach based on an input signature. To conform to ourdefinition of a balanced module, we implemented an iterative algorithm that carefully bootstraped and dynamicallyadjusted the signature at each step. We measured the error for each sub-hierarchy and defined convergence as aresidual sum of squares < 1.Results: Using 2000 concepts as an initial signature, our algorithm converged after seven iterations and extracted amodule 4.7 % the size of SNOMED CT. Seven sub-hierarhies were either over or under-represented within a range of18 %.Conclusions: Our study shows that balanced modules from large terminologies can be extracted using ontologygraph-traversal modularization techniques under certain conditions: that the process is repeated a number of times,the input signature is dynamically adjusted in each iteration, and a moderate under/over-representation of somehierarchies is tolerated. In the case of SNOMED CT, our results conclusively show that it can be squeezed to less than5 % of its size without any sub-hierarchy losing its shape more than 8 %, which is likely sufficient in most use cases.Keywords: SNOMED CT, Ontology modularization, Biomedical terminologyBackgroundThe large size and complexity of SNOMED CT [1] con-stitute a problem in many biomedical applications andstudies have shown that using a much smaller subset ofinterest is often sufficient [2]. Applications include prob-lem lists [3], tagging medical images [4], and annotatingtexts from cardiology [5], among others. A well-knownexample of the benefits of using a subset of interest isthe CORE problem list subset of SNOMED CT, whichcontains only 16 874 terms (roughly 1 % the terms ofSNOMED CT), while covering over 95 % of its usage [6].*Correspondence: pablo.lopez@medunigraz.atInstitute for Medical Informatics, Statistics and Documentation, MedicalUniversity of Graz, Auenbruggerplatz 2, Graz, AustriaThe theory of how to extract such subsets is studied bythe ontology modularization area of research [7]. Ontol-ogy modularization techniques are generally focused onobtaining a minimal subset (also called module or seg-ment) that maximally covers a specific domain or thatis representative for a particular application. This is thecase of the problem lists or annotation cases mentionedabove, or the study by Seidenberg and Rector [8], wherethey described how they extracted a representative seg-ment of the GALEN ontology [9] for cardiology using theseed concept Heart as a signature.A signature is an initial set of concepts (called seeds)that bootstraps the modularization process, on which© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 2 of 7many ontology modularization techniques rely, includ-ing graph-traversal [8, 1012] and logic-based techniques[13, 14].Often, these modules are not balanced when it comesto representing the original distribution or shape of sub-hierarchies shown by the original ontology or termi-nology. For example, in the CORE problem list subsetof SNOMED CT, most concepts belong to the Clini-cal Finding, Procedure, Situation with Explicit Context,and Event sub-hierarchies. The opposite case is also pos-sible: in a previous study, we found that modules canexcessively and uncontrollably grow and spread acrosssub-hierarchies, especially when using graph-traversaltechniques [5].These results are not surprising, because most priorwork on ontology modularization has not focused on pre-serving the representativity of the sub-hierarchies of theoriginal ontology, so the shape of the original ontology isinevitably lost in the modules.There is a special class of use cases, however, where itis essential that modules are representative of the sub-hierarchies of the original ontology and therefore show asimilar shape, such as: In ontology-based quality assurance, where small butrepresentative samples of a huge ontology are to beinspected [15]; for obtaining a demonstration version that isunderstandable for users or facilitates visualization[16, 17]; for alignment with a highly constrained upper levelontology, such as the Basic Formal Ontology (BFO)[18], especially the upcoming BFO 2.0 OWL version,which includes relations, DOLCE [19] orBioTopLite [20], where reasoning has to be tested onsmall subsets and in iterative debugging steps; for performing scaling experiments for real-timeperformance of a large OWL DL ontology; for the description logics community, who welcomesscalable testbeds for developing tools like editors andreasoners.To the knowledge of the authors, little research onontology modularization has focused on extracting bal-anced modules for such applications, where keeping theoriginal shape of a large ontology such as SNOMEDCT interms of its sub-hierarchies is a requirement.In this paper, we study the concept distribution ofSNOMEDCTs sub-hierarchies, and we propose and eval-uate an iterative algorithm for extracting balanced mod-ules. Our main goal is to investigate to what extent itis possible to obtain modules that preserve the origi-nal shape of SNOMED CT in order to be used in ouridentified class of use cases.MethodsAs input for our experiments, we used the July 2014International Release of SNOMED CT [21]. We first gen-erated its corresponding OWL-EL version using the Perlscript included in SNOMEDCTs official distribution. Wethen removed the SNOMED CT Model Components sub-hierarchy, which contains metadata concepts only. For theremainder of this text, we refer to SNOMED CT and ourinput version (containing 229 330 classes) termed SCTinterchangeably.SNOMED CT concept distributionTable 1 shows the main 18 sub-hierarchies of SNOMEDCT and their concept distribution. Four sub-hierarchies(Clinical Finding, Procedure, Organism, and Body Struc-ture) contain over 10 % of SNOMED CTs concepts each,accounting for over 70 % of the concepts when consideredaltogether. As a useful way of visualizing concept distribu-tion and for comparative purposes (see Section Results),the same information is displayed in the form of a treemapin Fig. 1. The treemap represents SNOMED CTs hierar-chical information as a set of colored rectangles, wherethe area (and color) of each rectangle is proportional(and darker/lighter) to the number of concepts in thesub-hierarchy.Table 1 Main sub-hierarchies of SNOMED CT. The metadataconcepts sub-hierarchy (SNOMED CTModel Components) was notconsideredSubhierarchy (Abbreviation) Concepts DistributionClinical Finding (CF) 100 893 33.57 %Procedure (PR) 53 914 17.94 %Organism (OR) 33 273 11.07 %Body Structure (BS) 30 685 10.21 %Substance (SU) 24 021 7.99 %Pharmaceutical/Biologic Product 16 881 5.62 %Qualifier Value (QV) 9 055 3.01 %Observable Entity (OE) 8 307 2.76 %Social Context (SO) 4 703 1.56 %Physical Object (PO) 4 522 1.50 %Situation with Explicit Context (SI) 3 695 1.23 %Event (EV) 3 673 1.22 %Environment or Geogr. Location (EG) 1 814 0.60 %Specimen (SN) 1 447 0.48 %Staging and Scales (ST) 1 309 0.44 %Special concept (SP) 649 0.44 %Record Artifact (RA) 227 0.22 %Physical Force (PF) 171 0.08 %López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 3 of 7Fig. 1 SNOMED CTs shape represented with a treemap. Sub-hierarchies containing less than 10 % of SNOMED CT concepts are shown in acronyms(see Table 1)Balanced SNOMED CTmodulesIn a comprehensive study, dAquin et al. [22] concludedthat there is no universal way to extract ontology mod-ules and that the chosen approach should be guided byeach domain or application. It is therefore important toclearly define what constitutes a module. For our pur-poses, presented in the introduction, we define a bal-anced SNOMED CT module (M) as a minimal collec-tion of classes from SCT that conform to the followingrequirements:(a) All classes in M are hierarchically connected toSNOMED CTs root concept in the same way as inSCT.(b) All classes in M share the same axiomatical classdefinition as in SCT.(c) Sub-hierarchies in M are distributed (approximately)in the same proportion as in SCT. In practical terms,when visualized using a treemap, M should looksimilar to the treemap of SNOMED CT shown inFig. 1.(d) Our model is restricted to classes. SNOMED CTmetadata concepts are excluded and not subject tomodularization.Module construction from seedsTo extract our module M, we followed a graph-traversalontology modularization approach, adapted from Seidenbergand Rector [8]. Using their terminology, concepts (inour case, classes) are represented as nodes in a graph,and seed concepts are called target nodes. The strategytakes seeds that conform an initial signature as input,and then iteratively adds classes that appear in the right-hand expressions of their definitions (i.e., are connectedby attribute links) and their links up the hierarchy, thenbecoming new target nodes. Figure 2 shows an exampleof a resulting module, where it can be seen that (a) allclasses are hierarchically connected to the root concept inthe same way as in the original ontology (Fig. 3), and (b)all classes share the same axiomatical class definition asthe original ontology (i.e., show the same structure whendisplayed as a graph).Seed adjustment: an iterative algorithmThe strategy to build a module using seeds presentedabove guarantees requirements (a) and (b) from our def-inition of M, but does not guarantee requirement (c), i.e.,that sub-hierarchies in M will be distributed (approxi-mately) in the same proportion as in SCT. The reason fornot guaranteeing requirement (c) is that there is no controlover classes from other sub-hierarchies that are added andbecome new target nodes when following the right-handexpressions of the seeds.Therefore, in order not to conflict with requirements(a) and (b) when creating M, the only possibility is tocarefully select the initial signature that bootstraps themodularization algorithm. For that purpose, we investi-gated an iterative algorithm that dynamically adjusts thedistribution of classes used as seeds in the initial signa-ture. Before presenting the algorithm, we introduce thefollowing notation: As introduced before, SCT represents the OWL ELversion of SNOMED CT used as input.Sub-hierarchies are termed SHk . M represents the output module, whosesub-hierarchy distribution should match SCTs asclosely as possible (Table 1).López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 4 of 7Fig. 2 Ontology modularization strategy to build our moduleM, starting from the seed concept (target node) labeled as 10. Figure 3 shows theoriginal ontology from which it was extracted SIGN is the input signature, consisting of classesfrom SCT, that is used to bootstrap themodularization process described inSubsection Module construction from seeds. Error(SHk) = Size(MSHk ) ? Size(SCTSHk ) indicatesthe error on a per sub-hierarchy basis. Errors arecalculated in percentage terms (see distribution inTable 1). RSS = 118?18k=1 Error(SHk)2, where RSS representsthe residual sum of squares. Convergence of thealgorithm is defined when RSS < 1.The algorithm, at each iteration i is the following:1. A random signature SIGNi consisting of 2000 classesfrom SCT is selected, following the same classsub-hierarchy distribution as SCT, and ensuring thatall sub-hierarchies in the signature contains at leastone class.2. A moduleMi is computed following the principlesdescribed in Subsection Module construction fromseeds. Its sub-hierarchy distribution is calculated.3. Convergence is checked. If RSS >= 1, Steps 1 to 3are repeated after adjusting the scaling factor for thesub-hierarchy distribution of the signatures in thenext iteration i + 1:f(SIGNi+1SHk)= f(SIGNiSHk)× f(SCTSHk)f(MiSHk) withf(MiSHk)being the relative frequency of sub-hierarchySH k measured in the resulting module in iteration i,Mi.Fig. 3 Sample ontology, with an initial signature containing the seed concept (target node) labeled as 10López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 5 of 7ResultsA moduleM with 10 834 classes was extracted from 2000seeds, the module being in 4.7 % the size of the originalSCT (229 330 classes). Figure 4 shows how the algorithmconverged after 7 iterations, the error for sub-hierarchiesexceeding an error of 1 %, and the residual sum of squares.As can be seen in the table below the graph, the sub-hierarchies Clinical Finding, Procedure, and Organismwere under-represented in M, while Body Structure andSubstancewere over-represented. The same results can beconfirmed graphically in the treemaps shown in Fig. 5, atiterations 1, 3, and 7.These results were partly expected, due to the natureof the modularization approach that uncontrollably addsextra classes that appear in right-hand expressions topreserve SNOMED CTs class definitions. The most rep-resentative example is the sub-hierarchy Body Structure,whose concepts appear often in definitions in ClinicalFindings, e.g. Finding site: Bone structure of femur (bodystructure) for Fracture of femur (clinical finding).Our experience indicates that there is a point (around7 iterations) where the algorithm starts oscillating, andthe residual sum of squares can not diminish any longer.In practice, this means that when the algorithm tries tocompensate the under-representation of Clinical Findingby adding more Clinical Finding seed concepts to thesignature in the next iteration, the result of the new bal-anced module inevitably includes also more Body Struc-ture concepts. This is better understood using Fig. 3 asan example. Assuming concept 10 is a Clinical Find-ing seed added to compensate their under-representation,the graph-traversal modularization algorithm would alsoadd Body Structure concepts 17, 16, 15, and 9 to thebalanced module, because concept 17 appears in a right-hand expression of concept 10, and 16, 15, and 9 are itsancestors (Fig. 2).DiscussionOur results suggest that it is difficult for ontology mod-ules to meet all of our modularization criteria withoutrelaxing the constraints of how concepts in the mod-ules are distributed by sub-hierarchies: this is becausemodularization criteria are conflicting. In our experi-ments, all obtained modules over-represented or under-represented some of SNOMED CTs sub-hierarchies tovarying degrees.The error figures that we obtained after convergence,however, never reached 8 % for any sub-hierarchy and allour modules contained a fair representation of every sub-hierarchy. Furthermore, convergence was reached afteronly 7 iterations and the resulting module was 4.7 % thesize SNOMED CT. Such modules might be sufficient inFig. 4 Execution of the algorithm, showing convergence in iteration 7. Each line represents the difference in distribution for a particularsub-hierarchy of a balanced module at a given iteration, when compared to SNOMED CT. For example, in the balanced module at iteration 1, BodyStructure is proportionally 11.26 % bigger than in SNOMED CT (it is over-represented), while Clinical Finding is 6.86 % (it is under-represented). Thedashed line represents the residual sum of squares of all sub-hierarchies, 0 meaning that the sub-hierarchies in the balanced module are distributedin exactly the same way as in SNOMED CT (perfectly balanced module)López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 6 of 7Fig. 5 Visual comparison of the shape between modulesM and SNOMED CT (d) in iterations 1 (a), 3 (b), and 7 (convergence, c). Clinical Finding,Procedure, and Organism were under-represented, while Body Structure and Substance were over-representedmany of the use cases that motivated their creation, i.e.,extracting modules that show an (approximate) conceptdistribution to the one shown in SNOMED CT.In this study, we focused on extracting balanced mod-ules in SNOMED CT only, both for practical purposes(useful input for related SNOMED CT research) andbecause its size and complexity make SNOMED CT anexcellent case study. Our approach, however, should worksimilarly with for any ontology where graph-traversalmodularization techniques based on an input signatureapply. The literature currently reports positive exper-iments with NCI, GALEN, GO, SUMO, SWEET, andDOLCE-Lite [8, 13].ConclusionsModules that preserve the concept distribution by sub-hierarchy of the original ontology have been generallyneglected in the field of ontology modularization. How-ever, balanced modules are extremely useful in applica-tions such as ontology-based quality assurance, scalingexperiments for real-time performance, or when develop-ing scalable testbeds for software tools.In this study, we have proposed and evaluated an iter-ative algorithm to investigate to what extent extractingsuch balanced modules in SNOMED CT is possible. Ourresults show that graph-traversal ontology modulariza-tion techniques relying on an input signature can indeedbe used, if: the process is repeated a number of times;the input signature is dynamically adjusted in each itera-tion; and a moderate under/over-representation of somehierarchies is tolerated.Several questions are still open and need to be addressedas future work: how to select a minimal signature; howsignature size influences the final size of the modules;and how a change in the randomization process of thesignature selection (e.g., by stratifying the randomizationby node depth) influences the concept distribution of themodule. In addition, a validation of our experiments usingother ontologies and comparing the results would providea more comprehensive overview.Our results, however, conclusively show that SNOMEDCT can be squeezed to less than 5 % its size without anysub-hierarchy losing its shape more than 8 %, which islikely to be sufficient in most use cases.López-García and Schulz Journal of Biomedical Semantics  (2016) 7:56 Page 7 of 7AcknowledgementsThis manuscript is a revised version of the work presented in the ICBO 2015conference. The authors acknowledge ICBO 2015 organizers and participantsfor their useful feedback and suggestions, and Marcus Bloice for checking andrevising the final version of the manuscript.Authors contributionsPLG wrote most of the manuscript, verified the experiments, summarized theresults, and presented the work in the ICBO 2015 conference. SS formulatedthe original idea, implemented the algorithm, and sketched the first version ofthe manuscript. Both authors read and approved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 5 March 2016 Accepted: 9 September 2016RESEARCH Open AccessOBIB-a novel ontology for biobankingMathias Brochhausen1*, Jie Zheng2, David Birtwell3, Heather Williams3, Anna Maria Masci4, Helena Judge Ellis5and Christian J. Stoeckert Jr.2AbstractBackground: Biobanking necessitates extensive integration of data to allow data analysis and specimen sharing.Ontologies have been demonstrated to be a promising approach in fostering better semantic integration ofbiobank-related data. Hitherto no ontology provided the coverage needed to capture a broad spectrum ofbiobank user scenarios.Methods: Based in the principles laid out by the Open Biological and Biomedical Ontologies Foundry twobiobanking ontologies have been developed. These two ontologies were merged using a modular approachconsistent with the initial development principles. The merging was facilitated by the fact that both ontologies use thesame Upper Ontology and re-use classes from a similar set of pre-existing ontologies.Results: Based on the two previous ontologies the Ontology for Biobanking (http://purl.obolibrary.org/obo/obib.owl)was created. Due to the fact that there was no overlap between the two source ontologies the coverage ofthe resulting ontology is significantly larger than of the two source ontologies. The ontology is successfullyused in managing biobank information of the Penn Medicine BioBank.Conclusions: Sharing development principles and Upper Ontologies facilitates subsequent merging of ontologies toachieve a broader coverage.Keywords: Ontologies, Biobanking, Biorepository, TerminologyBackgroundThe field of biobanking demands data integration. Thisneed arises on multiple levels: institutional, cross-institutional, and sometimes even cross-national. Mostinstitutions operate multiple biobanks that were estab-lished to fulfill diverse user requirements and thereforeuse different data representations and schemata. Inte-grating the data from these biobanks is a challenge atbest and impossible at worst.In recent years a number of projects have set out toanswer the call for more specimen and data sharingacross multiple biobanks and multiple institutions, bothon a national and on a transnational level [1, 2]. One ofthe earliest examples of a transnational sample sharingproject is the European Biobanking and BioMolecularResearch Infrastructure (BBMRI) [3]. Using large amountsof data from multiple biobanks is an important way to en-able statistical analysis, which can lead to uncoveringassociations between phenotypes and diseases [4]. Thetwo key challenges are a) identifying specimens for re-search, and b) utilizing the existent wealth of informationpresent in biobanks effectively by integrating data storedin those repositories [5, 6].Andrade et al. have pointed out that semantically richontologies provide a promising approach to integratingdata from diverse biobanks [7]. In 2013 Brochhausen et al.published Ontologized MIABIS (OMIABIS), a WebOntology Language (OWL)-coded ontology for biobankadministration based on use cases and competency ques-tions derived from BBMRI (http://purl.obolibrary.org/obo/omiabis/merged/omiabis.owl) [8]. OMIABIS is one ofthe source ontologies that was used in creating the Ontol-ogy for Biobanking (OBIB). It is linked to the BBMRI ef-fort [3] and is based on the Minimum Data Set for sharingbiobank samples, information and data [9]. One of the keyadvantages of using ontologies, and in particular re-usingpre-existing ontological representations is the possibilityto link data from biobanks to other biological and bio-medical repositories using common identifiers, e.g. from* Correspondence: mbrochhausen@uams.edu1Department of Biomedical Informatics, University of Arkansas for MedicalSciences, 4301 W. Markham St., #782, Little Rock, AR 72205-7199, USAFull list of author information is available at the end of the article© 2016 Brochhausen et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 DOI 10.1186/s13326-016-0068-yGene Ontology [6]. This possibility will improve the utilityof the shared biobank data tremendously since it allowseasy retrieval of related data using semantic web technolo-gies, such as RDF, SPARQL, and OWL. The capability touse biobank data in that way will make biobanks evenmore important for and accessible to translational re-search since the data already exist in a way that allowseasy linkage to data across other disciplines from basic sci-ence to clinical research.In addition, the use of a formal logical model to repre-sent aspects of biobanking procedures and protocolsprevents unnecessary complexity that can lead to cum-bersome and error-prone data management and retrievalprocesses. One example of a complexity issue is a biobankprotocol involving something as simple as specimen type.Consider a blood collection protocol that collects two vialsof blood. Each vial contains an anti-coagulant, in this caseone is ethylenediaminetetraacetic acid (EDTA) and theother monosodium citrate (NaCit). Each specimen goesthrough centrifugation producing a number of plasma andbuffy coat specimens. The lab decides to tag their speci-mens in the following way: "edta_plasma", "buffy_edta","nacit_plasma", "buffy_nacit". The "edta_plasma" and "buf-fy_edta" specimens are the derivatives of the EDTA parentspecimen and analogously for "nacit_plasma" and "buffy_-nacit" specimens. So the lab is encoding two pieces of in-formation in the "specimen type", the parent specimenand the anticoagulant. Notice also that they chose theanticoagulant as the prefix for plasma specimens, but asthe suffix for the buffy specimens, due to their perceptionthat the anticoagulant is more important for the plasmaspecimens and less so for the buffy specimens. Anotherlab has a simpler protocol. They collect one EDTA speci-men and produce plasma and buffy specimens. Since theycollect only one type of blood specimen, they tag theirspecimens simply as "plasma" and "buffy". The informationabout the anticoagulant is implied as is information aboutthe parent specimen. Imagine all these specimens end upin a unified biobank. The following specimen types wouldbe present: edta_plasma, buffy_edta, nacit_plasma, buffy_-nacit, plasma, and buffy. The inconsistencies are readilyapparent. An ontology can provide a formal model fordifferent attributes related to a specimen type, such as theparent specimen and the anticoagulant. Based on theseattributes and others specimen types can be created asaxiomatically defined classes (Fig. 1).In this paper we present and describe the Ontology forBiobanking (OBIB). OBIB has been created by mergingOMIABIS with a more specimen-focused biobank ontol-ogy developed at the University of Pennsylvania calledthe Biobank Ontology (BO). We provide an overview ofthe methodologies used to a) build both ontologies andb) merge them. We also give an outline of the domaincovered by the newly created ontology and describe itscurrent use. Finally, we discuss the next steps in expand-ing OBIB and linking it to ongoing efforts regarding bio-bank terminology. While we think that shared ontologiesare one way to facilitate sharing information acrossFig. 1 Representation of edta_plasma, buffy_edta, nacit_plasma, buffy_nacit, plasma, and buffy specimens according to the OBIB strategy. Blue boxesrepresent classes; red boxes represent individuals; red arrows represent rdfs:subClassOf; green arrows represent rdf:type; blue arrows represent OWLobject properties (the labels are specified). While all OWL object properties link instance to instance, in this figure there are object propertiesconnecting OWL classes to each other. This represents a property restriction on the source class with existential quantification (all-some restriction)Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 2 of 9multiple sites, the description and discussion of how totechnically implement such a system (federated queries)are regarded as out of scope for this paper.MethodsOBIB is the result of merging two pre-existing ontologies,OMIABIS and BO. In this section we describe criteria andmethodologies used by both the developers of OMIABISand BO. We will give a brief overview over the two sourceontologies and describe the merging process in detail.Both ontologies, OMIABIS and BO are built based onthe principles of the Open Biological and BiomedicalOntologies (OBO) Foundry (http://obofoundry.github.io/principles/fp-000-summary.html) [10]. Both are exten-sions of the Ontology of Biomedical Investigations (OBI)(http://purl.obofoundry.org/obo/obi.owl) [11]. OBI isbased on Basic Formal Ontology (BFO), an UpperOntology frequently used to represent biological andbiomedical domains [12, 13]. One of the OBO Foundryprinciples is re-use of pre-existing ontologies or theirclasses and object properties in order to prevent multiplerepresentations of the same entities. Both, OMIABIS andBO re-use numerous terms from existing ontologies. Bothontologies were developed based on a methodology thatfocuses on representation of the real-world phenomenathat are described by the data, which is intended to bemanaged, instead of creating OWL representations ofexisting data schemata. Smith and Ceusters have coinedthe term ontological realism for this approach [14]. The ex-ample provided of specimen type shows that relying exclu-sively on pre-existing data representations can lead toproblems integrating data from heterogeneous resources.Instead of providing representations base on the term used,such as "edta_plasma", "buffy_edta", "nacit_plasma", "buf-fy_nacit", "plasma", and "buffy", ontologies should representthe specimens, the parent specimens, the anticoagulantsand the different processes that were necessary to createthe specimen. Thus, individual specimens can be sortedinto specimen types based on what their parent specimensare and which processes they passed through. This meth-odological paradigm fosters linking the biological sourcesto the specimens to the data about those specimens.OMIABISOntologized MIABIS (OMIABIS) (http://purl.obolibrary.org/obo/omiabis/merged/omiabis.owl) was created as anOWL implementation of the BBMRI's Minimum Informa-tion About BIobank data Sharing (MIABIS). It is based onthe BBMRI use cases, which are mostly population and co-hort based. Due to juridical and ethical reasons searchingindividual specimens was out of scope for the initial imple-mentation of OMIABIS [8].The competency questions for the development ofOMIABIS were: Which biobanks hold frozen specimens? Which biobanks hold blood, plasma and serum? Which blood plasma specimens are owned by onespecific biobank organization? Which departments of a specific university havemembers that are serving as biobank contacts? What are the e-mail addresses of all biobank contactpersons at one specific biobank organization?These competency questions demonstrate that thefocus of the OMIABIS development was less to retrieveinformation about individual specimens and to orderthose specimens, but more to obtain basic population-level and repository specific information about biobankadministration and related study administration. OMIA-BIS represents both relevant objects, such as "biobank"or "biobank organization", and relevant processes, suchas "specimen handling" and "sampling specimens for bio-bank". Terms were re-used from the Cell Type Ontology(CL), Chemical Entities of Biological Interest (ChEBI),Common Anatomy Reference Ontology (CARO), Informa-tion Artifact Ontology (IAO), NCBITaxonomy, Ontologyof Medically Relevant Social Entities (OMRSE), PhenotypicQuality Ontology (PATO), Proper Name Ontology (PNO),and Reagent Ontology (REO). One hundred twenty-six en-tities were created specifically for OMIABIS resulting in atotal of 428 classes, 15 individuals, 75 object properties,and 990 logical axioms. The ontology and additional detailscan be found at https://github.com/OMIABIS/omiabis-dev.Biobank ontology (BO)Another biobank ontology based on OBI, BO, was gen-erated independently of OMIABIS to address use casesprovided by the Penn Medicine BioBank, which servedalong with OMIABIS as a starting point for OBIB. Thecompetency questions for the development of BO were: How many study subjects have filled out a patientquestionnaire for which there is an associatedcollection packet? What blood specimens are available from studyparticipants? What chemical additive was usedin the container? What is the storage state of the specimen ofinterest? How has the specimen been processed?The BO aimed to address these competency questionsby covering the processes along with inputs and outputsassociated with a specimen in a biobank repository.These included human subject enrollment , informedconsent process , document editing (filling out case re-port forms), specimen collection process , material pro-cessing of the specimen, storage of the specimen, andshipping and handling processes. Patient-related termsBrochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 3 of 9(e.g., smoking behavior) were also included. Terms werere-used from BFO, IAO, OBI, ChEBI, EFO, NCBITaxon,OGMS, PATO, and UBERON. Approximately 50 termswere added to address biobank specific needs (e.g., col-lection packets for specimens) resulting in 227 classes,18 individuals, 34 object properties, and 526 logical ax-ioms. The ontology and additional details can be foundat https://github.com/biobanking/Penn-Biobank.Methodology of merging OMIABIS and BOThe fact that both OMIABIS and BO were built based onOBI and used BFO as top-level ontology greatly facili-tated the integration of the two ontologies. However,various BFO versions were used and some commonterms were included in both OMIABIS and BO. The fol-lowing processes were performed before integration ofOMIABIS and BO:1. Converted both ontologies using BFO version 2.0 Grazrelease (http://purl.obolibrary.org/obo/bfo/2014-05-03/classes-only.owl). The conversion was made using BFOconverter (http://bfoconvert.hegroup.org/).2. Separated terms defined in OMIABIS or BO fromthose defined in external resources (OBO FoundryOntologies) and saved those in different OWL files.3. Identified terms defined in both OMIABIS and BOand merged the overlapping terms where necessary.This pre-processing of OMIABIS and BO beforemerging resulted in the following files: omiabis.owl or biobank.owl: OMIABIS or BOspecific terms. import_OBI_subset.owl: OBI subset upon whichOMIABIS or BO was built containing terms neededfrom both IAO and OBI. The OBI subset wasretrieved using Ontodog, a tool that can retrieve aset of terms of interest and all related axioms from asource ontology [15]. import_OBO.owl: terms defined in external OBOFoundry ontologies and retrieved using OntoFox, atool that can retrieve terms of interest from a sourceontology based on MIREOT mechanism [16]. externalByhand.owl: terms defined in external OBOFoundry ontologies added manually.These files are available on the OMIABIS projectwebsite:OMIABIS: https://github.com/OMIABIS/omiabis-dev/tree/master/BFO2%20omiabis and BO: https://github.com/OMIABIS/omiabis-dev/tree/master/biobank-omiabis/BFO2%20biobank.Since OMIABIS and BO focused on different aspectsof biobanking, no overlap in terms was found betweenOMIABIS and BO specific terms (omiabis.owl andbiobank.owl).In preparing BO for merging it with OMIABIS, wealso compared BO to other OBO Foundry ontologiesrelevant to the domain. We found a few BO terms re-lated to informed consent that overlapped with termsfrom the Informed Consent Ontology (ICO) [17]. TheseBO terms were replaced by ICO terms. Since BO wasnot officially registered as an OBO Foundry community,BO specific terms were assigned OBIB term identifiersafter merging.Protégé was used to perform the merging of theOWL files prepared based on the two ontologies.The process was a series of merges for pairs ofequivalent OWL files used by OMIABIS and BO tocreate: import_OBI_subset.owl from subset OWLfiles, import_OBO.owl from OWL files of externalterms retrieved using OntoFox [16], externalByhan-d.owl from manually imported external terms OWLfiles, and biobank-omiabis.owl from omiabis.owl andbiobank.owl. The one remaining BFO 1.1 class, Con-nectedTemporalRegion, was dealt with by taking ad-vantage of its definition as equivalent to the unionof its two subclasses, temporal_instant and tempora-l_interval. These have both been mapped to BFO 2.0classes, zero-dimensional temporal region and one-dimensional temporal region, which were used to replaceConnectedTemporalRegion.These merged OWL files are available on github:https://github.com/OMIABIS/omiabis-dev/tree/master/biobank-omiabis.The merged OMIABIS and BO ontology is availableon:https://raw.githubusercontent.com/OMIABIS/omiabis-dev/master/biobank-omiabis/biobank-omiabis_merged.owl.Finally, the consistency of the merged ontology waschecked using Hermit 1.3.4 and no inconsistencies werefound.ResultsOntology of biobanking (OBIB)The first release of OBIB (http://purl.obolibrary.org/obo/2014-09-22/obib.owl) was made based on the OWL filesdescribed in section Methodology of merging OMIABISand BO. The development of OBIB followed the OBOFoundry principles. OBIB is freely and openly available.The latest release can be obtained from http://purl.obolibrary.org/obo/obib.owl. The community drivendevelopment is done using an open code repository,https://github.com/biobanking/biobanking. Issues andterm requests can be communicated at https://github.com/biobanking/biobanking/issues. Currently, OBIB con-tains 516 classes including 126 OMIABIS and 46 OBIBBrochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 4 of 9classes, 19 individuals, 83 object properties, and 1172 lo-gical axioms.One of the central terms of OBIB is biobank. This termwas merged from OMIABIS. OBIB defines biobank as:"A biobank is a collections of samples of biologicalsubstances (e.g. tissue, blood, DNA) which are linked todata about the samples and their donors. They have adual nature as collections of samples and data".The equivalent class axiom, which provides a for-mal machine-parsable definition, can be accessed at:http://www.ontobee.org/ontology/OBIB?iri=http://purl.obolibrary.org/obo/OMIABIS_0000000.From these definitions it is obvious that from the per-spective of OBIB, a biobank consists of both the speci-mens and the data about the specimens and the specimencollections. OBIB differentiates between a biobank and abiobank organization. The latter is defined as:"An organization bearing legal personality that owns oradministrates at least one biobank."This differentiation enables concise representation ofan organization running more than one biobank, as isregularly the case for hospitals, research facilities andothers. So, OBIB fills gaps regarding the representationof biobanking that have so far existed in OBO Foundryontologies and other ontologies.Due to the fact that OBIB is the result of merging twoindependently developed ontologies, coverage is broad.Naturally, it covers information about specimens andprocesses like specimen collection, specimen handlingand storage. It also represents donors and patients andmedical record data pertaining to those. In addition, itprovides representation for clinical studies and specimensand data accrued during those. In the current release, theOBIB-generated terms are related to the containment ofthe specimen (e.g., OBIB_0000028: collection packet), ad-ditives (e.g. OBIB_0000022: blood additive role), and stor-age mechanisms (e.g., OBIB_0000030: blood spot card).OBIB specific terms were also generated for patient forms(e.g., OBIB_0000017: data confirm questionnaire) and as-sociated health-related questions (e.g., OBIB_0000053:duration time of smoking).Most terms needed by OBIB are not specific to bio-banks and available from other OBO ontologies andimported. Figure 2 shows a selection of the most rele-vant classes of OBIB and their superclasses. The figureshows that many classes in OBIB are re-used from othercommonly used ontologies of the biomedical domain,such as OBI, IAO, and others, or subclasses of thoseclasses. Notably, some of those are central classes suchas "specimen" and "specimen collection", both re-usedFig. 2 Selection of central classes of OBIB and their superclasses. The leftmost four BFO classes are subclasses of further BFO classes which are notshown here for readabilityBrochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 5 of 9from OBI. Other OBI classes, such as "organization" and"material maintenance" are superclasses for classeshighly relevant to the biobanking domain, such as "bio-bank organization" and "specimen freezing".This not only highlights the relevance of OBI for therepresentations that are part of OBIB, but also providesan important opportunity regarding the use of semanticweb technologies in translational research. Terms like"specimen" are used by multiple ontologies. The NationalCenter for Biomedical Ontologies' (NCBO) BioPortal [18],an ontology lookup service, retrieved 18 different repre-sentations of the term "specimen" in BioPortal ontologies(when queried Nov. 18, 2015). OBI's representation ofspecimen is one of them. However, the only specimen rep-resentations that are referred to by other ontologies arethe OBI representation (referenced by 11 other ontologies)and the Semanticscience Integrated Ontology (SIO) [19]representation (referenced by 1 other ontology). Thisshows that the representation of specimen by OBI is byfar the one most widely used by other ontologies. Re-useof OWL entities (such as "specimen") in multiple ontol-ogies and in multiple applications using those ontologiesis relevant, since the representation comes with a UniqueResource Identifier (URI) that allows linking data in RDF.This is a key strategy of semantic web technology thatholds huge promise for the translational science commu-nity. Data created for use in a specific domain (e.g. bio-banking) can be linked easily with data created in anotherdomain (e.g. digital pathology) by using the same URIs torefer to the same entities.Usage of OBIBAt the Penn Medicine Biobank, OBIB is used as thesemantic framework for a search system that supportscohort identification and deep data mining of the infor-mation associated with biobank specimens and specimendonors. We explored how OBIB could be used througha specific competency question of case/control match-ing. For cases, we wanted to identify patients who wereconsented to the biobank, had a history of type 2 dia-betes, took a particular statin medication on or beforethe date of recruitment, and had a banked EDTA bloodspecimen. Eligible controls needed to have type 2 dia-betes, have no history of taking any statin medications,and have a banked EDTA specimen. Controls needed tobe matched to cases by gender, age at the time of re-cruitment, and Body-Mass Index (BMI). The end goalwas an integrated graph database that contained the in-stances and semantics of our data and could be queriedto answer our competency question, one which exempli-fied a typical request made about biobank data.The data needed to answer the competency questionwere located in several relational data sources. Diagnosisand prescription data were stored in relational datasources derived from the patients' electronic medical rec-ord. Dates of birth and recruitment, gender, and BMI attime of recruitment were gathered in case report forms(CRF) at the time of enrollment. There were several itera-tions of the CRFs that were stored in databases with differ-ent table structures. A snapshot of the inventory data wasgenerated from the specimen inventory system.The process of building an RDF search system to an-swer our question was divided into 4 parts: 1) semanticmodeling, 2) data mapping and instantiation, 3) domainknowledge linking, and 4) querying and testing (Fig. 3).For semantic modeling, local domain experts andOBO Foundry ontology experts generated an ontologymodel using OBIB that included the portions of OBOFoundry ontologies relevant to the data sources. In ourcase this took the form of several Cmap [20] documents.Each data source was mapped separately and encapsu-lated the relational data, the semantics intrinsic in therelational data and any relevant domain knowledge. Anadditional model was created to show how the separatedata sources were related. This model served as a guideto adding additional data sources and to the namingconvention of International Resource Identifiers sharedbetween data sources.During data mapping and instantiation, the ontologymodels were referenced to generate a concrete map ofhow the relational data would be transformed into triples.This mapping can be expressed in RDB to RDF MappingLanguage (R2RML) (http://www.w3.org/TR/r2rml), a lan-guage developed by the World Wide Web Consortium.There are several software conversion tools that useR2RML to instantiate relational data as RDF triples. Inour case, we used D2RQ [21] as the conversion softwareand a D2RQ specific mapping language that is a derivativeof R2RML.The ontology models were updated as neces-sary while writing and testing the conversion files.Domain knowledge linking involved loading the instan-tiated RDF data and any related OBO ontologies into agraph database. We used Stardog (http://www.stardog.com) as our graph database into which we loaded RDFinstance data and OBIB.Querying and testing involved verifying the instantiateddata were correct and answering the competency ques-tion. Equivalent queries were generated against the rela-tional and graph data to ensure the data were accuratelymodeled.Future workRecent research has shown that in spite of aiming to fosterclear and concise class representation, definitions from on-tologies do not always rate well with domain experts [22].This highlights the need for closer collaboration betweenontology curators and domain terminology experts regard-ing definitions, term descriptions and real life applicability.Brochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 6 of 9Therefore, the OBIB developers have started a collabor-ation with domain experts heavily engaged in the area ofbiobank terminology, the Duke Biobank, a consortiumorganization within the Duke Translational Research Insti-tute. The Duke Biobank led the effort to select and imple-ment a commercial biospecimen information managementsystem (BIMS) to integrate information from Dukes di-verse biobanking entities. A priority at the outset of theselection process was the establishment of the policy thatall biobanks participating in the BIMS must use a commonterminology. To that end, terms were identified and de-fined through a consensus driven process with biobankingdomain experts across the Duke campus. Sources consid-ered for terms included publications related to biobankingterminology as well as related to biobanking pre-analyticalvariables, existing data elements in use in the existing bio-banks, a public comment period, and out-of-the-box termsfrom the commercial BIMS, once it was purchased. The18 month effort overall resulted in over 500 data elementscovering the lifecycle of the biospecimen, as defined by theNational Cancer Institute [23]. In an effort to further ex-pand and share Dukes work, a collaboration betweenDuke and OBIB began in the fall of 2014. Currently thecollaboration is focused on the comparison of terms andclasses between the Duke terminology and OBIB in aneffort to identify intersections and gaps while implement-ing OBIB classes, with the goal of extending OBIB to coverthe use cases underlying the development at Duke. Todate, this approach has resulted in two different outcomes:i) A Duke term mapped exactly the OBIB term but thenaming was different (e.g. "collect" in Duketerminology correspond to "specimen collection"in OBIB).ii) Duke term was not present in OBIB (e.g. Duke term"sample set" and "sample family").This led to the creation of new terms specifyingalready existing classes in OBIB and highlighted the de-mand for additional general classes in OBIB (e.g.,OBI_0002080: human specimen set; OBI_0002077: spec-imens derived from shared ancestor). The aim of thisclose interaction between the additional ontology users(Duke) and OBIB is to create a resource fulfilling the re-quirements of heterogeneous users. The value of thiscollaboration is in joining a robust biobanking termin-ology developed for a specific institutions use, with abiobanking ontology created at two other institutions, le-veraging domain knowledge in both biobanking andontology in order to establish a single, relevant ontologyFig. 3 The process used for building the prototype RDF search system to answer the Penn Medicine Biobank case/control competency question.1. Semantic Modeling-Ontology models are developed to model the semantics of the relational data and any OBO ontologies that are relevantto the data sources and potential queries. 2. Data Mapping and Instantiation-The models developed in step 1 are used to write mapping files toconcretely map the relational data as RDF. Software tools to use these maps to instantiate the relational data as RDF data. 3. Domain KnowledgeLinking-The instantiated RDF data and any relevant OBO Foundry Ontologies are loaded into a graph database. 4. Querying and Testing-Queriesover the graph data can be created by referencing the OBIB model. To test, equivalent queries against the graph data and relational data areconstructed and run to ensure data correctnessBrochhausen et al. Journal of Biomedical Semantics  (2016) 7:23 Page 7 of 9resource with broad coverage to be applied to the fieldof biobanking.Another key aspect of biobanking is related to in-formed consent and retrieving information about theconsent given by the donors to facilitate research. Wehave already pointed out that OBIB already contained abasic representation of informed consent from the ICO.However, in order to retrieve the actual content of theconsent it is not sufficient to limit the representation tothe consent documents and the consent process. It willalso be necessary to represent the rights and obligationsthat are created through the consenting. We have begunworking with the developers of ICO to address this issuein the context of biobanking. While the development ofan in depth representation of informed consent for bio-banking is still ongoing, one strategy that has been iden-tified as key, is using a pre-existing ontology that allowsthe representation of rights and obligations and thesocio-legal processes that give rise to them. The funda-mental aspects of this have already been addressed bythe Ontology of Document Acts (d-acts) (http://purl.obofoundry.org/obo/iao/d-acts.owl) [24, 25].ConclusionWe have seen that for domains as central as biobankingmore than one ontology might exist. In order to fostersemantic integration of data for the largest possiblenumber of users and consumers, it might be necessaryto merge two or more ontologies.In this paper we demonstrated that merging ontologiesthat share a common design methodology, and that ex-tend the same Upper Ontology and Reference Ontologycan be done fairly easily and with consistency. BothOMIABIS and BO adhere to OBO Foundry principlesand were created based on the methodological paradigmof ontological realism. Both ontologies are extensions ofOBI, which is based on BFO.We have also demonstrated how the result of the mer-ger is currently used and allows answering competencyquestions based on real-world use cases in the PennMedicine Biobank. Finally, we illustrated how OBIB canserve as a means to capture the semantics and share thevalue of terminologies developed for institutional biobanks.AbbreviationsBBMRI: Biobanking and BioMolecular Research Infrastructure; BFO: basicformal ontology; BIMS: biospecimen information management system;BMI: body-mass index; BO: biobank ontology; CARO: common anatomyreference ontology; ChEBI: chemical entities of biological interest; CL: celltype ontology; CRF: case report form; EDTA: ethylenediaminetetraacetic acid;IAO: information artefact ontology; ICO: informed consent ontology;IRI: International Resource Identifier; MIABIS: minimum information aboutbiobank data sharing; MIREOT: minimum information to reference anexternal ontology term; NaCit: monosodium citrate; NCBO: National Centerfor Biomedical Ontologies; OBI: Ontology for Biomedical Investigations;OBIB: ontology for biobanking; OBO: open biological and biomedicalontologies; OMIABIS: Ontologized MIABIS; OMRSE: ontology of medicallyrelevant social entities; OWL: web ontology language; PATO: phenotypicquality ontology; PNO: proper name ontology; RDF: resource descriptionframework; REO: reagent ontology; SIO: semanticscience integrated ontology;SPARQL: SPARQL protocol and RDF query language; URI: unique resourceidentifier.Competing interestsThe authors declare that they have no competing interests.Authors contributionsMB participated in the development of OBIB and led the effort of writing themanuscript. CJS participated in the development of OBIB, coordinated effortson its application, and helped write the manuscript. JZ participated thedevelopment of OBIB, took responsibility of OBIB release, contributed toOBIB applications and manuscript preparation. AMM contributed to themapping between Duke terminology and OBIB, the creation of new OBIBterms, and helped write the manuscript. DB contributed to the mappingbetween PMBB terminology and OBIB, the creation of new OBIB terms, OBIBapplications, and helped write the manuscript. HW contributed to themapping between PMBB terminology and OBIB, the creation of new OBIBterms, OBIB application development, and helped write the manuscript. HJEled the development of the Duke common terminology, contributed to themapping between Duke terminology and OBIB, the creation of new OBIBterms, and helped write the manuscript. All authors approved of the manuscript.All authors read and approved the final manuscript.AcknowledgementsThe research presented here was partially funded by the UAMS' TranslationalResearch Institute (TRI), grant UL1TR000039 through the NIH National Centerfor Research Resources and National Center for Advancing TranslationalSciences. by the Duke University Center for AIDS Research (CFAR), an NIHfunded program (5P30 AI064518), Dukes CTSA grant UL 1RR024128. Thecontent is solely the responsibility of the authors and does not necessarilyrepresent the official views of the NIH.Author details1Department of Biomedical Informatics, University of Arkansas for MedicalSciences, 4301 W. Markham St., #782, Little Rock, AR 72205-7199, USA.2Department of Genetics, Institute for Translational Medicine andTherapeutics, Institute for Biomedical Informatics, Perelman School ofMedicine, University of Pennsylvania, Philadelphia, USA. 3Penn MedicineBioBank, Institute for Translational Medicine and Therapeutics, PerelmanSchool of Medicine, University of Pennsylvania, Philadelphia, USA.4Department of Biostatistics and Bioinformatics, Duke Medical Center, DukeUniversity, Durnham, USA. 5Duke Biobank, Duke Translational ResearchInstitute, Duke University, Durnham, USA.Received: 1 December 2015 Accepted: 21 April 2016Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 DOI 10.1186/s13326-016-0103-zRESEARCH Open AccessThe early bird catches the term:combining twitter and news data for eventdetection and situational awarenessNicholas Thapen* , Donal Simmie and Chris HankinAbstractBackground: Twitter updates now represent an enormous stream of information originating from a wide variety offormal and informal sources, much of which is relevant to real-world events. They can therefore be highly useful forevent detection and situational awareness applications.Results: In this paper we apply customised filtering techniques to existing bio-surveillance algorithms to detectlocalised spikes in Twitter activity, showing that these correspond to real events with a high level of confidence. Wethen develop a methodology to automatically summarise these events, both by providing the tweets which bestdescribe the event and by linking to highly relevant news articles. This news linkage is accomplished by identifyingterms occurring more frequently in the event tweets than in a baseline of activity for the area concerned, and usingthese to search for news. We apply our methods to outbreaks of illness and events strongly affecting sentiment andare able to detect events verifiable by third party sources and produce high quality summaries.Conclusions: This study demonstrates linking event detection from Twitter with relevant online news to providesituational awareness. This builds on the existing studies that focus on Twitter alone, showing that integratinginformation from multiple online sources can produce useful analysis.Keywords: Twitter, Situational awareness, Event detectionIntroductionUpdates posted on social media platforms such as Twittercontain a great deal of information about events in thephysical world, with the majority of topics discussed onTwitter being news related [1]. Twitter can therefore beused as an information source in order to detect realworld events. The content and metadata contained in thetweets can then be leveraged to describe the events andprovide context and situational awareness. Applicationsof event detection and summarisation on Twitter haveincluded the detection of disease outbreaks [2], naturaldisasters such as earthquakes [3] and reaction to sportingevents [4].Using the Twitter stream for event detection yields avariety of advantages. Normally in order to automaticallydetect real-world events a variety of official and media*Correspondence: nicholas.thapen@imperial.ac.ukInstitute for Security Science and Technology, Imperial College London,Exhibition Road, London, UKsources would have to be tracked. These are usually pub-lished with some lag time, and any system monitoringthem programmatically would require customisation foreach source since they are not formatted in any standardway. Twitter provides a real-time stream of informationthat can be accessed via a single API. In addition a richvariety of sources publish information to Twitter, since itis a forum both for the traditional media and for a newerbrand of citizen journalists [5]. Tweets also contain meta-data that can be mined for information, including locationdata, user-supplied hashtags and user profile informationsuch as follower-friend relationships. The primary draw-back of using Twitter is that it is an unstructured sourcethat contains a great deal of noise along with its signal.Tweets can be inaccurate as a result of rumour, gossip oractive manipulation via spamming.In this paper we apply existing bio-surveillance algo-rithms, which are those used to detect outbreaks of illness,to detect candidate events from the Twitter stream. We© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 2 of 14employ customised filtering techniques to remove spu-rious events. We then extract the terms from the eventtweets which best characterise the event and aremost effi-cacious in retrieving related news. These terms, in theform of unigrams and bigrams, are used to filter and rankthe most informative tweets for presentation to the useralong with the most relevant news articles. Where thenews articles cover the exact event being discussed onTwitter they act as direct confirmation and explanationfor the event. Where a Twitter event has not yet been cov-ered in the news media related background articles canstill provide additional context.Our techniques are evaluated using two case studies,both using a dataset of geo-located tweets from EnglandandWales collected in 2014. The primary case study is thedetection of illness outbreak events. We then generaliseour techniques to events strongly affecting Twitter sen-timent, such as celebrity deaths and big sports matches.We evaluate our event detection using ground truth datain the form of health practitioner and news reports. Thesituational awareness techniques are evaluated by com-parisons to existing term extraction methods and human-coded event explanations.BackgroundMuch of the work on event detection using social mediahas focused on using topic detection methods to iden-tify breaking news stories. Streaming document similaritymeasures [6], [7] and online incremental clustering [8]have been shown to be effective for this purpose. Thesemethods have no concept of location, and focus purely onpicking up distinct events being discussed in the generalstream of Twitter data.Other approaches have aimed to pick up more localisedevents. These have included searching for spatial clus-ters in tweets [9], leveraging the social network structure[10], analysing the patterns of communication activity[11] and identifying significant keywords by their spatialsignature [12].In the field of disease outbreak detection efforts havemostly focused on tracking levels of influenza by com-paring them to the level of self-reported influenza onTwitter, in studies such as [13] and [14]. Existing dis-ease outbreak detection algorithms have also been appliedto Twitter data, for example in a case study [15] ofa non-seasonal disease outbreak of EnterohemorrhagicEscherichia coli (EHEC) in Germany. They searched fortweets from Germany matching the keyword EHEC, andused the daily tweet counts as input to their epidemicdetection algorithms. Using this methodology an alert forthe EHEC outbreak was triggered before standard alert-ing procedures would have detected it. Our study uses amodified and generalised version of this event detectionapproach.Diaz-Aviles et al. also attempted to summarize out-break events by selecting the most relevant tweets, using acustomized ranking algorithm. Other studies which havesummarised events on Twitter by selecting the most rele-vant tweets include [4] and [16]. Analysis of using Twitterfor situational awareness has been carried out in [17]and [18].There have been fewer related works on linking or sub-stantiating events detected from Twitter with traditionalnews media. One study [19] analysed various methodsof contextualizing Twitter activities by linking them tonews articles. The methods they examined included find-ing tweets with explicit URL links to news articles, usingthe content of tweets, hashtags and entity recognition.The best non-URL based strategy that they found wasthe comparison of named entities extracted from newsarticles using OpenCalais with the content of the tweets.MethodsProblem definitionOur definition of a real-world event within the context ofTwitter is taken from [8], with the exception that we haveadded a concept of event location. We are interested inonly those events that attract discussion on Twitter, sinceall others would be invisible to our methods.Definition 1. (Event) An event is a real-world occurrencee with (1) an associated time period Te and (2) a time-ordered stream of Twitter messages Me, of substantialvolume, discussing the occurrence and published duringtime Te. The event has a location Le where it took place,which may be specific or cover a large area, and the mes-sages have a set of locations LM1,. . . ,LMn which they weresent from.From the above definition, when given a time-orderedstream of Twitter messages M, the event detection prob-lem is therefore one of identifying the events e1,. . . ,enthat are present in this stream and their associated timeperiods Te and messages Me. It is also valuable to iden-tify the primary location or locations LMi that messageshave originated from, and if possible the event locationLe. The situational awareness problem is one of takingthe time period Te and messages Me and producing anunderstandable summary of the event and its context.OverviewOur approach to the event detection problem incorpo-rates location by detecting deviations from baseline levelsof tweet activity in specific geographical areas. This allowsus to track the location of messages relating to events, andin some cases determine the event location itself.In this paper we focus on two distinct types of event: Outbreaks of symptoms of illness, such as coughingor itching.Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 3 of 14 Events causing strong emotional reactions, such ashappiness or sadness.Initially the system was designed with disease outbreakdetection as the primary use case; this led to a systemdesign focused around keywords and aliases for theirkeywords, since a limited range of illness symptoms char-acterises most common diseases and the vocabulary usedto describe these symptoms is also relatively limited. Afterseveral iterations of this approach we noted that it couldbe viable as a general event detection and situationalawareness method, so we added another type of eventto determine the feasibility of the general approach. Wechose events causing strong emotions as a contrasting andless specific event type, with the intention of picking upa variety of localisable events such as important footballmatches and rock concerts.For each type of event we define a list of keywords,each describing a particular sub-type of that event. Forexample when looking at illness each keyword relates toa particular symptom, such as coughing or vomiting.When looking at events that cause emotional reactionseach keyword relates to a particular emotion. Each ofthese sub-type keywords is then expanded with a list ofaliases, synonyms and related terms to form a keywordgroup. For more details on how we identified the relevantkeywords and synonyms see the sections below.We track the number of tweets mentioning each key-word (consolidating all that lie in the same keywordgroup), in each of the geographical areas and use bio-surveillance algorithms to detect spikes in activity. Eachspike is treated as a potential event, and we use vari-ous criteria to single out those with a high probabilityof being actual events as defined above, i.e. those thatare caused by discussion of real-world occurrences onTwitter.Our situational awareness approach is based on iden-tifying terms from the event tweets which characterisethe events and using them to retrieve relevant news arti-cles and identify the most informative tweets. The newssearch uses metrics based on cosine similarity to ensurethat searches return related groups of articles.ArchitectureThe general approach can be described by the architecturein Fig. 1. Every new event type requires a list of keywordsand their associated aliases. Optionally a specific data pre-processing step can be included for the event type. Forexample in the health symptom case we employ amachinelearning classifier to remove noise (those tweets not actu-ally concerning health). These are the only two aspectsof the design that need to be altered to provide eventdetection and situational awareness to a new problemdomain.Fig. 1 Event Detection and Situational Awareness architecture: To apply to a new example a user needs to provide a keyword group list andoptionally a noise filter to remove tweets that do not strictly match the criteria of interestThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 4 of 14Event typesWe now go into a more detailed explanation of our eventtypes and how we formulated the keywords and associ-ated aliases. Each keyword group consists of a primarykeyword which is used to identify the group, e.g. vomit,and a number of aliases that expand the group, e.g. throw-ing up, being sick, etc. see Tables 2 and 3 for the full list ofkeyword groups.Illness symptomsTo build up a list of symptoms and related keywordswe searched Freebase for /medicine/symptom. Eachof these symptoms is defined as a keyword. They arereturned with a list of aliases that are then associated withthat keyword. This returned around 2000 symptoms. Inorder to filter down to a more manageable number wenext filtered these symptoms by their frequency in theTwitter data; any symptoms not appearing frequently inthis data would not produce enough activity to gener-ate events for analysis. All symptoms with fewer than 10mentions in the Twitter data were removed from the can-didate list. This excluded a large proportion of symptoms,reducing the set to around 200.We further limited the set by removing symptomsnot related to infectious diseases. We also added pri-mary keywords and aliases for some common condi-tions such as hayfever and flu. This step resulted in areduction to the 46 symptoms which formed our search.The average number of aliases per primary keywordwas 3.8.Emotion statesFor a list of emotion states and associated keywords weused the work of Shaver et al. They conducted research[20] to determine which sets of words were linked toemotions and how these cluster together. We took thesix basic emotions identified in the work as primary key-words: love, joy, surprise, sadness, anger and fear. Shaverswork associated each of these with a list of terms to forma tree. We took the terms from lower leaves on the tree foreach emotion as our alias sets (see Table 1 for examples).The average number of aliases per primary emotion key-word was 7.3. The only alteration we made was that aftersome initial analysis we discovered that the term happyfrom the joy category was a very strong signal of specialTable 1 Selected emotion keyword groups and some of theiraliasesKeyword AliasesSurprise Amazed, astonished, surprised...Sadness Depressed, unhappy, crying...Joy Glad, delighted, pleased...events such as Valentines Day, Mothers Day and Easter.It was also very often used on a daily basis due to peo-ple offering birthday greetings. We therefore separatedhappy into its own category separate from joy.In addition we employed SentiStrength [21], a sentimentanalysis tool, to classify our tweets into positive and nega-tive emotional sentiments.We took those tweets classifiedas being very positive and very negative as additionalcategories.Data collectionUsing Twitters live streaming API we collected geo-tagged tweets between 11th February 2014 and 11thOctober 2014. Tweets were collected from within a geo-graphical bounding box containing England and Wales.Retweets were excluded due to our focus on tweets as pri-mary reports or reactions to events. This resulted in adata-set of 95,852,214 tweets from 1,230,015 users. 1.6 %of users geo-tag their tweets [22], so our data is a lim-ited sample of the total tweet volume from England andWales during this period.We chose to use only geo-taggedtweets since they contain metadata giving an accuratelocation for the user. This allows us to locate each tweetwithin our geographical model. In total we found 240,928matches for our symptom keywords in the set of tweetsclassified as health-related, and 20,570,753 matches forour emotion keywords. See Tables 2 and 3 for details.Location assignmentOurmethodology relies on the collection of baseline levelsof tweet activity in an area, so that alarms can be triggeredwhen this activity increases. We therefore amalgamatedthe fine-grained location information from the geo-codedtweets by assigning them to broader geographical areas.We used a data driven approach to generate the geograph-ical areas rather than using administrative areas such astowns or counties. This technique allowed us to selectonly those areas with a minimum level of tweet activity,and also did not require any additional map data. It wouldtherefore be be reusable for any region or country with asufficient level of Twitter usage.We began by viewing a sample of the collected tweetsas geo-spatial points. Viewed on a map these clearlyclustered in the densely populated areas of England andWales. We therefore decided to use a clustering algo-rithm on these points in order to separate out areas forstudy. We employed the Density-Based Spatial Cluster-ing of Applications with Noise (DBSCAN) algorithm [23]for clustering, as this does not require a priori knowledgeof the number of clusters in the data. The features pro-vided to DBSCANwere the latitudes and longitudes of thetweets.The clusters produced by the algorithm matched themost populated areas, corresponding to the largest citiesThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 5 of 14Table 2 Tweets matching each symptom keyword groupSymptom Number of tweetsHeadache 42947Vomit 30429Hayfever 24175Sore throat 21744Pain 15142Malaise 12354Flu 10913Cough 9589Tonsillitis 7283Common cold 6768Infection 5955Abdominal pain 5582Sneeze 5131Asthma 4457Shortness of breath 4037Earache 2990Nasal congestion 2930Tremor 2727Itch 2410Anxiety 2250Fever 2198Nosebleed 1971Faint 1944Skin rash 1633Cramp 1444Diarrhea 1365Chest pain 1293Swollen gland 1138Conjunctivitis 941Stinging sensation 891Bleeding 854Chickenpox 835Runny nose 785Swelling 692Meningitis 641Pneumonia 622Seizure 413Constipation 389Palpitation 360Norovirus 239Neck pain 203Scarlet fever 142Dehydration 68Dysentery 28Tearing 16Dry mouth 10Table 3 Tweets matching each emotion keyword groupEmotion Number of tweetsVery negative 5716797Love 4943706Very positive 3823994Joy 2129279Happy 1613447Anger 1228890Sadness 562193Fear 395770Surprise 156677in the UK as shown in Fig. 2. They also separated mostcities into distinct clusters (a notable exception being theconglomeration of Liverpool and Manchester). In total 39clusters were created for England andWales and each wasgiven an ID and a label. We then created a convex hullaround each cluster, providing a polygon that can be usedto check whether a point is in the cluster or outside it.Points outside all of the clusters were assigned to a specialnoise cluster, and not included in the analysis. Overall80 % of tweets were assigned to specific clusters and theremainder to noise, giving us good coverage of geo-taggedtweets using our cluster areas.Tweet processingAs tweets are received by our system they are processedand assigned to the symptom and emotion state classesif they contain one of the relevant keywords. They areassigned a location by checking whether they fall into oneof our cluster areas.For the illness symptoms we introduce a noise removalstage at this point. It is particularly relevant for this classof events because there are many fewer tweets relatingto illness than showing emotion states. This means thatthe signal is more easily blocked out by random noise.To remove noise we construct a machine learning classi-fier with the aim of removing tweets containing alterna-tive word usages or general illness discussion rather thanreporting of illness events. The classifier therefore classi-fies tweets into those that are self-reports of illness andthose that are not. The classifier we use is a linear SVMtrained on a semi-supervised cascading training set, cre-ated on the principles described in Sadilek et al. [24]. Ourclassifier uses the LibSVM [25] library, and was initiallytrained on 4600 manually classified tweets. It achieves aclassification accuracy of 96.1 % on a held out test set of920 manually classified tweets.The number of tweets assigned to each class in each areaare then saved on a daily basis. These counts are first nor-malised to take account of Twitters daily effect pattern,Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 6 of 14Fig. 2 UK population density (left) compared to a sample of geo-located tweets (centre) and the clusters found (right). Note that only clusterslocated in England and Wales were used in this study. Contains Ordnance Survey data c Crown copyright and database right, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=26070175which shows more tweeting on weekends than weekdays.Event detection is run daily since we are attempting topick up temporally coarse-grained events. Disease out-breaks take weeks to develop, and events that shift publicsentiment or emotion will generally take hours or days tounfold.Detecting eventsOur event detection methodology leverages considerableexisting syndromic surveillance research by using an algo-rithm designed and developed by the Centers for DiseaseControl and Prevention (CDC), the Early AberrationReporting System (EARS) [26].Definition 2. (Alarm) An alarm is an alert produced bythe first stage of our event detection system. The alarmhas an associated keyword group and location. It also hasa start and end date, and associated tweet counts for eachdate within this period. When certain criteria are met analarm is deemed to be an event.We employ the C2 and C3 variants of EARS. These algo-rithms operate on a time series of count data, which in ourcase is a count of daily symptomatic tweet activity. The C2algorithm uses a sliding seven day baseline, and signals analarm for a time t when the difference between the actualcount at t and the moving average at t exceeds 3 standarddeviations. The C3 algorithm is based on C2, and in effecttriggers when there have beenmultiple C2 alarms over theprevious 3 days.These C2 and C3 candidate alarms are then groupedtogether so that alarms for the same keyword group andarea on consecutive days are treated as a single alarm. Analarm is therefore made up of one or more days, each withan observed count of tweets. An alarm ends when the C2and C3 algorithms no longer signal an outbreak occurring.Some of our Twitter count time series data is zero-skewed and non-normal, since the number of geo-taggedusers reporting illness can be low. The number of stan-dard deviations from the mean used in the C2 and C3algorithms can be an unreliable measure of central ten-dency in those circumstances. Hence to determine howfar above general baseline activity an observed countis we employ the median of the series to date and itsMedian Absolute Deviation (MAD) to produce a newmetric of alarm severity. Here the series is defined as allof the previous observed counts for the keyword groupand location in question. The number of Median Abso-lute Deviations from the median, ?, gives a comparablefigure across alarms as to how sharp a rise has been overexpected levels. This figure is produced from the followingequation:? = (observation ? median)/MAD (1)We then find the highest metric for an alarm, ?max,by finding the highest value of ? within the observationsmaking up the alarm.?max = argmax?(observations in alarm) (2)Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 7 of 14The ?max is the primary statistic which we use to deter-mine which events are real and which have just beengenerated by random noise.Another statistic which we employ in order to filter outnoise is the tweet-user ratio. This is the ratio of tweets inan event to that of distinct users involved in an event. Ahigh value of this statistic would imply that some usershave tweeted a large number of times across a short timeperiod, which is an indication that they may be spammersand that the alarm is spurious.In summary, we use the output from EARS to producealarms. We filter the alarms to a set of high likelihoodevents by using the ?max and tweet-user ratio parameters.From this point we refer to those alarms that are high-likelihood as events, according to our earlier event defi-nition. The alarms have an associated stream of Twittermessages and a location given by the node which theyoccur in. The following situational awareness results showthat the Twitter messages in these alarms discuss real-world occurrences, therefore fulfilling all of our definition.Situational awarenessOnce an event has been identified our next objective is toautomatically provide additional context for it, which mayprovide an explanation of the underlying cause. A humaninterpreter could achieve this by reading all of the tweetsand synthesizing them into a textual explanation, whichmight be some text such as People reacting to the deathof Robin Williams. We do this in two main ways: by pro-viding themost representative tweets from those that trig-gered the alarm, and by linking to relevant news articles.The steps involved in the Terms, News and Tweets (TNT)Event Summarisation process are detailed in Algorithm 1.The steps and terminology are then explained in moredetail.1© The first step is to retrieve the relevant tweets fromthe processed tweet and alarm databases. Tweets arefetched for both the alarm gist and from a historical base-line. 3©We discard those events with fewer than 30 tweetsas we found that they did not contain sufficient data toproduce good summarisation results.Definition 3. (Gist) The gist consists of the tweets for thetime period of the event which match the events keywordgroup and area.Definition 4. (Baseline) The baseline consists of thetweets for the same keyword group and area as an eventfrom the 28 days prior to that event.5© The next task is to find unigrams and bigrams thatare more prevalent in the gist than in the baseline. Theseare likely to come from tweets discussing the event andwill thus be characteristic of the event. We first extract themost common unigrams and bigrams from both sets oftweets, after removal of stopwords. Our list of stopwordsAlgorithm 1 Terms, News and Tweets (TNT) Event Sum-marisation1© Fetch gist tweets and baseline tweets2© if ??gist tweets?? < 30 then3© Do not attempt to summarise event4© else5© Extract unigrams and bigrams appearing in atleast 5% of the gist tweets6© for all ngrams extracted do7© Perform Fishers Exact Test to determinewhether ngram is significantly more likely to appearin gist than baseline8© for Top 2 most significant unigrams and bigramsand the primary keyword do9© Search news database using ngram for thealarms date range and return the top 10 documents10© Compute PCSS for documents returned11© for ngrams with PCSS values above threshold do12© Compute title similarity PCSS between ngramdocuments and those for each other ngram13© Good search terms ? term with title similar-ity PCSS above threshold14© Good articles ? documents returned from goodsearch terms15© Filtered tweets? tweets containing a good searchterm16© Rank good articles by cosine similarity to averagevector of good news articles17© Rank filtered tweets by cosine similarity to averagevector of filtered tweetsincludes a standard list, plus the 200 most frequent wordsfrom our tweet database.We select all non-stopwords thatappear in at least 5 % of the tweets.7© We then do a Fishers Exact Test to determine whichof the common unigrams and bigrams in the gist appearsignificantly more frequently (? < 0.05) here than in thebaseline set. Our candidate terms are the top two mostsignificant unigrams and bigrams. We select the top twoas this was found to give the best results on our testexamples. To this set we append the primary keyword thattriggered the alarm.9© For this research Google was used as the newsdatabase. Using the candidate terms we perform a searchon Google for documents published in the United King-dom during the time period of the alarm. Due to GooglesTerms of Service this step was performedmanually. A fullyautomated system would replace this step with a search ofa news database, which could be created by pulling downnews articles from RSS feeds of major content providers.10© We take the first 10 documents retrieved for eachsearch term, remove stopwords and apply stemming usingThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 8 of 14a Lancaster stemmer. We then convert each documentinto a Term Frequency/Inverse Document Frequency(TF/IDF) vector. In order to determine whether the searchterm has retrieved a coherent set of related documents wedefine a metric based on cosine similarity, the PairwiseCosine Similarity Score (PCSS): The Pairwise Cosine Similarity Score of a group ofTF/IDF vectors is calculated by taking the cosinesimilarity between each pair of vectors and addingthem to a set. The standard deviation of this set issubtracted from its mean to form a score.The PCSS rewards articles which are similar andpenalises any variance across those article similarities.This reduces the effect of some articles being stronglyrelated in the document set and others being highly unre-lated. Any term which retrieves a set of documents with ascore below a threshold value is not considered further.It is possible for a search term to hit on a coherentset of documents purely by chance, perhaps by find-ing news articles related to another event in a differentpart of the world. In order to guard against this weinstitute another check to ensure that the set of docu-ments returned from a search term is sufficiently closelyrelated to the set returned from at least one other searchterm.12© In order to perform this check we compare the titlesof the articles returned from the two different searchesusing a similar process to our earlier document compar-ison. We found it more effective to compare titles thanwhole documents, since sets of documents with similartopics can contain similar language even for fairly unre-lated search terms. For example the terms ebola andflu will both return health-related documents contain-ing similar language, but we would not wish to say thatthese search terms are related. To convert the titles toTF/IDF vectors we remove stopwords but do not applystemming. Since the titles are so short we include all uni-grams, bigrams and trigrams in the vector representation.We then compute a PCSS between the two documentsets, pairing each document in the first set with each inthe second and vice versa. 13© A search term must berelated to at least one other term for it to be used goingforward.14© Once TNT has identified good search terms we thenreturn the news articles fetched using those terms. 16© Inorder to rank the top news articles for a search we takethe mean TF/IDF vector of the articles. and then rank thearticles by cosine similarity to this mean vector. We returnthe top ranked articles from each search term.17© To select the summary tweets for an event we firstlydetermine the set of tweets to consider and then choosethe most relevant tweets within that set. The set of tweetscan either be: 1) All tweets in the gist. 2) The gist tweets containing one of the extractedterms. 3) The gist tweets containing one of the good searchterms (as determined by the TNT algorithm).1) is always available and is labelled the Gist Top Tweets(GTT). If the TNT algorithm has found terms that aresignificantly different in frequency from the baseline thenset 2) is available for use and if terms from that set havegood newsmatches then set 3) can be used. The SummaryTop Tweets (STT) are from set 3) if it exists and fallbackto set 2) if the good news match terms are not available. Ifno terms were found to be significantly different from thebaseline then only the GTT is available.In order to choose the top tweets we rank them by theircosine similarity to the mean TF/IDF vector of all tweetsin the set, an approach similar to that of [4]. This attemptsto finds tweets which capture and summarise the aggre-gate information of all of those in the set. The top fivetweets ranked by this measure are returned.Results and discussionCandidate event selectionOver the course of the study the bio-surveillance algo-rithms generated 820 disease-related alarms and 2021emotion-related alarms. A brief survey of these revealedthat many were false alarms generated by random fluctu-ations in the noisy social media data. In order to separateout alarms that could be labelled as events with highconfidence we conducted the following analysis.Firstly we compiled an initial set of 13 focus examplealarms. These were taken from events that the authorsknew had happened in the evaluation time period andfrom those alarms in our dataset with low and high valuesof ?max.The most important threshold parameter in the contextof the event detection is the ?max figure which mea-sures the deviation of the alarm counts from the medianlevel. Examining the distribution of the number of alarmsfor each value of ?max revealed that it started to tail offsharply at ?max ? 5. The distribution of alarms for eachvalue of ?max is shown in Fig. 3. We therefore took this asa value to segment additional test examples, drawing tenmore at random with a ?max less than 5 and ten with a?max greater than or equal to 5. The resulting evaluationset of 33 candidate events is shown in Table 4. The eventID used to refer to the events is composed of the first twoletters of the event keyword followed by a 12 letter areacode. The final part of the ID is the day and month of theevent start date.Thapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 9 of 14Fig. 3 Alarms detected with differing values of ?maxEvent detection evaluationIt is difficult to provide a completely automated evaluationprocedure for detecting previously unknown events. Diazet al. used the time to detection on a known outbreak astheir evaluation criterion [15]. In our case we do not knowa priori that these are genuine outbreaks or events. Hencewe need to make an assessment of the alarms producedto see what they refer to and if there is a way of exter-nally verifying that they are genuine events. For all 33 ofthe selected alarms the authors read the tweets and deter-mined whether they described a real world event. Thecoders found 26 YES answers, 5 NO answers and 2 DIS-AGREED answers, producing a 94 % agreement. Wherean event was present they wrote a short summary.For external verification of events two different meth-ods were used, depending on whether the event wassymptom-related or emotion-based. For symptom relatedevents the activity spike was checked against officialsources for the same time period. The General Prac-titioner (GP) in hours bulletin for England and Wales[27] was used and an event was deemed verified if thesymptom exhibited an increasing trend for that period.Table 4 Evaluation set of eventsID Event ?max Keyword Node ID Event ?max Keyword NodeSAL-11-08 YES 20 Sadness London HFB-10-04 YES 5 Hayfever BirminghamHFM-01-06 YES 19 Hayfever Manchester VOL-20-04 YES 5 Vomit LondonSAL-07-04 YES 14 Sadness London SAC-05-05 YES 5 Sadness CardiffFEL-18-07 YES 13 Fear London HFL-04-07 NO 5 Hayfever LondonASL-02-04 YES 12 Asthma London FLB-23-09 NOa 5 Flu BirminghamFLP-06-10 YES 11 Flu Portsmouth VPBR-10-05 YES 4 VeryPos BristolHAM-02-04 YES 9 Happy Manchester FRL-30-05 YES 4 Fever LondonHAM-18-04 YES 9 Happy Manchester FLM-19-09 YES 4 Flu ManchesterSAL-08-07 YES 8 Sadness London VOL-22-02 NO 3 Vomit LondonHALE-01-08 YES 8 Happy Leeds HFB-29-04 NO 3 Hayfever BirminghamHFL-14-05 YES 7 Hayfever Leeds JONO-23-02 YES 2 Joy NorwichSUN-29-08 YES 7 Surprise Newcastle HEM-06-03 NO 2 Headache ManchesterITL-08-06 YES 6 Itch London SUC-23-05 NO 2 Surprise CardiffSAB-09-06 YES 6 Sadness Birmingham SUL-16-08 NO 1 Surprise LondonHABE-01-03 YES 5 Happy Bridgend FEBR-17-04 NO 0 Fear BristolSAL-21-03 YES 5 Sadness London STL-26-08 NO 0 Sore Throat LondonHFC-09-04 YES 5 Hayfever CardiffShows whether events were externally verifiable and their ?max valueaNote: this event not confirmed by the GP in hours report of that week. However, the following week showed an increase and it is possible that social media detectedincreased Influenza activity before this was confirmed by GP visitsThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 10 of 14This detail is noted in the summary document pro-duced by Public Health England for that reporting period.Emotion-based events were verified by checking if therewere any articles (via Web search) that could corroboratethe cause of the event (as given by the summary).We manually investigated all examples from the initialfocus set and found initial parameters for the score func-tions in our algorithms that worked reasonably well. Theseprovided possible ranges of values which were evaluatedmore systematically over the entire alarm set. For eventdetection we evaluated which alarms were flagged asevents by the system for each parameter value againstwhether those events were externally verifiable. The finalevaluation for all algorithms contains all 33 of the alarmsin both sets, not just the twenty expanded test examples.To determine if an alarm is an event that we should beconcerned about we consider two properties of the alarm.The first is the tweet-user ratio. From exploratory testingwe found a value of 1.5 separated our spam and genuinealarms very well, leaving only a small number of alarmswith large tweet sets and some spam. The spam detectionproblem should be straightforward and will be addressedmore completely in future work.The second figure which gives the strength of the activ-ity above the usual baseline is the ?max figure. This isthe essence of the modified EARS algorithm and thevalue of this figure should generally separate events fromnon-events.The criterion for selecting the best threshold for ?maxis context dependent. We have used the balanced measurefor this scenario as that is a fair representation of both pre-cision and recall. For each threshold value of ?max testedthe classification success and error types are: True positive: instances at or above the thresholdthat are verified events False positive: instances at or above the thresholdthat are not verified events True negative: instances below the threshold thatare not verified events False negative: instances below the threshold thatare verified eventsThe precision, recall and F1 values for all the testedvalues of ?max are displayed in Fig. 4. All figures were cal-culated with reference to the set of 33 example events dis-cussed above. The maximum F1 value, 0.9362, is observedat ?max ? 4, so this is a well balanced threshold andthe recommended parameter. Those seeking higher con-fidence events (willing to accept that some events may bemissed) could use a value of 6 for this parameter whichyields a precision of 1. The maximum observed recallvalue is at the minimum parameter value and is not veryinformative. Essentially it says that everything is an eventand hence does not produce any false negatives.In summary the event detection mechanism based onthe EARS C2 and C3 algorithms with the addition of the?max and tweet-user ratio was found to perform well atdetecting events that could be externally verified as gen-uine. The recommended ?max parameter (4) produced agood balance of precision and recall in our sample set. Itmust be noted however that we cannot gain a true pictureof the overall recall of the system, since we have no wayof analysing the number of genuine events that were notpicked up.Situational awareness evaluationBoth situational awareness components were evaluated.Firstly the news linkage was tested to see whether rele-vant news was retrieved for the sample events. As part ofthis analysis we compared our method of extracting infor-mative search terms (the TNT algorithm) with a compa-rable automated technique. Secondly the tweet rankingwas validated to determine whether highly ranked tweetseffectively summarised the events.Comparative news linkage evaluationThe news linkage component works by selecting goodsearch terms for articles based on the TNT algorithm.Within this there is a term extraction step to generatesearch terms, and then a filtering step using PCSS toremove terms which retrieve unrelated sets of articles.We iterate over different threshold values for the PCSSscore to find the optimum, using an F0.5 measure asthe evaluation criterion. F0.5 was selected because preci-sion was judged to be more important than recall in thissetting. As a further evaluation we compare the resultsof replacing our term extraction algorithm with LatentDirichlet Allocation (LDA). LDA is a popular topic mod-elling technique that extracts sets of terms characterisingeach topic in a group of documents. The success and errortypes used to compute the F0.5 measure are: True positive: relevant news returned fornewsworthy event False positive: news returned for an event with nogenuine news True negative: no news returned for an event withno genuine news False negative: no news returned for newsworthyeventThe evaluation is presented in Figs. 5 and 6 as well asthe different levels of article PCSS that were iterated overto find the maximum F0.5 value in a step-wise procedure.It is clear from these images that the TNT algorithm has ahigher F0.5 at all tested values of the article PCSS, due toits higher recall. The outcome of the parameter selectionprocess was that a PCSS threshold of ?0.08 produced thebest results. Using this value the F0.5 was 0.79, showingThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 11 of 14Fig. 4 ?max event detection parameter selectionthat our system was successful in retrieving relevant newsfor the sample events.Selecting top ranked relevant news articles is one part ofour situational awareness contribution. The second is theselection of tweets that provide a representative summaryof an event.Top ranked tweets evaluationWe have employed two evaluations for the tweet rankingexercise: comparison to human-coded event explanationand comparison between GTT and STT. The human-coded event explanations were created by two of theauthors after reading through all of the tweets linked toeach event. There were 26 alarms that had an identifiablecause. The tweet ranking match (to human-coded eventassessment) performance is presented in Table 5. Thetweets were considered a full match if a human summaryof the 5 top ranked tweets would match the human-codedevent explanation for the whole set of tweets.The partial matches were: FRL-30-05 (Fever: London,May) and FLP-06-10 (Flu: Birmingham, October).These events had more than one explanatory cause. Cur-rently our algorithms work best in the single event case.The three cases that did not match were: JONO-23-02(Joy: Norwich, February), STL-26-08 (Sore throat:London, August) and SUN-29-08 (Surprise, Newcastle,August). The coders disagreed as to whether STL-26-08was actually an event. The remaining two examples wereFig. 5 News linkage accuracy from Terms, News, Tweets termsThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 12 of 14Fig. 6 News linkage accuracy from Latent Dirichlet Allocation termsnot summarised well by the significant tweets as they bothexhibited high disparity in terms used to describe a con-textually related event and SUN-29-08 also included anumber of spam tweets that distorted the results of TNT.The second evaluation for the tweet ranking exercisewas a comparison between the GTT and the STT. A qual-itative assessment of the tweets led to the conclusion thatSTT tweets were better in 11 out of 33 cases and therewas no significant difference between the two for 21 casesout of 33. In one case, FLP-06-10, the GTT included amention of flu jab (one of the manually selected terms)which the STT did not include. Hence the STT providedan improvement over ranking based off the alarm tweetsin one out of three instances.Notable examples discussionWe now discuss four example events that highlight thestrengths and limitations of our approach. These examplesare listed in Table 6.The first example case is JONO-23-02. From a read-ing of the tweets there were definitely some relating to aTable 5 STT tweet ranking evaluationMatch CountFull 21Partial 2No 3The STT tweet summary fully matched the human-coded event summarisation in21 cases. This yields a full match fraction of 0.81single event: Norwich City Football Club beating Totten-hamHotspur Football Club 1?0 in a football match. BothTNT and LDA term extraction failed to find terms repre-sentative of this event. This was due to the disparity of thelanguage used; the following example tweets should helpelucidate this point: #canarycall absolutely delighted with the win :) goodperformance, good result #yellows almost didnt go today glad i did so glad i chose to come today!#ncfcIt is difficult for a term-based solution to find any com-mon thread here. Finding the cause of this event wouldrequire contextual knowledge of football matches, teamnames and commonly employed aliases. The news linkagealgorithm did initially find a news story for the term joyon this date. The British Prime Minister let out a little cryof joy over David Bowie Scottish independence comments(Telegraph, Feb 24, 2014). The articles returned all con-cerned this story and were found to be closely related, butwere dropped from the news linkage because they did notmatch those returned from the other search terms. Thishighlights the benefits of searching with multiple termsand ensuring that the results are related.The second example is ASL-02-04. This event wasdue to increased levels of air pollution observed in Lon-don at the beginning of April, caused by a Saharan dustcloud. This event had a ?max of 12 indicating a signifi-cant increase in baseline activity for the alert period. It waswell summarised by all aspects of our situational aware-ness algorithm. The top ranked tweets provided by ourThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 13 of 14Table 6 Example cases and the terms extracted for themID TNT terms LDA termsJONO-23-02 Joy, enjoy Enjoy, glad, lossASL-02-04 Asthma, air pollution, smog, pollution Asthma, smog, pollution, attack airVOL-20-04 Vomit, chocolate, easter Chocolate, eaten, easter, vomit, headacheSAL-11-08 Sadness, robin williams, sad news, robin, williams Sad, robin, williams, rip, riprobinwilliamssummary method (STT) produced tweets more represen-tative of the event than those from all tweets in the gist.This is demonstrated by the top tweet selected by both: STT top tweet: i cant breathe #asthma #smog GTT top tweet: my asthma is literally so badHere selecting the top tweets from the filtered eventset captures tweets representative of the event as opposedto the baseline illness activity. The news linkage for thisexample worked well, with all five of the top selectedarticles being representative of the event. The top article,Air pollution reaches high levels in parts of England -BBC, gives the cause of the event in the first few lines:People with health problems have been warned to takeparticular care because of the pollution - a mix of localemissions and dust from the Sahara.The third case is VOL-20-04. Reading the tweetsmakes it clear that this one day event is caused by peo-ple feeling sick after eating too much chocolate on EasterSunday. In this case the TNT summary and all tweetranking return similar tweets as there is little baselineactivity and that baseline activity is not strongly related.The top tweets from both sets therefore both producegood summaries: STT top tweet: seriously i feel sick having all thischocolate GTT top tweet: eaten too much chocolate feel sickWhile the top ranked tweets are similar the event tweetfiltering does remove baseline tweets referring to generalillness. No good news searches were found in this case.This event may be valid in the context of social media butit is not newsworthy.The fourth example is SAL-11-08 which is the UKTwitter reaction to the death of Robin Williams. Thesetweets from the sadness keyword group exhibit both thehighest ?max (20) and the highest overall tweet countfor any single event (4472). The prominence of celebritydeaths within our detected events mirrors earlier find-ings [6]. As with all of our high ?max events the TNTtweet ranking and news linkage work well. The top newsarticle returned is an article reporting the death of Mr.Williams: Robin Williams dies aged 63 in suspected sui-cide (Telegraph, August 12, 2014). The top five rankedtweets by TNT tweet filtering are better than those rankedon all tweets as they remove baseline general sadnesstweets from the ranking: STT top tweet: rip robin williams. sad day GTT top tweet: yep , very sadConclusionsWe have presented techniques for event detection and sit-uational awareness based on Twitter data. We have shownthat they are robust and generalisable to different eventclasses. New event classes could be added to this systemsimply by producing a list of keywords of interest andan optional noise filter. Our event detection is based onthe EARS bio-surveillance algorithm with a novel filteringmechanism. The maximum Median Absolute Deviationsfrom the median provides a robust statistic for determin-ing the strength of relative spikes in count-based timeseries. As it is based on the median, this measure handlescases where data is non-normal as was the case for some ofour symptom based geo-tagged tweets. The event detec-tion approach achieved an F1 score of 0.9362 on our eventexamples.By filtering to words that are significantly different (? <0.05) in frequency from baseline levels we have extractedterms to search news sources for related articles. Wheregood news matches are found these revise our eventterm list. We have created two novel algorithms that pro-vide additional situational awareness about an event fromthese event terms. The baseline tweet activity thus pro-vides valuable context in allowing the character of thedetected event to be discerned.Firstly, we rank the filtered set of news articles to pro-duce the top five representative articles. The news linkage,weighted towards precision, achieved an F0.5 score of 0.79on our example set, with no false positives.Secondly, we produce a top five ranked list of tweetsthat summarise an event. These ranked tweets are cal-culated from the tweet set, filtered by those that containthe extracted event terms. The top ranked tweets fullymatched our human-coded event summaries in 21 out of26 cases.In future work we aim to improve our news linkagealgorithm with a final step checking whether the arti-cles returned are similar to the event tweets, using cosineThapen et al. Journal of Biomedical Semantics  (2016) 7:61 Page 14 of 14similarity or other features such as entities identifiedin the news articles. Additional improvements to eventdetection would lie in improving spam detection andadding sentiment classification to our emotion exampleas a classifier. Collecting data over longer time periodswould also allow us to look into using bio-surveillancealgorithms which require seasonal baseline information.AcknowledgementsThis research was carried out in cooperation with the UK Defence Science andTechnology Laboratory. It was funded by the U.S. Department of DefensesDefense Threat Reduction Agency (DTRA), through contractHDTRA1-12-D-0003-0010.Authors contributionsNT and DS jointly undertook the data collection, statistical analysis and writingthe manuscript. CH conceived of the study, and participated in its design andcoordination as well as helping to draft the manuscript. All authors read andapproved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Received: 2 December 2015 Accepted: 20 September 2016Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 DOI 10.1186/s13326-016-0091-zRESEARCH Open AccessSupporting the analysis of ontologyevolution processes through the combinationof static and dynamic scaling functions inOQuaREAstrid Duque-Ramos1, Manuel Quesada-Martínez1, Miguela Iniesta-Moreno1,Jesualdo Tomás Fernández-Breis1* and Robert Stevens2AbstractBackground: The biomedical community has now developed a significant number of ontologies. The curation ofbiomedical ontologies is a complex task and biomedical ontologies evolve rapidly, so new versions are regularly andfrequently published in ontology repositories. This has the implication of there being a high number of ontologyversions over a short time span. Given this level of activity, ontology designers need to be supported in the effectivemanagement of the evolution of biomedical ontologies as the different changes may affect the engineering andquality of the ontology. This is why there is a need for methods that contribute to the analysis of the effects ofchanges and evolution of ontologies.Results: In this paper we approach this issue from the ontology quality perspective. In previous work we havedeveloped an ontology evaluation framework based on quantitative metrics, called OQuaRE. Here, OQuaRE is used asa core component in a method that enables the analysis of the different versions of biomedical ontologies using thequality dimensions included in OQuaRE. Moreover, we describe and use two scales for evaluating the changesbetween the versions of a given ontology. The first one is the static scale used in OQuaRE and the second one is anew, dynamic scale, based on the observed values of the quality metrics of a corpus defined by all the versions of agiven ontology (life-cycle). In this work we explain how OQuaRE can be adapted for understanding the evolution ofontologies. Its use has been illustrated with the ontology of bioinformatics operations, types of data, formats, andtopics (EDAM).Conclusions: The two scales included in OQuaRE provide complementary information about the evolution of theontologies. The application of the static scale, which is the original OQuaRE scale, to the versions of the EDAMontology reveals a design based on good ontological engineering principles. The application of the dynamic scale hasenabled a more detailed analysis of the evolution of the ontology, measured through differences between versions.The statistics of change based on the OQuaRE quality scoresmake possible to identify key versions where somechanges in the engineering of the ontology triggered a change from the OQuaRE quality perspective. In the case ofthe EDAM, this study let us to identify that the fifth version of the ontology has the largest impact in the qualitymetrics of the ontology, when comparative analyses between the pairs of consecutive versions are performed.Keywords: Ontology quality, Ontology metrics, Oquare, Ontology repositories*Correspondence: jfernand@um.esEqual contributors1Universidad de Murcia, IMIB-Arrixaca, Campus de Espinardo, 30071 Murcia,SpainFull list of author information is available at the end of the article© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 2 of 20BackgroundIn recent years the biomedical community has increasedits effort in the development of ontologies and this islikely to continue [1]. Ontology developers tend to pub-lish their ontologies on the Web and they are accessi-ble from different sources. BioPortal [2], for instance,contains more than 500 ontologies at the time of writ-ing and new content is published frequently. BioPortalenables updates by user submissions of new versions,which are accessible via web browsers and through webservices [2].The curation of ontologies is often a complex taskbecause of their high level of activity and rapid evolution[3]. For this reason, the number of versions of an ontologymay grow rapidly. The evolution process turns the devel-opment of an ontology into a dynamic process. Each ofthe different versions of an ontology constitutes a snap-shot of this process. The analysis of versions was intro-duced by [4], in which ontology versioning was definedas the ability to handle changes in ontologies by creatingand managing different variants of it and which pointedout the importance of highlighting differences betweenversions. Later, [5] claimed that a versioning system forontologies must compare and present structural changesrather than changes in text representation or sourcefiles. They described a version-comparison algorithmthat produces a structural difference between ontologies,which was presented to users through an interface foranalysing them [6]. As mentioned in [7], there is no dis-tinction between versioning and evolution in ontologiessince both account for the management of changes inontologies.If we approach ontology changes from a logical perspec-tive those changes are usually materialised by modifyingthe axioms of a given ontology. Those modifications mayimply the addition or removal of classes, properties, indi-viduals or constraints, as well as modifying the charac-teristics, domains and ranges of properties. Such numberand types of changes have been the inputs for differentapproaches that have tried to understand the evolution ofontologies: Bubastis [3, 8] analysed the degree of activity inbiomedical ontologies by considering 5 major typesof ontology changes between two consecutiveversions: added or removed axioms to an existingnamed class (NC), NCs added, NCs made obsoleteand edited annotation properties. Copeland et al. 2013 [9] focused on changes inasserted and inferred axioms taking into accountreasoning capabilities in ontologies [10]. In [11] a web application providing an interactive anduser-friendly interface to identify (un)stable regionsin large life science ontologies is proposed. A methodthat computes change intensities for regions based onchanges between several succeeding versions of anontology within a specific time interval is used.It makes sense to think that the changes made toan ontology across its different versions should have animpact on its quality. In addition, assuming that thechanges in an ontology should have a positive impact onthe quality of that ontology is also reasonable. In this con-text, the main contribution of this work is to the study ofthe evolution of ontologies from the perspective of ontol-ogy quality, since, to the best of our knowledge, this aspecthas not been significantly researched to date. The analy-sis of quality in ontologies has been addressed in differentways in the ontology evaluation community, such as in thefollowing works: Gangemi et al. 2006 [12] approached it as a diagnostictask based on ontology descriptions, using threecategories of criteria (structural, functional andusability profiling). Rogers 2006 [13] proposed an approach using fourqualitative criteria (philosophical rigour, ontologicalcommitment, content correctness, and fitness for apurpose). Yao et al. 2005, Tartir and Arpinar 2007 [14, 15]presented metrics for evaluating structural propertiesin the ontology. Duque-Ramos et al. 2011 [16] proposed OQuaRE,which adapts the SQuaRE standard for softwarequality evaluation for defining a qualitative andquantitative ontology quality framework.Our proposal is based on the OQuaRE Framework [16],which is a qualitative and quantitative ontology qualityframework. The OQuaRE is based on the standard forSoftware product Quality ISO/IEC 25000:2005 (SQuaRE)[17]. The application of SQuaRE (1) provides a compre-hensive specification and evaluation model for softwareproduct quality; (2) makes quality evaluation reproducibleand objective, based on observations; and (3) allows for acommon language for specifying user requirements thatis understandable by users, developers and evaluators. Allthese properties are desirable for an ontology quality eval-uation approach. Ontologies, conceived as a special kindof information object or computational artifact [18], havea series of shared notions with Object Oriented Design[19]. For example, the existence of classes, individualsand properties can be exploited to adapt Object Ori-ented Programming metrics to ontologies. This leads usto believe that the principles of SQuaRE can be adaptedto ontologies. Thus, the main goal of OQuaRE is to pro-vide an objective, standardised framework for ontologyquality evaluation, applicable to different ontology evalu-ation scenarios, in which the ontologies are evaluated asDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 3 of 20final products. For this, OQuaRE includes a generic scal-ing function that transform metrics values into qualityscores.In this work, we adapt OQuaRE for the purpose ofmeasuring the impact of the evolution of ontologies intheir quality. In [20], we described how OQuaRE couldbe used to evaluate the quality of the different versions ofthe ontology of Bioinformatics operations, types of data,formats, and topics (EDAM) [21]. The standard qualitymodel and metrics defined in OQuaRE were used and themethod was able to detect changes in the measured qual-ity of the different versions of the EDAM. The presentwork is an extension of [20], presenting methodologi-cal evolution and progress. First, we further formalisethe method to measure differences between versions ofthe same ontology based on the OQuaRE performance.Second, we take advantage of such a formalisation forproposing a more sensitive scaling function to be able todetect small differences between consecutive versions ofan ontology from the quality metrics perspective. Thiswill let OQuaRE to have two different scaling functions;one for evaluating ontologies and final products and onefor evaluating the different versions of a given ontology.The latter is used as feedback to adjust or define newprofiles of the static scale. Third, a statistical analysisof the relation of changes in OQuaRE with the profileof activity of the ontology is included. This extensionof the OQuaRE framework will allow a better under-standing of the evolution of ontologies from a qualityperspective and will contribute to demonstrating howontology quality methods can be used to study ontologyevolution.MethodsOQuaREOQuaRE is adapted from SQuaRE [17]. SQuaRE definesa quality model and the process for software productevaluation through five divisions: Quality Model, Qual-ity Measurement, Quality Requirements, Quality Evalua-tion and Quality Management. First, quality requirementsare identified. Second, the requirements are measuredusing a quality model, which is quantified through qual-ity metrics. These three divisions are used by the qual-ity evaluation division, which is managed by the qualitymanagement division. The usage of SQuaRE requires thedefinition of these five divisions. OQuaRE defines allthe elements required for ontology evaluation: evalua-tion support, evaluation process and metrics. OQuaREstructures the evaluation of the quality of an ontologyusing the four levels proposed by SQuaRE: quality require-ments, quality characteristics, subcharacteristics andmet-rics. OQuaRE uses the six quality characteristics proposedby SQuaRE for measuring quality: functional adequacy,reliability, operability, maintainability, compatibility, andtransferability. Besides, OQuaRE defines a new charac-teristic, structural, which accounts for the quality of thestructure of the ontology (see Table 1). Each quality char-acteristic has a set of associated quality subcharacteris-tics, which are measured through quality metrics. Thequality metrics are the units of measurement of qual-ity evaluation. The current version of OQuaRE has 49subcharacteristics and 14 metrics. Some OQuaRE sub-characteristics are reused and adapted from SQuaRE,but some others are specific to ontology evaluation. Forexample, the functional adequacy subcharacteristics areTable 1 OQuaRE characteristics and subcharacteristics used in our methodCharacteristic Description Associated subcharacteristicsStructural Formal and semantic relevant ontological properties thataccount for: the correct use of formal properties, clarity ofcognitive distinctions and appropriate use of ontologymodelling primitives and principlesformalisation, formal relations support, redundancy,consistency, tangledness, cohesionFunctionalAdequacyCapability of theontologies to be deployed fulfilling functionalrequirements, that is, the appropriateness for its intendedpurpose according to state-of-the art literature [22]reference ontology, controlled vocabulary, schema andvalue reconciliation, consistent search and query, knowledgeacquisition, clustering and similarity, indexing and linking,results representation, text analysis, guidance and decisiontrees and knowledge reuse and inferencingReliability Capability of an ontology tomaintain its level of performanceunder stated conditions for a given period of timerecoverability and availabilityOperability Effort needed for the ontology use. Individual assessment ofsuch use, by a stated or implied set of userslearnabilityCompatibility Capability of two ormore ontologies to exchange informationand/or to perform their required functions while sharing ahardware/software environmentreplaceabilityMaintainability Capability of ontologies to be modified for changes inenvironments, in requirements or in functional specificationsmodularity, reusability, analysability, changeability,modification stability and testabilityTransferability Degree to which the ontology can be transferred from oneenvironment (e.g., operating system) to anotheradaptabilityDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 4 of 20extracted from the intended uses for ontologies identi-fied in [22]. Following a bottom-up approach, OQuaREmetrics are combined in order to compose the subchar-acteristics, and the subcharacteristics are grouped by thecharacteristics. Tables 2 and 3 describe respectively howthe 14 OQuaRE metrics are calculated and how some ofthe metrics are associated with the subcharacteristics. Wehave not included all of them for simplicity, but they areavailable at [16, 23].The evaluation of an ontology comprises a score forthose requirements measured through the quality model.OQuaRE metrics reuse and adapt a set of well knownmetrics from both ontology evaluation and software engi-neering communities [14, 22, 24]. The quality metricsprovide quantitative values in different ranges, which arecalled raw quality metrics values. OQuaRE applies a scal-ing method recommended in SQuaRE that assigns valuesin the range [1,5] (5 levels): 1 - Not Acceptable 2 - Not Acceptable - Improvement Required 3 - Minimally Acceptable 4 - Acceptable 5 - Exceeds RequirementsTable 2 OQuaRE metrics and a brief description of how wecalculate themOQuaRE metric DescriptionANOnto Mean number of annotation properties per classAROnto Number of restrictions of the ontology per classesCBOnto Number of superclasses divided by the number of classminus the subclasses of ThingCROnto Mean number of individuals per classDITOnto Length of the largest path from Thing to a leaf classINROnto Mean number of subclasses per classNACOnto Mean number of superclasses per leaf classNOCOnto Mean number of the direct subclasses per class minusthe subclasses of ThingNOMOnto Mean number of object and data property usagesper classLCOMOnto Mean length of all the paths from leaf classes to ThingRFCOnto Number of usages of object and data properties andsuperclasses divided by the number of classes minusthe subclasses of ThingRROnto Number of usages of object and data properties dividedby the number of subclassof relationships and propertiesTMOnto Mean number of classes with more than 1 direct ancestorWMCOnto Mean number of properties and relationships per classLet us suppose that a user wants to evaluate the ontol-ogy requirement Multiple inheritance of an ontology,which might require to evaluate the Structural char-acteristic. This characteristic has 9 subcharacteristics,but only two will be used in this example (see Fig. 1)for simplicity, namely, Tangledness and Formal rela-tion support. The traceability from the OQuaRE qualitymetrics to the quality requirements is shown in Fig. 1.Tangledness depends on the TMOnto metric, whosevalue depends on the mean number of classes with morethan 1 direct ancestor, so two primitive measurements(number of classes and number of direct ancestors) areused for computing the raw value of the metric, whichin this example is 1.28. Raw values are transformedinto quality scores using a scaling function. The scalingmethod (see Table 4) is based on the recommendationsand best practices of the Software Engineering commu-nity for software metrics and ontology evaluation met-rics. For TMOnto, the scaling function transforms thisvalue into the quality score 5 because the raw value isin the range [1, 2]. Given that Tangledness has onlythe TMOnto metric associated, this is also its score.In case one subcharacteristic has more than one met-ric associated, its score would be the weighted mean ofthe quality scores of the metrics. In Fig. 1 we can seethat quality score for Formal relation support is 2, sothe score of the Structural characteristic is 3.5, that is,(5+2)/2.Adapting OQuaRE for ontology evolutionDefinitionsIn this section, we define a series of concepts related toontology evolution from the OQuaRE perspective.Definition 1 Versioned corpus of an ontology (vC? ): isa list of versions {vi} of the same ontology ? , where irepresents the chronological position of vi in vC? .The comparison of different versions of the same ontol-ogy highlights changes and commonalities between theversions [5]. The comparison can be done using metrics ofdifferent nature (real-valued metrics, factor, ordered fac-tors, etc.). In order to include all of them in a commoncontext, the method requires the adaptation of the met-rics, because they need to satisfy the constraints describedin Definition 2.Definition 2 Comparison criteria (f? ): is a discretisationframework that, for every version vi ? vC? , provides a vec-tor si of integers that can be used to rank those versions invC? .The number of components of the vector si is r. Forexample, if we use TMOnto as a unique comparisonDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 5 of 20Table 3 Summary of the associations between the characteristics, subcharacteristics and the associated metricsOQuaRE characteristic OQuaRE subcharecteristic OQuaRE metricStructural Formal relations support RROntoTangledness TMOntoCohesion LCOMOnto. . . . . .Functional adequacy Controlled vocabulary ANOntoInference RROnto, CROntoConsistent search and query ANOnto, RROnto, AROnto, INROntoKnowledge acquisition and representation ANOnto, RROnto, NOMOnto. . . . . .Maintainability Modularity WMCOnto, CBOOntoAnalysability WMCOnto, DITOnto, RFCOnto, NOMOnto, LCOMOnto, CBOOntoModification stability WMCOnto NOCOnto RFCOnto LCOMOnto CBOOnto. . . . . .Reliability Recoverability WMCOnto, DITOnto, NOMOnto, LCOMOnto,Availability LCOMOnto. . . . . .Operability Learnability WMCOnto, LCOMOnto, RFCOnto, NOMOnto, CBOnto, NOCOnto. . . . . .The associations of the reminding 36 subcharacteristics with metrics can be found at http://miuras.inf.um.es/oquarewikicriterion, f? discretises its real-value, using the qualityscore, to the range [1,5]. Moreover, in this case these inte-gers are related to the different qualitative levels definedby OQuaRE, although different levels could be used.Then, given two versions vi and vj, if f? produces the scores5 and 1 respectively, that means that vj is more tangledthan vi. Similarly, the remaining 13 metrics can be addedto the comparison criteria, and this is what we propose asa means to analyse the evolution of ontologies. Therefore,the application of f? to vi generates a vector si of 14components. The more components the vector si has, theharder it is to compare and interpret the changes. For thisreason we provide the user with some definitions whoseaim is to describe different types of changes. Hence, giventwo consecutive versions vi?1, vi ? vC? , with i > 1, andgiven the vectors si?1 and si obtained by the application ofFig. 1 OQuaRE example that represents the traceability from the OQuaRE quality requirement to quality metrics divisionsDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 6 of 20Table 4 OQuaRE static scale with [1-5] values, where 1 means not acceptable, 3 minimally acceptable and 5 exceeds the requirementsMetric\Score 1 2 3 4 5LCOMOnto >8 (6-8] (4, 6] (2, 4] <=2WMCOnto >15 (11, 15] (8, 11] (5, 8] <=5DITOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]NACOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]NOCOnto >12 (8, 12] (6, 8] (3, 6] [1, 3]CBOnto >8 (6, 8] (4, 6] (2, 4] [1, 2]RFCOnto >12 (8, 12] (6, 8] (3, 6] [1, 3]NOMOnto >8 (6, 8] (4, 6] (2, 4] <= 2RROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %AROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %INROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %CROnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %ANOnto [0, 20] % (20, 40] % (40, 60] % (60, 80] % >80 %TMOnto >8 (6, 8] (4, 6] (2, 4] (1, 2]Those metrics adapted from object oriented programming have been scaled based on the best practices for object oriented programming and the metrics whose result is arelative value are scaled in percentagethe comparison criteria f? , a change in scale of version vifrom version vi?1 is described in Definition 3.Definition 3 Change in scale: vector of change associ-ated with different values of the components of the vectorsi with respect to si?1. The vector li, which is calculated assi ? si?1, represents the levels in size and direction of thechanges from vi?1 to vi version, with i > 1.It should be pointed out that the change in scale appliesto all the versions of an ontology except to the first one,which corresponds to i = 1 in vC? . Since the OQuaREquality scores are the comparison criteria the level rangesfrom [-4, 4], so the direction can be positive or negative.For example, let us suppose a vC? that contains six ele-ments v1, . . ., v6. The application of f? to vC? generates amatrix with 6 rows, like the one shown in Expression 1.The row i represents the vector si and has 14 components,with i = 1,. . . ,6.1 . . . . . . . . . . . . 14(r)s1s2s3s4s5s6????????5 4 2 1 . . . .5 4 2 1 . . . .4 3 2 1 . . . .3 4 5 1 . . . .1 5 5 2 . . . .5 1 4 3 . . . .????????(1)Using as input the matrix in Expression 1 we apply theDefinition 3 and obtain a matrix with 5 rows, like the oneshown in Expression 2. The row i represents the changein scale by the vector li, with i = 2, . . . , 6. In the contextof quality scores, a negative component in li representsa decreasing level in the corresponding quality score ofvi from vi?1, a positive one means the opposite and 0indicates that the metric score remains invariant.1 . . . . . . . . . . . . 14(r)l2l3l4l5l6??????0 0 0 0 . . . .-1 -1 0 0 . . . .-1 1 3 0 . . . .-1 0 0 1 . . . .4 -4 -1 1 . . . .??????(2)We propose to use a summarised representation of thechange in scale of the r metrics and between vi and vi?1by using the frequency distribution Fi associated with thechange in scale li, which is defined in the following way:Definition 4 Frequency distribution of the chase in scale(Fi): it is an ordered list of the frequencies fl associated withthe different change levels l in the vector li.The change levels range between lmin and lmax. In thecontext of OQuaRE quality scores, lmin and lmax are ?4and 4 respectively. Therefore, in this case the frequencydistribution Fi has 9 components, which represent thefrequencies fl of the ranks l from -4 to 4. For example,Expression 3 shows the frequency distributions of ourrunning example. The interpretation of F2 is: there are 4out of r metrics that have not suffered any change in scalebetween v1 and v2. The change is larger between v2 and v3(F3) as there are 2 metrics that have decreased one scaleand other 2 remain unchanged.Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 7 of 20f?4 f?3 f?2 f?1 f0 f1 f2 f3 f4F2F3F4F5F6??????0 0 0 0 4 0 0 0 00 0 0 2 2 0 0 0 00 0 0 1 1 1 0 1 00 0 0 1 2 1 0 0 01 0 0 1 0 1 0 0 1??????(3)Hence the frequency distribution Fi can be used fordescribing different types of changes between two con-secutive versions vi?1 and vi with respect to the set ofOQuaRE quality scores. Next, we define some associatedstatistics such as weighted means.Definition 5 Forward Mean Change: weighted mean ofthe positive change levels l, calculated as:?lmax1 l × fl?lmax1 flDefinition 6 Backward Mean Change: weighted meanof the negative change levels l, calculated as:??1lmin l × fl??1lmin flTo avoid possible undefined values of the forward orbackward means, we also use the size of the forward andbackward changes defined as the numerator of the pre-vious definitions, but considering absolute values |l| inbackward mean changes. Now, Definition 7 provides thedefinition for the global mean change.Definition 7 Mean change: weightedmean of the changelevels l, calculated as:?lmaxlmin l × fl?lmaxlmin flIn our running example, the frequency distribution F3does not provide a determined finite value for the forwardmean change, whereas the backward mean change is ?1and the mean change is?0.5. The sizes of the forward andbackward changes are 0 and 2, respectively.The value of the mean change can be interpreted asfollows: It takes a positive value when the forward meanchange is greater than the backward one and negativewhen the opposite. It becomes zero when forward and backward meanchanges take equal and finite values. It becomes zero if vi and vi?1 are identical. In thiscase forward and backward mean changes do nottake a determined finite value (undefined value).The mean change provides information about changesin quality scores. For analysing the number of metrics thathave changed regardless of the direction of the change, wedefine next the conceptmagnitude of change.Definition 8 Magnitude of change: percentage of metricswith change in scale, which is calculated as follows:?l =0 fl?lmaxlmin flIn our example, the magnitude of change of versionv2 is 50 %. The largest number of metrics with changeshappens in v6 (see F6 in Expression 3), having a magni-tude of change of 100 %, but the mean change is 0.0. Themajor increase in quality scores happens in v4 (see F4 inExpression 3) withmean change 0.75.A dynamic scaling function for ontology evolutionWe propose to take advantage of the information availablein the vC? to derive a dynamic scaling function. For thispurpose, each ontology in such a corpus is processed withOQuaRE, so calculating the raw values of the 14 qual-ity metrics. These original values are used for generatinga scale in k categories determined by k-means clustering[25], which groups similar values into the same categoryby minimising the intra-class variance and emphasisesthe differences among categories maximising the inter-class variance. In this paper, the number of categories isk = 5 because the OQuaRE scale is [1,5]. This is illus-trated using Fig. 2. The metric RROnto measures therichness of relations and it is calculated using the meannumber of usages of object and data properties dividedby the number of subClassOf relationships and objectproperties. The standard scale for RROnto is shown inTable 4.The RROnto raw values obtained for all the versionswithin a vC? are represented in the x-axis of Fig. 2. Thestatic scale is represented in the upper-part of the figure,and the dynamic scale obtained using k-means is shownin the bottom-part. While the raw RROnto value 0.74is matched with the quality score 4 in the static scale,it is matched with 5 in the dynamic scale. It should bepointed out that the dynamic scale forces data to be cat-egorised between 1 and 5, 1 being the lowest raw valuefound in vC? and 5 the highest. If the amount of differentdata is not enough to generate 5 categories the algorithmdoes not include any value in the lowest categories ofthe scaling function (see for example the solid line forDITOnto metric in Fig. 3). Therefore, the application ofthe dynamic scale should help users to study the evolutionof the observed quality metrics values for all the versionswithin a vC? .Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 8 of 20Fig. 2 Example of the static and dynamic scale for RROnto metric. The x-axis represent the observer raw values of the metric for a vC.Semi-transparent rectangles shows the limits of the levels of the scale. While the static scale remains constant, the dynamic will depends of theobserver raw values of RROnto in a vCThe ontology of Bioinformatics operations, types of data,formats, and topics (EDAM)We are going to study the evolution of the EDAM ontol-ogy [21, 26]. The EDAM is an ontology of well establishedand familiar concepts that are prevalent within bioinfor-matics. The EDAM includes types of data, data identifiers,data formats, operations and topics. We have chosen thisontology as an example because: It is well documented and its developers use a controlversion system (CVS) [27] so that we can tracechanges. Its source files are accessible online. The latestversion (v1.9) is published in the official project webpage. Links to old versions can be found in BioPortal(18 versions) and in the CVS (13 versions). It has received 900 mean visits per month sinceOct-2013 to Apr-2014 and 6 declared projects use theEDAM. The number of versions (18) makes it an ontology ofinterest for studying its evolution. Its size (2 597classes as mean) is intermediate, which facilitates theanalysis of the results in this first application of themethod.Results and discussionExperimental setupThe versioned corpus comprised the 18 EDAM versionsin BioPortal as CVS content, which was processed usinga software tool developed in house that implements theOQuaRE framework. This framework and tool are pub-licly accessible at http://sele.inf.um.es/oquare as a webform and a web service. The framework uses the OWLAPI [28] and Neo4j [29] for the calculation of OQuaREmetrics. We carried out the computation of the dynamicscaling by using the function bin.var of the packageRcmdrMisc of R [30].We applied a normalisation process to the 18 versions.In the normalisation, we removed deprecated classes andchecked the consistency of the ontology. Before applyingthe normalisation, 4 out of 18 versions were discarded bythe tool: one could not be processed by the OWL API,Fig. 3 Graphical representation of the static and dynamic scaled metrics along the versionsDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 9 of 20and the other three were found to be inconsistent by thereasoner Hermit [31]. Therefore, the versioned corpuscontained 14 ontologies. In the remainder of this paper,we label each version according to its original id version. Itshould be pointed out that the statistics of change of a cer-tain version vi were calculated with respect to the previousprocessed version. For example, the change in v16 was cal-culated with respect to v12 because v13, v14 and v15 couldnot be processed.The normalisation process made consistent v13 andv14 and, therefore, they were included in the study. Wedecided to perform two types of experiment: one withthe deprecated classes (14 consistent ontologies) and onewithout the deprecated classes (16 consistent ontologies)with the goal of studying the impact of the obsolete classesin the structure of the ontology. We applied the tool toobtain the scores of the metrics, subcharacteristics andcharacteristics for all the versions. Such measurementswere the comparison criteria, which allowed the scoresto be obtained by using both the static scaling functionand the dynamic one. After presenting those results, wewill discuss the evolution of the EDAM in terms of qual-ity scores and analyse the advantages and disadvantages ofboth scaling methods. The whole set of results is availableat http://miuras.inf.um.es/oquare/jbsm2016.Analysis of quality characteristics with the static scaleTable 5 shows the results obtained at the quality character-istics level. Two quality scores are shown for each qualitycharacteristic: original (org) and normalised (nrm). Boldnumbers highlight changes in scale. Next, we discuss thechanges observed in the quality characteristics.We can observe in Table 5 that the mean quality scoreranges from 3.99 in the first version to 3.85 in the lastone, so its quality scores have always stayed between 3and 4. A quality score higher than 3 reveals that goodontological engineering principles have been applied bythe EDAM developers. However, this difference has notproduced a change in scale in global terms. Despite thisfact, investigating why the quality score decreased is rel-evant because lower OQuaRE levels provide users withmore fine grain information. For example, those decisionsmade during the construction or modification of large andcomplex ontologies may have collateral effects in theirengineering, which may have different implications froma quality perspective. For example, reducing the usage ofproperties might benefit the maintainability of the ontol-ogy but fewer queries might be asked. Therefore, a lowervalue in OQuaRE metrics related to the usage of proper-ties would contribute positively to the Maintainabilityof the ontology but negatively to the Formal relationssupport. Understanding how different changes influencedifferent quality aspects is difficult to study if we use onlythe mean quality score. This is why the analysis at the levelof characteristics, subcharacteristics and even metrics isrecommended.First, we describe which characteristics have changes inscale. The analysis of the evolution of quality scores ofthe characteristics (between the first version and last one)shows that 6 out of the 7 quality characteristics had achange in scale: 4 positive and 2 negative. In the remain-ing case, there was no change in scale for FunctionalAdequacy. The score of the Reliability characteristicdecreased from 3 to 2 in v2; and the Structural onedecreased from 4 to 3 in v11. The scores for Operabil-ity, Compatibility, Maintainability and Transferabil-ity increased from level 3 to 4 in v5. Moreover, theontology has maintained the score at this level sincethen. This behaviour happened for all their associatedsub-characteristics. The scores for the whole set of sub-characteristics can be found at http://miuras.inf.um.es/oquare/jbsm2016.Analysis of the quality metrics with the static scaleNext, we describe the changes observed at the level ofOQuaRE metrics because this enables us to focus on con-crete structural changes, which can help us to discuss andexplain the variations obtained in higher levels. Figure 3(dashed lines) shows the quality scores of the static scalefor the 14 OQuaRE metrics. It can be observed that 9OQuaRE metrics did not change for any version. The 5metrics that have changed are LCOMOnto, NOMOnto,RFCOnto, TMOnto and RROnto. Next, we discuss theimpact of the changes in these metrics at the level ofOQuaRE characteristics and sub-characteristics. RROnto had 3 changes in scale. The first 2 changeswere consecutive and due to the usage of properties,which decreased 86 % between v4 and v6. Refactoringtowards a common set of properties can often be asign of good ontology engineering practise, howeverthe usage measures the number of times that aproperty is linked with an entity through an axiom.For example, while v4 defines 16 properties with6 734 usages, v5 and v6 define the same number ofproperties but with 1 979 and 937 usages respectively.The usage of properties also decreased 8 % betweenv10 and v11. This variation is smaller than the previousone but, together with an unusual increase in thenumber of relations (18 %), it triggered the change inscale of RROnto. This increase in the number ofrelations is a consequence of a structural change inv11: deprecated classes were grouped as descendantsof an ontology class in the first taxonomic level andthis increased the number of relations. RFCOnto and NOMOnto had 1 change in scalegrowing from 4 to 5 in v4. This behaviour was alsorelated to the usage of properties. However, for theseDuque-Ramosetal.JournalofBiomedicalSemantics (2016) 7:63 Page10of20Table 5 OQuaRE characteristics metric values for eighteen versions of the EDAM ontologyV. Date StatusStruct. F. Adeq. Reliab. Operab. Compat. Maint. Transf. MeanOrg. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm. Org. Nrm.1 2010-05-14 beta 4.67 4.67 4.61 4.61 3.25 3.25 3.83 3.83 3.75 3.75 4.10 4.10 3.75 3.75 3.99 3.992 2010-05-28 beta 4.50 4.50 4.60 4.60 2.88 2.88 3,67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.883 2010-08-18 beta 4,50 4.50 4.60 4.60 2.88 2.88 3.67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.884 2010-10-07 beta 4,50 4.50 4.60 4.60 2.88 2.88 3.67 3.67 3.75 3.75 3.99 3.99 3.75 3.75 3.88 3.885 2010-12-01 beta 4.17 4.17 4.46 4.46 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.94 3.946 2011-01-22 beta 4.00 4.00 4.28 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.90 3.907 2011-06-17 beta 4.00 4.00 4.28 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.90 3.908 2011-12-05 beta 4.00 3.83 4.28 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.90 3.7810 2012-12-10 beta 4.00 3.83 4.28 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.90 3.7811 2012-12-14 release 3.83 3.83 4.11 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.85 3.7812 2014-02-18 update 3.83 3.83 4.11 4.27 2.75 2.38 4.00 3.83 4.00 4.00 4.23 4.12 4.00 4.00 3.85 3.7813 2014-09-26 update - 3.83 - 4.27 - 2.38 - 3.83 - 4.00 - 4.12 - 4.00 - 3.7814 2014-11-14 update - 4.00 - 4.28 - 2.75 - 4.00 - 4.00 - 4.23 - 4.00 - 3.9016 2014-12-08 update 3.83 4.00 4.11 4.28 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.9017 2014-12-16 update 3.83 3.83 4.11 4.11 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.8518 2015-02-02 update 3.83 3.83 4.11 4.11 2.75 2.75 4.00 4.00 4.00 4.00 4.23 4.23 4.00 4.00 3.85 3.85These values are scaled from 1 to 5, where 1 is not acceptable and 5 exceeds the requirements. Bold numbers highlight changes in scale between two consecutive versionsDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 11 of 20metrics such a primitive metric influences positivelythe quality score because, in the case of NOMOnto,the lower the mean number of property usage perclass is the easier the maintainability of the ontologyis. This behaviour triggered the change in scale forthe characteristics Operability, Compatibility andTransferability in v5. TMOnto measures the distribution of the parents inthe ontology. 10 % of the classes had more than 1direct parent in v4, while this value grew up to 24 % inv5. This metric has a negative effect across theontology because of the multiple inheritance,although this might be needed to reflect some aspectswithin the ontology. This fact influenced the decreasein the Tangledness subcharacteristic, which alsocontributed to the decrease of the the Structuralcharacteristic. However, for this metric this changedid not trigger by itself a change in scale, which wasproduced in v11 with the collaboration of RROnto. LCOMOnto uses the number of paths in theontology in its calculation and it suffered one changein scale in v2. This metric is used in thesubcharacteristics Cohesion, Knowledge reuse,Learnability , Recoverability and Availability.Moreover, this metric is the unique used to measureCohesion and Availability, so it has a deeperimpact for these two subcharacteristics than for theothers. On the one hand the lowest score for theStructural characteristic was for Cohesion but thisdid not trigger a change in scale for v2. On the otherhand, the Recoverability and Availability aregrouped in the Reliability characteristics and for it,the behaviour of the LCOMOnto metric triggeredthe change in scale in v2.Influence of deprecated classesThe presence of deprecated classes grew from 3.51 %(v1) to 29.58 % (v18). Deprecated classes caused incon-sistencies in v13 and v14. Table 5 shows that there wereno significant changes at the characteristic level betweenthe ontologies with (Org) and without the deprecatedclasses (Nrm), but some changes happened at the met-ric level. The change in the Structural characteristic withdeprecated classes anticipated the drop of RROnto to v11,whereas it happened in v17 in the normalised version.Besides, LCOMOnto temporarily descended to score level2 between v8 and v13 in the normalised version. This effecton LCOMOnto could not be appreciated in the ontologieswith the deprecated classes. Deprecated classes remain inthe ontology, so they are influencing the OQuaRE results.For example, RROnto uses the number of subClassOfrelations in the denominator, to which deprecated classes(see Table 2) contribute. The removal of the deprecatedclasses had an impact on this metric, which produced thiseffect of anticipating or delaying changes in scale. More-over, the scaling function cushioned smaller changes suchas the one produced by LCOMOnto.Application of the dynamic scaleWe have obtained a dynamic scale using the EDAM ontol-ogy versions composing the experimental vC? . The valuesobtained after applying the k-means clustering are shownin Table 6. Moreover, Fig. 3 shows the evolution of thevalues of the metrics for both the static (dashed lines)and dynamic scales (solid lines). It can be seen that thedynamic scale is able to capture more changes in thosevalues than the static one. This is an expected result asthe [1,5] scale limits for each metric is derived from theraw values of the metrics for the different versions ofthe ontologies. This means that both scales reflect differ-ent aspects and, therefore, are complementary in helpingto understand the engineering and the evolution of theontologies. Next, we discuss how changes are detected byboth scales.The changes in some metrics were detected by bothscales. In the case of RROnto, although the first versionstarts in 4 for the static scale and in 5 for the dynamicscale, both scales detected changes between the samepairs of versions, except for v17. However, this did not hap-pen for RFCOnto, TMOnto, NOMOnto or LCOMOnto.The dynamic scale is more sensitive so it detected morechanges between pairs of versions for these 4 character-istics. The static scale did not detect changes for ninemetrics, but the dynamic one did. For example, while theDITOnto value remained in 1 in the static scale, in thedynamic scale it started in 5 and ended in 4. Moreover, itdecreased to 2 in v7.The value of DITOnto remained in 1 with the static scalefor all the versions of the EDAM ontology. DITOnto mea-sures the depth of the ontology. The raw values obtainedfor our corpus were (11, 11, 11, 11, 13, 13, 14, 13, 13,13, 13, 12, 12, 12). All of them are greater than 8, whichis scaled to the quality score 1, according to the bestpractice applied. However, in the field of ontologies anappropriate value for DITOnto might depend on manyfactors, and it is here where the dynamic scale can com-plement the static one. According to [32], well-structuredOO systems have a forest of classes rather than onelarge inheritance lattice. However, whether a high or lowvalue is desired from a metric for better code qualitystill must exercise judgement when determining the bestapproach for the task at hand. According to [32], the lowerthe DITOnto the better, so the OQuaRE scaling methodmatches DITOnto low values to 5 and high values to 1.Then, the dynamic scale uses the lowest and highest val-ues observed for the versions of the ontology to assignthe scores 5 and 1, respectively. With this scale, the high-est quality scores were reached from v1-v4, then it wentDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 12 of 20Table 6 Coordinates of the dynamic scale obtained after applying the k-means algorithm using the versions of the EDAM within theexperimental vCMetric\Score 1 2 3 4 5LCOMOnto [5.646945, 5.782834] [5.505317, 5.505317] [5.158599, 5.190406] [5.072177, 5.093400] [3.874391, 4.109421]WMCOnto [4.123580, 4.176131] [1.931285, 1.931285] [1.536827, 1.559519] [1.401986, 1.478964] [1.334862, 1.347192]DITOnto [, ] [14, 14] [13, 13] [12, 12] [11, 11]NACOnto [1.275837, 1.279578] [1.261488, 1.264644] [1.245146, 1.245352] [1.228666, 1.230561] [1.099615, 1.104098]NOCOnto [1.332252, 1.342622] [1.276790, 1.286796] [1.263569, 1.263569] [1.229043, 1.230952] [1.103604, 1.108706]CBOnto [1.602873, 1.637277] [1.559101, 1.559101] [1.404925, 1.456697] [1.230693, 1.281911] [1.143644, 1.152976]RFCOnto [4.364891, 4.383669] [2.306886, 2.306886] [1.900142, 2.022841] [1.541327, 1.564187] [1.438217, 1.475449]NOMOnto [3.068605, 3.115014] [0.799515, 0.799515] [0.3790453, 0.3790453] [0.2754958, 0.3078338] [0.2071335, 0.2423935]RROnto [0.144421, 0.144694] [0.164751, 0.180672] [0.2195698, 0.2562910] [0.4139807, 0.4139807] [0.7441604, 0.7459092]AROnto [4.14, 5.00] [7.0, 7.0] [14.0, 14.0] [16.0, 16.0] [21.0, 21.0]INROnto [1.037050, 1.061705] [1.094152, 1.099919] [1.13177, 1.13177] [1.227018, 1.228871] [1.261331, 1.277758]CROnto [0.0, 0.0] [0.35285e?3, 0.36778e?3] [0.40420e?3, 0.40453e?3] [0.45433e?3, 0.45433e?3] [0.47103e?3, 0.48123e?3]ANOnto [1.097413, 1.102329] [1.114493, 1.117287] [1.131656, 1.131656] [1.144306, 1.144975] [1.150423, 1.153622]TMOnto [0.2556087, 0.2599199] [0.2461048, 0.247064] [0.2400970, 0.2436178] [0.2171334, 0.2173766] [0.09961501, 0.10456901]down from v4-v5 and again from v6-v7, then it remainedstable until v12, where it again increased one level. As wehave explained previously, it should be pointed out thatthe dynamic scaling method for DITOnto did not spanthe range [1. . .5] because there were only 4 raw valuesobserved.Analysis of major changes between versionsThe graphical representation of the frequency distribu-tions Fi is shown in Fig. 4. The left-half of the Figure showsthe frequency distributions Fi obtained with the staticscale, on the right-half the ones obtained with the dynamicscale. For each box, the y-axis represents the componentsfrom the levels lmin to lmax; it should be pointed out thatthis figure just represents in the y-axis those componentswith at least one observed frequency fl distinct than 0for any version in vC? ). Finally, the x-axis represents thefrequency of each component. For example, with the staticscale and for v8 (Fi = 8) the frequency of l0 is 13 becausethe value of 13metrics did not change in scalewith respectto the previous version; similarly, with the dynamic scaleand for v2 (F2) the frequency of l?1 is 1 because 1 metric(LCOMOnto) decreased one level.Now we describe how to use the magnitude and meanchange to analyse major changes between consecutive ver-sions. This will be done by discussing the data shown inTable 7, where rows 25 show the values of the four statis-tics of change using the static scale, and rows 69 showthose statistics for the dynamic scale.Analysis of magnitude of changeThe magnitude of change with the static scale was dif-ferent than 0 for v2, v5, v6 and v11 (see Table 7 row 2).For example, the largest magnitude of change happenedFig. 4 Frecuency distributions of the changes in scaleDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 13 of 20Table 7 Statistics for static and dynamic scales:magnitude of change,mean change forward,mean change backward, andmean changeChange v2 v3 v4 v5 v6 v7 v8 v10 v11 v12 v16 v17 v18Sta. Magnitude 7 % 0 % 0 % 28 % 7 % 0 % 0 % 0 % 7 % 0 % 0 % 0 % 0 %Mean. For 1 - - 1 - - - - - - - - -Mean. Back - - - 1 1 - - - 1 - - - -Mean -0.07 0.00 0.00 0.00 -0.07 0.00 0.00 0.00 -0.07 0.00 0.00 0.00 0.00Dyn. Magnitude 7 % 0 % 7 % 79 % 42 % 71 % 36 % 56 % 56 % 21 % 64 % 14 % 0 %Mean. For - - - 1.25 1.33 1.00 1.50 1.50 1.75 1.00 1.38 - -Mean. Back 1.00 - 1.00 1.67 1.33 1.12 1.00 1.25 1.75 - 1.00 1.00 -Mean -0.07 0.00 -0.07 -0.71 0.00 -0.50 0.36 0.07 0.00 0.21 0.71 -0.14 0.00The symbol - in this table represents the undefined valuefor v5, 28 %, and this was a consequence of the changesin RFCOnto, NOMOnto, TMOnto and RROnto; thesechanges in the OQuaRE quality metrics can be observedin Fig. 3 (dashed lines). For v2, v6 and v11, the magnitudeof the change is 7 % because only one metric had a changeof level. There were no changes in the quality scores forthe rest of the versions. The magnitude of change withthe dynamic scale was different than 0 for 11 out of 13versions. This is a consequence of the higher sensitivityof the dynamic scale. This scale enabled the identifica-tion of versions like v3 or v18 to be very similar withrespect to their previous one, because the magnitude andmean changewere 0 % and 0.00 respectively. By similar wemean that there were not enough changes between themthat produced a change in scale for any of the OQuaREmetrics.In order to analyse pairs of consecutive versions, we aregoing to use the median (Me) of the absolute differencebetween the values of the 14 metrics, and the Wilcoxontest for contrasting the alternative hypothesis Me > 0.Table 8 sorts the versions by increasing critical value andp-value associated with the null hypothesis (Me = 0) foreach test performed. These results show that: We reject the null hypothesis (Me = 0) in all thecomparisons, so we can interpret that all the changesare significant. We have evaluated the magnitude of change usingthe quality scores (scaled metrics). The critical valueshows the magnitude from which the differencemedian (Me) is significantly higher at the 0.05 level ofsignificance. Using this criterion for sorting thechanges between versions we obtain that the largestchange happens in v5. The four versions with the largest changes accordingto this analysis are also the four versions with thehighest magnitude of change for the dynamic scale,as shown in Table 7 row Magnitude. This shows thegoodness of the criteria used in the dynamic scalingfunction.Analysis of mean changesThemean change using the static scale is negative becausethe score of one metric decreased for v2, v6 and v11(see Table 7 row 6). However, the magnitude of changehad a different evolution. The largest magnitude hap-pened for v5, but the mean change for v5 was 0.0,because the number of positive weighted changes wasequal to the number of negative ones. For this partic-ular case, two metrics increased 1 level (RFCOnto andNOMOnto) and 2 exactly the opposite (TMOnto andRROnto) (see dashed line in Fig.3). The higher sensitiv-ity of the dynamic scale is also observed in the meanchange values, because more changes were detected. Forexample, if we focus on v5, the Mean. Back (1.67) washigher than the Mean. For (1.25) regardless of the num-ber of metrics that had changed. Therefore, the Mean is-0.71, so there were more negative changes than positiveones.Table 8 Versions sorted from less to high critical value andp-value associated with the null hypothesisMe after applying thetest of Wilcoxon using the difference in absolute values of themedian of 14 OQuaRE metrics and consecutive pairs of versionsVersion Critical_value P_value18 0.0001825782 1.263087e?32 0.0013102421 1.263087e?33 0.0020865871 1.263087e?34 0.0021207897 1.263087e?317 0.0044867447 1.263087e?38 0.0072293707 6.103516e?512 0.0113504041 1.263087e?310 0.0119625746 8.308472e?46 0.0259642025 8.308472e?411 0.0303236313 8.308472e?416 0.0324480617 8.308472e?47 0.0420278822 6.103516e?55 0.1587347761 8.308472e?4Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 14 of 20As a complement, the graphical representation of thebackward and forwardmean change size is shown in Fig. 5.The upper-half of this figure (size forward) stands forthe positive changes, whereas the lower-half (size back)represents the negative ones. The largest positive changehappened for version v16, and the largest negative one wasfor v5.Profile of change in quality scoresRegardless of the scale used, the information providedby the mean change can be used to calculate a profileof quality based on the OQuaRE framework. This profiletakes into account the accumulativemean changes duringthe whole life-cycle of the ontology. Figure 6 shows theevolution of the quality scores using both scales: The use of the static scale shows a trend of negativemean change. The accumulative mean change valueremained negative for all the versions and all thepairs, which is also reflected in the decrease of thequality scores of the characteristics as mean from3.99 to 3.85, which was discussed previously. The complementary use of the dynamic scale allows adifferent evolution to be observed. The mean changefor the first 7 versions was negative, whereas it waspositive for the next 9 versions. As a consequence,the accumulative mean change growed from -1,35 to1,63. Finally, it decreased until 1.49 for v17 andremained constant for v18.Finally, if we take into account the status used to defineeach version in BioPortal: they are considered beta fromversion 1 to 10. Using the dynamic scale, we observe thatthe quality scores decreased until v7, and in particularin v5 with the lowest mean change (see Fig. 6). Havingsuch changes during the beta stage makes sense. Once theontology is considered released, the increase of the qualityscores was over the mean.Relation between quality scores and the level of activity inan ontologySo far, we have analysed aspects related to variability inthe quality scores. Now, we study the possible relationbetween these changes and the level of activity in an ontol-ogy. The level of activity has beenmeasured in [3] in termsof changes in ontology classes, namely, number of classesthat have been added, deleted or modified. These threevariables are calculated by Bubastis [8], so we call themthe Bubastis variables.In http://miuras.inf.um.es/oquare/jbsm2016, severalPrincipal Component Analysis (PCA) studies can befound. Here, we use the three statistics related to meanchange (using the dynamic scale) and the Bubastisvariables for performing a PCA, with the objectiveof obtaining the relation between these two differentontology aspects, as well as obtaining a bi-dimensionalrepresentation of the changes between two versions.The coordinates of the variables for the new axis areshown in Table 9, and they are graphically representedin Fig. 7 upper half. The variable representation of Fig. 7suggests the presence of two normalised uncorrelatedfactors: The Bubastis variables have the largest positivecorrelations (0.88, 0.80 and 0.85, for new, changedFig. 5 Statistics of size for static and dynamic scales: forwardmean change size and backwardmean change sizeDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 15 of 20Fig. 6 Graphical representation of the accumulativemean change using the static and dynamic scalesand deleted classes respectively) with Factor 1(represented in the x-axis), so we interpret this factoras a gradient representing the increasing volume ofactivity associated with the Bubastis activity. We callthis factor Bubastis Activity.Table 9 Representation in 2-dimensions of the coordinates ofthe variables for the new axis(x-axis) Factor 1 (y-axis) Factor 2Number.New.Classes 0.8862 0.3539Number.Changed.Classes 0.7970 0.3300Number.Deleted.Classes 0.8458 0.4253Dynamic.Backward.Size 0.6883 ?0.4895Dynamic.Forward.Size 0.3823 0.6623Dynamic.Mean.Change ?0.3557 0.9186Factor Name Bubastis Activity OQuaRE Dynamic QualityThree statistics related tomean change (using the dynamic scale) and the Bubastisvariables have been used for performing a PCA, with the objective of obtaining therelation between these two different ontology aspects. The variable representationof Fig. 7 suggests the presence of two normalised uncorrelated factors: BubastisActivity and OQuaRE Dynamic Quality. The representation of these coordinates canbe found in Fig. 7 above The Dynamic mean change has the largest positivecorrelation (0.92) with Factor 2 (represented in they-axis), whereas dynamic backward size has anegative correlation with this factor. Those factsallow us to interpret this second factor as a gradientfrom lower OQuaRE quality scores to higher ones.We call this factor OQuaRE Dynamic Quality.According to the previous comments, the versions rep-resented in the first diagonal will be relevant in activityand quality, the more the farther from the origin theyare.The two previous factors explain more than 80 % ofthe information contained in the six variables shown inTable 9; and the first factor explains roughly 48 % ofsuch information. Apart from the two factors, in Fig. 7we also observe the next correlations using the Pearsontest: (1) the number of classes deleted and new classes(0.99, p-value 0.0000); and (2) the dynamic mean changeis almost independent of new (-0.01, p-value 0.9650) anddeleted classes (0.07, p-value 0.8137) and (3) the dynamicforward size is almost independent of the numberDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 16 of 20Fig. 7 Principal Component Analysis: factors and principal components plotsof changed classes (0.01, p-value 0.9808). Those pairswhose p-value is lower than 0.05 indicate a significantcorrelation.Figure 7 bottom represents the principal componentsof the changes between consecutive versions in our vC? ,where four changes can be highlighted: The Bubastis activity of v16 was below the meanvalue. However, this activity produces a remarkableincrement in the OQuaRE quality scores using thedynamic scale. The Bubastis activity of v10 and v11 was atypicallyhigh with respect to the rest of the versions.Moreover, the OQuaRE quality scores using thedynamic scale are over the mean value. The Bubastis activity of v5 was over the mean,producing a decrease in the OQuaRE quality scoresusing the dynamic scale and a high level in thenumber of classes changed. The Bubastis activity of v18 was the lowest andaround the mean value in OQuaRE quality scoresusing the dynamic scale.The most relevant changes obtained by this representa-tion are the same as those obtained by the mean changestatistics shown in Fig. 5, where v5 and v16 had the highestvalue of back and forward size respectively.Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 17 of 20A view on the evolution of the EDAM ontologyIn this section we discuss how the application of ourmethod enables some insights about the EDAM ontologyand its evolution in terms of quality scores as well as thebenefits of using the static or dynamic scales.If we analyse the quality of the EDAMontology from theOQuaRE perspective, we can identify different strengthsand flaws, driving our attention to those quality scoresobtained for the latest version analysed v18 (see Table 5).According to the OQuaRE static scale, the mean value3.85 reveals that good ontological engineering principleshave been applied. The analysis of the characteristics andsub-characteristics gives us more information. Next, wecomment on the results for the highest and lowest score:maintainability, functional adequacy and reliability (4.23,4.11 and 2.75 respectively). The highest quality score is obtained formaintainability (4.23). All its subcharacteristicsassociated have quality score over 4 (see values athttp://miuras.inf.um.es/oquare/jbsm2016). Thisreveals some strengths of the EDAM, such as thereduced rate of negative side-effects due to changesin the ontology (modification stability 4.60) and thepossibility to validate the ontology and detect flawson it (testability 4.00). The second highest quality score applies tofunctional adequacy (4.11). For example, the EDAMis good for use as a controlled vocabulary to avoidheterogeneity of terms because all their classes havelabels expressed in natural language. However, not allits subcharacteristics obtain high scores. Forexample, one weakness of the EDAM is elucidated bythe score of the inference subcharacteristic. Its scoreis 1.0 due to the low usage of properties, despite thefact it is defined using a formal language. The absenceof instances also contributed to this score. The lowest score is obtained for reliability (2.75),whose subcharacteristics are recoverability (2.50) andavailability (3.00). The recoverability score is below 3,so it can be considered as a weakness of the EDAMbecause in case of inconsistency, incompleteness orredundancy of the content of the ontology, thatwould be difficult to re-established and to recover theontologys performance.There is only another subcharacteristic with a qual-ity score under 3, formal relation support, whose scoreis 1. The formal relations support measures the capabil-ity of the ontology to represent relations supported byformal theories different from taxonomy. This is calcu-lated by analysing the usage of properties (RROnto). Aswe have shown in previous sections, RROnto has a scoreof 1 in the latest versions whereas the value of the firstversion was 4, which makes it a potential weakness of theontology in the latest versions. The previous discussionabout RROnto comes from the comparison of differentversions, so it is done in terms of evolution. Continuingwith the analysis of the evolution of the EDAM ontologyfrom the OQuaRE perspective, we can draw the followingconclusions: v5, v2, v7 and v11 were the versions with the highestmagnitude of change, that is, number of metrics withchanges. The analysis of the characteristics using thestatic scale has revealed that, as mean, there are nochanges in scale in the EDAM ontology. This is alsoobserved in the negative trend of the accumulativemean change when the static scale is used (Table 6).Interestingly, the dynamic scale has revealed theobservation that the accumulative mean changetrend is positive from v7 to v18. At the characteristics level, the application of thestatic scale to the EDAM ontology has revealed thatthe evolution of the ontology has produced higherquality scores for four characteristics, and lower onesfor two of them, as can be observed in Table 1. The analysis of changes at the OQuaRE metrics levelhelps us to identify that the usage of properties is thereason that has triggered the major descend in qualityscores between v4-v6, and again between v10-v11.Moreover, an unusual increment of the number ofrelations in v11 triggered this change in scale. Itshould be pointed out that the application of ourmethod can draw out these types ofsuggestions.Discussion about the methodIn the previous sections we have described the mainresults of our work, as well as provided some discussionabout the application of the method to the EDAM. Next,we provide some discussion about different aspects of themethod.In our previous work, the application of the standard,static scaling function used by OQuaRE proved its useful-ness to detect strengths and flaws of ontologies and evento detect changes between versions of the same ontol-ogy. However, we believed that the use of more preciseand sensitive methods for detecting changes would allowOQuaRE to be more supportive of ontology evolutionprocesses. This is whywe have proposed the dynamic scal-ing function, which should be used in conjunction withthe static one, because they provide complementary infor-mation. Hence, this does not mean that the static scalingfunction cannot be used on its own for ontology evolution.It can be used to measure how the different versions havechanged across their history, taking into account fixedcriteria. For example, here we have evaluated the EDAMDuque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 18 of 20using the static function using as reference the currentconfiguration that evaluates the ontology from an engi-neering point of view. This static scaling approach enablesusers to measure the quality of ontologies using a com-mon framework, but, of course, this framework can beextended or fit to certain contexts in case that the con-text is clearly identified. Nevertheless, the dynamic scal-ing function should provide more useful information forontologies for which new versions are frequently releasedor that do not constitute major changes with respect to theprevious ones.The development a common reference framework thatcan be used for those different requirement scenarios is achallenging task. An open question is whether the rangescan be universally set for the static scaling method. Thedynamic scaling function tries to overcome this uncer-tainty by performing an evaluation based on the behaviourof the ontology during its evolution. It should be pointedout that the goal of the dynamic scale is not to replaceor substitute the static one. In fact, the dynamic functiondoes not discretise the raw values of the metrics using acontinuous function, but the limits are set on the observedvalues (see Fig. 2). However, the dynamic scale result couldbe used to define new profiles based on re-adjusted staticscales.As future work, we propose to use the lessons learnedin this experiment to analyse a larger set of ontolo-gies. From our experience, reaching a community agree-ment for certain aspects of ontologies is not alwaysan easy task, such as to what extent axiomatic rich-ness is needed in biomedical ontologies [33]. On theone hand, those biomedical ontologies used as sim-ple plain taxonomies or controlled vocabularies do notneed a complex axiomatisation. On the other hand,those biomedical ontologies used as domain ontolo-gies should be as rigorous and axiomatically rich aspossible.This debate is also related to the OQuaRE qualitymodel. For example, the static scaling of the metricNOMOnto (see Table 4) could be interpreted as favouringmore plain taxonomies over heavily axiomatised ontolo-gies, because it would not be very difficult for ontolo-gies with low axiomatisation to obtain a high qualityscore for NOMOnto. Another example, ontologies with-out instances have lower scores for some metrics, butsometimes the absence of instances is a design criterionfor such ontologies. In such cases, the metrics that takeinto account instances should not be applied, or not con-sidered relevant.We are currently working on enablingOQuARE profiles, which would allow users or commu-nities of users to customise the associations betweenOQuaRE metrics, subcharacteristics and characteristics.The future OQuaRE users will be able to include newmet-rics or to define the scaling functions. The new metricswill have to be associated with current sub-characteristics.This solution is useful for users and communities withparticular needs.We consider that we could extend the idea of thedynamic scale and obtain a repository-based scale byusing a repository like Bioportal [2] or AberOWL [34]as reference. The repository-based scale would be theresult of applying the dynamic scaling method proposedin this paper but considering a vC? where ? representsthe ontologies and versions within the repository. Thisrepository-based scale would provide users some feed-back to determine the ranges of the static scaling functionbased on a large set of existing ontologies. However, work-ing with large repositories that can contain hundreds orthousands of versions for some ontologies can be chal-lenging. We plan to use a sliding window approach,which would include the last 10-20 versions of an ontol-ogy, or x versions that cover the whole life-cycle of theontology and having them equally separated across thetime period. Such representative sample of versions wouldbe used for creating the dynamic scaling function. Finally,the inclusion of new unsupervised clustering algorithmsthat automatically decide the number of categories ofquality scores for each metric based on the raw data is alsoin our future work.ConclusionsWe have developed a method that combines the analysisof versions with an ontology quality evaluation frame-work. The main objective of this paper was to study howthe OQuaRE framework can support ontology evolutionprocesses by informing, from the perspective of ontologyquality, about the changes observed across the differentversions of an ontology.The two scaling functions proposed in this work shouldbe jointly used for a better understanding of the engi-neering and the evolution of an ontology. The static scaleis more useful when a single version of an ontologyneeds to be inspected and evaluated from an engineer-ing point of view, or when there are significant dif-ferences between consecutive versions. However, whenthe different versions of an ontology are less distinctand evolution-oriented studies are our goal, the dynamicscale is able to provide more information. If we assumethat the scaling function normalises the values regard-less of the type of scale used, the values can be groupedand compared as done in this work with the magnitudeof the change or the mean change between versions. Itshould be noted that judging the evolution of an ontol-ogy in terms of how its content conforms to the domainthat is to be represented by the ontology are beyondthe scope of this work. That would be the main objec-tive of complementary methods such as realism-basedones [35, 36].Duque-Ramos et al. Journal of Biomedical Semantics  (2016) 7:63 Page 19 of 20The application of themethod to the EDAM reveals thatgood ontological engineering principles were applied in itsdevelopment. The analysis of changes in the quality scoresat both subcharacteristic andmetric levels have shown thecapability of the OQuaRE framework to identify weak-nesses and strengths of the ontology. The OQuaRE met-rics are capable of identifying changes in the engineeringof the different versions of the ontology. The design deci-sions of the developers of the ontology have produced 18versions of the EDAM ontology, and we have been able todescribe the impact of such decisions from the quality per-spective provided by OQuaRE: the scores for four charac-teristics increased, one characteristic remained invariant,and the scores for two characteristics decreased. Further-more, our study has found relations between the level ofclass activity and the variability of quality scores for theEDAM ontology. Evaluating the relation between thesechanges in the quality scores and the design decisionsof the ontology developers is beyond the scope of thepresent work. Our method provides the developers withdata they can use for evaluating whether their decisionshave the expected impact on the quality scores of theontology.In summary, we believe that the OQuaRE frameworkcontributes to the engineering of the analysis of the evolu-tion of ontologies and that provides relevant informationfor developers about the evolution of their ontologies.AbbreviationsEDAM: ontology of bioinformatics operations, types of data, formats, andtopics; OQuaRE: ontology quality requirements and evaluation; OO: objectoriented; PCA: principal component analysis; SQuaRE: software product qualityrequirements and evaluationFundingThis paper is an extension of the paper presented at ICBO 2015 [20]. Thisproject has been possible thanks to the Spanish Ministry of Science andInnovation and the FEDER Programme through grants TIN2014-53749-C2-2-R,BES-2011-046192 (MQM), and by the Fundación Séneca through grants15295/PI/10 and 19371/PI/14.Availability of data andmaterialsThe description of the OQuaRE framework is available at http://miuras.inf.um.es/oquarewiki. The OQuaRE web platform is available at http://sele.inf.um.es/oquare. The ontologies used in this study and the complete set of resultsincluded in this paper are available at http://miuras.inf.um.es/oquare/jbsm2016.Authors contributionsConceived and designed the approach: ADR, MQM, MIM, JTFB, RS.Implemented the approach and performed the experiments: MQM, ADR, MIM,JTFB, RS. Analysed the results: ADR, MQM, MIM, JTFB, RS. Contributed to thewriting of the manuscript: MQM, ADR, MIM, JTFB, RS. All the authors haveapproved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Ethics approval and consent to participateNot applicable.Author details1Universidad de Murcia, IMIB-Arrixaca, Campus de Espinardo, 30071 Murcia,Spain. 2School of Computer Science, University of Manchester, Oxford Road,M13 9PL Manchester, UK.Received: 8 March 2016 Accepted: 2 August 2016REVIEW Open AccessReporting phenotypes in mouse modelswhen considering body size as a potentialconfounderAnika Oellrich1,2, Terrence F. Meehan3, Helen Parkinson4, Sirarat Sarntivijai4,5, Jacqueline K. White6and Natasha A. Karp1*AbstractGenotype-phenotype studies aim to identify causative relationships between genes and phenotypes. The InternationalMouse Phenotyping Consortium is a high throughput phenotyping program whose goal is to collect phenotype datafor a knockout mouse strain of every protein coding gene. The scale of the project requires an automatic analysispipeline to detect abnormal phenotypes, and disseminate the resulting gene-phenotype annotation data into publicresources. A body weight phenotype is a common result of knockout studies. As body weight correlates with manyother biological traits, this challenges the interpretation of related gene-phenotype associations. Co-correlation canlead to gene-phenotype associations that are potentially misleading. Here we use statistical modelling to account forbody weight as a potential confounder to assess the impact. We find that there is a considerable impact on previouslyestablished gene-phenotype associations due to an increase in sensitivity as well as the confounding effect.We investigated the existing ontologies to represent this phenotypic information and we explored ways toontologically represent the results of the influence of confounders on gene-phenotype associations. With thescale of data being disseminated within the high throughput programs and the range of downstream studiesthat utilise these data, it is critical to consider how we improve the quality of the disseminated data andprovide a robust ontological representation.IntroductionIn genotype-phenotype studies, one approach to iden-tify abnormal phenotypes is a statistical comparisonof data collected from control and gene-altered ani-mals. In this paper we use the International MousePhenotyping Consortium (IMPC) statistical analysispipeline as a use case study [1]. The goal of theIMPC is to produce and phenotypically characterise20,000 knockout mouse strains in a reproduciblemanner across multiple research centres. This high-throughput phenotyping is based on a pipeline con-cept where a mouse is characterised in a series ofphenotype screens underpinned by standard operatingprocedures defined by the IMPC in the InternationalMouse Phenotyping Resource of Standardised Screens(IMPReSS) resource [2]. This pipeline approach char-acterises seven males and seven females for eachknockout line and results in data for over 200 physio-logical variables that cover a variety of disease-relatedand biological systems. As the scale of the programrequires the statistical analysis to be automated, wehave developed the statistical package PhenStat [3] toanalyse genotype-phenotype associations. In order toprovide a consistent representation of results, area ex-perts have reviewed the IMPReSS screens and haveassociated one or more terms from the MammalianPhenotype Ontology (MP) [4] with each variable. Forexample, the variable fasted blood glucose concentra-tion is associated to three MP terms: abnormal-,increased-, and decreased- -fasted circulating glu-cose level. Using this approach, abnormal phenotypesidentified via statistical analysis are summarised asgene-phenotype associations, easily understood by thebiological community and facilitating dissemination to* Correspondence: nk3@sanger.ac.uk1Mouse Informatics Group, Wellcome Trust Sanger Institute, Hinxton,Cambridgeshire, UKFull list of author information is available at the end of the article© 2016 Oellrich et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 DOI 10.1186/s13326-016-0050-8the community (Fig. 1). The current analysis pipelineonly takes sex into consideration when identifying ab-normal phenotypes. Sharing these gene-phenotype anno-tations also enables data mining across species and studiese.g. for disease gene candidate discovery, pharmacogenet-ics and evolutionary studies [57].During the statistical comparison of control and gene-altered data, confounding variables associated with boththe genotype change and the phenotype of interest canlead to an association that is true but potentially bio-logically misleading. The presence of the confoundingrelationship can lead to errors in the estimates of the re-lationship between the treatment of interest (here thegenotype change) and the variable of interest (here thephenotype). Good experimental design can managemany potential confounders using standardisation e.g.with the potential confounder of age, the study wouldonly test animals of the same age. An alternative strategyis randomisation, in which animals of multiple ages aretested in both control and the experimental knockoutgroup. Yet another strategy is grouping (blocking) ac-cording to a confounding variable (e.g. pup or adult).Depending on the strategy applied, the final annotationcould be specific to one particular age. To minimise thepotential impact of confounders within IMPC, the com-munity identified critical sources of variation in screensand used this to develop a standardised operating pro-cedure which, where possible, minimises variation andcaptures potential sources of variation as metadata witheach dataset. Metadata parameters (e.g. X-ray equip-ment) are included in the IMPReSS protocols and sub-mitted metadata is used to determine comparisongroups as part of the statistical analysis pipeline.In many research studies, it is not possible to man-age confounding variables during the design. For ex-ample, in many gene knockout studies, the knockoutanimals show an abnormal body weight change.Therefore, any other phenotypic traits (e.g. abnormalbody fat mass MP:0012320) that correlate with bodyweight will also be impacted. As the experimentercannot control this potential confounder through thedesign, it is necessary to consider statistical methodsfor non-equivalent groups [8]. These include regres-sion methods where the confounder is treated as acovariate, meaning the statistical test will assess theeffect of the genotype on the phenotype after adjust-ing for the confounders relationship. This requires adataset to be processed twice, first without and thenwith the confounder in the statistical analysis; givingtwo sets of results for the test of genotype. Thisgranularity has a high potential value to improve ourinterpretation of the relationship between a gene andassociated phenotypes. However, the vast majority ofMP terms represent absolute phenotype changes in avariable of interest. The Mouse Genome Informaticsdatabase (MGI) [9] developed MP to manually curatethe scientific literature. However, only in rare, clearcause and effect cases are confounding variables rep-resented as part of the ontology. For example, theterm progressive muscle weakness (MP:0000748) isdefined as a muscle weakness that increases withtime. Time or age are clearly contributing to the se-verity of the phenotype and thus represent knowledgethat should be represented in the ontology [10]. How-ever, in many studies a confounding variable is notedby authors to contribute to a phenotype, but a clearcause and effect relationship is not established. Thecurrent mechanism employed by MGI is to manageconfounders at the level of annotation by utilisingfree text qualifiers. For example, the curator will noteif an author states body weight was a confounderwhen associating a phenotype to a genotype. Withthe scale of IMPC data and the automated aspect ofstatistical analysis and subsequent annotation, we havethe potential to manage these issues in a consistentway and through standardisation better supportdownstream informatic analysis. The interest in in-cluding body weight as a covariate, in both highthroughput phenotyping studies and small scale stud-ies, is growing [8, 1113]. This manuscript aims toraise awareness of the issues and demonstrate the po-tential value of addressing the problems. We thenidentify adaptations to the existing mechanismsStatisticalanalysisExperimentReportingMPtermsDisease gene predictionApplicationPhenoDigmAnnotationsRawdataandAnnotationsControlGene-alteredFig. 1 The phenotyping pipeline. The high throughput phenotypingpipeline integrates a series of screens to assess the impact of thegenotype amendment on a variety of disease-related and biologicalsystems. Statistical analysis comparing data from the gene alteredand control animals allows the identification of abnormal phenotypes,assignment of ontology annotation and dissemination of data topublic database for data mining across species and studies. IMPCrepresents the International Mouse Phenotyping Consortium webportal [26] where the data is collected, analysed and annotationsdisseminated. Annotations are assigned using the Mammalianphenotype ontology (MP)Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 2 of 9utilised by the community that could address thisnew aspect where we wish to disseminate the out-come of an analysis that considers body weight as aconfounder.Data and scripts used to investigate and demonstrateissues presented within this manuscript are available atZenodo [14].Body weight as a confounderBody weight is a highly heritable trait and is estimated tobe a potential latent variable in a third of experimentsstudying knockout mice [11]. It has been shown that bodyweight correlates with many variables, ranging from bodycomposition to clinical chemistry [15]. Including bodyweight in the computational analysis allows the phenotypeto be assessed after adjusting for weight differences (seeAdditional file 1: Supplementary Methods).Dual analysis can lead to annotations that differ de-pending on the analysis pipeline (Table 1) as one canthen assess whether the phenotype has changed in arelative and absolute sense. For example, when the ab-normality is due solely to correlation with a body weightphenotype, then the inclusion of body weight as a covar-iate adjusts for this confounding relationship and thephenotype (as a relative term) would no longer be calledsignificant (Table 1 row 1). Alternatively, a line may onlyhave a significant abnormal annotation in the analysispipeline when body weight is included. The inclusion ofbody weight accounts for more variation in the data, in-creasing the sensitivity to detect other phenotypes(Table 1, row 3). Lines can also be significant in bothanalysis pipelines (Table 1, row 4), and this can arisefrom two scenarios which differ in whether there is abody weight difference or not. As the difference arisesfrom presence or absence of a body weight difference, itcould be argued that the interpretation could be drivenby the assessment of whether a body weight phenotypewas also annotated. However, a body weight phenotypemight be the reason statistically, but the abnormal bodyweight annotation might not have been made due to lowstatistical sensitivity (ability to detect a difference).For example, consider the Dlg4 knockout mouse linethat has a reduced body weight phenotype (MP:0001262)where we are also interested in assessing the impact of thegenotype change on body composition. As body compos-ition variables such as lean mass (MP:00039590) aredependent on the body weight, we would expect these tobe decreased as an absolute phenotype change (Fig. 2aand b). When we include body weight in the analysis, wefind that the change in lean mass is as expected for thechange in body weight and determine that the phenotyperelative to body weight is not statistically significant(Fig. 2c) (Equivalent to row 1 of Table 1). The knockoutgene Akt2 similarly has a body weight phenotype (Fig. 3a).However, the inclusion of body weight in the analysis findsthat the relative lean mass is still statistically significant(Fig. 3b-d) (Equivalent to row 4 of Table 1). By adding astatistical step where we study the phenotype after adjust-ing for body weight, we gain a more detailed understand-ing of the impact of the genotype on the phenotype.Even in cases where it is clear that body weight is trulyacting as a confounding variable and is not just explain-ing data variance (Table 1, row 1), causality is not deter-mined. For example, we cannot assess whether the leanmass is lower in the Dlg4 line because the body weightis fundamentally lower or because there is less lean massleading to a lower body weight. The refinement is there-fore to consider the data and assess for both relative andabsolute changes and disseminate this richness.Magnitude of impact and complexityThe Wellcome Trust Sanger Institutes (WTSI) MouseGenetics Project (MGP) is part of the IMPC communityeffort to phenotype knockouts for all mouse protein cod-ing genes [16]. To support the argument that we need toconsider body weight, we provide the results of a sup-porting analysis of the WTSI MGP data (see Additionalfile 1: Supplementary Methods for details). Firstly, wedemonstrate that for the majority of the dataset, weightTable 1 Possible outcomes of a dual analysis processRow A1 A2: + weight Conclusion Insight1 + - Absolute phenotype ? No longer significantconfounded by BW2 - - No abnormality3 - + Relative phenotype ? Adding weight increases sensitivity to detect ?4 + + Absolute phenotype ? and relative phenotype ? Two scenarios1. BW difference: still there is a significant ? as ? larger/smaller thanexpect for BW difference2. BW same: a significant ?. Weight explains variation but does notlead to phenotype difference.Possible outcomes when assessing for a genotype effect for a variable of interest when the analysis excludes (A1) or includes body weight as a covariate (A2). Inthis table, + indicates a statistically significant genotype effect;?indicates a non-significant genotype effect; ? indicates change; BW indicates body weightOellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 3 of 9is often a significant source of variation (Fig. 4). This isseen across biological processes and not only includesscreens that assess body composition but also screenssuch as plasma chemistry. Secondly, this data allows usto compare the impact of the dual analysis processusing the standard pipeline (A1) which does not ac-count for weight, compared to the additional analysispipeline (A2) including body weight as a covariate. Thisanalysis demonstrates that including body weight has asignificant impact on the final abnormality annotations(Fig. 5). We find that 70 % of the abnormal annotationsfrom the standard pipeline were also annotated whenwe included body weight in the analysis. Furthermore,we find that 30 % of annotations in the standard pipe-line (A1) were no longer significant in A2 as they arosefrom the confounding impact of body weight (equiva-lent to row 1 of Table 1). 21 % of the annotations in A2only occurred when body weight was included andBAAnalysis Genotype rolep valueGenotype effect p valueWithout weight (A1) 5.25e-6Female*knockout =-1.26±0.610.04017Male*knockout =-2.86±0.581.35e-6With weight (A2) 0.29345 -0.44±0.39 0.257CFig. 2 Example line Dlg4, where body weight confounds the phenotype. Body composition data were collected with a dual-energy X-rayabsorptiometry at 14 weeks of age for the Dlg4tm1e (EUCOMM) Wtsi/Dlg4tm1e (EUCOMM) Wtsi knockout line on the C57BL6/N genetic background.The comparison was based on 249 female and 227 male wildtype mice and 7 female and 7 male knockout mice. a A scatterplot of thelean mass readings for the control and knockout animals for the males. b A scatterplot of the lean mass readings for the control andknockout animals for the females. c The genotype estimate with associated standard error and statistical significance when estimated using standardmethodology (A1: Analysis Pipeline 1) and then after inclusion of body weight as a covariate (A2: Analysis Pipeline 2). As there was evidence of sexualdimorphism in the phenotype in A1, the genotype effect was estimated for male and female knockout mice separately. The scatter plots and analysishighlight how a body weight phenotype is observed in both sexes of the knockout animals and as the lean mass is associated with bodyweight, a statistically significant difference is seen in the lean mass until assessed as a relative abnormalityOellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 4 of 9arose from the increase in sensitivity from includingbody weight (equivalent to row 3 of Table 1).Challenges applying existing solutionsAs demonstrated with the provided analysis, taking con-founding variables such as body weight into accountmay lead to a more comprehensive dataset and shouldbe further investigated (see Table 1). The disseminationof the resulting annotation data is achieved through acollaboration between different communities. IMPC cur-rently uses MP to annotate genes with phenotypes. MPis a pre-composed phenotype ontology in which everyconcept semantically describes one particular phenotype,e.g. decreased lean body mass (MP:0003961). While thispaper generalises to gene-phenotype annotations, MGIdistinguishes further the additional data such as the gen-etic background or the sex if there is a difference be-tween male and female mice. While the majority of theannotations contained in MGI do not take confoundersinto consideration, sex in the presence of sexual di-morphism could be regarded as such and is captured attimes in MGI. For example, the gene Dmxl2 [17]BAAnalysis Genotype role p valueGenotype effectWithout weight (A1) 0.00e-7 -5.41±0.36With weight (A2) 4.02e-3 -1.01±0.34DCWTAkt2-/-Fig. 3 Example line Akt2, where body weight confuses the phenotype interpretation. Body composition data were collected with dual-energyX-ray absorptiometry at 14 weeks of age for the Akt2tm1e (KOMP) Wtsi/Akt2tm1e (KOMP) Wtsi knockout line on the 129S5/SvEvBrd/Wtsi;129S7/SvEvBrd/Wtsigenetic background. The comparison was based on 71 female and 84 male wildtype mice and 12 female and 14 male knockout mice. a A scatterplotof the lean mass readings for the wildtype and knockout animals for the males. b A scatterplot of the lean mass readings for the wildtype andknockout animals for the females. c Representative photograph demonstrating body weight phenotype. d The genotype estimate with associatedstandard error and statistical significance when estimated using the standard methodology (A1: Analysis Pipeline 1) and then after inclusion ofbody weight as a covariate (A2: Analysis Pipeline 2). The scatterplots of the lean mass against body weight highlight that there is a clear bodyweight phenotype and the difference between the knockouts and wildtype mice cannot be fully explained by the association between lean mass andbody weightOellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 5 of 9exhibits sexual dimorphism such that the phenotype wasonly found to be significant in the females of heterozy-gous mice and this is recorded as a curator note.Body weight is not the only variable that could be usedto adjust for the size of the animal; alternatives includebody length or width. Adjustment for body size as a con-founder has unique challenges (see section Body weightas a confounder) and particular issues with determin-ing causality. Thus, we investigated solutions for thestandardised reporting of phenotypes after consideringbody weight as a confounder as a relative phenotypechange within existing semantic frameworks and reportour findings here. Potential solutions were limited tothose we believed could be implemented as they had thelowest modification requirements on the existing dis-semination pipelines, such as those maintained by MGI.We note that the discussed solutions only focus on fu-ture dissemination but do not include strategies on howto deal with legacy data.Use of pre-composed ontologiesAs mentioned before, the vast majority of phenotypesrepresented in the current version of MP constituteabsolute changes that cannot readily be applied toconfounder-adjusted phenotypes. In order to representthe results of a confounder-sensitive analysis, additionalMP concepts would be needed that would allow a userto report relative phenotype changes (see column 2,Table 1, rows 3 and 4). For example, to represent thechanges in the absolute and relative changes in mouseline Dlg4, we would need the additional concept relativeincrease in lean body mass after body weight adjust-ment. However, pre-composing concepts for relativephenotype changes would mean that for each phenotypethat is influenced by one or multiple confounders (e.g.body size or length), multiple concepts for each uniquephenotype-confounder relationship would need to beadded (abnormal/increased/decreased). This would leadto a vast increase in the number of terms (i.e. term ex-plosion) that need to be added and maintained withinMP, which would be untenable. This may also be confus-ing for the user community of curators and annotatorsas the number and complexity of terms exposed forsearch and/or annotation grows.Tagging pre-composed termsAn adaption to the pre-composed term is to associate anattribute to the annotation by addition of free text tags.This is equivalent to the current implementation used inliterature curation at MGI. For example, a gene couldpossess an annotation increased lean body mass, withan annotation or tag on this annotation detailing if any/Modelling that included weight (%)Number of variables0 20 40 60 80 1000510152025Fig. 4 The inclusion of weight as a source of variation. Thedistribution of weight inclusion in the PhenStat analysis of 85086control-knockout datasets which covers 154 variables (averagenumber datasets = 552) from the high throughput phenotypingdata collected at the WTSI MGP. The PhenStat analysis was completedusing the Mixed Model framework with a starting model that includedweight. The model optimisation process means that the finalmodel will only include weight if it is statistically significant inexplaining variation in the data (p < 0.05)- Weight + WeightTested: 850861703751 521Fig. 5 The impact of including body weight as a covariate onabnormal phenotype annotations. The relationship between theabnormal phenotype annotations made when assessing for agenotype effect by processing through A1 (standard statisticalanalysis pipeline) and A2 (statistical analysis including bodyweight as a covariate). The analysis used a mixed model methodimplemented within PhenStat [9] on data collected by the WTSIMGP (for more details see Additional file 1: Supplementary Methods).Shown in red are those annotations, where the phenotype differencewas due to the confounding effect of body weight (row 1 of Table 1).Shown in green are those annotations where adding weight to theanalysis has increased sensitivity (row 3 of Table 1). Shown in yelloware annotations made in common by both pipelines (row 4 of Table 1).Data available from Zenodo [14]Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 6 of 9which confounder has been used for adjustment, e.g.after adjusting for body weight. However, as the tagsare not standardised this may result in non-comparableannotations of genes and an increase in curatorial work-load. Furthermore, informatics tools are not capable ofinterpreting tags of gene annotations and may lead to er-roneous presumptions (in the case of a relative changeafter confounder adjustment that would not be reportedwith absolute changes only; row 3, Table 1).In order to disseminate relative phenotype changes tothe broader community using tagged pre-composedphenotype ontology annotations, existing gene-annotationdatabases need to be able to store this additional data andexpose this for query. This may require not only changesto the database itself, but also to web interfaces as well asservices for data download, in addition to strategies forhandling legacy data.Standardised qualifiers of pre-composed termA refinement to the preceding method, is to add stan-dardised qualifiers to the genotype-phenotype annota-tion. One ontology that can be used to represent thesestandardised qualifiers is the Phenotype And TraitOntology (PATO) [18, 19]. The difference between thissolution and the previous is that the free text tag is re-placed with an ontology term. This suggestion is similarto how sexually dimorphic associations are currentlytreated. For example, Kcne2 knockout mice have a num-ber of abnormalities that are specific to the male miceand this is captured as a MP term with associated sexclassification tag [20]. The advantage of this solution isthat the variability that may occur with free-text tags isreduced to a defined set of ontology concepts. However,following this solution would need an agreed set of on-tologies used for the annotation of relative changes andpossibly extension to these to account for all possibleconfounders.Similar to the latter approach, third parties such asMGI can then choose to add these additional annota-tions to their data storage to hold the information forrelative phenotype changes. This may mean that data-base schemes as well as provision and distributionmethods need to be adapted to handle the additionaldata and be able to distinguish between absolute andrelative phenotype changes. If these changes were to beintegrated in existing databases, ways of handling legacydata need to be taken into consideration.Post-composed phenotypesAn alternative to pre-composed phenotype annotationsis the use of post-composed phenotypes. One method topost-composed phenotypes are entity-quality statements[18, 19], where the phenotype is broken down into an af-fected entity and a quality describing the entity further,e.g. increased body weight (MP:0001260) would bebroken down into the entity multicellular organism(UBERON:0000468, UBERON is a species-agnostic anat-omy ontology) [21] and the quality increased weight(PATO:0000582). The following example illustrates howa post-composed ontology-representation could be usedto represent a relative phenotype change:Entity 1: lean body massQuality: relative toEntity 2: body weightQualifier: increasedApplying a post-composed representation to confounder-adjusted phenotypes may lead to multiple sets of annota-tions to the same set of data as it still needs to be createdfor each confounder. Where required (e.g. Table 1, row 4),the absolute phenotype change could then be added as ithas been done so far with MP annotations or if desired,uniformly with post-composed phenotype annotations.Representation of confounder association with RDF triplerepresentationThe Standardised qualifiers of pre-composed termsapproach could be formally represented with the Re-source Description Framework (RDF) triple model [22].In an RDF triple, the annotation conforms to the formatof < subject, predicate, object>. In our scenarios thiswould be an MP term as the subject which would be re-lated to the confounder body weight (the object) via therelationship specified as the relative to (the predicate).The triple representation is only needed in the annota-tion arising from including the potential confounders ascovariates in the analysis and is a natural extension ofthe preceding approach Post composed phenotypes.There are multiple advantages of using RDF models.The first advantage arises from the graphical nature ofontologies in which the inter-relationships of multipletiers are captured with a graph schema. In an ontology,a class can have multiple parents leading to the inherit-ance of qualities from different parents, which can bewell and efficiently defined within RDF models. Thealternative of storing this information is to use a Rela-tional Database Management Systems (RDBMS). InRDBMS, a table scheme is used which faces the compu-tational challenges of multiple joins when queryingacross many tables and is therefore less scalable. Thesecond advantage is that RDF is a well-established com-munity standard recommended by the World WideWeb Consortium (W3C) [22] and is readily extendible.For example, the same MP term can be associated toother confounders (e.g. body length) using the samepredicate. This common structure will lead to a robustdata model which will improve efficiency when searchingOellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 7 of 9for information. The Ontology for Biomedical AssociatioN(OBAN) is an example of an RDF implementation andhas been successfully exploited to represent disease-phenotype associations [23] (Extended version will bepublished within this special issue) [24].RDF triples can be stored within relational as well asgraphical databases and data queries are performed withthe SPARQL query language [25]. In consequence, onewould need to understand the technology and the querylanguage to work with the data effectively, throughprovision of a (non-SPARQL) Application ProgramInterface (API) would address this for accessing the data.Conclusions and future perspectivesIn gene-phenotype studies, we have identified challengeswith reporting phenotypes after adjusting for bodyweight using currently available semantic data represen-tation frameworks. Weight is a complex confounder, asit cannot be controlled within the experiment and caus-ality cannot be determined. However, analysing the datawith and without body weight returns a richer under-standing of the phenotypic abnormality. With interestgrowing in the impact of body weight on phenotypesand the scale of projects being conducted by highthroughput phenotyping consortiums, being able to dis-seminate annotated phenotype data has become an im-portant issue. We have demonstrated that the impact ofincluding weight as a confounder in the analysis has sig-nificant impact on the annotations returned. While thisexample focuses on the description of mouse pheno-types, we perceive that this is a general problem withaccessing phenotypes in all mammals including humans.The current solution implemented with mouse data hasarisen from adapting the mechanisms developed for cur-ating literature to a high throughput scenario and use ofthe ontology for analyses.We coordinated our efforts with Medical ResearchCouncil (MRC) Harwell and MGI in discussions on re-fining annotation in high throughput phenotyping stud-ies, where MRC Harwell focused on aging studies andhow to manage time course studies [10]. The issues weredetermined to be distinct, as the interpretation is morecomplex when considering body weight as a confounder.The complexity arises as we cannot determine causality,rather we are annotating the outcome of the statisticalanalyses.In the process of this study, we were able to identifyseveral possible solutions (see Challenges applyingexisting solutions) that could help with applyingconfounder-relevant information to gene-phenotype as-sociations. These options have been limited to what webelieve have the lowest modification requirements onexisting dissemination pipelines, such as those main-tained by MGI. However, each of these outlined optionshave to be assessed now in the broader community toarrive at a conclusion what is the best to pursue.In future work, we aim to not only communicate withthe broader community to find the most suitable solu-tion, but also to assess the impact for other potentialconfounders not just body weight. These additional con-founders will then be verified with what has been deter-mined as the best solution to see that it can scale withthe demands of the different confounders.While we have assessed in this study the impact of con-founders of gene-phenotype associations in mouse, this ishighly likely to be equally relevant in other mammalianmodel organisms (e.g. rat). However, we identifiedpractical solutions based on the mouse annotation-dissemination pathways and these might not be theoptimal for other model organisms. The discussionswithin this manuscript are a good starting point formanaging confounder in their community.Additional fileAdditional file 1: Supplementary Methods. (DOCX 21 kb)AbbreviationsA1: analysis pipeline 1; A2: analysis pipeline 2; API: application programinterface ; BW: body weight; IMPC: international mouse phenotypingconsortium; IMPReSS: international mouse phenotyping resource ofstandardised screens; MGI: mouse genome informatics database;MGP: mouse genetics project; MP: mammalian phenotype ontology;MRC: medical research council; OBAN: ontology for biomedical association;PATO: phenotype and trait ontology; RDBMS: relational databasemanagement systems; RDF: resource description framework ; WTSI: wellcometrust sanger institute.Competing interestsThe authors have declared that no competing interests exist.Authors contributionsNAK conceived the question and executed the statistical analysis. AO, NAK,SS, TFM and HP analysed several ontological aspects that form thediscussions in the Challenges applying existing solutions section of thispaper. JKW managed the WTSI MGP phenotyping pipelines generating thedata used in the analysis. All authors have contributed to the writing of thispaper. All authors read and approved the final manuscript.AcknowledgementsWe thank staff from the Sanger Institutes Research Support Facility, MouseGenetics Project and Mouse Informatics Group for their excellent support.NAK, AO, TFM were funded by the National Institutes of Health CommonFund grant: (NIH) [1 U54 HG006370-01], HP is funded by EMBL-EBI Corefunds. SS is funded by the Centre for Therapeutic Target Validation. Inaddition, the individuals affiliated to the WTSI institute were also funded bythe Wellcome Trust grant: WT098051.We thank Cynthia Smith, Susan Bello and Janan Eppig from the JacksonLaboratory, James Malone of FactBio Ltd, and Ann-Marie Mallon and MichelleSimon from MRC Harwell for helpful discussion on these issues.The funders had no role in study design, data collection and analysis,decision to publish or preparation of the manuscriptAuthor details1Mouse Informatics Group, Wellcome Trust Sanger Institute, Hinxton,Cambridgeshire, UK. 2Social Genetic & Developmental Psychiatry, KingsCollege London, London, UK. 3Samples, Phenotypes and Ontologies,European Molecular Biology LaboratoryEuropean Bioinformatics Institute,Oellrich et al. Journal of Biomedical Semantics  (2016) 7:2 Page 8 of 9Hinxton, Cambridge, UK. 4Samples, Phenotypes and Ontologies, EuropeanBioinformatics Institute (EMBL-EBI), European Molecular Biology Laboratory,Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK. 5TheCentre for Therapeutic Target Validation, Wellcome Trust Genome Campus,Hinxton, Cambridge CB10 1SD, UK. 6Mouse Genetics Project, Wellcome TrustSanger Institute, Hinxton, Cambridgeshire, UK.Received: 21 October 2015 Accepted: 2 February 2016SOFTWARE Open AccessKnowledge Author: facilitating user-driven,domain content development to supportclinical information extractionWilliam Scuba1, Melissa Tharp1, Danielle Mowery1, Eugene Tseytlin2, Yang Liu3, Frank A. Drews4and Wendy W. Chapman1*AbstractBackground: Clinical Natural Language Processing (NLP) systems require a semantic schema comprised ofdomain-specific concepts, their lexical variants, and associated modifiers to accurately extract information fromclinical texts. An NLP system leverages this schema to structure concepts and extract meaning from the free texts.In the clinical domain, creating a semantic schema typically requires input from both a domain expert, such as aclinician, and an NLP expert who will represent clinical concepts created from the clinicians domain expertiseinto a computable format usable by an NLP system. The goal of this work is to develop a web-based tool,Knowledge Author, that bridges the gap between the clinical domain expert and the NLP system developmentby facilitating the development of domain content represented in a semantic schema for extracting informationfrom clinical free-text.Results: Knowledge Author is a web-based, recommendation system that supports users in developing domaincontent necessary for clinical NLP applications. Knowledge Authors schematic model leverages a set of semantictypes derived from the Secondary Use Clinical Element Models and the Common Type System to allow the userto quickly create and modify domain-related concepts. Features such as collaborative development and providingdomain content suggestions through the mapping of concepts to the Unified Medical Language SystemMetathesaurus database further supports the domain content creation process.Two proof of concept studies were performed to evaluate the systems performance. The first study evaluatedKnowledge Authors flexibility to create a broad range of concepts. A dataset of 115 concepts was created ofwhich 87 (76 %) were able to be created using Knowledge Author. The second study evaluated the effectivenessof Knowledge Authors output in an NLP system by extracting concepts and associated modifiers representing aclinical element, carotid stenosis, from 34 clinical free-text radiology reports using Knowledge Author and an NLPsystem, pyConText. Knowledge Authors domain content produced high recall for concepts (targeted findings:86 %) and varied recall for modifiers (certainty: 91 % sidedness: 80 %, neurovascular anatomy: 46 %).Conclusion: Knowledge Author can support clinical domain content development for information extraction bysupporting semantic schema creation by domain experts.Keywords: Natural Language Processing, Information extraction, Semantics, Knowledge representation, UnifiedMedical Language System* Correspondence: wendy.chapman@utah.edu1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT84108, USAFull list of author information is available at the end of the article© 2016 Scuba et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 DOI 10.1186/s13326-016-0086-9BackgroundNatural language processingNatural Language Processing (NLP) provides a set ofcomputational methods and techniques for automaticallyextracting and structuring information from free-textdocuments. NLP research has been successfully appliedto free texts for several applications ranging from se-mantic search to information extraction to text analytics[13]. The development and availability of biomedicalknowledge resources such as the Unified Medical LanguageSystem [4], has enabled biomedical NLP to move beyondretrieval and classification to modeling of semantic predi-cates represented in the literature [5]. Within the clinicaldomain, NLP systems have been implemented to supportpharmaco-vigilance, patient screening, patient narrativesummarization, and quality improvement [613]. The de-velopment of text processing pipelines and componentsspecific to clinical text such as the Medical LanguageExtraction and Encoding System (MedLEE) [14], clinicalText Analysis and Knowledge Extraction System (cTAKES)[15], and Health Information Text Extraction [16] have per-mitted the analysis of clinical free texts e.g., emergencydepartment notes, radiology reports, etc. using lexical, syn-tactic, and semantic information [17].OntologiesNLP tools designed to support information extractionroutinely use the Web Ontology Language (OWL) toprovide a structured way to represent domain content[1821]. In order for an NLP tool to use a given domainontology, the tool must contain code to parse and inter-pret the data model represented in the ontology. Thiscreates a close coupling between the ontology and theNLP tool. It is generally not possible to directly sharethe domain ontology used in one NLP tool with anotherNLP tool and semantic schematic changes are not easilypropagated between tools. To help resolve this issue ofincompatibility, a common type system [22] was devel-oped which provides a common framework to create on-tologies across a range of clinical domains. Our lab hasconverted the common type system described by Wu etal. into OWL format and extended its content using theSecondary Use Clinical Element Models (Secondary UseCEMs) [23]. We use this new OWL-base common typesystem, which we call the Schema Ontology, as theframework to create domain specific ontologies.The Schema Ontology can be loaded into Protégé [24]or other OWL editors and used as the template for do-main ontology creation. Domain ontology creation in thismanner, however, requires deep understanding of OWLand an understanding of the structure of the SchemaOntology data model. This creates a potentially burden-some learning curve for those users who simply want tocreate Schema-Ontology-based domain ontologies andhave little training in ontology development. To improveease of use and support wide-spread adoption of theSchema Ontology, a system which minimizes complexityand allows simple interaction for users is needed. Know-ledge Author provides a simple user interface to guideusers in development of complex domain ontologies.Furthermore, a domain ontology development tool thatsupports collaborative editing and has built-in access tothe UMLS would speed up the domain ontology cre-ation process. Many OWL editors, such as Protégé orNeOn [25] allow user-created plugins to extend theirfunctionality; however, there are no editors that are suf-ficiently modifiable to support all of this desired func-tionality. In this paper we introduce Knowledge Authorwhich provides a web-based interface that is simple touse, facilitates domain content development with directUMLS terminology lookup, and supports collaborativedomain content creation.ImplementationTerminologyThe terminology used in the domains of clinical NLPand ontology creation can often vary; however, for thepurposes of this paper the following terms are definedas such: Semantic Schema  the target extraction templatefor an NLP tool. Atomic Concept  a concept found in a standardizedterminology such as the UMLS. For example PNEUMONIA, TEMPERATURE, COUGH, orIBUPROFEN. Lexical Variant  a lexical variant is another way ofphrasing an atomic concept or modifier in the clinicaltext. This can include synonyms, misspellings andabbreviations. For example  two lexical variants forTEMPERATURE are temperature and temp. Modifier  additional information that narrowsdown, or modifies an atomic concept. KnowledgeAuthor separates the modifiers into two distincttypes  shared and semantic. Shared modifiers areapplicable to all concepts (with the exception ofPerson concepts which has its own unique set ofmodifiers such as age, race and gender). Semanticmodifiers vary depending on the semantic typeassociated with the concept. For example, a conceptwith a semantic type of Medication will have adifferent set of available modifiers than a conceptwith semantic type Vital Sign. Concept  the combination of the atomic conceptwith its associated modifiers and their lexical variants.For example  80 mg Ibuprofen is comprised of theatomic concept IBUPROFEN and semantic modifiersof Dosage: 80 mg. Lexical variants for IBUPROFENScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 2 of 11can include Advil, Midol, Motrin, Ibu, andIbuprofen, etc. Lexical variants for Dosage caninclude 80 mg, 80 mg, and 0.08 g, etc.Knowledge Author overviewThe overall goal of Knowledge Author is to aid a user inquickly creating a semantic schema, which is the targetextraction template for a clinical NLP tool. The semanticschema represents salient concepts of interest to be ex-tracted from the clinical text. It contains a list of atomicconcepts, associated modifiers and the lexical variants forthose concepts and modifiers. It is the job of the NLP sys-tem to extract words and phrases associated with theseconcepts and modifiers from the clinical text then mapthis information to the concepts in the semantic schema.Knowledge Author provides a web-based graphical userinterface that guides the user in developing a semanticschema, which is output as an OWL ontology. KnowledgeAuthor standardizes the semantic schema creation processby constraining concept creation to a set of standard se-mantic types (e.g., Procedure, Medication) and by onlyallowing the user to assign a pre-defined set of modifiersto the concept. The semantic types and modifiers arebased on the Secondary Use Clinical Element Models andthe Common Type System (CTS). The Secondary UseCEMS are semantic types and modifier sets used for com-puterized provider entry and secondary use of clinicaldata, and the CTS are semantic type sets used for infor-mation extraction from clinical text. By adhering to a stan-dardized data model it becomes possible to use the outputof Knowledge Author in any NLP system which imple-ments that model.Knowledge Author also supports the semantic schemacreation process by: Providing domain content suggestions throughmapping of user-created concepts to concepts in theUMLS Metathesaurus database. This allows the auto-matic import of synonyms, concept definition, and se-mantic type into the Knowledge Author interface. Supporting modifier creation through the use ofdropdown menus and the filtering of possiblemodifiers to only those relevant to a given concepttype. Dropdown menus are possible because theKnowledge Author data model has a fixed set ofallowable modifiers. Allowing the user to store and share their semanticschemas in an organized way. Supporting collaborative development of domaincontent.Using Knowledge AuthorTo illustrate the use of Knowledge Author, the creationof an example semantic schema for carotid stenosis willbe walked through. Figure 1 illustrates the KnowledgeAuthor workflow to be described below.Defining a conceptThe first step in Knowledge Author is to create a con-cept. Knowledge Author supports creation of two typesof concepts: Person and Event. A Person concept can bedefined with modifiers such as birth date, death date,race, age, and gender to facilitate creation of complexFig. 1 Illustrates the common set of steps to create a semanticschema using Knowledge Author. It is not required to map a conceptto UMLS terminology as the synonyms, definition and semantic typecan be entered in manually through the Knowledge Author interfaceScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 3 of 11concepts such as African American females above65 years of age.The carotid stenosis use case only has Event concepts.To create the first concept  aneurysm  the new con-cept button + (Fig. 2a) was clicked and the conceptname, aneurysm, was entered. Upon saving the newconcept, the Terminology Lookup button (Fig. 2b) be-comes available. Clicking that button allows the user tosearch the UMLS Metathesaurus for the concept nameand displays a list of potential matches (Fig. 3). For thisconcept there is a UMLS atomic concept ANEURYSMwhich we choose. Knowledge Author will now downloadthe definition, synonyms, semantic type and ConceptUnique Identifier (CUI) for that atomic concept. Allimported information can be changed, deleted, or sup-plemented as necessary. For the carotid stenosis ex-ample, twenty-six of the twenty-eight concepts were ableto be mapped to UMLS concepts.Choosing a semantic typeThe next step is to assign a semantic type to the con-cept. If the concept is mapped to a UMLS atomic con-cept, the semantic type for the atomic concept will havealready been downloaded and assigned to the concept(Fig. 2d). If not, the user can manually assign a semantictype. In the context of Knowledge Author, there are twotypes of modifiers  shared and semantic. The semantictype determines which type of semantic modifiers canbe assigned to the concept.Selecting semantic modifiersSemantic modifiers are a type of modifier that is associ-ated with specific semantic types. Each semantic typecontains a number of possible semantic modifiers basedon the Secondary Use CEMs and CTS. Each of the se-mantic modifiers has, in turn, a number of possiblevalues associated with it. For example, the semantic typeMedication allows the user to choose from semanticmodifiers such as dosage or delivery route. The deliveryroute modifier has a number of possible values such asoral or intravenous. Table 1 lists the 12 semantic types,the modifier classes associated with each semantic typeand the number of semantic modifiers associated witheach modifier class.Semantic modifier values can either be chosen from adropdown list, or for the case of numeric values, entereddirectly into an editable text box. Some modifiers, suchas medication dosage, consist of two numeric valueboxes and a dropdown list. The numeric value boxesallow the user to specify a value range, and the drop-down list is for units (Fig. 4). For example the user couldcreate a concept for 80 to 100 mg Ibuprofen (Fig. 4). Byleaving one or the other numeric value box empty con-cepts such as >80 mg Ibuprofen, or <80 mg Ibuprofencan be created. To create a single numeric value such as80 mg Ibuprofen, enter the same number into bothboxes. For the aneurysm concept created earlier, onlythe mild form is of interest so the sematic modifier of se-verity is enabled, and the value of mild is chosen fromthe dropdown list.Fig. 2 Knowledge Author concept creation interface. The large red letters with arrows point out a) concept creation button; b) terminologylookup button; c) shared modifiers; d) semantic type; e) concept listScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 4 of 11Selecting shared modifiersA user can also narrow the definition of a conceptthrough the use of shared modifiers. For all Event con-cepts, Knowledge Author allows the user to specify thetemporality (whether the concept occurs in the past,present, or future), certainty (whether the concept isasserted, negated, or hedged), and experiencer (whetherthe patient or someone else experiences the concept)(Fig. 2c). Several other shared modifiers are also avail-able (Table 2). For the carotid stenosis example, a con-cept for no occlusion is needed, so a new concept iscreated and linked to the atomic concept, OCCLUSION,which is then assigned the lexical variant for sharedmodifier for certainty: no from the certainty dropdownlist (Fig. 5). The user could also use shared modifiers tocreate concepts such as family history of breast canceror probable chest pain.Building a semantic schemaOnce a concept is created and saved, the + button isclicked to create a new concept and the process de-scribed above is repeated. A concept, once created, isadded to the concept list on the left hand side of theKnowledge Author GUI (Fig. 2e). The concept list canbe arranged by the order in which the concepts werecreated, or by the semantic type they belong to. Once allof the concepts are created, the user can export the se-mantic schema for use in an NLP system.Exporting dataAs the user works, Knowledge Author saves the userswork to an internal database that is available upon login.Once all of the domain content is entered into Know-ledge Author, the user can choose to export the data foruse in an NLP system. The Export button at the top ofthe application will prompt the user to save the outputfile to their computer.The file exported from Knowledge Author is OWLbased and imports and uses the classes defined in theSchema Ontology file. This file contains the semanticcategories and modifiers used by the interface as classes.The Schema Ontology is the base ontology file that or-ganizes these classes into appropriate hierarchies. ThisSchema Ontology file is then imported into every newdomain ontology created by Knowledge Author. Duringthe export process, each of the concepts is exported as asubclass of the appropriate semantic category class (i.e.,mild aneurysm is a subclass of the "Problem" class foundin the Schema Ontology). All of the concept metadata(i.e., synonyms, misspellings, preferred term, CUI fromUMLS, etc.) is added as annotation properties to thatclass. The modifiers are added as restrictions on theconcept class (i.e., mild aneurysm has the restriction"hasSemAttribute some Mild_Severity"). Therefore, all ofthe data gathered by the Knowledge Author user inter-face is transformed into an ontological representationthat can be parsed by a compatible NLP system.Fig. 3 UMLS terminology lookup interfaceScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 5 of 11Table 1 Semantic types, modifier classes, and modifiers available to the userSemantic Type Modifier Class # of Modifiers Sample of ModifiersAllergy Intolerance Allergy/Intolerance Type 2 allergy, intoleranceAllergen unlimited any drug or food conceptSeverity 7 mild, moderate, severeAnatomical Site Body Side 3 right, left, bilateralBody Laterality 33 dorsal, medial, superiorDisease Disorder Course 37 increased, worsened, maintainedSeverity 7 mild, moderate, severeEncounter From Location unlimited home, ER, SICU, nursing homeTo Location unlimited home, ER, SICU, nursing homeLab/Test/Measurement Abnormal Interpretation 3 abnormal, not abnormal, very abnormalDelta Flag 8 changed, unchanged, increasedLab/Test/Measurement Value unlimited 500 cc, 100 kg, 12000 WBCsOrdinal Interpretation 35 excessive, high, low, positiveMedication Medication Form 27 capsule, cream, liquid, tablet, pillMedication Route 21 inhalation, intradermal, oralMedication Strength unlimited 500 mgStatus Change 8 changed, unchanged, increasedDosage unlimited 250 mg, 16 unitsPatient Demographic Birth Date unlimitedDeath Date unlimitedAge unlimitedGender 2First Name unlimitedLast Name unlimitedMiddle Name unlimitedProblem Course 37 increased, worsened, maintainedSeverity 7 mild, moderate, severeProcedure Intervention Delta Flag 8 changed, unchanged, increasedProcedure Completion 3 complete, incomplete, N/AProcedure/Intervention Device unlimitedProcedure/Intervention Method unlimited arthroscopic surgerySign or Symptom Course 37 increased, worsened, maintainedSeverity 7 mild, moderate, severeSocial Risk Factor Delta Flag 8 changed, unchanged, increasedSocial Risk Qualifier 6 occasional, frequent, socialSocial Risk Quantity unlimited 5 packs, 3 drinksSocial Risk Status 5 former risk, current riskVital Sign Abnormal Interpretation 3 abnormal, not abnormal, very abnormalDelta Flag 8 changed, unchanged, increasedOrdinal Interpretation 37 excessive, high, low, positiveVital Sign Value unlimited 19 bpm, 86 %, 101.4 FScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 6 of 11It is also of note that the Knowledge Author outputfile can be viewed and modified directly by any OWLeditor such as Protégé. This could be useful for userswho want to use the Knowledge Author feature set, suchas UMLS terminology mapping, semantic schema man-agement, and dropdown lists, but have a small numberof concepts with rare features that are not currently sup-ported in Knowledge Author. Those concepts could beadded by hand using the OWL editor.Collaborative development and semantic schemamanagementOver time a user can develop a large number of seman-tic schemas. Each schema a user creates is saved to theKnowledge Author database and is accessible to the userupon login. The five most recent schemas a user workedon are displayed in the quick launch window. All otherschemas can be viewed in a searchable table.A semantic schema can be designated by the creatoras either public or private. Public schemas can beviewed and edited by anyone using Knowledge Author.This allows multiple users to work on the sameschema. It also allows for the creation of a library ofpublic schemas which can be used as the startingpoint for building a new schema in a similar domain.Private schemas can only be viewed and edited by theoriginal creator.Software tools and specificationsKnowledge Author is a web-based platform written inJava 7 on top of a MySQL database. It runs on anApache Tomcat 7 Server. The SeaCore [26] frameworkis used to facilitate the web development. The UMLSterminology is accessed through both the use of a localcopy of the UMLS database and the Java based UMLSTerminology Service API 2.0 [27] which queries a re-mote UMLS Metathesaurus service. The mapping of ausers concept to a UMLS atomic concept uses theUMLS Terminology Service API because of the com-plexity of performing that operation. The synonyms, def-inition, and semantic type for a concept are retrievedfrom the local copy of the UMLS for speed. The OWLAPI 3.4 [28] is used for converting the semantic schemasto OWL XML.Integration with existing NLP toolsCurrently, only the pyConText [29] NLP system acceptsthe output from Knowledge Author as input. Work isalso underway to integrate cTAKES and a developmen-tal system called Moonstone [30] with the KnowledgeAuthor output.Results and discussionKnowledge Author standardizes the concept creationprocess by constraining the semantic types and modi-fiers that can be assigned to a concept to a discreet set.This enables the use of dropdown lists for assigningmodifiers and allows for a standard output format whichmakes it possible to build NLP systems that use the out-put directly. We conducted two proof-of-concept stud-ies, using different datasets, to assess the usability ofKnowledge Author by demonstrating that (a) the userinterface is sufficiently flexible to allow for the creation ofmost concepts a user will want to create and (b) the out-put of Knowledge Author can be utilized by an NLP sys-tem to produce viable results.Fig. 4 Semantic modifier interface box showing numeric range input boxes with units dropdown listTable 2 Shared modifiers available to the userCategory Shared ModifiersCertainty Definite Existence, Definite Negated Existence, ProbableExistence, Probable Negated ExistenceExperiencer Patient, Family Member, Donor Family Member, DonorOther Member, Other MemberTemporality Before, Before-Overlap, Overlap, AfterContextualAspectContinues, Initiates, Intermittent, Novel, Reinitiates,TerminatesContextualModalityHypothetical, ConditionalDegree Little, MostPermanence Finite, PermanentScuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 7 of 11User interface flexibility assessmentWe assessed the flexibility of the Knowledge Authoruser interface by assembling a dataset of 115 concepts tobe created using Knowledge Author. The Additional file1 contains a full list of the concepts. The concepts weredrawn from three disease or procedure areas: pneumo-nia, colonoscopy quality, and influenza. The conceptswere selected to cover a range of complexity and providea broad view of the types of concepts that can and can-not be created using Knowledge Author.In order to assess whether or not the required con-cepts could be created using Knowledge Author, we con-sidered three degrees of representation: completecreation, partial creation, and no creation supported.We observed that 76 % (87 of 115) of the concepts forthe pneumonia, colonoscopy, and influenza use casescould be completely created using Knowledge Author.Table 3 describes the 24 % (28 of 115) of concepts thatcould be partially created in their entirety (see Additionalfile 1 for a full list of 115 concepts created). KnowledgeAuthor supported the creation of a very high proportionof simple concepts (69 of 73), but a lower proportion ofcomplex concepts (18 of 42) by the knowledge engineer.Complex concepts include compound concepts developedfrom two semantic types, such as lab test positive for in-fluenza. Knowledge Author supports creation of the con-cept lab test positive and influenza but does not yetsupport linking the two into a single concept. KnowledgeAuthor, also, does not support creation of concept repre-senting a single atomic concept with a set of modifierscombined with a disjunction, such as new or progressiveinfiltrate. The four simple concepts that were not ableto be created in Knowledge Author are a result of the re-quired modifiers not being listed in the Knowledge Authordata model.Even though Knowledge Author does not support thecreation of some concepts, it is possible to add the de-sired data by hand outside of Knowledge Author. TheKnowledge Author data model allows for the use of theSemantic Web Rule Language (SWRL) [31] rules, eventhough the Knowledge Author interface itself does not.SWRL is an OWL-based rule language. Through man-ual editing of the Knowledge Author output file, com-plex variables can be created by inserting SWRL rules.For modifiers that are not in the data model, it is pos-sible to add the appropriate modifier classes by handto the Knowledge Author output file. Correctly de-signed NLP tools that use the Knowledge Author out-put are able to handle user created classes. Having toadd information outside of the Knowledge Authorinterface is time consuming and as Knowledge AuthorFig. 5 Certainty shared modifier dropdown listTable 3 Types and number of concepts that were not able tobe created in Knowledge AuthorReason Not Created Total # ofConcepts% of Total(115)Element or modifier typenot found in Schema Ontology21 18 %Relation between concepts missing - couldonly create separate concepts without linking7 6 %Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 8 of 11matures we expect to expand its functionality to coverthe vast majority of concepts.Knowledge Author-powered information extractionevaluationWe assessed the viability of the Knowledge Author out-put for use in clinical NLP by creating a semanticschema for carotid stenosis in Knowledge Author andusing it as the target extraction template in the pyCon-Text [32, 33] NLP system.pyConText is a regular-expression, rule-based infor-mation extraction system which accepts two files  onefor target concepts and one for associated modifiers.The target file contains regular expressions or lexicalvariants describing target concepts of interest such asthose representing carotid disease. The modifier filecontains regular expressions or lexical variants describ-ing the types of modifiers such as certainty, anatomicallocation or temporality. A software script was written toautomatically marshal the data contained in the Know-ledge Author output file into the file format and schemasupported by pyConText.We selected 34 carotid ultrasound reports from theMT Samples corpus [34] that were used in a previousstudy [32]. The reports were de-identified and selectedat random from the MT Samples corpus. Two physi-cians independently annotated each report and adjudi-cated each disagreement with consensus review using anannotation tool called eHOST [35]. Each report was an-notated for the targeted finding concepts for carotidstenosis along with the following associated modifiers:certainty, sidedness, and neurovascular anatomy.We applied pyConText using the Knowledge Authorsemantic schema to the texts and converted its outputto Knowtator.xml to be read into eHOST to conductour error analysis. We computed recall for each type oftarget and modifiers (the proportion of concept men-tions correctly identified from the reference standard)because we are predominately concerned with whetherwe have enough lexical variants to identify these con-cepts from free-text.Reasonably high recall was achieved identifying tar-geted finding concepts (86 %) and shared modifiers (cer-tainty: 91 %) and high to low recall for the semanticmodifiers (sidedness: 80 %, neurovascular anatomy:46 %) (Table 4).The low recall can be partially attributed to missingcues from the terminology lookup. In particular, manyfalse negatives were due to missing acronyms and abbre-viations in the semantic modifier file e.g., ICA whichstands for neurovascular anatomy: Internal carotid ar-tery and l which stands for sidedness: left which arecommonly used in carotid ultrasound reports. Addition-ally, low recall can be partially attributed to the inabilityfor Knowledge Author to represent ranges of severity forsome semantic modifiers e.g., 70-80 % which indicatessignificant stenosis. We are actively incorporating thisfunctionality in the system. A manual input of additionalacronyms and abbreviations using the Knowledge Au-thor synonym interface and manual input of regular ex-pressions for semantic modifiers using an OWL editorcould improve the results. Overall, this result suggeststhat the Knowledge Author output has the potential tobe used by an NLP system to create viable results.Future developmentWe are continuing to develop Knowledge Author andadd new features. Some of the features that we expect tobe added in the near future include: Adding constructs that will allow users to linkconcepts together using relationships (i.e. ibuprofentreats pain) and logical operators. Allowing the user to search a default corpus of de-identified medical records for phrases that would po-tentially be retrieved for the new concept. This wouldallow the user to test the accuracy of synonyms andnumeric thresholds. Allowing the user to share and collaboratively workon an ontology with a select group of users.Knowledge Author is the first part of a pipeline thatwill allow the user to create an NLP schema, annotatedocuments, process documents using various NLP sys-tems, and analyze the results. We envision an end-to-end system that allows the user to rapidly build customclinical text queries using a variety of NLP systems. Weare actively developing a recommendation modulewithin the pipeline that will suggest new lexical variantsfor concepts and modifiers from clinical text leveragingactive learning methods to improve recall i.e., acronymsand abbreviations observed from development data inreal-time. Currently, only the pyConText algorithm usesthe output from Knowledge Author. Additional systemsare under development.ConclusionsKnowledge Author is a new, web-based tool for buildinga semantic schema of domain content that could be usedTable 4 pyConText performance leveraging Knowledge Authorknowledge baseConcept Types Total Correct RecallTargets Findings 79 68 86 %Modifiers Certainty 11 10 91 %Sidedness 41 33 80 %Neurovascular Anatomy 41 19 46 %Scuba et al. Journal of Biomedical Semantics  (2016) 7:42 Page 9 of 11in an NLP application. It leverages three existing know-ledge resources  the Secondary Use CEMs, CTS, andthe UMLS  to provide the user with relevant informa-tion for creation of domain-specific concepts, which al-lows for rapid semantic schema creation. The output ofKnowledge Author can be used directly as input intocompatible NLP systems.Availability and requirementsKnowledge Author is publically available and can be foundat http://blulab.chpc.utah.edu/KA/. The user can create anaccount to access the tool by clicking on the Create Ac-count link. The data model used by Knowledge Authorcan be found at http://blulab.chpc.utah.edu/ontologies/SchemaOntology.owl. The completed carotid stenosis se-mantic schema can be found at http://blulab.chpc.utah.edu/ontologies/schemas/bscuba/carotid_stenosis.owl and in theAdditional file 2.Additional filesAdditional file 1: Full list of use case concepts. (XLSX 12 kb)Additional file 2: Carotid Stenosis OWL file. (OWL 232 kb)AbbreviationscTAKES, clinical Text Analysis and Knowledge Extraction System; CTS,common type system; CUI, Concept Unique Identifier; eHOST, extensibleHuman Oracle Suite of Tools; HiTex, Health Information Text Extraction;MedLEE, Medical Language Extraction and Encoding System; NLP, NaturalLanguage Processing; OWL, Web Ontology Language; Secondary Use CEM,Secondary Use Clinical Element Model; SWRL, Semantic Web Rule Language;UMLS, United Medical Language System; XML, Extensible Markup LanguageAcknowledgementsWe would like to acknowledge Effective Dynamics for their excellent workprogramming the system. This work was supported by National Library ofMedicine grant LM010964.Authors' contributionsWWC, MT and WS designed the Knowledge Author interface. WS was thesoftware architect and project manager. WWC provided the vision for the project.MT managed all things related to the Schema and Modifier Ontologies. ET andMT coded the OWL file input and output. FD provided interface design support.YL coded the initial Knowledge Author prototype. MT created the use cases fortesting. DM implemented and assessed the proof of concept study leveragingpyConText. MT, WS, DM, and WWC drafted the manuscript. All authors read andapproved the final manuscript.Competing interestsThe authors declare that they have no competing interests.Author details1Department of Biomedical Informatics, University of Utah, Salt Lake City, UT84108, USA. 2Department of Biomedical Informatics, University of Pittsburgh,Pittsburgh, PA 15206, USA. 3University of California, San Diego, CA 92093,USA. 4Department of Psychology, University of Utah, Salt Lake City, UT 84108,USA.Received: 28 February 2015 Accepted: 1 June 2016DATABASE Open AccessAn ontology for major histocompatibilityrestrictionRandi Vita1* , James A. Overton1, Emily Seymour1, John Sidney1, Jim Kaufman2, Rebecca L. Tallmadge3,Shirley Ellis4, John Hammond4, Geoff W. Butcher5, Alessandro Sette1 and Bjoern Peters1AbstractBackground: MHC molecules are a highly diverse family of proteins that play a key role in cellular immune recognition.Over time, different techniques and terminologies have been developed to identify the specific type(s) of MHC moleculeinvolved in a specific immune recognition context. No consistent nomenclature exists across different vertebrate species.Purpose: To correctly represent MHC related data in The Immune Epitope Database (IEDB), we built upon apreviously established MHC ontology and created an ontology to represent MHC molecules as they relate toimmunological experiments.Description: This ontology models MHC protein chains from 16 species, deals with different approaches used toidentify MHC, such as direct sequencing verses serotyping, relates engineered MHC molecules to naturally occurringones, connects genetic loci, alleles, protein chains and multi-chain proteins, and establishes evidence codes for MHCrestriction. Where available, this work is based on existing ontologies from the OBO foundry.Conclusions: Overall, representing MHC molecules provides a challenging and practically important test casefor ontology building, and could serve as an example of how to integrate other ontology building efforts intoweb resources.Keywords: Major histocompatibility complex, Ontology, MHC, Immune epitopeBackgroundMajor histocompatibility complex (MHC) proteins playa central role in the adaptive immune system. Firstdiscovered due to their role in transplant rejection,MHC molecules are encoded by a large family of geneswith wide variation within each species. MHC moleculestypically bind peptide fragments of proteins and displaythem on the cell surface where they are scanned by Tcells of the immune system. If a peptide fragment isdisplayed by MHC, it can trigger a T cell immune re-sponse. Peptides triggering a response are referred to asepitopes. Thus, binding of epitopes to MHC moleculesis an integral step for immune recognition. The specificMHC molecule that presents an epitope to a T cell isknowns as its MHC restriction, often called its MHCrestriction (or restricting) element. Accurately representingthis MHC restriction, which can be determined in differentmanners, is the goal of the work presented here. MostMHC molecules consist of two protein chains, of which atleast one gene is present within the MHC locus. In humansthis locus is known as the human leukocyte antigen (HLA)and is depicted in Fig. 1a. There are thousands of differentallelic variants of these genes coding for different proteinsthat result in diverse MHC binding specificities found inthe human population. The most precise way of specifyingMHC restriction is to identify the exact protein chains thatmake up the MHC molecule. However, until recently suchexact molecular typing was not possible, and patterns ofantibody binding were utilized to group MHC moleculestogether into serotypes that share a common serological(antibody based) recognition pattern, as shown in Fig. 1b.Tying such traditional serotype information together withcurrent sequence based MHC typing techniques is one ofthe goals of our study. In yet other cases, such as inbredmouse strains, MHC restriction is narrowed down basedon the haplotype of the animal, the set of alleles present on* Correspondence: rvita@liai.org1La Jolla Institute for Allergy and Immunology, 9420 Athena Circle La Jolla,San Diego, California 92037, USAFull list of author information is available at the end of the article© 2016 Vita et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Vita et al. Journal of Biomedical Semantics  (2016) 7:1 DOI 10.1186/s13326-016-0045-5a single chromosome and thus expressed consistentlytogether in select subspecies or strains. Another way MHCrestriction is sometimes inferred is based on the T cells rec-ognizing the epitope. MHC molecules are divided intothree classes: MHC class I, MHC class II, and non-classicalMHC. MHC class I molecules present epitopes to CD8+ Tcells and are made up of one alpha chain and one ?2microglobulin chain, which is invariant and encoded out-side the MHC locus. MHC class II molecules present epi-topes to CD4+ T cells and are composed of one alpha andone beta chain, as shown in Fig. 1c. Thus knowing if theresponding T cell expresses CD4 verses CD8 can be used tonarrow down the possible MHC restriction into classes. Atthe same time, current research has identified thatsome T cell populations do not follow this pattern exactly(e.g. some T cells recognizing MHC-II restricted epitopesexpress CD8). It is therefore important to capture not onlythe inferred restriction information, but also the evidenceupon which it was based.MethodsThe Immune Epitope Database (www.iedb.org) presentsthousands of published experiments describing the rec-ognition of immune epitopes by antibodies, T cells, orMHC molecules [1]. The data contained in the IEDB isprimarily derived through manual curation of publishedliterature, but also includes some directly submitteddata, primarily from NIAID funded epitope discoverycontracts [2]. The goal of the current work was to repre-sent MHC data as they are utilized by immunologists tomeet the needs of the IEDB users. We collected userinput at workshops, conferences and the IEDB help sys-tem regarding how they wanted to retrieve data fromthe IEDB regarding MHC restriction. These requestswere used to identify goals for this ontology project andthe final ontology was evaluated if it could answer theserequests. As shown in Additional file 1: Table S1, anexample of such a request was to be able to query forepitopes restricted by MHC molecules with serotype A2and retrieve not only serotyped results but also thosewhere the restriction is finer mapped e.g. to MHC mol-ecule A*02:01 which has serotype A2. We set out tologically represent the relationships between the genesencoding MHC, the haplotypes linking together groupsof genes in specific species, and the individual proteinscomprising MHC complexes, in order to present immuno-logical data in an exact way and to improve the functional-ity of our website. Our work builds on MaHCO [3], anontology for MHC developed for the StemNet project,using the well-established MHC nomenclature resources ofthe international ImMunoGeneTics information sys-tem (IMGT, http://www.imgt.org) for human data andThe Immuno Polymorphism Database (IPD, http://www.ebi.ac.uk/ipd) for non-human species. It containsFig. 1 MHC presentation and restriction. a. HLA locus of human chromosome 6 encodes specific MHC protein chains. b. The MHC on APCpresenting epitopes can be bound by antibodies to establish the serotype. c. If responding effector cells are known to be CD4 cells, the MHCpresenting the epitope can be presumed to be class II restrictedVita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 2 of 7118 terms for MHC across human, mouse, and dog.We were encouraged by the success of MaHCO in ex-pressing official nomenclature using logical defini-tions. However, we needed to extend it for thepurpose of the IEDB to include data from a growinglist of 16 species, as well as data about MHC proteincomplexes (not just MHC alleles), haplotypes and se-rotypes. Thus, our current work goes beyond MaHCO,and we have utilized this opportunity to also enhancethe integration with other ontological frameworks.We used the template feature of the open sourceROBOT ontology tool [4] to specify the content of ourontology in a number of tables. Most of the tables cor-respond to a single branch of the ontology hierarchy,in which the classes have a consistent logical structure,e.g. gene loci, protein chains, mutant MHC molecules,haplotypes, etc. The OWL representation of our ontol-ogy is generated directly from the tables using ROBOT.This method enforces the ontology design patterns wehave chosen for each branch, and makes certain editingtasks easier than with tools such as Protégé.Results and discussionOur MHC Restriction Ontology (MRO) is available in apreliminary state at https://github.com/IEDB/MRO. It isbased on existing ontology terms, including: materialentity from the Basic Formal Ontology (BFO) [5], pro-tein complex from The Gene Ontology (GO) [6], pro-tein from The Protein Ontology (PRO) [7], organismfrom The Ontology for Biomedical Investigations (OBI)[8], genetic locus from The Reagent Ontology (REO)[9], has part, in taxon, and gene product of  from TheRelation Ontology (RO) [10]. The NCBI Taxonomy wasused to refer to each species [11]. Although it is not yetcomplete, we strive to conform to Open Biological andBiomedical Ontologies (OBO) [12] standards. MROcurrently contains 1750 classes and nearly 9000 axioms,including more than 2100 logical axioms. Its DL expres-sivity is ALEI, and the HermiT reasoner [13] completesreasoning in less than 10 seconds on a recent laptop.Synonyms were also included, as immunologistsoften utilize synonyms that are either abbreviations orbased on previous states of the nomenclature. Thecurrent MHC nomenclatures for various species havebeen revised through several iterations. In order toensure accuracy and remain up to date with the latestnomenclature, we referred to the well-establishedMHC nomenclature resources of the IMGT and IPD.For specific species where the literature was mostformidable, such as chicken, cattle, and horse, we col-laborated with experts in these fields. These expertsreviewed the encoded hierarchy by determining whetherthe inferred parentage hierarchy in their area of expertisereflected their input.Each MHC molecule for which the IEDB has data ismodeled as a protein complex consisting of two chains.Each chain is a gene product of a specific MHC geneticlocus. For certain species, sub-loci are also defined, whenuseful. For example, as shown in Fig. 2 HLA-DPA1*02:01/DPB1*01:01 consists of one HLA-DPA1*02:01 chain,encoded by the DPA sub-locus of DP, and one HLA-DPB1*01:01 chain, encoded by the DPB1 sub-locus of DP.Together these two chains make up one DPA1*02:01/DPB1*01:01 MHC molecule.When the identity of only a single chain of the com-plex is known, a generic second chain is used tomake up the MHC complex. Thus, MHC restriction ofHLA-DPB1*04:02 is modeled as one HLA-DPB1*04:02chain in complex with an HLA-DPA chain that is notfurther specified, as shown within the context of thehierarchy in Fig. 3.The data in the ontology drives the Allele Finder onthe IEDB website, available at http://goo.gl/r8Tgrz, aninteractive application that allows users to browse MHCrestriction data in a hierarchical format. We evaluatedthe ability of MRO to meet the needs of IEDB users, asshown in Additional file 1: Table S1, and found it tomeet our initial goals. Currently the use of the ontologyis behind the scenes, but we have requested namespaceand permanent identifiers from The Open BiomedicalOntologies (OBO). As soon as these identifiers are inplace, they will be utilized and displayed on the IEDBwebsite to allow users to link out to the ontology.In MHC binding and elution assays, the exact MHCmolecule studied is typically known; however this isoften not the case for T cell assays. When a T cell re-sponds to an epitope, the identity of the MHC moleculepresenting the epitope may not be known at all, it maybe narrowed down to a subset of all possible moleculesor it may be exactly identified. In the context of T cellassays, the MHC restriction can be determined by thegenetic background of the host, conditions of the experi-ment, or the biological process being measured; there-fore we represent MHC molecules at a variety of levelsand specify the rationale behind the determined restric-tion using evidence codes.As shown in Fig. 4a, IEDB Evidence codes includeauthor statement for cases where authors report pre-viously defined restriction and MHC ligand assayused for MHC restriction established via an experimentthat demonstrated the ability of the epitope to bindstrongly to the MHC molecule or to have been elutedfrom that molecule. Figure 4b shows the metadata asso-ciated with this evidence code. MHC binding predic-tion is used when computer algorithms are used topredict the likelihood of an epitope to bind to a specificMHC molecule. In cases where authors analyze theMHC phenotype of a study population and conclude aVita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 3 of 7likely restriction based upon epitope recognition pat-terns among the subjects, statistical association isused as the evidence code. We use a set of evidencecodes to communicate restriction shown by the response ofT cells to the epitope: MHC complex. These include SingleMHC available for cases where T cells respond to the epi-tope when only a single MHC molecule is available and re-activity of same T cells with different MHC is used whendifferent APC expressing different MHC are used to nar-row the potential restriction. The use of antibodies to blockor purify subsets of MHC molecules typically determinesrestriction to an imprecise level, such as HLA-DR and isconveyed by set of MHC available. When the Tcells beingstudied are known to be CD8 or CD4 cells, the restrictioncan be deduced to be class I or class II, respectively, due tothe known binding pattern of the molecules, as depicted inFig. 1c. This case is communicated by the evidence code oftype of effector T cell. Lastly, certain T cell responses canindicate the effector cell phenotype of CD8 or CD4, basedupon known functions of the subsets and thus, class I or IIrestriction can be inferred and is noted by the evidencecode of biological process measured. Figure 4c shows theFig. 2 Ontologic relationships between MRO termsFig. 3 Ontological model showing human MHC class II moleculesVita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 4 of 7modeling of these evidence codes in terms of the specificexperiments, data transformations performed (using OBIterms), and the type of conclusion drawn. This work isbeing conducted in parallel with the general alignment ofthe Evidence Ontology (ECO) [14], which provides succinctcodes for such types of evidence, with OBI, which canbreak down how such a code translates to specific experi-ments performed.The IEDB MHC Allele Finder application, shown inFig. 5, now allows users to browse data in differentviews. MHC molecules are first categorized into classI, class II or non-classical, and then further subdividedby species. Within each species, MHC molecules areorganized by genetic locus. For select species, such ashuman, there are a large number of MHC moleculesknown and studied per genetic locus, thus sub-loci arealso used in order to present the data in a more user-friendly format. Each MHC molecule is presentedunder its locus, its haplotype, and/or its serotype,when available, all representing newly added function-alities. The haplotype the host species expresses isrepresented as immunologists often rely on the knownhaplotypes of research animals to narrow the potentialMHC restriction. For example, when BALB/c (H2d)mice demonstrate a response to an epitope and theresponding T cells are CD4+, the restricting MHC canbe assumed to be one of the two MHC class II mole-cules of that haplotype, namely H2 IAd or IEd.The serotype of an MHC molecule, defined by anti-body staining patterns, is relevant in immunology as thiswas the method of choice to identify MHC moleculesuntil quite recently. In contrast to molecular definitionsFig. 4 Evidence codes in MROVita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 5 of 7of MHC molecules based on their specific nucleotide oramino acid sequence, serotyping classifies MHC mole-cules based entirely on antibody binding patterns to theMHC molecule. These patterns are linked to the panelof antibodies used. Changing the antibody panel changesthe serotype of a molecule. This can result in serotypesplits where MHC molecules that were previously con-sidered identical by one antibody panel, are later foundto actually be two different molecules by a different anti-body panel. To reflect this extrinsic nature of serotyping,we refer to serotypes as information entities rather thanphysical entities. Alternatively, the concept of serotypecould also be modeled as collections of binding disposi-tions, but we chose what we thought was the simplerapproach. MHC for all 16 species currently having MHCdata in the IEDB are modeled to give users the ability tobrowse the tree in multiple ways and search IEDB databroadly, by entire MHC class, for example, or narrowlyby a specific MHC protein chain. As new MHC mole-cules are encountered, they can be easily incorporatedinto this ontology.ConclusionsIn conclusion, we formally represented MHC databuilding on established ontologies in order to representMHC restrictions as required by immunologists. Ac-cordingly, we modeled MHC molecules as a proteincomplex of two chains and established the relationshipsbetween the genes encoding these proteins, the haplo-types expressed by specific species, and the MHC clas-ses. Traditional serotype information was also relatedto specific MHC molecules. Precise MHC restrictionwas conveyed, as well as inferred MHC restriction andalso the experimental evidence upon which the restric-tion was established. We will continue to formalize thiswork and will release a completed interoperable ontol-ogy later this year. Thus, MHC data in the IEDB is nowpresented to its users in a hierarchical format whichsimplifies searching the data and additionally instructsusers on the inherent relationships between MHCgenes and MHC restriction.Additional fileAdditional file 1: Goals and status of the MRO project. (XLSX 16 kb)AbbreviationsMHC: Major histocompatibility complex; IEDB: The Immune EpitopeDatabase; APC: Antigen presenting cell; HLA: Human leukocyte antigen;IMGT: ImMunoGeneTics; IPD: Immuno Polymorphism Database; MROMHC: Restriction Ontology; BFO: Basic Formal Ontology; GO: Gene Ontology;PRO: Protein Ontology; OBI: Ontology for Biomedical Investigations;ECO: Evidence Ontology; OBO: The Open Biomedical Ontologies.Competing interestsThe authors declare that they have no competing interests.Authors contributionsRV, JAO and BP conceived of the ontology, and participated in its design andcoordination and helped to draft the manuscript. ES prepared and analyzedMHC datasets. JS, JK, RLT, SE, JH, GWB, AS and BP provided expert guidancewith relationship to specific MHC subsets to direct the development of theontology. All authors read and approved the final manuscript.AcknowledgementsWe wish to thank Kirsten Fischer Lindahl and Lutz Walter for their kindassistance with the mouse and rat MHC molecule nomenclatures, respectively.The Immune Epitope Database and Analysis Project is funded by theNational Institutes of Health [HHSN272201200010C].Fig. 5 IEDBs MHC Allele Finder, demonstrating chicken haplotypesVita et al. Journal of Biomedical Semantics  (2016) 7:1 Page 6 of 7Author details1La Jolla Institute for Allergy and Immunology, 9420 Athena Circle La Jolla,San Diego, California 92037, USA. 2University of Cambridge, Trinity Ln,Cambridge CB2 1TN, UK. 3Cornell University College of Veterinary Medicine,Ithaca, New York 14853-6401, USA. 4The Pirbright Institute, Ash Rd, WokingGU24 0NF, UK. 5The Babraham Institute, Cambridge CB22 3AT, UK.Received: 29 September 2015 Accepted: 3 January 2016Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 DOI 10.1186/s13326-016-0077-xRESEARCH Open AccessThe Orthology Ontology: developmentand applicationsJesualdo Tomás Fernández-Breis1*, Hirokazu Chiba2, María del Carmen Legaz-García1 and Ikuo Uchiyama2AbstractBackground: Computational comparative analysis of multiple genomes provides valuable opportunities tobiomedical research. In particular, orthology analysis can play a central role in comparative genomics; it guidesestablishing evolutionary relations among genes of organisms and allows functional inference of gene products.However, the wide variations in current orthology databases necessitate the research toward the shareability of thecontent that is generated by different tools and stored in different structures. Exchanging the content with otherresearch communities requires making the meaning of the content explicit.Description: The need for a common ontology has led to the creation of the Orthology Ontology (ORTH) followingthe best practices in ontology construction. Here, we describe our model and major entities of the ontology that isimplemented in the Web Ontology Language (OWL), followed by the assessment of the quality of the ontology andthe application of the ORTH to existing orthology datasets. This shareable ontology enables the possibility to developLinked Orthology Datasets and a meta-predictor of orthology through standardization for the representation oforthology databases. The ORTH is freely available in OWL format to all users at http://purl.org/net/orth.Conclusions: The Orthology Ontology can serve as a framework for the semantic standardization of orthologycontent and it will contribute to a better exploitation of orthology resources in biomedical research. The resultsdemonstrate the feasibility of developing shareable datasets using this ontology. Further applications will maximizethe usefulness of this ontology.Keywords: Semantic web, Knowledge representation, Ontology, Comparative genomics, OrthologyBackgroundOwing to rapid progress in sequencing technologies, thenumber of genome sequences determined has signifi-cantly increased; recently, the targets of genome projectsare not limited to the model organisms but include unin-vestigated organisms of particular interest. In this newgenomic era, the role of computational analysis is becom-ing increasingly important. There is an urgent need forconsolidating a comprehensive foundation of comparativeanalysis toward effective knowledge discovery. In par-ticular, the orthology information is a key resource; itguides establishing evolutionary histories among genes ofmultiple organisms and provides a basis for functionalinference of gene products.*Correspondence: jfernand@um.esEqual contributors1Departamento de Informática y Sistemas, Universidad de Murcia,IMIB-Arrixaca, 30071 Murcia, SpainFull list of author information is available at the end of the articleThe concepts of orthology and paralogy are definedas specific types of homology [1]; homologs are genesdiverged from an ancestral gene, and specifically,orthologs are those diverged by a speciation event,whereas paralogs diverged by a duplication event. Figure 1shows a schematic representation of evolutionary rela-tions among genes of multiple organisms, which exem-plifies orthology/paralogy. Orthologs are usually moreconserved in biological functions than paralogs; thus, theorthology relation is particularly useful in transferring thebiological knowledge of model organisms to organismswith newly sequenced genomes. Whereas the homologyrelations are basically calculated in a pairwise perspec-tive, they are often represented as a cluster of homologs.Likewise, an ortholog cluster stands for a group of genesderived from a speciation event, and a paralog clusterfor a group of genes derived from a duplication event.Ortholog/paralog clusters can be structured in a form ofnested hierarchies, reflecting their evolutionary histories.© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 2 of 11Fig. 1 A schematic representation of the evolutionary relations among genes of multiple organisms. The leaf nodes of the tree represent the genesand the internal nodes correspond to evolutionary events. X1 has two ancestral nodes associated with speciation events; one is the last commonancestor with Y1, and the other is common with Z1. Thus, Y1 and Z1 are orthologs of X1. On the other hand, X2 and Y2 are paralogs to X1, sincetheir last common ancestor has a duplication event associated. Likewise, all the pairwise orthology/paralogy relations can be defined according tothe strucutre of the given treeA simple example of hierarchical clusters can be seen inFig. 1.The Quest for Orthologs (QfO) Consortium has iden-tified more than forty resources about orthology (http://questfororthologs.org/orthology_databases), which re-flect different scopes of information management in theorthology field. Many of these databases store informationabout prediction of gene evolutionary relations and thereis a diversity of objectives for these databases. There is het-erogeneity in how data are stored and provided by thesedatabases. For example, InParanoid [2] stores orthologyrelations between two species, whereas OrthoMCL [3]and MBGD [4] stores ortholog groups among multiplegenomes. OMA [5] provides various types of orthologyrelations including pairwise orthologs and hierarchicalorthologous groups. Traditionally, each resource has usedits own representation format based on tabular files,but this community has developed in the last years theOrthoXML format [6] to standardize the representationof orthology data. OrthoXML permits the comparisonand integration of orthology data from different resourceswithin the orthology community. However, only a limitednumber of databases have provided their content usingOrthoXML so far.In recent years, the Semantic Web formats have beenused for representing orthology data. OGO [7] wascreated with the purpose of providing an integratedresource of information about genetic human diseasesand orthologous genes. OGO integrated information fromorthology databases such as InParanoid or OrthoMCL,plus OMIM [8]. This resource developed an OWLontology for representing the domain knowledge. Morerecently, RDF has been used to share the content of theMicrobial Genome Database for Comparative Analysis(MBGD) [9]. This resource also developed an OWL ontol-ogy for representing the domain knowledge, calledOrthO,which had similar concepts to the OGO ones, despitebeing developed independently.The report of the 2013 QfO meeting [10] identifieda series of aspects about semantics that have been thekey drivers of our activities: (1) the orthology commu-nity should use shared ontologies to facilitate data sharing;(2) exploiting automated reasoning should be beneficialfor the QfO consortium. In this paper, we describe theconstruction of the Orthology Ontology by reusing theexisting related ontologies and we explain how we inte-grated the existing orthology datasets using the SemanticWeb technologies. This work provides a step forwardtowards the standardization in the orthology community.Construction and contentConstruction of the Orthology OntologyAs the first principle, we followed the best practices inontology engineering: reusing the existing ontologies toFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 3 of 11facilitate interoperability across biomedical domains; anddesigning the ontology with amodular perspective, so thatdifferent modules of the ontology are created in differ-ent sub-taxonomies of the ontology, and the classes fromdifferent modules are connected through object proper-ties. The second principle is to define local URIs for basicterms of the domain. In case that equivalent classes arefound in the reused ontologies, such equivalency is statedby means of axioms.Two application-oriented domain ontologies were thestarting point for this work, namely, OGO [7] and OrthO[9]. These ontologies provided a basis for discussion andidentification of the relevant classes and properties for thisdomain. Those ontologies already reused some ontologiessuch as the Relations Ontology (RO) or the NCBI Taxon-omy (NCBIT), so these were included in the initial set ofcandidate ontologies to reuse.The objective of the Orthology Ontology (ORTH) isto become the reference in the orthology domain andacross the biological domains, so it must be beyondthe application-oriented ontologies. In order to facilitateinteroperability, we decided to search for existing ontolo-gies which could play such interoperability enabler role.We searched repositories such as BioPortal [11], Onto-bee [12] and AberOWL [13], and identified ontologiescontaining classes and properties for the entities identi-fied in our analysis. This list of ontologies is describednext: Comparative Data Analysis Ontology (CDAO)1 [14]:Classes and properties relevant for evolutionarystudies. Relations Ontology (RO) 2 [15]: Collection ofbiomedical properties to support standardizationacross biomedical ontologies. Homology Ontology (HOM)3 [16]: Classes related tohomology. Sequence Ontology (SO)4 [17]: A set of classes andproperties to define sequence features used inbiological sequence annotation. Ontology of Genes and Genomes (OGG)5 [18]:Classes and properties to represent relations amonggenes, genomes and organisms. Protein Ontology (PR)6 [19]: Protein-related entities,including evolutionary relations between proteins. Semanticscience Integrated Ontology (SIO)7 [20]:Classes and properties for rich description ofbiomedical objects and processes. NCBI Taxonomy (NCBIT)8 [21]: Curatedclassification and nomenclature for all the organisms. Clusters of Orthologous Groups Analysis Ontology(CAO)9 [22]: Classes to support the Clusters ofOrthologous Groups enrichment method usingFishers exact test.The HOM and the CAO ontologies were discarded fordifferent reasons. On the one hand, we found that the ROproperties were more appropriate than the HOM classesfor describing relations between biological sequences. Onthe other hand, the CAO was found too specific andpresented overlaps with other ontologies that we con-sider more relevant for our goal. OGG and PR were notused because their classes of interest are covered by otherontologies. Next, we enumerate the ontologies selected forreuse: The RO is the main reference for the propertiesincluded in the ORTH. The CDAO provides classes for representingevolutionary events such as speciation andduplication, which are fundamental for the orthologydomain. Besides, it defines classes and properties forrepresenting the tree, which is a hierarchicalstructure widely used to represent evolutionaryrelations. This ontology is reused specially forevolution-oriented entities. The SIO provides classes and properties that describebiomedical objects and processes, therefore it is amore general ontology than the CDAO. This is whywe have used it as a reference for the generalbiomedical entities. The SO provides classes related to biologicalsequences, some of which are of interest for theorthology domain: biological region, gene and protein. The NCBIT provides the classes for the speciesassociated with the biological sequences.Besides, it must be taken into account that some SIOproperties are equivalent to RO ones. For those cases,we have selected the SIO one. In summary, we selectedto reuse SIO and RO for a more general content, CDAOfor the evolution-oriented content, SO for the biologicalsequence types, and NCBIT for the organisms. The abovedescribed ontologies provide the biological backgroundknowledge for the orthology domain. Besides, the ORTHreuses other vocabularies: dcterms10: It includes the metadata terms maintainedby the Dublin Core Metadata Initiative. We reusedproperties such as identifier. VoID11: RDF Schema vocabulary for expressingmetadata about RDF datasets. ORTH needs torepresent orthology databases, so the properties andclasses representing datasets and membership tothem are reused.The content of the Orthology OntologyThe Orthology Ontology is available at http://purl.org/net/orth in OWL format. In our model, evolutionaryinformation among sequences are primarily representedFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 4 of 11as membership of the sequences to clusters ofhomologs, orthologs or paralogs. Note that the pairwiseorthologs/paralogs can be obtained by traversing the treestructure of the clusters. When we see the example shownin Fig. 1, each gene represented by the leaf node belongsto ancestral nodes corresponding to clusters of orthologsor paralogs, from which pairwise orthology/paralogy canbe extracted.Figure 2 shows the core classes and properties includedin the ORTH, where three areas can be distinguished asfollows. The left side of the figure contains the CDAOmodule that defines cladogenetic changes, that is, thetypes of evolutionary events relevant for the orthologydomain, such as cdao:speciation or cdao:geneDuplication.The central part of the figure describes the mainorthology-specific classes from our modeling perspec-tive, that is, the clusters of homologs, orthologs andparalogs, which are represented by means of the classesHomologsCluster, OrthologsCluster and ParalogsClusterrespectively. Given that these clusters are usually orga-nized as trees, we have also defined the class GeneTreeN-ode, which is a subclass of cdao:Node. Provided that thetypes of cluster are related to a specific type of cladoge-netic change, the property cdao:has links the types of clus-ters with the corresponding cladogenetic changes. Again,we are reusing CDAO content to provide interoperableevolutionary content. Besides, the membership to a givencluster is expressed through the property hasHomologous,which is a subproperty of sio:has_part, whose inverseproperty is equivalent to ro:part _of. We use this prop-erty instead of two hasOrthologous and hasParalogous,because the pairwise relations are obtained by analyzingthe tree.The right side of the figure focuses on the defini-tion of the biological sequences relevant for the orthol-ogy domain: genes, subgenes and proteins. These classesare subclasses of SequenceUnit, which is a subclassof cdao:TU, which represents taxonomic units. Theclass Subgene has been created because of the increas-ing interest in creating evolutionary analyses of genesubsequences. Hence, its relation with gene has beenmade explicit through the property sio:is_part_of. Theseclasses of the ontology are connected with the cen-tral module through the rdfs:subClassOf relationshipbetween SequenceUnit and GeneTreeNode. Genes andproteins, which are defined equivalent to classes in theSO, are linked to ncbit:organisms through the prop-erty ro:in_taxon and to biological databases throughro:contained_in.Although the original terms orthology and paralogy arebinary relationships between genes, the ORTH does notinclude these terms. Instead, the ORTH defines theseFig. 2 The core classes and properties of the Orthology Ontology. The classes are represented as boxes and the properties as arrows. The prefixescdao, sio, ro, ncbit and void represent entities reused from the corresponding ontologies. The entities without prefix are defined in the ORTH. On thewhole, this figure includes three kinds of classes, each shown in the left/center/right parts, respectively: (left) classes for evolutionary changes; (center)classes for groups of biological sequences holding particular evolutionary relations; and (right) classes for biological sequences of interestFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 5 of 11concepts through the classes OrthologsCluster and Par-alogsCluster. This is because the relationships orthologyand paralogy are not transitive [23], and the tree (orhierarchical clustering) representation as shown in Fig. 1is a better representation for these relationships amongmultiple genes. In fact, any pairwise orthology/paralogyrelation can be extracted from this representation using aquery as shown in the next sections. The content has beenmodeled with the aim of providing an appropriate degreeof axiomatization. For instance, we have mentioned thatclasses such as OrthologsCluster and ParalogsCluster areassociated with the corresponding evolutionary eventthrough an object property. For example, the semanticallyequivalent definition for anOrthologsCluster according toour ontology and thanks to the axiomatization would be acluster of homologs whose event associated is speciation.This corresponds to the OWL axiomorth:OrthologsCluster rdfs:subClassOf(orth:HomologsCluster andcdao:has only cdao:CDAO_0000121)and in SPARQL as follows:?cluster rdf:type/rdfs:subClassOf*orth:HomologsCluster.?cluster cdao:has cdao:CDAO_0000121.Ontology metricsThe current version of the ORTH has a core that consistsof 21 classes, 14 object properties, 5 datatype propertiesand 142 axioms, whereas the whole knowledge frame-work, that is, with the imported ontologies, consists of4613 classes, 806 object properties, 15 datatype propertiesand 43140 axioms.We have applied the OQuaRE framework [24] toevaluate the quality of the ontology produced. Withthis framework a series of metrics can be calculated,providing scores in the range 1 (lowest) to 5 (high-est) for the OQuaRE quality characteristics. Table 1shows the OQuaRE scores for the ontology with theimported, reused ontologies (complete) and the ontol-ogy without the imports (no imports). The ontology alsopassed successfully the test of the OOPS! Ontology PitfallScanner [25].Applying the ORTH to orthology datasetsIn this section we illustrate through an example howthe availability of ORTH can benefit the exploitation oforthology data. Experiences have been gained with OMAand InParanoid [26], which have been recently extendedto TreeFam [27] in the context of the BioHackathon 2015.Let us suppose that we are doing some research on serumamyloid A1 (SAA1) protein which is known as an inflam-matory marker, and that we are interested in finding outif this human gene has orthologs in mouse, because thiscould permit to carry out some related research withmice.In this example we assume the existence of three orthol-ogy resources: OMA, InParanoid and TreeFam. The useof the original resources to answer this question wouldrequire to perform three queries, one per resource andto process and interpret the set of results knowing howorthology relations are represented in each resource. TheORTH ensures that each data represented has a pre-cise meaning, so the user can focus on interpreting theresults. Besides, the use of the ORTH for representingthe datasets enables to obtain the results with one, nonresource-dependent query. The joint exploitation of theorthology datasets requires (1) generating RDF versionsof the datasets; and (2) defining and executing the corre-sponding queries in SPARQL. Both tasks are described inthe next subsections.Generation of the RDF datasetsWe describe next an example of how the source dataare transformed into RDF. Let us consider the infor-mation available in InParanoid 8 about Homo sapiens -Mus musculus orthologs12. Table 2 shows fragments ofthe corresponding OrthoXML file. We use OrthoXML asdata schema because it is considered a standard in theorthology community. The species tags are used to spec-ify the name of the species, the database from whichthe genes/proteins are retrieved, and the genes used inthis file. For each <gene> three attributes are shown: (1)id, whose scope is the OrthoXML file; it is the ID usedfor associating a gene with the corresponding clusters;(2) protId, which is the identifier of the protein in thedatabase; and (3) geneId, which represents a gene sym-bol in this example. For example, the gene with id 33162is the human protein whose UniProt accession number(AC) is P0DJI8 and whose gene symbol is SAA1. In thisfragment we can see that it contains genes from humansand mice. After the declaration of species and genes, theOrthoXML file includes the cluster with id 16021, whichcontains the human genes SAA1 and SAA2 and themousegenes Saa1 and Saa2. This implies: (1) a many-to-manyTable 1 Scores of the OQuaRE quality characteristics for the ORTHORTH Structural Funct. adequacy Compatibility Maintainability Operability Reliability TransferabilityComplete 4.5 4.56 3.0 3.97 4.33 3.12 4.0No imports 4.0 4.03 4.25 4.09 3.66 3.0 4.0The first row shows the scores for the OWL file including the imported ontologies, whereas the second row shows the ones for the ontology without the imported onesFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 6 of 11Table 2 Fragments of the InParanoid OrthoXML file that storesorthology relations between human and mouse<species name="Homo sapiens " NCBITaxId="9606" ><database name="UniProt"version="UniProt_Complete_Proteomes_2013_06"protLink="http://www.uniprot.org/uniprot/"><genes><gene id="33162" protId="P0DJI8" geneId="SAA1"/><gene id="33163" protId="P0DJI9" geneId="SAA2"/></genes></species><species name="Mus musculus " NCBITaxId="10090"><database name="UniProt"version="UniProt_Complete_Proteomes_2013_06"protLink="http://www.uniprot.org/uniprot/"><genes><gene id="33164" protId="P05366" geneId="Saa1"/><gene id="33165" protId="P05367" geneId="Saa2"/></genes></species><orthologGroup id="16021"><geneRef id="33162"/><geneRef id="33163"/><geneRef id="33164"/><geneRef id="33165"/></orthologGroup>orthology relation between the human and mouse genes,that is, <(SAA1, SAA2), (Saa1, Saa2)>; and (2) the paral-ogy relation between the genes of the same species, thatis, (SAA1, SAA2) and (Saa1, Saa2).The RDF representation of the XML content using theORTH is obtained by (1) mapping the OrthoXML for-mat to the ORTH; and (2) applying the mappings to thedata. Briefly speaking, the mappings associate entitiesand attributes of the OrthoXML schema with owl:Class,owl:DatatypeProperty and owl:ObjectProperty defined inthe ORTH. The mapping file can be found at https://github.com/qfo/OrthologyOntology. An example of themapping for the clusters of orthologs is shown in Fig. 3,where the left part shows the OrthoXML schema andthe right side shows the ORTH schema. There, we cansee that the entity orthologGroup is mapped to the classOrthologsCluster and that the membership of a gene to anorthologGroup, which is represented by the link betweenorthologGroup and geneRef, is mapped to the objectproperty hasHomologous. Consequently, for each geneRefincluded in an orthologGroup, the corresponding triple isobtained in the form of OrthologsCluster hasHomologousGene.Technically speaking, the generation of the RDFdatasets is supported by the SWIT tool 13. SWIT is ableto generate RDF and OWL content by applying the map-ping rules to the OrthoXML versions of OMA, InParanoidand TreeFam. Besides, SWIT uses automated reasoningto ensure that only logically consistent content is trans-formed. This means that the data instances inconsistentwith the axioms of the ORTH are not transformed intoRDF or OWL. Table 3 shows the RDF triples generated fordescribing the orthologous group 16021.The RDF datasets generated from InParanoid 8, OMAhierarchical orthologous groups (Sep 2014) and TreeFam9 are available on our website, and contain 8798758 genesfrom OMA, 1713180 genes for Homo sapiens orthologsand 1367940 genes for Mus musculus orthologs fromInParanoid, and 1376021 genes from TreeFam. Overall,the complete dataset has over 2 billion triples.Exploitation of the RDF datasetsHere we assume the existence of an RDF repositorywith the data from the three resources, which has beengenerated as described in the previous section. In caseof using three distinct RDF repositories, the SPARQLqueries should be adapted by including the correspondingSERVICE clauses. Given that we are interested in retriev-ing the orthologs of the human gene SAA1 in mouse,answering this query requires to extract pairwise orthol-ogy relations betwen the human gene and mouse genesfrom the repository. The ORTH associates each gene with(hierarchical) clusters of homologs to which it belongs andeach cluster with an evolutionary event. Thus, extractingthe pairwise orthologs means searching for such genesthat are members of the same cluster as the human geneSAA1 belongs to and the last ancestor cluster is a clusterof orthologs (i.e., it has a speciation event associated).Such description can be expressed as the SPARQLquery shown in Table 4. This query can extract orthol-ogous pairs by identifying their last common ancestors;it extracts pairs of genes (?gene1 and ?gene2), that arethe descendants of respective two distinct nodes of thetree (?tree_node1 and ?tree_node2) whose common parent(?common_ancestor) is a cluster of orthologs. The tree-based cluster analysis is facilitated by the use of the singleproperty hasHomologous, which is transitively used whenthe symbol * is attached to it. The FILTER and VALUESclauses serve to define the gene and organisms of interestfor the query, meaning that it could be used as a templatefor finding pairwise orthologs for any given gene; only theVALUES clause would have to be modified.The results of the query are shown in Table 5. Wecan see that there is a one-to-many relation between thehuman gene SAA1 and its orthologs in mouse (SAA1,Fernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 7 of 11Fig. 3 Excerpt of the mapping from the OrthoXML schema (left) to the ORTH (right). The dashed lines represent the concrete mappings fromOrthoXML entities to ORTH classes or propertiesSAA2, SAA3) and that two resources contain the results.The relationship with SAA1 and SAA2 is supported by thetwo resources, and only OMA proposes the one for SAA3.It should be noted that the predicted orthology relation isgenerally reliable in the case of one-to-one, but conflictsbetween methods often happen when the relationship isnot one-to-one. More details, including sample queriesthat exploit the three resources, can be found at https://github.com/qfo/OrthologyOntology.Table 3 RDF triples for the cluster of orthologs 16021orth_data:orthologsCluster_16021 rdf:type orth:OrthologsCluster.orth_data:orthologsCluster_16021 void:inDataset orth_data:orthologyDataset_InParanoid.orth_data:orthologsCluster_16021 dcterms:identifier "16021".orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:gene_33162.orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:gene_33163.orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:gene_33164.orth_data:orthologsCluster_16021 orth:hasHomologous orth_data:gene_33165.Utility and discussionPotential applicationsThe development of the Orthology Ontology enables aseries of activities that will show progress in how orthol-ogy data are represented and exploited.1) Linked Orthology Data to promote interoperability.Many biological databases include information aboutorthology relations, which derive from different orthol-ogy resources created by different methods. The ORTHvocabulary can be used to generate shareable RDFdatasets that could be queried by biomedical informat-ics tools. An initial research on how the ORTH can drivethe transformation of orthology databases in OrthoXMLis reported in this work. The development of a LinkedData API for ORTH datasets would permit to standard-ize a series of methods that would return data fromdifferent resources preserving the meaning of the enti-ties, so promoting the standardization of the orthologydata obtained from different resources such as UniProt orEnsembl.2) Meta-predictor of orthology for better prediction ofbiological functions. Predicting biological functions islikely to be the most widespread application of orthologyresources. The availability of the ORTH and the exis-tence of RDF orthology datasets based on the ORTHwill facilitate the development of methods for improv-ing orthology prediction by exploiting the predictionsFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 8 of 11Table 4 A sample query for getting the orthologs of a given geneSELECT ?gene ?species ?database WHERE {?common_ancestor a orth:OrthologsCluster .?common_ancestor ort:hasHomologous ?tree_node1 .?common_ancestor orth:hasHomologous ?tree_node2 .?common_ancestor void:inDataset ?dataset .?dataset orth:hasSource ?database .?tree_node1 orth:hasHomologous* ?gene1 .?tree_node2 orth:hasHomologous* ?gene2 .?gene1 a orth:Gene .?gene2 a orth:Gene .?gene1 obo:RO_0002162 ?species1 .?gene2 obo:RO_0002162 ?species2 .?gene1 dcterms:identifier ?id .?gene2 dcterms:identifier ?gene .?species2 rdfs:label ?species .bind( str(?id) as ?str_id )FILTER (?tree_node1 != ?tree_node2 && ?species1 != ?species2)VALUES (?str_id ?species1 ?species2) {(SAA1 ncbit:9606 ncbit:10090)}}In this example, the mouse (ncbit:10090) orthologs of the human (ncbit:9606) geneSAA1 are retrieved. obo:RO_0002162 stands for the property in_taxonof many of the existing orthology resources, which canimprove the function prediction by orthology relations.The potential of this meta-approach will be reinforced bythe standardization effort of the orthology content.3)Migration of existing resources. Datamigration of exist-ing orthology resources described by previous ontologiessuch as OrthO is also necessary and can be done with thesupport of the results of the present work. We provideinformation that helps the ontology users catch up theevolution of the ontologies and work on the data migra-tion. As an example, we have summarized the term-by-term correspondence between previous ontologies (OGO,OrthO) and the current ORTH (see https://github.com/qfo/OrthologyOntology). This table will help replace theprevious ontologies with the current ontology. In fact,we have already replaced the OrthO ontology of MBGDdatabase (http://mbgd.genome.ad.jp/sparql) according toTable 5 Results of the query shown in Table 4 for a repositorythat integrates InParanoid and OMAGene Species DatabaseSAA1 Musmusculus OMASAA3 Musmusculus OMASAA2 Musmusculus OMASaa1 Musmusculus InParanoidSaa2 Musmusculus InParanoidthis table. The replacement was straightforward as thecurrent ontology covers the previously used concepts.Toward the ontology standardization, not the distinctresearchers but a community-oriented approach is cru-cial. The current ontology with enhanced consensusand semantics will be more suitable than previous onesfor standardization and further application of orthologyresources.Data integration issuesORTH-based data integration can be carried out followingtwo main approaches: links or warehouse. In the link-based approach, there would be one RDF dataset usingthe ORTH vocabulary per orthology resource, and it is thedata integration strategy used in projects such as Bio2RDF[28]. The application of the link-based strategy to OMAand InParanoid for the SAA1 example would produceone instance of SAA1 in each repository. Both instancescould have the same URI (e.g., http://identifiers.org/hgnc.symbol/SAA1) or different ones, which would dependon the decision made by the data providers, since genenomenclature is well maintained only for limited species.In this latter case, owl:sameAs links should be defined toidentify that they refer to the same gene.The warehouse approach stores the whole dataset inthe same repository, and it is the approach followed inprojects such as OGOLOD [29]. This approach requiresto be able to identify which instances from the differentdatasets refer to the same gene or protein, which is easy tofind in case shared identifiers are used, but difficult other-wise. The application of the warehouse strategy to OMAand InParanoid for the SAA1 example would produce oneinstance of SAA1, which would integrate the content fromOMA and InParanoid.In the current work, we have followed a warehouseapproach using resource-oriented URIs, with the objec-tive of studying and make visible the data integrationissues that would impede the orthology community tohave semantically interoperable datasets even with theavailability of the ORTH.We wanted to test to what extentthe availability of the ORTH and the definition of a com-mon transformation process could help, and what addi-tional work should be done. The results obtained are threedatasets that use the same knowledge framework, whichcan be jointly queried, which is one of the contributions ofthe present work, since those datasets could not be jointlyqueried to date. This means that there is one instance ofSAA1 for the gene from each resource. Besides, the properintegration of data has not been carried out through links.The heterogeneity at the identifier level is an impor-tant issue in the application of the ORTH. Table 6 showsthat different identifiers are used by the three resources:InParanoid, OMA and TreeFam. OMA uses local identi-fiers for the proteins; InParanoid uses the UniProt AC forFernández-Breis et al. Journal of Biomedical Semantics  (2016) 7:34 Page 9 of 11Table 6 Summary of sequence sources and identifiers used inthree different orthology resourcesOrthology resource Database source Protein ID Gene IDOMA Multiple sources OMA ID Gene symbolaInParanoid UniProt UniProt AC Gene symbolaTreeFam Multiple sources Multiple sources Multiple sourcesaThe gene symbol is used for model organisms, including human and mouse, butthis is not the case for other organisms in generalproteins and the gene symbol for genes; and the identi-fiers used in TreeFam depend on the database used forthe corresponding species (e.g., Ensembl, FlyBase, Worm-Base). We think that the generation of highly interop-erable RDF orthology datasets following the link-basedapproach would require the data providers to supportwith a mapping service for generating the correspondinglinks or to use common reference identifiers. One practi-cal approach is that each database provider should provideRESEARCH Open AccessTrivalent influenza vaccine adversesymptoms analysis based on MedDRAterminology using VAERS data in 2011Jingcheng Du1, Yi Cai2, Yong Chen3 and Cui Tao1*AbstractBackground: Trivalent Influenza Virus Vaccine (FLU3) is a traditional flu vaccine to protect people against threedifferent flu viruses, including influenza A H1N1 virus, an influenza A H3N2 virus and one B virus.Methods: We searched Vaccine Adverse Event Reporting System (VAERS) for US reports after FLU3 vaccination inthe year of 2011. We conducted descriptive analyses on symptoms from serious reports (i.e., death, life-threateningillness, hospitalization, prolonged hospitalization, or permanent disability). We then further grouped these symptoms tothe System Organ Classes (SOC) based on the MedDRA Terminology using NCBO Web Services. We fitted zero-truncatedPoisson regression models to estimate the average number of symptoms per subject and compared it across differentage groups and between genders. In addition, we compared the risk of occurrence for an SOC across different agegroups and between genders by using logistic regression models. Finally, we constructed the pairwise correlation matrixof the SOCs by calculating Spearmans rank correlation coefficients.Results: We identified 638 unique serious FLU3 reports from year 2011. There are 1410 unique symptoms from thesereports. Descriptive statistics shows that the most common symptom and symptom pair are Pyrexia and Guillain-Barresyndrome  Hypoesthesia respectively. The estimated average number of symptoms per subject in the study cohort is8.74 (95 % CI 6.76, 10.73). There are statistically significant differences in number of symptoms among four age groupsand between genders. Age category and gender are significantly associated with several individual SOCs. Pairwisecorrelation matrix shows that Endocrine disorders and Neoplasms benign, malignant and unspecified (incl cystsand polyps) are strongly correlated.Conclusions: This paper reports a novel method that combining statistical analyses with terminology groupingusing VAERS data. The analyses revealed differences of reactions among different age groups and between gendersand correlation on both symptoms and System Organ Class level independently. The results may lead to additionalstudies to uncover factors contributing to the individual differences in susceptibility to influenza infection. This methodcan also be applied to other vaccine types and conduct similar analysis.Keywords: Trivalent influenza virus vaccine, VAERS, MedDRA* Correspondence: cui.tao@uth.tmc.eduEqual contributors1The University of Texas School of Biomedical Informatics, 7000 Fannin StSuite 600, Houston, TX 77030, USAFull list of author information is available at the end of the article© 2016 Du et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Du et al. Journal of Biomedical Semantics  (2016) 7:13 DOI 10.1186/s13326-016-0056-2BackgroundInfluenza (flu) is a contagious respiratory illness causedby influenza viruses, which may cause mild to severe ill-ness including hospitalization or even death. Certaingroups of people, such as the elders, young children, andthose with certain health conditions, are at high risk forserious flu complications [1]. The primary and mostused method for the control of influenza and its compli-cations are influenza vaccines. Over the years, hundredsof millions of Americans have received seasonal influenzavaccines to protect themselves against the flu viruses.Commonly, the side effects following flu vaccinations aremild, including symptoms such as soreness, redness orswelling at the injection sites, headache, muscle aches andnausea after the shot. Serious adverse reactions, however,could happen, which may cause some life-threating illnesseven death.Clinical trials are generally not large enough to detectrare influenza vaccine adverse events. In 1990, VaccineAdverse Event Reporting System (VAERS) was createdas a passive surveillance system to accept reports ofadverse events following any US licensed vaccines formproviders, health care workers, and the public [2]. VAERSis co-administered by Center for Disease Control andPrevention (CDC) and the Food and Drug Administration(FDA). It is one of the largest databases containing adverseevents reported in temporal association with vaccination.Since 1990, VAERS has received more than 400,000 vac-cine adverse event reports, which makes it one of the mostimportant sources to detect rare vaccine adverse events.Although VAERS cannot usually prove the causal relation-ships between vaccines and adverse events, it could beused to detect signals to be tested with more rigorousmethods [3].For years, trivalent Influenza Virus Vaccine (FLU3) isthe traditional flu vaccine to protect people against threedifferent flu viruses, including an influenza A H1N1virus, an influenza A H3N2 virus and one B virus [4].Among all the VAERS reports, FLU3 is the most com-mon vaccine type reported. Thus VAERS is a very im-portant data source for studying FLU3 adverse events.Much work has been done on the study of influenzaadverse events using VAERS data [57]. Most of themonly deal with some specific symptoms on the FLU3vaccine adverse reports. A single vaccine and AE associ-ation, however, should not be considered as an isolatedevent. The associations of other vaccines with the sameAE and other AEs with the same vaccine should also betaken into consideration [8].Our research takes advantage of the Medical Dictionaryfor Regulatory Activities (MedDRA) terminology systemfor semantically grouping the VAERS adverse symp-toms, which are already coded using MedDRA terms[2]. The MedDRA Terminology is the internationalmedical terminology developed under the auspices ofthe International Conference on Harmonization (ICH)of Technical Requirements for Registration of Pharma-ceuticals for Human Use. MedDRA Terminology has afive-level structural hierarchy. They are lowest LevelTerm (LLT), Preferred Term (PT), High Level Term(HLT), High Level Group Term (HLGT), and SystemOrgan Class (SOC). VAERSSYMPTOMS contains symp-toms terms that are in the level of LLT [9]. VAERS usedsymptom terms from PT, which always have its own iden-tical term as LLT. The full MedDRA has 72,637 LLTsymptoms and VAERS uses 9593 of them (13 %) [10].System organ class (SOC) is the highest level of the hier-archy that provides the broadest concept for data retrieval,which comprises grouping by etiology, manifestation siteand purpose. MedDRA has 26 different types of SOCs andeach LLT is at least linked to one SOC [9].As the number of unique symptoms is relatively largein VAERS, we consider them in the SOCs to facilitatefurther statistical analyses. We leveraged The NationalCenter for Biomedical Ontology (NCBO) web service toautomatically map LLTs to their corresponding SOCs.The NCBO offers a range of Web services that wouldallow users to access various biomedical terminologiesand ontologies and to identify terms from controlled ter-minologies and ontologies that can describe and indexthe contents of online data sets (data annotation) [11].All the MedDRA terminology system is stored in JSONformat on the NCBO web services and each term is aJSON node. We used NCBO web services to search thehierarchical information of the symptom terms in theVAERS reports and assigned each symptom term one pri-mary SOC. After grouping the symptoms to SOCs, weconducted multiple statistical analyses on the SOC level.Materials and methodsData sourceWe searched the VAERS for US reports after FLU3vaccination in year 2011 and extracted serious reports(i.e., death, life-threatening illness, hospitalization, pro-longed hospitalization, or permanent disability). VAERSraw data of each year contains three Comma-separated-value (CSV) files: VAERSDATA.CSV, VAERSVAX.CSVand VAERSSYMPTOMS.CSV. VAERSDATA is about thepatients demographic information, lab test, symptomtext and outcomes. VAERSVAX is about vaccine types.VAERSSYMPTOMS is about symptoms that are equi-valent to the PT TERM from the MedDRA codebook.These three tables are linked by using VAERS_ID. Foreach report, the VAERS also provides annotations forpost-vaccination symptoms in MedDRA terms. To fa-cilitate further statistical analyses, we further groupedthese symptoms based on the MedDRA SOC using theNCBO Web Services [11].Du et al. Journal of Biomedical Semantics  (2016) 7:13 Page 2 of 7Descriptive analysisWe calculated descriptive statistics including the num-ber of reports, symptoms, and unique symptoms in theselected reports. We also calculated the frequency ofeach symptom and co-occurrence of symptom pairs.We grouped the reports in five age groups based oncut points (0.5, 17, 49, 64) suggested by CDC [12]. Thefrequency of observations in age category (0 to 0.5) isrelatively small (n = 14) compared to other age categories.In order to be consistent with our previous analysis, weexcluded those subjects in our data analysis, which lead to1663 subjects [13].Grouping symptoms using MedDRA terminologySOC is the highest level of the MedDRA terms, whichcomprises grouping by etiology, manifestation site orpurpose. Each LLT is linked to only one PT and each PTis linked to at least one SOC. This indicates that eachLLT could be grouped to more than one SOC. To avoiddouble counting, we will need to identify the primarySOC for each term [9]. The rules to assign SOC to thesymptoms, however, are complicated, which needs expertreviews that could be time consuming and expensive.Our study proposed a simple way to group them intoSOCs by using the international agreed order of theSOC list (see Table 1). The order of the SOCs was basedupon the relative importance of each SOC, which isdetermined by the Expert Working Group [9]. First, weretrieved all possible SOCs a VAERS symptom term be-longs to. We applied Depth-first Search (DFS) algorithmby using recursive tree-traversing method to find allthe SOCs that is linked by a symptom term. If onesymptom term belongs to several SOCs, we choose theSOC that ranked highest as its primary SOC. If a re-port has N symptoms that belong to the same specificSOC, we count N times of that specific SOC. Table 2shows the sample data set (partial) we prepared forfurther analysis.Statistical methodsAfter grouping the symptoms into 26 SOCs, we exploredthe grouped data with regression models and correlationanalysis. Specifically, we estimated the average numberof symptoms per subject given stratified age groups andgender. For individual SOC, we fitted logistic regressionto evaluate the association of occurrence of an SOC withage and gender. A rank-based correlation matrix is alsoestimated to investigate the correlation among SOCs.Zero-truncated Poisson regressionAs each subject has at least one SOC to be reported(i.e. the number of SOCs > 0), we fitted a zero-truncatedPoisson regression model (a modified model of Poisson re-gression by excluding the probability mass at 0) to conductTable 1 International Agreed Orders of SOCsSOC OrderInfections and infestations 1Neoplasms benign, malignant and unspecified(incl cysts and polyps)2Blood and lymphatic system disorders 3Immune system disorders 4Endocrine disorders 5Metabolism and nutrition disorders 6Psychiatric disorders 7Nervous system disorders 8Eye disorders 9Ear and labyrinth disorders 10Cardiac disorders 11Vascular disorders 12Respiratory, thoracic and mediastinal disorders 13Gastrointestinal disorders 14Hepatobiliary disorders 15Skin and subcutaneous tissue disorders 16Musculoskeletal and connective tissue disorders 17Renal and urinary disorders 18Pregnancy, puerperium and perinatal conditions 19Reproductive system and breast disorders 20Congenital, familial and genetic disorders 21General disorders and administration site conditions 22Investigations 23Injury, poisoning and procedural complications 24Surgical and medical procedures 25Social circumstances 26Table 2 Sample data set we prepared for further analysis(partial, data of year 2011)ID  SOC7 SOC8 SOC9 SOC10 413830 0 1 0 0413836 0 0 0 0413913 0 3 0 0413937 0 11 0 2413946 0 5 1 0413959 0 5 0 0413966 3 15 3 0413993 1 5 0 0413994 1 3 0 0 ID refers to the report ID number. The number in a cell refers to the numberof appearance in the corresponding SOC in that reportDu et al. Journal of Biomedical Semantics  (2016) 7:13 Page 3 of 7data analysis [14]. Firstly, we fitted a model with interceptonly to estimate the average number of symptoms for allsubjects. After that, we included covariates of age categoryand gender in the model to estimate and compare thenumber of symptoms in different age and gender strata.Logistic regressionTo explore the association of the occurrence of individualSOC with age and gender, we conducted a logistic regres-sion with covariates of age group and gender [15]. Theoriginal count number of SOC is dichotomized into binaryoutcomes (1: SOC ? 1, 0: SOC = 0).Spearman rank correlation coefficientAs the number of SOCs is highly right skewed and non-normally distributed, we use a nonparametric correlationcoefficient to measure the correlation structure amongSOCs. This rank based correlation coefficient is known asSpearmans ? [16]. We constructed a correlation matrixof SOCs to present the pairwise correlation coefficientsbetween SOCs.ResultsDuring the study period (year 2011), VAERS received7986 FLU3 reports; 638 were serious. Out of the 638 re-ports, 324 were for female patients, 295 were for malepatients, and 19 were for unknown sex, see Fig. 1. Outof these reports, there were 5447 symptom terms (notunique) appeared in total, which were grouped into 26SOCs. The most frequent SOCs in the 638 reports arenervous system disorders, general disorders, and admin-istration site condition and investigations. Please notethat some symptom terms cannot be grouped to anyMedDRA SOCs. There were 48 symptom terms (21 ofthem are unique) cant be mapped into MedDRA SOCs,including Drug exposure during pregnancy, Herpeszoster multi-dermatomal and etc. (The complete listcan be seen in Additional file 1: Table S1A). This maydue to the MedDRA version differences between theNCBO Web Services and VAERS. As the number ofthese symptoms is relatively small (0.9 %), we did nottake these symptoms into consideration.We then calculated the frequency of each symptomsand symptom concurrences. The most frequent symp-toms happened after FLU3 vaccination in year 2011were Pyrexia (131 times), Hypoaesthesia (95 times), andGuillain-Barre syndrome (90 times), the visualizationresult can be seen in Fig. 2. (The complete results canbe seen in Additional file 1: Table S1B). The most fre-quent symptom co-concurrences were Guillain-Barresyndrome + Hypoaesthesia (24 times).Analysis using zero-truncated Poisson model withintercept only indicated that the estimated average num-ber of symptoms per subject in the study cohort is 8.74(95 % CI: 6.76, 10.73). The results from fitting zero-truncated Poisson regression with covariates age groupsand gender suggest that there are statistically significantdifferences in the numbers of symptoms among four ageFig. 1 Descriptive results of 2011 VAERS FLU3 data. a the proportion of serious reports of the total FLU3 reports. b the gender distribution ofthese serious reports. c the age distribution of these serious reportsDu et al. Journal of Biomedical Semantics  (2016) 7:13 Page 4 of 7groups and between different genders. The average num-ber of symptoms per year for a female patient with agebetween 0.5 years and 17 years is estimated as 7.76(95 % CI: 5.769.76). We use this group as the referenceage group. The youngest age group (0.517 years) hasthe smallest number of symptoms per year, followed byage group 4 (>64 years), age group 2 (1749) and finallyage group 3 (4964 years). The average number ofsymptoms for subjects of 1749 years old is 1.13 timesof the average number of symptoms for subjects of 0.517 years old with the same gender. This is consistentwith previous reports about different immune responsesafter vaccination for different age groups. For example,there is high-dose influenza vaccine available for elders(>65) because ageing decreases the body's ability of im-mune response after vaccination [17]. In addition, themales have 3.2 % lower number of symptoms. This isalso consistent with previous studies that female experi-ence more adverse reactions to influenza vaccine [18].We plotted the estimated residuals versus fitted valuesof zero-truncated Poisson regression mode. We foundthat the residuals are generally small and scatteredaround zero for most of the data points with only a fewextreme values. This suggests that the zero-truncatedPoisson regression model may fit the data well.Analysis on individual SOCs also revealed some in-teresting results. Overall, there are 15 SOCs that showsignificant association with the age groups or betweengenders. The males have fewer responses for most SOCsexcept SOC1 (infections and infestations, 48.1 % more).Males are 0.369, 0.552 and 0.535 times less likely to haveSOC 9 (Eye disorders), SOC 11 (Cardiac disorders) andSOC 12 (Vascular disorders) than females respectively.For SOC 14 (Gastroin-testinal disorders) and SOC 1(Infections and infestations), female has significanthigher possibilities than male. For SOC 1 (Infectionsand infestations), SOC 9 (Eye disorders), SOC 12 (Vasculardisorders), and SOC 17 (Musculoskeletal and connectivetissue disorders), people who are older than 17 show sig-nificant higher possibilities to get those adverse symptoms.Age group 3 (4964) shows significant higher chance ofexperiencing symptoms relevant to infections and infesta-tions, immune system disorders, eye disorders, vasculardisorders, musculoskeletal and connective tissue disorders,surgical and medical procedures, and social circumstancescompared to age group 1 (0.517) with the same gender.To evaluate the overall goodness of fit of the logisticregression model for each SOC, we conducted Hosmer-Lemeshow test [15] and calculated the corresponding p-values. Most of the p-values for all 26 tests are muchsmaller than 0.05, which suggests that the logistic regres-sion model fits the data well.We also calculated the pairwise correlation matrix ofSOCs determined by Spearmans method [16]. Fig. 3shows the correlation plot. The color and area of spotrepresents the strength of correlation between SOCs.The threshold to assert a correlation used by us iswhether the correlation coefficient is larger or equalthan 0.2 as suggested by Evans [19]. As illustrated inFig. 3, SOC 5 (Endocrine disorders) has the strongestcorrelation with SOC 2 (Neoplasms benign, malignantand unspecified (incl cysts and polyps)). Besides, wefind that SOC 23 (Investigations) has a correlation withSOC 8 (Nervous system disorders). We can also findthat SOC 22 (General disorders and administration siteconditions) has correlations with SOC6 (Metabolismand nutrition disorders), and SOC 17 (Musculoskeletaland connective tissue disorders). In addition, SOC 12Fig. 2 Symptoms frequency visualization for year 2011, the bigger the frequency the larger the font size of that symptomDu et al. Journal of Biomedical Semantics  (2016) 7:13 Page 5 of 7(Vascular disorders) shows correlations with SOC 3(Blood and lymphatic system disorders), and SOC 6(Metabolism and nutrition disorders). In addition, SOC18 (Renal and urinary disorders) has correlations withSOC 3 (Blood and lymphatic system disorders), SOC 6(Metabolism and nutrition disorders) and SOC 12(Vascular disorders).Conclusions and future workThis paper reports a novel method that combining stat-istical analyses with terminology grouping using VAERSdata to study Trivalent Influenza Vaccine. Our prelimin-ary statistical analyses reveal differences of reactionsamong different age groups and between genders. Toour best knowledge, there are very few studies about theadverse events analysis on MedDRA SOC level. Most ofour findings on the relationship between adverse eventswith individual difference have not been reported byother studies. The results may lead to additional studies touncover factors contributing to the individual differencesin adverse reactions to influenza vaccination.For the limitations of this paper, due to the multipleinheritance nature of MedDRA, many symptom termscan be mapped to more than one SOCs. The order listwe used to assign the primary SOC may be subjective.Ontology of Adverse Events (OAE) could provide a betterhierarchy than MedDRA. The current version of OAE,however, does not include all the symptoms in the VAERS(coded in MedDRA) yet. We will consider using OAE tomap the symptom terms as the OAE grows.There are a few future directions we would like topursue: (1) we will extend and apply the methodologyto more VAERS reports over different years; and (2)this method can also be applied to other vaccine typesand conduct similar analysis.Additional fileAdditional file 1: Table S1A. Symptom terms that cant be mappedinto SOC level in the year 2011. Table S1B. Symptoms and theirfrequency for year 2011. Top 50 frequently occurred symptoms in seriousFLU3 reports in the year of 2011. (DOCX 16 kb)Competing interestsThe authors declare that they have no competing interests.Authors contributionsJD collected that data, wrote the initial draft and revised subsequent draft.JD and YC developed the method, preformed the evaluation, and conductedanalysis of the results. YC guided the statistical analysis design and modeling.CT provided institutional support, contributed to research design, and guidedthe data analysis. All authors read and approved the final manuscript.AcknowledgmentsThis research is partially supported by the National Library Of Medicine ofthe National Institutes of Health under Award Number R01LM011829. Theauthors also gratefully acknowledge the support from the UTHealth Innovationfor Cancer Prevention Research Training Program Pre-doctoral Fellowship(Cancer Prevention and Research Institute of Texas grant # RP140103).Author details1The University of Texas School of Biomedical Informatics, 7000 Fannin StSuite 600, Houston, TX 77030, USA. 2The University of Texas School of PublicHealth, 1200 Pressler Street, Houston, TX 77030, USA. 3Department ofBiostatistics and Epidemiology, University of Pennsylvania, 423 GuardianDrive, Philadelphia, PA 19104, USA.Received: 10 November 2015 Accepted: 12 March 2016RESEARCH Open AccessAn accurate and precise representation ofdrug ingredientsJosh Hanna* , Jiang Bian and William R. HoganAbstractBackground: In previous work, we built the Drug Ontology (DrOn) to support comparative effectiveness researchuse cases. Here, we have updated our representation of ingredients to include both active ingredients (and theirstrengths) and excipients. Our update had three primary lines of work: 1) analysing and extracting excipients, 2)analysing and extracting strength information for active ingredients, and 3) representing the binding of activeingredients to cytochrome P450 isoenzymes as substrates and inhibitors of those enzymes.Methods: To properly differentiate between excipients and active ingredients, we conducted an ontologicalanalysis of the roles that various ingredients, including excipients, have in drug products. We used the valuespecification model of the Ontology for Biomedical Investigations to represent strengths of active ingredients andthen analyzed RxNorm to extract excipient and strength information and modeled them according to the results ofour analysis. We also analyzed and defined dispositions of molecules used in aggregate as active ingredients tobind cytochrome P450 isoenzymes.Results: Our analysis of excipients led to 17 new classes representing the various roles that excipients can bear. Wethen extracted excipients from RxNorm and added them to DrOn for branded drugs. We found excipients for 5,743branded drugs, covering ~27 % of the 21,191 branded drugs in DrOn.Our analysis of active ingredients resulted in another new class, active ingredient role. We also extracted strengthsfor all types of tablets, capsules, and caplets, resulting in strengths for 5,782 drug forms, covering ~41 % of the14,035 total drug forms and accounting for ~97 % of the 5,970 tablets, capsules, and caplets in DrOn.We represented binding-as-substrate and binding-as-inhibitor dispositions to two cytochrome P450 (CYP)isoenzymes (CYP2C19 and CYP2D6) and linked these dispositions to 65 compounds. It is now possible to queryDrOn automatically for all drug products that contain active ingredients whose molecular grains inhibit or aremetabolized by a particular CYP isoenzyme.DrOn is open source and is available at http://purl.obolibrary.org/obo/dron.owl.BackgroundIn previous work, we built the Drug Ontology (DrOn) tosupport comparative effectiveness research use cases andreported on its theoretical basis, the methodology weused to build it, and its ability to meet the use cases [13].Motivated by critiques and requests from end-users ofDrOn of its representation of ingredients, we describehow we have improved the accuracy and coverage of ourrepresentation of ingredients.The work involved three major components. The firstcomponent was the inclusion of excipients. Althoughactive ingredients and their strengths have obviouseffects on the efficacy of a drug, excipients also influencedrug effects in significant ways [46]. Additionally, it isnot uncommon for excipients to cause allergic reactionsin patients [7, 8]. The second component was the im-provement and extension of the representation of activeingredients, including the addition of strength informa-tion. The last component was representing for the firsttime in an open-access, machine-readable ontology thebinding disposition of certain molecules to cytochromeP450 (CYP) isoenzymes as substrates and / or inhibitors.* Correspondence: joshhanna@ufl.eduBiomedical Informatics Program, Department of Health Outcomes and Policy,University of Florida, Gainesville, FL, USA© 2016 Hanna et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 DOI 10.1186/s13326-016-0048-2MethodsIn Hogan et al. [1], we differentiated between excipientsand active ingredients but did not define or represent theirdifferences explicitly. To do so, we first conducted anontological analysis of the roles various ingredients havein drug products. We also represented strengths of activeingredients according to the value specification model ofthe Ontology for Biomedical Investigations (OBI) [9]. Wedocumented and reviewed our definitions and proposedclasses and their axiomatizations on the DrOn wiki page[10]. Once complete, we then analyzed RxNorm [11] toextract excipient and strength information and modeledthem according to the results of our analysis.Analysis of excipients and method of extracting themfrom RxNormWe reviewed publicly available sources of informationabout the various roles of excipients and conducted anontological analysis of them from the realist perspective.Excipients have numerous roles that aid in the manufac-ture, administration, identification, and preservation ofdrug products. To represent these roles, we defined thefollowing and included them in DrOn: excipient role, lu-bricant excipient role, glidant excipient role, anti-adherent excipient role, anti-friction excipient role, bind-ing excipient role, coating excipient role, protective coat-ing excipient role, enteric coating excipient role,administration coating excipient role, flavor coating ex-cipient role, lubricant coating excipient role, color excipi-ent role, flavor excipient role, disintegrant excipient role,preservative excipient role, sorbent excipient role, and ve-hicle excipient role. We present the results of our onto-logical analysis, including textual and axiomaticdefinitions of these terms in the Results section.RxNorm contains excipient information that it obtainsfrom Structured Product Labels (SPLs). SPLs are a digitalform of the physical product label that the Food and DrugAdministration (FDA) collects from drug manufacturers.RxNorm includes information extracted from SPLs andstores it with a source abbreviation (used to identify thesource of the information) of MTHSPL. RxNorm in-cludes a has_inactive_ingredient relationship extractedfrom the SPLs, which we used to identify the excipientsfor drug products in DrOn. Since DrOn previously onlycontained information from RxNorm under the sourceabbreviation RXNORMwhich is data collected fromthe other sources and then normalizedwe needed tomatch the MTHSPL atoms to the appropriate RxNormconcepts and then to the appropriate DrOn entities. Itshould be noted that the MTHSPL data is denoted sourcerestriction level 0 in RxNorm, meaning it is licensed forcreation of derivative open source works.We also make extensive use of Semantic Clinical Drugs(SCDs) and Semantic Branded Drugs (SBDs) in RxNorm.Each SCD represents a unique combination of active in-gredients, their strengths, and dose form. An SBD repre-sents everything that an SCD represents plus informationabout a drug products trade name.1 Both SCDs and SBDsare the result of RxNorms normalization process, andthus are assigned concept identifiers (RxCUIs).Using the April, 2015, release of RxNorm, we:(1) Found all the atoms in the RXNREL table that havea source abbreviation of MTHSPL and arelationship type of has_inactive_ingredient.(2) Mapped both atoms to the appropriate RxNormconcept unique identifier (RxCUI).(3) Mapped the RxCUIs to atoms within theRXNCONSO table that have a source abbreviationof RXNORM and a term type of IN (foringredients) or SBD (for drugs).(4) Mapped the RxCUIs to DrOn drug product andingredient classes that have the same RxCUIannotated on them.This process gave us a mapping that connectedbranded drugs in DrOn to various excipient ingredients.Because we used unique identifiers from both DrOn andRxNorm (RxCUIs) to create this mapping, the processwas straightforward, and required no manual resolutionof ambiguity.We excluded excipients linked to SCDs in RxNormbecause we found that multiple generic and brandedproducts extracted from SPLs were linked to SCDs butnot SBDs, resulting in SCDs being linked to all the excipi-ents of many drug products at the same time. For ex-ample, dimethicone 10 MG/ML Topical Cream (RxCUI200010) is associated with 39 different SPL drug products,including many branded drugs like Proshield Glove SkinProtectant (RxAUI 4232431) or Better Than Nature EyeEssence (RxAUI 4660113), for which there does not alsoexist in RxNorm a SBD. Future work involves represent-ing these products distinctly in DrOn.Analysis of active ingredients and extracting their strengthsfrom RxNormAlthough in Hogan et al. [1], we recognized the ac-tive ingredient as being a scattered molecular aggre-gate as defined and represented in the Ontology ofBiomedical Investigations, the Web Ontology Lan-guage (OWL) representation of DrOn lagged behindthis recognition. Our first major change, then, was toupdate the OWL representation of active ingredientsfrom, for example:: (has_proper_part someramipril) was updated to (has_proper_partsome (scattered molecular aggregateand (has granular part someramipril))).Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 2 of 9The second update was to define active ingredient asa role (see Results) and assert that the scattered molecu-lar aggregate is the bearer of this role:has_proper_part some (scattered molecu-lar aggregate and (has granular partsome ramipril) and (is bearer of someactive ingredient role))The third update was to begin capturing strengthinformation starting with the most prevalent and easi-est case: tablets, capsules, and caplets. DrOn alreadycontains all of the active ingredients found withinRxNorm with a source abbreviation of RXNORM. InRxNorm, strengths are related to Semantic DrugComponents (SCDCs), which are not represented inDrOn. RxNorm creates one SCDC per unique com-bination of active ingredient and strength and also re-lates a drug to its active ingredients via SCDCs witha consists_of relationship. We therefore carried outthe following steps to map the active ingredients ofdrug products in DrOn to their appropriate strengths.We did this using the April, 2015, version of RxNormas follows:(1) Mapped the clinical drugs within DrOn to RxNormconcepts in the RXNCONSO table with a sourceabbreviation of RXNORM and a term type ofSCD using the annotated RxCUI.(2) Mapped the SCDs from the previous step to theappropriate concepts with a source abbreviation ofRXNORM, a relationship of consists_of , and termtype of SCDC using the RXNCONSO andRXNREL tables.(3) Mapped the SCDC concepts from the previous stepto the appropriate concepts with a sourceabbreviation of RXNORM, a relationship ofhas_ingredient, and term type of IN using theRXNCONSO and RXNREL tables.(4) Mapped the IN concepts to the ingredients withinDrOn using its RxCUI.(5) Pulled out the strength of the SCDC from theRXNSAT table using the RXN_STRENGTHattribute name.This process gave us a mapping between clinical drug,ingredient, and strength that we then used to build theOWL representation as illustrated below.In DrOn, we place branded drug classes (corre-sponding to SBDs) as subclasses of classes that repre-sent preparations of specific active ingredients, theirstrengths, and dose form (corresponding to SCDs).Thus, we only needed create axioms representingstrengths at the SCD-equivalent level since these ax-ioms are inherited by classes further down the hier-archy and thus apply to the branded drugs.ResultsOur work has three key contributions: 1) a realist analysisand resulting ontological representation of drug excipientsand the various roles they play, 2) a realist analysis of ac-tive ingredients and their strengths, and 3) a realist ana-lysis of cytochrome P450 isoenzyme binding. In the rest ofthis section, we will describe them in detail.Realist analysis of drug excipientsThe excipients used in drug products have varied roles.We define an excipient role as a role of a scattered mo-lecular aggregate in aiding the manufacture, prolongingthe shelf life, aiding the identification, or ensuring properadministration of a drug product.Before creating a new term, we surveyed other OBOFoundry resources for existing terms that met our needs.The Chemical Entities of Biological Interest (ChEBI)ontology [12] defines an excipient role as a generallypharmacologically inactive substance that is formulatedwith the active ingredient of a medication.This definition would seem to be inline with ourusage, but the term seems to be used within ChEBI toapply to individual molecules rather than aggregates,meaning every molecule of magnesium stearate in somedrug tablet has its own role to, for instance, decrease theadhesion between the other ingredient molecules andthe manufacturing machinery. Although it is true thateach molecule has some disposition that, in aggregate,leads to lower adhesion, a single molecule is not suffi-cient when added to a drug preparation by itself. Itsintended usage, and thus its role, can only be realized inthe aggregate, and thus we assign the role to the aggre-gate of all magnesium stearate molecules used in themanufacture of the drug product (not just those mole-cules remaining).Furthermore, an excipient role as defined in ChEBI istoo general. An excipient is added to a drug productwith a specific intent, unless we are to count contami-nants. If, in the process of manufacturing a drug prod-uct, some minor contaminant makes it into a gelcapsule, it is not an excipient. Therefore, assigning therole of exicpient to all things formulated with the activeingredient is too broad.In addition to a general excipient role, we have identi-fied sixteen specific subtypes of excipients based on spe-cific uses. Figure 1 shows the various types of excipientroles and the relations between them.Lubricant excipient role: An excipient role that is real-ized by a process of drug administration or a process ofdrug manufacturing and results in either 1) decreasedadhesion between drug ingredients and manufacturingequipment or between drug ingredients and some part ofan organism; 2) decreased friction between drug ingredi-ents and manufacturing equipment or between drugHanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 3 of 9ingredients and some part of an organism; or 3) de-creased cohesion among particles within the drugpreparation.Lubricant excipients are added to drug preparations toprevent ingredients from sticking to themselves (cohe-sion) and to other things with which they come intoFig. 1 Excipients. The various excipient roles and their is-a relationshipsHanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 4 of 9contact (adhesion). Common lubricants are minerals likemagnesium stearate. There are three major subtypes of lu-bricants: glidants (glidant excipient role), anti-adherents(anti-adherent excipient role), and anti-friction lubricants(anti-friction excipient role). In defining the three subtypes,we make the distinction between adhesion (which is asteady or firm atachment) and friction (which is the forcethat provides resistance to relative motion). To see the dif-ference consider a wet piece of paper: it will adhere to aplate of glass, but offer minimal friction to movementalong the glass.Glidant excipient role: A lubricant excipient role thatis realized by a process of drug administration or aprocess of drug manufacturing and results in decreasedcohesion or friction among particles within a drugpreparation.A glidant is added to a drug product to reduce cohe-sion and interparticle friction. Common glidants are talcand magnesium carbonate.Anti-adherent excipient role: A lubricant excipient rolethat is realized by a process of drug administration or aprocess of drug manufacturing and results in decreasedadhesion between drug ingredients and manufacturingequipment or between drug ingredients and some part ofan organism.Anti-adherents are added to drug products to decreasethe tendency of drug molecules to adhere to manufac-turing equipment or some body part such as the throator esophagus during swallowing.Anti-friction excipient role: A lubricant excipient thatis realized by a process of drug administration or aprocess of drug manufacturing and results in decreasedfriction between drug ingredients and manufacturingequipment or between drug ingredients and some part ofan organism.Anti-friction excipients are added to decrease eitherinternal friction (i.e., friction between ingredient parti-cles) or friction between the drug ingredients or productand some other object, such as manufacturing equip-ment or some body part.Binding excipient role: An excipient role that is real-ized by a process of drug manufacturing and results inincreased volume or cohesion of the drug product.Binding excipients are added to drug preparations to1) bind active ingredients together, and 2) increase thevolume of the preparation (which is especially importantfor formulations with otherwise small volumes). Com-mon binding agents are saccharides (like sucrose) orsynthetic compounds like polyethylene glycol.Coating excipient role: An excipient role borne by anaggregate of molecules on the surface of a solid drugproduct that is realized by a process of delaying inter-action between entities outside the drug product and theother ingredients in the drug product.Coatings are extremely common excipients, added toprotect the drug preparation from destruction or con-tamination, to ease administration by making it easier toconsume, or to improve flavor. There are five major sub-types of coating excipient.Protective coating excipient role: A coating excipientrole that is realized by delaying denaturation, disintegra-tion, or some other method of destruction of a drug prep-aration including its active ingredients.A protective coating acts against destruction orcontamination of a drug preparation by keeping theother drug ingredients, especially active ingredients,away from potentially reactive substances like oxygen,water, and various forms of electromagnetic radiation(e.g., light).Enteric coating excipient role: A protective coating ex-cipient role that is realized by a process of delaying releaseof one or more active ingredients from the drug productuntil some targeted time or location, typically the small orlarge intestine, within an organism.An enteric coating also protects the drug preparationfrom destruction or contamination, but also is designedto disintegrate on a controlled timeline or in a particularplace. For instance, some enteric coatings are designedto withstand the relatively high PH of the stomach, butbreak down in the relatively low PH of the large intes-tine, allowing an ingredient that would otherwise bedestroyed by or absorbed by the stomach to be absorbedin the intestine.Administration coating excipient role: A coating ex-cipient role that is realized by facilitating a process ofdrug administration.An administration coating is one that somehow im-proves administration of the drug, by for example mak-ing insertion or consumption of the drug easier ormasking undesirable flavors.Flavor coating excipient role: An administration coat-ing excipient role that is realized by a drug manufactur-ing process that results in the drug product bearing aparticular flavor quality.Flavored coatings make it more palatable to consumea drug product by improving its taste, often by maskingthe unpleasant taste of the active ingredients.Lubricant coating excipient role: An administrationcoating excipient role that is realized by decreased fric-tion between the drug preparation and some part of anorganism during drug administration.A lubricant coating makes it easier to consume or in-sert a drug product by decreasing the friction or adhe-sion between the drug preparation and some body partsuch as the throat or esophagus.Color excipient role: An excipient role that is realizedby a process of drug manufacturing that results in a par-ticular, desired color quality of the drug product.Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 5 of 9Colored excipients are added to a drug preparation tomake various kinds of drugs more easily identifiable bysight to decrease the possibility of using the wrong dos-age or wrong drug product altogether.Flavor excipient role: An excipient role that is realizedby a process of drug manufacturing that results in thedrug product bearing a particular flavor quality.Like a flavored coating, a flavored excipient is addedto the drug preparation to make it more palatable. Thisis especially important for drug products targeted to-wards children to make administration easier.Disintegrant excipient role: An excipient role that is re-alized by a process of drug administration followed bythe drug product breaking apart.A disintegrant is added to a drug preparation to causeit to break apart whenever it is introduced to moisture.A disintegrant can improve administration (such as oralmedications that dissolve in the mouth) or improve up-take of active ingredients for example, in the intestine.Preservative excipient role: An excipient role that is re-alized by increasing the duration of time that a drugproduct is effective or by inhibiting contamination of thedrug product with a microorganism.Preservatives are added to a drug preparation to in-crease the lifetime of the drug preparation. Examples in-clude antioxidants such as ascorbic acid that preventoxidation-reduction reactions that change active ingredi-ents into inactive compounds and methyl paraben whichis an antimicrobial preservative.Sorbent excipient role: An excipient role that is real-ized by its bearer binding with water in the environmentto prevent water binding with other ingredients in thedrug product.Sorbents are added to protect the drug preparationfrom destruction or disintegration by water. A commonexample is a desiccant, which is a sorbent that preventsabsorption of water into the drug product.Vehicle excipient role: An excipient role that is realizedby a completed process of the active ingredient reaching itsintended destination during drug administration.Generally, vehicles are the media in which the activeingredient is dispersed to facilitate the active ingredientreaching its intended target tissue. For example, activeingredients that exist in solid form such as a powdercannot be directly injected intravenously without causingdamage to veins or becoming emboli that cause damageto the lungs. Thus they are dissolved in solution for safeand proper administration. Other examples of vehiclesinclude creams, ointments, lotions, gels, and solvents forophthalmic, otic, and oral solutions.Having reviewed and defined the major subtypes of ex-cipients, we next illustrate how we represent molecularaggregates and their excipient roles in the DrOn OWLfiles. Consider a drug tablet that contains povidone andpregelatinized starch as excipients. We axiomatize thistablet as follows:tablet and (has_proper_part some ('scat-tered molecular aggregate' and(has_granular_part some povidone)and(bearer_of some 'binding excipientrole'))) and(has_proper_part some ('portion ofpregelatinized starch' and(bearer_of some 'binding excipientrole')))Our extraction of excipient information from RxNormresulted in the representation of excipients for 5,743branded drugs, covering ~27 % of the 21,191 total num-ber found in DrOn. There are a total of 35,455 differentdrug productexcipient relationships. By comparison,there are 22,845 relationships between drug productsand active ingredients. The main reasons there are fewerrelationships between drugs and active ingredients thanthere are between drugs and excipients is that there arefewer active ingredients and that active ingredients arespecified at a higher level in the taxonomy of drugs. Ac-tive ingredients are defined at the level of clinical drugform (for example, furosemide oral tablet) whereas ex-cipients exist at the level of branded drug (more specificthan clinical drug form), because each brand of a drugproduct such as furosemide 20 mg oral tablet typicallycontains a different set of excipients.Realist analysis of active ingredientsAlthough DrOn has always included active ingredients,we have updated the representation to more accuratelyreflect reality and to allow us to add strengths to drugproducts. To do so, it was necessary to represent activeingredient role, which we define as a role borne by anaggregate of molecules that is a proper part of a drugproduct and that is realized by (1) administration of thedrug to an organism followed by (2) some change in thestructure or functioning of some part of the organism orits endosymbiotic organisms.This definition meets several criteria we identified dur-ing our analysis of active ingredients. First, it is arealizable entity. Note that an active ingredient doesnothing until and unless the drug product is appropri-ately administered. Second, it is a role rather than a dis-position (or, more specifically, a function). Someingredients can serve as either an excipient or an activeingredient depending on the specific drug product. Forexample, calcium carbonate is an active ingredient incertain antacid tablets, but an excipient in other prod-ucts. Furthermore, calcium carbonate neither evolvednor was designed to neutralize acids (a key criterion offunctions per BFO). Of course, there is some dispositionHanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 6 of 9at the molecular level that the realization of the activeingredient role depends on; in the case of calcium car-bonate, its physical makeup causes it to react withstrong acids, releasing carbon dioxide. But this dispos-ition inheres in each individual molecule of calcium car-bonate whereas the active ingredient role inheres in theentire aggregate in the tablet: clinically signficant acidneutralization occurs only with the aggregate deliveredvia the tablet.We represent the active ingredient role in OWL in amanner similar to how we represent the excipient role.We represent a drug tablet that has acetaminophen asan active ingredient with a strength of 325 MG as thefollowing:tablet and (has_proper_part some ('scat-tered molecular aggregate' and(has_granular part some acetaminphen)and(bearer_of some active ingredientrole') and(bearer_of some (mass and(has_specified_value 325)and(has_measurement_unit valuemilligram)))))We added strengths to 5,782 clinical drugs, cover-ing ~41 % of the 14,035 total number, and account-ing for ~97 % of the 5,970 tablets, capsules, andcaplets in DrOn. Representing strengths for drugproducts in other dose forms (e.g., injectable solu-tions, creams, lotions, etc.) is future work.Realist analysis of cytochrome P450 isoenzyme bindingWhen a particular molecule binds to one of the isoen-zymes in the cytochrome P450 (CYP) family, it does soas substrate, inhibitor, or both. Induction of CYP isoen-zymes does not involve binding to individual enzymemolecules themselves, but rather it involves increasingtranscription of CYP isoenzyme genes so that more indi-vidual enzymes come into existence. We could not findin the literature any case where molecular binding of asmall molecule to a CYP isoenzyme induced or facili-tated the activity of the isoenzyme.We therefore represent binding of a small molecule toa CYP isoenzyme as a disposition of the molecule. It isnot a function because the small molecule neitherevolved nor was designed to have this binding effect. Itis not a role because the tendency to bind is internal tothe physical structure of the molecule itself. Nothing ex-ternal or socially-designated causes the binding tendencyto exist (note: we are loosly using the word tendencyhere to equate to what BFO calls realizable entity).We subdivide the binding disposition based onwhether its realization results in transformation of themolecule into another type of molecule (that is, metab-olism of the molecule into something else) versuswhether its realization causes inhibition of the isoen-zyme in metabolizing other small molecules. We notethat many types of molecules used as active ingredientdrugs have both substrate and inhibitory dispositions(for example, esomeprazine is both an inhibitor and sub-strate of CYP2C19).To represent enzyme binding, we identified an enzymebinding class in the Gene Ontology (GO) and importedit into DrOn via the Minimum Information to Referencean External Ontology Term (MIREOT) methodology[13]. Its definition is the following: interacting selectivelyand non-covalently with any enzyme. For completeness,we also import protein binding and binding, the parentand grandparent of enzyme binding, respectively, intoDrOn.For binding to the active site of an enzyme, we wereunable to identify a candidate term from any other real-ist ontology. Thus, we created the term enzyme activesite binding disposition, which we defined as a dispos-ition borne by some molecular entity that is realized bybinding to some enzyme and being destroyed in a processthat realizes some function of said enzyme. Similarly, wecould not find and therefore created the term function-inhibiting enzyme-binding disposition, and defined it as adisposition borne by some molecular entity that is real-ized by 1) binding to some enzyme and 2) subsequent in-ability of the active site of the enzyme to bind itssubstrate(s).We represented substrate and inhibitory binding dis-positions for CYP2C19 and CYP2D6, because these arethe major two isoenzymes targeted by the personalizedmedicine program at the University of Florida [14, 15].We represented these in OWL by adding axioms as fol-lows, using CYP2C19 inhibitory disposition as an ex-ample, to the molecular entities for which they areapplicable:subclassOf (bearer_of some function-inhibiting CYP2C19 binding disposition)In total, we added CYP2C19 and CYP2D6 binding dis-positions to 65 molecules, with some of them being thebearer of both an inhibitory and substrate definition(Table 1). Our source of data for the types of moleculesthat bear the particular types of binding dispositions wasTable 1 Number of CYP binding dispositions of various types inDrOn. The total number of molecular entities is 65; many havemore than one dispositionSubstrate InhibitoryCYP2C19 26 16CYP2D6 45 36Hanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 7 of 9the P450 drug interaction table of the Indiana UniversitySchool of Medicine [16].DiscussionWe have significantly updated and improved the repre-sentation of ingredients in the Drug Ontology. In theprocess, we have defined a number of key terms inDrOn including active ingredient role, excipient role,terms for numerous subtypes of excipient, and variousterms for cytochrome P450 substrate and inhibitorybinding. This representation enables automated algo-rithms to distinguish active ingredients from excipientsin drug products, as well as determine the strength ofdrug products that are capsules, tablets, and caplets.Given that excipients have important clinical conse-quences, including hypersensitivity reactions, their inclu-sion could help improve research on drug products,pharmacogenomics, and clinical decision support.A key use of DrOn is in the improvement andstandardization of knowledge of drug-drug interactions(DDIs) [17]. This work requires accurate representationsof active ingredients with strengths and excipients sincethey impact the potential for, likelihood, and severity ofinteractions. Our work on CYP isoenzymes further en-ables query of DrOn for all drug products that containan ingredient whose molecular grains bind particularCYP isoenzymes. A researcher could use these represen-tations, for example, to identify all patients who are tak-ing one drug that inhibits a given isoenzyme andanother drug that is metabolized by it, and thus is at riskfor adverse effects of the latter drug (that is, the inhib-ition caused by the first drug will reduce the metabol-lism of the second drug, leading to increased levels andthus increased risk of toxicity).Other work seeks to infer DDIs based on commonproperties including the structure of compounds [18].Specifically, if some but not all compounds with a givenproperty or structure X are asserted to have a DDI withsome compound Y, then this method identifies theremaining compounds with X as candidates for also hav-ing a DDI with Y. As DrOn is increasingly used for DDIrepresentation, it will be interesting future work to com-pare this method applied to DrOn-based DDI represen-tations vs. other artifacts.For DrOns representation of strengths, we were ableto reuse the value specification of the Ontology of Bio-medical Investigations as well as its object and datatypeproperties. We used the MIREOT Protégé plugin we de-veloped [13] to include these properties as well as theunits of measure required.While adding excipients, we discovered that there was asignificant sparsity of branded drugs in RxNorm with ex-cipient information. The reason is likely that RxNormbegan incorporating SPLs, the current source of excipientinformation, only recently in 2012. Additionally, RxNormhas mapped many drugs with FDA SPLs to SemanticClinical Drugs only. For example, dimethicone 10 MG/MLTopical Cream (RxCUI 200010) is associated with 39 dif-ferent SPL drug products, including many branded drugslike Proshield Glove Skin Protectant (RxAUI 4232431) orBetter Than Nature Eye Essence (RxAUI 4660113). ThisSCD has around 170 different excipients associated withit. Another example is Dextromethorphan Hydrobromide2 MG/ML / Guaifenesin 20 MG/ML Oral Suspension(RxCUI 1605844), which is associated with Tussin Coughand Chest Congestion DM Adult (RxAUI 6836489). Theexcipients linked to these Semantic Clinical Drugs appearto be a superset of all the excipients of the SPL-deriveddrug products that RxNorm links to the SCD. BecauseRxNorm does not represent generic drug products, theexcipients of all generic products also appear to be linkedto the SCD. Of course, these observations are likely re-lated. Further analysis is required.In the process of defining the active ingredient role,we added the capability to represent pharmaceuticalstrength. We began with tablets, capsules, and capletsbecause they represent the total quantity of active ingre-dient, which is simpler to represent than concentrations.For other dose forms, RxNorm specifies the quantity ofactive ingredient per unit of drug product (e.g., per milli-liter of solution, per gram of ointment) and the totalquantity of drug product (e.g., 5 mL vial, 25 g tube ofointment) is not always available from which the totalmass of active ingredients could be derived.Future WorkWe have three primary directions for future work. First,we intend to increase coverage of excipients andstrengths of active ingredients. Our strength coveragefor the dose forms we used in this analysis is sufficientlyhigh, but we still need to work out the representationand then extract strength information for other doseforms, which are expressed as relative vs. total quantityof active ingredient. Additionally, we intend to tease outthe excipients that are currently mapped to SCDs inRxNorm, which requires further analysis.Second, we intend to represent the induction of CYPisoenzymes by particular active ingredients in drug prod-ucts. The inductive effect is indirect through an increasedrate of genetic transcription that creates additional copiesof CYP isoenzymes, rather than through mere binding tothe isoenzyme. It is therefore somewhat more complex torepresent. It is also likely an aggregate effect as opposed aproperty of any individual molecule (although dispositionsof the molecules are certainly involved along the way).Third, we intend to represent therapeutic indicationsof drug products. We currently posit that a therapeuticindication is a function borne by a drug product that isHanna et al. Journal of Biomedical Semantics  (2016) 7:7 Page 8 of 9realized by a process of administration to an organism,distribution of one or more active ingredients to sometarget tissue, and resulting in some physical change inthe targeted tissue. However, this work requires furtherdevelopment of use cases and ontological analysis.ConclusionsIn this paper, we describe three primary lines of work: 1)an update to our representation of active ingredients, in-cluding adding strengths; 2) a new representation of ex-cipients; and 3) a new represention of substrate andinhibitory binding dispositions for CYP2C19 andCYP2D6. We created new terms and definitions for ex-cipient role and sixteen different subtypes, the active in-gredient role, and various terms to represent substrateand inhibitory binding dispositions for CYP2C19 andCYP2D6. We also reported on how these terms wereused in the Drug Ontology, and made the updated rep-resentations available at http://purl.obolibrary.org/obo/dron.owl.Endnotes1And hence only branded drug products, and not genericdrug products, of manufacturers are assigned RxCUIs.Competing interestsThe authors declare that they have no competing interests.Authors' contributionsAuthor JH contributed to the class definitions, the mining of strengths andexcipients from RxNorm, and preparation of this manuscript. Author WRHcontributed to the class definitions, the representational work for ingredientsand strengths, and preparation of this manuscript. Author JB contributed tothe preparation of this manuscript. All authors read and approved the finalmanuscript.AcknowledgementsThis work was supported in part by the NIH/NCATS Clinical and TranslationalScience Award to the University of Florida UL1 TR000064.Received: 11 November 2015 Accepted: 2 February 2016RESEARCH Open AccessOntology-based collection, representationand analysis of drug-associated neuropathyadverse eventsAbra Guo1, Rebecca Racz1, Junguk Hur2, Yu Lin1, Zuoshuang Xiang1, Lili Zhao1, Jordan Rinder1, Guoqian Jiang3,Qian Zhu4 and Yongqun He1,5*AbstractBackground: Neuropathy often occurs following drug treatment such as chemotherapy. Severe instances ofneuropathy can result in cessation of life-saving chemotherapy treatment.Results: To support data representation and analysis of drug-associated neuropathy adverse events (AEs), wedeveloped the Ontology of Drug Neuropathy Adverse Events (ODNAE). ODNAE extends the Ontology of AdverseEvents (OAE). Our combinatorial approach identified 215 US FDA-licensed small molecule drugs that induce signsand symptoms of various types of neuropathy. ODNAE imports related drugs from the Drug Ontology (DrON) withtheir chemical ingredients defined in ChEBI. ODNAE includes 139 drug mechanisms of action from NDF-RT and 186biological processes represented in the Gene Ontology (GO). In total ODNAE contains 1579 terms. Our analysis ofthe ODNAE knowledge base shows neuropathy-inducing drugs classified under specific molecular entity groups,especially carbon, pnictogen, chalcogen, and heterocyclic compounds. The carbon drug group includes 127 organicchemical drugs. Thirty nine receptor agonist and antagonist terms were identified, including 4 pairs (31 drugs)of agonists and antagonists that share targets (e.g., adrenergic receptor, dopamine, serotonin, and sex hormonereceptor). Many drugs regulate neurological system processes (e.g., negative regulation of dopamine or serotoninuptake). SPARQL scripts were used to query the ODNAE ontology knowledge base.Conclusions: ODNAE is an effective platform for building a drug-induced neuropathy knowledge base and foranalyzing the underlying mechanisms of drug-induced neuropathy. The ODNAE-based methods used in this studycan also be extended to the representation and study of other categories of adverse events.BackgroundThe word neuropathy is derived from two parts: neuroreferring to the nerve and pathy indicating disease.Neuropathy refers herein to nerve damaging. The mani-festation of neuropathy often includes chronic pain, lossof sensation, paresthesia, dysesthesia, and motor move-ment disorders [1]. Drug-induced neuropathies are usuallyuncommon (24 % of cases in one outpatient neurologysetting), but crucial to recognize because intervention canlead to significant improvement or symptom resolution[2]. Typically, chemotherapy drugs cause higher inci-dences of severe neuropathy than other drugs. For ex-ample, bortezomib (indicated for multiple myeloma andmantle cell lymphoma) caused treatment-related severeperipheral neuropathy (PN) (grade 34) in ~35 % of 331relapsed multiple myeloma patients (Drugs@FDA). Be-sides affecting patient quality of life, an effective treatmentcould be discontinued if PN is intolerable. The signs,symptoms and severity of drug-induced neuropathy arerelated to many variables such as mechanism of drug ac-tion, drug dose, duration of treatment, and host factors.Drug targets in the nervous system are diverse and includecell bodies in the dorsal root ganglia, ion channels, myelinsheath, and neuronal mitochondria. These neurotoxic tar-gets often overlap with drug therapeutic mechanisms. For* Correspondence: yongqunh@umich.eduEqual contributors1University of Michigan Medical School, Ann Arbor, MI 48109, USA5Unit for Laboratory Animal Medicine, Department of Microbiology andImmunology, Center for Computational Medicine and Bioinformatics, andComprehensive Cancer Center, University of Michigan Medical School, 1301MSRB III, 1150 W. Medical Dr., Ann Arbor, MI 48109, USAFull list of author information is available at the end of the article© 2016 Guo et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Guo et al. Journal of Biomedical Semantics  (2016) 7:29 DOI 10.1186/s13326-016-0069-xexample, taxanes, which interfere with cell division andapoptosis by binding to ?-tubulin subunits, can disruptaxonal transport in neurons and eventually lead to axono-pathy. While therapeutic strategies to alleviate neuropathyexist, a better understanding of pathophysiological mecha-nisms of the drug-induced neurotoxicity is needed to aidthe development of novel chemotherapeutics with a lowerneurotoxic profile.The study of drug-associated neuropathy adverseevents (AEs) relies on the use of different ontologies.Biomedical ontologies are sets of terms and relationsthat represent entities in the scientific world and howthey relate to each other. Ontologies have been used inapplications such as the establishment of knowledgebase and computer-assisted automated reasoning.The Ontology of Adverse Events (OAE; http://www.oae-ontology.org/) is a community-based bio-medical ontology in the domain of adverse events[3]. OAE provides a logically defined terminologyand term relations for various adverse events, in-cluding different types of neuropathy adverse events.OAE, together with related theories, also provides asemantic framework that links clinical adverse eventphenotypes with underlying biological mechanisms[4, 5]. Drug Ontology (DrON) is a newly generatedontology of drugs and related drug information [6].DrON incorporates drug information from RxNorm,a normalized drug naming system provided by theNational Library of Medicine at NIH [7]. DrON alsolinks drugs to chemical names based on chemicalnomenclature as represented in Chemical Entities ofBiological Interest (ChEBI) [8]. NDF-RT is anotherontology that includes mechanisms of action fordrugs. The mechanisms of actions may be linked toBiological Processes, a part of the Gene Ontology(GO) [9]. All these ontologies provide the basis forinterdisciplinary study, representation, and analysisof neuropathy adverse events.By integrating these ontologies with known drug-associated neuropathy AEs, it is possible to generate adomain-specific ontology to represent and study drug-associated neuropathy AEs. In this paper, we report ourefforts in developing a community-driven Ontology ofDrug Neuropathy Adverse Events (ODNAE). We col-lected neuropathy-inducing drugs from a number ofdatasets, ontologically represented the drugs and theirmechanisms, and generated scientific insights usingontology-based approaches.MethodsIdentification of FDA-approved drugs with neuropathy intheir labelsSeveral methods were applied to identify the US Foodand Drug Administration (FDA)-approved drugs knownto cause neuropathy. First, our study included a list ofneuropathy-associated drugs identified from a previousstudy using literature mining, survey of three databases(Drugs@FDA, DailyMed, and SIDER), and manual cur-ation [10]. This study uses neuropathy related termsfrom CTCAE [11] and MedDRA [12]. Secondly, we usedan ADEpedia dataset developed at Mayo Clinic (http://adepedia.org) [13] to obtain the information on drugsassociated with neuropathy. In the ADEpedia dataset,drugs are represented using the RxNorm codes (i.e.,RxCUIs) and AEs are represented using the SNOMEDCT [14] codes. Thirdly, we searched LinkedSPLs, aLinked Data resource that published the informationof FDA-approved drug package inserts from Dai-lyMed [15]. Lastly, we manually reviewed all thepackage insert documents and selected drugs aftermanual confirmation.ODNAE editing and existing ontology term importODNAE was developed using the format of W3C standardWeb Ontology Language (OWL2) (http://www.w3.org/TR/owl-guide/). For efficient editing of OAE, Protégé 4.3 or 5.0OWL ontology editor (http://protege.stanford.edu/) wasused. Based on the annotated data, we used OntoFox(http://ontofox.hegroup.org/) [16] to extract subsets of re-lated terms from different ontologies. Neuropathy AEsfrom OAE and drugs from DrON, were retrieved andimported to ODNAE, respectively. The mechanisms ofmost of these drugs are extracted from NDF-RT andimported to ODNAE. Gene Ontology (GO) biological pro-cessing terms that match the drug mechanisms were manu-ally identified and imported to ODNAE using OntoFox.Given that many terms from multiple ontologies (OAE,DrON, NDF-RT, and GO) were imported into ODNAE,the alignment of all the imported terms was a challengeand had been solved by a carefully designed strategy tomanually assert top level terms of these imported ontologysubsets under the ODNAE ontology hierarchical structure.Once the top level terms are aligned, the middle and bot-tom level ontology terms will be aligned automatically. Inaddition, we used Ontorat, another internally developedweb-based program (http://ontorat.hegroup.org/) [17],to assign RxNorm and NDF-RT identifiers to corre-sponding DrON drug terms using the annotationproperty rdfs:seeAlso.Generation of new ODNAE terms and axioms related todrug-induced neuropathy AEsOntorat was used to generate specific drug-inducedneuropathy AE terms with ODNAE_ prefix, and definenew axioms to link the newly generated ODNAE termswith corresponding drugs and neuropathy AEs. To runthe Ontorat program, all the related data were formal-ized into a structure Excel template. Ontorat scriptsGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 2 of 12were developed to identify sets of data and insert theminto ODNAE under appropriate hierarchical structures.ODNAE access, visualization, and licensingThe ODNAE project website is located at Github:https://github.com/odnae. ODNAE has been depositedin the repositories of Ontobee (http://www.ontobee.org/ontology/ODNAE) and NCBO BioPortal (http://bio-portal.bioontology.org/ontologies/ODNAE). TheODNAE source code is also freely available under theCreative Commons 3.0 License (http://creativecommon-s.org/licenses/by/3.0/). This licensing allows ODNAEusers to freely distribute and use ODNAE.SPARQL query of ODNAEThe Ontobee [18] SPARQL query web page (http://www.ontobee.org/sparql) was used to perform SPARQLqueries of the ODNAE ontology to answer specificallydesigned questions. In total, six files of 20 SPARQLscripts were generated for this study. These files arestored on the Github website: https://github.com/odnae/odnae/tree/master/docs/SPARQL. Additional file 1 con-tains a summary of these 20 scripts.Heatmap analysis of ODNAE dataThe correlation between drug molecular entities and ad-verse events were presented using a heatmap. The heat-map was created using n ×m count matrix, where n isthe number of AEs and m is the number of drug mo-lecular entities (i.e., the top level drug chemical entitygroups). Each cell in the matrix is the number of drugsunder the drug chemical group (i.e., column) that are as-sociated with a specific AE (i.e., the row). The matrixwas generated by first using SPARQL to obtain the rawdata of each drug chemical group, drug, and drug-associated AEs, and using an R program to process andtransfer the data to the desired format. The heatmapwas ordered using the Manhattan distance and the mo-lecular entities were clustered using the complete link-age method. The heatmap was plotted using R 3.1.3.ResultsThe overall goal of this project is to generate andanalyze an ontology knowledge base of drug-associatedneuropathy AEs. By ontology knowledge base, wemean that the ontology itself serves as a knowledge basethat integrates various aspects of knowledge related to aspecific domain, promoting knowledge integration anddiscovery. Therefore, the ODNAE serves as a knowledgebase comprising drug components, chemical entities ofactive drug ingredients, drug mechanisms, and drug-inducing neuropathy AEs. To achieve this goal, we firstused different methods to identify drugs associated withdifferent types of neuropathy AEs. Related informationwas then represented in the ODNAE and furtheranalyzed.In what follows, single quotation marks  are used toquote specific ontology terms.Drugs associated with neuropathy adverse eventsAs described in the Methods section, three methods(i.e., literature mining followed by manual curation [10],ADEpedia query [13], and LinkedSPLs query [15]) wereused to obtain drugs associated with neuropathyAEs. Each of these methods identified from 150-230neuropathy-inducing drugs. After a second round ofmanual verification, we verified 215 chemical drugsknown to induce neuropathy AEs. This list of drugsdoes not include 36 drugs that were originally iden-tified from our data sources due to either a lack ofDrON IDs, a clear label of a neuropathy AE, or ab-sence of a subclass of neuropathy AE. It is notedthat the data from user-reported FDA adverse eventcase reporting systems (FAERS) [19] were not usedsince the FAERS results can be quite noisy. An Excelfile containing 215 annotated drugs and neuropathyAEs is stored in the ODNAE GitHub repository:https://github.com/odnae/odnae/raw/master/src/ontology/Ontorat_inputs/odnae-data-outputupdate.xlsx.General ODNAE design and statisticsThe top level hierarchy of ODNAE is demonstrated inFig. 1 and explained below.First, ODNAE extends OAE and reuses the upper levelof OAE. Like OAE, ODNAE uses the Basic FormalOntology (BFO) [20] as the upper level ontology. BFOcontains two branches, continuant and occurrent[21, 22]. The continuant branch represents time-independent entities such as material entity and qual-ity. The occurrent branch represents time-related en-tities such as adverse event, drug administration, drugmetabolism, and dose accumulation in human. Byaligning different terms under the two branches ofBFO, knowledge from broad biological areas relatedto drug-associated neuropathy AEs were captured andorganized under a unified ontology-level structure.Among several drug ontologies (RxNorm, NDF-RT,and DrON), we selected DrON as the default ontologyfor representing drugs, as DrON allows mapping be-tween drugs and ChEBI chemical terms. In addition, likeODNAE and OAE, DrON is also aligned with BFO. Theadvantage of using BFO is that BFO has been adoptedby over 100 biomedical ontologies. All these ontologiesfollow ontology design principles of the Open Biomed-ical Ontologies (OBO) Foundry [22]. Therefore, we wereable to easily import and integrate related terms fromDrON, OAE, and other OBO ontologies into ODNAE.In order to enable data integration and data reuse, weGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 3 of 12added links from the DrON terms to RxNorm andNDF-RT IDs by annotation property rdfs:seeAlso inODNAE.Figure 2 shows the basic design pattern of ODNAErepresentation of drug-associated neuropathy AEs(Fig. 2a) and one example of implementing the design(Fig. 2b). Specifically, a drug-associated neuropathy AE(e.g., bupropion-associated neuropathy AE) occurs after(preceded_by) an administration of a drug (e.g., Bupro-pion Oral Tablet or Aplezin) in a human patient. Thehuman has different qualities (such as age, gender, anddisease history) and genomics background which mayaffect adverse event outcomes. The drug has a propercomponent of a molecular entity (e.g., bupropion). Thedrug also has a specific role in a biological process. TheNDF-RT mechanism of action (MoA) terms (e.g.,dopamine uptake inhibitor) is represented as role(BFO_0000023), which is realized in a Gene Ontology(GO) biological process (e.g., negative regulation ofdopamine uptake GO_0051585) (Fig. 2).The linked information illustrated in Fig. 2 is logicallydefined in ODNAE. Logical constraints allow proper in-tegration and hierarchies among terms from differentontologies of drugs, the chemicals of active drug ingredi-ents, GO processes, and other information cross-linkedwith axioms. As a result, the ontology knowledge base ofdrug neuropathy AEs can be analyzed at different levelsof classification.As shown in Figs. 1 and 2, ODNAE imports termsfrom many existing ontologies and also contains newlygenerated, ODNAE-specific terms. In total, ODNAEcontains 1579 terms, including 249 terms withODNAE_ prefix and terms imported from other on-tologies such as 25 OAE terms, 500 ChEBI terms,and 331 DrON terms. The detailed statistics ofODNAE is available at the Ontobee website: http://www.ontobee.org/ontostat/ODNAE.In the next sections, we will provide more detailsabout the ODNAE contents and scientific insights fromODNAE data analysis.Various types of neuropathy AEs are associated withdrugsOur study identified 20 types of neuropathy AEs, each ofwhich is associated with at least one drug (Fig. 3). Repre-sented in a hierarchical structure, these AEs are logicallydefined and cross-referenced to existing AE representa-tion systems including MedDRA [12].Ontology representation of neuropathy-associated drugsand drug ingredientsThe active ingredient of a drug product plays a vital rolein its mechanism. The chemical structures of the drugactive ingredients are represented in ODNAE usingChEBI terms. Additional ChEBI terms are also importedto form the hierarchy of these active ingredients ofdrugs. The relation between a drug and a ChEBI chem-ical is presented by an object property has_proper_part(Fig. 2).Fig. 1 Top level ODNAE hierarchyGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 4 of 12Most drug-associating ChEBI terms are under the branchof molecular entity (CHEBI_23367). There are 23 classes,for example, carbon group molecular entity (CHEBI_33582), at the third layer below ChEBI term molecular en-tity (Fig. 4a). Among all these 23 classes, the carbon groupmolecular entity class is associated with 127 drugs (thehighest number). All drugs under this group were indeedall organic molecular entities (Fig. 4a). Among 13 sub-classes of organic entities, heterorganic entities link to 116neuropathy-inducing drugs (Fig. 4a). Figure 4b provides anexample of a subclass of heterorganic entities (Fig. 4b). Allthe results can be counted from the ontology display in theProtégé OWL editor. Alternatively, as detailed later, aSPARQL script can obtain the same count results.Ontology-based representation and analysis of drugmechanismsA total of 139 mechanisms of action (MoA) terms re-lated to neuropathy-inducing drugs was identified fromNDF-RT and imported to ODNAE. We identified 13 GObiological processes that directly realize roles, or MoAsfrom NDF-RT. Many MoA terms do not have matchedGO terms. ODNAE also includes 173 GO terms that arethe ancestor (or related) terms of these 13 GO terms.Much insight was gained by examining the NDF-RTMoAs collected in ODNAE (Fig. 5). All NDF-RT roleswere organized as subclasses of role in cellular and mo-lecular interactions, including the roles as enzyme in-hibitors, immunological and biological factors, andreceptors of different biological interactions. Our resultsshowed that 12 neuropathy AE related drugs inhibit theuptake of three neurotransmitters [dopamine (1), nor-epinephrine (10), and serotonin (11)]. There are 20drugs that interact with the G-protein receptors thatcontribute to neuropathy adverse events. We identified39 drug agonist and antagonist terms, including 16 ago-nists and 23 antagonists. Among them, there are fourpairs of agonists and antagonists of the same targetsFig. 2 ODNAE design pattern and example. a ODNAE design pattern of representing drug-associated neuropathy AE. b ODNAE representingbupropion-associated neuropathy AEGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 5 of 12Fig. 3 Various drug-associated neuropathy AEs as represented in OAE and imported to ODNAE. Red numbers represent the numbers of drugsassociated with the corresponding AEs. Circled are 3 terms not asserted but inferred under peripheral neuropathy AE using a Hermit reasoner inProtege OWL editorFig. 4 Example ChEBI classification of drug chemicals inducing neuropathy AEs. a 14 neuropathy-inducing drugs are classified under nucleotide.b 21 drugs containing organohalogen compounds as active ingredients were found to induce neuropathy AEsGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 6 of 12(Table 1). Specifically, there are 3 agonist drugs and 7antagonist drugs of the adrenergic receptor, 4 agonistsand 2 antagonists of dopamine, 4 agonists and 3 antago-nists of serotonin, and 5 agonists and 2 antagonists ofhormone receptor (Table 1). In addition, there are 3 hor-mone receptor modulators (i.e., Leuprolide, Leuprolideacetate, and Taxoxifen) that are also associated withneuropathy AE (Table 1).The GO terms in ODNAE cover a variety of processes,including negative regulation of neurotransmitter uptakeand synaptic transmission. GO terms are linked to genesand proteins. We will investigate in the future howODNAE can represent gene/protein-based neuropathymechanisms with the support of GO.Query of drug-induced neuropathy AEsThe ODNAE knowledge base can be queried through theOntobee SPARQL program. Different questions can beaddressed using SPARQL queries. For example, a SPARQLscript was generated to identify what drugs act as a sero-tonin agonist (Fig. 6). The query resulted in four drugs:eletriptan, almotriptan, sumatriptan, and zolmitriptan.In addition to the query shown in Fig. 6, many otherSPARQL scripts were also generated to meet differentrequirements for many studies introduced in this article,we have generated many SPARQL scripts. All thesequery scripts have been collected and provided in theAdditional file 1.Heatmap analysis of the correlations between drugmolecular entities and neuropathy AEsOne question is how to correlate the drug molecular en-tity group with specific neuropathy AEs. To address thisquestion, a heatmap analysis was performed (Fig. 7). Thedata for the heatmap analysis was achieved using SPARQLin the Methods section. The data obtained from SPARQLqueries include the drug molecular entities and AEs thatare associated with different drugs in ODNAE. The heat-map explores the relation between drug molecular entitiesand various neuropathy AEs (Fig. 7).Fig. 5 Various roles in cellular and molecular interactions played bydrugs associated with neuropathy AEs. The branch circled in blueindicates inhibitor roles related to neurotransmission. Roles circled inred indicate either agonists or antagonists associated withneuropathy AEsTable 1 Four pairs of agonists and antagonists of neuropathy-inducing drugsTarget Agonists/antagonists Drugsadrenergic receptor agonists 3: Tizanidine, Salmeterol, Salmeterol xinafoateantagonists 7: Ziprasidone, Amiodarone, Amiodarone HCL, Propafenone, Betaxolol, Sotalol, Sotalol HCLdopamine agonists 4: Bromocriptine, Pergolide, Pramipexole, Ropiniroleantagonists 2: Haloperidol, Ziprasidoneserotonin agonists 4: Almotriptan, Eletriptan, Sumatriptan, Zolmitriptanantagonists 3: Alosetron, Cyproheptadine, Ziprasidonehormone receptor agonists 5: Nevirapine, Megestrol, Dexamethasone, Fluticasone, Fluticasone propionate,antagonists 2: Bicalutamide, Megestrolmodulators 3. Leuprolide, Leuprolide acetate, TaxoxifenGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 7 of 12Our results showed that drug-associated carbon groupmolecular entities (CHEBI_33582), pnictogen (CHEBI_33302), chalcogen compounds (CHEBI_33304), andheterocyclic compounds (CHEBI_5686) were associ-ated with the highest numbers of AE cases, and thesefour groups of chemicals also form a cluster by them-selves in the heatmap analysis (Fig. 7). The chemicalsin each group are also associated with different typesof neuropathy AEs. For example, in the carbon groupmolecular entities, 45 drugs are directly associatedwith the top level neuropathy AE, 40 drugs associatedwith peripheral neuropathy AE, 6 drugs with neuro-toxicity AE, and 5 with paresthesia AE. In addition tothe four groups of chemicals with the highest num-bers of neuropathy-inducing drugs, other groups ofchemicals, including monocyclic, bicyclic, and poly-cyclic compounds, are also associated with high num-bers of neuropathy-inducing drugs. The chemicalgroups that are the least associated with neuropathyAEs include copper group entity, ring assembly, andgold molecular entity chemicals (Fig. 7).DiscussionThe contributions of this article are multiple. First, 215neuropathy-inducing drugs were manually collected andcurated from different reliable resources. Second,ODNAE serves as a ontology knowledge base that repre-sents drug-induced neuropathy AEs and links these AEsto different sets of entities (e.g., drugs, chemical charac-teristics, drug targets, drug mechanisms of action, andbiological processes). Third, the knowledge in the ODNAEknowledge base was analyzed for obtaining scientific in-sights into drug-associated neuropathy AEs from differentaspects, including related neuropathy AE classifications,chemical structure patterns, findings from mechanisms ofactions, and the heatmap relations between chemical clas-sifications and AE types. ODNAE also provides a semanticplatform for further knowledge addition/integration andadvanced analysis. For example, the ODNAE frameworkcan be extended for other drug and AE studies.Different from many reference ontologies (e.g., OAEand DrON) that represent terms and relations amongthe terms in a specific domain (e.g., adverse events anddrugs), ODNAE serves as a ontology knowledge basethat reuses reference ontology terms and provides logicalaxioms to link different pieces of information such asneurophathy AEs, drugs, chemical elements of drug ac-tive ingredients, and mechanisms of action. As a know-ledge base, ODNAE captures knowledge extracted frombiomedical bench research, clinical practices, and publichealth. Owing to the parsable and machine readablenature of the AE knowledge base, ODNAE supportsneuropathy AE data exchange, data integration, andautomated reasoning and classification.To demonstrate the advantages of ontology-supporteddata integration and classification, we have mined theODNAE knowledge base through systematic classifica-tion and statistical analysis and obtained many scientificfindings from this study. First, we identified the neur-opathy AE types induced by 215 drugs. Our systematicclassification identified the major drug chemical elementgroups (e.g., carbon molecular groups) and theirFig. 6 SPARQL query of drugs acting as a serotonin agonist. The query was done in Ontobee SPARQL website (http://www.ontobee.org/sparql)Guo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 8 of 12subgroups that are associated with neuropathy AEs(Figs. 4 and 7). We have also found an interesting obser-vation that many agonists and antagonists of the sametargets (e.g., dopamine, serotonin, and sex hormone re-ceptor) both lead to neuropathy AEs (Fig. 5 and Table 1).Such observation suggests that these target moleculesrequire a balanced level in the host, and too high or toolow may lead to neuropathy AEs. We have alsogenerated a heatmap to further identify the relations be-tween drug chemical entities and different types of neur-opathy AEs (Fig. 7).It is noted that many findings from our ontologyknowledge base analysis have been reported in the litera-ture [2331]. For example, agonists and antagonists ofthe same targets associated with neuropathy AEs havebeen reported previously [23, 24]. Specific chemicalFig. 7 Heatmap analysis of drug molecular entity-AE relations. Drug molecular entities include 20 DrON terms at the third layer under ChEBI termmolecular entity. Color scheme indicates the numbers of AEs for different groups of drugs: light grey is 0, dark grey is 1, and the rest are orderedby yellow, orange and redGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 9 of 12structures, which are among the structures found in ouranalysis, have been found to be required for the induc-tion of neuropathy [2531]. These literature reports in-deed confirm our analysis results from the usage of theontology-based neuropathy-inducing drugs as the onlyinput data. Given the complete list of the neuropathy-inducing drugs in our study, our analysis also provides acomprehensive view of features covered in the ontology.In addition, our ontology-based strategy generates a se-mantic framework that brings related information to-gether in a structured and logical format and supportsknowledge integration and analysis. Such a machine-readable ODNAE framework is novel and has not beenreported in any neuropathy adverse event studies.ODNAE also provides a basis for educational learning,further extension, and interaction with external domainsof knowledge to support integrative neuropathy pharma-covigilance research.Beyond the papers primary focuses on the data collec-tion, ontology representation, and ONDAE data analysisfor discovering scientific insights, ODNAE can be fur-ther used for more case studies in the future. For ex-ample, the integrated ODNAE knowledge and data canbe used to possibly predict potential neuropathy AEs forparticular drugs based on the structures of the drugsthat have been enriched in our study. Our study foundthat over half of the neuropathy-inducing drugs areorganic carbon molecules with special enrichment onheteroorganic and organic cyclic compounds (Fig. 4). Itis known that some specific chemical structures arerequired for the induction of neuropathy [2531]. Forexample, 1,2-diacetylbenzene (1,2-DAB) (but not itsisomer 1,3-DAB), 1,2-Diethylbenzene (1,2-DEB), and1,2,4-Triethylbenzene (1,2,4-TEB) are able to inducechromogenic changes and neurotoxicity; and the 1,2-spaced ethyl (or acetyl) moieties on a benzene ring ofthese hydrocarbons have been found to be a critical mo-lecular arrangement resulting neurotoxic properties [2527]. It is interesting that our results show 3 benzenedrug compounds (i.e., fentanyl, sulfasalazine, andacetylsalicylic acid) and 3 other benzoid drug com-pounds (i.e., mitoxantrone, fluoxetine, and losartan)are also associated with neuropathy AEs. Any spe-cific structures in these and other drug chemicalcompounds that may facilitate neuropathy processedrequire further analysis. It is likely that structuralsimilarity analysis combined with biological studies[2831] could be conducted among these drugs. Tovalidate the association between drug structures andspecific neuropathy, observational clinical trials andlaboratory experiments with valid animal models canbe considered. If a structure (e.g., 1,2-spaced ethylmoieties on a benzene ring) is found to be morepreferentially than others in inducing neuropathy, wecan specifically avoid or modify the structure (with abalance of efficiency) to increase drug safety.Another future use of ODNAE is to make ODNAE aplatform to model and represent other information re-lated to neuropathy-inducing drugs. Chemical character-istics of drug, drug disposition in humans, and patientfactors could all play a role in the induction of adversedrug events. For example, drug dosage, environmentalfactors, individual patient age, disease, genotype (e.g.,genetic variations compared to others), and physiologicalconditions each plays a critical role in specific drugneuropathy AEs. These parameters can be linked toother elements presented in the ODNAE semanticframework. Such an ontology-based semantic frameworkcan also be guided by related biological network theor-ies, including the OneNet Theory of Life [5, 32]. TheODNAE-based and theory-guided integrative analysiswould be able to identify relations between those factorsand drug-associated neuropathy. Therefore, our workdefines a very important framework for understandingdrug-induced peripheral neuropathy. Ultimately it willallow us to advance personalized medicine, including thedevelopment of neuroprotective strategies for cancer pa-tients or patients suffering from neurological disorderssuch as diabetic neuropathy.Our ODNAE will be continuously expanded and com-puterized to integrate multiple layers of information, in-cluding chemical characteristics of drugs, biologicalreceptors and processes at the cellular and molecularlevels, drug disposition in patients, pharmacogenetics,and population level variables. Drug-associated neur-opathy AEs are most likely associated with various per-sonal backgrounds such as age, gender, and genotype.ODNAE can be expanded to cover these more personal-ized factors to find trends in neuropathy and better pre-dict events.ConclusionsDrugs of diverse pharmacological classes may cause dif-ferent levels of neuropathy AEs. In this study, 215 drugswere collected and represented in the Ontology of DrugNeuropathy Adverse Events (ODNAE). ODNAE servesas a knowledge base that reuses existing ontologies andincludes logical axioms to represent the relations amongdifferent entities including these 215 drugs, drug-associated chemical elements, specific neuropathy types,mechanisms of drug action, and biological processes.The analyses of logically formed ODNAE informationrevealed remarkable scientific insights into drug-associated neuropathy adverse events. Particularly, ourstudy found different types of neuropathy AEs inducedby these 215 drugs, major neuropathy-inducing drugchemical entity groups, the observation of agonists andantagonists of the same targets that are associated withGuo et al. Journal of Biomedical Semantics  (2016) 7:29 Page 10 of 12neuropathy AEs, and specific relations between chemicalgroups and types of neuropathy AEs. These findings areconsistent with existing reports, further confirming thevalidity of our ontology-based analyses that use the listof neuropathy-inducing drugs as the only input. Overall,ODNAE provides a useful platform for integrating andanalyzing currently known information related to drug-induced neuropathy AEs and is extensible for future newknowledge representation, analysis and discovery.Additional fileAdditional file 1: SPARQL scripts developed for the ODNAE analysesused in the ODNAE manuscript. (PDF 475 kb)AbbreviationsAE: adverse event; CTCAE: common terminology criteria for adverse events;DL query: description logics query; FDA: Food and Drug Administration;NCBO: The National Center for Biomedical Ontology; OAE: ontology ofadverse events; OBI: ontology for biomedical investigations; OBO: The OpenBiological and Biomedical Ontologies; ODNAE: ontology of drug neuropathyadverse events; OWL: web ontology language; RDF: resource descriptionframework; SPARQL: SPARQL protocol and RDF query language.Competing interestsThe authors declare that they have no competing interests.Authors contributionsAG: Manually annotated drugs and their associated neuropathy AEinformation from FDA and other reliable resources, helped data analysis; RR:Manually annotated and verified information of drug-inducing neuropathyAEs from FDA and other reliable resources, served as a pharmacology do-main expert, helped data analysis and interpretation; JH: Literature mining ofneuropathy-inducing drugs from FDA package insert documents; YL: SPARQLimplementation, and data interpretation; ZX: Generated SPARQL queries andanalysis; LZ: Conduced the initial Heatmap analysis; JR: Standardized theHeatmap method initiated by LZ; GJ: Queried ADEpedia for neuropathy-inducing drugs; QZ: Queried LinkedSPLs for neuropathy-inducing drugs; YH:Overall project design, primary ODNAE developer, design pattern generation,adverse event domain expert, and data interpretation, and manuscriptdrafting. All authors edited and approved the manuscript.AcknowledgementsThis work was supported by a NIH grant R01AI081062 and a University ofMichigan MCubed 2.0 grant to YH. We appreciate Dr. Jane Bais help indiscussion and data interpretation in the domain of neuropathy adverseevents.Author details1University of Michigan Medical School, Ann Arbor, MI 48109, USA. 2Schoolof Medicine and Health Sciences, University of North Dakota, Grand Forks,ND 58203, USA. 3Mayo Clinic, Rochester, MN, USA. 4University of Maryland,Baltimore County, Baltimore, MD 21250, USA. 5Unit for Laboratory AnimalMedicine, Department of Microbiology and Immunology, Center forComputational Medicine and Bioinformatics, and Comprehensive CancerCenter, University of Michigan Medical School, 1301 MSRB III, 1150 W.Medical Dr., Ann Arbor, MI 48109, USA.Received: 5 April 2016 Accepted: 28 April 2016Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 DOI 10.1186/s13326-016-0075-zRESEARCH Open AccessGeneration of open biomedical datasetsthrough ontology-driven transformation andintegration processesMaría del Carmen Legaz-García1, José Antonio Miñarro-Giménez2, Marcos Menárguez-Tortosa1and Jesualdo Tomás Fernández-Breis1*AbstractBackground: Biomedical research usually requires combining large volumes of data from multiple heterogeneoussources, which makes difficult the integrated exploitation of such data. The Semantic Web paradigm offers a naturaltechnological space for data integration and exploitation by generating content readable by machines. Linked OpenData is a Semantic Web initiative that promotes the publication and sharing of data in machine readable semanticformats.Methods: We present an approach for the transformation and integration of heterogeneous biomedical data withthe objective of generating open biomedical datasets in Semantic Web formats. The transformation of the data isbased on the mappings between the entities of the data schema and the ontological infrastructure that provides themeaning to the content. Our approach permits different types of mappings and includes the possibility of definingcomplex transformation patterns. Once the mappings are defined, they can be automatically applied to datasets togenerate logically consistent content and the mappings can be reused in further transformation processes.Results: The results of our research are (1) a common transformation and integration process for heterogeneousbiomedical data; (2) the application of Linked Open Data principles to generate interoperable, open, biomedicaldatasets; (3) a software tool, called SWIT, that implements the approach. In this paper we also describe how we haveapplied SWIT in different biomedical scenarios and some lessons learned.Conclusions: We have presented an approach that is able to generate open biomedical repositories in SemanticWeb formats. SWIT is able to apply the Linked Open Data principles in the generation of the datasets, so allowing forlinking their content to external repositories and creating linked open datasets. SWIT datasets may contain data frommultiple sources and schemas, thus becoming integrated datasets.Keywords: Semantic web, Ontologies, Biomedical open data, Data transformationIntroductionBiomedicine is a knowledge based discipline, in which theproduction of knowledge from data is a daily activity. Cur-rent biomedical research generates an increasing amountof data, whose efficient use requires computing support.Traditionally, biomedical data have been stored in hetero-geneous formats in various scientific disciplines. Since the*Correspondence: jfernand@um.es1Departamento de Informática y Sistemas, Universidad de Murcia,IMIB-Arrixaca, 30071 Murcia, SpainFull list of author information is available at the end of the articledevelopment of the Protein DataBank [1] in the seventies,life scientists have developed many biological databases,and there are more than 1500 biological databases accord-ing to the 2015 Molecular Biology Database Update [2].As a consequence, biological data are represented in dis-parate resources [3], which makes data retrieval and man-agement hard for life scientists because they are requiredto know: (1) which resources are available and contain thedesired information; (2) the meaning of the data types andfields used in each resource; and (3) how such resourcescan be accessed and queried. There is, therefore, a clear© 2016 The Author(s). Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 2 of 17need for facilitating the integrated use of such resources.Unfortunately, there is also heterogeneity in the formatsused for storing such data, since they are not usually themost machine-friendly ones [4].On the medical and clinical side, the advent of elec-tronic health records (EHRs) contributes to making moredata available for computer processing, but suffers fromsimilar problems. The heterogeneity of EHR systems canbe assimilated to the one of biological databases. Thesemantic interoperability of EHR data not only has beenidentified as a need but also considered as a reason forinefficiencies within the healthcare system [5, 6] andfor the waste of billions of dollars in the United Statesannually [7].Translational research aims at applying basic biologi-cal results and data into clinical activities and routine. Inrecent years, supporting data-driven medicine has beenset as a challenge for translational bioinformatics [8].For this purpose, the integration and joint analysis andexploitation of heterogeneous data, both biological andmedical becomes critical, hence, solutions in this area arerequired.In the technical side, the Semantic Web [9] describesa new form of Web content meaningful to computers,in which the meaning is provided by ontologies. Anontology represents a common, shareable and reusableview of a particular application domain [10]. The factthat machines know the meaning of content enablesthe use of automated reasoning, which permits to infernew information or to check the logical consistency ofthe content. The Semantic Web has been proposed asa technological space in which biomedical data can beintegrated and exploited [11]. The growing interest ofthe biomedical community in the Semantic Web canbe illustrated by the fact that repositories such as Bio-Portal [12] contain at the time of writing more than500 biomedical ontologies, controlled vocabularies andterminologies.The Semantic Web community wishes to achieve theWeb of Data, which would semantically connect datasetsdistributed over the Internet. The Linked Open Data(LOD) effort1 pursues the publication and sharing ofbiomedical datasets using semantic formats such as RDF2or OWL3. The biomedical community is heavily involvedin the development of the LOD cloud [13], since integra-tion and interoperability are fundamental for biomedicaldata analysis [14]. The LOD cloud offers a promisinginfrastructure for such a goal. The availability of consen-sus ontologies generated by the biomedical communityfacilitates the publication of data in the LOD cloud, sincethose ontologies can be used as vocabularies for the RDFdatasets. Most efforts in this area have been solved byin-house solutions, implementing resource-specific trans-formation scripts. Hence, we believe that there is a needfor methods and tools that contribute to standardisethe process of getting biomedical datasets in semanticformats.Since the development and success of the Gene Ontol-ogy [15], ontologies have been used to support data anno-tation processes. The development and evolution of theSemantic Web technologies has permitted to increasethe variety of use of such technologies in biomedicaldomains. In the area of biomedical databases we can pointout two efforts of particular significance. First, the Euro-pean Bioinformatics Institute (EBI) has developed an RDFplatform which permits the semantic exploitation of thecontent of a series of EBI resources, including UniProt[16]. Second, the Bio2RDF initiative [13] has created RDFversions of thirty five biomedical resources (Release 3July 2014). These efforts pursue the development of thebiomedical LOD. In the area of EHRs, the SemanticHealthproject identified that ontologies should play a fundamen-tal role for the achievement of the semantic interoperabil-ity of EHRs [6]. Since then, Semantic Web technologieshave been increasingly applied in the EHR domain withdifferent purposes: representation of clinical models anddata [1719]; interoperability of models and data [2022];application of quality measurements and protocols todata [23, 24].The main objective of the present work is to pro-pose a method that could serve to simplify the processof generating integrated semantic repositories from het-erogeneous sources. The approach will be able to workwith relational databases, XML documents, and EHRdata and will produce datasets described by means ofontologies. The transformation process is independent ofthe formalism used for capturing the data to be trans-formed. This process will be driven by the semantics ofthe domain to ensure the correctness and logical con-sistency of the resulting content. This will be achievedby defining mappings between the data schemas and theontologies, which will provide the semantic content. Ourapproach will be able to create a repository from mul-tiple sources, which will require to define mechanismsfor merging the data about the same entity containedin the different resources. Besides, the resulting contentwill be generated according to the principles of LinkedOpen Data. We will also describe our Semantic WebIntegration Tool (SWIT), which implements the trans-formation and integration methods, and the applicationof our method in different use cases. The expected con-tributions of our research are (1) the common trans-formation and integration process for heterogeneousbiomedical data; (2) enabling the design of reusable map-pings between schemas driven by domain knowledge; (3)the application of Linked Open Data principles to gen-erate interoperable, semantically-rich, open, biomedicaldatasets.Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 3 of 17BackgroundBiomedical dataThe term biomedical data covers a wide range of typesof data used in biomedicine. Such data are usuallystored and represented in different, heterogeneous for-mats, which makes their joint exploitation difficult. Inthis work we are specially interested in the informationcontained in biomedical databases and in the content ofelectronic healthcare records because of their importancefor biomedical and clinical research.On the one hand, biomedical databases contain largevolumes of complex, dynamic information about biomed-ical entities. The information about a concrete biomedicalentity, like a protein, is distributed along many differentdatabases, which makes necessary to combine informa-tion from different sources to get all the information.These heterogeneous resources do not even share iden-tifiers for the biological entities, although this particularaspect is being addressed by initiatives like identifiers.org[25]. XML files and relational databases are popular for-mats used for the representation and sharing of biomedi-cal databases. For instance, OrthoXML and SeqXML [26]are two XML formats to standardise the representation oforthology data. Relational databases have gained popular-ity in the last years because they are effective in retrievingdata through complex queries. Biomedical resources suchas the Gene Ontology [15] or CHEBI [27] provide theirdata in relational format.On the other hand, the electronic health record ofa patient stores all the information digitally recordedfrom the interactions of the patient with the health sys-tem. In the last decades, many efforts have addressedthe development of EHR standards and specifications,such as HL7 [28], openEHR [29], and ISO EN 13606[30]. Such standards and specifications are based on thedual model architecture, which distinguishes two mod-elling levels. On the one hand, the information modelprovides the generic building blocks to structure theEHR information. On the other hand, clinical mod-els are used to specify clinical recording scenarios byconstraining the information model structures. In bothopenEHR and ISO EN 13606, clinical models are namedarchetypes and they have been considered a promisingway of sharing clinical data in a formal and scalable way[5]. Archetypes are used to specify clinical recording sce-narios. An archetype may be used to record clinical dataabout a laboratory test, a blood pressure measurement,a medication order, etc. They constitute a standardisedway of capturing clinical data according to the archetypemodel [31]. They are usually defined in the ArchetypeDefinition Language (ADL)4. EHR data extracts areusually represented as XML documents, whose con-tent should satisfy the constraints specified in thearchetype.The joint semantic exploitation of data stored in XMLfiles or in relational databases requires methods forthe transformation of the data into semantic formats.Both XML technologies and relational databases provideschemas which define the structure of the datasets. In ourapproach, such schemas will be used to define generic pro-cessing methods able to transform and exploit XML andrelational data using semantic technologies. More con-cretely, XML schemas, ADL archetypes and the schemaof relational databases will be managed in our approach.In practical terms, ADL archetypes play the role of XMLSchemas.Semantic representation and access to biomedical dataThe World Wide Web Consortium has developed a seriesof Semantic Web standards for exchanging data (e.g.,RDF), for representing their semantics (e.g., OWL) and forquerying these data (e.g., SPARQL5). Automated reason-ers (e.g., Hermit [32], Pellet [33]) can be used in conjunc-tion with Semantic Web content to check for the consis-tency of data or to infer new information. Semantic Webtechnologies also offer mechanisms for storing seman-tic data called triplestores and whose performance forcomplex queries is continuously improving [34]. LinkedOpen Data is a Semantic Web initiative aiming to materi-alise the Web of Data through the publication and sharingof datasets using semantic formats. Linked Open Datadatasets meet four requirements [35]: (1) use URIs asnames for things; (2) use HTTP URIs so that people canlook up those names; (3) when someone looks up an URI,provide useful information, using the SemanticWeb Stan-dards like RDF and SPARQL; and (4) include links to otherURIs, so related things can be discovered.The data published in the Linked Open Data (LOD)cloud are diverse in granularity, scope, scale and origin,and the LOD cloud is constantly growing with informa-tion from new domains. Berners-Lee6 suggested a five-star deployment scheme for Open Data, where each levelimposes additional conditions. The use of RDF and anappropriate use of URIs permit the achievement of four-stars datasets. The fifth one can be achieved by linkingyour dataset to external ones. It should be noted that thecommunity is trying to impose additional conditions toget such stars [36]. The number of biomedical datasetsin the LOD cloud is still reduced in comparison withthe number of existing biomedical databases, but ourapproach aims at facilitating biomedical communities tojoin and follow the LOD principles and effort. We believethat the development of methods that permit to get five-star datasets would contribute to the development of theSemantic Web.Next, we describe the two major approaches fordata exploitation using semantic technologies: (1) thetransformation of data into semantic formats; and (2)Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 4 of 17ontology-based data access, which works on traditionalformats.Semantic transformation of biomedical dataData transformation methods have been traditionallyused in projects that use the data warehouse approach andOLAP for the semantic exploitation of data [37], and withboth XML datasets and relational databases. On the XMLside, [38] presented an approach that transforms XML ele-ments into RDF statements, but does not transform XMLattributes. Another approach7 transforms XML instancesinto RDF according to a mapping between XSD andOWL8. These XSD2OWL mappings are canonical, sinceall the XML files are transformed into RDF by applyingthe same rules. Canonical XSLT-based approaches havealso been proposed [39, 40]. More recently, [41] proposedthe transformation of XML into RDF by applying XPath-basedmappings. On the relational database side, theW3CRDB2RDF specification9 proposes a canonical transfor-mation/mapping for relational databases to RDF. Such atransformation can be considered a change of format,because the real meaning of the entities represented is notused in such a process. This is an important limitation wefind in the state of the art transformation approaches andtools, since they do not take into account the underlyingmodel of meaning.In the last years, Bio2RDF has become the most promi-nent initiative for the generation of biomedical RDFdatasets. Bio2RDF has developed RDF versions for 35datasets (Release 3 July 2014), and its website containsnon-canonical transformation scripts for such resources.To the best of our knowledge, the links between the dataand the domain knowledge are not made explicit in suchtransformation scripts. For instance, there is no guaran-tee that content about a protein from different resourcesis transformed using the same meaning, and this makesmore difficult to expand the approach and to find errors.From a process perspective, the semantic transfor-mation of data requires the execution of Extraction-Transformation-Load (ETL) processes. Canonicaltransformation approaches apply the same ETL processto all the data. The required information about thesemantics of the data sources is sometimes missing inthe data schema or coded in natural language [42], whichmakes such canonical processes not effective enough toobtain semantically-rich datasets. Ontology-driven ETLprocesses use ontologies for giving precise meaning tothe source data, which will be made explicit in the trans-formation phase. This also enables consistency checkingin the transformation and/or load phases, which preventsfrom the creation of logically inconsistent content. Toolslike RDB2OWL [43] and Karma [44] are examples of toolsthat exploit mappings between relational schemas andontologies to generate RDF content.Ontology-based data accessOntology-Based Data Access (OBDA) permits to exploitrepositories in traditional formats using semantic tech-nologies. As stated in [45], the underlying idea is to facili-tate access to data by separating the user from the raw datasources. In OBDA, an ontology provides the user-orientedview of the data and makes it accessible via queries for-mulated solely in semantic languages such as SPARQL. InOBDA approaches, a mapping between the ontology andthe data sources defines the view on the source data thatcan be exploited using semantic technologies.Different OBDA approaches for accessing XML andrelational data can be found in the literature. On theXML side, XSPARQL10 was proposed as a query languagecombining XQuery and SPARQL for data transforma-tion between XML and RDF, and XS2OWL [46] createsOWL ontologies from XML schemas for allowing query-ing XML data using SPARQL queries. On the relationaldatabases side, Triplify [47], D2RQ [48], Virtuoso [49],Quest [50], Ultrawrap [51] and Ontop [52] are likely tobe the most popular OBDA systems nowadays. Such sys-tems differ in how they express the mappings, how theytranslate the queries and in the reasoning capabilities.Current OBDA approaches are limited in their supportfor reasoning. For example, D2RQ does not support rea-soning and OWL2 QL is the level of reasoning offeredby Ontop. OBDA tools are starting to provide supportto rule languages such as SWRL11 for enabling users toexploit Semantic Web rules over data in traditional for-mats. Given that our approach will rely on reasoning forguaranteeing the consistency of the transformation andintegration of data, OBDA is not the best option for ourwork.Integration of biomedical dataA variety of XML-based approaches for the integrationof data are presented in [53]. The main conclusion ofsuch study is that XML has succeeded in the integrationof data, and has opened new opportunities for research,but the variety of XML-based data formats makes verydifficult the effective integration of data sources. The solu-tion proposed is the adoption of semantic formats, whichleads us to semantic data integration scenarios, in whichontologies ideally provide the global schema. When thishappens, the integration process can also take advan-tage of the benefits described for ETL processes such asthe use of precise meaning or consistency checking. Thissemantic approach is also supported by the fact that theSemantic Web is a natural space for the integration andexploitation of data [11].There are four major types of data integration archi-tectures [53]: data warehouse, mediator-based, service-oriented and peer-based. The data warehouse approachis more related to the semantic transformation methods,Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 5 of 17and the other three are more related to ODBA, since theyperform a virtual integration. In the literature, we canfind biomedical semantic integration approaches such asOntofusion [54] or TAMBIS [55], which fall in the areaof mediator-based systems or OGO [56], which followsthe data warehouse approach. Bio2RDF uses a data inte-gration approach based on links. This is a case of virtualintegration that uses owl:sameAs statements to identifyinstances referring to the same entity in other resources.One limitation of state of the art approaches and toolsis that they are not generic enough in the sense of theirapplicability to both XML and relational data. Data inte-gration has to overcome issues such as redundancy orinconsistency between the data sources. Most media-tor or link-based approaches aggregate the data fromthe different sources, but the availability of mechanismsfor preventing redundancy or inconsistency is not com-mon. Those mechanisms are easier to include in datawarehouse-oriented methods, which provide more con-trol on the data. Our approach will be mostly based ondata warehouse, since the integrated datasets (from XMLand relational resources) are assumed to be created ina common repository. Besides, in order to preserve theoriginal datasets in the integrated, semantic repository,the configuration of the integration process will enable tomerge those equivalent instances or linking them throughowl:sameAs statements.MethodsIn this section we describe the methods included inour approach for the generation of the open biomedicaldatasets. Figure 1 provides a generic description of themethod for a single input data resource. Our data trans-formation approach is based on the definition of rulesbetween an input schema and an OWL ontology. Oncedefined the mapping rules, the transformation approachmay also take into account identity rules defined over theOWL ontology. Identity rules establish which propertiespermit identifying an individual of a certain ontologyclass. Thus, these rules permit to merge different individ-uals of the same class. Besides, the transformationmethodwill be able to detect and, therefore, avoid the creation oflogically inconsistent content by checking the consistencyof the OWL ontology. This is done because the wholeprocess is supported by OWL ontologies and, therefore,automated reasoning techniques can be applied. In gen-eral, the approach can be applied to any input data modelproviding entities, attributes and relations. In this work,we will use XML and relational databases as input datamodels. A practical requirement for our approach is thatthe input schema and the ontology should have somedomain content in common. In addition to this, the outputdata instances shown in Fig. 1 can be expressed in RDF orOWL.Data transformation rulesThe transformation rules define how the content of theinput dataset is transformed into a semantic format, andplay two major roles: (1) controlling that the informationrepresented according to the input schema is correctlytransformed into the semantic format; and (2) prevent-ing redundancy in the set of output dataset. For thispurpose, two major types of rules are defined in ourapproach, namely, mapping rules and identity rules. Bothare described in the next subsections.Mapping rulesThe definition of the mapping rules will be illustrated withthe example described in Fig. 2. In this example, (1) theinput schema is OrthoXML [26] (Figure 2 top left), whichis a standardised format for the representation of infor-mation about orthologous genes, (2) the ontology is theOrthology Ontology (ORTH)12(Fig. 2 top right), whichmodels domain knowledge about orthology. In the exam-ple, the entities of the input schema are represented withboxes, the attributes with@, and the relations with arrows.Fig. 1 Overview of the transformation approachLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 6 of 17Fig. 2 Description of the mapping between the OrthoXML schema (left) and the orthology ontology (ORTH)(right), where dashed lines representthe links between the content of the OrthoXML schema and the ontology and corresponding instances that fulfill the relation of congruenceIn the ontology, the classes are represented with roundedcorner boxes, the datatype properties with a pin attachedto the classes, and the object properties with arrows.Map-ping rules link entities, attributes and relations of the inputmodel with the ontology classes, datatype properties andobject properties. In Fig. 2, dashed lines represent themappings from the XML Schema to the ontology. Forsimplicity, this figure does not include mappings involv-ing relations or object properties. The ontology containsa series of prefixes, which refer to ontologies reused inthe ORTH: ro (Relations Ontology13), ncbi (NCBI Tax-onomy14), cdao (Comparative Data Analysis Ontology15),and sio (Semanticscience Integrated Ontology16)Generally speaking, the application of a mapping rule toan instance of the input dataset generates one individualin the ontology. The instance and the individual must ful-fill a relation of congruence. For us, an ontology individualt is congruent with an input instance s if t can be obtainedfrom s by applying a mapping rule and t is consistent withthe axioms expressed in the ontology and with s. The con-sistency with the axioms expressed in the ontology has tobe understood in OWL DL terms. The individual mustbe, in logical terms, a member of the class to which themembership is stated.The bottom of Fig. 2 shows an input instance (left)and the result of transforming it into an ontologyindividual (right) by applying the corresponding mappingrules. The input instance is a gene with attributes pro-tId=O17732, geneId=pyc-1, it is included in UniProtand it is associated with the species Caenorhabdi-tis elegans (NCBITaxId=6239). The ontology individ-ual Gene_1 is a member of the class Gene, it has adatatype property Identifier with value pyc-1. It islinked to other individuals through the properties encodes(Protein_O17732), ro:in_taxon (6239), and contained_in(UniProt). These individuals are members of the classesProtein, ncbi:organisms, and sio:database, respectively.The ontology individual Gene_1 is consistent with theORTH ontology and consistent with the data defined forthe input instance. Therefore, both entities are congruent.Our approach requires transforming entities, attributesand relations, so themapping rules must permit to achievecongruence at those three levels. To this end, three typesof basic mapping rules have been defined:Entity rule. It links an entity of the input schema withan OWL ontology class. It permits to create individ-uals in the OWL ontology. Let S be an entity of theinput schema and T be a class of the target ontol-ogy. Then, the entity_rule(S, T) means that for everyinstance s of S, there is a congruent individual t whichis an instance of T. For example, an entity rule in Fig. 2serves to link the element Gene of the XML SchemaLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 7 of 17and the class Gene in the ontology. An entity rulecan be complemented by a conditional statement thattransforms only those instances of S holding a cer-tain condition on the value of a particular attribute.Using the classes of the previous example, let A1 bean attribute associated with S, and let c1 be a booleancondition over A1. The entity_rule(S, T, c1) meansthat for every instance s of the entity S fulfilling c1there is a congruent individual t which is an instanceof T.Attribute rule. It links an attribute of an entity of theinput schema with the datatype property of an OWLontology class. It permits to assign values to datatypeproperties in the ontology. Let S be an entity of theinput schema, T an ontology class, and A1 and A2attributes/datatype properties associated with S andT respectively. Then, the attribute_rule((S, A1), (T,A2)) means that for each instance of S associated withA1 from the same schema, there is a congruent indi-vidual of T associated with the datatype property A2from the ontology and that A1 and A2 have the samevalue. For example, an attribute rule in Fig. 2 linksthe attribute id of the element gene in OrthoXML andthe datatype property Identifier of the ontology classGene.Relation rule. It links a relation associated with twoentities of the input schema with an object propertyassociated with two classes of the OWL ontology. LetS1 and S2 be entities of the input schema associatedthrough R1 and T1 and T2 be classes of the ontol-ogy associated through the object property R2. Then,the relation_rule((S1, R1, S2), (T1, R2, T2)) means thatgiven S1 and S2 linked through the relation R1, andgiven the entity_rule(S1, T1) and the entity_rule(S2,T2), then for each instance of S1 and S2 there willbe individuals of T1 and T2 respectively that will belinked through R2. For example, a relation rule inFig. 2 would link the hierarchical relation betweenspecies and gene in the XML Schema and the objectproperty ro:in_taxon in the ontology.Ontology transformation patternsThe previous basic rules do not support all the types oftransformations needed in order to get semantically-richdatasets, because sometimes we need (1) to define rulesthat involvemultiple input entities and one ormany ontol-ogy classes, or (2) to add additional information to enrichthe input data. Consequently, more complex transforma-tions are needed. For this purpose we have adopted theontology pattern approach. Our ontology transformationpatterns represent a partial or complete semantic descrip-tion of a class of the ontology. Patterns are intermedi-ary entities among the input schema and the ontologyfrom the perspective of the definition of mappings. Ourpatterns are templates designed by using OWL ontologyclasses, datatype properties, object properties and con-straints. Such patterns have variables which are bound tothe corresponding entities, attributes or relations.A pattern can be defined as the tuple < S, V >, whereS stands for the set of classes, datatype properties, objectproperties and individuals used in the pattern that are asubset of those defined in the OWL ontology, and V is theset of variables associated with the instances of classes orthe values of properties in S. A pattern is instantiated bylinking the variables with entities of the input schema, andcan be used for creating new content in the OWL ontol-ogy. A pattern allows creating several new individuals,giving value to datatype properties and linking individu-als through object properties. Moreover, a pattern can bereused several times acting as a template. A pattern alsoallows for specifying fixed content that does not dependon the input dataset or that cannot be obtained from it, socontributing to the semantic enrichment of the content.Figure 3 shows an example of mapping between anXML Schema (left) that represents information aboutmolecules and a molecule ontology (right). The ontol-ogy classes Molecule, Atom and Bond have a directmapping with elements of the XML Schema but, forinstance, the ontology does not have a class for represent-ing chiral molecules. A chiral molecule can be definedas a molecule with the chemical property of chirality. InOWL, such definition can be represented as Moleculeand has_chemical_property some Chirality. In the inputschema, chirality is represented by the element propertywith attribute name isChiral and whose value is repre-sented in the element val, whose value is 0 or 1. Thepattern shown in Table 1, which is expressed in OPPL217,defines the variable ?chiralMolecule and such rule definesthe axioms to be generated. The data instances with value1 for the isChiral attribute (not shown in the table) are theinput for this pattern.We could have used basic mapping rules for defining theentity_rule(molecule, Molecule, on_condition(property(@name = isChiral)/val, 1))), which links moleculeswith value 1 for the isChiral property with the OWLClass Molecule. The OWL instances would be incom-plete, because they would not contain information aboutchirality. That would make the input instances and theontology individuals not congruent. The use of the patternallows defining the mapping with the variable ?chiral-Molecule, and the additional, fixed information pro-vided by the pattern permits to satisfy the congruencerelation.Identity rulesIdentity rules define the set of datatype properties andobject properties that permit to distinguish each individ-ual in the ontology. These rules are useful to prevent theLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 8 of 17Fig. 3 Description of the mapping between a XML schema and an ontology in the domain of moleculescreation of redundant content and to support the inte-gration of data from multiple sources, since identity rulespermit to identify which entities with different URI, fromthe same or different datasets, represent the same entity.Let IR be the set of datatype properties and object prop-erties of the ontology that univocally defines the identityfor the class C. The identity_rule(C, IR) means that allthe individuals of C with the same value for the elementsin IR are considered the same. We can define an iden-tity rule for the class Gene using the datatype propertyIdentifier and the object property ? Gene, ro:in_taxon,ncbi:organisms ?. This identity rule means that two indi-viduals of the class Gene (see Fig. 2), associated with thesame individual of the class ncbi:organisms through theobject property ro:in_taxon, and with the same value forthe datatype property Identifier are the same individual.The execution of the transformation processThe method runs the mapping rules in the followingspecific order: The basic entity rules are retrieved and executed. Asa result of this step, a set of new individuals of eachclass of the ontology, written I, is generated. The group of patterns represents a special case, sincethey may generate new individuals (not obtained inthe previous step) and may also add content to newTable 1 Definition of the pattern for ChiralMolecule?chiralMolecule: INDIVIDUALBEGINADD ?chiralMolecule instanceOf (Molecule andhas_chemical_properties some Chirality)END;generated ones. Therefore, the statement of the pat-terns that create new individuals are executed andthose new individuals are added to I. The identifica-tion of which statements of the patterns generate newindividuals is done by checking their definition. For each instance of the set I the process continues asfollow: The rest of statements of the patterns are exe-cuted to add any additional content to the individ-uals. The basic attribute rules are retrieved and exe-cuted, so the values of the datatype properties ofthe individuals are assigned. The basic relations rules are retrieved and exe-cuted, so the object properties are instantiated. The identity rules are checked and, in case theinstance is unique, it is added to the outputdataset. Otherwise it is merged or linked tothe equivalent one, depending on the behaviourdefined for the rule.Data integrationThe integration approachOur approach for the integration of heterogeneousresources is based on the application of the transforma-tion approach described above to the different resources,using the same OWL ontology as driver of the process.The construction of the integrated content requires map-ping the schemas to the OWL ontology. The OWL ontol-ogy may have a series of ontology transformation patternsassociated, which support the integration process. Theuse of patterns facilitates (1) reusing the transformationrules with different resources, and (2) overcoming thestructural heterogeneity of input data schemas. Table 2shows the pattern that defines a protein in the OWLLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 9 of 17Table 2 Definition of the pattern for protein?protein: INDIVIDUAL?cds: INDIVIDUAL?transcript: INDIVIDUALBEGINADD ?protein instanceOf Polypeptide,ADD ?protein derives_from ?cds,ADD ?cds instanceOf CDS,ADD ?cds part_of ?transcript,ADD ?transcript instanceOf TranscriptEND;ontology used in one of our use cases. This pattern notonly avoids the user the need for knowing the struc-ture of the ontology but also can be applied with minormodifications to data resources which store the relationprotein-cds-transcript in different ways, or might evennot be defined in the input schema. Table 3 shows howparametrizing the variable ?cds from the variable ?protein,the pattern can be applied to data resources with a directprotein-transcript relation without cds.The integration processThe integration of data is carried out through the trans-formation of each input resource. The mapping rulesenable to generate OWL content, and the identity rules areapplied during the transformation process to limit redun-dancy and tomerge data instances. Their role is to identifywhich instances from different resources correspond tothe same domain instance. Obviously, individuals with thesame URI are considered the same one.The integration method makes the following decisionsconcerning the usual problems in integration processes: Naming conflicts: Different input schemas may usedifferent terms for the same element (i.e., entity,Table 3 Definition of the pattern for protein with minormodification for resources without CDS content?protein: INDIVIDUAL?cds: INDIVIDUAL = create(?protein.RENDERING+_CDS)?transcript: INDIVIDUALBEGINADD ?protein instanceOf Polypeptide,ADD ?protein derives_from ?cds,ADD ?cds instanceOf CDS,ADD ?cds part_of ?transcript,ADD ?transcript instanceOf TranscriptEND;attribute, relation). The output OWL ontology pro-vides the common vocabulary for the integratedrepository, so the mappings from the differentresources to the OWL ontology solve this problem. Data redundancy: More than one instance of theinput resource may describe the same domain entity,so they are mapped to the same class in the OWLontology. Identity rules permit to detect such situa-tions and to merge or link the corresponding OWLdata to minimise redundancy. Inconsistency due to incomplete data: The inputschema may store less attributes and relations for agiven entity than the OWL ontology. This may leadto an inconsistent OWL knowledge base in case theidentity rules cannot be checked. When such situa-tion is detected, the corresponding source data arenot transformed, so inconsistencies are prevented.Patterns providing values for missing data to suchidentity properties could be used to overcome thissituation. Differences between the resources: It may happenthat an OWL individual is created by using differentinstances of the data resources, which may have dif-ferent values for common attributes or relations. Thismay be an issue for properties associated with theidentity rules. In such case, they are considered differ-ent individuals, which are created unless they wouldmake the knowledge base inconsistent.ResultsIn this section we describe the main results of our work.First, we will describe the tool that implements the trans-formation approach. Second, wewill describe how the toolhas been used in different biomedical scenarios.The SWIT toolThe transformation approach has been implemented ina web tool called SWIT18. SWIT provides a web inter-face through which the user is guided to perform all thesteps of the process. SWIT is currently supportingMySQLdatabases, XML schemas and ADL archetypes as inputschemas. SWIT permits to generate the output datasetin OWL or RDF formats, which can be downloaded ordirectly stored in Virtuoso [49] or in a Jena knowledgebase19.The user can define the mappings between the inputschema and the OWL ontology. For this purpose, map-pings created in other transformation processes can beuploaded and reused. Once the mappings have beendefined, they can be executed, thus generating the cor-responding RDF/OWL content. SWIT applies the map-ping rules to the data source to generate the semanticcontent, checking the identity rules to guarantee thatredundant individuals are not created. This process alsoLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 10 of 17uses automated reasoning to ensure that only logicallyconsistent content is transformed. SWIT uses both theOWLAPI [57] and the Jena API for processing and gener-ating the RDF/OWL content, Hermit [32] as reasoner, andthe patterns are implemented using OPPL2.Figure 4 shows a part of the mapping interface, whichhas three main parts. The left side shows the input schemausing a hierarchical representation. The right side cor-responds to the OWL ontology. The lower part of thefigure is a text box, which contains the mapping rulesdefined. For example, the third line defines the mapping ofthe attribute coorddimension of the entity molecule to thedatatype property coord_dimension of the ontology classMolecule.Figure 5 is a screen snapshot of the definition of themapping of entities of the input schema to a transfor-mation pattern. In this case the input schema consistson openEHR archetypes (left), which are mapped ontoan ontology transformation pattern for histopathologyreports. In the figure, we can see that the mapping wouldassociate a particular element of the archetypes with eachvariable of the pattern. In this case, the expression corre-sponding to the mapping rule is not shown in the figure.Data transformation use casesIn this section we explain how we have used SWIT inthree biomedical domains. Our website contains moreinformation about these efforts, including the mappingfiles and examples of content generated.Orthology dataWe have used SWIT to generate the integrated OGOLODLinked Open Dataset [58]. The first version of OGOLODwas created with the purpose of providing an integratedresource of information about genetic human diseasesand orthologous genes, given the increasing interest inorthologs in research [59]. OGOLOD integrates informa-tion from orthology databases such as Inparanoid [60]and OrthoMCL [61], with the OMIM database [62]. Thecontent of OGOLOD was generated using a method totransform the content of relational databases into anRDF repository. The OGOLOD repository uses the OGOontology [56] as a scaffold to link the integrated informa-tion about orthologs and diseases.SWIT is currently being used to support the standardi-sation of orthology content20 [63] promoted by the Questfor Orthologs consortium21. For this purpose, OrthoXML[26] is the input schema. OrthoXML defines a standard-ised XML schema that provides the elements to describean orthology relationship in a uniform way, and this for-mat is being used by a number of orthology databases.We have defined and executed the corresponding map-ping rules between OrthoXML format and the OrthologyOntology (ORTH) using SWIT. So far, this has permittedFig. 4Mapping interface of SWIT: (left) part of an XML schema about molecules; (right) part of the classes and properties of domain ontology;(bottom) excerpt of the mappings defined between the XML schema and the ontologyLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 11 of 17Fig. 5 Example of pattern mapping in SWITto generate an integrated dataset containing more than 2billion triples.EHR dataSWIT was the tool used for transforming EHR data ofcolorectal cancer patients in the study described in [24].The study was performed with real anonymised data fromthe Colorectal Cancer Program of the Region of Murciaand included data from more than two thousand patients.This work required to transform data from a proprietaryformat to a semantic format in order to apply colorec-tal cancer protocols to the patient data. Such protocolswere applied using automatic reasoning over the seman-tic content to determine the level of risk of each patient.SWIT was the tool employed in the processing and trans-formation of the EHR data once transformed from theproprietary format into openEHR XML extracts. In thiscase, those extracts and archetypes were the source dataand input schema for SWIT. The domain ontology usedwas developed in the context of this study.Figure 5 is an example of transformation pattern appliedin this research study, whose implementation in OPPL2 isshown in Table 4. This pattern defines a histopathologyreport according to the domain ontology, which containsa set of findings (hasFinding), records the total numberof adenomas found (number) and the size of its biggestadenoma (maxsize).Chemical compoundsA third use case is currently in progress, although thegeneration of the semantic dataset has been completed.The objective of this effort is to use semantics to improvecompound selection for virtual screening. Virtual screen-ing methods use libraries of small molecules to findthe most promising structures that could bind withdrug targets. One of such libraries is ZINC [64], a freedatabase of commercially-available compounds for vir-tual screening. ZINC data can be downloaded in XMLformat. In this effort, we created the XML Schema anddefined the mappings with an ontology developed by ourgroup.DiscussionThe availability of biomedical datasets in open, semanticformats would facilitate the interoperability of biomedi-cal data and would enable to carry out scientific studiesTable 4 Definition of the pattern for histopathology reports?histopathologyReport:INDIVIDUAL,?finding:INDIVIDUAL,?size:CONSTANT,?number:CONSTANTBEGINADD ?histopathologyReport instanceOf HistopathologyReport,ADD ?histopathologyReport hasFinding ?finding,ADD ?histopathologyReport number ?number,ADD ?histopathologyReport maxsize ?sizeEND;Legaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 12 of 17with larger, connected datasets. In this paper we have pre-sented a solution based on the transformation and inte-gration of heterogeneous data resources in which ontolo-gies play a central role. A series of important aspects ofour work are discussed next.The transformation and integration approachOur transformation process follows a data warehouseapproach instead of an OBDA one because of the fol-lowing reasons. First, we believe that the availability ofRDF/OWL resources is the most natural way of develop-ing the LOD. In our opinion, efforts like Bio2RDF or theEBI RDF platform are correct ways of proceeding for thepractical exploitation of biomedical data and the develop-ment of the Semantic Web. Second, we are interested ingenerating OWL knowledge bases over which OWL2 DLreasoning can be applied, which is not ensured by cur-rent OBDA approaches. That would be a limitation fromour exploitation point of view. We aim to obtain datasetslinked to external sources, which is also easier to achievewith our approach. Third, to the best of our knowledge,current OBDA approaches do not facilitate the applicationof ontology patterns as we do in this work, which also per-mits a semantically-richer representation and exploitationof data.Most state-of-the-art transformation approaches andtools from XML or relational databases into RDF/OWLare based on canonical transformations or are not basedon mappings with domain knowledge (i.e., ontologies).Such tools mainly perform a syntactic transformation ofthe traditional formats, making the semantic interoper-ability of the obtained datasets difficult. Besides, thereare no methods that can be applied to both XML andrelational databases. Ourmethod provides a semantic rep-resentation of the input datasets by performing a trans-formation guided by domain knowledge using an OWLontology, so performing an ontology-driven ETL pro-cess. This is similar to RDB2OWL [43] and Karma [44].SWIT and RDB2OWLhave in common that themappingsbetween the input model and the ontology are manuallydefined, but RDB2OWL is limited to input datasets inrelational format and does not provide any solution for theproblem of complexity on the manual definition of map-pings when using complex ontologies or data integration.Karma has the advantage of performing semi-automaticmapping of input databases and ontologies. However, thismapping process depends on the existence of a knowledgebase of previous mappings.We follow a data warehouse-oriented integrationmethod, although our approach has features associatedwith the integration based on links, because our map-ping rules permit to define links to external datasets. Thisarchitecture is similar to the one applied in Bio2RDF, withthe difference that our repositories may contain data frommultiple sources. Although that could also be possible inthe Bio2RDF effort, it is focused on transforming singledatasets. In fact, we believe that an effort such as Bio2RDFcould benefit from our approach. Currently, one transfor-mation script has to be written to include a new dataset inBio2RDF. SWIT would reduce the implementation effortfor relational or XML sources in the sense that only thedefinition of the mappings would be needed, since SWITwould execute the data transformation. Besides, SWITmappings could be reused for new datasets. Using SWITwould have the cost of making explicit the mappings withan OWL ontology, but it would also provide benefits interms of consistency checking and homogeneity in boththe richness of the semantic description and the structureof the data.Next, some additional aspects concerning the differentsubprocesses are discussed.MappingData transformation and integration are based on thedefinition of mappings between the data schema andthe OWL ontology. The difficulty and the complexityof mapping not only relies on finding the correspond-ing entities in the domain ontologies but also on beingable to design the corresponding ontology content pat-terns. Once the rules and patterns are designed, SWITreduces the implementation effort by executing themand generating the corresponding semantic content. Pat-terns are also used in Populous [65], which is focusedon creating ontologies from spreadsheets. Our experiencereveals that semi-automatic mapping techniques wouldcontribute to significantly reduce the mapping time, soefforts in this area are key to support the mappingprocess.To the best of our knowledge, there is no standard lan-guage to define mappings from different types of inputmodels to OWL ontologies. We are currently using alanguage based on the former ontology alignment for-mat22, which has evolved into EDOAL23. The W3C hasdeveloped the R2RML mapping language24 for mappingrelational databases to RDF, but does not cover XML. Ourcurrent language permits to express mappings from rela-tional databases and XML schemas to OWL ontologies,and it could be easily extended to cover new types ofinputmodels (i.e., OWL ontologies). Ourmappings can bereused, especially for data transformation processes thatuse the same OWL ontology, but the lack of standardisa-tion in this area forces third parties to do some additionalwork in order to include the mappings generated withSWIT.In this paper, we have used the mapping rules for cre-ating OWL individuals from XML or relational data, butthe process can also be applied for the creation of ontol-ogy classes. This might be helpful in case the content ofLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 13 of 17the OWL ontology used is not sufficient for mapping theinput schema. For this purpose, the mapping rules wereextended to produce OWL classes instead of individuals.This class-based approach also permits to use patterns.The only difference is that the set of variables associatedwith the pattern are bound to entities instead of instances.We have actually applied such approach for the generationof openEHR archetypes from CEM clinical models [22].In that study, the input schemas were OWL ontologiescorresponding to CEM clinical models and the openEHROWL ontology was the output schema. Basically, the cre-ation of the openEHR clinical models was approached asextending the openEHR OWL ontology with the specificcontent of the clinical model. In OWL, being an individualor a class can be seen as the role played by a given con-cept [66]. The representation of knowledge may thereforebe based on individuals or classes, this decision dependingon the expected use of such knowledge. In fact, punningwas included in OWL 2 DL to enable different uses ofthe same term, so an individual and a class can have thesame URI. From a formal ontology perspective, enablingthis possibility might be reason enough for a criticism toour approach, but it is needed from a practical perspec-tive. This situation can be exemplified in the orthology usecase. Orthology databases basically contain informationabout genes, proteins and organisms. In the database theyare represented as individuals, but they semantically cor-respond to classes, since there are many instances of eachgene, protein and organism.TransformationThe transformation method checks all the formal aspectsthat guarantee the generation of consistent content, inde-pendently of the use case and the intended exploitation ofthe data. The logical consistency of the content is guaran-teed by the application of OWL reasoning. Consistency isgranted independently of the output format, that is, RDFor OWL, because OWL DL semantics is applied duringthe transformation process. The individuals are expressedin RDF or OWL at the end of the process. In case themethods find that inconsistent content is going to begenerated, such content is automatically discarded. UsingOWL for ensuring the consistency of the data generatedin ETL processes has also been done in other works, suchas [42].Our experience in semantic data transformation in thelast years reveals that the semantic representation of thedata sometimes needs additional content that is not madeexplicit in the XML schema or in the corresponding table,so the additional meaning is not provided by the canonicaltransformation methods. We believe that such additionalmeaning can be included during the ETL process. Inthis context, our patterns are equivalent to ontology con-tent patterns [67], which are focused on the definitionof templates for the semantically-rich, precise descriptionof the meaning of the content. We believe that ontol-ogy content patterns are a solution for those situationsin which the source data does not contain all the infor-mation needed to generate semantically-rich RDF/OWLcontent.The computational complexity of the full methoddepends on the number of individuals and the meannumber of properties and relations per individual. As aconsequence, for medium and large datasets, the transfor-mation time may be longer than expected because of thenumber of instances of axioms to be generated. Thismightnot be a problem in case of stable datasets or batchedtransformation processes. However, according to ourexperience with SWIT datasets, the intended exploitationof the datasetmight permit to relax some conditions of thetransformation process. In case of not performing inte-gration processes, identity rules are only needed if, forinstance, two entities from the input are mapped onto thesame ontology class. In case of not requiring automatedreasoning on the transformed dataset, the generation ofsome types of axioms might be omitted, saving time andspace. owl:differentFrom axioms are an example of a timeand space consuming type of axiom, but they might beskipped in some cases. The lesson learned here is that theoptimal configuration of transformation depends on theuse case, so the flexibility of the process is basic for get-ting the desired semantic dataset. All these aspects can beconsidered adjustable parameters for the execution of theprocess using tools like SWIT.IntegrationThe integrationmethod is useful for scenarios that requirethe creation of a repository that includes portions of datafrom different resources. In case of wishing a link-basedintegration, the mechanism offered by SWIT to includelinks in the mapping rules could be sufficient. The keyobjectives of the integration method are (1) detectingequivalent data instances to reduce redundancy and (2)ensuring the consistency of the resulting repository. Bothtasks are supported by OWL reasoning.Identity rules are fundamental in the integration pro-cess, because they control the redundancy of the indi-viduals created. They describe which properties permitidentifying an individual of a certain ontology class. Forexample, we could integrate two resources about proteinswhich use different identifiers for the proteins, and thoseidentifiers are used in the URI of the individual. Thoseresources might be using the Human Genome nomencla-ture for naming the gene associated with the protein. If thegene name is used in the identity rule, then SWIT wouldfind that both individuals refer to the same protein.In addition to this, the meaning of our identity rulesis similar to the identity criteria proposed by formalLegaz-García et al. Journal of Biomedical Semantics  (2016) 7:32 Page 14 of 17ontologists [68], because they determine which conditionsare sufficient for identity. The properties used in iden-tity rules are those that would be included in OWL Keyaxioms25. Key axioms associate a set of object propertiesand datatype properties with a class, so each individ-ual is identified by the values of such set of properties.Hence, when two individuals have the same values forsuch properties, they are considered the same individual.Key axioms are only applied over those individuals withasserted membership to a class. Such inferencing-relatedlimitation made us to define our identity rules.InteroperabilityNext, we discuss how and to what extent SWIT promotesor facilitates the interoperability of the datasets. Let usconsider a resource about proteins, which uses a local URIfor each protein, but also stores the UniProt AccessionNumbers (AC). Let us suppose that we want to link everyprotein to the corresponding URI in UniProt. It shouldbe noted that the UniProt URI for each protein differsin the AC. For example, the URI for the protein P63284is http://purl.uniprot.org/uniprot/P63284. SWIT providestwo different ways for creating such link:1. Redefinition of the URI. We can use the UniProt URIinstead of the dataset URI, since SWIT permits todefine which prefix has to be used in the transforma-tion of each entity.2. Linkage of resources with owl:sameAs. The transfor-mation of the protein uses the dataset URI but createsan owl:sameAs link to the UniProt URI.Either action can be applied to transform the data withRESEARCH Open AccessMicrO: an ontology of phenotypic andmetabolic characters, assays, and culturemedia found in prokaryotic taxonomicdescriptionsCarrine E. Blank1*, Hong Cui2, Lisa R. Moore3 and Ramona L. Walls4AbstractBackground: MicrO is an ontology of microbiological terms, including prokaryotic qualities and processes, materialentities (such as cell components), chemical entities (such as microbiological culture media and mediumingredients), and assays. The ontology was built to support the ongoing development of a natural languageprocessing algorithm, MicroPIE (or, Microbial Phenomics Information Extractor). During the MicroPIE designprocess, we realized there was a need for a prokaryotic ontology which would capture the evolutionarydiversity of phenotypes and metabolic processes across the tree of life, capture the diversity of synonyms andinformation contained in the taxonomic literature, and relate microbiological entities and processes to termsin a large number of other ontologies, most particularly the Gene Ontology (GO), the Phenotypic QualityOntology (PATO), and the Chemical Entities of Biological Interest (ChEBI). We thus constructed MicrO to berich in logical axioms and synonyms gathered from the taxonomic literature.Results: MicrO currently has ~14550 classes (~2550 of which are new, the remainder being microbiologically-relevantclasses imported from other ontologies), connected by ~24,130 logical axioms (5,446 of which are new), andis available at (http://purl.obolibrary.org/obo/MicrO.owl) and on the project website at https://github.com/carrineblank/MicrO. MicrO has been integrated into the OBO Foundry Library (http://www.obofoundry.org/ontology/micro.html), sothat other ontologies can borrow and re-use classes. Term requests and user feedback can be made using MicrOsIssue Tracker in GitHub. We designed MicrO such that it can support the ongoing and future development ofalgorithms that can leverage the controlled vocabulary and logical inference power provided by the ontology.Conclusions: By connecting microbial classes with large numbers of chemical entities, material entities, biologicalprocesses, molecular functions, and qualities using a dense array of logical axioms, we intend MicrO to be a powerfulnew tool to increase the computing power of bioinformatics tools such as the automated text mining of prokaryotictaxonomic descriptions using natural language processing. We also intend MicrO to support the development of newbioinformatics tools that aim to develop new connections between microbial phenotypes and genotypes (i.e., thegene content in genomes). Future ontology development will include incorporation of pathogenic phenotypes andprokaryotic habitats.Keywords: Prokaryotes, Microbes, Ontology, ChEBI, Gene Ontology, Metabolic characters, Natural language processing,Prokaryotic taxonomy, Bacteria, Archaea, Microbial bioinformatics* Correspondence: carrine.blank@umontana.edu1Department of Geosciences, University of Montana, Missoula, MT 59812,USAFull list of author information is available at the end of the article© 2016 Blank et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Blank et al. Journal of Biomedical Semantics  (2016) 7:18 DOI 10.1186/s13326-016-0060-6BackgroundMicroorganisms comprise most of the evolutionary andgenetic diversity in the tree of life [13], and produce asignificant proportion of the standing crop of cellularcarbon on the Earth [4, 5]. Prokaryotic microorganismsmanifest their diversity in the form of morphologicalphenotypes (such as biofilm formation, multicellularity,and differentiation into specialized structures), ecologicalphenotypes (inhabiting environments that have particu-lar temperature, salinity, and pH values), metabolicphenotypes (the ability to catalyze discrete chemicalreactions), and the ability to perform biological pro-cesses (carrying out photosynthesis) [6]. Several studieshave examined the evolution of microbial phenotypictraits in deep time [713]. Nevertheless, most of thesestudies have focused on relatively small taxonomic groups,or have used a small number of phenotypic traits. This isbecause the taxon-by-character matrices (which recordthe presence and absence of traits for each taxon) requiredfor these studies have been constructed manually and thusrequire significant efforts to build. Hence, the field needsto develop tools that can allow the accelerated, broad-scale study of the evolution of phenotypic traits across theprokaryotic domains of life.Bioinformatics resources that are needed to acceleratesuch evolutionary studies include tools that permit therapid processing of large amounts of legacy text anddatabases (which contains detailed information onphenotypes and metadata) as well as tools that facili-tate the rapid processing of genotypic data (genomicsequences). Such tools could lead to new profoundinsights in broad-scale microbial evolution, as well aslead to new mechanisms for genome annotation (byassociating novel phenotypes with genotypes). To ad-dress some of these needs, our team has developedan ontology to assist development of a new naturallanguage processing (NLP) algorithm, MicroPIE (orMicrobial Phenomics Information Extractor; https://github.com/biosemantics/micropie2) [14]. MicroPIE isdesigned to automatically extract text from prokary-otic taxonomic descriptions and to export a charactermatrix. The character matrix can then be used tostudy the evolution of traits using phylogenetic com-parative methods. Most prokaryotic taxonomic de-scriptions are published in the International Journalof Systematic and Evolutionary Microbiology (IJSEM)and follow a semi-formalized structure. However, thisstructure has changed over time, and the contentwithin descriptions (types of reported or assayedphenotypic characters, as well as naming conventionsfor chemical entities) has also changed. Some taxo-nomic descriptions (such as for the Cyanobacteria)are usually published outside the IJSEM and have his-torically followed the botanical code [15, 16], thusthey often have different information content. Also,during the development of MicroPIE, we observedthat different authors can have markedly differentways of naming or describing synonymous prokaryoticstructures and processes (for example they might de-scribe the morphology of rods as an elongated cocci,short cylinders, or bacilli), making NLP treatment oftext from taxonomic descriptions challenging.Presently, some of the text extraction algorithmswithin MicroPIE use a list of terms that includes all thesynonyms we have found in a sampling of the prokary-otic taxonomic literature. However, the term lists treatall synonyms as distinct terms. Also, in prokaryotic taxo-nomic descriptions there is variability in how commontraits are described. For example, we have observed thatauthors report a positive result of the indole assay asindole test positive, indole-positive, indole pro-duction, indole reaction is positive, indole formed,or tryptophanase produced. While a domain-expertwould immediately recognize that these are all syn-onymous, a computer or non-domain expert may not.Finally, NLP supported by term lists lacks inferencepower. For instance, NLP cannot infer that an organ-ism with an optimal growth temperature of 60 °C is athermophile.Through the MicroPIE development process, it be-came evident that the field needed a robust ontology.While an early version of the Ontology of MicrobialPhenotypes (OMP) was available [17, 18], it was focusedon E. coli phenotypes and had a structure that did notreadily lend itself to term re-use. Thus, we created anew ontology, MicrO, which suited our projects needsand that could be usable by the ontology community atlarge. This ontology captures much of the evolutionarydiversity of prokaryotic traits and processes and the richlegacy of material entity, quality, and assay terms thatencompasses the vast diversity found throughout theprokaryotic taxonomic literature. We also designed theontology to use as a controlled vocabulary that linkedthe diversity of synonyms found in the literature to cen-tral terms that will help support text mining algorithmssuch as MicroPIE. The ontology leverages logical infer-ence power (for example, to predict that an aerobicmicroorganism that metabolizes glucose is both a che-moorganotroph and uses oxygen as a terminal electronacceptor) to help populate character matrices and toinfer higher-order character states that are not explicitlystated in taxonomic descriptions. Finally, MicrO relatesmicrobiological entities and processes to entities andprocesses in a large number of other ontologies, includ-ing the Gene Ontology (GO), the Phenotypic QualityOntology (PATO), and the Chemical Entities of BiologicalInterest (ChEBI) [1921]. The relationship of classes inMicrO to classes in other ontologies is formalized in aBlank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 2 of 10logical, structured, computable way such that MicrOwill be able to support future advances in microbialbioinformatics, for example in the automated extractionof text using NLP and integrating microbial charactersfrom different databases or repositories. We anticipatethe ontology will provide an important new tool for fa-cilitating the incorporation of massive amount of textdescriptions into future generations of biological ana-lysis and computational tools.MethodsFor the development of MicrO, we took a hybrid top-down and bottom-up approach. For the top-down ap-proach, we used established ontology developmentprinciples and practices, such as the use of an upperontology. In following a bottom-up approach, we usedthe principles of literary and user warrants [22] andattempted to make the ontology capture the vast di-versity of phenotypic character information reportedin the prokaryotic taxonomic literature.Top-down ontology developmentThe ontology was constructed using Protege OWL(Web Ontology Language; version 4.3) [23]. It is builtupon a Basic Formal Ontology (BFO) foundation, andfollowed OBO Foundry principles [24, 25]. During theearly developmental stages of MicroPIE and MicrO, wecreated extensive term listsmanually generated lists ofterms and synonyms (including variations on spelling)from a large corpus (~1,500) of diverse prokaryotic taxo-nomic descriptions obtained from the primary scientificliterature. We focused on taxonomic descriptions fromthe Archaea, Cyanobacteria, Mollicutes, Bacteroidetes,and Firmicutes. In this way, we sampled characters fromextremophilic chemotrophs, Cyanobacteria (which oftenhave very different taxonomic descriptions and morpho-logical traits), as well as a rich diversity of heterotrophicand chemotrophic, non-pathogenic and pathogenic,species found in the Mollicutes, Bacteroidetes, andFirmicutes. Most non-cyanobacterial descriptions wereobtained from the IJSEM, while most cyanobacterialdescriptions were sampled from AlgaeBase (an onlinedatabase of taxonomic descriptions from cyanobac-teria and algae) [26].The term lists were organized hierarchically usingcategories and subcategories, and this organizationalstructure is currently used to support MicroPIE. Ex-amples of categories include Colony Morphology, CellShape, Metabolic Substrates, Growth Conditions, andAntibiotic Physiology. Each category has a variednumber of subcategories, for example, the categoryColony Morphology included Colony Shape, ColonyTexture, and Colony Color as subcategories. Categor-ies and subcategories in the term lists were matchedto higher-level ontology classes and re-organized intocandidate qualities, processes, and material entities(including chemical entities and cellular components).These were then used to create the higher-level ontol-ogy classes in MicrO, which were then incorporatedinto the upper level BFO hierarchy.Bottom-up ontology developmentLower-level terms in the term list were manuallygrouped into candidate classes and synonyms in MicrosoftExcel. Terms synonymous to existing classes in ontologiesin the OBO Foundry were identified using OntoBee [27].These were imported into MicrO using OntoFox [28].Imported classesImported classes (Additional file 1: Table S1) wereused to provide higher-level classes for the nesting ofMicrO-specific classes, to represent microbiologicalconcepts present in other ontologies, and to constructlogical axioms for classes in MicrO. Eight classes inBFO were imported, to provide the top-level structure ofthe ontology. For many ontologies, a relatively small num-ber of lower-level classes were imported. These includedBSPO, CHMO, CL, DRON, IAO, NCBI Taxonomy, NDF-RT, OBI, PO, PR, REO, RO, and Uberon (respectively: theBiological Spatial Ontology, Chemical Methods Ontology,Cell Ontology, Drug Ontology, Information ArtifactOntology, NCBI Taxonomy, National Drug File ReferenceTerminology, Ontology for Biomedical Investigations,Plant Ontology, Protein Ontology, Reagent Ontology, Re-lations Ontology, and Uber Anatomy Ontology) [2937].For other ontologies (ChEBI, GO, PATO), a larger numberof higher- and lower-level classes were imported. This wasbecause these classes were used to construct the bulk ofthe logical axioms in MicrO. Classes from CL, ENVO,and IDO (the Cell Ontology, Environment Ontology, andInfectious Disease Ontology) [38, 39] were imported tohelp support the future construction of logical axioms asMicrO expands to incorporate new sets of classes (such aspathogenic phenotypes and microbial habitats). For IAOand RO, imported terms were nearly entirely object anddatatype properties. These were used to construct logicalaxioms, and also served as parent classes for new objectproperties in MicrO.Because much of microbial diversity lies in the meta-bolic transformation of chemicals, most of the importedclasses were from ChEBI (~6,450 classes). Imported clas-ses included various chemical substances (e.g., lecithin,bacitracin, collagen), roles (e.g., biological pigment, bio-marker, visual indicator, reducing agent), and largenumbers of inorganic chemicals, organic chemicals, andmixtures. In addition, we submitted term requests forseveral hundred microbial-specific compounds to ChEBI,including minerals, antibiotics, dyes/stains, lipids, cellBlank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 3 of 10wall constituents, and metabolic substrates and prod-ucts. These new chemical classes were then importedinto MicrO. Finally, a large number of synonyms wereadded to existing and new chemical classes in ChEBI.Because phenotypes (size, shape, relationships ofcells, cell parts, and colonies) are frequently present inprokaryotic taxonomic descriptions, a large number ofimported classes (1,580) came from PATO. Importedclasses included quality classes (such as morphology,size, shape, physical quality and their children),process quality classes, and increased and decreasedquality classes.Features of prokaryotic cells (e.g. vacuoles or flagella)as well as biological processes and enzymatic activitiesare common in prokaryotic descriptions. Hence, manyclasses (632) were imported from GO. These includedclasses involved in prokaryotic cell parts (e.g., cell hair,pilus, periplasmic flagellum), biological processes (e.g.,photorespiration), enzymatic activities (e.g., metalloen-dopeptidase activity), and biological responses to variouschemicals (e.g. response to bile acid).One hundred and fifteen classes were imported fromUberon. These included classes associated with anatom-ical structures and organism substances, which can serveas disease targets for pathogenic microorganisms and asmaterial that is processed to generate chemical entities(e.g., 'brain heart infusion') used in the cultivation ofmicroorganisms.A handful of classes (22) were imported from OBI,including assay, various entities involved in microbio-logical assays such as test tube, microscope slide, micro-scope, culture medium and associated entities such ascultured cell population and cultured clonal cell popu-lation. One hundred and five classes were importedfrom CHMO, and included classes such as evaporation,grinding, autoclaving, and sample heating. These wereused to construct axioms involved in microbiologicalmedium ingredients. Imported classes from BSPO (83)included anatomical margin, anatomical region, andanatomical side and their respective children, to sup-port creation of logical axioms relating to the spatial re-lationships of differentiated prokaryotic structures.Classes from CL (284) included native cell, prokaryoticcell, eukaryotic cell, and differentiated red and whiteblood cells (associated with pathogenic phenotypes andused in microbiological diagnostic assays). Seven classeswere imported from PO, these included fruit, seed,plant embryo. These classes were used in the construc-tion of logical axioms for microbiological medium ingre-dients for MicrO classes such as malt extract, soyaextract, soy peptone, olive oil, and filtered tomato juice.Over 500 classes were imported from NCBI Taxonomyto construct logical axioms for entities and qualities thatinhere to particular prokaryotic taxa, and to logicallyconnect culture medium recipes used to cultivate par-ticular prokaryotic taxa.A large number of classes relevant to microbiologicalhabitats and processes (1,962) were imported fromENVO. Although currently few of these classes are usedin logical axioms in the current version of MicrO, theirpresence will support the future development of MicrO(which will involve the incorporation of microbial habi-tats). Similarly, microbiologically relevant classes fromIDO (81 classes) were imported to support the future in-corporation of pathogenic phenotypes into MicrO.MicrO-specific classesIf no relevant classes in existing ontologies in the OBOFoundry Library could be identified, the candidate clas-ses were converted into ontology classes, and enteredinto MicrO. Some classes were derived from informationcontained in commercial and non-commercial websitesoutlining microbiological concepts (such as colonymorphologies, diagnostic assays, and culture mediumrecipes) or from scientific publications. In such cases,the definition source (website or publication) was cited.Each class also has a list of synonyms found in the cor-pus of taxonomic descriptions. Class synonyms were an-notated in the ontology as exact synonyms, broadsynonyms, or related synonyms using naming conven-tions developed by GO [40]. Classes under the parentimported class OBI:assay were created and structuredusing the conventions used by OBI. Compound classnaming followed the ANSI/NISO guidelines [22]. Finally,we made use of the HermiT 1.3.8 and the FaCT++ rea-soner in Protege to verify performance of logical axioms.AvailabilityMicrO is available in OWL format as a permanent URL[41] and from the project website [42]. MicrO has beenincorporated into the OBO Foundry Library so thatother ontologies can import classes and build upon it[43]. The contents of the ontology are available under aCC-BY license [44].Results and discussionOverview of ontology contentsMicrO (version 1.3, released on March 23, 2016) con-sists of ~2550 classes (plus thousands of synonyms) de-rived from text contained in the taxonomic descriptionsof diverse prokaryotic taxa that span the archaeal andbacterial domains of life. MicrO incorporates more than12,000 additional relevant terms from 19 other ontol-ogies in the OBO Foundry Library and these importedterms are connected to MicrO classes using a largenumber of logical axioms (over 24,130, with 5,446 spe-cific to MicrO). The largest categories of classes in theontology include assays (enzymatic, metabolic, andBlank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 4 of 10phenotypic assays), microbiological culture media andmedia ingredients, and prokaryotic qualities (includingcolony morphologies, shapes, and sizes). Other types ofclasses (such as those describing prokaryotic cell and cellparts) are scattered and nested within GO classes. Fi-nally, a handful of classes in MicrO are scattered invarious other parts of the ontology. The large-scalearchitecture of classes of material entities, processes,and qualities in MicrO, and how they nest in otherontologies, is shown in Additional file 1: Figures S1-S3.Prokaryotic chemical entitiesA large number of new chemical classes (>750) wereentered into ChEBI as a result of MicrO development.New ChEBI classes include minerals (including sulfideminerals), stains/dyes, metabolic substrates, lipids, inor-ganic chemicals, and antibiotics. In addition, requestswere made to add synonyms (188) to existing and newChEBI classes. Many microbiologically specific chem-ical mixtures, however, were retained under MicrO.These were categorized into defined inorganic chem-ical mixture (62 classes), undefined inorganic chemicalmixture (4 classes), defined organic chemical mixture(29 classes), and undefined organic chemical mixture(121 classes; Additional file 1: Figure S4). Examples ofdefined inorganic chemical mixtures include trace ele-ments solution SL-6 and modified MJ synthetic seawater. Examples of undefined inorganic chemical mix-tures, used as ingredients in microbiological culturemedia, include filtered aged seawater and sea salt. Ex-amples of defined organic chemical mixtures includeBalch vitamin solution, dried bovine hemoglobin, andhemin solution. Examples of undefined organic chem-ical mixtures include clarified rumen fluid, ox bilesalts, egg yolk oil, laked rabbit blood, and inspissatedserum. Additional classes were created for complexmixtures that were produced from hydrous, enzymatic,or chemical extraction of other material entities (e.g.,yeast extract, proteose peptone, casamino acids, crudeoil extract, and casein hydrolysate).Culture media recipesMicrobiological culture media recipes (~910 classes)were included, under the parent class OBI:culturemedium (Fig. 1). Annotations include the recipe, thecitation or web link to the recipe, and synonyms ofthe class. Logical axioms included the chemical ingre-dients used for each medium (connecting MicrOterms to ChEBI terms). Value Partitions were createdto categorize different types of culture media. For ex-ample, one Value Partition is related to the pH of themedium; whether it was strongly acidic (pH <4), mod-erately acidic (pH 45.5), slightly acidic (pH 5.56.5), nearneutral pH (pH 6.57.5), slightly alkaline (pH 7.58.5),moderately alkaline (pH 8.510.0), or strongly alkaline(pH >10.0). Another Value Partition related to the salinityof the medium using salinity values that are commonlyused in biology; whether it was freshwater (<0.05 % salts),brackish (0.053.0 %), marine (3.05.0 %), or hypersaline(> 5.0 %). A third Value Partition related to the redox (theoxidation-reduction potential) of the medium; whether itwas oxidizing (oxygen or air were present and not con-taining reducing agents), mildly reducing (containingorganosulfides or thiosulfate), or strongly reducing (con-taining cysteine, glutathione, 2-mercaptoethanol, dithio-threitol, sodium sulfide, hydrogen sulfide, dithionite, ortitanium citrate). Covering axioms were put in place foreach of the Value Partitions. The logical axioms that werecreated were designed to facilitate future studies that relyon the logical inference power of the ontology to gainhigher-order knowledge of microbial taxa based on thechemical composition of their growth media, such asstudies seeking to identify correlations between phylogenyand culture medium chemistry [45]. Finally, the logical ax-ioms put in place can help fill out the knowledge gap ofMicroPIE. For instance, taxonomic descriptions will oftenstate the type of media in which an organism is capable ofgrowing. The logical inference power made possible bythe ontology allows MicroPIE to immediately computethe chemical conditions under which that particular or-ganism is capable of growing (even if given only the namesof the culture medium).AssaysA large number of classes (~570) describe microbio-logical diagnostic assays, under the parent class OBI:as-say. Assays include cell staining assays, commercialsuites of diagnostic assays (e.g., API microbial ID testkits, Biolog, RapID, and VITEK), salinity, pH and redoxassays), a large number of organic carbon metabolismassays (including organic acid alkalinization assays, or-ganic carbon assimilation assays, organic carbon fermen-tation assays, and organic carbon fermentation/oxidationassays), milk reactivity assays, motility assays, hemad-sorption/hemagglutination/hemolysis assays, coagulaseassays, growth response assays (including growth re-sponse to various antibiotics, inorganic chemicals, andorganic chemicals), and finally a large number of specificenzymatic assays (e.g. beta-galactosidase assay, catalaseassay, lecithinase assay, pyruvate decarboxylase assay).Assays, with axioms connecting substrates, products,and enzymatic activities were important to have in theontology, because most prokaryotic taxonomic descrip-tions describe the outcomes of particular assays per-formed on the particular isolate being described andlogical axioms for this set of classes tended to be morecomplex. The assays are logically connected to chemicalentities (e.g. is an assay for the metabolic product someBlank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 5 of 10hydrogen sulfide and is an assay using the culturemedium some sulfide indole motility agar) and pro-cesses (e.g., is an assay for the biological process of some cell motility and is an assay for the enzymaticactivity of  some tryptophanase activity; Fig. 2 andAdditional file 1: Figure S5). Logical axioms also in-clude the enzymatic substrates (some of which arecolorimetric compounds, such as 5-bromo-4-chloro-3-indolyl beta-D-galactoside) and products, and theculture medium used to perform the test (e.g., is anassay using the culture medium some sulfide indolemotility agar).Sometimes, taxonomic descriptions will report lists ofenzymatic reactions that were tested and provided apositive or negative test result (e.g., positive for valinearylamidase), while other times they will report lists ofFig. 1 Screen Capture Showing Microbiological Culture Medium Recipes and Logical Axioms Employed. Logical axioms for these classes includedthe chemical ingredients used to make up the medium in addition to several Value Partitions that described the pH, salinity, and redox of theculture medium (for example: has salinity some brackish salinity)Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 6 of 10the substrates hydrolyzed or not hydrolyzed (e.g., L-valine-2-naphthylamide hydrolyzed). The structure ofthe ontology connects these two concepts and recog-nizes that they both relate to the same enzymatic trait(in this case, valine arylamidase activity, assayed usingthe L-valine arylamidase assay). This is accomplishedby including the assay substrates (in this case L-valine-2-naphthylamide) as a substrate in the logicalaxiom for the valine arylamidase assay class.Prokaryotic qualitiesSeveral classes (97) were created to describe prokary-otic qualities. These include prokaryotic cell partqualities (such as gas vacuole quality, thylakoid qual-ity, Gram stain quality, and prokaryotic cell wall lysissusceptibility), prokaryotic cell qualities (such as cellgranulation, cell pigmentation, cell size quality, andflagellar quality), and prokaryotic colony quality.Classes also included prokaryotic metabolic qualities(aerobic, microaerophilic, aerotolerant, obligately aerobic,photofermentative, chemolithoautotrophic, photoorgano-heterotrophic, etc.) and prokaryotic physiological qualities(including barophilic, obligately barophilic, barotolerant,and requires magnesium for growth).Prokaryotic cell and cellular componentsMany new classes (255) were placed under the parentprokaryotic cell including flagellated cell (with subclassesincluding multiply flagellated, amphilophotrichous cell,amphitrichous cell, lophotrichous cell, and peritrichouscell), gas vacuolated cell, granulated cell, nanocytes, andpigmented cell. Classes under morphologically distinctprokaryotic cell include bacilloid cell, cuboidal cell, pear-shaped cell, and prosthecate cell. Classes under prokary-otic differentiated cell include hormogonium, centralendospore, lateral endospore, subterminal endospore,basal heterocyte, and terminal heterocyte. Classes underprokaryotic metabolically differentiated cell include auto-troph, obligate aerobe, and chemoorganoheterotroph.Classes under prokaryotic physiologically differentiatedcell include acidophile, obligate barophile, thermophile,and facultative halophile. Classes under differentiatedcyanobacterial filament part include conical apical cell,tapered by apical narrowing, isopolar metameric, multi-seriate filament, and subterminal meristematic zones.Classes (49) were created to describe prokaryotic col-onies. The structural organization of classes relating tocolonies with distinct morphologies, sizes, and shapes,mirrored the class organization of morphology, size,and shape in PATO (Additional file 1: Figure S6). Thishelped to facilitate the construction of logical axioms be-tween classes in MicrO and PATO. For example, underthe parent class prokaryotic colony were placed theclasses morphologically distinct colony, physicallydistinct colony, and colony having distinct processquality. Morphologically distinct colony is logicallydefined as prokaryotic colony and has morphologysome PATO:morphology.Fig. 2 Pattern for Assay Classes. Schematic showing part of the logical design pattern for microbiological diagnostic assays (e.g. the class sulfideindole mobility assay). Multiple logical axioms connect various assay classes to other classes (such as microbiological culture medium) in otherparts of the ontology using object properties (shown with curved, bolded lines)Blank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 7 of 10MicrO classes of cell parts (~128 classes) includepseudopeptidoglycan-based cell wall, teichoic acid-based cell wall, sheath, and proteinaceous sheath.Additional prokaryotic cell parts include cyanobacter-ial filament part, filament branch, trichome, hetero-polar trichome, tapered trichome, isopolar trichome,trichome part, apical cell, basal heterocyte, medialcell, necritic cell, etc. Under cyanobacterial filament,classes include multi-trichomous filament, multiseri-ate filament, biseriate filament, and uniseriate filament.Our plan is to submit term requests for relevant classes ofcell parts that should belong in GO.Prokaryotic biological processesFinally, 41 classes were created that defined prokaryoticbiological processes (lithotrophy, mixotrophy, anaerobicrespiration using various electron acceptors and donors).These classes are embedded into GO classes, and maybe expanded upon and incorporated into GO in the fu-ture. Logical axioms connect these biological processeswith chemical entities (e.g. uses electron acceptor somenitrate, uses carbon source some organic molecular en-tity), other processes (e.g., has part some phototrophyand has part some heterotrophy), and biological en-tities (e.g., is prokaryotic metabolic process occurring insome mixotroph).Object and datatype propertiesIn order to connect classes in MicrO to those in externalontologies, we imported object properties from IAO,OBI, RO, and Uberon. We also created ~77 new objectand datatype properties to relate microbial-specific clas-ses to one another (Additional file 1: Table S2). Many ofthe new Object Properties are nested within OBI or ROparent classes. New object properties were assigned defi-nitions and (when possible) domains and ranges.Application and future directionsMicrobial diversity is vast. Our ontology did not focuson pathogenic phenotypes (such as hosts, target organs,and diseases). These are areas that will need furtherontology integration with other existing ontologies (forexample, with OMP, the Disease Ontology, InfectiousDisease Ontology, the Pathogenic Disease Ontology, andthe Human Disease Ontology) [4648]. MicrO also didnot focus on microbial habitats. Development of ENVOis ongoing and the incorporation of microbial habitatsinto ENVO is a potential fruitful new approach for inte-grating MicrO with ENVO. Also, there are a number ofnew prokaryote-focused ontologies in development fo-cusing on microbial metagenomic metadata and micro-bial habitats/environments (such as MEOWL; MicrobialEnvironments described using OWL; https://github.com/hurwitzlab/meowl). These can be incorporated intoMicrO and formal logical axiom linkages added to fur-ther increase axiomization of microbial terms. Finally,our ontology did not cover traits associated with micro-bial eukaryotes.In the near future, we plan to incorporate MicrO intoour developing NLP program (MicroPIE), and in doingso will greatly increase the computing power of Micro-PIE. Currently, MicroPIE relies on term lists, which treateach term as an individual entity. MicroPIE cannot de-termine that the terms rod, bacillus, bacilli, elongatedcocci, and short cylinders are all synonyms for the sameconcept (a bacillus shape). MicrO, with its controlled vo-cabulary, logical axioms, and annotations including syn-onyms, can inform NLP programs like MicroPIE thatthese are indeed the same class, and hence streamlinethe functionality of the algorithm. The ontology will helpMicroPIE recognize that terms such as mixotroph andmixotrophic all point to the same concept (the abilityto carry out process of mixotrophy). The ontology willalso reduce confusion in facilitating the identification ofsynonymous concepts when it comes to the variedreporting of the results of prokaryotic diagnostic assays(as discussed above).Because of the logical inference power provided by theontology, MicrO will allow algorithms like MicroPIE toinfer new information about a microbial taxon that isnot explicitly stated in the taxonomic description. Forexample, if an organism metabolizes glucose and isphotosynthetic, MicrO-enabled MicroPIE can infer thatit is a photoorganotroph. If an organism grows at 89 °C,MicrO-enabled MicroPIE can infer that it is a hyperther-mophile (given that the logical definition for a hyper-thermophile in MicrO constrains an organisms optimalgrowth temperature to being above 85 °C). If an organ-ism has akinetes, MicrO-enabled MicroPIE will be ableto infer that it is in the Nostocales or Stigonematales(two Orders in the Cyanobacteria). These inferred char-acter states can help to populate cells of a matrix thatcan be quite sparse when NLP is used to extract literalcharacters from text.Additionally, MicrO will be able to support a futuregeneration of bioinformatics capabilities for the micro-biological community. For example, because MicrO con-nects phenotypic information and diagnostic assays withthe enzymatic activities in GO, it could be used to sup-port future work aimed at connecting microbial pheno-types with genotypes (i.e., the gene content in genomes).Exciting new tools and approaches for connecting phe-notypes with genotypes are being developed for meta-zoans [4951]. These tools could be adapted andexpanded to similarly function with microbial taxa andmicrobial genomes in the future, given that the field ofmicrobiology now has a rich ontology. In this manner,MicrO could be a useful tool for other researchers in theBlank et al. Journal of Biomedical Semantics  (2016) 7:18 Page 8 of 10field of metagenomics and evolution of microbial pheno-typic traits.ConclusionsMicrO is an ontology of prokaryotic phenotypes andmetabolic characters, which also includes classes formicrobiological media recipes and diagnostic assays. Theontology uses a controlled vocabulary, detailed annota-tions, and an extensive set of logical axioms to connectprokaryotic classes (including qualities, processes, assays,and entities) to terms from 19 outside ontologies. Byconnecting microbial concepts with chemical entities,material entities, biological processes, molecular func-tions, and qualities from existing ontologies in the OBOFoundry using logical axioms, we intend MicrO to be apowerful new tool which will help push forward progresson the natural language processing of prokaryotictaxonomic descriptions, and make possible new con-nections between microbial phenotypes and genotypes(i.e. gene content in genomes). Future ontology devel-opment will include incorporation of pathogenic phe-notypes (such as hosts, target organs, and diseases)and prokaryotic habitats.Additional fileAdditional file 1: Supplemental Figures (Figures S1-S6) and Tables(Tables S1 and S2). (DOCX 1785 kb)AbbreviationsBFO: basic formal ontology; BSPO: biological spatial ontology;ChEBI: chemical entities of biological interest; CHMO: chemical methodsontology; CL: cell ontology; DRON: drug ontology; NDF-RT: national drug filereference terminology; ENVO: environment ontology; GO: gene ontology;IAO: information artifact ontology; IDO: infectious disease ontology;OBI: ontology for biomedical investigations; OMP: ontology of microbialphenotypes; MicroPIE: microbial phenomics information extractor; NLP: naturallanguage processing; OBO: open biomedical ontologies; OWL: web ontologylanguage; PATO: phenotype quality ontology; PO: plant ontology; PR: proteinontology; REO: reagent ontology; RO: relations ontology; Uberon: uber anatomyontology.Competing interestsThe authors declare that they have no competing interests.Authors contributionsCEB constructed the term lists and the ontology. LRM assisted with thecollection of the corpus of taxonomic descriptions and term/synonym lists.HC and RW provided significant input on the composition of terms and theconstruction of logical axioms. RW oversaw formatting of ontology importfiles and establishing purl addresses. All authors contributed to themanuscript. All authors read and approved the manuscript.AcknowledgementsThis work was funded by grants from the National Science Foundation(award DEB-1208534 to CEB, DEB-1208567 to HC, and DEB-1208685 to LRM)and by a travel grant (to CEB) to attend the 2013 NESCent Ontologiesfor Evolutionary Biology workshop. RW was supported by CyVerse andthe National Science Foundation under award numbers DBI-0735191 andDBI-1265383. Many thanks to Elvis Hsin-Hui Wu (University of Arizona),Gail Gasparich (Towson University), and Gordon Burleigh (University ofFlorida) for comments and/or assistance with ontology construction andcompilation of taxonomic descriptions. We would also like to thank ChrisMungall (LBNL), Oliver He (University of Michigan) for technical assistance withOntoBee and OntoFox, and Gareth Owen (ChEBI project leader, head curator)and other curators at ChEBI for assistance in the incorporation of microbial-specific chemical terms and synonyms into ChEBI. Thanks also to the instructors(Melissa Haendel, Matt Yoder, Jim Balhoff) and students of the 2013 NESCentOntologies for Evolutionary Biology workshop, and to Karen Cranston (NESCent)and the support staff at NESCent. Thanks also to the OBI-devel team forcomments regarding the overall structure of assay terms, and associatedobject properties, in MicrO.Author details1Department of Geosciences, University of Montana, Missoula, MT 59812,USA. 2School of Information, University of Arizona, Tucson, AZ 85719, USA.3Department of Biological Sciences, University of Southern Maine, Portland,ME 04104, USA. 4CyVerse, University of Arizona, Tucson, AZ 85721, USA.Received: 3 December 2015 Accepted: 2 April 2016EDITORIAL Open AccessVaccine and Drug Ontology Studies(VDOS 2014)Cui Tao1*, Yongqun He2 and Sivaram Arabandi3AbstractThe Vaccine and Drug Ontology Studies (VDOS) international workshop series focuses on vaccine- and drug-relatedontology modeling and applications. Drugs and vaccines have been critical to prevent and treat human and animaldiseases. Work in both (drugs and vaccines) areas is closely related - from preclinical research and development tomanufacturing, clinical trials, government approval and regulation, and post-licensure usage surveillance andmonitoring. Over the last decade, tremendous efforts have been made in the biomedical ontology community toontologically represent various areas associated with vaccines and drugs  extending existing clinical terminologysystems such as SNOMED, RxNorm, NDF-RT, and MedDRA, developing new models such as the Vaccine Ontology (VO)and Ontology of Adverse Events (OAE), vernacular medical terminologies such as the Consumer Health Vocabulary(CHV). The VDOS workshop series provides a platform for discussing innovative solutions as well as the challenges inthe development and applications of biomedical ontologies for representing and analyzing drugs and vaccines, theiradministration, host immune responses, adverse events, and other related topics. The five full-length papers included inthis 2014 thematic issue focus on two main themes: (i) General vaccine/drug-related ontology development andexploration, and (ii) Interaction and network-related ontology studies.Introduction and backgroundDrugs and vaccines have been critical to prevent andtreat human and animal diseases. Work in both (drugsand vaccines) areas is closely related - from preclinicalresearch and development to manufacturing, clinical trials,government approval and regulation, and post-licensureusage surveillance and monitoring. Ontologies can serveimportant roles in managing, normalizing, sharing, andleveraging drug and vaccine relevant data. The 2014Vaccine and Drug Ontology Studies workshop (VDOS2014) was an international forum for researchers toidentify, propose, and discuss solutions for importantresearch problems in ontology representation and analysisof vaccine and drug development, administration, func-tion mechanisms, safety, and education.VDOS 2014 was held on October 7, 2014, in Houston,Texas. This workshop was part of the Fifth InternationalConference on Biomedical Ontology (ICBO 2014). Theworkshop attracted interests from many internationalattendees, including paper presenters, academic andgovernment scientists, postdoctoral fellows, and graduatestudents. After a rigorous peer review process (all submis-sions have been reviewed by at least three independentreviewers), five full-length papers were accepted for pro-ceeding paper publications and oral presentations in theworkshop. After additional reviewing by the independentreviewers, workshop co-organizers, and the journaleditors, the selected five full-length papers were extendedand accepted for publication in the thematic track of theJournal of Biomedical Semantics (JBMS).The VDOS-2014 workshop is the 3rd in this series.The first workshop of the series was organized as theVaccine and Drug Ontology in the Study of Mechanismand Effect workshop (VDOSME 2012) [1] on July 21,2012, in Graz, Austria, as part of the third InternationalConference on Biomedical Ontology (ICBO 2012). In 2013,the name has been changed to Vaccine and Drug OntologyStudies (VDOS) to reflect the expansion in the scope tomore than just mechanism and effect. The workshopseries also covers vaccine and drug-related clinical datarepresentation and analysis, including clinically reportedvaccine and drug adverse events. VDOS 2013 was heldon July 7, 2013, in Montreal, Qc, Canada [2].* Correspondence: cui.tao@uth.tmc.edu1University of Texas, Health Science Center at Houston, of BiomedicalInformatics, Houston, TX, USAFull list of author information is available at the end of the articleJOURNAL OFBIOMEDICAL SEMANTICS© 2016 Tao et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Tao et al. Journal of Biomedical Semantics  (2016) 7:6 DOI 10.1186/s13326-015-0039-8Summary of selected papers in the thematic issueThe five papers selected for this thematic issue are ex-tended versions of the original full-length papers presentedat the VDOS 2014. This year the workshop focuses on twomain themes: (i) General vaccine/drug-related ontologydevelopment and exploration, and (ii) Interaction andnetwork-related ontology studies.In the area of general vaccine/drug-related ontology de-velopment and exploration, Winnenburg and Bodenreiderintroduced their approach on identifying common class-level signals for every individual drug in a class [3]. Theycreated a dataset of Adverse Drug Events (ADE) based oninformation extracted from PubMed abstracts and aggre-gated drugs into Anatomical Therapeutic Chemical (ATC)classes and ADEs into Medical Subject Headings (MeSH)terms. They then applied a visual approach to uncoverknown associations and a computational approach tosystematically analyze the associations between drug classesand ADEs. Amith and Tao introduced their VaccineInformation Statement Ontology (VISO), which is acomprehensive vaccine information ontology that cansupport personal health information applications usingpatient-consumer lexicon, and lead to outcomes thatcan improve patient education [4]. The current VISO con-tains the knowledge included in the Vaccine InformationStatement (VIS) documents collected from the USACenters for Disease Control and Prevention (CDC)website. The overarching goal of the VISO is to positivelyinfluence the development of intelligent ontology-drivenapplications and mitigate the knowledge gap that oftenexists in patients seeking accurate and reliable informationbut encountering complex or inaccurate sources.In the area of interaction and network-relatedontology studies, Peters et al. investigated the drug drug interaction (DDI) information in two publicallyavailable drug resources, NDF-RT and DrugBank [5].Their study indicated that the overlaps of DDIs be-tween the two resources are limited. Further studiesneed to be conducted to either determine how to bestleverage the DDI information included in these tworesources in clinical and translational applications.Hur et al. introduced their work on the developmentof the interaction Network Ontology (INO) with anaim to classify over 800 interaction keywords for literaturemining various interactions between genes and proteins[6]. Based on the INO-classified interaction type hierarchyand the Vaccine Ontology (VO)-based vaccine hierarchy, amodified Fishers exact test on PubMed literature minedresults was established to successfully analyze signifi-cantly over- and under-represented enriched gene-geneinteraction types within the vaccine domain. This studydemonstrated that hierarchical ontological definitionsfacilitate novel approaches for literature mining andanalysis of gene interaction networks. Zhang et al.introduce a novel approach that combines ontologiesand network analysis technologies for studying sex-associated patterns in vaccine adverse events [7]. Theauthors leverage data downloaded from the VaccineAdverse Event Report System (VAERS) and con-structed a condition-specific association network. Theoutcome of this research can potentially provide guidanceon sex-specific dose recommendations in personalizedvaccinology.Workshop presentations and discussionsIn the 2014 VDOS workshop, the five full-length papersdescribed above were orally presented. In addition, theprogram also included a keynote speech and a podiumabstract presentation. The keynote speech was given byDr. Khalid F. Almoosa, Associate Professor of Medicine atthe Department of Internal Medicine, UTHealth MedicalSchool, Houston, TX. Dr. Almoosa also serves as theVice-Chair for Healthcare Quality and the Director ofInterprofessional Collaboration at UTHealth. In hiskeynote, Dr. Almoosa discussed important issues inhealthcare quality and safety and the roles of vaccine anddrug safety in healthcare quality management. For thepodium abstract presentation, Dr. Yongqun He providedupdates on the development of the Ontology of AdverseEvents (OAE) and its applications. An in-depth discussionof the OAE is available in the recently published article inJournal of Biomedical Semantics [8].The discussion session of the workshop focused onthe topic of Drug and vaccine ontology informatics, wefirst discussed the similarities and differences betweenthe adverse event ontologies in the Open Biological andBiomedical Ontologies (OBO) domain (e.g., OAE andAERO) and non-OBO style ontologies (e.g., MedDRAand SNOMED). The necessity and possible approacheson how to bridge OBO and non-OBO ontologies wereexplored. Furthermore, the participants of theworkshop discussed ontology-based clinical and ex-perimental data processing and analysis, includingontology-supported literature mining of biomedical andelectronic health records. We all agreed to continue theworkshop in 2015 which would be held in conjunctionwith the ICBO-2015 conference at Lisbon, Portugal, in theend of July 2015.Overall, the VDOS 2014 workshop was well receivedwith positive feedbacks. The VDOS workshop serieshave served and will continue to serve a platform forbiomedical and clinical researchers to discuss ontology-relevant issues and progress in drug and vaccineresearch and applications.Competing interestsThe authors declare that they have no competing interests.Tao et al. Journal of Biomedical Semantics  (2016) 7:6 Page 2 of 3AcknowledgementsAs editors of this thematic issue, we thank all the authors who submittedpapers, the Program Committee members and the reviewers for theirexcellent work. We appreciate the support and help from the ICBO 2014meeting organizers. We are grateful for editorial reviews from Dr. DietrichRebholz-Schuhmann from JBMS.Author details1University of Texas, Health Science Center at Houston, of BiomedicalInformatics, Houston, TX, USA. 2Unit for Laboratory Animal Medicine,Department of Microbiology and Immunology, and Center forComputational Medicine and Bioinformatics, University of Michigan MedicalSchool, Ann Arbor, MI, USA. 3Ontopro LLC, Houston, TX, USA.Received: 17 September 2015 Accepted: 12 November 2015RESEARCH Open AccessLinking rare and common disease:mapping clinical disease-phenotypes toontologies in therapeutic target validationSirarat Sarntivijai1,2* , Drashtti Vasant1,2, Simon Jupp1, Gary Saunders1,2, A. Patrícia Bento1,2, Daniel Gonzalez1,2,Joanna Betts2,3, Samiul Hasan2,3, Gautier Koscielny2,3, Ian Dunham1,2, Helen Parkinson1 and James Malone1,2AbstractBackground: The Centre for Therapeutic Target Validation (CTTV - https://www.targetvalidation.org/) wasestablished to generate therapeutic target evidence from genome-scale experiments and analyses. CTTV aims tosupport the validity of therapeutic targets by integrating existing and newly-generated data. Data integration hasbeen achieved in some resources by mapping metadata such as disease and phenotypes to the ExperimentalFactor Ontology (EFO). Additionally, the relationship between ontology descriptions of rare and common diseases andtheir phenotypes can offer insights into shared biological mechanisms and potential drug targets. Ontologies are notideal for representing the sometimes associated type relationship required. This work addresses two challenges;annotation of diverse big data, and representation of complex, sometimes associated relationships between concepts.Methods: Semantic mapping uses a combination of custom scripting, our annotation tool Zooma, and expertcuration. Disease-phenotype associations were generated using literature mining on Europe PubMed Central abstracts,which were manually verified by experts for validity. Representation of the disease-phenotype association was achievedby the Ontology of Biomedical AssociatioN (OBAN), a generic association representation model. OBAN representsassociations between a subject and object i.e., disease and its associated phenotypes and the source of evidence forthat association. The indirect disease-to-disease associations are exposed through shared phenotypes. This was appliedto the use case of linking rare to common diseases at the CTTV.Results: EFO yields an average of over 80 % of mapping coverage in all data sources. A 42 % precision is obtainedfrom the manual verification of the text-mined disease-phenotype associations. This results in 1452 and 2810disease-phenotype pairs for IBD and autoimmune disease and contributes towards 11,338 rare diseases associations(merged with existing published work [Am J Hum Genet 97:111-24, 2015]). An OBAN result file is downloadableat http://sourceforge.net/p/efo/code/HEAD/tree/trunk/src/efoassociations/. Twenty common diseases are linked to85 rare diseases by shared phenotypes. A generalizable OBAN model for association representation is presented inthis study.Conclusions: Here we present solutions to large-scale annotation-ontology mapping in the CTTV knowledgebase, a process for disease-phenotype mining, and propose a generic association model, OBAN, as a meansto integrate disease using shared phenotypes.Availability: EFO is released monthly and available for download at http://www.ebi.ac.uk/efo/.Keywords: Rare disease, Phenotype disease associations, OBAN, CTTV, EFO* Correspondence: siiraa@ebi.ac.uk1European Bioinformatics Institute (EMBL-EBI), European Molecular BiologyLaboratory, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD,UK2Centre for Therapeutic Target Validation, Wellcome Trust Genome Campus,Hinxton, Cambridge CB10 1SD, UKFull list of author information is available at the end of the article© 2016 Sarntivijai et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Sarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 DOI 10.1186/s13326-016-0051-7IntroductionDrug discovery research involves diverse analytical activ-ities and integration of many sources of data about di-verse entities from single nucleotide polymorphisms(SNPs) to pathways, proteins to populations. The Centrefor Therapeutic Target Validation (CTTV) is a collabor-ation between the European Bioinformatics Institute(EMBL-EBI), GlaxoSmithKline (GSK) and the WellcomeTrust Sanger Institute (WTSI) to develop a knowledgebase of evidence for drug targets based on genomic ex-periments and bioinformatics analyses. A CTTV goal isto develop a better understanding of the rare and com-mon disease relationship via shared phenotypes, genes,and pathways, as information from rare disease can pro-vide mechanistic insight to common disease and viceversa. This requires integration of data generated byCTTV projects with existing data residing in EMBL-EBI,WTSI and GSK resources. Data types include variants,genes, proteins, gene expression, pathways, compounds,literature and related experimental variables such as dis-ease and phenotype with data generation on different ex-perimental platforms such as Genome Wide AssociationStudies and next generation sequencing.The integration of disease and phenotypic information,where a group of phenotypes are associated with a dis-ease, becomes increasingly important when consideringrare diseases where research is typically fragmentedacross omics types and disease. Rare disease data are notalways compatible with each other as they come fromdifferent resources, e.g., OMIM [1] and ORPHANET[2], represent different perspectives of the diseases, suchas diagnostics or treatment, and data are typically popu-lation, or even individual, specific. The sparseness andheterogeneity of this data therefore introduces a majorchallenge in the integration of rare and common diseaseinformation [3].CTTV uses the Experimental Factor Ontology (EFO)[4] as its application ontology to provide an integratedand consistent ontological representation of the CTTVplatform data. EFO provides an integration frameworkfor ontologies and reuses components of domain-specificontologies such as Orphanet Rare Disease Ontology(ORDO) [5], ChEBI [6], Gene Ontology [7] and Uberon[8]. Typically a data or use case driven SLIM (a subset ofthe referenced ontology with MIREOT import closures[9]) of a source ontology is created, and then importedinto EFO. Figure 1 illustrates the exponential growth ofEFO where a large amount of classes are imported fromexternally-sourced ontologies. This presents challengesrepresenting the imported knowledge in EFO without los-ing the structural integrity of the original ontologies. Wetherefore use MIREOT to import classes, or small sectionsof hierarchies from external ontologies to avoid potentiallyimporting the whole or most of a source ontology intoEFO due to the complexity of class organization. This alsohelps ensure amenability of EFO to wider data integration.For example, rare disease terms are imported from ORDOand phenotypes from Human Phenotype Ontology termsas both ontologies are compatible with EFOs disease andphenotype design pattern respectively and common dis-ease terms are defined locally with EFO-namespace URI.Even though other ontologies exist that aim to describedisease, there is not one single-origin representation ofcommon disease in any of the available ontologies that iscompatible with the current design pattern of disease rep-resentation used in EFO, thus creating common diseaseclasses in the EFO namespace is currently necessary forCTTV. Figure 1 shows that despite considerable growthin EFO-native classes (3992 EFO-native classes in 2015, asopposed to 2214 classes in 2010), EFO use of importedclasses from external domain ontologies is increasing.EFO uses common design patterns that are consistentthroughout the EFO ontology development process (e.g.,term creation, and term importing) to integrate andorganize the ontologies imported. For example, the designpattern for cell line representation: cell line derives_-from a cell type, which is part_of an organism, which isa bearer_of some disease links an EFOs cell line classto the Cell Ontologys cell type class, an NCBI Tax-onomy class, and EFOs or ORDOs disease class. Thiscell line design pattern as shown in Fig. 2 is also sharedwith the Cell Line Ontology [10]. Webulous [11] (ex-tended publication in JBMS Bioontologies SIG The-matic issue), a tool which implements these designpatterns in a Google Sheets add-on, is used to createnew terms (the class), and to allow users to define newterms for EFO in spreadsheet format. These are trans-formed to OWL and imported prior to each monthlyrelease. The use of design patterns also providesconsistency with other ontology consuming resourcessuch as the EBI RDF Platform [12]. In order to be inter-operable with OBO foundry ontologies EFO uses BFO1.1 [13] upper level classes. For example EFO repre-sents disease as a child of BFO:Disposition [14]whereas, following the same process, HP:phenotype ismodelled as a child of BFO:Quality. In EFO, a commondesign pattern is such that an EFO:disease has_pheno-type HP: links EFO disease terms and HP. EFO diseasesare organized utilizing an object property has_disease_-location using anatomical classes imported fromUBERON.Data resources integrated into CTTV have local stan-dards for annotation and many aggregate data from mul-tiple external sources, where each external resource alsohas a resource specific annotation and/or curationprocess. They have also historically used different ontol-ogies and dictionaries for disease and phenotype annota-tion; examples include Online Mendelian Inheritance inSarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 Page 2 of 11Man (OMIM) [15], the Systematized Nomenclature ofMedicine  Clinical Terms (SNOMED-CT) [16], theHuman Disease Ontology (DO) [17], and the MedicalDictionary for Regulatory Activities (MedDRA) [18] asseen in Table 1. We note that these resources often donot differentiate between disease and phenotype whenselecting and applying the vocabularies to their data. Wehave standardized this for CTTV, differentiating pheno-type from disease, and defaulting to HP imported termsin EFO for the description of phenotypes where possible.For example, the GWAS Catalog trait myopia is anno-tated to the HPs IRI http://purl.obolibrary.org/obo/HP_0000545 Myopia. EFO therefore contains pheno-typic terms that are clearly distinguished from diseaseterms for annotation of CTTV data.Diseases are associated with phenotypes which mani-fest in the disease with qualifying information about thenature of the association. The disease-phenotype associ-ation is established to represent disease connections viashared phenotypes. For example, the rare diseaseAicardi-Gourtieres syndrome has several associated phe-notypes affecting the brain, immune system, and skin,such as microcephaly, hepatosplenomegaly, elevatedblood liver enzymes, thrombocytopenia, and abnormalneurological response. It is often not observable at birth,and all phenotypes are unlikely to be present in all pa-tient presentations. Additionally phenotypes may alsovary by kindred and/or by population in their frequencyand penetrance. The same is true for common disease,for example, phenotypes of Crohns disease may rangefrom inflammation of any part of the gut (but mostlikely ileum or colon), diarrhea, or constipation, but notall symptoms are necessarily present in one patient.Fig. 1 There were 2214 EFO-native classes in January 2010, and 3992 EFO-native classes in January 2015. Although EFO has significantly grown inits number of native classes, the number of imported classes has grown at a much higher rate. Importing more than 6000 rare disease classesfrom ORDO in 2012, and axiomatizing them into EFO has resulted in a sudden increase between 2012 and 2013. This reflects the use of EFO asan application ontology providing interoperability across domain ontologies through semantic axiomatizationFig. 2 The cell line design pattern in EFO links an EFO class cell lineto external ontologies via import mechanism. An EFO cell linederives_from a cell type class from Cell Ontology, which is part_of anorganism  a class imported from NCBI Taxon. EFO cell line class isalso a bearer_of a disease  a class imported from ORDO or classnative to EFO itselfTable 1 An overview of ontologies usage by each CTTV datasource. Cross-reference sources of each CTTV data resource arenormalized to EFO for CTTV data validation processDatabase Cross-reference annotation sourcesEVA OMIM, SNOMED-CT, MeSHArrayExpress GO, OMIM, EFOUniProt OMIM, Orphanet, MeSHReactome OMIM, GOChEMBL MedDRA, ATC, GOGWAS Catalog EFO, DOSarntivijai et al. Journal of Biomedical Semantics  (2016) 7:8 Page 3 of 11Representation of the disease-phenotype association inan OWL ontology with the statement disease has_phe-notype some phenotype requires that all instances of adisease have that specific phenotype and our examplesabove illustrate that this representation is problematicfor many cases. We have therefore chosen to representdisease-phenotype association in a generic associationmodel OBAN (the Open Biomedical AssociatioN),which allows us to represent both the disease-phenotypeassociation and qualify the association with evidence,and, in the future, to represent information such as fre-quency of association. In order to test this model, and topopulate it with disease-phenotype associations forInflammatory Bowel Disease we used a text mining ap-proach to extract these from the literature, building acorpus using an expert nominated set of journals as ourexperience described in Vasant et al. [19], indicates thatconstraining the corpus improves precision on post-hocvalidation by experts. Abstracts were accessed using theEuropePMC API [20] and the Whatizit text miningpipeline [21] was usd to mine the corpus using a dic-tionary comprised of phenotype terms from the HumanPhenotype Ontology [22] and the Mammalian Phenotypeontology [23].MethodsMapping CTTV data sources disease and phenotype termsto EFOIn order to perform semantic integration of multiple re-sources for CTTV, the data from each source (listed inTable 1) was mapped to EFO identifiers. Challenges inperforming such mapping pertain in the non-standardized use of vocabulary sets by different re-sources. Some of the resources used an ontology, e.g.,Disease Ontology, a taxonomy such as MeSH [24], orcross-referenced another resource such as OMIM. Dis-eases and phenotypes are often mixed in the same re-source and sometimes in the same category annotation.For example, the European Variation Archive (EVA http://www.ebi.ac.uk/eva/) [25] trait names labeling usesa mixed set of vocabularies from HP, SNOMED-CT,OMIM, and non-standardized local identifiers used in-ternally at source from the ClinVar records. The identi-RESEARCH Open AccessVICO: Ontology-based representation andintegrative analysis of Vaccination InformedConsent formsYu Lin1,2,3,4, Jie Zheng5 and Yongqun He1,2,3,4*AbstractBackground: Although signing a vaccination (or immunization) informed consent form is not a federal requirement inthe US and Canada, such a practice is required by many states and pharmacies. The content and structures of theseinformed consent forms vary, which makes it hard to compare and analyze without standardization. To facilitatevaccination informed consent data standardization and integration, it is important to examine various vaccinationinformed consent forms, patient answers, and consent results. In this study, we report a Vaccination InformedConsent Ontology (VICO) that extends the Informed Consent Ontology and integrates related OBO foundry ontologies,such as the Vaccine Ontology, with a focus on vaccination screening questionnaire in the vaccination informedconsent domain.Results: Current VICO contains 993 terms, including 248 VICO specific terms and 709 terms imported from 17 OBOFoundry ontologies. VICO ontologically represents and integrates 12 vaccination informed consent forms from theWalgreens, Costco pharmacies, Rite AID, University of Maryland College Park, and the government of Manitoba,Canada. VICO extends Informed Consent Ontology (ICO) with vaccination screening questionnaires and questions.Our use cases and examples demonstrate five usages of VICO. First, VICO provides standard, robust and consistentrepresentation and organization of the knowledge in different vaccination informed consent forms, questionnaires, andquestions. Second, VICO integrates prior knowledge, e.g., the knowledge of vaccine contraindications imported fromthe Vaccine Ontology (VO). Third, VICO helps manage the complexity of the domain knowledge using logically definedontological hierarchies and axioms. VICO glues multiple schemas that represent complex vaccination informed consentcontents defined in different organizations. Fourth, VICO supports efficient query and comparison, e.g., through theDescription Language (DL)-Query and SPARQL. Fifth, VICO helps discover new knowledge. For instance, by integratingthe prior knowledge imported from the VO with a users answer to informed consent questions (e.g., allergic reactionquestion) for a specific vaccination, we can infer whether or not the patient can be vaccinated with the vaccine.Conclusions: The Vaccination Informed Consent Ontology (VICO) represents entities related to vaccination informedconsents with a special focus on vaccination informed consent forms, and questionnaires and questions in the forms.Our use cases and examples demonstrated how VICO could support a platform for vaccination informed consent datastandardization, data integration, and data queries.* Correspondence: yongqunh@med.umich.edu1Unit for Laboratory Animal Medicine, University of Michigan Medical School,Ann Arbor, MI 48109, USA2Department of Microbiology and Immunology, University of MichiganMedical School, Ann Arbor, MI 48109, USAFull list of author information is available at the end of the article© 2016 Lin et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Lin et al. Journal of Biomedical Semantics  (2016) 7:20 DOI 10.1186/s13326-016-0062-4BackgroundDifferent countries and organizations have various regu-lations in terms of the requirement of informed consentto vaccination before vaccinating a patient. Signing avaccination (or immunization) informed consent form isnot a federal requirement in the US and Canada. As anexception, the 1976 swine flu immunization programestablished by the US Federal legislation, included re-quirements that recipients of the swine flu vaccine befully informed of the risks and benefits of immunizationand that written consent forms be used (42 U.S.C.A.§247b(j)(1)(F) (Supp.1977) [1]. The USA National ChildhoodVaccine Injury Act of 1986 (NCVIA - 42 U.S.C. § 300aa-26) requires that the Vaccine Information Statements(VISs) (http://http://www.immunize.org/vis/) must beprovided by all public and private vaccination providers tothe patient (or parent or guardian) prior to every dose ofspecific vaccines. Although there is no US federal require-ment, the documentation of consent is recommended orrequired by some states, local health authorities, schoolauthorities, or pharmacies. The immunization protocolsfrom many pharmacies require the signatures of recipientsor legal representatives on specific vaccination informedconsent forms before vaccination in accordance with localstates legislation (https://www.pharmacist.com/guidelines-pharmacy-based-immunization-advocacy). The ManitobaProvince in Canada provides vaccination informed consentguidelines for routine immunization in accordance withThe Public Health Act (C.C.S.M. c. P210) in Canada [2].Additionally, new laws in Texas were passed in 2013to allow pregnant minors and minor parents to con-sent to their own vaccination [3]. Under this circum-stance, one can conclude that there must be a varietyof vaccination informed consent forms if providedfrom different sources, such as pharmacies, schoolsand hospitals. An important question is whether theinformed consent for vaccination provides enoughadequate information for the patients to understandtheir risks and benefits prior to their vaccinations. Toaddress this question, an overview and comparison ofcurrent informed consent forms are critical.Due to different policies and vaccination informedconsent forms offered by authorities and pharmacies, itis difficult to compare, evaluate, and manage vaccinationinformed consent procedures and data from differentresources. Ontologies, sets of terms and relations thatrepresent entities in a domain and how they relate toeach other, support computer-assisted data integration,knowledge management, and new knowledge discovery.Developing a vaccination informed consent ontologythat can advance integrating data from different re-sources may facilitate the research in informed consentand vaccine policy. Therefore, more efficient patientsafety management can be achieved.Based on the principle of ontology reuse, two establishedontologies can be re-utilized to ontologically representvaccination informed consents. One is the community-based Informed Consent Ontology (ICO) [4]. ICO repre-sents the documentations and processes involved in in-formed consent. ICO aims to support informed consentdata integration and reasoning in the clinical researchspace. Vaccine Ontology (VO) [5, 6] represents licensedvaccines, vaccines in clinical trials, and experimentallyverified vaccines in research laboratories. In addition,vaccine-related information including vaccine compo-nents, vaccine licenses, vaccine manufacture, vaccin-ation, and vaccination doses are also represented inVO. Both Informed Consent Ontology and VaccineOntology are aligned with the upper level Basic FormalOntology (BFO) [7] and compliant with the Open Bio-logical and Biomedical Ontologies (OBO) Foundry ontol-ogy development principles [8]. Built on the same upperontology, Informed Consent Ontology and VaccineOntology can be seamlessly integrated for represent-ing vaccination informed consent.In this paper, we report the development of theVaccination Informed Consent Ontology (VICO) byextending Informed Consent Ontology and integratingother OBO Foundry ontologies, such as Vaccine Ontology.The goal of VICO is to represent the vaccination informedconsent document, organize vaccination informed consent-related entities, and to establish relations between thoseentities so that the knowledge of vaccination in-formed consent can be captured explicitly. Thecurrent development focus is to consistently representdifferent vaccination/immunization informed consentforms, especially the immunization screening ques-tionnaires inside these forms. We then apply VICO toconduct systematic vaccination/immunization informedconsent form comparison and patient centered informedconsent data query and analysis. We hypothesized thatVICO would significantly enhance standard data represen-tation, integration and query of informed consent relatedto various vaccine immunization procedures. To test thehypothesis, we lay out five major usages of VICO relatedto the hypothesis, and provide use cases to justify theseusages.MethodsCollection of vaccination informed consent formsTwelve vaccination informed consent forms werecollected from the following resources: Costco Pharmacywebsite [9] (accessed on 01/26/2016), Walgreens Phar-macy website [10] (accessed on 0126/2016), Rite AIDwebsite [11] (accessed on 01/26/2016), Manitoba govern-ment Public Health division [12] (accessed on 01/26/2016), and the University of Maryland College Park [13](accessed on 01/26/2016).Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 2 of 14VICO ontology developmentVICO was formatted in the Web Ontology Language(OWL2) [14]. The backbone of VICO composed of aportion of Informed Consent Ontology (ICO), a portionof Vaccine Ontology (VO), and VICO specific terms.The workflow of developing VICO includes followingfour processes:1) Extract subset of ICO. After manually identifyingthe ICO terms related with vaccination informedconsent, the OntoFox tool [15] was used to extractportion of ICO.2) Extract subset of VO. This process includes twosteps of extracting portion of VO using two differenttools, Ontobee SPARQL endpoint and OntoFox tool.First, all licensed vaccines in USA and Canada wereretrieved using the Ontobee SPARQL endpoint(http://www.ontobee.org/sparql) [16] based onthe logical axiom defined in VO: bearer_of someUSA licensed vaccine role. Next, OntoFox toolwas applied to extract a portion of VO thatcontains all the related terms, logical axioms(classes and relations), definitions, andannotations of each licensed vaccine retrievedfrom the first step.3) Importing ICO and VO subsets to VICO. TheICO and VO subsets form the backbone of VICO.The importing process is done by using anowl:import statement in the VICO owl file. Thisbackbone of VICO with imported ICO and VOsubsets can be displayed using the Protégé OWLEditor tool.4) Enrich VICO with VICO-specific terms. We firstidentify a vocabulary from abovementionedquestions, build hierarchy from this vocabulary,and then relate these terms using existing definedrelations or VICO specific relations. We establishthe logical axioms, so that the questions can benormalized with clear semantics. For example, aquestion of have you had received any vaccinationsin the past 4 weeks is related with the vaccinationterm from VO, and is asserted as a subclass ofquestions on past vaccination information VICOterm, which has a mother term of question textualentity from IAO. Categorizing questions fromdifferent forms are mainly based on ontologistsmanual assessment in accordance with hierarchiesof question related entities established in existingOBO Foundry ontologies. This way, differentforms and questions appeared as different textstrings can be related to common existingontology entities, thus enable automatedanalyzation of vaccine informed consent formprogrammatically.The Protégé OWL Editor version 5.0 beta [17] wasused to develop VICO. The HermiT reasoner (http://hermit-reasoner.com/) tool was employed to perform thereasoning over VICO to detect inconsistencies orconflicts.The VICO Github project (https://github.com/VICO-ontology/VICO) was created to facilitate the projectversion control and tracking issues.VICO evaluation by use casesUse cases and examples were laid out to evaluate anddemonstrate the applications of VICO in supporting dif-ferent usages. For application evaluation, SPARQL and/or DL languages were often applied. The SPARQL querieswere performed using the Protégé SPARQL program orOntobees SPARQL query endpoint (http://www.ontobee.org/sparql) [16]. The DL queries were performed using DLQuery plugin of Protégé 5.0 (beta 15) to answer questionsfrom use case 2. Query scripts generated for this projectwere stored in the Github under the folder:https://github.com/VICO-ontology/VICO/tree/master/src/SPARQL%20query.Ontology source access and licenseThe VICO is an open source project. The source codeincluding development version and released version arefreely available at the URL: https://github.com/vico-ontology/VICO. VICO is released under a Creative Com-mons 3.0 License.ResultsVICO ontology design and top level structureVICO is a community-driven ontology that crosses bothinformed consent domain and vaccine domain. In VICO,we extended Informed Consent Ontology (ICO) andVaccine Ontology (VO) by adding VICO specific termsrepresenting vaccination informed consent forms (Fig. 1).For example, specific VICO terms were generated torepresent the vaccination/immunization informed con-sent forms from different vaccination providers or gov-ernments, such as Costco vaccination informed consentform (Fig. 1). Another example is vaccination screeningquestionnaire, defined as A questionnaire that containsdifferent questions of a vaccination patients healthhistory, allergy history, and current condition, in orderto assess the contraindication and precaution for admin-istering a vaccine in VICO. This term was generated forrepresenting questionnaire embedded in a vaccinationinformed consent form. The questions for vaccinationpatients can be answered by the patients themselves, ortheir legal representatives, prior to a vaccination proced-ure. VICO defines these questions in a structured logicalmanner (Fig. 1).Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 3 of 14The basic VICO ontology design pattern is composedof various entities linked by defined relations as illus-trated in Fig. 2. Specifically, before a vaccination process,a vaccination informed consent form is documented byan organization/company (e.g., the company Costco). Avaccination screening questionnaire containing a list ofquestions is provided for a vaccination patient or his/herlegal representative to answer. The documented_by is arelation defined by VICO as an object property that rep-resents a relation between a document and an entity thatwrites, maintains and releases the document. A specificvaccination consent form is often restricted to be usedin a specific geographic location (e.g., Manitoba inCanada or South Carolina in USA). The contents of thequestions in the form often cover different topics suchas the vaccination patients current health status, currenttreatment, allergic reaction history, past vaccinationhistory, and so on. The vaccination patients age and bio-logical sex (female or male) are modelled using PATOterm has_quality linking the age and biological sex topatient. The vaccination process occurs via a specifiedvaccination route (e.g., intranasal influenza vaccinationfor FluMist) (Fig. 2). A question, for example, questionon serious nasal condition (asked for FluMistvaccination only) is about vaccination procedure someFluMist vaccination, which is a live attenuated influenzavaccine. A vaccination related question may also berelated to a disease (e.g., cancer) or an adverse eventprocess, which has been expressed in VICO using therelation is about (Fig. 2). Details about ontologicalmodeling of vaccine/vaccination can be found in theprevious VO paper [5, 6]. Informed consent representationFig. 1 Top level terms and hierarchical structure of VICO. VICO imports many top level terms from VO and ICO and includes VICO-specific termsas exampled with two VICO terms (bold)Fig. 2 Basic VICO design patternLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 4 of 14is introduced in previous ICO paper [4]. By integratingVO/ICO representations and including VICO-specificcontents, VICO provides a framework to link vaccinationpatient, vaccinee quality, vaccine, vaccine quality,vaccination, vaccination targets, informed consentprocess, informed consent forms, questions, question-naires, and related information.As of January 30, 2016, VICO (version 1.0.51) contains993 terms, including 707 classes, and 92 object proper-ties. VICO includes 243 VICO-specific classes and prop-erties with the VICO_ prefix, which are new ontologyterms not covered in ICO, VO, or any other OBO Foun-dry ontologies. As shown in the Ontobee VICO statisticspage (http://www.ontobee.org/ontostat/VICO), in additionto ICO and VO, VICO also reused terms from other OBOFoundry ontologies such as Information Artifact Ontology(IAO) [18] and Ontology for Biomedical Investigations(OBI) [19], which mainly indirectly imported due to im-ports of ICO and VO. VICO is deposited in the OntobeeRDF triple store [16], and can be visualized and quer-ied on the Ontobee website: http://www.ontobee.org/ontology/VICO.VICO UsagesIn general, ontologies can be used in different aspects ofknowledge management, including: (i) provide robust andconsistent knowledge representation and organization, (ii)integrate prior knowledge, (ii) manage the complexity ofdomain knowledge, (iv) support efficient query andcomparison, and (v) help discover new knowledge. Belowwe will elaborate how VICO is applied in these abovecategories of usages.Usage 1: Provide robust and consistent knowledgerepresentation and organizationVICO provides a robust and consistent representation ofthe knowledge of vaccination informed consent in thethree major branches: vaccination informed consentform, vaccination screening questionnaire, and ques-tions. The basic relations between these three branchesare that a vaccination informed consent form 'has part' avaccination screening questionnaire that 'has compo-nent' many questions. Each of the branches is organizedwith a hierarchical structure through the is a relation.For example, Costco vaccination informed consent formis a vaccination informed consent form. The followingexample axioms illustrate the relations of terms in thethree branches:1. Costco vaccination informed consent form:'has part' some 'questionnaire for Costcovaccination consent'2. questionnaire for Costco vaccination consent: 'hascomponent' some 'question whether allergic to egg'All the questions in VICO are organized in a hierarch-ical structure under ICO term: question textual entity(ICO_0000141). VICO modelled 158 questions collectedfrom 12 vaccination informed consent forms. Afterstandardization with newly generated mother terms andhierarchical terms, there are 168 VICO question termsarranged under the IAO term question textual entity.For robust representation, VICO sometimes breaksdown a question with mixed information to more thanone specific question. For example, the Costco form lists12 questions (Fig. 3a). Figure 3b demonstrates how VICOrepresents these questions, questionnaires and theirrelations. The question Do you have allergies to medica-tions, food or vaccines? is represented in VICO as threeVICO question terms: question whether allergy to food,question whether allergy to medication, and questionwhether allergy to vaccine.VICO consistently represents linguistic variants ofquestions with standardized VICO terms. For example,the are you sick today and are you well today is anno-tated as term question whether currently sick. Textualdefinitions, definition sources, and comments are alsoprovided to ensure clarity and consistency.The answer to an informed consent question can beYes, No, or unknown. VICO represents the answerto a question using answer option text entity (ICO_0000171), a subclass of textual entity (IAO_0000300). Wehave two ICO terms to represent the Yes or Noanswer: yes answer text entity (ICO_0000172) and noanswer text entity (ICO_0000173). VICO created its owndont know answer text entity (VICO_0000006), since insome vaccination informed consent form, dont knowwas another choice of answer in addition to Yes or No.The standard, robust, and coherent representation andorganization of the knowledge in the domain of vaccin-ation informed consent are the foundation of the otherusages as described below.Usage 2: Integrate prior knowledgeVICO provides a way to seamlessly integrate priorknowledge as represented in other ontologies such asthe Vaccine Ontology (VO). As a vaccination informedconsent ontology, VICO does not focus on the attributesof any licensed vaccines. Fortunately, such informationis available in the VO. VO provides informed consentrelated information for licensed vaccines, includingvaccine ingredients, vaccine manufactures, vaccine admin-istration routes, contraindications, etc. Such informationis important for interpreting vaccination patients answersof informed consent questions. Instead of generating suchinformation from scratch, VICO imported the relatedinformation directly from VO.As a demonstration of importing prior knowledge fromVO, Fig. 4 shows the VOs representation of a vaccine,Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 5 of 14Afluria. Afluria is a human influenza viral vaccine licensedfor use in the USA. It is an inactivated vaccine againstInfluenza virus A. It is manufactured by the CSL Limited,and distributed by Merck & Co, Inc. Various characteris-tics of Afluria were represented using logical axioms inVO. For example, the axiom: has vaccine allergen somechicken egg protein allergen encodes that the Afluriavaccine contains a trace of chicken egg protein, which isable to induce allergic reaction in certain population.Therefore, the hypersensitivity to chicken egg becomes acontraindication to this vaccine. Another vaccine allergenassociated with this vaccine is neomycin. This vaccine isadministered via an intramuscular route. The vaccineinformation has been imported to VICO from VO asdetailed in the Methods section. Such importing mechan-ism provides a valid strategy for VICO to integrate priorknowledge as recorded in VO.In addition, VICO also imports prior knowledgerelated to informed consent questions by adding diseaseinformation and organizing the questions based ondiseases or symptoms. To support the interoperability,the VICO questionnaire questions have been mapped tothe Logical Observation Identifiers Names and Codes(LOINC; https://loinc.org/) [20, 21] and semanticallylinked to the Disease Ontology (DOID) [22], SymptomOntology (SYMP), Ontology of Adverse Event (OAE),and some newly generated VICO terms for medicalprocedure or health condition. For example, in VICO,question whether currently sick with a moderate to highfever, vomiting/diarrhea is linked to SYMP:fever andDOID:diarrhea through the IAO relation is_about.Another example is allergy question, which is relatedto DOID term hypersensitivity reaction type I diseaseusing is_about relation. Likewise, question whetherallergy to food is about food allergy (DOID). Thisway, the VICO bridges questions with existing terms andprior knowledge from these other ontologies.Usage 3: Manage the complexity of domain knowledgeThe complexity of the vaccination informed consent liesin the complex relations between vaccination patients,patient attributes (e.g., age and gender), vaccine, vaccineattributes, and different levels of question complexity.Such complexity is managed in VICO by generating andFig. 3 Questionnaire in Costco vaccination informed consent form and its representation in VICO. a All the questions shown in the in Costco informedconsent form. b VICO representation of the questions in the Costco form questionnaireLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 6 of 14following a concise and valid design pattern (Fig. 2) thatsemantically link all the related classes together. Thelinkages among different classes are established withsolid object properties (i.e., relations).The complexity of the vaccination informed consent isalso reflected by the fact that various vaccination infor-med consent forms exist for different purposes and areused in different locations. For example, in Rite Aids,different informed consent forms are distributed in dif-ferent states: South Carolina, North Carolina, California,and other states. Informed consent forms for kids areoften different with those for adults, or college students.Although many informed consent forms are generallyapplied for different vaccinations, in our corpus of vaccin-ation informed consent forms, four different informedconsent forms are all about Flu vaccination: University ofMaryland College Park injectable influenza vaccinationinformed consent, University of Maryland College ParkFlumist vaccination informed consent, Rite Aid injectableflu vaccination informed consent in South Carolina, andManitoba Seasonal Influenza and Pneumococcal vaccin-ation informed consent form.The management of these complex forms and ques-tions in VICO relies on the consistent and robust repre-sentation and organization (see Usage 1), and the use oflogical axioms to clearly lay out the differences. Forexample, the relation between a form and its source isclearly defined using the relation documented_by asexemplified here: Costco vaccination informed consentform: documented by value Costco. Another exampleaxiom to link a form with a purpose is here: UM CollegePark flumist vaccination informed consent: is aboutvaccination procedure some Flumist vaccination. TheFlumist vaccination is then linked to the influenza vac-cine Flumist with another axiom. Compared to differentschemas representing various vaccination informed con-sent contents from different organizations, VICO isunique in that it glues multiple schemas together usingwell-organized ontological representations.Usage 4. Support efficient query and comparisonBased on the consistent and well-organized VICO repre-sentation and organization of complex knowledge, wecan easily browse in an ontology editor or web ontologybrowser, or perform SPARQL queries against the VICOto more specifically compare the questions in differentforms. SPARQL is an Resource Description Framework(RDF) query language able to retrieve ontology datastored in the RDF format [23]. These ontology browsingand queries can be used to compare questions from dif-ferent forms.For example, SPARQL queries were used to compareCostco and Walgreen vaccination informed consentforms. Figure 5 shows how a SPARQL query can be usedto identify the common (Fig. 5a) and different (Fig. 5b)questions raised in Costco and Walgreen vaccinationinformed consent forms. In this SPARQL, we used anobject property documented by that represents therelation between an informed consent form and anorganization (e.g., pharmacy) (Fig. 5b). Our comparativeanalysis using SPARQL queries found 39 unique ques-tions listed in Walgreens and Costco vaccination in-formed consent forms (Table 1). Among these questions,12 questions are shared by both forms, six questions arelisted by Costco form only, and 21 questions are uniqueto the Walgreen form. Compared to the Costco form,four more questions are listed in the Walgreens form.Note that Walgreens unique question question onwhether fainted or felt dizzy after immunization (ques-tion #19) is the subclass of Costco unique questionquestion on reaction after immunization (question#18) in VICO. Similarly, question whether currentlysick (question #13) used by Costco is the parent termof question whether currently sick with a moderate tohigh fever, vomiting/diarrhea (question #21) used byWalgreens. These observations reveal that although bothCostco and Walgreens ask the adverse events aftervaccination, Walgreens asks more specific and nar-rower questions.Fig. 4 The ontology hierarchy and associated axioms of the Afluriainfluenza vaccine. The contents are defined in VO and imported toVICO. The figure is a screenshot from Ontobee [16]. The highlightedegg allergen axiom is used for a use case demo detailed later inthis paperLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 7 of 14The questions asked are typically consistent with pa-tient requirements before vaccinations. For example, thequestion on current aspirin therapy (question #25 inTable 1) is a question specifically for the vaccine FluMistQuadrivalent, asked by Walgreens. To investigate moreon why Walgreens asks these vaccine-specific questions,we examined the package insert document for FluMistQuadrivalent. The contraindication section of theFluMist Quadrivalent package insert document says:Concomitant aspirin therapy in children and adoles-cents [24]. This contraindication statement indicatesthat a patient with concomitant aspirin therapy cannotbe vaccinated with the FluMist Quadrivalent vaccine.This information provides the solid reason why theWalgreens form asks whether a patient is taking aspirin.Usage 5: Help discover new knowledgeThe consistent representation, robust organization, andprior knowledge integration of informed consent formsmake VICO a useful platform for new knowledge discov-ery based on patients answers of a questionnaire.Based on a patients answers to informed consentquestions, we will demonstrate with a use case on howVICO and OWL-based technologies can be used to dis-cover whether a patient can or cannot be vaccinated witha vaccine containing a trace of egg protein. This use caseis related to vaccine contraindication (e.g., allergic reactionto chicken egg), a rare condition in a recipient that in-creases the risk for a serious adverse reaction. Ignoringcontraindications can lead to dangerous vaccine adversereactions. As shown in Fig. 3, the Afluria influenza vaccinehas the contraindication of hypersensitivity to chickenegg since it has an egg allergen. Therefore, vaccination ofegg-allergic patients with a vaccine (e.g., Afluria) that hasan egg allergen is currently not recommended [25].Basically, this use case contains two-steps: Step 1: Findif a vaccine contains egg allergen, and Step 2: Find if apatient is allergic to egg. If the answers to both questionsare positive, this patient cannot be administered with thevaccine found in step 1. For the first step, we developeda SPARQL query to identify vaccines that have restric-tion on egg allergic reaction (Fig. 6). As a result, eightlicensed vaccines were identified as vaccines containingegg protein allergen. This step is essentially to query theprior knowledge that is imported from VO to VICO.To implement the second step in our sandbox demon-stration, first we ontologically represented a patientsanswers to the questions in a filled informed consentform. In this sandbox use case study, we hypotheticallycreated twelve patients answers to the questions in theCostco vaccination informed consent form, the Walgreensvaccination informed consent form, or the Manitoba in-formed consent form respectively. All the answers were in-stantiated as VICOs instance data (owl:NamedIndividual).Fig. 5 SPARQL query of shared and different questions in Costco and Walgreens vaccination informed consent forms. a Query common questions.b Query different questions. The two screenshots show the query executions and results generated using the Protégé OWL editorLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 8 of 14As we mentioned before, in VICO, a patients answers to aquestion is represented as a component of a filled question-naire instance (Fig. 7). For example, in a Costco vaccinationinformed consent form, a patients No answer to the ques-tion whether the patient is allergic to egg is represented as:filled questionnaire for Costco vaccination consentand (has component some (question whetherallergic to egg and (has component some no answertext entity)))Table 1 Comparison of specific questions listed in Walgreens and Costco vaccination informed consentsNumber Question Walgreens/Costco1 question on vaccination in past 4 weeks Walgreens; Costco2 question on blood transfusion in past year Walgreens; Costco3 question whether allergic to vaccine Walgreens; Costco4 question on asthma or wheezing history Walgreens; Costco5 question on leukemia Walgreens; Costco6 question whether allergic to medication Walgreens; Costco7 seizure disorder question Walgreens; Costco8 question on cancer Walgreens; Costco9 question whether allergic to egg Walgreens; Costco10 X-ray treatment question Walgreens; Costco11 question whether allergic to latex Walgreens; Costco12 question on woman pregnancy Walgreens; Costco13 question whether currently sick Costco14 question on long-term heart disease Costco15 cortisone treatment question Costco16 immunocompromisation question Costco17 question whether allergic to food Costco18 question on reaction after immunization Costco19 question on whether fainted or felt dizzy after immunization Walgreens20 question on TB skin test in past 4 weeks Walgreens21 question whether currently sick with a moderate to high fever, vomiting/diarrhea Walgreens22 question on serious nasal condition Walgreens23 question on high-dose steroid therapy for longer than 2 weeks Walgreens24 question on thymus disease Walgreens25 question on current aspirin therapy Walgreens26 question on current antibiotics usage Walgreens27 question on history of thrombocytopenia or thrombocytopenic purpura Walgreens28 question on CSF leak Walgreens29 question on asplenia Walgreens30 question on azathioprine or 6-mercaptopurine usage Walgreens31 question on cochlear implant Walgreens32 question on high-dose methotrexate usage Walgreens33 question on home infusion Walgreens34 question on weekly injection Walgreens35 question on weekly injection of adalmumab Walgreens36 question on weekly injection of etanercept Walgreens37 question on weekly injection of infliximab Walgreens38 question whether recieving aspirin-containing therapy Walgreens39 question on current anti-malarial medication WalgreensLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 9 of 14Fig. 6 SPARQL query of vaccines that have egg protein allergen. This query was performed using both the Protégé 5.0.0 SPARQL Query plug-inand the Ontobee SPARQL query website (http://www.ontobee.org/sparql/). The SPARQL query codes are available in the Github repository. Thisfigure is a screenshot of the query execution and results using the Protégé SPARQL programFig. 7 DL query for patients who are allergic to egg. a Example of the answers of a Costco patients answers to questionnaire. b A DL query andits results. The DL query was performed using the Protégé OWL editorLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 10 of 14After the patients answers to the informed consentquestions were transformed in VICOs representation,we then performed queries to find out the patient whomay be in the danger of trigger a serious egg allergicreaction if given a specific vaccine, e.g., Afluria. In Fig. 7,an OWL Description Logics (DL) query written in theManchester OWL syntax, a user-friendly syntax forOWL DL [26], was shown and executed using DL QueryTab in the Protégé OWL editor [27]. Out of these twelvepatients answers, we identified four patients (one Costcopatient, one Walgreens patient, and two Manitobapatients) who answered that they were allergic to egg pro-teins. Therefore, these patients may not be recommendedfor vaccination with those vaccines containing eggallergen (Fig. 7).This simplified use case demonstrated how VICO canutilize, integrate prior knowledge from VO (Usage 2),and discover new knowledge (Usage 5) that the patientcannot be vaccinated with a vaccine that contains thechicken egg allergen (e.g., Aflura). In addition, this usecase queries patients answers (Usage 4) from threedifferent forms in a consistent way, demonstrating theadvantages of consistent representation and organizationof complex information in the vaccination informedconsent forms (see Usages 1 and 2).DiscussionThis paper introduces the development and applicationof a Vaccination Informed Consent Ontology (VICO).VICO represents 12 vaccination informed consent formsand corresponding screening questionnaires fromdifferent organizations, and over 150 questions inthese forms. The top level hierarchical structure andgeneral VICO design pattern are represented, followedby the description of five usages of VICO with exam-ples and use cases.VICO was developed by following the best practiceand recommended strategy of ontology reusing [15].Instead of coding everything from scratch, weimported related terms from both Vaccine Ontology(VO) and Informed Consent Ontology (ICO) intoVICO. The VO includes the information of licensedhuman vaccines in different countries and relatedvaccine characteristics. ICO is an ontology represent-ing informed consent that may be applied in broaderareas (e.g., clinical trials, clinical research) than vac-cine immunization. By importing related terms fromthese two BFO-based ontologies, VICO demonstratesa seamless integration of existing ontologies. On topof the imported terms, VICO can then focus on therepresentation of more specific entities such asvaccination informed consent forms from differentpharmacies and authorities. As demonstrated in theResults section, VICO has also provided a solution tomap vaccination consent questions to the commonly usedLOINC standard and generate logical axioms to directlylink questions to the diseases shown in the DiseaseOntology (DOID). Such a development strategy has beenproven successful in our VICO development.The VICO ontology can not only be used tosupport Semantics Web applications, but they are alsoapplicable to support relational database systems. Oneprimary reason is that relational database schemasalso need consistent, structured, and computer-understandable representation of the informed consentforms, questions, and the relations among questions,vaccines, and patients. VICO standardizes these termswith logical and textual definitions and consistentlyrepresents the relations among these terms. VICOalso efficiently includes prior knowledge. VICOprovides an integrative framework to well representorganize the complexity of different levels of informa-tion and knowledge. These usages are also needed fora typical relational database system to occupy. Itwould be very difficult to hard code all theinformation, relations, and prior knowledge in soft-ware code or relational database. By incorporatingVICO to a relational database system, such a systemwill obtain all the ontology benefits.VICO is also able to support the integration ofdifferent relational databases that most likely usedifferent database schemas. The 12 vaccinationinformed consent forms collected in our study can bematched to multiple relational database schemas fromdifferent organizations. Assuming we want to com-pare and integratively query the data from these orga-nizations, it is almost impossible to do with relationaldatabases since each organization very likely does notknow the database schemas from other organizations.However, with the support of VICO, such a task canbe done efficiently as shown in our paper. If eachdatabase schema understands the VICO contents,VICO can indeed serve as a hub system that makesdifferent relational database schemas understand eachother. This is another reason why VICO can supportrelational database system.The Semantic Web formats (e.g., RDF and OWL)provide an inherent capability of reasoning. Oneexample of such reasoning can be found in the refer-ence [28]. Our ontology provides a foundation forreasoning. New rules and equivalent classes can beadded to the ontology to support reasoning. The OWLontology is established based on the RDF technology.Individual RDF triple stores may exist independently andlack an appropriate way to communicate with each other.An ontology provides an effective way to address such asilo issue and semantically link different RDF triple storedata to support better data integration and queries.Lin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 11 of 14Linguistic polishing has been used in our process ofmapping questions and assigning ontology IDs. How-ever, linguistic polishing cannot achieve many featuresthat we would like to gain by representation in ontology.For example, linguistic polishing does not provide: (i) ahierarchy of questions, (ii) relations of questions todiseases, and (iii) classification of the questions based ondiseases and symptoms. Different questions may be asso-ciated with the same diseases and concerns. Differentquestionnaires may have different questions that arerelated to different vaccines and vendors. These onto-logical strategy is flexible and extensible to address thesequestions. In addition, ontology can easily reuse existingknowledge represented in other ontologies (e.g., VO).Most of these use cases cannot be achieved by simplequestion mapping and intersection of two sets ofquestions, and their analyses require logical axiomsspecifically defined in the ontology.Our VICO usage demonstration makes it feasible todevelop electronic interoperable vaccination informedconsent forms, e.g., by building up a list of questions forgenerating the questionnaire in the form. Furthermore,it is now possible for vaccine recipients or their legalrepresentatives to sign electronic informed consentforms. With advantage of sharing Electronic Health Rec-ord (EHR), patients allergic history, treatment history,and severe adverse event history will be integrated intopharmacys IT system, software programs can be devel-oped to automate the screening procedure before recipi-ent or his/her legal representative sign the informedconsent form. Clinical decision making system can helpto prevent avoidable contraindications prior to vaccin-ation. Compared to an electronic information manage-ment system without ontology support, the usage ofVICO for supporting an electronic vaccination informedconsent management system has many advantages. First,acting as a separate middle layer from the functionalitiesof the management system, the VICO ontology can beeasily updated without additional and costly softwareengineering work. Secondly, the ontology layer makes itpossible to easily perform decision support. Lastly,different informed consent data can be seamlessly inte-grated together using the same data representationontology, supporting data integration.To make VICO useful in practice, there are still severalissues to solve. Given the power of our VICO-basedSPARQL or a DL queries, in reality, it may be impossiblefor a clinical professional to type in a SPARQL or a DLquery. To bypass the difficulty of using DL or SPARQL,it is possible to develop template-based and naturallanguage-implemented query capability to easily queryontologies [2931]. A user-friendly query web interfacecan be provided to support convenient search for thosewho do not know any programming language. Oneclosely related work is the usage of ontologies for adap-tive questionnaires for clinical risk assessment [32, 33].Adaptive questionnaires are able to dynamically modifythe behavior of the structure of the questionnaire inresponse to user interaction. The context-sensitiveadaptation approach can use an ontology as the basic forrobust adapted information collection and patient riskassessment [32]. It is possible to use VICO as the onto-logical basis for adaptive questionnaire formation in aninteractive vaccination informed consent managementsystem. To make VICO usable in practice, it is importantto link VICO to existing standards and ontologies suchas LOINC and DOID, which has been incorporated inour study. Furthermore, we will also need to convincepharmacies and governments who perform or monitorthe vaccination procedures with more complete infor-mation in the ontology and empirical examples.VICO-based vaccination informed consent systemmay be linked to computerized immunization informa-tion systems (IIS, or called immunization registries) thatare developed to collect and consolidate vaccination datafrom multi health-care providers, generate automaticnotifications, and assess vaccination coverage. Such IIShave been widely established in the US [34]. For example,KSWebIZ Kansas Immunization Registry is a web-basedstatewide immunization registry that provides a central-ized birth to death database of complete and accurateimmunization records for all Kansas residents (https://www.contactkswebiz.info/). It would be ideal to eventuallylink electronic informed consent data to IIS directly, andVICO would facilitate such an effort.In the future, VICO can also be expanded to cover theinformation related to vaccine adverse events. Typically,such information is included in the VIS, and the patientsneed to be notified of the VIS before their consent. Insteadof the plain text described in the Vaccine InformationStatements (VIS) documents, an Ontology of VaccineAdverse Event (OVAE) was recently developed to repre-sent various adverse events for each licensed vaccine [35].OVAE will be imported into VICO, which allows recipientsbetter understanding of possible side effects of vaccinationsprior to vaccination, therefore, enhances the informedconsent process.In addition, the original methods identified in thisstudy can be applied to represent informed consentforms in other domains of research (e.g., biobanking[36]). Interoperability strategies applied in this study canbe incorporated into existing electronic health recordsor decision support systems as some stage in the future.It is noted that current study is still at the prototypestage to prove the rationale and feasibility of applyingontology to solve the issue of data and query disinte-gration in the area of vaccination inform consents fromdifferent vendors and agents. For the current stage ofLin et al. Journal of Biomedical Semantics  (2016) 7:20 Page 12 of 14development, manual efforts with software supports havebeen used to design the ontology, process the data, andimplement the use cases. For real usage, automatic com-putational systems such as natural language processingtext mining programs can further be used to improve theefficiency of informed consent content processing.Overall, such a strategy is promising to be used in realclinical setting.ConclusionsVICO ontologically represents various entities related tovaccination informed consent including vaccinationinformed consent forms, various questions in question-naires, and answers to those questions. Current VICOrepresents the contents of 12 vaccination informed con-sent forms from pharmacies, a collage health center, andthe government of Manitoba, Canada. Our SPARQL andOWL DL queries demonstrated that VICO could beused as a standard platform for consistently and system-atically representing vaccination informed consent formquestions, linking question answers to vaccine attributes,identifying potential vaccine contraindications, andenforcing safe vaccination procedures.AbbreviationsAE: adverse event; BFO: Basic Formal Ontology; DL query: Description Logicsquery; DOID: Disease Ontology; IAO: Information Artifact Ontology;ICO: Informed Consent Ontology; LOINC: Logical Observation IdentifiersNames and Codes; NCBO: The National Center for Biomedical Ontology;OAE: Ontology of Adverse Events; OBI: Ontology for BiomedicalInvestigations; OBO: The Open Biological and Biomedical Ontologies;OVAE: Ontology of Vaccine Adverse Events; OWL: Web Ontology Language;RDF: Resource Description Framework; SPARQL: SPARQL Protocol and RDFQuery Language; VAE: Vaccine Adverse Event; VICO: Vaccination InformedConsent Ontology; VIOLIN: Vaccine Investigation and Online InformationNetwork; VO: Vaccine Ontology.Competing interestsThe authors declare that they have no competing interests.Authors contributionsYL: Overall project design, VICO developer, SPARQL and DL-query design,implementation, and data interpretation. JZ: VICO developer, SPARQL andDL-query design, and data interpretation. YH: Overall project design, primaryVICO developer, design pattern generation, and data interpretation. Allcontributed to manuscript preparation and revision. All authors read andapproved the final manuscript.AcknowledgementsThis research was supported by a discretionary bridge fund in the Unit forLaboratory Animal Medicine in the University of Michigan Medical School.Author details1Unit for Laboratory Animal Medicine, University of Michigan Medical School,Ann Arbor, MI 48109, USA. 2Department of Microbiology and Immunology,University of Michigan Medical School, Ann Arbor, MI 48109, USA. 3Center forComputational Medicine and Bioinformatics, University of Michigan MedicalSchool, Ann Arbor, MI 48109, USA. 4Comprehensive Cancer Center, Universityof Michigan Medical School, 1301 MSRB III, 1150 W. Medical Dr., Ann Arbor,MI 48109, USA. 5Department of Genetics, University of PennsylvaniaPerelman School of Medicine, Philadelphia, PA 19104, USA.Received: 16 November 2015 Accepted: 5 April 2016RESEARCH Open AccessLarge scale biomedical texts classification: akNN and an ESA-based approachesKhadim Dramé*, Fleur Mougin and Gayo DialloAbstractBackground: With the large and increasing volume of textual data, automated methods for identifying significanttopics to classify textual documents have received a growing interest. While many efforts have been made in thisdirection, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not alwaysfreely available. Then, using only partial information to annotate these documents is promising but remains avery ambitious issue.Methods: We propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicitsemantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, itneeds to be improved to perform well in this specific classification problem which deals with partial information.Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for rankingthe labels. Additional features are also investigated in order to improve the classifiers performance. In addition,the combination of several learning algorithms with various techniques for fixing the number of relevant topics isperformed. On the other hand, ESA seems promising for this classification task as it yielded interesting results inrelated issues, such as semantic relatedness computation between texts and text classification. Unlike existingworks, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, ourESA-based method builds a standalone classifier. Furthermore, we investigate if the results of this method couldbe useful as a complementary feature of our kNN-based approach.Results: Experimental evaluations performed on large standard annotated datasets, provided by the BioASQorganizers, show that the kNN-based method with the Random Forest learning algorithm achieves goodperformances compared with the current state-of-the-art methods, reaching a competitive f-measure of 0.55 %while the ESA-based approach surprisingly yielded unsatisfactory results.Conclusions: We have proposed simple classification methods suitable to annotate textual documents using onlypartial information. They are therefore adequate for large multi-label classification and particularly in the biomedicaldomain. Thus, our work contributes to the extraction of relevant information from unstructured documents in orderto facilitate their automated processing. Consequently, it could be used for various purposes, including documentindexing, information retrieval, etc.Keywords: Biomedical text classification, Semantic indexing, Multi-label classification, k-nearest neighbours,Explicit semantic analysis, Information extraction, Machine learning* Correspondence: khadim.drame@u-bordeaux.frUniversity of Bordeaux, ERIAS, Centre INSERM U897, F-33000 Bordeaux,France© 2016 Dramé et al. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 DOI 10.1186/s13326-016-0073-1IntroductionThe amount of textual data is rapidly growing with anabundant production of digital documents, particularlyin the biomedical domain (biomedical scientific articles,medical reports, patient discharge summaries, etc.). Fur-thermore, these data are generally expressed in an un-structured form (i.e., in natural language), which makesits automated processing increasingly difficult. Thus, anefficient access to useful information is challenging. Todo so, a suitable representation of textual documents iscrucial. Controlled and structured vocabularies, such asthe Medical Subject Heading (MeSH®) thesaurus, arewidely used to index biomedical texts [1] and conse-quently to facilitate access to useful information [2, 3].As regards conceptual indexing, concepts defined in the-sauri or ontologies are often used to annotate documents.For example, the MEDLINE® citations are manuallyindexed by the National Library of Medicine® (NLM)indexers using the MeSH descriptors. Although the taskof annotators is now facilitated by a semi-automaticmethod [4], the rapid growth of biomedical literaturemakes manual-based indexing approaches complex, time-consuming and error-prone [5]. Thus, fully automatedindexing approaches seem to be essential. While manyefforts have been made to this end, indexing full biomed-ical texts according to specific segments of these texts,such as their title and abstract, remains a real challenge[6]. Furthermore, with the large amounts of data, usingonly partial information to annotate documents is promis-ing (reduction of computational cost).In this paper, we propose two classification methods fordiscovering and selecting relevant topics of new (unanno-tated) documents: a) a kNN-based approach and b) anESA-based approach. Our main contribution is to be ableto suggest relevant topics to any new document basedsolely on portion of it thanks to a classification modellearnt from a large collection containing several hundredsof thousands of previously annotated documents.Text classification is the process of assigning labels(categories) to unseen documents. The principle of thekNN-based approach is to consider the set of topics(MeSH descriptors, in this case) assigned manually tothe k most similar documents of the target document.Then, these topics are ordered by their relevance scoreso that the most relevant ones are used to classify thedocument. In a previous work [5], authors noted thatover 85 % of MeSH descriptors relevant for classifying agiven document are contained in its 20 nearest neigh-bours. This appears to better represent the documentsrather than what can be found in their title and abstractsolely.First, we have developed a method based on the vectorspace model (VSM) [7] to determine similar documents.The latter uses the TF.IDF (term frequency  inversedocument frequency) weighting scheme for representingdocuments by vectors constituted by unigrams they con-tain, and the cosine measure for retrieving the documentneighbours. Then, we have investigated different types offeatures and several ML algorithms for selecting relevanttopics in order to classify a given document.On the other hand, ESA [8] has yielded good results inrelated issues such as semantic relatedness computationbetween texts [8] and even the text classification [9]. Forthis reason, we propose to explore it using differentassociation measures in the context where only partial in-formation is exploited for classifying a whole document.Unlike most works in document classification, ourapproaches use only partial information (titles and ab-stracts) of documents in order to predict relevant topicsfor representing their full content. Since the content ofdocuments is not fully exploited, using large datasets forbuilding the classifiers could be useful for capturingmore information. For this reason, we used classifiersbuilt from large collections of previously annotated doc-uments. This is a very challenging task, which has moti-vated the recent launch of BioASQ: an internationalchallenge on large-scale biomedical semantic indexingand question answering1 [6].The rest of the paper is organized as follows. First,related work concerning biomedical document indexingand, more generally, multi-label classification is reviewedin Section 2. Then, the two proposed methods are detailedin Section 3. In Section 4, the experiments are shownwhile the results are described in Section 5 and discussedin Section 6. Conclusion and future work are finallypresented in Section 7.Related workThe identification of relevant topics from documents inorder to describe their content is a very important taskwidely addressed in the literature. In the biomedical do-main, the MTI (Medical Text Insdexer) tool [4] is one ofthe first attempts to index biomedical documents (MED-LINE citations) using controlled vocabularies. To mapbiomedical text to concepts from the Unified MedicalLanguage System® (UMLS) Metathesaurus - a systemthat includes and unifies more than 160 biomedical ter-minologies - the MTI tool uses the well-known conceptmapper MetaMap [8] and combines its results with thePubMed Related Citations algorithm [10]. The combin-ation of these methods results in a list of UMLS con-cepts which is then filtered and recommended to humanexperts for indexing citations. Recently, the MTI wasextended with various filtering techniques and ML algo-rithms in order to improve its performance [11]. Ruchhas designed a data independent hybrid system usingMeSH for automatically classifying biomedical texts [12].The first module is based on regular expressions to mapDramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 2 of 12texts to concepts while the second is based on a VSM[7] considering the vocabulary concepts as documentsand documents as queries. Then, the rankers of the twocomponents are merged to produce a final ranked list ofconcepts with their corresponding relevance scores. Hisresults showed that this method achieved good per-formances, comparable to ML-based approaches. Onelimitation of this system is that it may return MeSHconcepts which match partially the text [1].ML-based approaches are also proposed to deal withsuch a task. The idea is to learn a model from a trainingset constituted of already annotated documents and thento use this model to classify new documents. Trieschnigget al. [1] have presented a comparative study of six sys-tems which aim at classifying medical documents usingthe MeSH thesaurus. In their experiments, they showedthat the kNN-based method outperforms the others,including the MTI and the approach developed by Ruch[12]. In their work, the kNN classifier uses a languagemodel [13] to retrieve documents which are similar to agiven document. The relevance of MeSH descriptors isthe sum of the retrieval scores of documents annotated bythese descriptors among the neighbouring documents. Asimilar kNN-based approach has been proposed in [5]. Alanguage model is used to retrieve the neighbours of agiven document. Then, a learning-to-rank model [14] isused to compute relevance scores and consequently torank candidate labels2 collected from these documentneighbours. In this work, the number of labels to classify adocument is set to 25. Experiments on two small standarddatasets (respectively 200 and 1000 documents) showedthat it achieves better performances than the MTI tool.On the other hand, indexing biomedical documents inwhich each document of the dataset is assigned one orseveral categories (also called labels) can be assimilatedas a multi-label classification task. Multi-label classifica-tion (MLC) is increasingly studied and especially for textclassification purposes [15]. Several methods have beendeveloped to deal with this task [16, 17]. They can becategorized into two main approaches [15]: the problemtransformation approach [18] and the algorithm adapta-tion approach [17, 19]. The problem transformation ap-proach splits up a multi-label learning problem into a setof single-label classification problems whereas the algo-rithm adaptation approach adjusts learning algorithms toperform MLC.In MLC, the kNN-based approach is widely used. Thisapproach has been proven efficient for MLC in termsof simplicity, time complexity, computation cost andperformance [17]. Zhang and Zhou [19] proposed a ML-KNN (for Multi-Label kNN) method which extends thetraditional kNN algorithm and uses the maximum a pos-teriori principle to determine relevant labels of an unseeninstance. For an instance t, the ML-KNN identifies itsneighbours and estimates respectively the probabilitiesthat t has and has not a label l based on the training set,for each label l. Then, it combines these probabilities withthe number of neighbours of t having l as a category tocompute the confidence score of l. Spyromitros et al. [17]propose a similar method, named BR-KNN (for BinaryRelevance KNN), and two extensions of this method. Theproposed approach is an adaptation of the kNN algorithmusing a BR method which trains a binary classifier for eachlabel. Confidence scores for each label are computed usingthe number of neighbours among the k neighbours thatinclude this label. In [20], an experimental comparison ofseveral multi-label learning methods is presented. In thiswork, different approaches were investigated using variousevaluation measures and datasets from different appli-cation domains. In their experiments, authors showed thatthe best performing method is based on the RandomForest classifier [21]. Other recent works address MLCwith large number of labels [22]. Indeed, in many applica-tions, the number of labels used to categorize instances isgenerally very large. For example, in the biomedicaldomain, the MeSH thesaurus consisting of thousandsdescriptors (27,149 in the 2014 version) is often used toclassify documents. This large number of descriptors canaffect the effectiveness and performance of multi-labelmodels. To address this issue, a label selection based onrandomized sampling is performed [22].MethodsIn this section, we present the text classification ap-proaches developed in our work: a kNN-based approachand an ESA-based approach.The kNN-based approach: kNN-classifierThis approach consists of two steps. First, for a givendocument, represented by a vector of unigrams, its k mostsimilar documents are retrieved. To do so, the TF.IDFweighting scheme is used to determine the weights of dif-ferent terms in the documents. Then, the cosine similaritybetween documents is computed. Once the k nearest doc-uments of a target document are retrieved, the set oflabels assigned to them are used for training the classifiers(in the training step) or as candidates for classifying thedocument (in the classification step). Labels, which arethe instances here, are first represented by a set of attri-butes. Thereafter, ML algorithms are used to build modelswhich are then used to rank candidate labels for annotat-ing a given document. For ranking labels, different learn-ing algorithms are explored.Nearest neighbours retrievalOur kNN-based approach requires a collection of docu-ments previously annotated for the neighbours retrieval.For a given document, the aim is to retrieve its k mostDramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 3 of 12similar documents. To do so, like the PubMed RelatedCitations approach [10], we consider that two docu-ments are similar if they address the same topics. Thecosine similarity measure, which is commonly used intext classification and information retrieval (IR) with theVSM [7], is chosen for this purpose. The documents arefirst segmented into sentences and tokens, while stopwords are removed. From these pre-processed texts, allunigrams are extracted and normalized according to astemming technique [23]. Then, the cosine measureenables to compute similarity between documents, whichare represented by vectors of unigrams. Formally, letC = {d1, d2,, dn}, a collection of n documents, T ={t1, t2,, tm}, the set of terms appearing in the docu-ments of the collection and the documents di and djbeing represented respectively by the weighted vectorsdi = (w1i , w2i ,, wmi ) and dj = (w1j ,w2j ,,wmj ), their cosinesimilarity is defined by [12, 24]:Sim di; ; dj  ¼Xmk¼1wikwjkffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiXmkwik 2q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiXmkw jk 2rwhere wkl , is the weight of the term tk in the documentdl. It is the TF.IDF value of the term.In order to optimize the search, the documents in thesearch space are indexed beforehand using the opensource IR API Apache Lucene.3 The k-nearest neighboursretrieval thus becomes an IR problem where the targetdocument is the query to be processed.Collection of candidate labelsFor a given document, once its kNN are retrieved, alllabels assigned to these documents are gathered in orderto constitute a set of candidate labels likely to annotatethis document. Since this can be seen as a classificationproblem, we use ML techniques to rank these candidatelabels. Thus, classical classifiers are used to build classifi-cation models which are then exploited to determine therelevant labels for annotating any unseen document. Forthat purpose, candidate labels are used as training in-stances (in the training step) or instances to be classified(in the classification step).Feature extractionTo determine the relevance of a candidate label, it isrepresented by a vector of features (also called attributes).In the training step, its class is set to 1 if the label isassigned to the target document and otherwise 0 while inthe classification step, the model uses the label features todetermine its class. We defined six features based onrelated works [5, 17].For each candidate label, the number of neighbourdocuments to which it is assigned is used as a feature(Feature 1). This value represents an important clue todetermine the class of the label. Moreover, in the clas-sical kNN-based approach, it is the only factor used toclassify a new instance. In practice, a voting technique isused to assign the instance to the class that is the mostcommon among its k nearest neighbours.For each candidate label, the similarity scores betweenthe document to classify and its nearest neighbours an-notated with this candidate label are summed and thissum is another feature (Feature 2). Since the distancebetween a document and each of its neighbours is notthe same, we consider that the relevance of the labelsassigned to them for the target document is inverselyproportional to this distance. In other words, the closera document is to the target document, the more its asso-ciated labels are likely to be relevant for the latter. In [1],this is the only feature used to determine the relevancescores of candidate labels.Formally, like in [17], let L = {lj}, j = 1,, n, be the can-didate labels set of a new document d, and V = {di}, i =1,, k, its k nearest neighbours, the values of theseattributes for the label lj are respectively defined as:f 1 lj  ¼ 1kXni¼1assigned lj; di f 2 lj  ¼ 1kXnlj?disim d; dið Þwhere the binary function assigned(lj, di) returns 1 if thelabel lj is assigned to the document di, 0 otherwise;sim(d, di) is the similarity score between the documentsd and di and is computed using the cosine measure.For each candidate label, we also checked if all theconstituent tokens appear in the title and abstract ofthe document and consider it as the third feature(Feature 3). This binary feature has been chosen becauseit captures disjoint terms (terms constituted of disjointwords) which are frequent in the biomedical texts.In addition to these features, we computed two otherfeatures using term synonyms. Indeed, for indexingbiomedical documents, the MeSH thesaurus is commonlyused. The latter is composed of a set of descriptors (alsocalled main headings) organized into a hierarchical struc-ture. Each descriptor includes synonyms and relatedterms, which are known as its entry terms. Thus, for eachlabel (called descriptor here), we check whether one of itsentries appears in the document. If this is the case, thefourth binary feature (Feature 4) is set to 1 and thedescriptor frequency in the document is computed as avalue corresponding to the fifth feature (Feature 5), other-wise the two features are set to 0.Finally, another feature (Feature 6) is used to verifywhether a candidate label is contained in the documentsDramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 4 of 12title. Our assumption is that if a label appears in thetitle, it is relevant for representing this document.The relevance of each of these features is estimatedusing the information gain measure (Table 1). The firsttwo features mainly permit to compute relevance scoresof candidate labels.Classifier buildingTo build the classifiers, a labelled training set consistingof a collection of documents with their manually associ-ated labels is constituted. For each document in thetraining set, its nearest neighbours and their manuallyassigned labels are collected. Each label of this collectedset is considered as an instance for the training. Thus,for each label, its different features (see the previous sec-tion) are computed. Thereafter, labels obtained fromneighbours of the different documents of the training setare gathered to form the training data. Then, classifiersare built from this labelled training data. We have testedthe following classification algorithms: Naive Bayes (NB)[25], Decision Trees (DT also known as C4.5 in our case[24]), Multilayer Perceptron (MLP) and Random Forest(RF) [26]. We chose these classifiers as they have yieldedthe best performances in our tests.For the implementation of these classifiers, we use theWEKA4 (Waikato Environment for Knowledge Analysis)tool, which integrates many ML algorithms [27], includ-ing the four ones we have tested.Document classificationGiven a document to be classified, the candidate labelscollected from its neighbours are represented as thetraining ones (see the previous section). The trainedmodel is then used to estimate the relevance score ofeach candidate label. Indeed, the model computes, foreach candidate label, its probabilities to be relevant ornot. From these probability measures, the relevancescore of each label is derived. Candidate labels are thenranked according to their corresponding scores and theN top-scoring ones are selected to annotate the docu-ment, where N is determined using three differenttechniques.Selection of the optimal value of NIn order to determine the optimal value of N, we explorethree strategies:a) Initially, N is set as the number of labels having arelevance score greater than or equal to a thresholdarbitrarily set to 0.5. This strategy based only on therelevance score of the label regarding the documentis inspired by the original kNN algorithm.b) We then set the value of N as the average size(number of labels assigned) of the sets of labelscollected from the neighbours. This strategy hasbeen successfully used for extending the kNN-basedmethod proposed in [17].c) Finally, in the third strategy, we use the methoddescribed in [28]. The principle is to compare therelevance scores of successive labels of a list ofcandidate labels ranked in descending order fordetermining the cut off condition enabling todiscard the irrelevant or insignificant ones. Thisstrategy is defined by the following formula:siþ1si?iiþ 1þ ?where si is the relevance score of a label being at pos-ition i and ? a constant whose optimal value is deter-mined empirically.The ESA-based approachESA is an approach proposed for representing textualdocuments in a semantic way [8]. In this method, thedocuments are represented in a conceptual space consti-tuted of explicit concepts automatically extracted from agiven knowledge base.5 For this, statistical techniquesare used to explicitly represent any kind of text (simplewords, fragments of text, entire document) by weightedvectors of concepts. In the approach proposed in [8], thetitles of Wikipedia articles are defined as concepts. Thus,each concept is represented by a vector consisting of allterms (except stop words) that appear in the correspond-ing Wikipedia article. The weight of each word of thisvector is the association score between the term and thecorresponding concept. Theses scores are computed usingthe TF.IDF weighting scheme [29].At the end of this step, each concept is represented bya vector of weighted terms. Then, an inverted index,Table 1 Importance of each feature for the predictionaccording to the Information Gain measureFeature Description Information gainFeature 1 Number of neighbours in which thelabel is assigned0.16Feature 2 Sum of similarity scores between thedocument and all the neighboursdocument where the label appears0.17Feature 3 Check whether all constituted tokensof the label appear in the targetdocument0.01Feature 4 Check whether one of the labelentries appears in the target document0.03Feature 5 Frequency of the label if it iscontained in the document0.03Feature 6 Check if the label is contained inthe document title0.02Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 5 of 12wherein each term is associated with a vector of itsrelated concepts, is created. In this inverted index, theless significant concepts (i.e., concepts with low weight)for a vector are removed. The index is then used to clas-sify unseen textual documents.The classification process consists of two steps. For agiven document, it is first represented by a vector ofterms. The concepts corresponding to these terms arethen retrieved in the inverted index and merged to con-stitute a vector of concepts representing the document.The retrieved concepts are finally ranked according totheir relevance score in descending order. The mostrelevant ones are then selected. This process is illus-trated by Fig. 1.Formally, let T be a text, {ti} the terms appearing in Tand vi, their respective weights. Let kj, be the associationscore between the term ti and the concept cj with cj ?C, C the set of Wikipedia concepts. The weight of theconcept cj for the text T is defined by:W cj  ¼ Xwi?Tvi:kjOur ESA-based approach explores this technique inthe specific case where only partial information is con-sidered (i.e., the title and abstract in the case of scientificarticles). First, we assume the availability of concepts(generally defined in semantic resources) to be used fordocument classification as well as a labeled training setin which each document is assigned a set of concepts.Unlike the original ESA method where each article is as-sociated with a single concept, in our approach, eachdocument in the training set may be assigned one ormore concepts (also called labels here).From the training set, we use statistical techniques to es-tablish associations between labels and terms extractedfrom the texts. Thus, for each label, the unigrams that aremore strongly associated with it are used for its represen-tation. If the concepts are seen as documents, we face withan IR problem where the goal is to retrieve the most rele-vant documents (concepts) for a given query (a new docu-ment). Therefore, the classical IR models can be used torepresent documents and queries, but also to compute therelevance of a document with respect to a given query. Inthis work, the VSM is used to determine the most relevantconcepts for annotating the given document. Like in thekNN-based approach, the documents are processed usingthe following techniques: segmentation into sentences,tokenization, removal of stop words and normalizationusing the Porter's stemming algorithm [23].For computing the association scores between aconcept c and a term t, we experimented the followingmeasures: The TF.ICF measure (the TF.IDF scheme adapted toconcepts) [7]:TF :ICF t; cð Þ ¼ TF t; cð Þ  logNniwhere N is the total number of concepts, ni the numberof concepts associated with t. The factor TF(t, c) is thenumber of occurrences of t in the documents annotatedby the concept c and is defined by:TF t; cð Þ ¼Xd?Dcfreq t; dð Þdj jFig. 1 The process of the Explicit Semantic Analysis based approach. The two steps of the ESA-based approach are presented: the indexing stepand the classification stepDramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 6 of 12where freq(t, d) is the frequency of t in the documentd, |d| is the number of words of d and Dc the set ofdocuments annotated by the concept c. The Jaccard coefficient [30]:J t; cð Þ ¼ cocc t; tð Þocc tð Þ þ occ cð Þ?cocc t; cð Þwhere cocc(t, c) is the number of documents in whichthe concept c and the term t co-occur occ(c) is thenumber of documents annotated by the concept c andocc(t) is the number of documents in which the term tappears.Finally, to estimate the relevance of a concept toannotate a document, we use the following metric. Therelevance score of a concept c for a new document d isdefined by:Rel c; dð Þ ¼Xw?dTF :IDF t; dð Þ  score t; cð Þwhere score(w,c) is the association score between theterm t and the concept c and TF. IDF(t, d) is the TF.IDFvalue of the term in the document d.EvaluationIn order to assess the effectiveness of our approaches,we performed two different experiments: one in the con-text of the task 2a of the international BioASQ challengeto which we participated [31] and the second experimentconducted on a derived dataset from the BioASQ chal-lenge, as described below.DatasetsThe BioASQ organizers, within the 2014 edition, pro-vided a collection of over 4 million documents consti-tuted by only titles and abstracts of articles (called alsocitations), coming from specific scientific journals forthe task 2a of this challenge [6]. These documents, ex-tracted from the MEDLINE database, are annotated bydescriptors of the MeSH thesaurus.In addition, during the challenge, the organizers pro-vided each week PubMed® citations not yet annotatedwhich were used as test sets to evaluate the systems par-ticipating in the task 2a. Participants were asked to classifythese test sets using descriptors of the MeSH thesaurus.The test sets have subsequently been annotated byPubMed® human indexers for evaluating the proposals ofthe participating systems.ExperimentsFirst experimentFor the kNN retrieval, we used a dataset consisting of allarticles of this collection published since 2000 (2,268,724documents). The motivation for this choice is to discardold documents which are not annotated by descriptorsrecently added to the MeSH thesaurus (the MeSH the-saurus is regularly updated). This dataset is thereafterextended to the entire collection. For training the classi-fiers we randomly selected 20.000 articles out of thosepublished since 2013; the citations of the training set arediscarded from the former dataset. We assume thistraining set sufficient to capture relevant information forbuilding the classifiers.Only the kNN-based approach was used for our par-ticipation to the challenge. To assess this method, five ofthe different test sets provided by the challenge orga-nizers were used.Second experimentFor the second experiment, we first extracted all articlespublished since 2013 (133,770 documents) from the pre-vious dataset provided by the challenge organizers. Wethen selected randomly 20,000 documents to be used fortraining the classifiers and one thousand for constitutingthe test set. The data used to train the classifiers werethen extended to 50,000 documents, since we believed itcould improve the classification performances; usinglarge training dataset should enable the classifiers tocapture more information. The test collection was alsoincreased to 2,000 documents. Like in the training data-set, each document in the test set was assigned a set oflabels by PubMed® annotators. These manually assignedlabels were thus used to evaluate the results of our dif-ferent methods.Regarding the evaluation of our ESA-based approach,except the documents in the test set, the rest of the collec-tion (i.e., 4,430,399 documents) was exploited to computethe association scores between words and labels.Evaluation measuresAs previously said, indexing biomedical documents canbe assimilated to a multi-label classification (MLC) prob-lem. Instead of one class label, each document is assigneda list of labels. Thus, measures usually used for evaluatingindexing methods were adapted for the MLC context [15].The example-based precision (EBP) measures how manyof the predicted labels are correct while the example-based recall (EBR) measures how many of the manuallyassigned labels are retrieved. Since EBP and EBR evaluatepartially the performance of a method, the example-basedf-measure (EBF) combines both measures for a globalevaluation. The accuracy (Acc) is also a complementarymeasure [15]. These measures are computed as follows.Let Yi be, the set of true labels (labels manually assignedto the documents), Zi the set of predicted labels and mthe size of the test set:Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 7 of 12EBP ¼ 1mXmi¼1Y i?Zij jZij jEBR ¼ 1mXmi¼1Y i?Zij jY ij jEBF ¼ 1mXmi¼1Y i?Zij jZij j þ Y ij jAcc ¼ 1mXmi¼1Y i?Zij jZi ?Y ij jThese measures, in addition to being common, arerepresentative and enable the global evaluation of thesystems performances. The results of our two approachesare presented in the next section.Experiment environmentIn our different experiments, we used the computingfacility of the Bordeaux Mésocentre, Avakas,6 whichincludes: the compute nodes c6100 (x264), which are themachines on which algorithms are executed.They have the following characteristics:? Two processors of hexa-cores (12 cores pernode) Intel Xeon X5675 @ 3.06 GHz;? 48 GB RAM. the computation nodes bigmem R910 (x4), whichhave more memory and whose cores have slowerprocessors:? 4 processors of 10 cores (40 cores per node)Intel Xeon E7-4870 @ 2.4 GHz;? 512 GB RAM.In our case, we used two computation nodes c6100,which provide 48 GB of RAM and 24 cores Intel XeonX5675.ResultsResults of the kNN-based approachExperiment within the BioASQ challengeFirst, we present the results obtained in the task 2a ofthe BioASQ challenge. For that purpose, we report re-sults of batch 3 in terms of EBP, EBR and EBF. Wechose only these measures since they are representativeand allow estimating the global performance of the MLCmethods. Table 2 shows the results of our best systemusing the kNN-based approach and the ones whichobtained the highest measures within the different testsof batch 3. In tests 2 and 5, our best system uses a NaïveBayes classifier and selects only labels having a confi-dence score greater than or equal to 0.5 while in theothers, the best system sets N to the average size of thesets of labels collected from the neighbours. In mostcases, using this value for N yields better or similar re-sults than the other strategy. In the challenge, we do notuse the automatic cut-off method to fix the number oflabels as described in [28] but in the second experiment,this technique is explored.Second experimentWe evaluate our kNN-based approach with differentconfigurations in the test set of the second experimentand compare the achieved performances. Thus, we testcombinations of various classifiers with different tech-niques for determining the number of labels for annotat-ing a given document. The evaluation of configurationswith the two best classifiers in our experiments, NB andRF, are presented in Table 3. The parameter k is empiric-ally set to 25 using a cross-validation technique. Whenthe minimal score threshold is used, the precision oftenincreases significantly, mainly with the RF classifier butthe recall is lower. Regarding the average size strategy,it yields a good recall but the precision decreasesslightly. In this case, the results of both classifiers aresimilar but the RF one slightly outperforms the NB clas-sifier. The best results are achieved with the cut-offTable 2 Results of our kNN-based system and the best systemsparticipating in the BioASQ challenge on the different tests ofthe batch 3Test Number of documents System EBP EBR EBFTest 1 2,961 kNN-Classifier 0.55 0.48 0.49Best 0.59 0.62 0.58Test 2 5,612 kNN-Classifier 0.52 0.50 0.48Best 0.62 0.60 0.60Test 3 2,698 kNN-Classifier 0.55 0.49 0.49Best 0.64 0.63 0.62Test 4 2,982 kNN-Classifier 0.49 0.55 0.49Best 0.63 0.62 0.62Test 5 2,697 kNN-Classifier 0.50 0.53 0.48Best 0.64 0.61 0.61Table 3 Results of the kNN-Classifier according to the classifierand strategy used for fixing N: a) 0.5 as the minimal confidencescore threshold, b) the average size of the sets of labelscollected from the neighbours and c) the cut-off method.A training set of 20,000 documents is usedStrategy Classifier EBP EBR EBFa) NB 0.58 0.49 0.49RF 0.74 0.34 0.43b) NB 0.51 0.54 0.51RF 0.52 0.54 0.52c) NB 0.56 0.52 0.51RF 0.61 0.52 0.53Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 8 of 12method which balances both precision and recall, andyields the best F-measure. Except for the minimumthreshold technique where the NB classifier results arebetter, the best F-measure is achieved with the RF classi-fier. The DT (C4.5 algorithm of Weka) and the Multi-layer Perceptron (MLP) classifiers have also been testedbut their results are less interesting. The former yieldslower performances while the latter performs very slowlyand gets results comparable to the RF ones. The MLPclassifier requires more CPU and memory during thetraining process.When the training set is raised from 20,000 to 50,000,the performances are slightly improved in two test sets(one of 1000 documents and another of 2000). Table 4presents the results of the different classifiers in thislarger training set. The value of ? (constant used in thestrategy based on label scores comparison - strategy c- foroptimizing N) also affects the classification performance.The lower the value of ?, the higher the precision is butthe lower the recall is and vice versa. In these experiments,we set ? to 1.6 which yields the best results using cross-validation techniques. Furthermore, we note that whenthe classifiers are trained on this extended dataset, theyyield similar performances but the RF classifier slightlyoutperforms the others. Table 5 gives an example of labelssuggested for classifying the document having the PMID23044786 (Table 6) with the kNN-based approach.In terms of training time, NB, DT and RF classifiersperformed similarly with respectively 4, 6 and 9 minonce data were represented in suitable format for Weka(e.g. ARFF format (Attribute-Relation File Format)). Thepre-processing step (retrieval of neighbours and compu-tation of features values) however takes more time (1 hand 43 min). Note that since we have different types(binary and numeric) of attributes, we discretize the lat-ter in nominal attributes. The MLP classifier is, mean-while, very costly in terms of training time (23 h).Results of the ESA-based approachAfter processing the training set composed of a collec-tion of 4,432,399 documents (titles and abstracts), weobtain 1,630,405 distinct words and 26,631 descriptorsassigned to these documents among the 27,149 MeSHdescriptors (98.1 %). To simplify the computation andoptimize the results of the classification, each concept isrepresented by a vector consisting of 200 terms, whichare the most strongly associated with it. Only termsappearing in at least five documents are considered. Ourchoice is motivated by the will to simplify the scorescomputation by excluding the less representative terms.Here, since we used test sets already labelled, the num-ber of concepts which are relevant to annotate the docu-ment is known and is used; therefore, EBP and EBR areequivalent; thus we only report the EBF and the accuracymeasures.After evaluating the ESA-based approach, we note, asin previous work, that its performance varies dependingon the measure used to estimate the association scoresbetween words and concepts. This behaviour is illustratedin Table 7 where the Jaccard measure yields the bestresults.DiscussionWhile textual classification has been widely investigated,few approaches are currently able to efficiently handlelarge collections of documents, in particular when only aportion of the information is available. This is a challen-ging task, particularly in the biomedical domain.Our experiments show that our kNN-based approachis promising for biomedical documents classification inthe context of a large collection. Our results confirm thefindings presented in [1], where among the multipleclassification systems, the kNN-based one yielded thebest results. If we compare our method with the latter, weuse more advanced features to determine the relevance ofTable 4 Results of the kNN-Classifier according to the classifierusing the cut-off method with a training set of 50,000 documentsClassifier EBP EBR EBF AccNB 0.59 0.54 0.54 0.39RF 0.62 0.54 0.55 0.41C4.5 0.63 0.52 0.54 0.39MLP 0.64 0.46 0.51 0.36Table 5 Labels generated by the kNN-Classifier with theircorresponding relevance scores for the document having the23044786 PMIDLabels Relevance Manual validationHumans 0.99 YesPostoperative care 0.75 YesFemale 0.60 YesMale 0.60 YesMiddle aged 0.32 YesGeneral surgery 0.32 YesMedical errors 0.32 YesPatient care team 0.32 NoPostoperative complications 0.32 NoAdult 0.26 YesSafety management 0.26 NoAged 0.25 YesProspective studies 0.21 YesLength of stay 0.21 NoPatient safety 0.20 YesSurgical procedures, operative 0.20 NoDramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 9 of 12a candidate label. Indeed, Trieschnigg and his colleaguesdetermine the relevance of a label by summing the re-trieval scores of the k neighbour documents that areassigned to the label [1]. In our method, this sum is onlyconsidered as one feature among others for determiningthe confidence scores of labels. While the results of ourmethod do not outperform the extended (and improved)MTI system [11] which is currently used by the NLMcurators, it gets promising results (0.49 against 0.56of F-measure). A direct comparison with the methodproposed in [5] is not simple since the authors usedan older collection than the official datasets providedin the BioASQ challenge, which are recent and anno-tated with descriptors of the recent MeSH thesaurus(2014 version). Similarly to their experiments, whenour method is evaluated on 1,000 randomly selecteddocuments, it outperforms this method (0.55 against0.50 for the F-measure). But a comparison with theirrecent results in the first challenge of BioASQ [28]where they integrated the MTI outputs, their systemperforms better than ours (F-measure of 0.56 against0.49). Compared with the two approaches proposed in[32], one based on the MetaMap tool [33] and anotherusing IR techniques, our method gets better results (0.49against 0.42 for the F-measure). Our approach outper-forms also the hierarchical text categorization approachproposed in [34].As part of our participation in the challenge, the NBclassifier is combined with the average size of labelsassigned to the neighbours to determine relevant descrip-tors for a given document. In the second experiment, wenote however that a combination of RF with the cut-offtechnique proposed in [28] yields better results [35]. Amore recent evaluation of our kNN-based approach usinga large dataset (50,000 documents) for training the classi-fiers shows that it provides better performances, com-parable to the best methods described in the literature(with an f-measure of 0.55). Moreover, unlike the exten-ded MTI system [11], we do not use any specific filteringrules. This makes our approach generic and its reuse inother domains straightforward. A comparison of our basickNN-based system (trained on 20,000 documents, andimproved later) to the performing classification systems[3638], which also participated in the 2014 BioASQchallenge [31] and the baseline (extended MTI) [11] isshown in Table 8. The two best systems, Antinomyra [36]and L2R [38], rely on the learning to rank (LTR) method.Table 6 Example of a PubMed® (23044786) citation manually annotated by human indexers using MeSH descriptors. This is anexample of a PubMed citation, consisting of a title and an abstract, with MeSH descriptors manually selected by indexers forannotating itTitle An observational study of the frequency, severity, and etiology of failures in postoperative care after majorelective general surgeryAbstract Objective:To investigate the nature of process failures in postoperative care, to assess their frequency and preventability,and to explore their relationship to adverse events.Background:Adverse events are common and are frequently caused by failures in the process of care. These processes areoften evaluated independently using clinical audit. There is little understanding of process failures in terms oftheir overall frequency, relative risk, and cumulative effect on the surgical patient.Methods:Patients were observed daily from the first postoperative day until discharge by an independent surgeon.Field notes on the circumstances surrounding any non routine or atypical event were recorded. Field noteswere assessed by 2 surgeons to identify failures in the process of care. Preventability, the degree of harm causedto the patient, and the underlying etiology of process failures were evaluated by 2 independent surgeons.Results:Fifty patients undergoing major elective general surgery were observed for a total of 659 days of postoperativecare. A total of 256 process failures were identified, of which 85% were preventable and 51% directly led topatient harm. Process failures occurred in all aspects of care, the most frequent being medication prescribingand administration, management of lines, tubes, and drains, and pain control interventions. Process failuresaccounted for 57% of all preventable adverse events. Communication failures and delays were the mainetiologies, leading to 54% of process failures.Conclusions:Process failures are common in postoperative care, are highly preventable, and frequently cause harm topatients. Interventions to prevent process failures will improve the reliability of surgical postoperative care andhave the potential to reduce hospital stay.MeSH descriptors assignedmanually to the citationAdult, Aged, Aged, 80 and over, Digestive System Surgical Procedures*, Elective Surgical Procedures*, Female,General Surgery, Hospitals, Teaching, Urban, Humans, Interprofessional Relations, London, Male, Medical Errors,Medical, Errors, Middle Aged, Outcome and Process Assessment (Health Care)*, Patient Safety, Postoperative,Care, Postoperative Care, Prospective StudiesTable 7 Results of the ESA-based approach according to theassociation scoreAssociation score EBF AccJaccard coefficient 0.26 0.16TF.ICF 0.22 0.13Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 10 of 12The former extends features generated from the neigh-bour document retrieval with binary classifiers and theresults of the MTI and uses then the LTR method to rankthe candidate labels. Meanwhile, the other system, com-bines information obtained by the neighbours retrieval,binary classifiers and the MTI results as features and alsouses the LTR for the ranking. The Hippocrates systempresented in [37] only relies on binary SVM (SupportVector Machine) classifiers and trains them on a largedataset (1.5 million documents) in contrast to ourbasic kNN approach trained on 20,000 documents.Note that these three systems use binary classifiersfor building a model for each label [31]. These sys-tems require therefore considerable resources in terms ofcomputation and storage compared to our kNN-basedapproach.For the kNN retrieval, we have investigated the cosinesimilarity which is widely used in IR. It would be inter-esting to combine this measure with domain knowledgeresources, such as ontologies, to overcome the limitationof similarity computation based only on common words.The second method based on the ESA, meanwhile, yieldsvery low performances comparable to basic methods usinga simple correspondence between the text and the seman-tic resource inputs. Thus, although the ESA technique hasshown interesting results in text classification [9], it doesnot seem appropriate for our targeted classification prob-lem where only partial information is available. Indeed, tocompute the association scores between a term and alabel, this method exploits the occurrences of this term inthe documents annotated by the label. However, in thisspecific classification problem, labels used to annotate adocument are not always explicitly mentioned in the later.Documents are short and it is thereby unlikely that theycontain mentions of all relevant labels. It is worth men-tioning that in our approach, each concept is representedby a vector consisting of 200 terms, and only termsappearing in at least five documents are considered.For example, the most associated stemmed terms (withtheir corresponding Jaccard scores) to the label Body MassIndex are: index (0.1), waist (0.087), mass (0.079),bodi (0.077), circumfer (0.068), anthropometr (0.062),fat (0.059), adipos (0.048), smoke (0.039), weight (0.038),nutrit (0.037).Note that we do not use the large Wikipedias know-ledge base, like the work presented in [8], for the con-ceptual representation of documents since most of theMeSH descriptors cannot be directly mapped to thisresource. Furthermore, contrary to existing works [9],which use ESA for enriching the bag-of-words approachwith additional knowledge-based features, our ESA-basedmethod builds a standalone classifier. However, this ap-proach will be explored in the future in order to enrichthe features and consequently improve the performanceof our k-NN approach.ConclusionIn this paper, we have described two approaches forimproving the classification of large collections of bio-medical documents. The first one is based on the kNNalgorithm while the second approach relies on the ESAtechnique. The former uses the cosine measure with theTF.IDF weighting method to compute similarity betweendocuments and therefore to find the nearest neighboursfor a given document. Simple classification methods de-termine the most relevant labels from a set of candidatesof each document. We have investigated an importantfeature of the classification problem: the decision bound-ary which permits to determine the relevant label(s) fora target document. Thus, instead of using voting tech-niques like in the classical kNN algorithm, ML methodswere used to classify documents. The latter is based onthe ESA technique which exploits associations betweenwords and labels.Thanks to an evaluation on standard benchmarks, wenoted that the kNN based method using the RF classifierwith the cut-off method yielded the best results. We alsonoted that this approach achieved promising performancescompared with the best existing methods. In contrast, ourfindings suggest that the ESA is not suitable for classifyinga large collection of documents when only partial informa-tion is available.For indexing purpose, the representation of docu-ments as bags of words is limited since similarity be-tween the latter is only based on the words theyshare. Therefore, to improve the performance of ourkNN-based approach, we plan to use a wide biomed-ical resource, such as the UMLS Metathesaurus, forcomputing the similarity between documents (exploit-ation of synonyms and relations) and thus overcomethis limitation. Other features and similarity measureswill be studied to improve the performances of ourmethod.Table 8 Comparison of our kNN-Classifier used for participatingin the challenge with the best systems and the MTI baseline onthe test set of the week 2 of batch 3 consisting of 3009 documents.The used measures are: example-based precision (EBP),example-based recall (EBR), example-based f-measure (EBF)and micro f-measure (MiF) (Source BioASQ 2014)Systems EBP EBR EBF MiFAntinomyra [36] 0.59 0.62 0.59 0.60L2R [38] 0.59 0.60 0.58 0.59Hippocrates [37] 0.59 0.60 0.57 0.59MTI 0.59 0.58 0.56 0.57kNN-Classifier 0.55 0.49 0.49 0.51Dramé et al. Journal of Biomedical Semantics  (2016) 7:40 Page 11 of 12Endnotes1http://bioasq.lip6.fr2Labels are categories used to classify documents3http://lucene.apache.org/core/4http://www.cs.waikato.ac.nz/ml/weka/5Wikipedia in most cases6http://www.mcia.univ-bordeaux.fr/index.php?id=45AcknowledgementsThe work presented in this paper is supported by the French Fondation PlanAlzheimer. The authors would like to thank the BioASQ 2014 challengeorganizers who provided the datasets used in this study for evaluating theclassification methods. They would also like to thank the anonym reviewersof the previous version of our paper in the (Symposium on Semantic Miningin Biomedicine) 2014.Authors' contributionsKD, FM and GD all participated in designing the methods and contributed tothe results analysis. KD performed the experiments, discussed the results anddrafted the manuscript. GD and FM participated in the correction of themanuscript. All authors read and approved the final version of themanuscript.Competing interestsThe authors declare that they have no competing interest.Received: 1 March 2015 Accepted: 8 May 2016